0|17|Public
40|$|The {{comparative}} {{thermal analysis}} of theoretical and {{experimental studies of}} modified indirect evaporative cooler having cross flow heat exchanger with one fluid mixed and the <b>other</b> <b>unmixed</b> is presented in this research paper. A heat and mass transfer mathematical model is developed to simulate the properties of indirect evaporative cooler. The theoretical result analysis was done by plotting the curves between various performance parameters. This work presents the fabrication and experiments carried out on the indirect evaporative cooler at various outdoor air conditions. The data acquired by experiment were analyzed by plotting the curves between various performance parameters. The theoretical and experimental results were compared and analyzed. The theoretical model {{can be used to}} predict the performance of modified indirect evaporative cooler...|$|R
5000|$|Colored people {{consist of}} three mixed race populations in South Africa {{who were given}} more social {{privilege}} than <b>other,</b> <b>unmixed,</b> indigenous African groups. During the apartheid era, {{in order to keep}} divisions and maintain a race-focused society, the government used the term Coloured to describe one of the four main racial groups identified by law: Blacks, Whites, Coloureds and Indians. (All four terms were capitalised in apartheid-era law.) Many Griqua began to self-identify as [...] "Coloureds" [...] during the apartheid era. There were certain advantages in becoming classified as [...] "Coloured". For example, Coloureds did not have to carry a dompas (an identity document designed to limit the movements of the non-white populace), while the Griqua, who were seen as another indigenous African group, did.|$|R
40|$|International audienceThis paper proposes an {{unsupervised}} Bayesian algorithm for unmixing successive hyperspectral images while {{accounting for}} {{temporal and spatial}} variability of the endmembers. Each image pixel is modeled as a linear combination of the end-members weighted by their corresponding abundances. Spatial endmember variability is introduced by considering the normal compositional model that assumes variable endmembers for each image pixel. A prior enforcing a smooth temporal variation of both endmembers and abundances is considered. The proposed algorithm estimates the mean vectors and covariance matrices of the endmembers and the abundances associated with each image. Since the estimators are difficult to express in closed form, we propose to sample according to the posterior distribution of interest and use the generated samples to build estimators. The performance of the proposed Bayesian model and the corresponding estimation algorithm is evaluated by comparison with <b>other</b> <b>unmixing</b> algorithms on synthetic images...|$|R
40|$|Abstract—This paper studies a new Bayesian unmixing {{algorithm}} for hyperspectral images. Each pixel of {{the image}} is modeled as a linear combination of so-called endmembers. These endmembers {{are supposed to be}} random in order to model uncertainties regarding their knowledge. More precisely, we model endmembers as Gaussian vectors whose means have been determined using an endmember extraction algorithm such as the famous N-finder (N-FINDR) or Vertex Component Analysis (VCA) algorithms. This paper proposes to estimate the mixture coefficients (referred to as abundances) using a Bayesian algorithm. Suitable priors are assigned to the abundances in order to satisfy positivity and additivity constraints whereas conjugate priors are chosen for the remaining parameters. A hybrid Gibbs sampler is then constructed to generate abundance and variance samples distributed according to the joint posterior of the abundances and noise variances. The performance of the proposed methodology is evaluated by comparison with <b>other</b> <b>unmixing</b> algorithms on synthetic and real images. Index Terms—Bayesian inference, hyperspectral images, Monte Carlo methods, normal compositional model, spectral unmixing...|$|R
40|$|This paper studies a new Bayesian unmixing {{algorithm}} for hyperspectral images. Each pixel of {{the image}} is modeled as a linear combination of so-called endmembers. These endmembers {{are supposed to be}} random in order to model uncertainties regarding their knowledge. More precisely, we model endmembers as Gaussian vectors whose means have been determined using an endmember extraction algorithm such as the famous N-finder (N-FINDR) or Vertex Component Analysis (VCA) algorithms. This paper proposes to estimate the mixture coefficients (referred to as abundances) using a Bayesian algorithm. Suitable priors are assigned to the abundances in order to satisfy positivity and additivity constraints whereas conjugate priors are chosen for the remaining parameters. A hybrid Gibbs sampler is then constructed to generate abundance and variance samples distributed according to the joint posterior of the abundances and noise variances. The performance of the proposed methodology is evaluated by comparison with <b>other</b> <b>unmixing</b> algorithms on synthetic and real images...|$|R
40|$|This chapter {{discusses}} some algorithms {{for the use}} of non-negativity constraints in unmixing problems, including {{positive matrix}} factorization, nonnegative matrix factorization (NMF), and their combination with <b>other</b> <b>unmixing</b> methods such as non-negative independent component analysis and sparse non-negative matrix factorization. The 2 D models can be naturally extended to multiway array (tensor) decompositions, especially non-negative tensor factorization (NTF) and non-negative tucker decomposition (NTD). The standard NMF model has been extended in various ways, including semi-NMF, multilayer NMF, tri-NMF, orthogonal NMF, nonsmooth NMF, and convolutive NMF. When gradient descent is a simple procedure, convergence can be slow, and the convergence can be sensitive to the step size. This can be overcome by applying multiplicative update rules, which have proved particularly popular in NMF. These multiplicative update rules have proved to be attractive since they are simple, do not need the selection of an update parameter, and their multiplicative nature, and non-negative terms on the RHS ensure that the elements cannot become negative. © 2010 Elsevier Ltd. All rights reserved...|$|R
40|$|This paper {{addresses}} {{the problem of}} minimizing a convex cost function under non-negativity and equality constraints, {{with the aim of}} solving the linear unmixing problem encountered in hyperspectral imagery. This problem can be formulated as a linear regression problem whose regression coefficients (abundances) satisfy sum-to-one and positivity constraints. A normalized scaled gradient iterative method (NSGM) is proposed for estimating the abundances of the linear mixing model. The positivity constraint is ensured by the Karush Kuhn Tucker conditions whereas the sum-to-one constraint is fulfilled by introducing normalized variables in the algorithm. The convergence is ensured by a one-dimensional search of the step size. Note that NSGM can be applied to any convex cost function with non negativity and flux constraints. In order to compare the NSGM with the well-known fully constraint least squares (FCLS) algorithm, this latter is reformulated in term of a penalized function, which reveals its suboptimality. Simulations on synthetic data illustrate the performances of the proposed algorithm in comparison with <b>other</b> <b>unmixing</b> algorithms and, more particulary, demonstrate its efficiency when compared to the popular FCLS. Finally, results on real data are given...|$|R
50|$|Similar to Doyle's {{releases}} when with Hed Kandi, the CD compilations {{are mostly}} divided into genres with some crossover. All releases contain 3 discs except Fierce Disco V and VI, which contain 2 discs, and A Little Fierce, which contains 1 disc. Digital Angel 2006, Es Vive and A Little Fierce are mixed, while the <b>other</b> releases are <b>unmixed.</b>|$|R
40|$|We {{present a}} highly {{sensitive}} and accurate method for quantitativedetection and characterization of noninteracting or weakly interactinguniaxial single domain particles (UNISD) in rocks and sediments. Themethod {{is based on}} high-resolution measurements of first-order reversalcurves (FORCs). UNISD particles have a unique FORC signature that can beused to isolate their contribution among other magnetic components. Thissignature has a narrow ridge along the H(c) axis of the FORC diagram,called the central ridge, which {{is proportional to the}} switching fielddistribution of the particles. Therefore, the central ridge is directlycomparable with other magnetic measurements, such as remanentmagnetization curves, with the advantage of being fully selective to SDparticles, rather than other magnetic components. This selectivity isunmatched by <b>other</b> magnetic <b>unmixing</b> methods, and offers usefulapplications ranging from characterization of SD particles forpaleointensity studies to detecting magnetofossils and ultrafineauthigenically precipitated minerals in sediments. ...|$|R
40|$|A new hyperspectral unmixing {{algorithm}} which considers endmember variability is presented. In {{the proposed}} algorithm, the endmembers {{are represented by}} correlated random vectors using the stochastic mixing model. Currently, there is no published theory for selecting the appropriate distribution for endmembers. The proposed algorithm first uses a linear transformation called material signature orthonormal mapping (MSOM), which transforms the endmembers to correlated Gaussian random vectors. The MSOM transformation reduces computational requirements by reducing the dimension and improves discrimination of endmembers by orthonormalizing the endmember mean vectors. In the original spectral space, the automated endmember bundles (AEB) method extracts a set of spectra (endmember set) for each material. The mean vector and covariance matrix of each endmember estimated directly from endmember sets in the MSOM space. Second, a new maximum likelihood method, called NCM_ML, is proposed which estimates abundances in the MSOM space using the normal compositional model (NCM). The proposed algorithm is evaluated and compared with <b>other</b> state-of-the-art <b>unmixing</b> algorithms using simulated and real hyperspectral images. Experimental results demonstrate that the proposed unmixing algorithm can unmix pixels composed of similar endmembers in hyperspectral images {{in the presence of}} spectral variability more accurately than previous methods...|$|R
40|$|OSP {{has been}} used widely in {{detection}} and abundance estimation for about twenty years. But it can’t apply nonnegative and sum-to-one constraints when being used as an abundance estimator. Fully constrained least square algorithm does this well, but its time cost increases greatly {{as the number of}} endmembers grows. There are some tries for unmixing spectral under fully constraints from different aspects recently. Here in this paper, a new fully constrained unmixing algorithm is prompted based on orthogonal projection process, where a nearest projected point is defined onto the simplex constructed by endmembers. It is much easier, and it is faster than FCLS with the mostly same unmixing results. It is also compared with <b>other</b> two constrained <b>unmixing</b> algorithms, which shows its effectiveness too. Keywords:Orthogonal projection, Fully constrained unmixing 1...|$|R
40|$|Hyperspectral unmixing is an {{important}} technique for estimating fractions of various materials from remote sensing imagery. Most unmixing methods make the assumption that no prior knowledge of endmembers is available before the estimation. This is, however, not true for some unmixing tasks for which part of the endmember signatures may be known in advance. In this paper, we address the hyperspectral unmixing problem with partially known endmembers. We extend nonnegative-matrix-factorization-based unmixing algorithms to incorporate prior information into their models. The proposed approach uses the spectral signature of known endmembers as a constraint, among <b>others,</b> in the <b>unmixing</b> model, and propagates the knowledge by an optimization process which minimizes {{the difference between the}} image data and the prior knowledge. Results on both synthetic and real data have validated the effectiveness of the proposed method and have shown that it has outperformed several state-of-the-art methods that use or do not use prior knowledge of endmembers. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|R
40|$|The goal of source {{separation}} is {{to detect and}} extract the individual signals present in a mixture. Its application to sound signals and, in particular, to music signals, is of interest for content analysis and retrieval applications arising {{in the context of}} online music services. <b>Other</b> applications include <b>unmixing</b> and remixing for post-production, restoration of old recordings, object-based audio compression and upmixing to multichannel setups. This work addresses the task of source separation from monaural and stereophonic linear musical mixtures. In both cases, the problem is underdetermined, meaning that there are more sources to separate than channels in the observed mixture. This requires taking strong statistical assumptions and/or learning a priori information about the sources in order for a solution to be feasible. On the other hand, constraining the analysis to instrumental music signals allows exploiting specific cues such as spectral and temporal smoothness, note-based segmentation and timbre similarity for the detection and extraction of sound events...|$|R
40|$|Advanced Very High Resolution Radiometer imagery {{provides}} {{frequent and}} low-cost {{coverage of the}} earth, but its coarse spatial resolution (approx. 1. 1 km by 1. 1 km) does not lend itself to standard techniques of automated categorization of land cover classes because the pixels are generally mixed; that is, {{the extent of the}} pixel includes several land use/cover classes. Unmixing procedures were developed to extract land use/cover class signatures from mixed pixels, using Landsat Thematic Mapper data as a source for the training set, and to estimate fractions of class coverage within pixels. Application of these unmixing procedures to mapping forest clearcuts and regrowth in Oregon indicated that unmixing is a promising approach for mapping major trends in land cover with AVHRR bands 1 and 2. Including thermal bands by unmixing AVHRR bands 1 - 4 did not lead to significant improvements in accuracy, but experiments with unmixing these four bands did indicate that use of weighted least squares techniques might lead to improvements in <b>other</b> applications of <b>unmixing...</b>|$|R
40|$|The authors {{evaluated}} {{how well}} two 300 -hp mixer pumps would mix {{solid and liquid}} radioactive wastes stored in Hanford double-shell Tank 241 -AZ- 102 (AZ- 102) and confirmed the adequacy of a three-inch (7. 6 -cm) pipeline system to transfer the resulting mixed waste slurry to the AP Tank Farm and a planned waste treatment (vitrification) plant on the Hanford Site. Tank AZ- 102 contains 854, 000 gallons (3, 230 m{sup 3 }) of supernatant liquid and 95, 000 gallons (360 m{sup 3 }) of sludge made up of aging waste (or neutralized current acid waste). The study comprises three assessments: waste chemistry, pump jet mixing, and pipeline transfer. The waste chemical modeling assessment indicates that the sludge, consisting of the solids and interstitial solution, and the supernatant liquid are basically in an equilibrium condition. Thus, pump jet mixing would not cause much solids precipitation and dissolution, only 1. 5 % or less of the total AZ- 102 sludge. The pump jet mixing modeling indicates that two 300 -hp mixer pumps would mobilize up to about 23 ft (7. 0 m) of the sludge nearest the pump but would not erode the waste within seven inches (0. 18 m) of the tank bottom. This results in {{about half of the}} sludge being uniformly mixed in the tank and the <b>other</b> half being <b>unmixed</b> (not eroded) at the tank bottom...|$|R
40|$|In {{precision}} farming, field {{management is}} based on observing and responding to intra-field variations. Hyperspectral remote sensing has shown great potential in providing timely and accurate information on the spatial variability of field- and plant conditions. However, due to the discontinuous open canopies typical of most (perennial) cropping systems, {{the size of the}} image pixels will exceed, in many cases, the size of the objects of interest. The reflectance signal of a pixel is thus the integrated result of spectral contributions of both the crop and non-crop components (i. e. soils, weeds and shadows) building up a pixel footprint. As a consequence, image interpretation and the extraction of the biophysical parameters from the measured hyperspectral signature is hampered. Accurate site-specific monitoring of the crop thus requires removing all the undesired background effects from a measured mixed pixel, resulting in a purified vegetation signature, which can be used to derive the desired information regarding the plant. To this end, Signal Unmixing (SU) methodologies are presented in this dissertation, deriving the pure spectral signature of the crop component on a per-pixel basis. The basis is Multiple Endmember Spectral Mixture Analysis (MESMA). Spectral libraries or Look-up Tables (LUTs) are used, i. e. a collection of spectra representing the possible reflectance values of the different endmembers. The MESMA algorithm is an iterative process that selects endmember combinations from spectral libraries, and the combination which results in the lowest reconstruction error of the mixed signal is selected as the best representation of the components present within the pixel. The selected signature can on its turn be used to derive the desired information regarding the crops vigour status. In {{the first part of this}} work, solutions for the major bottlenecks of the MESMA methodology are presented. As the accuracy of MESMA is determined by the adequacy of the spectral library, it is crucial that the spectral library used for the unmixing of the image is representative of all endmembers present. An extensive tree LUT was thus created using a radiative transfer model, incorporating a high level of detail in the spectral signatures. However, variability in endmembers may lead to more than one pure spectrum combination resulting in the same mixture spectrum, a problem commonly referred to as ill-posedness. In Chapter 3, the integration of in situ measured soil moisture content into the SU model is therefore proposed to provide an estimation of the soil signature, as such reducing the number of possible solutions. This integration leads to a better extraction of the vegetation spectra, which on its turn results in an improved estimation of the trees vigour. Finally, the large size of the LUTs restricts the computational efficiency of the SU model. Incorporating geometric unmixing principles into MESMA enables a more efficient evaluation of all the different endmember combinations (Chapter 4). Whereas the traditional MESMA explores alldifferent endmember combinations separately, and selects the most appropriate combination as a final step, our approach selects the best endmember combination prior to unmixing, as such increasing the computational efficiency of MESMA. In addition to MESMA, two <b>other</b> <b>unmixing</b> methodologies are presented. Alternating Least Squares (ALS) unmixing is proposed as a Signal Unmixing methodology in Chapter 5. While MESMA requires extensive LUTs from which the most representative signal can be selected, ALS only needs an initial estimate of the spectral signature of each of the components present in the mixed pixel. This initial estimate is further optimised by ALS, and the pure spectral signature of the tree can thus be extracted from the mixed pixel signal. All the previous methodologies are tested on mixtures comprised of trees, soil and shadows. As the high spectral similarity between the trees and weeds hampers an accurate extraction of the tree signature, the performance of shape-based unmixing for separating spectrally similar endmembers is evaluated in Chapter 6. These insights can then be used to develop shape-based unmixing further into an SU model. Overall, this work provides a conceptual framework for the operational implementation of SU methodologies in a precision farming context. New methodologies are presented to extract the pure tree signature from hyperspectral mixed pixels, as well as to improve the computational efficiency and the accuracy of these methods. With the extracted tree signatures, an improved monitoring of the trees condition is achieved. SU thus provides a new avenue to explore the use of hyperspectral imagery in a precision farming context. nrpages: 151 status: publishe...|$|R

