411|200|Public
25|$|A New York Times/CBS Poll {{found that}} 60% of Americans opposed restricting {{collective}} bargaining while 33% were for it. The poll {{also found that}} 56% of Americans opposed reducing pay of public employees compared to the 37% who approved. The details of the poll also stated that 26% of those surveyed, thought pay and benefits for public employees were too high, 25% thought too low, and 36% thought about right. Mark Tapscott of the Washington Examiner criticized the poll, accusing it of <b>over-sampling</b> union and public employee households.|$|E
2500|$|Shotgun {{sequencing}} is a sequencing method {{designed for}} analysis of DNA sequences longer than 1000 base pairs, {{up to and including}} entire chromosomes. It is named by analogy with the rapidly expanding, quasi-random firing pattern of a shotgun. Since this method can only be used for fairly short sequences (100 to 1000 base pairs), longer DNA sequences must be broken into random small segments which are then sequenced to obtain reads. Multiple overlapping reads for the target DNA are obtained by performing several rounds of this fragmentation and sequencing. Computer programs then use the overlapping ends of different reads to assemble them into a continuous sequence. [...] Shotgun sequencing is a random sampling process, requiring <b>over-sampling</b> to ensure a given nucleotide is represented in the reconstructed sequence; the average number of reads by which a genome is over-sampled is referred to as coverage.|$|E
5000|$|The <b>over-{{sampling}}</b> ratio (OSR), where [...] is {{the sampling}} frequency and [...] is Nyquist rate, {{is defined by}} ...|$|E
50|$|Here, {{the spectra}} are resampled. To perform this operation, the spectra from Level 1a are <b>over-sampled</b> {{by a factor}} of 5. These <b>over-sampled</b> spectra are finally {{interpolated}} on a new constant wave-number basis (0.25 cm−1), by using a cubic spline interpolation.|$|R
3000|$|... {{shows that}} the {{relevant}} signal parts are locally <b>over-sampled</b> in time {{with respect to their}} local bandwidths [6, 11].|$|R
30|$|The {{data mining}} {{software}} Weka {{was used for}} implementing the SMOTE oversampling technique. The <b>over-sampled</b> data is then randomized twice for class balancing.|$|R
50|$|SLIMbus CLK {{frequencies}} and data transport protocols will support all common digital audio converter <b>over-sampling</b> {{frequencies and}} associated sample rates.|$|E
50|$|<b>Over-sampling</b> peak {{programme}} meter. This is {{a sample}} PPM {{in which the}} signal has first been over-sampled, typically {{by a factor of}} four, to alleviate the problem with a basic sample PPM.|$|E
50|$|Amcotts Moor Woman is {{the name}} given to bog body {{discovered}} in 1747 in a bog near Amcotts, Lincolnshire, England. Because little was known about preservation at the time, as well as <b>over-sampling,</b> only her left shoe has survived.|$|E
30|$|Given (17), (18), and (20), {{the moments}} Mpq of the <b>over-sampled</b> data {{when the two}} signals are BPSK {{modulated}} and ω 1 [*]=[*]ω 2 can be obtained as (21).|$|R
40|$|Abstract:- This {{paper will}} {{demonstrate}} a new method {{to compute the}} Cross Ambiguity Function (CAF) using <b>over-sampled</b> Perfect Reconstruction Discrete Fourier Transform (DFT) filter Banks, and compare it to previous work with maximally decimated DFT Filter Banks [1]. As was shown in our previous work, the DFT Filter Bank {{can be used to}} efficiently filter the signal into sub-bands, compute the CAF in each sub-band, and then reconstruct the CAFs coherently. This method has the advantage that Narrow Band (NB) interference can be removed prior to the reconstruction. If the prototype filter satisfies specific conditions, the CAF can be reconstructed coherently, thereby improving the Time Difference of Arrival (TDOA) estimate while maintaining the Frequency Difference of Arrival (FDOA) estimate. Maximally decimated Filter Banks are most efficient from a computational viewpoint, but the choice of the prototype filter is limited to a very simple filter with poor (13 dB) side-lobes. The <b>over-sampled</b> DFT filter bank is somewhat more computationally complex, but filters can be designed with better side-lobe properties, so that interference can be removed more efficiently. The prototype filter for the <b>over-sampled</b> filter bank can be designed with lower side-lobes, which removes more of the interferer and less of the signal of interest. The design constraints for the prototype filter for the <b>over-sampled</b> filter bank are {{the same as that of}} the cosine modulated filter bank...|$|R
30|$|From researchers’ perspective, the metrics {{presented}} in this article offer methods to identify and remove this kind of <b>over-sampled</b> accounts from Twitter’s Sample API and can serve as features for bot detection.|$|R
5000|$|For <b>over-sampling</b> [...] {{is set to}} [...] with N' > N, {{which results}} in N' > N {{summation}} coefficients in the second sum of the discrete Gabor representation. In this case, the number of obtained Gabor-coefficients would be MN'>K. Hence, more coefficients than sample values are available and therefore a redundant representation would be achieved.|$|E
50|$|In spread-OFDM, {{spreading}} {{is performed}} across orthogonal subcarriers {{to produce a}} transmit signal expressed byx = F−1Sbwhere F−1 is an inverse DFT, S is a spread-OFDM code matrix, and b is a data symbol vector. The inverse DFT typically employs an <b>over-sampling</b> factor, so its dimension is KxN (where K > N {{is the number of}} time-domain samples per OFDM symbol block), whereas the dimension of the spread-OFDM code matrix is NxN.|$|E
5000|$|Marxist {{economics}} {{was assessed}} as lacking relevance in 1988 by Robert M. Solow, who criticized the New Palgrave Dictionary of Economics for <b>over-sampling</b> articles on Marxism themes, giving a [...] "false {{impression of the}} state of play" [...] in the economics profession. Solow stated that [...] "Marx was an important and influential thinker, and Marxism has been a doctrine with intellectual and practical influence. The fact is, however, that most serious English-speaking economists regard Marxist economics as an irrelevant dead end." ...|$|E
2500|$|Kennedy J continued, [...] "At {{every stage}} of the job analysis, IOS, by {{deliberate}} choice, <b>over-sampled</b> minority firefighters {{to ensure that the}} results which IOS would use to develop the examinations—would not intentionally favor white candidates." ...|$|R
50|$|Lomb's {{periodogram}} method, on {{the other}} hand, can use an arbitrarily high number of, or density of, frequency components, as in a standard periodogram; that is, the frequency domain can be <b>over-sampled</b> by an arbitrary factor.|$|R
5000|$|Kennedy J continued, [...] "At {{every stage}} of the job analysis, IOS, by {{deliberate}} choice, <b>over-sampled</b> minority firefighters {{to ensure that the}} results which IOS would use to develop the examinations—would not intentionally favor white candidates." ...|$|R
50|$|A New York Times/CBS Poll {{found that}} 60% of Americans opposed restricting {{collective}} bargaining while 33% were for it. The poll {{also found that}} 56% of Americans opposed reducing pay of public employees compared to the 37% who approved. The details of the poll also stated that 26% of those surveyed, thought pay and benefits for public employees were too high, 25% thought too low, and 36% thought about right. Mark Tapscott of the Washington Examiner criticized the poll, accusing it of <b>over-sampling</b> union and public employee households.|$|E
5000|$|Beam tracing solves certain {{problems}} related to sampling and aliasing, which can plague conventional ray tracing approaches. [...] Since beam tracing effectively calculates the path of every possible ray within each beam (which {{can be viewed as}} a dense bundle of adjacent rays), it is not as prone to under-sampling (missing rays) or <b>over-sampling</b> (wasted computational resources). The computational complexity associated with beams has made them unpopular for many visualization applications. In recent years, Monte Carlo algorithms like distributed ray tracing (and Metropolis light transport?) have become more popular for rendering calculations.|$|E
5000|$|The VSA-100 {{supports}} a hardware accumulation buffer, {{known as the}} [...] "T-buffer". When rendering to the T-buffer, VSA-100 can store the combined outputs of several frames. This mechanism allows for creation of effects such as motion blur (if used temporally) and anti-aliasing (if used spatially). VSA-100 supports rotated-grid super-sampling anti-aliasing (RGSS AA) modes, with a maximum anti-aliasing level determined {{by the number of}} VSA-100 chips in the SLI configuration. One chip allows 2X AA, two chips allows 4X AA, four chips provides for 8X AA and so on. The RGSS method of anti-aliasing combines multiple samples of each frame, resulting in higher quality than the brute force ordered-grid <b>over-sampling</b> of ImgTech PowerVR, ATI Radeon DDR and NVIDIA GeForce 2.|$|E
40|$|In this paper, an <b>over-sampled</b> {{periodogram}} higher criticism (OPHC) test {{is proposed}} for the global detection of sparse periodic effects in a complex-valued time series. An explicit minimax detection boundary is established between the number and magnitude of the complex sinusoids hidden in the series. The OPHC test is shown to be asymptotically powerful in the detectable region. Numerical simulations illustrate and verify {{the effectiveness of the}} proposed test. Furthermore, the periodogram <b>over-sampled</b> by O(logN) is proven universally optimal in global testing for periodicities under a mild minimum separation condition. Connections to the problem of detecting a stream of pulses from frequency measurements in signal processing is also discussed...|$|R
30|$|This paper {{studies the}} general {{theoretical}} {{value of the}} cumulants of the <b>over-sampled</b> received data and adopts the HOS feature to identify the co-frequency mixed signal. The proposed identification method’s performance is obtained by Monte-Carlo simulations using MATLAB software.|$|R
40|$|In {{this study}} we aimed to {{investigate}} trap efficiency and specificity by three widely used live trapping methods (Sherman, mesh, and pitfall traps) in an agriculture landscape of NE Spain.  We trapped 243 small mammals of 8 different species. Sherman traps yielded high number of species (6) than mesh (5) and pitfall (3) traps. Frequencies of occurrence of small mammal species differed depending on the sampling methods used, as revealed by a statistical log-linear model for multidimensional contingency tables. Apodemus sylvaticus, Mus spretus and Eliomys quercinus were <b>over-sampled</b> by mesh traps, whereas Crocidura russula was under-sampled by mesh traps. Crocidura russula, Apodemus sylvaticus, Mus domesticus, and Mus spretus, were <b>over-sampled</b> by Sherman traps, whereas Suncus etruscus was under-sampled by Sherman traps. Finally, Suncus etruscus and Microtus duodecimcostatus were <b>over-sampled</b> by pitfall traps. The composition of the small mammal community studied was rather similar when using Sherman and mesh traps, but differed strongly from the community sampled by pitfall traps. So, {{in order to have}} a good reflex of the composition of the small mammal community a combination of  trapping techniques is necessary, as pointed out by many authors...|$|R
5000|$|Shotgun {{sequencing}} is a sequencing method {{designed for}} analysis of DNA sequences longer than 1000 base pairs, {{up to and including}} entire chromosomes. It is named by analogy with the rapidly expanding, quasi-random firing pattern of a shotgun. Since this method can only be used for fairly short sequences (100 to 1000 base pairs), longer DNA sequences must be broken into random small segments which are then sequenced to obtain reads. Multiple overlapping reads for the target DNA are obtained by performing several rounds of this fragmentation and sequencing. Computer programs then use the overlapping ends of different reads to assemble them into a continuous sequence. [...] Shotgun sequencing is a random sampling process, requiring <b>over-sampling</b> to ensure a given nucleotide is represented in the reconstructed sequence; the average number of reads by which a genome is over-sampled is referred to as coverage.|$|E
5000|$|This {{can also}} be {{expressed}} aswhere [...] is {{the separation of the}} images of the two objects on the film, and [...] is the distance from the lens to the film.If we take the distance from the lens to the film to be approximately equal to the focal length of the lens, we findbut [...] is the f-number of a lens. A typical setting for use on an overcast day would be [...] (see Sunny 16 rule). For violet 380-450nm the shortest wavelength visible light, the wavelength λ is about 420 nanometers (see cone cells for sensitivity of S cone cells). This gives a value for [...] of about 4 µm. In a digital camera, making the pixels of the image sensor smaller than this would not actually increase optical image resolution. However, it may improve the final image by <b>over-sampling,</b> allowing noise reduction.|$|E
50|$|There are {{a number}} of methods {{available}} to oversample a dataset used in a typical classification problem (using a classification algorithm to classify a set of images, given a labelled training set of images). The most common technique is known as SMOTE: Synthetic Minority <b>Over-sampling</b> Technique. To illustrate how this technique works consider some training data which has s samples, and f features in the feature space of the data. Note that these features, for simplicity, are continuous. As an example, consider a dataset of birds for clarification. The feature space for the minority class for which we want to oversample could be beak length, wingspan, and weight (all continuous). To then oversample, take a sample from the dataset, and consider its k nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those k neighbors, and the current data point. Multiply this vector by a random number x which lies between 0, and 1. Add this to the current data point to create the new, synthetic data point.|$|E
3000|$|... [*]v, consonants {{are ignored}} during the signal {{acquisition}} process, and are considered as low amplitude noise. In contrast, vowels are locally <b>over-sampled</b> like any harmonic signal [6, 10, 11]. This intelligent signal acquisition further avoids {{the processing of}} useless samples, within the [...]...|$|R
40|$|The {{performance}} of the Normalized Least Mean Square (NLMS) algorithm for adaptive filtering {{is dependent on the}} spectral flatness of the reference input. Thus, the standard NLMS algorithm does not perform well in <b>Over-Sampled</b> Subband Adaptive Filters (OS-SAFs) because colored subband signals are generated even for white input signals. Thus we propose the use of the Affine Projection Algorithm (APA) to adapt the individual subband filters in OS-SAF systems. The OS-SAF using APA for adaptation is implemented on a fast, low-resource <b>over-sampled</b> filterbank. Through both theoretical and experimental analyses, it is demonstrated that a low order APA will significantly improve the convergence behavior, offering a low computational complexity compared to the Recursive Least Squares (RLS) method. We employ a recursive method of calculating the correlation matrix to further decrease the computation cost without affecting the performance. 1...|$|R
30|$|Note {{that the}} {{expectation}} of the term E{Re[∙]} is zero except when ω 1 [*]−[*]ω 2 [*]=[*] 0 and the two baseband signal xi,k are BPSK modulated. The HOS of the <b>over-sampled</b> data rk when the two baseband signal xi,k are BPSK modulated and ω 1 [*]−[*]ω 2 [*]=[*] 0 is discussed in Appendix 2.|$|R
5000|$|PopMatters said [...] "The {{animated}} Jay Sherman...is {{perfect for}} Lovitz. He may never find anything in live action that serves him quite so well. I don’t mean {{that as a}} dig at his comic abilities (or his looks); it’s just that Sherman is an ideal outlet for the actor’s ham." [...] It said the cartoon format allows for [...] "his two biggest strengths as a performer: sarcasm and ironic overacting." [...] It added [...] "A marriage of voice-acting, writing, and animation that rivals {{some of the best}} Pixar work, Sherman is nonetheless hampered occasionally by the writers’ <b>over-sampling</b> from the Homer Simpson playbook, mainly gags concerning Jay’s girth and accompanying appetites. More effectively." [...] PopMatters said [...] "when it originally aired...the series was (for better or worse) slightly ahead of its time, outlandish in a way that The Simpsons would not adopt until later. Rewatching it now, The Critic seems most similar stylistically to the more recent series Family Guy, with its frequent cutaway gags...and blurring of fantasy and reality. The Simpsons introduced these qualities in moderation; The Critic and Family Guy are addicted to them, sometimes to a crippling degree." [...] It explained that [...] "The reference-heavy, media-saturated, sketch-like structure works better for The Critic than Family Guy, though, because the former is less in love with itself and its desire to shock or offend. Indeed, it’s more strange than twisted, Unlike Family Guy, it has a frame of reference beyond television, beyond even its self-created film niche." [...] It adds that [...] "The satire isn’t always as biting as it could be; many of the movie parodies eschew real critique in favor of non sequiturs or homage. Too often the writers rely on audience familiarity with popular movies. It’s amusing to be sure, but rarely as deadpan hilarious as The Simpsons parade of fabricated Troy McClure B-movies." ...|$|E
40|$|In this paper, we {{analyze the}} effect of {{resampling}} techniques, including undersampling and <b>over-sampling</b> used in active learning for word sense disambiguation (WSD). Experimental results show that under-sampling causes negative effects on active learning, but <b>over-sampling</b> is a relatively good choice. To alleviate the withinclass imbalance problem of <b>over-sampling,</b> we propose a bootstrap-based oversampling (BootOS) method that works better than ordinary <b>over-sampling</b> in active learning for WSD. Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning. According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions. ...|$|E
40|$|Imbalanced {{datasets}} {{are commonly}} encountered in real-world classification problems. However, many {{machine learning algorithms}} are originally designed for well-balanced datasets. Re-sampling has become an important step to preprocess imbalanced dataset. It aims at balancing the datasets by increasing the sample size of the smaller class or decreasing the sample size of the larger class, which are known as <b>over-sampling</b> and under-sampling respectively. In this paper, a novel sampling strategy based on both <b>over-sampling</b> and under-sampling is proposed, in which the new samples of the smaller class are created by the Synthetic Minority <b>Over-sampling</b> Technique (SMOTE). The improvement of the datasets {{is done by the}} evolutionary computational method of CHC that works on both the minority class and majority class samples. The result is a hybrid data preprocessing method that combines both <b>over-sampling</b> and under-sampling techniques to re-sample datasets. The evaluation is done by applying the learning algorithm C 4. 5 to obtain a classification model from the re-sampled datasets. Experimental results reported that the proposed approach can decrease the <b>over-sampling</b> rate about 50 % with only around 3 % discrepancy on the accuracy. Department of Electronic and Information EngineeringRefereed conference pape...|$|E
5000|$|Children in the FFCWS {{were born}} in {{hospitals}} in 20 large cities across the United States. These cities were selected for diversity in child support enforcement, labor market conditions, and welfare generosity. Within each city, hospitals were sampled and births were sampled in each hospital. The study design called for an <b>over-sample</b> of births to unmarried couples.|$|R
30|$|Since ROC {{curve is}} a binary {{classifier}} system {{but we have}} five class labels for the grade so we are presenting five ROC curves. For each ROC curve one class is considered as True class {{and the rest of}} the classes are considered as False class. ROC curves change when <b>over-sampled</b> data was used for classification which are discussed in the Section 4.5.|$|R
40|$|Abstract- This paper {{presents}} a new denoising method for <b>over-sampled</b> constant within intervals signals corrupted by additive noise. The novelty {{of this paper}} is a special MAP filter, called composed bishrink. A complete statistical analysis of this filter is reported. Some simulations are presented. The results obtained are compared with the results of other denoising methods and with other state-of-the art filtering techniques...|$|R
