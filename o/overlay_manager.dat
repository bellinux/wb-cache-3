12|7|Public
500|$|Bridgewater {{launched}} its All Weather hedge fund and pioneered the risk parity approach to portfolio management in 1996. The firm's {{assets under management}} grew from US$5billion in the mid-1990s to US$38billion by the year 2003. In June 2000, the firm was ranked as the best performing global bond manager for that year and the prior five years by Pensions & Investments magazine. In 2002, the company was ranked by Nelson Information as the World's Best Money Manager {{in recognition of the}} 16.3% return on its International Fixed Income program. The firm received the Global Investor Awards for Excellence-Global Bonds award in 2003. The following year the company received the Global Pensions (magazine) Currency <b>Overlay</b> <b>Manager</b> of the Year award, and 2 [...] "best in class" [...] awards from the PlanSponsor Operations Survey.|$|E
50|$|The Overlay format {{works by}} adding little stubs to code {{so that when}} they branch into a sub-module, it calls an <b>overlay</b> <b>manager,</b> which loads the {{requisite}} module. Commodore defined a standard <b>overlay</b> <b>manager</b> so that C code could automatically have these stubs inserted, and also generate an overlay table, which the standard <b>overlay</b> <b>manager</b> knew how to read.|$|E
5000|$|Currency hedging {{can be done}} passively or actively. The {{stream of}} returns from passive {{currency}} overlay is negatively correlated with international equities, has an expected return of zero, and does not employ any capital. The <b>overlay</b> <b>manager</b> uses forward contracts to match the portfolio’s currency exposures {{in such a way}} as to insure against exchange rate fluctuations. A decision list for a passive <b>overlay</b> <b>manager</b> would include ...|$|E
50|$|Fundamental {{managers}} {{believe they}} can exploit price inefficiencies using models and processes in which economic and financial data are used as the ‘exogenous’ variables, including balance of payments, capital flows, price levels, monetary conditions, etc. Millennium Global Investments {{is one of the}} most well established fundamental discretionary currency <b>overlay</b> <b>managers</b> in the investment industry.|$|R
40|$|Protocols A and B are {{competing}} to replace protocol H {{in the future}} Internet. I plan to first review {{the historical development of}} protocol H. Protocol H has become the dominant mechanism and de-facto standard for delivering content in the Internet [Est 93]. It allows flexible user specification of QoS (quality of service) parameters and a flexible user interface for content selection and viewing. Protocol H has two major modes of operation: client-serer and peer-to-peer. In client-server mode, users either get content indexed from their ISP (Internet service provider), or must subscribe to a third party service. In either case, a session is established that allows the user to browse indexed content using a standard Web browser. Once a user selects the desired content, a UDP-port- 9898 flow is established over which the content is streamed. In peer-to-peer mode, users join an overlay session using the join-overlay command to port 9899. This requires the location of one of the <b>overlay</b> <b>managers,</b> typically by searching the Web for “protocol h p 2...|$|R
40|$|Content-based publish-subscribe is {{emerging}} as a communication paradigm {{able to meet the}} demands of highly dynamic distributed applications, such as those made popular by mobile computing and peer-to-peer networks. Nevertheless, the available systems implementing this communication model are still unable to cope efficiently with dynamic changes to the topology of their distributed dispatching infrastructure. This hampers their applicability in the aforementioned scenarios. This thesis addresses this problem and presents a complete approach to the reconfiguration of content-based publish-subscribe systems. In Part I, it proposes a layered architecture for reconfigurable publish-subscribe middleware consisting of an overlay, a routing, and an event-recovery layer. This architecture allows the same routing components to operate in different types of dynamic network environments, by exploiting different underlying overlays. Part II addresses the routing layer with new protocols to manage the recon- figuration of the routing information enabling the correct delivery of events to subscribers. When the overlay changes as a result of nodes joining or leaving the network or as a result of mobility, this information is updated so that routing can adapt to the new environment. Our protocols manage to achieve this with as little overhead as possible. Part III addresses the overlay layer and proposes two novel approaches for building and maintaining a connected topology in highly dynamic network sce- narios. The protocols we present achieve this goal, while managing node degree and keeping reconfigurations localized when possible. These properties allow our <b>overlay</b> <b>managers</b> to be applied not only in the context of publish-subscribe mid- dleware but also as enabling technologies for other communication paradigms like application-level multicast. Finally, the thesis integrates the overlay and routing layers into a single frame- work and evaluates their combined performance both in wired and in wireless scenarios. Results show that the optimizations provided by our routing reconfig- uration protocols allow the middleware to achieve very good performance in such networks. Moreover, they highlight that our overlay layer is able to optimize this performance even further, significantly reducing the network traffic generated by the routing layer. The protocols presented in this thesis are implemented in the REDS middle- ware framework developed at Politecnico di Milano. Their use enables REDS to operate efficiently in dynamic network scenarios ranging from large-scale peer-to- peer to mobile ad hoc networks...|$|R
50|$|The {{currency}} <b>overlay</b> <b>manager</b> {{will conduct}} foreign-exchange hedging on their behalf, selectively placing and removing hedges {{to achieve the}} objectives of the client.|$|E
50|$|Although Borland's Turbo Pascal (TP) had useful {{single-stepping}} and {{conditional breakpoint}} facilities, {{the need for}} a more powerful debugger became apparent when TP started to be used for serious development. Initially a separate company, Turbopower, produced a debugger, T-Debug, and also their Turbo Analyst and <b>Overlay</b> <b>Manager</b> for Turbo Pascal for TP versions 1-3. Turbopower released T-Debug Plus 4.0 for TP 4.0 in 1988, but by then Borland's Turbo Debugger had been announced.|$|E
50|$|However, the Overlay format {{was rarely}} used, {{especially}} {{in the way it}} was intended. It was more commonly used with a custom <b>overlay</b> <b>manager.</b> A popular use of overlay format was with the Titanics Cruncher, which compressed executables. Instead of loading the entire compressed executable into memory before unpacking, the Titanics Cruncher used an overlay, so only a tiny decruncher was loaded into memory, then it read and decompressed data as it went.|$|E
50|$|The MBR is {{not located}} in a partition; it is located at a first sector of the device (physical offset 0), {{preceding}} the first partition. (The boot sector present on a non-partitioned device or within an individual partition is called a volume boot record instead.) In cases where the computer is running a DDO BIOS <b>overlay</b> or boot <b>manager,</b> the partition table may be moved to some other physical location on the device; e.g., Ontrack Disk Manager often placed {{a copy of the}} original MBR contents in the second sector, then hid itself from any subsequently booted OS or application, so the MBR copy was treated as if it were still residing in the first sector.|$|R
40|$|Hybrid {{governance}} {{forms that}} seek to meld the virtues of both market control and traditional hierarchical control are alluring governance forms. While extensive research has examined such hybrids forms, the research has been restricted largely to external hybrids ⎯ market exchanges infused with elements of hierarchical control. Comparatively little research, outside of the M-form literature, has examined internal hybrids ⎯ hierarchical forms infused with elements of market control. This paper contends that common change initiatives, such as TQM, reengineering, autonomous work teams, and group-based rewards, are appropriately viewed as attempts to craft internal hybrids by selectively infusing elements of market control within hierarchy. However, these common change initiatives are implemented typically in isolation and, as a consequence, violate patterns of complementarity that both sustain traditional hierarchy or support the stable infusion of market control. <b>Managers</b> <b>overlay</b> new measures on existing, functionallyoriented structures; they implement new structures without new performance measures and without new pay systems; they implement new pay systems, but fail to restructure or develop new performance measures. The paper argues that these violations of complementarity often unravel the bundle of elements that support traditional hierarchy and spiral hierarchies toward fundamental transformation. The clear trajectory of these transformations is toward quite radically, disaggregated organizations structured around teams. The paper presents both logic and evidence supporting the presence of complementarities in common change initiatives...|$|R
40|$|A {{significant}} {{trend in}} science research {{for at least}} the past decade has been the increasing uptake of computational techniques (modelling) for insilico experimentation, which is trickling down from the grand challenges that require capability computing to smaller-scale problems suited to capacity computing. Such virtual experiments also establish an opportunity for collaboration at a distance. At the same time, the development of web service and cloud technology, is providing a potential platform to support these activities. The problem on which we focus is the technical hurdles for users without detailed knowledge of such mechanisms - in a word, 'accessibility' - specifically: (i) the heavy weight and diversity of infrastructures that inhibits shareability and collaboration between services, (ii) the relatively complicated processes associated with deployment and management of web services for non-disciplinary specialists, and (iii) the relative technical difficulty in packaging the legacy software that encapsulates key discipline knowledge for web-service environments. In this paper, we describe a light-weight framework based on cloud and REST to address the above issues. The framework provides a model that allows users to deploy REST services from the desktop on to computing infrastructure without modification or recompilation, utilizing legacy applications developed for the command-line. A behind-the-scenes facility provides asynchronous distributed staging of data (built directly on HTTP and REST). We describe the framework, comprising the service factory, data staging services and the desktop file <b>manager</b> <b>overlay</b> for service deployment, and present experimental results regarding: (i) the improvement in turnaround time from the data staging service, and (ii) the evaluation of usefulness and usability of the framework through case studies in image processing and in multi-disciplinary optimization...|$|R
50|$|Constructing an {{overlay program}} {{involves}} manually dividing a program into self-contained object code blocks called overlays {{laid out in}} a tree structure. Sibling segments, those at the same depth level, share the same memory, called overlay region or destination region. An <b>overlay</b> <b>manager,</b> either part of the operating system or part of the overlay program, loads the required overlay from external memory into its destination region when it is needed. Often linkers provide support for overlays.|$|E
50|$|The IDE {{provided}} several debugging facilities, including single stepping, {{examination and}} changing of variables, and conditional breakpoints. In later versions assembly-language blocks could be stepped through. The user could add breakpoints on variables and registers in an IDE window. Programs using IBM PC graphics mode could flip between graphics and text mode automatically or manually, or display both on two screens. For {{cases where the}} relatively simple debugging facilities of the IDE were insufficient, Turbopower Software produced a more powerful debugger, T-Debug. The same company produced Turbo Analyst and <b>Overlay</b> <b>Manager</b> for Turbo Pascal. T-Debug was later updated for Turbo Pascal 4, but discontinued {{with the release of}} Borland's Turbo Debugger (TD), which also allowed some hardware intervention on computers equipped with the new 80386 processor.|$|E
5000|$|Bridgewater {{launched}} its All Weather hedge fund and pioneered the risk parity approach to portfolio management in 1996. The firm's {{assets under management}} grew from US$5 billion in the mid-1990s to US$38 billion by the year 2003. In June 2000, the firm was ranked as the best performing global bond manager for that year and the prior five years by Pensions & Investments magazine. In 2002, the company was ranked by Nelson Information as the World's Best Money Manager {{in recognition of the}} 16.3% return on its International Fixed Income program. The firm received the Global Investor Awards for Excellence-Global Bonds award in 2003. The following year the company received the Global Pensions (magazine) Currency <b>Overlay</b> <b>Manager</b> of the Year award, and 2 [...] "best in class" [...] awards from the PlanSponsor Operations Survey.|$|E
40|$|The {{computational}} Grid has popularized {{the idea}} of sharing resources such as compute cycles, storage space, and data across administrative domains. Grid resources traditionally encompassed dedicated super-computers, clusters, or storage units. The growing computational and storage capabilities of the modern everyday-use computers such as desktop and laptop machines make them potential candidates for serving as nodes in a federated resource sharing system. However, managing such resources poses several major challenges since such resources can be heterogeneous, highly dynamic, and with distinct sharing preferences, and can pose various security challenges due to cross-domain sharing. ^ In recent years, peer-to-peer (p 2 p) {{has emerged as a}} powerful paradigm for constructing large-scale distributed systems. It provides self-organization, decentralization, redundancy, efficient locality-aware routing, and eliminates much of the cost, difficulty, and time required to deploy, configure, and maintain large-scale distributed systems. Previously, the p 2 p paradigm has been limited to file sharing applications. This thesis explores the potential of the powerful p 2 p paradigm in Grid computing and proposes a p 2 p framework for discovering and managing federated dynamic resources. ^ The framework supports a two-level hierarchical organization, in which nodes within each administrative domain are organized into a local p 2 p <b>overlay,</b> and the <b>manager</b> nodes from individual domains form the higher level p 2 p overlay. Such organization retains the sovereignty of resources within each domain while allowing flexible sharing of local resources with remote clusters. The framework is successfully applied to develop two Grid services. First, the framework is applied to develop a self-organizing distributed compute cycle sharing service that allows users to utilize local and remote resources even in the presence of resources leaving and joining the system. Second, the framework is used to develop a fault-tolerant distributed storage system that harvests unused disk space on desktop machines or cluster nodes. The distributed storage retains the Network File System interface which allows users to transparently utilize the distributed storage via the standard NFS mechanisms. The thesis presents the design and evaluation of these services, and also investigates the issues of secure and fair resource sharing across domains. ...|$|R
30|$|The UCLP is {{the most}} {{promising}} project for federation in which the identities of virtual resources (e.g. LightPath, End 2 End object) is managed by UCLP Admins (SePs) and the identities of physical resources are managed by network owners (InPs). The UCLP end users cannot control UCLP virtual resources. The UCLP uses decentralized JavaSpace storage for the identities at InPs’ sites. The <b>overlay</b> <b>manager</b> (SeP as well as InP) in X-bone manages the identity of virtual nodes at central repository. The X-bone user cannot control {{the identity of the}} virtual resources. The NouVeau is an identity management for a abstract network virtualization model and is similar to UCLP. It is based on three main principles: separation of identity and location, local autonomy, and global identifier space. It also requires special entities called controller and adapters for the managements of identities in SePs and InPs.|$|E
40|$|Abstract. Limits on {{applications}} and hardware technologies {{have put a}} stop to the frequency race during the 2000 s. Designs now can be divided into homogeneous and heterogeneous ones. Homogeneous types are the easiest to use since most toolchains and system software do not need too much of a rewrite. On {{the other end of the}} spectrum, there are the type two heterogeneous designs. These designs offer tremendous computational raw power, but at the cost of hardware features that might be necessary or even essential for certain types of system software and programming languages. An example of this architectural design is the Cell processor which exhibits both a heavy core and a group of simple cores designed as a computational engine. Even though the Cell processor is very well known for its accomplishments, it is also well known for its low programmability. Among many efforts to increase its programmability, there is the Open OPELL project. This framework tries to port the OpenMP programming model to the Cell architecture. The OPELL framework is composed of four components: a single source toolchain, a very light SPU kernel, a software cache and a partition / code <b>overlay</b> <b>manager.</b> To reduce the overhead, each of these components can be further optimized. This paper concentrates on optimizing the partition manager by reducing the number of long latency transactions. The contributions of this work are as follows. 1. The development of a dynamic framework that loads and manages partitions across function calls to bypass the problem with restrictive memory spaces. 2. The implementation of replacement policies that are useful to reduce the number of DMA calls across partitions. 3. A quantification of such replacement policies given a selected set of applications 4. An API which can be easily ported and extended to several types of architectures. ...|$|E

