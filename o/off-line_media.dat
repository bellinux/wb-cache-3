14|6|Public
5000|$|The {{format for}} <b>off-line</b> <b>media</b> files is {{specified}} in Part 10 of the DICOM Standard. Such files are {{sometimes referred to}} as [...] "Part 10 files".|$|E
50|$|Information {{repositories}} feature robust, client based data {{search and}} recovery capabilities that, based on permissions, enable end users {{to search the}} information repository, view information repository contents, including data on <b>off-line</b> <b>media,</b> and recover individual files or multiple files to either their original network computer or another network computer.|$|E
5000|$|Blue laser {{technology}} gives the 30 GB UDO {{more than three}} times the capacity of previous generation MO (Magneto Optical) and DVD technologies. Being removable, UDO cartridges, combined with <b>off-line</b> <b>media</b> management capabilities typical of optical storage libraries, makes UDO a much more scalable format. Rarely used data can be removed from a library, freeing up capacity yet remaining managed and accessible.|$|E
40|$|Modern I/O {{subsystems}} include {{large storage}} servers which are configured to include multiple on-line and <b>off-line</b> storage <b>media</b> and {{to deal with}} a large number of requests with unpredictable access patterns. The problem of minimizing the cost of accessing data stored in all media is critical for the performance of the system. Given the large storage requirements of modern applications, Tertiary Storage Subsystems have become a crucial component of modern large-scale storage servers. This paper'...|$|R
5000|$|Off-Line Backup: Off-Line Backup allows {{along with}} {{and as part}} of the online backup {{solution}} to cover daily backups in time when network connection is down. At this time the remote backup software must perform backup onto a local media device like a tape drive, a disk or another server. The minute network connection is restored remote backup software will update the remote datacenter with the changes coming out of the <b>off-line</b> backup <b>media</b> [...]|$|R
40|$|The GenBank {{nucleotide}} sequence database now contains sequence data and associated annotation corresponding to 56, 000, 000 nucleotides in 45, 000 entries. The input stream of data {{coming into the}} database has largely been shifted to direct submissions from the scientific community on electronic media. The data have been installed in a relational database management system and are made available in this form through on-line access, and through various network and <b>off-line</b> computer-readable <b>media.</b> In addition, GenBank provides the U. S. distribution center for the BIOSCI electronic bulletin board service...|$|R
50|$|Information {{repositories}} {{were developed}} to mitigate problems arising from data proliferation and {{eliminate the need for}} separately deployed data storage solutions because of the concurrent deployment of diverse storage technologies running diverse operating systems. They feature centralized management for all deployed data storage resources. They are self-contained, support heterogeneous storage resources, support resource management to add, maintain, recycle, and terminate media, track of <b>off-line</b> <b>media,</b> and operate autonomously.|$|E
50|$|Since each library administers its own LOCKSS {{peer and}} {{maintains}} its own copy of preserved material, and {{since there are}} libraries doing so worldwide (see the list of participating libraries below), the system provides a much higher degree of replication than is usual in a fault-tolerant system. The voting process makes use of this high degree of replication to {{eliminate the need for}} backups to <b>off-line</b> <b>media,</b> and to provide robust defenses against attacks aimed at corrupting preserved content.|$|E
50|$|Local {{search is}} the natural {{evolution}} of traditional off-line advertising, typically distributed by newspaper publishers and TV and radio broadcasters, to the Web. Historically, consumers relied on local newspapers and local TV and radio stations to find local product and services. With {{the advent of the}} Web, consumers are increasingly using search engines to find these local products and services online. In recent years, the number of local searches online has grown rapidly while off-line information searches, such as print Yellow Page lookups, have declined. As a natural consequence of this shift in consumer behavior, local product and service providers are slowly shifting their advertising investments from traditional <b>off-line</b> <b>media</b> to local search engines.|$|E
40|$|The GenBank {{nucleotide}} sequence database now contains sequence data and associated annotation corresponding to 85, 000, 000 nucleotides in 67, 000 entries from {{a total of}} 3, 000 organisms. The input stream of data coming into the database is primarily as direct submissions from the scientific community on electronic media, {{with little or no}} data being keyboarded from the printed page by the databank staff. The data are maintained in a relational database management system and are made available in flatfile form through on-line access, and through various network and <b>off-line</b> computer-readable <b>media.</b> The data are also distributed in relational form through satellite copies at a number of institutions in the U. S. and elsewhere. In addition, GenBank provides the U. S. distribution center for the BIOSCI electronic bulletin board service...|$|R
40|$|The Digital Imaging and Communications in Medicine (DICOM) Standard {{specifies}} a non-proprietary data interchange protocol, {{digital image}} format, and file structure for biomedical images and image-related information. The fundamental {{concepts of the}} DICOM message protocol, services, and information objects are reviewed as background {{for a detailed discussion}} of the functionality of DICOM; the innovations and limitations of the Standard; and the impact of various DICOM features on information system users. DICOM addresses five general application areas: (1) network image management, (2) network image interpretation management, (3) network print management, (4) imaging procedure management, (5) <b>off-line</b> storage <b>media</b> management. DICOM is a complete specification of the elements required to achieve a practical level of automatic interoperability between biomedical imaging computer systems—from application layer to bit-stream encoding. The Standard is being extended and expanded in modular fashion to support new applications and incorporate new technology. An interface to other Information Systems provides for shared management of patient, procedure, and results information related to images. A Conformance Statement template enables a knowledgeable user to determine if interoperability between two implementations is possible. Knowledge of DICOM's benefits and realistic understanding of its limitations enable one to use the Standard effectively as the basis for a long term implementation strategy for image management and communications systems...|$|R
40|$|Prolonged {{exposure}} {{of humans and}} experimental animals to microgravity {{is known to be}} associated with a variety of physiological and cellular disturbances. With advancements in aerospace technology and prolonged space flights, both organism and cellular level understanding of the effects of microgravity on cells will become increasingly important in order {{to ensure the safety of}} prolonged space travel. To understand these effects at the cellular level, on-line sensor technology for the measurement and control of cell culture processes is required. To do this measurement, multiple sensors must be implemented to monitor various parameters of the cell culture medium. The model analytes used in this study were pH and dissolved oxygen which have physiological importance in a bioreactor environment. In most bioprocesses, pH and dissolved oxygen need to be monitored and controlled to maintain ionic strength and avoid hypoxia or hyperoxia. Current techniques used to monitor the value of these parameters within cell culture media are invasive and cannot be used to make on-line measurements in a closed-loop system. In this research, a microfabricated hydrogel microarray sensor was developed to monitor each anlyte. Either a pH or an oxygen sensitive fluorescent agent was immobilized into a hydrogel structure via a soft lithography technique and the intensity image of the sensor varied from the target analyte concentration. A compact detection system was developed to quantify concentration of each analyte based on the fluorescence image of the sensor. The system included a blue LED as an illumination source, coupling optics, interference filters and a compact moisture resistant CCD camera. Various tests were performed for the sensor (sensitivity, reversibility, and temporal/spatial uniformity) and the detection system (temporal/spatial stability for the light source and the detector). The detection system and the sensor were tested with a buffer solution and cell culture <b>media</b> <b>off-line.</b> The standard error of prediction for oxygen and pH detection was 0. 7 % and 0. 1, respectively, and comparable to that of commercial probes, well within the range necessary for cell culture monitoring. Lastly, the system was coupled to a bioreactor and tested over 2 weeks. The sensitivity and stability of the system was affordable to monitor pH and dissolved oxygen and shows potential to be used for monitoring those analytes in cell culture media noninvasively...|$|R
50|$|Images may {{be stored}} both locally and {{remotely}} on <b>off-line</b> <b>media</b> such as disk, tape or optical media. The use of storage systems, using modern data protection technologies {{has become increasingly}} common, particularly for larger organizations with greater capacity and performance requirements. Storage systems may be configured and attached to the PACS server in various ways, either as Direct-Attached Storage (DAS), Network-attached storage (NAS), or via a Storage Area Network (SAN). However the storage is attached, enterprise storage systems commonly utilize RAID and other technologies to provide high availability and fault tolerance to protect against failures. In the event {{that it is necessary}} to reconstruct a PACS partially or completely, some means of rapidly transferring data back to the PACS is required, preferably while the PACS continues to operate.|$|E
5000|$|With its spread, {{two general}} {{uses of the}} term netizen developed. Hauben explained, [...] "The first is a broad usage to refer to anyone who uses the Net, for {{whatever}} purpose.... The second usage is closer to my understanding,... {{people who care about}} Usenet and the bigger Net and work towards building the cooperative and collective nature which benefits the larger world. These are people who work towards developing the Net. … Both uses have spread from the online community, appearing in newspapers, magazines, television, books and other <b>off-line</b> <b>media.</b> As more and more people join the online community and contribute towards the nurturing of the Net and towards the development of a great shared social wealth, the ideas and values of Netizenship spread. But with the increasing commercialization and privatization of the Net, Netizenship is being challenged." [...] He called on scholars, [...] "to look back at the pioneering vision and actions that have helped make the Net possible and examine what lessons they provide." [...] He argued that is what he and the Netizens book tried to do.|$|E
5000|$|Prior to Apollo {{there were}} other {{attempts}} to provide single source measurement in the U.S., including Arbitron’s ScanAmerica, which used pushbutton peoplemeters; IRI’s use of household tuning meters in its BehaviorScan markets; Adtel; and ERIM. Outside the U.S. {{there have also been}} such efforts which have been discontinued except in England and in France where small single source panels survive. In Germany and the Netherlands, research agency Gfk is currently running single source panels under the name [...] "Media Efficiency Panel" [...] (MEP). In MEP online behavior and advertising contacts are measured in detail using a Nurago browser plug-in. <b>Off-line</b> <b>media</b> consumption is measured using validated media consumption questionnaires. FMCG purchases are captured using a household scanner and durable purchases using an online system asking respondents to check in and register what goods they bought, where they bought it and for what price. The German panel was launched in 2008 and is currently experimenting with audio measurement using a mobile phone to capture advertising contacts on TV. More than 70 studies have currently been done in this panel by a large variety of advertisers. The Dutch panel was launched in July 2010.|$|E
40|$|The paper {{introduces}} {{preliminary results}} of our research on DICOM – JPEG 2000 coupling: DICOM standard. DICOM standard defines method for transferring digital images of various formats and associated data between devices manufactured by various vendors. PACS (Picture Archiving and Communication System) inside a hospital that can interface with other informational subsystems is based on this standard. DICOM standard is applicable to a network and/or to an <b>off-line</b> <b>media</b> environment. These features of DICOM standard manage communication between PACS in different hospitals. That means that if patient is transferred to another hospital his new physician can access patient’s medical records over the network. They also manage online accession to medical records from physician’s home. The standard supports operations {{that are based on}} other computer protocols (TCP/IP, ISO 9660, etc). Digital image and associated data are coupled in a single DICOM file. Image pixels are encapsulated inside this file. Format of image can be native DICOM format and/or some other lossless or lossy standard (jpeg, jpeg 2000, etc). At the end, development of PACS system of Hospital for chest-diseases which is based on DICOM standard is discussedi...|$|E
40|$|Throughout the 1990 s the {{introduction}} of the internet and ecommerce reshaped the way that businesses do business and the way that consumers interact with businesses. One clear example is the way that spending on advertising has begun to shift from traditional <b>off-line</b> <b>media</b> to online and digital media as advertisers have seen an opportunity to better connect with their target audience. IBM forecasts 22 % growth in mobile, digital and interactive advertising format between 2006 and 2010 against 4 % growth in traditional advertising formats. Mobile commerce, often referred to as m-commerce, builds on the advances made by e-commerce (such as automated, electronic processes) but makes interaction available to a wider audience in a more personalized way. There are currently over 3 billion mobile phones worldwide. It means that approximately 40 % of the world’s population currently carries a mobile phone. Mobile phone adoption continues to grow. By 2012, it is expected that there will be 5 billion mobile phones worldwide. In many developed countries mobile phone penetration is well above 90 %, so saying “everyone has a mobile phone ” is very close to reality. ‘Brands and agencies are being forced to change their advertising strategies to engage with consumers and create a dialogue, rather than simply push messages to them...|$|E
40|$|Purpose – The {{purpose of}} this paper is to review, {{critique}} and develop a research agenda for the Elaboration Likelihood Model (ELM). The model was introduced by Petty and Cacioppo over three decades ago and has been modified, revised and extended. Given modern communication contexts, it is appropriate to question the model’s validity and relevance. Design/methodology/approach – The authors develop a conceptual approach, based on a fully comprehensive and extensive review and critique of ELM and its development since its inception. Findings – This paper focuses on major issues concerning the ELM. These include model assumptions and its descriptive nature; continuum questions, multi-channel processing and mediating variables before turning to the need to replicate the ELM and to offer recommendations for its future development. Research limitations/implications – This paper offers a series of questions in terms of research implications. These include whether ELM could or should be replicated, its extension, a greater conceptualization of argument quality, an explanation of movement along the continuum and between central and peripheral routes to persuasion, or to use new methodologies and technologies to help better understanding consume thinking and behaviour? All these relate to the current need to explore the relevance of ELM in a more modern context. Practical implications – It is time to question the validity and relevance of the ELM. The diversity of on- and <b>off-line</b> <b>media</b> options and the variants of consumer choice raise significant issues...|$|E
40|$|Purpose: The {{purpose of}} this paper is to review, {{critique}} and develop a research agenda for the Elaboration Likelihood Model (ELM). The model was introduced by Petty and Cacioppo over three decades ago and has been modified, revised and extended. Given modern communication contexts, it is appropriate to question the model’s validity and relevance. Design/methodology/approach: The authors develop a conceptual approach, based on a fully comprehensive and extensive review and critique of ELM and its development since its inception. Findings: This paper focuses on major issues concerning the ELM. These include model assumptions and its descriptive nature; continuum questions, multi-channel processing and mediating variables before turning to the need to replicate the ELM and to offer recommendations for its future development. Research limitations/implications: This paper offers a series of questions in terms of research implications. These include whether ELM could or should be replicated, its extension, a greater conceptualization of argument quality, an explanation of movement along the continuum and between central and peripheral routes to persuasion, or to use new methodologies and technologies to help better understanding consume thinking and behaviour? All these relate to the current need to explore the relevance of ELM in a more modern context. Practical implications: It is time to question the validity and relevance of the ELM. The diversity of on- and <b>off-line</b> <b>media</b> options and the variants of consumer choice raise significant issues. Originality/value: While the ELM model continues to be widely cited and taught as one of the major cornerstones of persuasion, questions are raised concerning its relevance and validity in 21 st century communication contexts. ...|$|E
40|$|The Goddard Space Flight Center (GSFC) Version 0 (V 0) Distributed Active Archive Center (DAAC) {{has been}} {{developed}} to support existing and pre Earth Observing System (EOS) Earth science datasets, facilitate the scientific research, and test Earth Observing System Data and Information System (EOSDIS) concepts. To ensure that no data is ever lost, each product received at GSFC DAAC is archived on two different media (VHS and Digital Linear Tape (DLT)). The first copy is made on VHS tape and is {{under the control of}} UniTree. The second and third copies are made to DLT and VHS media under a custom built software package named "Archer". While Archer provides only a subset of the functions available with commercial software like UniTree, it supports migration between near-line and <b>off-line</b> <b>media</b> and offers much greater performance and flexibility to satisfy the specific needs of a Data Center. Archer is specifically designed to maximize total system throughput, rather than focusing on the turn-around time for individual files. The Commercial Off the Shelf Software (COTS) Hierarchical Storage Management (HSM) products evaluated were mainly concerned with transparent, interactive, file access to the end-user, rather than as a batch-oriented, optimizable (based on known data file characteristics) data archive and retrieval system. This is critical to the distribution requirements of the GSFC DAAC where orders for 5000 or more files at a time are received. Archer has the ability to queue many thousands of file requests and to sort these requests into internal processing schedules that optimize overall throughput. Specifically, mount and dismount, tape load and unload cycles, and tape motion are minimized. This feature {{did not seem to be}} available in many COTS packages. Archer also [...] ...|$|E

