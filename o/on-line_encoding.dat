4|5|Public
50|$|XVidCap works {{using an}} <b>on-line</b> <b>encoding</b> {{facility}} with the FFmpeg libavcodec / libavformat. It can capture any movement on an X11 display either as single frames (like {{a number of}} JPEG images) or it can encode the captured frames to a video on-line. It can also grab and embed an audio recording provided users have an OSS compatible system and FFMPEG libraries with compiled audio capture support.|$|E
40|$|We {{describe}} {{the theory of}} quantum convolutional error correcting codes. These codes are aimed at protecting a flow of quantum information over long distance communication. They are largely inspired by their classical analogs which are used in similar circumstances in classical communication. In this article, we provide an efficient polynomial formalism for describing their stabilizer group, derive an <b>on-line</b> <b>encoding</b> circuit with linear gate complexity and study error propagation together with the existence of on-line decoding. Finally, we provide a maximum likelihood error estimation algorithm with linear classical complexity for any memoryless channel. ...|$|E
40|$|Four {{experiments}} explored <b>on-line</b> <b>encoding</b> {{strategies and}} memory for high imagery and low imagery texts. Results consistently indicated that concreteness effects in memory for text {{depend on how}} materials are presented in several different respects. Most importantly, the experiments clarified apparently contradictory results of previous studies by indicating that concreteness effects generally do not occur in memory for prose when imageability is manipulated between-subjects, and that their occurrence when imageability is manipulated within-subjects depends {{on the order of}} presentation. In addition, moving window analyses of text processing strategies indicated that differential strategies observed in previous studies when subjects listened to high vs low imagery text do not generalise to reading of the same materials. Potential explanations for the pattern of results are evaluated, and implications for theories of mental imagery and memory are considered...|$|E
40|$|Journal PaperTwo-sided unitary {{transformations of}} {{arbitrary}} 2 x 2 matrices {{are needed in}} parallel algorithms based on Jacobi-like methods for eigenvalue and singulare value decompositions of complex matrices. This paper presents a two-sided unitary transformation structured to facilitate the integrated evaluation of parameters and application of the typically required tranformations using only the primitives afforded by CORDIC; thus enabling significant speedup in the computation of these transformations on special-purpose processor array architectures implementing Jacobi-like algorithms. We discuss implementation in (nonredundant) CORDIC to motivate and lead up to implementation in the redundant and on-line enhancements to CORDIC. Both variable and constant scale factor redundant (CFR) CORDIC approaches are detailed and it is shown that the transformations may be computed in 10 n+ o time, where n is the data precision in bits and o is a constant accounting for accumulated on-line delays. A more area-intesive approach uisng a novel <b>on-line</b> CORDIC <b>encode</b> angle summation/difference scheme reduces computation time to 6 n+ o. The area/time complexities involved in the various approaches are detailed. National Science Foundatio...|$|R
40|$|We {{present a}} {{methodology}} of an <b>on-line</b> variable-length binary <b>encoding</b> {{of a set}} of integers. The basic principle of this methodology is to maintain the prefix property amongst the codes assigned on-line to a set of integers growing dynamically. The prefix property enables unique decoding of a string of elements from this set. To show the utility of this on-line variable length binary encoding, we apply this methodology to encode the LZW codes. Application of this encoding scheme significantly improves the compression achieved by the standard LZW scheme. This encoding can be applied in other compression schemes to encode the pointers using variable-length binary codes. (Also cross-referenced as UMIACS-TR- 95 - 39...|$|R
40|$|No single data {{encoding}} scheme or fault model {{is right for}} all data. A versatile storage system allows these to be data-specific, {{so that they can}} be matched to access patterns, reliability requirements, and cost goals. Ursa Minor is a cluster-based storage system that allows data-specific selection of and <b>on-line</b> changes to <b>encoding</b> schemes and fault models. Thus, different data types can share a scalable storage infrastructure and still enjoy customized choices, rather than suffering from "one size fits all. " Experiments with Ursa Minor show performance penalties as high as 2 [...] 3 # for workloads using poorly-matched choices. Experiments also show that a single cluster supporting multiple workloads is much more efficient when the choices are specialized rather than forced to use a "one size fits all" configuration...|$|R
40|$|Huffman {{codes are}} a widely used and very {{effective}} technique for compressing data. In this paper, {{we focus on}} the relationship between the computing time and space that is needed when compressing data with Huffman codes. We propose a further improvement to the Huffman method, called buffered adaptive Huffman coding. This approach dynamically changes the structure of Huffman code trees when encoding and decoding. Unlike original adaptive Huffman coding, the time when to update the tree is adjusted to not change the tree every time we read a symbol. It is changed only when the updating point is reached and it will save the number of updating the adaptive Huffman tree. The scheme is fast and useful to <b>on-line</b> <b>encoding.</b> We also propose a method of nonbinary Huffman coding based on adaptive encoding, called m-ary adaptive Huffman coding...|$|E
40|$|No single {{encoding}} scheme or fault model is optimal for all data. A versatile storage system {{allows them to}} be matched to access patterns, reliability requirements, and cost goals on a per-data item basis. Ursa Minor is a cluster-based storage system that allows data-specific selection of, and <b>on-line</b> changes to, <b>encoding</b> schemes and fault models. Thus, different data types can share a scalable storage infrastructure and still enjoy specialized choices, rather than suffering from "one size fits all. " Experiments with Ursa Minor show performance benefits of 2 [...] 3 when using specialized choices {{as opposed to a}} single, more general, configuration. Experiments also show that a single cluster supporting multiple workloads simultaneously is much more efficient when the choices are specialized for each distribution rather than forced to use a "one size fits all" configuration. When using the specialized distributions, aggregate cluster throughput nearly doubled...|$|R
40|$|In this paper, we {{describe}} an interdisciplinary project in which visualization techniques were developed for {{and applied to}} scholarly work from literary studies. The aim was to bring Christof Schöch's electronic edition of Bérardier de Bataut's Essai sur le récit (1776) to the web. This edition {{is based on the}} Text Encoding Initiative's XML-based encoding scheme (TEI P 5, subset TEI-Lite). This now de facto standard applies to machine-readable texts used chiefly in the humanities and social sciences. The intention of this edition is to make the edited text freely available on the web, to allow for alternative text views (here original and modern/corrected text), to ensure reader-friendly annotation and navigation, to permit <b>on-line</b> collaboration in <b>encoding</b> and annotation as well as user comments, all in an open source, generically usable, lightweight package. These aims were attained by relying on a GPL-based, public domain CMS (Drupal) and combining it with XSL-Stylesheets and Java Script...|$|R

