6181|10000|Public
5|$|On {{the issue}} of {{geographic}} origin, linguists during the 20th century agreed that the Uto-Aztecan language family originated in the southwestern United States. Evidence from archaeology and ethnohistory supports a southward diffusion across the American continent thesis, specifically that speakers of early Nahuan languages migrated from Aridoamerica into central Mexico in several waves. But recently, the traditional assessment has been challenged by Jane H. Hill, who proposes instead that the Uto-Aztecan language family originated in central Mexico and spread northwards {{at a very early}} date. This hypothesis and the analyses <b>of</b> <b>data</b> <b>that</b> it rests upon have received serious criticism.|$|E
5|$|Tyche was a {{hypothetical}} gas giant proposed {{to be located}} in the Solar System's Oort cloud. It was first proposed in 1999 by astrophysicists John Matese, Patrick Whitman and Daniel Whitmire of the University of Louisiana at Lafayette. They argued that evidence of Tyche's existence could be seen in a supposed bias in the points of origin for long-period comets. In 2013, Matese and Whitmire re-evaluated the comet data and noted that Tyche, if it existed, would be detectable in the archive <b>of</b> <b>data</b> <b>that</b> was collected by NASA's Wide-field Infrared Survey Explorer (WISE) telescope. In 2014, NASA announced that the WISE survey had ruled out any object with Tyche's characteristics, indicating that Tyche as hypothesized by Matese, Whitman, and Whitmire does not exist.|$|E
5|$|The {{separation}} of static and dynamic data to reduce write amplification {{is not a}} simple process for the SSD controller. The process requires the SSD controller to separate the LBAs with data which is constantly changing and requiring rewriting (dynamic data) from the LBAs with data which rarely changes and does not require any rewrites (static data). If the data is mixed in the same blocks, as with almost all systems today, any rewrites will require the SSD controller to garbage collect both the dynamic data (which caused the rewrite initially) and static data (which did not require any rewrite). Any garbage collection <b>of</b> <b>data</b> <b>that</b> would not have otherwise required moving will increase write amplification. Therefore, separating the data will enable static data to stay at rest and if it never gets rewritten it will have the lowest possible write amplification for that data. The drawback to this process is that somehow the SSD controller must still find a way to wear level the static data because those blocks that never change will not get a chance to be written to their maximum P/E cycles.|$|E
50|$|In cryptography, {{a secure}} channel {{is a way}} <b>of</b> {{transferring}} <b>data</b> <b>that</b> is resistant to overhearing and tampering. A confidential channel is a way <b>of</b> transferring <b>data</b> <b>that</b> is resistant to overhearing (i.e., reading the content), but not necessarily resistant to tampering. An authentic channel is a way <b>of</b> transferring <b>data</b> <b>that</b> is resistant to tampering but not necessarily resistant to overhearing.|$|R
30|$|Metric: success fee: {{percentage}} <b>of</b> <b>data</b> correctly {{delivered to}} the correspondent constituent, considering the amount <b>of</b> <b>that</b> <b>data</b> <b>that</b> {{is intended to be}} delivered.|$|R
50|$|After {{learning}} a function {{based on the}} training set <b>data,</b> <b>that</b> function is validated on a test set <b>of</b> <b>data,</b> <b>data</b> <b>that</b> did {{not appear in the}} training set.|$|R
25|$|A set <b>of</b> <b>data</b> <b>that</b> {{arises from}} the {{log-normal}} distribution has a symmetric Lorenz curve (see also Lorenz asymmetry coefficient).|$|E
25|$|Techniques {{can also}} be combined. For sorting very large sets <b>of</b> <b>data</b> <b>that</b> vastly exceed system memory, even the index {{may need to be}} sorted using an {{algorithm}} or combination of algorithms designed to perform reasonably with virtual memory, i.e., {{to reduce the amount of}} swapping required.|$|E
25|$|Transcriptomics studies {{generate}} {{large amounts}} <b>of</b> <b>data</b> <b>that</b> has potential applications {{far beyond the}} original aims of an experiment. As such, raw or processed data may be deposited in public databases to ensure their utility for the broader scientific community. For example, as of 2016, the Gene Expression Omnibus contained millions of experiments.|$|E
5000|$|No {{restriction}} {{on the number}} <b>of</b> <b>data</b> records <b>that</b> can be processed.|$|R
5000|$|Some <b>of</b> the <b>data</b> <b>that</b> can be {{investigated}} to strengthen personalizing the offer includes: ...|$|R
5000|$|... {{includes}} properties <b>of</b> the <b>data</b> <b>that</b> {{is required}} by the view, and populates itself.|$|R
25|$|Data relay {{satellite}} (数据中继卫星) Tianlian I (天链一号), specially {{developed to}} decrease the communication time between the Shenzhou 7 spaceship and the ground; it will also improve the amount <b>of</b> <b>data</b> <b>that</b> can be transferred. The current orbit coverage of 12 percent will thus be increased {{to a total of}} about 60 percent.|$|E
25|$|As {{worldwide}} {{sales of}} smartphones began exceeding those of feature phones, the NSA {{decided to take}} advantage of the smartphone boom. This is particularly advantageous because the smartphone combines a myriad <b>of</b> <b>data</b> <b>that</b> would interest an intelligence agency, such as social contacts, user behavior, interests, location, photos and credit card numbers and passwords.|$|E
25|$|Billions {{of dollars}} per year are spent, by {{agencies}} such as the Information Awareness Office, National Security Agency, and the Federal Bureau of Investigation, to develop, purchase, implement, and operate systems such as Carnivore, ECHELON, and NarusInsight to intercept and analyze the immense amount <b>of</b> <b>data</b> <b>that</b> traverses the Internet and telephone system every day.|$|E
5000|$|Below are a {{few areas}} <b>of</b> <b>data</b> flows <b>that</b> may need {{perennial}} DQ checks: ...|$|R
5000|$|The {{number of}} {{instances}} <b>of</b> training <b>data</b> <b>that</b> must be compared {{at each step}} is reduced ...|$|R
40|$|The {{depiction}} of the workflow for the evaluation <b>of</b> scientific <b>data</b> <b>that</b> are planned for dissemination by the NASA Socioeconomic Data and Applications Center (SEDAC) demonstrates the diversity of roles, groups, and activities {{that are involved in}} the evaluation <b>of</b> scientific <b>data</b> disseminated by SEDAC. The evaluation efforts described in the workflow include reviews, assessments, and testing activities <b>of</b> scientific <b>data</b> <b>that</b> are conducted throughout the data acquisition, product development, and dissemination processes...|$|R
25|$|Presentation <b>of</b> <b>data</b> <b>that</b> {{seems to}} support claims while suppressing or refusing to {{consider}} data that conflict with those claims. This {{is an example}} of selection bias, a distortion of evidence or data that arises from the way that the data are collected. It is sometimes referred to as the selection effect.|$|E
25|$|When {{the amount}} <b>of</b> <b>data</b> <b>that</b> an {{attacker}} can inject into the target process is too limited to execute useful shellcode directly, {{it may be}} possible to execute it in stages. First, a small piece of shellcode (stage 1) is executed. This code then downloads a larger piece of shellcode (stage 2) into the process's memory and executes it.|$|E
25|$|The amount <b>of</b> <b>data</b> <b>that</b> can be {{transmitted}} (and therefore the number of channels) is directly affected by channel capacity and the modulation method of the channel. The modulation method in DVB-T is COFDM with either 64 or 16-state Quadrature Amplitude Modulation (QAM). In general, a 64QAM channel is capable of transmitting a greater bit rate, but is more susceptible to interference. 16 and 64QAM constellations can be combined in a single multiplex, providing a controllable degradation for more important program streams. This is called hierarchical modulation.|$|E
50|$|Here we see {{the central}} {{property}} <b>of</b> Poisson <b>data,</b> <b>that</b> the variance {{is equal to the}} mean.|$|R
30|$|Average {{end-to-end}} delay <b>of</b> <b>data</b> packets: The average end-to-end {{delay is}} the transmission delay <b>of</b> <b>data</b> packets <b>that</b> are delivered successfully.|$|R
30|$|The scales <b>of</b> <b>data</b> streams <b>that</b> {{need to be}} anonymized in some {{applications}} are increasing tremendously.|$|R
25|$|In {{medical and}} {{scientific}} research, asking subjects {{for information about}} their behaviors is normally strictly scrutinized by institutional review boards, for example, to ensure that adolescents and their parents have informed consent. It {{is not clear whether}} the same rules apply to researchers who collect data from social networking sites. These sites often contain a great deal <b>of</b> <b>data</b> <b>that</b> is hard to obtain via traditional means. Even though the data are public, republishing it in a research paper might be considered invasion of privacy.|$|E
25|$|The mainshock was {{recorded}} by few seismometers in the region. In contrast, the 1986 event {{occurred at a}} time when far more instruments were operating in southern California, and the additional volume <b>of</b> <b>data</b> <b>that</b> was generated provided a means for relocating and confirming the mechanism of faulting for the older event. The amplitude of the waveforms shown on the 1948 seismograms were 20–30% larger than those of the later shock. Like the 1986 event, the 1948 event was presumed to have occurred near the northwest-striking (and steeply-dipping) Banning Fault, which was relatively unknown at the time.|$|E
25|$|The {{simplest}} {{gravitational waves}} are those with constant frequency. The waves given {{off by a}} spinning, non-axisymmetric neutron star would be approximately monochromatic: a pure tone in acoustics. Unlike signals from supernovae of binary black holes, these signals evolve little in amplitude or frequency over the period it would be observed by ground-based detectors. However, {{there would be some}} change in the measured signal, because of Doppler shifting caused by the motion of the Earth. Despite the signals being simple, detection is extremely computationally expensive, because of the long stretches <b>of</b> <b>data</b> <b>that</b> must be analysed.|$|E
5000|$|Encryption <b>of</b> the <b>data</b> <b>that</b> will {{be stored}} in device for later / {{off-line}} analysis by the customer.|$|R
30|$|We briefly review {{research}} on user demographic prediction from the aspect <b>of</b> <b>data</b> types <b>that</b> are recorded.|$|R
3000|$|... 2. In particular, d_ 1 /E[d] {{describes}} the proportion <b>of</b> the <b>data</b> <b>that</b> has been offloaded onto Wi-Fi network.|$|R
25|$|There {{is further}} {{complication}} in that restoration ecologists {{who want to}} collect large scale data on restoration projects can face enormous hurdles in obtaining the data. Managers vary in how much data they collect, and how many records they keep. Some agencies keep {{only a handful of}} physical copies <b>of</b> <b>data</b> <b>that</b> make it difficult for the researcher to access. Many restoration projects are limited by time and money, so data collection and record keeping are not always feasible. However, this limits the ability of scientists to analyze restoration projects and give recommendations based on empirical data.|$|E
25|$|An {{example of}} an amplified DDoS attack through NTP is through a command called monlist, which sends {{the details of the}} last 600 people who have {{requested}} the time from that computer back to the requester. A small request to this time server can be sent using a spoofed source IP address of some victim, which results in 556.9 times the amount <b>of</b> <b>data</b> <b>that</b> was requested back to the victim. This becomes amplified when using botnets that all send requests with the same spoofed IP source, which will send a massive amount of data back to the victim.|$|E
25|$|Skeptics such as Paul Edwards have {{analyzed}} many {{of these}} accounts, and called them anecdotal, while also suggesting that claims of evidence for reincarnation originate from selective thinking and from the false memories that often result from one's own belief system and basic fears, and thus cannot be counted as empirical evidence. Carl Sagan referred to examples apparently from Stevenson's investigations in his book The Demon-Haunted World {{as an example of}} carefully collected empirical data, though he rejected reincarnation as a parsimonious explanation for the stories. Sam Harris cited Stevenson's works in his book The End of Faith as part of a body <b>of</b> <b>data</b> <b>that</b> seems to attest to the reality of psychic phenomena.|$|E
50|$|Recently HUD {{published}} a paper called 32 Years <b>of</b> Housing <b>Data</b> <b>that</b> summarizes <b>data</b> {{from all the}} national surveys.|$|R
40|$|P ast {{research}} ha s shown <b>that</b> <b>data</b> reusers {{are concerned}} with the issue <b>of</b> <b>data</b> quality and the identified attributes <b>of</b> quality. While <b>data</b> reusers find evidence of the attributes <b>of</b> <b>data</b> quality during their assessment <b>of</b> <b>data</b> for reuse, there may be other dimensions <b>of</b> <b>data</b> quality <b>that</b> reusers are concern ed about but that are not always visible to them. This study explores these invisible dimensions <b>of</b> <b>data</b> quality <b>that</b> have been identified by data reusers. The findings of this study indicate <b>that</b> <b>data</b> reusers are concern ed with two kinds of invisible characteristics for assessing the data : the efforts put on data, and the ethics behind the data. While these quality dimensions cannot be easily measured at face - level, data reusers find proxy evidence that indicate s the presence of these invisibilities. This finding signifies the role <b>of</b> <b>data</b> management <b>that</b> can make these invisible data qualit ies visibl...|$|R
30|$|In the {{following}} section we detail how we set about the collection <b>of</b> the <b>data</b> <b>that</b> comprise this study. We detail how we drew on the relevant literature to develop framework for an analysis <b>of</b> <b>that</b> <b>data</b> and how this analysis was carried out both computationally and by the two human evaluators.|$|R
