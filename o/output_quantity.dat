149|361|Public
25|$|Since the Bertrand model {{assumes that}} firms compete on price and not <b>output</b> <b>quantity,</b> it predicts that a duopoly {{is enough to}} push prices down to {{marginal}} cost level, meaning that a duopoly will result in perfect competition.|$|E
2500|$|The production–possibility {{frontier}} (PPF) is an expository {{figure for}} representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say [...] "guns" [...] and [...] "butter"). The PPF is a table or graph (as at the right) showing the different quantity combinations {{of the two}} goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible <b>output</b> <b>quantity</b> of the other good.|$|E
5000|$|The {{specific}} {{applications of}} log-linear models are where the <b>output</b> <b>quantity</b> {{lies in the}} range 0 to ∞, for values {{of the independent variables}} , or more immediately, the transformed quantities [...] in the range −∞ to +∞. This may be contrasted to logistic models, similar to the logistic function, for which the <b>output</b> <b>quantity</b> lies in the range 0 to 1. Thus the contexts where these models are useful or realistic often depends on the range of the values being modelled.|$|E
5000|$|... #Subtitle level 3: Models {{with any}} number of <b>output</b> <b>quantities</b> ...|$|R
50|$|Reduce {{investment}} costs for production lines without endangering the required <b>output</b> <b>quantities.</b>|$|R
50|$|When the {{measurement}} model is multivariate, that is, it has {{any number of}} <b>output</b> <b>quantities,</b> the above concepts can be extended. The <b>output</b> <b>quantities</b> are now described by a joint probability distribution, the coverage interval becomes a coverage region, the law of propagation of uncertainty has a natural generalization, and a calculation procedure that implements a multivariate Monte Carlo method is available.|$|R
5000|$|... the GUM {{uncertainty}} framework, constituting {{the application}} of the law of propagation of uncertainty, and the characterization of the <b>output</b> <b>quantity</b> [...] by a Gaussian or a -distribution, ...|$|E
5000|$|Formally, the <b>output</b> <b>quantity,</b> {{denoted by}} , about which {{information}} is required, is often related to input quantities, denoted by , about which information is available, by a measurement {{model in the}} form of ...|$|E
5000|$|The {{calculation}} stage {{consists of}} propagating the probability distributions for the input quantities through the measurement model {{to obtain the}} probability distribution for the <b>output</b> <b>quantity</b> , and summarizing by using this distribution to obtain ...|$|E
40|$|In {{this paper}} the {{uncertainty}} of a robust photometer circuit (RPC) was estimated. Here, the RPC was considered as a measurement system, having input quantities that were inexactly known, and <b>output</b> <b>quantities</b> that consequently were also inexactly known. Input quantities represent information obtained from calibration certificates, specifications of manufacturers, and tabulated data. <b>Output</b> <b>quantities</b> describe the transfer function of the electrical part of the photodiode. Input quantities were the electronic components of the RPC, {{the parameters of the}} model of the photodiode and its sensitivity at 670 nm. The <b>output</b> <b>quantities</b> were the coefficients of both numerator and denominator of the closed-loop transfer function of the RPC. As an example, the gain and phase shift of the RPC versus frequency was evaluated from the transfer function, with their uncertainties and correlation coefficient. Results confirm the robustness of photodiode design...|$|R
40|$|In many cases, the {{uncertainty}} of <b>output</b> <b>quantities</b> may be computed by assuming that the distribution represented by the result of measurement and its associated standard uncertainty is a Gaussian. This assumption may be unjustified and {{the uncertainty}} of the <b>output</b> <b>quantities</b> determined in this way may be incorrect. One tool to deal with different distribution functions of the input parameters and the resulting mixed-distribution of the <b>output</b> <b>quantities</b> is given through the Monte Carlo techniques. The resulting empirical distribution can be used to approximate the theoretical distribution of the <b>output</b> <b>quantities.</b> All required moments of different orders can then be numerically determined. To evaluate the procedure of derivation and evaluation of output parameter uncertainties outlined in this paper, a case study of kinematic terrestrial laserscanning (k-TLS) will be discussed. This study deals with two main topics: the refined simulation of different configurations by taking different input parameters with diverse probability functions for the uncertainty model into account, and the statistical analysis of the real data in order to improve the physical observation models in case of k-TLS. The solution of both problems is essential for the highly sensitive and physically meaningful application of k-TLS techniques for monitoring of, e. g., large structures such as bridges...|$|R
5000|$|JCGM 102:2011. Evaluation of {{measurement}} data - Supplement 2 to the [...] "Guide to {{the expression of}} uncertainty in measurement" [...] - Extension to any number of <b>output</b> <b>quantities.</b>|$|R
5000|$|The {{standard}} uncertainty [...] {{associated with}} the estimate [...] of the <b>output</b> <b>quantity</b> [...] is not given by {{the sum of the}} , but these terms combined in quadrature, namely by expression that is generally approximate for measurement models ...|$|E
5000|$|Since the Bertrand model {{assumes that}} firms compete on price and not <b>output</b> <b>quantity,</b> it predicts that a duopoly {{is enough to}} push prices down to {{marginal}} cost level, meaning that a duopoly will result in perfect competition.|$|E
50|$|The items {{required}} by a measurement model {{to define a}} measurand are known as input quantities in a measurement model. The model {{is often referred to}} as a functional relationship. The <b>output</b> <b>quantity</b> in a measurement model is the measurand.|$|E
50|$|Systems can {{be defined}} as dynamic or non-dynamic in an {{equilibrium}} state. Besides the usual transient condition, where at least one quantity changes with time, stable dynamic systems may be in a steady state condition or equilibrium state where the system is at rest. This special condition is possible after sometime, when all input and <b>output</b> <b>quantities</b> are and remain constant. The relation between input and <b>output</b> <b>quantities</b> for a system in a steady state condition is called “Static Transfer Response of the Dynamic System”. Dynamic systems {{can be defined}} as static and transient, while this is seemingly contradictory, this indicates that the system is always a dynamic system, even if it remains momentarily in a steady state condition. The opposite of dynamic is not static but non-dynamic.|$|R
40|$|A set of test {{statistics}} are specified {{and the corresponding}} <b>output</b> <b>quantities</b> computed by the characteristic function. Two sets of classification accuracies, one at the input and one at the output are estimated. The scanner's {{instantaneous field of view}} is changed and the variation of the output classification performance is monitored...|$|R
40|$|Manufacturing {{technology}} {{integration is}} an arising paradigm that aims at the functional integration of diverse manufacturing technologies into machine tools which are called multi-technology platforms. So far, machine tool builders {{have attempted to}} justify manufacturing technology integration through the machine hour rate calculation. However, this calculation approach is inadequate for such purpose since it neglects <b>output</b> <b>quantities</b> and {{the configuration of the}} manufacturing system. This dissertation applies models of production, cost, and queuing theory to derive the conditions under which manufacturing technology integration leads to greater productivity, lower cost, and smaller throughput times than a conventional manufacturing system consisting of single-technology machine tools. Such a conventional manufacturing system is called segregated manufacturing system. Based on the efficiency models the design of multi-technology platforms is discussed with regard to the number and the type of manufacturing technologies to be integrated as well as the number of workspaces. It is found that manufacturing technology integration is particularly cost-efficient for low <b>output</b> <b>quantities.</b> However, although the logistic chain in a plant is shortened through manufacturing technology integration the throughput times of an integrated manufacturing system might be greater than the throughput times of a segregated manufacturing system. This {{is due to the fact}} that for low <b>output</b> <b>quantities</b> the resource utilization and as such the waiting times might be greater in an integrated manufacturing system than in a segregated manufacturing system...|$|R
5000|$|To {{obtain the}} profit maximizing <b>output</b> <b>quantity,</b> we start by {{recognizing}} that profit {{is equal to}} total revenue (TR) minus total cost (TC). Given a table of costs and revenues at each quantity, we can either compute equations or plot the data directly on a graph. The profit-maximizing output is the one at which this difference reaches its maximum.|$|E
5000|$|Solow {{assumed a}} very basic model of annual {{aggregate}} output over a year (t). He said that the <b>output</b> <b>quantity</b> would be governed {{by the amount of}} capital (the infrastructure), the amount of labour (the {{number of people in the}} workforce), and the productivity of that labour. He thought that the productivity of labour was the factor driving long-run GDP increases. An example economic model of this form is given below: ...|$|E
5000|$|If {{a voltage}} {{is applied to}} the BJT base-emitter {{junction}} as an input quantity and the collector current is taken as an <b>output</b> <b>quantity,</b> the transistor will act as an exponential voltage-to-current converter. By applying a negative feedback (simply joining the base and collector) the transistor can be [...] "reversed" [...] and it will begin acting as the opposite logarithmic current-to-voltage converter; now it will adjust the [...] "output" [...] base-emitter voltage so as to pass the applied [...] "input" [...] collector current.|$|E
40|$|International audienceModel Order Reduction (MOR) {{methods are}} applied in {{different}} areas of physics {{in order to reduce}} the computational time of large scale systems. It has been an active field of research for many years, in mechanics especially, but it is quite recent for magnetoquasistatic problems. Although the most famous method, the Proper Orthogonal Decomposition (POD) has been applied for modelling many electromagnetic devices, this method can lack accuracy for low order magnitude <b>output</b> <b>quantities,</b> like flux associated with a probe in regions where the field is low. However, the Balanced Proper Orthogonal Decomposition (BPOD) is a MOR method which takes into account these <b>output</b> <b>quantities</b> in its reduced model to render them accurately. Even if the BPOD may lead to unstable reduced systems, this can be overcome by a stabilization procedure. Therefore, the POD and stabilized BPOD will be compared on a 3 D linear magnetoquasistatic field problem...|$|R
40|$|In this paper, {{we examine}} how {{uncertainty}} can affect successive markets, when uncertainty can jointly influence both the {{upstream and downstream}} markets' conditions. The main result of the paper is that the equilibrium input and <b>output</b> <b>quantities</b> under stochastic dependence can be higher or lower than the corresponding quantities {{in the case of}} certainty equivalence depending on how much dependent are the events...|$|R
5000|$|Average Total Cost (ATC) = Total Cost / Q (<b>Output</b> is <b>quantity</b> {{produced}} or ‘Q’) ...|$|R
5000|$|In economics, an 'inverse demand {{function}}', P = f−1(Q), is {{a function}} that maps the quantity of output demanded to the market price (dependent variable) for that <b>output.</b> <b>Quantity</b> demanded, Q, {{is a function}} of price; the inverse demand function treats price as a function of quantity demanded, and is also called the price function. Note that the inverse demand function is not the reciprocal of the demand function - the word [...] "inverse" [...] refers to the mathematical concept of an inverse function.|$|E
50|$|In economics, {{average cost}} and/or unit cost {{is equal to}} total cost divided {{by the number of}} goods {{produced}} (the <b>output</b> <b>quantity,</b> Q). It is also equal to the sum of average variable costs (total variable costs divided by Q) plus average fixed costs (total fixed costs divided by Q). Average costs may be dependent on the time period considered (increasing production may be expensive or impossible in the short term, for example). Average costs affect the supply curve and are a fundamental component of supply and demand.|$|E
50|$|The diagram to {{the right}} {{demonstrates}} a negative supply shock; The initial position is at point A, producing <b>output</b> <b>quantity</b> Y1 at price level P1. When there is a supply shock, this has an adverse effect on aggregate supply: the supply curve shifts left (from AS1 to AS2), while the demand curve stays in the same position. The intersection of the supply and demand curves has now moved and the equilibrium is now point B; quantity {{has been reduced to}} Y2, while the price level has been increased to P2.|$|E
40|$|In this paper, {{we examine}} how {{uncertainty}} can affect successive mar- kets, when uncertainty can jointly influence both the upstream and down- stream markets conditions. The main {{result of the}} paper is that the equi- librium input and <b>output</b> <b>quantities</b> under stochastic dependence can be higher or lower than the corresponding quantities {{in the case of}} certainty equivalence depending on how much dependent are the events. ...|$|R
50|$|NASTRAN was {{designed}} {{from the beginning}} to consist of several modules. A module is a collection of FORTRAN subroutines designed to perform a specific task - processing model geometry, assembling matrices, applying constraints, solving matrix problems, calculating <b>output</b> <b>quantities,</b> conversing with the database, printing the solution, and so on. The modules are controlled by an internal language called the Direct Matrix Abstraction Program (DMAP).|$|R
30|$|In {{the process}} of modeling, a 1 year {{information}} record provided from SCADA historical measurements and NWP/WRF model historical weather forecasts are used to train an ANFIS that successfully can estimate a transfer function between specific patterns of input and <b>output</b> <b>quantities.</b> Then, BP is applied to optimize {{the parameters of the}} membership functions of ANFIS. This process continues until the prediction error reaches to a suitable value.|$|R
50|$|The {{error signal}} e must be {{transformed}} to the <b>output</b> <b>quantity</b> qo (representing the participant's muscular efforts affecting the mouse position). Experiments {{have shown that}} in the best model for the output function, the mouse velocity Vcursor {{is proportional to the}} error signal e by a gain factor G (that is, Vcursor = G*e). Thus, when the perceptual signal p is smaller than the reference signal r, the error signal e has a positive sign, and from it the model computes an upward velocity of the cursor that is proportional to the error.|$|E
5000|$|The production-possibility {{frontier}} (PPF) is an expository {{figure for}} representing scarcity, cost, and efficiency. In the simplest case an economy can produce just two goods (say [...] "guns" [...] and [...] "butter"). The PPF is a table or graph (as at the right) showing the different quantity combinations {{of the two}} goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible <b>output</b> <b>quantity</b> of the other good.|$|E
5000|$|Prior {{knowledge}} about the true value of the <b>output</b> <b>quantity</b> [...] can also be considered. For the domestic bathroom scale, {{the fact that the}} person's mass is positive, and that it is the mass of a person, rather than that of a motor car, that is being measured, both constitute prior {{knowledge about}} the possible values of the measurand in this example. Such additional information can be used to provide a probability distribution for [...] that can give a smaller standard deviation for [...] and hence a smaller standard uncertainty associated with the estimate of [...]|$|E
40|$|ABSTRACT: Sparse {{polynomial}} chaos expansions {{have recently}} emerged in uncertainty quantification anal-ysis {{as a tool}} to solve high dimensional problems, e. g. stochastic problems involving a few dozens to a few hundred random variables. Based on penalized regression analysis and the so-called least angle regression al-gorithm the method has proven efficiency in a number of applications. The approach was so far rather limited to scalar <b>output</b> <b>quantities,</b> e. g. quantities of interest that are post-processed from the solution of a stochastic partial differential equation (SPDE). In this paper we extend this approach to vector <b>output</b> <b>quantities</b> in order to obtain the complete solution field. This is carried out by using principal component analysis before computing the PC expansions of the various components. As a whole a complete non intrusive framework is obtained that is only based on a set of deterministic solutions of the underlying deterministic problem. The approach is illus-trated by the computation of the displacement field of a tension rod with lognormal, spatially variable Young’s modulus. The problem exhibits 62 stochastic dimensions. ...|$|R
40|$|In {{many studies}} of dynamic systems, the {{stochastic}} aspects {{are as important}} as the dynamic. It is then important to consider uncertainty in the results. Furthermore, dynamics and stochastics interact because the stochastics excite the dynamics and the dynamics change the conditions for the stochastics. Poisson Simulation is an extension of Continuous System Simulation to model and simulate dynamic and stochastic processes. The power of this idea has been described and verified. However, introducing randomness into a CSS model makes the results stochastic. Therefore, a number of simulation runs followed by a statistical analysis have to be performed. Two cases of the statistical analysis must be distinguished: I. Estimation of <b>output</b> <b>quantities,</b> which is straight-forward, and II. Estimation of parameters, which requires a more complex procedure based on bootstrap methods. This documentation presents methods to solve these problems with the help of specially designed tools for repeated simulation and subsequent statistical analysis. Part I presents the tool StocRes for estimation of <b>output</b> <b>quantities</b> and Part II the tool ParmEst for paramete...|$|R
40|$|We compute {{approximate}} {{solutions to}} inverse problems for determining parameters in differential equation models with stochastic data on <b>output</b> <b>quantities.</b> The {{formulation of the}} problem and modeling framework define a solution as a probability measure on the parameter domain for a given σ-algebra. In the case where the number of <b>output</b> <b>quantities</b> is less than the number of parameters, the inverse of the map from parameters to data defines a type of generalized contour map. The approximate contour maps define a geometric structure on events in the σ-algebra for the parameter domain. We develop and analyze an inherently non-intrusive method of sampling the parameter domain and events in the given σ-algebra to approximate the probability measure. We use results from stochastic geometry for point processes to prove convergence of a random sample based approximation method. We define a numerical σ-algebra on which we compute probabilities and derive computable estimates for the error in the probability measure. We present numerical results to illustrate the various sources of error for a model of fluid flow past a cylinder. Comment: 26 pages, 24 figure...|$|R
