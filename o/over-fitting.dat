1099|0|Public
2500|$|Regularized least squares: the {{elements}} of [...] must satisfy [...] (choosing [...] {{in proportion to the}} noise standard deviation of y prevents <b>over-fitting).</b>|$|E
5000|$|Recently, {{the issues}} above were {{resolved}} by , with the proposal of the Bayesian predictive information criterion (BPIC). Ando (2010, Ch. 8) provided a discussion of various Bayesian model selection criteria. To avoid the <b>over-fitting</b> problems of DIC, [...] developed Bayesian model selection criteria from a predictive view point. The criterion is calculated as ...|$|E
50|$|Appropriate {{analytical}} {{approaches for}} toxgnostic studies include candidate gene studies, GWAS and whole genome sequencing. GWAS and whole genome sequencing {{are the most}} comprehensive approaches though careful considerations must {{be applied to the}} relevance, analysis and interpretation of the results to prevent <b>over-fitting,</b> which produces false-positive results, a proposed GWAS workflow is shown below.|$|E
5000|$|One {{variation}} {{that attempts}} to reduce complexity is the star model proposed by Fergus et al. The reduced dependencies of this model allows for learning in [...] time instead of [...] This allows for {{a greater number of}} model parts and image features to be used in training. Because the star model has fewer parameters, it is also better at avoiding the problem of <b>over-fitting</b> when trained on fewer images.|$|E
50|$|An {{ensemble}} {{is itself}} a supervised learning algorithm, {{because it can be}} trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to <b>over-fitting</b> of the training data.|$|E
50|$|The SAMPL {{challenge}} {{seeks to}} accelerate progress in developing quantitative, accurate drug discovery tools by providing prospective validation and rigorous comparisons for computational methodologies and force fields. Computer-aided drug design {{methods have been}} considerably improved over time, along with {{the rapid growth of}} high-performance computing capabilities. However, their applicability in the pharmaceutical industry are still highly limited, due to the insufficient accuracy. Lacking large-scale prospective validations, methods tend to suffer from <b>over-fitting</b> the pre-existing experimental data. To overcome this, SAMPL challenges have been organized as blind tests: each time new datasets are carefully designed and collected from academic or industrial research laboratories, and measurements are released shortly after the deadline of prediction submission. Researchers then can compare those high-quality, prospective experimental data with the submitted estimates.|$|E
50|$|One of {{the main}} issues with {{stepwise}} regression is that it searches a large space of possible models. Hence it is prone to overfitting the data. In other words, stepwise regression will often fit much better in sample than it does on new out-of-sample data. This problem can be mitigated if the criterion for adding (or deleting) a variable is stiff enough. The key {{line in the sand}} is at what {{can be thought of as}} the Bonferroni point: namely how significant the best spurious variable should be based on chance alone. On a t-statistic scale, this occurs at about , where p is the number of predictors. Unfortunately, this means that many variables which actually carry signal will not be included. This fence turns out to be the right trade-off between <b>over-fitting</b> and missing signal. If we look at the risk of different cutoffs, then using this bound will be within a 2logp factor of the best possible risk. Any other cutoff will end up having a larger such risk inflation.|$|E
5000|$|Bayesian {{parameter}} averaging (BPA) is {{an ensemble}} technique {{that seeks to}} approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law. Unlike the Bayes optimal classifier, Bayesian model averaging (BMA) can be practically implemented. Hypotheses are typically sampled using a Monte Carlo sampling technique such as MCMC. For example, Gibbs sampling {{may be used to}} draw hypotheses that are representative of the distribution [...] It has been shown that under certain circumstances, when hypotheses are drawn in this manner and averaged according to Bayes' law, this technique has an expected error that is bounded to be at most twice the expected error of the Bayes optimal classifier. Despite the theoretical correctness of this technique, early work showed experimental results suggesting that the method promoted <b>over-fitting</b> and performed worse compared to simpler ensemble techniques such as bagging; however, these conclusions appear to be based on a misunderstanding of the purpose of Bayesian model averaging vs. model combination. Additionally, there have been considerable advances in theory and practice of BMA. Recent rigorous proofs demonstrate the accuracy of BMA in variable selection and estimation in high-dimensional settings, and provide empirical evidence highlighting the role of sparsity-enforcing priors within the BMA in alleviating overfitting.|$|E
5000|$|In {{the earlier}} works, {{researchers}} employed the Fourier transform technique {{to interpret the}} obtained tactile information for texture classification. However, the Fourier transform is not appropriate for analysing non-stationarysignals in which textures are irregular or non-uniform. Short time Fourier transform or Wavelet {{might be the most}} appropriate techniques to analyse non-stationary signals. However, these methods deal with a large number ofdata points, thereby causing difficulties at the classification step. More features require more training samples resulting in the growth of the computational complexity as well as the risk of <b>over-fitting.</b> To overcome these issues Kaboli et al. [...] proposed a set of fundamental tactile descriptor inspired by Hjorth parameters. Although Hjorth parameters are defined in the time domain, they can be interpreted in the frequency domain as well. The Activity parameter is the total power of the signal. It is also the surface of the power spectrum in the frequency domain (Parseval's theorem). The Mobility parameter is determined as the square root of the ratio of the variance of the first derivative of the signal to that of the signal. This parameter is proportional to a standard deviation of the power spectrum. It is an estimate of the mean frequency. Complexity gives an estimate of the bandwidth of the signal, which indicates the similarity of the shape of the signal to a pure sine wave. Since the calculation of the Hjorth parameters is based on variance, the computational cost of this method is sufficiently low, which makes them appropriate for the real-time task.|$|E
30|$|In the {{previous}} study, two main reasons {{have been addressed}} for model prediction failure in extrapolation investigations: the <b>over-fitting</b> problem of the model and unusual behavior of the process outside of the training data set. An <b>over-fitting</b> problem in the RSM model was not observed because of no evidence of lack of fit in the quadratic model (p_value =  0.6568). In addition, for the ANFIS model, good prediction ability for unseen data, presented to the model as test data, confirmed that the model was clear from any <b>over-fitting</b> problem.|$|E
30|$|To {{evaluate}} the method without <b>over-fitting,</b> we use threefold cross-validation.|$|E
30|$|Moreover, {{we assume}} that our NMF-based VC creates a natural-sounding voice {{compared}} to statistical VC. In [6], <b>over-fitting</b> and over-smoothing problems are reported in statistical VC. Because our NMF-based VC is not a statistical but an exemplar-based method, {{we assume that}} our approach can avoid the <b>over-fitting</b> problem and create a natural-sounding voice.|$|E
30|$|While the {{contrast}} between Figs.  2 and 3 shows that in small-world networks OAS_tg may be particularly susceptible to <b>over-fitting</b> at higher uniform thresholds, when considering larger community sizes, {{the contrast}} between Figs.  4 and 5 shows that both significant <b>over-fitting</b> and overspending may impact a traditional greedy planner with access only to noisy G' (except at the lowest thresholds).|$|E
40|$|For {{practical}} automatic speaker verification (ASV) systems, {{replay attack}} poses a true risk. By replaying a pre-recorded speech signal of the genuine speaker, ASV systems {{tend to be}} easily fooled. An effective replay detection method is therefore highly desirable. In this study, we investigate a major difficulty in replay detection: the <b>over-fitting</b> problem caused by variability factors in speech signal. An F-ratio probing tool is proposed and three variability factors are investigated using this tool: speaker identity, speech content and playback & recording device. The analysis shows that device is the most influential factor that contributes the highest <b>over-fitting</b> risk. A frequency warping approach is studied to alleviate the <b>over-fitting</b> problem, as verified on the ASV-spoof 2017 database...|$|E
40|$|In {{the field}} of {{empirical}} modeling using Genetic Programming (GP), {{it is important to}} evolve solution with good generalization ability. Generalization ability of GP solutions get affected by two important issues: bloat and <b>over-fitting.</b> We surveyed and classified existing literature related to different techniques used by GP research community to deal with these issues. We also point out limitation of these techniques, if any. Moreover, the classification of different bloat control approaches and measures for bloat and <b>over-fitting</b> are also discussed. We believe that this work will be useful to GP practitioners in following ways: (i) to better understand concepts of generalization in GP (ii) comparing existing bloat and <b>over-fitting</b> control techniques and (iii) selecting appropriate approach to improve generalization ability of GP evolved solutions...|$|E
40|$|In {{this paper}} we {{investigate}} {{the performance of}} penalized variants of the forwards-backwards algorithm for training Hidden Markov Models. Maximum likelihood estimation of model parameters can result in <b>over-fitting</b> and poor generalization ability. We discuss the use of priors to compute maximum a posteriori estimates and describe a number of experiments in which models are trained under different conditions. Our results show that MAP estimation can alleviate <b>over-fitting</b> and help learn better parameter estimates...|$|E
30|$|K- 1 occurs, {{which implies}} the {{appearance}} of <b>over-fitting.</b> The algorithm finally outputs the clusters available in the (K- 2)th iteration.|$|E
30|$|Finally, {{the optimal}} neural {{structure}} is attained using {{trial and error}} strategy. The entire weather data obtained from ([URL] is divided into training (50 %), validation (30 %) and test set (20 %). To forecast 2 days ahead, output layer contains two neurons. To handle the errors in the training phase and to avoid <b>over-fitting,</b> the neural network is evaluated after certain iterations and its training is stopped immediately, if an error occurred or <b>over-fitting</b> is observed.|$|E
40|$|Keywords-:BP network, GABP model, <b>over-fitting</b> operation, {{transformer}} {{fault diagnosis}} Abstract. According to the measured gas content in power transformers, we use BP neural network {{to accomplish the}} pattern recognition of transformer fault. The recognition effect of BP network pattern was studied from the aspects of adding <b>over-fitting</b> operation and genetic algorithm. Four kinds of neural network models, BP model,BP & over-fitted identification model,GABP model and GABP & over-fitted identification model, were constructed respectively, making the pattern recognition effect further enhanced. I...|$|E
40|$|Model {{selection}} {{strategies for}} {{machine learning algorithms}} typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator {{can be broken down}} into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for <b>over-fitting</b> in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to <b>over-fitting</b> the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of <b>over-fitting</b> are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias {{as a result of this}} form of <b>over-fitting</b> and hence are unreliable. We discuss methods to avoid <b>over-fitting</b> in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds...|$|E
30|$|Stage 3 : When υ 1  <  0.05, RMSE {{increases}} dramatically {{with the}} reducing υ 1, {{due to the}} <b>over-fitting</b> effect with a redundant sample dictionary.|$|E
30|$|Also, in this case, higher {{orders were}} {{excluded}} {{to avoid an}} <b>over-fitting</b> of the data and to avoid modeling artefacts that would be introduced by the periodicity.|$|E
30|$|Hyper-parameters of CNN are {{determined}} {{in an ad}} hoc manner without applying parameter tuning. However, we confirmed that <b>over-fitting</b> was not occurred and learnt parameters were converged sufficiently.|$|E
40|$|Being a cross-camera {{retrieval}} task, person re-identification {{suffers from}} image style variations caused by different cameras. The art implicitly addresses {{this problem by}} learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle {{can serve as a}} data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against <b>over-fitting,</b> also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which <b>over-fitting</b> often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of <b>over-fitting.</b> We also report competitive accuracy compared with the state of the art...|$|E
40|$|Deep neural {{networks}} are learning models {{with a very}} high capacity and therefore prone to <b>over-fitting.</b> Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt {{to solve the problem}} of <b>over-fitting</b> by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces <b>over-fitting</b> without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures...|$|E
30|$|Since {{non-linear}} {{method of}} mapping camera responses onto reflectance values may cause <b>over-fitting</b> the characterization surface, regularization {{can be done}} {{as described in the}} subsection below to solve this problem.|$|E
3000|$|... aEarly {{stopping}} rule {{is a method}} to avoid <b>over-fitting,</b> but it improves the generalization ability of ANNs. This method halts the training process when the performance with validation data stops improving.|$|E
3000|$|... [...]. Even though {{using only}} one {{sequence}} of observations to learn an HMM {{might lead to}} <b>over-fitting,</b> this technique is only an intermediate step that aims to capture the characteristics of each sequence. The produced HMM model is meant to give a maximal description of each sequence, and therefore, <b>over-fitting</b> {{is not an issue}} in this context. In fact, it is desired that the model perfectly fits the observation sequence. In this case, the likelihood of each sequence with respect to its corresponding model is expected to be higher than those with respect to the remaining models.|$|E
40|$|Abstract — We {{demonstrate}} that a simple hidden Markov model can achieve {{state of the}} art performance in unsupervised part-of-speech tagging, by improving aspects of standard Baum-Welch (EM) estimation. One improvement uses word similarities to smooth the lexical tag ¢ word probability estimates, which avoids <b>over-fitting</b> the lexical model. Another improvement constrains the model to preserve a specified marginal distribution over the hidden tags, which avoids <b>over-fitting</b> the tag ¢ tag transition model. Although using more contextual information than an HMM remains desirable, improving basic estimation still leads to significant improvements and remains a prerequisite for training more complex models. I...|$|E
3000|$|... 1). A {{cross-validation}} {{strategy has}} been used during training to avoid <b>over-fitting,</b> following the k-fold approach with k[*]=[*] 5. The validation set is composed of 10, 000 patterns, 5, 000 from each hypothesis.|$|E
30|$|These {{approaches}} often {{suffer from}} over-smoothing or <b>over-fitting</b> problems. GMM-based approaches represent acoustic features using multiple Gaussian distributions, which are estimated by averaging observations with similar context descriptions in the training. Therefore, the outputs of the GMM distribute near the modes (means) of the Gaussians, which causes problems with over-smoothing. Furthermore, <b>over-fitting</b> problems arise when we give more Gaussian mixtures due to precise {{estimation of the}} observed distribution. In the NN-based approaches, the model is often over-fitted due to its complexity because it exaggerates small fluctuations in the unknown data {{if the number of}} training data is not enough relative to the number of parameters.|$|E
40|$|Decision {{trees are}} among the most {{effective}} and interpretable classification algorithms while ensembles techniques have been proven to alleviate problems regarding <b>over-fitting</b> and variance. On the other hand, decision trees show a tendency to lack stability given small changes in the data, whereas interpreting an ensemble of trees is challenging to comprehend. We propose the technique of Ensemble-Trees which uses ensembles of rules within the test nodes to reduce <b>over-fitting</b> and variance effects. Validating the technique experimentally, we find that improvements in performance compared to ensembles of pruned trees exist, but also that the technique does less to reduce structural instability than could be expected. status: publishe...|$|E
40|$|The study {{presents}} {{two approaches}} {{to increase the}} generalization capability, or to overcome the <b>over-fitting</b> tendency, of neural networks so that their prediction accuracies for unseen data can be further enhanced. The use of early stopping and Bayesian regularization approaches are considered. Data used are the artificial Mackey-Glass time series and the real time series of Mississippi River. Results show that the Bayesian regularization approach is best to overcome the <b>over-fitting</b> problems. It is observed that in the scenario when the data set considered is quite clean and large in size, the overfitting effect is very small; thus, only marginal prediction improvement can be expected from the proposed approaches...|$|E
3000|$|... [...]). The regularizer λ = 25 was empirically chosen. The {{summation}} {{is performed}} {{over all the}} features in the feature vector. The combination helps avoid <b>over-fitting</b> and balances the preservation of clinically relevant events and morphological similarity.|$|E
3000|$|... where JCDL(W,b) is {{an energy}} function, M {{is the number}} of {{training}} samples, the first term is an average sum-of-squares error term within all the samples, and the second term is the Frobenius norm that prevents <b>over-fitting.</b>|$|E
30|$|AdaBoost is a fast, {{simple and}} easy to use {{iterative}} algorithm which needs only one parameter to tune, i.e. number of iteration, T. It does not subject to <b>over-fitting</b> and easily identifies outliners which are either misclassified or hard to classify.|$|E
