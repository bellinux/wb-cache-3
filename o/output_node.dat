259|476|Public
25|$|Figure 3 {{indicates}} the <b>output</b> <b>node,</b> {{but not the}} choice of output variable. A useful choice is the short-circuit current output of the amplifier (leading to the short-circuit current gain). Because this variable leads simply {{to any of the}} other choices (for example, load voltage or load current), the short-circuit current gain is found below.|$|E
25|$|In {{practice}} the ideal impedances are {{not possible to}} achieve. For any particular circuit, a small-signal analysis {{is often used to}} find the actual impedance. A small-signal AC test current Ix is applied to the input or <b>output</b> <b>node,</b> all external sources are set to AC zero, and the corresponding alternating voltage Vx across the test current source determines the impedance seen at that node as R = Vx / Ix.|$|E
25|$|Sentinel node may simplify certain list operations, by {{ensuring}} that the next or previous nodes exist for every element, and that even empty lists {{have at least one}} node. One may also use a sentinel node {{at the end of the}} list, with an appropriate data field, to eliminate some end-of-list tests. For example, when scanning the list looking for a node with a given value x, setting the sentinel's data field to x makes it unnecessary to test for end-of-list inside the loop. Another example is the merging two sorted lists: if their sentinels have data fields set to +âˆž, the choice of the next <b>output</b> <b>node</b> does not need special handling for empty lists.|$|E
5000|$|An <b>output</b> or sink <b>node</b> {{has only}} {{incoming}} branches (represents a dependent variable). Although any node {{can be an}} <b>output,</b> explicit <b>output</b> <b>nodes</b> are often used to provide clarity. Explicit <b>output</b> <b>nodes</b> are characterized by having one or more attached arrows pointing into the node and no arrows pointing away from the <b>node.</b> Explicit <b>output</b> <b>nodes</b> are not required.|$|R
5000|$|Extension {{neural network}} has a neural network like appearance. Weight vector resides between the input <b>nodes</b> and <b>output</b> <b>nodes.</b> <b>Output</b> <b>nodes</b> are the {{representation}} of input nodes by passing them through the weight vector.|$|R
30|$|Wire {{the input}} and <b>output</b> <b>nodes</b> {{of all the}} {{functions}} that appear on the block diagram.|$|R
5000|$|... : Gets {{data from}} its input node, {{temporarily}} stores {{it in an}} internal buffer of size , and propagates it to its <b>output</b> <b>node</b> (whenever this <b>output</b> <b>node</b> is ready to take data).|$|E
50|$|Is a given number n {{a member}} of the <b>output</b> <b>node.</b>|$|E
5000|$|... : Atomically gets {{data from}} its input node and propagates {{it to its}} <b>output</b> <b>node.</b>|$|E
40|$|Suppose we {{concatenate}} two directed graphs, each isomorphic to a $d$ dimensional butterfly (but {{not necessarily}} identical to each other). Select any set of $ 2 ^k$ input and $ 2 ^k$ <b>output</b> <b>nodes</b> on the resulting graph. Then there exist node disjoint paths from the input <b>nodes</b> to the <b>output</b> <b>nodes.</b> If we take two standard butterflies and permute {{the order of}} the layers, then the result holds on sets of any size, not just powers of two...|$|R
50|$|Jnet uses two neural {{networks}} for its prediction. The first network is fed with {{a window of}} 17 residues over each amino acid in the alignment plus a conservation number. It uses a hidden layer of nine nodes and has three <b>output</b> <b>nodes,</b> one for each secondary structure element.The second network is fed with a window of 19 residues (the result of first network) plus the conservation number. It has a hidden layer with nine nodes and has three <b>output</b> <b>nodes.</b>|$|R
30|$|We remark the our CRNs {{need to have}} {{at least}} two <b>output</b> <b>nodes,</b> one for the desired product and one to collect all waste products.|$|R
5000|$|Is the <b>output</b> <b>node</b> empty, does it {{contain a}} {{specific}} element, is it equal to ? ...|$|E
5000|$|... : Same as Sync, but {{can lose}} data if its <b>output</b> <b>node</b> {{is not ready}} to take data.|$|E
50|$|The {{output drive}} of a gate {{for a given}} input {{is equal to the}} drive at its <b>output</b> <b>node.</b>|$|E
40|$|In {{this paper}} a network will be {{considered}} an acyclic graph. It has several input nodes (inputs) and some (at least one) <b>output</b> <b>nodes</b> (<b>outputs).</b> The <b>nodes</b> of the network are characterized by fan-in (the number of incoming edges) and fan-out (the number of outgoing edges), while the network has a certain size (the numbe...|$|R
50|$|INK (for I/O Node Kernel) is the {{operating}} system that runs on the input <b>output</b> <b>nodes</b> of the IBM Blue Gene supercomputer. INK is a Linux-derivative.|$|R
3000|$|Therefore, <b>output</b> <b>nodes</b> have {{equivalent}} path-sets (i.e. the path-sets {{have equal}} availabilities and transition rates). For each configuration, one output is considered, and their path-sets are: [...]...|$|R
5000|$|Forward path. A {{path from}} an input node (source) to an <b>output</b> <b>node</b> (sink) {{that does not}} re-visit any node.|$|E
50|$|Forward path: A {{path from}} an input node to an <b>output</b> <b>node</b> {{in which no}} node is touched more than once.|$|E
50|$|The {{first problem}} is {{reducible}} {{to the third}} one, by asking if the node n is {{a subset of the}} <b>output</b> <b>node.</b>|$|E
40|$|The Cascade-Correlation {{architecture}} {{which is}} designed for classification tasks has been studied {{to see if it}} can be modified to perform function evaluation and interpolation tasks. 1. Introduction It is well known that the Cascade-Correlation (Cascor) architecture developed by Fahlman [2] results in a very powerful classification system. In this system it is typical to have as many <b>output</b> <b>nodes</b> as there are classes and to train them to give one of two extreme values. The activation function of the <b>output</b> <b>nodes</b> is non-linear, typically a squashing function such as a sigmoid. Under these circumstances what is important is that the input to the squashing function has a value that effectively forces the node to be hard on or hard off. The actual value needed to do this is of less importance. For such tasks as function evaluation and interpolation the actual output values are important. In these cases linear activation functions are used in the <b>output</b> <b>node(s).</b> If Cascor is used in this m [...] ...|$|R
30|$|Recurrent neural {{networks}} contain connections from <b>output</b> <b>nodes</b> to hidden layer and/or input layer nodes, and they allow interconnections between nodes of same layer, particularly between the nodes of hidden layers.|$|R
5000|$|... {{such that}} , for all [...]If [...] is a channel, then [...] {{is called the}} set of input nodes of [...] and [...] is called the set of <b>output</b> <b>nodes</b> of [...]|$|R
5000|$|... : Atomically gets {{data from}} its input node and propagates {{it to its}} <b>output</b> <b>node</b> if the filter {{condition}} [...] is satisfied; loses the data otherwise.|$|E
5000|$|The {{derivative}} to {{be calculated}} depends on the induced local field , which itself varies. It is easy to prove that for an <b>output</b> <b>node</b> this derivative can be simplified to ...|$|E
50|$|As an {{algorithmic}} problem, {{the problem}} is to find if a given natural number {{is an element of}} the <b>output</b> <b>node</b> or if two circuits compute the same set. Decidability is still an open question.|$|E
40|$|Neural networks, {{trained with}} the {{backpropagation}} algorithm have: {{been applied to}} various classification problems. For linearly separable and nonseparahle problems, they {{have been shown to}} approximate the a posteriori probability of an input vector X belonging to a specific class C. In order to achieve high accuracy, large training data sets have to be used. For a small number of input dimensions, the accuracy of estimation was inferior to estimates using the Parzen density estimation. In this thesis, we propose two new techniques, lowering the mean square estimation error drastically and achieving better classification. In the past, t:he desired output patterns used for training have been of binary nature, using one for the class C the vector belongs to, and zero for the other classes. This work will show that by training against the columns of a Hadamard matrix, and then taking the inverse Hadamard transform of the network output, we can obtain more accurate estimates. The second change proposed in comparison with standard backpropagation networks will be the use of redundant <b>output</b> <b>nodes.</b> In standard backpropagat:ion the number of <b>output</b> <b>nodes</b> equals the number of different classes. In this thesis, it is shown that adding redundant <b>output</b> <b>nodes</b> enables us to decrease the mean square error at the output further, reaching better classification and lower mean square error rates than the Parzen density estimator. Comparisons between the statistical methods, the Parzen density estimation and histogramming, the conventional neural network and the Hadamard transformed neural network with redundant <b>output</b> <b>nodes</b> are given. Further, the effects of the proposed changes to the backpropagation algorithm on the convergence speed and the risk of getting stuck in a local minimum are: studied...|$|R
40|$|FDREMOR A simple {{selective}} learning algorithm for {{use with}} Hultilayer Per-ceptrons (MLPs) is presented. This algorithm has proved useful in certain types of problems where learning failure occurs using standard back propaga-tion. Examples of these problems are included. The algorithm {{is based on the}} rms output error, computed across all <b>output</b> <b>nodes</b> and all training patterns. The learning rate is decreased for all individual <b>output</b> <b>nodes</b> each time the error is less than a user chosen multiple of the rms error corresponding to the previous pass. This algorithm has produced convergence where the standard fixed gain back propagation failed. / This work has been supported by the Warfare Systems Architecture and Engineering Program and has been conducted in the Space and Ocean Geodesy Branch...|$|R
5000|$|Consider {{an ideal}} {{inverting}} voltage amplifier of gain [...] with an impedance [...] connected between its input and <b>output</b> <b>nodes.</b> The <b>output</b> voltage is therefore [...] Assuming that the amplifier input draws no current, {{all of the}} input current flows through , and is therefore given by ...|$|R
50|$|Complementary metal-oxide-semiconductor (CMOS) is a {{non-volatile}} medium. It {{is used in}} microprocessors, microcontrollers, static RAM, {{and other}} digital logic circuits. Memory is read {{through the use of}} a combination of p-type and n-type metal-oxide-semiconductor field-effect transistors (MOSFETs). In CMOS logic, a collection of n-type MOSFETs are arranged in a pull-down network between the <b>output</b> <b>node</b> and the lower-voltage power supply rail, named Vss, which often has ground potential. By asserting or de-asserting the inputs to the CMOS circuit, individual transistors along the pull-up and pull-down networks become conductive and resistive to electric current, and results in the desired path connecting from the <b>output</b> <b>node</b> to one of the voltage rails.|$|E
50|$|As {{a special}} case, a propositional formula or Boolean {{expression}} is a Boolean circuit {{with a single}} <b>output</b> <b>node</b> in which every other node has fan-out of 1. Thus, a Boolean circuit {{can be regarded as}} a generalization that allows shared subformulas and multiple outputs.|$|E
50|$|Figure 6 {{indicates}} the <b>output</b> <b>node,</b> {{but does not}} indicate the choice of output variable. In what follows, the output variable is selected as the short-circuit current of the amplifier, that is, the collector current of the output transistor. Other choices for output are discussed later.|$|E
50|$|The {{feedforward}} {{neural network}} {{was the first and}} simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the <b>output</b> <b>nodes.</b> There are no cycles or loops in the network.|$|R
50|$|This {{process is}} {{repeated}} for each input vector for a (usually large) {{number of cycles}} Î». The network winds up associating <b>output</b> <b>nodes</b> with groups or patterns in the input data set. If these patterns can be named, the names can {{be attached to the}} associated nodes in the trained net.|$|R
50|$|The three final <b>output</b> <b>nodes</b> {{deliver a}} {{score for each}} {{secondary}} structure element for the central position of the window. Using the secondary structure with the highest score, PSIPRED generates the protein prediction. The Q3 value is the fraction of residues predicted correctly in the secondary structure states, namely helix, strand, and coil.|$|R
