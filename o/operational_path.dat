14|46|Public
5000|$|Until the {{war with}} the Soviet Union was finished, Hitler was {{reluctant}} to have a war with the United States, and insisted upon avoiding [...] "incidents" [...] with the U.S. Navy as much as possible, whereas Raeder was all for a war with the United States. Hitler had cancelled the Z Plan again in late 1940, only to order it restarted {{in the middle of}} 1941 when it seemed that the war against the Soviet Union would soon be over and again cancelled the Z Plan in late 1941. When Hitler cancelled the Z Plan for the final time, Raeder forgot to cancel a contact he had placed with engineering firms for the engines of the first four of the planned H-class super battleships. As a result of that oversight, in June 1944 the Kriegsmarine had to accept and pay for four gigantic engines that were meant to power battleships that did not exist. From Hitler's viewpoint, it was better to wait until the Z Plan was complete before going to war with the United States. Raeder by contrast thought only of the [...] "immediate operational advantages" [...] that would accrue to Germany if the Reich went to war with the United States. On 11 December 1941, Germany declared war on the United States, which was at least in part due to the pressure of Raeder, who was very pleased with going to war with America. Even before the declaration of war on 11 December, Hitler had given orders to Raeder on 8 December 1941 that the Kriegsmarine could now sink on sight American warships and warships of all the Latin American republics except Argentina as well. Raeder gave orders that Kriegsmarine was now to begin Operation Drumbeat, the plan to defeat the United States by sending [...] "wolf-packs" [...] of submarines off the Atlantic coast of the United States to destroy all American shipping. On 12 December 1941, Raeder told Hitler that prospects for victory over the United States were good and that [...] "The situation in the Atlantic will be eased by Japan's successful intervention". Continuing his analysis of the naval situation, Raeder told Hitler: [...] "Reports have already been received of the transfer of some American battleships from the Atlantic to the Pacific. It is certain that light forces, especially destroyers will be required in increased numbers in the Pacific. The need for transport ships will be very great, so that a withdrawal of American merchant ships from the Atlantic can be expected. The strain on British merchant shipping will increase ... The U.S will have to concentrate all her strength in the Pacific during the next few months. Britain will not to run any risks after her severe losses of big ships is referring to sinkings of the [...] It is hardly likely that transport tonnage is available for such occupation tasks or bringing up supplies ... It is improbable that the enemy will give up East Asia even temporarily; by so doing Britain will endanger India very seriously, and the U.S. cannot withdraw her fleet from the Pacific as long as the Japanese fleet has the upper hand". [...] Much to Raeder's annoyance, Hitler followed up declaring war on the U.S. by sending 23 U-boats to the Mediterranean to attack British shipping and another 16 to Norway to guard against a phantom British invasion instead of focusing the U-boat fleet off the eastern United States. Because the United States Navy under the leadership of Admiral Ernest King was not ready for anti-submarine warfare, U-boat operations off the east coast of America {{in the first half of}} 1942 were very successful, and only the diversion of the U-boat fleet to the Mediterranean and Norway kept them from being more successful. The entry of the United States into the war meant the ultimate defeat of the Kriegsmarine as the tremendous productive capacity of American industry meant that the Allies could replace every ship sunk by the U-boats, and then build some more. In 1943, American shipyards turned out enough ships to almost equal the number of all the ships sunk by U-boats between 1939 and 1942. Murray and Millet accused Raeder and the rest of the Seekriegsleitung of wanting war with America because the United States was an [...] "easy target" [...] and of [...] "taking the easiest tactical and <b>operational</b> <b>path</b> without the slightest thought to the strategic or long-range consequences".|$|E
40|$|Intra-operative {{ultrasound}} (iUS) {{can generate}} 2 D images in real-time {{as well as}} near real-time 3 D datasets of the current situation during an intervention. Tracked ultrasound can locate the images in 3 D space and relate them to patient, devices, and pre-operative planning data. Therefore, tracked US is an efficient means for controlling the validity of pre-operative planning, recognition of changes (brain shift) during the intervention, replanning of the <b>operational</b> <b>path</b> due to situational changes (iterative navigation), and finally, controlling the results (residual tumor). This paper describes a neuronavigation system exploiting this potential of interventional tracked US for permanent control of intervention progress and iterative adaptation of the planned procedure to the current situation...|$|E
40|$|Architectural {{interventions}} in historical contexts {{throughout most of}} the 20 th century were inter-linked by the two extremes of conservation and transformation. In the 1980 s, in Italy, an important step forward was taken along the theoretical and <b>operational</b> <b>path</b> towards the approach as demanded by the current and future conditions of historical urban areas: long-term strategy oriented; more responsible; multi-scale and including the entire contemporary urban settlement. The experience of Culotta & Leone in the Historic Centre of Cefalù may well constitute a unicum, {{as a result of their}} particular anchoring to the social context, and yet it offers an interesting example of continuity (between plan, architectural design and execution) that gave substance to architectural solutions which may be seen as an evolving continuity of the historical remains. </p...|$|E
25|$|The company's hotels, {{which had}} closed {{during the war}} years, were reopened to the public by mid-1947 {{although}} the Midland Station Hotel in Belfast, which had suffered severe damage during the 1941 Blitz, was not fully <b>operational.</b> <b>Paths</b> and bridges at Glenariff were repaired but the Gobbins cliff path, on which maintenance had ceased in 1942, would not reopen under NCC management.|$|R
40|$|This paper {{describes}} {{the concept of}} Time-Expanded Decision Networks (TDN), a new methodology to design and analyze flexibility in large-scale complex systems. This includes a preliminary application of the methodology {{to the design of}} Heavy Lift Launch Vehicles for NASA’s space exploration initiative. Synthesizing concepts from Decision Theory, Real Options Analysis, Network Optimization, and Scenario Planning, TDN provides a holistic framework to quantify the value of system flexibility, analyze development and <b>operational</b> <b>paths,</b> and identify designs which can allow managers and systems engineers to react more easily to exogenous uncertainty. TDN consists of five principle steps, which can be implemented as a software tool: 1. Design a set of potential system configurations 2. Quantify switching costs to create a “static network ” that captures the difficulty of switching among these configurations 3. Create a time-expanded decision network by expanding the static network in time, including chance and decision nodes 4. Evaluate minimum cost paths through the network under plausible operating scenarios 5. Modify the set of initial design configurations to exploit high-leverage switches and repeat the process to convergence. Results can inform decisions about how and where to embed flexibility in order to enable system evolution along various development and <b>operational</b> <b>paths...</b>|$|R
40|$|Various {{hypotheses}} exist {{about the}} paths used for {{communication between the}} nodes of complex networks. Most studies simply suppose that communication goes via shortest paths, while others have more explicit assumptions about how routing (alternatively navigation or search) works or should work in real networks. However, these assumptions are rarely checked against real data. Here we directly analyze the structure of <b>operational</b> <b>paths</b> using real measurements. For this purpose we use existing and newly created datasets having both the topology of the network and {{a sufficient number of}} empirically-determined paths over it. Such datasets are processed for air transportation networks, the human brain, the Internet and the fit-fat-cat word ladder game. Our results suggest that from the great number of possible paths, nature seems to pick according to some simple rules, which we will refer to as routing policies. First we confirm, that the preference of short paths is an inevitable policy element, however the observed stretch of the paths suggests that there are other policies at work simultaneously. We identify two additional policies common in our networks: the "conform hierarchy", meaning that the paths should obey the structural hierarchy of the network, and the "prefer downstream" policy which promotes avoiding the network core if possible. Building upon these simple policies, we propose a synthetic routing policy which can recover the basic statistical properties of the <b>operational</b> <b>paths</b> in networks. Our results can be helpful in estimating the reaction of complex systems for stress coming from the outside more accurately than the shortest path assumption permits. Comment: 8 pages 5 figure...|$|R
40|$|Copyright © 2014 Chao-Chun Ting and Min-Sheng Lin. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Consider a probabilistic graph G in which the edges are perfectly reliable, but vertices may fail with some known probabilities. The 2 -terminal reliability of G {{is defined as the}} probability that one <b>operational</b> <b>path</b> exists between a given source and destination pair vertices of G. This 2 -terminal reliability problem is known to be #P-complete for general graphs but solvable in polynomial time for interval graphs. This work presents a polynomial-time algorithm for computing the 2 -terminal reliability of probe interval graphs, which is a superclass of interval graphs...|$|E
40|$|Communication between {{processors}} is {{the essence}} of distributed computing: clearly, without communication distributed computation is impossible. However, as networks become larger and larger, the frequency of link failures increases. The End-to-End Communication is a classical problem that asks how to carry out fault-free communication between two processors over a network, in spite of such frequent communication faults. The sole minimum assumption is that the two processors that are trying to communicate are not permanently disconnected (i. e., the communication should proceed even in the case that there does not (ever) simultaneously exist, at any time, any <b>operational</b> <b>path</b> between the two processors that are trying to communicate.) For the first time, we present a protocol which solves this fundamental problem with logarithmicspace and polynomial-communication at the same time. This is an exponential memory improvement to all previous polynomial-communication solutions. That is, all [...] ...|$|E
40|$|In recent years, {{there has}} been a renewed {{interest}} in developing faster and simpler transfer path analysis (TPA) methods. A dominant class of these new approaches, often referred to as <b>Operational</b> <b>Path</b> Analyses (OPA), is designed to achieve this goal by using only operational data in conjunction with the application of the transmissibility concept. Despite the reduction in measurement time and complexity, these suffer from a number of limitations, such as problems related to the estimation of transmissibility, or the unreliability of the results due to cross coupling between path inputs, etc, which makes them prone to errors. Some of these only apply to one specific method while others are common to all transmissibility based approaches. The goal {{of this paper is to}} identify and describe these limitations and point out the potential dangers of applying such methods without taking these into account. ...|$|E
40|$|Abstract—Load {{testing of}} web {{applications}} {{can be specified}} by simulating realistic user behavior with stochastic form-oriented analysis models. Stochastic models have advantages over load test models that simply play back recorded session data: they are easier to specify and achieve a higher coverage of the different <b>operational</b> <b>paths.</b> There are challenges when specifying load tests such as the generation of form parameters and the recognition of pages returned by the system. We propose how these challenges can be overcome by adding additional specifications to a form-oriented model. Furthermore, we discuss several workload models and explain why some commonly used workload models are in fact unrealistic and produce misleading results. The stochastic form-oriented load testing approach can be generalized to deal with other submit-response systems such as those consisting of web services. I...|$|R
40|$|Movement of materials, {{plant and}} site {{operative}} {{from one place}} to another on construction sites and construction workplaces are of paramount importance to site planners as savings in travel distance can reduce cost and increase productivity. In addition, risks on construction sites can be reduced if the use of vehicles and mobile plant is properly managed by setting out paths avoiding high risks areas. The work reported in this paper presents a framework for supporting path planning analysis of construction sites based on multi-objective evaluation of transport cost, safety, and visibility. This paper investigates the use of fuzzy-based multi-objective optimisation approach in making a more informed strategic decisions regarding the movement path of people and vehicles on construction sites, and detailed decisions regarding travel distance and <b>operational</b> <b>paths</b> on workplaces, enabling site planners to examine paths scenarios that are subjected to a high degree of uncertainty and subjectivity...|$|R
40|$|<b>Operational</b> Transfer <b>Path</b> Analysis (OTPA) is very {{attractive}} {{from a practical}} point of view, as it not requires an expensive measurement campaign in which transfer functions are measured, {{as is the case}} in the traditional Transfer Path Analysis (TPA). Instead, transmissibilities are measured from operational data, making the method relatively cheap in terms of measurement efforts. In practice, however, a lot of difficulties have to be overcome in order to obtain reliable path estimates. In this paper the <b>Operational</b> Transfer <b>Path</b> Analysis (OTPA) technique is applied to a small gearbox. Laboratory experiments are presented, discussing the use of singular value decomposition to determine the number of physical transmission paths involved, the development of verification checks, and finally the application of OTPA to estimate the strengths of the transmission paths. A comparison with the classical Transfer Path Analysis (TPA) will be included as well. Shortcomings of both the OTPA and TPA method are discussed and conclusions are drawn from the experiments. status: publishe...|$|R
40|$|A {{comparison}} between transfer path analysis and <b>operational</b> <b>path</b> analysis methods using an electric vehicle {{is presented in}} this study. Structure-borne noise paths to the cabin from different engine and suspension points have been considered. To realise these methods, two types of test have been performed; operational tests on a rolling road and hammer tests in static conditions. The main aim of this work is assessing the critical paths which are transmitting the structure-borne vibrations from the electric vehicle's vibration sources to the driver's ear. This assessment includes {{the analysis of the}} noise contribution of each path depending on the frequency and vehicle speed range and moreover, the assessment of the path noise impact for harmonic orders which arise due to the physical components of the electric vehicle. Furthermore, the applicability of these methods to electric vehicles is assessed as these techniques have been extensively used for vehicles powered with internal combustion engines...|$|E
40|$|This {{paper was}} {{accepted}} for publication in the journal Applied Acoustics and the definitive published version is available at [URL] comparison between transfer path analysis and <b>operational</b> <b>path</b> analysis methods using an electric vehicle is presented in this study. Structure-borne noise paths to the cabin from different engine and suspension points have been considered. To realise these methods, two types of test have been performed; operational tests on a rolling road and hammer tests in static conditions. The main aim of this work is assessing the critical paths which are transmitting the structure-borne vibrations from the electric vehicle's vibration sources to the driver's ear. This assessment includes {{the analysis of the}} noise contribution of each path depending on the frequency and vehicle speed range and moreover, the assessment of the path noise impact for harmonic orders which arise due to the physical components of the electric vehicle. Furthermore, the applicability of these methods to electric vehicles is assessed as these techniques have been extensively used for vehicles powered with internal combustion engines...|$|E
40|$|The NPOESS Program was {{designed}} primarily to serve operational users who typically need near real time observations and products. Consequently, NPOESS {{does not provide}} reprocessing, data record gap filling, or assurance that its products {{are consistent with those}} of heritage missions. However, these characteristics are critical for climate science and applications since climate signatures are generally small compared to normal observation variability. In this article, we describe a joint NOAA, NASA and USGS program plan to develop climate data records (CDRs). The proposed program systematically evolves a candidate algorithm through a 6 -level research and <b>operational</b> <b>path</b> to maturity, and includes ongoing algorithm maintenance and technology insertion. The proposed program is jointly managed by the responsible agencies, but its execution relies extensively on community expertise and resources. The CDRs resulting from this program would provide a comprehensive set of climate data and information records (CIRs) useful for spatio-temporal detection, analysis and prediction of environmental change, and for development of a complete and coherent environment for climate model execution...|$|E
40|$|There {{have been}} several {{approaches}} to provisioning traffic between core network nodes in Internet Service Provider (ISP) networks. Such approaches aim to minimize network delay, increase network capacity, and enhance network security services. MATE (Multipath Adaptive Traffic Engineering) protocol has been proposed for multipath adaptive traffic engineering between an ingress node (source) and an egress node (destination). Its novel idea is to avoid network congestion and attacks that might exist in edge and node disjoint paths between two core network nodes. This paper builds an adaptive, robust, and reliable traffic engineering scheme for better performance of communication network operations. This will also provision quality of service (QoS) and protection of traffic engineering to maximize network efficiency. Specifically, we present a new approach, S-MATE (secure MATE) is developed to protect the network traffic between two core nodes (routers or switches) in a cloud network. S-MATE secures against a single link attack/failure by adding redundancy {{in one of the}} <b>operational</b> <b>paths</b> between the sender and receiver. The proposed scheme can be built to secure core networks such as optical and IP networks...|$|R
30|$|The {{field of}} network science might {{provide us with}} some answers. In this field, {{assessing}} a complex network’s reliability is usually formulated as a vulnerability problem. Studies extract the underlying problem domain as a graph G (N, L) {{with a set of}} Nodes N and Links L between the nodes and study topological connectivity after potential disruption events. As many real life networks have heterogeneous distribution of connectivity, removal of certain nodes or links would result in greater damage to the network than others. Global measures that assess vulnerability include operational pairs (Grubesic et al. 2008), <b>operational</b> <b>paths</b> (Jenelius et al. 2006), minimum shortest paths, cyclomatic number, maximum network circuits, alpha index, beta index (see Newman 2010 for a review). On the other hand, local measures examine the individual nodes or links whose damage would impact the network the most. These include betweenness centrality, degree centrality, closeness centrality, eigenvector centrality amongst others (Borgatti and Everett 2006, Ledwoch et al. 2017). Critical edge definition methods assess the minimum number of nodes and links whose removal would disconnect the network (e.g. Duque-Anton et al. 2000, Goyal and Caffery 2002, Jorgic et al. 2004). Dinh et al. (2010) argued that none of these measures are able to consolidate disruption scenarios at the global scale, and created a pseudo approximation algorithm to find the minimum set of nodes whose removal will result in a given amount of degradation (pairwise connectivity) to the network.|$|R
40|$|A new {{measurement}} procedure to obtain transmissibilities for application in a two-step transfer path analysis is presented. In the proposed method an external excitation, for instance {{by means of}} a non-instrumented hammer, is used to bring the structure into vibration. The resulting response is then used to estimate the transmissibilities. In most cases the transmissibilities thus determined are of a better quality as compared to the conventionally determined transmissibilities obtained during machine operation, i. e. in conventional <b>operational</b> transfer <b>path</b> analysis procedures. The reason of this is that when a structure is excited by hammer strokes, its responses are largely independent from each other, whilst vibrations induced by machine operation are in general not. Subsequently the transmissibilities are applied in a transfer path analysis (TPA) like approach. It was found that the identification of the transfer paths were better as compared to the identification transfer paths {{by means of a}} conventional <b>operational</b> transfer <b>path</b> analysis. The advantages of the method are illustrated by means of an experiment on a small gearbox. Additionally, a procedure to determine the number of significant transmission paths is proposed which is based upon a singular value decomposition of the response matrix. In the application discussed in the paper the number of significant structural transmission paths could be estimated by means of this procedure, as well as the order of magnitude of the strength of the acoustic path relative to that of the structural transmission paths. status: publishe...|$|R
40|$|This book chapter was {{published}} in the book NHV Analysis Techniques for Design and Optimization of Hybrid and Electric Vehicles [© Shaker Verlag] and appears here with the permission of the publisher. In the current framework, hybrid and electric vehicles have the advantage of polluting the environment less than other conventional technology-based vehicles, nevertheless one issue of concern has been the noise impact that they generate. The main objective of this proposal is to improve the acoustic perception of the driver in the cabin. This is the reason why Transfer Path Analysis and <b>Operational</b> <b>Path</b> Analysis methods were performed in an electric vehicle. A comparison between both methods taking into account the structure-borne paths from the motor and the suspension points to the cabin has been performed. One of the aims {{of this study is to}} check the versatility of these methods, commonly used in petrol engine cars. This assessment includes the analysis of the noise contribution of each path depending on the frequency and vehicle speed range and moreover, the path noise impact for harmonic excitation due to the physical components of the electric vehicle...|$|E
40|$|Today several {{experimental}} TPA (Transfer Path Analysis) methods {{exist for}} identifying the vibro- acoustic transfer paths in a system, from the active system component(s), generating the structural and acoustic loads, through the physical connections and along airborne pathways, to the target(s) at the passive system component(s) responding to these loads. Amongst these, {{one of the}} most recent methods is the OPAX (<b>Operational</b> <b>Path</b> Analysis with eXogeneous inputs) technique which uses parametric models (e. g. dynamic stiffness model for the mounts) for identifying the operational loads. The advantage of such a method is that only a limited amount of measurement data is needed to build up the TPA model since only a few model parameters are to be estimated to describe the loads over the whole frequency range. This is different in the traditional Matrix Inversion (MI) method where the parameter estimation has to be done separately for each frequency line, requiring a much larger amount of data. This makes the OPAX method more robust, fast and scalable, enabling the engineer to use a smaller amount of measurement data for quick troubleshooting. This paper reports on a validation test campaign which was carried out on a full vehicle to assess the OPAX method and compare it to the traditional Mount Stiffness (MS) and Matrix Inversion (MI) TPA methods. Specific focus is on the validation of the OPAX models by comparing the dynamic stiffness estimations to mount measurement data. status: publishe...|$|E
40|$|Information and Communication Technologies (ICT) {{have become}} {{important}} tools to promote an achieve {{a variety of}} public goals and policies. The growing importance of ICT in daily life, business activities and governance prompts the {{need to consider the}} role of ICT more explicitly in urban administrations and policies. What are the city-makers' expectations of ICT? And how do they assess the future implications of ICT for their city? An analysis of these questions is needed to provide us with {{a better understanding of the}} extent to which urban authorities are willing to invest in, and to adopt, ICT policy. This paper offers both a conceptual and an operational model that aims to map out the causes and implications of ICT perceptions and views of urban policy-makers and/or administrative officials (denoted as urban front-liners). This is followed by the presentation of an <b>operational</b> <b>path</b> model-i. e. a linear structural equations model (Lisrel). The model serves to describe and test the relationships between perceptions of the city, policy-makers' beliefs about ICT and the associated urban ICT policy. According to the model, respondents who perceive their city as having many urban functions (such as commercial centre, service centre, higher education centre) have more awareness of various ICT tools and are likely to consider a multiplicity of ICT measures as relevant to their city. Respondents who consider their city as having severe bottlenecks (such as traffic congestion, housing shortage) are less likely to think of ICT measures and ICT-related goals as relevant to their city, and nor do they think that the municipality impacts significantly on ICT in the city. Furthermore, respondents who perceive their city as suffering from many socioeconomic problems (unemployment, ageing population, industrial decline and so on) are likely to consider many ICT tools as relevant to their city, although they have a low awareness of the specific tools to be deployed. Finally, respondents who believe that ICT will significantly (and positively) affect the city and its administration also tend to believe that the municipality has a high municipal influence on ICT and consider many ICT initiatives as relevant to their city. © 2004 The Editors of Urban Studies...|$|E
40|$|International audienceA new {{measurement}} procedure to obtain transmissibilities for application in a two-step transfer path analysis is presented. In the proposed method an external excitation, for instance {{by means of}} a noninstrumented hammer, is used to bring the structure into vibration. The resulting response is then used to estimate the transmissibilities. In most cases the transmissibilities thus determined are of a better quality as compared to the conventionally determined transmissibilities obtained during machine operation, i. e. in conventional <b>operational</b> transfer <b>path</b> analysis procedures. The reason of this is that when a structure is excited by hammer strokes, its responses are largely independent from each other, whilst vibrations induced by machine operation are in general not. The hammer stroke measurements can be performed relatively easy whilst the experimental effort is relatively low. Subsequently the transmissibilities are applied in a transfer path analysis (TPA) like approach. It was found that the transfer paths identification was of a better quality as compared to the identification of the transfer paths {{by means of a}} conventional <b>operational</b> transfer <b>path</b> analysis. The advantages of the method are illustrated by means of an experiment on a small gearbox. Additionally, a procedure to determine the number of significant transmission paths is proposed which is based upon a singular value decomposition of the response matrix. In the application discussed in the paper the number of significant structural transmission paths could be estimated by means of this procedure, as well as the order of magnitude of the strength of the acoustic path relative to that of the structural transmission paths...|$|R
40|$|The {{formation}} of patterns that are {{proportional to the}} size of the embryo is an intriguing but poorly understood feature of development. Molecular mechanisms controlling such proportionality, or scaling, can be probed through quantitative interrogations of the properties of morphogen gradients that instruct patterning. Recent studies of the Drosophila morphogen gradient Bicoid (Bcd), which is required for anterior-posterior (AP) patterning in the early embryo, have uncovered two distinct ways of scaling. Whereas between-species scaling is achieved by adjusting the exponential shape characteristic of the Bcd gradient profile, namely, its length scale or length constant (λ), within-species scaling is achieved through adjusting the profile’s amplitude, namely, the Bcd concentration at the anterior (B 0). Here, we report a case in which Drosophila melanogaster embryos exhibit Bcd gradient properties uncharacteristic of their size. The embryos under investigation were from a pair of inbred lines that had been artificially selected for egg size extremes. We show that B 0 in the large embryos is uncharacteristically low but λ is abnormally extended. Although the large embryos have more total bcd mRNA than their smaller counterparts, as expected, its distribution is unusually broad. We show that the large and small embryos develop gene expression patterns exhibiting boundaries that are proportional to their respective lengths. Our results suggest that the large-egg inbred line has acquired compensating properties that counteract the extreme length of the embryos to maintain Bcd gradient properties necessary for robust patterning. Our study documents, for the first time to our knowledge, a case of within-species Bcd scaling achieved through adjusting the gradient profile’s exponential shape characteristic, illustrating at a molecular level how a developmental system can follow distinct <b>operational</b> <b>paths</b> towards the goal of robust and scaled patterning...|$|R
40|$|<b>Operational</b> {{transfer}} <b>path</b> analysis (OTPA) is {{a diagnosis}} method aiming {{to identify and}} rank noise transmission paths in dynamic systems. The particularity of the method is to require no preliminary acquisition of a transfer matrix between excitation and response dofs, {{as it is the}} case for classical TPA approaches. OTPA is based on the identification of a transmissibility matrix between some input and output responses measured for various operating conditions. The first difficulty of the method concerns the definition of this transmissibility matrix, from either a theoretical or experimental point of view. The second difficulty is the use of this matrix for diagnosis purposes, requiring some assumptions leading to potential misunderstandings. Theoretical aspects of the method are firstly discussed in this work. Secondly, an experimental validation is carried out on an academic test setup, and OTPA results are compared to the classical TPA approach. status: publishe...|$|R
40|$|In {{the modern}} urban lifestyle, {{more and more}} people are exposed to noise {{pollution}} in form of traffic noise. As a response to this, the automotive OEMs (Original Equipment Manufacturer) are put under pressure to reduce the emitted noise from vehicles. To be able to meet the upcoming, stricter regulations, the automotive OEMs seeks new techniques to be able to front load the pass-by noise engineering in the vehicle development process and to identify and understand the different sources that contributes to the exterior noise. Earlier exterior sources ranking using ASQ (Airborn Source Quantification) with an energetic approach during pass-by noise test has yielded very good and reliable results for an ICE (Internal Combustion Engine) vehicle. In this Master Thesis, two exterior source ranking methods have been tested and evaluated for an electric vehicle during in-room pass-by noise test. The two methods were: ASQ and OPA (<b>Operational</b> <b>Path</b> Analysis). In total, five models were built from the two methods and each model was evaluated for, in total, three driving conditions corresponding to the current ISO 362 - 1 : 2007 and the proposed, revised version. The results show that the ASQ models are not capable to correctly estimate the engine contribution due to its high tonality. Moreover, it was seen that the energetic ASQ model is very sensitive to small changes. Both ASQ models underestimated the tire noise. The OPA model on the other hand managed to estimate the total contribution very well. Both the engine contribution and the tire contributions are well estimated. Nevertheless, OPA as method has several weaknesses and building an OPA model is not a straightforward task. Its weaknesses and the process to reach a final OPA model are discussed in this thesis. It was seen that one of the most crucial steps in an OPA model is to have clean references to get meaningful results. A MIMO-FIR filter was therefore used to filter out engine harmonics from the tire references. Its principles and importance for the end results are also discussed. Included is also an overview of the basic principles in TPA (Transfer Path Analysis), ASQ, OPA and in room pass by noise test as well as a description of the test campaign...|$|E
40|$|Quickly {{adapting}} to changing market conditions {{is crucial for}} every firm, especially in volatile and capital intensive markets. These are two characteristics of the maritime industry. Incorporating flexibility in design have gathered an increased focus as means to cope with such a dynamic context. To be flexible is not always enough, partially because exploiting this flexibility causes a time delay. This time delay causes more uncertainty, which could make a good investment decision into a bad one. Hence, this master thesis investigates the contribution from investment lags in engineering systems by the following: (i) Structuring and presenting a real option framework for identification of possible design solutions that reduces investment lag (ii) Propose value models for system design changes that are exposed for investment lag (iii) Propose design solutions that limits the investment lag. For this, an elongation of a dry bulk vessel is a thorough example. Agility represents {{the ability of a}} system to change quickly, and investment lag is the time from the decision is made until the system has changed capabilities. All investments have lags to some degree, which is often disregarded in traditional real options analyses. For systems with relatively long time lags, {{there is a need to}} better understand the impact of these investment lags. To identify design solutions that enables agility, a further separation in the real options framework has been proposed as a contribution to the existing literature. On options is seen as the superior option, which is the operational option on a projects future cash flow. This is often related to changes in value enabling variables, and as this is the conventional real option, there exist an established valuation framework for it. Further, on options are separated in two classes, Built-in-Design options and Design Change options. The former is multifunctionality that are implemented in the system from the initial design stage, the latter are options that make changes in the design, often referred to as in options, which comes with an investment lag due to off-hire when performing the design change. By this separation of on options, path enablers could be tied to real options. Path enablers are features that enables easier change in value enabling variables, making it easier to exercise options. Design path enablers are design features that enables easier exercise of design change option, thereby reducing the investment lag and cost of exercise. Thus, Design Change options could be seen as <b>operational</b> <b>path</b> enablers, that enables change of operation, which by definition is an on options. The relation between the option value of a capacity expansion/market entry, and the investment lag is defined to value agility. This gives a possible value for investment lag reducing path enablers. A design neutral capacity expansion in the dry bulk freight market is used as an illustrative case, and by introducing the investment lag as a parameter in the real option model, the value of a capacity expansion option could be considered as a function of investment lag. For this, the cost of investment lag is defined as the opportunity cost of operating in the market. The analysis is conducted in several steps, by three models with increasing degree of complexity. Qualitative and quantitative time series analysis on dry bulk freight rates in the Capesize segment in the time span between 1990 and 2017 resulted in freight market replications for two of the models using the geometric Mean Reversion model, and the last with market rates and asset prices as correlated mean reversion paths. Then, by Monte Carlo simulation, a now-or-never investment analysis determines the value of agility by simulation of the projects cash flows, thereafter through including investment timing flexibility based on the Least Squares Monte Carlo algorithm, which gives the value of agility through an established option valuation framework. For this model two approaches were taken, one with correlated vessel values and freight rates, and one with vessel values as a value of underlying freight rates. The option value reflects a premium of second hand vessel prices with different investment lags, and with no investment lag the maximum value will be respectively 147, 60 and 12. 2 USD/dwt for the now-or-never approach, LSMC and LSMC and with correlated asset paths, which is reduced to 85 %, 65 % and 55 % of maximum value for an investment lag of 6 months. The results of the different models are quite consistent in their form, and the spread between the maximum value can be explained through different approaches and different underlying stochastic processes. The LSMC model with vessel values as function of freight rates is seen as the most realistic, as it includes investment timing flexibility and geometric Mean Reversion as underlying stochastic process for freight market dynamics. Further, design path enablers for the illustrative case is proposed, an elongation of a dry bulk carrier which gives a capacity expansion. Design path enablers for this elongation is elements that make the vessel ready for elongation, examples of this can be thickness of plates and visibility. As the design still water bending moment and wave bending moment is a function of L^ 3, a greater thickness is required. Further, the visibility of water surface shall not be obscured by more than 1 xLOA forward of the bow. Hence, path enablers for an elongation from 150 m to 200 m could be to build the initially 150 m long vessel with bottom plate thickness of 12 mm instead of the required 11 mm and a higher bridge. This will make the initial structure ready for 200 m LOA. By implementing design path enablers one increases the design freedom over the systems lifetime. In conventional design thinking, a design is often optimised for a base case. Though, by implementing design path enablers, one admit that the future is uncertain and partly unpredictable, and enables easier future design changes. This will increase the design freedom over the systems lifetime, which again is reflected by flexibility in operations at a strategic level. By implementing such design path enablers, together with Built-in design options, agility is enabled through design, thus, Agility by Design...|$|E
40|$|The <b>operational</b> {{transfer}} <b>path</b> analysis (OTPA) {{method is}} {{a variation of}} the classical transfer path analysis (TPA) method, both of which are used particularly for noise, vibration and harshness (NVH) testing in the vehicle industry. The methods differ such that classical TPA requires a physical isolation of the critical source-receiver paths to eliminate cross-talk prior to determining the frequency response functions (FRFs), whereas OTPA uses singular value decomposition (SVD) and principal component analysis (PCA) for cross-talk cancellation (CTC), prior to computing the transmissi-bility of each critical path. In some cases, it has been found that the OTPA results yield an over-prediction of the airborne sound contribution in the low frequency range (20 to 100 Hz). This over-prediction seems to occur when several microphones are placed within close proximity of each other (i. e. within a fraction of the wavelength). This thesis begins with a study of the underlying theory for the classical TPA and th...|$|R
40|$|International Entrepreneurship (IE) {{theory has}} {{developed}} extensively {{over the last}} two decades by drawing on various theoretical perspectives. While this growing body of knowledge has provided rich insights into the internationalisation behaviour of firms from multiple theoretical perspectives, it has also rendered IE theory fragmented and devoid of a unifying theoretical direction. Using a qualitative approach, this study intends to address the gap identified above by developing a framework for the entrepreneurial internationalisation process. As such, the study focuses on the entrepreneurial aspects of “opportunity identification and exploitation”: an area to which IE researchers have paid little attention. It is argued that this focus is appropriate as it can extend the scope of international business and IE research by strengthening the foundations of the entrepreneurial theory of internationalisation. The study findings extend key insights into the internationalisation process of entrepreneurial firms. The research context provided unique perspectives of how firms in the agriculture-base primary industry in a developing country internationalise. The case findings identified prior knowledge, creativity, selfefficacy, perseverance, and passion as drivers of the opportunity development process. Also, the study supported the idea that both access to resources and entrepreneurs’ social capital have significant influence on how opportunities are developed. The results elucidated a new concept – “entrepreneurial insight” − to explain how thinking, knowledge, and dynamic capabilities integrate to act as the core processes of opportunity development. These three factors can be identified as idiosyncratic entrepreneurial resources in the process of opportunity development and exploitation. The exploitation of opportunities thus leads to new strategic and <b>operational</b> <b>paths</b> and positions, which then affect the firm’s performance in terms of degree of internationalisation, growth, survival, and profitability. The findings provide a better understanding of internationalisation using three defining elements in the internationalisation process: entrepreneurial intention, opportunity development, and value innovation. These factors provide an insightful explanation of different international trajectories that firms take, and how these trajectories sustain their international activities over time. Finally, the study provides managerial and theoretical implications that can guide practitioners towards an appreciation of the dynamics of individual capacities, the value of networks, and the resources that need to be harnessed by learning, adapting, and taking timely decisions to generate value-creating opportunities in international markets...|$|R
40|$|This paper aims {{to study}} how {{different}} psycho-demographic traits may influence the activities and career progression of project managers (PM). We carried out an empirical qualitative research with PMs in Brazil, {{taking into account the}} peculiarities of this culture. There were conducted interviews with project managers that  were transcribed and subjected to discourse analysis. We found out that the soft skills proved to be crucial abilities needed by this manager for his/her career progression, and that these professionals perceive their professional success by either the growing complexity of the projects leaded by him/her, or also by taking a more strategic and less <b>operational</b> career <b>path.</b> Besides it was possible to infer that the soft skills are influenced by the local culture. This research has also suggested the existence of glass doors and ceilings for ethnic, gender, and sexual minorities. These professionals seem to carry out their functions with a cost greater than the one required by their male, white and heterosexuals peers, as well as they need to prove resilient to such domination. </h 1...|$|R
40|$|The {{maintenance}} of system flow {{is critical for}} effective network operation. Any type of disruption to network facilities (arcs/nodes) potentially risks loss of service, leaving users without access to important resources. It is therefore an important goal of planners to assess infrastructures for vulnerabilities, identifying those vital nodes/arcs whose debilitation would compromise the most source-sink (s-t) interaction or system flow. Due to the budgetary limitations of disaster management agencies, protection/fortification and planning for the recovery of these vital infrastructure facilities is a logical and efficient proactive approach to reducing worst-case risk of service disruption. Given damage to a network, evaluating the potential for flow between s-t pairs requires assessing the availability of an <b>operational</b> s-t <b>path.</b> Recent models proposed for identifying infrastructure vital to system flow have relied on enumeration of all s-t paths to support this task. This paper proposes an alternative model constraint structure that does not require complete enumeration of s-t paths, providing computational benefits over existing models. To illustrate the model, an application to a practical infrastructure planning problem is presented...|$|R
40|$|The {{theory that}} the {{prospect}} of liability for damages deters risky behavior has been developed in countless articles and books. The literature is far sparser, however, on how deterrence is operationalized. And prior work slights an equally important effect of damage actions, to incentivize claims management in addition to harm-reduction responses that are cost- rather than liabilityminimizing. This article works in the intersection of these two understudied areas, focusing on claims management steps taken by frequently sued organizations, and opening {{a window into the}} black box of deterrence to see how those steps may end up serving harm-reduction purposes as well. To summarize, I observe that damage actions regulate risky enterprise by inducing organizations to develop claims management capabilities—that is, the capacity to process any resulting disputes. I then argue that these claims management practices and personnel are sometimes used, secondarily but importantly, to improve safety, reduce risk, and increase compliance with external legal requirements. Organizations’ internal claims management operations can, though they need not, facilitate care-taking in four important ways: (a) promoting the gathering and analysis of claims information; (b) requiring the hiring of specialized personnel with a mission to reduce claim payouts; (c) encouraging bureaucratized procedures that may be harm-reducing, and (d) increasing the salience of claims to various actors within the organization. I discuss the theory underlying these four points, drawing on organizational economics and sociology, as well as on psychology and behavioral law and economics. Then I discuss these four channels of influence in particular factual settings which serve as case studies, looking at a single large retailer, and then more generally at hospitals and hospital doctors, and jails and prisons. Because organizational theory tells us that this kind of transformation or repurposing is quite ordinary, the preliminary evidence I canvass suggests that claims management should be included in any study of how damage action deterrence is operationalized within large risk-creating organizations. This article thus makes two chief scholarly contributions. It proposes and theorizes concrete <b>operational</b> <b>paths</b> by which damage actions may elicit organizational compliance with external norms. And it describes in-house claims management, a heretofore underobserved arena in which law influences organizational activity. In the conclusion, I propose that who performs claims management functions may matter, as well, and suggest that in future research, claims management should be considered along with liability and loss prevention as the trio of liability-related operational areas in which firms must implement a “make-or-buy” decision...|$|R
40|$|<b>Operational</b> Transfer <b>Path</b> Analysis (OTPA) is very {{attractive}} {{from a practical}} point of view, as {{it does not require}} an expensive measurement campaign in which transfer functions are measured, {{as is the case in}} the traditional Transfer Path Analysis (TPA). In practice, however, a number of requirements have to be met in order to achieve reliable results. One aspect is that one needs to considered sufficient number of transmission paths in order to get meaningful results. It is known, for instance, that an omission of a transmission path in a TPA and especially an OTPA analysis leads to incorrect source strength estimates. To assist in this matter, a method is presented to estimate the number of physical transfer paths based on a singular value decomposition of the matrices involved. In addition a rather unexpected phenomena is discussed in the paper, causing big errors in the estimation of the transmissibilities that are required for OTPA. From the experiments it was found that resonant behavior of metal springs drastically affects the effective transmissibilities of the structure, causing erroneous results. It is believed that the main cause of this is the introduction of torques in addition to the forces in normal direction. The above described aspects are illustrated by means of experimental results. Results obtained with classical TPA will be compared with results obtained with OTPA. status: publishe...|$|R
40|$|Network defense {{today is}} largely {{reactive}} rather than proactive, and lacks sufficient context for optimal countermeasures. Administrators and security analysts are overwhelmed by constant outside threats, complexity of security measures, and network growth. Today’s status quo for network defense is often reduced to mere triage and after-the-fact remediation. This chapter examines proactive methods of attack risk reduction and response through attack graphs. Our attack graphs map potential paths of vulnerability through a network, showing exactly how attackers may penetrate a network. Attack graph analysis identifies critical vulnerabilities and provides strategies for protection of critical network assets. But because of <b>operational</b> realities, vulnerability <b>paths</b> often remain. In such cases attack graphs provide an ideal methodology for planning appropriate attack responses. This includes optimal placement of intrusion detection sensors, correlating intrusion alarms, accounting for missed detections, prioritizing alarms, and predicting next possible attack steps...|$|R
40|$|This {{contribution}} {{presents a}} new physically based methodology for {{the prediction of}} rain attenuation experienced by a microwave terrestrial link. The meteorological environment for the site of interest is reproduced {{through a set of}} synthetic rain fields generated by the MultiEXCELL model starting from the local rainfall statistics. The fade levels are calculated numerically by simulating the interaction of a given link geometry with the precipitation field. Eventually, the performance of the proposed methodology is evaluated against the reference data included in the ITU-R DBSG 3 database of line-of-sight radio link measurements. When compared to the method currently adopted by the ITU-R in recommendation P. 530 – 13, the proposed methodology shows much better results, both in terms of overall root mean square of the estimation error and in terms of performance stability with the link characteristics, namely <b>operational</b> frequency and <b>path</b> length...|$|R
40|$|A Flight Path Generator {{is defined}} as the module of an {{automated}} Air Traffic Control system which plans aircraft trajectories in the terminal area with respect to operational constraints. The flight path plans have to be feasible and must not violate separation criteria. The problem of terminal area trajectory planning is structured by putting the emphasis on knowledge representation and air-space organization. A well-defined and expressive semantics relying on the use of flexible patterns is designed to represent aircraft motion and flight paths. These patterns are defined so as to minimize the need for replanning and to smoothly accommodate <b>operational</b> deviations. Flight <b>paths</b> are specified by an accumulation of constraints. A parallel, asyn-chronous implementation of a computational model based on the propagation of constraints provides mechanisms to efficiently build feasible flight path plans. A methodology for a fast and robust conflict detection between flight pat...|$|R
40|$|Current network {{security}} tools generally lack sufficient context for maintaining a well informed and proactive defense posture. Vulnerabilities are usually assessed in isolation, without considering how {{they contribute to}} overall attack risk. Similarly, intrusion alarms are logged as isolated events, with limited correlation capabilities. Security professionals are overwhelmed by constant threats, complexity of security data, and network growth. Our approach to network defense applies attack graphs for advanced vulnerability analysis and intrusion detection. Attack graphs map paths of vulnerability, showing how attackers can incrementally penetrate a network. We can then identify critical vulnerabilities and provide strategies for protection of critical network assets. Because of <b>operational</b> constraints, vulnerability <b>paths</b> may often remain. The residual attack graph then guides optimal intrusion detection and attack response. This includes optimal placement of intrusion detection sensors, correlating intrusion alarms, accounting for missed detections, prioritizing alarms, and predicting next possible attack steps. 1...|$|R
