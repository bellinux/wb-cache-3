2|7|Public
40|$|The idea of {{directional}} types is {{to describe}} the computational behaviour of Prolog programs by associating an input and an <b>output</b> <b>assertion</b> to every predicate. The input assertion puts a restriction on {{the form of the}} arguments of the predicate in the initial atomic goals. The <b>output</b> <b>assertion</b> describes the form of the arguments at success, given that the predicate is called as specified by its input assertion. This paper discusses two aspects of directional types: the declarative notion of input-output correctness and the operational notion of call correctness. By separating these two concepts, we readily obtain better correctness criteria than those existing in the literature. We further show how directional types can be used for controlling execution of logic programs through a delay mechanism. 1 Introduction Recently there has been a growing interest in the notion of directional types for Prolog programs [6, 12, 2, 3, 1, 11]. This kind of prescriptive typing describes the intende [...] ...|$|E
40|$|A {{description}} of methods and an implementation of a system for automalic generation of programs is given. The problems of writing programs for numerical computation, symbol manipulation, robot control and everyday planning have been studied and some programs generated. A particular formalism, i. e. a FRAME, has been developed to define the programming environment and permit the statement of a problem. A frame, F _ is formulated within the Logic of Prot_rams [Hoare 1969, Hoare and Wirth 1972] and includes primitive functions and procedures, axioms, definitions and rules of program composition. Given a frame, F, a problem for program constuction may be stated as a pair , where] is an input assertion and G is an <b>output</b> <b>assertion.</b> The program generation task is to construct a program A such that I{A}r, where I ': _ G. This process {{may be viewed as}} a search in the Logic of Programs for a proof that the generated program satisfies the given input-output assertions. Correctness of programs generated using the formal algorithm is discussed. A frame is translated into a backtrack problem solver augmented by special search procedures. The system is interactive, responds to simple advice and allows incremental and structured program development. The output or solution program is a subset of ALGOL containing procedure calls, assignments, while loops and conditional statements. This research was largely done at Stanford University as the author's Ph. D. Thesis...|$|E
30|$|To handle those weak-points the Robust Assertions {{technique}} is proposed, whose effectiveness {{is shown by}} extensive fault injection experiments. With this technique a system follows a new failure model, that is called Fail-Bounded, where with high probability all results produced are either correct or, if wrong, they are within a certain bound of the correct value, whose exact distance depends on the <b>output</b> <b>assertions</b> used.|$|R
40|$|Abstract — Assertions are a {{mechanism}} {{that can be}} used to enhance the effectiveness of software testing. Where software testing is concerned with verification of a program’s <b>output,</b> <b>assertions</b> provide a method for verifying the internal values of a program. When used properly, assertions can provide assurance that a program is correct. However, as far as we are aware, there has been little research into methods for assertion creation. Too often, assertions written by programmers are often added to programs in a haphazard manner. Such assertions provide poor verification of system behavior because they only address lowlevel details of the implementation that have little relationship to each other or the overall high-level specification We introduce a technique for assertion creation based on deriving assertions from a program’s formal specification. In order to evaluate our approach, we applied it to Nova-Solver, an existing fault tree analysis tool. We present preliminary results of comparing specification-derived assertions to traditional programmer-created assertions and discuss differences in the errors found by the two assertion creation approaches. From this experiment we report on the experience of manually translating a formal specification into assertions and the insights we gained into the strengths, weaknesses, and cost-effectiveness of specificationderived assertions. I...|$|R
40|$|In {{this paper}} the {{behavior}} of assertion-based error detection mechanisms is characterized under faults injected according to a quite general fault model. Assertions based on {{the knowledge of the}} application can be very effective at detecting corruption of critical data caused by hardware faults. The main drawbacks of that approach are identified as being the lack of protection of data outside the section covered by assertions, namely during input and output, and the possible incorrect execution of the assertions. To handle those weak-points the Robust Assertions technique is proposed, whose effectiveness is shown by extensive fault injection experiments. With this technique a system follows a new failure model, that is called Fail- Bounded, where with high probability all results produced are either correct or, if wrong, they are within a certain bound of the correct value, whose exact distance depends on the <b>output</b> <b>assertions</b> used. Any kind of assertions can be considered, from simple likelihood tests to high coverage assertions such as those used in the Algorithm Based Fault Tolerance paradigm. We claim that this failure model is very useful to describe {{the behavior of}} many low-cost fault-tolerant systems, that have low hardware and software redundancy, like embedded systems, were cost is a severe restriction, yet full availability is expected...|$|R
50|$|Some have {{alleged that}} departures in {{normality}} {{in the process}} output significantly reduce {{the effectiveness of the}} charts {{to the point where it}} may require control limits to be set based on percentiles of the empirically-determined distribution of the process <b>output</b> although this <b>assertion</b> has been consistently refuted. See Footnote 6.|$|R
40|$|White-box test {{generator}} tools rely only on {{the code}} under test to select test inputs, and capture the implementation's <b>output</b> as <b>assertions.</b> If there is a fault in the implementation, it could get encoded in the generated tests. Tool evaluations usually measure fault-detection capability using {{the number of such}} fault-encoding tests. However, these faults are only detected, if the developer can recognize that the encoded behavior is faulty. We designed an exploratory study to investigate how developers perform in classifying generated white-box test as faulty or correct. We carried out the study in a laboratory setting with 54 graduate students. The tests were generated for two open-source projects {{with the help of the}} IntelliTest tool. The performance of the participants were analyzed using binary classification metrics and by coding their observed activities. The results showed that participants incorrectly classified a large number of both fault-encoding and correct tests (with median misclassification rate 33 % and 25 % respectively). Thus the real fault-detection capability of test generators could be much lower than typically reported, and we suggest to take this human factor into account when evaluating generated white-box tests. Comment: 13 pages, 7 figure...|$|R
40|$|Includes bibliographical {{references}} (pages [73]- 77) This thesis {{describes the}} details of a parallel data flow trigger processor which is used in Fermilab’s E 789. E 789 is an experiment to study charmless two- prong decays and other low-multiplicity decays of particles containing b or c quarks. The trigger processor’s sole purpose is to determine, by postulating particle tracks, whether interesting particles exist in the current analysis period. The author has designed, implemented and tested the trigger processor. The single most significant contribution was the design of the vertex processor. The vertex processor utilizes silicon detector hits to calculate the particle origin and to verify particle track consistency with the downstream particle track. The capability to calculate particle origin did not exist in previous trigger processors. The processor consists of an upstream vertex processor and a downstream track processor. The algorithms which reconstruct the postulated particle paths and calculate particle origin are implemented via interconnected function specific hardware modules. The algorithm is directly dependent upon the organization of the modules, the specific arrangement of the inter-module cabling, on-board wire patch patterns, and where appropriate, on-board memory data. The processor provides an indication of the presence of at least one interesting particle pair in the current event by asserting Read on its Read/Skip <b>output.</b> The Read <b>assertion</b> is then used as a trigger to capture all of the event’s data for subsequent extensive off-line analysis. M. S. (Master of Science...|$|R
40|$|Large {{accounting}} firms have been structuring their audit divisions along industry lines for some years. Industry specialisation {{is seen as}} a means of differentiation between otherwise similar {{accounting firms}}. At the individual level industry specialists are identified as being so designated within their firm. They spend a substantial amount of their time auditing clients in that industry. The {{purpose of this study is}} to determine what industry specialist auditors do that is different and similar when working on industry-based tasks, one of which they specialise in. Behavioural decision theory is used to investigate the differences and similarities in the decision-making processes of industry specialist and other auditors. It is known that industry specialists perform better on tasks set in their industry. The purpose of this study is to learn why. To that end, the pre-information search, information search and decision processing phases of the decision-making process are examined. It is expected that industry specialists are more efficient and effective at each stage of the decision-making process when completing a case set in the industry they specialise in. Two controlled experiments were conducted in the offices of each of the Big 4 international accounting firms. Participants included manufacturing and superannuation industry specialists from each firm. Each participant was invited to take part in both experiments, which were conducted consecutively via the internet. The first experiment comprised two cases, one set in each industry setting (manufacturing and superannuation). Participants completed both cases. The purpose of the first experiment was to conduct a within-subject examination unveiling similarities and differences between industry specialists and other auditors during the pre-information search, information search and decision processing phases of the decision-making process. Their performance on each case was also monitored and measured. Significant results were found for information search and performance. Moderate results were found for one proxy each of the pre-information search and the decision processing phases. The relationship between efficiency at each stage of the decision-making process and performance was also measured. A significant relationship was found for the pre-information search and decision processing phases. The second experiment comprised two strategic business risk tasks set in each industry setting (manufacturing and superannuation). Participants completed both sets of tasks. The purpose of the second experiment was to examine effectiveness during the pre-information search (listing key strategic business risks), information search (listing key inputs) and decision processing (listing key processes) phases of the decision-making process and their ability to identify and list key <b>outputs</b> (accounts and <b>assertions)</b> for an identified risk (technological change for the manufacturing industry task and solvency due to insufficient funding for the superannuation industry task) within each industry setting. The results were very significant overall. Industry specialist auditors were able list more key strategic business risks, inputs, processes and outputs when the task was set in the industry in which they specialise. The relationship between effectiveness at each stage of the decision-making process and performance was also measured. A significant relationship was found between effectiveness in listing key inputs and effectiveness in listing key outputs (accounts) ...|$|R

