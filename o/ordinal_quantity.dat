3|11|Public
40|$|Genome-wide {{association}} studies {{provide an}} unprecedented opportunity to identify combinations of genetic variants {{that contribute to}} disease susceptibility. The combinatorial problem of jointly analyzing the millions of genetic variations accessible by high-throughput genotyping technologies is a difficult challenge. One approach to reducing the search space of this variable selection problem is to assess specific combinations of genetic variations based on prior statistical and biological knowledge. In this work, we provide a systematic approach to integrate multiple public databases of gene groupings and sets of disease-related genes to produce multi-SNP models that have an established biological foundation. This approach yields a collection of models which can be tested statistically in genome-wide data, along with an <b>ordinal</b> <b>quantity</b> describing the number of data sources that support any given model. Using this knowledge-driven approach reduces the computational and statistical burden of large-scale interaction analysis while simultaneously providing a biological foundation for the relevance of any significant statistical result that is found...|$|E
40|$|In {{this work}} {{we aim at}} two objects: quantifying, by a binomial-beta {{probabilistic}} model, the uncertainty involved {{in the assessment of}} the intensity decay, an <b>ordinal</b> <b>quantity</b> often incorrectly treated as real variable, and, given the finite dimension of the fault, modelling non-symmetric decays but exploiting information collected from previous studies on symmetric cases. To this end we transform the plane so that the ellipse having the fault length as maximum axis is changed into a circle with fixed diameter. We start from an explorative analysis of a set of macroseismic fields representative of the Italian seismicity among which we identify three different decay trends by applying a hierarchical clustering method. Then we focus on the exam of the seismogenic area of Etna volcano where some fault structures are well recognizable as well as the anisotropic trend of the attenuation. As in volcanic zones the seismic attenuation is much quicker than in other zones, we first shrink and then transform the plane so that the decay becomes again symmetric. Following the Bayesian paradigm we update the model parameters and associate the estimated values of the intensity at site with the corresponding locations in the original plane. Backward validation and comparison with the deterministic law are also presented...|$|E
40|$|We examine {{heritability}} {{estimation of}} an ordinal trait for osteoarthritis, using {{a population of}} pig-tailed macaques from the Washington National Primate Research Center (WaNPRC). This estimation is non-trivial, as the data consist of ordinal measurements on 16 intervertebral spaces throughout each macaque’s spinal cord, with many missing values. We examine the resulting heritability estimates from different model choices, and also perform a simulation study to compare the performance of heritability estimation with these different models under specific known parameter values. Under both the real data analysis and the simulation study, we find that heritability estimates from an assumption of normality of the trait differ greatly from those of ordered probit regression, which considers the ordinality of the trait. This finding indicates that some caution should be observed regarding model selection when estimating heritability of an <b>ordinal</b> <b>quantity.</b> Furthermore, we find evidence that our real data have little information for valid heritability estimation under ordered probit regression. We thus conclude with an exploration of sample size requirements for heritability estimation under this model. For an ordinal trait, an incorrect assumption of normality can lead to severely biased heritability estimation. Sample size requirements for heritability estimation of an ordinal trait under the threshold model depends on the pedigree structure, trait distribution {{and the degree of}} relatedness between each phenotyped individual. Our sample of 173 monkeys did not have enough information from which to estimate heritability, but estimable heritability can be obtained with as few as 180 related individuals under certain scenarios examined here...|$|E
5000|$|... ordinal {{quantity-value scale}}: quantity-value scale for <b>ordinal</b> <b>quantities</b> ...|$|R
5000|$|... {{quantity}} calculus: set {{of mathematical}} rules and operations applied to <b>quantities</b> other than <b>ordinal</b> <b>quantities</b> ...|$|R
5000|$|An SI unit or WHO where {{relevant}} - (for measurable properties, i.e. differential or rational <b>ordinal</b> <b>quantities).</b>|$|R
40|$|International audienceWhen solving {{arithmetic}} problems, {{semantic factors}} influence the representations built (Gamo, Sander & Richard, 2010). In order to specify such interpretative processes, we created structurally isomorphic word problems {{that could be}} solved with two distinct algorithms. We tested whether a distinction between cardinal and <b>ordinal</b> <b>quantities</b> would lead solvers, due to their daily-life knowledge, to build different representations, influencing their strategies {{as well as the}} nature of their drawings. We compared 5 th grade children and adults in order to assess the validity of this hypothesis with participants of varying arithmetic proficiency. The results confirmed that the distinction between cardinal and ordinal situations led to different solving strategies and to different drawings among both age groups. This study supports the ontological distinction of cardinal versus <b>ordinal</b> <b>quantities</b> and calls for the consideration of the role of daily-life semantics when accounting for arithmetic problem solving processes...|$|R
5000|$|... <b>ordinal</b> quantity: <b>quantity,</b> {{defined by}} a {{conventional}} measurement procedure, for which a total ordering relation can be established, according to magnitude, with other quantities of the same kind, but for which no algebraic operations among those quantities exist ...|$|R
40|$|Associated {{movements}} (AMs) are {{a classical}} diagnostic tool to assess differences between normal children {{and children with}} some motor dysfunction. This paper presents a methodology to produce age- and gender-dependent reference-curves for AMs of normal children, for various tasks of a test battery. Data available consist of separate ratings of duration and extent of AMs, which are <b>ordinal</b> <b>quantities</b> with few levels. Other problems are severe age- and gender-dependent floor-effects (as well as some ceiling-effects), leaving little information for analysis at older ages. To get a better scale, we combined the two ordinal ratings into one meaningful and quasi-continuous quantity referred to as intensity of AMs. In order to solve problems due to floor-effects, ceiling-effects and discreteness, we assumed left-, right- and interval-censored values, respectively. We considered a censored regression problem and postulated a truncated normal distribution for the non-censored values (after an appropriate transformation of the data). Using Wei and Tanner's poor man's data augmentation algorithm, together with the technique of linear mixed effects modelling, useful reference-curves could be produced. In contrast to the cumulative probabilities approach for ordinal data, our methodology allows the calculation of individual age- and gender-standardized values, which puts us {{in a position to}} investigate numerous scientific questions...|$|R
40|$|The SimCalc Vision and Contributions Advances in Mathematics Education 2013, pp 419 - 436 Modeling as a Means for Making Powerful Ideas Accessible to Children at an Early Age Richard Lesh, Lyn English, Serife Sevis, Chanda Riggs … {{show all}} 4 hide » Look Inside » Get Access Abstract In modern {{societies}} in the 21 st century, significant changes have been occurring {{in the kinds of}} “mathematical thinking” that are needed outside of school. Even in the case of primary school children (grades K- 2), children not only encounter situations where numbers refer to sets of discrete objects that can be counted. Numbers also are used to describe situations that involve continuous quantities (inches, feet, pounds, etc.), signed quantities, quantities that have both magnitude and direction, locations (coordinates, or <b>ordinal</b> <b>quantities),</b> transformations (actions), accumulating quantities, continually changing quantities, and other kinds of mathematical objects. Furthermore, if we ask, what kind of situations can children use numbers to describe? rather than restricting attention to situations where children should be able to calculate correctly, then this study shows that average ability children in grades K- 2 are (and need to be) able to productively mathematize situations that involve far more than simple counts. Similarly, whereas nearly the entire K- 16 mathematics curriculum is restricted to situations that can be mathematized using a single input-output rule going in one direction, even the lives of primary school children are filled with situations that involve several interacting actions—and which involve feedback loops, second-order effects, and issues such as maximization, minimization, or stabilizations (which, many years ago, needed to be postponed until students had been introduced to calculus). …This brief paper demonstrates that, if children’s stories are used to introduce simulations of “real life” problem solving situations, then average ability primary school children are quite capable of dealing productively with 60 -minute problems that involve (a) many kinds of quantities in addition to “counts,” (b) integrated collections of concepts associated with a variety of textbook topic areas, (c) interactions among several different actors, and (d) issues such as maximization, minimization, and stabilization. ...|$|R
40|$|In {{contrast}} to quantity processing, up to date, {{the nature of}} ordinality has received little attention from researchers {{despite the fact that}} both quantity and ordinality are embodied in numerical information. Here we ask if there are two separate core systems that lie at the foundations of numerical cognition: (1) the traditionally and well accepted numerical magnitude system but also (2) core system for representing ordinal information. We report two novel experiments of ordinal processing that explored the relation between ordinal and numerical information processing in typically developing adults and adults with developmental dyscalculia (DD). Participants made "ordered" or "non-ordered" judgments about 3 groups of dots (non-symbolic numerical stimuli; in Experiment 1) and 3 numbers (symbolic task: Experiment 2). In {{contrast to}} previous findings and arguments about quantity deficit in DD participants, when quantity and ordinality are dissociated (as in the current tasks), DD participants exhibited a normal ratio effect in the non-symbolic ordinal task. They did not show, however, the ordinality effect. Ordinality effect in DD appeared only when area and density were randomized, but only in the descending direction. In the symbolic task, the ordinality effect was modulated by ratio and direction in both groups. These findings suggest that there might be two separate cognitive representations of <b>ordinal</b> and <b>quantity</b> information and that linguistic knowledge may facilitate estimation of ordinal information...|$|R
40|$|This work {{presents}} {{a class of}} functions serving as generalized neuron models {{to be used in}} artificial neural networks. They are cast in the common framework of computing a it similarity function, a flexible definition of a neuron as a pattern recognizer. The similarity endows the model with a clear conceptual view and serves as a unification cover for many of the existing neural models, including those classically used for the MultiLayer Perceptron (MLP) and most of those used in Radial Basis Function Neural Networks (RBF). The possibilities of deriving new instances are then explored and several neuron models [...] representative of their families [...] are proposed. These families of models are conceptually unified and their relation is clarified. In addition, the similarity view leads naturally to further extensions of the models to handle heterogeneous information, that is to say, information coming from sources radically different in character, including continuous and discrete (<b>ordinal)</b> numerical <b>quantities,</b> nominal (categorical) quantities, and fuzzy quantities. Missing data are also treated explicitly as such. A neuron of this class is called an em heterogeneous neuron and any neural structure making use of them is an Heterogeneous Neural Network (HNN), regardless of the specific architecture or learning algorithm. In this work the experiments are restricted to feed-forward networks, as the initial focus of study. The learning procedures may include a great variety of techniques, basically divided in derivative-based methods (such as the conjugate gradient) and evolutionary ones (such as genetic algorithms). Postprint (published version...|$|R
40|$|This {{research}} {{introduces a}} general class of functions serving as generalized neuron models {{to be used}} in artificial neural networks. They are cast in the common framework of computing a similarity function, a flexible definition of a neuron as a pattern recognizer. The similarity endows the model with a clear conceptual view and leads naturally to handle heterogeneous information, in the form of mixtures of continuous numbers (crisp or fuzzy), linguistic information and discrete <b>quantities</b> (<b>ordinal,</b> nominal and finite sets). Missing data are also explicitly considered. The absence of coding schemes and the precise computation attributed to the neurons makes the networks highly interpretable. The resulting heterogeneous neural networks are trained by means of a special-purpose genetic algorithm. The cooperative integration of different soft computing techniques (neural networks, evolutionary algorithms and fuzzy sets) makes these networks capable of learning from non-trivial data sets with a remarkable effectiveness, comparable or superior to that of classical models. This claim is demonstrated by a set of experiments on benchmarking realworld data sets...|$|R
40|$|A new {{architecture}} for fusing {{information and}} data from heterogeneous sources is proposed. The approach takes criminalistics as a model. In analogy {{to the work}} of detectives, who attempt to investigate crimes, software agents are initiated that pursue clues and try to consolidate or to dismiss hypotheses. Like their human pendants, they can, if questions beyond their competences arise, consult expert agents. Within the context of a certain task, region, and time interval, specialized operations are applied to each relevant information source, e. g. IMINT, SIGINT, ACINT, [...] ., HUMINT, data bases etc. in order to establish hit lists of first clues. Each clue is described by its pertaining facts, uncertainties, and dependencies in form of a local degree-of-belief (DoB) distribution in a Bayesian sense. For each clue an agent is initiated which cooperates with other agents and experts. Expert agents support to make use of different information sources. Consultations of experts, capable to access certain information sources, result in changes of the DoB of the pertaining clue. According to the significance of concentration of their DoB distribution clues are abandoned or pursued further to formulate task specific hypotheses. Communications between the agents serve to find out whether different clues belong to the same cause and thus can be put together. At the end of the investigation process, the different hypotheses are evaluated by a jury and a final report is created that constitutes the fusion result. The approach proposed avoids calculating global DoB distributions by adopting a local Bayesian approximation and thus reduces the complexity of the exact problem essentially. Different information sources are transformed into DoB distributions using the maximum entropy paradigm and considering known facts as constraints. Nominal, <b>ordinal</b> and cardinal <b>quantities</b> can be treated within this framework equally. The architecture is scalable by tailoring the number of agents according to the available computer resources, to the priority of tasks, and to the maximum duration of the fusion process. Furthermore, the architecture allows cooperative work of human and automated agents and experts, as long as not all subtasks can be accomplished automatically...|$|R

