10000|10000|Public
5|$|Relatives of the Reuleaux {{triangle}} {{arise in}} the problem of finding the minimum perimeter shape that encloses a fixed amount of area and includes three specified points in the plane. For {{a wide range of}} choices of the area parameter, the <b>optimal</b> <b>solution</b> to this problem will be a curved triangle whose three sides are circular arcs with equal radii. In particular, when the three points are equidistant from each other and the area is that of the Reuleaux triangle, the Reuleaux triangle is the optimal enclosure.|$|E
5|$|The central {{pathways}} of metabolism described above, such as glycolysis and the {{citric acid}} cycle, {{are present in}} all three domains of living things and {{were present in the}} last universal common ancestor. This universal ancestral cell was prokaryotic and probably a methanogen that had extensive amino acid, nucleotide, carbohydrate and lipid metabolism. The retention of these ancient pathways during later evolution {{may be the result of}} these reactions having been an <b>optimal</b> <b>solution</b> to their particular metabolic problems, with pathways such as glycolysis and the citric acid cycle producing their end products highly efficiently and in a minimal number of steps. The first pathways of enzyme-based metabolism may have been parts of purine nucleotide metabolism, while previous metabolic pathways were a part of the ancient RNA world.|$|E
25|$|In {{optimization}} problems, heuristic algorithms {{can be used}} to find {{a solution}} close to the <b>optimal</b> <b>solution</b> in cases where finding the <b>optimal</b> <b>solution</b> is impractical. These algorithms work by getting {{closer and closer to the}} <b>optimal</b> <b>solution</b> as they progress. In principle, if run for an infinite amount of time, they will find the <b>optimal</b> <b>solution.</b> Their merit is that they can find a solution very close to the <b>optimal</b> <b>solution</b> in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.|$|E
30|$|Definition 3 (The set of Pareto <b>optimal</b> <b>solutions)</b> The set of Pareto <b>optimal</b> <b>solutions</b> (or non-dominated solutions) is a {{collection}} of all Pareto <b>optimal</b> <b>solutions.</b>|$|R
40|$|International audienceWe {{study in}} this paper the {{generation}} of the Choquet <b>optimal</b> <b>solutions</b> of biobjective combinatorial optimization problems. Choquet <b>optimal</b> <b>solutions</b> are solutions that optimize a Choquet integral. The Choquet integral is used as an aggregation function, presenting different parameters, and allowing {{to take into account}} the interactions between the objectives. We develop a new property that characterizes the Choquet <b>optimal</b> <b>solutions.</b> From this property, a general method to easily generate these solutions in the case of two objectives is defined. We apply the method to two classical biobjective optimization combinatorial optimization problems: the biobjective knapsack problem and the biobjective minimum spanning tree problem. We show that Choquet <b>optimal</b> <b>solutions</b> that are not weighted sum <b>optimal</b> <b>solutions</b> represent only a small proportion of the Choquet <b>optimal</b> <b>solutions</b> and are located in a specific area of the objective space, but are much harder to compute than weighted sum <b>optimal</b> <b>solutions...</b>|$|R
40|$|Alternative <b>optimal</b> <b>solutions</b> {{can give}} more choice for {{practical}} decision making. Therefore, {{the provision of}} methods for finding alternative <b>optimal</b> <b>solutions</b> {{is an important component}} part of the solution techniques for optimization models. The aim {{of this paper is to}} present a branch-and-bound algorithm for finding all <b>optimal</b> <b>solutions</b> of the linear assignment problem. Numerical experimental results are also given. Linear programming, assignment problem, alternative <b>optimal</b> <b>solutions,</b> branch-and-bound...|$|R
25|$|The <b>optimal</b> <b>solution</b> to the flux-balance {{problem is}} rarely unique with many possible, and equally optimal, {{solutions}} existing. Flux variability analysis (FVA), built into some analysis software, returns the boundaries for the fluxes through each reaction that can, paired {{with the right}} combination of other fluxes, estimate the <b>optimal</b> <b>solution.</b>|$|E
25|$|The result {{need not}} be an <b>optimal</b> <b>solution</b> to the problem.|$|E
25|$|Generally, {{sequence}} alignment means {{constructing a}} string from {{two or more}} given strings with the greatest similarity by adding, deleting letters, or adding a space for each string. The multiple sequence alignment problem is generally based on pairwise sequence alignment and currently, for pairwise sequence alignment problem, biologists can use dynamic programming approach to obtain its <b>optimal</b> <b>solution.</b> However, the multiple sequence alignment problem {{is still one of}} the intractable problems in bioinformatics, because finding the <b>optimal</b> <b>solution</b> of multiple sequence alignment has been proved as a NP-complete problem so that only approximate <b>optimal</b> <b>solution</b> can be obtained.|$|E
3000|$|Step 2.1 Determine all <b>optimal</b> <b>solutions</b> {{of problem}} (12), and suppose B is {{the set of}} <b>optimal</b> <b>solutions</b> of problem (12), [...]...|$|R
40|$|The {{problem of}} {{determining}} whether quadratic programming models possess either unique or multiple <b>optimal</b> <b>solutions</b> {{is important for}} empirical analyses which use a mathematical programming framework. Policy recommendations which disregard multiple <b>optimal</b> <b>solutions</b> (where they exist) are potentially incorrect and less than efficient. This paper proposes a strategy and the associated algorithm for finding all <b>optimal</b> <b>solutions</b> to any positive semidefinite linear complementarity problem. One of the main results is that the set of complementary solutions is convex. Although not obvious, this proposition {{is analogous to the}} well-known result in linear programming which states that any convex combination of <b>optimal</b> <b>solutions</b> is itself <b>optimal.</b> Research Methods/ Statistical Methods,...|$|R
40|$|When {{controlling}} {{the motion of}} tendon-based Stewart platforms, {{it is necessary to}} maintain the tendon forces inside an acceptable range, between a minimum and a maximum. This leads to a polyhedral set in the force space, where <b>optimal</b> <b>solutions</b> are represented as points of minimum norm. <b>Optimal</b> <b>solutions</b> may fail to be continuous along a trajectory, but they can be approximated by almost <b>optimal</b> <b>solutions</b> which are continuous except in singular postures...|$|R
25|$|The {{seemingly}} rational decisions {{during the}} game are in fact clearly irrational once one realizes that they {{are nothing more than}} a greedy algorithm, which is of course not guaranteed to give a globally <b>optimal</b> <b>solution.</b> In this case there is actually a globally <b>optimal</b> <b>solution,</b> which is to not play the game at all unless one is certain that there are no other players.|$|E
25|$|The maximum theorem of Claude Berge (1963) {{describes}} {{the continuity of}} an <b>optimal</b> <b>solution</b> {{as a function of}} underlying parameters.|$|E
25|$|Christofides {{made a big}} {{advance in}} this {{approach}} of giving an approach for which we know the worst-case scenario. Christofides algorithm given in 1976, at worst is 1.5 times longer than the <b>optimal</b> <b>solution.</b> As the algorithm was so simple and quick, many hoped it would {{give way to a}} near <b>optimal</b> <b>solution</b> method. However, until 2011 when it was beaten by less than a billionth of a percent, this remained the method with the best worst-case scenario.|$|E
40|$|Different {{implementation}} {{possibilities of}} an algorithm into hardware {{offer a variety}} of design solutions. Only best solutions so called pareto <b>optimal</b> <b>solutions</b> are for design decision of interest. Heuristic methods are preferred for the generation of these pareto <b>optimal</b> <b>solutions,</b> because exhaustive search methods cannot efficiently cope with this problem. This paper describes the mapping of this problem to a genetic algorithm. Further the generation of pareto <b>optimal</b> <b>solutions</b> is presented within an example. 1...|$|R
40|$|This paper clarifies {{the role}} of {{alternative}} <b>optimal</b> <b>solutions</b> in the clustering of multidimensional observations using data envelopment analysis (DEA). The paper shows that alternative <b>optimal</b> <b>solutions</b> corresponding to several units produce different groups with different sizes and different decision making units (DMUs) at each class. This implies that a specific DMU may be grouped into different clusters when the corresponding DEA model has multiple <b>optimal</b> <b>solutions.</b> © 2011 Elsevier B. V. All rights reserved...|$|R
30|$|In {{an attempt}} to address few {{challenges}} like computational efforts, <b>optimal</b> <b>solutions</b> and consistency in providing <b>optimal</b> <b>solutions,</b> this paper proposes a new optimization technique named social group optimization (SGO) based on the human behavior of learning and solving complex problems.|$|R
25|$|Convexity in {{existence}} theory of <b>optimal</b> <b>solution.</b> Actes du Congrès International des Mathématiciens (Nice, 1970), Tome 3, pp.187–192. Gauthier-Villars, Paris, 1971.|$|E
25|$|While the {{admissibility}} criterion guarantees an <b>optimal</b> <b>solution</b> path, it {{also means}} that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the <b>optimal</b> <b>solution</b> path. This new guarantee is referred to as ε-admissible.|$|E
25|$|The {{envelope}} theorem {{describes how}} {{the value of an}} <b>optimal</b> <b>solution</b> changes when an underlying parameter changes. The process of computing this change is called comparative statics.|$|E
30|$|The core of multi-objective {{optimization}} is {{to coordinate}} the relationships between objective functions, {{and to find out}} the <b>optimal</b> <b>solutions</b> which make the value of each objective function as small as possible. The multi-objective optimization algorithm has three main performance indices: 1) the obtained solutions should be as close to the true Pareto <b>optimal</b> <b>solutions</b> as possible; 2) try to keep the distribution and diversity of the individuals; 3) avoid missing the Pareto <b>optimal</b> <b>solutions</b> in the solving process.|$|R
40|$|A {{sequence}} of identification problems of coefficients in the parabolic equation with nonlinear boundary conditions is considered. The parameter (index of {{an element of}} the sequence) appears in the cost functionals as well as boundary data. It is proved that the <b>optimal</b> <b>solutions</b> exist and that under some continuous convergence of the cost functionals and the convergence of the data, the sets of <b>optimal</b> <b>solutions</b> converge in some sense to the set of <b>optimal</b> <b>solutions</b> of the limit problem...|$|R
40|$|Abstract. This paper {{addresses}} {{the problem of}} scheduling a parallel program represented by a directed acyclic task graph onto homogeneous cluster systems to minimize its execution time. Many sub-optimal algorithms have been pro-posed that promises acceptable running time but not <b>optimal</b> <b>solutions.</b> We pro-pose an algorithm that guarantees <b>optimal</b> <b>solutions</b> but not acceptable running time. Experiments show that in many practical cases the proposed algorithm performs very effectively in comparison with sub-optimal algorithms while still getting the <b>optimal</b> <b>solutions.</b> ...|$|R
25|$|In {{contrast}} to the simplex algorithm, which finds an <b>optimal</b> <b>solution</b> by traversing the edges between vertices on a polyhedral set, interior-point methods move through {{the interior of the}} feasible region.|$|E
25|$|Dynamic {{programming}} algorithms {{are often}} used for optimization. A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. In comparison, a greedy algorithm treats the solution as some sequence of steps and picks the locally optimal choice at each step. Using a greedy algorithm does not guarantee an <b>optimal</b> <b>solution,</b> because picking locally optimal choices {{may result in a}} bad global solution, but it is often faster to calculate. Some greedy algorithms (such as Kruskal's or Prim's for minimum spanning trees) are however proven to lead to the <b>optimal</b> <b>solution.</b>|$|E
25|$|The Max-Cut Problem is APX-hard, {{meaning that}} there is no polynomial-time {{approximation}} scheme (PTAS), arbitrarily close to the <b>optimal</b> <b>solution,</b> for it, unless P = NP. Thus, every polynomial-time approximation algorithm achieves an approximation ratio strictly less than one.|$|E
3000|$|Finally, we {{will show}} how to {{characterize}} all <b>optimal</b> <b>solutions</b> for the case when A and B have distinct non-zero eigenvalues (thus, m[*]=[*]n). The <b>optimal</b> <b>solutions</b> need to give equality in (48) and thus Lemma 3 gives that V Σ Σ [...]...|$|R
40|$|International audienceWe {{study in}} this paper the {{computation}} of Choquet <b>optimal</b> <b>solutions</b> in decision contexts involving multiple criteria or multiple agents. Choquet <b>optimal</b> <b>solutions</b> are solutions that optimize a Choquet integral, {{one of the most}} powerful tools in multicriteria decision making. We develop a new property that characterizes the Choquet <b>optimal</b> <b>solutions.</b> From this property, a general method to generate these solutions in the case of several criteria is proposed. We apply the method to different Pareto non-dominated sets coming from different knapsack instances with a number of criteria included between two and seven. We show that the method is effective for a number of criteria lower than five or for high size Pareto non-dominated sets. We also observe that the percentage of Choquet <b>optimal</b> <b>solutions</b> increase with the number of criteria...|$|R
30|$|In {{the next}} section, an {{analytical}} model {{is presented to}} achieve the <b>optimal</b> <b>solutions</b> for the frequency of CHs of sensor nodes. The basic idea is to formulate the problem as an ILP problem and to utilize ILP solvers [8] to compute the <b>optimal</b> <b>solutions.</b> These solutions are employed to evaluate the performance of previous heuristic algorithms.|$|R
25|$|Heuristics and metaheuristics make {{few or no}} {{assumptions}} about the problem being optimized. Usually, heuristics do not guarantee that any <b>optimal</b> <b>solution</b> need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems.|$|E
25|$|Various {{heuristics}} and approximation algorithms, which quickly yield good solutions {{have been}} devised. Modern methods can find solutions for extremely large problems (millions of cities) {{within a reasonable}} time which are with a high probability just 2–3% away from the <b>optimal</b> <b>solution.</b>|$|E
25|$|A greedy {{algorithm}} {{is similar to}} a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the <b>optimal</b> <b>solution</b> while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of {{greedy algorithm}}s is for finding the minimal spanning tree where finding the <b>optimal</b> <b>solution</b> is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.|$|E
40|$|We study interior-point {{methods for}} {{optimization}} {{problems in the}} case of infeasibility or unboundedness. While many such methods are designed to search for <b>optimal</b> <b>solutions</b> even when they do not exist, we show that they can be viewed as implicitly searching for well-de ned <b>optimal</b> <b>solutions</b> to related problems whose <b>optimal</b> <b>solutions</b> give certicates of infeasibility for the original problem or its dual. Our main development is in the context of linear programming, but we also discuss extensions to more general convex programming problems...|$|R
40|$|AbstractWe relate average optimal {{stationary}} {{policies in}} countable space Markov decision processes and <b>optimal</b> <b>solutions</b> of an associated infinite dimensional linear program. Using {{the theory of}} linear programming in abstract spaces, sufficient conditions for existence of <b>optimal</b> <b>solutions</b> are presented and some previous ones are interpreted...|$|R
30|$|B − 2 {{have the}} same <b>optimal</b> <b>solutions.</b>|$|R
