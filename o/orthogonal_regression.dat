172|179|Public
25|$|In applied statistics, total {{least squares}} {{is a type}} of errors-in-variables regression, a least squares data {{modeling}} technique in which observational errors on both dependent and independent variables are taken into account. It is a generalization of Deming regression and also of <b>orthogonal</b> <b>regression,</b> and can be applied to both linear and non-linear models.|$|E
2500|$|When the {{independent}} variable is error-free a residual represents the [...] "vertical" [...] distance between the observed data point and the fitted curve (or surface). In total least squares a residual represents the distance between a data point and the fitted curve measured along some direction. In fact, if both variables are measured in the same units and the errors on both variables are the same, then the residual represents the shortest distance between the data point and the fitted curve, that is, the residual vector is perpendicular to the tangent of the curve. For this reason, this type of regression is sometimes called two dimensional Euclidean regression (Stein, 1983) or <b>orthogonal</b> <b>regression.</b>|$|E
50|$|A {{trigonometric}} {{representation of}} the <b>orthogonal</b> <b>regression</b> line was given by Coolidge in 1913.|$|E
40|$|The paper {{introduces}} a construction algorithm for sparse kernel modelling using the leave-one-out test score {{also known as}} the PRESS (Predicted REsidual Sums of Squares) statistic. An efficient subset model selection procedure is developed in the <b>orthogonal</b> forward <b>regression</b> framework by incrementally maximizing the model generalization ca-pability to construct sparse models with good generaliza-tion properties. The proposed algorithm achieves a fully automated model construction without resort to any other validation data set for costly model evaluation. Index Terms — <b>orthogonal</b> forward <b>regression,</b> structure identification, cross validation, generalization. ...|$|R
40|$|System {{identification}} is a challenging and interesting engineering {{problem that has}} been studied for decades. In particular, the NARMAX methodology has been extensively used with interesting results. Such methodology identifies a deterministic parsimonious model by ranking a set of candidate terms using a linear dependency metric {{with respect to the}} output. Other metrics have been used that identify nonlinear dependencies, like the mutual information, but they are hard to interpret. In this work, the distance correlation metric is implemented together with the bagging method. These two implementations enhance the performance of the NARMAX methodology providing interpretability of nonlinear dependencies and uncertainty measures in the model identified. A comparison of the new BOFR-dCor (Bagging <b>Orthogonal</b> Forward <b>Regression</b> using distance Correlation) algorithm is done with respect to the traditional OFR (<b>Orthogonal</b> Forward <b>Regression)</b> algorithm and the OFR-MI (<b>Orthogonal</b> Forward <b>Regression</b> using Mutual Information) algorithm showing interesting results that improve interpretability and uncertainty analysis...|$|R
40|$|ODRPACK (TOMS Algorithm 676) has {{provided}} a complete package for weighted <b>orthogonal</b> distance <b>regression</b> for many years. The code is complete with user selectable reporting facilities, numerical and analytic derivatives, derivative checking, and many more features. The foundation for the algorithm is a stable and efficient trust region Levenberg-Marquardt minimizer that exploits {{the structure of the}} <b>orthogonal</b> distance <b>regression</b> problem. ODRPACK 95 is a modification of the original ODRPACK code that adds support for bound constraints, uses the newer Fortran 95 language, and simplifies the interface to the user called subroutine...|$|R
5000|$|... 2012/10 Origin 9 {{with high}} {{performance}} OpenGL 3D Graphing, <b>orthogonal</b> <b>regression</b> for implicit/explicit functions ...|$|E
5000|$|If Z ≠ 0, the <b>orthogonal</b> <b>regression</b> line {{goes through}} the {{centroid}} and {{is parallel to the}} vector from the origin to [...]|$|E
50|$|In {{the case}} of three non-collinear points in the plane, the {{triangle}} with these points as its vertices has a unique Steiner inellipse that is tangent to the triangle's sides at their midpoints. The major axis of this ellipse falls on the <b>orthogonal</b> <b>regression</b> line for the three vertices.|$|E
40|$|A novel program ESTAC, {{written in}} FORTRAN language, {{represents}} an effective tool for processing the meassured data containig uncertainty in both independent and dependent variables. The use of <b>orthogonal</b> distance <b>regression</b> package ODRPACK improves {{the estimation of}} complexation constant from experimental results...|$|R
40|$|At {{present the}} {{preferred}} method for fitting a general curve through scat-tered data {{points in the}} plane is <b>orthogonal</b> distance <b>regression,</b> i. e., by min-imising the sum of squares of the distances from each data point to its nearest neighbour on the curve. While generally producing good fits, in theory orthog-onal distance regression can be both biased and inconsistent: in practice this manifest itself in overfitting of convex curves or underfitting of corners. The paper postulates this occurs because <b>orthogonal</b> distance <b>regression</b> {{is based on an}} incomplete stochastic model of the problem. It therefore presents an extension of the standard model that takes into accounts both the noisy mea-surement of points on the curve and their underlying distribution along the curve. It then derives the likelihood function of a given curve being observed under this model. Although this cannot be evaluated exactly for anything other than the simplest curves, it lends itself naturally to asymptotic approximation. <b>Orthogonal</b> distance <b>regression</b> corresponds to a first order approximation to the maximum likelihood estimator in this model: the paper also derives a sec-ond order approximation, which {{turns out to be a}} simple modification of the least squares penalty that includes a contribution from the curvature at the closest point. Analytical and numerical examples are presented to demonstrate the improvement achieved using the higher order estimator. APPROVED FOR PUBLIC RELEASE AQ fö 3 -ohoi 4...|$|R
40|$|Abstract. Nonparametric <b>orthogonal</b> series <b>regression</b> {{function}} estimation is {{investigated in}} the case of a fixed point design where the observation points are irregularly spaced in a finite interval [a, b]i⊂R. Convergence rates for the integrated mean-square error and pointwise mean-square error are obtained {{in the case}} of estimators constructed using the Legendre polynomials and Haar functions for regression functions satisfying the Lipschitz condition...|$|R
5000|$|Deming {{regression}} — {{assumes that}} the ratio δ = σ²ε/σ²η is known. This could be appropriate for example when errors in y and x are both caused by measurements, and the accuracy of measuring devices or procedures are known. The case when δ = 1 {{is also known as}} the <b>orthogonal</b> <b>regression.</b>|$|E
50|$|In applied statistics, total {{least squares}} {{is a type}} of errors-in-variables regression, a least squares data {{modeling}} technique in which observational errors on both dependent and independent variables are taken into account. It is a generalization of Deming regression and also of <b>orthogonal</b> <b>regression,</b> and can be applied to both linear and non-linear models.|$|E
50|$|Knowing the {{shortest}} distance {{from a point}} to a line {{can be useful in}} various situations - for example, finding {{the shortest}} distance to reach a road, quantifying the scatter on a graph, etc. In Deming regression, a type of linear curve fitting, if the dependent and independent variables have equal variance this results in <b>orthogonal</b> <b>regression</b> in which the degree of imperfection of the fit is measured for each data point as the perpendicular distance of the point from the regression line.|$|E
40|$|This {{correspondence}} {{introduces a}} new <b>orthogonal</b> forward <b>regression</b> (OFR) model identification algorithm using D-optimality for model structure selection {{and is based}} on an M-estimators of parameter estimates. M-estimator is a classical robust parameter estimation technique to tackle bad data conditions such as outliers. Computationally, The M-estimator can be derived using an iterative reweighted least squares (IRLS) algorithm. D-optimality is a model structure robustness criterion in experimental design to tackle ill-conditioning in model Structure. The <b>orthogonal</b> forward <b>regression</b> (OFR), often based on the modified Gram-Schmidt procedure, is an efficient method incorporating structure selection and parameter estimation simultaneously. The basic idea of the proposed approach is to incorporate an IRLS inner loop into the modified Gram-Schmidt procedure. In this manner, the OFR algorithm for parsimonious model structure determination is extended to bad data conditions with improved performance via the derivation of parameter M-estimators with inherent robustness to outliers. Numerical examples are included to demonstrate the effectiveness of the proposed algorithm...|$|R
40|$|The minimal model {{structure}} detection (MMSD) {{problem in}} nonlinear dynamic system identification is formulated as {{a search for}} the optimal orthogonalization path. While an exhaustive search for a model with 20 candidate terms would involve 2. 43 x 10 (18) possible paths, it is shown that this can typically be reduced to 2 x 10 (3) by augmenting the orthogonal estimation algorithm with genetic search procedures. The MMSD algorithm provides the first practical solution for optimal structure detection in NARMAX modelling, training neural networks and fuzzy systems modelling. Based on the MMSD algorithm, a refined forward <b>regression</b> <b>orthogonal</b> (RFRO) algorithm is developed. The RFRO algorithm initially detects a parsimonious model structure using the forward <b>regression</b> <b>orthogonal</b> algorithm and then refines the model structure by applying the MMSD algorithm to the reduced model term set. The RFRO algorithm cannot guarantee to find the minimal model structure, but it is computationally more efficient than the MMSD algorithm and can find a smaller model than the forward <b>regression</b> <b>orthogonal</b> algorithm...|$|R
40|$|A common {{method of}} fitting curves and {{surfaces}} to data is {{to minimize the}} sum of squares of the orthogonal distances from the data points to the curve or surface, {{a process known as}} <b>orthogonal</b> distance <b>regression.</b> Here we consider fitting geometrical objects to data when some orthogonal distances are not available. Methods based on the Gauss-Newton method are developed, analysed and illustrated by examples...|$|R
5000|$|When the {{independent}} variable is error-free a residual represents the [...] "vertical" [...] distance between the observed data point and the fitted curve (or surface). In total least squares a residual represents the distance between a data point and the fitted curve measured along some direction. In fact, if both variables are measured in the same units and the errors on both variables are the same, then the residual represents the shortest distance between the data point and the fitted curve, that is, the residual vector is perpendicular to the tangent of the curve. For this reason, this type of regression is sometimes called two dimensional Euclidean regression (Stein, 1983) or <b>orthogonal</b> <b>regression.</b>|$|E
40|$|Global depth, tangent {{depth and}} simplicial depths for {{classical}} and <b>orthogonal</b> <b>regression</b> are compared in examples, and properties that {{are useful for}} calculations are derived. The robustness of the maximum simplicial depth estimates is shown in examples. Algorithms for the calculation of depths for <b>orthogonal</b> <b>regression</b> are proposed, and tests for multiple regression are transferred to <b>orthogonal</b> <b>regression.</b> These tests are distribution free {{in the case of}} bivariate observations. For a particular test problem, the powers of tests that are based on simplicial depth and tangent depth are compared by simulations. <b>Orthogonal</b> <b>regression</b> Tangent depth Global depth Simplicial depth Asymptotic tests...|$|E
40|$|<b>Orthogonal</b> <b>regression</b> {{is one of}} the {{standard}} linear regression methods to correct for the effects of measurement error in predictors. We argue that <b>orthogonal</b> <b>regression</b> is often misused in errors-in-variables linear regression, because of a failure to account for equation errors. The typical result is to overcorrect for measurement error, i. e., overestimate the slope, because equation error is ignored. The use of <b>orthogonal</b> <b>regression</b> must include a careful assessment of equation error, and not merely the usual (often informal) estimation of the ratio of measurement error variances. There are rarer instances, e. g., an example from geology discussed here, where the use of <b>orthogonal</b> <b>regression</b> without proper attention to modeling may lead to either overcorrection or undercorrection, depending on the relative sizes of the variances involved. Thus, our main point, which {{does not seem to be}} widely appreciated, is that <b>orthogonal</b> <b>regression</b> requires rather careful modeling of error. 1 INT [...] ...|$|E
40|$|A novel {{iterative}} learning {{algorithm is}} proposed to improve the classic <b>Orthogonal</b> Forward <b>Regression</b> (OFR) algorithm {{in an attempt to}} produce an optimal solution under a purely OFR framework without using any other auxiliary algorithms. The new algorithm searches for the optimal solution on a global solution space while maintaining the advantage of simplicity and computational efficiency. Both a theoretical analysis and simulations demonstrate the validity of the new algorithm...|$|R
40|$|In this paper, a {{modified}} <b>Orthogonal</b> Forward <b>Regression</b> (OFR) least-squares algorithm is presented for system identification and modelling from noisey regressors. Under the asumption {{that the energy}} and signal-to-noise ratio (SNR) of the signals are known or can be estimated, it is shown that unbiased estimates of the Error Reducation Ratios (ERRs) and the parameters can be obtained in each forward regression step. Examples are provided to illustrate the proposed approach...|$|R
40|$|The {{problem is}} {{considered}} of tting surfaces to measured data using the least squares norm, {{where it is}} assumed that a parameter-ization of the surface is available. Examples of practical applications include the product design and quality assurance of manufactured parts. There has been much recent algorithmic development based on conventional tting ideas, mainly <b>orthogonal</b> distance <b>regression.</b> A dierent approach is taken here which explicitly takes account of th...|$|R
40|$|AbstractGlobal depth, tangent {{depth and}} simplicial depths for {{classical}} and <b>orthogonal</b> <b>regression</b> are compared in examples, and properties that {{are useful for}} calculations are derived. The robustness of the maximum simplicial depth estimates is shown in examples. Algorithms for the calculation of depths for <b>orthogonal</b> <b>regression</b> are proposed, and tests for multiple regression are transferred to <b>orthogonal</b> <b>regression.</b> These tests are distribution free {{in the case of}} bivariate observations. For a particular test problem, the powers of tests that are based on simplicial depth and tangent depth are compared by simulations...|$|E
40|$|We {{present a}} {{comparison}} of different depth notions which are appropriate for classical and <b>orthogonal</b> <b>regression</b> with and without intercept. We consider the global depth and tangential depth introduced by Mizera (2002) and the simplicial depth studied for regression in detail at first by Müller (2005). The global depth and the tangential depth are based on quality functions. These quality functions can be based on likelihood functions as was used in Mizera and Müller (2004) and Müller (2005) or on residuals. For <b>orthogonal</b> <b>regression</b> {{the approach based on}} residuals is more appropriate. Classical regression and <b>orthogonal</b> <b>regression</b> differ only by the definition of the residuals. In classical regression, residuals are defined by the distance of the data points to the regression line or plane in vertical direction, i. e. parallel to the y-axis. In <b>orthogonal</b> <b>regression,</b> the residual is the distance between the data point and the regression line or plane in perpendicular direction to the line or plane. This residual is invariant with respect to rotations of the axis since only the direction perpendicular to the regression line or plane is used. <b>Orthogonal</b> <b>regression</b> should be used in cases where no natural x- and y-axes exists like in image analysis. 2 Global and tangential depth for regression with intercep...|$|E
40|$|Until {{a decade}} ago, {{regression}} analyses for conversions between {{different types of}} magnitude were using only the ordinary least squares method,which assumes that the independent variable is error free, or the simple <b>orthogonal</b> <b>regression</b> method,which assumes equal uncertainties for the two variables. The recent literature {{became aware of the}} inadequacy of such approaches and proposes the use of general <b>orthogonal</b> <b>regression</b> methods that account for different uncertainties of the two regression variables. Under the common assumption that only the variance ratio η between the dependent and independent variables is known, we compared three of such general <b>orthogonal</b> <b>regression</b> methods that have been applied to magnitude conversions: the chi-square regression, the general <b>orthogonal</b> <b>regression,</b> and the weighted total least squares. Although their formulations might appear quite different, we show that, under appropriate conditions, they all compute almost exactly the same regression coefficients and very similar (albeit slightly different) formal uncertainties. The latter are in most cases smaller than those estimated by bootstrap simulation but the amount of the deviation depends on the data set and on the assumed variance ratio...|$|E
40|$|Abstract—This {{correspondence}} {{introduces a}} new <b>orthogonal</b> forward <b>regression</b> (OFR) model identification algorithm using D-optimality for model structure selection {{and is based}} on an M-estimators of parameter estimates. M-estimator is a classical robust parameter estimation technique to tackle bad data conditions such as outliers. Computationally, The M-estimator can be derived using an iterative reweighted least squares (IRLS) algorithm. D-optimality is a model structure robustness criterion in experimental design to tackle ill-conditioning in model structure. The <b>orthogonal</b> forward <b>regression</b> (OFR), often based on the modified Gram–Schmidt procedure, is an efficient method incorporating structure selection and parameter estimation simultaneously. The basic idea of the proposed approach is to incorporate an IRLS inner loop into the modified Gram–Schmidt procedure. In this manner, the OFR algorithm for parsimonious model structure determination is extended to bad data conditions with improved performance via the derivation of parameter M-estimators with inherent robustness to outliers. Numerical examples are included to demonstrate the effectiveness of the proposed algorithm. Index Terms—Forward regression, Gram–Schmidt, identification, M-estimator, model structure selection. I...|$|R
40|$|Wavelet based nonparametric {{additive}} NARX {{models are}} proposed for nonlinear input-output system identification. By expanding each functional {{component of the}} nonparametic NARX model into wavelet multiresolution expansions, the nonparametric estimation problem becomes a linear-in-the-parameters problem and least-squares-based methods such as the <b>orthogonal</b> forward <b>regression</b> (OFR) approach {{can be used to}} select the model terms and estimate the parameters. Wavelet based additive models, combined with model order determination and variable selection approaches, are capable of handling problems of high dimensionality...|$|R
40|$|What factors {{influence}} state economic growth? This article uses annual state (and local) {{data for}} the years 1947 through 1997 for the forty-eight contiguous states to estimate {{the effects of a}} large number of factors, including taxation and expenditure policies, on state economic growth. A special feature of the empirical work is the use of <b>orthogonal</b> distance <b>regression</b> (ODR) to deal with the likely presence of measurement error in many of the variables. The results indicate that the correlation between state (and state and local) taxation policies is often statistically significant but also quite sensitive to the specific regressor set and time period; in contrast, the effects of expenditure policies are much more consistent. Of some interest, there is moderately strong evidence that a state’s political orientation has consistent and measurable effects on economic growth; perhaps, surprisingly, a more ‘‘conservative’’ political orientation is associated with lower rates of economic growth. Finally, correction for measurement error is essential in estimating the growth impacts of policies. Indeed, when measurement error is considered via ODR estimation, the estimation results do not support conditional convergence in state per capita income. fiscal policies; regional economic growth; <b>orthogonal</b> distance <b>regression...</b>|$|R
40|$|We use {{the local}} maxima of a redescending M-estimator to {{identify}} cluster, a method proposed already by Morgenthaler (in: H. D. Lawrence, S. Arthur (Eds.), Robust Regression, Dekker, New York, 1990, pp. 105 - 128) for finding regression clusters. We {{work out the}} method not only for classical regression but also for <b>orthogonal</b> <b>regression</b> and multivariate location and show that all three approaches are special cases of a general approach which includes also other cluster problems. For the general case we show consistency for an asymptotic objective function which generalizes the density in the multivariate case. The approach of <b>orthogonal</b> <b>regression</b> {{is applied to the}} identification of edges in noisy images. Kernel density estimation M-estimation Consistency Multivariate cluster Regression cluster <b>Orthogonal</b> <b>regression</b> Edge identification in noisy images...|$|E
40|$|Ordinary least-squares {{regression}} {{treats the}} variables asymmetrically, designating a dependent variable {{and one or}} more independent variables. When it is not obvious how to make this distinction, a researcher may prefer to use <b>orthogonal</b> <b>regression,</b> which treats the variables symmetrically. However, the usual procedure for <b>orthogonal</b> <b>regression</b> is not equivariant. A simple modification is proposed to overcome this serious defect. Illustrative computations involving 15 observations on 5 variables are provided, and a robust version of the method is discussed. The modified <b>orthogonal</b> <b>regression</b> allows a researcher to explore a symmetric, equivariant, and robust linear relationship among a set of variables. (Contains 6 references.) (Author/SLD) * Reproductions supplied by EDRS are the best {{that can be made}} from the original document...|$|E
30|$|It is {{well known}} that in the EV model, the {{ordinary}} least-squares (OLS) estimators are biased and inconsistent and that <b>orthogonal</b> <b>regression</b> is better in that case Fuller [17]. However, both methods are very sensitive to outliers in the data and some robust alternatives have been proposed. Brown [18] and Ketellapper and Ronner [19] applied robust ordinary regression techniques in the EV model. Zamar [20] proposed robust <b>orthogonal</b> <b>regression</b> M-estimators and showed that it outperformed the robust ordinary regression. Cheng and Van Ness [21] generalized the proposal of Zamar by defining robust orthogonal Generalized M-estimators which had bounded influence function in the simple case. He and Liang [22] proposed a regression quantile approach in the EV model which allowed for heavier tailed errors distribution than the gaussian distribution. Fekri and Ruiz-Gazen [23] proposed robust weighted <b>orthogonal</b> <b>regression.</b>|$|E
40|$|Numerous {{regression}} {{approaches to}} isotherm parameters estimation {{appear in the}} literature. The real insight into the proper modeling pattern can be achieved only by testing methods on a very big number of cases. Experimentally, it cannot {{be done in a}} reasonable time, so the Monte Carlo simulation method was applied. The objective {{of this paper is to}} introduce and compare numerical approaches that involve different levels of knowledge about the noise structure of the analytical method used for initial and equilibrium concentration determination. Six levels of homoscedastic noise and five types of heteroscedastic noise precision models were considered. Performance of the methods was statistically evaluated based on median percentage error and mean absolute relative error in parameter estimates. The present study showed a clear distinction between two cases. When equilibrium experiments are performed only once, for the homoscedastic case, the winning error function is ordinary least squares, while for the case of heteroscedastic noise the use of <b>orthogonal</b> distance <b>regression</b> or Margart’s percent standard deviation is suggested. It was found that in case when experiments are repeated three times the simple method of weighted least squares performed as well as more complicated <b>orthogonal</b> distance <b>regression</b> method...|$|R
40|$|A {{recently}} proposed {{method for}} automatic radiometric normalization of multi- and hyperspectral imagery {{based on the}} invariance property of the Multivariate Alteration Detection (MAD) transfortnation and <b>orthogonal</b> linear <b>regression</b> is extended by using an iterative re-weighting scheme involving no-change probabilities. The procedure is first investigated with partly artificial data and then applied to multitemporal, multispectral satellite imagery. Substantial improvement over the previous method is obtained for scenes which exhibit {{a high proportion of}} change. (C) 2007 Elsevier Inc. All rights reserved...|$|R
40|$|In this paper, a fast {{identification}} algorithm for nonlinear dynamic {{stochastic system}} identification is presented. The algorithm extends the classical <b>Orthogonal</b> Forward <b>Regression</b> (OFR) algorithm so {{that instead of}} using the Error Reduction Ratio (ERR) for term selection, a new optimality criterion —Shannon’s Entropy Power Reduction Ratio(EPRR) is introduced to deal with both Gaussian and non-Gaussian signals. It is shown that the new algorithm is both fast and reliable and examples are provided to illustrate {{the effectiveness of the}} new approach...|$|R
