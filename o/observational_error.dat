269|562|Public
5|$|In the 1980s and 1990s, Robert Harrington led {{a search}} to {{determine}} the real cause of the apparent irregularities. He calculated that any PlanetX would be at roughly three times the distance of Neptune from the Sun; its orbit would be highly eccentric, and strongly inclined to the ecliptic—the planet's orbit would be at roughly a 32-degree angle from the orbital plane of the other known planets. This hypothesis was met with a mixed reception. Noted PlanetX sceptic Brian G. Marsden of the Minor Planet Center pointed out that these discrepancies were a hundredth the size of those noticed by Le Verrier, and could easily be due to <b>observational</b> <b>error.</b>|$|E
5|$|It is {{possible}} that many transient phenomena might not {{be associated with the}} Moon itself but could be a result of unfavourable observing conditions or phenomena associated with the Earth. For instance, some reported transient phenomena are for objects near the resolution of the employed telescopes. The Earth's atmosphere can give rise to significant temporal distortions that could be confused with actual lunar phenomena (an effect known as astronomical seeing). Other non-lunar explanations include the viewing of Earth-orbiting satellites and meteors or <b>observational</b> <b>error.</b>|$|E
5|$|One {{attempt to}} {{overcome}} the above problems with transient phenomena reports was made during the Clementine mission by a network of amateur astronomers. Several events were reported, of which four of these were photographed both beforehand and afterward by the spacecraft. However, careful analysis of these images shows no discernible differences at these sites. This does not necessarily imply that these reports were a result of <b>observational</b> <b>error,</b> as {{it is possible that}} outgassing events on the lunar surface might not leave a visible marker, but neither is it encouraging for the hypothesis that these were authentic lunar phenomena.|$|E
40|$|In this study, {{we apply}} the local {{ensemble}} transform Kalman filter (LETKF) to the Nonhydrostatic Icosahedral Atmospheric Model (NICAM) {{to develop the}} NICAM-LETKF. In addition, an algorithm to adaptively estimate the inflation parameter and the <b>observational</b> <b>errors</b> is introduced to the LETKF. The feasibility and stability of the NICAM-LETKF are investigated under the perfect model scenario. According to the results, we confirm that the converged analysis errors of the NICAM-LETKF are smaller than the <b>observational</b> <b>errors,</b> and the magnitude and distribution of the root mean square errors (RMSEs) are {{comparable to those of}} the ensemble spreads. In our experiments, we find that the inflation parameter is optimally tuned and the <b>observational</b> <b>errors</b> are close to the true value. It is concluded that the NICAM-LETKF works appropriately and stably under the perfect model scenario even if the inflation parameter and the <b>observational</b> <b>errors</b> are adaptively estimated within the LETKF...|$|R
2500|$|When the <b>observational</b> <b>errors</b> are {{uncorrelated}} and {{the weight}} matrix, W, is diagonal, these may be written as ...|$|R
40|$|The {{determination}} of galaxy merger fraction of field galaxies using automatic morphological indices and photometric redshifts {{is affected by}} several biases if <b>observational</b> <b>errors</b> are not properly treated. Here, we correct these biases using maximum likelihood techniques. The method {{takes into account the}} <b>observational</b> <b>errors</b> to statistically recover the real shape of the bidimensional distribution of galaxies in redshift - asymmetry space, needed to infer the redshift evolution of galaxy merger fraction. We test the method with synthetic catalogs and show its applicability limits. The accuracy of the method depends on catalog characteristics such as the number of sources or the experimental error sizes. We show that the maximum likelihood method recovers the real distribution of galaxies in redshift and asymmetry space even when binning is such that bin sizes approach the size of the <b>observational</b> <b>errors.</b> We provide a step-by-step guide to applying maximum likelihood techniques to recover any one- or bidimensional distribution subject to <b>observational</b> <b>errors.</b> Comment: Accepted for publication in PASP. 18 pages, 4 figures, 6 table...|$|R
5|$|It {{was thought}} at the time that empty space was filled with a {{background}} medium called the luminiferous aether in which the electromagnetic field existed. Some physicists thought that this aether acted as a preferred frame of reference for the propagation of light and therefore {{it should be possible to}} measure the motion of the Earth with respect to this medium, by measuring the isotropy of the speed of light. Beginning in the 1880s several experiments were performed to try to detect this motion, the most famous of which is the experiment performed by Albert A. Michelson and Edward W. Morley in 1887. The detected motion was always less than the <b>observational</b> <b>error.</b> Modern experiments indicate that the two-way speed of light is isotropic (the same in every direction) to within 6 nanometres per second.|$|E
25|$|Note: Population {{statistics}} {{by religious}} affiliations {{are based upon}} statistical science and are subject to <b>observational</b> <b>error</b> (technically referred to as estimates). The proportion of Christians {{is based on the}} proportion of the population in each country who are members of a Christian denomination or who identify themselves as Christian. It says nothing about the proportion of such as believe in God and are regularly in the church. People who mix Christianity with tribal religions are defined in this article as Christians. Most of the percentage of Christian population of each country was taken from the US State Department's International Religious Freedom Report, the CIA World Factbook, Joshua Project, Open doors, Pew Forum and Adherents.com.|$|E
500|$|The {{fields of}} {{probability}} and statistics frequently use the normal distribution {{as a simple}} model for complex phenomena; for example, scientists generally assume that the <b>observational</b> <b>error</b> in most experiments follows a normal distribution. The Gaussian function, which is the probability density function of the normal distribution with mean [...] and standard deviation , naturally contains : ...|$|E
30|$|<b>Observational</b> <b>errors</b> {{that account}} for the errors in the Gauss {{coefficients}} estimated through the optimization process defined by Eq. (17).|$|R
40|$|We {{discuss a}} {{methodology}} {{of the machine}} learning to deduce the neutron star equation of state from a set of mass-radius observational data. We propose an efficient procedure {{to deal with a}} mapping from finite data points with <b>observational</b> <b>errors</b> onto an equation of state. We generate training data and optimize the neural network. Using independent validation data (mock observational data) we confirm that the equation of state is correctly reconstructed with precision surpassing <b>observational</b> <b>errors.</b> Comment: 5 pages, 4 figures; typos correcte...|$|R
50|$|If {{the model}} is linear, the prior {{probability}} density function (PDF) is homogeneous and <b>observational</b> <b>errors</b> are normally distributed, the theory simplifies to the classical optimal experimental design theory.|$|R
2500|$|The term [...] "margin of error" [...] {{is often}} used in non-survey {{contexts}} to indicate <b>observational</b> <b>error</b> in reporting measured quantities.|$|E
2500|$|Note {{that the}} {{oscillation}} data, {{rather than being}} continuous functions, are actually discrete samples in space and time, and are subject to <b>observational</b> <b>error.</b> [...] When computing transforms, interpolation is implied, a process which inevitably introduces further errors.|$|E
2500|$|To {{illustrate}} further, {{consider the}} question: [...] "Does our Universe rotate?" [...] To answer, we might {{attempt to explain}} {{the shape of the}} Milky Way galaxy using the laws of physics, although other observations might be more definitive, that is, provide larger discrepancies or less measurement uncertainty, like the anisotropy of the microwave background radiation or Big Bang nucleosynthesis. The flatness of the Milky Way depends on its rate of rotation in an inertial frame of reference. If we attribute its apparent rate of rotation entirely to rotation in an inertial frame, a different [...] "flatness" [...] is predicted than if we suppose part of this rotation actually is due to rotation of the universe and should not be included in the rotation of the galaxy itself. Based upon the laws of physics, a model is set up in which one parameter is the rate of rotation of the Universe. If the laws of physics agree more accurately with observations in a model with rotation than without it, we are inclined to select the best-fit value for rotation, subject to all other pertinent experimental observations. If no value of the rotation parameter is successful and theory is not within <b>observational</b> <b>error,</b> a modification of physical law is considered, for example, dark matter is invoked to explain the galactic rotation curve. So far, observations show any rotation of the universe is very slow, no faster than once every 60·1012 years (10−13 rad/yr), and debate persists over whether there is any rotation. However, if rotation were found, interpretation of observations in a frame tied to the universe would have to be corrected for the fictitious forces inherent in such rotation in classical physics and special relativity, or interpreted as the curvature of spacetime and the motion of matter along the geodesics in general relativity.|$|E
40|$|Aims. An {{effort has}} been made to {{simulate}} the expected Gaia Catalogue, including the effect of <b>observational</b> <b>errors.</b> We statistically analyse this simulated Gaia data to better understand what can be obtained from the Gaia astrometric mission. This catalogue is used to investigate the potential yield in astrometric, photometric, and spectroscopic information and the extent and effect of <b>observational</b> <b>errors</b> on the true Gaia Catalogue. This article is a follow-up to Robin et al. (A&A 543, A 100, 2012), where the expected Gaia Catalogue content was reviewed but without the simulation of <b>observational</b> <b>errors.</b> Methods. We analysed the Gaia Object Generator (GOG) catalogue using the Gaia Analysis Tool (GAT), thereby producing a number of statistics about the catalogue. Results. A simulated catalogue of one billion objects is presented, with detailed information on the 523 million individual single stars it contains. Detailed information is provided for the expected errors in parallax, position, proper motion, radial velocity, and the photometry in the four Gaia bands and for the physical parameter determination including temperature, metallicity, and line of sight extinction...|$|R
40|$|The random <b>observational</b> <b>errors</b> for {{meteorological}} variables {{within the}} Comprehensive Ocean–Atmosphere Dataset (COADS) have been determined using the semivariogram statistical technique. The error variance has been calculated using {{four months of}} data, spanning summer and winter months and the start {{and end of the}} dataset. The random errors found range from 1. 3 to 2. 8 m s− 1 for 10 -m-corrected wind speed, 1. 2 to 7. 1 mb for surface pressure, 0. 8 ° to 3. 3 °C for 10 -m air temperature, 0. 4 ° to 2. 8 °C for sea surface temperature, and 0. 6 to 1. 8 g kg− 1 for 10 -m specific humidity. The air temperature and specific humidity random <b>observational</b> <b>errors</b> contain a dependence on their mean values, but correlations between errors and mean values are low for the other variables analyzed. The accuracy of the error estimates increases with the number of observational data pairs used in the analysis. Wind speed random <b>observational</b> <b>errors</b> were reduced by height correction and by the use of the Lindau Beaufort Scale...|$|R
40|$|Aims: An {{effort has}} been {{undertaken}} to simulate the expected Gaia Catalogue, including the effect of <b>observational</b> <b>errors.</b> A statistical analysis of this simulated Gaia data is performed {{in order to better}} understand what can be obtained from the Gaia astrometric mission. This catalogue is used in order to investigate the potential yield in astrometric, photometric and spectroscopic information, and the extent and effect of <b>observational</b> <b>errors</b> on the true Gaia Catalogue. This article is a follow-up to Robin et. al. (2012), where the expected Gaia Catalogue content was reviewed but without the simulation of <b>observational</b> <b>errors.</b> Methods: The Gaia Object Generator (GOG) catalogue is analysed using the Gaia Analysis Tool (GAT), producing a number of statistics on the catalogue. Results: A simulated catalogue of one billion objects is presented, with detailed information on the 523 million individual single stars it contains. Detailed information is provided for the expected errors in parallax, position, proper motion, radial velocity, photometry in the four Gaia bands, and physical parameter determination including temperature, metallicity and line of sight extinction. Comment: 16 pages, 23 figures, 5 tables. Accepted to A&A on 02 / 04 / 201...|$|R
50|$|Independence of <b>observational</b> <b>error</b> from {{potential}} confounding effects.|$|E
5000|$|... where [...] {{denotes the}} {{background}} error covariance, [...] the <b>observational</b> <b>error</b> covariance.|$|E
5000|$|The term [...] "margin of error" [...] {{is often}} used in non-survey {{contexts}} to indicate <b>observational</b> <b>error</b> in reporting measured quantities.|$|E
30|$|Understanding how genetic {{networks}} act in embryonic development {{requires a}} detailed and statistically significant dataset integrating diverse observational results. The fruit fly (Drosophila melanogaster) {{is used as}} a model organism for studying developmental genetics. In recent years, several laboratories have systematically gathered confocal microscopy images of patterns of activity (expression) for genes governing early Drosophila development. Due to both the high variability between fruit fly embryos and diverse sources of <b>observational</b> <b>errors,</b> some new nontrivial procedures for processing and integrating the raw observations are required. Here we describe processing techniques based on genetic algorithms and discuss their efficacy in decreasing <b>observational</b> <b>errors</b> and illuminating the natural variability in gene expression patterns. The specific developmental problem studied is anteroposterior specification of the body plan.|$|R
3000|$|... 18 The {{residual}} ε {{cannot be}} interpreted as reflecting random preferences due to unobserved family characteristics. Otherwise, error terms would be correlated across alternatives. It is better {{to think of this}} term as describing <b>observational</b> <b>errors,</b> or possibly optimization errors or transitory departures from best choice by agents.|$|R
5000|$|In 1992 {{a simple}} check of 466 Tisiphone's {{position}} {{was made by}} the Association of Lunar and Planetary Observers (ALPO). The asteroid was found to be in the expected position to within <b>observational</b> <b>errors.</b> [...] Further checks were carried out in 1996, and 2006 with the asteroid in its expected position both times.|$|R
5000|$|Alternatively, ISO defines {{accuracy}} as {{describing a}} combination of both types of <b>observational</b> <b>error</b> above (random and systematic), so high accuracy requires both high precision and high trueness.|$|E
5000|$|One {{finished}} and treated item might be measured repeatedly to obtain ten test results. Only one item was measured {{so there is}} no replication. The repeated measurements help identify <b>observational</b> <b>error.</b>|$|E
50|$|Note {{that the}} {{oscillation}} data, {{rather than being}} continuous functions, are actually discrete samples in space and time, and are subject to <b>observational</b> <b>error.</b> When computing transforms, interpolation is implied, a process which inevitably introduces further errors.|$|E
40|$|We {{reconstruct}} the primordial {{spectrum of the}} curvature perturbation, P(k) from the observational data of the Wilkinson Microwave Anisotropy Probe (WMAP) by the cosmic inversion method developed recently. In contrast to conventional parameter-fitting methods, our method can reproduce small features in P(k) with good accuracy. To take the <b>observational</b> <b>errors</b> into account, we perform Monte Carlo simulations with fixed values of the cosmological parameters. As a result, we obtain an oscillatory P(k). We confirm that thus reconstructed P(k) recovers the WMAP angular power spectrum with the resolution of Δℓ≃ 5. However, a very similar oscillatory behavior is found in test calculations using artificial CMB data generated from a scale-invariant primordial spectrum by adding random numbers which mimic the <b>observational</b> <b>errors.</b> Thus, the oscillation found in P(k) for the WMAP data may be simply caused by the dispersion of data...|$|R
40|$|Principal Component Analysis (PCA) is {{the most}} common nonparametric method for {{estimating}} the volatility structure of Gaussian interest rate models. One major difficulty in the estimation of these models is the fact that forward rate curves are not directly observable from the market so that non-trivial <b>observational</b> <b>errors</b> arise in any statistical analysis. In this work, we point out that the classical PCA analysis is not suitable for estimating factors of forward rate curves due to the presence of measurement errors induced by market microstructure effects and numerical interpolation. Our analysis indicates that the PCA based on the long-run covariance matrix is capable to extract the true covariance structure of the forward rate curves in the presence of <b>observational</b> <b>errors.</b> Moreover, it provides a significant reduction in the pricing errors due to noisy data typically founded in forward rate curves. Comment: 27 page...|$|R
25|$|In applied statistics, total {{least squares}} {{is a type}} of errors-in-variables regression, a least squares data {{modeling}} technique in which <b>observational</b> <b>errors</b> on both dependent and independent variables are taken into account. It is a generalization of Deming regression and also of orthogonal regression, and can be applied to both linear and non-linear models.|$|R
5000|$|<b>Observational</b> <b>error</b> (or {{measurement}} error) is {{the difference}} between a measured value of a quantity and its true value. In statistics, an error is not a [...] "mistake." [...] Variability is an inherent part of the results of measurements and of the measurement process.|$|E
5000|$|The {{fields of}} {{probability}} and statistics frequently use the normal distribution {{as a simple}} model for complex phenomena; for example, scientists generally assume that the <b>observational</b> <b>error</b> in most experiments follows a normal distribution. The Gaussian function, which is the probability density function of the normal distribution with mean [...] and standard deviation , naturally contains : ...|$|E
5000|$|Statistical theory {{provides}} {{a guide to}} comparing methods of data collection, {{where the problem is}} to generate informative data using optimization and randomization while measuring and controlling for <b>observational</b> <b>error.</b> Optimization of data collection reduces the cost of data while satisfying statistical goals, while randomization allows reliable inferences. Statistical theory {{provides a}} basis for good data collection and the structuring of investigations in the topics of: ...|$|E
50|$|The HWB {{solution}} is very fast to compute {{but it is}} optimal only if <b>observational</b> <b>errors</b> do not correlate between the data blocks. The generalized canonical correlation analysis (gCCA) is the statistical method of choice for making those harmful cross-covariances vanish. This may, however, become quite tedious depending {{on the nature of}} the problem.|$|R
50|$|In applied statistics, total {{least squares}} {{is a type}} of errors-in-variables regression, a least squares data {{modeling}} technique in which <b>observational</b> <b>errors</b> on both dependent and independent variables are taken into account. It is a generalization of Deming regression and also of orthogonal regression, and can be applied to both linear and non-linear models.|$|R
30|$|This paper {{focuses on}} the second application, SV prediction, with an {{explicit}} goal to provide an SV candidate model to the IGRF system. This application demands not only a theoretical understanding of the numerical dynamo model and geomagnetic data, but also practical techniques to improve the prediction accuracy. Both require proper assessment of model and <b>observational</b> <b>errors.</b>|$|R
