195|930|Public
25|$|An {{input neuron}} has no predecessor, but {{serves as an}} input {{interface}} for the whole network. Similarly, an <b>output</b> <b>neuron</b> has no successor, and thus serves as the output interface of the whole network.|$|E
50|$|When {{multiple}} perceptrons {{are combined}} in an artificial neural network, each <b>output</b> <b>neuron</b> operates independently {{of all the}} others; thus, learning each output can be considered in isolation.|$|E
50|$|An {{input neuron}} has no {{predecessor}} but serves as input interface {{for the whole}} network. Similarly an <b>output</b> <b>neuron</b> has no successor and thus serves as output interface of the whole network.|$|E
40|$|The rat {{nucleus accumbens}} {{contains}} acetylcholine-releasing interneurons, presumed {{to play a}} regulatory role in the electrical activity of medium spiny <b>output</b> <b>neurons.</b> In order to examine this issue in detail, we made electrophysiological recordings in rat nucleus accumbens slices. These experiments showed that γ-aminobutyric acid-mediated inhibition of the <b>output</b> <b>neurons</b> might be facilitated by activation of nicotinic acetylcholine receptors, {{in addition to being}} suppressed via activation of muscarinic acetylcholine receptors. In contrast, glutamatergic excitation of <b>output</b> <b>neurons</b> appeared to be inhibited by activation of muscarinic acetylcholine receptors and to be insensitive to activation of nicotinic acetylcholine receptors. The spontaneous firing frequency of cholinergic neurons appeared to be under control of both a muscarinic and a nicotinic pathway in a bi-directional manner. Finally, we made paired recordings in which the functional connection between cholinergic <b>neurons</b> and <b>output</b> <b>neurons</b> was monitored. Driving the cholinergic neurons at physiological firing frequencies stimulated γ-aminobutyric acid-mediated inhibition of the <b>output</b> <b>neurons,</b> via activation of nicotinic acetylcholine receptors. The onset of this effect was slow and lacked a fixed delay. These data indicate that activation of nicotinic acetylcholine receptors in rat nucleus accumbens may mediate the facilitation of γ-aminobutyric acid-mediated inhibition of medium spiny <b>output</b> <b>neurons.</b> Possible mechanisms of neurotransmission, mediating this cholinergic modulation are discussed...|$|R
40|$|Huntington {{disease is}} a dominantly inherited, untreatable {{neurological}} disorder featuring a progressive loss of striatal <b>output</b> <b>neurons</b> that results in dyskinesia, cognitive decline, and, ultimately, death. Neurotrophic factors have recently {{been shown to be}} protective in several animal models of neurodegenerative disease, raising the possibility that such substances might also sustain the survival of compromised striatal <b>output</b> <b>neurons.</b> We determined whether intracerebral administration of brain-derived neurotrophic factor, nerve growth factor, neurotrophin- 3, or ciliary neurotrophic factor could protect striatal <b>output</b> <b>neurons</b> in a rodent model of Huntington disease. Whereas treatment with brain-derived neurotrophic factor, nerve growth factor, or neurotrophin- 3 provided no protection of striatal <b>output</b> <b>neurons</b> from death induced by intrastriatal injection of quinolinic acid, an N-methyl-D-aspartate glutamate receptor agonist, treatment with ciliary neurotrophic factor afforded marked protection against this neurodegenerative insult...|$|R
3000|$|The type of the {{controlling}} neural network was chosen a three-layer feedforward network with a sigmoidal transfer function, {{which has been}} adapted by backpropagation. Here are the input neurons labeled X_i, hidden <b>neurons</b> Z_i, and <b>output</b> <b>neurons</b> Y_i [...]. Hidden and <b>output</b> <b>neurons</b> have a threshold equaled to value “ 1 ”.|$|R
5000|$|Therefore, the {{derivative}} {{with respect}} to [...] can be calculated if all the derivatives {{with respect to}} the outputs [...] of the next layer - the one closer to the <b>output</b> <b>neuron</b> - are known.|$|E
5000|$|The {{gradient}} descent method involves calculating the derivative of the squared error function {{with respect to}} the weights of the network. This is normally done using backpropagation. Assuming one <b>output</b> <b>neuron,</b> the squared error function is: ...|$|E
5000|$|A {{mechanism}} that permits the neurons {{to compete for}} the right to respond to a given subset of inputs, such that only one <b>output</b> <b>neuron</b> (or only one neuron per group), is active (i.e. [...] "on") at a time. The neuron that wins the competition is called a [...] "winner-take-all" [...] neuron.|$|E
40|$|SummaryGamma {{oscillations}} {{are commonly}} observed in sensory brain structures, {{notably in the}} olfactory bulb. The mechanism by which gamma is generated in the awake rodent and its functional significance are still unclear. We combined pharmacological and genetic approaches in the awake mouse olfactory bulb to show that gamma oscillations required the synaptic interplay between excitatory <b>output</b> <b>neurons</b> and inhibitory interneurons. Gamma oscillations were amplified, or abolished, after optogenetic activation or selective lesions to the bulbar <b>output</b> <b>neurons.</b> In response to a moderate increase of the excitation/inhibition ratio in <b>output</b> <b>neurons,</b> long-range gamma synchronization was selectively enhanced while the mean firing activity and the amplitude of inhibitory inputs both remained unchanged in <b>output</b> <b>neurons.</b> This excitation/inhibition imbalance also impaired odor discrimination in an olfactory learning task, suggesting that proper fast neuronal synchronization may be critical for the correct discrimination of similar sensory stimuli...|$|R
40|$|Introduction Backpropagation and {{contrastive}} Hebbian learning (CHL) are two {{supervised learning}} algorithms for training networks with hidden neurons. They are of interest, {{because they are}} generally applicable to wide classes of network architectures. In backpropagation (Rumelhart, Hinton, & Williams, 1986 b, 1986 a), an error signal for the <b>output</b> <b>neurons</b> is computed and propagated back into the hidden neurons through a separate teacher network. Synaptic weights are updated based on the product between the error signal and network activities. CHL updates the synaptic weights based on the steady states of neurons in two different phases: one with the <b>output</b> <b>neurons</b> clamped to the desired values {{and the other with}} the <b>output</b> <b>neurons</b> free (Movellan, 1990; Baldi & Pineda, 1991). Clamping the <b>output</b> <b>neurons</b> Neural Computation 15, 441 [...] 454 (2003) c 2002 Massachusetts Institute of Technology causes the hidden neurons to change their activities, and this change constitutes the basis for th...|$|R
40|$|The {{ability of}} {{primates}} to make rapid and accurate saccadic eye movements {{for exploring the}} natural world {{is based on a}} neuronal system in the brain that has been studied extensively and is known to include multiple brain regions extending throughout the neuraxis. We examined the characteristics of signal flow in this system by recording from identified <b>output</b> <b>neurons</b> of two cortical regions, the lateral intraparietal area (LIP) and the frontal eye field (FEF), and from neurons in a brainstem structure targeted by these <b>output</b> <b>neurons,</b> the superior colliculus (SC). We compared the activity of neurons in these three populations while monkeys performed a delayed saccade task that allowed us to quantify visual responses, motor activity, and intervening delay activity. We examined whether delay activity was related to visual stimulation by comparing the activity during interleaved trials when a target was either present or absent during the delay period. We examined whether delay activity was related to movement by using a Go/Nogo task and comparing the activity during interleaved trials in which a saccade was either made (Go) or not (Nogo). We found that LIP <b>output</b> <b>neurons,</b> FEF <b>output</b> <b>neurons,</b> and SC neurons can all have visual responses, delay activity, and presaccadic bursts; hence in this way they are all quite similar. However, the delay activity tended to be more related to visual stimulation in the cortical <b>output</b> <b>neurons</b> than in the SC neurons. Complementing this, the delay activity tended to be more related to movement in the SC neurons than in the cortical <b>output</b> <b>neurons.</b> We conclude, first, that the signal flow leaving the cortex represents activity at nearly every stage of visuomotor transformation, and second, that ther...|$|R
5000|$|It {{consists}} of one <b>output</b> <b>neuron,</b> K hidden neurons and K*N input neurons. Inputs {{to the network}} take 3 values: The weights between input and hidden neurons take the values: Output value of each hidden neuron is calculated as a sum of all multiplications of input neurons and these weights: Signum is a simple function, which returns -1,0 or 1: ...|$|E
50|$|If {{the purpose}} is not to perform strict {{interpolation}} but instead more general function approximation or classification the optimization is somewhat more complex {{because there is no}} obvious choice for the centers. The training is typically done in two phases first fixing the width and centers and then the weights. This can be justified by considering the different nature of the non-linear hidden neurons versus the linear <b>output</b> <b>neuron.</b>|$|E
5000|$|... where [...] is {{the number}} of neurons in the hidden layer, [...] is the center vector for neuron , and [...] is the weight of neuron [...] in the linear <b>output</b> <b>neuron.</b> Functions that depend only on the {{distance}} from a center vector are radially symmetric about that vector, hence the name radial basis function. In the basic form all inputs are connected to each hidden neuron. The norm is typically taken to be the Euclidean distance (although the Mahalanobis distance appears to perform better in general) and the radial basis function is commonly taken to be Gaussian ...|$|E
40|$|AbstractThe {{ability of}} {{primates}} to make rapid and accurate saccadic eye movements {{for exploring the}} natural world {{is based on a}} neuronal system in the brain that has been studied extensively and is known to include multiple brain regions extending throughout the neuraxis. We examined the characteristics of signal flow in this system by recording from identified <b>output</b> <b>neurons</b> of two cortical regions, the lateral intraparietal area (LIP) and the frontal eye field (FEF), and from neurons in a brainstem structure targeted by these <b>output</b> <b>neurons,</b> the superior colliculus (SC). We compared the activity of neurons in these three populations while monkeys performed a delayed saccade task that allowed us to quantify visual responses, motor activity, and intervening delay activity. We examined whether delay activity was related to visual stimulation by comparing the activity during interleaved trials when a target was either present or absent during the delay period. We examined whether delay activity was related to movement by using a Go/Nogo task and comparing the activity during interleaved trials in which a saccade was either made (Go) or not (Nogo). We found that LIP <b>output</b> <b>neurons,</b> FEF <b>output</b> <b>neurons,</b> and SC neurons can all have visual responses, delay activity, and presaccadic bursts; hence in this way they are all quite similar. However, the delay activity tended to be more related to visual stimulation in the cortical <b>output</b> <b>neurons</b> than in the SC neurons. Complementing this, the delay activity tended to be more related to movement in the SC neurons than in the cortical <b>output</b> <b>neurons.</b> We conclude, first, that the signal flow leaving the cortex represents activity at nearly every stage of visuomotor transformation, and second, that there is a gradual evolution of signal processing as one proceeds from cortex to colliculus...|$|R
40|$|We {{propose a}} new {{principle}} for replicating receptive field properties of neurons {{in the primary}} visual cortex. We derive a learning rule for a feedforward network, which maintains a low firing rate for the <b>output</b> <b>neurons</b> (resulting in temporal sparseness) and allows only a small subset of the neurons in the network to fire {{at any given time}} (resulting in population sparseness). Our learning rule also sets the firing rates of the <b>output</b> <b>neurons</b> at each time step to near-maximum or near-minimum levels, resulting in neuronal reliability. The learning rule is simple enough to be written in spatially and temporally local forms. After the learning stage is performed using input image patches of natural scenes, <b>output</b> <b>neurons</b> in the model network are found to exhibit simple-cell-like receptive field properties. When the output of these simple-cell-like neurons are input to another model layer using the same learning rule, the second-layer <b>output</b> <b>neurons</b> after learning become less sensitive to the phase of gratings than the simple-cell-like input neurons. In particular, some of the second-layer <b>output</b> <b>neurons</b> become completely phase invariant, owing to the convergence of the connections from first-layer neurons with similar orientation selectivity to second-layer neurons in the model network. We examine the parameter dependencies of the receptive field properties of the model neurons after learning and discuss their biological implications. We also show that the localized learning rule is consistent with experimental results concerning neuronal plasticity and can replicate the receptive fields of simple and complex cells...|$|R
40|$|The {{midbrain}} periaqueductal gray (PAG), and its descending projections to the rostral ventromedial medulla (RVM), {{provide an}} essential neural circuit for opioid-produced antinociception. Recent anatomical {{studies have reported}} that the projections from the PAG to the RVM are sexually dimorphic and that systemic administration of morphine significantly suppresses pain-induced activation of the PAG in male but not female rats. Given that morphine antinociception is produced in part by disinhibition of PAG <b>output</b> <b>neurons,</b> it is hypothesized that a differential activation of PAG <b>output</b> <b>neurons</b> mediates the sexually dimorphic actions of morphine. The present study examined systemic morphine-induced activation of PAG-RVM neurons {{in the absence of}} pain. The retrograde tracer Fluorogold (FG) was injected into the RVM to label PAG-RVM <b>output</b> <b>neurons.</b> Activation of PAG neurons was determined by quantifying the number of Fos-positive neurons 1 h following systemic morphine administration (4. 5 mg/kg). Morphine produced comparable activation of the PAG in both male and female rats, with no significant differences in either the quantitative or qualitative distribution of Fos. While microinjection of FG into the RVM labeled significantly more PAG <b>output</b> <b>neurons</b> in female rats than male rats, very few of these neurons (20 %) were activated by systemic morphine administration in comparison to males (50 %). The absolute number of PAG-RVM neurons activated by morphine was also greater in males. These data demonstrate widespread disinhibition of PAG neurons following morphine administration. The greater morphine-induced activation of PAG <b>output</b> <b>neurons</b> in male compared with female rats is consistent with the greater morphine-induced antinociception observed in males...|$|R
50|$|However, once trained, {{the network}} {{can also be}} run in reverse, being asked to adjust the {{original}} image slightly so that a given <b>output</b> <b>neuron</b> (e.g. the one for faces or certain animals) yields a higher confidence score. This {{can be used for}} visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles Backpropagation, however instead of adjusting the network weights, the weights are held fixed and the input is adjusted.|$|E
50|$|Classical {{conditioning}} of a Purkinje cell deficient mutant mouse strain {{helped to}} determine the extent to which spared regions in cerebellar cortex were compensating for lesioned regions in the studies mentioned above. These mice are born with PCs that die after about 3 weeks of life. Because PCs are the sole <b>output</b> <b>neuron</b> of the cortex, this model effectively lesions all of cerebellar cortex. Results of conditioning were similar to the cortical aspiration mice. Mice took significantly longer to produce CRs, and the timing and gain of the response were distorted (Chen et al., 1996). Therefore, although eyeblink CR learning deficits are associated with cerebellar cortex lesions, the structure does not appear, ultimately, to be essential for CR learning or retention.|$|E
50|$|Coincidence {{detection}} {{relies on}} separate inputs converging {{on a common}} target. Consider a basic neural circuit with two input neurons, A and B, that have excitatory synaptic terminals converging on a single <b>output</b> <b>neuron,</b> C (Fig. 1). If each input neuron's EPSP is subthreshold for an action potential at C, then C will not fire unless the two inputs from A and B are temporally close together. Synchronous arrival of these two inputs may push the membrane potential of a target neuron over the threshold required to create an action potential. If the two inputs arrive too far apart, the depolarization of the first input may have time to drop significantly, preventing the membrane potential of the target neuron from reaching the action potential threshold. This example incorporates the principles of spatial and temporal summation. Furthermore, coincidence detection can reduce the jitter formed by spontaneous activity. While random sub-threshold stimulations by neuronal cells may not often fire coincidentally, coincident synaptic inputs derived from a unitary external stimulus will ensure that a target neuron fires {{as a result of}} the stimulus.|$|E
50|$|BRNN can {{be trained}} using similar {{algorithms}} compared to RNN, because the two directional neurons {{do not have any}} interactions. However, when back-propagation is applied, additional processes are needed because updating input and output layers cannot be done at once. General procedures for training are as follows: For forward pass, forward states and backward states are passed first, then <b>output</b> <b>neurons</b> are passed. For backward pass, <b>output</b> <b>neurons</b> are passed first, then forward states and backward states are passed next. After forward and backward passes are done, the weights are updated.|$|R
40|$|In the electrosensory {{system of}} weakly {{electric}} fish, descending pathways to a first-order sensory nucleus {{have been shown}} to influ-ence the gain of its <b>output</b> <b>neurons.</b> The underlying neural mecha-nisms that subserve this descending gain control capability are not yet fully understood. We suggest that one possible gain control mechanism could involve the regulation of total membrane conduc-tance of the <b>output</b> <b>neurons.</b> In this paper, a neural model based on this idea is used to demonstrate how activity levels on descend-ing pathways could control both the gain and baseline excitation of a target neuron. ...|$|R
40|$|International audiencePattern {{recognition}} {{is used to}} classify the input data into different classes based on extracted key features. Increasing the recognition rate of pattern recognition applications is a challenging task. The spike neural networks inspired from physiological brain architecture, is a neuromorphic hardware implementation of network of neurons. A sample of neuromor-phic architecture has two layers of <b>neurons,</b> input and <b>output.</b> The number of input neurons is fixed based on the input data patterns. While the number of <b>outputs</b> <b>neurons</b> can be different. The goal {{of this paper is}} performance evaluation of neuromorphic architecture in terms of recognition rates using different numbers of <b>output</b> <b>neurons.</b> For this purpose a simulation environment of N 2 S 3 and MNIST handwritten digits are used. Our simulation results show the recognition rate for various number of <b>output</b> <b>neurons,</b> 20, 30, 50, 100, 200, and 300 is 70 %, 74 %, 79 %, 85 %, 89 %, and 91 %, respectively...|$|R
30|$|The {{activation}} {{function of the}} <b>output</b> <b>neuron</b> is also different; the <b>output</b> <b>neuron</b> is always activated by the linear function y = u, where u is {{the potential of the}} <b>output</b> <b>neuron</b> calculated as the scalar product of the weight vector v and the output vector o.|$|E
3000|$|... that {{contains}} the connection weights between the hidden neuron outputs and the <b>output</b> <b>neuron</b> input, and [...]...|$|E
3000|$|... is a {{variable}} {{that contains the}} addition of the weighted inputs to the <b>output</b> <b>neuron,</b> as presented in (7). Moreover, consider that [...]...|$|E
50|$|Kenyon {{cells are}} {{presynaptic}} to mushroom body <b>output</b> <b>neurons</b> in the lobes. However, the lobes {{are not only}} output regions; Kenyon cells are both pre and postsynaptic in these regions.|$|R
30|$|The SOM {{neural network}} is an {{unsupervised}} competitive learning technique that can {{carry out the}} characteristic identification and clustering, e.g. load analysis and clustering [16 – 18]. SOM network consists of two layers neurons of input and <b>output.</b> Each <b>neuron</b> in the input layer is connected with <b>neurons</b> of the <b>output</b> layer through a variable weight, and the <b>output</b> <b>neurons</b> form a two-dimensional planar array.|$|R
50|$|The echo state network (ESN), is a {{recurrent}} neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of <b>output</b> <b>neurons</b> {{can be learned}} so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden <b>neurons</b> to <b>output</b> <b>neurons.</b> Thus, the error function is quadratic {{with respect to the}} parameter vector and can be differentiated easily to a linear system.|$|R
30|$|In the {{considered}} application, {{the output}} layer {{is composed of}} a three neurons (which value ranges between − 1 and 1) corresponding {{to the type of}} particle to identify. Each output refers to a gamma, proton, and muon particle, respectively. If the value of an <b>output</b> <b>neuron</b> is positive, it may be assumed that the corresponding particle has been identified by the network. In the case that more than one <b>output</b> <b>neuron</b> is activated, the maximum value is taken into account.|$|E
3000|$|Finally, the <b>output</b> <b>neuron</b> {{may provide}} the desired results, that is ŷ_i = S_N_j/S_D [...] j = 1, 2, [...]...,k. 2.2. Niche Particle Swarm Optimization (NPSO).|$|E
3000|$|... where z_i^L is an ith {{element of}} the {{excitation}} vector zL. The value of the ith <b>output</b> <b>neuron</b> a_i^L represents the probability Pdnn(i|P) that the observation P(m,k) belongs to class i (speech or non-speech).|$|E
50|$|The {{fact that}} {{competitive}} networks recode sets of correlated inputs {{to one of}} a few <b>output</b> <b>neurons</b> essentially removes the redundancy in representation which {{is an essential part of}} processing in biological sensory systems.|$|R
5000|$|... where [...] {{defines the}} {{synaptic}} weight or connection strength between the th input and th <b>output</b> <b>neurons,</b> [...] and [...] are the {{input and output}} vectors, respectively, and [...] is the learning rate parameter.|$|R
40|$|Information {{processing}} {{in certain}} neuronal {{networks in the}} brain {{can be considered as}} a map of binary vectors, where ones (spikes) and zeros (no spikes) of input neurons are transformed into spikes and no spikes of <b>output</b> <b>neurons.</b> A simple but fundamental characteristic of such a map is how it transforms distances between input vectors. In particular what is the mean distance between output vectors given certain distance between input vectors? Using combinatorial approach we found an exact solution to this problem for networks of perceptrons with binary weights. he resulting formulas allow for precise analysis how network connectivity and neuronal excitability affect the transformation of distances between the vectors of neuronal spiking. As an application, we considered a simple network model of information processing in the hippocampus, a brain area critically implicated in learning and memory, and found a combination of parameters for which the <b>output</b> <b>neurons</b> discriminated similar and distinct inputs most effectively. A decrease of threshold values of the <b>output</b> <b>neurons,</b> which in biological networks may be associated with decreased inhibition, impaired optimality of discrimination...|$|R
