0|10000|Public
50|$|The IAT is {{influenced}} by individual differences in average IAT <b>response</b> <b>times</b> such that those with slower <b>overall</b> <b>response</b> <b>times</b> {{tend to have more}} extreme IAT scores. Older subjects also tend to have more extreme IAT scores, and this may be related to cognitive fluency, or slower <b>overall</b> <b>response</b> <b>times.</b>|$|R
40|$|We {{propose a}} multi-class queuing network model {{to study the}} impact of {{multimedia}} traffic on the <b>overall</b> <b>response</b> <b>times</b> to Web requests for cases with and without a proxy cache server (PCS). We find that an increase of multimedia traffic percentage will significantly impact the <b>response</b> <b>times</b> for both cases with and without a PCS. The PCS “hit rate” is identified {{as one of the}} factors that impact the performance of a PCS for multimedia traffic. Further, we find that it may not pay to choose an overly-powerful PCS, as the marginal reduction in the <b>overall</b> <b>response</b> <b>time</b> cannot be justified. We show that the required time to travel through the client network represents the dominant factor of <b>overall</b> <b>response</b> <b>time.</b> Although the firm’s network bandwidth does not impact the decision of whether to install a PCS, improving it has a significant benefit of shortening the <b>overall</b> <b>response</b> <b>time</b> to users’ Web requests...|$|R
40|$|Cloud {{computing}} {{has become}} a key component {{as well as a}} measure of success for various organizations today. Apart from benefits obtained, it is important {{to take into account the}} location of user-base and data-center, which is essential for performance and security reasons, since the location of data-center and user-base can impact the <b>overall</b> <b>response</b> <b>time.</b> In this paper evaluation of the effect on <b>overall</b> <b>response</b> <b>time,</b> of relevant factors such as the location of data-center and the serviced user-base is done...|$|R
5000|$|Application Programming Interfaces {{used on the}} {{server-side}} {{can increase}} <b>overall</b> <b>response</b> <b>time,</b> as is common in Restful APIs that process large amounts of data from a database.|$|R
3000|$|As {{the number}} of Lagrange {{multipliers}} is proportional to K_n and T, the computation time involved in updating λ will increase approximately with O(K_n T). Therefore, the overall computational complexity is O(K_n^ 2 T^ 2 D_n [...]). This indicates the computational complexity of our algorithm is polynomial with respect to problem size. Suppose the depth is V. In a distributed MAS architecture, a supply chain with depth V has at most V node in each directed path from a leaf node to the final node. Therefore, the <b>overall</b> <b>response</b> <b>time</b> will be O(∑_n K_n^ 2 T^ 2 D_n [...]). Let K = _n K_n. Then, the <b>overall</b> <b>response</b> <b>time</b> will be bounded by O(VK_^ 2 T^ 2 D_n [...]).|$|R
40|$|Tor (The Onion Routing) {{provides}} a secure mechanism for offering TCP-based services while concealing the hidden server’s IP address. In general {{the acceptance of}} services strongly relies on its QoS properties. For potential Tor users, provided the anonymity is secured, {{probably the most important}} QoS parameter is the time until they finally get response by such a hidden service. Internally, <b>overall</b> <b>response</b> <b>times</b> are constituted by several steps invisible for the user. We provide comprehensive measurements of all relevant latencies and a detailed statistical analysis with special focus on the <b>overall</b> <b>response</b> <b>times.</b> Thereby, we gain valuable insights that enable us to give certain statistical assertions and to suggest improvements in the hidden service protocol and its implementation. 1...|$|R
30|$|Response latency for face-present {{trials in}} the {{detection}} task did not correlate with <b>overall</b> <b>response</b> <b>times</b> in the KFMT (r(68)[*]=[*] 0.224, uncorrected p =[*] 0.062) nor with <b>response</b> <b>times</b> on match trials (r(68)[*]=[*] 0.101, uncorrected p =[*] 0.404). However, detection speed correlated with <b>response</b> <b>times</b> on mismatch trials (r(68)[*]=[*] 0.286, uncorrected p =[*] 0.016) and remained significant following the Benjamini-Hochberg correction.|$|R
3000|$|Let’s {{analyze the}} <b>overall</b> <b>response</b> <b>time</b> for a {{centralized}} computing architecture as follows. If we solve the scheduling problem for V echelon supply chain {{based on a}} centralized computing architecture, the Petri net models of all V workflow agents will be merged first. As the number of nodes in the network associated with L(λ [...]) is proportional to (_ nK_ n [...])T and the flow is D_n, the computational complexity to compute L(λ [...]) is bounded by O(V^ 2 K_^ 2 T^ 2 D_n [...]). As the number of Lagrange multipliers is proportional to (∑_n K_n [...])T, the computation time involved in updating λ will increase approximately with O((_n K_n [...])T) and is bounded by O(VKT). Therefore, the <b>overall</b> <b>response</b> <b>time</b> for a centralized architecture will be bounded by O(V^ 2 K_^ 2 T^ 2 D_n [...]).|$|R
30|$|Next, {{correlations}} were tested between the mean correct <b>response</b> <b>times</b> in the detection and matching tasks. Note that the CFMT+ {{does not provide}} such data, hence it {{is not included in}} this analysis. <b>Response</b> <b>times</b> to faces in the detection task did not correlate with <b>overall</b> <b>response</b> <b>times</b> in the KFMT (r(28)[*]=[*] 0.323, uncorrected p =[*] 0.081) or for the match and mismatch subcomponents (r(28)[*]=[*] 0.336, uncorrected p =[*] 0.070 and r(28)[*]=[*] 0.280, uncorrected p =[*] 0.134, respectively).|$|R
40|$|We {{consider}} a new workload allocation policy addressing fairness foruserlevelperformance measures. More specifically the criterion used for optimal workload allocation {{is the one}} which minimizes the maximum expected <b>response</b> <b>time</b> at computer systems to which jobs are routed. The policy to attain this criterion is therefore {{referred to as the}} min-max policy. It is shown that this optimization criterion is tantamount to routingtothefastestM processors, where M depends on system statistics, and equalizing the expected <b>response</b> <b>times</b> on these processors. The algorithm to compute job routing probabilities is applicable to increasing continuous functions of system <b>response</b> <b>time</b> versus the job arrival rate. We next investigate some properties of the minimax policy and show that it results in minimizing the coefficient of variation of <b>response</b> <b>time</b> when the job processing times are exponentially distributed. We compare the min-max policy with the one that minimizes the mean <b>overall</b> <b>response</b> <b>time.</b> It is shown that the new policy attains fairness by equalizing the mean <b>response</b> <b>times</b> at different systems, at a tolerable increase in <b>overall</b> <b>response</b> <b>time.</b> Finally, we report on a sensitivity analysis with respect to changes in job arrival rate and errors in estimating this rate. ...|$|R
40|$|This {{paper is}} {{motivated}} by the increasing need for scalable, distributed management architectures for integrated network, system and application management. Such integration and extension of management functionality impose heavy performance requirements, in direct conflict with the increased volume of management traffic produced. We present an attempt to minimize this traffic and <b>overall</b> <b>response</b> <b>time,</b> by applying the concept of “quasi-copies...|$|R
40|$|This paper {{presents}} {{a comparison of}} two versions of an Emergency Automated Response System (EARS), a fully manual version and a partially automated version. User evaluations involving both versions of the system were conducted using a low workload task and a high workload task. The {{results indicate that the}} automation employed by the partially automated system decreased <b>overall</b> <b>response</b> <b>time</b> and perceived workload for both tasks, but accuracy decreased and <b>response</b> <b>times</b> increased from low workload to high workload with both versions...|$|R
30|$|Next, {{correlation}} {{analyses were}} performed between the mean correct <b>response</b> <b>times</b> in the detection and matching tasks. Once again, the CFMT+ {{is not included in}} this analysis as it does not provide such data. <b>Response</b> <b>times</b> to faces in the detection task correlated positively with <b>overall</b> <b>response</b> <b>times</b> in the KFMT (r(28)[*]=[*] 0.386, uncorrected p =[*] 0.035) and with <b>response</b> <b>times</b> on mismatch trials (r(28)[*]=[*] 0.420, uncorrected p =[*] 0.021). Both of these associations remained significant following the Benjamini-Hochberg adjustment. There was no association between detection speed and <b>response</b> <b>times</b> on match trials (r(28)[*]=[*] 0.286, uncorrected p =[*] 0.125).|$|R
40|$|There is {{a growing}} need for high {{performance}} enterprise distributed systems that provide the scalability and availability required by modern enterprise portals and e-commerce systems. New technologies such as Enterprise Java Beans assist in building these systems by providing the framework to support such increasingly complex applications. The need for different <b>response</b> <b>times</b> for transactions, according to the situation, is often not taken into account. We present a new approach for optimising the <b>overall</b> <b>response</b> <b>time</b> of the system, introducing transaction priorities and adapting the initial transaction distribution and component migration algorithms accordingly...|$|R
40|$|AbstractWe (1) {{introduce}} a primed flanker task as an objective method to measure perceptual grouping, and (2) {{use it to}} directly compare the efficiency of different grouping cues in rapid visuomotor processing. In two experiments, centrally presented primes were succeeded by flanking targets with varying stimulus-onset asynchronies (SOAs). Primes and targets were grouped by the same or by different grouping cues (Exp. 1 : brightness/shape, Exp. 2 : brightness/size) and were consistent or inconsistent {{with respect to the}} required response. Subjective grouping strength was varied to identify its influence on <b>overall</b> <b>response</b> <b>times,</b> error rates, and priming effects, that served as a measure of visual feedforward processing. Our results show that stronger grouping in the targets enhanced <b>overall</b> <b>response</b> <b>times</b> while stronger grouping in the primes enhanced priming effects in motor responses. Also, we obtained differences between rapid visuomotor processing and the subjective impression with cues of brightness and shape but not with cues of brightness and size. Our findings establish the primed flanker task as an objective method to study the speeded visuomotor processing of grouping cues, making it a useful method for the comparative study of feedforward-transmitted base groupings (Roelfsema & Houtkamp, 2011) ...|$|R
40|$|The Memento {{protocol}} {{provides a}} uniform approach to query individual web archives. Soon after its emergence, Memento Aggregator infrastructure was introduced that supports querying across multiple archives simultaneously. An Aggregator generates a response by issuing the respective Memento request against {{each of the}} distributed archives it covers. As the number of archives grows, it becomes increasingly challenging to deliver aggregate responses while keeping <b>response</b> <b>times</b> and computational costs under control. Ad-hoc heuristic approaches have been introduced to address this challenge and {{research has been conducted}} aimed at optimizing query routing based on archive profiles. In this paper, we explore the use of binary, archive-specific classifiers generated {{on the basis of the}} content cached by an Aggregator, to determine whether or not to query an archive for a given URI. Our results turn out to be readily applicable and can help to significantly decrease both the number of requests and the <b>overall</b> <b>response</b> <b>times</b> without compromising on recall. We find, among others, that classifiers can reduce the average number of requests by 77 % compared to a brute force approach on all archives, and the <b>overall</b> <b>response</b> <b>time</b> by 42 % while maintaining a recall of 0. 847. Comment: 10 pages, 6 figures, 11 tables, accepted to be published at JCDL 201...|$|R
5000|$|... "Load-balancing" [...] {{clusters}} are configurations {{in which}} cluster-nodes share computational workload to provide better overall performance. For example, a web server cluster may assign different queries to different nodes, so the <b>overall</b> <b>response</b> <b>time</b> will be optimized. However, approaches to load-balancing may significantly differ among applications, e.g. a high-performance cluster used for scientific computations would balance load with different algorithms from a web-server cluster which may just use a simple round-robin method by assigning each new request {{to a different}} node.|$|R
40|$|Issues {{with the}} {{performance}} of business applications can cause detritions of an organization business performance. A recent research study indicates that organizations could lose significant part of their revenues due to delay beyond defined baselines for performance of their web applications. This paper shows that end users would wait between 4 and 6 seconds for a page to open. This paper works on several issues in IT industry like delay <b>time,</b> <b>response</b> <b>time</b> i. e. <b>overall</b> <b>response</b> <b>time</b> with the data centre processing time. The result {{of this paper is to}} reduced delay & <b>response</b> <b>time</b> towards throughput. IT industry improve application performance such as revenue growth, cost saving and reputation...|$|R
40|$|A {{framework}} {{to enable the}} systematic analysis of Response Phase behaviours is presented and applied to an unannounced evacuation trial in a university library in the Czech Republic. The framework not only provides a consistent method for describing Response Phase behaviour, but also provides a framework for classifying and quantifying the Response Phase other than simply using the <b>overall</b> <b>response</b> <b>time.</b> The framework also provides a means of predicting average <b>response</b> <b>times</b> based {{on a number of}} empirical factors. This work forms the basis of a large study concerned with investigating the impact of culture on evacuation behaviour...|$|R
40|$|An open Jackson-type queuing {{network model}} is {{proposed}} {{to study the}} impact of the servers breakdown on the <b>overall</b> <b>response</b> <b>times</b> to Web requests. The primary aim of the present paper is to modify the performance model of the Proxy Cache Server to a more realistic case when both the Proxy Cache Server and the Web server are unreliable. The main performance and reliability measures are derived, and some numerical calculations are carried out by the help of the MOSEL tool. The nu- merical results are graphically displayed to illustrate the effect of the nonreliabilit...|$|R
5000|$|Hotspot Shield has {{generally}} received positive reviews by industry publications and websites. PC Magazine rated the software [...] "excellent" [...] and praised its status indicator, traffic encryption, connection speed {{at times and}} payment flexibility - but criticized the software's ad platform, website code injection, slowdown of <b>overall</b> <b>response</b> <b>time</b> and browser setting modifications. Best VPN lauded Hotspot Shield's value and speed, but was less enthusiastic about its features, reliability and tech support. Their review said [...] "While Hotspot Shield does its job effectively, it probably won’t appeal to technical users...".|$|R
30|$|Several {{experiments}} are conducted {{to evaluate the}} performance of AppaaS including the <b>overall</b> <b>response</b> <b>time,</b> system overhead and footprint, and system scalability. The first experiment investigates the <b>response</b> <b>time</b> of the various system activities such as the user authentication, location identification, and the OnExit procedure, in which the system applies certain actions on the current application upon leaving its designated space. The second experiment studies the overhead of state preservation and access constraints. The third experiment explores the system scalability. Each experiment is repeated 10 times and {{the average of the}} respective performance parameters is calculated. We remark that confidence intervals were found to be small and hence not reported.|$|R
40|$|Abstract: The primary aim of {{the present}} paper is to modify the {{performance}} model of Bose and Cheng [1] to a more realistic case when external visits are also allowed to the remote Web servers and the Web servers have limited buffer. We analyze how many parameters affect {{the performance of a}} Proxy Cache Server. Numerical results are obtained for the <b>overall</b> <b>response</b> <b>time</b> with and without a PCS. These results show that the benefit of a PCS depends on various factors. Several numerical examples illustrate the effect of visit rates, visit rates for the external users, and the cache hit rate probability on the mean <b>response</b> <b>times...</b>|$|R
40|$|Abstract. We {{propose a}} tunable {{scheduling}} strategy that lies between FIFO and shortest- rst, {{based on the}} value of a coe cient Alpha. If Alpha is set to zero then this strategy is just FIFO. Larger Alpha gets us closer to shortest- rst strategy which isknown to provide optimal <b>response</b> <b>time.</b> However, unlike the shortest- rst, proposed scheduling strategy is starvation free. This scheduling strategy, called Alpha scheduling with no preemption, allows to improve <b>overall</b> <b>response</b> <b>time</b> per HTTP request more than 3 times under heavy load. We demonstrate our results with a simple simulation model using SpecWeb 96 to generate representative WWW workload. ...|$|R
40|$|Part 3 : StorageInternational audienceData backup and {{archiving}} is {{an important}} aspect of business processes to avoid loss due to system failures and natural calamities. As the amount of data and applications grow in number, concerns regarding cost efficient data preservation force organizations to scout for inexpensive storage options. Addressing these concerns, we present Tape Cloud, a novel, highly cost effective, unified storage solution. We leverage the notably economic nature of Magnetic Tapes and design a cloud storage infrastructure-as-a-service that provides a centralized storage platform for unstructured data generated by many diverse applications. We propose and evaluate a proficient middleware that manages data and IO requests, overcomes latencies and improves the <b>overall</b> <b>response</b> <b>time</b> of the storage system. We analyze traces obtained by live archiving applications to obtain workload characteristics. Based on this analysis, we synthesize archiving workloads and design suitable algorithms to evaluate the performance of the middleware and storage tiers. From the results, we see that the use of the middleware provides close to 100 % improvement in task distribution efficiency within the system leading to a 70 % reduction in <b>overall</b> <b>response</b> <b>time</b> of data retrieval from storage. Due to its easy adaptability with {{the state of the art}} storage practices, the middleware contributes in providing the much needed boost in reducing storage costs for data archiving in cloud and colocated infrastructures...|$|R
40|$|Abstract. The {{growing number}} of {{commercial}} Internet-based data mining service providers {{is indicative of the}} emerging trend of data mining application services. It validates the recognition that knowledge is a key resource in strategic organisational decision-making. The trend also establishes that the Application Service Provider (ASP) paradigm is seen as a cost-effective approach to meet the business intelligence needs of small to medium range organisations that are the most constrained by the high cost of niche software technologies. An important issue in a service context is the optimization of performance in terms of throughput and <b>response</b> <b>time.</b> The paper presents a hybrid distributed data mining (DDM) model that improves the <b>overall</b> <b>response</b> <b>time...</b>|$|R
40|$|A robust, high {{temperature}} mixed metal oxide catalyst for propellant composition, including high concentration hydrogen peroxide, and catalytic combustion, including methane air mixtures. The uses include target, space, and on-orbit propulsion systems and low-emission terrestrial power and gas generation. The catalyst system requires no special preheat apparatus or special sequencing to meet start-up requirements, enabling a fast <b>overall</b> <b>response</b> <b>time.</b> Start-up transients {{of less than}} 1 second have been demonstrated with catalyst bed and propellant temperatures as low as 50 degrees Fahrenheit. The catalyst system has consistently demonstrated high decomposition effeciency, extremely low decomposition roughness, and long operating life on multiple test particles...|$|R
40|$|Abstract — Due to {{applications}} and {{systems such as}} sensor networks, data streams, and peer-to-peer (P 2 P) networks, data generation and storage become increasingly distributed. Therefore a challenging problem is to support best-match query processing in highly distributed environments. In this paper, we present a novel framework for top-k query processing in largescale P 2 P networks, where the dataset is horizontally distributed to peers. Our proposed framework returns the exact results to the user, while minimizing the number of queried super-peers and transferred data. Through simulations we demonstrate the feasibility of our approach in terms of <b>overall</b> <b>response</b> <b>time.</b> I...|$|R
40|$|The {{primary goal}} of the {{database}} system is to provide the user a convenient and efficient access to the query related data. With this concern this paper provides semantic based cache mechanism techniques for optimizing the user queries. Here the frame work for optimization is analyzed which supports data and computation reuse, query scheduling and cache efficient utilization algorithm is presented {{in order to improve}} the evaluation process and minimize the <b>overall</b> <b>response</b> <b>time.</b> Further the case study is analyzed to test the performance and extended the same for multi-queries to achieve parallelism. Key words semantic, cache efficiency, optimization. 1...|$|R
30|$|Following a {{sequential}} dipping in Hg(II) solutions differing {{in their}} concentrations {{by a factor}} of 10, the measurement results were evaluated. <b>Response</b> <b>time</b> inherent to SPEs are only measurable if the <b>overall</b> <b>response</b> <b>time</b> of the potentiometric system is governed by the properties of the paste of electrode, i.e. if the time constant of the response function of the electrode is much larger than the time constant of the electrochemical cell and the electronic EMF–measuring device. As a matter of fact, the entire <b>response</b> <b>time</b> is determined by many factors, such as the measuring instrument’s time constant, the ion transfer reaction rate through the paste–pattern interface, the impedance of the equivalent electric circuit of the paste or the stabilization of a liquid–junction potential at the reference electrode (Umezawa et al. 1995).|$|R
40|$|A {{method for}} {{designing}} and assembling a high performance catalyst bed gas generator {{for use in}} decomposing propellants, particularly hydrogen peroxide propellants, for use in target, space, and on-orbit propulsion systems and low-emission terrestrial power and gas generation. The gas generator utilizes a sectioned catalyst bed system, and incorporates a robust, high temperature mixed metal oxide catalyst. The gas generator requires no special preheat apparatus or special sequencing to meet start-up requirements, enabling a fast <b>overall</b> <b>response</b> <b>time.</b> The high performance catalyst bed gas generator system has consistently demonstrated high decomposition efficiency, extremely low decomposition roughness, and long operating life on multiple test articles...|$|R
40|$|Workload {{management}} coordinates {{access to}} and use of shared computational resources; adaptive workload execution revises resource allocation decisions dynamically in response to feedback about {{the progress of the}} workload or the behavior of the resources. Where the workload contains or consists of database queries, adaptive query processing (AQP) changes the way in which a query is being evaluated while the query is running. In parallel environments, available adaptations may change the allocation of query fragments to a machine, for example to remove load imbalance or change the parallelism level. Most AQP strategies act on individual queries with the objective of reducing <b>response</b> <b>times.</b> However, where adaptations affect the usage of shared resources, or the principal goal is to meet quality of service targets rather than to minimize <b>overall</b> <b>response</b> <b>times,</b> locally beneficial decisions may have globally detrimental effects. This paper describes the use of utility functions to coordinate adaptations that assign resources to query fragments from multiple queries, and demonstrates how a common framework can be used to support different objectives, specifically to minimize <b>overall</b> query <b>response</b> <b>times</b> and to maximize the number of queries meeting quality of service goals. Experiments using simulation compare the use of utility functions with the more common heuristic control strategies, demonstrating situations in which significant benefits can be obtained...|$|R
30|$|In {{terms of}} accuracy, face {{detection}} did not correlate with face matching and face memory. On the other hand, response speed in the detection task correlated with <b>overall</b> <b>response</b> <b>times</b> in the KFMT, and with mismatch, but not match trials. However, these {{results should be}} interpreted cautiously as evidence that these tasks engage similar mechanisms, and they may instead simply reflect observers’ capacity for responding quickly. More importantly, the primary aim of this experiment was to ascertain whether there is an association between combinations of response speed in the face detection task and accuracy in the matching and memory task. No such associations were found, providing further evidence that face detection is dissociated from face matching and face memory.|$|R
40|$|We present HSAN - {{a hybrid}} storage area network, which uses both in-band (like NFS [13]) and {{out-of-band}} virtualization (like SAN FS [10]) access models. HSAN uses hybrid servers {{that can serve}} as both metadata and NAS servers to intelligently decide the access model per each request, based {{on the characteristics of}} requested data. This is in contrast to existing efforts that merely provide concurrent support for both models and do not exploit model appropriateness for requested data. The HSAN hybrid model is implemented using low overhead cacheadmission and cache-replacement schemes and aims to improve <b>overall</b> <b>response</b> <b>times</b> {{for a wide variety of}} workloads. Preliminary analysis of the hybrid model indicates performance improvements over both models...|$|R
40|$|We {{introduce}} the EP queue [...] a significant generalization of the MB/G/ 1 queue that has state-dependent service time probability distributions and incorporates power-up for first arrivals and power-down for idle periods. We derive exact {{results for the}} busy-time and response-time distributions. From these, we derive power consumption metrics during nonidle periods and <b>overall</b> <b>response</b> <b>time</b> metrics, which together provide a single measure of the trade-off between energy and performance. We illustrate these trade-offs for some policies and show how numerical results can provide insights into system behavior. The EP queue has application to storage systems, especially hard disks, and other data-center components such as compute servers, networking, and even hyperconverged infrastructure...|$|R
40|$|Many {{embedded}} multimedia systems employ special hardware {{blocks to}} co-process {{with the main}} processor. Even though an efficient handling of such hardware blocks is critical on the overall performance of real-time multimedia systems, traditional real-time scheduling techniques cannot afford to guarantee a high quality of multimedia playbacks with neither delay nor jerking. This paper presents a hardware-aware rate monotonic scheduling (HA-RMS) algorithm to manage hardware tasks efficiently and handle special hardware blocks in the embedded multimedia system. The HA-RMS prioritizes the hardware tasks over software tasks not only to increase the hardware utilization of the system but also to reduce the output jitter of multimedia applications, which results in reducing the <b>overall</b> <b>response</b> <b>time...</b>|$|R
