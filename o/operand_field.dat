6|16|Public
5000|$|The <b>operand</b> <b>field</b> {{could begin}} in any column {{to the right}} of the {{operation}} code, separated from the operation code by at least one blank. Blanks were invalid in operands except in character constants. The <b>operand</b> <b>field,</b> consisting of one or more operands, was optional depending on the operation code.|$|E
5000|$|Optional {{comments}} could {{appear to}} the right of the <b>operand</b> <b>field,</b> separated by at least one blank.|$|E
5000|$|Parameter-Field: The {{parameter}} field, {{also sometimes}} referred to as the <b>operand</b> <b>field,</b> contains parameters separated by commas. Parameter field should be coded as follows: ...|$|E
3000|$|The second {{option is}} {{referred}} to as [...] "Addresses in op-fields". It makes use of special instructions that each contains a complete vector of 8 addresses (with a maximum of 14 -bit per address) in their <b>operand</b> <b>fields.</b> Being contained by the instruction, no additional memory access is required to obtain the LUT vector data. In the current iVAG architecture implementations this data is directly output as an address vector and no computations can be performed on it.|$|R
5000|$|The Forth {{virtual machine}} and other [...] "0-operand" [...] {{instruction}} sets lack any <b>operand</b> specifier <b>fields,</b> such as some stack machines including NOSC.|$|R
25|$|Number of {{instructions:}} 39 types from a 4-bit op code {{by using}} five {{bits of the}} <b>operand</b> address <b>field</b> for instructions which do not access memory.|$|R
50|$|A stack machine's compact code {{naturally}} fits more {{instructions in}} cache, and therefore could achieve better cache efficiency, reducing memory costs or permitting faster memory systems {{for a given}} cost. In addition, most stack-machine instruction is very simple, made from only one opcode field or one <b>operand</b> <b>field.</b> Thus, stack-machines require very little electronic resources to decode each instruction.|$|E
50|$|Both the 1900 {{series and}} System/360 {{provided}} hardware support for multi-programming. On the 1900 all user memory addresses were modified by a datum (base address) register and checked against a limit register, preventing one program interfering with another. The System/360 gave each process and every 2048 byte block of memory a four-bit key, {{and if a}} process key did not match the memory block key an exception would result. The 1900 system required programs to occupy a contiguous area of memory but allowed processes to be relocated during execution, simplifying {{the work of the}} operating system. The 1900 also allowed any process direct access to the first 4096 words of its address space. (Both the 1900 and 360 had a 12-bit <b>operand</b> <b>field,</b> but on the 360 addresses were physical addresses so a program could directly access the first 4096 bytes of physical memory).|$|E
40|$|Recently, a compiler-assisted {{approach}} to multiple instruction retry was developed. In this scheme, a read buffer of size 2 N, where N represents the maximum instruction rollback distance, {{is used to}} resolve one type of data hazard. This hardware support helps to reduce code growth, compilation time, {{and some of the}} performance impacts associated with hazard resolution. The 2 N read buffer size requirement of the compiler-assisted approach is worst case, assuring data redundancy for all data required but also providing some unnecessary redundancy. By adding extra bits in the <b>operand</b> <b>field</b> for source 1 and source 2 it becomes possible to design the read buffer to save only those values required, thus reducing the read buffer size requirement. This study measures the effect on performance of a DECstation 3100 running 10 application programs using 6 read buffer configurations at varying read buffer sizes...|$|E
50|$|Since {{there are}} no <b>operand</b> <b>fields</b> to decode, stack {{machines}} fetch each instruction and its operands at same time. Stack machines can omit the operand fetching stage of a register machine. In addition, except explicit load from memory instructions, the order of operand usage is identical with {{the order of the}} operands in the data stack. So excellent prefetching is accomplished easily by keeping operands on the top of stack in fast storage. For example, in the Java Optimized Processor (JOP) microprocessor the top 2 operands of stack directly enter a data forwarding circuit that is faster than the register file.|$|R
50|$|LVDC {{instruction}} {{words were}} split into a 4-bit opcode field (least-significant bits) and a 9-bit <b>operand</b> address <b>field</b> (most-significant bits). This left it with sixteen possible opcode values {{when there were}} eighteen different instructions: consequently, three of the instructions used the same opcode value, and used two bits of the address value to determine which instruction was executed.|$|R
40|$|Centralized {{register}} file architectures scale {{poorly in}} terms of clock rate, chip area, and power consumption and are thus not suitable for consumer electronic devices. The consequence {{is the emergence of}} architectures having many interconnected clusters each with a separate register file and a few functional units. Among the many inter-cluster communication models proposed, the extended operand model extends some of <b>operand</b> <b>fields</b> of instruction with a cluster specifier and allows an instruction to read some of the operands from other clusters without any extra cost. Scheduling for clustered processors involves spatial concerns (where to schedule) as well as temporal concerns (when to schedule). A scheduler is responsible for resolving the conflicting requirements of aggressively exploiting the parallelism offered by hardware and limiting the communication among clusters to available slots. This paper proposes an integrated spatial and temporal scheduling algorithm for extended operand clustered VLIW processors and evaluates its effectiveness in improving the run time performance of the code without code size penalty. 1...|$|R
40|$|Physical {{register}} {{access time}} increases the delay between scheduling and execution in modern out-of-order processors. As {{the number of}} physical registers increases, this delay grows, forcing designers to employ register files with multicycle access. This paper advocates more efficient utilization of a fewer number of physical registers {{in order to reduce}} the access time of the physical register file. Register values with few significant bits are stored in the rename map using physical register inlining, a scheme analogous to inlining of <b>operand</b> <b>fields</b> in data structures. Specifically, whenever a register value can be expressed with fewer bits than the register map would need to specify a physical register number, the value is stored directly in the map, avoiding the indirection, and saving space in the physical register file. Not surprisingly, we find that a significant portion of all register operands can be stored in the map in this fashion, and describe straightforward microarchitectural extensions that correctly implement physical register inlining. We find that physical register inlining performs well, particularly in processors that are register-constrained. 1...|$|R
40|$|Abstract. Information hiding {{has been}} studied in many {{security}} applications such as authentication, copyright management and digital forensics. In this work, we introduce a new application where successful information hiding in compiled program binaries could bring system-wide performance improvements. Our goal is to enhance computer system performance by providing additional information to the processor, without changing the instruction set architecture. We first analyze the statistics of typical programs to demonstrate the feasibility of hiding data in them. We then propose several techniques to hide {{a large amount of}} data in the <b>operand</b> <b>fields</b> with very low computation and storage requirements during the extraction process. The data embedding is made reversible to recover the original instructions and to ensure the correct execution of the computer program. Our experiments on the SPEC CPU 2000 benchmark programs show that up to 110 K bits of information can be embedded in large programs with as little as 3 K bits of additional run-time memory {{in the form of a}} simple look-up table. ...|$|R
40|$|AbstractEmbedded systems {{often have}} severe memory {{constraints}} requiring careful encoding of programs. For example, smart cards {{have on the}} order of 1 K of RAM, 16 K of non-volatile memory, and 24 K of ROM. A virtual machine can be an effective approach to obtain compact programs but instructions are commonly encoded using one byte for the opcode and multiple bytes for the operands, which can be wasteful and thus limit the size of programs runnable on embedded systems. Our approach uses canonical Huffman codes to generate compact opcodes with custom-sized <b>operand</b> <b>fields</b> and with a virtual machine that directly executes this compact code. We present techniques to automatically generate the new instruction formats and the decoder. In effect, this automatically creates both an instruction set for a customized virtual machine and an implementation of that machine. We demonstrate that, without prior decompression, fast decoding of these virtual compressed instructions is feasible. Through experiments on Scheme and Java, we demonstrate the speed of these decoders. Java benchmarks show an average execution slowdown of 9 %. The reductions in size highly depend on the original bytecode and the training samples, but typically vary from 40 % to 60 %...|$|R
40|$|Embedded systems {{often have}} strong memory {{constraints}} requiring careful encoding of programs. For example, smart cards {{have on the}} order of 1 K of RAM, 16 K of non-volatile memory, and 24 K of ROM. A virtual machine can be an effective approach to obtain compact programs but instructions are commonly encoded using one byte for the opcode and multiple bytes for the operands, which can be wasteful. We use another approach, using canonical Huffman codes to generate compact custom-sized opcodes and custom-sized <b>operand</b> <b>fields</b> along with a virtual machine that directly executes the encoded operations. We present techniques that automatically generate the opcodes and the decoder. In effect, this automatically creates both an instruction set for a customized virtual machine and an implementation of that machine. We demonstrate that, without prior decompression, fast decoding of these virtual compressed instructions is feasible. We also discuss the relevant difficulties in generating C code for such decoders, in particular the problem of efficient program memory access. Through experiments we demonstrate the speed of these decoders. Synthetic and Java benchmarks show an execution slowdown ranging from 10 % to 30 %, with an average of 9 % for good decoders. For the Java bytecode, the average overall compression factor is 60 %...|$|R
50|$|The Cyclone, was {{a vacuum}} tube computer, built by Iowa State College (later University) at Ames, Iowa. The machine was placed into {{operation}} in July 1959. It {{was based on}} the IAS architecture developed by John von Neumann. The prototype of this machine is ILLIAC, the University of Illinois Digital Computer. The Cyclone used 40-bit words, used two 20-bit instructions per word, and each instruction had an eight-bit op-code and a 12-bit <b>operand</b> or address <b>field.</b> In general IAS-based computers were not code compatible with each other, although originally math routines which ran on the ILLIAC would also run on the Cyclone.|$|R
50|$|The two-operand form {{uses the}} unused 5 {{combinations}} (27-31) in the 5-bit <b>field.</b> <b>Operand</b> a is not used, and the 2-bit field for its low bits is reassigned; one bit {{is used for}} an additional opcode bit, {{and the other is}} used as an additional combination register specifier, doubling the number of available combinations to 10, and allowing all 9 combinations of 3×b+c to be represented. This is done {{in a manner similar to}} bi-quinary coded decimal: the combination, modulo 5, is stored in the 5-bit field (as (3×b+c) mod 5 + 27), and the 1-bit quotient (⌊(3×b+c)/5⌋) is stored in instruction bit 5 (marked with an asterisk in the table above).|$|R
40|$|Abstract. After {{years of}} {{discussion}} within the RFID security commu-nity {{whether or not}} asymmetric cryptography would be feasible for RFID tags, we present a major breakthrough towards RFID products incorpo-rating asymmetric authentication. For the challenge-response protocol applied, the response is calculated by performing an elliptic curve point multiplication using a random challenge and the tag’s secret key. The design is resistant against side-channel attacks such as timing, simple power, differential power, and fault attacks. Some side-channel coun-termeasures depend on random numbers which are provided by a syn-thesizable true random number generator. The ISO/IEC 15693 / 18000 - 3 Mode 1 compliant RFID tag we present {{is based on a}} concept developed by Siemens Corporate Technology and published earlier this year. It in-corporates Infineon’s energy efficient 163 bit ECC engine with a single clock cycle addition and 41 clock cycles 4 bit parallel multiplication for the <b>field</b> <b>operands.</b> The energy spent during the ECC calculation is less than our target of 10 µJ and the overall size of the tag’s silicon is less than 0. 8 mm 2 in a 220 nm RFID technology. For early verification and host software development we designed an FPGA based emulation and test setup connected to an RF frontend. First samples of our ECC RFID chip are being manufactured and will be available in summer...|$|R
40|$|The use of {{registers}} {{instead of}} memory operands {{is an effective}} performance enhancement {{as well as a}} power saving mechanism, but compilers still cannot allocate many of the memory references of a program into registers, for two fundamental reasons unrelated {{to the size of the}} register file: (1) dynamically varying load/store operand addresses, and (2) inherent limitations of compile time alias analysis. Previously proposed architectural features such as CRegs [2] or IA- 64 ALAT [1] mechanisms have only addressed the aliasing portion of the two fundamental impediments. In this paper, we have designed an ideal “limit register allocation machine, ” as a theoretical tool to gain insight into the essence of the register allocation problem, and to address both of the fundamental impediments. The limit machine uses only register operations in its ISA (performs memory operations only for automatic register spills and fills) and has a unique capability to modify the register fields of instructions at run time. Based on the experience with this limit machine, we have also proposed: (1) A software emulation method for the limit machine, which, after performing optimizations such as partial redundancy elimination, can result in efficient register allocation of memory references for loads/stores with dynamically varying addresses, which could not be register-allocated with any of the previous techniques. (2) A hardware implementation of concepts from the limit machine, by adding a dynamically changing <b>operand</b> location prediction <b>field</b> (“Predicted Register Number”) to each load/store instruction, which can improve the performance of data L 1 caches by speculatively accessing the predicted operand location in the D-L 1 cache array directly (as if it were a “register file”), as soon as the instruction is available, without first going through the base register read, address arithmetic, address translation, and associative search phases. ...|$|R

