1|33|Public
40|$|Pixel Array Detectors (PADs) {{consist of}} an x-ray sensor layer bonded pixel-by-pixel to an {{underlying}} readout chip. This approach allows both the sensor and the custom pixel electronics to be tailored independently to best match the x-ray imaging requirements. Here we present characterizations of CdTe sensors hybridized with two different charge-integrating readout chips, the Keck PAD and the Mixed-Mode PAD (MM-PAD), both developed previously in our laboratory. The charge-integrating architecture {{of each of}} these PADs extends the instantaneous counting rate by many orders of magnitude beyond that obtainable with photon counting architectures. The Keck PAD chip consists of rapid, 8 -frame, in-pixel storage elements with framing periods < 150 ns. The second detector, the MM-PAD, has an extended dynamic range by utilizing an in-pixel <b>overflow</b> <b>counter</b> coupled with charge removal circuitry activated at each overflow. This allows the recording of signals from the single-photon level to tens of millions of x-rays/pixel/frame while framing at 1 kHz. Both detector chips consist of a 128 × 128 pixel array with (150 μm) ^ 2 pixels. Comment: Revised version after 2 nd peer-revie...|$|E
30|$|The performance-monitoring {{hardware}} generates an NMI {{according to}} <b>counter</b> <b>overflow.</b>|$|R
50|$|Medipix-2 MXR is an {{improved}} version of Medipix-2 device with better temperature stability, pixel <b>counter</b> <b>overflow</b> protection, increased radiation hardness {{and many other}} improvements.|$|R
50|$|The {{operation}} of the PLL is further disrupted by <b>overflow</b> in the <b>counters.</b> This effect is only relevant in multibit PLLs; for Unibit PLL, there is only the one-bit signal MSB, therefore no overflow is possible.|$|R
3000|$|... ′<k chosen {{counters}} for an item equal χ is appropriately small. In essence, {{choosing an}} appropriate value for χ is a trade-off between storage saved, {{the number of}} <b>counter</b> <b>overflows,</b> {{and the number of}} expected lookup failures.|$|R
25|$|Interrupts {{could be}} {{triggered}} when the <b>counters</b> <b>overflowed.</b> The T3rupt and Dsrupt interrupts were produced when their counters, {{driven by a}} 100Hz hardware clock, overflowed after executing many Pinc subsequences. The Uprupt interrupt was triggered after its counter, executing the Shinc subsequence, had shifted 16bits of uplink data into the AGC.|$|R
50|$|A {{representative}} {{example is}} a do loop incrementing some <b>counter</b> until it <b>overflows</b> and becomes 0 again.Although the do loop executes the same increment command iteratively, so the program graph executes a cycle, in its state space is not a cycle, but a line.This results from the state being the program location (here cycling) combined with the counter value, which is strictly increasing (until the overflow), so different states are visited in sequence, until the overflow.After the <b>overflow</b> the <b>counter</b> becomes 0 again, so the initial state is revisited in the state space, closing a cycle in the state space (assuming the counter was initialized to 0).|$|R
50|$|The latest {{time that}} can be {{represented}} in Unix's signed 32-bit integer time format is 03:14:07 UTC on Tuesday, 19 January 2038 (2-1 = 2,147,483,647 seconds after 1 January 1970). Times beyond that will wrap around and be stored internally as a negative number, which these systems will interpret as having occurred on 13 December 1901 rather than 19 January 2038. This is caused by integer <b>overflow.</b> The <b>counter</b> runs out of usable digit bits, flips the sign bit instead, and reports a maximally negative number (continuing to count up, toward zero). Resulting erroneous calculations on such systems are likely to cause problems for users and other relying parties.|$|R
30|$|We first {{evaluate}} {{the performance of}} the EHT with respect to lookups. To achieve deterministic lookup performance, it is crucial that counter value distribution and bucket loads behave as expected. Counter distribution affects the maximum allowed counter value, which in turn affects the effectiveness of summary compression and the number of entries that have to be moved to CAM due to <b>counter</b> <b>overflows.</b>|$|R
5000|$|If time is unlimited, the {{machines}} can run in expected exponential time [...] - [...] for example, keep a counter and increment it with probability [...] and zero it otherwise; halt when the <b>counter</b> <b>overflows.</b> If zero error (alternatively, one-sided error) is allowed, the class equals NL [...] - [...] the machine can simulate NL by trying random paths for an exponential {{amount of time}} and using NL=coNL.|$|R
30|$|The Xenoprof toolkit {{consists}} of a VMM-level layer responsible for servicing <b>counter</b> <b>overflow</b> interrupts from the performance monitoring hardware and a domain-level layer derived from OProfile responsible for attributing samples to specific routines within the domain [15]. The OProfile layer drives the performance profiling through hypercalls supported by Xenoprof and Xenoprof delivers samples to the OProfile layer using Xen's virtual interrupt mechanism. System-wide profiling is generated through the coordination of multiple domain-level profilers. While our current implementation is dependent on OProfile, other statistical profilers (e.g., VTune) also can be ported for using the Xenoprof interface.|$|R
50|$|The {{accidents}} {{occurred when}} the high-power electron beam was activated instead of the intended low power beam, and without the beam spreader plate rotated into place. Previous models had hardware interlocks in place to prevent this, but Therac-25 had removed them, depending instead on software interlocks for safety. The software interlock could fail due to a race condition. The defect was as follows: a one-byte counter in a testing routine frequently overflowed; if an operator provided manual input to the machine at the precise moment that this <b>counter</b> <b>overflowed,</b> the interlock would fail.|$|R
2500|$|The period saw the {{emergence}} of a gaming media, publications dedicated to video games, in the form of video game journalism and strategy guides. The enormous popularity of video arcade games led to the very first video game strategy guides; these guides (rare to find today) discussed in detail the patterns and strategies of each game, including variations, to a degree that few guides seen since can match. [...] "Turning the machine over"—making the score <b>counter</b> <b>overflow</b> and reset to zero—was often the final challenge of a game for those who mastered it, and the last obstacle to getting the highest score.|$|R
40|$|Linux kernel {{vulnerabilities}} {{are often}} long lived {{and in some}} cases challenging to patch after discovery. The current focus in upstream Linux security has therefore been on categorical protections against whole error classes, not only reactive patching of specific vulnerabilities. Our work contributes to these efforts by tackling memory errors in the Linux kernel from two different fronts. First, we contributed to the upstream Linux kernel by working on a mechanism to prevent use-after-free errors caused by reference <b>counter</b> <b>overflows.</b> Second, we explored the applicability of Intel MPX as a general mechanism to prevent spatial memory errors in the Linux kernel...|$|R
5000|$|The period saw the {{emergence}} of a gaming media, publications dedicated to video games, in the form of video game journalism and strategy guides. The enormous popularity of video arcade games led to the very first video game strategy guides; these guides (rare to find today) discussed in detail the patterns and strategies of each game, including variations, to a degree that few guides seen since can match. [...] "Turning the machine over"—making the score <b>counter</b> <b>overflow</b> and reset to zero—was often the final challenge of a game for those who mastered it, and the last obstacle to getting the highest score.|$|R
5000|$|On December 1, 2014, the YouTube video {{garnered}} over 231 − 1 views, <b>overflowing</b> the YouTube <b>counter</b> to {{a negative}} number, {{resulting in a}} public comment from Google/YouTube saying [...] "We never thought a video would be watched in numbers greater than a 32-bit integer (=2,147,483,647 views), {{but that was before}} we met PSY. [...] "Gangnam Style" [...] has been viewed so many times we had to upgrade to a 64-bit integer (9,223,372,036,854,775,808)!" [...] Hovering over the counter of the YouTube video triggered an easter egg. A YouTube representative later revealed that the comment was a joke and that the company had already updated to a 64-bit integer months ago.|$|R
40|$|Flow {{statistics}} {{is a basic}} task {{of passive}} measurement and has been widely used to characterize {{the state of the}} network. Adaptive Non-Linear Sampling (ANLS) {{is one of the most}} accurate and memory-efficient flow statistics method proposed recently. This paper studies the parameter setting problem for ANLS. A parameter self-tuning algorithm is proposed in this paper, which enlarges the parameter to a equilibrium tuning point and renormalizes the <b>counter</b> when <b>counter</b> <b>overflows.</b> It is demonstrated that the estimation error of ANLS with parameter self-tuning algorithm is improved by about 89 times for real trace, 70 times for Pareto traffic scenario and 370 times for exponential traffic, while giving the same memory size...|$|R
30|$|The FGM- 3 and FGM- 3 h sensors both produce TTL pulse streams with {{frequencies}} {{ranging from}} the 10 s to 100 s of kHz. We {{have found that the}} 8 -bit “PIC” series of microcontrollers from Microchip, Inc. are ideal high-speed pulse counters, because, unlike many similar microcontrollers, PIC microcontrollers allow external clocking of some of their internal 16 -bit registers, which can thus act as counters. The NPM uses twin PIC 18 F 252 microcontrollers (Microchip Technology Inc., 2006), each having two externally clocked 16 -bit counters. Because the output frequency of the magnetic sensors is sufficiently high to <b>overflow</b> the <b>counters</b> over the course of one second, frequency is measured by reading the stored counts added to the counter range multiplied by the number of timer overflows that occurred over the course of 1 second. The PIC counters return the resulting frequency measurement over a 9600 -baud TTL-level serial data stream, which is easily read by the control microcontroller. As each PIC is able to count two pulse streams, one of the PICs returns the frequencies of sensors X and Y, while the second PIC returns the frequency of sensor Z, leaving a spare channel.|$|R
40|$|Abstract—Flow {{statistics}} {{is a basic}} task {{of passive}} measurement and has been widely used to characterize {{the state of the}} net-work. Adaptive Non-Linear Sampling (ANLS) {{is one of the most}} accurate and memory-efficient flow statistics method proposed recently. This paper studies the parameter setting problem for ANLS. A parameter self-tuning algorithm is proposed in this paper, which enlarges the parameter to a equilibrium tuning point and renormalizes the <b>counter</b> when <b>counter</b> <b>overflows.</b> It is demonstrated that the estimation error of ANLS with parameter self-tuning algorithm is improved by about 89 times for real trace, 70 times for Pareto traffic scenario and 370 times for exponential traffic, while giving the same memory size. Index Terms—Network measurement, flow statistics, counting, unbiased estimation I...|$|R
40|$|The {{security}} {{of billions of}} devices worldwide depends on the security and robustness of the mainline Linux kernel. However, {{the increasing number of}} kernel-specific vulnerabilities, especially memory safety vulnerabilities, shows that the kernel is a popular and practically exploitable target. Two major causes of memory safety vulnerabilities are reference <b>counter</b> <b>overflows</b> (temporal memory errors) and lack of pointer bounds checking (spatial memory errors). To succeed in practice, security mechanisms for critical systems like the Linux kernel must also consider performance and deployability as critical design objectives. We present and systematically analyze two such mechanisms for improving memory safety in the Linux kernel: (a) an overflow-resistant reference counter data structure designed to accommodate typical reference counter usage in kernel source code, and (b) runtime pointer bounds checking using Intel MPX in the kernel...|$|R
40|$|A {{telemetry}} {{period of}} length measuring process achieves an increased measuring resolution by first counting timing pulses {{over a period}} length of the measurement signal to detect an approximate period length by then reading out a table containing the period length counter status {{in order to obtain}} a measuring period quantity assigned to a period length range, whereby the quantity of measuring periods is selected so that counting the timing pulses by means of a counter does not cause a <b>counter</b> <b>overflow</b> even in the event of the longest period length of said period length range, this being followed by counting the timing pulses over the period length quantity of measuring signal periods, so that finally, the counter status thus obtained can be transmitted together with a coding of the measuring period quantity to a telemetry receiver...|$|R
40|$|Protection from {{hardware}} attacks such as snoopers and mod chips {{has been}} receiving increasing attention in computer architecture. This paper presents a new combined memory encryption/authentication scheme. Our new split counters for counter-mode encryption simultaneously eliminate <b>counter</b> <b>overflow</b> problems and reduce per-block counter size, {{and we also}} dramatically improve authentication performance and security by using the Galois/Counter Mode of operation (GCM), which leverages counter-mode encryption to reduce authentication latency and overlap it with memory accesses. Our {{results indicate that the}} split-counter scheme has a negligible overhead even with a small (32 KB) counter cache and using only eight counter bits per data block. The combined encryption/authentication scheme has an IPC overhead of 5 % on average across SPEC CPU 2000 benchmarks, which is a significant improvement over the 20 % overhead of existing encryption/authentication schemes. 1...|$|R
40|$|Bloom {{filters are}} {{efficient}} randomized data structures for membership queries {{on a set}} with a certain known false positive probability. Counting bloom filters (CBFs) allow the same operation on dynamic sets that can be updated via insertions and deletions with larger memory requirements. This paper first presents a new upper bound for <b>counters</b> <b>overflow</b> probability in CBFs. This bound is much tighter than that usually adopted in literature and it allows for designing more efficient CBFs. Three novel data structures are proposed, which introduce {{the idea of a}} hierarchical structure as well as the use of Huffman code. Our algorithms improve standard CBFs in terms of fast access and limited memory consumption (up to 50 % of memory saving) : the target could be the implementation of the compressed data structures in the small (but fast) local memory or "on-chip SRAM" of devices such as network processors...|$|R
40|$|In this paper, {{we present}} and {{evaluate}} two techniques that use {{different styles of}} hardware support to provide data structure specific processor cache information. In one approach, hardware performance <b>counter</b> <b>overflow</b> interrupts are used to sample cache misses. In the other, cache misses within regions of memory are counted to perform an n-way search for {{the areas in which}} the most misses are occurring. We present a simulation-based study and comparison of the two techniques. We find that both techniques can provide accurate information, and describe the relative advantages and disadvantages of each. 1 Introduction As processor speeds have rapidly increased, the gap between these speeds and the access time of main memory has widened. Because of this, it is becoming ever more important for applications to make effective use of memory caches. Information about an application 's interaction with the cache is therefore crucial to tuning its performance. To be most useful to a programmer [...] ...|$|R
5000|$|Several {{versions}} of Turbo Pascal, including {{the latest version}} 7, include a CRT unit used by many fullscreen text mode applications. This unit contains code in its initialization section to determine the CPU speed and calibrate delay loops. This code fails on processors with a speed greater than about 200 MHz and aborts immediately with a [...] "Runtime error 200" [...] message. (the error code 200 {{had nothing to do}} with the CPU speed 200 MHz). This is caused because a loop runs to count the number of times it can iterate in a fixed time, as measured by the real-time clock. When Turbo Pascal was developed it ran on machines with CPUs running at 1 to 8 MHz, and little thought was given to the possibility of vastly higher speeds, so from about 200 MHz enough iterations can be run to <b>overflow</b> the 16-bit <b>counter.</b> A patch was produced when machines became too fast for the original method, but failed as processor speeds increased yet further, and was superseded by others.|$|R
40|$|Abstract. Performance {{monitoring}} hardware {{is available}} on most modern microprocessors {{in the form of}} hardware counters and other registers that record data about processor events. This hardware may be used in counting mode, in which aggregate events counts are accumulated, and/or in sampling mode, in which time-based or event-based sampling is used to collect profiling data. This paper discusses uses of these two modes and considers the issues of efficiency and accuracy raised by each. Implications for the PAPI cross-platform hardware counter interface are also discussed. 1 Introduction Most modern microprocessors provide hardware support for collecting performance data [2]. Performance monitoring hardware usually consists of a set of registers that record data about the processor's function. These registers range from simple event counters to more sophisticated hardware for recording data such as data and instruction addresses for an event, and pipeline or memory latencies for an instruction. The performance monitoring registers are usually accompanied by a set of control registers that allow the user to configure and control the performance monitoring hardware. Many platforms provide hardware and operating system support for delivering an interrupt to performance monitoring software when a <b>counter</b> <b>overflows</b> a specified threshold...|$|R
40|$|We {{describe}} {{a proposal for}} the zero suppression and data compression for the Silicon Drift Detectors in the ALICE experiment. The proposal seeks to maintain maximum precision {{within the limits of}} data transmission bandwidth, to retain two-dimensional cluster reconstructability and to monitor statistically the background. Two thresholds (high and low) are employed to facilitate understanding of the cluster neighbourhoods. This choice also helps to suppress single high background peaks and provides a statistically cleaner sample for background monitoring. Background average and standard deviation are monitored by counting the zero signal (due to negative inputs to the ADCs) and the signals above the thresholds, then using a minimisation algorithm. Background counts which <b>overflow</b> the small <b>counter</b> ranges are discarded to avoid wasting bits and then corrected statistically offline. First the 10 -bit output of the ADCs is compressed to 8 bits using a quasi-parabolic monotonic characteristic which requires no conversion table, no clock cycles and in which all codes are utilised. The expansion to the original values for offline analysis includes a compensation for the truncated signals which is statistically without bias. The main algorithm is purely combinatorial and thus requires only one clock cycle to code the signals and send them on through a circular twisted barrel shifter to the output FIFO. Simulations of the circuits using FPGAs and of the compression algorithm in software are in proress...|$|R
40|$|Sequential and {{concurrency}} bugs are {{widespread in}} deployed software. They cause severe failures and huge financial loss during production runs. Tools that diagnose production-run failures with low overhead are needed. The state-of-the-art diagnosis techniques use software instrumentation to sample program properties at run time and use off-line statistical analysis to identify properties most correlated with failures. Although promising, these techniques suffer from high run-time overhead, which is sometimes over 100 %, for concurrency-bug failure diagnosis and hence are {{not suitable for}} production-run usage. We present PBI, a system that uses existing hardware performance counters to diagnose production-run failures caused by sequential and concurrency bugs with low overhead. PBI is designed based on several key observations. First, a few widely supported performance counter events can reflect {{a wide variety of}} common software bugs and can be monitored by hardware with almost no overhead. Second, the <b>counter</b> <b>overflow</b> interrupt supported by existing hardware and operating systems provides a natural and effective mechanism to conduct event sampling at user level. Third, the noise and non-determinism in interrupt delivery complements well with statistical processing. We evaluate PBI using 13 real-world concurrency and sequential bugs from representative open-source server, client, and utility programs, and 10 bugs from a widely used software-testing benchmark. Quantitatively, PBI can effectively diagnose failures caused by these bugs with a small overhead that is never higher than 10 %. Qualitatively, PBI does not require any change to software and presents a novel use of existing hardware performance counters...|$|R
40|$|Vector clock {{algorithms}} {{are basic}} wait-free building blocks that facilitate causal ordering of events. As wait-free algorithms, they {{are guaranteed to}} complete their operations within {{a finite number of}} steps. Stabilizing algorithms allow the system to recover after the occurrence of transient faults, such as soft errors and arbitrary violations of the assumptions according to which the system was designed to behave. We present the first, {{to the best of our}} knowledge, stabilizing vector clock algorithm for asynchronous crash-prone message-passing systems that can recover in a wait-free manner after the occurrence of transient faults. In these settings, it is challenging to demonstrate a finite and wait-free recovery from (communication and crash failures as well as) transient faults, bound the message and storage sizes, deal with the removal of all stale information without blocking, and deal with <b>counter</b> <b>overflow</b> events (which occur at different network nodes concurrently). We present an algorithm that never violates safety in the absence of transient faults and provides bounded time recovery during fair executions that follow the last transient fault. The novelty is that in the absence of execution fairness, the algorithm guarantees a bound on the number of times in which the system might violate safety (while existing algorithms might block forever due to the presence of both transient faults and crash failures). Since vector clocks facilitate a number of elementary synchronization building blocks (without requiring remote replica synchronization) in asynchronous systems, we believe that our analytical insights are useful for the design of other systems that cannot guarantee execution fairness...|$|R
40|$|Program phase {{analysis}} has many applications in computer architecture design and optimization. Recently, {{there has been}} a growing interest in employing wavelets as a tool for phase analysis. Nevertheless, the examined scope of workload characteristics and the explored benefits due to wavelet-based analysis are quite limited. This work further extends prior research by applying wavelets analysis to abundant types of program execution statistics and quantifying the benefits of wavelet analysis in terms of accuracy, scalability and robustness in phase classification. Experimental results on SPEC CPU 2000 benchmarks show that compared with methods that work in the time domain, wavelet domain phase analysis achieves higher accuracy and exhibits superior scalability and robustness. We examine and contrast the effectiveness of applying wavelets {{to a wide range of}} runtime workload execution characteristics. We find that wavelet transform significantly reduces temporal dependence in the sampled workload statistics and therefore simple models which are insufficient in the time domain become quite accurate in the wavelet domain. More attractively, we show that different types of workload execution characteristics in wavelet domain can be assembled together to further improve phase classification accuracy. For long-running, complex and real-world workloads, a scalable phase analysis technique is essential to capture the manifested large-scale program behavior. In this study, we show that such scalability can be achieved by applying wavelet analysis of high dimension sampled workload statistics to alleviate the <b>counter</b> <b>overflow</b> problem which can negatively affect phase classification accuracy. By exploiting the wavelet denoising capability, we show in this paper that phase classification can be performed robustly under program execution variability. To our knowledge, this work presents the first effort on using wavelets to improve scalability and robustness in phase analysis. 1...|$|R
40|$|Algorithmic DNA tiles {{systems are}} fascinating. From a {{theoretical}} perspective, they {{can result in}} simple systems that assemble themselves into beautiful, complex structures through fundamental interactions and logical rules. As an experimental technique, they provide a promising method for programmably assembling complex, precise crystals that can grow to considerable size while retaining nanoscale resolution. In the journey from theoretical abstractions to experimental demonstrations, however, lie numerous challenges and complications. In this thesis, to examine these challenges, we consider the physical principles behind DNA tile self-assembly. We survey recent progress in experimental algorithmic self-assembly, and explain the simple physical models behind this progress. Using direct observation of individual tile attachments and detachments with an atomic force microscope, we test some of the fundamental assumptions of the widely-used kinetic Tile Assembly Model, obtaining results that fit the model to within error. We then depart from the simplest form of that model, examining the effects of DNA sticky end sequence energetics on tile system behavior. We develop theoretical models, sequence assignment algorithms, and a software package, StickyDesign, for sticky end sequence design. As a demonstration of a specific tile system, we design a binary counting ribbon that can accurately count from a programmable starting value and stop growing after overflowing, resulting in a single system that can construct ribbons of precise and programmable length. In the process of designing the system, we explain numerous considerations that provide insight into more general tile system design, particularly with regards to tile concentrations, facet nucleation, the construction of finite assemblies, and design beyond the abstract Tile Assembly Model. Finally, we present our crystals that count: experimental results with our binary counting system that represent a significant improvement in the accuracy of experimental algorithmic self-assembly, including crystals that count perfectly with 5 bits from 0 to 31. We show some preliminary experimental results {{on the construction of}} our capping system to stop growth after <b>counters</b> <b>overflow,</b> and offer some speculation on potential future directions of the field. ...|$|R
40|$|Distributed {{computing}} is {{an established}} computing paradigm of modern computing systems. The nodes of a distributed system interact either by sharing resources or via a communication network. In both cases, provisioning of shared resources is a challenge, for example when resource demand and supply varies {{or when the}} system is prone to failures. Analytical tools for evaluating system performance and for provisioning shared resources enhance system design and implementations. In this thesis, we develop analytical tools for the evaluation and self-stabilizing provisioning of shared-resources in distributed systems. We first focus on systems where resource demand and supply varies, and study cases of reusable and non-reusable resources. We study shared-object systems, where system nodes demand mutually exclusive access {{to a number of}} objects in a continuous fashion. We develop analytical tools for computing the expected delay and throughput of such systems, {{in a wide range of}} system utilization scenarios, including saturation points. Moreover, we study systems where nodes share energy resources, and focus on optimizing the available resources on a system-level. We develop online algorithms that use the flexibility on resource demand, to optimize the utilization of the available supply, and prove their competitive ratios. Recovery from failures is necessary for provisioning shared resources. Dynamic and complex systems are often designed based on a failure model, but it is important that they recover even after the occurrence of unexpected failures, outside the failure model. Such failures can include topological changes in the network, stale information in the nodes' memory, communication failures, etc. These failures are further amplified by the system's asynchrony. In these settings, we first focus on provisioning of network resources, in terms of network control and ordering of distributed events. We study Software-Defined Networks (SDNs) and specifically their control planes. We provide a self-stabilizing distributed algorithm for a fault-tolerant SDN control plane, that deals with communication failures, topological changes, as well as, with transient faults, that can bring the system in an arbitrary state. Moreover, we focus on ordering distributed events in asynchronous message-passing systems, in the absence of execution fairness. In these extreme asynchronous settings, we provide a practically-self-stabilizing distributed algorithm, that uses bounded memory and yet, can tolerate concurrent <b>counter</b> <b>overflows,</b> when counting distributed events, as well as transient faults...|$|R

