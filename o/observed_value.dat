1606|10000|Public
25|$|Compute {{from the}} {{observations}} the <b>observed</b> <b>value</b> tobs {{of the test}} statistic T.|$|E
25|$|In {{probability}} theory, a martingale is {{a sequence}} of random variables (i.e., a stochastic process) for which, {{at a particular time}} in the realized sequence, the expectation of the next value in the sequence is equal to the present <b>observed</b> <b>value</b> even given knowledge of all prior observed values.|$|E
25|$|Fisher {{emphasized}} the importance of measuring the tail – the <b>observed</b> <b>value</b> of the test statistic and all more extreme – rather than simply the probability of specific outcome itself, in his The Design of Experiments (1935). He explains this as because a specific set of data may be unlikely (in the null hypothesis), but more extreme outcomes likely, so seen in this light, the specific but not extreme unlikely data should not be considered significant.|$|E
50|$|The {{sample mean}} is a vector each of whose {{elements}} is the sample mean {{of one of}} the random variablesthat is, each of whose elements is the arithmetic average of the <b>observed</b> <b>values</b> {{of one of the}} variables. The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of <b>observed</b> <b>values</b> of two of the variables and whose i, i element is the sample variance of the <b>observed</b> <b>values</b> of one of the variables. If only one variable has had <b>values</b> <b>observed,</b> then the sample mean is a single number (the arithmetic average of the <b>observed</b> <b>values</b> of that variable) and the sample covariance matrix is also simply a single value (a 1x1 matrix containing a single number, the sample variance of the <b>observed</b> <b>values</b> of that variable).|$|R
30|$|The {{calculated}} values were roughly {{equal to the}} <b>observed</b> <b>values,</b> but all the <b>observed</b> <b>values</b> were larger than the {{calculated values}}, which suggests that other energies than the lattice vibration energy are {{also included in the}} total kinetic energy. The difference between the <b>observed</b> <b>values</b> at high temperature above 200 °C and those at low temperature below 200 °C {{may be due to the}} contribution from the free motion of atoms and/or ions in the experiment performed for Te, which might cause the abnormally large values at high temperatures above 200 °C in the specific heat of Te as shown in Figure 5.|$|R
2500|$|The {{estimated}} parameter {{values are}} linear combinations of the <b>observed</b> <b>values</b> ...|$|R
25|$|A {{systematic}} error or bias occurs {{when there is}} a difference between the true value (in the population) and the <b>observed</b> <b>value</b> (in the study) from any cause other than sampling variability. An example of {{systematic error}} is if, unknown to you, the pulse oximeter you are using is set incorrectly and adds two points to the true value each time a measurement is taken. The measuring device could be precise but not accurate. Because the error happens in every instance, it is systematic. Conclusions you draw based on that data will still be incorrect. But the error can be reproduced in the future (e.g., by using the same mis-set instrument).|$|E
25|$|Based {{upon the}} size of the Earth and the force of gravity on its surface, the average density of the planet Earth is 5.515g/cm3, and typical densities of surface rocks are only half that (about 2.75g/cm3). If any {{significant}} portion of the Earth were hollow, the average density would be much lower than that of surface rocks. The only way for Earth to have the force of gravity that it does is for much more dense material to make up {{a large part of the}} interior. Nickel-iron alloy under the conditions expected in a non-hollow Earth would have densities ranging from about 10 to 13g/cm3, which brings the average density of Earth to its <b>observed</b> <b>value.</b>|$|E
25|$|In 1928, Paul Dirac {{extended}} the Pauli equation, which described spinning electrons, {{to account for}} special relativity. The result was a theory that dealt properly with events, such as {{the speed at which}} an electron orbits the nucleus, occurring at a substantial fraction of the speed of light. By using the simplest electromagnetic interaction, Dirac was able to predict the value of the magnetic moment associated with the electron's spin, and found the experimentally <b>observed</b> <b>value,</b> which was too large to be that of a spinning charged sphere governed by classical physics. He was able to solve for the spectral lines of the hydrogen atom, and to reproduce from physical first principles Sommerfeld's successful formula for the fine structure of the hydrogen spectrum.|$|E
5000|$|Estimate {{the false}} {{discovery}} rate based on expected versus <b>observed</b> <b>values</b> ...|$|R
5000|$|The {{estimated}} parameter {{values are}} linear combinations of the <b>observed</b> <b>values</b> ...|$|R
60|$|In every instance, the <b>Observed</b> <b>values</b> {{are seen}} to exceed the Random.|$|R
500|$|Similar {{considerations}} for the electron {{proved to be}} much more successful. [...] In quantum electrodynamics (QED), the anomalous magnetic moment of a particle stems from the small contributions of quantum mechanical fluctuations to the magnetic moment of that particle. [...] The g-factor for a [...] "Dirac" [...] magnetic moment is predicted to be [...] for a negatively charged, spin 1/2 particle. [...] For particles such as the electron, this [...] "classical" [...] result differs from the <b>observed</b> <b>value</b> by a small fraction of a percent; the difference compared to the classical value is the anomalous magnetic moment. [...] The actual g-factor for the electron is measured to be [...] [...] QED results from the mediation of the electromagnetic force by photons. [...] The physical picture is that the effective magnetic moment of the electron results from the contributions of the [...] "bare" [...] electron, which is the Dirac particle, and the cloud of [...] "virtual," [...] short-lived electron–positron pairs and photons that surround this particle as a consequence of QED. [...] The small effects of these quantum mechanical fluctuations can be theoretically computed using Feynman diagrams with loops.|$|E
2500|$|... where Γ(z) is the gamma function. [...] The beta function, , is a {{normalization}} constant {{to ensure}} that the total probability integrates to1. In the above equations x is a realizationan <b>observed</b> <b>value</b> that actually occurredof a random processX.|$|E
2500|$|Decide {{to either}} reject the null {{hypothesis}} {{in favor of the}} alternative or not reject it. The decision rule is to reject {{the null hypothesis}} H0 if the <b>observed</b> <b>value</b> tobs is in the critical region, and to accept or [...] "fail to reject" [...] the hypothesis otherwise.|$|E
5000|$|Let [...] be the <b>observed</b> <b>values,</b> in {{increasing}} order. Then the statistic is ...|$|R
3000|$|..., {{indicating}} a satisfactorily {{high level of}} similarity between our weights prediction and their <b>observed</b> <b>values.</b>|$|R
30|$|For the analysis, we {{made for}} each country ten {{forecasts}} of life expectancy at age 65, {{men and women}} combined, using data for ten different fitting periods: from 1960 to 2005, 1960 to 2006, …, and 1960 to 2014. The forecasts are calculated using the six different alternatives of the jump-off rates: the model <b>values,</b> the <b>observed</b> <b>values,</b> and an average of two/three/four/five <b>observed</b> <b>values.</b>|$|R
2500|$|The most {{important}} application is in data fitting. [...] The best {{fit in the}} least-squares sense minimizes the sum of squared residuals (a residual being: {{the difference between an}} <b>observed</b> <b>value,</b> and the fitted value provided by a model). When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares.|$|E
2500|$|The {{anomalous}} {{magnetic dipole}} moment {{is the difference between}} the experimentally <b>observed</b> <b>value</b> of the {{magnetic dipole moment}} and the theoretical value predicted by the Dirac equation. The measurement and prediction of this value is very important in the precision tests of QED (quantum electrodynamics). The E821 experiment at Brookhaven National Laboratory (BNL) studied the precession of muon and anti-muon in a constant external magnetic field as they circulated in a confining storage ring. E821 reported the following average value in 2006: ...|$|E
2500|$|The {{probability}} of observing value X is then proportional to N(X) P(X). A generic feature of {{an analysis of}} this nature is that the expected values of the fundamental physical constants should not be [...] "over-tuned", i.e. {{if there is some}} perfectly tuned predicted value (e.g. zero), the <b>observed</b> <b>value</b> need be no closer to that predicted value than what is required to make life possible. The small but finite value of the cosmological constant can be regarded as a successful prediction in this sense.|$|E
30|$|Residuals {{increased}} with distance between our stations and estimate points. Note also that despite having a higher correlation with <b>observed</b> <b>values,</b> the nearest NIWA station estimates were biased overall, with <b>observed</b> <b>values</b> generally larger than NIWA station values. Bias {{did not appear}} to be related to any particular feature, such as distance from, station or differences between NIWA station elevation and elevation of sample point.|$|R
2500|$|... are the <b>observed</b> <b>values</b> of {{the sample}} items, [...] is the mean value of these observations, and ...|$|R
30|$|This data {{proves that}} the values {{expected}} by the model {{are close to the}} actual <b>observed</b> <b>values.</b>|$|R
2500|$|Many {{physical}} effects attributed to zero-point energy have been experimentally verified, such as spontaneous emission, Casimir force, Lamb shift, magnetic {{moment of the}} electron and Delbrück scattering, these effects are usually called [...] "radiative corrections". [...] In more complex nonlinear theories (e.g. QCD) zero-point energy can {{give rise to a}} variety of complex phenomena such as multiple stable states, symmetry breaking, chaos and emergence. Many physicists believe that [...] "the vacuum holds the key to a full understanding of nature" [...] and that studying it is critical in the search for the theory of everything. Active areas of research include the effects of virtual particles, quantum entanglement, the difference (if any) between inertial and gravitational mass, variation in the speed of light, a reason for the <b>observed</b> <b>value</b> of the cosmological constant and the nature of dark energy.|$|E
2500|$|Dicke later {{reasoned}} that the density of matter in the universe must be almost exactly the critical density needed to prevent the Big Crunch (the [...] "Dicke coincidences" [...] argument). The most recent measurements may suggest that the observed density of baryonic matter, and some theoretical predictions {{of the amount of}} dark matter account for about 30% of this critical density, with the rest contributed by a cosmological constant. Steven Weinberg gave an anthropic explanation for this fact: he noted that the cosmological constant has a remarkably low value, some 120 orders of magnitude smaller than the value particle physics predicts (this has been described as the [...] "worst prediction in physics"). However, if the cosmological constant were only several orders of magnitude larger than its <b>observed</b> <b>value,</b> the universe would suffer catastrophic inflation, which would preclude the formation of stars, and hence life.|$|E
2500|$|The {{function}} A(t|ν) is {{the integral}} of Student's probability density function, f(t) between −t and t, for t ≥ 0. [...] It thus gives {{the probability that}} a value of t less than that calculated from observed data would occur by chance. [...] Therefore, the function A(t|ν) can be used when testing whether the difference between the means of two sets of data is statistically significant, by calculating the corresponding value of t and the probability of its occurrence if the two sets of data were drawn from the same population. [...] This is used in a variety of situations, particularly in t-tests. [...] For the statistic t, with ν degrees of freedom, A(t|ν) is the probability that t would be less than the <b>observed</b> <b>value</b> if the two means were the same (provided that the smaller mean is subtracted from the larger, so that t ≥ 0). [...] It can be easily calculated from the cumulative distribution function Fν(t) of the t-distribution: ...|$|E
50|$|If all <b>observed</b> <b>values</b> are {{strictly}} positive, existence and uniqueness of MLEs and therefore convergence is ensured.|$|R
5000|$|A {{sequence}} of 'reward' functions [...] which {{depend on the}} <b>observed</b> <b>values</b> of the random variables in 1.: ...|$|R
40|$|A {{rigorous}} analytic {{evaluation of}} an emitter model that includes Auger recombination but excludes bandgap narrowing is presented. It is shown {{that such a}} model cannot explain the experimentally <b>observed</b> <b>values</b> of the open-circuit voltage in p-n-junction silicon solar cells. Thus physical mechanisms in addition to Auger recombination {{are responsible for the}} experimentally <b>observed</b> <b>values</b> of the open-circuit voltage in silicon solar cells and the common-emitter current gain in bipolar transistors...|$|R
5000|$|... where Y is the <b>observed</b> <b>value,</b> Bottom is {{the lowest}} <b>observed</b> <b>value,</b> Top is the highest <b>observed</b> <b>value,</b> and the Hill {{coefficient}} gives the largest absolute value of {{the slope of the}} curve.|$|E
5000|$|... where [...] is the <b>observed</b> <b>value</b> of [...] and [...] is the <b>observed</b> <b>value</b> of [...] Exact inferences on [...] {{based on}} probabilities and {{expected}} values of [...] are possible because its distribution and the <b>observed</b> <b>value</b> are both free of nuisance parameters.|$|E
5000|$|In {{statistics}} and optimization, errors and residuals are two closely related and easily confused {{measures of the}} deviation of an <b>observed</b> <b>value</b> of an element of a statistical sample from its [...] "theoretical value". The error (or disturbance) of an <b>observed</b> <b>value</b> is the deviation of the <b>observed</b> <b>value</b> from the (unobservable) true value of a quantity of interest (for example, a population mean), and the residual of an <b>observed</b> <b>value</b> {{is the difference between}} the <b>observed</b> <b>value</b> and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.|$|E
5000|$|In Bayesian statistics, the {{posterior}} predictive distribution is {{the distribution of}} possible unobserved values conditional on the <b>observed</b> <b>values.</b>|$|R
5000|$|In statistics, the hat matrix H {{projects}} the <b>observed</b> <b>values</b> y of response variable {{to the predicted}} values ŷ: ...|$|R
40|$|In this thesis, shows {{a current}} issue for free space optics, {{limiting}} {{their use in}} practice {{with regard to the}} availability and reliability. They conducted measurements on wireless optical links and the <b>observed</b> <b>values</b> is calculated by the availability of the connection. Subsequently, measurements are performed on alternate versions of wireless and copper connections. <b>Observed</b> <b>values</b> were compared with a wireless optical link. It is made of optical design of wireless connections at home...|$|R
