29|0|Public
2500|$|Modern sensitivities come {{in three}} broad ranges: geophones, 50 to 750 V/m; local {{geologic}} seismographs, about 1,500 V/m; and teleseismographs, used for world survey, about 20,000 V/m. Instruments come in three main varieties: short period, long period and broadband. [...] The {{short and long}} period measure velocity and are very sensitive, however they 'clip' the signal or go <b>off-scale</b> for ground motion that {{is strong enough to}} be felt by people. [...] A 24-bit analog-to-digital conversion channel is commonplace. [...] Practical devices are linear to roughly one part per million.|$|E
2500|$|On 27 March, TEPCO {{reported}} {{stagnant water}} {{in the basement of}} unit 2 (inside the reactor/turbine building complex, but outside the primary containment) was measured at 1000mSv/h or more, which prompted evacuation. The exact dose rate remains unknown as the technicians fled the place after their first measurement went <b>off-scale.</b> Additional basement and trench-area measurements indicated 60mSv/h in unit 1, [...] "over 1000"mSv/h in unit 2, and 750mSv/h in unit 3. The report indicated the main source was iodine-134 with a half-life of less than an hour, which resulted in a radioactive iodine concentration 10million times the normal value in the reactor. TEPCO later retracted its report, stating that the measurements were inaccurate and attributed the error to comparing the isotope responsible, iodine-134, to normal levels of another isotope. Measurements were then corrected, stating that the iodine levels were 100,000 times the normal level. On 28 March, the erroneous radiation measurement caused TEPCO to reevaluate the software used in analysis.|$|E
50|$|Clinic are an English {{rock band}} from Liverpool, formed in 1997. They are noted {{for their use}} of vintage keyboards/organs and <b>off-scale</b> chord progressions.|$|E
5000|$|Here {{the user}} of the slide rule must {{remember}} {{to adjust the}} decimal point appropriately to correct the final answer. We wanted to find 2×7, but instead we calculated (2/10)×7=0.2×7=1.4. So the true answer is not 1.4 but 14. Resetting the slide {{is not the only}} way to handle multiplications that would result in <b>off-scale</b> results, such as 2×7; some other methods are: ...|$|E
50|$|The {{illustration}} below {{demonstrates the}} computation of 5.5/2. The 2 {{on the top}} scale is placed over the 5.5 on the bottom scale. The 1 on the top scale lies above the quotient, 2.75. There {{is more than one}} method for doing division, but the method presented here has the advantage that the final result cannot be <b>off-scale,</b> because one has a choice of using the 1 at either end.|$|E
50|$|Most Plasticville {{buildings}} are 1:64 scale with 1:48 scale doors, a design compromise {{that allows them}} to be used with O gauge, O27 gauge, or S gauge train layouts without looking far <b>off-scale.</b> This allowed one product line to serve Lionel's low-end and high-end product lines as well as American Flyer's product line in the 1950s. Later, as HO scale gained popularity, Bachmann produced a line of 1:87 scale buildings for that standard.|$|E
50|$|Accelerographs {{are useful}} for when the {{earthquake}} ground motion {{is so strong that}} it causes the more sensitive seismometers to go <b>off-scale.</b> There is an entire science of strong ground motion, that is dedicated to placing accelerographs in the vicinity of major faults. The type of information gathered (such as rupture velocity) would not be possible with the standard seismometers. The best known example is the Parkfield Experiment which involved a massive set of strong motion instrumentation.|$|E
50|$|Modern sensitivities come {{in three}} broad ranges: geophones, 50 to 750 V/m; local {{geologic}} seismographs, about 1,500 V/m; and teleseismographs, used for world survey, about 20,000 V/m. Instruments come in three main varieties: short period, long period and broadband. The {{short and long}} period measure velocity and are very sensitive, however they 'clip' the signal or go <b>off-scale</b> for ground motion that {{is strong enough to}} be felt by people. A 24-bit analog-to-digital conversion channel is commonplace. Practical devices are linear to roughly one part per million.|$|E
5000|$|Md designates various scales that {{estimate}} magnitude {{from the}} duration or length of {{some part of}} the seismic wave-train. This is especially useful for measuring local or regional earthquakes, both powerful earthquakes that might drive the seismometer <b>off-scale</b> (a problem with the analog instruments formerly used) and preventing measurement of the maximum wave amplitude, and weak earthquakes, whose maximum amplitude is not accurately measured. Even for distant earthquakes, measuring the duration of the shaking (as well as the amplitude) provides a better measure of the earthquake's total energy. Measurement of duration is incorporated in some modern scales, such as [...] and [...]|$|E
5000|$|Some {{of their}} number managed to obtain liquid oxygen {{from the nearby}} Arthur D. Little company, which was {{developing}} a portable military [...] "lox" [...] generator and was dumping excess product fuming and freezing everything, into the gutter. The club hauled it to their test site in 5-gallon steel cans insulated with fiber mat. This gave their most successful test—the rocket motor roared with a ten-foot plume of flame filled with standing shock waves, with the thrust gauge <b>off-scale</b> for ten seconds or more. But the aluminum motor burned out its throat and set the test stand on fire, which they put out with a Pyrene hand fire extinguisher. However the carbon tetrachloride produced phosgene and chlorine gasses (they deduced), which corroded all the metal.|$|E
5000|$|On 27 March, TEPCO {{reported}} {{stagnant water}} {{in the basement of}} unit 2 (inside the reactor/turbine building complex, but outside the primary containment) was measured at 1000 mSv/h or more, which prompted evacuation. The exact dose rate remains unknown as the technicians fled the place after their first measurement went <b>off-scale.</b> Additional basement and trench-area measurements indicated 60 mSv/h in unit 1, [...] "over 1000" [...] mSv/h in unit 2, and 750 mSv/h in unit 3. The report indicated the main source was iodine-134 with a half-life of less than an hour, which resulted in a radioactive iodine concentration 10 million times the normal value in the reactor. TEPCO later retracted its report, stating that the measurements were inaccurate and attributed the error to comparing the isotope responsible, iodine-134, to normal levels of another isotope. Measurements were then corrected, stating that the iodine levels were 100,000 times the normal level. On 28 March, the erroneous radiation measurement caused TEPCO to reevaluate the software used in analysis.|$|E
50|$|Gorbachenko, a {{radiation}} monitoring technician, {{at the beginning}} of his shift checked Unit 3; he skipped the check of Unit 4 as it was being shut down, so at the moment of the accident he was located in the duty room. A flat and powerful thud shook the building; he and his assistant Pshenichnikov thought it was a water hammer occurring during a turbine shutdown. Another flat thud followed, accompanied by lights going out, the control panel of Unit 4 losing signal, latched double doors being blown apart by the blast, and black and red powder falling from the ventilation vent; emergency lights then switched on. Telephone connection with Unit 4 was cut. The corridor to the deaerator galleries was full of steam and white dust. The radiation counters went <b>off-scale,</b> and the high-range one burned out when switched on; the portable instruments were capable of showing at most 4 roentgens per hour (36 nA/kg), while the radiation on the roof ranged between 2,000 and 15,000 roentgens per hour (18 and 130 µA/kg). He went to the turbine hall to survey the damage, saw scattered pieces of concrete, and returned to the duty room. Meeting two men there, together with them he went to search for Shashenok, found him unconscious in a damaged instrument room, and carried him down. Gorbachenko returned to his post and changed clothes and shoes. He was then ordered to look for Khodemchuk, but the search was unsuccessful. He went to the control room and with Dyatlov went outside to survey the reactor building. At 5:00 am, he began feeling weak and vomiting and was transported to hospital, from where he was released on 27 October.|$|E
40|$|This report {{covers the}} {{unintended}} extremity exposure to an operator while handling a metallurgical mount sample of irradiated fuel following an <b>off-scale</b> high beta radiation {{reading of the}} sample. The {{decision was made to}} continue working after the meter indicated high <b>off-scale</b> by the HPT Supervisor, which resulted in the operator at the next operation being exposed...|$|E
40|$|Thermocouple-signal-conditioning circuit {{acting in}} {{conjunction}} with thermocouple, exhibits electrical behavior of voltage in series with resistance. Combination part of input bridge circuit of controller. Circuit configured for either of two specific applications by selection of alternative resistances and supply voltages. Includes alarm circuit detecting open circuit in thermocouple and provides <b>off-scale</b> output to signal malfunctions...|$|E
40|$|The <b>off-scale</b> low {{indications}} of the voltmeters for batteries {{one and two}} at initial powerup of the Apollo 16 lunar roving vehicle were analyzed. No single condition was found that would explain the anomalous meter indications. It is concluded that the intermittent conditions must have existed in the multiple wire splices to the meters...|$|E
30|$|We use tsunami {{waveforms}} {{recorded at}} coastal tide and wave gauges, and offshore GPS wave gauges, deep-ocean bottom-pressure gauges (Fig. 1). While {{most of the}} coastal tide gauge stations went <b>off-scale</b> and did not record the tsunami peak, the arrival time and the initial slope recorded provide information on the tsunami source, hence we include them in the inversion. In order to retrieve a tsunami signal, we first approximate a tidal component by fitting a polynomial function, and then remove it from the original record.|$|E
40|$|We analyse {{geomagnetic}} recordings at four subauroral and midlatitude Russian observatories in 1850 – 1862. The data {{consist of}} spot readings made once in {{hour of the}} north and east components of the magnetic field. We use the hourly change of the horizontal field vector as the measure of activity. We compare these values to data from modern observatories at corresponding magnetic latitudes (Nurmijärvi, Finland, magnetic latitude ~ 57  N; Tartu, Estonia, ~ 54. 5  N; Dourbes, Belgium, ~ 46  N) by reducing their data to the 1 -h format. The largest variations at the Russian observatories occurred during the Carrington storm in September 1859 and they reached about 1000  nT/h, which was the instrumental <b>off-scale</b> limit. When the time stamp for the spot readings happens to be optimal, the top variation in the Nurmijärvi data is about 3700  nT/h (July 1982), and at Tartu the maximum is about 1600  nT/h (November 2004). At a midlatitude site Nertchinsk in Russia (magnetic latitude ~ 45  N), the variation during the Carrington storm was at the <b>off-scale</b> limit, and exceeded the value observed at Dourbes during the Halloween storm in October 2003. At Nertchinsk, the Carrington event was at least four times larger than any other storm in 1850 – 1862. Despite the limitations of the old recordings and in using only hourly spot readings, the Carrington storm was definitely a very large event at midlatitudes. At higher latitudes, it remains somewhat unclear whether it exceeds the largest modern storms, especially the one in July 1982...|$|E
40|$|A {{magnitude}} 4. 3 earthquake {{was recorded}} on {{an array of}} accelerometers at Pacoima Dam on January 13, 2001. The records are used for two purposes: (1) to analyze the effects that canyon topography has on the ground motion along the abutments, and (2) as input for a system identification study, leading to a calibrated finite element model of Pacoima Dam. The quantified amplification and time delay characteristics of the 2001 abutment motions serve {{as a basis for}} generating records to replace ones that went <b>off-scale</b> during the 1994 Northridge earthquake. These generated records were then used in the finite element model to verify that nonuniform ground motion caused by the topography has {{a significant impact on the}} dam response. Forced vibration tests were also conducted in July/August 2002...|$|E
30|$|The 1896 tsunami was instrumentally {{recorded}} on three tide gage stations at regional distances in Japan: Hanasaki (440  km from the epicenter), Ayukawa (250  km), and Choshi (500  km) (Fig.  1 b, Honda et al. 1908; Imamura and Moriya 1939). Tanioka and Satake (1996 b) examined these waveforms, estimated the clock timing errors {{as large as}} 5  min, and modeled the waveforms without timing information. The 2011 tsunami was also recorded at these tide gage stations, although the Ayukawa record went <b>off-scale</b> immediately following the first tsunami arrival at ~  30  min from the earthquake (Satake et al. 2013 b). Comparison of the 1896 and 2011 tsunami waveforms indicates that both periods and amplitudes of the 2011 waveforms are larger {{than those of the}} 1896 tsunami (Fig.  1 d), probably due to the different sizes of tsunami source.|$|E
40|$|The AutoSCAN- 3 (American MicroScan, Mahwah, N. J.) is an {{instrument}} capable of automated reading of commercially available microdilution trays for identification and quantitative susceptibility testing of rapidly growing bacteria. This study compared {{the results of}} visual and automated reading of microdilution trays for determination and interpretation of minimum inhibitory concentrations of 471 selected gram-negative and gram-positive clinical bacterial isolates. Visual and automated readings were performed in a double-blind fashion, and all discrepancies were examined by a referee. A quantitative comparison of minimum inhibitory concentrations was performed for 201 organisms, yielding 2, 472 drug-organism combinations. After exclusion of <b>off-scale</b> values, complete quantitative agreement was obtained in 94 % of 959 on-scale combinations, and agreement within +/- 1 well was obtained in 99. 3 %. Considering the minimum inhibitory concentration interpretations routinely furnished by the instrument, a qualitative comparison was performed for all 471 organisms. Complete agreement in interpretation was obtained in 97. 6 % of 5, 843 drugs-organism combinations, with very major discrepancies accounting for only 0. 1 % and major discrepancies accounting for 0. 2 % of all combinations tested...|$|E
40|$|Aims To {{test the}} {{reliability}} of a new vibrometer (Maxivibrometer) which was constructed so that vibration perception threshold (VPT) could be determined without the disadvantage of the <b>off-scale</b> measurements frequently experienced with the Biothesiometer. Methods The two devices were compared and tested {{on a group of}} diabetic neuropathic subjects and a group of healthy, matched control subjects. VPT was tested on the plantar surface of the feet. Results The Maxivibrometer gave an actual measurement in all cases even if subjects were severely neuropathic. The replication-to-replication and day-to-day intraclass correlation coefficients for the Maxivibrometer VPT were, except in one case, above 0. 94, indicating excellent reliability. The Biothesiometer VPT could also be measured with excellent reliability but only within a limited range of mild to moderate neuropathy, so it appears to be an appropriate screening tool. The replication-to-replication intraclass correlation coefficient was 0. 93. Conclusions Because VPT could be measured over a wide range with the Maxivibrometer, it was demonstrated that loss of sensation in diabetic neuropathy can progress far beyond the maximum VPT value of the Biothesiometer. The wide measurement range and the excellent reliability make the Maxivibrometer a valuable research tool to quantify loss of sensation, particularly in the presence of severe neuropathy and to record changes over time...|$|E
3000|$|A {{similarly}} significant MCS {{was observed}} at Yamagawa (31.2 ^∘N, 130.62 ^∘E; epicentral distance 1124  km), Japan, after the M 7.7 aftershock (36.12 ^∘N, 141.25 ^∘E; 0615 : 34 UTC) of the massive M 9.0 Tohoku-Oki earthquake in 2011, as shown in Fig.  2 a. The maximum amplitude of the Rayleigh wave responsible for this MCS was 5.0 mm/s at Tashiro (31.19 ^∘N, 130.91 ^∘E), near Yamagawa (difference in epicentral distances of 23  km), {{as shown by the}} horizontal bar at ∼ 0621 : 30 UTC in Fig.  2 b. Figure  3 a shows another example of an MCS ionogram observed at Yamagawa during the same earthquake. In this ionogram, the amplitude of the deformation was small, showing wiggles at frequencies of 3.5 – 6 MHz, which is almost the detection limit of coseismic ionogram deformation. Figure  3 b shows the ground motion at Tashiro. The large-amplitude signals before ∼ 0556 UTC (goes <b>off-scale</b> in the plot) were the Rayleigh waves excited by the M 9.0 main shock. The ground motion responsible for the MCS ionogram in Fig.  3 a is shown by the horizontal bar in Fig.  3 b and was most probably excited by the M 6.6 aftershock at 0558 UTC; the amplitude was 0.5 – 1  mm/s. (Note that the ionosonde was operated at each quarter-hour and no ionogram corresponding to the main shock was obtained.) [...]...|$|E
40|$|CTX-M extended-spectrum beta-lactamases (ESBLs) {{have emerged}} as {{the most common type}} of ESBL globally, their {{incidence}} easily surpassing those of SHV and TEM ESBLs in most locales. This study compared the performance of two MicroScan dried panels with CLSI reference broth microdilution and disk diffusion methods on a collection of genetically characterized ESBL-producing isolates. These included 64 Enterobacteriaceae isolates that produced CTX-M 8, - 14, - 15, or - 16 according to PCR and sequencing of the bla gene, 17 isolates that produced a SHV or TEM ESBL, and 19 that produced both CTX-M and SHV ESBLs. Each isolate was tested by a frozen reference microdilution panel, the MicroScan ESβL plus confirmation panel, and a routine dried panel containing streamlined ESBL confirmation dilutions (MicroScan Neg MIC panel type 32) that included cefotaxime and ceftazidime tested alone or with a fixed concentration of 4 μg/ml of clavulanate. Each isolate was also tested by the standard CLSI double-disk confirmation tests. The disk diffusion method detected all ESBL-producing isolates, the frozen reference panel detected 90 % of isolates (10 out of 100 could not be analyzed because of <b>off-scale</b> MICs that exceeded the clavulanate combination concentrations in the panel), the ESβL plus panel detected 98 % (1 missed and 1 off scale), and the streamlined ESBL panel detected 95 % (5 off scale). Very high MICs for a few strains that produced SHV or both CTX-M and SHV ESBLs precluded noting the required three twofold-dilution differences with clavulanate needed to confirm an ESBL primarily in the reference panel and the Neg type 32 panel...|$|E
40|$|Background Although second {{generation}} sequencing (2 GS) technologies allow re-sequencing of previously gold-standard-sequenced genomes, whole genome shotgun sequencing and de novo assembly of large and complex eukaryotic genomes is still difficult. Availability of a genome-wide physical map is therefore still {{a prerequisite for}} whole genome sequencing for genomes like barley. To start such an endeavor, large insert genomic libraries, i. e. Bacterial Artificial Chromosome (BAC) libraries, which are unbiased and representing deep haploid genome coverage, need to be ready in place. Result Five new BAC libraries were constructed for barley (Hordeum vulgare L.) cultivar Morex. These libraries were constructed in different cloning sites (Hind III, EcoR I, Mbo I and BstX I) of the respective vectors. In order to enhance unbiased genome representation and to minimize the number of gaps between BAC contigs, which are often due to uneven distribution of restriction sites, a mechanically sheared library was also generated. The new BAC libraries were fully characterized in depth by scrutinizing the major quality parameters such as average insert size, degree of contamination (plate wide, neighboring, and chloroplast), empty wells and <b>off-scale</b> clones (clones with 250 fragments). Additionally a set of gene-based probes were hybridized to high density BAC filters and showed that genome coverage of each library is between 2. 4 and 6. 6 X. Conclusion BAC libraries representing > 20 haploid genomes are available as a new resource to the barley research community. Systematic utilization of these libraries in high-throughput BAC fingerprinting should allow developing a genome-wide physical map for the barley genome, which will be instrumental for map-based gene isolation and genome sequencing. </p...|$|E
40|$|Background: Although second {{generation}} sequencing (2 GS) technologies allow re-sequencing of previously gold-standard-sequenced genomes, whole genome shotgun sequencing and de novo assembly of large and complex eukaryotic genomes is still difficult. Availability of a genome-wide physical map is therefore still {{a prerequisite for}} whole genome sequencing for genomes like barley. To start such an endeavor, large insert genomic libraries, i. e. Bacterial Artificial Chromosome (BAC) libraries, which are unbiased and representing deep haploid genome coverage, need to be ready in place. Result: Five new BAC libraries were constructed for barley (Hordeum vulgare L.) cultivar Morex. These libraries were constructed in different cloning sites (HindIII, EcoRI, MboI and BstXI) of the respective vectors. In order to enhance unbiased genome representation and to minimize the number of gaps between BAC contigs, which are often due to uneven distribution of restriction sites, a mechanically sheared library was also generated. The new BAC libraries were fully characterized in depth by scrutinizing the major quality parameters such as average insert size, degree of contamination (plate wide, neighboring, and chloroplast), empty wells and <b>off-scale</b> clones (clones with 250 fragments). Additionally a set of gene-based probes were hybridized to high density BAC filters and showed that genome coverage of each library is between 2. 4 and 6. 6 X. Conclusion: BAC libraries representing > 20 haploid genomes are available as a new resource to the barley research community. Systematic utilization of these libraries in high-throughput BAC fingerprinting should allow developing a genome-wide physical map for the barley genome, which will be instrumental for map-based gene isolation and genome sequencing. Daniela Schulte, Ruvini Ariyadasa, Bujun Shi, Delphine Fleury, Chris Saski, Michael Atkins, Pieter deJong, Cheng-Cang Wu, Andreas Graner, Peter Langridge and Nils Stei...|$|E
40|$|The VITEK 2 {{is a new}} {{automated}} {{instrument for}} rapid organism identification and susceptibility testing. It has the capability of performing rapid susceptibility testing of Streptococcus pneumoniae with specially configured cards that contain enriched growth medium and antimicrobial agents relevant for this organism. The present study compared the results of testing {{of a group of}} 53 challenge strains of pneumococci with known resistance properties and a collection of clinical isolates examined in two study phases with a total of 402 and 416 isolates, respectively, with a prototype of the VITEK 2. Testing was conducted in three geographically separate laboratories; the challenge collection was tested by all three laboratories, and the unique clinical isolates were tested separately by the individual laboratories. The VITEK 2 results of tests with 10 antimicrobial agents were compared to the results generated by the National Committee for Clinical Laboratory Standards reference broth microdilution MIC test method. Excellent interlaboratory agreement was observed with the challenge strains. The overall agreement within a single twofold dilution of MICs defined by the VITEK 2 and reference method with the clinical isolates was 96. 3 %, although {{there were a number of}} <b>off-scale</b> MICs that could not be compared. The best agreement with the clinical isolates was achieved with ofloxacin and chloramphenicol (100 %), and the lowest level of agreement among those drugs with sufficient on-scale MICs occurred with trimethoprim-sulfamethoxazole (89. 7 %). Overall there were 1. 3 % very major, 6. 6 % minor, and no major interpretive category errors encountered with the clinical isolates, although > 80 % of the minor interpretive errors involved only a single log 2 dilution difference. The mean time for generation of susceptibility results with the clinical isolates was 8. 1 h. The VITEK 2 provided rapid, reliable susceptibility category determinations with both the challenge and clinical isolates examined in this study...|$|E
40|$|Extended-spectrum β-lactamases (ESBLs) are enzymes {{found in}} gram-negative bacilli that mediate {{resistance}} to extended-spectrum cephalosporins and aztreonam. In 1999, the National Committee for Clinical Laboratory Standards (NCCLS) published methods for screening and confirming {{the presence of}} ESBLs in Klebsiella pneumoniae, Klebsiella oxytoca, and Escherichia coli. To evaluate the confirmation protocol, we tested 139 isolates of K. pneumoniae that were sent to Project ICARE (Intensive Care Antimicrobial Resistance Epidemiology) from 19 hospitals in 11 U. S. states. Each isolate met the NCCLS screening criteria for potential ESBL producers (ceftazidime [CAZ] or cefotaxime [CTX] MICs were ≥ 2 μg/ml for all isolates). Initially, 117 (84 %) isolates demonstrated a clavulanic acid (CA) effect by disk diffusion (i. e., an increase in CAZ or CTX zone diameters of ≥ 5 mm {{in the presence of}} CA), and 114 (82 %) demonstrated a CA effect by broth microdilution (reduction of CAZ or CTX MICs by ≥ 3 dilutions). For five isolates, a CA effect could not be determined initially by broth microdilution because of <b>off-scale</b> CAZ results. However, a CA effect was observed in two of these isolates by testing cefepime and cefepime plus CA. The cefoxitin MICs for 23 isolates that failed to show a CA effect by broth microdilution were ≥ 32 μg/ml, suggesting either the presence of an AmpC-type β-lactamase or porin changes that could mask a CA effect. By isoelectric focusing (IEF), 7 of the 23 isolates contained a β-lactamase with a pI of ≥ 8. 3 suggestive of an AmpC-type β-lactamase; 6 of the 7 isolates were shown by PCR to contain both ampC-type and blaOXA genes. The IEF profiles of the remaining 16 isolates showed a variety of β-lactamase bands, all of which had pIs of ≤ 7. 5. All 16 isolates were negative by PCR with multiple primer sets for ampC-type, blaOXA, and blaCTX-M genes. In summary, 83. 5 % of the K. pneumoniae isolates that were identified initially as presumptive ESBL producers were positive for a CA effect, while 5. 0 % contained β-lactamases that likely masked the CA effect. The remaining 11. 5 % of the isolates studied contained β-lactamases that did not demonstrate a CA effect. An algorithm based on phenotypic analyses is suggested for evaluation of such isolates...|$|E
40|$|The {{statement}} Grands ensembles {{is issued}} {{to be used}} for indicate the new neighborhood with high density population built around the Second World War, when the French government was trying to respond to the growing demand for housing. The term was initially used to identify the morphological system characterized, then it began to indicate low-cost housing for the lower classes. The "mission" of Banlieue ' 89, in the eighties, has sanctioned an important time for opening of a true disciplinary debate on the state and destiny of the grands ensembles. After the actions and debates promoted by Banlieue ' 89, the problem of rehabilitation/renovation of the grands ensembles has become central. Many differents laws have been enacted : the law Solidarité et renouvellement urbain (SRU), and the law of Droit au logement opposable (DALO). In 2003, the Agence nationale pour le renouvellement urbain (ANRU) has been founded. Currently the issue of the grands ensembles and their destiny is one of the central themes of the Grand Paris project. The current response to the rhetoric of the demolition are two actions: the "residentialitation" and "remodelage". The residentialisation is based on spatials actions : subdivision of buildings in differents parts to reduce the effect of the <b>off-scale,</b> subdivision and articulation of the space generically collective through the definition of a series of gradual intermediate spaces for filtering progressively the transition from public to private. The most popular projects on residentialisation were elaborated by P. Panarai, who was the first to define this type of intervention. The remodelage, as defined by Roland Castro, consists of interventions to the embeselliment of the architecture of buildings. In both cases, the action of the transformation doesn't include the "dimension time": the resident is the recipient of a project that has the ambition to change his modus habitandi. A third way is indicate in which part from the work of Lucien Kroll. Kroll established, through the development of his work, a key issue : "project as a process" that is able to compare with (confrontarsi con) the dimension time. It is an architecture-process that he calls "incrementalism", according to which the project is interpreted as a dynamic entity in relationship with its context...|$|E
40|$|A {{study of}} the {{emulsification}} process of lithographic ink and water was conducted to investigate methods for determining the water content of lithographic ink and water emulsions. Experiments using infra-red spectroscopy {{and analysis of the}} hydroxyl and carbonyl absorption bands revealed several variables which must be considered when attempting to use infra-red spectroscopic methods for the determination of the water content of ink emulsions. These factors include the choice of pigment, (either transparent or opaque), the type of cell medium, and methods for dispersing added water into an ink. The initial problem encountered was a lack of reproducibility when attempting to construct a calibration system. The carbonyl absorption band at 1745 cm^- 1 was shown to vary with each sample of emulsion. The intensity of the hydroxyl absorption band at 1645 cm^- 1 was discovered to be erratic between differing samples of ink and water emulsions. This resulted in hydroxyl contents which appeared to be inconsistent with known concentrations of water. The simulation of 2 ̆ 2 on-press 2 ̆ 2 emulsification, as it occurs in the lithographic process on an offset press, is a very important factor. The importance of added water attaining the internal phase of the lithographic emulsion is the prime criterion for valid and reproducible data obtained from infra-red spectroscopic analysis. This is accomplished by applying sufficient forces to generate an adequate rate of shear on the ink as added water is introduced into the emulsification process. The two modes of vibration of a water molecule were studied utilizing data obtained from infra-red spectra. The O-H stretching mode exhibited at 3400 cm^- 1 proved to be more sensitive to changes in water content of an ink than the bending mode exhibited at 1645 cm^- 1. Valid data used to construct a calibration system for water content determination of lithographic emulsions relies on the elimination of any moisture contamination of the cell medium used in the optics system of an infra-red spectrophotometer. Likewise, use of a transparent pigmented ink yields a continuous spectra. An opaque pigmented ink causes excessive light scattering resulting in <b>off-scale</b> deflections of absorption bands. Present methods of laboratory simulation of the emulsification of water into an ink were investigated to determine the optimum system which would yield stable ink and water emulsions in which the added water would assume the internal phase of the lithographic emulsion. Factors such as the rate of shear and time of dispersion were studied and found to significantly influence the formation of stable lithographic emulsions. Finally, the behavior of added water on the tack of an ink was also investigated. The tack values of a 2 ̆ 2 dry 2 ̆ 2 ink were closely monitored for extended periods of time. Immediate changes in tack were noted and recorded upon the addition of water. The addition of water to an ink in which the tack had apparently stabilized, effectively lowered the tack to the point where a homogeneous, stable emulsion was possible...|$|E

