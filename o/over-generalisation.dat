36|4|Public
50|$|Critics have {{highlighted}} {{what they see}} as the loose-thinking, <b>over-generalisation,</b> and substitution of attitudes for ideas of Beattie's book, as well as the way it has lent itself to a commodification of mental health.|$|E
5000|$|The idea of phonosemantics was {{sporadically}} discussed {{during the}} Middle Ages and the Renaissance. In 1690, Locke wrote {{against the idea}} in an essay called [...] "An Essay on Human Understanding". His argument was {{that if there were}} any connection between sounds and ideas, then we would all be speaking the same language, but this is an <b>over-generalisation.</b> Leibniz's book New Essays on Human Understanding published in 1765 contains a point by point critique of Locke's essay. Leibniz picks up on the generalization used by Locke and adopts a less rigid approach: clearly there is no perfect correspondence between words and things, but neither is the relationship completely arbitrary, although he seems vague about what that relationship might be.|$|E
3000|$|... x is (sec[*]x[*]tan[*]x) 2. This error {{may have}} {{originated}} from the algebraic <b>over-generalisation</b> that if a[*]=[*]b, then a [...]...|$|E
40|$|Numerous {{theories}} {{have attempted to}} overcome the anti-essentialist scepticism {{about the possibility of}} defining art. While significant advances have been made in this field, it seems that most modern definitions fail to successfully address the issue of the ever-changing nature of art raised by Morris Weitz, and rarely even attempt to provide an account which would be valid in more than just the modern Western context. This thesis looks at the most successful definitions currently defended, determines their strengths and weaknesses, and offers a new, cultural definition which can preserve the good elements of other theories, solve or avoid their problems, and have a scope wide enough to account for art of different times and cultures. The resulting theory is a synthetic one in that it preserves the essential institutionalism of Dickie's institutional views, is inspired by the historical and functional determination of artistic phenomena present in Levinson's historicism and Beardsley's functionalism, and presents the reasons for something becoming art in a disjunctive form of Gaut's cluster account. Its strengths lie in the ability to account for the changing art-status of objects in various cultures and at various times, providing an explanation of not only what is or was art, but also how and why the concept 'art' changes historically and differs between cultures, and successfully balancing between the <b>over-generalisations</b> of ahistorical and universalist views, and the uninformativeness of relativism. More broadly, the cultural theory stresses the importance of treating art as a historical phenomenon embedded in particular social and cultural settings, and encourages cooperation with other disciplines such as anthropology and history of art...|$|R
40|$|This {{article is}} about how the "SP theory of intelligence" and its {{realisation}} in the "SP machine" (both outlined in the article) may help to solve computer-related problems {{in the design of}} autonomous robots, meaning robots that do not depend on external intelligence or power supplies, are mobile, and are designed to exhibit as much human-like intelligence as possible. The article is about: how to increase the computational and energy efficiency of computers and reduce their bulk; how to achieve human-like versatility in intelligence; and likewise for human-like adaptability in intelligence. The SP system has potential for substantial gains in computational and energy efficiency and reductions in the bulkiness of computers: by reducing the size of data to be processed; by exploiting statistical information that the system gathers; and via an updated version of Donald Hebb's concept of a "cell assembly". Towards human-like versatility in intelligence, the SP system has strengths in unsupervised learning, natural language processing, pattern recognition, information retrieval, several kinds of reasoning, planning, problem solving, and more, with seamless integration amongst structures and functions. The SP system's strengths in unsupervised learning and other aspects of intelligence may help to achieve human-like adaptability in intelligence via: the learning of natural language; learning to see; building 3 D models of objects and of a robot's surroundings; learning regularities in the workings of a robot and in the robot's environment; exploration and play; learning major skills; and secondary forms of learning. Also discussed are: how the SP system may process parallel streams of information; generalisation of knowledge, correction of <b>over-generalisations,</b> and learning from dirty data; how to cut the cost of learning; and reinforcements, motivations, goals, and demonstration...|$|R
40|$|Expansion of Higher Education has {{resulted}} in increasing provision of Access and Foundation programmes, often aimed at mature learners. Adults returning to learn mathematics bring with them a wealth of prior understanding and expectations The two common teaching approaches, remedial 'fill in the gaps' or mythical 'start again', are popular with students but argued to be unrealistic because the teaching of adults is better likened to building on a brownfield site. The purpose {{of this research was}} to consider what understandings adults brought with them and explore how these understandings interacted with new learning. 203 foundation students were given questions, on proportional reasoning, percentage calculation and <b>over-generalisations.</b> Responses, and response hierarchies, were compared with those from children in the 1970 's CSMS survey (Hart, 1981 a). Individual behaviours were then explored through interview using a framework developed from ideas of Schoenfeld (1992) and Leron and Hazzan (1997). It emerged that multiple interactions and choices of behaviour were taking place, indicating final answers, right or wrong, represented only one possibility from a selection of outcomes. Method selection might be influenced by number and beliefs in non-conservation of operation (Greer, 1994) causing potential difficulties for building new learning by method extrapolation. The habit of selt-checkinq and testing for reasonableness might cause difficulties when reasonableness could not be recognised or was counter-intuitive. Other themes identified included: the false recall of certain number calculations with potential for interference with diagnostic practices and the belief in the 'one right method' based on perceived outcome requirements or confidence from previous success, causing reluctance to consider more efficient or appropriate methods. This research highlights the benefits of making processes and choices explicit to teachers and students facilitating the integration of previous understandings with new ways of working without disempowerment and increasing the potential for new learning to be built. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
30|$|Interpretation errors, {{according}} to Olivier (1989), occur when students wrongly interpret a concept due to <b>over-generalisation</b> of the already existing schema.|$|E
3000|$|... x is dy/dx=([...] [...] x tan x)^ 2 [...]. This {{originated}} from the algebraic <b>over-generalisation</b> such as if a[*]=[*]b, then a [...]...|$|E
3000|$|... x is dy/dx= tan x [...]. This error {{originated}} from <b>over-generalisation</b> of the symmetric property, {{which states that}} for any quantities a and b, if a[*]=[*]b, then b[*]=[*]a. This {{is not the case}} in derivatives.|$|E
40|$|The {{manifesto}} of the Norwegian terrorist Anders Behring Breivik {{is based}} on the “Eurabia” conspiracy theory. This theory is a key starting point for hate speech amongst many right-wing extremists in Europe, but also has ramifications beyond these environments. In brief, proponents of the Eurabia theory claim that Muslims are occupying Europe and destroying Western culture, {{with the assistance of the}} EU and European governments. By contrast, members of Al-Qaeda and other extreme Islamists promote the conspiracy theory “the Crusade” in their hate speech directed against the West. Proponents of the latter theory argue that the West is leading a crusade to eradicate Islam and Muslims, a crusade that is similarly facilitated by their governments. This article presents analyses of texts written by right-wing extremists and Muslim extremists in an effort to shed light on how hate speech promulgates conspiracy theories in order to spread hatred and intolerance. The aim of the article is to contribute to a more thorough understanding of hate speech’s nature by applying rhetorical analysis. Rhetorical analysis is chosen because it offers a means of understanding the persuasive power of speech. It is thus a suitable tool to describe how hate speech works to convince and persuade. The concepts from rhetorical theory used in this article are ethos, logos and pathos. The concept of ethos is used to pinpoint factors that contributed to Osama bin Laden's impact, namely factors that lent credibility to his promotion of the conspiracy theory of the Crusade. In particular, Bin Laden projected common sense, good morals and good will towards his audience. He seemed to have coherent and relevant arguments; he appeared to possess moral credibility; and his use of language demonstrated that he wanted the best for his audience. The concept of pathos is used to define hate speech, since hate speech targets its audience's emotions. In hate speech it is the emotions that prevail, rather than reason. Sensational and dramatic claims are used to exploit existing feelings of anger, irritation and fear. The speech is aimed at those who may be persuaded of its negative content, and who may spread the message further. A distinct feature is its absence of logos: argumentation aimed at listeners' reason. To the extent logos is used in hate speech it is for the most part only apparent logos. The speech is often based on falsehoods, exaggerations, stereotypes, <b>over-generalisations,</b> and startling formulations. Hate speech therefore requires an uncritical audience – an audience that is either unable to see through the fallacies, or unwilling to do so because the arguments and conclusions fit well with their worldview. The overall aim of the article is to contribute to a more thorough understanding of hate speech’s nature and its role in disseminating conspiracy theories. However, through analyses of text examples from al-Qaeda’s leader, Osama bin Laden, and right-wing European extremists the article also contributes to explaining the terror attack in Oslo in July 2011 and the terror attack on September 11, 2001, in New York and other similar acts of terror. <br /...|$|R
30|$|A {{consequence}} of aiming to minimise the overall size of G and E (the principle of minimum length encoding) is that G may generalise beyond {{the data in}} I without over-generalising. This means that errors of omission may be corrected in G (via generalisation) but, normally, the system would not introduce new errors via <b>over-generalisation.</b>|$|E
40|$|There is {{increasing}} acknowledgement that teachers’ knowledge for teaching mathematics is multifaceted and topic specific. Given {{the paucity of}} research on the teaching and learning of financial mathematics in general, little can be known about teachers’ knowledge for teaching compound interest. However, since financial mathematics is a component of the school curriculum in South Africa, and an important element of financial literacy more broadly, attention needs to be given to knowledge for teaching financial mathematics, and compound interest in particular. Drawing from a larger study in which the author taught a financial mathematics course to pre-service secondary mathematics teachers, a theoretical elaboration is provided of the underlying mathematics of compound interest, and connections with the world of banking. Based on findings from the study, two key student errors are identified: the <b>over-generalisation</b> of linear thinking in multiplicative scenarios, and the <b>over-generalisation</b> of reversible operations in percentage-change scenarios. Taken together, teachers’ knowledge of relevant mathematics, of the banking context and of learners’ conceptions will contribute to building a knowledge-base for teachers’ knowledge for teaching compound interest...|$|E
3000|$|One student {{differentiated}} [...] y= 4 x^ 1 ex 3 / - 1 ex 2. {{to obtain}} dy/dx= 6 x^ 1 ex 1 / - 1 ex 2. and differentiated [...] y=x^ 1 ex 3 / - 1 ex 2. again to obtain dy/dx= 3 / 2 x^ 1 ex 1 / - 1 ex 2.. This error originates from an <b>over-generalisation</b> of the differentiation of the composite function of trigonometric functions {{which does not}} apply in algebraic terms. For example, to differentiate y[*]=[*]sin[*]x [...]...|$|E
40|$|Abstract: This study {{examines}} {{secondary school students}} ’ understandings and misconceptions when working with logarithms using a specially designed test instrument administered to 81 students in two Singapore schools. Questions were classified by cognitive level. The data were analysed to uncover the kinds of errors made and their possible causes. Students appear capable of doing routine calculations but less capable when answering questions which require higher levels of cognitive thinking. In addition, many errors are not {{due to lack of}} knowledge but appear to be based on <b>over-generalisation</b> of algebraic rules. Suggestions for practice based on these findings are provided...|$|E
40|$|Computer-mediated {{communication}} (CMC) renewed {{the interest}} for collaborative learning. Empirical {{findings show that}} collaborative learning is efficient, but only under some conditions. These conditions are not guaranteed {{by the use of}} CMC tools. It would be an <b>over-generalisation</b> to expect any type of computer-supported collaborative learning (CSCL) to be efficient. It is also difficult to translate the conditions identified in face-to-face collaboration into conditions for success of distance collaboration, because too many other factors separate face-to-face from distance interactions. However, the analysis of these conditions reveals some mechanisms which explain the effects of collaborative learning. This contribution reviews the mechanisms which have been proposed and considers to which extent these mechanisms could be triggered in CSCL. 1...|$|E
40|$|In a {{previous}} paper (Compléments pour une théorie des distorsions cognitives, Journal de Thérapie Comportementale et Cognitive, 2007), we introduced some elements aimed at {{contributing to a}} general theory of cognitive distortions. Based on the reference class, the duality and the system of taxa, these elements allow to define the general cognitive distortions {{as well as the}} specific cognitive distortions. This model is extended here to the description of two other classical cognitive distortions: <b>over-generalisation</b> and mislabelling. The definition of the two latter cognitive distortions is based on preliminary distinction between three levels of reasoning: primary, secondary and ternary pathogenic arguments. The latter analysis also leads to define two other cognitive distortions which insert themselves into this framework: ill-grounded inductive projection and confirmation bias...|$|E
30|$|This study {{investigated}} errors displayed by students registered for Mathematics in Chemical Engineering. This {{was done in}} order to trace causes or origins of the errors displayed to enable the lecturer (researcher) to develop strategies to eliminate the errors in the learning of derivatives of trigonometric functions. Strategies include designing learning activities that may lead students to explore and discuss how they commit these errors. Errors displayed by {{students in this study}} mostly originated from their prior learning of mathematics and <b>over-generalisation</b> of certain mathematical rules. The students’ prior learning had been dominated by rote learning of routines or procedures without their having made sense of these. The findings of this study inform lecturers on how they should prepare lessons to guide students to understand and apply restrictions of certain differentiation rules and formulae.|$|E
40|$|Abstract. We {{address the}} problem of finding the common {{generalisation}} of a set of Haskell function definitions so that each function can be defined by partial application of the generalisation. By analogy with unification, which derives the most general common specialisation of two terms, we aim to infer the least general common generalisation. This problem has a unique solution in a first-order setting, but not in a higher-order language. We define a smallest minimal common generalisation which is unique and consider how it might be used for automated program improvement. The same function can have many definitions; we risk <b>over-generalisation</b> if equality is not recognised. A normalising rewrite system is used before generalisation, so many equivalent definitions become identical. The generalisation system we describe has been implemented in Haskell. ...|$|E
40|$|In a {{previous}} paper (Compléments pour une théorie des distorsions cognitives, Journal de Thérapie Comportementale et Cognitive, 2007), we did present some elements aimed at {{contributing to a}} general theory of cognitive distortions. Based on the reference class, the duality and the system of taxa, these elements led {{to distinguish between the}} general cognitive distortions (dichotomous reasoning, disqualification of one pole, minimisation, maximisation) and the specific cognitive distortions (disqualifying the positive, selective abstraction, catastrophism). By also distinguishing between three levels of reasoning - the instantiation stage, the interpretation stage and the generalisation stage - we did also define two other cognitive distortions: <b>over-generalisation</b> and mislabelling (Théorie des distorsions cognitives : la sur-généralisation et l'étiquetage, Journal de Thérapie Comportementale et Cognitive, 2009). We currently extend this model to another classical cognitive distortion: personalisation...|$|E
40|$|We {{address the}} problem of finding the common {{generalisation}} of a set of Haskell function definitions so that each function can be defined by partial application of the generalisation. By analogy with unification, which derives the most general common specialisation of two terms, we aim to infer the least general common generalisation. This problem has a unique solution in a first-order setting, but not in a higher-order language. We define a smallest minimal common generalisation which is unique and consider how it might be used for automated program improvement. The same function can have many definitions; we risk <b>over-generalisation</b> if equality is not recognised. A normalising rewrite system is used before generalisation, so many equivalent definitions become identical. The generalisation system we describe has been implemented in Haskell. ...|$|E
30|$|Errors {{displayed}} by students were conceptual and procedural; {{there were also}} errors of interpretation and linear extrapolation. Conceptual errors showed a failure to grasp the concepts in a problem and a failure to appreciate the relationships in a problem. Procedural errors occurred when students failed to carry out manipulations or algorithms, even if concepts were understood. Interpretation errors occurred when students wrongly interpreted a concept due to <b>over-generalisation</b> of the existing schema. Linear extrapolation errors occurred when students over-generalised the property f(a[*]+[*]b)[*]=[*]f(a)[*]+[*]f(b), which applies only when f is a linear function, to the form f(a[*]*[*]b)[*]=[*]f(a)[*]*[*]f(b), where f is any function and * any operation. The findings revealed that the participants were not familiar with basic operational signs such as addition, subtraction, multiplication and division of trigonometric functions. The participants demonstrated poor ability to simplify once they had completed differentiation.|$|E
3000|$|... 4 is dy/dx= tan(x^ 4) 4 x^ 3 [...]. This {{originates}} {{from the}} fact that the derivative of y[*]=[*]tan[*]x is dy/dx=^ 2 x [...] and from an <b>over-generalisation</b> of the symmetric property of equality: if a[*]=[*]b, then b[*]=[*]a. This can be interpreted differently; the error might originate if f(x)[*]=[*]tan[*]x then f^'(x)=d/dx(^ 2 x) [...]. One student failed to apply the sum and the chain rule to differentiate y[*]=[*]sin[*] 7 x[*]+[*]ln[*] 5 x; instead, he applied the product rule. This student is not yet at a schema level of APOS Theory regarding application of the sum and chain rule in differentiation of trigonometric functions. Four students differentiated [...] y=tan^ 3 √([...] 7 x) as a product of two functions. As a result, they applied the product rule instead of the chain rule. Their attempts show that they are at a pre-action stage regarding the differentiation of a composite function.|$|E
40|$|For several years, {{scholars}} {{have attempted to}} measure and understand the effects of collaborative learning. This contribution reviews the empirical work concerning {{the conditions under which}} collaborative learning is efficient. We also review the mechanisms that have been proposed to explain the cognitive effects of collaboration. We stress the fact that these findings have been obtained in situations where two or more individuals have to solve a problem together. It would be an <b>over-generalisation</b> to expect similar results by the simple the use of recent Internet-information and communication tools, e. g. for activities where students simply have to talk to each other, without the pressure of reaching a common goal and maintaining some agreement or at least some mutual understanding. 1. INTRODUCTION When one refers to 'distance education', the word 'distance' often sounds as the key word because it implies the use of salient technological tools. Recent widespread tools based on Intern [...] ...|$|E
40|$|This study {{examines}} {{to what extent}} English speakers of L 2 Dutch reconstruct the meanings of placement verbs when moving from a general L 1 verb of caused motion (put) to two specific caused posture verbs (zetten/leggen ‘set/lay’) in the L 2 and whether the existence of low-frequency cognate forms in the L 1 (set/lay) alleviates the reconstruction problem. Evidence from speech and gesture indicates that English speakers have difficulties with the specific verbs in L 2 Dutch, initially looking for means to express general caused motion in L 1 -like fashion through <b>over-generalisation.</b> The gesture data further show that targetlike forms are often used to convey L 1 -like meaning. However, the differentiated use of zetten for vertical placement and dummy verbs (gaan ‘go’ and doen ‘do’) and intransitive posture verbs (zitten/staan/liggen ‘sit, stand, lie’) for horizontal placement, and {{a positive correlation between}} appropriate verb use and target-like gesturing suggest a beginning sensitivity to the semantic parameters of the L 2 verbs and possible reconstruction...|$|E
40|$|Abstract. Classical {{supervised}} learning techniques are generally {{based on an}} inductive mechanism able to generalise a model from a set of positive examples, assuring its consistency {{with respect to a}} set of negative examples. In case of learning from positive evidence only, the problem of <b>over-generalisation</b> comes into account. This paper proposes a general technique for incremental multi-class learning from positive examples only, which has been embedded in the learning system INTHELEX. The idea is to incrementally suppose the positive evidence for a class to be a negative evidence for all other classes until the environment explicitly declares the contrary. An application of the proposed technique to the agent learning domain has been provided. The proposed framework has been used to simulate an agent learning and revising in an incremental way a logical model of a task by imitating skilled agents. In particular, demonstrations are incrementally received and used as training examples while the agent interacts in a stochastic environment. The experimental results prove the validity of the proposed approach on this application domain...|$|E
40|$|It {{has been}} {{recognised}} that many student perspectives on equations {{and their use}} of the equals sign have not mirrored those that mathematicians would like to see in tertiary students. This paper tracks transition of understanding of the equals sign by comparing secondary school students ’ thinking with that of first year university students. We analyse the understanding displayed in terms of properties of the constituent parts of equations, identifying a number of incomplete or pseudo-conceptions that are sometimes influenced by representational aspects of the properties, and other times by apparent <b>over-generalisation</b> of a property. A start is made on constructing a framework for understanding of the mathematical equation object that could assist in the transition from school to tertiary mathematics study. Background Ubiquitous mathematical concepts such as equation, where understanding forms a crucial part of the mathematical experience from early school years right through to tertiary study, need to form part of any discussion of the transition from school to university. While to the experienced mathematical eye equations appea...|$|E
40|$|When {{relearning}} words, {{patients with}} semantic dementia (SD) exhibit a characteristic rigidity, including {{a failure to}} generalise names to untrained exemplars of trained concepts. This {{has been attributed to}} an over-reliance on the medial temporal region which captures information in sparse, non-overlapping and therefore rigid representations. The current study extends previous investigations of SD relearning by re-examining the additional contribution made by the degraded cortical semantic system. The standard relearning protocol was modified by careful selection of foils to show that people with semantic dementia were sometimes able to extend their learning appropriately but that this correct generalisation was minimal (i. e. the patients under-generalised their learning). The revised assessment procedure highlighted the fact that, after relearning, the participants also incorrectly over-generalised the learned label to closely related concepts. It is unlikely that these behaviours would occur if the participants had only formed sparse hippocampal representations. These novel data build on the notion that people with semantic dementia engage both the degraded cortical semantic (neocortex) and the episodic (medial temporal) systems to learn. Because of neocortical damage to the anterior temporal lobes, relearning is disordered with a characteristic pattern of under- and <b>over-generalisation...</b>|$|E
40|$|The major aim of {{my thesis}} {{project has been}} to develop a {{non-human}} primate model of trait anxiety, using a new world monkey, the common marmoset. The {{first step was to}} identify animals high or low in trait anxiety. Based on the findings that high trait-anxious individuals display over-generalization of fear responses, a pathogenic marker of elevated trait anxiety in humans, a new aversive discriminative conditioning paradigm was designed. Testing a normal cohort of marmosets revealed that 26 % of the animals displayed both behavioural and physiological signs of fear generalization, i. e. failure to discriminate safety from danger cues (‘failed’ group). The remaining 74 % showed successful discrimination (‘passed’ group). Additional regression analysis on several behavioural and physiological responses early in training revealed two potential biomarkers of high trait anxiety in marmosets: suppressed baseline blood pressure, indicative of contextual effects, and hyper cue-specific vigilance. These measures predicted the animal’s likelihood of passing or failing the discrimination. The finding that the ‘failed’ group showed intact discriminative performance in the appetitive domain rules out an interpretation of the results in terms of a general impairment in learning, per se. To further determine whether these hypothetically high trait-anxious animals would display enhanced anxiety-related responses in more classical primate models of anxiety, human intruder and rubber snake tests were performed on a large sample of marmosets. Principal component analysis on multiple behavioural measures revealed two components underlying performance: ‘emotionality’ and ‘coping strategy’. Although no difference was found in the human intruder test, the ’failed’ group displayed significantly elevated levels of ‘emotionality’ in comparison to the ‘passed’ group in the rubber snake test. Moreover, the two biomarkers of fear <b>over-generalisation</b> also reliably predicted the ‘emotionality’ scores. Finally, having developed a marmoset model of trait anxiety, investigations into the neural underpinnings, especially prefrontal involvement in trait anxiety mechanisms, were carried out by testing the animals on two cognitive flexibility tests: an orbitofrontal cortex (OFC) -dependent incongruent object discrimination test and a lateral prefrontal cortex (lPFC) -dependent detour reaching rule transfer test. Whilst group differences did not reach significance, the two biomarkers of fear <b>over-generalisation,</b> the suppressed baseline blood pressure and hyper cue-specific vigilance, were inversely and differentially correlated with perseverative performance on the two tests, the lPFC- and OFC-dependent tests, respectively. This not only indicates that high trait anxiety can lead to improvements in certain aspects of prefrontal cognitive function but also suggests that changes in the activity of at least two distinct prefronto-subcortical neural circuits, a cue-sensitive amygdala-OFC and a context-sensitive hippocampus-lPFC circuit, may contribute to trait anxiety...|$|E
40|$|Perceptions of {{intelligence}} based on facial features {{can have a}} profound impact on many social situations, but findings have been mixed as to whether these judgements are accurate. Even if such perceptions were accurate, the underlying mechanism is unclear. Several possibilities have been proposed, including evolutionary explanations where certain morphological facial features are associated with fitness-related traits (including cognitive development), or that intelligence judgements are <b>over-generalisation</b> of cues of transitory states that can influence cognition (e. g., tiredness). Here, we attempt to identify the morphological signals that individuals use to make intelligence judgements from facial photographs. In a genetically informative sample of 1660 twins and their siblings, we measured IQ and also perceptions {{of intelligence}} based on facial photographs. We found that intelligence judgements were associated with both stable morphological facial traits (face height, interpupillary distance, and nose size) and more transitory facial cues (eyelid openness, and mouth curvature). There was a significant association between perceived intelligence and measured IQ, but of the specific facial attributes only interpupillary distance (i. e., wide-set eyes) significantly mediated this relationship. We also found evidence that perceived intelligence and measured IQ share a familial component, though we could not distinguish between genetic and shared environmental sources...|$|E
40|$|Australia has {{a larger}} and more diverse {{immigrant}} population than most Western societies. Australia's immigration history {{is linked to the}} story of family migration as Australia sought immigrants for permanent settlement. However, it is important to aviod <b>over-generalisation</b> when studying immigrant families in Australia today. The main hypothesis is that {{in order to understand the}} immigrant family in Australia today it is necessary to study the intersection of factors such as ethnicity, class, gender and racism. This article approaches the study of Australia's immigrant families by first making the distinction between immigrants from English-speaking countries and those from non-English-speaking countries. It then looks to differences that emerge from a more detailed study of the immigrant family by ethnic origin before exploring the impact of social class on the lives of immigrant families, viewed through the prism of the social construction of immigrants in Australia. The conclusion is that the immigrant family in Australia is an elusive concept, with immigrant families experiencing a wide variety of situations due to particular intersections of ethnicity, class, gender and racism. Nevertheless, immigrant families from non English-speaking backgrounds tend to be relatively disadvantaged - according to the socio-economic indicators - compared to immigrant families from English-speaking countries. ...|$|E
40|$|The {{notion of}} implicature was first {{introduced}} by Grice (1967, 1989), who defined it essentially as what is communicated less what is said. This definition contributed {{in part to the}} proliferation {{of a large number of}} different species of implicature by neo-Griceans. Relevance theorists have responded to this by proposing a shift back to the distinction between explicit and implicit meaning (corresponding to explicature and implicature respectively). However, they appear to have pared down the concept of implicature too much, ignoring phenomena which may be better treated as implicatures in their <b>over-generalisation</b> of the concept of explicature. These problems have their roots in the fact that explicit and implicit meaning intuitively overlap, and thus do not provide a suitable basis for distinguishing implicature from other types of pragmatic phenomena. An alternative conceptualisation of implicature based on the concept of implying with which Grice originally associated his notion of implicature is thus proposed. From this definition it emerges that implicature constitutes something else inferred by the addressee that is not literally said by the speaker. Instead, it is meant in addition to what the literally speaker says, and consequently, it is defeasible like all other types of pragmatic phenomena. Full Tex...|$|E
40|$|This is {{the third}} {{in a series of}} special issues on {{tertiary}} education and academic life (Saravanamuthu and Filling, 2004; Saravanamuthu and Tinker, 2002). This issue touches on two aspects of ethical education. The first section engages with the dilemma of educating (overseas) Chinese students in universities in the UK, USA, Australia, Canada, New Zealand and parts of Western Europe: the Chinese Learner. The second part of the issue reviews ethics in education in the post-Enron environment. The genesis of Chinese Learner research is often attributed to John Biggs and David Watkins, University of Hong Kong. John has long retired, and David continues to lead the research through his doctoral students. Even though Chinese Learner studies originated in the Educational Psychology enclave, it has now permeated into other discipline areas. Theories of learning methods and techniques are inherently contestable, even before considering their cultural complications. The Chinese Learner literature touches on a number of problematic issues, beginning with the identity of this student. Any attempt to associate the Chinese Learner with a specific culture, nation, personality trait and/or learning approach immediately lends itself to criticisms of <b>over-generalisation</b> and stereotyping. This issue engages with the multi-layered realities of the Chinese student market, which the Corporate University (following Saravanamuthu and Tinker, 2002) plays down as it eagerly collects full-fees for its depleted coffers...|$|E
40|$|AbstractAnomia therapy {{typically}} aims {{to improve}} patients' communication ability through targeted practice in naming {{a set of}} particular items. For such interventions to be of maximum benefit, the use of trained (or relearned) vocabulary must generalise from the therapy setting into novel situations. We investigated relearning in three patients with semantic dementia, a condition that {{has been associated with}} poor generalisation of relearned vocabulary. We tested two manipulations designed to improve generalisation of relearned words by introducing greater variation into the learning experience. In the first study, we found that trained items were retained more successfully when they were presented in a variety of different sequences during learning. In the second study, we found that training items using a range of different pictured exemplars improved the patients' ability to generalise words to novel instances of the same object. However, in one patient this came at the cost of inappropriate over-generalisations, in which trained words were incorrectly used to name semantically or visually similar objects. We propose that more variable learning experiences benefit patients because they shift responsibility for learning away from the inflexible hippocampal learning system and towards the semantic system. The success of this approach therefore depends critically on the integrity of the semantic representations of the items being trained. Patients with naming impairments in the context of relatively mild comprehension deficits are most likely to benefit from this approach, while avoiding the negative consequences of <b>over-generalisation...</b>|$|E
40|$|Historians of English {{crime and}} {{criminal}} justice agree that females are more leniently treated by the criminal justice system. Fewer females are prosecuted for unlawful activities, and, when they are, they are more readily acquitted, or receive lighter sentences than males. However, reasons for this remain elusive. References to the paternalism {{of those involved in}} the system, together with notions about masculinity and femininity in a patriarchally ordered society, have been offered in the absence of other more focused and systematic evidence.;This thesis follows a systematic enquiry about three crimes which attributed the death sentence - shoplifting, pickpocketing, and uttering forged Bank of England notes. The period of the study covers the 1780 s to the 1830 s, and is centred on London and Middlesex. It considers involvement in each crime by gender. The approach seeks to avoid the <b>over-generalisation</b> resulting from synthesis of statistics {{for a wide variety of}} offences, and to allow a clearer view of how men and women operated in committing offences. This systematic approach follows the offenders involved in the three crimes through the criminal justice system, so far as it is possible to do so, since the public trial and sentencing at the Old Bailey were not the end of the decision-making story. Previous studies have largely neglected to follow-through to the stage of commutation of sentences and pardons where influences on the decision-makers differed from those on decision-makers at earlier stages of the system.;In particular, this thesis focuses on the gendered context of the specific behaviour of male and female offenders in the selected offences, on the effects of a patriarchal system of justice, and on the needs of the State to make political decisions about the disposal of offenders...|$|E
40|$|Thesis (MScEng (Electrical and Electronic Engineering)) [...] University of Stellenbosch, 2009. Statistical speech {{recognition}} systems typically utilise {{a set of}} statistical models of subword units based {{on the set of}} phonemes in a target language. However, in continuous speech {{it is important to consider}} co-articulation e ects and the interactions between neighbouring sounds, as <b>over-generalisation</b> of the phonetic models can negatively a ect system accuracy. Traditionally co-articulation in continuous speech is handled by incorporating contextual information into the subword model by means of context-dependent models, which exponentially increase the number of subword models. In contrast, transitional models aim to handle co-articulation by modelling the interphone dynamics found in the transitions between phonemes. This research aimed to perform an objective analysis of diphones as subword units for use in hidden Markov model-based continuous-{{speech recognition}} systems, with special emphasis on a direct comparison to a context-dependent biphone-based system in terms of complexity, accuracy and computational e ciency in similar parametric conditions. To simulate practical conditions, the experiments were designed to evaluate these systems in a low resource environment { limited supply of training data, computing power and system memory { while still attempting fast, accurate phoneme recognition. Adaptation techniques designed to exploit characteristics inherent in diphones, as well as techniques used for e ective parameter estimation and state-level tying were used to reduce resource requirements while simultaneously increasing parameter reliability. These techniques include diphthong splitting, utilisation of a basic diphone grammar, diphone set completion, maximum a posteriori estimation and decision-tree based state clustering algorithms. The experiments were designed to evaluate the contribution of each adaptation technique individually and subsequently compare the optimised diphone-based recognition system to a biphone-based recognition system that received similar treatment. Results showed that diphone-based recognition systems perform better than both traditional phoneme-based systems and context-dependent biphone-based systems when evaluated in similar parametric conditions. Therefore, diphones are e ective subword units, which carry suprasegmental knowledge of speech signals and provide an excellent compromise between detailed co-articulation modelling and acceptable system performanc...|$|E
40|$|This {{is a brief}} {{report on}} Andrey Mokhov’s visit to Josep Carmona’s {{research}} lab in Uni-versitat Politècnica de Catalunya. The report summarises potential benefits that Parameterised Graphs (PGs) [1] can bring to the domain of process mining [2] and outlines important challenges {{to be addressed in}} future research. The authors intend to elaborate on the presented ideas in follow-up peer reviewed publications. Andrey Mokhov, Josep Carmona: Process Mining using Parameterised Graphs 1 Motivation for using PGs in process mining In this section we discuss key reasons that motivate us to study the application of PGs in process mining, namely: (i) their ability to compactly represent complex behaviours without excessive <b>over-generalisation,</b> and (ii) the possibility of capturing event log meta data as part of PG representations and taking advantage of the data for the purpose of explaining the processes under observation. Note that in this report we work with PGs whose equivalence is considered up to the transitive closure. Such PGs have historically been studied under the name of Conditional Partial Order Graphs [3]. 1. 1 Compact representation We have performed a number of experiments with state-of-the-art process mining software and found out that the existing solutions often produce too general solutions for event logs corresponding to processes with non-trivial mix of concurrency, causilty and choice. One example of such an event log is L = {abcd,cdab,badc,dcba}. One can notice that in this example the order between events a and b always coincides with the order between events c and d, which we consider an important piece of information about the process. The existing process mining methods, however, fail to capture this information and produce the most general explanation for the process in which all four events are concurrent. x x x_ x_ y y y y__ (a) PG representation x = 1 y = 1 x = 1 y = 0 x = 0 y = 1 x = 0 y = 0 (b) Four projections of the P...|$|E
