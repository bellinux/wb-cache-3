1475|10000|Public
25|$|Confidence limits can {{be found}} if the {{probability}} distribution of the parameters is known, or an asymptotic approximation is made, or assumed. Likewise statistical tests on the residuals can be made if the probability distribution of the residuals is known or assumed. The probability distribution of any linear combination of the dependent variables can be derived if the probability distribution of experimental errors is known or assumed. Inference is particularly straightforward if the errors are assumed to follow a normal distribution, which implies that the parameter estimates and residuals will also be normally distributed conditional on the values <b>of</b> <b>the</b> <b>independent</b> <b>variables.</b>|$|E
2500|$|... {{involving}} the partial derivative of y with respect tox1. [...] The {{sum of the}} partial differentials with respect to all <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> is the total differential ...|$|E
2500|$|... "12. Algorithmic theories... In {{setting up}} a {{complete}} algorithmic theory, {{what we do is}} to describe a procedure, performable for each set of values <b>of</b> <b>the</b> <b>independent</b> <b>variables,</b> which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, [...] "yes" [...] or [...] "no," [...] to the question, [...] "is the predicate value true?"" [...] (Kleene 1943:273) ...|$|E
40|$|Establishing a {{functional}} relationship between <b>the</b> <b>independent</b> and <b>the</b> dependent <b>variable</b> is <b>the</b> primary focus <b>of</b> applied behavior analysis. Accurate and reliable description and observation <b>of</b> both <b>the</b> <b>independent</b> and dependent <b>variables</b> {{are necessary to}} achieve this goal. Although considerable attention {{has been focused on}} ensuring <b>the</b> integrity <b>of</b> <b>the</b> dependent variable in the operant literature, similar effort has not been directed at ensuring <b>the</b> integrity <b>of</b> <b>the</b> <b>independent</b> <b>variable.</b> Inaccurate descriptions <b>of</b> <b>the</b> application <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> may threaten <b>the</b> reliability and validity of operant research data. A survey <b>of</b> articles in <b>the</b> Journal <b>of</b> Applied Behavior Analysis demonstrated that <b>the</b> majority <b>of</b> articles published do not use any assessment <b>of</b> <b>the</b> actual occurrence <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> and a sizable minority do not provide operational definitions <b>of</b> <b>the</b> <b>independent</b> <b>variable.</b> <b>The</b> feasibility and utility <b>of</b> ensuring <b>the</b> integrity <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> is described...|$|R
2500|$|In {{engineering}} and science, one often {{has a number}} of data points, obtained by sampling or experimentation, which represent <b>the</b> values <b>of</b> a function for a limited number <b>of</b> values <b>of</b> <b>the</b> <b>independent</b> <b>variable.</b> It is often required to interpolate (i.e., estimate) <b>the</b> value <b>of</b> that function for an intermediate value <b>of</b> <b>the</b> <b>independent</b> <b>variable.</b>|$|R
30|$|B is <b>the</b> {{coefficient}} <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> <b>of</b> {{population of}} Thailand.|$|R
2500|$|There are m {{observations}} in y and n parameters in β with m>n. X is a m×n matrix whose elements are either constants or functions <b>of</b> <b>the</b> <b>independent</b> <b>variables,</b> x. The weight matrix W is, ideally, the inverse of the variance-covariance matrix [...] of the observations y. The independent variables {{are assumed to}} be error-free. The parameter estimates are found by setting the gradient equations to zero, which results in the normal equations ...|$|E
2500|$|However, if {{the errors}} are not {{normally}} distributed, a {{central limit theorem}} often nonetheless implies that the parameter estimates will be approximately normally distributed {{so long as the}} sample is reasonably large. [...] For this reason, given the important property that the error mean is independent <b>of</b> <b>the</b> <b>independent</b> <b>variables,</b> the distribution of the error term is not an important issue in regression analysis. [...] Specifically, it is not typically important whether the error term follows a normal distribution.|$|E
2500|$|When {{performing}} statistical analysis, {{independent variables}} that affect a particular dependent variable {{are said to}} be orthogonal if they are uncorrelated, since the covariance forms an inner product. In this case the same results are obtained for the effect of any <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> upon the dependent variable, regardless of whether one models the effects of the variables [...] individually with simple regression or simultaneously with multiple regression. If correlation is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the expected value (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).|$|E
5000|$|... is <b>the</b> sum <b>of</b> <b>the</b> non-modal {{frequencies}} {{for each}} value <b>of</b> <b>the</b> <b>independent</b> <b>variable.</b>|$|R
30|$|We {{start with}} a change <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> in (3), which will be useful.|$|R
5000|$|Boundary value {{problems}} {{are similar to}} initial value problems. A boundary value problem has conditions specified at <b>the</b> extremes ("boundaries") <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> in <b>the</b> equation whereas an initial value problem has all <b>of</b> <b>the</b> conditions specified at <b>the</b> same value <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> (and that value is at <b>the</b> lower boundary <b>of</b> <b>the</b> domain, thus the term [...] "initial" [...] value).|$|R
5000|$|... a {{variable}} {{omitted from the}} model may {{have a relationship with}} both the dependent variable and one or more <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> (omitted-variable bias); ...|$|E
5000|$|... {{involving}} the partial derivative of y {{with respect to}} x1. The sum of the partial differentials with respect to all <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> is the total differential ...|$|E
5000|$|... where [...] is any <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> of the problem, but {{has to be}} {{selected}} in advance.Take for example the patternWith the time as the preselected exponential factor.|$|E
50|$|Consider <b>the</b> {{following}} set <b>of</b> {{data for}} y obtained {{at various levels}} <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> x.|$|R
3000|$|When all <b>of</b> <b>the</b> {{conditions}} hold in {{the predicted}} direction, <b>the</b> effect <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> (s [...]...|$|R
2500|$|... x, y, and z are all {{functions}} <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> t which ranges {{over the}} real numbers.|$|R
5000|$|... where [...] is the Minkowski metric. The tetrads encode the {{information}} about the space-time metric and will be taken as one <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> in the action principle.|$|E
5000|$|... where y is a vector of {{dependent}} variable observations, and X is a matrix each of whose columns is {{a column of}} observations on one <b>of</b> <b>the</b> <b>independent</b> <b>variables.</b> The resulting estimator is ...|$|E
50|$|High-leverage {{points are}} those observations, if any, made at extreme or {{outlying}} values <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> {{such that the}} lack of neighboring observations means that the fitted regression model will pass close to that particular observation.|$|E
5000|$|... {{underlying}} {{values are}} quantified <b>the</b> ratio <b>of</b> <b>the</b> change <b>of</b> a dependent <b>variable</b> to that <b>of</b> <b>the</b> <b>independent</b> <b>variable.</b>|$|R
2500|$|Manipulations <b>of</b> <b>the</b> <b>in{{dependent}}</b> <b>variable</b> so {{that its}} effects on the dependent variable may be quantitatively or qualitatively analyzed ...|$|R
3000|$|... are the coded {{value and}} actual value <b>of</b> <b>the</b> <b>independent</b> <b>variable,</b> respectively, Α 0 is <b>the</b> actual value <b>of</b> <b>the</b> Α [...]...|$|R
5000|$|For {{a surface}} S given {{explicitly}} {{as a function}} [...] <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> [...] (e.g., [...] ), its normal {{can be found in}} at least two equivalent ways.The first one is obtaining its implicit form , from which the normal follows readily as the gradient ...|$|E
5000|$|If [...] is unobserved, and {{correlated}} {{with at least}} one <b>of</b> <b>the</b> <b>independent</b> <b>variables,</b> then it will cause omitted variable bias in a standard OLS regression. However, panel data methods, such as the fixed effects estimator or alternatively, the First-difference estimator can be used to control for it.|$|E
5000|$|It is {{generally}} advised [...] that when performing extrapolation, one should accompany the estimated {{value of the}} dependent variable with a prediction interval that represents the uncertainty. Such intervals tend to expand rapidly as the values <b>of</b> <b>the</b> <b>independent</b> <b>variable(s)</b> moved outside the range covered by the observed data.|$|E
5000|$|The {{variance}} in <b>the</b> prediction <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> {{as a function}} <b>of</b> <b>the</b> dependent variable is given in polynomial least squares ...|$|R
3000|$|... is {{dimensional}} coded value <b>of</b> <b>the</b> <b>independent</b> <b>variable,</b> xi is <b>the</b> {{real value}} <b>of</b> this variable at this coded value, x [...]...|$|R
5000|$|In {{probability}} theory and statistics, Goodman & Kruskal's lambda ('''''') {{is a measure}} of proportional reduction in error in cross tabulation analysis. For any sample with a nominal <b>independent</b> <b>variable</b> and dependent variable (or ones that can be treated nominally), it indicates {{the extent to which the}} modal categories and frequencies for each value <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> differ from <b>the</b> overall modal category and frequency, i.e. for all values <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> together. [...] can be calculated with the equation ...|$|R
50|$|In {{statistics}} and data analysis, backcasting {{can be considered}} the opposite of forecasting. Whereas forecasting is predicting the future (unknown) values of the dependent variables based on known values of the independent variable, backcasting can be considered the prediction of the unknown values <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> that might have existed to explain the known values of the dependent variable.|$|E
50|$|The {{coefficient}} of multiple correlation takes values between 0 and 1; a higher value indicates a better predictability {{of the dependent}} variable from the independent variables, with a value of 1 indicating that the predictions are exactly correct and a value of 0 indicating that no linear combination <b>of</b> <b>the</b> <b>independent</b> <b>variables</b> is a better predictor than is the fixed mean of the dependent variable.|$|E
50|$|In statistics, the Breusch-Pagan test, {{developed}} in 1979 by Trevor Breusch and Adrian Pagan, {{is used to}} test for heteroskedasticity in a linear regression model. It was independently suggested with some extension by R. Dennis Cook and Sanford Weisberg in 1983. It tests whether the variance of the errors from a regression {{is dependent on the}} values <b>of</b> <b>the</b> <b>independent</b> <b>variables.</b> In that case, heteroskedasticity is present.|$|E
2500|$|The {{variance}} in <b>the</b> prediction <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> {{as a function}} <b>of</b> <b>the</b> dependent variable is given in the article Polynomial least squares ...|$|R
3000|$|... or in words: {{expected}} {{value of a}} concave real function (e.g. lnx) is less than <b>the</b> same function <b>of</b> <b>the</b> <b>independent</b> <b>variable’s</b> {{expected value}}.|$|R
2500|$|Moreover, given <b>the</b> {{total number}} <b>of</b> {{observations}} N, <b>the</b> number <b>of</b> levels <b>of</b> <b>the</b> <b>independent</b> <b>variable</b> n, and <b>the</b> number <b>of</b> parameters in <b>the</b> model p: ...|$|R
