14|39|Public
40|$|Today the Internet is a {{valuable}} source of information {{as well as a}} powerful communication medium, with undoubted social and economic benefits, however it also poses some security risks. Virsuses may hide in email attachments or in appartently innocent applications directly downloadable from the Internet. In this work we give a brief overview of virus types and main defense techniques. Then we present statistical data of virus attacks revealed by an anti-virus SW activated on <b>our</b> <b>e-mail</b> server, and discuss results in terms of virus types and temporal distribution...|$|E
40|$|While we sleep, or {{attend to}} other issues, SAS can run some routine {{processing}} for us. A SAS job can be instructed {{to send an}} e–mail to confirm its completion, to provide results, or to warn of particular issues. At some suitable time we can then read <b>our</b> <b>e–mail</b> for confirmation of the process, for information that is distilled from the process, or for data that requires verification. If our process handles 1, 000 records daily, this e–mail process can help us to save time by drawing our attention to only the problems in that data. This paper examines one such process {{as an example of}} “e–mailing exceptions”. We look at a number of ways to send e–mail from the SAS process. The methods include a very simple approach, a generic macro approach, a batch e–mail process, and reporting exceptions from, of all things, <b>our</b> <b>e–mail</b> server. For SAS ® 8. 2 and later, the techniques work on most computer systems, although we touch on the use of “sendmail ” functionality on UNIX from SAS sessions that are running SAS ® 6. 12 or that have tighter security settings. We also touch on similar processes for mainframes that are running earlier SAS versions or that have security constraints. The product that we use is Base SAS®, and the code might be less familiar, but it is well within the reach of all levels of SAS users...|$|E
40|$|Abstract. Proposed {{evolutionary}} model provides a powerful means of rule discovery {{in the field}} of text categorization. In present paper, the model focuses on the particular problem of preventing spam to enter <b>our</b> <b>e-mail</b> accounts. Spam blockers that have been provided by Internet companies so far are not very effective. An application built through the means of our model learns the approximate difference between spam and non-spam mail and labels incoming new mail efficiently. The rules that led to the membership of training mails to either spam or non-spam category are discovered, having been evolved through the principles of a special evolutionary metaheuristics, genetic chromodynamics. Resulting rules ar...|$|E
5000|$|On December 2, 2010, Dowd penned {{an opinion}} {{piece in the}} National Journal {{defending}} Wikileaks, writing that, [...] "Republicans and Democrats seem {{to agree on a}} few things: That the government, in the name of fighting terrorism, has the right to listen in on all of our phone conversations and read <b>our</b> <b>e-mails,</b> even if it has no compelling reason for doing so." ...|$|R
2500|$|On July 5, 2015, the Twitter {{account of}} the company was {{compromised}} by an unknown individual who published an announcement of a data breach against HackingTeam's computer systems. The initial message read, [...] "Since {{we have nothing to}} hide, we're publishing all <b>our</b> <b>e-mails,</b> files, and source code …" [...] and provided links to over 400 gigabytes of data, including alleged internal e-mails, invoices, and source code; which were leaked via BitTorrent and Mega. [...] An announcement of the data breach, including a link to the bittorrent seed, was retweeted by WikiLeaks and by many others through social media.|$|R
40|$|We {{live life}} in the network. We check <b>our</b> <b>e-mails</b> regularly, make mobile phone calls from almost any location, swipe transit cards to use public transportation, and make {{purchases}} with credit cards. Our movements in public places may be captured by video cameras, and our medical records stored as digital files. We may post blog entries accessible to anyone, or maintain friendships through online social networks. Each of these transactions leaves digital traces that can be compiled into comprehensive pictures of both individual and group behavior, {{with the potential to}} transform our understanding of our lives, organizations, and societies...|$|R
40|$|We {{introduce}} {{and study}} A-infinity persistence {{of a given}} homology filtration of topological spaces. This is a family, one for each n > 0, of homological invariants which provide information not readily available by the (persistent) Betti numbers of the given filtration. This may help to detect noise, {{not just in the}} simplicial structure of the filtration but in further geometrical properties in which the higher codiagonals of the A-infinity structure are translated. Based in the classification of zigzag modules, a characterization of the A-infinity persistence in terms of its associated barcode is given. Comment: 22 pages, no figures. In versions 2 and 3, we added <b>our</b> <b>e-mail</b> addresses and made some minor corrections, thanks to Jim Stashef...|$|E
40|$|We {{live our}} lives in digital networks. We {{wake up in the}} morning, check <b>our</b> <b>e-mail,</b> make a quick phone call, commute to work, buy lunch. Many of these {{transactions}} leave digital breadcrumbs – tiny records of our daily experiences, as illustrated in Figure 1. Reality mining, which pulls together these crumbs using statistical analysis and machine learning methods, offers an increasingly comprehensive picture of our lives, both individually and collectively, with the potential of transforming our understanding of ourselves, our organizations, and our society in a fashion that was barely conceivable just a few years ago. It {{is for this reason that}} reality mining was recently identified by Technology Review as one of “ 10 emerging technologies that could change the world ” (Technology Review, Apri...|$|E
40|$|Abstract. We {{address the}} problem of using {{semantic}} technology for capturing informal organizational structure by means of a light-weight ontology. The idea is to support knowledge management in analyzing communication between people in an organization. As an example we use social network of a mid size research institution obtained based on e-mail communication. We propose an approach consisting of five major steps that enable transformation of the data from a given e-mail transactions recordings to a light-weight ontology estimating structure of the organization. The experimental evaluation shows that on <b>our</b> <b>e-mail</b> transaction data the obtained organizational structure closely corresponds to the formal organizational structure. We conclude that the proposed approach is useful and applicable in real life situations where the goal is to model social structures based on communication records...|$|E
5000|$|On July 5, 2015, the Twitter {{account of}} the company was {{compromised}} by an unknown individual who published an announcement of a data breach against HackingTeam's computer systems. The initial message read, [...] "Since {{we have nothing to}} hide, we're publishing all <b>our</b> <b>e-mails,</b> files, and source code …" [...] and provided links to over 400 gigabytes of data, including alleged internal e-mails, invoices, and source code; which were leaked via BitTorrent and Mega. [...] An announcement of the data breach, including a link to the bittorrent seed, was retweeted by WikiLeaks and by many others through social media.|$|R
40|$|Many {{people have}} contributed to the final result, and {{we would like to thank}} all {{contributors}} for coping with <b>our</b> questions, <b>e-mails</b> and phone calls. Special thanks to our supervisors Bertil I Nilsson at Lund University and Trygve Engelbert at EuroMaint Rail, without whom this thesis would not have been possible...|$|R
40|$|Many {{randomized}} algorithms M in {{the literature}} have the following features: M may produce different valid outputs for different random strings, may output erroneous values, and/or may fail to give any output at all. This paper formalizes and studies these features, and compares the probabilistic function classes thus defined to language classes such as BPP, RP, and ZPP. The two main problems we study are whether the distribution of outputs can be skewed in favor of one valid value, and whether the probability that M behaves correctly can be amplified. We show that if a certain symmetry between two values in fully-polynomial randomized approximation schemes can be broken, then {{the answer to the}} former is yes, and we prove many cases in which the answer to the latter is no. Note: This paper was originally submitted to the Complexity 1994 conference, before the first author adopted “Ogihara ” as his Roman spelling. <b>Our</b> <b>e-mails</b> are no...|$|R
40|$|Abstract: In {{this paper}} we propose an {{artificial}} intelligent approach focusing information filtering problem. First, we give {{an overview of the}} information filtering process and a survey of different models of textual information filtering. Second, we present <b>our</b> <b>E-mail</b> filtering tool. It consists of an expert system in charge of driving the filtering process in cooperation with a knowledge-based model. Neural networks are used to model all system knowledge. The system is based on machine learning techniques to continuously learn and improve its knowledge all along its life cycle. This email filtering tool assists the user in managing, selecting, classify and discarding non-desirable messages in a professional or non-professional context. The modular structure makes it portable and easy to adapt to other filtering applications such as web browsing. The performance of the system is discussed...|$|E
40|$|We {{evaluate}} {{a number of}} possible enhancements to the FCC auctions. We consider only changes to the current auction rules that stay within the basic format of the simultaneous multiple round auction for individual licenses. This report summarizes and extends <b>our</b> <b>e-mail</b> exchanges with FCC staff on this topic. A subsequent report will cover auctions with combination bids. Overall, the FCC spectrum auctions have been an enormous success. However, there are two design goals in the auction where important improvement can be achieved within the basic rules structure. These are restricting collusion among bidders and reducing the time taken to complete the auction. This report focuses on enhancements that help to achieve these two goals. Some of the suggested changes also streamline the auction process so large auctions can be conducted more quickly without sacrificing efficiency. Auctions; Spectrum Auctions; Multiple-Round Auctions; Efficiency...|$|E
40|$|The {{designing}} {{of system}} utterances {{is a very}} crucial part of speech user interfaces, especially if speech synthesis is used. Although speech synthesis is quite intelligible in well formed and simple sentences {{it is very difficult}} for users to understand when complex structural elements like tables are spoken. Furthermore, most users do not like the way synthesizers use prosody. Most of the previous research has focused on what information should be presented to the user. Recent research has also brought up the question of how this information should be presented. In order to improve intelligibility and naturalness of synthetic speech we arranged an experiment {{to find new ways to}} use prosody. In our experiment subjects listened to three human readers and a speech synthesizer reading system utterances from <b>our</b> <b>e-mail</b> system. Questions were asked to measure how well the utterances were understood. Subjective evaluations of the voices were also collected. We used the results to find those [...] ...|$|E
40|$|Without the {{cooperation}} of state financial aid administrators, this study would have nothing to report. We {{thank all of you}} who completed surveys and who withstood <b>our</b> many <b>e-mails</b> and telephone calls. We hope you find our results helpful in working with your state programs. The comments of two reviewers helped take us out of the trees to see the forest. Jane Wellman from the Institute fo...|$|R
40|$|We {{live life}} in the network. We check <b>our</b> <b>e-mails</b> regularly, make mobile phone calls from almost any {{location}} … make purchases with credit cards … [and] maintain friendships through online social networks. … These transactions leave digital traces that can be compiled into compre-hensive pictures of both individual and group behavior, {{with the potential to}} transform our understanding of our lives, organizations, and societies. —Lazer et al. (2009, 721). Powerful computational resources combined with the availability of massive social media data-sets has given rise to a growing body of work that uses a combination of machine learning, natural language processing, network analysis, and statis-tics for the measurement of population structure and human behavior at unprecedented scale. however, mounting evidence suggests that many of the forecasts and analyses being produced misrepresent the real world. —Ruths and Pfeffer (2014, 1063) The exponential growth in “the volume, velocity and variability ” (Dumbill 2012, 2) of structured and unstructured social data has confronted fields such as political science, soci-ology, psychology, information systems, public health, public policy, and communication with a unique challenge: how can scientists best use computational tools to analyze such data, prob-lematical as they may be, with the goal of understanding individuals and their interac-tions within social systems? The unprecedented availability of information on discrete behav...|$|R
40|$|Notes: Center Discussion Papers are {{preliminary}} materials circulated {{to stimulate}} discussions and critical comments. We thank the Green Bank of Caraga for implementing this experiment, John Owens and the USAID/Philippines Microenterprise Access to Banking Services Program team for facilitating {{this and other}} experiments with the Green Bank, and Chona Echavez from Xavier University for coordination of the household surveys. We thank two anonymous referees and the editor, John List, for useful comments. We thank Tomoko Harigaya for excellent research assistance. We thank the National Science Foundation (SGER SES- 0313877) for funding support. All views opinions and errors are <b>our</b> own. <b>E-mails...</b>|$|R
40|$|We {{present a}} lexical {{platform}} {{that has been}} developed for the Spanish language. It achieves portability between different computer systems and efficiency, in terms of speed and lexical coverage. A model for the full treatment of Spanish inflectional morphology for verbs, nouns and adjectives is presented. This model permits word formation based solely on morpheme concatenation, driven by a feature-based unification grammar. The run-time lexicon {{is a collection of}} allomorphs for both stems and endings. Although not tested, it should be suitable also for other Romance and highly inflected languages. A formalism is also described for encoding a lemma-based lexical source, well suited for expressing linguistic generalizations: inheritance classes, lemma encoding, morpho-graphemic allomorphy rules and limited type-checking. From this source base, we can automatically generate an allomorph indexed dictionary adequate for efficient retrieval and processing. A set of software tools has been implemented around this formalism: lexical base augmenting aids, lexical compilers to build run-time dictionaries and access libraries for them, feature manipulation libraries, unification and pseudo-unification modules, morphological processors, a parsing system, etc. Software interfaces among the different modules and tools are cleanly defined to ease software integration and tool combination in a flexible way. Directions for accessing <b>our</b> <b>e-mail</b> and web demonstration prototypes are also provided. Some figures are given, showing the lexical coverage of our platform compared to some popular spelling checkers...|$|E
40|$|Network {{communications}} {{represent an}} easy {{means for the}} soread viruses. Internet uses are constantly threatened by the spread of new viruses hidden in applealing objects such as jokes, games, chats, and e-mails ostensibly sent by friends. Although e-mail and www systems represent the main "open doors", floppy and CD disk are still minor "contributors". The damage provoked by infections can be very costrly for an organization 2 ̆ 7 s time and resources and can become critical when it affects sensitive systems and data. A basic rule in computer security is to make frequent backups to avoid any kind of data destruction or corruption. Various countermeasures can be appplied to prevent infections, such as activating anti-virus SW, or setting filter rules on the e-mail server in order to discard dangerous files present in the message. Another important factor is the awareness of the problem: users must become familiar with both risks and elementary defense techniques (i. e. do not open executable files, disable automated macro activation, etc.). In this work we present data statistics of virus attacks revealed by an anti-virus SW activated on <b>our</b> <b>e-mail</b> server and discuss results in terms of virus types and temporal distribution. The work in organized into two parts. The first part includes {{a brief overview of}} virus types (parasitic, macro, etc.) and main defense techniques; the second shows the results of our experimentation...|$|E
40|$|No {{doubt about}} it, library users {{are feeling the}} weight and stress of {{information}} overload. In addition to the sheer enormity {{of the amount of}} information "out there," the tools used to archive, categorize, and access. information are becoming increasingly complex. No group understands this overload better than those of us who work in libraries. Like our users, we struggle daily to keep up with our reading, wade through all <b>our</b> <b>e-mail,</b> and identify and learn to use new sources of information, and new access tools. Unlike our users, however, we are obligated by dejinition, to provide what I call "information guidance" - the best access to quality information. Library staff members must proactively rise to the challenge and must provide guidance through the infomation glut. I watch the users in our library. What do they state that they need? What do they need that they do not know they need? How can we use new technologies to improve their access to information? How can we best point them toward the most accurate, timely, and useful information? As the complexity of the tools increases, the need for training {{in the use of the}} tools increases. If we are to guide, we must educate. In the case of the library where I work, the Clarian Health Partners Medical Library, we are part of the Educational Services Department, and so, specifically charged with educating users...|$|E
40|$|Most of today’s e-mail tools do {{not address}} the main problem {{experienced}} by people who {{get a lot of}} e-mail messages: information overload. This project aims to propose an applicable solution to this problem using database technology. During this work we develop a set of deferred classes which defines the main components of <b>our</b> proposed <b>e-mail</b> client. Since the proposed tool should deal with a huge amount of data we payed particular attention to performance issues. We develop a search algorithm which, according to our test, achieves a performance comparable with the fastest today’s tools. We believe that the object-oriented design and the algorithm described here are possible steps towards solving the information overload problem in the e-mai...|$|R
40|$|We came to {{collaborative}} autoethnography quite by accident. In this methodological paper, {{we consider}} our experiences as we embraced a new methodology, taught and researched collaboratively in an interdisciplinary space, and grappled with {{how we might}} nestle our work in a journal with no history of publishing autoethnographies—all while becoming awakened to critiques against and arguments for autoethnographic research. Our discussions are presented along with portions of <b>our</b> lengthy <b>e-mail</b> correspondences written during our research process and center on two prominent facets of our research experience: interdisciplinarity and the research process. Entangled in our methodological unpacking, we highlight “Productive Tensions” that emerged from both our collaboration and reviewer feedback that is presented alongside our discussion. Through seeing these tensions as productive, we argue that embracing diverse perspectives can serve to strengthen the depth of engagement, quality, and potential impact of (collaborative) autoethnographic research...|$|R
40|$|Something {{has gone}} wrong in modem America, argues Jeffrey Rosen in The Unwanted Gaze. Our medical records are bought and sold by health care providers, drug companies, and the {{insurance}} industry. <b>Our</b> <b>e-mails</b> are intercepted and read by our employers. Amazon. com knows {{everything there is to}} know about our reading and web-browsing habits. Poor Monica Lewinsky 2 ̆ 7 s draft love letters to President Bill Clinton were seized by the villainous Ken Starr, and ultimately plastered all over the nation 2 ̆ 7 s newspapers. To Rosen, the nature of the problem is clear: These examples are all part of a troubling 2 ̆ 2 phenomenon that affects all Americans: namely, the erosion of privacy at home, at work, and in cyberspace, so that intimate personal information [...] . is increasingly vulnerable to being wrenched out of context and exposed to the world. 2 ̆ 2 Rosen is, of course, hardly unusual in viewing all these issues as quintessential privacy violations. In the past few years the media seem to have woken up to privacy issues, and most of us have been sympathetic readers of dozens of popular articles addressing just such a range of 2 ̆ 2 privacy violations. 2 ̆ 2 At the moment, the language of privacy seems to be the only language we have for talking about issues such as workplace e-mail monitoring, electronic cookies, medical records, and Monica 2 ̆ 7 s love letters. Is this a good thing? Unquestionably, Rosen 2 ̆ 7 s examples are troubling, but are they all troubling in precisely the same way? Does it make sense to analyze them all as solely or primarily examples of the erosion of 2 ̆ 2 privacy 2 ̆ 2 ? Moreover, is there a coherent and articulate conception of 2 ̆ 2 privacy 2 ̆ 2 that underlies all of Rosen 2 ̆ 7 s examples...|$|R
40|$|This {{dissertation}} {{explores the}} impact of tax policy and institutions on decisions {{in the market for}} housing. The first essay is joint work with Andrew Hanson. In it, we estimate the sensitivity of mortgage interest deducted on federal tax returns to the availability of the Mortgage Interest Deduction (MID). Our primary results show that for every one percentage point increase in the tax rate that applies to deductibility, the amount of mortgage interest deducted increases by $ 303 – 590. The second essay simulates changes to average home prices in twenty-seven cities that would result were the MID reformed. I use local variation in housing parameters to simulate home price changes for three different reforms: eliminating the MID, converting the MID to a fifteen percent credit, and capping the MID at fifteen percent. City price changes vary in response to a single policy by as much as 12. 8 percentage points. Spatial variation within cities is also notable, with areas high in income experiencing steeper price declines and areas of lower income experiencing shallow declines. The third essay is joint work with Andrew Hanson, Zack Hawley, and Bo Liu. We design and implement an experimental test for differential response by Mortgage Loan Originators (MLOs) to requests for information about loans. <b>Our</b> <b>e-mail</b> correspondence experiment is designed to analyze differential treatment by client race and credit score. Our results show net discrimination of 1. 8 percent by MLOs through non-response...|$|E
40|$|We are {{undoubtedly}} {{living in an}} age where we are exposed to a remarkable array of visual imagery. While we may have historically had confidence in the integrity of this imagery, today’s digital technology has begun to erode this trust. From the tabloid magazines to the fashion industry and in mainstream media outlets, scientific journals, political campaigns, courtrooms, and the photo hoaxes that land in <b>our</b> <b>e-mail</b> in-boxes, doctored photographs are appearing with a growing frequency and sophistication. Over the past five years, the field of digital forensics has emerged to help restore some trust to digital images. Here I review {{the state of the}} art in this new and exciting field. Digital watermarking has been proposed as a means by which an image can be authenticated (see, for example, [21] Digital Object Identifier 10. 1109 /MSP. 2008. 931079 and [5] for general surveys). The drawback of this approach is that a watermark must be inserted at the time of recording, which would limit this approach to specially equipped digital cameras. In contrast to these approaches, passive techniques for image forensics operate in the absence of any watermark or signature. These techniques work on the assumption that although digital forgeries may leave no visual clues that indicate tampering, they may alter the underlying statistics of an image. The set of image forensic tools can be roughly grouped into five categories: 1) pixel-based techniques that detect statistical anomalies introduced at the pixel level; 2) format-based techniques that leverage the statistical correlations introduced by a specific lossy compression scheme; 3) camera-based techniques that exploit artifacts introduced by the camera lens, sensor, or on-chip postprocessing; 4) physically based techniques that explicitly model and detect anomalies in the three-dimensional interaction between physical objects, light, and th...|$|E
40|$|All marine {{scientists}} {{who work in}} the field have them—personal stories of amazement, discovery, awe, excitement, and even danger while conducting research. They are the stories we love to tell to friends over a beer or to rapt high school students aspiring to become marine biologists or oceanographers. And in the telling, we ourselves somehow reconnect to the deepest motivations that brought us to marine research in the first place. Reliving those marvelous adventures displaces <b>our</b> disgruntlement with <b>e-mail,</b> proposals, and mundane paper work and reminds us how lucky we are to be marine scientists...|$|R
40|$|In {{this paper}} we {{introduce}} a presentation agent framework for speech applications. In this framework presentation agents {{are used to}} produce dynamic, adaptive and prosody rich speech outputs. Using this framework in <b>our</b> speechonly <b>e-mail</b> reader {{we have been able}} to handle multilingual issues and support different user groups. Our goal is to build unique computer `voices' to make speech outputs more intelligible and pleasant for the users. Keywords Speech user interfaces, speech output, user interface agents, dynamic output generation, prosody INTRODUCTION In current speech user interfaces speech output is often treated in a very inefficient way. Usually speech output is generated using a fixed string that is sent to a synthesizer. It is not uncommon that the resulting utterance is unpleasant and unintelligible for the user. To improve speech output we should use dynamic, adaptive and highly context sensitive messages. It is noteworthy that different people prefer different types of [...] ...|$|R
40|$|In today’s world, e-mail {{has become}} one of the most {{important}} means of communication in business and private lives due to its efficiency. However, the problems start as soon as mail volumes go beyond the scope of human information processing capabilities. Firstly, time does not allow for leaving certain messages unanswered for a long time, and in certain cases, for reading all messages. Secondly, the dilemma of electronic filters leaves a choice of too many junk mails getting through versus a risk of solicited mails being dumped. In this paper we present a new interactive visual data mining approach for analyzing individual e-mail communication. It combines classical visual analytics (help to identify pattern such as peaks and trends over time) with geo-spatial map distortions (help to understand the routes of e-mails). Experiments show that <b>our</b> visual <b>e-mail</b> explorer produces useful and interesting visualizations of large collections of e-mail and is practical for exploring temporal and geo-spatial patterns hidden in the e-mail data...|$|R
40|$|I {{hope you}} had a {{relaxing}} summer. Over {{the past few months}} we have been busy making progress on many fronts, as you will notice. The OIT Three Year Strategic Plan to manage our technology resources more effectively has been finalized and posted. Thank you to all the students, staff and faculty who participated in its development. Over the summer we have upgraded half of the NAC lab with new PCs and plan to upgrade the other half during the fall semester, subject to funding. These PCs will utilize a new technology called Virtual Desktop Infrastructure (VDI). <b>Our</b> new <b>e-mail</b> and communications system for students, CityMail, has been upgraded to Office 365 which introduced many new features for improved productivity. Along with Campus Facilities we created a new Smart Classroom in NAC 7 / 312. And this fall we have a planned rollout of SysAid, a feature rich service desk software to replace RemedyForce and integrate all the essential IT tools into one place. This will enhance efficiency and greatly improve the end-user experience. This year we will be undertaking the very importan...|$|R
40|$|The {{purpose of}} the {{contribution}} is to show how software and the internet have changed {{the way in which}} we carry out research, and what additional possibilities these new resources and tools provide. The internet and <b>e-mail</b> broaden <b>our</b> horizon for cooperation. There are no borders for information exchange: our library is a virtual one, electronic databases are at <b>our</b> fingertips. <b>E-mail</b> discussion groups provide an electronic community of CE users. Such forums have provided a basis for worldwide scientific cooperation amongst scientists; the present contribution is only one of several examples. Estimation software provides us with estimates of component properties in those cases where this information is not available from literature or from experiments (an estimated value is better than no value). Several examples will illustrate the use ofestimation software in capillary zone electrophoresis and micellar electrokinetic capillary chromatography. Simulation software presents visualization of experimental results to be expected, both as a training tool and as the first step in methoddevelopment. Other simulations yield valuable insights into phenomena that are not readily accessible experimentally for reasons of size or time-scale...|$|R
40|$|System {{operators}} play {{a critical}} role in maintaining server dependability yet lack powerful tools to help them do so. To help address this unfulfilled need, we describe Operator Undo, a tool that provides a forgiving operations environment by allowing operators to recover from their own mistakes, from unanticipated software problems, and from intentional or accidental data corruption. Operator Undo starts by intercepting and logging user interactions with a network service before they enter the system, creating a record of user intent. During an undo cycle, all system hard state is physically rewound, allowing the operator to perform arbitrary repairs; after repairs are complete, lost user data is reintegrated into the repaired system by replaying the logged user interactions while tracking and compensating for any resulting externally-visible inconsistencies. We describe the design and implementation of an application-neutral framework for Operator Undo, and detail the process by which we instantiated the framework in the form of an undo-capable e-mail store supporting SMTP mail delivery and IMAP mail retrieval. <b>Our</b> proof-of-concept <b>e-mail</b> implementation imposes only a small performance overhead, and can store days or weeks of recovery log on a single disk...|$|R
40|$|Abstract International new ventures use e-mail {{frequently}} {{to communicate with}} globally dispersed contacts. In this paper we present and discuss a qualitative research method to map international network development based on company <b>e-mails.</b> <b>Our</b> approach also allows for combinations of inductive and deductive analysis and combines the possibility of content analysis and descriptive statistics with sensemaking through qualitative analysis of the narratives unfolding in the e-mail chains between the firm and its alters. We discuss these issues and examine to what extent and how structural and relational network variables and dimensions can be examined using e-mails and why this is relevant. In this discussion e-mails are also triangulated with and compared to other sources of data. The data used t...|$|R
40|$|This paper proposes the {{development}} of a drug product Manufacturing Classification System (MCS) based on processing route. It summarizes conclusions from a dedicated APS conference and subsequent discussion within APS focus groups and the MCS working party. The MCS is intended as a tool for pharmaceutical scientists to rank the feasibility of different processing routes for the manufacture of oral solid dosage forms, based on selected properties of the API and the needs of the formulation. It has many applications in pharmaceutical development, in particular, it will provide a common understanding of risk by defining what the "right particles'' are, enable the selection of the best process, and aid subsequent transfer to manufacturing. The ultimate aim is one of prediction of product developability and processability based upon previous experience. This paper is intended to stimulate contribution from a broad range of stakeholders to develop the MCS concept further and apply it to practice. In particular, opinions are sought on what API properties are important when selecting or modifying materials to enable an efficient and robust pharmaceutical manufacturing process. Feedback can be given by replying to <b>our</b> dedicated <b>e-mail</b> address (mcs@apsgb. org); completing the survey on our LinkedIn site; or by attending one of our planned conference roundtable sessions...|$|R
40|$|As {{wildlife}} {{scientists and}} professionals, {{most of us}} rely on computer software in our daily work. We invest {{a large proportion of}} our time in learning how to use different kinds of software effectively to do our jobs. Choosing which software to learn in the first place is therefore an important long-term decision. Many of us rely on commercial software such as Word, Excel, Out-look and SPSS or SAS when we analyse data, correspond with <b>our</b> colleagues by <b>e-mail</b> and write papers. In this paper, we will briefly discuss some important aspects of commercial software in general. More impor-tantly, we will introduce the reader to a specific alter-native to commercial software known as free software. Our impression is that the awareness of this phenom-enon is much smaller among biologists than in othe...|$|R
40|$|International new ventures use e-mail {{frequently}} {{to communicate with}} globally dispersed contacts. In this paper we present and discuss a qualitative research method to map international network development based on company <b>e-mails.</b> <b>Our</b> approach also allows for combinations of inductive and deductive analysis and combines the possibility of content analysis and descriptive statistics with sensemaking through qualitative analysis of the narratives unfolding in the e-mail chains between the firm and its alters. We discuss these issues and examine to what extent and how structural and relational network variables and dimensions can be examined using e-mails and why this is relevant. In this discussion e-mails are also triangulated with and compared to other sources of data. The data used to exemplify our method are taken from an in-depth case study of a Dutch International New Venture. © Springer Science+Business Media, LLC 2007...|$|R
