2|10000|Public
30|$|Software: In this architecture, the {{software}} application GridVis from Janitza collects data from power network. GridVis also requests data over the Modbus TCP/IP protocol from energy analyzers. Every second, it requests {{data in the}} background and makes them available at a RESTful web service. The software aggregates the measured data to configurable values and save them in a MySQL 5.7. 4.0 database. In order to do this, it is necessary to use an additional application called GridVis Service [37]. In the test environment, we installed GridVis 6.0. 2 - 64 Bit and GridVis Service 6.0. 2 - 64 Bit on the server. We implement small Java applications which send requests to the web service and measure the latency (time from sending the request to receiving the response data completely) and the size <b>of</b> <b>the</b> <b>request</b> <b>data</b> in the HTTP package. In addition, we use the application Wireshark to capture network traffic during tests.|$|E
40|$|Using an {{innovative}} Value-based Self-Disclosure model, this paper examines the motivating factors in consumer decisions {{to disclose information}} to a firm requesting personal data online. It focusses on {{the costs and benefits}} customers associate with disclosure, and how they relate to privacy concerns. We conducted a controlled experiment in which we manipulated two situational factors: request context and data quantity. We gathered 252 subjects from a panel of French Web users and conducted the experiment online. We used a SEM-PLS approach to validate the measures and test the hypotheses. The quantity of data requested is shown to have no impact on customer value perceptions. The request context (and most notably, greater levels of customization) only has a significant impact on customer enjoyment. Not all four dimensions of value are important and cognitive costs and benefits appear to outweigh affective ones. Two specific perceived value dimensions - usefulness and psychological costs - are major determinants of self-disclosure intentions, and privacy concerns only influence behavioral intention indirectly through these two value dimensions. By encompassing both individual-level (i. e., privacy concern) and contextual-level factors (context <b>of</b> <b>the</b> <b>request,</b> <b>data</b> quantity and perceived costs and benefits), the VSD model offers a comprehensive explanatory framework. In addition to theoretical literature contributions on self-disclosure and privacy issues, our results offer useful implications for e-business, addressing the increasingly urgent question of how to elicit useful personal data from customers without losing their business...|$|E
5000|$|... whether <b>the</b> {{provision}} <b>of</b> <b>the</b> <b>requested</b> <b>data</b> {{is voluntary}} or required; ...|$|R
50|$|A {{component}} <b>of</b> <b>the</b> logical <b>request</b> generation takes <b>the</b> {{parse tree}} and makes of it a logical request tree that describes <b>the</b> precise semantics <b>of</b> <b>the</b> <b>requested</b> <b>data</b> in simplifieds SQL.|$|R
50|$|Usually, {{read and}} write {{requests}} are blocking operations, which means that <b>the</b> execution <b>of</b> <b>the</b> source process, upon writing, is suspended until all data could be written to the destination process, and, likewise, <b>the</b> execution <b>of</b> <b>the</b> destination process, upon reading, is suspended until at least some <b>of</b> <b>the</b> <b>requested</b> <b>data</b> could {{be obtained from the}} source process. This cannot lead to a deadlock, where both processes would wait indefinitely for each other to respond, since at least one <b>of</b> <b>the</b> two processes will soon thereafter have its <b>request</b> serviced by <b>the</b> operating system, and continue to run.|$|R
40|$|We {{present a}} compiler–based {{technique}} to automatically identify and extract Remote Procedure Calls, so–called Function Splices, out of potentially arbitrary sequences of Java code compiled for a software DSM. The {{goal is to}} lower communication latencies and message traffic by replacing data shipping by function shipping. Dynamic Function Splicing dynamically decides at runtime whether to invoke a function splice on the local machine or to execute it remotely on <b>the</b> home node <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> On proof–of–concept micro–benchmarks Dynamic Function Splicing reduces the execution wall time by approximately 29 %; about 25 % <b>of</b> <b>the</b> messages can be saved...|$|R
40|$|In-flight Image Quality {{calibration}} {{and performance}} assessment activities depend on specific acquisitions and, {{for some of}} them, on dedicated guidance <b>of</b> <b>the</b> satellite platform. The operational constraints may be tedious during the commissioning phase. Moreover, <b>the</b> length <b>of</b> <b>the</b> <b>requested</b> <b>data</b> collection may be conditioned by uncontrolled parameters such as climatic hazard. The new French high resolution earth observing satellite Pleiades-HR will be launched at <b>the</b> beginning <b>of</b> 2010. A specific design and new technologies have been embarked to provide great agility. These capabilities offer new methods to perform the image quality activities. Two are described in this paper. The first one concerns the so-called AMETHIST method to compute the normalizatio...|$|R
50|$|In PCI, a {{transaction}} that cannot be completed immediately is postponed {{by either the}} target or the initiator issuing retry-cycles, during which no other agents can use the PCI bus. Since PCI lacks a split-response mechanism to permit the target to return data at a later time, the bus remains occupied by the target issuing retry-cycles until the read data is ready. In PCI-X, after the master issues <b>the</b> <b>request,</b> it disconnects from the PCI bus, allowing other agents to use the bus. The split-response containing <b>the</b> <b>requested</b> <b>data</b> is generated only when the target is ready to return all <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> Split-responses increase bus efficiency by eliminating retry-cycles, during which no data can be transferred across the bus.|$|R
3000|$|... [...]. The adopted {{short-term}} throughout prerequisites inherent attribute <b>of</b> fulfilling <b>the</b> <b>requested</b> <b>data</b> rate <b>of</b> an RT user within short-term time intervals, {{instead of}} converging to it within long-term intervals (as in [16 – 18]), allows <b>the</b> efficient support <b>of</b> both CBR and VBR traffic.|$|R
40|$|Non Uniform Cache Architectures (NUCA) {{are a new}} de-sign {{paradigm}} for large last-level on-chip caches and have been introduced to deliver low access latencies in wire-delay dominated environments. Their structure is parti-tioned into sub-banks and the resulting access latency is a function <b>of</b> <b>the</b> physical position <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> Typi-cally, NUCA caches make use of a switched network to con-nect the different sub-banks and the cache controller. While on-chip networks [2, 3] have been adopted as communica-tion infrastructures in other scenarios, NUCA caches rep-resent an emerging technology and <b>the</b> influence <b>of</b> <b>the</b> net-work parameters on their performance needs to be inves-tigated. This work analyzes how different parameters for the on-chip network, namely hop latency and buffering ca...|$|R
5000|$|At any {{specific}} time, processes can be grouped into two categories: {{those that are}} waiting for input or output (called [...] "I/O bound"), {{and those that are}} fully utilizing the CPU ("CPU bound"). In primitive systems, the software would often [...] "poll", or [...] "busywait" [...] while waiting for requested input (such as disk, keyboard or network input). During this time, the system was not performing useful work. With <b>the</b> advent <b>of</b> interrupts and preemptive multitasking, I/O bound processes could be [...] "blocked", or put on hold, pending <b>the</b> arrival <b>of</b> <b>the</b> necessary data, allowing other processes to utilize the CPU. As <b>the</b> arrival <b>of</b> <b>the</b> <b>requested</b> <b>data</b> would generate an interrupt, blocked processes could be guaranteed a timely return to execution.|$|R
40|$|Earth data {{covering}} geodetic, meteorological and oceanographic information provides petabyte {{volumes of}} multi-dimensional data {{that can be}} accessed via dynamic data queries specifying the data subset to be processed and rendered in real-time. Our proposed X 3 D-EarthBrowser interfaces these queries that are standardized by the Open Geospatial Consortium (OGC) to real-time visualization technologies based on WebGL and X 3 D. In this context, we have developed rendering technologies for terrain and raster data {{to be used in}} standard web browsers, which have been evaluated in different use cases and run on various operating systems. Besides interfacing these technologies we have developed data streaming components that instantaneously offer pre-renderings in lower LODs <b>of</b> <b>the</b> <b>requested</b> <b>data</b> volume while providing high responsiveness and smooth interaction...|$|R
40|$|We {{present a}} {{distributed}} proactive caching approach that exploits user mobility information {{to decide where}} to proactively cache data to support seamless mobility, while efficiently utilizing cache storage using a congestion pricing scheme. The proposed approach is applicable to the case where objects have different sizes and to a two-level cache hierarchy, for both <b>of</b> which <b>the</b> proactive caching problem is hard. Additionally, our modeling framework considers the case where the delay is independent <b>of</b> <b>the</b> <b>requested</b> <b>data</b> object size and the case where the delay is a function <b>of</b> <b>the</b> object size. Our evaluation results show how various system parameters influence <b>the</b> delay gains <b>of</b> <b>the</b> proposed approach, which achieves robust and good performance relative to an oracle and an optimal scheme for a flat cache structure. Comment: 10 pages, 9 figure...|$|R
5000|$|At any {{specific}} time, processes can be grouped into two categories: {{those that are}} waiting for input or output (called [...] "I/O bound"), {{and those that are}} fully utilizing the CPU ("CPU bound"). In early systems, processes would often [...] "poll", or [...] "busywait" [...] while waiting for requested input (such as disk, keyboard or network input). During this time, the process was not performing useful work, but still maintained complete control <b>of</b> <b>the</b> CPU. With <b>the</b> advent <b>of</b> interrupts and preemptive multitasking, these I/O bound processes could be [...] "blocked", or put on hold, pending <b>the</b> arrival <b>of</b> <b>the</b> necessary data, allowing other processes to utilize the CPU. As <b>the</b> arrival <b>of</b> <b>the</b> <b>requested</b> <b>data</b> would generate an interrupt, blocked processes could be guaranteed a timely return to execution.|$|R
25|$|A {{data subject}} can also <b>request</b> a <b>data</b> {{controller}} to correct or delete personal {{data about the}} data subject that is held by the data controller and which is inaccurate, irrelevant, excessive, out of date, incomplete, or misleading (Section 33(1)). Upon receipt <b>of</b> <b>the</b> <b>request,</b> <b>the</b> <b>data</b> controller must either comply with <b>the</b> <b>request</b> or provide <b>the</b> <b>data</b> subject with credible evidence in support <b>of</b> <b>the</b> data. (Section 33(2)).|$|R
40|$|Summary: <b>The</b> process <b>of</b> {{localization}} <b>of</b> <b>the</b> software, {{and therefore}} <b>of</b> <b>the</b> digital learning resources, {{is divided into}} three parts: the first part is adjusting to the "local environment" (locale), the second part is a translation and adaptation <b>of</b> <b>the</b> user interface and the third part is the translation and adaptation <b>of</b> <b>the</b> documentation. The third part includes <b>the</b> localization <b>of</b> metadata. Metadata contain all relevant information about digital learning resources and therefore they are the most important element in searching and retrieving. The fact is that <b>the</b> discovery <b>of</b> learning resources that have "English" metadata is much easier than in <b>the</b> case <b>of</b> learning objects that have "non-English" metadata. There are two issues identified that generally affecting the search and discovery <b>of</b> <b>the</b> <b>requested</b> <b>data</b> in localized repositories <b>of</b> digital resources: <b>the</b> problem <b>of</b> grammatical rules and <b>the</b> problem <b>of</b> transliteration. Keywords: digital learning resources, metadata, localization...|$|R
40|$|In #at cache-only memory {{architectures}} (COMA), an attraction-memory miss must "rst interrogate {{a directory}} before a copy <b>of</b> <b>the</b> <b>requested</b> <b>data</b> can be located, which often involves three network traversals. By keeping track <b>of</b> <b>the</b> identity <b>of</b> a potential holder <b>of</b> <b>the</b> copycalled a hint one network traversal {{can be saved}} which reduces the read penalty. We have evaluated <b>the</b> reduction <b>of</b> <b>the</b> read-miss penalty provided by hints using detailed architectural simulations and four benchmark applications. The results show that a previously proposed protocol using hints can actually make the read-miss penalty larger because when the hint is not correct, an extra network traversal is needed. This has motivated us to study a new protocol, using hints, that simultaneously sends a <b>request</b> to <b>the</b> potential holder and to the directory. This protocol reduces the read-miss penalty for all applications but the protocol complexity {{does not seem to}} justify the performance improvement...|$|R
40|$|Packet-based data {{communication}} systems suffer from packet loss under high network traffic conditions. As a result, the receiver is often left with an incomplete description <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> Multiple description source coding addresses <b>the</b> problem <b>of</b> minimizing <b>the</b> expected distortion caused by packet loss. An equivalent {{problem is that}} of source coding for data transmission over multiple channels where each channel has some probability of breaking down. Recent work in practical multiple description coding explores <b>the</b> design <b>of</b> multiple description scalar and vector quantizers for <b>the</b> case <b>of</b> two channels or packets. This paper presents a new practical algorithm, based on a ternary tree structure, for <b>the</b> design <b>of</b> both fixed- and variable-rate multiple description vector quantizers for an arbitrary number of channels. Experimental results achieved by codes designed with this algorithm show that they perform well under {{a wide range of}} packet loss scenarios...|$|R
50|$|The DNS {{protocol}} {{is independent}} of its Transport Layer protocol. Queries and replies may be transmitted over IPv6 or IPv4 transports regardless <b>of</b> <b>the</b> address family <b>of</b> <b>the</b> <b>data</b> <b>requested.</b>|$|R
40|$|Rapid {{development}} in wireless communication technology has paved {{a path for}} the increase in mobile users to access the information from anywhere and at anytime. Data broadcasting plays {{a vital role in}} asymmetrical communication environment. Here server disseminates the information required by the mobile users at a faster rate as <b>the</b> bandwidth <b>of</b> <b>the</b> downlink channel is much more than the uplink. In data broadcasting, the mobile users put their <b>request</b> for <b>the</b> required <b>data</b> item and continuously listen for the response upon the channel from the server. In order to save the precious battery power <b>of</b> <b>the</b> mobile clients it should be that they have to be served quickly by the server. That is <b>the</b> access time <b>of</b> <b>the</b> <b>requested</b> <b>data</b> item should be minimized. This paper focus on <b>the</b> lengths <b>of</b> <b>the</b> data items which uses stretch as the metric to devise a server broadcasting algorithm in order to achieve optimal results. Simulation results are obtained to evaluate <b>the</b> performance <b>of</b> <b>the</b> proposed algorithm...|$|R
40|$|Biology is {{entering}} {{a new era in}} which data are being generated that cannot be published in the traditional literature. Databases are taking <b>the</b> role <b>of</b> scientific literature in distributing this information to the community. <b>The</b> success <b>of</b> some major biological undertakings, such as the Human Genome Project, will depend upon <b>the</b> development <b>of</b> a system for electronic data publishing. Many biological databases began as secondary literature-reviews in which certain kinds of data were collected from the primary literature. Now these databases are becoming a new kind of primary literature with findings being submitted directly to the database and never being published in print form. Some databases are offering publishing on demand services, where users can identify subsets <b>of</b> <b>the</b> data that are of interest, then subscribe to periodic distributions <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> New systems, such as the Internet Gopher, make building electronic information resources easy and affordable while offering a powerful search tool to the scientific community. Although many questions remain regarding th...|$|R
40|$|Replicating or caching popular {{content in}} {{memories}} distributed across {{the network is}} a technique to reduce peak network loads. Conventionally, <b>the</b> performance gain <b>of</b> caching was thought to result from making part <b>of</b> <b>the</b> <b>requested</b> <b>data</b> available closer to end users. Recently, {{it has been shown}} that by using a carefully designed technique to store the contents in the cache and coding across data streams a much more significant gain can be achieved in reducing the network load. Inner and outer bounds on the network load v/s cache memory tradeoff were obtained in (Maddah-Ali and Niesen, 2012). We give an improved outer bound on the network load v/s cache memory tradeoff. We address <b>the</b> question <b>of</b> to what extent caching is effective in reducing the server load when <b>the</b> number <b>of</b> files becomes large as compared to <b>the</b> number <b>of</b> users. We show that <b>the</b> effectiveness <b>of</b> caching become small when <b>the</b> number <b>of</b> files becomes comparable to <b>the</b> square <b>of</b> <b>the</b> number <b>of</b> users...|$|R
40|$|Abstract—Named Data Networking (NDN) is a {{proposed}} information-centric {{design for the}} future Internet architecture, where application names are directly used to route <b>requests</b> for <b>data.</b> This key component <b>of</b> <b>the</b> architecture raises concerns about scalability <b>of</b> <b>the</b> forwarding system in NDN network, i. e., {{how to keep the}} routing table sizes under control given unbounded nature of application data namespaces. In this paper we apply a well-known concept of Map-and-Encap to provide a simple and secure namespace mapping solution to the scalability problem. More specifically, whenever necessary, application data names can be mapped to a set of globally routable names that are used to retrieve the data. By including such sets in <b>data</b> <b>requests,</b> we are informing (more precisely, hinting) <b>the</b> forwarding system <b>of</b> <b>the</b> whereabouts <b>of</b> <b>the</b> <b>requested</b> <b>data,</b> and such hints can be used when routers do not know from where to retrieve the data using application data names alone. This solution enables NDN forwarding to scale with the Internet’s well-understood routing protocols and operational practice, while keeping all <b>the</b> benefits <b>of</b> <b>the</b> new NDN architecture. I...|$|R
40|$|An {{increasing}} cache latency {{in future}} processors incurs profound performance impacts {{in spite of}} advanced out-of-order execution techniques. In this paper, we describe an early address resolution mechanism that accurately resolves both regular and irregular load addresses. The basic idea is to build dynamic dependence links from the instruction that updates the base register to the consumer load instructions. Once a new base address is available, it triggers calculations <b>of</b> <b>the</b> new load addresses for dependent loads. Furthermore, the exact cache location <b>of</b> <b>the</b> <b>requested</b> <b>data</b> is predicted based on the newly resolved load address. As a result, this direct load can access the data cache directly to achieve a zerocycle load latency. Performance evaluation using SPEC integer programs shows that the dynamic dependence links can be established accurately. Combined with a stride-based predictor, the proposed early address resolution achieves about 97 % average accuracy with less than 1 % misprediction. Based on a modified SimpleScalar model, the proposed method can potentially improve the IPC by about 18 %...|$|R
50|$|Since sources <b>of</b> data within <b>the</b> data grid {{will consist}} of data from {{multiple}} separate systems and networks using different file naming conventions, {{it would be difficult}} for a user to locate data within the data grid and know they retrieved what they needed based solely on existing physical file names (PFNs). A universal or unified name space makes it possible to create logical file names (LFNs) that can be referenced within the data grid that map to PFNs. When an LFN is requested or queried, all matching PFNs are returned to include possible replicas <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> <b>The</b> end user can then choose from the returned results the most appropriate replica to use. This service is usually provided as part of a management system known as a Storage Resource Broker (SRB). Information about <b>the</b> locations <b>of</b> files and mappings between the LFNs and PFNs may be stored in a metadata or replica catalogue. The replica catalogue would contain information about LFNs that map to multiple replica PFNs.|$|R
25|$|On 13 February 1844, James Challis, {{director}} <b>of</b> <b>the</b> Cambridge Observatory, <b>requested</b> <b>data</b> on <b>the</b> position <b>of</b> Uranus, for Adams, from Astronomer Royal George Biddell Airy at the Royal Observatory, Greenwich. Adams certainly completed some calculations on 18 September 1845.|$|R
3000|$|In this paper, {{we use the}} {{algorithm}} for a simplified model <b>of</b> <b>the</b> <b>data</b> <b>request</b> in SDN network, taking the occupied time into account under circumstances of 0 – 1 bandwidth. The network status can be simplified as (T [...]...|$|R
40|$|In the European Union (EU), the {{residential}} sector {{is responsible for}} approximately 40 % <b>of</b> <b>the</b> total energy consumption. The existing building stock of member states is inefficient and can and must be retrofitted. This paper describes an innovative approach for <b>the</b> analysis <b>of</b> <b>the</b> potential energy savings of retrofitting existing building stocks. In particular, this study considers the actual technological and economic constraints <b>of</b> <b>the</b> implementation <b>of</b> feasible energy efficiency measures. The analysis was applied to five municipalities in <b>the</b> province <b>of</b> Milan that have signed <b>the</b> Covenant <b>of</b> Mayors, committing to meet and exceed the 20 % CO 2 reduction objective <b>of</b> <b>the</b> EU by 2020. Because <b>the</b> scale <b>of</b> <b>the</b> analysis is municipal, <b>the</b> resolution <b>of</b> <b>the</b> <b>requested</b> <b>data</b> is high. In order to achieve realistic and achievable energy savings, we realise an energy cadastre and conduct a large in-field survey for each municipality. By detecting <b>the</b> characteristics <b>of</b> <b>the</b> building stock, {{we are able to}} know which energy retrofit interventions are feasible from a technical, legal and economic point <b>of</b> view. <b>The</b> result is a tool that does not overestimate the potential energy savings, helping the public administration make energy saving policies...|$|R
40|$|Non Uniform Cache Architectures (NUCA) are a novel design {{paradigm}} for large last-level on-chip caches {{that has been}} introduced to deliver low access latencies in wiredelay dominated environments. Their structure is partitioned into sub-banks and the resulting access latency is a function <b>of</b> <b>the</b> physical position <b>of</b> <b>the</b> <b>requested</b> <b>data.</b> Typically, to connect the different sub-banks and the cache controller, NUCA caches employ a switched network, made up of links and routers with buffered queues; <b>the</b> characteristics <b>of</b> such switched network may affect <b>the</b> performance <b>of</b> <b>the</b> entire system. This work analyzes how different parameters for the routers, namely cut-through latency and buffering capacity, affect <b>the</b> overall performance <b>of</b> NUCA-based systems for the single processor case, assuming a reference organization proposed in literature. The results indicate that <b>the</b> sensitivity <b>of</b> <b>the</b> system to the cut-through latency is very high and that limited buffering capacity is sufficient to achieve a good performance level. As a consequence, we propose an alternative NUCA organization that limits <b>the</b> average number <b>of</b> hops experienced by cache accesses. This organization is better performing in most <b>of</b> <b>the</b> cases and scales better as the cut-through latency increases, thus simplifying <b>the</b> implementation <b>of</b> routers...|$|R
40|$|Replicating or caching popular {{content in}} {{memories}} distributed across {{the network is}} a technique to reduce peak network loads. Conventionally, the main performance gain of this caching was thought to result from making part <b>of</b> <b>the</b> <b>requested</b> <b>data</b> available closer to end users. Instead, we recently showed that a much more significant gain {{can be achieved by}} using caches to create coded-multicasting opportunities, even for users with different demands, through coding across data streams. These coded-multicasting opportunities are enabled by careful content overlap at the various caches in the network, created by a central coordinating server. In many scenarios, such a central coordinating server may not be available, raising the question if this multicasting gain can still be achieved in a more decentralized setting. In this paper, we propose an efficient caching scheme, in which the content placement is performed in a decentralized manner. In other words, no coordination is required for the content placement. Despite this lack <b>of</b> coordination, <b>the</b> proposed scheme is nevertheless able to create coded-multicasting opportunities and achieves a rate close to the optimal centralized scheme. Comment: To appear in IEEE/ACM Transactions on Networkin...|$|R
40|$|In <b>the</b> summer <b>of</b> 1997, {{electrical}} geophysical {{data was}} collected north of Beatty, Nevada. Audio-magnetotellurics (AMT) was the geophysical method used to collect 16 stations along two profiles. <b>The</b> purpose <b>of</b> this data collection {{was to determine the}} depth to the alluvial basement, based upon <b>the</b> needs <b>of</b> <b>the</b> geologists <b>requesting</b> <b>the</b> <b>data...</b>|$|R
5000|$|To provide {{higher levels}} of {{automation}} to operator services, CCI introduced in the early 1980s various Automatic Voice Response (AVR) systems tightly integrated with its popular Directory Assistance systems. AVR provided voice response <b>of</b> <b>the</b> customer <b>requested</b> <b>data,</b> almost universally starting the prompt with a variant <b>of</b> <b>the</b> phrase, [...] "The number is". Early systems were based on very small vocabulary synthesised speech chips, follow-on systems utilized 8-bit PCM, and later ADPCM voice playback using audio authored either by CCI or the local phone company.|$|R
40|$|Time Series Data Server (TSDS) is a {{software}} package for implementing a server that provides fast super-setting, sub-setting, filtering, and uniform gridding of time series-like data. TSDS {{was developed to}} respond quickly to requests for long time spans of data. Data may be served from a fast database, typically created by aggregating granules (e. g., data files) from a remote data source and storing them in a local cache that is optimized for serving time series. The system was designed specifically for time series data, and is optimized for <b>requests</b> where <b>the</b> longest dimension <b>of</b> <b>the</b> <b>requested</b> <b>data</b> structure is time. Scalar, vector, and spectrogram time series types are supported. The user can interact with <b>the</b> server by <b>requesting</b> a time series, a date range, and an optional filter {{to apply to the}} data. Available filters include strides, block average/minimum/maximum, exclude, and inequality. Constraint expressions are supported, which allow such operations as a <b>request</b> for <b>data</b> from one time series when a different time series satisfied a specified relationship. TSDS builds upon DAP (Data Access Protocol), NcML (netCDF Mark-up language) and related software libraries. In this work, we describe <b>the</b> current design <b>of</b> this server, as well as planned features and potential implementation strategies. Comment: Submitted to Earth Science Informatic...|$|R
40|$|Banking {{has been}} {{identified}} as one <b>of</b> <b>the</b> effective methods using which memory energy can be reduced. We propose a novel approach that improves <b>the</b> energy effectiveness <b>of</b> a banked memory architecture by performing extra computations if doing so makes it unnecessary to reactivate a bank which is in the low-power operating mode. More specifically, when an access to a bank, which is in the low-power mode, is to be made, our approach first checks whether the data required from that bank can be recomputed by using the data that are currently stored in already active banks. If this is the case, we do not turn on the bank in question, and instead, recalculate <b>the</b> value <b>of</b> <b>the</b> <b>requested</b> <b>data</b> using <b>the</b> values <b>of</b> <b>the</b> data stored in the active banks. Given the fact that <b>the</b> contribution <b>of</b> <b>the</b> leakage consumption to overall energy budget keeps increasing, the proposed approach has <b>the</b> potential <b>of</b> being even more attractive in the future. Our experimental results collected so far clearly show that this recomputation based approach can reduce energy consumption significantly. c○ACM, 2006. This is <b>the</b> author’s version <b>of</b> <b>the</b> work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in th...|$|R
40|$|This paper {{presents}} <b>the</b> architecture <b>of</b> DIET (Distributed Interactive Engineering Toolbox), {{a hierarchical}} set of components to build Network Enabled Server applications in a Grid environment. This environment {{is built on}} top of dierent tools which are able to locate an appropriate server depending <b>of</b> <b>the</b> client's <b>request,</b> <b>the</b> <b>data</b> location and the dynamic performance characteristics <b>of</b> <b>the</b> system...|$|R
40|$|More {{and more}} spatial data is {{accessible}} over the web or through portals of wireless service providers. In this context the main selection {{criteria for the}} data are <b>the</b> type <b>of</b> <b>the</b> <b>requested</b> <b>data</b> objects and {{their position in the}} real world. Integration and performance issues are challenged by the need to process ad hoc queries in an interactive fashion. In this paper we investigate how a main memory query engine can be used to meet these requirements. It has <b>the</b> added benefit <b>of</b> being easily deployable to many components in a large-scale data integration system. Hence, we analyze how such a query engine can best exploit the query characteristics by employing an index structure that leverages spatial and type dimensions. In order to support query processing in the best possible way we investigate a specific multi-dimensional main memory index structure. Compared to the straightforward approach using separate indexes on type and position we can increase the performance up to almost an order of magnitude in several important usage scenarios. This requires to tweak <b>the</b> mapping <b>of</b> type IDs to values in the type dimension, which we discuss extensively. This enables the overall system to be used interactively, even with large data sets. Pages: 35 - 5...|$|R
