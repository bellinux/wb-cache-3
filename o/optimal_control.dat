10000|4279|Public
25|$|J. Warga: <b>Optimal</b> <b>control</b> of {{differential}} and functional equations. Academic Press, 1972.|$|E
25|$|<b>Optimal</b> <b>control</b> {{theory is}} a {{generalization}} of the {{calculus of variations}} which introduces control policies.|$|E
25|$|Contribution to {{the time}} <b>optimal</b> <b>control</b> problem. Abh. Deutsch. Akad. Wiss. Berlin Kl. Math. Phys. Tech. 1965 1965 no. 2, 438—446 (1966).|$|E
40|$|<b>Optimal</b> nonanticipating <b>controls</b> {{are shown}} {{to exist in}} nonautonomous {{piecewise}} deterministic control problems with hard terminal restrictions. The assumptions needed are completely analogous to those needed to obtain <b>optimal</b> <b>controls</b> in deterministic control problems. The proof is based on well-known results on existence of deterministic <b>optimal</b> <b>controls...</b>|$|R
30|$|We {{organize}} {{this paper}} as follows. In Sect.  2, we prove {{the existence of}} <b>optimal</b> <b>controls</b> for problem (1.3) and discuss some properties of the <b>optimal</b> <b>controls</b> (see Lemma  2.1). Then we prove Theorem  1.1.|$|R
40|$|Abstract <b>Optimal</b> nonanticipating <b>controls</b> {{are shown}} {{to exist in}} nonautonomous {{piecewise}} deterministic control problems with hard terminal restrictions. The assumptions needed are completely analogous to those needed to obtain <b>optimal</b> <b>controls</b> in deterministic control problems. The proof is based on well-known results on existence of deterministic <b>optimal</b> <b>controls.</b> <b>Optimal</b> controls; terminal restrictions...|$|R
25|$|He is the {{co-author}} of Applied <b>Optimal</b> <b>Control,</b> and an influential researcher in differential games, pattern recognition, and discrete event dynamic systems.|$|E
25|$|In <b>optimal</b> <b>control</b> theory, the Lagrange {{multipliers}} {{are interpreted}} as costate variables, and Lagrange multipliers are reformulated as the minimization of the Hamiltonian, in Pontryagin's minimum principle.|$|E
25|$|Multivariate {{calculus}} {{is used in}} the <b>optimal</b> <b>control</b> {{of continuous}} time dynamic systems. It is used in regression analysis to derive formulas for estimating relationships among various sets of empirical data.|$|E
30|$|In this section, the {{existence}} of <b>optimal</b> <b>controls</b> is investigated.|$|R
40|$|We {{study the}} {{existence}} of solutions and <b>optimal</b> <b>controls</b> for some fractional impulsive equations of order 1 < α< 2. By means of Gronwall’s inequality and Leray-Schauder’s fixed point theorem, the sufficient condition for {{the existence of}} solutions and <b>optimal</b> <b>controls</b> is presented. Finally, an example is given to illustrate our main results...|$|R
30|$|The {{following}} proposition {{is important}} in studying the stability of <b>optimal</b> <b>controls.</b>|$|R
25|$|One may {{reformulate}} the Lagrangian as a Hamiltonian, {{in which}} case the solutions are local minima for the Hamiltonian. This is done in <b>optimal</b> <b>control</b> theory, in the form of Pontryagin's minimum principle.|$|E
25|$|Economic studies {{consistently}} {{show that}} the costs incurred with poorly controlled asthma are higher than those for a well-controlled patient with the same severity of disease. For severe asthma, {{it has been estimated}} that the savings produced by <b>optimal</b> <b>control</b> would be around 45% of the total medical costs.|$|E
25|$|In his {{doctoral}} dissertation Carathéodory originated his method {{based on the}} use of the Hamilton–Jacobi equation to construct a field of extremals. The ideas are closely related to light propagation in optics. The method became known as the royal road to the calculus of variations. More recently the same idea has been taken into the theory of <b>optimal</b> <b>control.</b> The method can also be extended to multiple integrals.|$|E
30|$|Constructing {{approximating}} minimizing {{sequences of}} functions twice {{plays a key}} role in the proof of looking for <b>optimal</b> <b>controls,</b> which enable us to deal with the multiple solution problem of feasible pairs. More importantly, this will allow us to study more extensive and complex evolution equations and <b>optimal</b> <b>controls</b> problems. Moreover, we have the following consequences.|$|R
30|$|In the section, we {{give the}} {{existence}} of <b>optimal</b> <b>controls</b> for system (3.1).|$|R
30|$|The {{following}} Theorem {{is needed}} in the proof {{of the existence of}} <b>optimal</b> <b>controls.</b>|$|R
25|$|In the 20th century David Hilbert, Emmy Noether, Leonida Tonelli, Henri Lebesgue and Jacques Hadamard {{among others}} made {{significant}} contributions. Marston Morse applied {{calculus of variations}} {{in what is now}} called Morse theory. Lev Pontryagin, Ralph Rockafellar and F. H. Clarke developed new mathematical tools for the calculus of variations in <b>optimal</b> <b>control</b> theory. The dynamic programming of Richard Bellman is an alternative to the calculus of variations.|$|E
25|$|People with {{narcolepsy}} can {{be substantially}} helped, but not cured. Treatment is {{tailored to the}} individual, based on symptoms and therapeutic response. The time required to achieve <b>optimal</b> <b>control</b> of symptoms is highly variable and may take several months or longer. Medication adjustments are frequently necessary, and complete control of symptoms is seldom possible. While oral medications are the mainstay of formal narcolepsy treatment, lifestyle changes are also important.|$|E
25|$|Some {{research}} groups have recently explored {{the use of}} quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is {{to rely on a}} physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the <b>optimal</b> <b>control</b> parameters that best represent the empirical distribution of a given dataset.|$|E
40|$|In {{this paper}} we {{consider}} Lagrange type control problem for systems involving dynamic boundary conditions that is, with boundary operators containing time derivatives. Assuming {{the existence of}} <b>optimal</b> <b>controls,</b> B -evolutions theory is used to present necessary conditions of optimality. The result is illustrated by an example from heat transfer problem and also an algorithm for computing <b>optimal</b> <b>controls</b> is presented. </p...|$|R
40|$|Abstract: An <b>optimal</b> climate <b>control</b> {{has been}} {{designed}} for a solar greenhouse to achieve optimal crop production with sustainable instead of fossil energy. The solar greenhouse extends a conventional greenhouse with an improved roof cover, ventilation with heat recovery, a heat pump, a heat exchanger and an aquifer. It was found that in the <b>optimal</b> <b>controlled</b> solar greenhouse, gas use can be seriously reduced (by 52 %), while the crop production is significantly increased (by 39 %), as compared to an <b>optimal</b> <b>controlled</b> conventional greenhouse without the solar greenhouse elements...|$|R
30|$|Now we {{can give}} the {{following}} results on existence of periodic <b>optimal</b> <b>controls</b> for Bolza problem (P).|$|R
25|$|While PID {{controllers}} {{are applicable}} to many control problems, and often perform satisfactorily without any improvements or only coarse tuning, they can perform poorly in some applications, {{and do not}} in general provide <b>optimal</b> <b>control.</b> The fundamental difficulty with PID control {{is that it is}} a feedback control system, with constant parameters, and no direct knowledge of the process, and thus overall performance is reactive and a compromise. While PID control is the best controller in an observer without a model of the process, better performance can be obtained by overtly modeling the actor of the process without resorting to an observer.|$|E
500|$|Following Richard Bellman's work on dynamic {{programming}} and the 1962 English translation of L. Pontryagin et al.'s earlier work, <b>optimal</b> <b>control</b> theory was used more extensively in economics in addressing dynamic problems, especially as {{to economic growth}} equilibrium and stability of economic systems, • Martos, Béla (1987). [...] "control and coordination of economic activity", [...] The New Palgrave: A Dictionary of Economics. [...] Description [...] • Brock, W. A. (1987). [...] "optimal control and economic dynamics", The New Palgrave: A Dictionary of Economics[...] • ] of which a textbook example is optimal consumption and saving. A crucial distinction is between deterministic and stochastic control models. [...] Other applications of <b>optimal</b> <b>control</b> theory include those in finance, inventories, and production for example. • [...] Scroll to chapter-preview ...|$|E
500|$|Economic {{dynamics}} {{allows for}} changes in economic variables over time, including in dynamic systems. [...] The problem of finding optimal functions for such changes is studied in variational calculus and in <b>optimal</b> <b>control</b> theory. [...] Before the Second World War, Frank Ramsey and Harold Hotelling used the calculus of variations to that end.|$|E
3000|$|... {{for some}} {{switching}} controlled systems. We first prove {{the existence of}} time <b>optimal</b> <b>controls</b> to the problem [...]...|$|R
30|$|Now we {{can give}} the {{following}} results {{on the existence of}} <b>optimal</b> <b>controls</b> for the Lagrange problem (P).|$|R
40|$|This {{paper is}} {{the first part of}} our series work to {{establish}} pointwise second-order necessary conditions for stochastic <b>optimal</b> <b>controls.</b> In this part, both drift and diffusion terms may contain the control variable but the control region is assumed to be convex. Under some assumptions in terms of Malliavin calculus, we establish the desired necessary condition for stochastic singular <b>optimal</b> <b>controls</b> in the classical sense. Comment: 28 page...|$|R
2500|$|... {{existence}} theorems for <b>optimal</b> <b>control</b> {{problem with}} unbounded controls and multidimensional cost functions; ...|$|E
2500|$|Pierre-Louis Lions [...] {{developed}} viscosity solutions into stochastic {{control and}} <b>optimal</b> <b>control</b> methods.|$|E
2500|$|The {{achievements}} of Tadeusz Ważewski in the mathematical theory of <b>optimal</b> <b>control.</b> (Polish) Wiadom. Mat. (2) 20 (1976), no. 1, 66—69. 49-03 ...|$|E
3000|$|... [...]. Then, we derive the bang-bang {{property}} for time <b>optimal</b> <b>controls</b> to this problem, through utilizing the Pontryagin maximum principle.|$|R
30|$|In the following, {{we discuss}} the {{cellular}} simulations we obtain in the case when the <b>optimal</b> <b>controls</b> (11) are introduced.|$|R
40|$|International audienceWe {{prove the}} {{existence}} of <b>optimal</b> relaxed <b>controls</b> as well as strict <b>optimal</b> <b>controls</b> for systems governed by non linear forward–backward stochastic differential equations (FBSDEs). Our approach is based on weak convergence techniques for the associated in the Jakubowski S-topology and a suitable Skorokhod representation theorem...|$|R
