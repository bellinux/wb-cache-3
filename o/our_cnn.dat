73|144|Public
30|$|<b>Our</b> <b>CNN</b> {{classifiers}} {{may have}} model-specific biases arising from training using only NICAM data.|$|E
30|$|Convolutional layer. One hundred twenty-eight kernels of 3 × 7 × 7 and 3 × 3 × 3 {{combined}} with the ReLU layer, the features in the input image are extracted furthermore. Same as the first layer in <b>our</b> <b>CNN</b> architecture.|$|E
30|$|While several {{previous}} studies [16, 30, 39, 43] compared several algorithms, {{they did not}} use a widely accepted audio classification method for benchmarking their neural networks. In our study, we used the classification results of SVMs that use the MFCC features to benchmark <b>our</b> <b>CNN</b> algorithm.|$|E
40|$|Abstract — In this study, we {{research}} a new layer arrangement of three layer {{cellular neural network}} (CNN). In this paper, we investigate the output characteristics by using <b>our</b> proposed <b>CNN</b> and also <b>our</b> designed templates to image processing of gray-scale image and binary image. The simulation results show the effectiveness with <b>our</b> proposed <b>CNN.</b> I...|$|R
5000|$|Trump's tape won't sway his base, {{but will}} it change <b>our</b> culture?, <b>CNN,</b> October 10, 2016 ...|$|R
30|$|While no feature {{extraction}} {{needs to be}} performed, as we are teaching <b>our</b> <b>CNNs</b> to read from scratch, character-level data must still be represented {{in a manner that}} is acceptable as input for our networks. For this purpose we propose a new character embedding offering greatly reduced memory consumption and training time. We benchmark our approach against the embedding used by Zhang and LeCun [1]. We also investigate using different sized character alphabets.|$|R
30|$|Based on [58], we {{formulated}} the shattering {{coefficient for}} <b>our</b> <b>CNN</b> architecture that {{was composed of}} a single convolutional layer. The shattering coefficient of the SVM was also calculated, according to SLT [17, 48]. Those formulations ensure our framework presents learning guarantees {{in the context of}} speech recognition.|$|E
40|$|Given a scene, what {{is going}} to move, and in what {{direction}} will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. <b>Our</b> <b>CNN</b> model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because <b>our</b> <b>CNN</b> model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins...|$|E
30|$|In this study, we experimented using CNN {{algorithms}} in audio classification. Since MFCC features {{combined with}} SVM is a generally accepted practice for audio classification, we {{used it as}} a benchmark for <b>our</b> <b>CNN</b> algorithm. We found out that spectrogram image classification with CNN algorithm works as well as the SVM system.|$|E
30|$|As {{mentioned}} in Section 2.2, {{the local and}} global features are automatically extracted by <b>our</b> proposed <b>CNN.</b> Similar to human’s visual system, <b>our</b> proposed <b>CNN</b> can extract the global features including the color, shape of the refrigerators and the local features from the display, crack, ice maker, and handle of the refrigerators. The global features integrated with local features form a layout for each category of refrigerators. The whole layout is utilized to establish the mapping between the input front-view image of the refrigerator and the output classification label.|$|R
40|$|We {{present an}} Automatic License Plate Recognition system {{designed}} around Convolutional Neural Networks (CNNs) and trained over synthetic plate images. We first design CNNs suitable for plate and character detection, sharing a common architecture and training procedure. Then, we generate synthetic images {{that account for}} the varying illumination and pose conditions encountered with real plate images and we use exclusively such synthetic images to train <b>our</b> <b>CNNs.</b> Experiments with real vehicle images captured in natural light with commodity imaging systems show precision and recall in excess of 93 % despite our networks are trained exclusively on synthetic images...|$|R
40|$|Recent works about {{convolutional}} {{neural networks}} (CNN) show breakthrough performance on various tasks. However, {{most of them}} only use the features extracted from the topmost layer of CNN instead of leveraging the features extracted from different layers. As the first group which explicitly addresses utilizing the features from different layers of CNN, we propose cross-layer CNN features which consist of the features extracted from multiple layers of <b>CNN.</b> <b>Our</b> experimental results show that <b>our</b> proposed cross-layer <b>CNN</b> features outperform not only the state-of-the-art results but also the features commonly used in the traditional CNN framework on three tasks – artistic style, artist, and architectural style classification. As shown by the exper-imental results, <b>our</b> proposed cross-layer <b>CNN</b> features achieve the best known performance on the three tasks in different domains, which makes <b>our</b> proposed cross-layer <b>CNN</b> features promising solutions for generic tasks. Index Terms — Convolutional neural networks (CNN), cross-layer features, generic classification tasks 1...|$|R
40|$|Computational {{approaches}} to drug discovery {{can reduce the}} time and cost associated with experimental assays and enable the screening of novel chemotypes. Structure-based drug design methods rely on scoring functions to rank and predict binding affinities and poses. The ever-expanding amount of protein-ligand binding and structural data {{enables the use of}} deep machine learning techniques for protein-ligand scoring. We describe convolutional neural network (CNN) scoring functions that take as input a comprehensive 3 D representation of a protein-ligand interaction. A CNN scoring function automatically learns the key features of protein-ligand interactions that correlate with binding. We train and optimize <b>our</b> <b>CNN</b> scoring functions to discriminate between correct and incorrect binding poses and known binders and non-binders. We find that <b>our</b> <b>CNN</b> scoring function outperforms the AutoDock Vina scoring function when ranking poses both for pose prediction and virtual screening...|$|E
40|$|While egocentric {{video is}} {{becoming}} increasingly popular, browsing it is very difficult. In this paper we present a compact 3 D Convolutional Neural Network (CNN) architecture for long-term activity recognition in egocentric videos. Recognizing long-term activities enables us to temporally segment (index) long and unstructured egocentric videos. Existing methods for this task are based on hand tuned features derived from visible objects, location of hands, as well as optical flow. Given a sparse optical flow volume as input, <b>our</b> <b>CNN</b> classifies the camera wearer's activity. We obtain classification accuracy of 89 %, which outperforms the current state-of-the-art by 19 %. Additional evaluation is performed on an extended egocentric video dataset, classifying twice the amount of categories than current state-of-the-art. Furthermore, <b>our</b> <b>CNN</b> is able to recognize whether a video is egocentric or not with 99. 2 % accuracy, up by 24 % from current state-of-the-art. To better understand what the network actually learns, we propose a novel visualization of CNN kernels as flow fields...|$|E
30|$|The {{remainder}} {{of this article is}} organized as follows. Brief descriptions of time domain and spectrogram feature of HRRP are introduced in Section 2. In the following section, we will present <b>our</b> <b>CNN</b> model in detail. The experimental results of our model with time domain and spectrogram feature on measured HRRP data are shown in Section 4. Finally, conclusions are addressed in Section 5.|$|E
30|$|After the {{training}} process, we conducted experiments to testify {{the performance of}} <b>our</b> proposed <b>CNN</b> architecture. We choose several state-of-the-art image classification methods [35 – 38] to compare with our proposed method. Different features including single feature and combined features are exploited by these SVM-based methods, respectively.|$|R
30|$|Similar {{to human}} being’s visual system, our {{proposed}} approach can extract the global and local {{features of the}} images of bananas. The global features combined with local features form a layout for each category of bananas. Figure  6 shows {{that in the first}} convolutional layer, the global features including shape of the banana are extracted. Then, in the two following convolutional layers, other features including color and texture are extracted hierarchically. Unlike the manually extracted features used in [35 – 38], these features are automatically extracted by <b>our</b> proposed <b>CNN</b> architecture. As illustrated in Fig.  6, the global and local features of banana image are automatically extracted by <b>our</b> proposed <b>CNN</b> architecture. The map between the input banana image and the output classification result could be obtained through the extracted features.|$|R
40|$|The horizon line is an {{important}} contextual attribute {{for a wide variety}} of image understanding tasks. As such, many methods have been proposed to estimate its location from a single image. These methods typically require the image to contain specific cues, such as vanishing points, coplanar circles, and regular textures, thus limiting their real-world applicability. We introduce a large, realistic evaluation dataset, Horizon Lines in the Wild (HLW), containing natural images with labeled horizon lines. Using this dataset, we investigate the application of convolutional neural networks for directly estimating the horizon line, without requiring any explicit geometric constraints or other special cues. An extensive evaluation shows that using <b>our</b> <b>CNNs,</b> either in isolation or in conjunction with a previous geometric approach, we achieve state-of-the-art results on the challenging HLW dataset and two existing benchmark datasets. Comment: British Machine Vision Conference (BMVC) 201...|$|R
40|$|We {{introduce}} Deep-HiTS, {{a rotation}} invariant {{convolutional neural network}} (CNN) model for classifying images of transients candidates into artifacts or real sources for the High cadence Transient Survey (HiTS). CNNs {{have the advantage of}} learning the features automatically from the data while achieving high performance. We compare <b>our</b> <b>CNN</b> model against a feature engineering approach using random forests (RF). We show that <b>our</b> <b>CNN</b> significantly outperforms the RF model reducing the error by almost half. Furthermore, for a fixed number of approximately 2, 000 allowed false transient candidates per night we are able to reduce the miss-classified real transients by approximately 1 / 5. To the best of our knowledge, {{this is the first time}} CNNs have been used to detect astronomical transient events. Our approach will be very useful when processing images from next generation instruments such as the Large Synoptic Survey Telescope (LSST). We have made all our code and data available to the community for the sake of allowing further developments and comparisons at [URL]...|$|E
40|$|We {{provide some}} {{visualization}} {{results of our}} CNN-based subitizing classifiers to {{provide insight into the}} model learned by the CNN. 1. Sample prediction results, showing example true positives, false positives and false negatives pro-duced by our Subitizing classifiers (Fig. 1). 2. Sample visualization results of <b>our</b> <b>CNN</b> subitizing classifiers using the method of [4] (Fig. 2). 3. 2 D-embedding of the fc 7 CNN feature: before and after fine-tuning using our SOS dataset (Fig. 3 - 4). Reference...|$|E
30|$|Now, {{we proceed}} by {{computing}} the generalization {{bound for the}} CNN (Eq. 4), as shown in Eq. 8. Considering δ= 0.05, that represents a probability of 0.95 (i.e., 95 %) {{to ensure that the}} empirical risk Remp(f) is a good estimator for the expected risk R(f), meaning the error results measured for our classifier indeed work on unseen examples. Observe that <b>our</b> <b>CNN</b> requires at least 216, 640 examples to converge, while we had in practice 1, 447, 869 examples in training set.|$|E
40|$|This paper {{presents}} a novel method to involve both {{spatial and temporal}} features for semantic video segmentation. Current work on convolutional neural networks(CNNs) has shown that CNNs provide advanced spatial features supporting a very good performance of solutions for both image and video analysis, especially for the semantic segmentation task. We investigate how involving temporal features also has a good effect on segmenting video data. We propose a module based on a long short-term memory (LSTM) architecture of a recurrent neural network for interpreting the temporal characteristics of video frames over time. Our system takes as input frames of a video and produces a correspondingly-sized output; for segmenting the video our method combines the use of three components: First, the regional spatial features of frames are extracted using a CNN; then, using LSTM the temporal features are added; finally, by deconvolving the spatio-temporal features we produce pixel-wise predictions. Our key insight is to build spatio-temporal convolutional networks (spatio-temporal CNNs) that have an end-to-end architecture for semantic video segmentation. We adapted fully some known convolutional network architectures (such as FCN-AlexNet and FCN-VGG 16), and dilated convolution into <b>our</b> spatio-temporal <b>CNNs.</b> <b>Our</b> spatio-temporal <b>CNNs</b> achieve state-of-the-art semantic segmentation, as demonstrated for the Camvid and NYUDv 2 datasets...|$|R
50|$|It {{recently}} acquired Penny Lane's <b>Our</b> Nixon. <b>CNN</b> plans to premiere {{the film in}} August, while Cinedigm will release it theatrically. The network has also then acquired the domestic television broadcast rights of the Sundance film selection Robert Stone's Pandora's Promise and is slated to air on CNN on November while it will be theatrically released on June 12.|$|R
30|$|Each {{captured}} {{image is}} divided into image patches with same sizes. For each image, a positive image (same category) and a negative image (different category) are chosen from the captured images. Then, the three image patches are jointly inputted into <b>our</b> proposed <b>CNN,</b> in which three parameter-sharing CNNs are presented to handle the original, the positive, and the negative images, respectively.|$|R
30|$|The {{architecture}} {{of a typical}} CNN consists of a stack of layers with (1) an input layer that encodes words in each relation instance by real-valued vectors; (2) a convolutional layer to capture contextual features, e.g., n-grams, in the given input; (3) a pooling layer to determine the most relevant features and, (4) an output layer which is a fully connected layer that performs the classification. “Classify candidate instances using CNN-based model” subsection provides a detailed discussion of each layer {{as well as the}} overall structure of <b>our</b> <b>CNN.</b>|$|E
40|$|We {{consider}} the non-Lambertian object intrinsic problem of recovering diffuse albedo, shading, and specular highlights {{from a single}} image of an object. We build a large-scale object intrinsics database based on existing 3 D models in the ShapeNet database. Rendered with realistic environment maps, millions of synthetic images of objects and their corresponding albedo, shading, and specular ground-truth images are used to train an encoder-decoder CNN. Once trained, the network can decompose an image into the product of albedo and shading components, along with an additive specular component. <b>Our</b> <b>CNN</b> delivers accurate and sharp results in this classical inverse problem of computer vision, sharp details attributed to skip layer connections at corresponding resolutions from the encoder to the decoder. Benchmarked on our ShapeNet and MIT intrinsics datasets, our model consistently outperforms the state-of-the-art by a large margin. We train and test <b>our</b> <b>CNN</b> on different object categories. Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories. Our analysis shows that feature learning at the encoder stage is more crucial for developing a universal representation across categories. We apply our synthetic data trained model to images and videos downloaded from the internet, and observe robust and realistic intrinsics results. Quality non-Lambertian intrinsics could open up many interesting applications such as image-based albedo and specular editing...|$|E
30|$|In testing, {{we first}} {{projected}} the original 3 D image stack onto the XY plane and generated a MIP image. We then cropped 2 D patches using a sliding window with n-pixel stride. These patches were classified into patches centered on foreground or background pixels using our trained CNN model. To further improve classification accuracy and exclude false positive patches, we applied mean shift [19] to the detected foreground patches and map {{them back to}} the actual 3 D locations based on the local maximum intensity along Z. Finally, we classified these 3 D detected signals using <b>our</b> <b>CNN</b> model again based on the MIPs of the local 3 D blocks.|$|E
30|$|Figure  7 {{shows that}} in the first {{convolutional}} layer (the first column in Fig.  7), the global features including shape and edge of the refrigerator are extracted. In the two following convolutional layers (the second and third columns in Fig.  7), local features are extracted hierarchically. Notable that these features, which are different from the handcrafted features exploited in [5 – 8], are automatically extracted through <b>our</b> proposed <b>CNN.</b>|$|R
30|$|For the CNNs and RNNs, {{applying}} the convolution or the recurrent time {{steps on the}} DER state does not seem plausible. Instead, <b>our</b> classification <b>CNN</b> processes the DER state in layers after the convolution (e.g., see Fig.  3 a). The CNNs for generating load profiles process the DER state in the first fully-connected layer (e.g., see Fig.  3 b). The RNN converts the DER state into initial states of the memory units (e.g., see Fig.  3 c).|$|R
40|$|In this research, {{we propose}} the new type two-layer {{cellular}} neural networks. In our two-layer cellular neural networks, calculation start time of each layer have difference. In partic-ular, the first layer is calculated in first, and {{output of the}} first layer is inputted to the second layer. By using diffusion pro-cessing in this structure, we expect that the complex image processing using the CNN is realized. We confirm the char-acteristics of <b>our</b> two-layer <b>CNN</b> by computer simulation. 1...|$|R
40|$|In recent years, deep {{convolutional}} {{neural networks}} have achieved {{state of the}} art performance in various computer vision task such as classification, detection or segmentation. Due to their outstanding performance, CNNs are more and more used in the field of document image analysis as well. In this work, we present a CNN architecture that is trained with the recently proposed PHOC representation. We show empirically that <b>our</b> <b>CNN</b> architecture is able to outperform {{state of the art}} results for various word spotting benchmarks while exhibiting short training and test times. Comment: published as conference paper at the International Conference on Frontiers in Handwriting Recognition 201...|$|E
40|$|One key step in {{audio signal}} {{processing}} is {{to transform the}} raw signal into representations that are efficient for encoding the original information. Traditionally, people transform the audio into spectral representations, {{as a function of}} frequency, amplitude and phase transformation. In this work, we take a purely data-driven approach to understand the temporal dynamics of audio at the raw signal level. We maximize the information extracted from the raw signal through a deep convolutional neural network (CNN) model. <b>Our</b> <b>CNN</b> model is trained on the urbansound 8 k dataset. We discover that salient audio patterns embedded in the raw waveforms can be efficiently extracted through a combination of nonlinear filters learned by the CNN model...|$|E
40|$|Accurate {{detection}} of fingertips in depth image {{is critical for}} human-computer interaction. In this paper, we present a novel two-stream convolutional neural network (CNN) for RGB-D fingertip detection. Firstly edge image is extracted from raw depth image using random forest. Then the edge information is combined with depth information in <b>our</b> <b>CNN</b> structure. We study several fusion approaches and suggest a slow fusion strategy as a promising way of fingertip detection. As shown in our experiments, our real-time algorithm outperforms state-of-the-art fingertip detection methods on the public dataset HandNet with an average 3 D error of 9. 9 mm, and shows comparable accuracy of fingertip estimation on NYU hand dataset. Comment: Accepted by ICIP 201...|$|E
40|$|In this paper, {{we study}} the {{performance}} of different classifiers on the CIFAR- 10 dataset, and build an ensemble of classifiers to reach a better performance. We show that, on CIFAR- 10, K-Nearest Neighbors (KNN) and Convolutional Neural Network (CNN), on some classes, are mutually exclusive, thus yield in higher accuracy when combined. We reduce KNN overfitting using Principal Component Analysis (PCA), and ensemble it with a CNN to increase its accuracy. Our approach improves <b>our</b> best <b>CNN</b> model from 93. 33 % to 94. 03 %...|$|R
30|$|<b>Our</b> {{proposed}} <b>CNN</b> framework {{can significantly}} enhance {{the performance of}} image classification by jointly optimizing the classification loss (between the label of the original image and the output classification result) and the similarity loss (between the original image, positive image, and negative image). The parameter λ in Eq. (3) {{that is used to}} balance the softmax loss and loss plays an essential role in the proposed CNN architecture. With the λ is set to 1 or 0, the performance of the structure would degenerate to softmax loss or triplet loss, respectively. According to the process of error and trial, it is reasonable to assign a value greater than 0.5 to λ, which shows that the softmax loss should be more important than triplet loss in <b>our</b> proposed <b>CNN</b> architecture. To note that the introduction of triplet loss contributes substantially to the image classification according to the complementary information from both the positive and negative images. Meanwhile, it could encourage the intra-class similarity and inter-class difference at the same time. The proposed CNN architecture is suitable for characteristics of banana. Through combining the softmax loss with the newly presented triplet loss, the subtle difference between intra-category and inter-categories of bananas during different ripening stages can be differentiated from each other.|$|R
40|$|In {{this work}} we propose a neural network based image {{descriptor}} suitable for image patch matching, {{which is an}} important task in many computer vision applications. Our approach is influenced by recent success of deep convolutional neural networks (CNNs) in object detection and classification tasks. We develop a model which maps the raw input patch to a low dimensional feature vector so that the distance between representations is small for similar patches and large otherwise. As a distance metric we utilize L 2 norm, i. e. Euclidean distance, which is fast to evaluate and used in most popular hand-crafted descriptors, such as SIFT. According to the results, our approach outperforms state-of-the-art L 2 -based descriptors and {{can be considered as}} a direct replacement of SIFT. In addition, we conducted experiments with batch normalization and histogram equalization as a preprocessing method of the input data. The results confirm that these techniques further improve the performance of the proposed descriptor. Finally, we show promising preliminary results by appending <b>our</b> <b>CNNs</b> with recently proposed spatial transformer networks and provide a visualisation and interpretation of their impact. Comment: The paper was published in ACCV 2016 Workshops proceedings (Workshop on Interpretation and Visualization of Deep Neural Nets...|$|R
