76|358|Public
5000|$|Operation 5 - Insertion of the lexical (morphological) item. The item is {{not present}} in the <b>output</b> <b>language.</b>|$|E
50|$|Languages {{that lack}} {{flexibility}} in specifying string literals make it particularly cumbersome to write programming code that generates other programming code. This {{is particularly true}} when the generation language is the same or similar to the <b>output</b> <b>language.</b>|$|E
50|$|Some screen reading {{programs}} also include language verbosity, which automatically detects verbosity settings related to speech <b>output</b> <b>language.</b> For example, if a user navigated to a website {{based in the}} United Kingdom, the text would be read with an English accent.|$|E
50|$|The {{process of}} {{disassembling}} and parsing input {{is more complex}} than the reverse process of assembling <b>output</b> in natural <b>language</b> generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when <b>outputting</b> <b>language.</b>|$|R
40|$|This {{deliverable}} {{describes the}} development and prototypical implementation of the AVANTSSAR Validation Platform. More specifically, we describe here the architecture, the input and <b>output</b> <b>languages,</b> and the two main compo-nents of the platform: the Orchestrator and the Validator. Deliverable details Deliverable version: v 1. 0 Classification: publi...|$|R
30|$|All {{articles}} {{relevant to}} ASD plus all {{articles published in}} autism journals were retrieved using Scopus database. VOSviewer software was used to create density and network visualization maps. Bibliometric indicators were investigated by analyzing annual research <b>output,</b> <b>languages,</b> countries, institutions, journals, title terms, highly cited articles, and co-authorship relations.|$|R
5000|$|Code generation: the {{transformed}} {{intermediate language}} is translated into the <b>output</b> <b>language,</b> usually the native machine {{language of the}} system. This involves resource and storage decisions, such as deciding which variables to fit into registers and memory and the selection and scheduling of appropriate machine instructions along with their associated addressing modes (see also Sethi-Ullman algorithm). Debug data may {{also need to be}} generated to facilitate debugging.|$|E
5000|$|A {{statistical}} {{language model}} is a probability distribution over sequences of words. Given such a sequence, say of length , it assigns a probability [...] to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications, especially ones that generate text as an <b>output.</b> <b>Language</b> modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, information retrieval and other applications.|$|E
5000|$|In compilers, {{the front}} end {{translates}} a computer programming source code into an intermediate representation, and the back end works with the intermediate representation to produce code in a computer <b>output</b> <b>language.</b> The back end usually optimizes to produce code that runs faster. The front-end/back-end distinction can separate the parser section that deals with source code and the back end that generates code and optimizes. Some designs, such as GCC, offer choices between multiple front ends (parsing different source languages) or back ends (generating code for different target processors).|$|E
40|$|The {{concept of}} macro tree {{transducer}} is a formal model for studying properties of syntax [...] directed translations. In this paper, for <b>output</b> <b>languages</b> of producing, nondeleting, and noncopying macro tree transducers, we introduce a pumping lemma. We apply the pumping lemma {{to gain the}} following result: there is no producing and nondeleting macro tree transducer which computes the set of all monadic trees with double exponential height as output. Keywords: Macro tree transducer, Primitive Recursion, Pumping Lemma 1 Introduction Pumping lemmata are useful tools to prove that a given language L is not an element of a class L of formal languages, where L usually is defined by a class of grammars or translation schemes. Pumping lemmata have been investigated for example for string languages, tree languages, and graph languages. First Aho and Ullman have inspected pumping lemmata for <b>output</b> <b>languages</b> of translation schemes in [1], namely for generalized syntax directed translations. Perraul [...] ...|$|R
40|$|This {{deliverable}} {{describes the}} development and prototypical implementation of the AVANTSSAR Validation Platform. More specifically, we describe here the architecture, the input and <b>output</b> <b>languages,</b> and the three main com-ponents of the platform: the Orchestrator, the Validator, and the Connec-tors layer. Each web service composing the AVANTSSAR Platform service-oriented architecture is described, and {{an overview of the}} AVANTSSAR Plat-form Web Interface is provided. Deliverable detail...|$|R
50|$|The Polychrony toolset is an Open Source {{development}} environment for critical/embedded systems based on SIGNAL, a real-time polychronous data-flow language. It provides a unified model-driven environment to perform design exploration by using top-down and bottom-up design methodologies formally supported by design model transformations from specification to implementation and from synchrony to asynchrony. It can {{be included in}} heterogeneous design systems with various input formalisms and <b>output</b> <b>languages.</b>|$|R
40|$|In the EMIME project we {{have studied}} {{unsupervised}} cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms {{to adapt the}} synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use {{the voice of the}} speaker in the input language to utter the translated sentences in the <b>output</b> <b>language.</b> In mobile environments this enhances the users ’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the <b>output</b> <b>language.</b> ...|$|E
40|$|This paper {{describes}} {{a novel approach}} based on voice conversion (VC) to speaker-adaptive speech synthesis for speech-tospeech translation. Voice quality of translated speech in an <b>output</b> <b>language</b> is usually {{different from that of}} an input speaker of the translation system since a text-to-speech system is developed with another speaker’s voices in the <b>output</b> <b>language.</b> To render the input speaker’s voice quality in the translated speech, we propose a voice quality control method based on one-tomany eigenvoice conversion (EVC) and language-dependent prosodic conversion. Spectral parameters of the translated speech are effectively converted by one-to-many EVC enabling unsupervised speaker adaptation. Moreover, prosodic parameters are modified considering their global differences between the input and output languages. The effectiveness of the proposed method is confirmed by experimental evaluations on cross-lingual VC among Japanese, English, and Chinese. Index Terms: speech-to-speech translation, speech synthesis...|$|E
40|$|INTERSPEECH 2011 : 12 th Annual Conference of the International Speech Communication Association, 28 - 31 August, 2011, Florence, Italy. This paper {{describes}} {{a novel approach}} based on voice conversion (VC) to speaker-adaptive speech synthesis for speech-to-speech translation. Voice quality of translated speech in an <b>output</b> <b>language</b> is usually {{different from that of}} an input speaker of the translation system since a text-to-speech system is developed with another speaker's voices in the <b>output</b> <b>language.</b> To render the input speaker's voice quality in the translated speech, we propose a voice quality control method based on one-to-many eigenvoice conversion (EVC) and language-dependent prosodic conversion. Spectral parameters of the translated speech are effectively converted by one-to-many EVC enabling unsupervised speaker adaptation. Moreover, prosodic parameters are modified considering their global differences between the input and output languages. The effectiveness of the proposed method is confirmed by experimental evaluations on cross-lingual VC among Japanese, English, and Chinese...|$|E
40|$|We {{show that}} the class of string {{languages}} generated by linear context-free rewriting systems {{is equal to the}} class of <b>output</b> <b>languages</b> of deterministic tree- walking transducers. From equivalences that have previously been established we know that this class of languages is also equal to the string languages generated by context-free hypergraph grammars, multicomponent tree-adjoining grammars, and multiple contextfree grammars and to the class of yields of images of the regular tree languages under finite-copying top- down tree transducers...|$|R
5000|$|Format Engine {{for data}} {{connectivity}} link between various geoscience software applications. The format engine uses <b>Output</b> Input <b>Language</b> (OIL), an interpreted language to define various data formats.|$|R
40|$|This paper investigates {{computation}} by linear assemblies {{of complex}} DNA tiles, {{which we call}} string tiles. By {{keeping track of the}} strands as they weave back and forth through the assembly, we show that surprisingly sophisticated calculations can be performed using linear self-assembly. Examples range from generating an addition table to providing O(1) solutions to CNF-SAT and DHPP. We classify the families of languages that can be generated by various types of DNA molecules, and establish a correspondence to the existing classes ET 0 L_(ml) and ET 0 L_(fin). Thus, linear self-assembly of string tiles can generate the <b>output</b> <b>languages</b> of finite-visit Turing Machines...|$|R
40|$|This paper proposes an {{improved}} cross-lingualspeaker adaptation technique with considering the differencesbetween language-dependent average voices in a Speech-to-Speech Translation system. A state mapping based method hadbeen introduced for cross-lingual speaker adaptation in HMMbasedspeech synthesis. In this method, the transforms estimatedfrom the input language {{are applied to}} average voice models ofthe <b>output</b> <b>language</b> {{according to the state}} mapping information. However, the differences between average voices in the inputand <b>output</b> <b>language</b> may degrade the adaptation performance. To reduce the differences, we apply a global linear transformto output average voice models, which minimizes the symmetricKullback-Leibler divergence between two average voice models. From the experimental results, our approach could not obtaina better result than the original state mapping based method. This is because the global transform affects not only speakercharacteristics but also language identity in acoustic features, andthis degrades the synthetic speech quality. Therefore, it becomesclear that a technique which separate speaker and languageidentities is required...|$|E
40|$|In this paper, {{we present}} GIDL, a {{grounder}} for FO+. FO+ {{is a very}} expressive extension of first-order logic with sev-eral constructs such as inductive definitions, aggregates and arithmetic. We describe the input and <b>output</b> <b>language</b> of GIDL, and provide details about its architecture. In partic-ular, the core grounding algorithm implemented in GIDL is presented. We compare GIDL with other FO+ grounders and with grounders for Answer Set Programming...|$|E
40|$|We {{applied a}} {{structure}} learning model, Max-Margin Structure (MMS), to {{natural language processing}} (NLP) tasks, where {{the aim is to}} capture the latent relationships within the <b>output</b> <b>language</b> domain. We formulate this model as an extension of multi–class Support Vector Machine (SVM) and present a perceptron–based learning approach to solve the problem. Experiments are carried out on two related NLP tasks: part–of–speech (POS) tagging and machine translation (MT), illustrating the effectiveness of the model. 1...|$|E
40|$|In {{this paper}} we show that, {{in terms of}} {{generated}} <b>output</b> <b>languages,</b> non-deterministic population protocols are strictly more powerful than deterministic ones. Analyzing {{the reason for this}} negative result, we propose two slightly enhanced models, in which non-deterministic population protocols can be exactly simulated by deterministic ones. First, we consider a model in which interactions are not only between couples of agents, but also between triples and in which non-uniform initial states are allowed. We generalize this transformation and we prove a general property for a model with interactions between any number of agents. Second, we simulate any non-deterministic population protocol by a deterministic one in a model where a configuration can have an empty output. Non-deterministic and deterministic population protocols are then compared in terms of inclusion of their <b>output</b> <b>languages,</b> that is, in terms of solvability of problems. Two transformations, which realize this inclusion, are presented. The first one uses (again) the natural model with interactions of triples, but does not need non-uniform initial states. As before, this result is generalized for the natural model with interactions between any number of agents. The second transformation is a parameterized one with parameters depending on the transition graph of the considered non-deterministic protocol and on the population. Note that the transformations in the paper apply to a whole class of non-deterministic population protocols (for a proposed model), in contrast with the transformations proposed in previous works, which apply only to a specific sub-class of protocols (satisfying a so called "elasticity" condition) ...|$|R
40|$|Abstract. We {{survey work}} on statically type {{checking}} XML transformations, covering {{a wide range}} of notations and ambitions. The concept of type may vary from idealizations of DTD to full-blown XML Schema or even more expressive formalisms. The notion of transformation may vary from clean and simple transductions to domain-specific languages or integration of XML in general-purpose programming languages. Type annotations can be either explicit or implicit, and type checking ranges from exact decidability to pragmatic approximations. We characterize and evaluate existing tools in this design space, including a recent result of the authors providing practical type checking of full unannotated XSLT 1. 0 stylesheets given general DTDs that describe the input and <b>output</b> <b>languages.</b> ...|$|R
40|$|In this paper, {{we present}} Neural Phrase-based Machine Translation (NPMT). Our method {{explicitly}} models the phrase structures in output sequences using Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we {{introduce a new}} layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines. We also observe that our method produces meaningful phrases in <b>output</b> <b>languages...</b>|$|R
40|$|Yakker is a parser {{generator}} that supports semantic actions, lexical binding of semantic values, and speculative parsing {{techniques such as}} backtracking and context-free lookahead. To avoid executing semantic actions in speculative parses that will eventually be discarded, we divide parsing into two conceptually independent phases. In the first (early) phase, the parser explores multiple possible parse trees without executing semantic actions. The second (late) phase executes the delayed semantic actions once the first phase has determined they are necessary. Execution of the two phases can be overlapped. We structure the early phase as a transducer which maps the input language to an <b>output</b> <b>language</b> of labels. A string in the <b>output</b> <b>language</b> {{is a history of}} the semantic actions that would have been executed in a parse of the input. The late phase is implemented as a deterministic, recursive descent parse of the history. We formalize delayed semantic actions and discuss a number of practical issues involved in implementing them in Yakker, including our support for regular right part grammars and dependent parsing, the design of the data structures that support histories, and memory management techniques critical for efficient implementation...|$|E
40|$|Abstract — In {{this paper}} we discuss, {{within the context}} of {{artificial}} assistants performing everyday activities, a resolution method to disambiguate missing or not satisfactorily inferred action-specific information via explicit clarification. While argu-ing the lack of preexisting robot to human linguistic interaction methods, we introduce a novel use of Controlled Natural Languages (CNL) as means of <b>output</b> <b>language</b> and sentence construction for doubt verbalization. We additionally provide implemented working scenarios, state future possibilities and problems related to verbalization of technical cognition when making use of Controlled Natural Languages. I...|$|E
40|$|The {{proceedings}} containing {{this paper}} are published as technical report UNSW-CSE-TR- 0819 (September 2008) {{of the university}} of New South Wales (Sydney 2052, Australia) In this paper, we present GidL, a grounder for FO+. FO+ is a very expressive extension of first-order logic with several constructs such as inductive definitions, aggregates and arithmetic. We describe the input and <b>output</b> <b>language</b> of GidL, and provide details about its architecture. In particular, the core grounding algorithm implemented in GidL is presented. We compare GidL with other FO+ grounders and with grounders for Answer Set Programming. status: publishe...|$|E
40|$|The {{concept of}} macro tree {{transducer}} is a formal model for studying properties of syntax-directed translations and of functional languages {{which are based}} on primitive recursion. In this paper, for <b>output</b> <b>languages</b> of producing and nondeleting macro tree transducers, we introduce and prove a pumping lemma. We apply this pumping lemma to gain two results: (1) there is no producing and nondeleting macro tree transducer which computes the set of all monadic trees with double exponential height as output and (2) there are hierarchies of producing and nondeleting macro tree transducers with respect to their number of functions. (orig.) Available from FIZ Karlsruhe / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|Subsequential {{transducers}} {{constitute a}} formal model for translation {{that may be}} considered perhaps too simple to model translation between natural languages. However, their capability can suffice in limited-domain translation tasks. The finite state nature of subsequential transducers makes their integration with well-known Continuous Speech Recognition technology both easy and efficient. A recent algorithm allows the automatic learning of these transducers, given a sufficiently large set of examples of sentences and their corresponding translations, and it also allows the incorporation of syntactic restrictions of the input and/or <b>output</b> <b>languages.</b> In this paper, we describe an implementation of a Speech Translation System for limited domains which is fully trainable and capable of real time translation from speech input. ...|$|R
40|$|Statistical machine {{translation}} translates text from one language into another language using available parallel corpora. Machine translation models extract phrase translation based on word alignment <b>output.</b> However, <b>languages</b> are different, in many language pairs, {{it is impossible}} to find an equivalent translatio...|$|R
40|$|We de ne a weak -calculus, w, as a {{subsystem}} of {{the full}} -calculus with explicit substitutions *. We claim that w could be the archetypal <b>output</b> <b>language</b> of functional compilers, just as the -calculus is their universal input language. Furthermore, * could be the adequate theory to establish the correctness of functional compilers. Here we illustrate these claims by proving the correctness of four simpli ed compilers and runtime systems modeled as abstract machines. The four machines we prove are the Krivine machine, the SECD, the FAM and the CAM. Thereby, we give the rst formal proofs of Cardelli's FAM and of its compiler...|$|E
40|$|This paper {{suggests}} why {{the statistical}} revolution has left Natural Language Generation (NLG) largely untouched, but identies areas where interacting constraints {{are hard to}} model by traditional methods, and where probabilistic models have something to oer. An interesting discourse phenomenon is discussed, to show {{how it might be}} dealt with using one of the handful of existing statistical approaches to NLG, and to identify diculties with their evaluation functions. The paper then argues that the maximum entropy framework oers a better approach, and suggests how it could be applied to problems in generation, using two heuristics to help an evaluator exploit features which would improve the quality of <b>output</b> <b>language...</b>|$|E
40|$|Projet PARAWe {{define a}} weak lambda-calculus, lambda-sigma-w, as a {{subsystem}} {{of the full}} lambda-calculus with explicit substitutions lambda-sigma-lift. We claim that lambda-sigma-w could be the archetypal <b>output</b> <b>language</b> of functional compilers, just as the lambda-calculus is their universal input language. Furthermore, lambda-sigma-lift could be the adequate theory to establish the correctness of simplified functional compilers. Here, we illustrate these claims by proving the correctness of four simplified compilers and runtime systems modeled as abstract machines. The four machines we prove are the Krivine machine, the SECD, the FAM and the CAM. Thereby, we give the first formal proofs of Cardelli's FAM and of its compiler...|$|E
3000|$|In the {{recognition}} process, the acoustic models provide {{a number of}} hypotheses as <b>output.</b> The <b>language</b> model provides complementary knowledge about how likely those hypotheses are. The balance of the two components in the Viterbi decoding can be controlled using the grammar scale factor [...]...|$|R
50|$|A {{programming}} {{language is a}} formal language that specifies a set of instructions {{that can be used}} to produce various kinds of <b>output.</b> Programming <b>languages</b> generally consist of instructions for a computer. Programming languages can be used to create programs that implement specific algorithms.|$|R
40|$|International audienceXSLT is a {{language}} for transforming XML documents into other XML documents. Despite its 16 years long life, the RDF Semantic Web language still waits its transformation language. Some propositions have been done, relying on and extending XSLT, {{but none of}} them became widely used. In this paper, we present a radically new transformation language for RDF, called STTL. It enables to transform RDF into RDF as well as any other text format. The originality and power of STTL is that it is based on SPARQL. We designed it as a lightweight extension to SPARQL and we compile it into standard SPARQL. We present a generic transformation rule engine implementing STTL and several RDF transformers we defined for various <b>output</b> <b>languages,</b> showing STTL's expressive power...|$|R
