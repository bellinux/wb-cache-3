26|13|Public
50|$|Hach Company {{manufactures}} and distributes analytical {{instruments and}} reagents {{used to test}} the quality of water and other liquid solutions. Manufactured and distributed worldwide, Hach systems are designed to simplify analysis by offering <b>on-line</b> <b>instrumentation,</b> portable laboratory equipment, prepared reagents, easy-to-follow methods, and technical support.|$|E
50|$|Although {{particles}} and TOC are usually measured using on-line methods, {{there is significant}} value in complimentary or alternative off-line lab analysis. The value of the lab analysis has two aspects: cost and speciation. Smaller UPW facilities that cannot afford to purchase <b>on-line</b> <b>instrumentation</b> often choose off-line testing. TOC {{can be measured in}} the grab sample at a concentration as low as 5 ppb, using the same technique employed for the on-line analysis (see on-line method description). This detection level covers the majority of needs of less critical electronic and all pharmaceutical applications. When speciation of the organics is required for troubleshooting or design purposes, liquid chromatography-organic carbon detection (LC-OCD) provides an effective analysis. This method allows for identification of biopolymers, humics, low molecular weight acids and neutrals, and more, while characterizing nearly 100% of the organic composition in UPW with sub-ppb level of TOC.|$|E
40|$|This paper {{provides}} {{a method for}} determining when, and for which instruments, direct interfacing with a computer is cost-effective. It discusses the differences between automatic and manual inputting of laboratory test results and analyzes {{the costs and benefits}} of <b>on-line</b> <b>instrumentation.</b> A complete theoretical model for analyzing projected benefits is described, along with a simplified version based on estimates of the time involved for both manual and automated systems...|$|E
40|$|CAMAC is a {{standard}} for <b>on-line</b> computer <b>instrumentation</b> and control. Owing to the world-wide acceptance of CAMAC, standardized and mutually compatible equipment is now offered by {{a great number of}} manufacturers. CAMAC systems are modular, and with the range of modules commercially available, it is possible to build up flexible and complex data processing systems. Use of CAMAC requires a CAMAC- compatible entrance (interface) to the computer. Once established, users are later on completely independent of the type of computer used. General aspects, CAMAC systems for NORD- 1 and NORD- 10 are presented. The former is developed at Physics Institute, Oslo, and the latter is developed {{as a result of the}} CERN-contract gained by NORSK DATAELEKTRONIKK...|$|R
5000|$|Handheld data {{collectors}} and analyzers are now commonplace on non-critical or {{balance of plant}} machines on which permanent <b>on-line</b> vibration <b>instrumentation</b> cannot be economically justified. The technician can collect data samples {{from a number of}} machines, then download the data into a computer where the analyst (and sometimes artificial intelligence) can examine the data for changes indicative of malfunctions and impending failures. For larger, more critical machines where safety implications, production interruptions (so-called [...] "downtime"), replacement parts, and other costs of failure can be appreciable (determined by the criticality index), a permanent monitoring system is typically employed rather than relying on periodic handheld data collection. However, the diagnostic methods and tools available from either approach are generally the same.|$|R
40|$|We {{present a}} method to access {{efficiently}} and effectively images in large astronomical archives. This method allows users to interactively participate to the retrieval process. They may virtually "navigate" through the archive for choosing images that are relevant for their purposes. The navigation process is performed by switching {{back and forth between}} three retrieval-system modes: a query mode, a browse mode and an inspection mode. The method has been applied to an image retrieval system to access solar radio spectrograms archived at ETH in Zurich. The system, called ASPECT, allows to access over 50, 000 data sets <b>on-line.</b> Keywords: <b>instrumentation,</b> information systems 1. Overview Retrieval of relevant information from large astronomical archives is becoming increasingly problematic as the amount of data grows. For a given application, images containing relevant information can become "diluted" in an ocean of irrelevant images. As a result, it is more and more difficult to mine [...] ...|$|R
40|$|Predicting the {{response}} of components or systems exposed to ionising radiation is often very difficult. Reliable radiation tolerance estimates therefore typically rely on extended radiation experiments. SCKCEN, the Belgian Nuclear Research Center, is operating different gamma irradiation facilities, with wide gamma dose rates ranging from 0. 1 Gy/h up to 50 kGy/h and with environmental control and <b>on-line</b> <b>instrumentation</b> capabilities. These facilities allow to assess the radiation tolerance of components or systems, intended for applications in harsh environments ranging from space missions to fusion reactor diagnostics. I...|$|E
40|$|Radiation {{hardness}} assurance {{for applications}} in harsh environments ranging from space missions to fusion reactor diagnostics {{need to be}} based on extended radiation experiments. SCKCEN, the Belgian Nuclear Research Center, is operating different gamma irradiation facilities, with gamma dose rates ranging from 0. 1 Gy/h up to 50 kGy/h and with environmental control and <b>on-line</b> <b>instrumentation</b> capabilities. In collaboration with the cyclotron of the University of Brussels, experiments in charged particle beams (protons and alpha up to 40 MeV, deuterium up to 22 MeV) can also be performed...|$|E
40|$|This paper {{describes}} how the Connoisseur advanced control package has been successfully applied to the broke and retention systems of a high-speed paper machine to provide increased stability and runnability. The paper provides {{an explanation of the}} plant testing, identification and modelling techniques used to develop a model-based predictive controller. In addition, details of the controller’s performance are presented. Increased competition within the paper industry has led to greater emphasis on continuous improvement of product quality and profit maximisation through optimisation of machine runnability. This demand for continuous improvement has {{led to the development of}} <b>on-line</b> <b>instrumentation,</b> allowing key wet-en...|$|E
40|$|This report {{describes}} {{a project that}} is designing and implementing a prototype of a generalized public-key certificate infrastructure that can provide a highly scalable and transparent infrastructure for both authentication and authorization of access to network based services. The goal is to produce a generalized security architecture that can encode, distribute, and protect the information needed to enable routine secure availability of remote instrumentation in a distributed collaboratory environment. We report here {{on the progress of}} a prototype implementation that will demonstrate the concepts, identify the issues, and contribute to the evaluation of currently competing approaches for several aspects of the problem. 1. 0 Introduction 1. 1 The need The need for strong, flexible security for <b>on-line</b> research <b>instrumentation</b> is motivated by the following observation: "The access to a remote collaborative environment, whether it is for monitoring or for control of experiments, requ [...] ...|$|R
40|$|EMISSARY is a {{new system}} under {{development}} for an advanced data visualization, spatio-temporal modeling interface, and field portable tools for in-situ data exploration of sensormicronets and distributed instrument management. Our research is defining a set of functional capabilities for an advanced system for in the field communication with a sensormicronet and distributed <b>instrumentation,</b> <b>on-line</b> and interactive modeling tools, and system diagnostics and configuration interfaces. Conceptually this system would be a hand-held PDA or laptop class computer with wireless communication to mote-class devices (sensor nodes), networked data-logging instruments, WAN/LAN, and the Internet...|$|R
40|$|Dynamic Dielectric {{measurements}} {{made over}} a wide range of frequency provide a sensitive and convenient means for monitoring the cure process in thermosets and thermoplastics. The measurement of dielectric relaxation is one of only a few instrumental techniques available for studying molecular properties in both the liquid and solid states. Furthermore, it is probably the only convenient experimental technique for studying the polymerization process of going from a monomeric liquid of varying viscosity to a crosslinked, insoluble, high temperature solid. The objective of the research is to develop <b>on-line</b> dielectric <b>instrumentation</b> for quantitative nondestructive material evaluation and closed loop smart cure cycle control. The key is to relate the chemistry of the cure cycle process to the dielectric properties of the polymer system by correlating the time, temperature, and frequency dependent dielectric measurements with chemical characterization measurements. Measurement of the wide variation in magnitude of the complex permittivity with both frequency and state of cure, coupled with chemical characterization work, have been shown in the laboratory to have the potential to determine: resin quality, composition and age; cure cycle window boundaries; onset of flow and point of maximum flow; extent of and completion of reaction; evolution of volatiles; T sub g; and, crosslinking and molecular weight buildup...|$|R
40|$|Within the European High Temperature Reactor Technology Network (HTR-TN) {{and related}} {{projects}} {{a number of}} HTR fuel irradiations are planned in the High Flux Reactor Petten (HFR), The Netherlands, with the objective to explore the potential of recently produced fuel for even higher temperature and burn-up. Irradiating fuel under defined conditions to extremely high burn-ups will provide {{a better understanding of}} fission product release and failure mechanisms if particle failure occurs. After an overview of the irradiation rigs used in the HFR, this paper sums up data collected from previous irradiation tests in terms of thermocouple data. Some R&D for further improvement of thermocouples and other <b>on-line</b> <b>instrumentation</b> will be outlined. JRC. DDG. F. 4 -Safety of future nuclear reactor...|$|E
40|$|High rate {{anaerobic}} treatment reactors {{are able to}} uncouple solids and liquid retention time, resulting in high biomass concentrations. Principal advantages of {{anaerobic treatment}} include: energy efficiency, low biomass yield, low nutrient requirement and high volumetric organic loadings. In order to facilitate small reactors operation with stable and good performance automatic process control systems in combination with <b>on-line</b> <b>instrumentation</b> are proposed. The paper reviews the development and availability of the principal instrumentation for anaerobic treatment processes. First, the most important measuring principles are discussed, followed by {{a review of the}} most important process variables with emphasis on the development of their instrumental measurement techniques and application in research. Finally, a summary of actual application of instrumentation in full-scale anaerobic treatment plants is presente...|$|E
40|$|Granular solid targets made of {{fluidized}} tungsten pow-der or {{a static}} pebble bed of tungsten spheres, {{have been proposed}} and are being studied as an alternative configu-ration for high-power (> 1 MW of beam power) target sys-tems, suitable for a future Super Beam or Neutrino Fac-tory. Due {{to the lack of}} experimental data on this field, a feasibility experiment was performed in HiRadMat facil-ity of CERN to address the effect {{of the impact of the}} SPS beam (440 GeV/c) on a static tungsten powder target. <b>On-line</b> <b>instrumentation</b> such as high-speed photography and laser-Doppler vibrometry was employed. Preliminary re-sults show a powder disruption speed of less than 0. 6 m/s at 3 1011 protons/pulse while the disruption speed appears to scale with the beam intensity...|$|E
40|$|Abstract—Most {{algorithms}} are run-to-completion {{and provide}} one answer upon completion and no answer if interrupted before completion. On the other hand, anytime algorithms have a monotonic increasing utility with {{the length of}} execution time. Our investigation focuses {{on the development of}} time-bounded anytime algorithms on Graphics Processing Units (GPUs) to trade-off the quality of output with execution time. Given a time-varying workload, the algorithm continually measures its progress and the remaining contract time to decide its execution pathway and select system resources required to maximize the quality of the result. To exploit the quality-time tradeoff, {{the focus is on the}} construction, <b>instrumentation,</b> <b>on-line</b> measurement and decision making of algorithms capable of efficiently managing GPU resources. We demonstrate this with a Parallel A * routing algorithm on a CUDA-enabled GPU. Th...|$|R
40|$|Gy 2 ̆ 7 s {{sampling}} theory (Gy, 1982) can {{be applied}} to coal sampling problems to calculate the dependence of sampling variance on both the intrinsic coal properties (size distribution and washability) and the variations in coal stream properties with time (characterised by the corrected variogram). The theory has not been widely used by the coal industry in Australia, perhaps {{because of a lack of}} understanding of Gy 2 ̆ 7 s exposition. An explanation of the theory is given in simple terms, and equations useful to the coal sampler are presented and their use illustrated. Further, the application of the theory to the handling of data from <b>on-line</b> ash measurement <b>instrumentation</b> is discussed and illustrated. It is demonstrated that an on-line ash gauge can be used to characterise the time variations in coal properties in terms of a variogram which can, in turn, be corrected for gauge accuracy and then used in design calculations for a mechanical sampling system...|$|R
40|$|Current {{trends in}} {{technology}} are {{leading to a}} need for ever smaller and more complex featured surfaces. The techniques for manufacturing these surfaces are varied but are tied together by one limitation; the lack of useable, <b>on-line</b> metrology <b>instrumentation.</b> Current metrology methods require the removal of a workpiece for characterisation which leads to machining down-time, more intensive labour and generally presents a bottle neck for throughput. In order {{to establish a new}} method for on-line metrology at the nanoscale investigation are made into the use of optical fibre interferometry to realise a compact probe that is robust to environmental disturbance. Wavelength tuning is combined with a dispersive element to provide a moveable optical stylus that sweeps the surface. The phase variation caused by the surface topography is then analysed using phase shifting interferometry. A second interferometer is wavelength multiplexed into the optical circuit in order to track the inherent instability of the optical fibre. This is then countered using a closed loop control to servo the path lengths mechanically which additionally counters external vibration on the measurand. The overall stability is found to be limited by polarisation state evolution however. A second method is then investigated and a rapid phase shifting technique is employed in conjunction with an electro-optic phase modulator to overcome the polarisation state evolution. Closed loop servo control is realised with no mechanical movement and a step height artefact is measured. The measurement result shows good correlation with a measurement taken with a commercial white light interferometer. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Tracking the {{changing}} dynamics of object-oriented frameworks[5], design patterns[7], architectural styles[8], and subsystems {{during the development}} and reuse cycle can aid producing complex systems. Unfortunately, current object-oriented programming tools are relatively oblivious to the rich architectural abstractions in a system. This paper shows that architecture-oriented visualization, the graphical presentation of system statics and dynamics {{in terms of its}} architectural abstractions, is highly beneficial in designing complex systems. In addition, the paper presents architecture-aware instrumentation, a new technique for building efficient <b>on-line</b> <b>instrumentation</b> to support architectural queries. We demonstrate the effectiveness and performance of the scheme with case studies {{in the design of the}} Choices object-oriented operating system. 1 Introduction Designers conceive complex systems as architectures with design patterns[7], frameworks[5], architectural styles[8], and subsyste [...] ...|$|E
40|$|As {{developers}} and maintainers of information infrastructure components {{in an open}} network environment [...] large-scale Web servers, digital libraries, <b>on-line</b> <b>instrumentation</b> systems, etc. [...] we are continually exposed to the malicious elements of cyberspace. We also {{believe that there is}} a great deal of emerging technology that can be used to protect on-line systems: public-key certificate-based authentication and authorization systems, public-key infrastructure to validate both human and system identities, various system-level techniques that will provide much more fine-grained control over what we expose to cyberspace, etc. This document provide an overview of some of the current thinking about the vulnerabilities and threats in cyberspace, and some of the directions for addressing those issues. We also consider the potential contributions of the Lawrence Berkeley National Laboratory, and the DOE Labs, in general...|$|E
40|$|Traditionally, a simulation-based {{optimization}} (SO) {{system is}} designed as a black-box in which the internal details of the optimization process is hidden from the user and only the final optimization solutions are presented. As {{the complexity of the}} SO systems and the optimization problems to be solved increases, instrumentation – a technique for monitoring and controlling the SO processes – is becoming more important. This paper proposes a white-box approach by advocating the use of instrumentation components in SO systems, based on a component-based architecture. This paper argues that a number of advantages, including efficiency enhancement, gaining insight from the optimization trajectories and higher controllability of the SO processes, can be brought out by an <b>on-line</b> <b>instrumentation</b> approach. This argument is supported by the illustration of an instrumentation component developed for a SO system designed for solving real-world multi-objective operation scheduling problems...|$|E
40|$|Biomass burning {{represents}} a major global source of aerosols impacting direct radiative forcing and cloud properties. Thus, {{the goal of}} a number of current studies involves developing {{a better understanding of how}} the chemical composition and mixing state of biomass burning aerosols evolve during atmospheric aging processes. During the Ice in Cloud Experiment – Layer Clouds (ICE-L) in fall of 2007, smoke plumes from two small Wyoming Bureau of Land Management prescribed burns were measured by <b>on-line</b> aerosol <b>instrumentation</b> aboard a C- 130 aircraft, providing a detailed chemical characterization of the particles. After ~ 2 – 4 min of aging, submicron smoke particles, produced primarily from sagebrush combustion, consisted predominantly of organics by mass, but were comprised primarily of internal mixtures of organic carbon, elemental carbon, potassium chloride, and potassium sulfate. Significantly, 100 % of the fresh biomass burning particles contained minor mass fractions of nitrate and sulfate, suggesting that hygroscopic material is incorporated very near or at the point of emission. The mass fractions of ammonium, sulfate, and nitrate increased with aging up to ~ 81 – 88 min and resulted in acidic particles, with both nitric acid and sulfuric acid present. Decreasing black carbon mass concentrations occurred due to dilution of the plume. Increases in the fraction of oxygenated organic carbon and the presence of dicarboxylic acids, in particular, were observed with aging. Cloud condensation nuclei measurements suggested all particles > 100 nm were active at 0. 5 % water supersaturation in the smoke plumes, confirming the relatively high hygroscopicity of the freshly emitted particles. For immersion/condensation freezing, ice nuclei measurements at − 32 °C suggested activation of ~ 0. 03 – 0. 07 % of the particles with diameters greater than 500 nm...|$|R
40|$|Most {{algorithms}} are run-to-completion {{and provide}} one answer upon completion and no answer if interrupted before completion. On the other hand, anytime algorithms have a monotonic increasing utility with {{the length of}} execution time. Our investigation focuses {{on the development of}} time-bounded anytime algorithms on Graphics Processing Units (GPUs) to trade-off the quality of output with execution time. Given a time-varying workload, the algorithm continually measures its progress and the remaining contract time to decide its execution pathway and select system resources required to maximize the quality of the result. To exploit the quality-time tradeoff, {{the focus is on the}} construction, <b>instrumentation,</b> <b>on-line</b> measurement and decision making of algorithms capable of efficiently managing GPU resources. We demonstrate this with a Parallel A* routing algorithm on a CUDA-enabled GPU. The algorithm execution time and resource usage is described in terms of CUDA kernels constructed at design-time. At runtime, the algorithm selects a subset of kernels and composes them to maximize the quality for the remaining contract time. We demonstrate the feedback-control between the GPU-CPU to achieve controllable computation tardiness by throttling request admissions and the processing precision. As a case study, we have implemented AutoMatrix, a GPU-based vehicle traffic simulator for real-time congestion management which scales up to 16 million vehicles on a US street map. This is an early effort to enable imprecise and approximate real-time computation on parallel architectures for stream-based timebounded applications such as traffic congestion prediction and route allocation for large transportation networks...|$|R
40|$|Interactance spectra (700 – 1100 nm) of intact fruit possess {{features}} {{interpreted as}} being {{due to the}} dilution and perturbation of water OH stretching and combination bands (with {{the second and third}} overtones of OH stretching at 960 nm and 740 nm, respectively), while the contribution from citric acid CH 2 and OH stretching and combination bands are not obvious. A model developed using interactance spectra collected from the cut surfaces of lime fruit (mean ± SD of 7. 3 ± 0. 51 g citric acid equivalents 100 mL– 1, units subsequently presented as % w/v) achieved prediction results of RMSEP = 0. 16 %, rp 2 = 0. 79, bias = – 0. 03 %. However, for intact lime fruit, model calibration results (RMSECV = 0. 16, rcv 2 = 0. 85) were markedly better than prediction results (RMSEP = 0. 30,rp 2 = 0. 49). For a low total titratable acidity (TTA) product, peach (with spectra collected across fruit maturity stages; mean ± SD of 0. 88 ± 0. 17 %), calibration results were relatively poor (RMSECV = 0. 09 %, rcv 2 = 0. 79) and the model failed in prediction (RMSEP = 0. 10 %, rp 2 = 0. 00, bias = 0. 02 %). It is concluded that interactance geometry shortwave (700 – 1100 nm) near infrared spectroscopy using diode array instrumentation and an interactance optical geometry suited to <b>on-line</b> or field-portable <b>instrumentation</b> used for Brix and DM assessment is not appropriate for assessment of the acidity of intact low TTA fruit and has limited use for high TTA fruit...|$|R
40|$|Anaerobic digesters {{are often}} not {{operated}} to full capacity and are recurrently subjected to adverse operational practices due to: temperature fluctuations, inconstant feeding regimes, variable solids content on the feed, changes in loading rates etc. The use of standard online monitoring indicators (pH, alkalinity, gas production and compositions) is insufficient to detect the perturbation {{at an early stage}} as these parameters are linked to the final product of the process, causing delays when diagnosing digester imbalances. On the other hand, volatile fatty acids (VFA) have been widely recognized as a key parameter for understanding and controlling anaerobic processes as they are intermediate products and real time indicators of the digester stability. Application of <b>on-line</b> <b>instrumentation</b> for VFA measurement has been limited as all developed instrumentations are based on expensive equipments (Gas Chromatograpy, High Performance Liquid Chromatography, Fourier Transform Infra-Red spectrometer etc.) and require. sample preparations involving the use of filtration, membranes, chemical additions, therefore triggering extensive maintenance. Cont/d...|$|E
40|$|The U. S. Department of Energy (DOE) has {{millions}} of gallons of radioactive liquid and sludge wastes that must be retrieved from underground storage tanks. This waste, in the form of slurries, must be transferred and processed to a final form, such as glass logs. <b>On-line</b> <b>instrumentation</b> to measure the properties of these slurries in real-time during transport is needed in order to prevent plugging and reduce excessive dilution. The results, describes a collaborative effort between Pacific Northwest National Laboratory (PNNL) and the University of Washington to develop a completely new method for using ultrasonics to measure the particle size and viscosity of a slurry. The concepts are based on work in optics on grating-light-reflection spectroscopy (GLRS) at the University of Washington and work on ultrasonic diffraction grating spectroscopy (UDGS) carried out at PNNL. The objective of the research was to extend the GLRS theory for optics to ultrasonics, and to demonstrate its capabilities of UDGS. The proposed ultrasonic method could result in an instrument that would be simple, rugged, and very compact, allowing it to be implemented as part of a pipeline wall at facilities across the DOE comple...|$|E
40|$|The {{objective}} {{of this paper is}} to explore the range of methods and strategies available for the process control and optimization of monoclonal antibody production by hybridoma cell culture. Emphasis will be placed on the choice of the level of complexity incorporated into the process control and optimisation procedure. It will be shown that the behaviour of hybridomas in culture is influenced by sophisticated cellular metabolic activities and various interactive environmental factors and that the understanding and modelling of the way hybridomas grow in the bioreactor should enable optimisation of bioreactor operating conditions to achieve maximum monoclonal antibody formation. However, due to the lack of <b>on-line</b> <b>instrumentation</b> of important biological variables and the incomplete knowledge of hybridoma cultivation process, there exist many limitations and challenges to the advent of applications of process control and optimisation in this field. To solve the problem, introduction of industrially practical biological measurements and development of new control concepts are inevitable. At the end of this paper, we shall discuss possible schemes for the control of the physiological state of cells in order that balanced cell growth and maximum monoclonal antibody synthesis may be achieved...|$|E
40|$|Proceedings of the 1995 Georgia Water Resources Conference, April 11 and 12, 1995, Athens, Georgia. Research {{efforts in}} {{wastewater}} treatment plants are becoming less difficult because more automated <b>on-line</b> <b>instrumentation</b> for water quality research has become available. Instrumentation for water quality research may now be found by calling instrumentation, process control, and automation suppliers. Engineers then can integrate instruments from various manufacturers into a system which meets the monitoring needs of the research experiment. Recent advances in instrument technology, coupled with custom system integration have met the needs of contemporary WWTP research. Sponsored and Organized by: U. S. Geological Survey, Georgia Department of Natural Resources, The University of Georgia, Georgia State University, Georgia Institute of TechnologyThis book was published by the Carl Vinson Institute of Government, The University of Georgia, Athens, Georgia 30602 with partial funding provided by the U. S. Department of Interior, Geological Survey, through the Georgia Water Research Institute as authorized by the Water Resources Research Act of 1990 (P. L. 101 - 397). The views and statements advanced in this publication are solely {{those of the authors}} and do not represent official views or policies of the University of Georgia or the U. S. Geological Survey or the conference sponsors...|$|E
40|$|For decades, {{resistivity}} instrumentation {{has been}} (and remains) {{the first choice}} to detect catastrophic events in de-ionized (DI) water purification systems immediately, though reliance upon TOC analyzers is growing. But with the quality and reliability of water purification systems always improving, resistivity instrumentation is sometimes taken for granted. Furthermore, the costs of water production are continually decreasing, and {{this has led to}} a more careful analysis of the selection criteria for <b>on-line</b> <b>instrumentation.</b> New technologies and customer demands for instrumentation require lower costs, 100 % reliability, ease-ofuse/calibration, smaller packages, and increased versatility – not just improved accuracy. For the last few years, we have been working on new resistivity measurement technologies {{to meet the needs of}} today’s UPW systems and those of the next century. We will demonstrate measurement instrumentation with 1) a dynamic range to measure high resistivity fluids such as pure water and organic cleansing reagents, all the way up to high conductivity fluids such as acids/base concentrations, 2) an improved temperature measurement by> 10 -fold, 3) an improved resistivity measurement to achieve reliable sub-ppb LODs, 4) a versatility to measure flow, pH, pressure, tank level, % rejection, % recovery, DI-Cap ® (de-ionization capacity), and TOC reduction, and 5) a cost per measurement that is> 50 % lower than 10 years ago...|$|E
40|$|Our {{capabilities}} of both generating and collecting data have been increasing {{rapidly in the}} last several decades. Contributing factors include the widespread use of bar codes for most commercial products, the computerization of many business, scienti c and government transactions and managements, and advances in data collection tools ranging from scanned texture and image platforms, to <b>on-line</b> <b>instrumentation</b> in manufacturing and shopping, and to satellite remote sensing systems. In addition, popular use of the World Wide Web as a global information system has ooded us with a tremendous amount of data and information. This explosive growth in stored data has generated an urgent need for new techniques and automated tools that can intelligently assist us in transforming the vast amounts of data into useful information and knowledge. This book explores the concepts and techniques of data mining, a promising and ourishing frontier in database systems and new database applications. Data mining, also popularly referred to as knowledge discovery in databases (KDD), is the automated or convenient extraction of patterns representing knowledge implicitly stored in large databases, data warehouses, and other massive information repositories. Data mining is a multidisciplinary eld, drawing work from areas including database technology, arti cial intelligence, machine learning, neural networks, statistics, pattern recognition, knowledge based systems, knowledge acquisition, information retrieval, high performance computing, and data visualization. We present the material i...|$|E
40|$|In {{previous}} conferences {{the authors}} have reported on toxic fumes generated by the detonation of ANFO. The research reported here extends the earlier work to include an emulsion blasting agent and ANFO/emulsion blends. Explosive mixtures were shot in 4 -inch Schedule 80 steel pipe in a chamber in the experimental mine at the Pittsburgh Research Laboratory (PRL). Following each shot, the fumes in the chamber were analyzed using <b>on-line</b> <b>instrumentation.</b> A major goal of the research was {{to gain a better}} understanding of the factors that lead to the generation of toxic fumes in blasting operations. Earlier studies have suggested that the high nitrogen dioxide and nitric oxide concentrations in product clouds might be the result of the poor confinement provided by relatively weak ground strata or the exposure of the explosive to ground water prior to shooting the shot. Various mixtures of ANFO and emulsion were detonated in schedule 80 steel pipe and galvanized sheet metal pipe to evaluate the effect of confinement. Explosive mixtures were also allowed to soak in water for less than one day, one week, one month, and two months to determine which explosive mixtures would be degraded and observe what effect this degradation had on fume production. Results indicated that the production of nitrogen dioxide increased with low confinement of the detonating explosive and with exposure of the explosive to water...|$|E
40|$|This study aims {{to develop}} a {{multi-stage}} scheme for damage detection for the cable-stayed Kap Shui Mun Bridge (Hong Kong) by using measured modal data from an <b>on-line</b> <b>instrumentation</b> system, and to perform a damage-identification simulation based on a precise three-dimensional finite element model of the bridge. This multi-stage diagnosis strategy aims at successive detection of the occurrence, location and extent of the structural damage. In the first stage, a novelty detection technique based on auto-associative neural networks is proposed for damage alarming. This method needs only a series of measured natural frequencies of the structure in intact and damage states, and is inherently tolerant of measurement error and uncertainties in ambient conditions. The goal in the second stage is to identify the deck segment or section that contains damaged member(s). For this purpose, the bridge deck is partitioned into 149 segments defined by 150 sections, and normalized index vectors derived from modal curvature and modal flexibility are presented for damage localization. The third stage consists in identifying specific damage member(s) and damage extent by using a multi-layer perceptron neural network. Only the structural members occuring in the identified segment are considered in the network input, and the combined modal parameters are used as the input vector for damage extent identification. Department of Civil and Environmental Engineerin...|$|E
40|$|The red-shifted S 65 T mutant green {{fluorescent}} protein (GFP) {{was used}} to compare the adenovirus (Ad) production and post-infection survival of 293 SF and 293 S cells in serum-free and serum-containing flask cultures, respectively. The GFP-expressing vector permitted the quantification of both the level of GFP expressed by infected cells and the infectious viral content of the cultures by flow cytometry in a simple, fast, sensitive, and reliable way. The GFP has the main advantage of fluorescing without any substrate addition. Infected cultures showed the coexistence of two populations of fluorescent cells, high-fluorescence cells (HFCs) and low-fluorescence cells (LFCs), in proportions that varied between 20 and 75 hpi. The gradual {{increase in the number of}} LFCs at the expense of HFCs correlated well with the increase in the number of dead cells. This relationship could be used for the continuous measure of a culture 2 ̆ 019 s viability with the appropriate <b>on-line</b> <b>instrumentation.</b> The post-infection death rate of infected 293 SF cells was higher than that of infected 293 S cells, but the level of GFP fluorescence in viable, highly fluorescent cells was similar in the two infected cell lines. The number of infectious viral particles (IVPs) was quantified in less than 24 h by an infection assay of 293 S cells in wells with viral particles extracted from the culture samples, and the results were more reproducible (110...|$|E
40|$|The US Department of Energy (DOE) has {{millions}} of gallons of radioactive liquid and sludge wastes that must be retrieved from underground storage tanks, transferred to treatment facilities, and processed to a final waste form. The wastes will be removed from the current storage tanks by mobilizing the sludge wastes and mixing them with the liquid wastes to create slurries. Each slurry would then be transferred by pipeline to the desired destination. To reduce the risk of plugging a pipeline, the transport properties (e. g., density, suspended solids concentration, viscosity, particle size range) of the slurry should be determined to be within acceptable limits prior to transfer. These properties should also be monitored and controlled within specified limits while the slurry transfer is in progress. The DOE issued a call for proposals for developing <b>on-line</b> <b>instrumentation</b> to measure the transport properties of slurries. In response to the call for proposals, several researchers submitted proposals and were funded to develop slurry monitoring instruments. These newly developed DOE instruments are currently in the prototype stage. Before the instruments were installed in a radioactive application, the DOE wanted to evaluate them under nonradioactive conditions to determine if they were accurate, reliable, and dependable. The goal of this project was to test the performance of the newly developed DOE instruments along with several commercially available instruments. The baseline method for comparison utilized the results from grab-sample analyses...|$|E
40|$|The {{membrane}} electro-bioreactor (MEBR) {{has demonstrated}} {{to be effective}} in the treatment of wastewater, where superior quality of the effluent was achieved. The MEBR is a compact hybrid unit that uses several processes, such as activated sludge, membrane filtration, and electrokinetic phenomena. The objective {{of this study was to}} improve the treatment of wastewater by monitoring and controlling MEBR processes on-line, which was accomplished by implementing an automation system. As the complexity of the processes increase in the treatment wastewater, it is difficult to their guarantee performance; the automation system maintained the wastewater treatment to satisfactory performance. Automation of the system was accomplished through control algorithms using <b>on-line</b> <b>instrumentation</b> of critical parameters such as: dissolved oxygen, aeration, and water levels. The MEBR system demonstrated removal of carbon and nutrients (phosphorus, and nitrogen) for water recovery. Automated aeration ensured biological treatment without excessive aeration, fluctuating low dissolved oxygen concentrations allowed for simultaneous aerobic and anoxic conditions without inhibiting biological treatment. Automated electrokinetic improved nutrient removal with reduced energy consumption, also biological treatment was not inhibited. Electrokinetic demonstrated even lower than previously observed energy consumption. A user interface was implemented to allow on-site monitoring of the processes as well as allow adjustment of process parameters. Having a completely automated MEBR allowed this novel wastewater treatment system to be implemented in a remote location, as a decentralized system, in order to simulate an effective wastewater treatment system which may be applied to improve the quality of life for the secluded population of northern Canada and Quebec...|$|E
40|$|Today, {{nanotechnology}} lets foresee {{many opportunities}} and benefits for new materials with significantly improved properties {{as well as}} revolutionary applications in large industrial fields. Analysts have estimated {{that the size of}} the market was 900 million Euro in 2005 and will be 11 billion Euro in 2010. However, nanomaterial industries are currently encountering potential problems with hazard control in their production plants. Release of nanoparticles in air can lead to violent chemical reactions and even explosion because of their small size and energetic properties, and hence high chemical reactivity. Another major safety concern is the impact of manufacturing nanoparticles on the environment and more specifically on the health of the workers. Thus, {{there is a need to}} detect a possible release of manufactured nanoparticles in the ambient background, which can be several order of magnitude higher than the release itself, in term of number or mass concentration. The key issue was then to develop <b>on-line</b> <b>instrumentation</b> could be able to detect leaks using size resolved chemical identification. Within the European project FP 6 NANOSAFE 2, an on line monitoring method based on the coupling of the LIBS technique with a DMA (Differential Mobility Analyzer) device has been proposed as an analytical tool for workplace survey. Nanoparticle chemical identification is performed using LIBS whereas particle physical characteristics are yielded by the DMA analysis. Experiments aiming at demonstrating the potentialities of LIBS coupled with a DMA have been carried out. The LIBS signal has been also precisely studied as a function of several parameters such as laser energy or particle size and chemical composition. Moreover analytical potentialities such as repeatability, linearity and detection limit have been also investigated showing the potentiality of such instrumentation as a leak detection tool for workplace survey...|$|E
