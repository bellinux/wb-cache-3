6|37|Public
5000|$|Data Dictionary: {{contains}} definitions for all {{the possible}} fields used by the model. It is here that a field is defined as continuous, categorical, or <b>ordinal</b> (<b>attribute</b> optype). Depending on this definition, the appropriate value ranges are then defined {{as well as the}} data type (such as, string or double).|$|E
40|$|This paper {{presents}} Opossum, a novel similarity-based clustering approach {{based on}} constrained, weighted graph-partitioning. Opossum is particularly attuned to real-life market baskets, characterized by very high-dimensional, highly sparse customer-product matrices with positive <b>ordinal</b> <b>attribute</b> values and {{significant amount of}} outliers. Since it is built on top of Metis, a well-known and highly efficient graph partitioning algorithm, it inherits the scalable and easily parallelizeable attributes of the latter algorithm. Results are presented on a real retail industry data-set of several thousand customers and products, {{with the help of}} Clusion, a cluster visualization tool...|$|E
40|$|In this paper, we {{introduce}} Clusion, a clustering visualization toolkit, {{that facilitates}} data exploration and validation of clustering results. Clusion is especially suitable for very high-dimensional spaces {{since it is}} based on pair-wise relationships of the data points rather than their original vector representation. Given a similarity semantic, guidance to answer the questions 'What is the right number of clusters?', 'Which cluster should be split?', and 'Which clusters should be merged?' can be drawn from the visualization. The visualization also induces a quality metric not biased to increase with the number of clusters. We present examples from market basket data characterized by high-dimensional (d > 10; 000), highly sparse customer-product matrices with positive <b>ordinal</b> <b>attribute</b> values and signi cant amount of outliers...|$|E
40|$|The ethical view of prioritarianism {{holds the}} following: if an extra bundle of {{attributes}} {{is to be}} allocated to either of two individuals, then priority {{should be given to}} the worse off among the two. We consider multidimensional poverty comparisons with cardinal and <b>ordinal</b> <b>attributes</b> and propose three axioms that operationalize the prioritarian view. Each priority axiom, in combination with a handful of standard properties, characterizes a class of poverty measures. We provide an empirical application to European Union Statistics on Income and Living Conditions data. For this application, we develop a unanimity criterion within the setting of a single cardinal attribute (income) augmented by several binary <b>ordinal</b> <b>attributes.</b> status: publishe...|$|R
40|$|Keeping audio {{features}} {{is important}} for audio index. However, {{in most cases the}} features number is huge, thus direct processing is time-consuming. Feature selection, as a preprocessing step of data mining, has turned to be very efficient in reducing dimensionality and removing irrelevant data. In this paper, we propose a feature selection algorithm based on Rough Set theory, which could find out the feature subset from audio stream. The definition of discernibility of <b>ordinal</b> <b>attributes</b> set is introduced to discover the subsets containing implicit features. Moreover, based on the discernibility definition, we consider the discernibility of two and three <b>ordinal</b> <b>attributes</b> set, together with the discernibility of individual attribute, thus the extracted reduct is more complete and meaningful...|$|R
40|$|AbstractIn data {{analysis}} dependencies between attributes are of central interest. An {{important question is}} whether there exist algebraic decriptions of these dependencies. In this paper it is shown that simple ordinal axioms already allow to describe dependencies between <b>ordinal</b> <b>attributes</b> by orderedn-quasigroup operations. This is done with respect tondistinguished attributes. Further questions concerning stronger representations by orderedn-quasigroups are discussed...|$|R
40|$|We provide {{foundations}} for robust normative evaluation of distributions of two attributes,of which is cardinally measurable and transferable between {{individuals and the}} other isand non-transferable. The result that we establish {{takes the form of}} an analogue to the standard Hardy, Littlewood, and Pólya (1934) theorem for distributions of one cardinal. More specifically, we identify the transformations of the distributions whichthat social welfare increases according to utilitarian unanimity provided that the utility function is concave in the cardinal attribute and that its marginal utility with respect to the same attribute is non-increasing in the <b>ordinal</b> <b>attribute.</b> We establish that thisranking of the distributions is equivalent to the Bourguignon (1989) orderedgap quasi-ordering. Finally, we show that, if one distribution dominates anotherto the ordered poverty gap criterion, then the former can be derived from the latter by means of an appropriate and finite sequence of such transformationsNormative Analysis, Utilitarianism, Bidimensional Stochastic Dominance,Inequality Reducing Transformations...|$|E
40|$|For the {{description}} of dependencies between a set of independent attributes Q and a dependent at-tribute p, a soft computing approach such as Rough Set Data Analysis (RSDA) uses only a very simple data representation model: The set of equivalence classes of feature vectors determined by Q, and p respectively. Although this model is satisfactory for many applications, there are sometimes problems to interpret the results, because this type of prediction {{does not take into}} account relational information within the attributes, for example, orderings. We consider the problem what form predic-tion should take in the “nominal attributes predict an <b>ordinal</b> <b>attribute</b> ” situation ((n; o) -prediction), as well as in the (o; o) -situation. We show how to define rough (n; o) - and (o; o) -prediction and ap-proximation in terms of relational compatibility, which respects the granularity information given by the attributes. A running example is presented to demonstrate the result of the three types of data analysis. ...|$|E
40|$|Background: Falls are {{a serious}} problem {{especially}} in the aging population. To accurately identify individuals at risk for falls and mitigate the devastating effects caused by falls has become prominent to geriatrics and public health agencies. Leveraging wearable technologies and clinical assessment information may improve fall risk classification. Objectives: The overall objectives of this thesis project are to: (1) investigate {{the similarities and differences}} in physical activity (PA), heart rate (HR) and night sleep (SP) in a sample of community-dwelling older adults with varying fall histories, using a smart wrist-worn device; and (2) examine the risk factors for falls in the target population, create fall risk classification models and evaluate classification performances based on: i) wearable data, ii) the Resident Assessment Instrument for Home Care (RAI-HC), and iii) the combination of wearable data and the RAI-HC system. Methods: Two parallel studies were conducted in this project. Study I was a community-based cross-sectional study, utilizing the RAI-HC system to examine the risk factors for falls in older people. In the primary analysis, the <b>ordinal</b> <b>attribute</b> of previous falls (0, 1, and ≥ 2) was used as the outcome variable to build the proportional odds models (POM) for ordinal logistic regression. In the secondary analysis, the binary attribute of falls (yes/no) was used to distinguish fallers and non-fallers. Study II, a prospective, observational study was conducted to investigate the similarities and differences among three independent faller groups (non-fallers, single fallers, and recurrent fallers) {{based on the number of}} previous falls in a sample of older adults living in community, with continuous measurements of PA, HR and SP using a smart wearable device. Descriptive statistics and simple statistical analyses were conducted to test the differences between groups. The wearable and RAI-HC assessment data were further analyzed and utilized to create fall risk classification models, with two supervised machine learning algorithms: logistic regression (LR) and decision tree (DT). The calculation of a set of performance metrics was performed to evaluate the classification performance of each final model. Results: Study I: Of 167, 077 individuals aged ≥ 65 in the RAI-HC data set, 113, 529 (68. 0 %) had no history of falls, 27, 320 (16. 4 %) had one fall, and 26, 226 (15. 7 %) experienced multiple (≥ 2) falls. Unsteady gait, Activities of Daily Living (ADL) decline, ADL self-performance on transfer dependency, short-term memory problem, primary modes of locomotion (indoors), stair climbing, bladder continence, and limit going outdoors due to fear of falling were significant predictors of fall risk in both human and computer feature selection models derived from the Minimum Data Set-Home Care (MDS-HC). The Method of Assigning Priority Levels (MAPLe) (1 vs. 5 : odds ratio (OR) = 0. 20; 95 % confidence internal (CI), 0. 18 - 0. 22), Changes in Health, End-Stage Disease, Signs, and Symptoms (CHESS) (0 vs. 5 : OR = 0. 27; 95 % CI, 0. 21 - 0. 36), ADL Clinical Assessment Protocol (CAP) (0 vs. 2 : OR = 0. 21; 95 % CI, 0. 20 - 0. 22), Cognitive CAP (0 vs. 2 : OR = 0. 33; 95 % CI, 0. 31 - 0. 35), and Urinary Incontinence CAP (3 vs. 0 : OR = 1. 77; 95 % CI, 1. 62 - 1. 94) were strong predictors in classifying older people with past fall histories based on the CAPs and a variety of summary scales and algorithms available within the RAI-HC assessment. The POM built on all available items on the RAI-HC data set achieved the best performance in classifying the three faller groups, with overall classification accuracy of 71. 5 %, and accuracies of 93. 3 %, 5. 5 % and 46. 0 % in classifying the non-faller, single faller and recurrent faller group, respectively. Likewise, the logistic regression model built on all available RAI-HC items achieved the best performance in distinguishing fallers and non-fallers, with the highest overall classification accuracy of 75. 1 %, the largest area under the curve (AUC) of 0. 769, and the lowest Brier score of 0. 171. Study II: Of 40 participants aged 65 - 93, 16 (40 %) had no previous falls, while 8 (20 %) and 16 (40 %) had experienced one and multiple (≥ 2) falls, respectively. The wearable components of PA measurements extracted from the smart wrist-worn device were significantly different among three faller groups. Daily walking HR and daily activity time were identified as the best subset of predictors of fall risk with wearable data. Classification models derived from the RAI-HC data set containing 40 participants’ latest assessments outperformed those based on wearable data only. The best classification model was a decision tree based on the combination of both data sets with 80. 0 % of overall classification accuracy, and accuracies of 87. 5 %, 50. 0 % and 87. 5 % in classifying the non-faller, single faller and recurrent faller group, respectively. Conclusions: Continuous measurements of PA, HR and SP appear to supplement the RAI-HC system in facilitating fall risk stratification. Future fall risk assessment studies should consider leveraging wearable technologies to supplement resident assessment instruments...|$|E
40|$|The {{literature}} on health inequality with <b>ordinal</b> <b>attributes</b> is {{benefiting from the}} development of inequality measures, which are useful in any wellbeing assessment involving ordinal variables (e. g. subjective wellbeing). Lv, Wang, and Xu ("On {{a new class of}} measures for health inequality based on ordinal data", Journal of Economic Inequality, 2015) recently characterized a new class of this type of inequality measures axiomatically. In addition to their appealing functional forms, these measures are the only ones in the literature satisfying a property of independence, inspired by Kolm ("Unequal inequalities I", Journal of Economic Theory, 1976). As acknowledged by the authors, it is reasonable {{to be concerned about the}} robustness of inequality comparisons with <b>ordinal</b> <b>attributes</b> to the several alternative suitable measures within the class. This note derives the stochastic dominance condition whose fulfilment guarantees that all inequality measures within the class rank a pair of distributions consistently; thereby providing an empirically implementable robustness test for this class of measures...|$|R
40|$|For {{classification}} {{problems with}} <b>ordinal</b> <b>attributes</b> very often the class attribute should increase with each {{or some of}} the explaining attributes. These are called classification problems with monotonicity constraints. Classical decision tree algorithms such as CART or C 4. 5 generally do not produce monotone trees, even if the dataset is completely monotone. This paper surveys the methods that have so far been proposed for generating decision trees that satisfy monotonicity constraints. A distinction is made between methods that work only for monotone datasets and methods that work for monotone and non-monotone datasets alike...|$|R
40|$|Classification {{rules are}} a {{convenient}} method of expressing regularities that exist within databases. They are particularly useful when {{we wish to}} find patterns that describe a defined class of interest, i. e. for the task of partial classification or "nugget discovery". In this paper we address the problems of finding classification rules from databases containing nominal and <b>ordinal</b> <b>attributes.</b> The number of rules that can be formulated from a database is usually potentially vast due {{to the effect of}} combinatorial explosion. This means that generating all rules in order to find the best rules (according to some stated criteria) is usually impractical and alternative strategies must be used. In this paper we present an algorithm that delivers a clearly defined set of rules, the pc'-optimal set. This set describes the interesting associations in a database but excludes many rules that are simply minor variations of other rules. The algorithm addresses the problems of combinatorial explosion and is capable of finding rules from databases comprising nominal and <b>ordinal</b> <b>attributes.</b> In order to find the pc'-optimal set efficiently, novel pruning functions are used in the search that take advantage of the properties of the pc'-optimal set. Our main contribution is a method of on-the-fly pruning based on exploiting the relationship between pc'-optimal sets and ordinal data. We show that using these methods results in a very considerable increase in efficiency allowing the discovery of useful rules from many databases...|$|R
40|$|In {{this article}} {{we would like to}} show the need for {{developing}} knowledge access systems that can account for the imperfections in human perception, information processing and memory (Higgins et al., 1996). The implementation of such systems will result in enormous savings in the process of learning at all three stages of knowledge acquisition (by the mind) : (1) access knowledge to, (2) learning and (3) knowledge retention (Clark et al., 1997). In particular, we will try to stress the importance of repetition spacing algorithms (Woźniak and Gorzelańczyk, 1994), as well as the importance of (1) (2) and the application of the newly introduced concept of processing, <b>ordinal</b> <b>attributes</b> in hypertext documents, semantics (Wiesman et al., 1997; Gillham, 1988) ...|$|R
40|$|Supervised {{classification}} {{problems have}} received considerable {{attention from the}} machine learning community. We propose a novel genetic algorithm based prototype learning system, PLEASE, for this class of problems. Given a set of prototypes {{for each of the}} possible classes, the class of an input instance is determined by the prototype nearest to this instance. We assume <b>ordinal</b> <b>attributes</b> and prototypes are represented as sets of feature-value pairs. A genetic algorithm is used to evolve the number of prototypes per class and their positions on the input space as determined by corresponding feature-value pairs. Comparisons with C 4. 5 on a set of artificial problems of controlled complexity demonstrate the effectiveness of the proposed system. ...|$|R
40|$|A data {{warehouse}} {{is designed to}} consolidate and maintain all attributes that are relevant for the analysis processes. Due to the rapid increase {{in the size of}} the modern operational systems, it becomes neither practical, nor necessary to load and maintain in the {{data warehouse}} every operational attribute. This paper presents a novel methodology for automated selection of the most relevant independent attributes in a data warehouse. The method is based on the information-theoretic approach to knowledge discovery in databases. Attributes are selected by a stepwise forward procedure aimed at minimizing the uncertainty in the values of key performance indicators (KPI’s). Each selected attribute is assigned a score, expressing its degree of relevance. Using the method does not require any prior expertise in the domain of the data and it can be equally applied to nominal and <b>ordinal</b> <b>attributes.</b> An attribute will be included in a data warehouse schema, if it is found as relevant to at least one KPI. We demonstrate the applicability of the method by reducing the dimensionality of a direct marketing database. ...|$|R
40|$|Abstract—The core {{concepts}} of classical rough sets are lower and upper approximations based on indiscernibility relations. In some cases, {{it is necessary}} to generalize indiscernibility relation by using some other binary relations. We first consider the indiscernibility relations defined from nominal attributes, similarity relations defined from quantitative attributes, and outranking relations defined from <b>ordinal</b> <b>attributes.</b> These relations defined by single attributes are aggregated into the global relations {{at the level of the}} set of attributes. The lower approximation and the upper approximation of the upward and downward unions of decision classes are defined using the global relations. Then, we investigate some of common properties of the relation based lower and upper approximation operations. Index Terms—binary relations, rough sets, lower and upper approximations, similarity, outranking I...|$|R
40|$|Abstract]: Individual privacy {{will be at}} risk if a {{published}} data set is not properly de-identified. k-anonymity is a major technique to de-identify a data set. A more general view of k-anonymity is clustering with a constraint of the minimum number of objects in every cluster. Most existing approaches to achieving k-anonymity by clustering are for numerical (or <b>ordinal)</b> <b>attributes.</b> In this paper, we study achieving k-anonymity by clustering in attribute hierarchical structures. We define generalisation distances between tuples to characterise distortions by generalisations and discuss {{the properties of the}} distances. We conclude that the generalisation distance is a metric distance. We propose an efficient clustering-based algorithm for k-anonymisation. We experimentally show that the proposed method is more scalable and causes significantly less distortions than an optimal global recoding k-anonymity method...|$|R
40|$|Abstract Subgroup {{discovery}} is a Knowledge Discovery task that aims at finding subgroups {{of a population}} with high generality and distributional unusualness. While several subgroup discovery algorithms have been presented in the past, they focus on databases with nominal attributes or make use of discretization {{to get rid of}} the numerical attributes. In this paper, we illustrate why the replacement of numerical attributes by nominal attributes can result in suboptimal results. Thereafter, we present a new subgroup discovery algorithm that prunes large parts of the search space by exploiting bounds between related numerical subgroup descriptions. The same algorithm can also be applied to <b>ordinal</b> <b>attributes.</b> In an experimental section, we show that the use of our new pruning scheme results in a huge performance gain when more that just a few split-points are considered for the numerical attributes...|$|R
40|$|In this paper, {{the use of}} the {{definite}} {{article in}} semantic and pragmatic categories in the Greek and Classical Armenian New Testament translation is compared. Greek and Classical Armenian agree in their use of the definite article only in NPs determined by contrastive attributes. In all other categories the systems of both languages differ. Generally, Armenian avoids the definite article with proper nouns and nouns with unique reference, while definite articles with proper names in Greek are common (with the exception of sacred or especially “respected” persons such as prophets). If the definite article is present in Greek, it is often motivated by pragmatic factors (e. g. re-topicalization, etc.). There is no clear evidence in Armenian for {{the use of the}} definite article as a marker of generic reference, nor for the use in NPs determined by superlative, comparative or <b>ordinal</b> <b>attributes...</b>|$|R
40|$|Subgroup {{discovery}} is a Knowledge Discovery task that aims at finding subgroups {{of a population}} with high generality and distributional unusualness. While several subgroup discovery algorithms have been presented in the past, they focus on databases with nominal attributes or make use of discretization {{to get rid of}} the numerical attributes. In this paper, we illustrate why the replacement of numerical attributes by nominal attributes can result in suboptimal results. Thereafter, we present a new subgroup discovery algorithm that prunes large parts of the search space by exploiting bounds between related numerical subgroup descriptions. The same algorithm can also be applied to <b>ordinal</b> <b>attributes.</b> In an experimental section, we show that the use of our new pruning scheme results in a huge performance gain when more that just a few split-points are considered for the numerical attributes...|$|R
40|$|Discretization {{can turn}} numeric {{attributes}} into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi 2, {{a simple and}} general algorithm that uses the 2 statistic to discretize numeric attributes repeatedly until some inconsistencies {{are found in the}} data, and achieves feature selection via discretization. The empirical results demonstrate that Chi 2 is effective in feature selection and discretization of numeric and <b>ordinal</b> <b>attributes.</b> 1 Introduction Feature selection is a task to select the minimum number of attributes needed to represent the data accurately. By using relevant features, classification algorithms can in general improve their predictive accuracy, shorten the learning period, and result in the simpler concepts. There are abundant feature selection algorithms [5]. Our work adopts an approach that selects a subset of the original attributes since it not only has the above virtues, but also serves as an indicator on what kind [...] ...|$|R
40|$|We {{present a}} novel use of ordinal {{evaluation}} (OrdEval) algorithm as a promising technique to study various marketing phenomena. OrdEval algorithm has originated in data mining {{and is a}} general tool to analyze data with <b>ordinal</b> <b>attributes,</b> including surveys. Its many favorable features, including context sensitivity, ability to exploit meaning of ordered features and ordered response, and robustness to noise and missing values in the data, offer marketing practitioners a perspective, not available with classical analytical toolbox. We present a case study applying OrdEval algorithm on country-of-origin (COO) information. We demonstrate some interesting advantages it has to offer and show how to extract and interpret new insights allowing marketing practitioners to further optimize the management of products abroad. Data for the empirical study was gathered by means of 1225 questionnaires. Results indicate that, contrary to the classical view on COO-effects, the processing of country-related cognitions, affects and conations is a non-linear and asymmetric phenomenon. The practical implications of this finding for marketers are discussed more in detail...|$|R
40|$|Abstract. For {{classification}} {{problems with}} <b>ordinal</b> <b>attributes</b> very often the class attribute should increase with each {{or some of}} the explanatory attributes. These are called classification problems with monotonicity constraints. Standard classification tree algorithms such as CART or C 4. 5 are not guaranteed to produce monotone trees, even if the data set is completely monotone. We look at pruning based methods to build monotone classification trees from monotone as well as nonmonotone data sets. We develop a number of fixing methods, that make a non-monotone tree monotone by additional pruning steps. These fixing methods can be combined with existing pruning techniques to obtain a sequence of monotone trees. The performance of the new algorithms is evaluated through experimental studies on artificial as well as real life data sets. We conclude that the monotone trees have a slightly better predictive performance and are considerably smaller than trees constructed by the standard algorithm. ...|$|R
40|$|In {{subgroup}} discovery, {{also known}} as supervised pattern mining, discovering high quality one-dimensional subgroups and refinements of these is a crucial task. For nominal attributes, this is relatively straightforward, as we can consider individual attribute values as binary features. For numerical attributes, the task is more challenging as individual numeric values are not reliable statistics. Instead, we can consider combinations of adjacent values, i. e. bins. Existing binning strategies, however, are not tailored for subgroup discovery. That is, they do not directly optimize {{for the quality of}} subgroups, therewith potentially degrading the mining result. To address this issue, we propose FLEXI. In short, with FLEXI we propose to use optimal binning to find high quality binary features for both numeric and <b>ordinal</b> <b>attributes.</b> We instantiate FLEXI with various quality measures and show how to achieve efficiency accordingly. Experiments on both synthetic and real-world data sets show that FLEXI outperforms state of the art with up to 25 times improvement in subgroup quality. Comment: Submission to SDM 201...|$|R
40|$|This paper {{discusses}} {{the design and}} implementation of interactive smooth zooming of a starfield display (which is a visualization of a multi-attribute database) and introduces the zoom bar, a new widget for zooming and panning. Whereas traditional zoom techniques are based on zooming towards or away from a focal point, this paper introduces a novel approach based on zooming towards or away from a fixed line. Starfield displays plot items from a database as small selectable glyphs using two of the <b>ordinal</b> <b>attributes</b> of the data as the variables along the display axes. One way of filtering this visual information is by changing the range of displayed values on either of the display axes. If this is done incrementally and smoothly, the starfield display appears to zoom in and out, and users can track {{the motion of the}} glyphs without getting disoriented by sudden, large changes in context. KEYWORDS starfield display, smooth zooming, animation, zoom bar, dynamic queries, information visua [...] ...|$|R
40|$|Abstract. We {{present a}} novel use of ordinal {{evaluation}} (OrdEval) algorithm as a promising technique to study various marketing phenomena. OrdEval algorithm has originated in data mining {{and is a}} general tool to analyze data with <b>ordinal</b> <b>attributes,</b> including surveys. Its many favorable features, including context sensitivity, ability to exploit meaning of ordered features and ordered response, and robustness to noise and missing values in the data, offer marketing practitioners a perspective, not available with classical analytical toolbox. We present a case study applying OrdEval algorithm on country-of-origin (COO) information. We demonstrate some interesting advantages it has to offer and show how to extract and interpret new insights allowing marketing practitioners to further optimize the management of products abroad. Data for the empirical study was gathered by means of 1225 questionnaires. Results indicate that, contrary to the classical view on COO-effects, the processing of country-related cognitions, affects and conations is a non-linear and asymmetric phenomenon. The practical implications of this finding for marketers are discussed more in detail. ...|$|R
40|$|Arguably, {{analogy is}} one of the most {{important}} aspects of intelligent reasoning. It has been hypothesized that, given suitable background knowledge, analogy can be viewed as a logical inference process. This study follows another school of thought that argues that similarity can provide a probabilistic basis for inference and analogy. Most similarity measures (which are frequently viewed as being conceptually equivalent to distance measures) are restricted to either nominal or <b>ordinal</b> <b>attributes,</b> and some are confined to classification tasks. This paper proposes a flexible similarity measure that is task-independent and applies to both nominal and ordinal data in a conceptually uniform way. The proposed similarity measure is derived from a probability function and corresponds to the intuition that if we consider all neighborhoods around a data point, the data points closer to this point should be included in more of these neighborhoods than more distant points. Experiments we have conducted to demonstrate the usefulness of this measure indicate that it fares very competitively with commonly used similarity measures. ...|$|R
40|$|Abstract—Architecting {{dependable}} {{systems is}} {{a daunting task}} since it requires trade-offs among attributes such as reliability with a precisely computed, <b>ordinal</b> value and <b>attributes</b> such as security whose value is neither precisely computed nor <b>ordinal.</b> Quality <b>attribute</b> driven architecture design [1] techniques rely on models of the attributes that provide values that can be compared and ranked. Qualitative modeling techniques hold promise {{for being able to}} model quality attributes such as confidentiality and integrity for which quantitative models do not exist, but which need to be compared against one another. In this position paper we present a sketch of our work in progress on a hybrid approach that blends quantitative and qualitative techniques to design a dependable system. We introduce a chain of existing architecture modeling tools and briefly describe how they can be integrated to support the development of a rigorous design. I...|$|R
40|$|This paper {{discusses}} {{the design and}} implementation of user controlled smooth zooming of a starfield display. A starfield display is a two dimensional graphical visualization of a multidimensional database where every item from the database is represented as a small colored rectangle whose position is determined by its ranking along <b>ordinal</b> <b>attributes</b> of the items {{laid out on the}} axes. One way of navigating this visual information is by using a zooming tool to incrementally zoom in on the items by varying the attribute range on either axis independently - such zooming causes the rectangles to move continuously and to grow or shrink. To get a feeling of flying through the data, users should be able to track the motion of each rectangle without getting distracted by flicker or large jumps - conditions that necessitate high display refresh rates and closely spaced rectangles on successive frames. Although the use of high-speed hardware can achieve the required visual effect for small databases, the twin software bottlenecks of rapidly accessing display items and constructing a new display image fundamentally retard the refresh rate. Our work explores several methods to overcome these bottlenecks, presents a taxonomy of various zooming methods and introduces a new widget, the zoom bar, that facilitates zooming. <P...|$|R
40|$|This paper {{discusses}} {{the design and}} implementation of interactive smooth zooming of a starfield display. A starfield display is a two dimensional scatterplot of a multidimensional database where every item from the database is represented as a small colored glyph whose position is determined by its ranking along <b>ordinal</b> <b>attributes</b> of the items {{laid out on the}} axes. One way of navigating this visual information is by using a zooming tool to incrementally zoom in on the items by varying the attribute range on either axis independently - such zooming causes the glyphs to move continuously and to grow or shrink. To get a feeling of flying through the data, users should be able to track the motion of each glyph without getting distracted by flicker or large jumps - conditions that necessitate high display refresh rates and closely spaced glyphs on successive frames. Although the use of high-speed hardware can achieve the required visual effect for small databases, the twin software bottlenecks of rapidly accessing display items and constructing a new display image fundamentally retard the refresh rate. Our work explores several methods to overcome these bottlenecks, presents a taxonomy of various zooming methods and introduces a new widget, the zoom bar, that facilitates zooming. (Also cross-referenced as CAR-TR- 714) (Also cross-referenced as ISR-TR- 94 - 46...|$|R
40|$|Language {{profile of}} fragile-X {{syndrome}} individuals {{looks like the}} one of Down syndrome individuals, except for phonological and pragmatic abilities. If the pragmatic aspect of language is relatively preserve in Down syndrome, {{it is one of}} the most impaired language component in fragile-X syndrome. One aspect of the pragmatic component of language remains almost unexplored in this pathology : the common ground management and the organization of the old and the new information in conversation. Sample : 4 fragile-X boys aged from 6 to 12 years-old and 4 typically developing children matched on the lexical age Tasks : 6 referential communication tasks proposed to children both in speaker condition and in listener condition. Tasks were distributed in two groups : 1] building tasks, and 2] combination tasks. Groups of 2 boys work together on referential communication tasks. In simple situations FXS boys can be as efficient speakers and listeners as typically developing children. Difficulties mainly appear when FXS boys have to deal with spatial and <b>ordinal</b> <b>attributes.</b> FXS boys manage less efficiently with an incomplete message especially when it is given by an adult. In FXS syndrome, the efficiency of communication mainly depends of the nature of the items attributes to communicate. Spatial information implies the mastery of vocabulary and concepts which seems to be deficient in several etiologies of mental retardation including FXS boys. Ordinal information implies the manipulation of information that are intrinsically more demanding than nominal dimensions because they are inherently relational. Peer reviewe...|$|R
40|$|In {{a context}} where several {{sectors of society}} compete for space, land use types must be {{carefully}} designed and spatially allocated to guarantee a sufficient level of relevant ecosystem services (ES) in a territory of interest. In this respect, contemporary land use planning involves multiple, often conflicting objectives and criteria. Consequently, major benefits can be expected from spatial decision support systems (sDSS) designed to deal with complex spatial allocation problems. This paper presents the generic conceptual framework ‘OSMOSE’ and its free and open source software implementation, for the generation of specific sDSSs for spatial land use planning. The specific sDSSs generated with OSMOSE are meant to (i) identify land units which meet multiple predefined ES-attribute values for a specific land use type (LUT) and (ii) rank land units for a given LUT according to these multiple ES-attributes. A complementary purpose is to (i) identify and (ii) rank LUTs for a given land unit. Whereas ‘identification’ is done by means of threshold query, ‘ranking’ {{is based on the}} Iterative Ideal Point Thresholding (IIPT) method. The proposed framework is extremely flexible as it can accommodate differentially weighted, continuous and/or <b>ordinal</b> <b>attributes</b> with, for the latter, equal or unequal number of classes, alternative land unit definitions and land use types. Moreover, assessments cannot only be made using ES-levels for the land unit/LUT combinations but also in terms of changes in ES-levels after a particular change of LUT. The OSMOSE framework is illustrated by means of the specific sDSS BoLa which is generated to support land use planning in the region of Flanders (Belgium) with focus on soil protection. Four cases are presented in which the decision support varies between the available approaches (threshold – selection, threshold – ranking, IIPT – selection, IIPT - ranking). status: publishe...|$|R
40|$|Abstract. k-Anonymity is {{a useful}} concept to solve the tension between data utility and {{respondent}} privacy in individual data (microdata) protection. However, the generalization and suppression approach proposed in the literature to achieve k-anonymity is not equally suited {{for all types of}} attributes: (i) generalization/suppression {{is one of the few}} possibilities for nominal categorical attributes; (ii) it is just one possibility for <b>ordinal</b> categorical <b>attributes</b> which does not always preserve ordinality; (iii) and it is completely unsuitable for continuous attributes, as it causes them to lose their numerical meaning. Since attributes leading to disclosure (and thus needing k-anonymization) may be nominal, ordinal and also continuous, it is important to devise k-anonymization procedures which preserve the semantics of each attribute type as much as possible. We propose in this paper to use categorical microaggregation as an alternative to generalization/suppression for nominal and ordinal k-anonymization; we also propose continuous microaggregation as the method for continuous k-anonymization...|$|R
40|$|We present here an R {{package for}} Riffle, a nonmetric {{clustering}} technique [2]. This is a algorithm for clustering (unsupervised learning) {{that does not}} rely on a similarity measure for multivariate data, and uses only nonparametric (order) statistics. It is suitable for mixed nominal, <b>ordinal,</b> etc. <b>attributes,</b> as are often found in environmental data analysis. The current implementation of Riffle in R {{has a number of}} improvements over the original implementation [2], utilizing a marginalized expectation maximization (EM) approach to speed up the search. This has advantages in avoiding local minima, as well. Also, a technique for creating useful seed clusterings has been developed (rather than completely random initial clusters, as in [2]), substantially speeding up the clustering and making the final cluster less susceptible to noise. Procedures are also provided to use the resulting clustering to find an optimal subset of the attributes, and to create a naive Bayes classifier. Riffle has been used successfully in a variety of clustering tasks, and we have found it to be a useful, intuitive technique for graduate and undergraduate students in environmental sciences. Although nonmetric clustering is over a decade old, it is not widely known. A recent authoritative survey [1],pp. 541 - 542, does not include a discussion of them, and laments, “How do we treat vector...|$|R
40|$|Measurement theory {{provides}} additive conjoint {{structures for}} additive representations of empirical data. Roughly, an additive conjoint structure {{is a product}} of (quasi) ordered sets with some properties connecting the different factors of the product. Well-known Debreu's Theorem says that every additive conjoint structure can be embedded in a vector space over the real numbers. This embedding yields a completion of the additive conjoint structure where every factor becomes a complete lattice. This paper introduces a synthetical way of constructing this completion without using real numbers. 1. Introduction Measurement theory is the theory of representing empirical data by relations on ordered or algebraic structures. Additive conjoint structures are basic for the additive representation of multidimensional data in measurement theory (cf. [3]). Additive representations reflect certain kinds of <b>ordinal</b> dependencies between <b>attributes</b> of the data. In this frame, the question of order-co [...] ...|$|R
40|$|In this paper, we {{introduce}} Ant-MinerMA {{to tackle}} mixed-attribute classification problems. Most classification problems involve continuous, <b>ordinal</b> and categorical <b>attributes.</b> The majority of Ant Colony Optimization (ACO) classification algorithms have {{the limitation of}} being able to handle categorical attributes only, with few exceptions that use a discretisation procedure when handling continuous attributes either in a preprocessing stage or during the rule creation. Using a solution archive as a pheromone model, inspired by the ACO for mixed-variable optimization (ACO-MV), we eliminate the need for a discretisation procedure and attributes can be treated directly as continuous, ordinal, or categorical. We compared the proposed Ant-MinerMA against cAnt-Miner, an ACO-based classification algorithm that uses a discretisation procedure in the rule construction process. Our results show that Ant-MinerMA achieved significant improvements on computational time due to the elimination of the discretisation procedure without affecting the predictive performance...|$|R
