0|7589|Public
40|$|We present here a new {{algorithm}} for accelerating volume rendering with an orthographic projection. Because volume rendering handles huge data sets, {{a reduction}} in the computational cost of voxel projection is required to obtain interactive volume rendering. We satisfy this issue by using the possibilities of orthographic projection that allows the quantization of voxel positions by subdividing the pixels. The same projection properties are given for all the voxels with the center falling within the same pixel subdivision. In contrast with classical algorithms that require several instructions to compute either the next traversed voxel or the next rasterized pixel, our method needs only one addition <b>instruction</b> and <b>one</b> <b>addressing</b> <b>instruction</b> that is sufficient to determine one projected pixel. Splatting can also have a decisive advantage of it. Our algorithm is well suited for low-end platforms when no hardware acceleration is available. Experimental results show that our rendering rate is better than other existing methods. This algorithm might allow obtaining real-time volume rendering on conventional computers soon [...] H Volume rendering, splatting, interactivity, orthographic viewing, low-end platform, quantization, rasterization. ...|$|R
5000|$|There were 32 bit {{locations}} per drum word, {{but only}} 31 were used, permitting a [...] "restoration of magnetic flux in the head" [...] at the 32nd bit time. Since {{there was only}} <b>one</b> <b>address</b> per <b>instruction,</b> a method was needed to optimise allocation of operands. Otherwise, each instruction would wait a complete drum (or disk) revolution each time a data reference was made. The LGP-30 provided for operand-location optimization by interleaving the logical addresses on the drum so that two adjacent addresses (e.g., 00 and 01) were separated by nine physical locations. These spaces allowed for operands to be located next to the instructions which use them. There were 64 tracks, each with 64 words (sectors). The time between two adjacent physical words was approximately 0.260 millisecond, and the time between two adjacent addresses was 9 x 0.260 or 2.340 milliseconds. The worst-case access time was 16.66 ms.|$|R
40|$|Reducing <b>address</b> {{arithmetic}} <b>instructions</b> by optimization {{of address}} offset assignment greatly improves {{the performance of}} DSP applications. However, minimizing address operations alone may not directly reduce code size and schedule length for multiple functional units DSPs. In this paper, we exploit address assignment and scheduling for application with loops on multiple functional units DSPs. Array transformation is used in our approach to leverage the indirect addressing modes provided {{by most of the}} DSP architectures. An algorithm, <b>Address</b> <b>Instruction</b> Reduction Loop Scheduling (AIRLS), is proposed. The algorithm utilizes the techniques of rotation scheduling, address assignment and array transformation to minimize both <b>address</b> <b>instructions</b> and schedule length. Compared to the list scheduling, AIRLS shows an average reduction of 35. 4 % in schedule length and an average reduction of 38. 3 % in <b>address</b> <b>instructions.</b> Compared to the rotation scheduling, AIRLS shows an average reduction of 19. 2 % in schedule length and 39. 5 % in the number of <b>address</b> <b>instructions...</b>|$|R
50|$|Also, a Harvard {{architecture}} {{machine has}} distinct code and data <b>address</b> spaces: <b>instruction</b> <b>address</b> zero {{is not the}} same as data <b>address</b> zero. <b>Instruction</b> <b>address</b> zero might identify a twenty-four bit value, while data address zero might indicate an eight-bit byte that is not part of that twenty-four bit value.|$|R
40|$|One of the {{important}} issues in embedded system design is to optimize program code for the microprocessor to be stored in ROM. In this paper, we propose an integrated approach to the DSP address code generation problem for minimizing the number of <b>addressing</b> <b>instructions.</b> Unlike previous works in which code scheduling and offset assignment are performed sequentially without any interaction between them, our work tightly couples offset assignment problem with code scheduling to exploit scheduling on minimizing <b>addressing</b> <b>instructions</b> more effectively. We accomplish this by developing a fast but accurate two-phase procedure which, for a sequence of code schedules, finds a sequence of memory layouts with minimum <b>addressing</b> <b>instructions.</b> Experimental results with benchmark DSP programs show improvements of 13 %- 33 % in the address code size over Solve-SOA/GOA [7]...|$|R
40|$|A {{microprocessor}} system {{is provided with}} added memories to expand its address spaces beyond its address word length capacity by using indirect <b>addressing</b> <b>instructions</b> of a type having a detectable operations code and dedicating designated address spaces of memory {{to each of the}} added memories, one space to a memory. By decoding each operations code of instructions read from main memory into a decoder to identify indirect <b>addressing</b> <b>instructions</b> of the specified type, and then decoding the address that follows in a decoder to determine which added memory is associated therewith, the associated added memory is selectively enabled through a unit while the main memory is disabled to permit the instruction to be executed on the location to which the effective address of the indirect <b>address</b> <b>instruction</b> points, either before the indirect address is read from main memory or afterwards, depending on how the system is arranged by a switch...|$|R
25|$|Counter machine – {{the most}} {{primitive}} and reduced theoretical {{model of a}} computer hardware. Lacks indirect <b>addressing.</b> <b>Instructions</b> are in the finite state machine {{in the manner of}} the Harvard architecture.|$|R
5000|$|One of {{the reasons}} for the 8051s {{popularity}} is its range of operations on single bits. Bits are always specified by absolute addresses; there is no register-indirect or indexed <b>addressing.</b> <b>Instructions</b> that operate on single bits are: ...|$|R
50|$|A Change <b>Address</b> Mode (CAM) <b>instruction</b> {{switched}} between 2-, 3- and 4-character {{address mode}}s.The address mode specified {{the number of}} characters needed for each operand <b>address</b> in <b>instructions.</b>|$|R
50|$|The {{instruction}} set implicitly subdivides the data format {{into the same}} fields as type A instructions: prefix, decrement, tag and <b>address.</b> <b>Instructions</b> exist to modify each of these fields in a data word without changing {{the remainder of the}} word.|$|R
40|$|PC {{instruction}} memory, {{fetch instruction}} Register numbers register file, read registers Depending on instruction class Use ALU to calculate Arithmetic result Memory address for load/store Branch target address Access data memory for load/store PC target address or PC + 4 3 Abstract / Simplified View Two types of functional units: elements that operate on data values (combinational) elements that contain state (sequential) 4 PC <b>address</b> <b>instruction</b> instruction memory data memory address data registers data register # register # register # ALU Abstract / Simplified View Cannot just join wires together Use multiplexers 5 PC <b>address</b> <b>instruction</b> instruction memory data memory address data registers data register # register # register # ALU Recall...|$|R
5000|$|... +------+ | nop | {{execute the}} {{following}} instruction +------+ [...] (Effective PC <b>address</b> = next <b>instruction</b> <b>address)</b> ...|$|R
50|$|The machine {{instructions}} can {{be grouped}} into six categories: accumulator instructions, branch instructions, memory reference <b>instructions,</b> <b>address</b> register <b>instructions,</b> scratchpad register instruction, miscellaneous instructions (interrupt, input, output, indirect scratchpad register, load, and store).|$|R
5000|$|Addressing modes include Immediate (operand in instruction), Direct or [...] "Symbolic" [...] (operand <b>address</b> in <b>instruction),</b> Register (operand in {{workspace}} register), Register Indirect (operand {{address in}} workspace register) {{with or without}} auto-increment, Indexed (operand <b>address</b> in <b>instruction</b> indexed with workspace register content), and Program Counter Relative.|$|R
50|$|With no byte <b>addressing</b> <b>instructions</b> at all, code had to {{be written}} to pack and shift {{characters}} into words. The very large words, and comparatively small amount of memory, meant that programmers would frequently economize on memory by packing data into words at the bit level.|$|R
5000|$|... +----+------------------------------+ |jump| offset | jump {{relative}} +----+------------------------------+ [...] (Effective PC <b>address</b> = next <b>instruction</b> <b>address</b> + offset, offset may be negative) ...|$|R
5000|$|... +------+-----+-----+ |skipEQ| reg1| reg2| {{skip the}} {{following}} instruction if reg1=reg2 +------+-----+-----+ [...] (Effective PC <b>address</b> = next <b>instruction</b> <b>address</b> + 1) ...|$|R
50|$|In {{both the}} {{original}} version and 1A, clocks for Program Store and Call Store were operated out of phase, so one would be delivering data while the other was still accepting an <b>address.</b> <b>Instruction</b> decoding and execution were pipelined, to allow overlapping processing of consecutive instructions in a program.|$|R
5000|$|The {{instruction}} set implicitly subdivides the data format {{into the same}} fields as type A instructions: prefix, decrement, tag and <b>address.</b> <b>Instructions</b> exist to modify each of these fields in a data word without changing {{the remainder of the}} word though the Store Tag instruction was not implemented on the IBM 704.|$|R
40|$|This {{dissertation}} has two parts, <b>one</b> <b>addressing</b> {{issues in}} the area of computer-aided software development and the other concerning parallel computer architecture. ^ The first part develops a methodology for serializing a concurrent software specification to achieve efficient execution on a single processor. A concurrent specification provides a natural, architecture-independent expression of the desired behavior of a concurrent system. Automation of the serialization process allows serial code to be produced quickly and without error. Serialization begins by translating the specification into labeled transition systems. These systems are combined into a single labeled transition system represented by a superstate graph. This graph is pruned according to rules that reduce its potentially enormous size while preserving correct system behavior. Improved pruning algorithms and heuristic techniques for simplifying the system representation make the resulting serialization more compact. ^ The second part of this work presents a new taxonomy for computer architecture and uses it to define the concept of processor autonomy, the potential of one processing element in a parallel computer to act differently from other processors during execution. Processor autonomy is possible when multiple data value, data <b>address,</b> <b>instruction</b> value, or <b>instruction</b> <b>address</b> streams are available. Parallel program execution can be significantly aided by processor autonomy, allowing various mappings and dynamic reassignment of processors to streams to avoid serialization of execution. Parallel program performance is evaluated for several sorting algorithms using one form of data address autonomy, indirect <b>addressing,</b> and <b>one</b> form of instruction value autonomy, branch selection. ...|$|R
5000|$|An {{incrementing}} {{counter that}} {{keeps track of}} the memory <b>address</b> of the <b>instruction</b> {{that is to be}} executed next or in other words, holds the <b>address</b> of the <b>instruction</b> to be executed next.|$|R
5000|$|... 0003 NOOP 00 0000 0000 No-operation <b>{{instruction}},</b> next <b>instruction</b> <b>address</b> is 0000 0000 HALT 01 0000 8000 Halt, next <b>instruction</b> <b>address</b> is {{the console}} (this Halt instruction was stored in 0000 by the STD instruction above) ...|$|R
40|$|The Public Health Association of Australia’s {{drug policy}} {{originally}} addressed both licit and illicit drug related problems in Australia. The previous overarching policy {{has now been}} developed into three distinct policies: <b>one</b> <b>addressing</b> alcohol, <b>one</b> <b>addressing</b> pharmaceutical drug misuse and this <b>one</b> <b>addressing</b> illicit drug related problems...|$|R
5000|$|A two-byte {{instruction}} specialized {{for program}} looping {{is new to}} the Z80. DJNZ (Decrement Jump if Non-Zero) takes a signed 8-bit displacement as an immediate operand. The B register is decremented. If the result is nonzero then program execution jumps relative to {{the address of the}} PC plus the displacement. The flags remain unaltered. To perform an equivalent loop on an 8080 would require separate decrement and jump (to a two-byte absolute <b>address)</b> <b>instructions,</b> and the flag register would be altered.|$|R
5000|$|A {{label to}} the left of an {{instruction}} mnemonic is converted to the memory <b>address</b> the <b>instruction</b> or data is stored at. i.e. loopstart INP ...|$|R
40|$|Communication {{components}} (<b>address,</b> <b>instruction,</b> {{and data}} buses and associated hardware like I/O pins, pads, and buffers) are contributing increasingly to the area/cost and power consumption of microprocessor systems. To decrease costs due to address buses, we propose to use narrow widths for underutilized buses (hardware-only compression) to transmit information in multiple cycles. We analyze performance and power consumption overheads of hardware-only compression and investigate {{the use of}} “address concatenation ” to mitigate performance loss and address offsets and XORs to reduce power consumption overheads. ...|$|R
40|$|PC → {{instruction}} memory, {{fetch instruction}} � Register numbers → register file, read registers � Depending on instruction class � Use ALU to calculate � Arithmetic result � Memory address for load/store � Branch target address � Access data memory for load/store � PC ← target address or PC + 4 3 Abstract / Simplified View data PC <b>address</b> <b>instruction</b> instruction memory register # registers register # register # ALU address data memory data � Two types of functional units: elements that operate on data values (combinational) elements that contain state (sequential) 4 Abstract / Simplified View data PC <b>address</b> <b>instruction</b> instruction memory register # registers register # register # ALU address data memory data � Cannot just join wires together � Use multiplexers 5 Recall: Logic Design Basics � Information encoded in binary � Low voltage = 0, High voltage = 1 � One wire per bit � Multi-bit data encoded on multi-wire buses � Combinational element � Operate on data � Output {{is a function}} of input � State (sequential) elements � Store information 6 Sequential Elements � Register: stores data in a circuit � Uses a clock signal to determine when to update the stored value � Edge-triggered: update when Clk changes from 0 to...|$|R
50|$|To {{work around}} this difficulty, most {{assembly}} languages (including the LMC) combine the mnemonics with labels. A label {{is simply a}} word {{that is used to}} either name a memory <b>address</b> where an <b>instruction</b> or data is stored, or to refer to that <b>address</b> in an <b>instruction.</b>|$|R
50|$|In {{the early}} {{instances}} {{of the architecture}} (System/360 and early System/370), the <b>instruction</b> <b>address</b> was 24 bits; in later instances (XA/370), the <b>instruction</b> <b>address</b> was 31 bits plus a mode bit (24 bit addressing mode if zero; 31 bit <b>addressing</b> mode if <b>one)</b> {{for a total of}} 32 bits.|$|R
40|$|A major hurdle {{of recent}} x 86 superscalar {{processor}} designs is limited instruction issue rate {{due to the}} overly complex x 86 instruction formats. To alleviate this problem, the machine states must be preserved and the <b>instruction</b> <b>address</b> routing paths must be simplified. We propose an <b>instruction</b> <b>address</b> queue, whose queue size has been estimated to handle saving of <b>instruction</b> <b>addresses</b> with three operations: allocation, access, and retirement. The <b>instruction</b> <b>address</b> queue will supply the stored <b>instruction</b> <b>addresses</b> as data for three mechanisms: changing instruction flow, updating BTB, and handling exceptions. It {{can also be used}} for internal snooping to solve self-modified code problems. Two CISC hazards in the x 86 architectures, the variable instruction length and the complex addressing mode, have been considered in this design. Instead of the simple full associative storing method in lower degree (< 4) superscalar systems, the line-offset method is used in this address queue. This will reduce by 1 / 3 the storage space for a degree- 5 superscalar x 86 processor with even smaller access latency. We use synthesis tools to analyze the design, and show that it produces optimized results. Because the address queue design can keep two different line <b>addresses</b> in an <b>instruction</b> access per cycle, this method can be extended for designing a multiple instruction block issue system, such as the trace processor...|$|R
40|$|The {{memory system}} stores {{information}} comprising primarily instructions and data and secondarily address information, such as cache tag fields. It {{interacts with the}} processor by supporting related traffic (again comprising <b>addresses,</b> <b>instructions,</b> and data). Continuing exponential growth in processor performance, combined with technology, architecture, and application trends, place enormous demands on the memory system to permit this information storage and exchange at a high-enough performance (i. e., to provide low latency and high bandwidth access to large amounts of information). This paper comprehensively analyzes the redundancy in the information (<b>addresses,</b> <b>instructions,</b> and data) stored and exchanged between the processor and the memory system and evaluates the potential of compression in improving performance of the memory system. Analysis of traces obtained with Sun Microsystems’ Shade simulator simulating SPARC executables of nine integer and six floating-point programs in the SPEC CPU 2000 benchmark suite yield impressive results. Well-designed compression schemes may provide benefits in performance that far outweigh the extra time and logic for compression and decompression. This will be more so in the future since the speed and size of logic (which {{will be used to}} perform compression/decompression) are improving and are projected to improve at a much higher rate compared to those of interconnect (which will be used to communicate the information), both on-chip and off-chip. Keyword...|$|R
2500|$|Simple {{addressing}} modes with complex <b>addressing</b> performed by <b>instruction</b> sequences ...|$|R
50|$|The primary design {{consultant}} for the Librascope computer was Stan Frankel, a Manhattan Project veteran {{and one of the}} first programmers of ENIAC. He designed a usable computer with a minimal amount of hardware. The single <b>address</b> <b>instruction</b> set had only 16 commands. Not only was the main memory on magnetic drum, but so were the CPU registers, timing information and the master bit clock, each on a dedicated track. The number of vacuum tubes were kept to a minimum by using solid-state diode logic, a bit-serial architecture and multiple usage of each of the 15 flip-flops.|$|R
5000|$|Dick and Carey made a {{significant}} contribution to the instructional design field by championing a systems view of instruction, in contrast to defining instruction as the sum of isolated parts. The model <b>addresses</b> <b>instruction</b> as an entire system, focusing on the interrelationship between context, content, learning and instruction. According to Dick and Carey, [...] "Components such as the instructor, learners, materials, instructional activities, delivery system, and learning and performance environments interact with each other and work together to bring about the desired student learning outcomes". The components of the Systems Approach Model, also known as the Dick and Carey Model, are as follows: ...|$|R
5000|$|IP/EIP/RIP: Instruction pointer. Holds {{the program}} counter, the current <b>instruction</b> <b>address.</b>|$|R
5000|$|... 64-bit {{processor}} {{status register}} (PSW), {{which includes a}} 24-bit <b>Instruction</b> <b>Address</b> ...|$|R
