2|679|Public
40|$|Online {{reviewing}} {{has been}} on the rise and is extremely useful and accessible to web users due to the rise in social networking and more reviewing platforms. Being that some reviews tend to contain more text than is necessary to convey the sentiment of the review, review summarization with respect to polarity classification has become necessary. This work gives a comparative investigation of three forms of summarization approaches used for polarity classification. These include using SentiWordNet for a lexical approach, SVM Light for a statistical approach, and the <b>Open</b> <b>Text</b> <b>Summarizer</b> for a traditional summarization based approach...|$|E
40|$|Text {{summarization}} (or automatic summarization) is {{the creation}} of a shortened version of a text by a computer program and work on it dates back as far as 40 years. Text summarization has been applied with high rates of success {{to a wide range of}} areas in the academic and commercial sectors; however, not as much work has been focused in the meeting domains. A feasibility study of incorporating a text summarization tool to generate meeting summaries is examined by the means of a product evaluation. The evaluation process is carried out in 4 distinct phases: (1) preparation, (2) criteria establishment, (3) characterization, and (4) testing. The experiment conducted in the testing phase compare the three identified and characterized text summarization products, with respect to performance and acceptability. The test data (meeting transcript) is derived from a specific software engineering meeting, known as SCRUM. As a performance measurement, each product is examined by the degree to which the important sentences generated by the software matches the manually identified important sentences. Acceptability is measured by a survey where meeting participants grade the summaries by the means of answering questionnaires. The three products to be evaluated are: (1) <b>Open</b> <b>Text</b> <b>Summarizer,</b> (2) Pertinent Summarizer, and (3) TextAnalyst. ...|$|E
40|$|We are {{presenting}} {{the construction of}} a Swedish corpus aimed at research 1 on Information Retrieval, Information Extraction, Named Entity Recognition and Multi Text Summarization, we will also present the results on evaluating our Swedish <b>text</b> <b>summarizer</b> SweSum with this corpus. The corpus has been constructed by using Internet agents downloading Swedish newspaper text from various sources. A small part of this corpus has then been manually annotated. To evaluate our <b>text</b> <b>summarizer</b> SweSum we let ten students execute our <b>text</b> <b>summarizer</b> with increasing compression rate on the 100 manually annotated texts to find answers to questions. The results showed that at 40 percent summarization/compression rate the correct answer rate was 84 percent...|$|R
30|$|The {{algorithm}} {{performs the}} task of text summarization is called as <b>text</b> <b>summarizer.</b> The <b>text</b> <b>summarizers</b> are broadly categorized in two categories which are single-document summarizer and multi-document summarizers. In single-document summarizers, a single large text document is summarized to another single document summary, whereas in multi-document summarization, a set of text documents (multi documents) are summarized to a single document summary which represents the overall glimpse of the multiple documents.|$|R
40|$|Abstract — The {{creation}} of an abstract over a text document prepared by a computer program is defined as an Automatic <b>Text</b> <b>Summarizer.</b> This abstract of the text document must however contain all the salient features of the original document. This paper tries to cover the necessary functional modules that complete an automatic <b>text</b> <b>summarizer.</b> It also highlights the trends and challenges in text summarization. Surveys of certain text summarization techniques are also mentioned...|$|R
40|$|We have {{constructed}} {{an integrated}} web-based system for collection of extract-based corpora and {{for evaluation of}} summaries and summarization systems. During evaluation and examination of the collected and generated data we found that {{in a situation of}} low agreement among the informants the corpus gives unduly favors to summarization systems that use sentence position as a central weighting feature. The problem is discussed and a possible solution is outlined. 1. Background When developing <b>text</b> <b>summarizers</b> and other information extraction tools it is extremely difficult to assess the performance of these tools. One {{reason for this is that}} evaluation is time-consuming and needs large manual efforts. When changing the architecture of the summarizer one needs to carry out the evaluation process again. Therefore it would be fruitful to have a tool that directly can assess the result from a <b>text</b> <b>summarizer</b> repeatedly and automatically. We have for this reason constructed the KTH extract tool to create an extract corpus that can be used to evaluate <b>text</b> <b>summarizers.</b> To create the extract corpus we need a large group of human informants. When the extract corpus is in place it can be used repeatedly with little effort. One other advantage is that one can create an extract corpus in any language and evaluate any language-dependant <b>text</b> <b>summarizer,</b> as long as one is sure about the quality of the corpus. In order to use the extract corpus for evaluation of a summarizer one needs careful preparation of the corpus, also it is important to discuss in what sense the extract corpus can correspond to the output of the summarizer. The specific target for our evaluation is the SweSum <b>text</b> <b>summarizer</b> for Swedish news text and the DanSum 1 <b>text</b> <b>summarizer</b> for Danish news text. SweSum is a <b>text</b> <b>summarizer</b> mainly developed to summarize Swedish news text (Dalianis 2000). SweSum works on sentence level – i. e. extracting sentences, judging the relevance of each sentence and then creating a shorter text (non-redundant extract) containing the highest-ranking sentences from the original text. SweSum has been ported to English, Spanish, French, Danish, Norwegian, German and Farsi so far. SweSum is freely available online a...|$|R
40|$|Abstract:- In this paper, {{we propose}} a {{practical}} approach for extracting {{the most relevant}} sentences from the original document to form a summary. We present this summarization procedure based upon statistical selection and WordNet. Experimental results show that our approach compares favourably to a commercial <b>text</b> <b>summarizer...</b>|$|R
40|$|As {{the amount}} of text {{available}} from electronic sources continues to increase, the study of automatic text summarization is more important now {{than it has ever}} been. Automatically generated summaries should be clear and concise without giving unimportant or redundant information. The process of extractive text summarization involves determining the most important sentences in a document and putting them together to make a summary. We have developed and tested methods for determining which sentences should be included in single-document summaries generated by PARE, an automatic <b>text</b> <b>summarizer.</b> Evaluation of our results using ROUGE, an automated system for comparing summaries, showed that summaries generated by our sentence selection method compare favorably to commercially available <b>text</b> <b>summarizers.</b> 2. PARE BACKGROUND PARE is an automatic <b>summarizer</b> of English <b>text</b> originally developed a...|$|R
40|$|The {{increasing}} {{availability of}} online information has triggered an intensive {{research in the}} area of automatic text summarization within the Natural Language Processing (NLP). Text summarization reduces the text by removing the less useful information which helps the reader to find the required information quickly. There are many kinds of algorithms that can be used to summarize the text. One of them is TF-IDF (Term Frequency-Inverse Document Frequency). This research aimed to produce an automatic <b>text</b> <b>summarizer</b> implemented with TF-IDF algorithm and to compare it with other various online source of automatic <b>text</b> <b>summarizer.</b> To evaluate the summary produced from each summarizer, The F-Measure as the standard comparison value had been used. The result of this research produces 67 % of accuracy with three data samples which are higher compared to the other online summarizers...|$|R
40|$|Abstract:- This paper {{presents}} an improved and practical approach to automatically summarizing unstructured document by extracting {{the most relevant}} sentences from plain text or html version of original document. This technique proposed is based upon Key Sentences using statistical method and WordNet. Experimental results show that our approach compares favourably to a commercial <b>text</b> <b>summarizer,</b> and some refinement techniques improves the summarization quality significantly...|$|R
40|$|Information {{overload}} is {{a global}} problem that requires solution. Automatic <b>text</b> <b>summarizer,</b> a computer program that summarizes a text, {{is one of the}} natural language processing technologies that have got researchers focus to help information users. In this study, three methods have been used for the development of Afan Oromo news <b>text</b> <b>summarizers</b> and the resulting three summarizers have been evaluated both objectively and subjectively. These are: S 1 that uses term frequency and position methods without Afan Oromo stemmer and language specific lexicons (synonyms and abbreviations); S 2 is a summarizer with combination of term frequency and position methods with Afan Oromo stemmer and language specific lexicons and S 3 is with improved position method and term frequency as well as the stemmer and language specific lexicons. The result of objective evaluation shows that S 3 outperformed the two summarizers (S 1 and S 2) by 47 % and 34 %. Moreover, the subjective evaluation result also shows that S 3 better than the other summarizers (S 1 and S 2) with informativeness, linguistic quality, and coherence and structure...|$|R
40|$|Abstract. Due to the {{explosive}} growth of the world-wide web, automatic text summarization has become an essential tool for web users. In this paper we present a novel approach for creating text summaries. Using fuzzy logic and word-net, our model extracts the most relevant sentences from an orig-inal document. The approach utilizes fuzzy measures and inference on the extracted textual information from the document {{to find the most}} significant sentences. Experimental results reveal that the proposed approach extracts the most relevant sentences when compared to other commercially available <b>text</b> <b>summarizers.</b> <b>Text</b> pre-processing based on word-net and fuzzy analysis is the main part of our work. 1...|$|R
40|$|This paper {{presents}} a speech summarizer that summarizes input speech via several prosodic features, unlike models that use a speech recognizer and conventional summarizing techniques proposed in natural language processing. Our approach analyzes {{the borders of}} summary units by employing prosodic features of pitch, power, and pause to summarize the input speech. Our summary generation trial implies robustness against noisy input compared with both a sequential connection model of a speech recognizer and a <b>text</b> <b>summarizer.</b> 1...|$|R
40|$|We {{present results}} from {{evaluations}} of an automatic text summarization technique {{that uses a}} combination of Random Indexing and PageRank. In our experiments we use two types of texts: news paper texts and government texts. Our results show that text type {{as well as other}} aspects of texts of the same type influence the performance. Combining PageRank and Random Indexing provides the best results on government texts. Adapting a <b>text</b> <b>summarizer</b> for a particular genre can improve text summarization. 1...|$|R
40|$|With the {{proliferation}} of the Internet and the huge amount of data it transfers, automatic text summarization is becoming more important. In this paper we first analyze some {{state of the art}} methods to text summarization., We also try to analyze one of the previous text summarization methods, "Machine learning Approach", and eliminate its shortcomings. Finally we present an approach to the design of an automatic <b>text</b> <b>summarizer</b> that generates a summary using fuzzy logic to obtain better results compared to previous methods. 1...|$|R
40|$|Abstract. This paper {{presents}} a context-aware <b>text</b> <b>summarizer</b> based on on-tologies {{intended to be}} used for adapting information to mobile devices. The system generates summaries from texts according to the profile of the user and the context where he/she is at the moment. Context is determined by spatial and temporal localization. Ontologies are used to allow identifying which parts of the texts are related to the user’s profile and to the context. Ontologies are structured as hierarchies of concepts and concepts are represented by keywords with a weight associated...|$|R
40|$|Abstract. This paper {{presents}} a summary evaluation method {{based on a}} complex network measure. We show how to model summaries as complex networks and establish a possible correlation between summary quality and the measure known as dynamics of the network growth. It is a generic and language independent method that enables easy and fast comparative evaluation of summaries. We evaluate our approach using manually produced summaries and automatic summaries produced by three automatic <b>text</b> <b>summarizers</b> for the Brazilian Portuguese language. The results are in agreement with human intuition and showed to be statistically significant. ...|$|R
40|$|We {{investigate}} {{different areas}} of the highdimensional vector space built by the automatic <b>text</b> <b>summarizer</b> HolSum, which evaluates sets of summary candidates using their similarity to the original text. Previously, the search for a good summary was constrained to a very limited area of the summary space. Since an exhaustive search is not reasonable we have sampled new parts of the space using randomly chosen starting points. We also replaced the simple greedy search with simulated annealing. Agreedy search from the leading sentences still finds the best summary. Finally, we also evaluate...|$|R
40|$|Abstract. We have {{designed}} and implemented a bottom-up text parsing strategy for English and integrated {{it into the}} automatic <b>text</b> <b>summarizer</b> PARE, replacing the old link-grammar parser previously used. Constituency trees from our parser provide all part-of-speech linkages as input to several other code modules in PARE. The parser was written to be highly modular, which facilitates its easy integration {{into the rest of}} the PARE system. The parser uses rules which are written in Chomsky Normal Form, a specialization of a general context-free grammar. These improvements should allow PARE to be more efficient and produce more accurate summaries. 1. PARE Backgroun...|$|R
40|$|The aim of {{this article}} is to {{describe}} an existing implementation of a <b>text</b> <b>summarizer</b> for Polish, to analyze the results and propose the possibilities of further development. The problem of text summarizing has been already addressed by science but until now there has been no implementation designed for Polish. The implemented algorithm is based on existing developments in the field but it also includes some improvements. It has been optimized for newspaper texts ranging from approx. 10 to 50 sentences. Evaluation has shown that it works better than known generic summarization tools when applied to Polish...|$|R
40|$|The thesis {{investigates the}} {{document}} aboutness task and proposes the design, implementation and {{test of a}} system that identifies the main focus of a text by detecting entities which are salient for its discourses and are drawn from Wikipedia. In order to design this system we deploy several Natural Language Processing tools, such as entity annotator, <b>text</b> <b>summarizer</b> and dependency parser. By using these tools we derive a large set of features upon which we develop a (binary) classifier that distinguishes salient versus non-salient entities. The efficiency and effectiveness of the developed system is checked via a large experimental test over the well-known annotated New York Times dataset...|$|R
40|$|This work proposes an {{approach}} {{to address the problem}} of improving content selection in automatic text summarization by using some statistical tools. This approach is a trainable summarizer, which takes into account several features, for each sentence to generate summaries. First, we investigate the effect of each sentence feature on the summarization task. Then we use all features in combination to train genetic programming (GP), vector approach and fuzzy approach in order to construct a <b>text</b> <b>summarizer</b> for each model. Furthermore, we use trained models to test summarization performance. The proposed approach performance is measured at several compression rates on a data corpus composed of 17 English scientific articles...|$|R
40|$|We present {{our system}} {{used in the}} DUC 2007 update task, which is our first entry {{in any of the}} DUC evaluations. We make use of ideas within our {{existing}} FreqDistSumm <b>text</b> <b>summarizer,</b> which has been shown to perform well in biomedical text summarization. Our system submitted to the DUC Update Task, called FreqDistUpdate, uses a context sensitive approach to scoring sentences based on a frequency distribution model. FreqDistUpdate performed {{in the middle of all}} systems in three out of the four evaluations. We believe the frequency distribution method is a promising approach for the update task, and that improvements in implementation and approach will lead to better performance in the future DUC evaluations. ...|$|R
40|$|We {{describe}} the porting of the English language REZIME <b>text</b> <b>summarizer</b> to the French language. REZIME is a single-document summarizer particularly focused on summarization of medical documents. Summaries {{are created by}} extracting key sentences from the original document. The sentence selection employs machine learning techniques, using statistical, syntactic and lexical features which are computed based on specialized language resources. The REZIME system was initially developed for English documents. In this paper we present the summarizer architecture, and {{describe the}} steps required to adapt it to the French language. The summarizer performance is evaluated for English and French datasets. Results show that the adaptation to French results in a system performance comparable to English...|$|R
40|$|With the {{overwhelming}} amount of textual information available in electronic formats on the web, {{there is a}} need for an efficient <b>text</b> <b>summarizer</b> capable of condensing large bodies of text into shorter versions while keeping the relevant information intact. Such a technology would allow users to get their information in a shortened form, saving valuable time. Since 1997, Microsoft Word has included a summarizer for documents, and currently there are companies that summarize breaking news and send SMS for mobile phones. I wish to create a <b>text</b> <b>summarizer</b> to provide condensed versions of original documents. My focus is on blogs, because people are increasingly using this mode of communication to express their opinions on a variety of topics. Consequently, it will be very useful for a reader to be able to employ a concise summary, tailored to his or her own interests to quickly browse through volumes of opinions relevant to any number of topics. Although many summarization methods exist, my approach involves employing the Lanczos algorithm to compute eigenvalues and eigenvectors of a large sparse matrix and SVD (Singular Value Decomposition) as a means of identifying latent topics hidden in contexts; and the next phase of the process involves taking a high-dimensional set of data and reducing it to a lower-dimensional set. This procedure makes it possible to identify the best approximation of the original text. Since SQL makes it possible to allow analyzing data sets and take advantage of the parallel processing available today, in most database management systems, SQL is employed in my project. The utilization of SQL without external math libraries, however, adds to challenge in the computation of the SVD and the Lanczos algorithm...|$|R
50|$|One of the {{earliest}} companies to use favored placement was <b>Open</b> <b>Text</b> Corporation for its <b>Open</b> <b>Text</b> Index search engine in 1996. However, the practice was met with complaints from consumers, and <b>Open</b> <b>Text</b> abandoned the idea within a few weeks.|$|R
40|$|FarsiSum is {{an attempt}} to create a text {{summarization}} system for Persian, based upon SweSum. SweSum is an automatic <b>text</b> <b>summarizer</b> for Swedish that is also available for Norwegian, Danish, Spanish, English, French and German texts. SweSum is supposed to work for any language in a so called generic mode. The program in the generic mode uses only general extraction algorithms in the summarization, and these do not refer to any language specific information. The aim of this study is • To investigate how SweSum behaves when applied to a text which is not transcribed in a Roman writing system? • To design and implement FarsiSum by using the methods and algorithms implemented in the SweSum project. • To evaluate FarsiSum...|$|R
40|$|This paper {{describes}} an alternative architecture for voicemail data retrieval on the move. It {{is comprised of}} three distinct components: a speech recognizer, a <b>text</b> <b>summarizer</b> and a WAP push service initiator, enabling mobile users to receive a text summary of their voicemail in realtime without an explicit request. Our approach overcomes the cost and usability limitations of the conventional voicemail retrieval paradigm which requires a connection establishment in order to listen to spoken messages. We report performance results on all different components of the system which has been trained on a database containing 1843 North American English messages {{as well as on}} the duration of the corresponding data path. The proposed architecture can be further customized to meet the requirements of a complete voicemail value-added service...|$|R
50|$|Bray {{left the}} new OED project in 1989 to co-found <b>Open</b> <b>Text</b> Corporation with two colleagues. <b>Open</b> <b>Text</b> commercialised the search engine {{employed}} in the new OED project.|$|R
40|$|Abstract—Automatic text {{summarization}} is {{technique of}} compressing the original text into shorter form which will provide same meaning and information as provided by original text. The brief summary produced by summarization system allows readers to {{quickly and easily}} understand the content of original documents without having to read each individual document. The overall motive of text summarization is to convey the meaning of text by using less number of words and sentences. Summaries are of two types: Abstractive summaries and Extractive summaries. Extractive summaries involve extracting relevant sentences from the source text in proper order. The relevant sentences are extracted by applying statistical and language dependent features to the input text. On the other hand, abstractive text summaries are made by applying natural language understanding. Human beings usually make summaries in abstractive way. Moreover abstractive summaries can also involve the words or sentences which are not present in the input text. Automatic generation of abstractive summary is more difficult as compared to producing extractive text summary. This paper concentrates on survey and performance analysis of automatic <b>text</b> <b>summarizers</b> for Indian languages. Index Terms—Indian <b>summarizers,</b> <b>summarizers,</b> <b>text</b> summarization system I...|$|R
5000|$|... 2006 - Hummingbird {{acquired}} by <b>Open</b> <b>Text</b> for $489 million. Hummingbird informed Texas Guaranteed Student Loan Company {{that it had}} lost a piece of equipment—albeit password protected—containing the unencrypted personal data (names and social security numbers) of an estimated 1.3 million Texas student loan recipients. The company's board of directors agreed {{to be sold to}} Symphony Technology Group, but following a hostile bid from <b>Open</b> <b>Text,</b> the board negotiated a deal with its rival and accepted an all-cash $489 million USD buy-out from <b>Open</b> <b>Text.</b> [...] Following the close of the deal in October, <b>Open</b> <b>Text</b> announced a 15% global workforce reduction.|$|R
40|$|With {{the coming}} of the {{information}} revolution, electronic documents are becoming a principle media of business and academic information. Thousands and thousands of electronic documents are produced and made available on the internet each day. In order to fully utilizing these on-line documents effectively, it is crucial to be able to extract the gist of these documents. Having a Text Summarization system would thus be immensely useful in serving this need. The objective of automatic text summarization is to extract essential sentences that cover almost all the concepts of a document so that users are able to comprehend the ideas the documents tries to address by simply reading through the corresponding summary. In this paper we investigate some novel technique to develop an effective automatic Oriya <b>text</b> <b>summarizer.</b> These techniques can efficiently and effectively save users‟ time while summarizing a particular text...|$|R
40|$|With the {{proliferation}} of the Internet and the huge amount of data it transfers, text summarization is be-coming more important. We present an approach {{to the design of}} an automatic <b>text</b> <b>summarizer</b> that gen-erates a summary by extracting sentence segments. First, sentences are broken into segments by special cue markers. Each segment is represented by a set of predefined features (e. g. location of the segment, average term frequencies of the words occurring in the segment, number of title words in the segment, and the like). Then a supervised learning algorithm is used to train the summarizer to extract important sentence segments, based on the feature vector. Re-sults of experiments on U. S. patents indicate that the performance of the proposed approach compares very favorably with other approaches (including Microsoft Word summarizer) in terms of precision, recall, and classification accuracy...|$|R
50|$|RedDot, {{founded in}} 1993, is a {{business}} unit of <b>Open</b> <b>Text</b> Corporation and {{is referred to as}} the Web Solutions Group of <b>Open</b> <b>Text.</b> The software assists in the management of content, with regulatory compliance and industry specific requirements.|$|R
40|$|We {{present a}} {{cut and paste}} based <b>text</b> <b>summarizer,</b> which uses {{operations}} derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer. 1 Introduction There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals. Certainly one factor contributing to this gap is that automatic systems can not always correctly identify the important topics of an article. Another factor, however, which has received little attention, is that automatic <b>summarizers</b> have poor <b>text</b> generation techniques. Most automatic summarizers rely on extract [...] ...|$|R
40|$|Methods for {{detecting}} sentences in an input document set, which are both relevant and novel {{with respect to}} an information need, would be of direct benefit to many systems, such as extractive <b>text</b> <b>summarizers.</b> However, satisfactory levels of agreement between judges performing this task manually have yet to demonstrated, leaving researchers {{to conclude that the}} task is too subjective. In previous experiments, judges were asked to first identify sentences that are relevant to a general topic, and then to eliminate sentences from the list that do not contain new information. Currently, a new task is proposed, in which annotators perform the same procedure, but {{within the context of a}} specific, factual information need. In the experiment, satisfactory levels of agreement between independent annotators were achieved on the first step of identifying sentences containing relevant information relevant. However, the results indicate that judges do not agree on which sentences contain novel information...|$|R
