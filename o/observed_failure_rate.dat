5|10000|Public
40|$|Unique highly {{reliable}} components are typical for aerospace industry. For such components, {{due to their}} high reliability and uniqueness, {{we do not have}} enough empirical data to make statistically reliable estimates about their failure rate. To overcome this limitation, the empirical data is usually supplemented with expert estimates for the failure rate. The problem is that experts tend to be [...] especially in aerospace industry [...] over-cautious, over-conservative; their estimates for the failure rate are usually much higher than the actual <b>observed</b> <b>failure</b> <b>rate.</b> In this paper, we provide a new fuzzy-related statistically justified approach for reducing this over-estimation...|$|E
40|$|Abstract—Unique highly {{reliable}} components are typical for aerospace industry. For such components, {{due to their}} high reliability and uniqueness, {{we do not have}} enough empirical data to make statistically reliable estimates about their failure rate. To overcome this limitation, the empirical data is usually supplemented with expert estimates for the failure rate. The problem is that experts tend to be – especially in aerospace industry – over-cautious, over-conservative; their estimates for the failure rate are usually much higher than the actual <b>observed</b> <b>failure</b> <b>rate.</b> In this paper, we provide a new fuzzy-related statistically justified approach for reducing this over-estimation. I. FORMULATION OF THE PROBLEM Reliability: how it is usually described and evaluated. Failures are ubiquitous. As a result, reliability analysis {{is an important part of}} engineering design...|$|E
40|$|Abstract Background The common frequentist {{approach}} {{is limited in}} providing investigators with appropriate measures for conducting a new trial. To answer such important questions and one {{has to look at}} Bayesian statistics. Methods As a worked example, we conducted a Bayesian cumulative meta-analysis to summarize the benefit of patient-specific instrumentation on the alignment of total knee replacement from previously published evidence. Data were sourced from Medline, Embase, and Cochrane databases. All randomised controlled comparisons of the effect of patient-specific instrumentation on the coronal alignment of total knee replacement were included. The main outcome was the risk difference measured by the proportion of failures in the control group minus the proportion of failures in the experimental group. Through Bayesian statistics, we estimated cumulatively over publication time of the trial results: the posterior probabilities that the risk difference was more than 5 and 10 %; the posterior probabilities that given the results of all previous published trials an additional fictive trial would achieve a risk difference of at least 5 %; and the predictive probabilities that <b>observed</b> <b>failure</b> <b>rate</b> differ from 5 % across arms. Results Thirteen trials were identified including 1092 patients, 554 in the experimental group and 538 in the control group. The cumulative mean risk difference was 0. 5 % (95 % CrI: − 5. 7 %; + 4. 5 %). The posterior probabilities that the risk difference be superior to 5 and 10 % was less than 5 % after trial # 4 and trial # 2 respectively. The predictive probability that the difference in failure rates was at least 5 % dropped from 45 % after the first trial down to 11 % after the 13 th. Last, only unrealistic trial design parameters could change the overall evidence accumulated to date. Conclusions Bayesian probabilities are readily understandable when discussing the relevance of performing a new trial. It provides investigators the current probability that an experimental treatment be superior to a reference treatment. In case a trial is designed, it also provides the predictive probability that this new trial will reach the targeted risk difference in failure rates. Trial registration CRD 42015024176...|$|E
40|$|Simple {{models for}} the <b>failure</b> (mortality) <b>rate</b> change point are considered. The {{relationship}} with the mean residual lifetime function change point problem is discussed. It is shown that when the change point is random, the <b>observed</b> <b>failure</b> (mortality) <b>rate</b> can be obtained via a specific mixture of lifetime distributions. The shape of the <b>observed</b> <b>failure</b> (mortality) <b>rate</b> is analyzed and the corresponding sim-ple but meaningful example is considered. ...|$|R
5000|$|Like many {{methods of}} birth control, {{reliable}} effect is achieved only by correct and consistent use. <b>Observed</b> <b>failure</b> <b>rates</b> of withdrawal {{vary depending on}} the population being studied: studies have found actual <b>failure</b> <b>rates</b> of 15-28% per year. In comparison, the pill has an actual use <b>failure</b> <b>rate</b> of 2-8%, while the intrauterine device (IUD) has an actual use <b>failure</b> <b>rate</b> of 0.8%. [...] The condom has an actual use <b>failure</b> <b>rate</b> of 10-18%. However, some authors suggest that actual effectiveness of withdrawal could be similar to effectiveness of condoms, and this area needs further research. (See Comparison of birth control methods.) ...|$|R
40|$|Deferring the {{maintenance}} activity {{can often be}} detrimental to the asset’s operation. This {{is often the case}} where minimal repair actions are performed for severe failures. Here, deferring {{the maintenance}} activity ends up accelerating equipment deterioration, thereby resulting in more severe failures. In this paper, a discrete event simulation model for quantifying the effect of deferred maintenance on system performance such as production loss and <b>observed</b> <b>failure</b> <b>rates</b> is proposed. The simulation study incorporates four maintenance activities proposed in the ISO 14224 that include inspection, modify, extensive repair and corrective replacement. Each maintenance activity is evaluated based on its impact on the remaining useful life of the component. A real life case of a thermal power plant is discussed. status: publishe...|$|R
40|$|Introduction: A {{proof-of-concept}} {{study was}} designed to evaluate the antiviral efficacy, safety and tolerability of a two-drug regimen with dolutegravir 50  mg once daily (QD) plus lamivudine 300  mg once daily as initial highly active antiretroviral therapy (HAART) among antiretroviral (ARV) -naive patients. Methods: PADDLE is a pilot study including 20 treatment-naive adults. To be selected, participants had no IAS-USA-defined resistance, HIV- 1 RNA ≤ 100, 000 copies/mL at screening and negative HBsAg. Plasma viral load (pVL) was measured at baseline; days 2, 4, 7, 10, 14, 21 and 28; weeks 6, 8 and 12; and thereafter every 12  weeks up to 96  weeks. Primary endpoint was the proportion of patients with HIV- 1 RNA < 50 copies/mL in an intention to treat (ITT) -exposed analysis at 48  weeks (the FDA snapshot algorithm). Results: Median HIV- 1 RNA at entry was 24, 128 copies/mL (interquartile range (IQR) : 11, 686 – 36, 794). Albeit as per protocol, all patients had pVL ≤ 100, 000 copies/mL at screening as required by inclusion criteria, four patients had ≥ 100, 000 copies/mL at baseline. Median baseline CD 4 + T-cell count was 507 per cubic millimetre (IQR: 296 – 517). A rapid decline in pVL was observed (median VL decay from baseline to week 12 was 2. 74 logs). All patients were suppressed at week 8 onwards up to week 24. At week 48, 90 % (18 / 20) reached the primary endpoint of a pVL < 50 copies/mL. Median change in CD 4 cell count between baseline and week 48 was 267 cells/mm 3 (IQR: 180 – 462). No major tolerability/toxicity issues were observed. Nineteen patients completed 48  weeks of the study, and one patient (with undetectable VL at last visit) committed suicide. One patient presented a low-level protocol-defined confirmed virological failure at week 36, being the only observed failure. This patient had pVL < 50 copies/mL at the end-of-study visit without having changed the two-drug regimen. <b>Observed</b> <b>failure</b> <b>rate</b> was 5 %. This is the first report of integrase strand transfer inhibitor/lamivudine dual regimen in ARV-naive patients. Conclusions: This novel dual regimen of dolutegravir and lamivudine warrants further clinical research and consideration as a potential therapeutic option for ARV-therapy-naive patients. ClinicalTrials. gov Identifier: NCT 02211482...|$|E
40|$|An {{a priori}} pharmacokinetic/pharmacodynamic (PK/PD) target of 40 % daily time above the MIC (T >MIC; {{based on the}} MIC 90 of 0. 06 μg/ml for Streptococcus pyogenes {{reported}} in the literature) was shown to be achievable in a phase 1 study of 23 children with a once-daily (QD) modified-release, multiparticulate formulation of amoxicillin (amoxicillin sprinkle). The daily T >MIC achieved with the QD amoxicillin sprinkle formulation was comparable to that achieved with a four-times-daily (QID) penicillin VK suspension. An investigator-blinded, randomized, parallel-group, multicenter study involving 579 children 6 months to 12 years old with acute streptococcal tonsillopharyngitis was then undertaken. Children were randomly assigned 1 : 1 to receive either the amoxicillin sprinkle (475 mg for ages 6 months to 4 years, 775 mg for ages 5 to 12 years) QD for 7 days or 10 mg/kg of body weight of penicillin VK QID for 10 days (up to the maximum dose of 250 mg QID). Unexpectedly, the rates of bacteriological eradication at the test of cure were 65. 3 % (132 / 202) for the amoxicillin sprinkle and 68. 0 % (132 / 194) for penicillin VK (95 % confidence interval, − 12. 0 % to 6. 6 %). Thus, neither antibiotic regimen met the minimum criterion of ≥ 85 % eradication ordinarily required by the U. S. FDA for first-line treatment of tonsillopharyngitis due to S. pyogenes. The results of subgroup analyses across demographic characteristics and current infection characteristics and by age/weight categories {{were consistent with the}} primary-efficacy result. The clinical cure rates for amoxicillin sprinkle and penicillin VK were 86. 1 % (216 / 251) and 91. 9 % (204 / 222), respectively (95 % confidence interval, − 11. 6 % to − 0. 4 %). The results of a post hoc PD analysis suggested that a requirement for 60 % daily T >MIC 90 more accurately predicted the observed high failure rates for bacteriologic eradication with the amoxicillin sprinkle and penicillin VK suspension studied. Based on the association between longer treatment courses and maximal bacterial eradication rates reported in the literature, an alternative composite PK/PD target taking into consideration the duration of therapy, or total T >MIC, was considered and provides an alternative explanation for the <b>observed</b> <b>failure</b> <b>rate</b> of amoxicillin sprinkle...|$|E
40|$|Abstract-Fault {{observability}} {{based on}} the behavior of memory references is studied. Traditional studies view memory as one monolithic entity that must completely work to be considered reliable. The usage patterns of a particular program’s memory are emphasized here. This paper develops a new model for the successful execution of a program taking into account the usage of the data by extending a cache memory performance model. Three variations, based on well known allocation schemes, are presented (Le., whether the program’s storage is preallocated, dynamically allocated, or constrained in allocation). This is contrasted to traditional memory reliability calculations to show that the actual mean time to failure may be more optimistic when program behavior is considered. It also develops expressions for the probability of unobserved faults. With several studies reporting correlations between increased workloads and increased <b>failure</b> <b>rates,</b> a new theory is proposed here that provides an explanation for this behavior. The model studies several program traces demonstrating that increased workloads could cause an increase of the <b>observed</b> <b>failure</b> <b>rates</b> in the range of 32 % to 53 %. index Terms-Progra...|$|R
40|$|Non radiation-hardened SRAM-based Field Pro- grammable Gate Arrays (FPGAs) {{are very}} {{sensitive}} to Single Event Upsets (SEUs) affecting their configuration memory and thus suitable hardening techniques are needed when they are intended to be deployed in critical applications. Triple Module Re- dundancy is a known solution for hardening digital logic against SEUs that is widely adopted for traditional techniques (like ASICs). In this paper we present an analysis of the SEU effects in circuits hardened according to the Triple Module Redundancy to investigate the possibilities of successfully applying TMR to designs mapped on commercial-off-the-shelf SRAM-based FPGAs, which are not radiation hardened. We performed dif- ferent fault-injection experiments in the FPGA configuration memory implementing TMR designs and we observed that the percentage of SEUs escaping TMR could reach 13 %. In this paper we report detailed evaluations {{of the effects of the}} <b>observed</b> <b>failure</b> <b>rates,</b> and we proposed a first step toward an improved TMR implementation...|$|R
40|$|Thermal cycling {{durability}} of Plastic {{ball grid array}} (PBGA) interconnects is known to decrease as I/O count increases. This is due, in part, to mechanistic effects; such as increasing thermal expansion mismatches between component and PWB, due to increasing package sizes. Failure prediction due to these mechanistic effects is a deterministic process {{and is based on}} the load level found in the critical joint (joint with the most severe loading). However, due to probabilistic effects, for example manufacturing variabilities, premature failure may result in one of several joints in the neighborhood of the critical one. Failure probability increases as the number of joints in this critical region increases. Thus, <b>observed</b> <b>failure</b> <b>rates</b> are due to a convolution of deterministic and probabilistic effects. In effect, for large BGAs, deterministic predictions may overestimate interconnect durability. This thesis uses thermal cycling experiments and detailed mechanistic modeling to present a methodology for adjusting deterministic predictions of solder joint failure with a suitable probabilistic correction factor...|$|R
5000|$|Catching [...] "wild" [...] {{chickenpox}} as a {{child has}} been thought to commonly result in lifelong immunity. Indeed, parents have deliberately ensured this in the past with [...] "pox parties". Historically, exposure of adults to contagious children has boosted their immunity, {{reducing the risk of}} shingles. The CDC and corresponding national organisations are carefully <b>observing</b> the <b>failure</b> <b>rate</b> which may be high compared with other modern vaccines—large outbreaks of chickenpox having occurred at schools which required their children to be vaccinated.|$|R
40|$|Mixtures of {{distributions}} {{are usually}} effectively used for modeling heterogeneity. It {{is well known}} that mixtures of DFR distributions are always DFR. On the other hand, mixtures of IFR distributions can decrease, at least in some intervals of time. As IFR distributions often model lifetimes governed by ageing processes, the operation of mixing can dramatically change the pattern of ageing. Therefore, the study of the shape of the <b>observed</b> (mixture) <b>failure</b> <b>rate</b> in a heterogeneous setting is important in many applications. We study discrete and continuous mixtures, obtain conditions for the mixture <b>failure</b> <b>rate</b> to tend to the <b>failure</b> <b>rate</b> of the strongest populations and describe asymptotic behavior as t tends to infty. Some demographic and engineering examples are considered. The corresponding inverse problem is discussed. ...|$|R
3000|$|... a. Impact {{of error}} removal on turbine {{performance}} (IETP): A hindrance for potential investors in wind energy is constituted by {{the economics of}} wind technology. It was <b>observed</b> that the <b>failure</b> <b>rates</b> of the WTGs now installed have almost continually declined in the first operational years. This {{is true for the}} age old turbines (under 500 / 600  kW class). However, the group of megawatt (MW) WTGs show a significantly higher <b>failure</b> <b>rate,</b> which also declines by increasing age.|$|R
30|$|In this study, the {{marginal}} bone loss was less {{in patients who}} had systemic diseases than healthy patients. There were significant differences in 6  months after the installation and after the functional loading at the distal area. Similarly, Mombelli and Cionca [42] <b>observed</b> that <b>failure</b> <b>rates</b> were not different between 98 systemically healthy subjects and 109 patients {{with a history of}} other systemic diseases. Moy et al. [1, 39] also mentioned that sex, hypertension, coronary artery disease, pulmonary disease, steroid therapy, chemotherapy, and not being on hormone replacement therapy for postmenopausal women were not associated with a significant increase in implant failure.|$|R
40|$|Xpert MTB/RIF is an {{automated}} cartridge-based nucleic acid amplification test that has demonstrated {{its potential to}} detect tuberculosis and rifampicin resistance with high accuracy. To assist scale-up decisions in India, a feasibility assessment of Xpert MTB/RIF implementation was conducted within microscopy centres of 18 RNTCP TB units. As part of programme-based demonstration of Xpert MTB/RIF implementation, we recorded and analysed association between key implementation factors {{and the ability of}} test to produce valid results. Factors contributing to test failures were analysed from GeneXpert software data which provides 'failure codes' and causes for test failures. From March' 12 to January' 13, total 40, 035 suspects were tested by Xpert MTB/RIF, and 39, 680 (99. 1 %) received valid results (Cumulative: 37157 (92. 8 %) on first attempt, 39410 (98. 4 %) on second attempt, 39637 (99. 0 %) on third attempt and 39680 (99. 1 %) on more attempts). Overall initial test failure was 2, 878 (7. 2 % (4 %- 17 %)); of these, 2, 594 (90. 1 %) were re-tested and produced valid results. Most frequent reason of test failure was inadequate sample processing or equipment malfunction (3. 9 %). Other reasons included power failure (1. 1 %), cartridge integrity/component failure (0. 8 %), device-computer communication error (0. 5 %), and temperature-related errors (0. 08 %). Significant variation was <b>observed</b> in <b>failure</b> <b>rates</b> both across instruments and over time; furthermore, substantial variation was <b>observed</b> in <b>failure</b> <b>rate</b> in two cartridges lots. Installation required minimal infrastructure modifications and concerns about adequacy of human resources under public sector facilities and temperature extremes proved unfounded. Under routine conditions, Xpert MTB/RIF provided 99. 1 % valid results in TB suspects with low overall <b>failure</b> <b>rates</b> (7. 2 % initial failure, 0. 9 % final failure); devices provided valuable real-time feedback on reasons for test failure, which were used for rapid corrective action. High modular replacement (32 %) and inter-lot cartridge performance variation remain sources of concern, and warrant close monitoring of <b>failure</b> <b>rates</b> as a key quality indicator...|$|R
40|$|This chapter {{reports a}} {{description}} {{and analysis of}} the factors that influenced the process of adoption and implementation of the e-Government initiative in Oman over the period 2000 - 2013. This research provides an explanation of why government organisations in Oman developed and then adopted e-Government projects, and how that affected their success as an example of what might also be the case in many developing countries. Using the concept of institutional decoupling, this research presents a framework that offers a new understanding of the <b>observed</b> high <b>failure</b> <b>rate</b> of e-Government implementation in many developing countries. In terms of practical contributions, this research concludes important lessons with regard to synchronising motivating factors with institutional, technological and organisational prerequisites, and expected outcomes...|$|R
40|$|BACKGROUND: Pentavalent antimonials {{have been}} the {{mainstay}} of antileishmanial therapy for decades, but increasing <b>failure</b> <b>rates</b> under antimonial treatment have challenged further use of these drugs in the Indian subcontinent. Experimental evidence has suggested that parasites which are resistant against antimonials have superior survival skills than sensitive ones {{even in the absence}} of antimonial treatment. METHODS AND FINDINGS: We use simulation studies based on a mathematical L. donovani transmission model to identify parameters which can explain why treatment <b>failure</b> <b>rates</b> under antimonial treatment increased up to 65 % in Bihar between 1980 and 1997. Model analyses suggest that resistance to treatment alone cannot explain the <b>observed</b> treatment <b>failure</b> <b>rates.</b> We explore two hypotheses referring to an increased fitness of antimony-resistant parasites: the additional fitness is (i) disease-related, by causing more clinical cases (higher pathogenicity) or more severe disease (higher virulence), or (ii) is transmission-related, by increasing the transmissibility from sand flies to humans or vice versa. CONCLUSIONS: Both hypotheses can potentially explain the Bihar observations. However, increased transmissibility as an explanation appears more plausible because it can occur in the background of asymptomatically transmitted infection whereas disease-related factors would most probably be observable. Irrespective of the cause of fitness, parasites with a higher fitness will finally replace sensitive parasites, even if antimonials are replaced by another drug...|$|R
40|$|Indoor {{air quality}} · Indoor {{environment}} · Bacteria measurement · Office · Subtropical climates · Statistical model Those involved in property management {{are under pressure}} to alleviate the risk of disease spread through buildings, especially via the Heating, ventilating and air-conditioning (HVAC) system. To aid this, the level of indoor bacteria {{can be used as}} a referent to identify the performance of the HVAC system. Epistemic assessment of an AC office requires prior knowledge of the probable <b>failure</b> <b>rate</b> of offices in a region, which may be obtained by extensive measurements. This study proposes a sta-tistical model to obtain a rapid estimation of the office probable <b>failure</b> <b>rate</b> over a region, that is, that fraction of offices with an average bacteria level above certain action levels. The model was based on transformation of small sample data to a geometric distribution using two analytical unbiased estimators: the weighted average and corrected sample variance of bacteria levels. These estimators could improve the accuracy of the estimates due to uncertainties of small sample sizes. Model param-eters were determined from a dataset of levels in 290 Hong Kong offices (average 703 cfu m 3 and 95 % range 204 – 2418 cfu m 3), using Monte Carlo simulations. The model was validated with another district survey of 109 offices in nine major Hong Kong commercial districts (average indoor bacteria level 297 cfu m 3 and 95 % range was 691266 cfu m 3). The <b>observed</b> office <b>failure</b> <b>rate</b> of each district was compared with the corresponding model estimate and consistent results were found...|$|R
40|$|Those {{involved}} in property management {{are under pressure}} to alleviate the risk of disease spread through buildings, especially via the Heating, ventilating and air-conditioning (HVAC) system. To aid this, the level of indoor bacteria {{can be used as}} a referent to identify the performance of the HVAC system. Epistemic assessment of an AC office requires prior knowledge of the probable <b>failure</b> <b>rate</b> of offices in a region, which may be obtained by extensive measurements. This study proposes a statistical model to obtain a rapid estimation of the office probable <b>failure</b> <b>rate</b> over a region, that is, that fraction of offices with an average bacteria level above certain action levels. The model was based on transformation of small sample data to a geometric distribution using two analytical unbiased estimators: the weighted average and corrected sample variance of bacteria levels. These estimators could improve the accuracy of the estimates due to uncertainties of small sample sizes. Model parameters were determined from a dataset of levels in 290 Hong Kong offices (average 703 cfu m- 3 and 95 % range 204 - 2418 cfu m- 3), using Monte Carlo simulations. The model was validated with another district survey of 109 offices in nine major Hong Kong commercial districts (average indoor bacteria level 297 cfu m- 3 and 95 % range was 69 - 1266 cfu m- 3). The <b>observed</b> office <b>failure</b> <b>rate</b> of each district was compared with the corresponding model estimate and consistent results were found. Department of Building Services Engineerin...|$|R
40|$|The use of reliability-based design (RBD) in {{foundation}} engineering offers several advantages over traditional methods. Uncertainties in the load and capacity {{terms of the}} design equations can be evaluated rationally using probability theory, and the resultant probability of failure is a comprehensible measure of risk for non-technical people. However, {{there are a few}} drawbacks that have not been addressed effectively so far. For example, it is necessary to select target safety levels, typically from the reliability implied in traditional design methods. Also, the resulting reliability is a nominal value that can be significantly different from the true reliability derived from <b>observed</b> <b>failure</b> <b>rates.</b> This issue is rarely mentioned in the literature, but it affects the utility of probability as a communication tool. In addition, costs are not considered explicitly in the design process, and calculations can be excessively complex and timeconsuming for simple projects. A new framework to determine optimum foundation designs that result in minimum life-cycle costs is presented herein. The traditional approach for design optimization is to minimize an objective function, such as the sum of initial costs and expected cost of failure. Existing optimization methods require a number of initial assumptions and use nominal probabilities of failure, leading to inaccurate results. In the proposed framework, the true probability of failure is estimated using Monte Carlo simulation (MCS) or the first order reliability method (FORM). This process considers the variability of input parameters and the probability of "human errors". Although optimum design parameters can be obtained with the proposed framework, it would not be used in practice often, because it requires knowledge of reliability methods. A simplified approach is necessary to avoid complex calculations and facilitate its widespread use in ordinary projects. Therefore, a simplified method for approximate economic optimization is proposed. In an effort to close the gap between research and practice in {{foundation engineering}}, all the calculations shown herein can be reproduced in a simple spreadsheet with nonlinear optimization capabilities. 2018 - 08 - 2...|$|R
40|$|Most of the {{software}} reliability growth models work {{under the assumption that}} reliability of software grows due to the bugs that cause failures being removed from {{the software}}. While correcting bugs will improve reliability, another phenomenon has often been <b>observed</b> – the <b>failure</b> <b>rate</b> of a software product, as observed by the user, improves with time irrespective of whether bugs are corrected or not. Consequently, the reliability of a product, as observed by users, varies, depending on the length of time they have been using the product. One reason for this reliability growth is that as the users gain experience with the product, they learn to use the product correctly and find work-around for failure-causing situations. Another factor that affects this growth is that following the product installation, the user discovers that other actions may be required, like installing new drivers, upgrading other software to a compatible version, etc. to properly configure the new product. In this paper we present a simple model to represent this phenomenon – we assume that the <b>failure</b> <b>rate</b> for a product decays with a factor α per unit time. Applying this <b>failure</b> <b>rate</b> decay model to the data collected on reported failures and number of units of the product sold, it is possible to determine the initial <b>failure</b> <b>rate,</b> the decay factor, and the steady state <b>failure</b> <b>rate</b> of a product. The paper provides a number of examples where this model has been applied to data captured from released products...|$|R
40|$|Vein graft maladaptation, {{leading to}} poor {{long-term}} patency, {{is a serious}} clinical problem in patients receiving coronary artery bypass grafts (CABGs). Mechanics is known to {{play a key role}} as a stimulus contributing towards vein graft failure. Mechanical stimuli are key to understanding disease progression and clinically <b>observed</b> differences in <b>failure</b> <b>rates</b> between arterial and venous grafts following coronary artery bypass graft surgery. But little has been done to quantify the mechanics in these grafts and its effects on long-term outcomes on grafts. Hence, {{one of the goals of}} this thesis was to quantify mechanical stimuli acting on the grafts and the other goal was to develop continuum mechanics based models of growth and remodeling (G&R) to simulate long-term adaptation. We quantify biologically relevant mechanical stimuli, not available from standard imaging, in patient-specific simulations incorporating non-invasive clinical data. We couple computational fluid dynamics with closed-loop circulatory physiology models to quantify biologically relevant indices, including wall shear, oscillatory shear, and wall strain. We account for vessel-specific material properties in simulating vessel wall deformation. Wall shear was significantly lower and atheroprone area significantlyhigher in venous compared to arterial grafts. Wall strain in venous grafts was significantly lower than in arterial grafts while no significant difference was observed in oscillatory shear index. Simulations demonstrate significant differences in mechanical stimuli acting on venous vs. arterial grafts, in line with clinically <b>observed</b> graft <b>failure</b> <b>rates,</b> offering a promising avenue for stratifying patients at risk for graft failure. We also propose a computational model of venous adaptation to altered hemodynamics based on a constrained mixture theory of G&R. We identify constitutive parameters that optimally match biaxial data from a mouse vena cava, then numerically subject the vein to altered hemodynamic conditions and quantify the extent of adaptation. We identify constitutive relations and parameters that enable adap-tations for a moderate perturbation in hemodynamics. We then fix these relations and parameters, and subject the vein to a range of combined loads (pressure and flow), from moderate to severe, and identify plausible mechanisms of adaptation versus maladaptation. We also explore the beneficial effects of a gradual increase in load on adaptation...|$|R
40|$|Objective: Children {{admitted}} to hospital commonly require peripheral intravenous catheters (PIVCs) for treatment. This {{study sought to}} address {{a gap in the}} literature about current practice in the securement and dressing of PIVCs in paediatric acute care, and to ascertain the duration and failure of these devices. Methods: A prospective cohort study conducted at the Royal Children’s Hospital in Queensland, Australia. All patients aged 0 – 15 years, who presented to the ED between 16 July and 16 October 2012, and had a PIVC inserted prior to emergent admission to the hospital were included. Results: Of 458 participants, median device duration was 29 h (IQR 13 – 58 h), and ranged from less than 1 h to 16 days. One quarter (113 / 456, 24. 8 %) of PIVCs were removed due to device failure, presenting as: infiltration (65 / 456, 14. 3 %); accidental dislodgement (23 / 456, 5. 0 %); blockage (12 / 456, 2. 6 %); phlebitis (7 / 456, 1. 5 %); or ‘other’ (6 / 456, 1. 3 %). PIVC securement and dressings were predominantly bordered polyurethane dressings and splints (n = 457 / 458, 99. 8 %). PIVC placement in the antecubital fossa, in comparison to the hand, was significantly associated with an increased risk for failure (P = 0. 03). No other patient and device characteristics had a significant association with device failure (P > 0. 05). The median dwell time of PIVCs that failed was significantly longer than the PIVCs that did not fail (44. 0 vs 25. 5 h; P = 0. 002). Less than half (53 / 113, 46. 9 %) of failed catheters were replaced with a new PIVC. Conclusions: <b>Observed</b> <b>failure</b> <b>rates</b> were high for a clinically essential device; however, there is no established rate of acceptability against which the results can be benchmarked against to facilitate effectiveness of practice. Many PIVCs appeared to remain in place longer than needed. Dressing and securement practice was homogenous. PIVC placement in the antecubital fossa should be minimised {{to reduce the risk of}} paediatric PIVC failure...|$|R
40|$|Backgroundline of {{treatment}} for management of hydrocephalus despite available new techniques andsystems of shunting. Associated complications should be recognized and managed properly,but the most recognized complications are shunt obstruction which its prevalencethrough surgical approach is discussed here. Two approaches (frontal and parietal) are usedto insert ventriculoperitoneal shunt. In this study we retrospectively examined patterns ofshunt failure in patients with symptoms of shunt malfunction. Factors analyzed includedsite of failure, time from shunt placement or last revision of failure,age of patient at time offailure,infection and primary etiology of hydrocephalus. Two approaches were comparedto determine {{which one is more}} associated with shunt failure. : Shunting procedures specifically ventriculoperitoneal shunts are the mainMethodsretrospectively examined, in 126 cases who were shunted through frontal approach, 48 casesand in 124 patients whose shunts were inserted through parietal approach 64 cases ofmalfunction observed. All data was analyzed with SPSS software and with T-test,and thenthe <b>failure</b> <b>rate</b> for frontal versus parietal approach was compared. : 250 patients with symptoms of shunt malfunction over 4 years period wereResultsof underlying cause of ventriculoperitoneal shunt <b>failure</b> was <b>observed,</b> with theless <b>failure</b> <b>rates</b> through frontal approach. :Significant difference in malfunction rate between these two approaches regardlessConclusionshunt failure and frontal approach demonstrated less <b>failure</b> <b>rate,</b> but as it isknown placing the catheter tip away from the choroids plexus is the most important factoravoiding obstruction. : Although proximal obstruction is {{the most common cause of}} ventriculoperitonea...|$|R
40|$|OBJECTIVE: To {{validate}} all diagnostic prediction {{models for}} ruling out pulmonary embolism that are easily applicable in primary care. DESIGN: Systematic review followed by independent external validation study to assess transportability of retrieved models to primary care medicine. SETTING: 300 general {{practices in the}} Netherlands. PARTICIPANTS: Individual patient dataset of 598 patients with suspected acute pulmonary embolism in primary care. MAIN OUTCOME MEASURES: Discriminative ability of all models retrieved by systematic literature search, assessed by calculation and comparison of C statistics. After stratification into groups with high and low probability of pulmonary embolism according to pre-specified model cut-offs combined with qualitative D-dimer test, sensitivity, specificity, efficiency (overall proportion of patients with low probability of pulmonary embolism), and <b>failure</b> <b>rate</b> (proportion of pulmonary embolism cases in group of patients with low probability) were calculated for all models. RESULTS: Ten published prediction models for the diagnosis of pulmonary embolism were found. Five of these models could be validated in the primary care dataset: the original Wells, modified Wells, simplified Wells, revised Geneva, and simplified revised Geneva models. Discriminative ability was comparable for all models (range of C statistic 0. 75 - 0. 80). Sensitivity ranged from 88 % (simplified revised Geneva) to 96 % (simplified Wells) and specificity from 48 % (revised Geneva) to 53 % (simplified revised Geneva). Efficiency of all models was between 43 % and 48 %. Differences were <b>observed</b> between <b>failure</b> <b>rates,</b> especially between the simplified Wells and the simplified revised Geneva models (<b>failure</b> <b>rates</b> 1. 2 % (95 % confidence interval 0. 2 % to 3. 3 %) and 3. 1 % (1. 4 % to 5. 9 %), respectively; absolute difference - 1. 98 % (- 3. 33 % to - 0. 74 %)). Irrespective of the diagnostic prediction model used, three patients were incorrectly classified as having low probability of pulmonary embolism; pulmonary embolism was diagnosed only after referral to secondary care. CONCLUSIONS: Five diagnostic pulmonary embolism prediction models that are easily applicable in primary care were validated in this setting. Whereas efficiency was comparable for all rules, the Wells rules gave the best performance in terms of lower <b>failure</b> <b>rates...</b>|$|R
40|$|To {{validate}} all diagnostic prediction {{models for}} ruling out pulmonary embolism that are easily applicable in primary care. Systematic review followed by independent external validation study to assess transportability of retrieved models to primary care medicine. 300 general {{practices in the}} Netherlands. Individual patient dataset of 598 patients with suspected acute pulmonary embolism in primary care. Discriminative ability of all models retrieved by systematic literature search, assessed by calculation and comparison of C statistics. After stratification into groups with high and low probability of pulmonary embolism according to pre-specified model cut-offs combined with qualitative D-dimer test, sensitivity, specificity, efficiency (overall proportion of patients with low probability of pulmonary embolism), and <b>failure</b> <b>rate</b> (proportion of pulmonary embolism cases in group of patients with low probability) were calculated for all models. Ten published prediction models for the diagnosis of pulmonary embolism were found. Five of these models could be validated in the primary care dataset: the original Wells, modified Wells, simplified Wells, revised Geneva, and simplified revised Geneva models. Discriminative ability was comparable for all models (range of C statistic 0. 75 - 0. 80). Sensitivity ranged from 88 % (simplified revised Geneva) to 96 % (simplified Wells) and specificity from 48 % (revised Geneva) to 53 % (simplified revised Geneva). Efficiency of all models was between 43 % and 48 %. Differences were <b>observed</b> between <b>failure</b> <b>rates,</b> especially between the simplified Wells and the simplified revised Geneva models (<b>failure</b> <b>rates</b> 1. 2 % (95 % confidence interval 0. 2 % to 3. 3 %) and 3. 1 % (1. 4 % to 5. 9 %), respectively; absolute difference - 1. 98 % (- 3. 33 % to - 0. 74 %)). Irrespective of the diagnostic prediction model used, three patients were incorrectly classified as having low probability of pulmonary embolism; pulmonary embolism was diagnosed only after referral to secondary care. Five diagnostic pulmonary embolism prediction models that are easily applicable in primary care were validated in this setting. Whereas efficiency was comparable for all rules, the Wells rules gave the best performance in terms of lower <b>failure</b> <b>rate...</b>|$|R
40|$|Future crewed {{missions}} to Mars present a maintenance logistics challenge that is unprecedented in human spaceflight. Mission endurance {{defined as the}} time between resupply opportunities will be significantly longer than previous missions, and therefore logistics planning horizons are longer {{and the impact of}} uncertainty is magnified. Maintenance logistics forecasting typically assumes that component <b>failure</b> <b>rates</b> are deterministically known and uses them to represent aleatory uncertainty, or uncertainty that is inherent to the process being examined. However, <b>failure</b> <b>rates</b> cannot be directly measured; rather, they are estimated based on similarity to other components or statistical analysis of <b>observed</b> <b>failures.</b> As a result, epistemic uncertainty that is, uncertainty in knowledge of the process exists in <b>failure</b> <b>rate</b> estimates that must be accounted for. Analyses that neglect epistemic uncertainty tend to significantly underestimate risk. Epistemic uncertainty can be reduced via operational experience; for example, the International Space Station (ISS) <b>failure</b> <b>rate</b> estimates are refined using a Bayesian update process. However, design changes may re-introduce epistemic uncertainty. Thus, there is a tradeoff between changing a design to reduce <b>failure</b> <b>rates</b> and operating a fixed design to reduce uncertainty. This paper examines the impact of epistemic uncertainty on maintenance logistics requirements for future Mars missions, using data from the ISS Environmental Control and Life Support System (ECLS) as a baseline for a case study. Sensitivity analyses are performed to investigate the impact of variations in <b>failure</b> <b>rate</b> estimates and epistemic uncertainty on spares mass. The results of these analyses and their implications for future system design and mission planning are discussed...|$|R
40|$|Submarine Communication Systems require {{usage of}} high-rel {{components}} to guarantee long term reliability. A standard reliability assessment, based on <b>observed</b> <b>failures</b> during test, may be achieved only employing {{a very large}} number of component-hour, which is often unaffordable. An alternative reliability technique is proposed in this paper, based on the degradation monitoring of the components most relevant electrical parameters, subjected to different test flows. This analysis leads to a quantitative estimate of the components <b>failure</b> <b>rate.</b> (C) 2002 Elsevier Science Ltd. All rights reserved...|$|R
40|$|A NASA {{stress index}} model, SINDEX, is {{discussed}} which establishes the relative stress magnitudes along a balloon gore {{as a function}} of altitude. Application of the model to a data base of over 550 ballon flights demonstrates the effectiveness of the method. The results show a strong correlation between stress levels, <b>failure</b> <b>rates,</b> and the point of maximum stress coinciding with the <b>observed</b> <b>failure</b> locations. It is suggested that the model may be used during the balloon design process to lower the levels of stress in the balloon...|$|R
40|$|In Bayesian {{non-parametric}} statistics, {{the extended}} gamma {{process can be}} used to model the class of monotonic hazard functions. However, numerical evaluations of the posterior process are very difficult to compute for realistic sample sizes. To overcome this, we use Monte Carlo methods introduced by Laud, Smith, and Damien (1996) to simulate from the posterior process. We show how these methods {{can be used to}} approximate the increasing <b>failure</b> <b>rate</b> of an item, given <b>observed</b> <b>failures</b> and censored times. We then use the results to compute the optimal maintenance schedule under a specified maintenance policy. ...|$|R
40|$|With {{advances}} in process technology, soft errors are becoming an increasingly critical design concern. Soft errors are manifested as a toggle in Boolean logic, which {{may result in}} failure of the system functionality. Owing to their large area, high density, and low operating voltages, caches are worst hit by soft errors. Although Error Correction Code (ECC) based mechanisms have been suggested to protect the data in caches, they have high performance and power overheads. We observe that in multimedia applications, not all data require {{the same amount of}} protection from soft errors. In fact, an error in the multimedia data itself does not result in a failure, but often just results in a slight loss of quality of service. Thus, it is possible to trade-off the power and performance overheads of soft error protection with quality of service. To this end, we propose a Partially Protected Cache (PPC) architecture, in which there are two caches, one protected and the other unprotected at the same level of memory hierarchy. We demonstrate that as compared to the existing unprotected cache architectures, PPC architectures can provide 47 × reduction in <b>failure</b> <b>rate,</b> at only 1 % runtime and 3 % power overheads. In addition, we <b>observe</b> that the <b>failure</b> <b>rate</b> reduction obtained by PPCs is very sensitive to th...|$|R
40|$|OBJETIVOS: determinar a ocorrência de falhas na triagem auditiva em escolares e comparar os resultados obtidos nas escolas da rede pública com os da particular. MÉTODO: participaram deste estudo de frequência, 90 escolares matriculados em uma escola da rede pública e outra da rede particular. Os alunos foram submetidos à meatoscopia e às emissões otoacusticas, verificando a influencia das variáveis sexo e rede de ensino aos resultados da triagem auditiva. RESULTADOS: foi constatado que 62, 2 % dos escolares passaram na triagem auditiva e 37, 8 % falharam, sendo observado maior índice de falha entre os alunos da escola pública. CONCLUSÃO: conclui-se que, na população estudada, a ocorrência de falha na triagem auditiva em escolares é 37, 8 %, sendo significantemente {{superior}} nos alunos da escola da rede pública quando comparados aos da particular. PURPOSES: {{to determine}} the occurrence of failures in the hearing screening in students and compare the results obtained in public schools with the particular ones. METHOD: 90 students enrolled in a public and private school took part in this study. The students were submitted to otoscopy and otoacoustic emissions, checking the influence of gender and education network as {{for the results of}} hearing screening. RESULTS: we observed that 62. 2 % of the students passed the hearing screening and 37. 8 % failed, and we <b>observed</b> higher <b>failure</b> <b>rate</b> among students from public schools. CONCLUSION: we may conclude that in the studied population, the occurrence of failed hearing screening in school is 37. 8 % and this was significantly higher in students from public schools...|$|R
40|$|Measurement of {{software}} reliability {{was carried out}} during the development of data base software for a multi-sensor tracking system. The failure ratio and <b>failure</b> <b>rate</b> {{were found to be}} consistent measures. Trend lines could be established from these measurements that provide good visualization of the progress on the job as a whole as well as on individual modules. Over one-half of the <b>observed</b> <b>failures</b> were due to factors associated with the individual run submission rather than with the code proper. Possible application of these findings for line management, project managers, functional management, and regulatory agencies is discussed. Steps for simplifying the measurement process and for use of these data in predicting operational software reliability are outlined...|$|R
40|$|AbstractFailure rate {{estimation}} of wires in electrical wire interconnect system (EWIS) is virtual to aircraft safety assessment {{for lack of}} large realistic data. Using some environment, we can estimate the relative <b>failure</b> <b>rate</b> through the help of experts’ experiment, and then get the real <b>failure</b> <b>rate</b> for wires. To analysis the effect of environment on EWIS <b>failure</b> <b>rate,</b> we compared the relative <b>failure</b> <b>rate</b> for different environment groups. The result of comparisons shows that non-realistic <b>failure</b> <b>rates</b> may improve the estimate conservative of relative EWIS <b>failure</b> <b>rate...</b>|$|R
5000|$|Passive <b>failure</b> <b>rate</b> factor: Factor {{by which}} the element <b>failure</b> <b>rate</b> is multiplied when {{operating}} in the passive state {{as opposed to the}} active state. By default this factor will be one and typically between zero and one, indicating a lower passive <b>failure</b> <b>rate</b> than active <b>failure</b> <b>rate.</b>|$|R
