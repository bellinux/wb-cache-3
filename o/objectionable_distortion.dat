4|5|Public
50|$|Bitmap fonts look best {{at their}} native pixel size. Some systems using bitmap fonts can create some font {{variants}} algorithmically. For example, the original Apple Macintosh computer could produce bold by widening vertical strokes and oblique by shearing the image. At non-native sizes, many text rendering systems perform nearest-neighbor resampling, introducing rough jagged edges. More advanced systems perform anti-aliasing on bitmap fonts whose size {{does not match}} the size that the application requests. This technique works well for making the font smaller but not as well for increasing the size, as it tends to blur the edges. Some graphics systems that use bitmap fonts, especially those of emulators, apply curve-sensitive nonlinear resampling algorithms such as 2xSaI or hq3x on fonts and other bitmaps, which avoids blurring the font while introducing little <b>objectionable</b> <b>distortion</b> at moderate increases in size.|$|E
40|$|The ideal binary mask (IBM) {{is widely}} {{considered}} to be the benchmark for time–frequency-based sound source separation techniques such as computational auditory scene analysis (CASA). However, {{it is well known that}} binary masking introduces <b>objectionable</b> <b>distortion,</b> especially musical noise. This can make binary masking unsuitable for sound source separation applications where the output is auditioned. It has been suggested that soft masking reduces musical noise and leads to a higher quality output. A previously defined soft mask, the ideal ratio mask (IRM), is found to have similar properties to the IBM, may correspond more closely to auditory processes, and offers additional computational advantages. Consequently, the IRM is proposed as the goal of CASA. To further support this position, a number of studies are reviewed that show soft masks to provide superior performance to the IBM in applications such as automatic speech recognition and speech intelligibility. A brief empirical study provides additional evidence demonstrating the objective and perceptual superiority of the IRM over the IBM...|$|E
40|$|The {{goal of this}} {{research}} is to improve the subjective quality of real world imagery encoded with spatial vector quantization (VQ). Improved subjective quality implies that a human perceives less visually <b>objectionable</b> <b>distortion</b> when looking at the coded images. Through study of several basic VQ schemes, the issues fundamental to achieving good subjective quality are uncovered and addressed in this work. Vector quantization is very good at reproducing quasi-uniform textures in an image, but has difficulty in reproducing abrupt changes in textures (edges) and fine detail and can cause a block effect which is subjectively annoying. A second generation coding scheme is developed which takes certain properties of the human visual system into account. A promising method which is developed utilizes omniscient finite state VQ, a new quadratic distortion measure which penalizes the misrepresentation of edges, and brightness compensation based on Steven's power law. The proposed subjective VQ is compared with several classical, first generation VQ methods...|$|E
40|$|It is {{well-known}} that antenna measurements are error prone {{with respect to}} reflections within an antenna measurements test facility. The influence on near-field (NF) measurements with subsequent NF to far-field (FF) transformation can be significantly reduced applying soft- or hard-gating techniques. Hard-gating systems are often used in compact range facilities employing fast PIN-diode switches (Hartmann, 2000) whereas soft-gating systems utilize a network analyzer to gather frequency samples and eliminate <b>objectionable</b> <b>distortions</b> in the time-domain by means of Fourier-transformation techniques. Near-field (NF) antenna measurements {{are known to be}} sensitive to various errors concerning the measurement setup as there have to be mentioned the accuracy of the positioner, the measurement instruments or the quality of the anechoic chamber itself. Two different approaches employing soft- and hard-gating techniques are discussed with respect to practical applications. Signal generation for the antenna under test (AUT) is implemented using a newly developed hard-gating system based on digital signal synthesis allowing gate-widths of 250 ps to 10 ns. Measurement results obtained from a Yagi-Uda antenna under test (AUT) and a dual polarized open-ended waveguide used as probe antenna are presented for the GSM 1800 frequency range...|$|R
40|$|Streaming media {{services}} delivered over IP {{networks are}} gaining popularity and momentum, and consumers show an {{increased interest in}} being able to play and enjoy their media wherever they are. The ever increasing deployment of high speed wired and wireless Internet access networks, and continuous development of more efficient compression schemes for audio and video, are two of many important factors that enable content and service providers to deliver higher quality IP-based multimedia services to end-users. The goal of any multimedia communication system obviously should be to maximize the end-user’s perceived quality of the delivered service. However, to measure the end-user perception of an audiovisual service quality, or similarly, to which extent they react <b>objectionable</b> to <b>distortions</b> introduced by compression an...|$|R
50|$|The 12AT7 high-mu dual triode was {{designed}} primarily for RF mixing applications {{where it was}} incorporated into the oscillator/mixer stage and used to heterodyne incoming RF signals with the local oscillator to create an intermediate frequency in TV and FM sets. Thus, it is intentionally designed to be extremely non-linear. Thus, circuits based around the 12AT7 exhibit a larger degree of non-linearity throughout the entire dynamic operating range, including the middle. As a result, it produces greater even-order harmonic <b>distortion</b> (less <b>objectionable</b> than odd-order <b>distortion).</b>|$|R
40|$|Three {{dimensional}} {{image and}} video communication systems offer significant benefits over conventional two dimensional systems. The presence of natural depth {{information in the}} scene affords the viewer an overall improved sense of reality and naturalness. A variety of systems attempting to reach this goal have been designed by many independent research groups, though the images displayed by these stereoscopic systems lack a sense of solidity and {{have been shown to}} cause viewer fatigue due to a physiological conflict resulting from supplying a different disparate planar image to each eye. A system developed at De Montfort University, integral three dimensional imaging, over-comes these limitations. It encodes a volume spatial model of an object scene on a plane sur-face, and is capable of real-time capture and display. The resolution requirements, however, are significantly greater than those of a conventional two dimensional display of a similar size. Hence for transmission via existing broadcast methods a reduction of the bandwidth required is necessary. The objective of this thesis is the development of compression systems optimised for use with the intensity distributions recorded by the integral 3 D imaging system. Such a com-pression system must be designed such that the transmission bandwidth consumed by the data stream resulting from compression of the captured integral 3 D image data is no greater than that available within a conventional television broadcast channel, while ensuring that the reconstructed decoded image contains no <b>objectionable</b> <b>distortion</b> noticeable to the viewer. The thesis also develops procedures for capture, pre-processing, post-processing and display for evaluation of the compression schemes. i ACKNOWLEDGEMENT...|$|E
40|$|The {{development}} of algorithms for synthesizing audio and speech {{has been a}} topic of interest for several decades now. Delivering high quality synthesis with low complexity remains a challenge in application settings requiring the flexibility to modify and enhance the input signal. The work undertaken in this thesis has resulted in an improved model for speech and audio synthesis that accommodates pitch-scale, time-scale, and vocal track modifications without introducing <b>objectionable</b> <b>distortions.</b> ^ While the algorithm advancements introduced in this thesis have application in many areas, the primary motivation for this work is language learning. Learning to speak a foreign language without an accent {{can be a very}} challenging task for many non-native speakers. Many users have turned to the use of Computer Assisted Language Learning (CALL) in their quest to improve pronunciation. Although the effectiveness of feedback provided by most CALL tools is still in debate, {{a number of studies have}} used speech modification algorithms to provide users with either their own corrected utterance or some form of modified model utterance as feedback. Reports indicate that significant improvement in pronunciation can be obtained. The speech modification techniques used in most of these studies have been Pitch Synchronous Overlap Add (PSOLA) methods, because of their simplicity. However, these methods result in degraded speech quality when large modification factors are involved. ^ Sinusoidal model based methods have been shown to have better speech quality than PSOLA under various speech modification conditions. The goal of this research work is to provide improved speech modification algorithm capability and quality, to improve computer assisted language learning. The approach taken in this work is based on Analysis-by-Synthesis Overlap Add (ABS/OLA). The ABS/OLA model has a fairly computationally intensive analysis stage which is not favorable for real-time implementation. And, the output synthetic speech can exhibit tonal distortions under various modification conditions. In this work, a new multi-component analysis approach has been designed to reduce the computational complexity of the analysis stage and significant improvements have been made to the synthesis algorithm to mitigate distortions in the presence of parameter modification for high quality computer assisted language learning. ...|$|R
40|$|Parallax {{handling}} is {{a challenging}} task for image stitch-ing. This paper presents a local stitching method to handle parallax {{based on the}} observation that input images {{do not need to}} be perfectly aligned over the whole overlapping re-gion for stitching. Instead, they only need to be aligned in a way that there exists a local region where they can be seam-lessly blended together. We adopt a hybrid alignment model that combines homography and content-preserving warp-ing to provide flexibility for handling parallax and avoiding <b>objectionable</b> local <b>distortion.</b> We then develop an efficient randomized algorithm to search for a homography, which, combined with content-preserving warping, allows for op-timal stitching. We predict how well a homography enables plausible stitching by finding a plausible seam and using the seam cost as the quality metric. We develop a seam finding method that estimates a plausible seam from only roughly aligned images by considering both geometric alignment and image content. We then pre-align input images using the optimal homography and further use content-preserving warping to locally refine the alignment. We finally compose aligned images together using a standard seam-cutting al-gorithm and a multi-band blending algorithm. Our exper-iments show that our method can effectively stitch images with large parallax that are difficult for existing methods. 1...|$|R

