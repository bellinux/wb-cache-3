10000|17|Public
5|$|In some situations, profit-maximizing {{prices are}} not an <b>optimal</b> strategy. For example, where scale economies are large (as they often are), {{capturing}} market share {{may be the key}} to long-term dominance of a market, so maximizing revenue or profit may not be the <b>optimal</b> strategy.|$|E
25|$|A posteriori methods aim at {{producing}} all the Pareto <b>optimal</b> solutions or {{a representative}} {{subset of the}} Pareto <b>optimal</b> solutions. Most a posteriori methods fall into either {{one of the following}} two classes: mathematical programming-based a posteriori methods, where an algorithm is repeated and each run of the algorithm produces one Pareto <b>optimal</b> solution, and evolutionary algorithms where one run of the algorithm produces a set of Pareto <b>optimal</b> solutions.|$|E
25|$|The <b>optimal</b> {{solution}} to the flux-balance problem is rarely unique with many possible, and equally <b>optimal,</b> solutions existing. Flux variability analysis (FVA), built into some analysis software, returns the boundaries for the fluxes through each reaction that can, paired with {{the right combination of}} other fluxes, estimate the <b>optimal</b> solution.|$|E
25|$|Another example where sub-Nyquist {{sampling}} is <b>optimal</b> arises {{under the}} additional constraint that the samples are quantized in an <b>optimal</b> manner, as in a combined system of sampling and <b>optimal</b> lossy compression. This setting is relevant {{in cases where}} the joint effect of sampling and quantization is to be considered, and can provide a lower bound for the minimal reconstruction error that can be attained in sampling and quantizing a random signal. For stationary Gaussian random signals, this lower bound is usually attained at a sub-Nyquist sampling rate, indicating that sub-Nyquist sampling is <b>optimal</b> for this signal model under <b>optimal</b> quantization.|$|E
25|$|In {{optimization}} problems, heuristic algorithms {{can be used}} to find {{a solution}} close to the <b>optimal</b> solution in cases where finding the <b>optimal</b> solution is impractical. These algorithms work by getting {{closer and closer to the}} <b>optimal</b> solution as they progress. In principle, if run for an infinite amount of time, they will find the <b>optimal</b> solution. Their merit is that they can find a solution very close to the <b>optimal</b> solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.|$|E
25|$|BMI Prime, a {{modification}} of the BMI system, is the ratio of actual BMI to upper limit <b>optimal</b> BMI (currently defined at 25kg/m2), i.e., the actual BMI expressed {{as a proportion of}} upper limit <b>optimal.</b> The ratio of actual body weight to body weight for upper limit <b>optimal</b> BMI (25kg/m2) is equal to BMI Prime. BMI Prime is a dimensionless number independent of units. Individuals with BMI Prime less than 0.74 are underweight; those with between 0.74 and 1.00 have <b>optimal</b> weight; and those at 1.00 or greater are overweight. BMI Prime is useful clinically because it shows by what ratio (e.g. 1.36) or percentage (e.g. 136%, or 36% above) a person deviates from the maximum <b>optimal</b> BMI.|$|E
25|$|In 2016, Maryna Viazovska {{announced}} a {{proof that the}} E8 lattice provides the <b>optimal</b> packing (regardless of regularity) in eight-dimensional space, and soon afterwards she {{and a group of}} collaborators {{announced a}} similar proof that the Leech lattice is <b>optimal</b> in 24 dimensions. This result built on and improved previous methods which showed that these two lattices are very close to <b>optimal.</b>|$|E
25|$|The {{vertices}} of any graph may {{always be}} ordered {{in such a}} way that the greedy algorithm produces an <b>optimal</b> coloring. For, given any <b>optimal</b> coloring in which the smallest color set is maximal, the second color set is maximal with respect to the first color set, etc., one may order the vertices by their colors. Then when one uses a greedy algorithm with this order, the resulting coloring is automatically <b>optimal.</b> More strongly, perfectly orderable graphs (which include chordal graphs, comparability graphs, and distance-hereditary graphs) have an ordering that is <b>optimal</b> both for the graph itself and for all of its induced subgraphs. However, finding an <b>optimal</b> ordering for an arbitrary graph is NP-hard (because it could be used to solve the NP-complete graph coloring problem), and recognizing perfectly orderable graphs is also NP-complete. For this reason, heuristics have been used that attempt to reduce the number of colors while not guaranteeing an <b>optimal</b> number of colors.|$|E
25|$|The IHS (Increasing Height Shelf) {{algorithm}} is <b>optimal</b> for 2D knapsack (packing squares into a two-dimensional unit size square): {{when there are}} at most five square in an <b>optimal</b> packing.|$|E
25|$|Complex {{networks}} have heterogeneous topology. To {{the extent that}} the <b>optimal</b> measure depends on the network structure of the most important vertices, a measure which is <b>optimal</b> for such vertices is sub-optimal {{for the remainder of the}} network.|$|E
25|$|Dynamic {{programming}} algorithms {{are often}} used for optimization. A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. In comparison, a greedy algorithm treats the solution as some sequence of steps and picks the locally <b>optimal</b> choice at each step. Using a greedy algorithm does not guarantee an <b>optimal</b> solution, because picking locally <b>optimal</b> choices {{may result in a}} bad global solution, but it is often faster to calculate. Some greedy algorithms (such as Kruskal's or Prim's for minimum spanning trees) are however proven to lead to the <b>optimal</b> solution.|$|E
25|$|Another {{motivation}} for using local alignments {{is that there}} is a reliable statistical model (developed by Karlin and Altschul) for <b>optimal</b> local alignments. The alignment of unrelated sequences tends to produce <b>optimal</b> local alignment scores which follow an extreme value distribution. This property allows programs to produce an expectation value for the <b>optimal</b> local alignment of two sequences, which is a measure of how often two unrelated sequences would produce an <b>optimal</b> local alignment whose score is greater than or equal to the observed score. Very low expectation values indicate that the two sequences in question might be homologous, meaning they might share a common ancestor.|$|E
25|$|Sphere packing {{in higher}} dimensions: In 2016, Maryna Viazovska {{announced}} proofs of the <b>optimal</b> sphere packings in dimensions 8 and 24. However, the <b>optimal</b> sphere packing question in dimensions other than 1, 2, 3, 8, and 24 is still open.|$|E
25|$|While the {{admissibility}} criterion guarantees an <b>optimal</b> solution path, it {{also means}} that A* must examine all equally meritorious paths to find the <b>optimal</b> path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the <b>optimal</b> solution path. This new guarantee is referred to as ε-admissible.|$|E
25|$|Claimed {{globally}} <b>optimal</b> I/O sorting and aggregation.|$|E
25|$|As {{for most}} NP-complete problems, {{it may be}} enough to find {{workable}} solutions {{even if they are}} not <b>optimal.</b> Preferably, however, the approximation comes with a guarantee on the difference between the value of the solution found and the value of the <b>optimal</b> solution.|$|E
25|$|It {{turns out}} that the minimal entropy of a closed surface can be related to its <b>optimal</b> {{systolic}} ratio. Namely, there is an upper bound for the entropy of a systolically extremal surface, in terms of its systole. By combining this upper bound with Katok's <b>optimal</b> lower bound in terms of the volume, one obtains a simpler alternative proof of Gromov's asymptotic estimate for the <b>optimal</b> systolic ratio of surfaces of large genus. Furthermore, such an approach yields an improved multiplicative constant in Gromov's theorem.|$|E
25|$|Find the <b>optimal</b> grammar {{parse tree}} (CYK algorithm).|$|E
25|$|While Lévy {{defines the}} notion of <b>optimal</b> sharing, he does not provide an {{algorithm}} to do it. In Vincent van Oostrom, Kees-Jan van de Looij, and Marijn Zwitserlood's paper , they provide such an algorithm by transforming lambda terms into interaction nets, which are then reduced. Roughly speaking, the resulting reduction is <b>optimal</b> because every term that {{would have the same}} labels as per Lévy's paper would also be the same graph in the interaction net. In the paper, they mention that their prototype implementation of Lambdascope performs as well as the optimised version of the reference <b>optimal</b> higher order machine BOHM.|$|E
25|$|The {{leaves of}} the tree contain the <b>optimal</b> alignment.|$|E
25|$|The {{other main}} {{environmental}} factors affecting cellulose production are pH, temperature, and dissolved oxygen. According to experimental studies, the <b>optimal</b> temperature for maximum production was between 28 and 30°C. For most species, the <b>optimal</b> pH was between 4.0-6.0. Controlling pH {{is especially important}} in static cultures as the accumulation of gluconic, acetic, or lactic acid decreases the pH far lower than the <b>optimal</b> range. Dissolved oxygen content can be varied with stirrer speed as it is needed for static cultures where substrates need to be transported by diffusion.|$|E
25|$|The result {{need not}} be an <b>optimal</b> {{solution}} to the problem.|$|E
25|$|The set C {{constructed}} {{this way}} is a vertex cover: suppose that an edge e is not covered by C; then M∪{e} is a matching and e∉M, which is a contradiction {{with the assumption that}} M is maximal. Furthermore, if e={u,v} ∈ M, then any vertex cover – including an <b>optimal</b> vertex cover – must contain u or v (or both); otherwise the edge e is not covered. That is, an <b>optimal</b> cover contains at least one endpoint of each edge in M; in total, the set C is at most 2 times as large as the <b>optimal</b> vertex cover.|$|E
25|$|Stiglitz made early {{contributions}} to {{a theory of}} public finance stating that an <b>optimal</b> supply of local public goods can be funded entirely through capture of the land rents generated by those goods (when population distributions are <b>optimal).</b> Stiglitz dubbed this the 'Henry George theorem' {{in reference to the}} radical classical economist Henry George who famously advocated for land value tax. The explanation behind Stiglitz's finding is that rivalry for public goods takes place geographically, so competition for access to any beneficial public good will increase land values by {{at least as much as}} its outlay cost. Furthermore, Stiglitz shows that a single tax on rents is necessary to provide the <b>optimal</b> supply of local public investment. Stiglitz also shows how the theorem could be used to find the <b>optimal</b> size of a city or firm.|$|E
25|$|Generally, {{sequence}} alignment means {{constructing a}} string from {{two or more}} given strings with the greatest similarity by adding, deleting letters, or adding a space for each string. The multiple sequence alignment problem is generally based on pairwise sequence alignment and currently, for pairwise sequence alignment problem, biologists can use dynamic programming approach to obtain its <b>optimal</b> solution. However, the multiple sequence alignment problem {{is still one of}} the intractable problems in bioinformatics, because finding the <b>optimal</b> solution of multiple sequence alignment has been proved as a NP-complete problem so that only approximate <b>optimal</b> solution can be obtained.|$|E
25|$|<b>Optimal</b> (health) range or {{therapeutic}} target (not to {{be confused}} with biological target) is a reference range or limit that is based on concentrations or levels that are associated with <b>optimal</b> health or minimal risk of related complications and diseases, rather than the standard range based on normal distribution in the population.|$|E
25|$|<b>Optimal</b> {{substructure}} {{means that}} the solution to a given optimization problem {{can be obtained by}} the combination of <b>optimal</b> solutions to its sub-problems. Such <b>optimal</b> substructures are usually described by means of recursion. For example, given a graph G=(V,E), the shortest path p from a vertex u to a vertex v exhibits <b>optimal</b> substructure: take any intermediate vertex w on this shortest path p. If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in Introduction to Algorithms). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman–Ford algorithm or the Floyd–Warshall algorithm does.|$|E
25|$|Find the <b>optimal</b> {{alignment}} {{between a}} sequence and the PCFG.|$|E
500|$|For example, {{problems}} of linear optimization are separable. Given a separable problem with an <b>optimal</b> solution, we fix an <b>optimal</b> solution ...|$|E
500|$|An <b>optimal</b> {{basket of}} goods occurs where the budget-line {{supports}} a consumer's preference set, {{as shown in}} the diagram. This means that an <b>optimal</b> basket is on the highest possible indifference curve given the budget-line, which is defined in terms of a price vector and the consumer's income (endowment vector). Thus, the set of <b>optimal</b> baskets {{is a function of the}} [...] prices, and this function is called the consumer's demand. If the preference set is convex, then at every price the consumer's demand is a convex set, for example, a unique <b>optimal</b> basket or a line-segment of baskets.|$|E
500|$|The <b>optimal</b> [...] "threading" [...] of {{a protein}} {{sequence}} onto a known {{structure and the}} production of an <b>optimal</b> multiple sequence alignment {{have been shown to be}} NP-complete. However, this does not imply that the structural alignment problem is NP-complete. Strictly speaking, an <b>optimal</b> solution to the protein structure alignment problem is only known for certain protein structure similarity measures, such as the measures used in protein structure prediction experiments, GDT_TS and MaxSub. These measures can be rigorously optimized using an algorithm capable of maximizing the number of atoms in two proteins that can be superimposed under a predefined distance cutoff. Unfortunately, the algorithm for <b>optimal</b> solution is not practical, since its running time depends not only on the lengths but also on the intrinsic geometry of input proteins.|$|E
500|$|Economic {{dynamics}} {{allows for}} changes in economic variables over time, including in dynamic systems. [...] The problem of finding <b>optimal</b> functions for such changes is studied in variational calculus and in <b>optimal</b> control theory. [...] Before the Second World War, Frank Ramsey and Harold Hotelling used the calculus of variations to that end.|$|E
2500|$|Suppose the {{feasible}} set [...] of the SAA {{problem is}} fixed, i.e., it {{is independent of}} the sample. Let [...] and [...] be the <b>optimal</b> value and the set of <b>optimal</b> solutions, respectively, of the true problem and let [...] and [...] be the <b>optimal</b> value and the set of <b>optimal</b> solutions, respectively, of the SAA problem.|$|E
2500|$|Given the {{hypotheses}} about the currency and the constraints, the <b>optimal</b> decision rule is the model's prediction {{of what the}} animal's best foraging strategy should be. Possible examples of <b>optimal</b> decision rules could be the <b>optimal</b> number of food items that an animal should carry back to its nesting site or the <b>optimal</b> size of a food item that an animal should feed on. [...] Figure 1, shows {{an example of how}} an <b>optimal</b> decision rule could be determined from a graphical model. The curve represents the energy gain per cost (E) for adopting foraging strategy x. [...] Energy gain per cost is the currency being optimized. [...] The constraints of the system determine the shape of this curve. The <b>optimal</b> decision rule (x*) is the strategy for which the currency, energy gain per costs, is the greatest. [...] <b>Optimal</b> foraging models can look very different and become very complex, depending {{on the nature of the}} currency and the number of constraints considered. [...] However, the general principles of currency, constraints, and <b>optimal</b> decision rule remain the same for all models.|$|E
2500|$|Scalarizing a multi-objective {{optimization}} problem is an a priori method, which means formulating a single-objective {{optimization problem}} such that <b>optimal</b> {{solutions to the}} single-objective optimization problem are Pareto <b>optimal</b> solutions to the multi-objective optimization problem. In addition, it is often required that every Pareto <b>optimal</b> solution can be reached with some parameters of the scalarization. With different parameters for the scalarization, different Pareto <b>optimal</b> solutions are produced. A general formulation for a scalarization of a multiobjective optimization is thus ...|$|E
2500|$|... where [...] denotes Hermitian {{transpose}} and [...] is {{the ratio}} between transmit power and noise power (i.e., transmit SNR). The <b>optimal</b> signal covariance [...] is achieved through singular value decomposition {{of the channel}} matrix [...] and an <b>optimal</b> diagonal power allocation matrix [...] The <b>optimal</b> power allocation is achieved through waterfilling, that is ...|$|E
