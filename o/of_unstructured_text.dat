155|4988|Public
25|$|A {{different}} {{method for}} determining sentiment {{is the use}} of a scaling system whereby words commonly associated with having a negative, neutral, or positive sentiment with them are given an associated number on a âˆ’10 to +10 scale (most negative up to most positive) or simply from 0 to a positive upper limit such as +4. This makes it possible to adjust the sentiment of a given term relative to its environment (usually on the level of the sentence). When a piece <b>of</b> <b>unstructured</b> <b>text</b> is analyzed using natural language processing, each concept in the specified environment is given a score based on the way sentiment words relate to the concept and its associated score. This allows movement to a more sophisticated understanding of sentiment, because it is now possible to adjust the sentiment value of a concept relative to modifications that may surround it. Words, for example, that intensify, relax or negate the sentiment expressed by the concept can affect its score. Alternatively, texts can be given a positive and negative sentiment strength score if the goal is to determine the sentiment in a text rather than the overall polarity and strength of the text.|$|E
50|$|BOBCAT is an {{analytical}} tool that provides guided prediction through large amounts <b>of</b> <b>unstructured</b> <b>text.</b> It has been featured at the United States Joint Forces Command Empire Challenge.|$|E
50|$|The {{flagship}} {{offering of}} Abzooba {{is in the}} field of Natural Language Processing and Text analytics for analysis of large amounts <b>of</b> <b>unstructured</b> <b>text</b> in social networking channels like Facebook, Twitter and other social review domains.|$|E
40|$|Abstract. Automated Assessment {{has a long}} {{tradition}} in technology-enhanced learning, however, in the recent years with a refreshing interest in evaluating free texts with respect to their content, i. e. the semantic of the text. While. LRN does not yet provide an automated essay scoring module, there is a plethora of (commercial) tools available in the market, many of them based on latent semantic analysis. With the essay scoring application (ESA), we present a research prototype for automatically evaluating the content <b>of</b> <b>unstructured</b> <b>texts</b> which invites interested prac-titioners and scientists to experiment with it...|$|R
40|$|This paper {{describes}} {{an application of}} the optimized pattern discovery framework to text and Web mining. In particular, weintroduce a class of simple combinatorial patterns over phrases, called proximity phrase association patterns, and consider the problem of #nding the patterns that optimizes a given statistical measure in a large collection <b>of</b> <b>unstructured</b> <b>texts.</b> For this class of patterns, we develop fast and robust text mining algorithms based on techniques from computational geometry and string matching. Then, we made experiments on large collections of documents and on Web pages to evaluate the proposed method. 1 Introduction The rapid progress of computer and network technologies {{makes it easy to}} collect and store a large amount <b>of</b> <b>unstructured</b> or semi-structured <b>texts</b> suchaswebpages, HTML#XML archives, emails, and text #les. These text data can be thought of large scale text databases, and thus it becomes important to develop an e#cient tools to discover interesting kn [...] ...|$|R
40|$|This paper {{describes}} {{applications of}} the optimized pattern discovery framework to text and Web mining. In particular, we introduce a class of simple combinatorial patterns over phrases, called proximity phrase association patterns, and consider the problem of finding the patterns that optimize a given statistical measure within the whole class of patterns in a large collection <b>of</b> <b>unstructured</b> <b>texts.</b> For this class of patterns, we develop fast and robust text mining algorithms based on techniques in computational geometry and string matching. Finally, we successfully apply the developed text mining algorithms to the experiments on interactive document browsing in a large text database and keyword discovery from Web bases. 1. Introduction The rapid progress of computer and network technologies {{makes it easy to}} collect and store a large amount <b>of</b> <b>unstructured</b> or semi-structured <b>texts</b> such as webpages, HTML/XML archives, E-mails, and text files. These text data can be thought of large sc [...] ...|$|R
50|$|The Rosette Linguistics Platform {{consists}} of a component library for multilingual text retrieval and analysis. Rosette provides automatic language identification, linguistic analysis, entity extraction, and entity translation from unstructured text. It can be integrated into applications to help analyse volumes <b>of</b> <b>unstructured</b> <b>text.</b>|$|E
50|$|Dynamic {{clustering}} {{based on}} the conceptual content of documents can also be accomplished using LSI. Clustering {{is a way to}} group documents {{based on the}}ir conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster. This is very useful when dealing with an unknown collection <b>of</b> <b>unstructured</b> <b>text.</b>|$|E
50|$|LSI is {{increasingly}} {{being used for}} electronic document discovery (eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections <b>of</b> <b>unstructured</b> <b>text</b> on a conceptual basis is essential. Concept-based searching using LSI {{has been applied to}} the eDiscovery process by leading providers as early as 2003.|$|E
40|$|Today user {{performs}} most of {{his work}} with electronic document. Due to huge volumes <b>of</b> <b>unstructured</b> electronic <b>texts</b> available, it requires automated techniques to analyze and extract knowledge from these repositories <b>of</b> information. This <b>unstructured</b> <b>text</b> can {{be available in the}} form of emails, normal text document and HTML files etc. Understanding meanings and semantics of these texts is a complicated problem. This problem becomes more vital, when meanings with respect to context, have to be extracted. Text mining, also known as Intelligent Text Analysis, extract interesting information and knowledge from <b>unstructured</b> <b>text.</b> The agent for Context Based Sense Extraction in Text formulates the standard Natural language processing rules with certain weights. These weights for each rule ultimately support in deciding the particular meaning of a word and sentence. In this paper architecture and design of Context Based Word Sense Extraction have been presented...|$|R
50|$|Document {{retrieval}} {{is defined}} as the matching of some stated user query against a set of free-text records. These records could be any type <b>of</b> mainly <b>unstructured</b> <b>text,</b> such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.|$|R
40|$|The Semantic Web {{requires}} {{us to be able}} to integrate information from a variety <b>of</b> sources, including <b>unstructured</b> <b>text</b> from web pages, semi-structured XML data, structured databases, and metadata sources suchasontologies. Integration of heterogeneous data sources is a problem that has been addressed by severa...|$|R
5000|$|... eDiscovery - Concept-based search {{technologies}} {{are increasingly being}} used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation. In eDiscovery, the ability to cluster, categorize, and search large collections <b>of</b> <b>unstructured</b> <b>text</b> on a conceptual basis is much more efficient than traditional linear review techniques. Concept-based searching is becoming accepted as a reliable and efficient search method that {{is more likely to}} produce relevant results than keyword or Boolean searches.|$|E
5000|$|Peyman is {{currently}} the founder of RobustLinks, funded by the National Science Foundation. The mission of RobustLinks is to use {{state of the art}} AI, NLP, and ML to compose Knowledge as a Service (KaaS). KaaS takes unstructured (textual) [...] "big data" [...] and reduces it to searchable knowledge. It integrates end-to-end technology stack that continuously gather, summarize and represent the semantics <b>of</b> <b>unstructured</b> <b>text,</b> in real-time and at scale. The derived semantics is then analyzed to condense the big data to a compact and searchable semantic graph.|$|E
5000|$|Enterprise Search and Enterprise Content Management (ECM) - Concept search {{technologies}} are being {{widely used in}} enterprise search. As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections <b>of</b> <b>unstructured</b> <b>text</b> on a conceptual basis has become essential. In 2004 the Gartner Group estimated that professionals spend 30 {{percent of their time}} searching, retrieving, and managing information. [...] The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.|$|E
5000|$|..[...] {{a set of}} {{research}} technologies that collect, store and analyze massive amounts <b>of</b> <b>unstructured</b> and semi-structured <b>text.</b> It is built on an open, extensible platform that enables the discovery of trends, patterns and relationships from data.|$|R
40|$|Formal Concept Analysis (FCA) is a {{mathematical}} technique {{that has been}} extensively applied to Boolean data in knowledge discovery, information retrieval, web mining, etc. applications. During the past years, the research on extending FCA theory to cope with imprecise and incomplete information made significant progress. In this paper, we give a systematic overview {{of the more than}} 120 papers published between 2003 and 2011 on FCA with fuzzy attributes and rough FCA. We applied traditional FCA as a text-mining instrument to 1072 papers mentioning FCA in the abstract. These papers were formatted in pdf files and using a thesaurus with terms referring to research topics, we transformed them into concept lattices. These lattices were used to analyze and explore the most prominent research topics within the FCA with fuzzy attributes and rough FCA research communities. FCA {{turned out to be an}} ideal metatechnique for representing large volumes <b>of</b> <b>unstructured</b> <b>texts...</b>|$|R
40|$|Commercial, {{non-profit}} {{and public}} organizations are accumulating {{huge amounts of}} electronically available text documents. Although composed <b>of</b> <b>unstructured</b> <b>texts,</b> documents contained in archives such as annual reports to shareholders, medical patient records and public announcements often share an inherent, though undocumented structure. In order to enable information integration of text collections with related structured data sources, this inherent structure should be made explicit as detailed as possible. The goal {{of this study is}} the establishment of a methodology for the integration of text documents with structured records into a hyper-archive of application-specific entities. The text documents are of implicit structure which has been explicated by data mining techniques as proposed in the DIAsDEM framework for semantic tagging of domain-specific text documents. The result is a probabilistic DTD that serves as a basis for the matching of schemata and for the matching of data instances. ...|$|R
50|$|Topic {{models are}} also {{referred}} to as probabilistic topic models, which refers to statistic algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections <b>of</b> <b>unstructured</b> <b>text</b> bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.|$|E
50|$|Handcrafted {{controlled}} vocabularies {{contribute to}} the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized. Controlled vocabularies require extensive human input and oversight {{to keep up with}} the rapid evolution of language. They also are not well suited to the growing volumes <b>of</b> <b>unstructured</b> <b>text</b> covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced. Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.|$|E
50|$|The {{earliest}} {{research into}} business intelligence focused in on unstructured textual data, rather than numerical data. As early as 1958, computer science researchers like H.P. Luhn were particularly {{concerned with the}} extraction and classification <b>of</b> <b>unstructured</b> <b>text.</b> However, only {{since the turn of}} the century has the technology caught up with the research interest. In 2004, the SAS Institute developed the SAS Text Miner, which uses Singular Value Decomposition (SVD) to reduce a hyper-dimensional textual space into smaller dimensions for significantly more efficient machine-analysis. The mathematical and technological advances sparked by machine textual analysis prompted a number of business to research applications, leading to the development of fields like sentiment analysis, voice of the customer mining, and call center optimization. The emergence of Big Data in the late 2000s led to a heightened interest in the applications of unstructured data analytics in contemporary fields such as predictive analytics and root cause analysis.|$|E
40|$|This paper {{considers}} {{the problem of}} finding all frequent phrase association patterns in a large collection <b>of</b> <b>unstructured</b> <b>texts,</b> where a phrase association pattern {{is a set of}} consecutive sequences of arbitrary number of keywords which appear together in a document. For the ordered and the unordered versions of phrase association patterns, we present efficient algorithms, called Levelwise-Scan, based on the sequential counting technique of Apriori algorithm. To cope with the problem of the huge feature space of phrase association patterns, the algorithm uses the generalized suffix tree and the pattern matching automaton. By theoretical and empirical analyses, we show that the algorithms runs quickly on most random texts {{for a wide range of}} parameter values and scales up for large disk-resident text databases. 1 Introduction Background. Recent progress of network and storage technologies have been rapidly increasing the size and the species of text databases such as webp [...] ...|$|R
40|$|We {{present a}} system for Emotion Analysis of Instant Messages (IM). Using Instance Based {{classifier}} we have shown that our system can outperform similar systems in the IM domain. Tagged instant messages and elaborate feature engineering can help a lot in increasing the performance <b>of</b> <b>text</b> classification <b>of</b> <b>unstructured,</b> ungrammatical <b>text.</b> The impact of class imbalance on classification has been studied and demonstration {{has been made of}} how undersampling can help mitigate this problem. ...|$|R
40|$|Proceedings of: The International Conference on Knowledge Discovery and Information Retrieval, October, 2009 (KDIR 2009). First International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC 3 K 2009), Funchal (Madeira, Portugal) The anonymization <b>of</b> <b>unstructured</b> <b>texts</b> is {{nowadays}} a task {{of great}} importance in several text mining applications. Medical records anonymization is needed both to preserve personal health information privacy and enable further data mining efforts. The described ANONYMITEXT system is designed to de identify sensible data from unstructured documents. It {{has been applied to}} Spanish clinical notes to recognize sensible concepts that would need to be removed if notes are used beyond their original scope. The system combines several medical knowledge resources with semantic clinical notes induced dictionaries. An evaluation of the semi automatic process has been carried on a subset of the clinical notes on the most frequent attributes. This work has been partially supported by MAVIR (S 0505 /TIC 0267) and by the TIN 2007 67407 C 03 01 project BRAVO...|$|R
50|$|A {{different}} {{method for}} determining sentiment {{is the use}} of a scaling system whereby words commonly associated with having a negative, neutral, or positive sentiment with them are given an associated number on a âˆ’10 to +10 scale (most negative up to most positive) or simply from 0 to a positive upper limit such as +4. This makes it possible to adjust the sentiment of a given term relative to its environment (usually on the level of the sentence). When a piece <b>of</b> <b>unstructured</b> <b>text</b> is analyzed using natural language processing, each concept in the specified environment is given a score based on the way sentiment words relate to the concept and its associated score. This allows movement to a more sophisticated understanding of sentiment, because it is now possible to adjust the sentiment value of a concept relative to modifications that may surround it. Words, for example, that intensify, relax or negate the sentiment expressed by the concept can affect its score. Alternatively, texts can be given a positive and negative sentiment strength score if the goal is to determine the sentiment in a text rather than the overall polarity and strength of the text.|$|E
40|$|We {{present an}} {{in-depth}} analysis of Curriculum Vitae documents consisting <b>of</b> <b>unstructured</b> <b>text.</b> We present {{a collection of}} Curriculum Vitae Topics with description. We introduce an ontology that gives a formal description of the domain Curriculum Vitae. We presents an analysis that compare {{the performance of the}} two PDF extractor algorithms, TIKA and PDFExtract, respectively. We presents an in-depth analysis of Curriculum Vitae documents consisting <b>of</b> <b>unstructured</b> <b>text.</b> We introduce a topic boundary detection algorithm that detects topic boundaries in Curriculum Vitae documents consisting <b>of</b> <b>unstructured</b> <b>text...</b>|$|E
40|$|Automated {{analysis}} <b>of</b> <b>unstructured</b> <b>text</b> documents (e. g., web pages, newswire articles, research publications, business reports) {{is a key}} {{capability for}} solving important problems in areas including decision making, risk assessment, social net-work analysis, intelligence analysis, scholarly research and others. However, as data sizes continue to grow in these ar-eas, scalable processing, modeling, and semantic analysis of text collections becomes essential. In this paper, we present the ParaText text analysis engine, a distributed memory software framework for processing, modeling, and analyzing collections <b>of</b> <b>unstructured</b> <b>text</b> documents. Results on sev-eral document collections using hundreds of processors are presented to illustrate the flexibility, extensibility, and scal-ability of the the entire process of text modeling from raw data ingestion to application analysis...|$|E
40|$|We {{consider}} a data mining {{problem in a}} large collection <b>of</b> <b>unstructured</b> <b>texts</b> based on association rules over subwords of texts. A two-words association pattern is an expression such as (TATA, 30, AGGAGGT)) C that expresses a rule that if a text contains a subword TATA followed by another subword AGGAGGT with distance no more than 30 letters then a property C will hold with high probability. The optimized confidence pattern problem is to compute frequent patterns (ff; k; fi) that optimize the confidence {{with respect to a}} given collection of texts. Although this problem is solved in polynomial time by a straightforward algorithm that enumerates all the possible patterns in time O(n 5), we focus on the development of more efficient algorithms that can be applied to large text databases. We present an algorithm that solves the optimized confidence pattern problem in time O(maxfk; mgn 2) and space O(kn), where m and n are the number and the total length of classification example [...] ...|$|R
40|$|We study a {{data mining}} {{problem in a}} large {{collection}} <b>of</b> <b>unstructured</b> <b>texts</b> based on association rules over subwords of texts. A two-word association rule is an expression such as (TATA, 30, AGGAGGT) &rArr; C that expresses a rule that if a text contains a subword &alpha; followed by another subword &beta; with distance no more than k then a condition C will holds with a probability. We present an efficient algorithm for computing frequent patterns that optimizes the confidence {{with respect to a}} given collection of texts. The algorithm runs in time O(mn&sup 2; log&sup 2; n) and in space O(kmn log n), where m and n are the number and the total length of classification examples, respectively, and k is a small constant around 30 &sim; 50. The algorithm employs the suffix tree data structure from string pattern matching and the orthogonal range query techniques from computational geometry. We also give a faster version that runs in time O(mn&sup 2;) and in space O(kmn) ...|$|R
40|$|The huge volumes <b>of</b> <b>unstructured</b> <b>texts</b> {{available}} online drives the increasing need for automated techniques {{to analyze and}} extract knowledge from these repositories of information. Resolving the ambiguity in these texts {{is an important step}} for any following analysis tasks. In this paper, we present a new method for one type of ambiguity resolving [...] term disambiguation. The method is based on machine learning and {{can be viewed as a}} context-based classification approach. In our experiments we apply it to gene and protein name disambiguation. We have extensively evaluated our method using around 600, 000 Medline abstracts and three different classifiers. The results show that our technique is effective in achieving impressive accuracy, precision, and recall rates, and outperforms the recently published results on this problem. The paper includes the details of the method and the experimental design. We plan to apply our technique to the general domain of word sense disambiguation in the future. 1...|$|R
40|$|Sentiment {{analysis}} {{is a technique}} to quantify how users feel through the direct analysis <b>of</b> <b>unstructured</b> <b>text</b> data like restaurant reviews or open-ended survey responses. The application of this powerful computational technique to the fields of user research, marketing analytics, and qualitative social science research will be presented...|$|E
40|$|This thesis {{describes}} selected algorithms {{and techniques}} used in processing <b>of</b> <b>unstructured</b> <b>text</b> and a particular application {{in the implementation}} of a computer system offering search on publicly available real estate advertising. The thesis covers whole process of the knowledge discovery: obtaining and pre-processing the data, application of the algorithms and evaluation and presentation of results...|$|E
40|$|The {{increasing}} {{availability of}} digitized text presents enormous opportunities for social scientists. Yet hand coding many blogs, speeches, government records, newspapers, or other sources <b>of</b> <b>unstructured</b> <b>text</b> is infeasible. Although computer scientists have methods for automated content analysis, most are optimized to classify individual documents, whereas social scientists instead want generalizations about {{the population of}} documents, such as the proportion in...|$|E
40|$|In these days, {{security}} of citizens {{is considered one}} of the major concerns of any government in the world. In every country, there is a huge amount <b>of</b> <b>unstructured</b> <b>texts</b> coming from investigating offenders in police departments. As a result, the importance of crime analysis is growing day after day. There is a little research; in methods and techniques that extract criminal networks from <b>unstructured</b> investigations <b>texts</b> especially in Arabic language. In our proposed system, we climb three main distinct contributions to discover forensics using investigation documents. The first by extracting offender names from <b>unstructured</b> <b>text.</b> Secondly, by constructing a crime network from real Arabic investigation documents. Finally, we provide analysis of the interaction between offenders in different documents that directly and indirectly related used to discover a new clue used to solve the crime puzzle. To evaluate the performance and effectiveness of the proposed system, real unstructured documents about investigations are obtained from police departments in the Gaza Strip. The experimental results show that the proposed system is effective in identifying proper offender person's name from real Arabic Documents. The average results for our system using the F-measure is 89 % also the average of F-measure in a proposed algorithm for discovery hidden relationship arrive to 92 %. In addition; we found that our approach achieves best F-measure results in most cases...|$|R
50|$|Semantic queries work on named graphs, linked-data or triples. This {{enables the}} query {{to process the}} actual {{relationships}} between information and infer the answers from the network of data. This {{is in contrast to}} semantic search, which uses semantics (the science <b>of</b> meaning) in <b>unstructured</b> <b>text</b> to produce a better search result (see natural language processing).|$|R
40|$|We {{present a}} {{graph-based}} semi-supervised la-bel propagation algorithm for acquiring open-domain labeled classes and their instances {{from a combination}} <b>of</b> <b>unstructured</b> and struc-tured <b>text</b> sources. This acquisition method significantly improves coverage compared to a previous set of labeled classes and instances derived from free text, while achieving com-parable precision...|$|R
