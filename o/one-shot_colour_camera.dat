0|350|Public
40|$|This thesis {{deals with}} the {{occupancy}} grid that is the sensorial representation of environment for mobile robots. The models for ultrasonic and laser rangefinder and <b>colour</b> <b>camera</b> are presented. The main contribution is the theorem about safe distance from ultrasonic measurement, building occupancy grid from <b>colour</b> <b>camera,</b> methods for data fusion of different sensors, and planning safe trajectory using occupancy grid. Available from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|R
25|$|On 28 September 2014, MOM {{controllers}} {{published the}} spacecraft's first global view of Mars. The image {{was captured by}} the Mars <b>Colour</b> <b>Camera</b> (MCC).|$|R
40|$|Many recent {{applications}} of computer graphics and human computer interaction have adopted both <b>colour</b> <b>cameras</b> and depth cameras as input devices. Therefore, an effective calibration of {{both types of}} hardware taking different colour and depth inputs is required. Our approach removes the numerical difficulties of using non-linear optimization in previous methods which explicitly resolve camera intrinsics {{as well as the}} transformation between depth and <b>colour</b> <b>cameras.</b> A matrix of hybrid parameters is introduced to linearize our optimization. The hybrid parameters offer a transformation from a depth parametric space (depth camera image) to a colour parametric space (<b>colour</b> <b>camera</b> image) by combining the intrinsic parameters of depth camera and a rotation transformation from depth <b>camera</b> to <b>colour</b> <b>camera.</b> Both the rotation transformation and intrinsic parameters can be explicitly calculated from our hybrid parameters {{with the help of a}} standard QR factorisation. We test our algorithm with both synthesized data and real-world data where ground-truth depth information is captured by Microsoft Kinect. The experiments show that our approach can provide comparable accuracy of calibration with the state-of-the-art algorithms while taking much less computation time (1 / 50 of Herrera’s method and 1 / 10 of Raposo’s method) due to the advantage of using hybrid parameters...|$|R
40|$|Image {{classification}} {{is one of}} {{the most}} important tasks of remote sensing projects including the ones that are based on using UAV images. Improving the quality of UAV images directly affects the classification results and can save a huge amount of time and effort in this area. In this study, we show that sensor fusion can improve image quality which results in increasing the accuracy of image classification. Here, we tested two sensor fusion configurations by using a Panchromatic (Pan) camera along with either a <b>colour</b> <b>camera</b> or a four-band multi-spectral (MS) camera. We use the Pan camera to benefit from its higher sensitivity and the <b>colour</b> or MS <b>camera</b> to benefit from its spectral properties. The resulting images are then compared to the ones acquired by a high resolution single Bayer-pattern <b>colour</b> <b>camera</b> (here referred to as HRC). We assessed the quality of the output images by performing image classification tests. The outputs prove that the proposed sensor fusion configurations can achieve higher accuracies compared to the images of the single Bayer-pattern <b>colour</b> <b>camera.</b> Therefore, incorporating a Pan camera on-board in the UAV missions and performing image fusion can help achieving higher quality images and accordingly higher accuracy classification results...|$|R
25|$|On 4 March 2015, the ISRO {{reported}} that MOM's methane sensors were functioning normally and are studying Mars' albedo, the reflectivity of the planet's surface. The Mars <b>Colour</b> <b>Camera</b> was also returning new {{images of the}} Martian surface.|$|R
3000|$|The MicroCam {{device is}} a {{miniature}} <b>colour</b> <b>camera</b> able {{to acquire a}} JPEG image through a serial or USB connection. Created for robotics and video surveillance, the camera can send 0.75 [*]fps video or compressed images ([...] [...]...|$|R
50|$|On 4 March 2015, the ISRO {{reported}} that MOM's methane sensors were functioning normally and are studying Mars' albedo, the reflectivity of the planet's surface. The Mars <b>Colour</b> <b>Camera</b> was also returning new {{images of the}} Martian surface.|$|R
50|$|The {{following}} {{is an example of}} the scene description language used by POV-Ray to describe a scene to render. It demonstrates the use of a background <b>colour,</b> <b>camera,</b> lights, a simple box shape having a surface normal and finish, and the transforming effects of rotation.|$|R
40|$|This paper {{considers}} 3 D imaging {{of moving}} objects and introduces {{a technique that}} exploits visible and x-ray images to recover dense 3 D models. While recent methods such as tomography from cone-beam x-ray can advantageously replace more expensive and higher-dose CT scanners, they still require specific equipment and immobilised pa-tients. We investigate an alternative strategy that combines a single x-ray source {{and a set of}} <b>colour</b> <b>cameras</b> to capture rigidly moving samples. The <b>colour</b> <b>cameras</b> allow for coarse marklerless motion tracking, which is further refined with the x-ray information. Once the sample poses are correctly estimated, a dense 3 D attenuation model is recon-structed from the set of x-ray frames. Preliminary results on simulated data compared to ground-truth as well as actual in-vivo experiments are presented...|$|R
50|$|Consequently, EMI {{chose to}} use a 4-tube version of the prism {{splitter}} for their new <b>colour</b> <b>camera,</b> in order to retain all {{the advantages of the}} method. However, devising a single prism arrangement for four tubes was less easy than for three and several alternatives were initially considered.|$|R
40|$|International audienceThis paper {{considers}} 3 D imaging {{of moving}} objects and introduces {{a technique that}} exploits visible and x-ray images to recover dense 3 D models. While recent methods such as tomography from cone-beam x-ray can advantageously replace more expensive and higher-dose CT scanners, they still require specific equipment and immobilised patients. We investigate an alternative strategy that combines a single x-ray source {{and a set of}} <b>colour</b> <b>cameras</b> to capture rigidly moving samples. The <b>colour</b> <b>cameras</b> allow for coarse marklerless motion tracking, which is further refined with the x-ray information. Once the sample poses are correctly estimated, a dense 3 D attenuation model is reconstructed from the set of x-ray frames. Preliminary results on simulated data compared to ground-truth as well as actual in-vivo experiments are presented...|$|R
40|$|We {{describe}} {{methods for}} using colour and texture to discriminate cloud and sky in images captured using a ground based <b>colour</b> <b>camera.</b> Neither method alone has proved sufficient {{to distinguish between}} different types of cloud, and between cloud and sky in general. Classification can be improved by combining the features using a Bayesian scheme. ...|$|R
5000|$|<b>Colour</b> Surveillance <b>camera</b> with light, 72:1 zoom, pan/tilt, {{low light}} {{switching}} capability ...|$|R
30|$|Tumour {{sections}} from mice {{injected with}} pimonidazole (given at 60 [*]mg/kg[*]i.p., 120 [*]min before sacrifice) were deparaffinised and immunostained with a commercial kit (Hydoxyprobe plus kit, USA) to specifically stain pimonidazole {{and determine the}} level of hypoxia. Stained sections were analysed using Axioscope A 1 ® coupled to an Axiocam 503 ® <b>colour</b> <b>camera</b> and ZEN® Software (Zeiss, Germany).|$|R
40|$|General {{description}} An experimental <b>colour</b> <b>camera</b> with 3 {{one inch}} Vidicons and a 4. 5 inch Image Orthicon tube. It {{was built to}} evaluate the use of 4 tube in a R G B & separate luminance scheme. There are few pictures and little information. References August 1963 journal of the Brit. IRE page 107 & 108. IBE “Earl...|$|R
50|$|The Advanced Moon micro-Imager Experiment was a {{miniature}} <b>colour</b> <b>camera</b> for lunar imaging. The CCD camera with three filters of 750, 900 and 950 nm {{was able to}} take images with an average pixel resolution of 80 m (about 260 ft). The camera weighed 2.1 kg (about 4.5 lb) and had a power consumption of 9 watts.|$|R
5000|$|<b>Colour</b> video <b>camera</b> with pan and {{zoom control}} {{in the front and}} back.|$|R
5000|$|G.E. ceased {{production}} of its 3 x I.O., the type PC-25 in 1966. Meanwhile, {{the company had}} brought out, in 1965, a 4-tube vidicon camera, the GE PE-24, for film scanner use. [...] This was followed by an all-Plumbicon 4-tube camera, the type PE250, which used conventional relay optics, rather than the prism optics of some other <b>colour</b> <b>cameras.</b> This camera was later followed by the PE350 and PE400, which continued to use the 4-tube format. (G.E. maintained, at that time, that 4-tube cameras gave the best pictures).|$|R
25|$|By {{the end of}} the decade, the {{practice}} of shooting on film for inserts in news broadcasts was declining, with the introduction of ENG technology into the UK. The equipment would gradually become less cumbersome the BBC's first attempts had been using a Philips <b>colour</b> <b>camera</b> with backpack base station and separate portable Sony U-matic recorder {{in the latter half of}} the decade.|$|R
3000|$|Frequently, {{standard}} CCD <b>colour</b> <b>cameras</b> have {{a higher}} resolution than range cameras, so the reprojection of 3 D points {{does not have a}} one-to-one equivalence. Hence, the ToF information is scaled up by bilinear interpolation. In addition to this, as only information of foreground 3 D points will be extracted, the automatic thresholding process is applied to the 3 D points P [...]...|$|R
50|$|By {{the end of}} the decade, the {{practice}} of shooting on film for inserts in news broadcasts was declining, with the introduction of ENG technology into the UK. The equipment would gradually become less cumbersome the BBC's first attempts had been using a Philips <b>colour</b> <b>camera</b> with backpack base station and separate portable Sony U-matic recorder {{in the latter half of}} the decade.|$|R
40|$|The paper {{presents}} {{a method of}} localization of licence plates in a traffic scene image. It is assumed that the input data comes from a <b>colour</b> <b>camera</b> {{and the results are}} grey scale images of isolated license plates. The method was implemented and tested. Test results confirm that it can be incorporated in a system for traffic monitoring or parking supervision...|$|R
50|$|The {{optronics}} of the FLW 100 {{are located}} in a container mounted behind the ammo box, located left of the gun. The sensors include a CCD <b>colour</b> <b>camera</b> with x10 magniciation, which offers an identification range of up to 1.5 km, and an uncooled thermal imager with a 640x480 resolution. The identification range of the thermal imager is 1 km.|$|R
5000|$|Blanca’s {{interest}} in photography began after {{he met his}} contemporary, Eva Veldhoen. She {{was the daughter of}} painter Aat Veldhoen. Blanca began using a small, <b>colour</b> <b>camera.</b> He then took black and white photographs using a 6 x 6 cm Hasselblad camera. In the 1980s Blanca received recognition for his violent self-portraits inspired by the works of Andres Serrano and Robert Mapplethorpe.|$|R
40|$|Chromatic {{aberration}} appears {{for almost}} all lenses for imaging based on white light. The effect degrades image quality for both b/w and <b>colour</b> <b>cameras.</b> This paper discusses the possibilities of modelling chromatic aberration with self-calibrating bundle adjustment for high-precision photogrammetric 3 D point measurement using all three channels of a true colour image. After a survey on common types of digital colour sensors, wave-length dependent imaging errors of usual lenses are discussed. Consequently, different options for the correction of the occurring effects are derived. The resulting effect on image and object accuracy is verified by various test field calibrations using different combinations of digital <b>colour</b> <b>cameras</b> and lenses. For standard imaging configurations in close-range photogrammetry, a significant enhancement of inner precision {{by a factor of}} 1. 7 can be proven. It can be shown that the length measurement error for standardized imaging configurations can be improved slightly whereby final results are still on investigation. 1...|$|R
40|$|The {{classification}} and {{the position}} estimation of objects {{become more and more}} relevant as the field of robotics is expanding in diverse areas of society. In this Bachelor Thesis, we developed a cone detection algorithm for an autonomous car using a LiDAR sensor and a <b>colour</b> <b>camera.</b> By evaluating simple constraints, the LiDAR detection algorithm preselects cone candidates in the 3 dimensional space. The candidates are projected into the image plane of the <b>colour</b> <b>camera</b> and an image candidate is cropped out. A convolutional neural networks classifies the image candidates as cone or not a cone. With the fusion of the precise position estimation of the LiDAR sensor and the high classification accuracy of a neural network, a reliable cone detection algorithm was implemented. Furthermore, a path planning algorithm generates a path around the detected cones. The final system detects cones even at higher velocity and has the potential to drive fully autonomous around the cones...|$|R
50|$|In {{collaboration}} with V.A. Petropavlovskii developed laboratory {{model of the}} first Soviet <b>colour</b> television <b>camera.</b>|$|R
5000|$|Curves and <b>Colour.</b> The <b>Camera</b> Studies Club, London, 1943. With Walter Bird and John Everard.|$|R
50|$|TWW {{was also}} {{a player in the}} {{development}} of 625-line colour transmission for the ITV network. Although the bulk of test transmissions and research were conducted for the Independent Television Authority (ITA) at the ABC studios at Teddington, TWW leased two prototype EMI <b>colour</b> <b>cameras</b> and associated equipment in 1966 and began running trials, with shows being transmitted on internal networks for viewing by employees.|$|R
40|$|Modern digital <b>colour</b> <b>cameras</b> {{are faced}} with a number of {{challenges}} in producing high-quality images, including noisy sensor measurements and chromatic aberration due to dispersion in the optics. In addition, most digital <b>colour</b> <b>cameras</b> use a single sensor combined with a set of colour filters to capture red, green and blue wavelengths of light at different spatial locations in a mosaic-like pattern. Hence, some form of interpolation, often called demosaicking, is required to produce a full colour image. These image restoration tasks are formulated as ill-posed inverse problems and solved through a regularisation inspired by the total variation (TV) denoising algorithm of Rudin, Osher and Fatemi. This leads to convex variational problems and edge-preserving image restorations. To solve these problems, an efficient primal-dual algorithm from convex analysis is adopted. In addition to some standard image restoration problems, we apply these methods to chromatic aberration and demosaicking. A TV-based demosaicking model is developed based on a decomposition of the image into luminance and chrominance components which are then regularised separately. The proposed method demonstrates improved results for demosaicking a set of standard test images...|$|R
50|$|Also in 1978, Venera 11 and Venera 12 flew past Venus, {{dropping}} descent {{vehicles on}} December 21 and December 25 respectively. The landers carried <b>colour</b> <b>cameras</b> and a soil drill and analyzer, which unfortunately malfunctioned. Each lander made measurements with a nephelometer, mass spectrometer, gas chromatograph, and a cloud-droplet chemical analyzer using X-ray fluorescence that unexpectedly discovered {{a large proportion}} of chlorine in the clouds, in addition to sulfur. Strong lightning activity was also detected.|$|R
40|$|Hyperspectral imaging cameras can {{determine}} if objects being viewed are hot or cold, wet or dry, their fat and sugar {{content and the}} presence of certain chemical elements. Therefore, it has a diverse range of applications in areas such as pharmaceuticals, food technology and homeland security. Whereas conventional <b>colour</b> <b>cameras</b> capture light in just three spectral windows, hyperspectral cameras have the ability to capture an entire section of the electromagnetic spectrum at every pixel...|$|R
50|$|The Studio {{opened with}} EMI 2001 cameras and was host to many {{programmes}} from London and locally produced programmes. During the early 1990s, the BBC technical resource department toured the UK's other BBC, ITV and Channel 4 studios {{to find new}} cameras to replace the ageing Link Electronics Ltd 125 <b>colour</b> <b>cameras.</b> Sony Broadcast BVP-370s were chosen and during this period asbestos {{was removed from the}} studio, gallery spaces and air plant.|$|R
5000|$|When observing old {{programme}}s, such {{as those}} from the 2001, {{it is very easy}} to tell if a programme used EMI 2001s (or any other first-generation PAL <b>colour</b> <b>camera)</b> to capture the images as the comet tails would often be coloured [...] "blobs" [...] or [...] "splodges" [...] (usually caused by a light source or light reflecting off a highly reflective or polished surface) simply because the camera did not have ACT circuits.|$|R
40|$|This paper {{presents}} identifies {{and addresses}} {{the difficulties that}} arise from implementing visual information into the Simultaneous Localization and Mapping (SLAM) problem, with an emphasis for outdoor applications. Through identifying these problems, techniques for integrating the visual & navigation are proposed with results from their preliminary applications. Video data is gathered through a standard <b>colour</b> <b>camera.</b> With the relative bearing obtained from the extracted features, the Simultaneous Localization & Mapping framework then bounds the dead-reckoning errors. ...|$|R
50|$|Vacuum, venting, {{nitrogen}} gas input (that {{can keep the}} oxygen volume at 10 percent or less), power and data interfaces are also provided within MSG. A video system consists of a self-standing subsystem of four <b>colour</b> <b>cameras,</b> two monitors, two analogue recorders and two digital recorders integrated into an International Sub-rack Interface Standard (ISIS) drawer. The command and monitoring panel monitors the facility status and performance and provides for manual operation of MSG by the crew.|$|R
