4950|272|Public
5|$|Studies by Wehner in the Sahara desert ant (Cataglyphis bicolor) {{demonstrate}} effective path integration {{to determine}} directional heading (by polarized light or sun position) and to compute distance (by monitoring leg movement or <b>optical</b> <b>flow).</b>|$|E
25|$|In vision-based {{activity}} recognition, a {{great deal}} of work has been done. Researchers have attempted a number of methods such as <b>optical</b> <b>flow,</b> Kalman filtering, Hidden Markov models, etc., under different modalities such as single camera, stereo, and infrared. In addition, researchers have considered multiple aspects on this topic, including single pedestrian tracking, group tracking, and detecting dropped objects.|$|E
25|$|What {{distinguished}} {{computer vision}} from the prevalent field of {{digital image processing}} {{at that time was}} a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, <b>optical</b> <b>flow,</b> and motion estimation.|$|E
40|$|This paper {{proposes a}} new {{approach}} to image-based rendering that generates an image viewed from an arbitrary camera position and orientation by rendering <b>optical</b> <b>flows</b> extracted from reference images. To derive valid <b>optical</b> <b>flows,</b> we develop an analysis technique that improves the quality of stereo matching. Without using any special equipments such as range cameras, this technique constructs reliable <b>optical</b> <b>flows</b> from a sequence of matching results between reference images. We also derive validity conditions of <b>optical</b> <b>flows</b> and show that the obtained flows satisfy those conditions. Since environment geometry is inferred from the <b>optical</b> <b>flows,</b> we are able to generate more accurate images with this additional geometric information. Our approach makes it possible to combine an image rendered from <b>optical</b> <b>flows</b> with an image generated by a conventional rendering technique through a simple Z-buffer algorithm...|$|R
40|$|In {{the human}} brain, {{independent}} components of <b>optical</b> <b>flows</b> from the medial superior temporal area are speculated for motion cognition. Inspired by this hypothesis, {{a novel approach}} combining independent component analysis (ICA) with principal component analysis (PCA) is proposed in this paper for multiple moving objects detection in complex scenes—a major real-time challenge as bad weather or dynamic background can seriously influence the results of motion detection. In the proposed approach, {{by taking advantage of}} ICA’s capability of separating the statistically independent features from signals, the ICA algorithm is initially employed to analyze the <b>optical</b> <b>flows</b> of consecutive visual image frames. As a result, the <b>optical</b> <b>flows</b> of background and foreground can be approximately separated. Since there are still many disturbances in the foreground <b>optical</b> <b>flows</b> in the complex scene, PCA is then applied to the <b>optical</b> <b>flows</b> of foreground components so that major <b>optical</b> <b>flows</b> corresponding to multiple moving objects can be enhanced effectively and the motions resulted from the changing background and small disturbances are relatively suppressed at the same time. Comparative experimental results with existing popular motion detection methods for challenging imaging sequences demonstrate that our proposed biologically inspired vision-based approach can extract multiple moving objects effectively in a complex scene...|$|R
30|$|Calculate <b>optical</b> <b>flows</b> {{between two}} {{adjacent}} frames (after registration as needed).|$|R
25|$|Clifford algebras {{have been}} applied in the problem of action {{recognition}} and classification in computer vision. Rodriguez et al. propose a Clifford embedding to generalize traditional MACH ﬁlters to video (3D spatiotemporal volume), and vector-valued data such as <b>optical</b> <b>flow.</b> Vector-valued data is analyzed using the Clifford Fourier Transform. Based on these vectors action filters are synthesized in the Clifford Fourier domain and recognition of actions is performed using Clifford Correlation. The authors demonstrate {{the effectiveness of the}} Clifford embedding by recognizing actions typically performed in classic feature ﬁlms and sports broadcast television.|$|E
500|$|Foraging ants travel {{distances}} {{of up to}} [...] from their nest [...] and scent trails allow them {{to find their way}} back even in the dark. In hot and arid regions, day-foraging ants face death by desiccation, so the ability to find the shortest route back to the nest reduces that risk. Diurnal desert ants of the genus Cataglyphis such as the Sahara desert ant navigate by keeping track of direction as well as distance travelled. Distances travelled are measured using an internal pedometer that keeps count of the steps taken and also by evaluating the movement of objects in their visual field (<b>optical</b> <b>flow).</b> Directions are measured using the position of the sun.|$|E
500|$|The seventh mission {{began in}} 2014 {{demanding}} more advanced behaviors than were currently possible by any aerial robot extant in 2014. The mission involves autonomous aerial robots controlling autonomous ground robots tactually. [...] The mission {{is divided into}} mission 7a and 7b. [...] Mission 7a requires a single autonomous aerial robot to herd {{as many of the}} 10 autonomous ground robot targets as possible, across the green boundary line in under 10 minutes. [...] The arena is 20m x 20m (65.62 feet x 65.62 feet) and has a green boundary line at one end, a red boundary line at the opposite end, and white sidelines. [...] The pattern {{on the floor of the}} arena is unknown to the aerial robot designers a priori, however it is known that there is a 1m x 1m (3.28 feet x 3.28 feet) white square grid pattern overlaid upon the arena. [...] Other than what is seen on the arena floor, there are neither walls for SLAM mapping nor GPS availability. [...] Techniques such as <b>optical</b> <b>flow</b> or optical odometry are possible solutions to navigation within the arena.|$|E
30|$|Rule 1 : Given 20 {{consecutive}} frames, {{the average}} vertical <b>optical</b> <b>flows</b> exhibit downward more than 75  % of frames.|$|R
30|$|To {{overcome}} such problems, {{we propose}} camera motion cancelled features. They are extracted by estimating camera motion for each frame from <b>optical</b> <b>flows</b> and cancelling it before feature description. <b>Optical</b> <b>flows</b> are computed in the peripheral region in a frame image. Since no tracking process is needed, it is applicable to large-scale event detection. We fuse this feature with other existing low-level features, which have complementary information, in a late fusion framework.|$|R
3000|$|Next, we {{show the}} {{calculation}} of optical flow-based vectors. For calculating <b>optical</b> <b>flows</b> from segments, we firstly divide regions of frame [...]...|$|R
50|$|An <b>optical</b> <b>flow</b> sensor is {{a vision}} sensor capable of {{measuring}} <b>optical</b> <b>flow</b> or visual motion and outputting a measurement based on <b>optical</b> <b>flow.</b> Various configurations of <b>optical</b> <b>flow</b> sensors exist. One configuration {{is an image}} sensor chip connected to a processor programmed to run an <b>optical</b> <b>flow</b> algorithm. Another configuration uses a vision chip, which is an integrated circuit having both the image sensor and the processor on the same die, allowing for a compact implementation. An {{example of this is}} a generic optical mouse sensor used in an optical mouse. In some cases the processing circuitry may be implemented using analog or mixed-signal circuits to enable fast <b>optical</b> <b>flow</b> computation using minimal current consumption. One area of contemporary research is the use of neuromorphic engineering techniques to implement circuits that respond to <b>optical</b> <b>flow,</b> and thus may be appropriate for use in an <b>optical</b> <b>flow</b> sensor. Such circuits may draw inspiration from biological neural circuitry that similarly responds to <b>optical</b> <b>flow.</b>|$|E
50|$|Thus:or This is an {{equation}} in two unknowns and cannot be solved as such. This {{is known as}} the aperture problem of the <b>optical</b> <b>flow</b> algorithms. To find the <b>optical</b> <b>flow</b> another set of equations is needed, given by some additional constraint. All <b>optical</b> <b>flow</b> methods introduce additional conditions for estimating the actual flow.|$|E
50|$|Motion {{estimation}} {{and video}} compression have {{developed as a}} major aspect of <b>optical</b> <b>flow</b> research. While the <b>optical</b> <b>flow</b> field is superficially similar to a dense motion field derived from the techniques of motion estimation, <b>optical</b> <b>flow</b> {{is the study of}} not only the determination of the <b>optical</b> <b>flow</b> field itself, but also of its use in estimating the three-dimensional nature and structure of the scene, as well as the 3D motion of objects and the observer relative to the scene, most of them using the Image Jacobian.|$|E
30|$|Rule 2 : The sum of {{the average}} {{vertical}} <b>optical</b> <b>flows</b> in 20 consecutive frames is larger than a threshold, say 10 in this study.|$|R
40|$|This paper {{presents}} a novel algorithm for road plane de-tection from an on-board camera. The algorithm employs the temporal difference of homography matrix, which is termed differential homography, caused by camera motion. Differential homography is estimated from <b>optical</b> <b>flows</b> of road plane regions, while using RANSAC algorithm to ex-tract the majority <b>optical</b> <b>flows.</b> Since differential homog-raphy is estimated using {{the relationship between}} the image coordinate (location in an image) and the flows at the loca-tions in an image. The proposed algorithm does not require the estimation of the homography matrix itself. Therefore, the proposed algorithm can be applied without calibration. The proposed algorithm effectively detect the <b>optical</b> <b>flows</b> from road region with using the pixel-pair feature match-ing. The algorithm is applied to the city traffic images distributed by UCL, and its average road detection ratio is found to be 75. 6 %. It is also applied to the previously col-lected suburban traffic images. A suitable detection result is obtained...|$|R
40|$|It {{is always}} a big {{challenge}} to extract moving objects in complex video scenes because bad weather or dynamic backgrounds can seriously influence the results of motion detection. In this research, a new hybrid approach combining independent component analysis (ICA) with principal component analysis (PCA) is proposed for multiple moving objects extraction in complex scenes. First, a fast ICA algorithm is {{used to analyze the}} <b>optical</b> <b>flows</b> of video frames, so that the <b>optical</b> <b>flows</b> of background and foreground can be approximately separated. Next, the PCA is applied to the <b>optical</b> <b>flows</b> of foreground components as such the major <b>optical</b> <b>flows</b> corresponding to target multi-objects can be extracted accurately and the motions resulting from changing backgrounds are cleared away simultaneously. Preliminary experimental results demonstrate that the proposed novel hybrid ICA and PCA-based approach can extract multiple objects effectively in a complex scene. Acknowledgements: This research is supported by The Royal Society of Edinburgh (RSE) and The National Natural Science Foundation of China (NNSFC) under the RSE-NNSFC joint project (2012 - 2015) [grant number 61211130309] with Anhui University, China, and the “Sino-UK Higher Education Research Partnership for PhD Studies” joint-project (2013 - 2015) funded by the British Council China and The China Scholarship Council (CSC). Amir Hussain and Erfu Yang are also funded, by the RSE-NNSFC joint project (2012 - 2015) [grant number 61211130210] with Beihang University, China...|$|R
50|$|The Horn-Schunck {{method of}} {{estimating}} <b>optical</b> <b>flow</b> is a global method which introduces a global constraint of smoothness to solve the aperture problem (see <b>Optical</b> <b>Flow</b> for further description).|$|E
50|$|<b>Optical</b> <b>flow</b> {{was used}} by {{robotics}} researchers in many areas such as: object detection and tracking, image dominantplane extraction, movement detection, robot navigation and visual odometry. <b>Optical</b> <b>flow</b> information has been recognized as being useful for controlling micro air vehicles.|$|E
50|$|Features are {{detected}} in the first frame, and then matched in the second frame. This information is then {{used to make the}} <b>optical</b> <b>flow</b> field for the detected features in those two images. The <b>optical</b> <b>flow</b> field illustrates how features diverge from a single point, the focus of expansion. The focus of expansion can be detected from the <b>optical</b> <b>flow</b> field, indicating the direction of the motion of the camera, and thus providing an estimate of the camera motion.|$|E
40|$|We {{present an}} {{algorithm}} for infinitesimal motion estimation and segmentation from multiple central panoramic views. We first {{show that the}} central panoramic <b>optical</b> <b>flows</b> corresponding to independent motions lie in orthogonal ten-dimensional subspaces of a higher-dimensional linear space. We then propose a factorization-based technique that estimates the number of independent motions, the segmentation of the image measurements and the motion of each object relative to the camera from a set of image points and their <b>optical</b> <b>flows</b> in multiple frames. Finally, we present experimental results on motion estimation and segmentation for a real image sequence with two independently moving mobile robots, and evaluate the performance of our algorithm by comparing the vision estimates with GPS measurements gathered by the mobile robots...|$|R
30|$|The {{system is}} {{suitable}} for detecting fast motion and big motions because it recognizes behaviors based on motion trajectories, which contain rich information about these motions. We plan to analyze local features, such as <b>optical</b> <b>flows,</b> in detail to expand the range of human behaviors that can be recognized.|$|R
5000|$|... #Subtitle level 2: Ultrasonic and <b>optical</b> {{time-of-flight}} <b>flow</b> meters ...|$|R
50|$|<b>Optical</b> <b>flow</b> meters {{are very}} stable with no moving parts and deliver a highly {{repeatable}} measurement {{over the life}} of the product. Because distance between the two laser sheets does not change, <b>optical</b> <b>flow</b> meters do not require periodic calibration after their initial commissioning. <b>Optical</b> <b>flow</b> meters require only one installation point, instead of the two installation points typically required by other types of meters. A single installation point is simpler, requires less maintenance and is less prone to errors.|$|E
50|$|Construct <b>optical</b> <b>flow</b> field (Lucas-Kanade method).|$|E
50|$|In short, {{the motion}} field cannot be {{correctly}} measured for all image points, and the <b>optical</b> <b>flow</b> is an approximation {{of the motion}} field. There are several different ways to compute the <b>optical</b> <b>flow</b> based on different criteria of how an optical estimation should be made.|$|E
40|$|This paper proposes {{the first}} {{end-to-end}} deep framework for {{high dynamic range}} (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging such as [13], the problem is formulated as an image composition problem, by first aligning input images using <b>optical</b> <b>flows</b> which are still error-prone due to occlusion and large motions. In our end-to-end approach, HDR imaging is formulated as an image translation problem and no <b>optical</b> <b>flows</b> are used. Moreover, our simple translation network can automatically hallucinate plausible HDR details {{in the presence of}} total occlusion, saturation and under-exposure, which are otherwise almost impossible to recover by conventional optimization approaches. We perform extensive qualitative and quantitative comparisons to show that our end-to-end HDR approach produces excellent results where color artifacts and geometry distortion are significantly reduced compared with existing state-ofthe-art methods. Comment: Submitted to CVPR 201...|$|R
40|$|We {{address the}} problem of {{recovering}} camera motion from video data, which does not require the establishment of feature correspondences or computation of <b>optical</b> <b>flows</b> but from normal flows directly. We have designed an imag-ing system that has a wide field of view by fixating a number of cameras together to form an approximate spherical eye. With a substantially widened visual field, we discover that estimating the directions of translation and rotation com-ponents of the motion separately are possible and particu-larly efficient. In addition, the inherent ambiguities between translation and rotation also disappear. Magnitude of rota-tion is recovered subsequently. Experimental results on syn-thetic and real image data are provided. The results show that not only the accuracy of motion estimation is compa-rable to those of the state-of-the-art methods that require explicit feature correspondences or <b>optical</b> <b>flows,</b> but also a faster computation time. 1...|$|R
30|$|It is {{difficult}} to detect small motions from only a trajectory, because a trajectory contains little information about small motions. Thus, {{it is hard to}} distinguish the trajectory from other normal trajectories in the trajectory feature space. Therefore, the system also uses <b>optical</b> <b>flows</b> [33] in the human region as a local feature for detecting small motions.|$|R
50|$|Motion detection. Area based, {{differential}} approach. <b>Optical</b> <b>flow.</b>|$|E
5000|$|Lucas-Kanade method An <b>optical</b> <b>flow</b> {{algorithm}} {{derived from}} reference 1.|$|E
5000|$|Marr Prize Paper: David Heeger, <b>Optical</b> <b>Flow</b> using Spatiotemporal Filters ...|$|E
40|$|This paper {{presents}} a structure-from-motion system which delivers dense structural {{information from a}} sequence of dense <b>optical</b> <b>flows.</b> Most traditional featurebased approaches cannot be extended to compute dense structure due to impractical computational complexity. We demonstrate that by decomposing uncertainty information into independent and correlated parts we can decrease these complexities from O(N 2) to O(N), where N {{is the number of}} pixels in the images. We also show that this dense structure-from-motion system requires only local <b>optical</b> <b>flows,</b> i. e. image matchings between two adjacent frames, instead of the tracking of features over a long sequence. 1 Introduction Structure from motion {{has been one of the}} most active areas in computer vision during the past decade. The goal is to recover 3 D structural information from a sequence of images taken under unknown relative motions between the camera and the scene. Most approaches proposed in the literature can be classified [...] ...|$|R
30|$|The system determines an {{occurrence}} {{of a specific}} behavior based on the distance from a trajectory to each class of specific behaviors in the trajectory feature space created using PCA. The system also has functions to verify detected behaviors. For example, it uses backward tracking to verify fast motions, and it calculates <b>optical</b> <b>flows</b> to verify small motions. These functions contribute to robust recognition of specific types of behavior.|$|R
40|$|Within PANTHER {{research}} project, we aim {{to develop}} multi-rate, multi-format, multi-reach and multi-flow terabit transceivers for data-center gateways, having the capability of flexibly controlling this enormous capacity and distributing it among independent <b>optical</b> <b>flows.</b> To this end, we combine electro-optic with passive polymers and we develop a novel photonic integration platform with unprecedented potential for high-speed modulation and optical functionality on-chip. We also rely on the combination of polymers with InP elements {{and the use of}} InP-DHBT electronics for driving circuits based on 3 -bit power-DACs and high-speed TIA arrays. Using 3 D integration techniques, we also aim to integrate these components in system-in-package transceivers capable of operation at 64 Gbaud, operation with formats up to DP- 64 -QAM and flexibility in the handling of multiple <b>optical</b> <b>flows</b> on-chip. In this paper, we present the system level vision and the technical approach for the development of these modules, and we present the concept for a thin software layer that will control the parameters of the transceivers and will extend the SDN hierarchy down to the flexible optical transport layer...|$|R
