1087|3975|Public
5|$|In {{regards to}} the 6 Plus, Engadget panned its design for being uncomfortable to hold and harder to grip in {{comparison}} to other devices such as the Galaxy Note 3 and LG G3, but praised its inclusion of <b>optical</b> <b>image</b> stabilization and slightly better battery life than the 6.|$|E
5|$|The G2's audio {{hardware}} and software is optimized to support 24-bit/192 kHz audio; during LG's press event, ringtones recorded by the Vienna Boys' Choir (which are also bundled with the device) were used to demonstrate the high quality audio from its internal speaker. The G2 also includes a 13-megapixel rear-facing camera with <b>optical</b> <b>image</b> stabilization, and an infrared emitter which allows it {{to serve as a}} universal remote with the accompanying QuickRemote app.|$|E
5|$|The HTC One is {{equipped}} with a 4.0-megapixel rear-facing camera module that contains a custom image sensor marketed as UltraPixel, which is composed of pixels that are 2.0µm in size. Most high-end smartphones {{at the time of its}} release used 8- or 13-megapixel cameras with pixel sizes ranging from 1.4 to 1.0µm, both of which are considerably smaller in size than the pixels found in the One’s UltraPixel sensor. Although these smaller pixel sizes were typically necessary to ensure that the camera sensor did not compromise the design of the phone, there were concerns that this could result in a loss of dynamic range and sensitivity, and also result in poor performance in low-light environments. As such, HTC stated that its camera design with larger sensor pixels could notably increase overall image quality, especially in low-light environments. The camera also includes <b>optical</b> <b>image</b> stabilization, and is further enhanced by improvements to the Sense camera software and the ImageChip 2 image processor.|$|E
5000|$|<b>Optical</b> <b>Imaging</b> Laboratory at Washington University in St. Louis (MCML) ...|$|R
5000|$|OIG ("Optical Imager Galileo"), CCD camera {{dedicated}} to <b>optical</b> <b>images</b> at high resolution; ...|$|R
40|$|In {{this paper}} {{we present a}} novel method to combine partial texture {{mappings}} of arbitrary sur-faces by using surface flattening. Partial texture maps {{can be obtained from}} <b>optical</b> <b>images</b> of an object acquired from different viewpoints. Combining <b>optical</b> <b>images</b> into a complete texture map that covers the entire object is not trivial. Surface flattening preserves the structure of a 3 D mesh and converts it into the 2 D space. <b>Optical</b> <b>images</b> can be converted to flattening based surface maps that can be merged based on the structure of the mesh. This paper describes the flattening and the merging methods and shows results on synthetic and real data. ...|$|R
25|$|Secondo Pia's 1898 {{photographs}} of the shroud allowed the scientific community to begin to study it. A variety of scientific theories regarding the shroud have since been proposed, based on disciplines ranging from chemistry to biology and medical forensics to <b>optical</b> <b>image</b> analysis. The scientific approaches {{to the study of}} the Shroud fall into three groups: material analysis (both chemical and historical), biology and medical forensics and image analysis.|$|E
25|$|The {{application}} of the sampling theorem to images should be made with care. For example, the sampling process in any standard image sensor (CCD or CMOS camera) is relatively far from the ideal sampling which would measure the image intensity at a single point. Instead these devices have a relatively large sensor area at each sample point {{in order to obtain}} sufficient amount of light. In other words, any detector has a finite-width point spread function. The analog <b>optical</b> <b>image</b> intensity function which is sampled by the sensor device is not in general bandlimited, and the non-ideal sampling is itself a useful type of low-pass filter, though not always sufficient to remove enough high frequencies to sufficiently reduce aliasing. When the area of the sampling spot (the size of the pixel sensor) is not large enough to provide sufficient spatial anti-aliasing, a separate anti-aliasing filter (optical low-pass filter) is typically included in a camera system to further blur the <b>optical</b> <b>image.</b> Despite images having these problems in relation to the sampling theorem, the theorem can be used to describe the basics of down and up sampling of images.|$|E
25|$|The iPhone 7's {{overall design}} {{is similar to}} the iPhone 6S, but {{introduces}} new color options, water and dust resistance, a new capacitive, static home button, and removes the 3.5mm headphone jack. The device's internal hardware also received upgrades, including a heterogeneous quad-core system-on-chip with improved system and graphics performance, and upgraded 12megapixel rear-facing cameras with <b>optical</b> <b>image</b> stabilization on all models and an additional telephoto lens on the iPhone 7 Plus model to provide enhanced zoom capabilities.|$|E
40|$|The {{fusion of}} {{synthetic}} aperture radar (SAR) and optical data is a dynamic research area, but image segmentation is rarely treated. While a few studies use low-resolution nadir-view <b>optical</b> <b>images,</b> we approached the segmentation of SAR and <b>optical</b> <b>images</b> acquired from the same airborne platform – leading to an oblique view with high resolution and thus increased complexity. To overcome the geometric differences, we generated a digital surface model (DSM) from adjacent <b>optical</b> <b>images</b> {{and used it to}} project both the DSM and SAR data into the optical camera frame, followed by segmentation with each channel. The fused segmentation algorithm was found to out-perform the single-channel version...|$|R
5000|$|L-3 Sonoma EO, Electro <b>Optical</b> <b>Imaging</b> Systems, 1508M Dragon Eyes, 1205MD, 2111X, 2514X, & 2711G ...|$|R
5000|$|... #Caption: [...] Figure 1. (a, b, d) <b>Optical</b> <b>images</b> of meridianiite, MgSO4•11H2O. Courtesy Genceli et al. 2007.|$|R
25|$|Because demosaicing is not {{required}} for the Foveon X3 sensor to produce a full-color image, the color artifacts ("colored jaggies") associated with that process are not seen. The separate anti-aliasing filter commonly used to mitigate those artifacts in a Bayer sensor is {{not required}}. This is because little aliasing occurs when the photodiodes for each color, {{with the assistance of}} the microlenses, integrate the <b>optical</b> <b>image</b> over a region almost as big as the spacing of sensors for that color.|$|E
25|$|The Centre for Development of Imaging Technology (C-DIT) {{is a total}} {{solution}} provider in information technology for the Government of Kerala. C-DIT also functions as the southern regional video software production facility of the National Council for Science and Technology Communication (NCSTC), New Delhi. Web services, GIS, video communication, animation, educational informatics and <b>optical</b> <b>image</b> processing are some exclusive teams that C-DIT offers. C-DIT offers a postgraduate diploma in science and development communication, postgraduate diploma in educational informatics, postgraduate diploma in multimedia development, animation film development course and short term courses in digital still photography, digital sound recording and nonlinear editing.|$|E
500|$|It was {{reported}} that the <b>optical</b> <b>image</b> stabilization systems on some iPhone 6 Plus models were faulty, failing to properly stabilize when the phone is being held perfectly still, leading to blurry photos and [...] "wavy"-looking videos. The <b>optical</b> <b>image</b> stabilization system was also found to {{have been affected by}} accessories that use magnets, such as third-party lens attachments; Apple issued advisories to users and its licensed accessory makers, warning that magnetic or metallic accessories can cause the OIS to malfunction.|$|E
5000|$|City College of NY 1994-2002 Director of NASA IRA Program on Tunable Solid State Lasers and <b>Optical</b> <b>Imaging</b> ...|$|R
3000|$|... max is {{the maximum}} {{value of the}} edge {{strength}} image of the SAR intensity image I(x,[*]y). Since noise in SAR images are modeled as multiplicative, typical edge detectors for <b>optical</b> <b>images</b> are not suitable here. We adopt the ROEWA detector [40] for SAR images to compute the edge strength image g, which here plays the same role as the gradient image in a typical watershed transform-based segmentation for <b>optical</b> <b>images.</b>|$|R
2500|$|... {{and used}} for {{producing}} simulated <b>optical</b> <b>images</b> by Luminet [...] and Marck, {{in which it is}} to be noted ...|$|R
500|$|The iPhone 6's {{rear-facing}} camera now has {{the ability}} to shoot 1080p video at either 30 or 60 frames per second and slow-motion video at either 120 or 240 frames per second. The camera also includes phase detection autofocus. It can also record [...] The iPhone 6 Plus camera is nearly identical, but also includes <b>optical</b> <b>image</b> stabilization. The front-facing camera was also updated with a new sensor and f/2.2 aperture, along with support for burst and HDR modes.|$|E
500|$|Despite their {{positive}} reception, the iPhone 6 and 6 Plus {{have been the}} subject of several hardware issues, including most prominently, being susceptible to bending under pressure (a design flaw nicknamed [...] "Bendgate"), and as a byproduct of this lack of rigidity, the touchscreen's internal hardware being susceptible to losing its connection to the phone's logic board (nicknamed [...] "Touch Disease"). The iPhone 6 Plus was also the subject of camera issues, including some devices with malfunctioning <b>optical</b> <b>image</b> stabilization or otherwise defects on rear cameras.|$|E
500|$|Shen Kuo {{experimented with}} the pinhole camera and burning mirror as the ancient Chinese Mohists {{had done in}} the 4th century BC. Although the Iraqi Muslim {{scientist}} Ibn al-Haytham (965–1039) {{was the first to}} experiment with camera obscura, Shen Kuo was the first to attribute geometrical and quantitative properties to the camera obscura, just several decades after Ibn al-Haytham's death. [...] Using a fitting metaphor, Shen compared <b>optical</b> <b>image</b> inversion to an oarlock and waisted drum. He also discussed focal points and noted that the image in a concave mirror is inverted. Shen, who never asserted that {{he was the first to}} experiment with camera obscura, hints in his writing that camera obscura was dealt with in the Miscellaneous Morsels from Youyang written by Duan Chengshi (d. 863) during the Tang Dynasty (618–907), in regard to the inverted image of a Chinese pagoda by a seashore.|$|E
3000|$|... {{remote sensing}} data {{acquired}} by air-borne or space-borne sensors such as multispectral <b>optical</b> <b>images</b> or Synthetic Aperture Radar (SAR) images.|$|R
40|$|The shape {{constitutes}} an essential property of surfaces and objects. It {{can be obtained}} by a variety of contact-free measurement principles based on <b>optical</b> <b>images.</b> In this contribution, common and modern methods of shape measurement based on <b>optical</b> <b>images,</b> which are suitable for determining the macroscopic shape of surfaces, are presented. The characteristic properties of the measurement principles (e. g. with respect to reflectance characteristics and inspection conditions) are highlighted and compared...|$|R
40|$|International audienceThe aim of {{this study}} was to develop an {{inversion}} approach to estimate surface soil moisture from X-band SAR data over irrigated grassland areas. This approach simulates a coupling scenario between Synthetic Aperture Radar (SAR) and <b>optical</b> <b>images</b> through the Water Cloud Model (WCM). A time series of SAR (TerraSAR-X and COSMO-SkyMed) and optical (SPOT 4 / 5 and LANDSAT 7 / 8) images were acquired over an irrigated grassland region in southeastern France. An inversion technique based on multi-layer perceptron neural networks (NNs) was used to invert the Water Cloud Model (WCM) for soil moisture estimation. Three inversion configurations based on SAR and <b>optical</b> <b>images</b> were defined: (1) HH polarization, (2) HV polarization, and (3) both HH and HV polarizations, all with one vegetation descriptor derived from optical data. The investigated vegetation descriptors were the Normalized Difference Vegetation Index "NDVI", Leaf Area Index "LAI", Fraction of Absorbed Photosynthetically Active Radiation "FAPAR", and the Fractional vegetation COVER "FCOVER". These vegetation descriptors were derived from <b>optical</b> <b>images.</b> For the three inversion configurations, the NNs were trained and validated using a noisy synthetic dataset generated by the WCM for a wide range of soil moisture and vegetation descriptor values. The trained NNs were then validated from a real dataset composed oTERRASAR-XCOSMO-SKYMED X-band SAR backscattering coefficients and vegetation descriptor derived from <b>optical</b> <b>images.</b> The use of X-band SAR measurements in HH polarization (in addition to one vegetation descriptor derived from <b>optical</b> <b>images)</b> yields more precise results on soil moisture (Mv) estimates. In the case of NDVI derived from <b>optical</b> <b>images</b> as the vegetation descriptor, the Root Mean Square Error on Mv estimates was 3. 6 Vol. % for NDVI values between 0. 45 and 0. 75, and 6. 1 Vol. % for NDVI between 0. 75 and 0. 90. Similar results were obtained regardless of the other vegetation descriptor used...|$|R
500|$|The G2's {{rear-facing}} {{camera was}} considered good for its class, with its processor contributing to quicker HDR photo processing than its competitors. The Verge remarked that despite LG having [...] "practically stole" [...] Samsung's camera design and modes, the G2's camera interface {{were among the}} better implementations of Android camera software due to its available options. However, its low-light photos {{and some of its}} other modes were panned for not being as good as those of other devices such as the Nokia Lumia 920 and HTC One. In a photography-focused review by Digital Photography Review, the <b>optical</b> <b>image</b> stabilization system was praised for helping maintain good levels of exposure, and well-lit photos were found to have a decent level of detail, noting that its lens was [...] "sharp pretty much all across the frame and free of chromatic aberrations." [...] However, it was noted that [...] "as the light gets dimmer and in the ISO starts to increase", the device began to suffer from [...] "very heavy-handed noise reduction which results in visible softness", and further noted that [...] " [...] detail starts to suffer as soon as you go higher than base ISO and by ISO 400 most low-contrast detail is gone." [...] However, in a December 2013 comparison against other recent phones such as the One, Galaxy S4 Zoom, Xperia Z1, iPhone 5S, and Lumia 1020 by TechRadar, the G2 was named the best cameraphone of the six for [...] " [...] very well in terms of picture quality, ease of use and functionality, as well as post processing", although it was panned for not having as many options as its competitors, and for the probability of fingers accidentally getting into landscape shots due to the positioning of the lens.|$|E
2500|$|... iOS 11 {{introduces}} <b>optical</b> <b>image</b> stabilization, {{flash photography}} and {{high dynamic range}} for portrait photos.|$|E
2500|$|Video camera {{tubes in}} the early days of {{television}} used the photoelectric effect, for example, Philo Farnsworth's [...] "Image dissector" [...] used a screen charged by the photoelectric effect to transform an <b>optical</b> <b>image</b> into a scanned electronic signal.|$|E
30|$|A {{variety of}} {{problems}} in image processing {{is related to the}} contour – object detection, extraction and recognition. This we encountered in cytology preparations, variety of <b>optical</b> <b>images,</b> radar images, spectrogram features [4], mixing and concatenating processing methods.|$|R
5000|$|... #Caption: <b>Optical</b> <b>images</b> reveal {{clouds of}} gas and dust in the Orion Nebula; an {{infrared}} image (right) reveals the new stars shining within.|$|R
40|$|Confocal laser {{scanning}} microscopy (CLSM or LSCM) is a technique for obtaining high-resolution <b>optical</b> <b>images</b> with depth selectivity. The key feature of confocal microscopy {{is its ability to}} acquire in-focus images from selected depths, a process known as <b>optical</b> sectioning. <b>Images</b> are acquired point-by-point and reconstructed with a computer, allowing three-dimensional reconstructions of topologically-complex objects...|$|R
2500|$|Unlike the {{previous}} model, the camera {{does not support}} <b>optical</b> <b>image</b> stabilization, as developers deemed it to be [...] "incompatible" [...] with the new depth sensor system. It was replaced by [...] "smart stabilization" [...] features enabled by the depth sensor. The operating system's camera interface was also streamlined, with a new menu for switching between photo, video, Zoe, and Pan 360 modes, and a revised settings interface.|$|E
2500|$|The iPhone 7 {{includes}} a 12-megapixel rear-facing camera with a quad-LED [...] "True Tone" [...] flash; its aperture was widened to f/1.8, and the standard-size phone model adds <b>optical</b> <b>image</b> stabilization {{a feature that}} was previously exclusive to Plus models. The iPhone 7 Plus {{includes a}} second 12-megapixel telephoto lens, {{which can be used}} to achieve 2× optical zoom, and up to 10× digital zoom. The front-facing camera was upgraded to a 7-megapixel sensor with automatic image stabilization.|$|E
2500|$|Serial time encoded amplified {{microscopy}} (STEAM) is an {{imaging method}} that provides ultrafast shutter speed and frame rate, by using <b>optical</b> <b>image</b> amplification {{to circumvent the}} fundamental trade-off between sensitivity and speed, and a single-pixel photodetector to {{eliminate the need for}} a detector array and readout time limitations [...] The method is at least 1000 times faster than the state-of-the-art CCD and CMOS cameras. Consequently, it is potentially useful for a broad range of scientific, industrial, and biomedical applications that require high image acquisition rates, including real-time diagnosis and evaluation of shockwaves, microfluidics, MEMS, and laser surgery.|$|E
40|$|<b>Optical</b> <b>Imaging</b> of {{intrinsic}} signals detects neural activation patterns by taking video {{images of the}} local activity-related changes in the light intensity reflected from neural tissue (intrinsic signals). At red light (605 nm), these signals are mainly caused by local variations of the tissue absorption following deoxygenation of blood. In this work, we characterize the image generation process during <b>Optical</b> <b>Imaging</b> by Monte Carlo simulations of light propagation through a homogeneous model tissue equipped with a local absorber. Conventional video-imaging and Scanning Laser imaging are compared to each other. We find that, compared to video imaging, Scanning Laser techniques drastically increase both the contrast and the lateral resolution of optical recordings. Also, the maximum depth up to which the signals can be detected, is increased by roughly a factor of 2 using Scanning Laser <b>Optical</b> <b>Imaging.</b> Further, the radial profile of the diffuse reflectance pattern for each pixel is subj [...] ...|$|R
40|$|Deformation {{monitoring}} by multipass {{synthetic aperture}} radar (SAR) interferometry is widely recognized as the only method to assess millimeter-level deformation over a large area from space. Recently, it is demonstrated that urban infrastructures can be monitored in a semantic level in SAR interferometry (InSAR) by fusing the InSAR point cloud with the classification labels derived from <b>optical</b> <b>images</b> [1], [2]. This paper proposes an algorithm for object-level joint InSAR deformation reconstruction using these classification labels. We derived an object-based multi-baseline InSAR reconstruction model, and proposed an efficient algorithm for bridge detection in <b>optical</b> <b>images...</b>|$|R
40|$|The {{combined}} use of {{high resolution}} SAR and <b>optical</b> <b>images</b> {{is of great}} interest, especially in complex scenes like urban areas. This requires accurate registration of images with big radiometric and geometric differences. This paper shows the accuracy {{that can be achieved}} by taking advantage of the geolocation accuracy of current sensors, instead of using more complex feature or intensity-based methods. An issue that can arise in layover areas is identified and a solution is shown. The approach is illustrated using very high resolution SAR and <b>optical</b> <b>images...</b>|$|R
