58|10000|Public
5000|$|The duo {{borrowed}} the title [...] "Electric Arguments" [...] from the poem [...] "Kansas City to St. Louis" [...] by Allen Ginsberg. In Wired magazine, McCartney stated {{this was because}} [...] "he's {{been looking at the}} beauty <b>of</b> <b>word</b> <b>combinations</b> rather than their meaning." ...|$|E
5000|$|Metody semanticheskogo issledovaniya ogranichennogo podyazyka /Methods of Semantic Investigation of a Restricted Sublanguage/ (414 pp.), Moscow University Press, 1971 (with B. Gorodetsky)* Slovari slovosochetaniy i chastotnye slovari slov ogranichennogo podyazyka /Dictionaries <b>of</b> <b>Word</b> <b>Combinations</b> and Dictionaries of Words with Frequencies of a Restricted Sublanguage/ (538 pp.), Moscow University Press, 1972 (with B. Y. Gorodetsky, A. E. Kibrik, L. S. Logakhina, G. V., Maksimova, and E. S. Prytkov) ...|$|E
5000|$|To explain {{observed}} {{language learning}} differences between children and adults, children are postulated to create countless new connections daily, and may handle the language learning process {{more effectively than}} do adults. This assumption, however, remains untested {{and is not a}} reliable explanation for children’s aptitude for L2 learning. Problematic of the behaviourist approach is its assumption that all learning, verbal and non-verbal, occurs through the same processes. A more general problem is that, as Pinker (1995) notes, almost every sentence anybody voices is an original combination of words, never previously uttered, therefore a language cannot consist only <b>of</b> <b>word</b> <b>combinations</b> learned through repetition and conditioning; the brain must contain innate means of creating endless amounts of grammatical sentences from a limited vocabulary. This is precisely what Chomsky (1965) (reprinted as [...] )argues with his proposition of a universal grammar (UG).|$|E
40|$|Review {{of a new}} {{dictionary}} <b>of</b> Italian <b>word</b> <b>combinations.</b> A {{full description}} of the dictionary is given from the points of view of headwords, the structure of entries, the types <b>of</b> <b>word</b> <b>combination</b> included, and indications of meaning and usage. The dictionary is then evaulated in terms of (a) {{the usefulness of the}} phrases included, (b) help given in using specific phrases, (c) presentation and accessibility, and (d) accuracy...|$|R
5000|$|Ezhuththathigaaram - Formation <b>of</b> <b>words</b> and <b>combination</b> <b>of</b> <b>words</b> ...|$|R
5000|$|Wasei-eigo ("Japanese-made English", [...] "English words coined in Japan") are Japanese {{language}} expressions {{based on}} English <b>words</b> or parts <b>of</b> <b>words</b> <b>combinations,</b> {{that do not}} exist in standard English language or whose meaning differs from the words they were derived from.Linguistics classifies them as pseudo-loanwords or pseudo-anglicisms.|$|R
5000|$|Nearly {{ten years}} had passed before McCartney and Youth started {{creating}} new material for their third studio album Electric Arguments. The duo recorded songs again at McCartney's Hog Hill studio yet did so one session at a time between 2007 and 2008. This was their first album to feature any vocals because both Youth and McCartney felt that their material needed a change; thus the genre of the band evolved from its more electronic roots to an experimental rock influence. Like their previous two albums however, McCartney played all of the instruments while Youth produced the tracks.The duo borrowed the title Electric Arguments from the poem [...] "Kansas City to St. Louis" [...] by Allen Ginsberg. In Wired magazine, McCartney stated this was because [...] "he's {{been looking at the}} beauty <b>of</b> <b>word</b> <b>combinations</b> rather than their meaning."This was also their first album to be released on the independent music label One Little Indian, changing from McCartney's usual label EMI. This was due to McCartney's sentiment that major record labels were not adaptive to the times, referring to the newfound popularity of online music at the time such as iTunes.|$|E
40|$|In {{this paper}} I present a study which {{contributes}} to the description of lexicographic traditions in Sweden and Denmark. More exactly, the study concerns the treatment <b>of</b> <b>word</b> <b>combinations,</b> which are followed by some kind of explanation, in two 19 th century monolingual dictionaries. The works are the Swedish dictionary Ordbok öfver svenska språket (1850 – 55) by A. F. Dalin and the Danish dictionary Dansk Ordbog (…) (1833) by C. Molbech. The focus is, among other things, on the frequencies and types <b>of</b> <b>word</b> <b>combinations</b> that are represented in the data. Furthermore, I consider in which entries the word combinations are found, and {{the position of the}} expressions in the microstrucure of the articles. Moreover, I discuss the form and meaning of the word combinations and, finally, the authors’ use of (signed) examples...|$|E
40|$|We {{report on}} three {{experiments}} aimed at comparing two popular methods for the automatic extraction <b>of</b> <b>Word</b> <b>Combinations</b> from corpora, {{with a view}} to evaluate: i) their efficacy in acquiring data to be included in a combinatory resource for Italian; ii) the impact of different types of benchmarks on the evaluation itself...|$|E
40|$|We {{proposed}} a novel kernel for text categorization. This kernel is an inner {{product in the}} feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of {{the product of the}} inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. By restricting the <b>words</b> <b>of</b> a <b>word</b> <b>combination</b> to the same sentence and considering multi-word <b>combinations,</b> the <b>word</b> <b>combination</b> features can capture similarity at a more specific level than single words. By discarding word order, the <b>word</b> <b>combination</b> features are more compatible with the flexibility of natural language and the dimensionality this kernel can be reduced significantly compared to the word-sequence kernel. We conducted a series of experiments on the Reuters- 21578 dataset and 20 Newsgroups dataset. This kernel consistently achieves better performance than the classical word kernel and word-sequence kernel on the two datasets. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance and compared the computing efficiency of this kernel to those <b>of</b> the <b>word</b> kernel and word-sequence kernel. We {{proposed a}} novel kernel for text categorization. This kernel is an inner product in the feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of the product of the inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. By restricting the <b>words</b> <b>of</b> a <b>word</b> <b>combination</b> to the same sentence and considering multi-word <b>combinations,</b> the <b>word</b> <b>combination</b> features can capture similarity at a more specific level than single words. By discarding word order, the <b>word</b> <b>combination</b> features are more compatible with the flexibility of natural language and the dimensionality this kernel can be reduced significantly compared to the word-sequence kernel. We conducted a series of experiments on the Reuters- 21578 dataset and 20 Newsgroups dataset. This kernel consistently achieves better performance than the classical word kernel and word-sequence kernel on the two datasets. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance and compared the computing efficiency of this kernel to those <b>of</b> the <b>word</b> kernel and word-sequence kernel...|$|R
40|$|In {{this paper}} we {{proposed}} a novel kernel for text categorization. This kernel is an inner product in the feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of {{the product of the}} inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. We conducted experiments on the 20 Newsgroups dataset. This kernel achieves better performance than the classical word kernel and word-sequence kernel. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance. © 2012 IEEE. IEEE Beijing Section; Hunan University of Humanities, Science and Technology; Tongji University; Xiamen University; Central South UniversityIn this paper {{we proposed a}} novel kernel for text categorization. This kernel is an inner product in the feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of the product of the inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. We conducted experiments on the 20 Newsgroups dataset. This kernel achieves better performance than the classical word kernel and word-sequence kernel. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance. © 2012 IEEE...|$|R
40|$|In many {{applications}} of {{natural language processing}} (NLP) {{it is necessary to}} determine the likelihood <b>of</b> a given <b>word</b> <b>combination.</b> For example, a speech recognizer may need to determine which <b>of</b> the two <b>word</b> <b>combinations</b> "eat a peach" and "eat a beach" is more likely. Statistical NLP methods determine the likelihood <b>of</b> a <b>word</b> <b>combination</b> from its frequency in a training corpus. However, the nature of language is such that many <b>word</b> <b>combinations</b> are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen <b>word</b> <b>combinations</b> using available information on "most similar" words. We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based [...] ...|$|R
40|$|Abstract. The paper {{presents}} {{a method for}} automatic detection of “non-trivial” word combinations in the text. It is based on automatic syntactic analysis. The method shows better precision and recall than the baseline method (bigrams). It was tested on a text in Spanish. The method {{can be used for}} enrichment of very large dictionaries <b>of</b> <b>word</b> <b>combinations.</b> ...|$|E
40|$|Fuzzy {{approach}} {{deals with}} the linguistic properties of elements such as beauty, coldness, hotness etc. Collocations are linguistically motivated. Decision of word combination for being collocation is a linguistic term as merely cooccurrence <b>of</b> <b>word</b> <b>combinations</b> does not signify the presence of collocation. Thus collocation extraction can be made possible by looking its linguistic aspect. In the present paper, an attempt {{has been made to}} make two different fuzzy sets <b>of</b> <b>word</b> <b>combinations</b> to be considered for collocations. Mutual information and t-test have been taken as basis for the construction of fuzzy sets. Two fuzzy set theoretical models have been proposed to identify collocations. It has been shown that fuzzy set theoretical approach works very well for collocation extraction. The working data has been based on a corpus of about one million words contained in different novels constituting project Gutenberg available on www. gutenberg. org...|$|E
40|$|This paper {{reports on}} work, {{carried out in}} the {{framework}} of the CombiNet project, focusing on the automatic extraction <b>of</b> <b>word</b> <b>combinations</b> from large corpora, with a view to represent the full distributional profile of selected lemmas. We describe two extraction methods, based on part-of-speech sequences (P-method) and syntactic patterns (S-method), respectively, evaluating their performance – contrastively, and with reference to external benchmarks – and discussing the relevance of automatic knowledge acquisition for lexicographic purposes. Our results indicate that both approaches provide valuable data and confirm previous claims that P-methods and S-methods are largely complementary, as they tend to retrieve different types <b>of</b> <b>word</b> <b>combinations.</b> In {{the second part of the}} paper, we present SYMPAThy, a data representation format devised to fruitfully merge the two methods by leveraging their respective points of strength. In order to explore SYMPAThy’s potentialities, a preliminary investigation on a small set of Italian idioms, and specifically their degree of fixedness/productivity, is also described...|$|E
5000|$|... tsukeai (付合): May also {{be called}} tsukekata (付け方) or tsukeaji (付け味). Refers to the mixing and {{matching}} <b>of</b> unlikely <b>word</b> <b>combinations</b> to spur imagination or evoke an image. One {{of the interesting}} features of renga.|$|R
2500|$|Loanwords often {{keep their}} {{original}} spellings: cadeau [...] 'gift' (from French). The Latin letters c, qu, x and y (from Greek υ) are sometimes adapted to k, kw, ks and i. Greek letters φ and ῥ become f and r, not ph or rh, but θ mostly becomes th (except before a consonant, after f or ch {{and at the}} end <b>of</b> <b>words).</b> <b>Combinations</b> -eon-, -ion-, -yon- in loanwords from French are written with a single n (mayonaise) except when a schwa follows (stationnement).|$|R
5000|$|Prior to Harris's {{discovery}} of transformations, grammar as so far developed could not yet treat <b>of</b> individual <b>word</b> <b>combinations,</b> but only <b>of</b> <b>word</b> classes. A sequence or ntuple <b>of</b> <b>word</b> classes (plus invariant morphemes, termed constants) specifies {{a subset of}} sentences that are formally alike. Harris investigated mappings from one such subset to another in the set of sentences. In linear algebra, a mapping that preserves a specified property is called a transformation, {{and that is the}} sense in which Harris introduced the term into linguistics. Harris's transformational analysis refined the word classes found in the 1946 [...] "From Morpheme to Utterance" [...] grammar of expansions. By recursively defining semantically more and more specific subclasses according to the combinatorial privileges <b>of</b> <b>words,</b> one may progressively approximate a grammar <b>of</b> individual <b>word</b> <b>combinations.</b> This relation <b>of</b> progressive refinement was subsequently shown in a more direct and straightforward way in a grammar of substring combinability resulting from string analysis (Harris 1962).|$|R
40|$|The present {{dissertation}} aims at simulating {{the construction}} of lexicographic layouts for an Italian combinatory dictionary based on real linguistic data, extracted from corpora by using computational methods. This work {{is based on the}} assumption that the intuition of the native speaker, or the lexicographer, who manually extracts and classifies all the relevant data, are not adequate to provide sufficient information on the meaning and use of words. Therefore, a study of the real use of language is required and this is particularly true for dictionaries that collect the combinatory behaviour of words, where the task of the lexicographer is to identify typical combinations where a word occurs. This study is conducted in the framework of the CombiNet project aimed at studying Italian Word Combinationsand and at building an online, corpus-based combinatory lexicographic resource for the Italian language. This work is divided into three chapters. Chapter 1 describes the criteria considered for the classification <b>of</b> <b>word</b> <b>combinations</b> according to the work of Ježek (2011). Chapter 1 also contains a brief comparison between the most important Italian combinatory dictionaries and the BBI Dictionary <b>of</b> <b>Word</b> <b>Combinations</b> in order to describe how word combinations are considered in these lexicographic resources. Chapter 2 describes the main computational methods used for the extraction <b>of</b> <b>word</b> <b>combinations</b> from corpora, taking into account the advantages and disadvantages of the two methods. Chapter 3 mainly focuses on the practical word carried out in the framework of the CombiNet project, with reference to the tools and resources used (EXTra, LexIt and "La Repubblica" corpus). Finally, the data extracted and the lexicographic layout of the lemmas to be included in the combinatory dictionary are commented, namely the words "acqua" (water), "braccio" (arm) and "colpo" (blow, shot, stroke) ...|$|E
40|$|Abstract. The paper {{presents}} {{a method of}} automatic enrichment {{of a very large}} dictionary <b>of</b> <b>word</b> <b>combinations.</b> The method is based on results of automatic syntactic analysis (parsing) of sentences. The dependency formalism is used for representation of syntactic trees that allows for easier treatment of information about syntactic compatibility. Evaluation of the method is presented for the Spanish language based on comparison of the automatically generated results with manually marked word combinations. Key words: collocations, parsing, dependency grammar, Spanish. ...|$|E
40|$|This paper {{reports on}} work {{carried out in}} the {{framework}} of an ongoing project aimed at building an online, corpus - based lexicographic resource for Italian Word Combinations. Our aim is to compare two of the most commonly used methods for the automatic extraction <b>of</b> <b>word</b> <b>combinations</b> from corpora, with a view to evaluate their performance – and ultimately their efficacy – with respect to the task of acquiring word combinations for inclusion in the lexi cographi c combinatory resource...|$|E
40|$|The {{frequency}} <b>of</b> <b>words</b> {{and other}} linguistic units plays {{a central role}} in all branches of corpus linguistics. Indeed, the use of frequency information distinguishes corpus-based methodology from other approaches to language. Thus, not surprisingly, the distribution <b>of</b> frequencies <b>of</b> <b>words</b> and <b>combinations</b> o...|$|R
40|$|It is now {{generally}} agreed that the native-like use <b>of</b> collocations (<b>word</b> <b>combinations</b> such as heavy smoker, make a speech, bitterly cold) is {{an important element of}} pro. cient language use (e. g. Sinclair, 1991; Wray, 2002). How-ever, researchers have found that L 2 learners rely heavily on creativity s...|$|R
50|$|Mimaland started {{operating}} in 1971. The name Mimaland {{is actually an}} acronym <b>of</b> the <b>word</b> <b>combination</b> Malaysia In Miniature Land. Mimaland was built on a hilly area of 300 acres in Ulu Gombak near Kuala Lumpur. It was owned by Mimaland Berhad, a member of Magnum Group of Companies (now Magnum Corporation).|$|R
40|$|This article {{deals with}} noun+verb {{combinations}} in bilingual Basque-Spanish and Spanish-Basque dictionaries. We {{take a look}} at morphosyntactic and semantic features <b>of</b> <b>word</b> <b>combinations</b> in both language directions, and compare them to identify differences and similarities. Our work reveals the high complexity of those constructions and, hence, the need to address them specifically in Natural Language Processing tools, for example in Machine Translation. All of our results are publicly available online, where users can query the combinations we have analysed...|$|E
40|$|We {{describe}} {{tools for}} {{the extraction of}} collocations {{not only in the}} form <b>of</b> <b>word</b> <b>combinations,</b> but also of data about the morphosyntactic properties of collocation candidates. Such data are needed for a detailed lexical description of collocations, and to support both their recognition in text and the generation of collocationally acceptable text. We describe the tool architecture, report on a case study based on noun+verb collocations, and we give a first rough evaluation of the data quality produced. selection: syntactic analysis (pattern matching...|$|E
40|$|Frequency {{effects are}} central in current second {{language}} research and theory {{but they have}} often been restricted to effects of input frequency in naturalistic environments (e. g. Ellis, 2002; Ellis and Ferreira-Junior, 2009). In {{a large majority of}} EFL contexts, however, the amount of input in the foreign language is poor and the influence of the first language may be strong (Krashen, 1981). Research on cross-linguistic influence in SLA has indeed shown that it can impact learning {{in a number of ways}} (Odlin, 1989; Jarvis and Pavlenko, 2008). The main objective of this paper is to provide empirical data to support Gass and Mackey’s (2002) view that transfer deserves a more prominent position in the ongoing debate and investigation of frequency effects. The paper draws on findings from three consecutive corpus-based investigations of EFL learners’ use <b>of</b> <b>word</b> <b>combinations</b> in which I made use of Jarvis’s (2000) methodological framework to investigate transfer effects on learners’ use of lexical bundles, i. e. “recurrent expressions, regardless of their idiomaticity, and regardless of their structural status” (Biber et al., 1999), in the French component of the International Corpus of Learner English. The studies focused on 3 -word lexical bundles that include a lexical verb, lexical bundles that function as text organizers (e. g. on the contrary, let us take the example) and bundles that are prominent in academic writing. Results suggest that transfer effects are detectable in French EFL learners’ selection of a number of English word combinations whose translational equivalents are deeply entrenched in their mental lexicon. L 1 frequency proved to contribute to transferability in a significant way and the different manifestations of L 1 influence displayed in the learners’ idiosyncratic use <b>of</b> <b>word</b> <b>combinations</b> were traced back to various properties of French words, including their preferred collocational use and syntactic structures, their functions and discourse conventions. Comparisons of interlanguages (e. g. the interlanguage of French vs. Spanish learners) and spot-checks in corpora of different first languages (e. g. French vs. Spanish) also provided further evidence that L 1 frequency effects play a crucial role in learners’ use <b>of</b> <b>word</b> <b>combinations</b> in input-poor environments...|$|E
40|$|Abstract. In many {{applications}} of {{natural language processing}} (NLP) {{it is necessary to}} determine the likelihood <b>of</b> a given <b>word</b> <b>combination.</b> For example, a speech recognizer may need to determine which <b>of</b> the two <b>word</b> <b>combinations</b> “eat a peach ” and “eat a beach ” is more likely. Statistical NLP methods determine the likelihood <b>of</b> a <b>word</b> <b>combination</b> from its frequency in a training corpus. However, the nature of language is such that many <b>word</b> <b>combinations</b> are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen <b>word</b> <b>combinations</b> using available information on “most similar ” words. We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similaritybased method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error. We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similaritybased methods perform up to 40 % better on this particular task...|$|R
50|$|The Court {{ruled that}} {{trademarks}} consisting <b>of</b> certain <b>word</b> <b>combinations</b> {{not used in}} a common phraseology may be deemed creations, bestowing distinctive power on the trademark. If the relevant goods or services or their essential characteristics are so formed, then they may be refused registration {{on the grounds that}} such marks are solely descriptive and non-distinctive.|$|R
40|$|The aim of {{this study}} was to {{identify}} some factors which are associated with further language development in toddlers. To this purpose a longitudinal study design with two assessments was used in order to evaluate to what extent some language variables measured at 24 - 30 months of age could predict language outcome one year later (36 - 42 months). Three measures (absence <b>of</b> <b>word</b> <b>combination,</b> reduced vocabulary, absence of morphological competence), potentially associated with later language development, were extracted from the MacArthur Communicative Development Inventory, compiled by the teachers at the first assessment. According to the results, a risk index was developed which allowed to identify a group of late talkers. The cognitive, behavioural and language profiles of children at risk showed some differences as compared with those of toddlers with typical language development...|$|R
40|$|Collocations are {{a feature}} of natural lan-guages that are not well {{addressed}} by cur-rent models used for NLP. Language is full <b>of</b> <b>word</b> <b>combinations</b> that occur more fre-quently than expected, are semantically opaque, or show surprising syntactic restric-tions. In her book, Nesselhauf studies col-locations in a corpus of second language learner (L 2) English. She focuses on German advanced learners of English and describes how these learners use collocation and what mistakes they make. She analyses what factors influence the mastery of collocations and what material interferes with learning. The structure {{of the book is}} very clear. Afte...|$|E
40|$|Three {{experiments}} {{were conducted to}} test the psychological relevance of objectively quantified word collocations. The first experiment showed that perceived frequency <b>of</b> <b>word</b> <b>combinations</b> roughly followed the objective count. Anot­her recurrent quality of words, constructional tendency, was supplemented as independent variable in the two following experiments. This variable reflects a words tendency to appear in word combinations and it was found to interact with frequency when subjects rated frequency and comprehensibility. The experiments showed that word collocations, defined at the levels of combinations and constructional tendency of individual words, can be supposed to have psychological counterparts; that linguis­tic recurrence seems to have cognitive representations. digitalisering@umu. s...|$|E
40|$|Historically, {{the word}} phraseology has been {{connected}} with lexicography (Knappe 2004), especially with bilingual dictionaries (Moon 2000) and their norm-setting role associated with foreignlanguage teaching and learning. Nineteenth-century specialised bilingual lexicography seems {{to depart from}} the then usual normbased approaches and to favour the inclusion of use-related phraseological combinations. This paper analyses the treatment <b>of</b> <b>word</b> <b>combinations</b> which {{can be referred to}} as ante-litteram collocations in two rather different dictionaries, namely Tarver’s Royal Phraseological English–French, French–English Dictionary (1845 – 53) and Nutt’s English–Italian Conversation Dictionary (1894), to see to what extent nineteenth-century specialised bilingual lexicography has marked a move away from prescriptivism and affected later studies on the lexicographical treatment of collocation and changes {{in the use of the}} word phraseology...|$|E
40|$|In {{the article}} under {{discussion}} the ways <b>of</b> creating illogical <b>word</b> <b>combinations</b> by means <b>of</b> using the lexical units aimed to create semantic-logical incongruity in fiction texts are analyzed. The use of lexical means causing incompatibility in fiction texts {{are illustrated in}} the classical literature quotations...|$|R
50|$|Dainas feature several stylistic {{devices to}} ensure euphony. Common devices use repetition; these include alliteration (repetition of similar consonants in {{stressed}} syllables), anaphora and epiphora (the use <b>of</b> the same <b>words</b> {{at the beginning}} and end of lines, the repetition <b>of</b> a <b>word,</b> <b>combination</b> <b>of</b> <b>words</b> or previous line, or starting new sentence with a word that has the same root as the last <b>word</b> <b>of</b> the previous sentence). Comparisons and other symbolic devices are also found in their range, including straightforward comparisons, epithets, metaphors, synecdoches, allegories, personifications and parallelisms where seemingly unrelated concepts are used to liken events from nature to human life and different social classes.|$|R
5000|$|Ḥet {{is one of}} the few Hebrew consonants {{that can}} take a vowel at the end <b>of</b> a <b>word.</b> This occurs when patach gnuva comes under the Ḥet at the end <b>of</b> the <b>word.</b> The <b>combination</b> is then {{pronounced}} [...] rather than [...] For example: פתוח (...) , and תפוח (...) [...]|$|R
