20|10000|Public
5000|$|Data {{consolidation}} - The {{process of}} capturing master data from multiple sources and integrating {{into a single}} hub (<b>operational</b> <b>data</b> <b>store)</b> for replication to other destination systems.|$|E
50|$|A {{data hub}} differs from a data {{warehouse}} {{in that it}} is generally unintegrated and often at different grains. It differs from an <b>operational</b> <b>data</b> <b>store</b> because a data hub {{does not need to be}} limited to operational data.|$|E
50|$|The {{data stored}} in the {{warehouse}} is uploaded from the operational systems (such as marketing or sales). The data may pass through an <b>operational</b> <b>data</b> <b>store</b> and may require data cleansing for additional operations to ensure data quality before it {{is used in the}} DW for reporting.|$|E
40|$|The {{corporate}} data warehouse integrates <b>data</b> from various <b>operational</b> <b>data</b> <b>stores</b> of a company. These <b>operational</b> <b>data</b> <b>stores</b> may be heterogeneous {{with respect to}} the represented information. The hetero-homogeneous data warehouse modeling approach overcomes issues associated with the integration of heterogeneous information from the <b>operational</b> <b>data</b> <b>stores</b> b...|$|R
40|$|Database {{technologies}} have {{evolved over the}} last two decades into different constructs to support the ever-growing information needs for organizations spanning the spectrum of operational and analytical processing. The paper examines the characteristics of transactional databases, <b>operational</b> <b>data</b> <b>stores,</b> <b>data</b> warehouses and virtual data warehouses. A framework is developed for an optimal data warehousing strategy based on organizational needs classified by the types of business processes defined by the requirements of supporting functional areas and the levels of decision structures. Enterprise architecture is described to provide an integrated and complimentary view of various data warehousing constructs...|$|R
5000|$|One of the {{fundamental}} elements of biomedical and translational research {{is the use of}} integrated data repositories. Survey conducted in 2010, defined [...] "integrated data repository" [...] (IDR) as a data warehouse incorporating various sources of clinical data to support queries for a range of research-like functions. Integrated data repositories are complex systems developed to solve a variety of problems ranging from identity management, protection of confidentiality, semantic and syntactic comparability of data from different sources, and most importantly convenient and flexible query. Development of the field of clinical informatics lead to the creation of large data sets with electronic health record data integrated with other data (such as genomic data). Types of <b>data</b> repositories include <b>operational</b> <b>data</b> <b>stores</b> (ODSs), clinical <b>data</b> warehouses, clinical data marts, and clinical registries. <b>Operational</b> <b>data</b> <b>stores</b> established for extracting, transferring and loading before creating warehouse or data marts. Clinical registries repositories have long been in existence, but their contents are disease specific and sometimes considered archaic. Clinical <b>data</b> <b>stores</b> and clinical <b>data</b> warehouses are considered fast and reliable. Though these large integrated repositories have impacted clinical research significantly, it still faces challenges and barriers. One big problem is the requirement for ethical approval by the institutional review board (IRB) for each research analysis meant for publication. Some research resources do not require individual IRB approval, example CDWs with data of deceased patients have been de-identified, and its usage does not require institutional review board (IRB) approval. However, privacy sensitive data may still be explored by researchers when shared through its metadata and services, for example by following a linked open data perspective. Another challenge is data quality. Methods that adjust for bias (such as using propensity score matching methods) assume that complete health record is captured. Tools that examine data quality (e.g., point to missing data) help in discovering data quality problems.|$|R
5000|$|An <b>operational</b> <b>data</b> <b>store</b> (or [...] "ODS") is a {{database}} designed to integrate data from multiple sources for additional operations on the data. Unlike a master data store, {{the data is}} not passed back to operational systems. It may be passed for further operations and to the data warehouse for reporting.|$|E
5000|$|The tools include data networks, file systems, a data warehouse, data marts, an <b>operational</b> <b>data</b> <b>store,</b> data mining, data analysis, data visualization, data {{federation}} {{and data}} virtualization. One {{of the newest}} tools, virtual master data management utilizes data virtualization and a persistent metadata server to implement a multi-level automated master data management hierarchy[...].|$|E
5000|$|In this scenario, SAP Enterprise Resource Planning (ERP) data {{goes into}} SAP HANA which {{acts as an}} <b>operational</b> <b>data</b> <b>store</b> for {{immediate}} analysis. Once the data is analyzed it is integrated into SAP IQ via Near-line storage mechanisms (as described above). Here SAP IQ acts as an enterprise data warehouse that receives data {{from a variety of}} traditional sources (such as OLTP Databases and files systems), and SAP HANA Operational Data Store(ODS) ...|$|E
40|$|The data vault model {{natively}} supports {{data and}} schema evolution, {{so it is}} often adopted to create <b>operational</b> <b>data</b> <b>stores.</b> However, it can hardly be directly used for OLAP querying. In this paper we propose an approach called Starry Vault for finding a multidimensional structure in data vaults. Starry Vault builds on the specific features of the data vault model to automate multidimensional modeling, and uses approximate functional dependencies to discover out of data the information necessary to infer the structure of multidimensional hierarchies. The manual intervention by the user is limited to some editing of the resulting multidimensional schemata, which makes the overall process simple and quick enough to be compatible with the situational analysis needs of a data scientist...|$|R
5000|$|... 2011 - ReServe Interactive {{launches}} ReServe Gateway, a {{web services}} platform that enables enterprise customers to integrate and share <b>operational</b> and financial <b>data</b> <b>stored</b> in their ReServe Interactive software with third-party software applications ...|$|R
40|$|This paper proposes an {{enterprise}} modeling {{framework for the}} deployment of data warehouses. The framework provides the information roadmap coordinating source data and different data warehouses across the business enterprise. The paper introduces a solution to address data warehousing issues at the enterprise level while avoiding the pitfalls of creating enterprise data warehouses and universal data marts. It further proposes a change of paradigm from point solutions focus to a methodology driven by enterprise requirements {{to meet the challenges}} of the new economy. The proposed framework emphasizes the separation of the conceptual construct from the physical and operational constructs of {{an enterprise}}. It points out the differences and dependencies of analytic and operational processes and how <b>data</b> warehouses and <b>operational</b> <b>data</b> <b>stores</b> respectively support their information requirements. This paper will demonstrate how the enterprise modeling framework for data warehousing can produce business benefits...|$|R
50|$|In computing, Extract, Transform, Load (ETL) {{refers to}} a process in {{database}} usage and especially in data warehousing. The ETL process became a popular concept in the 1970s. Data extraction is where data is extracted from homogeneous or heterogeneous data sources; data transformation where the data is transformed for storing in the proper format or structure {{for the purposes of}} querying and analysis; data loading where the data is loaded into the final target database, more specifically, an <b>operational</b> <b>data</b> <b>store,</b> data mart, or data warehouse.|$|E
50|$|Data {{warehouses}} (DW) often {{resemble the}} {{hub and spokes}} architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an <b>operational</b> <b>data</b> <b>store,</b> the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built {{on top of the}} Data warehouse.|$|E
50|$|The typical Extract, transform, load (ETL)-based data {{warehouse}} uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted {{from each of}} the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an <b>operational</b> <b>data</b> <b>store</b> (ODS) database. The integrated data are then moved to yet another database, often called the {{data warehouse}} database, where the data is arranged into hierarchical groups, often called dimensions, and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.|$|E
40|$|The {{amount and}} value of {{information}} available due to rapid spread of information technology is exploding. Typically, large enterprises have approximately a petabyte of <b>operational</b> <b>data</b> <b>stored</b> in hundreds of data repositories supporting thousands of applications. Data storage volumes grow in excess of 50 % annually. This growth {{is expected to continue}} due to vast proliferation of existing information systems and the introduction of new data sources. Wireless Sensor Networks (WSNs) represent one of the most notable examples of such new data sources. In recent few years, various types of WSNs have been deployed and the amount of information generated by wireless sensors increases rapidly. The information explosion requires establishing novel data processing and communication techniques for WSNs. This volume aims to cover both theoretical and practical aspects related to this challenge, and it explores directions for future research to enable efficient utilization of WSNs in the information-explosion era...|$|R
40|$|Efficient {{adaption}} of a company’s {{business and}} its business processes to a changing environment {{is a crucial}} ability to survive in today’s dynamic world. For optimizing business processes, a profound analysis of all relevant business data in the company is necessary. Analyses typically exclusively run on business process execution <b>data</b> or on <b>operational</b> busi-ness <b>data</b> <b>stored</b> in a <b>data</b> warehouse. However, to achieve a more informative analysis and to fully optimize a company’s business, a consolidation of all major business data sources is indispensable. For this consolidation our demonstration presents BIAEditor that allows to annotate and match pro-cess variables and <b>operational</b> <b>data</b> models in order to per-form such a global business impact analysis. The BIAEditor allows to match the data in two ways. Either variables and data models can be combined directly or described seman-tically by ontology concepts and matched automatically. 1...|$|R
40|$|Categorizing the {{end user}} {{in the web}} {{environment}} is a mindnumbing task. Huge amount of <b>operational</b> <b>data</b> is generated when end user interacts in web environment. This generated <b>operational</b> <b>data</b> is <b>stored</b> in various logs and may be useful source of capturing the end user activates. Pointing out the suspicious user in a web environment is a challenging task. To conduct efficient investigation in cyber space the available logs should be correlated. In this paper a prototype system is developed and implemented {{which is based on}} relational algebra to build the chain of evidence. The prototype system is used to preprocess the real generated data from logs and classify the suspicious user based on decision tree. At last various challenges in the logs managements are presented. Keywords cyber forensic; log file; correlation; decision tree,chain of evidence,cyber crime;. 1...|$|R
50|$|The {{staging area}} can support hosting {{of data to}} be {{processed}} on independent schedules, and data that {{is meant to be}} directed to multiple targets. In some instances data might be pulled into the staging area at different times to be held and processed all at once. This situation might occur when enterprise processing is done across multiple time zones each night, for instance. In other cases data might be brought into the staging area to be processed at drent times; or the staging area may be used to push data to multiple target systems. As an example, daily operational data might be pushed to an <b>operational</b> <b>data</b> <b>store</b> (ODS) while the same data may be sent in a monthly aggregated form to a data warehouse.|$|E
40|$|Building a Business Intelligence (BI) {{application}} is very challenging {{as it is}} a young discipline and does not yet offer well-established strategies and techniques for the developments process when compared to the software engineering discipline. Furthermore, information requirements analysis for BI applications which integrate data from heterogeneous sources differs significantly from requirements analysis for a conventional information system. Conceptual Design Model <b>Operational</b> <b>Data</b> <b>Store</b> (CoDMODS) to build BI application that focuses on operational information to support business operations is proposed. In this model, combination of community interaction and data integration approach were used to identify the requirements for developing BI application. Furthermore, how the <b>operational</b> <b>data</b> <b>store</b> can be used for operational and tactical information and can be transferred to a data warehouse for supporting analytical information and decision making is also presented. Finally, to verify and validate the proposed model, the case study approach using web application development in selected subject areas is elaborated...|$|E
40|$|This project built {{a medical}} {{informatics}} data warehouse (MedInfo DDW) in an Oracle database to analyze medical information {{which has been}} collected through Baylor Family Medicine Clinic (FCM) Logician application. The MedInfo DDW used Star Schema with dimensional model, FCM database as <b>operational</b> <b>data</b> <b>store</b> (ODS); the data from on-line transaction processing (OLTP) were extracted and transferred to a knowledge based data warehouse through SQLLoad, and the patient information was analyzed by using on-line analytic processing (OLAP) in Crystal Report...|$|E
40|$|International Telemetering Conference Proceedings / October 15 - 17, 1974 / International Hotel, Los Angeles, CaliforniaThe Viking Program {{will place}} two {{orbiting}} spacecraft around Mars {{in the summer}} of 1976. Each spacecraft will contain a Mars soft lander. The Telemetry Subsystem is that group of electronics on the Viking Lander which interfaces all sources of <b>operational</b> and science <b>data,</b> <b>stores</b> and conditions that data, and provides it to the communications subsystem in appropriate form for RF transmission...|$|R
40|$|Multiversion {{databases}} store both {{current and}} historical data. Rows are typically annotated with timestamps representing {{the period when}} the row is/was valid. We develop novel techniques for reducing index maintenance in multiversion databases, so that indexes can be used effectively for analytical queries over current data without being a heavy burden on transaction throughput. To achieve this end, we re-design persistent index data structures in the storage hierarchy to employ an extra level of indirection. The indirection level is stored on solid state disks that can support very fast random I/Os, so that traversing the extra level of indirection incurs a relatively small overhead. The extra level of indirection dramatically reduces the number of magnetic disk I/Os that are needed for index updates, and localizes maintenance to indexes on updated attributes. Further, we batch insertions within the indirection layer {{in order to reduce}} physical disk I/Os for indexing new records. By reducing the index maintenance overhead on transactions, we enable <b>operational</b> <b>data</b> <b>stores</b> to create more indexes to support queries. We have developed a prototype of our indirection proposal by extending the widely used Generalized Search Tree (GiST) open-source project, which is also employed in PostgreSQL. Our working implementation demonstrates that we can significantly reduce index maintenance and/or query processing cost, by a factor of 3. For insertions of new records, our novel batching technique can save up to 90 % of the insertion time. 1...|$|R
40|$|AbstractOperational Database Systems {{are keeping}} {{large amounts of}} {{information}} that are not used in any aspect on current business processes. It‟s inactive data that is maintained only for historic reasons or to ensure data patrimony and process heritage. Thus, database archiving is today a critical task for companies that are worried about their data storage devices, processing resources, and, obviously, data preservation. Extracting selectively relevant business information from operational database and storing it on a separate archive <b>data</b> <b>store</b> for consulting it‟s a good strategy for preserving data as well to reduce storage database resources and improve data processing services. In this paper we propose an archiving technique inspired on the most effective data warehousing dimensional modelling techniques. We designed and developed a set of mechanisms {{with the ability to}} analyze and characterize a conventional relational database, and based on a set of business preservation requirements and archiving rules, prepare it to be archived on a multidimensional system designed automatically. We‟ll present their description and functionalities, describing {{the way in which they}} can be used and the manner how <b>operational</b> <b>data</b> is <b>stored</b> in a archiving oriented data warehouse...|$|R
40|$|BUILDING AN <b>OPERATIONAL</b> <b>DATA</b> <b>STORE</b> FOR A DIRECT MARKETING APPLICATION SYSTEM by Chad Smith An <b>operational</b> <b>data</b> <b>store</b> (ODS) can be {{generally}} {{described as an}} architectural construct that is both similar and different in design and purpose to a data warehouse. It is similar {{in that it is}} subject-oriented and integrated from the various systems and sources that feed it. However, it is unique in that the data is volatile and updated on a relatively regular basis, it holds little to no archival data, and the data is kept at a detailed level with little summarization. The purpose of this thesis is to provide an overview of an ODS and the data that it holds, as well as to describe the author’s participation in a corporate project to build an ODS used to process and score customer information for a direct marketing application. This application gathers customer information from point-of-sale transactions and online orders. The author’s role on the project was to develop, test, and implement solutions concerning database performance, as well to provide database analysis and support throughout the project lifecycle. This thesis will provide an overview of the system’s functionality and the individual contributions the author made to the overall solution...|$|E
40|$|This paper {{presents}} research-in-progress. An extensive customer-centric {{data warehouse}} architecture should enable both complex analytical queries {{as well as}} standard reporting queries on customer data without performance restrictions for both requirements. This paper introduces a dichotomic approach, which brings together these contradicting tasks of a data warehouse. On the one hand, it elaborates on the qualities of customer data and their implications on data structures due to their change over time. The authors will present a data concept, that is specialized in gaining a realistic image of the customer, ideally over her entire customer lifecycle. On the other hand, the paper works out {{the role of the}} <b>operational</b> <b>data</b> <b>store</b> (ODS) in the light of CRM: it presents how the ODS supports its counterpart, the data warehouse, in the dichotomic approach for the maintenance of high performance and low response times. ...|$|E
40|$|This work investigates Pharmacy Data Warehouse {{model and}} practice. The {{development}} {{and design of}} data warehouse is faced with various problems. One of {{the biggest and most}} time consuming data warehousing design and development issues - is the data extraction from existing systems, transformations and load (ETL process) to target systems. Analyzed company uses Oracle database as <b>operational</b> <b>data</b> <b>store.</b> Examination of the current situation, the technical base, for the implementation of the data warehouse is selected the Microsoft SQL Server 2000 Analysis Services tools. Because these tools already exist in the company, thus reducing the overall costs needed. During this work is realized the cube of Pharmacy customers analysis, tested overall system performance, in addition, an example schedules for cube updating are created. This enables the analysts to more effectively analyze customer behavior, sales and offer customers the most appropriate solutions, while retaining existing customers and attracting new ones...|$|E
40|$|Computer aided {{process design}} is {{improving}} with newer and newer tools. One of such tools is the automatic calculation technique {{that enables the}} combination of different software tools to enhance {{the efficiency of the}} calculations. In our research work Aspen HYSYS model of a petrochemical plant is built in order to simulate responses of an existing plant to the changes in the composition and amount of feed material. The Aspen HYSYS is connected to Microsoft Excel program; simu-lated <b>operational</b> <b>data</b> are <b>stored</b> in an operational database and transported to Excel for further analysis. The automatic calculation completed with the two software tools mutually strengthens their merits and results in enhanced insight into the operational features of any plant. Comparison of the projected input parameters of the petrochemical plant studied shows that the extension of the plant is badly needed. Cash-flow analysis suggests that the extension is profitable...|$|R
40|$|Managing time-stamped data is {{essential}} to clinical research activities and often {{requires the use of}} considerable domain knowledge. Adequately representing and integrating temporal data and domain knowledge is difficult with the database technologies used in most clinical research systems. There is often a disconnect between the database representation of research data and corresponding domain knowledge of clinical research concepts. In this paper, we present a set of methodologies for undertaking ontology-based specification of temporal information, and discuss their application to the verification of protocol-specific temporal constraints among clinical trial activities. Our approach allows knowledge-level temporal constraints to be evaluated against <b>operational</b> trial <b>data</b> <b>stored</b> in relational databases. We show how the Semantic Web ontology and rule languages OWL and SWRL, respectively, can support tools for research data management that automatically integrate low-level representations of relational data with high-level domain concepts used in study design. Keywords: Clinical trials, temporal constraints, knowledge-based systems...|$|R
40|$|In data {{selection}} {{the data}} most {{relevant to the}} problem at hand is selected, leaving out redundant data. In the cleaning process, the obvious aws in the data are corrected. In the coding process, data is coded such {{that it can be}} used in data mining algorithms. In the data mining process, the actual inference takes place. In the reporting process the results are of course reported, usually in a visually attractive way. KDD is closely connected to an other important development which is called data warehousing. A data warehouse is a central storage facility in which <b>operational</b> <b>data</b> is <b>stored.</b> These warehouses usually contain enormous amounts of data. This gives a new view on information management. The combination of KDD and data warehousing 1 allows companies to use the knowledge hidden in the data as a production factor, instead of just support for their operations. A usual problem in the eld of data mining is that the combination of databases (often called fusion"...|$|R
40|$|Business Intelligence (BI), {{which is}} the process of collecting, analyzing, and {{transforming}} data using Data Warehouse (DW) is seen as one of the growing approaches to provide meaningful information for the Malaysian Ministry of Higher Education (MOHE). MOHE is responsible for managing various activities to encourage graduate entrepreneurs to venture into the businesses and ensure that the country has many successful entrepreneurs. Therefore, systematic and accurate information needs to be available for planning, implementing, and monitoring entrepreneurs' performances. This paper proposes the modeling and designing of the graduate entrepreneur profile system-Intelligent Profile Analysis Graduate Entrepreneur (iPAGE) using the BI and data integration approach. Two main methodologies were used namely: Conceptual Design Model <b>Operational</b> <b>Data</b> <b>Store</b> (CoDMODS) and Rapid Application Development (RAD) to model and design this system. The iPAGE was validated and evaluated by users, entrepreneur’s personnel and DW experts. Indeed, the approach will be used to benchmark the development of an entrepreneurial information system in the future...|$|E
40|$|A data {{warehouse}} (DW) is a database used for reporting Paper describes Object Oriented Data Warehousing using a narrower compassed data model. The data is offloaded from the operational systems for reporting. The data may pass through an <b>operational</b> <b>data</b> <b>store</b> for additional operations {{before it is}} used in the Data warehousing for reporting. An Object Oriented Data Warehouse system includes a {{data warehouse}} and underlying data sources. A narrower compassed model is used for storing the data in Object Oriented Data warehouse. Oracle 10 g Language is specifically used for the programming. Object Oriented Data Warehousing provides complex objects which include multiple atomic types and user defined object types. The steadiness between the data warehouse and the source databases is maintained by certain algorithms such as insertion, deletion and update. In the past research work on data warehouse was primarily focused on relational data models. The concept of object oriented data warehousing is introduced and implementation of Object oriented data warehouse in oracle 10 g. Data model will form new classes according to the definition of views such that query performance & security can be improved...|$|E
40|$|The {{increase}} use {{of information}} in Malaysia Ministry of Higher Education (MOHE) is resulted from the process of huge and complex data in several stages and at different locations. This situation leads to the difficulties of data management and problematic usage for decision making. Business Intelligence (BI), which is, the process of collecting, analyzing, and transforming data using Data Warehouse (DW) is seen {{as one of the}} growing approaches to provide meaningful information. The MOHE is responsible for managing various activities to promote the graduate entrepreneurs to venture into the business and ensure the country has many successful entrepreneurs. Therefore, systematic and accurate information needs to be available for planning, implementation, and monitoring entrepreneurs' performances. This paper proposes the modeling and designing of the graduate entrepreneur profile application called Intelligent Profile Analysis Graduate Entrepreneur (iPAGE) using BI technologies and supporting by a DW approach. Two main methodologies were used namely: Requirements Centric <b>Operational</b> <b>Data</b> <b>Store</b> (ReCODS) and Rapid Application Development (RAD) to develop this system. The iPAGE system was validated and evaluated by iPAGE users and DW experts. It is used to be a guideline for the development of an entrepreneur information system in the future...|$|E
40|$|Approved {{for public}} release, {{distribution}} unlimitedIn modern combat operations today {{the display of}} <b>operational</b> <b>data</b> is still tied to stove-piped and proprietary systems and software. Additionally, combat systems are still using 2 D displays of the battlefield in order to reflect {{a picture of the}} battlefield to the warfighter. Stepping away from stove-piped and proprietary systems and reflecting a 3 D picture of the battlefield is the direction that this thesis research explores. Research is conducted to explore technologies needed to provide operational forces with web-based 3 D visualizations of <b>operational</b> <b>data.</b> Technologies used in this research are XML, XSLT, JAVA, X 3 D and VRML. A prototype application is developed that allows for the 3 D display of <b>operational</b> <b>data.</b> The research demonstrates how <b>operational</b> <b>data</b> can be <b>stored</b> in a database and accessed through a web-based 3 D representation of the area of operation. Data sets used in this prototype include Digital Terrain Elevation <b>Data</b> and <b>operational</b> planning <b>data.</b> Access to the data is provided through a web-based interface. The web-based view of the data provides both 2 D and 3 D views. This research shows that current open source technology can provide the warfighter with a web-based 3 D view of the battlefield. Captain, United States Marine Corp...|$|R
40|$|Data {{warehouse}} {{is one of}} {{the most}} rapidly growing areas in management information systems. With this approach, data for EIS and DSS applications is separated from <b>operational</b> <b>data</b> and <b>stored</b> in a separate database called a data warehouse. Some of the advantages of this approach are improved performance, better data quality, and the ability to consolidate and summarize data from heterogeneous legacy systems. A data warehouse is part of a larger infrastructure that includes legacy data sources, external data sources, a repository data acquisition software, and user interface and related analytical tools. A powerful form of data analysis, called multidimensional data analysis, is often performed by users of a data warehouse. Data warehouses can be organized into two-tier or three-tier client/server systems. Despite the complexity of the data warehouse environment, little academic research has been performed in this area. This paper identifies a number of issues that arise in the context of developing and using a data warehouse. It develops a proposed research model to determine the impact of factors such as organizational factors, warehouse infrastructure, and management support on user satisfaction and development characteristics of a data warehouse...|$|R
40|$|An {{integrated}} reporting solutionthat {{addresses the}} needs ofproducing, distributing, and managing {{the reports of}} a diverse enterprise is now available through CONNX. SolutionsIQ’s Managed Report-ing Environment (MRE) solves many problems facing IS, Operations, and Strategic Managers today. In the past, <b>operational</b> <b>data</b> was <b>stored</b> throughout an enterprise in various operational systems in separate islands of data. This made {{it very difficult for}} end users to get the information they needed for decision making. If an enterprise was distributed amongst different states even the distribution of information became difficult. In combination with Crystal Info, MS SQL Server, and CONNX, SolutionsIQ’s MRE combines, moves, cleans, <b>stores,</b> and presents <b>operational</b> <b>data</b> in a standardized report format that is available when and where it is needed. Which data is moved and what business rules are followed is customized to individual customer needs during the MRE implementation. MRE helps the enterprise user by removing the need to ask “Which sales report? ” or “Are those numbers real?” Now end users can rely on operational reports because they can be certain that the data contained in the reports is consistent, up to date, and standard-ized. Users can also access the reports and manage the MRE through a stan-dard Web browser, thus removing the need for complex desktop installa-tions. CONNX is the lynchpin in the whole process by allowing access to the data in the first place. MRE solution implementations are provided by the SolutionsIQ Software Consulting/ Development Division. Tailored MRE packages are available that include training, setting up a data warehouse, and building mission critical reports...|$|R
