0|10000|Public
30|$|In summary, <b>our</b> <b>results</b> suggest <b>that</b> <b>achieving</b> {{an optimal}} {{consensus}} building process in collaboration systems requires an appropriate balance {{between those two}} factors.|$|R
30|$|In addition, {{when the}} quantifiers were induced using the SVM classifier, the {{differences}} between SQSI and SQSI-IS were greater. However, a statistical difference was not observed among all variations of SQSI-IS for all instance selection techniques. Adding {{a large number of}} datasets, the similarity between the SQSI and SQSI will be more evident. In general, <b>our</b> <b>results</b> suggest <b>that</b> <b>achieving</b> a confident quantification rate in scenarios with label scarcity is possible, especially when instance selection techniques are used.|$|R
30|$|We {{conduct a}} nuanced {{evaluation}} of HF-IHU {{over a large}} Twitter data set. We compare HF-IHU against several popular schemes, including k-nearest neighbors using Cosine similarity, k-popularity, and Naïve Bayes. <b>Our</b> <b>results</b> show <b>that</b> HF-IHU <b>achieves</b> substantially higher recall than the other schemes and is resistant to retweets.|$|R
40|$|In {{this paper}} we test {{the degree of}} tranfer of {{learning}} between two similar, n-player coordination games with Pareto-ranked equilibria with fixed cohorts. <b>Our</b> <b>results</b> show <b>that</b> <b>achieve</b> the payoff-dominants equilibrium in one game are then able to transfer this mutual best response outcome to the new game. However, cohorts which do not converge to the best outcome {{in the first game}} are not able to recover from coordination failure in the new game. Furthermore, the effects of tranfer on collective coordination change depending on the sequence with which the two games are played. -...|$|R
40|$|Abstract—We {{consider}} the protocol overhead of meeting an expected delay constraint in sending a bursty source over a binary erasure channel with feedback. In contrast to related work on stability and delay for medium-access control, {{we focus on}} physical layer aspects of the problem by jointly considering queuing and message encoding for transmission over the channel. Our model necessitates the lossy encoding of source idle times, and we investigate {{the amount of information}} implicitly communicated by their efficient encoding. We show that the stable throughput region for large, but finite, delay is the same as without this additional constraint; however, <b>our</b> <b>results</b> suggest <b>that</b> <b>achieving</b> the same average delay requires a significant increase in channel bandwidth. Outer bounds on the achievable rate-delay region are also developed. I...|$|R
30|$|Rather than {{frequent}} reburning, {{which is}} difficult for local units to achieve and may increase opportunities for introduction and spread of exotic species, a gradual move to more historically accurate seasonal ranges for reburning {{may be more effective}} at sustaining low wildfire severity potential. Either early or late season prescribed burns may be used as initial treatment, for ease of control, to achieve some surface fuel and forest floor reductions, and limit mortality of larger fire resistant trees. But, similar to Battaglia et al. (2008), Ryan et al. (2013) and Higgins et al. (2015), <b>our</b> <b>results</b> suggest <b>that</b> <b>achieving</b> and maintaining low surface fuels, shallow forest floor fuels, and high ladder fuels may require increased reburn severity by burning in summer or earlier in fall when temperatures are higher and humidity is lower.|$|R
40|$|Building a {{dense and}} {{accurate}} environment model {{out of range}} image data faces problems like sensor noise, extensive memory consumption or computation time. We present an approach which reconstructs 3 D environments using a probabilistic occupancy grid in real-time. Operating on depth image pyramids speeds up computation time, whereas a weighted interpolation scheme between neighboring pyramid layers boosts accuracy. In our experiments we compare our method with a state-of-the-art mapping procedure. <b>Our</b> <b>results</b> demonstrate <b>that</b> we <b>achieve</b> better <b>results.</b> Finally, we present its viability by mapping a large indoor environment. 1...|$|R
40|$|International audienceAn {{alternative}} to classical fault-tolerant approaches for large-scale clusters is failure avoidance, {{by which the}} occurrence of a fault is predicted and a preventive measure is taken. We develop analytical performance models for two types of preventive measures: preventive checkpointing and preventive migration. We also develop an analytical model {{of the performance of}} a standard periodic checkpoint fault-tolerant approach. We instantiate these models for platform scenarios representative of current and future technology trends. We find that preventive migration is the better approach in the short term by orders of magnitude. However, in the longer term, both approaches have comparable merit with a marginal advantage for preventive checkpointing. We also find that standard non-prediction-based fault tolerance achieves poor scaling when compared to prediction-based failure avoidance, thereby demonstrating the importance of failure prediction capabilities. Finally, <b>our</b> <b>results</b> show <b>that</b> <b>achieving</b> good utilization in truly large-scale machines (e. g., 220 nodes) for parallel workloads will require more than the failure avoidance techniques evaluated in this work...|$|R
40|$|In Simultaneous Multithreaded {{architectures}} many separate threads {{are running}} concurrently and are sharing processor resources. However, competiton for resources can actually degrade overall performance. In this {{paper we propose}} a fetch gating policy that avoids issuing instructions if they are unlikely to belong to the correct path. This fetch policy, called agstall, is based on a dynamic branch classification mechanism. <b>Our</b> <b>results</b> show <b>that</b> agstall <b>achieves</b> better performance than icount, and reduces by up to 86 % the number of wrong-path instructions executed. Moreover, we show that agstall outperforms an instruction fetch gating policy based on confidence estimation...|$|R
40|$|This paper {{describes}} {{a study that}} assessed the performance implications of aligning information technology (IT) strategy to overall business strategy {{across a variety of}} health care organization (HCO) structures. We obtained survey results from senior executives of 178 hospitals to identify key configurations of IT strategic practices, business strategy and HCO structures. Using K- means cluster analysis, we identified which business strategies correlate strongly with certain IT strategy types. <b>Our</b> <b>results</b> indicate <b>that</b> HCOs <b>achieve</b> superior performance through unique combinations of business and IT strategy, suggesting that correctly aligning these strategies is a critical decision for healthcare organizations...|$|R
40|$|International audienceWe present <b>our</b> {{preliminary}} <b>results</b> on {{the lowest}} moment hxi of quark distribution functions of the pion using two flavor dynamical simulations with Wilson twisted mass fermions at maximal twist. The calculation is done {{in a range of}} pion masses from 300 to 500 MeV. A stochastic source method is used to reduce inversions in calculating propagators. Finite volume effects at the lowest quark mass are examined by using two different lattice volumes. <b>Our</b> <b>results</b> show <b>that</b> we <b>achieve</b> statistical errors of only a few percent. We plan to compute renormalization constants non-perturbatively and extend the calculation to two more lattice spacings and to the nucleons...|$|R
40|$|This paper {{presents}} {{the design and}} implementation of a custombuilt event processing engine called BlueBay developed for live monitoring of soccer games. We experimentally evaluated our system using a real workload and report on its performance. <b>Our</b> <b>results</b> indicate <b>that</b> BlueBay <b>achieves</b> a throughput of up to 790 k events per second, therefore processing the game’s input sensor stream about 60 times faster than real-time. In addition to our custom implementation, we also investigated the applicability of offthe-shelf general-purpose event processing engines to address the soccer monitoring problem. This effort resulted in two additional and fully functional implementations based on Esper and Storm. 1...|$|R
40|$|International audienceWe {{consider}} {{in this paper}} an important Quality of Experience (QoE) indicator in cellular networks that is reneging of users due to impatience. We specifically consider a cell under heavy load conditions, modeled as a multiclass Processor Sharing system, and compute the reneging probability by using a fluid limit analysis. In order to enhance the user QoE, we propose a radio resource allocation control scheme that minimizes the global reneging rates. This control scheme {{is based on the}} α-fair scheduling framework and adapts the scheduler parameter depending on the traffic load. While the proposed scheme is simple, <b>our</b> <b>results</b> show <b>that</b> it <b>achieves</b> important performance gains...|$|R
40|$|In {{the medical}} imaging field, we need fast {{deformable}} registration methods especially in intra-operative settings characterized by their time-critical applications. Image registration studies {{which are based}} on Graphics Processing Units (GPUs) provide fast implementations. However, {{only a small number of}} these GPU-based studies concentrate on deformable registration. We implemented Demons, a widely used deformable image registration algorithm, on NVIDIA’s Quadro FX 5600 GPU with the Compute Unified Device Architecture (CUDA) programming environment. Using our code, we registered 3 D CT lung images of patients. <b>Our</b> <b>results</b> show <b>that</b> we <b>achieved</b> the fastest runtime among the available GPU-based Demons implementations. Additionally, regardless of the given datase...|$|R
40|$|This {{paper is}} {{concerned}} with fully distributed reputation-based mechanisms that improve security in MANETS. We introduce a number of optimisations to the current reputation schemes used in MANETs such as selective deviation tests and adaptive expiration timer that aim to deal with congestion and quick convergence. We use two different centrality measures for evaluation of the individual trust claims and resolving the aggregated ones. We design and build our prototype over AODV and test it in the NS- 2 {{in the presence of}} variable black hole attacks in highly mobile and sparse networks. <b>Our</b> <b>results</b> show <b>that</b> we <b>achieve</b> increased throughput while delay and jitter decrease and converge to AODV...|$|R
40|$|Abstract—An {{alternative}} to classical fault-tolerant approaches for large-scale clusters is failure avoidance, {{by which the}} occurrence of a fault is predicted and a preventive measure is taken. We develop analytical performance models for two types of preventive measures: preventive checkpointing and preventive migration. We also develop an analytical model {{of the performance of}} a standard periodic checkpoint fault-tolerant approach. We instantiate these models for platform scenarios representative of current and future technology trends. We find that preventive migration is the better approach in the short term by orders of magnitude. However, in the longer term, both approaches have comparable merit with a marginal advantage for preventive checkpointing. We also find that standard non-prediction-based fault tolerance achieves poor scaling when compared to prediction-based failure avoidance, thereby demonstrating the importance of failure prediction capabilities. Finally, <b>our</b> <b>results</b> show <b>that</b> <b>achieving</b> good utilization in truly large-scale machines (e. g., 2 20 nodes) for parallel workloads will require more than the failure avoidance techniques evaluated in this work. Index Terms—failure prediction; checkpointing; migration; parallel jobs; I...|$|R
40|$|Treating shallow parsing as {{part-of-speech tagging}} yields results {{comparable}} with other, more elaborate approaches. Using the CoNLL 2000 training and testing material, our best model had an accuracy of 94 : 88 %, {{with an overall}} FB 1 score of 91 : 94 %. The individual FB 1 scores for NPs were 92 : 19 %, VPs 92 : 70 % and PPs 96 : 69 %. 1 Introduction Shallow parsing has received {{a reasonable amount of}} attention {{in the last few years}} (for example (Ramshaw and Marcus, 1995)). In this paper, instead of modifying some existing technique, or else proposing an entirely new approach, we decided to build a shallow parser using an off-the-shelf part-of-speech (POS) tagger. We deliberately did not modify the POS tagger 's internal operation in any way. <b>Our</b> <b>results</b> suggested <b>that</b> <b>achieving</b> reasonable shallowparsing performance does not in general require anything more elaborate than a simple POS tagger. However, an error analysis suggested the existence of a small set of constructs that are not so easily charact [...] ...|$|R
40|$|We {{describe}} {{a set of}} evaluation techniques applied to domain tuning of bilingual lexicons for machine translation. Our overall objective is to translate a domain-specific document {{in a foreign language}} (in this case, Chinese) to English. First, we perform an intrinsic evaluation of the effectiveness of our domain-tuning techniques by comparing our domain-tuned lexicon to a manually constructed domain-specific bilingual termlist. <b>Our</b> <b>results</b> indicate <b>that</b> we <b>achieve</b> 66 % recall and 95 % precision with respect to a human-derived gold standard. Next, an extrinsic evaluation demonstrates that our domain-tuned lexicon improves the Bleu scores 50 % over a statistical system [...] -with a smaller improvement when the system is trained on a uniformly-weighted dictionary...|$|R
40|$|Flooding is an {{important}} communication primitive in mobile ad-hoc networks and {{also serves as a}} building block for more complex protocols such as routing protocols. In this paper, we propose a novel approach to flooding, which relies on proactive compensation packets periodically broadcast by every node. The compensation packets are constructed from dropped data packets, based on techniques borrowed from forward error correction. Since our approach does not rely on proactive neighbor discovery and network overlays it is resilient to mobility. We evaluate the implementation of Mistral through simulation and compare its performance and overhead to purely probabilistic flooding. <b>Our</b> <b>results</b> show <b>that</b> Mistral <b>achieves</b> a significantly higher node coverage with comparable overhead...|$|R
40|$|Good policy ” and “good luck ” {{have been}} {{identified}} as two of the possible drivers of the “Great Moderation, ” but their relative importance is still widely debated. This paper investigates the role played by equilibrium selection under indeterminacy in the assessment of their relative merits. We contrast the outcomes of counterfactual simulations conditional on the “continuity ” selection strategy—largely exploited by the literature—with those obtained with a novel “sign restriction ” based strategy. <b>Our</b> <b>results</b> suggest <b>that</b> conclusions <b>achieved</b> under “continuity ” are not necessarily robust to the selection of different—still economically sensible—equilibria. According to our simulations, the switch to a hawkish systematic monetary policy may very well induce an increase in output volatility. Hence, our sign restriction–selection strategy “resurrects ” the inflation–output policy tradeoff...|$|R
40|$|Good policy” and “good luck” {{have been}} {{identified}} as two of the possible drivers of the “Great Moderation,” but their relative importance is still widely debated. This paper investigates the role played by equilibrium selection under indeterminacy in the assessment of their relative merits. We contrast the outcomes of counterfactual simulations conditional on the “continuity” selection strategy—largely exploited by the literature—with those obtained with a novel “sign restriction” based strategy. <b>Our</b> <b>results</b> suggest <b>that</b> conclusions <b>achieved</b> under “continuity” are not necessarily robust to the selection of different—still economically sensible—equilibria. According to our simulations, the switch to a hawkish systematic monetary policy may very well induce an increase in output volatility. Hence, our sign restriction–selection strategy “resurrects” the inflation–output policy tradeoff...|$|R
40|$|In a large dynamic network, {{data can}} be shared and {{accessed}} in an easy manner among researchers from various locations and accessed provided an efficient protocol to manage the data. A protocol is needed to ensure the data is consistent and accessible. In this paper, we introduced a new protocol, named Clusteredbased Data Sharing (CDS) for data sharing in a large dynamic network such as grid computing by using Clustered-based techniques to improve the accessibility. This is due to Clustered-based techniques can ensure that the data is accessible {{among the members of}} the grid. To evaluate our model, we developed a simulation model in Java. <b>Our</b> <b>results</b> show <b>that</b> CDS <b>achieves</b> high data accessibility even when the number of nodes or sites are increasing...|$|R
40|$|We present <b>our</b> {{preliminary}} <b>results</b> on {{the lowest}} moment of quark distribution functions of the pion using two flavor dynamical simulations with Wilson twisted mass fermions at maximal twist. The calculation is done {{in a range of}} pion masses from 300 to 500 MeV. A stochastic source method is used to reduce inversions in calculating propagators. Finite volume effects at the lowest quark mass are examined by using two different lattice volumes. <b>Our</b> <b>results</b> show <b>that</b> we <b>achieve</b> statistical errors of only a few percent. We plan to compute renormalization constants non-perturbatively and extend the calculation to two more lattice spacings and to the nucleons. Comment: 7 pages, 3 figures. Talk given at the XXV International Symposium on Lattice Field Theory, July 30 - August 4 2007, Regensburg, German...|$|R
40|$|We present first strong {{observational}} {{evidence that}} the X-ray cool-core bias or the apparent bias in the abundance of relaxed clusters is absent in our REFLEX volume-limited sample (ReVols). We show that these previously observed biases are due to the survey selection method such as for an flux-limited survey, and are not due to the inherent nature of X-ray selection. We also find that the X-ray luminosity distributions of clusters for the relaxed and for the disturbed clusters are distinct and a displacement of approximately 60 per cent is required to match two distributions. <b>Our</b> <b>results</b> suggest <b>that</b> to <b>achieve</b> more precise scaling relation one may {{need to take the}} morphology of clusters and their fractional abundance into account. Comment: A&A, 606, L 4, 4 pages, 3 figure...|$|R
40|$|Problem statement: In a large dynamic network, {{data can}} be shared and {{accessed}} in an easy manner among researchers from various locations and access provided an efficient protocol to manage the data. Approach: A protocol is needed to ensure the data is consistent and accessible. Results: In this study, we introduced a new protocol, named Clustered-based Data Sharing (CDS) for data sharing in a large dynamic network such as grid computing by using Clustered-based techniques to improve the accessibility. This is due to Clustered-based techniques can ensure that the data is accessible {{among the members of}} the grid. To evaluate our model, we developed a simulation model in Java. Conclusion/Recommendations: <b>Our</b> <b>results</b> show <b>that</b> CDS <b>achieves</b> high data accessibility even when the number of nodes or sites is increasing...|$|R
40|$|Augmented reality (AR) is {{the mixing}} of computer-generated stimuli with {{real-world}} stimuli. In this paper, we present results from a controlled, empirical study comparing three ways of delivering spatialized audio for AR applications: a speaker array, headphones, and a bone-conduction headset. Analogous to optical-see-through AR in the visual domain, Hear-Through AR allows users to receive computer-generated audio using the bone-conduction headset, and real-world audio using their unoccluded ears. <b>Our</b> <b>results</b> show <b>that</b> subjects <b>achieved</b> the best accuracy using a speaker array physically located around the listener when stationary sounds were played, but {{that there was no}} difference in accuracy between the speaker array and the bone-conduction device for sounds that were moving, and that both devices outperformed standard headphones for moving sounds. Subjective comments by subjects following the experiment support this performance data...|$|R
40|$|AbstractThis paper {{considers}} {{the relationship between}} family background, academic achievement {{in high school and}} access to high-status postsecondary institutions in three developed countries (Australia, England and the United States). We begin by estimating the unconditional association between family background and access to a high status university, before examining how this relationship changes once academic achievement in high school is controlled. <b>Our</b> <b>results</b> suggest <b>that</b> high <b>achieving</b> disadvantaged children are much less likely to enter a high-status college than their more advantaged peers, and that the magnitude of this socio-economic gradient is broadly similar across these three countries. However, we also find that socio-economic inequality in access to high-status private US colleges is much more pronounced than access to their public sector counterparts (both within the US and when compared overseas) ...|$|R
40|$|A {{key issue}} {{in the design of}} source-based {{multicast}} congestion control schemes is how to aggregate loss indications from multiple receivers into a single rate control decision at the source. Such aggregation entails filtering out a portion of the loss indications received by the source, and then using the remaining for rate adjustments. In this paper, we first propose a set of goals guiding the design of loss indication filters. We then present a novel loss indication filtering approach, the Linear Proportional Response (LPR) approach. Analysis and simulation is used to compare LPR to two well-known approaches [...] the Random Listening Algorithm (RLA) ([1]), and the Worst Estimate-Based Tracking (WET) [2] approach. <b>Our</b> <b>results</b> indicate <b>that</b> LPR <b>achieves</b> a desirable tradeoff between stability and response, thereby making it more suitable than WET and RLA for deployment in an Internet-like environment...|$|R
40|$|Abstract — Wireless sensor {{networks}} {{are characterized by}} limited energy resources. To conserve energy, application-specific aggregation (fusion) of data reports from multiple sensors can be beneficial in {{reducing the amount of}} data flowing over the network. Furthermore, controlling the topology by scheduling the activity of nodes between active and sleep modes has often been used to uniformly distribute the energy consumption among all nodes by de-synchronizing their activities. We present an integrated analytical model to study the joint performance of in-network aggregation and topology control. We define performance metrics that capture the tradeoffs among delay, energy, and fidelity of the aggregation. <b>Our</b> <b>results</b> indicate <b>that</b> to <b>achieve</b> high fidelity levels under medium to high event reporting load, shorter and fatter aggregation/routing trees (toward the sink) offer the best delay-energy tradeoff as long as topology control is well coordinated with routing. I...|$|R
40|$|Main {{memory has}} become one of the largest {{contributors}} to overall energy consumption and offers many opportunities for power/energy reduction. In this paper, we propose a new memory organization, called {em Power-Aware Cached-DRAM} (PA-CDRAM), that integrates a moderately sized cache directly into a memory device. We use this cache to turn a memory bank off immediately after a memory access to reduce energy consumption. While other work has used CDRAM to improve memory performance, we modify CDRAM to reduce energy consumption. In this paper, we describe our memory organization and describe the challenges for achieving low energy consumption and how to address them. We evaluate the approach using a cycle accurate processor and memory simulator. <b>Our</b> <b>results</b> show <b>that</b> PA-CDRAM <b>achieves</b> an average 28 % improvement in the energy-delay product when compared to a time-out power management technique...|$|R
40|$|Existing gesture-recognition systems consume signifi-cant {{power and}} {{computational}} resources that limit how {{they may be}} used in low-end devices. We introduce AllSee, the first gesture-recognition system that can op-erate on a range of computing devices including those with no batteries. AllSee consumes three to four or-ders of magnitude lower power than state-of-the-art sys-tems and can enable always-on gesture recognition for smartphones and tablets. It extracts gesture information from existing wireless signals (e. g., TV transmissions), but does not incur the power and computational over-heads of prior wireless approaches. We build AllSee prototypes that can recognize gestures on RFID tags and power-harvesting sensors. We also integrate our hard-ware with an off-the-shelf Nexus S phone and demon-strate gesture recognition in through-the-pocket scenar-ios. <b>Our</b> <b>results</b> show <b>that</b> AllSee <b>achieves</b> classification accuracies as high as 97 % over a set of eight gestures. ...|$|R
40|$|In this paper, we {{introduce}} Ant-MinerMA {{to tackle}} mixed-attribute classification problems. Most classification problems involve continuous, ordinal and categorical attributes. The majority of Ant Colony Optimization (ACO) classification algorithms have {{the limitation of}} being able to handle categorical attributes only, with few exceptions that use a discretisation procedure when handling continuous attributes either in a preprocessing stage or during the rule creation. Using a solution archive as a pheromone model, inspired by the ACO for mixed-variable optimization (ACO-MV), we eliminate the need for a discretisation procedure and attributes can be treated directly as continuous, ordinal, or categorical. We compared the proposed Ant-MinerMA against cAnt-Miner, an ACO-based classification algorithm that uses a discretisation procedure in the rule construction process. <b>Our</b> <b>results</b> show <b>that</b> Ant-MinerMA <b>achieved</b> significant improvements on computational time due to the elimination of the discretisation procedure without affecting the predictive performance...|$|R
40|$|In {{this paper}} it is {{proposed}} a new wavelength converter placement scheme called Adaptive First Load Priority, useful for designing and planning of optical networks with sparsepartial wavelength conversion architecture. The adaptive FLP {{is an extension}} of the FLP (First Load Priority) placement scheme proposed by authors in a previous work [11]. It was conceived with the aim of decreasing the number of WCRs (Wavelength Converter Router) used by FLP scheme. Initially, we provide a performance evaluation of FLP working in different scenarios [...] Comparing to others wavelength converter placement schemes proposed in the literature like MBPF [9], TOT [8], XC [5] and to partial wavelength conversion, <b>our</b> <b>results</b> show <b>that</b> FLP <b>achieves</b> better performance in all scenarios studied, very close to <b>that</b> one <b>achieved</b> with a full-complete wavelength conversion architecture. Thus, we introduce the Adaptive FLP scheme comparing its performance to XC and FLP schemes. Results achieve...|$|R
40|$|Bayesian {{networks}} (BNs) {{are rapidly}} becoming a leading tool in applied Artificial Intelligence (AI). BNs may be built by eliciting expert knowledge or learned via causal discovery programs. Both approaches have limitations: expert elicitation is expensive, time-consuming and relies on experts having full domain knowledge, while discovery is often ineffective given small or noisy datasets. A hybrid {{approach is to}} incorporate prior information elicited from experts into the causal discovery process. We present several ways of using expert information as prior probabilities in the CaMML causal discovery program. We compare CaMML with and without prior information {{to a variety of}} other BN learners. <b>Our</b> <b>results</b> show <b>that</b> CaMML <b>achieves</b> comparable <b>results</b> without prior information and superior performance with prior information. We also present <b>results</b> showing <b>that</b> CaMML is well calibrated to variations in the expert’s skill and confidence. Keywords: Causal discovery, Bayesian networks, CaMML. ...|$|R
30|$|We {{focus on}} the {{optimization}} of real-time multimedia transmission over 802.11 -based ad hoc networks. In particular, we propose a simple and efficient cross-layer mechanism that considers both the channel conditions and characteristics of the media for dynamically selecting the transmission mode. This mechanism called media-oriented rate selection algorithm (MORSA) targets loss-tolerant applications such as VoD {{that do not require}} full reliable transmission. We provide an evaluation of this mechanism for MANETs using simulations with NS and analyze the video quality obtained with a fine-grain scalable video encoder based on a motion-compensated spatiotemporal wavelet transform. <b>Our</b> <b>results</b> show <b>that</b> MORSA <b>achieves</b> up to 4 Mbps increase in throughput and that the routing overhead decreases significantly. Transmission of a sample video flow over an 802.11 a wireless channel has been evaluated with MORSA. Important improvement is observed in throughput, latency, and jitter while keeping a good level of video quality.|$|R
40|$|Wireless sensor {{networks}} {{are characterized by}} limited energy resources. To conserve energy, application-specific aggregation (fusion) of data reports from multiple sensors can be beneficial in {{reducing the amount of}} data flowing over the network. Furthermore, controlling the topology by scheduling the activity of nodes between active and sleep modes has often been used to uniformly distribute the energy consumption among all nodes by de-synchronizing their activities. We present an integrated analytical model to study the joint performance of in-network aggregation and topology control. We define performance metrics that capture the tradeoffs among delay, energy, and fidelity of the aggregation. <b>Our</b> <b>results</b> indicate <b>that</b> to <b>achieve</b> high fidelity levels under medium to high event reporting load, shorter and fatter aggregation/routing trees (toward the sink) offer the best delay-energy tradeoff as long as topology control is well coordinated with routing. National Science Foundation (ANI- 0095988, ANI- 9986397, EIA- 0202067, ITR ANI- 0205294...|$|R
