432|728|Public
25|$|Adaptive estimation. If {{we assume}} that error terms are {{independent}} from the regressors , the <b>optimal</b> <b>estimator</b> is the 2-step MLE, where {{the first step is}} used to non-parametrically estimate the distribution of the error term.|$|E
2500|$|Note {{that the}} PDF at the {{previous}} timestep is inductively {{assumed to be}} the estimated state and covariance. This is justified because, as an <b>optimal</b> <b>estimator,</b> the Kalman filter makes best use of the measurements, therefore the PDF for [...] given the measurements [...] is the Kalman filter estimate.|$|E
5000|$|... {{choosing}} the <b>optimal</b> <b>estimator</b> by minimising a [...] "norm" [...] which measures {{the size of}} the covariance matrix of the estimators.|$|E
40|$|AbstractThe {{continuity}} and uniqueness properties of <b>optimal</b> <b>estimators</b> {{with respect to}} data are considered for different regularizations. It is found {{that there is a}} weak stability for <b>optimal</b> <b>estimators</b> as set-valued mappings under a weak regularization. For stronger regularization results are obtained that give stability in stronger topologies and the finiteness of the set of <b>optimal</b> <b>estimators.</b> Finally, we give conditions that imply uniqueness of <b>optimal</b> <b>estimators</b> and Lipschitz continuity with respect to data...|$|R
40|$|<b>Optimal</b> <b>estimators</b> are {{developed}} for computation of suspended-sediment concentrations in streams. The estimators are {{a function of}} parameters, computed by use of generalized least squares, which simultaneously account for effects of streamflow, seasonal variations in average sediment concentrations, a dynamic error component, and the uncertainty in concentration measurements. The parameters are used in a Kalman filter for on-line estimation and an associated smoother for off-line estimation of suspended-sediment concentrations. The accuracies of the <b>optimal</b> <b>estimators</b> are compared with alternative time-averaging interpolators and flow-weighting regression estimators by use of long-term daily-mean suspended-sediment concentration and streamflow data from 10 sites within the United States. For sampling intervals from 3 to 48 days, the standard errors of on-line and off-line <b>optimal</b> <b>estimators</b> ranged from 52 Ð 7 to 107 %, and from 39 Ð 5 to 93 Ð 0 %, respectively. The corresponding standard errors of linear and cubic-spline interpolators ranged from 48 Ð 8 to 158 %, and from 50 Ð 6 to 176 %, respectively. The standard errors of simple and multiple regression estimators, which did not vary with the sampling interval, were 124 and 105 %, respectively. Thus, the <b>optimal</b> off-line <b>estimator</b> (Kalman smoother) had the lowest error characteristics of those evaluated. Because suspended-sediment concentrations are typically measured at less than 3 -day intervals, use of <b>optimal</b> <b>estimators</b> will likely result in significant improvements in the accuracy of continuous suspended-sediment concentration records. Additional research on the integration of direct suspended-sediment concentration measurements and <b>optimal</b> <b>estimators</b> applied at hourly or shorter intervals is needed. KEY WORDS suspended sediments; optimal estimation; Kalman filtering; computation; flux; load...|$|R
40|$|We develop Hachemeister's {{regression}} model 111 credlbihty theory (without proofs) and indicate how the involved structural parameters {{can be estimated}} from the observable variables (with proofs for the simple results and those not yet published). Large famlhes of unbiased estinaators are available. From the practical viewpoint his is rather a handicap because it creates the problem to decide what estimators actually to use. In order to fix <b>optimal</b> <b>estimators,</b> we adopt the small-sample critermn of minimum-variance But in the research for general solutmns three kinds of difficulties arise. (i) The calculations become too lengthy. (fi) The <b>optimal</b> <b>estimators</b> depend {{on some of the}} parameters to be esti-mated (Then we call them pseudo-estimators). (ill) The <b>optimal</b> <b>estimators</b> depend on new structural parameters defined in terms of fourth-order moments Only a compromise allows to cope with this reahty. Sltuatmn (ill) create...|$|R
5000|$|Quite clearly, the {{resulting}} <b>optimal</b> <b>estimator</b> [...] is then simply {{given by the}} PCR estimator [...] based on the first [...] principal components.|$|E
5000|$|The {{orthogonality}} principle: When [...] is a scalar, an estimator constrained to be {{of certain}} form [...] is an <b>optimal</b> <b>estimator,</b> i.e. [...] {{if and only if}} ...|$|E
5000|$|This is {{identical}} to (1), except that [...] {{has been replaced by}} [...] Thus, the expression minimizing is given by , so that the <b>optimal</b> <b>estimator</b> has the form ...|$|E
30|$|There {{is no need}} to use <b>optimal</b> <b>estimators</b> in many applications. UFIR {{structures}} that ignore noise statistics and initial estimation error statistics are able to produce acceptable suboptimal estimates.|$|R
40|$|In many {{industrial}} {{cases it}} is not feasible to measure primary outputs, e. g. product quality, from production processes on-line. It is thus of interest to estimate such outputs from known process inputs and secondary process measurements. In an earlier paper it is shown that <b>optimal</b> <b>estimators</b> can be identified from data recorded during an informative experiment, with the primary outputs sampled at the same high rate as the inputs and secondary outputs. In the present paper it is shown that <b>optimal</b> <b>estimators</b> {{can also be found}} from data where the primary outputs are sampled at a low and possibly irregular rate...|$|R
5000|$|These {{analyses}} {{reveal the}} incredible and disconcerting lack of tracking literature that addresses fundamentals (e.g., optimal IMM interpolation, [...] boundary conditions, and acceleration-to-noise ratio) and comparisons with standard benchmarks (e.g.; 2nd order, 3rd order, or other <b>optimal</b> <b>estimators).</b>|$|R
5000|$|Adaptive estimation. If {{we assume}} that error terms are {{independent}} from the regressors , the <b>optimal</b> <b>estimator</b> is the 2-step MLE, where {{the first step is}} used to non-parametrically estimate the distribution of the error term.|$|E
5000|$|Note {{that the}} PDF at the {{previous}} timestep is inductively {{assumed to be}} the estimated state and covariance. This is justified because, as an <b>optimal</b> <b>estimator,</b> the Kalman filter makes best use of the measurements, therefore the PDF for [...] given the measurements [...] is the Kalman filter estimate.|$|E
5000|$|Unfortunately {{in general}} the risk cannot be minimized, since {{it depends on the}} unknown {{parameter}} [...] itself (If we knew what was the actual value of , we wouldn't need to estimate it). Therefore additional criteria for finding an <b>optimal</b> <b>estimator</b> in some sense are required. One such criterion is the minimax criteria.|$|E
40|$|The propensity-scoring-adjustment {{approach}} {{is commonly used}} to handle selection bias in survey sampling applications, including unit nonresponse and undercoverage. The propensity score is computed using auxiliary variables observed throughout the sample. We discuss some asymptotic properties of propensity-score-adjusted <b>estimators</b> and derive <b>optimal</b> <b>estimators</b> based on a regression model for the finite population. An <b>optimal</b> propensity-score-adjusted <b>estimator</b> can be implemented using an augmented propensity model. Variance estimation is discussed and the results from two simulation studies are presented...|$|R
40|$|We {{investigate}} {{the design of}} <b>optimal</b> state <b>estimators</b> for Markovian Jump Linear Systems. We consider that the output observations and the mode observations are affected by delays not necessarily identical. Our objective is to design <b>optimal</b> <b>estimators</b> for the current state, given current and past observations. We provide {{a solution to this}} paradigm by giving an <b>optimal</b> recursive <b>estimator</b> for the state, in the minimum mean square sense, and a finitely parameterized recursive scheme for computing the probability mass function of the current mode conditioned on the observed output. We also show that if the output delay is less then the one in observing the mode, then the optimal state estimation becomes nonlinear in the output observations. I...|$|R
40|$|We {{propose a}} class of bounded {{influence}} robust regression estimators with conditionally unbiased estimating functions given the design. <b>Optimal</b> <b>estimators</b> are found within this class. Applications are made to generalized linear models. An example applying logistic regression to food stamp data is discussed...|$|R
5000|$|Let [...] be an unknown, random, zero-mean, normally-distributed, wide-sense {{stationary}} process. We {{would like}} to estimate [...] from noisy measurements , where [...] is a noise process, uncorrelated with , which is likewise zero-mean, normally-distributed, and wide-sense stationary. Then, the <b>optimal</b> <b>estimator</b> is obtained by passing [...] through a linear filter whose frequency response is given bywhere [...] and [...] are the spectral densities of [...] and , respectively.|$|E
5000|$|Instead of [...] {{repeated}} measurements {{with one}} instrument, if the experimenter makes [...] {{of the same}} quantity with [...] different instruments with varying quality of measurements, {{then there is no}} reason to expect the different [...] to be the same. Some instruments could be more noisier than others. In the example of measuring the acceleration due to gravity, the different [...] "instruments" [...] could be measuring [...] from a simple pendulum, from analysing a projectile motion etc.The simple average is no longer an <b>optimal</b> <b>estimator,</b> since the error in [...] might actually exceed the error in the least noisy measurement if different measurements have very different errors. Instead of discarding the noisy measurements that increase the final error, the experimenter can combine all the measurements with appropriate weights so as to give more importance to the least noisy measurements and vice versa. Given the knowledge of , an <b>optimal</b> <b>estimator</b> to measure [...] would be a weighted mean of the measurements , for the particular choice of the weights [...] The variance of the estimator , which for the optimal choice of the weights become ...|$|E
50|$|In {{statistics}} and signal processing, the orthogonality principle {{is a necessary}} and sufficient condition for the optimality of a Bayesian estimator. Loosely stated, the orthogonality principle says that the error vector of the <b>optimal</b> <b>estimator</b> (in a mean square error sense) is orthogonal to any possible estimator. The orthogonality principle is most commonly stated for linear estimators, but more general formulations are possible. Since the principle is a necessary and sufficient condition for optimality, {{it can be used}} to find the minimum mean square error estimator.|$|E
40|$|AbstractThe {{estimation}} of spacially dependent flexural rigidity coefficients in Kirchhoff models is discussed using an output least-squares method in H 2 regularization. The regularity of <b>optimal</b> <b>estimators</b> is studied and these properties {{are used to}} develop an approximation theory. Several numerical examples are presented...|$|R
40|$|Optimality of estimators of a vector {{parameter}} {{in terms}} of the probability of the estimators being contained in suitable region(s) around the parameter point is defined. Conditions under which <b>optimal</b> <b>estimators</b> in the usual senses are also optimal in the above sense are investigated. Similar laws maximum probability estimator invariant orthonormal bases...|$|R
40|$|AbstractThe authors {{consider}} a linear model Ey=Xβ, Cov yϵV, where >V is a quadratic subspace (a Jordan-algebra). Using this property it is shown how multiplication, computation of g-inverses, determination of a neutral element {{can be done}} more efficiently in V. Finally this technique is applied to obtain formulae for locally <b>optimal</b> <b>estimators</b> and Bayesian estimators...|$|R
5000|$|Unlike its linear counterpart, the {{extended}} Kalman filter {{in general is}} not an <b>optimal</b> <b>estimator</b> (of course it is optimal if the measurement and the state transition model are both linear, as in that case {{the extended}} Kalman filter {{is identical to the}} regular one). In addition, if the initial estimate of the state is wrong, or if the process is modeled incorrectly, the filter may quickly diverge, owing to its linearization. Another problem with the extended Kalman filter is that the estimated covariance matrix tends to underestimate the true covariance matrix and therefore risks becoming inconsistent in the statistical sense without the addition of [...] "stabilising noise" ...|$|E
50|$|If the {{experimental}} errors, , are uncorrelated, have {{a mean of}} zero and a constant variance, , the Gauss-Markov theorem states that the least-squares estimator, , has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or <b>optimal,</b> <b>estimator</b> of the parameters. Note particularly that this property is independent of the statistical distribution function of the errors. In other words, the distribution function of the errors {{need not be a}} normal distribution. However, for some probability distributions, {{there is no guarantee that}} the least-squares solution is even possible given the observations; still, in such cases it is the best estimator that is both linear and unbiased.|$|E
40|$|Abstract. We {{obtain an}} <b>optimal</b> <b>estimator</b> {{for the total}} of a small area using a linear least-squares {{prediction}} approach under a design based model. The <b>optimal</b> <b>estimator</b> {{is the same as}} that obtained under a superpopulation ap-proach, and leads to the classical form of the synthetic estimator in an extreme case. We also obtain an <b>optimal</b> <b>estimator</b> of the vector of totals of the di®er-ent small areas, using unbiasedness and M-minimization of prediction MSE as the optimization criteria. 1...|$|E
40|$|The {{algebraic}} {{structure of}} certain classes of nonlinear systems is exploited {{in order to}} prove that the <b>optimal</b> <b>estimators</b> for these systems are recursive and finite dimensional. These systems are represented by certain Volterra series expansions or by bilinear systems with nilpotent Lie algebras. In addition, an example is presented, and the steady-state estimator for this example is discussed...|$|R
40|$|Abstract. Econometric theory {{concerns}} {{the study and}} development of tools and methods for applied econometric applications. The ordinary least squares (OLS) estimator is the most basic estimation procedure in econometrics. In this paper we will present the ordinary linear regression model and discussed how to estimate linear regression models by using the method of least squares. Also, we present some properties of the <b>optimal</b> <b>estimators</b> (OLS estimators) {{in the case of}} the linear regression model as well as some measures of the amount of information associated to these estimators...|$|R
40|$|We {{provide a}} {{comparative}} study of several widely used off-policy estimators (Empirical Average, Basic Importance Sampling and Normalized Importance Sampling), detailing the different regimes where they are individually suboptimal. We then exhibit properties <b>optimal</b> <b>estimators</b> should possess. In the case where examples have been gathered using multiple policies, we show that fused estimators dominate basic ones but can still be improved...|$|R
40|$|International audienceThis work {{concerns}} {{estimation of}} multidimensional nonlinear regression models using multilayer perceptron (MLP). The main problem with such model {{is that we}} have to know the covariance matrix of the noise to get <b>optimal</b> <b>estimator.</b> however we show that, if we choose as cost function the logarithm of the determinant of the empirical error covariance matrix, we get an asymptotically <b>optimal</b> <b>estimator...</b>|$|E
3000|$|... [...]. With unknown CSI, the <b>optimal</b> <b>estimator</b> is a non-coherent algorithm, since (28) {{depends on}} the square norm of [...]...|$|E
3000|$|... {{which is}} equal to CRLB {{expression}} in (8). Therefore, the optimal combining technique in (15) results in an approximately <b>optimal</b> <b>estimator</b> at high SNRs.|$|E
40|$|We {{address the}} {{experimental}} determination of entanglement for systems {{made of a}} pair of polarization qubits. We exploit quantum estimation theory to derive <b>optimal</b> <b>estimators,</b> which are then implemented to achieve ultimate bound to precision. In particular, we present a set of experiments aimed at measuring the amount of entanglement for states belonging to different families of pure and mixed two-qubit two-photon states. Our scheme is based on visibility measurements of quantum correlations and achieves the ultimate precision allowed by quantum mechanics in the limit of Poissonian distribution of coincidence counts. Although optimal estimation of entanglement does not require the full tomography of the states we have also performed state reconstruction using two different sets of tomographic projectors and explicitly shown that they provide a less precise determination of entanglement. The use of <b>optimal</b> <b>estimators</b> also allows us to compare and statistically assess the different noise models used to describe decoherence effects occuring in the generation of entanglement. Comment: revised version, 10 page...|$|R
40|$|International audienceThis paper {{introduces}} Mean Field Games (MFG) as {{a framework}} to develop <b>optimal</b> <b>estimators</b> in some sense for a general class of nonlinear systems. We show that under suitable conditions the estimation error converges exponentially fast to zero. Computer simulations are performed to illustrate the method. In particular we provide an example where the proposed estimator converges whereas both extended Kalman filter and particle filter diverge...|$|R
40|$|AbstractWe {{investigate}} optimal bounded influence M-estimators in {{the general}} normal regression model with respect to different sensitivities. As a result, we answer some open questions in F. R. Hampel et al. (Robust Statistics, Chap. 6, Wiley, New York). Moreover, we examine the relationship among different sensitives and their associated <b>optimal</b> <b>estimators</b> and extend the idea of change- of -variance sensitivity {{to the case of}} the predicted value...|$|R
