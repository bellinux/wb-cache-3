125|568|Public
5000|$|Automatically {{change from}} {{portrait}} to landscape with <b>orientation</b> <b>sensor</b> ...|$|E
50|$|Orientation Sensor—the robot {{can detect}} {{whether it has}} fallen over. The <b>orientation</b> <b>sensor</b> also {{contributes}} to the robot's mood.|$|E
5000|$|<b>Orientation</b> <b>sensor</b> that {{automatically}} tags {{the orientation of}} the image during capture and subsequently rotates the image to the correct orientation during playback ...|$|E
25|$|The display may be toggled between {{portrait}} {{and landscape}} mode by keystrokes, or will switch automatically on phones with <b>orientation</b> <b>sensors.</b> The default <b>orientation</b> can be changed.|$|R
5000|$|Besides navigational purposes, IMUs {{serve as}} <b>orientation</b> <b>sensors</b> in many {{consumer}} products. Almost all smartphones and tablets contain IMUs as <b>orientation</b> <b>sensors.</b> Fitness trackers and other wearables may also include IMUs to measure motion. Some gaming {{systems such as}} the remote controls for the Nintendo Wii use IMUs to measure motion. Low cost IMUs have enabled the proliferation of the consumer drone industry. They are also frequently used for sports technology (technique training), and animation applications. They are a competing technology for use in motion capture technology. [...] An IMU {{is at the heart}} of the balancing technology used in the Segway Personal Transporter.|$|R
50|$|DuOS-M {{supports}} key hardware peripherals in Windows including cameras, audio, {{microphone and}} sensors such as ambient light sensor, accelerometer, gyrometer, compass and <b>orientation</b> <b>sensors.</b> It also supports various screen sizes, resolutions, and screen orientation (portrait and landscape) along with 3D acceleration and HD video playback.|$|R
50|$|An <b>orientation</b> <b>sensor</b> can {{be found}} in some digital cameras. By {{recording}} the orientation at the time of capture, the camera's software can determine whether the image should be oriented to landscape or portrait format.|$|E
5000|$|The F-35's systems {{provide the}} edge in the [...] "observe, orient, decide, and act" [...] OODA loop; stealth and {{advanced}} sensors aid in observation (while being difficult to observe), automated target tracking helps in <b>orientation,</b> <b>sensor</b> fusion simplifies decision making, and the aircraft's controls allow the pilot to keep their focus on the targets, rather than the controls of their aircraft.|$|E
50|$|On February 12, 1961, the Soviet {{spacecraft}} Venera 1 was {{the first}} probe launched to another planet. An overheated <b>orientation</b> <b>sensor</b> caused it to malfunction, losing contact with Earth before its closest approach to Venus of 100,000 km. However, the probe was first to combine all the necessary features of an interplanetary spacecraft: solar panels, parabolic telemetry antenna, 3-axis stabilization, course-correction engine, and the first launch from parking orbit.|$|E
40|$|This paper {{presents}} a prototype kinematic and audio feedback based video game, availing of a scalable motion capture acquisition system, based around {{a number of}} <b>orientation</b> <b>sensors.</b> The <b>orientation</b> <b>sensors</b> used are USB based tri-axis magnetic and gravitational field transducers. The novel video-game is capable of incorporating the real time data from these sensors to control an on screen avatar, which in turn can be programmed to give appropriate instructions to the user i. e. play a sound file, once the user obtains a certain posture. The video game is designed to promote physical exercise and movement based relaxation, in particular; Yoga. In addition, design considerations; implementation and performance of the system are analyzed, discussed and the accuracy qualitatively analyzed by comparing movement data obtained from it {{to that of a}} validated motion analysis technique, the CODA motion analysis system...|$|R
30|$|Data was {{recorded}} {{from the following}} built-in sensors: acceleration and <b>orientation</b> <b>sensors</b> were used to measure body movement, the barometer measured atmospheric pressure and was used to infer whether individuals {{were on the same}} floor level and ANT-radio messages were sent and received to find out which team member was in proximity to another one.|$|R
40|$|The ERB {{algorithms}} {{and computer}} software data flow used to convert sensor data into equivalent radiometric data {{are described in}} detail. The NIMBUS satellite location, <b>orientation</b> and <b>sensor</b> <b>orientation</b> algorithms are given. The computer housekeeping and data flow and sensor/data status algorithms are also given...|$|R
5000|$|Fujifilm {{announced}} the FinePix F80EXR on 2 February 2010. The F80EXR is visually {{identical to the}} F70EXR except for some minor differences. The F80EXR uses a 12-megapixel, 1/2" [...] Super CCD EXR sensor (vs the F70EXR's 10-megapixel sensor). The camera also supports 720P (1280x720) at 30fps video recording along with a slightly larger 3 inch LCD display (vs the F70EXR's 2.7 inch LCD). New [...] "Pet detection" [...] and an integrated <b>orientation</b> <b>sensor</b> are also included.|$|E
50|$|HoloLens {{features}} IEEE 802.11ac Wi-Fi and Bluetooth 4.1 Low Energy (LE) wireless connectivity. The headset uses Bluetooth LE to {{pair with}} the included Clicker, a thumb-sized finger-operating input device {{that can be}} used for interface scrolling and selecting. The Clicker features a clickable surface for selecting, and an <b>orientation</b> <b>sensor</b> which provides for scrolling functions via tilting and panning of the unit. The Clicker features an elastic finger loop for holding the device, and a USB 2.0 micro-B receptacle for charging its internal battery.|$|E
5000|$|At Augmented World Expo 2013, Optinvent {{demonstrated}} {{a prototype of}} their ORA see-through mobile AR display platform. The demonstrator included a monocular see-through display with the patented [...] "Flip-Vu" [...] feature allowing two positions for the virtual image. The display can be positioned directly in the wearer's field of vision or below it. One position is the [...] "AR mode" [...] whereby the image is directly superimposed on the wearers central field of vision; then by flipping the display down (mechanically), the wearer can have a [...] "dashboard mode" [...] whereby the virtual display is below the wearer's field of vision. This gives the possibility of having both true AR and a [...] "glance at" [...] capability in one device. A developer's version of the device running Android 4.1 Jelly Bean called the ORA-S including an SDK {{was said to be}} released soon and will include Bluetooth and Wi-Fi connectivity, a nine-axis <b>orientation</b> <b>sensor,</b> a camera, a microphone, loudspeaker, and battery in the form of photochromic sunglasses.|$|E
40|$|Independent motion {{detection}} aims at identifying {{elements in}} the scene whose apparent motion is not due to the robot egomotion. In this work, we propose a method that learns the input-output relationship between the robot motion - described by the position and <b>orientation</b> <b>sensors</b> embedded on the robot - and the sparse visual motion detected by the cameras. We detect independent motion by observing discrepancies (anomalies) between the perceived motion and the motion that is expected given the position and <b>orientation</b> <b>sensors</b> on the robot. We then perform a higher level analysis based on the available disparity map, where we obtain dense profile of the objects moving independently from the robot. We implemented the proposed pipeline on the iCub humanoid robot. In this work, we report a thorough experimental analysis that covers typical laboratory settings, where {{the effectiveness of the}} method is demonstrated. The analysis shows in particular the robustness of the method to scene and object variations and to different kinds of robot's movements...|$|R
40|$|During {{recent years}} direct <b>sensor</b> <b>orientation</b> with GPS and IMU has gained popularity. The system can {{directly}} get exterior orientation elements without using ground control points. To achieve the full accuracy potential of direct <b>sensor</b> <b>orientation,</b> the compensation of systematic errors {{with the correct}} mathematical model and an optimum number of parameters for sensor calibration is required. However, experiments showed {{that there would be}} some problems with y-parallaxes of stereo models based on direct <b>sensor</b> <b>orientation.</b> In this paper we put up one method to resolve this problem. The method was the integration of GPS/IMU and (automatic) aerial triangulation (AAT) into bundle block adjustment, which was also called integrated <b>sensor</b> <b>orientation.</b> From experimental results, we could get some conclusions that direct <b>sensor</b> <b>orientation</b> currently allows the generation of orthoimages of the small image scale. But it is not always suitable for stereo plotting because of the large y-parallaxes in some models. To resolve this problem, the combination of GPS/IMU observations with ground control points (integrated <b>sensor</b> <b>orientation)</b> should be preferred in some cases. 1...|$|R
5000|$|OSC server: sends {{information}} about the location, <b>orientation,</b> and <b>sensor</b> data of the AudioCubes to other OSC applications (creating interactive applications) ...|$|R
30|$|As noted above, {{we found}} that the <b>orientation</b> <b>sensor</b> can {{successfully}} be used in conjunction with the mobile phone’s other sensors, to detect variations in a user’s walking path when in outdoor areas. In addition, we also checked the orientation angles of the subjects’ phones that were recorded when they were walking down these same paths and same directions. We found that the mobile phone’s orientation varied greatly, depending on the angle-position of the phone while it was carried in either a user’s pocket or purse. Although the phone’s compass angle readings changed considerably depending on the phone’s placement orientation, {{we found that}} we can use the compass sensor’s reading regardless the mobile phone’s placement or orientation. We did an experiment to check the readings of the phone’s <b>orientation</b> <b>sensor</b> when the phone is carried in different positions, and to see whether the <b>orientation</b> <b>sensor</b> could still accurately detect a user’s walking path turning points and variation.|$|E
40|$|The {{purpose of}} this study is to realize the high {{function}} of an electric wheelchair with the consideration of the driving environment. The <b>orientation</b> <b>sensor</b> is used to comprehend the driving environment. The estimated method can be obtained unknown parameters of the driving environment by the <b>orientation</b> <b>sensor.</b> The developed electric-wheelchair operates by the force-feedback joystick. When the marketed wheelchairs drive on the slope or the side-slope, the wheelchairs flow to the slope lower direction. In this case, the wheelchairs might go to roadway side in JAPAN, and such situations are very dangerous. The force-feedback joystick is developed to avoid dangerous situation. The force-feedback joystick is combined with the <b>orientation</b> <b>sensor</b> to the force feedback of joystick. The effectiveness of the force-feedback joystick is confirmed by comparison experiments with the control performance of the wheelchair without the force feedback. Therefore, this study is expected to be useful to social welfare in the future...|$|E
40|$|Context: Image {{processing}} and computer vision are rapidly {{becoming more and}} more commonplace, and the amount of information about a scene, such as 3 D geometry, that can be obtained from an image, or multiple images of the scene is steadily increasing due to increasing resolutions and availability of imaging sensors, and an active research community. In parallel, advances in hardware design and manufacturing are allowing for devices such as gyroscopes, accelerometers and magnetometers and GPS receivers to be included alongside imaging devices at a consumer level. Aims: This work aims to investigate the use of orientation sensors in the field of computer vision as sources of data to aid with image {{processing and}} the determination of a scene’s geometry, in particular, the epipolar geometry of a pair of images - and devises a hybrid methodology from two sets of previous works in order to exploit the information available from orientation sensors alongside data gathered from image processing techniques. Method: A readily available consumer-level <b>orientation</b> <b>sensor</b> was used alongside a digital camera to capture images of a set of scenes and record the orientation of the camera. The fundamental matrix of these pairs of images was calculated using a variety of techniques - both incorporating data from the <b>orientation</b> <b>sensor</b> and excluding its use Results: Some methodologies could not produce an acceptable result for the Fundamental Matrix on certain image pairs, however, a method described in the literature that used an <b>orientation</b> <b>sensor</b> always produced a result - however in cases where the hybrid or purely computer vision methods also produced a result - this was found to be the least accurate. Conclusion: Results from this work show that the use of an <b>orientation</b> <b>sensor</b> to capture information alongside an imaging device can be used to improve both the accuracy and reliability of calculations of the scene’s geometry - however noise from the <b>orientation</b> <b>sensor</b> can limit this accuracy and further research would be needed to determine the magnitude of this problem and methods of mitigation...|$|E
50|$|The SkyScout has a 12 channel GPS {{receiver}} and <b>orientation</b> <b>sensors</b> that measure location and pointing angle. From an internal database of some 6,000 celestial objects an object is identified simply by centering {{it in the}} device's zero-power optical finder and pressing a button. The LCD screen displays {{the name of the}} object (star, planet, deep sky object, etc.) and other relevant data. An audio presentation is available via earphones on 200 of the most popular celestial objects.|$|R
40|$|Direct {{georeferencing}} {{is defined}} as direct measurement of exterior orientation parameters, using positioning and <b>orientation</b> <b>sensors,</b> such as the Global Positioning System (GPS) and inertial navigation system (INS). Imaging sensors, most frequently supported by direct georeferencing, are digital cameras, lidar systems, multi-spectral scanners, or synthetic aperture radar (SAR). While for scanning sensors the use of direct georeferencing is compulsory, frame digital cameras can also directly benefit from this technique of <b>sensor</b> <b>orientation.</b> With direct <b>sensor</b> <b>orientation,</b> the requirement for ground control points (GCP), tie point matching and aerial triangulation (AT) is significantly reduced. The most expensive part of these three requirements is the need of GCP and under exclusion of this part the other two parts are always available for images from digital frame cameras. This paper {{is focused on the}} integration of this existing additional information into the Kalman filter used for direct georeferencing with GPS and INS. The aim is to use the relative orientation information of images extracted with the aid of tie points as an additional update to support the drifting gyros of the inertial measurement unit (IMU) directly like GPS does for the accelerometers. 1...|$|R
40|$|During {{recent years}} the direct <b>sensor</b> <b>orientation</b> with GPS and IMU has gained popularity. These systems allow the {{determination}} of all exterior orientation elements without using ground control points. This technology opens several new applications for photogrammetry and remote sensing. One precondition for direct <b>sensor</b> <b>orientation</b> with GPS and IMU is the correct sensor calibration. The related parameters {{as well as the}} relation between the IMU and the aerial camera (boresight misalignment) have to be determined by conventional bundle block adjustment. During this process a camera self calibration (focal length, principal point, additional parameters etc.) may be performed under operational conditions. To achieve the full accuracy potential of direct <b>sensor</b> <b>orientation,</b> the compensation of systematic errors with the correct mathematical model and an optimum number of parameters for sensor calibration is required. A series of tests was conducted and showed the good accuracy potential of direct GPS/IMU <b>sensor</b> <b>orientation.</b> First investigations showed also problems with y-parallaxes of stereo models based on direct <b>sensor</b> <b>orientation.</b> Future developments in GPS and IMU sensors and data processing may reduce this problem. Just now we do need another save solution. A promising one is the integration of GPS/IMU and (automatic) aerial triangulation (AAT) into bundle block adjustment, also called integrated <b>sensor</b> <b>orientation.</b> This paper presents the sensor calibration based on data from test flights in large image scales. Furthermore it demonstrates the accuracy potential at independent check points in object and in image space for direct and integrated <b>sensor</b> <b>orientation.</b> ...|$|R
40|$|Abstract. This paper {{proposes a}} non-intrusive {{authentication}} method {{based on two}} sensitive apparatus of smartphones, namely, the <b>orientation</b> <b>sensor</b> and the touchscreen. We have found that these two sensors are capable of capturing behavioral biometrics of a user while the user is engaged in relatively stationary activities. The experimental results with respect to two types of flick operating have an equal error rate of about 3. 5 % and 5 %, respectively. To {{the best of our}} knowledge, this work is the first publicly reported study that simultaneously adopts the <b>orientation</b> <b>sensor</b> and the touchscreen to build an authentication model for smartphone users. Finally, we show that the proposed approach can be used together with existing intrusive mechanisms, such as password and/or fingerprints, to build a more robust authentication framework for smartphone users...|$|E
40|$|The paper {{discusses}} a force vector {{and surface}} <b>orientation</b> <b>sensor</b> suitable for intelligent grasping. The {{use of a}} novel four degree-of-freedom force vector robotic fingertip sensor allows efficient, real time intelligent grasping operations. The basis of sensing for intelligent grasping operations is presented and experimental results demonstrate the accuracy and ease of implementation of this approach...|$|E
40|$|Without {{access to}} {{external}} guidance, such as landmarks or beacons, indoor mobile robots usually orientate themselves by using magnetic compasses or gyroscopes. However, compasses face interference from steel furniture, and gyroscopes suffer from zero drift errors. This paper proposes an <b>orientation</b> <b>sensor</b> {{that can be}} used on differentially driven mobile robots to resolve these issues. The sensor innovatively combines the general differentials and an optical encoder so that it can provide only the orientation information. Such a sensor has not been described in any known literature and is cost-efficient compared to the common method of using two encoders for differentially driven mobile robots. The kinematic analysis and the mechanical design of this sensor are presented in this paper. The maximum mean error of the proposed <b>orientation</b> <b>sensor</b> was about 0. 7 ° during the component tests. The application of the sensor on a vacuum cleaning robot was also demonstrated. The use of the proposed sensor may provide less uncertain orientation data for an indoor differentially driven mobile robot...|$|E
40|$|This paper {{concerns}} {{itself with}} compression strategies for orientation signals, seen as signals evolving {{on the space}} of quaternions. The compression techniques extend classical signal approximation strategies used in data mining, by explicitly {{taking into account the}} quotient-space properties of the quaternion space. The approximation techniques are applied to the case of human gesture recognition from cellphone-based <b>orientation</b> <b>sensors.</b> Results indicate that the proposed approach results in high recognition accuracies, with low storage requirements, with the geometric computations providing added robustness than classical vector-space computations. ...|$|R
40|$|On-shoe {{acceleration}} and <b>orientation</b> <b>sensors</b> have revealed as a potentially powerful means for capturing {{aspects of human}} gait. The placement of sensors however has been done intuitively and mostly without quantitative evaluation of sensor positioning. Based on recorded signals of the five placement options sole, heel, toe-cap, instep and ankle we built SVM classifiers using orientation-based features and evaluate their performance on three activity classes level walking, going upstairs and going downstairs. Finally we present an approach to a placement-invariant classification model and discuss the benefit for a bipedal sensing setup...|$|R
40|$|There is a {{need for}} a standard, {{accurate}} test bench for inertia-based <b>orientation</b> <b>sensors.</b> Static accuracy testing of these sensors is straightforward but dynamic accuracy testing is more difficult. A test bench system is developed with encoders and a PC 104 computer under the QNX Neutrino real-time operating system. A MicroStrain 3 DMGX 1 inertial sensor was used as the sensor to be tested. The dynamic error of this sensor was accurately recorded and found to be a function of the sensor velocity and acceleration. US Navy (USN) author...|$|R
40|$|Conforming to W 3 C specifications, mobile web browsers allow JavaScript code in a {{web page}} to access motion and <b>orientation</b> <b>sensor</b> data without the user’s permission. The {{associated}} risks to user security and privacy are however not considered in W 3 C specifications. In this work, for the first time, we show how user security can be compromised using these sensor data via browser, despite that the data rate is 3 to 5 times slower than what is available in app. We examine multiple popular browsers on Android and iOS platforms and study their policies in granting permissions to JavaScript code with respect to access to motion and <b>orientation</b> <b>sensor</b> data. Based on our observations, we identify multiple vulnerabilities, and propose TouchSig-natures which implements an attack where malicious JavaScript code on an attack tab listens to such sensor data measurements. Based on these streams, TouchSignatures is able to distinguish the user’s touch actions (i. e., tap, scroll, hold, and zoom) and her PINs, allowing a remote website to lear...|$|E
40|$|Abstract: This paper {{presents}} an augmented reality system that {{makes use of}} a consumer-level mo-bile device equipped with an inertial <b>orientation</b> <b>sensor.</b> The device maps orientational information to user interactions. Furthermore, we utilize orientation to determine portions of the operator’s context. The system makes use of the location- and context-aware platform Nexus [HKL + 99] to further refine the user’s context information. To evaluate {{the acceptance of the}} presented system a user study was performed...|$|E
40|$|An {{accurate}} localization system (Carrier Phase differential GPS receiver) {{allows the}} design and implementation of an absolute vehicle guidance system. The preliminary work, presented in this paper, was aimed at validating the use of one GPS receiver in a vehicle guidance system, without any <b>orientation</b> <b>sensor.</b> We designed and implemented a non linear control law to perform a line-following task. Real-time experiments {{have been carried out}} on a combine harvester. Keywords: GPS sensor, mobile robot, non linear robot control, localization 1...|$|E
40|$|Among a {{wide variety}} of sensing {{technologies}} vision systems are capable to derive a mobile robot’s relationship and orientation with respect to the surrounding environment. In this paper, we present an algorithmic approach to derive <b>sensor</b> <b>orientation</b> from image sequences acquired by a binocular vision-based mobile “robot”, which moves in an office, sensing its environment. The data acquisition, the extraction of characteristic features in the image frames, and the tracking and matching of the landmarks along the sequence is described. The determination of the <b>sensor</b> <b>orientation</b> by sequential estimation based on Givens transformations is demonstrated using the tracked data of the test sequence. The results of the vision system’s calibration and of the <b>sensor</b> <b>orientation</b> are presented. Finally, a comparison of the <b>sensor</b> <b>orientation</b> of the last stereo pair with reference data indicates the achieved accuracy of the algorithmic approach. 1...|$|R
40|$|Abstract — High {{integrity}} localization {{system is}} an important challenge to improve safety for road vehicles. A way {{to meet the requirements}} is to fuse information from several sensors, from position and <b>orientation</b> <b>sensors</b> to motion, speed and acceleration sensors. This paper tackles the problem of vehicle motion estimation using monocular vision. A geometric model of the road is used to learn a texture patch in the current image, this patch is then tracked through the successive frames to estimate in real time the motion of the vehicle. The proposed method was assessed using a centimeter accuracy Real Time Kinematic GPS receiver. I...|$|R
40|$|Abstract—Orientation sensors {{containing}} magnetometers use the earth’s {{magnetic field}} as a reference. Ferromagnetic objects may distort this magnetic field, leading to inaccurate orienta-tion output. We explored {{the viability of}} these orientation sen-sors for motion analysis in an assistive mobility device rehabilitative setting. We attached two MTx <b>orientation</b> <b>sensors</b> (XSens; Enschade, the Netherlands), connected to the XBus Master data collection unit (XSens), to a plastic frame such that the relative angle between sensors was constant. We then moved a series of mobility devices in proximity to the plastic frame: two knee-ankle-foot orthoses (aluminum, stainless steel), one ankle-foot orthosis, two transtibial prosthese...|$|R
