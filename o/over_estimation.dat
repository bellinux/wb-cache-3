163|938|Public
5000|$|Indicator {{measures}} can overlap, causing <b>over</b> <b>estimation</b> of single parameters.|$|E
50|$|The {{detection}} of laboratory parameters {{is based on}} physicochemical reactions between the substance being measured and reagents designed for this purpose. These reactions can be altered {{by the presence of}} drugs giving rise to an <b>over</b> <b>estimation</b> or an underestimation of the real results. Levels of cholesterol and other blood lipids can be overestimated {{as a consequence of the}} presence in the blood of some psychotropic drugs. These overestimates should not be confused with the action of other drugs that actually increase blood cholesterol levels due to an interaction with its metabolism.Most experts consider that these are not true interactions, so they will not be dealt with further in this discussion.|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceIn {{this paper}} we propose an {{enhancement}} of the Jacobian adaptation by estimating automatically a noise <b>over</b> <b>estimation</b> factor which yields to a closer approximation of Parallel model combination (PMC) than the traditional Jacobian adaptation. Noise <b>over</b> <b>estimation</b> factors are estimated at run-time {{for a set of}} clustered Gaussians obtained on the training set. Experiments conducted on a French natural number database show that similar performance as PMC can be obtained at the expense of a slight increase in computational complexity as compared to Jacobian adaptation...|$|E
30|$|Improve local state <b>estimation</b> <b>over</b> purely {{distributed}} approaches.|$|R
40|$|This paper briefly {{outlines}} {{the advantages of}} maximum likelihood (ML) <b>estimation</b> <b>over</b> other <b>estimation</b> methods. Then, the recently developed ML estimation procedures of Raju and Drasgow (2003) are described for use in validity generalization. Two examples are presented, comparing the traditional VG estimation methods based on the method of moments with the new ML estimation methods, across three different VG models/scenarios (bare-bones, use of artifact distributions, and direction corrections). The need for assessing the accuracy and comparability of the traditional and ML estimation procedures is addressed...|$|R
30|$|Even if TE-vectored gene {{fragments}} {{are rarely}} if ever true genes with a selected host function, {{they certainly are}} a complication to genome annotation. Even without internal gene fragments, low-copy-number TEs are often mis-annotated as genes, giving rise {{to as much as}} two-fold <b>over</b> <b>estimations</b> of gene numbers [7]. This type of over-estimation in gene number can play particular havoc with assessment of genic colinearity, as evidenced by studies in rice showing hundreds of gene differences between different races of O. sativa that were later shown to all be explained by mis-annotated TEs [9]. Hence, many early publications showing numerous genic exceptions to microcolinearity are incorrect because of this routine annotation error.|$|R
40|$|This paper {{considers}} {{an important}} concept which suggested {{to take stock}} of the <b>over</b> <b>estimation</b> in reliability characteristics or under estimation of hazard rate. Using this concept, the study considers the analysis of the reliability characteristics of an exponential lifetime model when prior variations in its parameters are suspected. Key Words: Robustness, adjustment factor, updated and predictive basic distributions 1...|$|E
40|$|This paper {{considers}} forecast averaging {{when the}} same model is used but estimation is carried out over different estimation windows. It develops theoretical results for random walks when their drift and/or volatility are subject {{to one or more}} structural breaks. It is shown that compared to using forecasts based on a single estimation window, averaging <b>over</b> <b>estimation</b> windows leads to a lower bias and to a lower root mean square forecast error for all but the smallest of breaks. Similar results are also obtained when observations are exponentially down-weighted, although in this case the performance of forecasts based on exponential down-weighting critically depends on the choice of the weighting coefficient. The forecasting techniques are applied to 20 weekly series of stock market futures and it is found that average forecasting methods in general perform better than using forecasts based on a single estimation window. ��Â Forecast combinations; averaging <b>over</b> <b>estimation</b> windows; exponentially down-weighting observations; structural breaks...|$|E
30|$|On {{the other}} hand, this {{research}} {{does not take}} into account that the social explorer is not able to distinguish between Twitterbots 4 and real users on Twitter. Therefore, all the estimates include Twitterbots causing an <b>over</b> <b>estimation</b> in the results. These data must be interpreted with caution since all the information collected from this study is mainly based on the Twitter response service.|$|E
25|$|Supercapacitors also {{experience}} electrolyte evaporation <b>over</b> time. <b>Estimation</b> {{is similar}} to wet electrolytic capacitors. Additional to temperature the voltage and current load influence the life time. Lower voltage than rated voltage and lower current loads as well as lower temperature extend the life time.|$|R
30|$|An {{important}} trend can {{be observed}} in the results of both methods. Rotational error decreases with increasing segment length. We may conclude from this that there is some noise present on the immediate poses estimated by both methods, which averages to zero <b>over</b> many <b>estimations.</b>|$|R
50|$|Supercapacitors also {{experience}} electrolyte evaporation <b>over</b> time. <b>Estimation</b> {{is similar}} to wet electrolytic capacitors. Additional to temperature the voltage and current load influence the life time. Lower voltage than rated voltage and lower current loads as well as lower temperature extend the life time.|$|R
40|$|Among present Japanese Greenhouse Gas Emission {{quantification}} methodologies, {{the methodology}} for Limestone and Dolomite origin carbon dioxide quantification is {{pointed out that}} there may be some duplication, oversight and <b>over</b> <b>estimation</b> due to the methodology mixes both industrial and technological classifications. The author tried to verify and evaluate the present methodology developing new methodology applying estimation methodology used in Japanese General Energy Statistics with I/O table and industrial statistics' data and information, comparing the emission amount with present one by time series from 1990; the author quantified the total Limestone, Dolomite and their derivatives' end use side demand with non-equivalent price quantity I/O table by industrial sector classification, and re-classified them by technological classification whether carbon dioxide emission happens or not for each industrial sector. As a result, the author found that present and new methodologies resulted in similar in 1990 base year, but after that they show unstable and large discrepancies. At the end the present methodology proved to be over estimate to the new methodology around one million metric tons of carbon dioxide in average from 2005 to 2007. The author analyzed causes for such an unstable and large scale <b>over</b> <b>estimation</b> and found that present methodology double counted Limestone supply and Quicklime consumption produced by end-use side in iron and steel industry, and over sighted several Limestone and Dolomite uses for earthenware materials or discharge gas sulfur reduction. And the author found that those million tons of errors offset each other and resulted in such an <b>over</b> <b>estimation.</b> The author recommends re-calculation to replace the present methodology after revising necessary statistics by cooperation with related organizations. ...|$|E
40|$|Abstract: As {{a general}} rule, most {{performance}} analysts are confident in fundamental relationships like one plus one equals two. Unfortunately, this time proven relationship is a gross <b>over</b> <b>estimation</b> {{of the actual}} capacity delivered by a dual or multiprocessor configurations. In this paper, we will present a simple conceptual model of multiprocessor performance and provide a generalized first order result for evaluating the capabilities of alternative multiprocessor designs. ...|$|E
40|$|This paper {{presents}} {{five different}} methods for performing on-wafer calibration of RF CMOS measurements. All methods are com- patible with standard CMOS technology. A comparison of method performance up to 12 GHz {{is made with}} measurements on RF CMOS devices. The results verify that substrate and metalization lasses must be considered to obtain high accuracy. Fixture design issues are discussed and a method for mitigating <b>over.</b> <b>estimation</b> of DUT performance is suggested. I...|$|E
30|$|Numerical {{evaluations}} {{show that}} the proposed solution can suppress {{a significant portion of}} the contamination at low and moderate levels of mobility. Even at high mobility, i.e., car speeds of 100 to 130 km/h, the proposed solution can provide a noticeable gain <b>over</b> conventional <b>estimation</b> methods.|$|R
40|$|Electromagnetic Articulography (EMA) {{technique}} {{is used to}} record the kinematics of different articulators while one speaks. EMA data often contains missing segments due to sensor failure. In this work, we propose a maximum a-posteriori (MAP) estimation with continuity constraint to recover the missing samples in the articulatory trajectories recorded using EMA. In this approach, we combine the benefits of statistical MAP estimation {{as well as the}} temporal continuity of the articulatory trajectories. Experiments on articulatory corpus using different missing segment durations show that the proposed continuity constraint results in a 30 % reduction in average root mean squared error in <b>estimation</b> <b>over</b> statistical <b>estimation</b> of missing segments without any continuity constraint...|$|R
40|$|Parameter called {{lubricant}} percent volume or cavity factor (XCAV) {{used primarily}} in calculation of ball or roller drag and, therefore, significantly affects calculated bearing-heat generation and temperature distribution. New equation accounts for sensitivity of XCAV to shaft speed, lubricant flow rate, and bearing size, and provides significant improvement <b>over</b> previous <b>estimation</b> methods...|$|R
40|$|The {{sardinella}} fleet {{landed the}} same quantity of sardines in 1983 (11200 tons) as in 1982. Sardinella aurita catches {{would be more}} important if the "Syndicat des Armateurs" did not impose quotas - in July 300 boxes and in August 100 boxes by fleet by trip. The extension {{of the effort to}} herring stock increased the catches of this species and an <b>over</b> <b>estimation</b> of 44 % of the effort have been observed in 1983...|$|E
40|$|The paper compares {{different}} methods for {{the estimation of}} the spectrum in the heart rate variability time series. Studying 32 hypertensive patients it is shown that the low-pass filtering of event series technique drives to an <b>over</b> <b>estimation</b> of the higt frequency component of the heart beat {{when compared with the}} results of both cubic spline interpolation and interval tachogram techniques. No differences are founded between the spectrum estimation obtained by autoregressive models and non parametric methods like the Fourier transform...|$|E
40|$|Subject of this {{analysis}} is to show how storage is operated optimally under renewable and load uncertainty in the electricity system context. We estimate a homogeneous Markov Chain representation of the residual load in Germany in 2014 on an hourly basis and design a very simple dynamic stochastic electricity system model with non-intermittent generation techno - logies and storage. We compare these results to perfect foresight findings and identify a significant <b>over</b> <b>estimation</b> of the storage potential under perfect foresight...|$|E
3000|$|... {{is shown}} in the {{appendix}} under the uniform interleaving assumption [14] which gives the average <b>estimations</b> <b>over</b> all possible deterministic random interleavers. Note that [...]...|$|R
40|$|The {{problem of}} the timelike Pomeron {{coupling}} to light mesons and photons is considered in light of available data on high-energy meson-proton scattering. Possible correspondence of $f_ 2 (1950) $ resonance to the ground state of the Pomeron is argued. Comment: 12 pages, 8 figures; the text has been worked <b>over,</b> the <b>estimations</b> are improve...|$|R
40|$|Working Paper GATE 2009 - 08 We {{simulate}} a closed rental housing market with search and matching frictions, {{in which both}} landlord and tenant agents are imperfectly informed. Homogeneous landlords set rents to maximise revenue, using information on the market to estimate the relationship between posted rent and time-on-the-market (TOM). Tenants, heterogeneous in income, engage in undirected search accepting residences based on their idiosyncratic tastes for housing and a disagreement point derived from information {{on the distribution of}} offers. The steady state to which the simulation evolves shows price dispersion, nonzero search times and vacancies. The main results concern the effects of increasing information {{on either side of the}} market. When tenants see a greater percentage of the distribution of offers, tenants learn to refuse high rents and so the population rises and tenants' utilities rise as does overall welfare. Conversely, when landlords have less information, their utility can rise as <b>over</b> <b>estimations</b> in best posting rent move the market to higher rents...|$|R
40|$|Neighborhood walkability {{is being}} {{promoted}} {{as an important}} factor in public health efforts to decrease rates of physical inactivity. Single entry communities (SEC), communities with only 1 entrance/exit, may result in an <b>over</b> <b>estimation</b> of walkability. This design makes direct walking routes outside the community nearly impossible and results in increased trip distance. The {{purpose of this study was}} to determine if accounting for SECs resulted in a significant difference in street connectivity. Methods:Twenty geographically different Las Vegas neighborhoods were chosen and the number of true intersections measured in ArcGIS. Neighborhoods were then assessed for the presence of SECs using google maps, ArcGIS land imagery, and field observation. Intersections inside SECs were removed. A paired t test was used to assess the mean difference of intersection density before and after adjustment. Results: There was a statistically significant decrease in the number of true intersections after the adjustment (before mean = 57. 8; after mean = 45. 7). The eta squared statistic indicates a large effect size (0. 3). Conclusions: Single entry communities result in an <b>over</b> <b>estimation</b> of street connectivity. If SECs are not accounted for, trip distances will be underestimated and public health efforts to promote walking through walkable neighborhoods may prove less effective...|$|E
30|$|The fire {{events in}} the United States is {{reported}} on by the NFPA. Between the years of 2007 – 2011 there were approximately 498, 500 fires per year (NFPA 2013 a, b). These statistics are important to characterize {{the environmental impact of}} fire. To use the statistics from the United States, and from other countries, {{it is important to note}} that just accounting for the number of fires will give an <b>over</b> <b>estimation</b> of the impacts, whereas accounting for the property damage will more closely equate to the impacts to the environment.|$|E
40|$|A simple {{whole blood}} {{clotting}} test (WBCT 20) was assessed for its efficacy in determination of severe defibrinogenation in patients envenomed by Bothrops snakes in Brazil. There {{was a close}} relationship between {{the results of the}} WBCT 20 and plasma fibrinogen levels in 69 moderately envenomed patients. The advantage of the WBCT 20 <b>over</b> <b>estimation</b> of plasma fibrinogen concentrations in patients {{is that it is a}} simpler, faster and more reliable test. It is also of use in assessing the effectiveness of antivenom therapy in relation to the restoration of blood coagulability. © 1994...|$|E
40|$|This paper explores how {{emergency}} managers {{make judgments}} regarding longterm policy {{and offers a}} sociological account of organizational decision making within an ecological context. Discussions with emergency managers focusing on {{the relative merits of}} rainfall estimation and tornado detection served as data to address these issues. Among the thirty-nine (n= 39) interviewees, a consensus emerged favoring tornado detection <b>over</b> rainfall <b>estimation.</b> From these findings, the paper attempts to a) understand why emergency managers prefer tornado detection <b>over</b> rainfall <b>estimation</b> and b) develop theoretical generalizations explaining trends in these preferences. Concerning the first goal, analysis revealed emergency managers stressed the relative threats of common hazards in Oklahoma, the capabilities of technology in hazard mitigation, and public opinion. Given the environmental, technological, and social concerns reflected in this reasoning, {{there appears to be a}} strong ecological context driving the need for tornado detection among emergency managers. Implications and concerns are presented in the final section...|$|R
30|$|In this section, {{we first}} {{illustrate}} the system {{model for the}} distributed <b>estimation</b> <b>over</b> sensor networks. Following this, we review the conventional DRLS algorithm with the fixed forgetting factor briefly.|$|R
30|$|In this section, {{we present}} the {{simulation}} {{results for the}} proposed LTVFF-DRLS and LCTVFF-DRLS algorithms when applied in two applications, that is, distributed parameter estimation and distributed spectrum <b>estimation</b> <b>over</b> sensor networks.|$|R
40|$|Weibull {{distribution}} is widely employed in modeling and analyzing lifetime data. The present paper considers {{the estimation of}} the scale parameter of two parameter Weibull distribution with known shape. Maximum likelihood estimation is discussed. Bayes estimator is obtained using Jeffreys ’ prior under linex loss function. Relative efficiency of the estimators are calculated in small and large samples for over-estimation and under-estimation using simulated data sets. It is observed that Bayes estimator fairs better especially in small sample size and when <b>over</b> <b>estimation</b> is more critical than under estimation...|$|E
40|$|The {{dimensions}} {{of a truck}} stuble axle were calculated {{according to the new}} FKM fatigue design regulations. While for constant amplitudeloading calculated and experimentally determined S-N curves were at about same level, the fatigue life of the parts under a typical truck loading spectrum was numerically overestimated. This resulted axle diameters 15 per cent smaller than for the actual parts. The reason for the fatigue life <b>over</b> <b>estimation</b> lies in the inconsistency of damage accumulation hypothesis to consider properly mean-stress flutuations of a load spectrum. This expencence must be implemented in the new regulations...|$|E
40|$|The {{impact of}} {{introducing}} stricter sulphur limits in the Mediterranean and the Atlantic was modelled using 12 priority corridors operating on these seas. Due to the topography of the Mediterranean coastline and {{the selection of}} studied routes, short sea shipping (SSS) {{was found to be}} dominant, holding approximately 95 % of the market share on the selected routes. This share is an <b>over</b> <b>estimation</b> as rail was not included in this study. The introduction of stricter sulphur limits in 2015 & 2020 will increase SSS costs and hence reduce the SSS market share...|$|E
40|$|Extreme value {{statistics}} {{provides a}} practical, flexible, mathematically elegant {{framework in which}} to develop financial risk management tools {{that are consistent with}} empirical data. In this introductory survey, we discuss some of the basic tools including power law distributions, the peaks <b>over</b> thresholds <b>estimation</b> procedure and point processes. Key words: extreme events, normal distribution, extreme value distribution, power law, Pareto distribution, peaks over thresholds, tail index...|$|R
3000|$|Compared {{with the}} variances of channel <b>{{estimation}}</b> <b>over</b> one OFDMA symbol as in (22)–(24), the estimation variances (29)–(31) of the weighted average estimator (15)–(18) are significantly reduced {{owing to the}} fact that [...]...|$|R
40|$|A method <b>over</b> of <b>estimation</b> {{of power}} losses, arising up during work of {{communication}} networks is brought in this article. The {{traditional methods of}} similar estimation do {{not take into account}} inevitable power losses, related to the delay of signal, disсreet of changeable parameters and errors in the chain of management work of the system. The detailed analysis of laws of distributing is resulted for the different canals of connection...|$|R
