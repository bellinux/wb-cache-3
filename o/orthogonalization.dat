1025|23|Public
2500|$|... yk. The l·d·lt square-root filter {{requires}} <b>orthogonalization</b> of {{the observation}} vector. This may {{be done with the}} inverse square-root of the covariance matrix for the auxiliary variables using Method 2 in Higham (2002, p.263).|$|E
2500|$|There {{are other}} methods than the Cholesky {{decomposition}} in use. [...] <b>Orthogonalization</b> methods (such as QR factorization) are common, for example, when solving problems by least squares methods. [...] While the theoretical fill-in {{is still the}} same, in practical terms the [...] "false non-zeros" [...] can be different for different methods. [...] And symbolic versions of those algorithms {{can be used in}} the same manner as the symbolic Cholesky to compute worst case fill-in.|$|E
2500|$|The same {{algorithm}} is {{implemented in the}} GNU Scientific Library (GSL). The GSL also offers an alternative method, which uses a one-sided Jacobi <b>orthogonalization</b> in step 2 [...] This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 × 2 SVD problems, similar to how the Jacobi eigenvalue algorithm solves a sequence of 2 × 2 eigenvalue methods [...] Yet another method for step 2 uses the idea of divide-and-conquer eigenvalue algorithms [...]|$|E
5000|$|These {{principles}} made it straightforward to find {{names and}} forms for all new Z80 instructions, {{as well as}} <b>orthogonalizations</b> of old ones, such as [...]|$|R
40|$|Summary. A {{continuation}} of [5]. We introduce more configurational axioms i. e. <b>orthogonalizations</b> of “scherungssatzes ” (direct and indirect), “Scherungssatz ” with orthogonal axes, Pappus axiom with orthogonal axes; we {{also consider the}} affine Major Pappus Axiom and affine minor Desargues Axiom. We prove a number of implications which hold between the above axioms. MML Identifier: CONMETR...|$|R
2500|$|Note {{that the}} {{singular}} values are real and right- and left- singular vectors {{are not required}} to form any similarity transformation. Alternating QR decomposition and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. QR decomposition gives [...] and LQ decomposition of [...] gives [...] Thus, at every iteration, we have , update [...] and repeat the <b>orthogonalizations.</b>|$|R
50|$|It is {{obtained}} by performing <b>orthogonalization,</b> via eigen analysis on geometric moments.|$|E
50|$|QR {{decomposition}} is Gram-Schmidt <b>orthogonalization</b> {{of columns}} of A, started {{from the first}} column.|$|E
50|$|RQ {{decomposition}} is Gram-Schmidt <b>orthogonalization</b> of rows of A, {{started from}} the last row.|$|E
40|$|The classic Lanczos {{method is}} an {{effective}} method for tridiagonalizing real symmetric matrices. Its block algorithm can significantly improve performance by exploiting memory hierarchies. In this paper, we present a block Lanczos method for tridiagonalizing complex symmetric matrices. Also, we propose a novel componentwise technique for detecting the loss of orthogonality to stablize the block Lanczos algorithm. Our experiments have shown our componentwise technique can {{reduce the number of}} <b>orthogonalizations...</b>|$|R
40|$|We {{consider}} {{the problem of}} matrix transpose on mesh-connected processor networks. On the theoretical side, we present the first optimal algorithm for matrix transpose on two-dimensional meshes. Then we consider issues on implementations, show that the theoretical best bound cannot be achieved and present an alternative approach that really improves the practical performance. Finally, we introduce the concept of <b>orthogonalizations,</b> which are generalization of matrix transposes. We show how to realize them efficiently and present interesting applications of this new technique...|$|R
40|$|A {{parallel}} {{algorithm is}} given for computation of a maximal linearly independent subset {{of a set}} of vectors over a field. The algorithm uses polylogarithmic time and uses a number of processors that differs by only a polylog factor from the number required for fast parallel matrix inversion. It is used to produce efficient parallel algorithms for <b>orthogonalizations</b> of arbitrary matrices over real fields, and for P-L-U factorizations of nonsingular matrices over arbitrary fields. These are the first processor-efficient highly parallel algorithms known for these problems. ...|$|R
50|$|Other <b>orthogonalization</b> {{algorithms}} use Householder transformations or Givens rotations. The algorithms using Householder transformations {{are more}} stable than the stabilized Gram-Schmidt process. On the other hand, the Gram-Schmidt process produces the th orthogonalized vector after the th iteration, while <b>orthogonalization</b> using Householder reflections produces all the vectors only at the end. This makes only the Gram-Schmidt process applicable for iterative methods like the Arnoldi iteration.|$|E
5000|$|... called Frenet vectors. They are {{constructed}} from the derivatives of [...] using the Gram-Schmidt <b>orthogonalization</b> algorithm with ...|$|E
5000|$|To allow {{identification}} <b>orthogonalization,</b> Volterra series must be rearranged {{in terms}} of orthogonal non-homogeneous G operators (Wiener series): ...|$|E
40|$|Summary. A {{continuation}} of [5]. We introduce more configurational axioms i. e. <b>orthogonalizations</b> of ”scherungssatzes ” (direct and indirect), ”Scherungssatz ” with orthogonal axes, Pappus axiom with orthogonal axes; we {{also consider the}} affine Major Pappus Axiom and affine minor Desargues Axiom. We prove a number of implications which hold between the above axioms. MML Identifier: CONMETR. The articles [2], [4], [1], [3], and [5] provide the notation and terminology for this paper. We adopt the following rules: X will denote a metric affine plane...|$|R
5000|$|Zilog's NMOS Z800 and CMOS Z280 were 16-bit Z80-implementations (before the HD64180 / Z180) with a 16 MB paged MMU address space; {{they added}} many <b>orthogonalizations</b> and {{addressing}} modes to the Z80 instruction set. Minicomputer features — such as user and system modes, multiprocessor support, on chip MMU, on chip instruction and data cache {{and so on}} — were seen rather as more complexity than as functionality {{and support for the}} (usually electronics-oriented) embedded systems designer, it also made it very hard to predict instruction execution times.|$|R
40|$|The Fock-Tani Hamiltonian {{is found}} for systems {{containing}} two protons and one electron. It is {{shown that a}} post-prior symmetrical T-matrix element for a+ + (b+ c-) →(a+ c-) +b+ may be found {{from that of the}} simpler proton-proton-electron system if a and b are treated as isospin projections of a single type of nucleon. The Coulomb-exchange contribution to the inelastic (isospin flip) scattering of this system gives a first-order T matrix that is completely symmetrical with respect to post and prior interactions and <b>orthogonalizations,</b> a symmetry of the exact T matrix...|$|R
5000|$|This {{method and}} its more {{efficient}} version (Fast Orthogonal Algorithm) were invented by Korenberg [...]In this method the <b>orthogonalization</b> is performed empirically over the actual input. It {{has been shown}} to perform more precisely than the Crosscorrelation method. Another advantage is that arbitrary inputs can be used for the <b>orthogonalization</b> and that fewer data-points suffice to reach a desired level of accuracy. Also, estimation can be performed incrementally until some criterion is fulfilled.|$|E
5000|$|Many noise {{reduction}} algorithms tend to damage {{more or less}} signals. The local signal-and-noise <b>orthogonalization</b> algorithm [...] {{can be used to}} avoid the damages to signals.|$|E
50|$|In linear algebra, <b>{{orthogonal}}ization</b> is {{the process}} of finding a set of orthogonal vectors that span a particular subspace. Formally, starting with a linearly independent set of vectors {v1,&#8239;...&#8239;,&#8239;vk} in an inner product space (most commonly the Euclidean space Rn), <b>orthogonalization</b> results in a set of orthogonal vectors {u1,&#8239;...&#8239;,&#8239;uk} that generate the same subspace as the vectors v1,&#8239;...&#8239;,&#8239;vk. Every vector in the new set is orthogonal to every other vector in the new set; and the new set and the old set have the same linear span.|$|E
40|$|A {{new method}} is presented, {{which makes it}} {{possible}} to partition molecular properties like multipole moments and polarizabilities, into atomic and interatomic contributions. The method requires a subdivision of the atomic basis set into occupied and virtual basis functions for each atom in the molecular system. The localization procedure is organized into a series of <b>orthogonalizations</b> of the original basis set, which will have as a final result a localized orthonormal basis set. The new localization procedure is demonstrated to be stable with various basis sets, and to provide physically meaningful localized properties. Transferability of the methyl properties for the alkane series and of the carbon and hydrogen properties for the benzene, naphtalene, and anthracene series is demonstrated...|$|R
40|$|Abstract. A {{polynomial}} filtered Davidson-type {{algorithm is}} proposed for symmetric eigenproblems, in which the correction-equation of the Davidson approach {{is replaced by a}} polynomial filtering step. The new approach has better global convergence and robustness properties when compared with standard Davidson-type methods. The typical filter used in this paper is based on Chebyshev polynomials. The goal of the polynomial filter is to amplify components of the desired eigenvectors in the subspace, which has the effect of reducing both the number of steps required for convergence and the cost in <b>orthogonalizations</b> and restarts. Numerical results are presented to show the effectiveness of the proposed approach. Key words. Polynomial filter, Davidson-type method, global convergence, Krylov subspace, correction-equation, eigenproblem...|$|R
40|$|AbstractRecently the GMRESR {{method for}} the {{solution}} of linear systems of equations has been introduced by Vuik and Van der Vorst (1991). Similar methods have been proposed by Axelsson and Vassilevski (1991) and Saad (1993) (FGMRES 11 Since FGMRES and GMRESR are very similar, the ideas presented will be relevant for FGMRES as well.). GMRESR involves an outer and an inner method. The outer method is GCR, {{which is used to}} compute the optimal approximation over a given set of search vectors {{in the sense that the}} residual is minimized. The inner method is GMRES, which computes a new search vector by approximately solving the residual equation. This search vector is then used by the outer algorithm to compute a new approximation. However, the optimality of the approximation over the space of search vectors is ignored in the inner GMRES algorithm. This leads to suboptimal corrections to the solution in the outer algorithm. Therefore, we propose to preserve the orthogonality relations of GCR in the inner GMRES algorithm. This gives optimal corrections to the solution and also leads to solving the residual equation in a smaller subspace and with an “improved” operator, which should also lead to faster convergence. However, this involves using Krylov methods with a singular, nonsymmetric operator. We will discuss some important properties of this. We will show by experiments that in terms of matrix-vector products, this modification (almost) always leads to better convergence. Because we do more <b>orthogonalizations,</b> it does not always give an improved performance in time. This depends on the costs of the matrix-vector products relative to the costs of the <b>orthogonalizations.</b> Of course, we can also use methods other than GMRES as the inner method. Methods with short recurrences like BiCGSTAB seem especially interesting. The experimental results indicate that, especially for such methods, it is advantageous to preserve the orthogonality in the inner method...|$|R
50|$|When {{performing}} <b>orthogonalization</b> on a computer, the Householder {{transformation is}} usually preferred over the Gram-Schmidt process {{since it is}} more numerically stable, i.e. rounding errors tend to have less serious effects.|$|E
50|$|In quantum {{mechanics}} {{there are several}} <b>orthogonalization</b> schemes with characteristics better suited for certain applications than original Gram-Schmidt. Nevertheless, it remains a popular and effective algorithm for even the largest electronic structure calculations.|$|E
50|$|A former {{graduate}} student under Ivar Waller, Löwdin formulated in 1950 the symmetric <b>orthogonalization</b> scheme for molecular orbital calculations. This scheme {{is the basis}} of the zero-differential overlap (ZDO) approximation used in semiempirical theories.|$|E
40|$|We {{show how}} the method of {{successive}} partial <b>orthogonalizations</b> {{can be used to}} obtain approximate Hartree-Fock (HF) pair functions directly from the hydrogenic pair functions. The method is more practical than accurate but with the explicit Z-dependence of the bar -nuclei pair functions just eleven such pair functions are needed to obtain an approximate HF pair function by this method for any pair of orbitals in any first row atom. We use the same method of analysis to extract HF orbitals directly from suitable trial functions for the He-He system at several internuclear distances and and also for the auto-ionizing 2 s^ 2 2 p^ 2 states of helium. This approach avoids a separate variational calculation for the HF wave function and furthermore it provides approximate HF pair functions for these auto-ionizing states...|$|R
40|$|We {{consider}} {{the problem of}} matrix transpose on mesh-connected processor networks. On the theoretical side, we present the first optimal algorithm for matrix transpose on twodimensional meshes. Then we consider issues on implementations, show that the theoretical best bound cannot be achieved and present an alternative approach that really improves the practical performance. Finally, we introduce the concept of <b>orthogonalizations,</b> which are generalization of matrix transposes. We show how to realize them efficiently and present interesting applications of this new technique. 1 Introduction Various models for parallel machines have been considered. Among the best studied machines with fixed interconnection networks, are meshes. Although meshes have a large diameter compared to hypercubic networks, they are of great importance because of their simple structure and efficient layout. In a d-dimensional mesh, the processing units, PUs, form an array of size n ΘΔΔΔ. ̇...|$|R
40|$|A {{polynomial}} filtered Davidson-type {{algorithm is}} proposed for solving symmetric eigenproblems. The correction-equation of the Davidson approach {{is replaced by a}} polynomial filtering step. The new approach has better global convergence and robustness properties when compared with standard Davidson-type methods. A typical filter, one that is used in this paper, is based on Chebyshev polynomials. The goal of the polynomial filter is to amplify components of the desired eigenvectors in the subspace, which has the effect of reducing the number of steps required for convergence and the cost resulting from <b>orthogonalizations</b> and restarts. Comparisons with JDQR, JDCG and LOBPCG methods are presented, as well as comparisons with the well-known ARPACK package. Key words. Polynomial filter, Davidson-type method, global convergence, Krylov subspace, correction-equation, eigenproblem. AMS subject classifications. 15 A 18, 15 A 23, 15 A 90, 65 F 15, 65 F 25, 65 F 5...|$|R
50|$|<b>Orthogonalization</b> is also {{possible}} with respect to any symmetric bilinear form (not necessarily an inner product, not necessarily over real numbers), but standard algorithms may encounter division by zero in this more general setting.|$|E
50|$|Various basis {{sets are}} used in practice, {{most of which are}} {{composed}} of Gaussian functions. In some applications, an <b>orthogonalization</b> method such as the Gram-Schmidt process is performed in order to produce a set of orthogonal basis functions. This can in principle save computational time when the computer is solving the Roothaan-Hall equations by converting the overlap matrix effectively to an identity matrix. However, in most modern computer programs for molecular Hartree-Fock calculations this procedure is not followed due to the high numerical cost of <b>orthogonalization</b> and the advent of more efficient, often sparse, algorithms for solving the generalized eigenvalue problem, of which the Roothaan-Hall equations are an example.|$|E
50|$|For the {{definition}} of a vector space and some further properties I will refer to the article Linear Algebra and Gram-Schmidt <b>Orthogonalization</b> or any textbook in linear algebra and mention only the most important facts for understanding the model.|$|E
40|$|This report {{presents}} preconditioning {{techniques for}} the conjugate gradient method (CG), an iterative method for {{the solution of}} large sparse symmetric positive definite systems. The availability of massively parallel processors draws attention to parallel preconditioners. In this context, polynomial preconditioning is shown to be suitable. The preconditioner increases the number of matrix-vector products, whereas {{the total number of}} iterations and therefore the number of dot products decreases. The parallel computation of matrix-vector products usually results in communication with a small number of processors. Dot products, however, require global synchronisation. This reduction also results in better stability of the CG-method because fewer <b>orthogonalizations</b> are executed. The implementation is based on Chebyshev polynomials; performance tests were carried out on two massively parallel computer systems of the Research Centre Juelich (KFA). (orig.) SIGLEAvailable from TIB Hannover: RA 831 (2913) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|Chaos and {{regularity}} {{are routinely}} discriminated by using Lyapunov exponents distilled {{from the norm}} of orthogonalized Lyapunov vectors, propagated during the temporal evolution of the dynamics. Such exponents are mean-field-like averages that, for each degree of freedom, squeeze the whole temporal evolution complexity into just a single number. However, Lyapunov vectors also contain a step-by-step record of what exactly happens with the angles between stable and unstable manifolds during the whole evolution, a big-data information permanently erased by repeated <b>orthogonalizations.</b> Here, we study changes of angles between invariant subspaces as observed during temporal evolution of Henon's system. Such angles are calculated numerically and analytically and used to characterize self-similarity of a chaotic attractor. In addition, we show how standard tools of dynamical systems may be angle-enhanced by dressing them with informations not difficult to extract. Such angle-enhanced tools reveal unexpected and practical facts that are described in detail. For instance, we present a video showing an angle-enhanced bifurcation diagram that exposes from several perspectives the complex geometrical features underlying the attractors. We believe such findings to be generic for extended classes of systems...|$|R
40|$|Krylov {{subspace}} {{recycling is}} a process for accelerating the convergence of sequences of linear systems. Based on this technique, the recycling BiCG algorithm has been developed recently. Here, we now generalize and extend this recycling theory to BiCGSTAB. Recycling BiCG focuses on efficiently solving sequences of dual linear systems, while the focus here is on efficiently solving sequences of single linear systems (assuming non-symmetric matrices for both recycling BiCG and recycling BiCGSTAB). As compared with other methods for solving sequences of single linear systems with non-symmetric matrices (e. g., recycling variants of GMRES), BiCG based recycling algorithms, like recycling BiCGSTAB, have the advantage that they involve a short-term recurrence, and hence, do not suffer from storage issues and are also cheaper with respect to the <b>orthogonalizations.</b> We modify the BiCGSTAB algorithm to use a recycle space, which is built from left and right approximate invariant subspaces. Using our algorithm for a parametric model order reduction example gives good results. We show about 40 % savings in the number of matrix-vector products and about 35 % savings in runtime. Comment: 18 pages, 5 figures, Extended version of Max Planck Institute report (MPIMD/ 13 - 21...|$|R
