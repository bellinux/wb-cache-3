13|7|Public
25|$|Shure {{introduced}} the FP31 mixer in 1983. The FP31 was smaller and lighter than similar {{products of the}} time—small enough to hold {{in the palm of}} the hand and weighing just 2.2 pounds. This positioned it to complement the one-piece Sony Betacam video camera, which had been widely adopted by remote video broadcast crews. The FP31 could operate up to eight hours on two standard 9-volt batteries, and included two separate microphone/line outputs for two-camera video shoots. Its master section featured an adjustable threshold limiter to prevent <b>overload</b> <b>distortion,</b> and there was a separate microphone/line switch with low-cut filter on each channel. By 1984, just a year after its introduction, the FP31 was being used by ABC, CBS, NBC, Turner Broadcasting System, and was later succeeded by the Shure FP33.|$|E
5000|$|... #Subtitle level 2: Granular {{distortion}} and <b>overload</b> <b>distortion</b> ...|$|E
5000|$|Wilhelm Furtwängler {{conducting}} the Lucerne Festival orchestra and chorus live in concert, with soloists Elisabeth Schwarzkopf and Hans Hotter, also recorded in 1948. The recording is incomplete, however, and suffers from severe wow, surface noise, and <b>overload</b> <b>distortion</b> ...|$|E
50|$|A roofing filter {{is a type}} of filter used in a HF radio {{receiver}}. It {{is usually}} found after the first receiver mixer. The goal of a roofing filter is to limit the passband of the first intermediate frequency (IF) stage. Strong signals outside the channel which may cause <b>overloading</b> and <b>distortion</b> by the following amplifier stages and mixers are blocked.|$|R
5000|$|With {{other types}} of {{modulation}} like FM or FSK the amplitude of the modulation does not vary with the radio signal strength, but in all types the demodulator requires a certain range of signal amplitude to operate properly. [...] Insufficient signal amplitude will cause an increase of noise in the demodulator, while excessive signal amplitude will cause amplifier stages to <b>overload</b> (saturate), causing <b>distortion</b> (clipping) of the signal.|$|R
40|$|The {{advent of}} new {{standards}} in wireless communication like the Long Term Evolution (LTE) {{has resulted in}} a need for newer and better design of receivers for wireless communication systems, the first step of which is to design a tunable integrated filter on the receiver front end. In this work we propose a new design for a passive tunable integrated Roofing filter for LTE bands. The role of the Roofing filter is to protect the rest of the circuitry from <b>overloading</b> and <b>distortions</b> caused due to large out-of-band signals. This filter protects the rest of the circuitry and hence it gets the name Roofing filter. The Roofing filter is present on the receiver front-end. The filter has a low insertion loss and a high return loss at the input. The bandwidth of the Roofing filter is around 200 MHz at the highest values. The filter uses off-chip inductors. The filter has a continuous center frequency tuning range of 2 GHz from 0. 7 GHz to 2. 7 GHz, which is the allocated frequency range for LTE bands. This continuous tuning is achieved by the use of MOSFET based varactors. The filter is a narrowband filter. The design is implemented in TSMC 65 nm CMOS technology...|$|R
5000|$|Often {{the design}} of a {{quantizer}} involves supporting only a limited range of possible output values and performing clipping to limit the output to this range whenever the input exceeds the supported range. The error introduced by this clipping is referred to as <b>overload</b> <b>distortion.</b> Within the extreme limits of the supported range, the amount of spacing between the selectable output values of a quantizer is referred to as its granularity, and the error introduced by this spacing is referred to as granular distortion. It is common for {{the design of}} a quantizer to involve determining the proper balance between granular distortion and <b>overload</b> <b>distortion.</b> For a given supported number of possible output values, reducing the average granular distortion may involve increasing the average <b>overload</b> <b>distortion,</b> and vice versa. A technique for controlling the amplitude of the signal (or, equivalently, the quantization step size [...] ) to achieve the appropriate balance is the use of automatic gain control (AGC). However, in some quantizer designs, the concepts of granular error and overload error may not apply (e.g., for a quantizer with a limited range of input data or with a countably infinite set of selectable output values).|$|E
50|$|Mid-1970s {{and later}} {{condenser}} microphones designed for 48-volt phantom powering often require much more current (e.g., 2-4 mA for Neumann transformerless microphones, 4-5 mA for the Schoeps CMC ("Colette") series and Josephson microphones, 5-6 mA for most Shure KSM-series microphones, 8 mA for CAD Equiteks and 10 mA for Earthworks). The IEC standard gives 10 mA as the maximum allowed current per microphone. If its required current is not available, a microphone may still {{put out a}} signal, but it cannot deliver its intended level of performance. The specific symptoms vary somewhat, but the most common result will be reduction of the maximum sound-pressure level that the microphone can handle without <b>overload</b> (<b>distortion).</b> Some microphones will also show lower sensitivity (output level for a given sound-pressure level).|$|E
50|$|Shure {{introduced}} the FP31 mixer in 1983. The FP31 was smaller and lighter than similar {{products of the}} time—small enough to hold {{in the palm of}} the hand and weighing just 2.2 pounds. This positioned it to complement the one-piece Sony Betacam video camera, which had been widely adopted by remote video broadcast crews. The FP31 could operate up to eight hours on two standard 9-volt batteries, and included two separate microphone/line outputs for two-camera video shoots. Its master section featured an adjustable threshold limiter to prevent <b>overload</b> <b>distortion,</b> and there was a separate microphone/line switch with low-cut filter on each channel. By 1984, just a year after its introduction, the FP31 was being used by ABC, CBS, NBC, Turner Broadcasting System, and was later succeeded by the Shure FP33.|$|E
50|$|Access to 'real jurors' is {{difficult}} to attain, and observation of jurors whilst performing their duty is prohibited {{for a variety of}} reasons, the most prominent of which is a reluctance to allow any imposition on the jurors whilst performing their duty, which might affect the trial's result. As such access to jurors, if allowed, is generally after they have been dismissed which raises difficulties with issues such as memory <b>distortion,</b> <b>overload</b> from stress or other factors which were more pertinent to the juror while on the jury; inability to recognise the influence of demographic factors on their deliberations, etc. These are all factors or variables which are difficult to tease out of the data, but the influence of which can be adjusted in the analysis of the data if the model is theoretically sound.|$|R
40|$|We thank Dr DeGroff for his {{comments}} on our study of ventric-ular mechanics during repair of tetralogy of Fallot. 1 We {{acknowledge that the}} use of 2 -dimensional echo sections to represent the 3 -dimensional left ventricle (LV) is limiting. The degree of error introduced by this compromise is unknown in the absence of more comprehensive imaging. However, we believe that our conclusions are essentially correct. As noted, Matsumori et al 2 defined limitations of a biplane Simpson rule for LV volume determination in the presence of right ventricular (RV) pressure and volume overload. A modified Simpson rule, including area planimetry of the LV short axis at 2 levels, was more accurate than a biplane model of the LV based on diameters and a prolate ellipsoid. Matsumori’s data also indicated that errors of echo algorithms versus angiography were qualitatively larger for RV volume <b>overload,</b> where septal <b>distortion</b> is greatest, than for R...|$|R
30|$|Naor and Shamir [1] {{proposed}} a model representation for visual secret sharing schemes in 1994, {{which is also}} referred to as the deterministic visual secret sharing model [9]. Multiple pixels are used to reconstruct one pixel of the original secret image. Thus, recent studies focus on size invariant visual secret sharing schemes [2, 10 – 14] to avoid the storage <b>overload</b> and dimension <b>distortion</b> caused by such pixel expansion [15] in the deterministic models. The earliest size invariant visual secret sharing scheme was proposed by Kafri and Keren [16] in 1987, named as “encryption of pictures by random grids” at that time. Several approaches have been proposed recently to perform size invariant visual secret sharing, such as the random grid-based visual secret sharing (RG-based VSS) [2, 10 – 14], the probabilistic visual cryptography (ProbVC) [17 – 19], and the multiple pixel sharing scheme [20 – 22]. This paper uses the RG-based VSS as our main testing model. The experiment using Naor and Shamir’s deterministic model is also provided at the end part of this paper to demonstrate that our metric could also be applied to general visual secret sharing schemes with pixel expansion.|$|R
5000|$|In the 1980s, digital {{recording}} methods were introduced, and analog tape recording was gradually displaced, {{although it has}} not disappeared by any means. (Many professional studios, particularly those catering to big-budget clients, use analog recorders for multitracking and/or mixdown.) Digital audio tape never became important as a consumer recording medium partially due to legal complications arising from [...] "piracy" [...] fears {{on the part of}} the record companies. They had opposed magnetic tape recording when it first became available to consumers, but the technical difficulty of juggling recording levels, <b>overload</b> <b>distortion,</b> and residual tape hiss was sufficiently high that unlicensed reproduction of magnetic tape never became an insurmountable commercial problem. With digital methods, copies of recordings could be exact, and copyright infringement might have become a serious commercial problem. Digital tape is still used in professional situations and the DAT variant has found a home in computer data backup applications. Many professional and home recordists now use hard-disk-based systems for recording, burning the final mixes to recordable CDs (CD-R's).|$|E
40|$|Asymptotic {{expressions}} for {{the optimal}} scaling factor and resulting minimum distortion, {{as a function}} of codebook size N, are found for fixed-rate k-dimensional lattice vector quantization of generalized Gaussian sources with decay parameter ff 1. These expressions are derived by minimizing upper and lower bounds to distortion. It is shown that the optimal scaling factor aN decreases as (ln N) 1 =ff N Γ 1 =k and that for scale-optimized lattice quantization, granular distortion asymptotically dominates <b>overload</b> <b>distortion.</b> Consequently, the minimum distortion is DN ¸ = c (ln N) 2 =ff N Γ 2 =k. This result indicates that the distortion of optimal lattice quantizers diverges from that of asymptotically optimal vector quantization, as N increases. Index terms: optimal lattice scaling, granular distortion, <b>overload</b> <b>distortion,</b> high resolution quantization theory, distortion bounds. This work was supported by an NSF Graduate Fellowship and by NSF Grant NCR- 9415754. Part [...] ...|$|E
40|$|Linear delta {{modulation}} systems use a fixed step size for the 'staircase' approximation of the input signal. Small {{values of the}} step size introduce slope <b>overload</b> <b>distortion</b> during bursts of large signal slope. With large values of step size input signals of low amplitude were quantized very coarsely. This results in an inadequate dynamic range. To increase the dynamic range, the step size must be controlled from the time-varying slope characteristic of the input signal...|$|E
40|$|This paper {{studies the}} {{asymptotic}} characteristics of uniform scalar quantizers that are optimal {{with respect to}} mean squared error. It is shown that when a symmetric source density with infinite support is sufficiently well behaved, the optimal step size D N for symmetric uniform scalar quantization decreases as 2 s N - 1 V - 1 1 / 6 N 2 (), where N {{is the number of}} quantization levels, s 2 is the source variance and V - 1 () is the inverse of V (y) = y - 1 P(s - 1 X > x) dx y. Equivalently, the optimal support length ND N increases as 2 s V - 1 1 / 6 N 2 (). Granular distortion is asymptotically well approximated by D N 2 / 12, and the ratio of <b>overload</b> to granular <b>distortion</b> converges to a function of the limit t lim y y - 1 E[X| X > y], provided, as usually happens, that t exists. When it does, its value is related to the number of finite moments of the source density; an asymptotic formula for the overall distortion D N is obtained; and t = 1 is both n [...] ...|$|R
40|$|This paper {{presents}} three heuristic {{methods for}} finding the support regions of truly optimal scalar quantizers. These {{are based on}} differentiation, exponent balancing, and the equi-distortion principle, respectively. In the case of generalized gamma densities these methods allow closed form estimates for the optimal support. The estimates are shown to be more accurate than the best prior estimates, as found by numerical algorithms rather than closed form expressions. They demonstrate that the support of an optimal quantizer {{is larger than the}} minimal asymptotically optimal support by a factor depending on the density but not N, and that the outer distortion of optimal quantizers decreases as 1 =N 3. Index terms: support region, key parameters, generalized gamma density, asymptotic quantization theory, high-resolution quantization theory, inner distortion, outer distortion, <b>overload</b> <b>distortion.</b> S. Na is with the Department of Electronics Engineering, Ajou University, Suwon, Korea. [...] ...|$|E
40|$|This paper {{determines how}} the support regions of {{asymptotically}} optimal scalar quantizers (with respect to mean square error) {{depend on the}} number of quantization points N and the probability density of the variable being quantized. It shows that, for asymptotic optimality, it is necessary and sufficient that the support region grow fast enough that the outer (or <b>overload)</b> <b>distortion</b> decreases as o(1 =N 2). Formulas are derived for the minimal support of asymptotically optimal quantizers for generalized gamma densities, including Gaussian and Laplacian. Interestingly, these turn out to be essentially the same as for the support of optimal uniform quantizers. Furthermore, for light-tailed generalized gamma densities, a sequence of intervals can be slightly smaller than the optimal uniform support and still be asymptotically optimal for nonuniform quantization. Index terms: support region, key parameters, generalized gamma density, asymptotic quantization theory, high-resolution quanti [...] ...|$|E
40|$|This paper {{determines how}} the support regions of optimal and {{asymptotically}} optimal fixed-rate scalar quantizers (with respect to mean squared error) {{depend on the}} number of quantization points N and the probability density of the variable being quantized. It shows that for asymptotic optimality it is necessary and sufficient that the support region grow fast enough that the outer (or <b>overload)</b> <b>distortion</b> decreases as o(1 /N&sup 2;). Formulas are derived for the minimal support of asymptotically optimal quantizers for generalized gamma densities, including Gaussian and Laplacian. Interestingly, these turn out to be essentially the same as for the support of optimal fixed-rate uniform scalar quantizers. Heuristic arguments are then used to find closed form estimates for the support of truly optimal quantizers for generalized gamma densities. These are found to be more accurate than the best prior estimates, as computed by numerical algorithms. They demonstrate that the support of an optimal quantizer is larger than the minimal asymptotically optimal support by a factor depending on the density but not N, and that the outer distortion of optimal quantizers decreases as 1 /N&sup 3;...|$|E
40|$|The need to {{transmit}} {{large amounts of}} data over limited bandwidth channels has resulted in many methods for digital data compression. The common approach is to identify and remove redundancy from the input data stream using knowledge of the source characteristics. In the case of signals intended for human observers (speech, music, pictures, etc.) it is also useful to consider {{the strengths and weaknesses}} of the human sensory systems in order to achieve a greater degree of data compression. Unfortunately, achieving perceptually transparent compression requires considerable computational resources. For situations requiring extremely low computational complexity without strictly transparent coding, such as multimedia applications on personal computer platforms, a new adaptive differential pulse code modulation (DPCM) data compression scheme is proposed. Although standard DPCM structures are widely used in singletalker speech coding systems, the models and statistical assumptions well-known for speech signals are not applicable to arbitrary audio signals such as music. The new DPCM formulation presented here includes a recursively indexed quantizer (RIQ) to eliminate the problem of <b>overload</b> <b>distortion,</b> a simple predictor structure to take advantage of the short-term correlation present in wideband audio signals, and an adaptation strategy to optimize the system to the local statistics of the input signal. Thus, the new RIQ-DPCM formulation is presented as a computationally efficient means of wideband audio compression. 1...|$|E

