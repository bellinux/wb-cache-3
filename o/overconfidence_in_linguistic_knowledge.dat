0|10000|Public
40|$|Minority students’ English {{learning}} is a special and an indispensable component of English education system in China. This article studies students’ <b>linguistic</b> <b>knowledge</b> that live <b>in</b> Northwestern China – Gan Nan Autonomy State of Gan Su Province with majority population of Tibetan, mixed with Chinese and some Muslim. An analogous analysis is conducted between L 2 students (Chinese students who learn English as a second Language) and L 3 students (Tibetan Students whose first language is Tibetan, second language is Chinese, and third Language is English) <b>in</b> English <b>Linguistic</b> <b>Knowledge.</b> The <b>linguistic</b> <b>knowledge</b> is constituted of vocabulary, grammar and reading skill. (Raykov,T,&Marcoulides,G. A, 2006) The paper concludes the remarkable difference exists between the Tibetan Students and Chinese students <b>in</b> <b>Linguistic</b> <b>Knowledge,</b> especially on vocabulary and grammar. The difference on reading skill is apparent, but not significant. The reasons that caused these distinctive or non-dramatic differences are explored and further discussed respectively. </p...|$|R
40|$|This study {{investigated}} how individual differences <b>in</b> <b>linguistic</b> <b>knowledge</b> and processing skills relate to {{individual differences in}} speaking fluency. Speakers of Dutch {{as a second language}} (N = 179) performed eight speaking tasks, from which several measures of fluency were derived such as measures for pausing, repairing, and speed (mean syllable duration). In addition, participants performed separate tasks, designed to gauge individuals’ second language <b>linguistic</b> <b>knowledge</b> and <b>linguistic</b> processing speed. The results showed that the linguistic skills were most strongly related to average syllable duration, of which 50 % of individual variance was explained; in contrast, average pausing duration was only weakly related to <b>linguistic</b> <b>knowledge</b> and processing skills...|$|R
3000|$|Are {{there any}} {{statistically}} {{significant differences in the}} students’ gain <b>in</b> <b>linguistic</b> and pragmatic <b>knowledge,</b> which can be attributed to instruction? [...]...|$|R
40|$|Feature {{structures}} play {{an important}} role <b>in</b> <b>linguistic</b> <b>knowledge</b> representation <b>in</b> computational linguistics. Given the proliferation of different feature structure formalisms it is useful to have a &quot;common language &quot; to express them in. This paper shows how a variety of feature structures and constraints on them can be expressed in predicate logic (except for the use of circumscription for non-monotonic devices), including sorted feature values, subsumption constraints and the non-monotonic ANY values and &quot;constraint equations&quot;. Many feature systems can be completely axiomatized in the Schonfinkel-Bernays class of first-order formulae, so the decidability of the satisfiability and validity problems for these systems follows immediately. ...|$|R
30|$|The second {{research}} question sought potentially {{significant differences in}} the students’ gain <b>in</b> <b>linguistic</b> and pragmatic <b>knowledge.</b> Statistically significant differences were found in the FFI group’s performance on the GST and DCT, but more so on the DCT, which may suggest that FFI has a greater effect on pragmatic than on <b>linguistic</b> <b>knowledge</b> acquisition.|$|R
50|$|This skill {{made that}} people {{entrusted}} him missions <b>in</b> which his <b>linguistic</b> <b>knowledge</b> was necessary, which simultaneously {{allowed him to}} keep on increasing his data gathering, collecting information from the authentic sources of all the places he was visiting.|$|R
40|$|In {{this paper}} {{we present a}} {{computer}} program PROSO a linguistic rule compiler which applies a number of prosodic rules inserting appropriate markers to any text inputted, {{on the basis of}} phonological rules and of syntactic structure, this one computed separately by an RTN parser(see Delmonte & Dolci, 1989) and made visible to PROSO only at a certain structural level, though. The output of this program can then be passed on to a system like VOXPC, the text-to-speech module commercialized by Olivetti, which allows the user to provide explicit phonetic and prosodic information in order to modify the internal coding procedures. VOXPC is a system completely lacking <b>in</b> <b>linguistic</b> <b>knowledge</b> apart from the well-known distinction between content and function words: the quality of the system is very poor both at word level and at sentence and text level...|$|R
40|$|Moses is a {{well-known}} representative of the phrase-based statistical machine translation systems family, which {{are known to be}} extremely poor <b>in</b> explicit <b>linguistic</b> <b>knowledge,</b> operating on flat language representations, consisting only of tokens and phrases. Treex, on the other hand, is a highly linguistically motivated NLP toolkit, operating on several layers of language representation, rich <b>in</b> <b>linguistic</b> annotations. Its main application is TectoMT, a hybrid machine translation system with deep syntax transfer. We review a large number of machine translation systems that have been built over the past years by combining Moses and Treex/TectoMT in various ways...|$|R
40|$|Whereas {{school and}} society pose high demands on youngsters’ {{reading and writing}} skills, many {{adolescents}} experience difficulties in understanding what they read and in expressing their thoughts in comprehensible texts. Especially low-achieving students in the lowest educational tracks in the Netherlands experience such difficulties. This dissertation addresses the roles of various types {{of knowledge and skills}} that are important to the reading comprehension and writing proficiency of these low-achieving adolescents. Three empirical studies focus on individual differences in Dutch reading comprehension and writing proficiency of 51 low-achieving adolescents, and {{on the extent to which}} these differences can be explained by individual differences <b>in</b> <b>linguistic</b> <b>knowledge,</b> fluency, and metacognitive knowledge. In addition, the development in reading comprehension and writing proficiency of these students from Grade 7 to 9 is analyzed, as well as contributions of <b>linguistic</b> <b>knowledge,</b> fluency, and metacognitive knowledge to this development. Since language-minority students form a large proportion of low-achieving adolescents, differences between language-minority and native students in the sample are also investigated. The results show the progress low-achieving adolescents make in reading comprehension and writing proficiency between Grade 7 and Grade 9. The findings also provide unique insights into the contributions of several components of reading and writing to this progress...|$|R
40|$|International audienceTwo {{recent studies}} have shown that pigeons and baboons can {{discriminate}} written English words from nonwords, and these findings were interpreted as demonstrating that orthographic processing is possible <b>in</b> absence of <b>linguistic</b> <b>knowledge.</b> Here, I emphasize a different idea, which is that these studies also inform comparative psychologists on the evolutionary history of statistical learning in nonhuman animals, and on its pervasiveness and flexibility...|$|R
40|$|This paper {{examines}} {{the relevance of}} differential language expertise in ordinary conversation between speakers of Japanese as a first and second language. Adopting a conversation analytic perspective, the study focuses on other-repair as one sequential {{environment in which the}} participants recurrently orient to their differential <b>linguistic</b> <b>knowledge.</b> Specifically, it will be shown that language expertise was made relevant (a) when one participant invited the other party’s repair and (b) when the participants encountered a problem in achieving mutual understanding. On such occasions, the interlocutors oriented to the differences <b>in</b> their <b>linguistic</b> <b>knowledge</b> through their talk and other interactional conduct. The study thus provides evidence for differential language expertise as a participant category that emerges on occasion but bears no relevance for the participants during most of their talk...|$|R
40|$|The {{so-called}} floating quantifier constructions in languages like Korean display intriguing properties whose successful processing {{can prove}} the robustness of a parsing system. This paper {{shows that a}} constraint-based analysis, in particular couched upon the framework of HPSG, can offer us an efficient way of analyzing these constructions together with proper semantic representations. It also shows how the analysis has been successfully implemented <b>in</b> the LKB (<b>Linguistic</b> <b>Knowledge</b> Building) system. ...|$|R
40|$|With {{the rapid}} {{evolution}} of the Internet, translation {{has become part of}} the daily life of ordinary users, not only of professional translators. Machine translation has evolved along with different types of computer-assisted translation tools. Qualitative progress has been made in the field of machine translation, but not all problems have been solved. The current times are auspicious for the development of more sophisticated evaluation tools that measure the performance of specific linguistic phenomena. One problem in particular, namely the poor analysis and translation of multi-word units, is an arena where investment <b>in</b> <b>linguistic</b> <b>knowledge</b> systems with the goal of improving machine translation would be beneficial. This paper addresses the difficulties multi-word units present to machine translation, by comparing translations performed by systems adopting different approaches to machine translation. It proposes a solution for improving the quality of the translation of multi-word units by adopting a methodology that combines Lexicon Grammar resources with OpenLogos lexical resources and semantico-syntactic rules. Finally, it discusses how an ideal machine translation evaluation tool might look to correctly evaluate the performance of machine translation engines with regards to multi-word units and thus to contribute to the improvement of translation quality...|$|R
40|$|Is {{the basic}} {{mechanism}} behind presupposition projection fundamentally asymmetric or symmetric? This {{is a basic}} question for the theory of presupposition, which also bears on broader issues concerning the source of asymmetries observed in natural language: are these simply rooted in superficial asymmetries of language use— language use unfolds in time, which we experience as fundamentally asymmetric— or can they be, at least in part, directly referenced <b>in</b> <b>linguistic</b> <b>knowledge</b> and representations? <b>In</b> this paper we aim to make progress on these questions by exploring presupposition projection across conjunction, which has typically been taken as a central {{piece of evidence that}} presupposition projection is asymmetric. As a number of authors have recently pointed out, however, whether or not this conclusion is warranted is not clear once we take into account independent issues of redundancy. Building on previous work by Chemla 2 ̆ 6 Schlenker (2012) and Schwarz (2015), we approach this question experimentally by using an inference task which controls for redundancy and presupposition suspension. We find strong evidence for left-to-right filtering across conjunctions, but no evidence for right-to-left filtering, suggesting that, at least as a default, presupposition projection across conjunction is indeed asymmetric...|$|R
40|$|Machine {{translation}} {{should be}} semanticalty-accurate, linguistically-principled, user-interactive, and extensible to multiple languages and domains. This paper presents the universal parser architecture that strives {{to meet these}} objectives. <b>In</b> essence, <b>linguistic</b> <b>knowledge</b> bases (syntactic, semantic, lexical, pragmatic), encoded in theoretically-motivated formalisms such as lexical-functional grammars, are unified and precompiled into fast run-time grammars for parsing and generation. Thus, the universal parser provides principled run-time integration of syntax and semantics, while preserving the generality of domain-independent syntactic grammars, and language-independent domain knowledge bases; the optimized cross product is generated automatically in the precornpllation phase. Initial results for bi-directional English-Japanese translation show considerable promise {{both in terms of}} demonstrating the theoretica...|$|R
40|$|In Korean {{morphological}} analysis, {{one reason}} of morphological over-analysis {{is lack of}} ordering restrictions in the connectivity information table used as morphotactics in many cases. To alleviate this problem, we use a subsumption relation {{as a kind of}} <b>linguistic</b> <b>knowledge.</b> <b>In</b> this paper, we define the subsumption relation and propose a method to reduce morphological ambiguities using the subsumption relation. Our experiment shows very promising results. We expect that the results may be positively reflected to probabilistic part-of-speech tagging systems with some difficulty <b>in</b> incorporating <b>linguistic</b> <b>knowledge.</b> 1 Introduction <b>In</b> agglutinative languages such as Korean, morphological analysis plays an important role since a word 1 can consist of several morphemes of which properties would include not only part-ofspeech (POS), but also syntactic cases and tenses. In Korean morphological analysis, there has been several approaches(Kim, 1992), which have many kinds of rules such as [...] ...|$|R
50|$|Later in the 1990s, the Philippine Bible Society {{realised}} a need {{to revise}} the Naimbag a Damag Biblia because of further advancement <b>in</b> <b>linguistic</b> and archaeological <b>knowledge.</b> Hence the birth of Ti Baro a Naimbag a Damag Biblia (The New Good News Bible). Like the earlier version, it used the original languages as the textual base and was jointly translated by Roman Catholic and Protestant scholars.|$|R
40|$|PURPOSE: The {{hypothesis}} that the linguistic deficit presented by children with specific language impairment (SLI) is caused by limited cognitive resources (e. g., S. Ellis Weismer & L. Hesketh, 1996) was tested against the hypothesis of a limitation <b>in</b> <b>linguistic</b> <b>knowledge</b> (e. g., M. L. Rice, K. Wexler, & P. Cleave, 1995). METHOD: The study examined {{the influence of the}} argument-structure complexity of a target sentence on the production of grammatical morphemes in French children with SLI compared with younger children matched for grammatical level in production (GL) and children of the same chronological age (CA). A sentence production task was used where the target sentences varied in terms of argument complexity and length. RESULTS: The results indicated that children with SLI used articles and auxiliaries in obligatory contexts significantly less often than both the GL and CA control groups: More complex argument structures elicited the highest number of grammatical morpheme omissions; this effect was larger in children with SLI than in the GL group and was independent of the length of the sentences, which failed to show any influence on the production of grammatical morphemes. CONCLUSION: These results support the {{hypothesis that}} grammatical-morpheme deficit in SLI depends at least in part on limited processing capacities...|$|R
40|$|Qualitative {{modeling}} may {{be applied}} when knowledge about {{a system is}} only available <b>in</b> <b>linguistic</b> form. The <b>knowledge</b> might be processed by a dynamic fuzzy system consisting of a rule base and an inference method modeling human reasoning. Conventional fuzzy inference methods do not consider this association to human reasoning and therefore are not suitable for the dynamic processing of <b>linguistic</b> <b>knowledge.</b> Inference has to provide both, quantitative and qualitative information about the model output. In this paper a new inference method based {{on the concept of}} interpolating rules is presented. It results in a mapping of fuzzy inputs onto fuzzy outputs with a feedback to the input, considering in particular the informational content of the fuzzy sets. Thus, a dynamic process model is built from given <b>linguistic</b> <b>knowledge</b> <b>in</b> form of rules. Finally, a controller design method is presented based on the qualitative process model. KEYWORDS: qualitative reasoning, dynamic fuzzy system, inf [...] ...|$|R
40|$|This paper {{describes}} a semantic linguistic processor which extracts the entities and their links from natural language texts. The conceptual model underlying the algorithmic developments is the extended semantic networks (ESN). This paper analyzes {{the use of}} the processor for text formalization in various subject fields: economy monitoring, criminal actions, mass media, terrorist activities (in Russian and English). Peculiarities of the texts are taken into account by <b>linguistic</b> <b>knowledge</b> of the processor: the system can be tuned to various subject areas. We describe the use of this processor for text formalization in different subject areas, such as economic crisis monitoring, criminology (summary of incidents, accusatory conclusions, etc.), the mass media documents about terrorist activities, personnel management (autobiographies, resume). Special features of each problem area are examined: the collections of extracted entities, the means for their identification, their connections, occurring contractions, punctuation and special signs, specific character of language constructions, etc. - all these special features were taken into account <b>in</b> the <b>linguistic</b> <b>knowledge</b> development. ...|$|R
25|$|Hearing loss {{among the}} aged {{community}} lessens elders' ability {{to compensate for}} other age related social and/or physical problems. Communication problems of elderly adults can be greatly impacted by mechanical problems such as: the translation of ideas into linguistic representation or expression, the perception of linguistic stimuli or the derivation of an idea from a given unit of disclosure. Changes in these mechanical problems {{are more important than}} changes <b>in</b> <b>linguistic</b> <b>knowledge.</b> The main goal of hearing aids is to improve communication and quality of life, not just to restore hearing. Presbycusis {{is an example of a}} hearing deficit that cannot be corrected by hearing aids. Presbycusis, the alteration of hearing sensitivity associated with normal hearing loss, is caused by the decreased amount of hair cells of the inner ear. This is normally caused by long periods of distressing noise that diminish the hair cells which with increasing age will not grow back. Presbycusis and other such hearing-related problems promote social withdrawal, as individuals begin to lose touch with the world around them. Hearing loss among the aged community lessens elders' ability to compensate for other age-related social and/or physical problems. This impairment can cause elders to lose touch of social skills because they may have trouble keeping up with fast-paced or hearing different pitched voices in conversation.|$|R
30|$|This paper {{presents}} {{the development of}} a hybrid decision support model to solve multicriteria route selection problem. The developed model integrates AHP as a multiple criteria decision-making method to determine priorities among route selection criteria, and TOPSIS method to obtain the ranking of all possible alternative routes between a source and destination. The best fuzzy description of each alternative route is obtained using AFS theory and the performance scores of all route are determined by performing pairwise comparisons between each pair of routes over cost or benefit criteria as described in routes’ best fuzzy description. Finally, the resulting scores (decision matrix) are used to rank the routes using TOPSIS methodology. The AFS theory is incorporated into the model to overcome the uncertainty and ambiguity <b>in</b> <b>linguistic</b> <b>knowledge</b> representation. The main advantage of the developed hybrid model is that it processes the linguistic values using AFS theory that copes the inconsistency caused by different types of fuzzy numbers. The hybrid method also copes the inconsistency due to choice of different normalization methods. A hypothetical application of best route selection is presented to illustrate the utilization of hybrid model. Finally, a sensitivity analysis is performed. As a result of comparative analysis and model validation carried out for the developed model, it is found that the developed model is practical for selecting best route under multicriteria environment.|$|R
50|$|Hearing loss {{among the}} aged {{community}} lessens elders' ability {{to compensate for}} other age related social and/or physical problems. Communication problems of elderly adults can be greatly impacted by mechanical problems such as: the translation of ideas into linguistic representation or expression, the perception of linguistic stimuli or the derivation of an idea from a given unit of disclosure. Changes in these mechanical problems {{are more important than}} changes <b>in</b> <b>linguistic</b> <b>knowledge.</b> The main goal of hearing aids is to improve communication and quality of life, not just to restore hearing. Presbycusis {{is an example of a}} hearing deficit that cannot be corrected by hearing aids. Presbycusis, the alteration of hearing sensitivity associated with normal hearing loss, is caused by the decreased amount of hair cells of the inner ear. This is normally caused by long periods of distressing noise that diminish the hair cells which with increasing age will not grow back. Presbycusis and other such hearing-related problems promote social withdrawal, as individuals begin to lose touch with the world around them. Hearing loss among the aged community lessens elders' ability to compensate for other age-related social and/or physical problems. This impairment can cause elders to lose touch of social skills because they may have trouble keeping up with fast-paced or hearing different pitched voices in conversation.|$|R
40|$|In this {{position}} paper, I {{argue that in}} order to create truly language-independent NLP systems, we need to incorporate <b>linguistic</b> <b>knowledge.</b> The <b>linguistic</b> <b>knowledge</b> <b>in</b> question is not intricate rule systems, but generalizations from linguistic typology about the range of variation <b>in</b> <b>linguistic</b> structures across languages. ...|$|R
40|$|The Constituent-Context Model (CCM) {{achieves}} promising {{results for}} unsupervised grammar induction. However, its performance drops for longer sentences. In this paper, we describe a general feature-based model for CCM, <b>in</b> which <b>linguistic</b> <b>knowledge</b> {{can be easily}} integrated as features. Features take the log-linear form with local normalization, so the Expectation-Maximization (EM) algorithm is still applicable to estimate model parameters. The ℓ 1 -norm is used to control the model complexity, leading to sparse and compact grammar. We also propose to use a separated development to perform model selection and an additional test set to evaluate the performance. Under this framework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but significant improvement on longer sentences. ...|$|R
30|$|Drawing on {{proficiency}} {{and cognitive}} models from previous studies, Schoonen et al. (2003) also recognized that different components {{of knowledge and}} skills are fundamentally relevant to writer’s performance. Exploring the relative contribution of <b>linguistic</b> <b>knowledge</b> (grammar, vocabulary and orthography) and metacognitive knowledge at the writer’s resource level and the speed of processing at the writer’s process level to writing proficiency across L 1 and L 1, they collected data from 281 grade 8 students using writing tasks to measure L 1 and L 2 proficiency; tests on grammar, vocabulary, orthography, and metaknowledge to assess the writer’s resources in the memory; and speed of lexical retrieval and sentence building to measure the writers’ fluency <b>in</b> accessing <b>linguistic</b> <b>knowledge.</b> Their study claimed that L 2 linguistic tests highly correlated with L 2 writing proficiency than L 1 linguistic tests with L 1 writing proficiency did. Also, the comparison of the contributions of <b>knowledge</b> tests (<b>linguistic</b> and metaknowledge) vis-a-vis speed of processing, showed that speed of processing, although correlated to writing proficiency, had almost no unique contribution in predicting writing proficiency in L 1 and L 2.|$|R
40|$|Reading and Language Learning {{is based}} on the {{relationship}} between knowledge of a language and reading. As the editor, Keiko Koda, says on the back cover: "reading necessitates <b>linguistic</b> <b>knowledge</b> [and] reading ability enhances <b>linguistic</b> <b>knowledge</b> expansion. " According to Koda, reading is a complex and multifaceted skill. The chapters in the book address some aspects of this skill, or what Koda calls "sub-skills," emphasizing each one's relevance and also stressing reading as a full construct. She has put together six significant papers that appeared in Language Learning between 2002 and 2005 and were later published in 2007 as a supplement to issue 57 of the same journal. Chapter 1 serves as Koda's introduction. In it she intends to shed light on current reading research and, in particular, explore its relationship with second language acquisition and what the constraints that influence L 2 reading are within and across languages. The author states that these relations are sophisticated because reading is a complex construct in which goal language and mother tongue interact and interfere (p. 28). To address these constraints across operations in both languages, it is necessary to approach analytically the different aspects of this mental information processing: <b>linguistic</b> <b>knowledge</b> <b>in</b> decoding, <b>linguistic</b> <b>knowledge</b> <b>in</b> text information building, <b>linguistic</b> <b>knowledge</b> <b>in</b> reader model building (the mental schema). According to the author, reading analysis requires not only identifying these components but also doing so in the learner's L 1 and L 2. Next, this chapter addresses future directions in L 2 reading research: (1) expanding and addressing more reading subskills, (2) exploring dual-language involvement, and (3) considering a wider variety of learners according to their L 2 learning stage. Lastly, Koda announces how she selected the contributions that follow: "each represents a new approach to exploring critical issues in L 2 reading " (p. 31) ...|$|R
40|$|This paper {{describes}} a vision <b>in</b> which <b>linguistic</b> <b>knowledge</b> and theories are introduced into conceptual modeling, and it {{sums up the}} advantages achieved by this approach. We will show how the extension of conceptual modeling techniques with linguistic theories increases their expressive power, the capability to formalize wellknown conceptual aspects, like object roles and constraints, and their internal consistency. Furthermore, we will explain the advantages gained from using such an extended conceptual modeling technique by describing the adjustments and improvements of the modeling process itself and the extensions to the validation and verification process of the sophisticated models. We will demonstrate this approach by introducing a linguistically based modeling environment, C o l o R-X. R'esum'e 1 Introduction The idea of introducing Natural Language (Nl) theories and knowledge in the construction process of a Conceptual Model (Cm) of the current or a desired Universe of D [...] ...|$|R
40|$|This paper {{describes}} {{an approach to}} conceptual analysis and understanding of natural language <b>in</b> whlch <b>linguistic</b> <b>knowledge</b> centers on individual words, and the analysis mechanisms consist of interactions amonR distributed procedural experts representing that knowledge. Each word expert models the process of alagnosing the intended usage of a particular word in context. The Word Expert Parser perform conceptual analysis through the interactions of the individual experts, which ask questions ano exchange information in converging on a single mutually acceptable sentence meaning. The Word Exper theory is advanced as a better cognitive model of natural languaRe understanding than the traditional rule-based approaches. The Word Expert Parser models parts o[ire theory, and the important issues of control and representation that arise in developing such a model {{form the basis of}} the technical discussion. An example from the prototype LISP implementation helps explain the theoretical results presente...|$|R
40|$|This study {{explores the}} {{development}} of second language (L 2) fluency during a semester abroad {{and its relationship to}} {{the development of}} grammar, vocabulary, and language processing speed. It also considers the influence of individual participants 2 ̆ 7 first language (L 1) and pre-study abroad (SA) L 2 fluency on the development of fluency during study abroad. Additionally, the study examines issues in the measurement of fluency, focusing on questions related to measuring pauses in L 2 speech. Thirty-nine undergraduate students (L 1 English) studying in Buenos Aires, Argentina, completed a pretest consisting of speaking tasks in English and Spanish, Spanish grammar and vocabulary tests, a picture-naming task, and a measure of sentence processing speed. Approximately three months later, near the end of their time abroad, they completed a posttest consisting of the same tasks, {{with the exception of the}} speaking tasks in English. Participants also filled out a questionnaire every other week during the semester in which they estimated the amount of time that they had spent interacting with native speakers of Spanish. Results show that participants experienced significant gains on most measures of fluency during study abroad. This finding was especially true for participants who began their time abroad with low L 2 fluency. Nevertheless, students who began the semester abroad with high L 2 fluency still had significantly higher fluency at the end of the semester than students who began with low L 2 fluency. Looking at the relationship between L 2 fluency and L 2 <b>linguistic</b> <b>knowledge</b> (vocabulary and grammar scores) and language processing speed (picture-naming and sentence-matching scores), the study found a moderate relationship between pretest measures of L 2 fluency and pretest measures of <b>linguistic</b> <b>knowledge</b> and processing speed. However, the results show no relationship between pre-SA <b>linguistic</b> <b>knowledge</b> and processing speed and gains in L 2 fluency, and little relationship between gains <b>in</b> <b>linguistic</b> <b>knowledge</b> and processing speed and gains in L 2 fluency. The best predictor of gains in L 2 fluency was pre-SA L 2 fluency. These results suggest that although there is a relationship between L 2 <b>linguistic</b> <b>knowledge</b> and L 2 fluency, having more advanced L 2 <b>linguistic</b> <b>knowledge</b> prior to study abroad does not necessarily give students an advantage in the area of fluency development during study abroad. Regarding the measurement of fluency, the data show that learners with low and high levels of lexical-grammatical competence significantly differed from one another on all measures of rates of pauses (short and long pauses, filled and unfilled pauses, and mid-clause and end-of-clause pauses) as well as in the percent of pauses occurring in the middle of a clause. However, they did not significantly differ from one another in the percent of filled pauses. The findings suggest that measuring all of these pauses may be useful in examining L 2 fluency. However, there is perhaps little or nothing to be gained from counting filled and unfilled pauses separately, as speakers 2 ̆ 7 tendency to use more of one or the other appears to be more closely related to personal speaking style than to L 2 ability...|$|R
40|$|Higher {{education}} studies, {{particularly at}} postgraduate level, are characterised by heavy reading loads {{which can be}} challenging for all students, but especially for those studying in a second language. This paper focuses {{on a group of}} Arabic-speaking international students studying on a British university-based presessional EAP programme to explore their perceptions of the difficulties they encounter and the strategies they employ when reading in English. Data was gathered in two phases in which the Phase 1 questionnaire results provided a basis for the Phase 2 semi-structured interviews. The resulting case studies revealed varying confidence levels, frequent use of context-level processing strategies, limited use of word-level strategies, automatic reliance on context, and gaps in phonological, orthographic and morpho-syntactic knowledge of English. Pedagogical implications and recommendations include the need for practitioners to pay more specific attention to language needs in order to develop lower-level processing strategies that increase efficiency and effectiveness in L 2 reading, thus addressing these gaps <b>in</b> Arabic-speakers’ <b>linguistic</b> <b>knowledge</b> of English...|$|R
40|$|Since the Web by far {{represents}} the largest public repository of natural language texts, recent experiments, methods, and {{tools in the}} area of corpus linguistics often use the Web as a corpus. For applications where high accuracy is crucial, the problem has to be faced that a non-negligible number of orthographic and grammatical errors occur in Web documents. In this article we investigate the distribution of orthographic errors of various types in Web pages. As a by-product, methods are developed for efficiently detecting erroneous pages and for marking orthographic errors in acceptable Web documents, reducing thus the number of errors <b>in</b> corpora and <b>linguistic</b> <b>knowledge</b> bases automatically retrieved from the Web. 1...|$|R
40|$|Nowadays, Linguistic Modeling is {{considered}} one of the most important applications of Fuzzy Set Theory, along with Fuzzy Control. Linguistic models have the advantage of providing a human-readable description of the system modeled in the form of a set of <b>linguistic</b> rules. <b>In</b> this chapter, we will analyze several approaches to improve the accuracy of linguistic models while maintaining their descriptive power. All these approaches will share the common idea of improving the way in which the Fuzzy Rule-Based System performs interpolative reasoning by improving the cooperation between the rules <b>in</b> the <b>linguistic</b> model <b>Knowledge</b> Base...|$|R
40|$|Abstract. Natural Language Generation systems usually require {{substantial}} {{knowledge about}} {{the structure of the}} target language in order to perform the final task in the generation process – the mapping from semantic representation to text known as surface realisation. Designing knowledge bases of this kind, typically represented as sets of grammar rules, may however become a costly, labour-intensive enterprise. In this work we take a statistical approach to surface realisation <b>in</b> which no <b>linguistic</b> <b>knowledge</b> is hard-coded, but rather trained automatically from large corpora. Results of a small experiment in the generation of referring expressions show significant levels of similarity between our (computer-generated) text and those produced by humans, besides the usual benefits commonly associated with statistical NLP such as low development costs, domain- and language-independency. ...|$|R
40|$|In {{this paper}} we present LXGram, a general purpose grammar for the deep {{linguistic}} processing of Portuguese that aims at delivering detailed and high precision meaning representations. LXGram is grounded on the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG). HPSG is a declarative formalism resorting to unification and a type system with multiple inheritance. The semantic representations that LXGram associates with linguistic expressions use the Minimal Recursion Semantics (MRS) format, {{which allows for}} the underspecification of scope effects. LXGram is developed <b>in</b> the <b>Linguistic</b> <b>Knowledge</b> Builder (LKB) system, a grammar development environment that provides debugging tools and efficient algorithms for parsing and generation. The implementation of LXGram {{has focused on the}} structure of Noun Phrases, and LXGram accounts for many NP related phenomena. Its coverage continues to be increased with new phenomena, and there is active work on extending the grammar’s lexicon. We have already integrated, or plan to integrate, LXGram in a few applications, namely paraphrasing, treebanking and language variant detection. Grammar coverage has been tested on newspaper text. ...|$|R
40|$|We {{explore the}} {{intersection}} of rule-based and statistical approaches in machine translation, with a particular focus on past and current work here at Microsoft Research. Until about ten years ago, the only machine translation systems worth using were rule-based and linguistically-informed. Along came statistical approaches, which use large corpora to directly guide translations toward expressions people would actually say. Rather than making local decisions when writing and conditioning rules, goodness of translation was modeled numerically and free parameters were selected to optimize that goodness. This led to huge improvements in translation quality {{as more and more}} data was consumed. By necessity, the pendulum is swinging towards the inclusion of <b>linguistic</b> features <b>in</b> MT systems. We describe some of our statistical and non-statistical attempts to incorporate linguistic insights into machine translation systems, showing what is currently working well, and what isn’t. We also look at trade-offs <b>in</b> using <b>linguistic</b> <b>knowledge</b> (“rules”) <b>in</b> pre- or post-processing by language pair, with a particular eye on the return on investment as training data increases in size. ...|$|R
