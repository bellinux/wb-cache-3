11|10000|Public
30|$|Tethered control {{methods have}} been {{developed}} previously which allow a mobile robot to follow a leader, using a winch to measure the length and orientation of a tether connected between the leader and the follower robot [6]. Although follower robots have proven successful using the previously developed control methods in person following experiments, {{they have not been}} tested with H.O.T. patients. Testing with patients is crucial to evaluate the suitability of a robot and control system, as their needs may differ significantly from healthy users, and a number of user factors may affect suitability, including gait, walking speed, reaction to obstacles and the user’s perceived effort while <b>operating</b> <b>a</b> <b>robot.</b>|$|E
40|$|DE 10237951 A UPAB: 20040405 NOVELTY - The method {{involves}} {{providing a}} table in which dynamic movement properties (1) of the robot (11) are associated with defined musical properties according to choreographic rules, analyzing detected (8) music in real time to extract musical properties (3), reading dynamic properties from the table associated with the extracted musical properties and controlling (9) the robot accordingly. DETAILED DESCRIPTION - AN INDEPENDENT CLAIM is also included for the following: (a) an arrangement for implementing the inventive method. USE - For <b>operating</b> <b>a</b> <b>robot</b> to playing music. ADVANTAGE - Enables the robot to be operated relative to simultaneously played music from the choreographic point of view...|$|E
40|$|The work {{reported}} {{deals with}} the problem of <b>operating</b> <b>a</b> <b>robot</b> manipulator under a rate control mode while the end effector is not in contact with the external environment, and then switching to a force control mode when contact is made. The paper details how the modal changeover may be accomplished in a manner transparent to the operator, and will allow operator applied forces to be reflected at the robot end effector. A one degree of freedom demonstration system is used to illustrate the concept, which is then applied to a PUMA manipulator. Sample code for the implementation of the control is provided, experimental results show that the optimum setting for the gain {{is a function of the}} compliance of the end effector, and the compliance of the external constraint...|$|E
5000|$|Robot Construction - Open: {{in which}} contestants choose a task, then design, make and <b>operate</b> <b>a</b> <b>robot</b> {{that will do}} that task.|$|R
40|$|To {{efficiently}} <b>operate</b> <b>a</b> <b>robot</b> in unknown environments, <b>a</b> <b>robot</b> {{should be}} able to perform a variety of tasks in parallel. The reported work investigates the parallelism involved in sensing for moving and stationary obstacles, while <b>a</b> <b>robot</b> is executing <b>a</b> planned motion. Three specific parallel behaviours are investigated, avoid collision, follow the leader, and wall following. The reported work was carried out on <b>a</b> LABMATE™ mobile <b>robot,</b> equipped with Polaroid Corp. Ultrasonic range finders. The parallel implementation was undertaken on a network of Transputers, using the Occam™ programming language...|$|R
40|$|This {{research}} {{involves a}} method and system which integrates multimodal human-computer interaction with reactive planning to <b>operate</b> <b>a</b> telerobot {{for use as}} an assistive device. The Multimodal User Supervised Interface and Intelligent Control (MUSIIC) strategy is a novel approach for intelligent assistive telerobotic system. This approach to robotic interaction is both a step towards addressing the problem of allowing individuals with physical disabilities to <b>operate</b> <b>a</b> <b>robot</b> in <b>an</b> unstructured environment and an illustration of general principles of integrating speech-deictic gesture control with a knowledge-driven reactive planner and a stereo-vision system...|$|R
40|$|The {{invention}} is {{a method}} of <b>operating</b> <b>a</b> <b>robot</b> in successive sampling intervals to perform a task, the robot having joints and joint actuators with actuator control loops, by decomposing the task into behavior forces, accelerations, velocities and positions of plural behaviors to be exhibited by the robot simultaneously, computing actuator accelerations of the joint actuators for the current sampling interval from both behavior forces, accelerations velocities and positions of the current sampling interval and actuator velocities and positions of the previous sampling interval, computing actuator velocities and positions of the joint actuators for the current sampling interval from the actuator velocities and positions of the previous sampling interval, and, finally, controlling the actuators {{in accordance with the}} actuator accelerations, velocities and positions of the current sampling interval. The actuator accelerations, velocities and positions of the current sampling interval are stored for use during the next sampling interval...|$|E
40|$|Robotic {{technology}} {{being developed}} {{out of necessity}} to keep the Hubble Space Telescope operating could also lead to new levels of man-machine team-work in deep-space exploration down the road-if it survives the near-term scramble for funding. Engineers here who have devoted their NASA careers {{to the concept of}} humans servicing the telescope in orbit are planning modifications to International Space Station (ISS) robots that would leave the humans on the ground. The work. forced by post-Columbia flight rules that killed a planned shuttle-servicing mission to Hubble, marks another step in the evolution of robot-partners for human space explorers. "Hubble has always been a pathfider for this agency," says Mike Weiss. Hubble deputy program manager technical. "When the space station was flown and assembled, Hubble was the pathfinder. not just for modularity, but for operations, for assembly techniques. Exploration is the next step. Things we're going to do on Hubble are going to be applied to exploration. It's not just putting a robot in space. It's <b>operating</b> <b>a</b> <b>robot</b> in space. It's adapting that robot to {{what needs to be done}} the next time you're up there. ...|$|E
40|$|Background and Objectives: We {{sought to}} provide {{informed}} recommendations on transitioning from laparoscopic radical prostatectomy (LRP) to robotic-assisted radical prostatectomy (RAP) through {{a study of}} the da Vinci robot. Methods: We performed a cost-benefit analysis to determine the impact that purchasing a $ 1. 5 million da Vinci robot with a $ 112, 000 service contract per year and $ 200 per case of disposables would have on profits of a mature laparoscopic prostatectomy program. Results: Seventy-eight cases per year are needed to cover the costs of a purchased robot, while only 20 cases per year are needed if a robot is donated. Once robot costs are covered, increases in caseload lead to increased income. Profit is not feasible at centers performing fewer than 25 cases annually. A donated robot lessens costs and allows reasonable revenue without drastic increases in caseload. Conclusions: Our data suggest a high-volume LRP program can convert to RAP and maintain profits; however, the cost of the robot precludes equal income as that with LRP. Purchasing a robot is not fiscally viable in a lowvolume program. Given comparable outcomes between LRP and RAP, hospitals need to decide whether market forces or the intangible benefits of robotics outweigh the expenses of obtaining and <b>operating</b> <b>a</b> <b>robot...</b>|$|E
40|$|To {{meet the}} {{emerging}} challenges {{of this century}} and stay competitive in the international marketplace, {{it is important that}} Australian students develop the skills they need for digital futures. However, many Australian students cannot access digital technologies like robots due to prohibitive costs and the ‘tyranny of distance’ and this means Australia is at risk of being further left behind in our region and globally. To overcome the current trade deficit in Information Technology (IT) and a looming shortage of workers skilled in Information Communication Technologies (ICT), Australian students must engage in Science, Technology, Engineering and Mathematics (STEM) learning and robotics. The aim {{of this study was to}} determine the learning impact of a Long Distance Control Robot (LDCR) system when used by Australian students who could not access or had limited access to <b>a</b> physical <b>robot.</b> The study investigated the use of the LDCR system with students (n= 32) aged 9 - 12 years at an Australian school of distance education during 2014. Students lived in a range of rural and remote and metropolitan settings throughout Queensland. They used the LDCR system over the Internet to operate the robot that was located in Brisbane. Three research questions were posed: 1. When students <b>operate</b> <b>a</b> <b>robot,</b> what are their perceptions of their learning? 2. What STEM skills do students learn through robots? 3. What complementary skills do students learn when they <b>operate</b> <b>a</b> <b>robot</b> remotely using <b>a</b> Long Distance Control Robot (LDCR) system? Data were collected from student surveys, a blog and video recording transcripts. The data were then thematically analysed using a case study approach that included coding density and content analysis. The research established that when students learned to <b>operate</b> <b>a</b> <b>robot</b> remotely using <b>a</b> LDCR system, their perception of learning was highly positive, their STEM learning accelerated, and they developed complementary skills such as procedural knowledge, technical skills and metacognition. With the expectation that Australian students will learn using robots, this study provides a way forward at very low cost irrespective of physical location...|$|R
40|$|Processing Real-Time image {{sequence}} {{is now possible}} because of advancement of technological developments in digital signal processing, wide-band communication, and highperformance VLSI. Recently, the demand for the indoor robots has increased. Therefore, increased opportunities for many people to operate the robots have emerged. However, for many people, {{it is often difficult}} to <b>operate</b> <b>a</b> <b>robot</b> using the conventional methods like remote control. To solve this problem, we propose <b>a</b> <b>robot</b> operation system using the hand gesture recognition. Our method pays attention to the direction and movement of the hand. We were able to recognize several gestures in real-time. I...|$|R
40|$|This {{demonstration}} will {{be about}} Roman Tutor, a system that we are developing to teach astronauts how to <b>operate</b> <b>a</b> <b>robot</b> manipulator deployed on the International Space Station (ISS). Operators {{do not have a}} direct view of the scene of operation on the ISS and must rely on cameras mounted on the manipulator and at strategic places of the environment where it operates. Roman Tutor uses <b>a</b> <b>robot</b> path-planner, not to control the manipulator, but to automatically check errors of a student learning to operate the manipulator, and to automatically produce illustrations of good and bad motions in training...|$|R
40|$|Abstract: The mining {{industry}} {{is interested in}} tele-operation systems to remove mining operators from hazardous or inconvenient environments while providing them with enough sensory information such that their efficiency is maintained or even enhanced. Similar systems are also used to provide collaboration and assistance in remote problem solving situations, including emergency scenarios. The increased availability of high-speed wired and wireless data networks is promoting the use of immersive environments, {{but there is not}} enough evidence yet to support whether such environments significantly improve the field-tested performance of tele-operation systems or not. We are interested in investigating a mixed-presence, tele-operation scenario involving an offsite operator remotely <b>operating</b> <b>a</b> <b>robot</b> as well as an onsite operator co-located with the robot. These scenarios are common in industry, yet poorly researched. We have conducted a trial to explore the effects of immersion on operator spatial awareness, sense of presence and satisfaction, in a mixed presence tele-operation scenario. This paper presents the results of our trial using a panoramic display system that provides some level of immersion. However, unlike virtual environments it provides an immersive view using video of a real remote space. The outcome of our work provides a first step in the exploration of cost effective technologies of potential value to the {{mining industry}}...|$|E
40|$|Spatial {{skills are}} {{critical}} for robot teleoperation. For example, {{in order to make}} a judgment of relative direction when <b>operating</b> <b>a</b> <b>robot</b> remotely, one must take different perspectives and make decisions based on available spatial information. Training spatial skills is thus critical for robot teleoperation, yet, current training programs focus primarily on psycho-motoric skills of the task, and less on the essential cognitive aspects of spatial skills. This work addresses this need by considering previous findings on relative direction judgments in training robot teleoperation. We developed and tested a basic training paradigm of perspective taking skill targeting the cognitive skill rather than psycho-motoric skill. An experiment tested a basic training paradigm using a stationary robot, with a training group receiving perspective taking training and a control group without training, and both tested on a transfer test with the robot. The results show that participants who went through a targeted cognitive skill training reached mastery level during the training, and performed better than the control group in an analogue transfer of learning test. Moreover, results reveal that the training facilitated participants with initial poor perspective taking skills reach the level of the high-skilled participants in transfer test performance. The study validates the possibility to target only cognitive aspects of spatial skills and result in better robot teleoperation...|$|E
40|$|Abstract-This paper {{discusses}} {{an experimental}} comparison of three user interface techniques for interaction with a mobile robot located remotely from the user. A typical means of <b>operating</b> <b>a</b> <b>robot</b> {{in such a}} situation is to teleoperate the robot using visual cues from a camera that displays the robot’s view of its work environment. However, the operator often has a difficult time maintaining awareness of the robot in its surroundings due to this single ego-centric view. Hence, a multi-modal system has been developed that allows the remote human operator to view the robot in its work environment through an Augmented Reality (AR) interface. The operator is able to use spoken dialog, reach into the 3 D graphic representation of the work environment and discuss the intended actions of the robot to create a true collaboration. This study compares the typical ego-centric driven view to two versions of an AR interaction system for an experiment remotely operating a simulated mobile robot. One interface provides an immediate response from the remotely located robot. In contrast, the Augmented Reality Human-Robot Collaboration (AR-HRC) System interface enables the user to discuss and review a plan with the robot prior to execution. The AR-HRC interface was most effective, increasing accuracy by 30 % with tighter variation, while reducing the number of close calls in operating the robot by factors of ~ 3 x. It thus provides the means to maintain spatial awareness and give the users the feeling they were working in a true collaborative environment. I...|$|E
5000|$|... "Isaac Asimovpredicted in 1976 that I {{would someday}} be <b>operating</b> with <b>a</b> <b>robot.</b> By golly, he was right!" [...] William D. Steers, MD. The University of Toledo Alumni Who Have Changed The World. Volume II. page 67.|$|R
30|$|Ideally, the {{assistance}} would not cause discomfort {{and result in}} a high work efficiency. One strategy for achieving {{this is for the}} operator to remain unaware of {{the assistance}}. If operators remain unaware, they obtain a feeling of control that is the same as manual control. Igarashi proposed assistance without human awareness [8]. Igarashi et al. modified the dynamical parameters of <b>a</b> <b>robot</b> to approach <b>an</b> internal model to <b>operate</b> <b>a</b> <b>robot</b> with complete control by an operator [9]. However, they failed because of discomfort felt by the operators when changing the actual <b>robot</b> dynamics. Therefore, <b>a</b> limit on the rate of change in the dynamics was proposed, and the assistance worked without human awareness and improved performance [8].|$|R
5000|$|T.O.M. - Talk <b>Operated</b> Machine, <b>a</b> <b>robot</b> {{designed}} by Tracy and built by Bertha to perform odd jobs around the factory. According to the song"Tom the Robot from the Bertha 12" [...] vinyl record, {{he is said}} to be Berthas robot son.|$|R
40|$|Graduation date: 2018 In shared autonomy, a robot {{and human}} user both have {{some level of}} control {{in order to achieve}} a shared goal. Choosing the balance of control given to the user and the robot can be a {{challenging}} problem since different users have different preferences and vary in skill levels when <b>operating</b> <b>a</b> <b>robot.</b> We propose using a novel formulation of Partially Observable Markov Decision Processes (POMDPs) to represent a model of the user's expertise in controlling the robot. The POMDP uses observations from the user's actions and from the environment to update the belief of the user's skill and chooses a level of control between the robot and the user. The level of control given between the user and the robot is encapsulated in macro-action controllers. The macro-action controllers encompass varying levels of robot autonomy and reduce the space of the POMDP, removing the need to plan over separate actions. As part of this research, we ran two users study, developed a method to automatically generate macro-action controller values, and applied our user expertise model to provide shared autonomy on a semi-autonomous underwater vehicle. In our first user study, we tested our user expertise model in a robot driving simulation. Users drove a simulated robot through an obstacle-filled map while the POMDP model chose appropriate macro-action controllers based on the belief state of the user's skill level. The results of the user study showed that our model can encapsulate user skill levels. The results also showed that using the controller with greater robot autonomy helped users of low skill avoid obstacles more than it helped users of high skill. We designed a controller value synthesis method to generate the variables that control the levels of autonomy in the macro-action controllers. We found differences in how the users drive the robot using a decision tree generated from the data recorded in the first user study, and we used these differences to program simulated user "bots" that mimic users of different skill levels. The "bots" were used to test a range of variables for the controllers, and the controller variables were found from minimizing obstacles hit, time to complete maps, and total distance driven from the simulated data. For our second user study, we looked at users' satisfaction without robot autonomy, with the highest amount of autonomy, and with the autonomy chosen by our expertise model. We found users we classified as beginners ranked the autonomy more favorably than those ranked as experts. We implemented our expertise model on a Seabotix vLBV 300 underwater vehicle and ran a trial off the coast of Newport, Oregon. During our trials, we recorded a user driving the vehicle to predetermined waypoints. When beginner actions were performed, the user expertise model provided an increased level of autonomy which either increased throttle when far from waypoints or decreased throttle when close to waypoints. This demonstrated an implementation of our algorithm on existing robot hardware in the field...|$|E
50|$|There {{was also}} a sequel series, Tetsujin 28 fx (Tetsujin-nijuhachi-go-Efu-Ekkusu), about {{the son of the}} {{original}} controller <b>operating</b> <b>a</b> new <b>robot</b> (with Daddy and the original FX-less #28 appearing from time-to-time to help), which ran in Japan in 1992.|$|R
40|$|Conventionally, in {{the system}} which {{contains}} a hu-man element in an evaluation system, a user’s physi-cal and mental load was a big problem. Many of con-ventional researches have mainly corresponded by the three improvement methods to this problem. In this research, the mechanism in which <b>a</b> <b>robot’s</b> action is automatically gained by human operation is mounted. The purpose {{of this research is}} reducing a user’s cog-nitive load. They are the improvement of an input in-terface, the improvement of a presentation interface, and improvement in the speed of EC convergence. On the other hand, in this research, we attention to <b>an</b> <b>operated</b> type <b>robot</b> as <b>an</b> object of research. It aim at the construction of a system design in consideration of interaction between <b>a</b> <b>robot</b> and <b>an</b> operator in this research. <b>A</b> user <b>operates</b> <b>a</b> <b>robot,</b> in order to attain a task. When a system uses a user’s operation in-formation for the instruction information on <b>a</b> <b>robot’s</b> action, <b>a</b> system gains <b>a</b> <b>robot’s</b> behavior automatically by managing a task, without a user being conscious of instruction. Therefore, it is possible to reduce a user’s cognitive load. In this research, we implement such a mechanism and verify by some experiments {{that it is possible to}} reduce a user’s cognitive load. ...|$|R
40|$|The {{surgical}} robot experienced rapid uptake throughout {{hospitals in the}} US despite lack of clinical evidence that it is superior to existing methods and undeterred by its high cost. This type of technology may be a “weapon” in the medical arms race hypothesis which asserts that competition among hospitals may be welfare reducing wherein it encourages resource use that is not commensurate with beneficial health outcomes. This paper is a case-study of the diffusion of the {{surgical robot}} among hospitals in Florida. We address the medical arms race hypothesis directly by investigating whether a hospital’s decision to adopt <b>a</b> <b>robot</b> is <b>a</b> function of the neighboring, competing hospitals’ decisions to do so. Using a spatial autoregressive probit model, {{we find that the}} spatial coefficient is significant and negative. That is, when neighboring hospitals <b>operate</b> <b>a</b> <b>robot,</b> <b>a</b> given hospital is less likely to operate one. Indeed, hospitals appear to consider the behavior of rival hospitals, but not {{in a way that would}} be consistent with a medical arms race. Support is lent to the hypothesis that as more hospitals become providers of robotic-assisted surgery, the less profitable it becomes to enter the market...|$|R
40|$|Objective: To {{determine}} how scores on standard spatial measures correlate {{with the ability}} to <b>operate</b> <b>a</b> <b>robot</b> under different teleoperation conditions. Background: Past work has demonstrated that there is a relationship between visual spatial ability and teleoperation performance. Method: In this experiment participants completed a spatial visualization (VZ- 2) and spatial relation (S- 2) measure, and teleoperated <b>a</b> <b>robot</b> through both low and high difficulty courses under direct line of sight (DLS) and teleoperation (TO) conditions. Performance was determined by course completion time and the total number of collisions made during navigation. Results and Conclusion: Aggregate visual spatial ability was inversely correlated with operator performance under each of the experimental conditions. Analyzed independently, only spatial relations ability correlated with TO performance, while both measures correlated with DLS operation. Application: Better understanding of the relationship between spatial abilities and teleoperation performance can assist in the selection and training of future operators, as well as the design of superior interfaces...|$|R
40|$|Abstract. Remote robot {{control systems}} are being {{developed}} {{for a wide range}} of applications, including robot control at dangerous environments, delivery tasks and security monitoring. This paper presents <b>a</b> <b>robot</b> control system with a web interface. The control system is divided in two subsystems: Central Control System (CCS) and the web interface for robot monitoring. Three modules compose the CCS: mapping module, localization module and path planning module. The mapping module models the environment. The localization module estimates the robot positioning in its environment. The path planning module is responsible for controlling the robots actuators, using the data supplied by the other modules, so that the robot can travel between 2 points without hitting objects. The web interface is responsible for sending new tasks for the CCS subsystem, monitoring the <b>robot</b> in <b>an</b> environment map and to show images captured by the robot during the task accomplishment. This remote web interface helps users not familiarized with robotics to <b>operate</b> <b>a</b> <b>robot</b> while keeping the operator away from dangerous environments. Experiments are being conducted at a real environment with <b>a</b> Pioneer I <b>robot...</b>|$|R
5000|$|RoboWars is an inter collegiate {{competition}} where {{teams of}} five must construct, deploy and <b>operate</b> <b>a</b> fighting <b>robot</b> in <b>a</b> gladiatorial arena. Rules follow the traditional [...] "RoboWar" [...] league guidelines {{with only one}} weight category (<25 kg). Viewed as more of an entertainment event than an educational one; the decision to include {{it was based on}} the enthusiasm of students; the mechanical engineering department in particular.|$|R
40|$|Abstract: In {{this paper}} we are {{presenting}} a PDA interface for <b>operating</b> <b>a</b> mobile <b>robot</b> remotely. The interface {{is designed for}} exploration robotics, in which a roving operator, equipped with a hand-held device can partly share with the robot the scenario to be explored. Based on user evaluation we improved the first prototype and provided some guidelines for PDA interfaces design. The second protoype with the contributed improvements is also presented...|$|R
40|$|We {{developed}} a robust real time hand gesture based interaction system to effectively communicate with <b>a</b> mobile <b>robot</b> which can <b>operate</b> in <b>an</b> outdoor environment. The system enables {{the user to}} <b>operate</b> <b>a</b> mobile <b>robot</b> using hand gesture based commands. In particular the system offers direct on site interaction providing better perception of environment to the user. To overcome the illumination challenges in outdoors, the system operates on depth images. Processed depth images are given as input to a convolutional neural network which is trained to detect static hand gestures. The system is evaluated in real world experiments on <b>a</b> mobile <b>robot</b> to show the operational efficiency in outdoor environment...|$|R
40|$|Recent {{developments}} in industrial robotics {{has led to}} cheaper robots, which have become more accessible to {{small and medium-sized enterprises}} (SMEs). The tra-ditional way of programming <b>a</b> <b>robot</b> however has not seen any such large scale advancements and still require a significant amount of man-hours, thus being {{a major part of the}} costs related to <b>operating</b> <b>an</b> industrial <b>robot.</b> Traditionally, robots are programmed using an on-line programming procedure. This requires <b>a</b> <b>robot</b> operator to manually control the <b>robot</b> to <b>a</b> number of poses along the actual trajectory, which will be stored in <b>a</b> <b>robot</b> program. This must be done for all individual robots on the floor, requiring a significant amount of work load. Another way of programming industrial robots made popular by increased absolute accuracy and a deman...|$|R
50|$|Sin was {{introduced}} in the Noosehead storyline {{as the president of the}} band's label, Sin Records. It was eventually revealed that he was in fact <b>a</b> small alien <b>operating</b> <b>a</b> humanoid <b>robot.</b> According to Sam, Sin is involved in various unusual schemes of the kind that Ninja Mafia Services deals with. Although it is clear that Sin had dealings with Fuzzy prior to Fuzzy's memory loss, little is known of his origins or what motivation (if any) he has beyond simple greed.|$|R
40|$|A small, {{non-contact}} {{optical sensor}} invented {{by the author}} attaches to <b>a</b> <b>robot</b> (or other machines), enabling the robot to detect objects, adjust its alignment in all {{six degrees of freedom}} (SixDOF), and read a task from a code on the part. Thus, the SixDOF sensor provides robots more intelligence to operate autonomously and adapt to changes without human intervention. A description of the sensor is provided. Also, <b>an</b> <b>operating</b> arrangement of <b>a</b> <b>robot</b> using the SixDOF sensor is presented with performance results described...|$|R
40|$|This chapter {{presents}} research {{designed to}} study and improve an operator’s abil-ity to navigate or teleoperate <b>a</b> <b>robot</b> that is distant from the operator {{through the use of}} <b>a</b> <b>robot</b> intelligence architecture and a virtual 3 D interface. To validate the use of the robot intelligence architecture and the 3 D interface, four user-studies are presented that compare intelligence modes and interface designs in navigation and exploration tasks. Results from the user studies suggest that performance is improved when the robot assumes some of the navigational responsibilities or the interface presents spatial information {{as it relates to the}} pose of the robot in the remote environment. The authors hope that understanding the roles of intelligence and interface design when <b>operating</b> <b>a</b> remote <b>robot</b> will lead to improved human-robot teams that are useful in a variety of tasks...|$|R
40|$|Animals {{have evolved}} to occupy every {{environment}} where one might wish to <b>operate</b> <b>a</b> <b>robot,</b> save outer space. In most cases animal performance transcends the efficiency and agility possible with current engineering solutions. Until recently, there were few examples of actuators that might permit the linear actuation characteristic of animal systems. Moreover advances in transducers and MEMs technology now permit sensors which code environmental information {{in the same fashion}} as animal sensors. Combined with new methods of computational network modeling, these advances have permitted development of a new class of truly biomimetic robots. Swimming robots achieve propulsion by whole body undulations or tail flapping. Walking robots use multiple jointed legs to mediate locomotion. Inroads are even being made in flying robots which locomote by flapping wings. These robotic systems represent the forefront of neurotechnology that implements engineering solutions through the application of biological control and transducer principles. This neurotechnology affords the opportunity to produce truly reactive autonomous robots. Studies of animal behavior have extended these capabilities to reactive navigation and investigation. The integration of these higher order animal control schemes with biomimetic sensors and actuators ma...|$|R
40|$|The {{purpose of}} this study is to {{document}} the design process as it is adapted to compressed timeframes. I have termed this adapted design process single-session design. This study will also explore the application of this type of design methodology in industry where the research and development phases of products are continually being compressed. The primary research for this study is extracted from the examples of rapid design observed repeatedly on the Robot Rivals television series on the Do It Yourself network. The scope of this television series is a competition between two teams of engineering students to design, build, and <b>operate</b> <b>a</b> <b>robot</b> in <b>a</b> single day. The show yields an ideal platform to study the design process in a highly adaptive and compressed form. This study will show how the design process can be adapted to function in a fast-paced situation. The design process in general has been studied for quite some time. However, to date there is no focused research on a specific design methodology that is intended for extremely short-term projects. This research provides insight into the situation where significant time constraints stimulate creativity and ingenuity in designs. ii...|$|R
40|$|Abstract. We {{consider}} {{the problem of}} remotely <b>operating</b> <b>an</b> autonomous <b>robot</b> through <b>a</b> wireless communication channel. Our goal is to achieve a satisfactory tracking performance while reducing network usage. To attain this objective we implement a self-triggered strategy that adjusts the triggering condition to the observed tracking error. After the theo-retical justification we present experimental results from the application of this adaptive self-triggered approach on a P 3 -DX mobile robot re-motely controlled. The experiments show a relevant reduction on the generated network traffic compared to a periodic implementation and to a non-adaptive self-triggered approach, while the tracking performance is barely degraded. ...|$|R
50|$|The main {{activity}} of the game was to write a computer program that would <b>operate</b> <b>a</b> (simulated) <b>robot.</b> The player could then select multiple robots who would do battle in an arena until only one was left standing. The robots did not have direct knowledge of the location or velocity {{of any of the}} other robots; they could only use radar pulses to deduce distance, and perhaps use clever programming techniques to deduce velocity. No physical dexterity was required or even relevant in RobotWar; there was no way for the player to actually take part in the battle.|$|R
40|$|The Astrophysical Institute Potsdam (AIP) and the Instituto de Astrofísica de Canarias (IAC) inaugurated {{the robotic}} telescopes STELLA-I and STELLA-II (STELLar Activity) on Tenerife on May 18, 2006. The {{observatory}} {{is located on}} the Izaña ridge at an elevation of 2400 [*]m near the German Vacuum Tower Telescope. STELLA consists of two 1. 2 [*]m alt-az telescopes. One telescope fiber feeds a bench-mounted high-resolution echelle spectrograph while the other telescope feeds a wide-field imaging photometer. Both scopes work autonomously by means of artificial intelligence. Not only that the telescopes are automated, but the entire observatory <b>operates</b> like <b>a</b> <b>robot,</b> and does not require any human presence on site...|$|R
