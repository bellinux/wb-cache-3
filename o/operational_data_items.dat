0|7562|Public
40|$|Abstract—NoSQL {{systems have}} grown in {{popularity}} for storing big data because these systems offer high availability, i. e., operations with high throughput and low latency. However, metadata in these systems are handled today in ad-hoc ways. We present Wasef, a system that treats metadata in a NoSQL database system, as first-class citizens. Metadata may include information such as: operational history for a database table (e. g., columns), placement information for ranges of keys, and <b>operational</b> logs for <b>data</b> <b>items</b> (key-value pairs). Wasef allows the NoSQL system to store and query this metadata efficiently. We integrate Wasef into Apache Cassandra, {{one of the most}} popular key-value stores. We then implement three important use cases in Cassandra: dropping columns in a flexible manner, verifying data durability during migrational operations such as node decommissioning, and maintaining data provenance. Our experimental evaluation uses AWS EC 2 instances and YCSB workloads. Our results show that Wasef: i) scales well with the size of the data and the metadata; ii) minimally affects throughput and operation latencies. 1...|$|R
40|$|A {{method of}} {{representing}} {{a group of}} <b>data</b> <b>items</b> comprises, for each of a plurality of <b>data</b> <b>items</b> in the group, determining the similarity between said <b>data</b> <b>item</b> and each of a plurality of other <b>data</b> <b>items</b> in the group, assigning a rank to each pair {{on the basis of}} similarity, wherein the ranked similarity values for each of said plurality of <b>data</b> <b>items</b> are associated to reflect the overall relative similarities of <b>data</b> <b>items</b> in the group...|$|R
50|$|In a list, {{the order}} of <b>data</b> <b>items</b> is significant. Duplicate <b>data</b> <b>items</b> are permitted. Examples of {{operations}} on lists are searching for a <b>data</b> <b>item</b> in the list and determining its location (if it is present), removing a <b>data</b> <b>item</b> from the list, adding a <b>data</b> <b>item</b> to the list at a specific location, etc. If the principal operations on the list are to be the addition of <b>data</b> <b>items</b> {{at one end and}} the removal of <b>data</b> <b>items</b> at the other, it will generally be called a queue or FIFO. If the principal operations are the addition and removal of <b>data</b> <b>items</b> at just one end, it will be called a stack or LIFO. In both cases, <b>data</b> <b>items</b> are maintained within the collection in the same order (unless they are removed and re-inserted somewhere else) and so these are special cases of the list collection. Other specialized operations on lists include sorting, where, again, {{the order of}} <b>data</b> <b>items</b> is of great importance.|$|R
50|$|In COBOL, union <b>data</b> <b>items</b> {{are defined}} in two ways. The first uses the RENAMES (66 level) keyword, which {{effectively}} maps a second alphanumeric <b>data</b> <b>item</b> {{on top of}} the same memory location as a preceding <b>data</b> <b>item.</b> In the example code below, <b>data</b> <b>item</b> PERSON-REC is defined as a group containing another group and a numeric <b>data</b> <b>item.</b> PERSON-DATA is defined as an alphanumeric <b>data</b> <b>item</b> that renames PERSON-REC, treating the data bytes continued within it as character data.|$|R
40|$|To improve data {{accessibility}} in ad hoc networks, {{in our previous}} work we proposed three methods of replicating <b>data</b> <b>items</b> by considering the data access frequencies from mobile nodes to each <b>data</b> <b>item</b> and the network topology. In this paper, we extend our previously proposed methods to consider the correlation among <b>data</b> <b>items.</b> Under these extended methods, the data priority of each <b>data</b> <b>item</b> is defined based on the correlation among <b>data</b> <b>items,</b> and <b>data</b> <b>items</b> are replicated at mobile nodes with the data priority. We employ simulations {{to show that the}} extended methods are more efficient than the original ones. ...|$|R
50|$|In a tree, {{which is}} {{a special kind of}} graph, a root <b>data</b> <b>item</b> has {{associated}} with it some number of <b>data</b> <b>items</b> which in turn have associated with them some number of other <b>data</b> <b>items</b> in what is frequently viewed as a parent-child relationship. Every <b>data</b> <b>item</b> (other than the root) has a single parent (the root has no parent) and some number of children, possibly zero. Examples of operations on trees are the addition of <b>data</b> <b>items</b> so as to maintain a specific property of the tree to perform sorting, etc. and traversals to visit <b>data</b> <b>items</b> in a specific sequence.|$|R
40|$|Abstract. 4 -ary vector {{expression}} {{was defined by}} 3 -ary vector to describe subject and <b>data</b> <b>item</b> in dataspace. Association method between subject and <b>data</b> <b>item</b> was represented. Correlation of <b>data</b> <b>item</b> was defined by 4 -ary vector. Correlation and association way between <b>data</b> <b>items</b> were represented by 4 -ary vector. The validity of these methods was verified in technical document library...|$|R
40|$|The {{corporate}} data warehouse integrates <b>data</b> from various <b>operational</b> <b>data</b> stores of a company. These <b>operational</b> <b>data</b> stores may be heterogeneous {{with respect to}} the represented information. The hetero-homogeneous data warehouse modeling approach overcomes issues associated with the integration of heterogeneous information from the <b>operational</b> <b>data</b> stores b...|$|R
5000|$|The [...] "Point" [...] message defines two {{mandatory}} <b>data</b> <b>items,</b> x and y. The <b>data</b> <b>item</b> {{label is}} optional. Each <b>data</b> <b>item</b> has a tag. The tag is defined after the equal sign. For example, x has the tag 1.|$|R
50|$|In a multiset (or bag), {{like in a}} set, {{the order}} of <b>data</b> <b>items</b> does not matter, {{but in this case}} {{duplicate}} <b>data</b> <b>items</b> are permitted. Examples of operations on multisets are the addition and removal of <b>data</b> <b>items</b> and determining how many duplicates of a particular <b>data</b> <b>item</b> are present in the multiset. Multisets can be transformed into lists by the action of sorting.|$|R
30|$|We {{found that}} the {{proposed}} algorithm is presently in the broadcast structure. The wireless broadcast scheduling has been considered the <b>data</b> <b>item</b> frequency of the fixed and it has an unreasonable supposition. The <b>data</b> <b>item</b> frequency would be {{the request of the}} client for a change under the factual dynamic environments. Each of the <b>data</b> <b>item</b> has a frequency value itself and the each frequency of <b>data</b> <b>item</b> should been computed for its weight value and adjusted for dynamic broadcast adaptive so the frequency of <b>data</b> <b>item</b> has no fixed probability value.|$|R
50|$|In a set, {{the order}} of <b>data</b> <b>items</b> does not matter (or is undefined) but {{duplicate}} <b>data</b> <b>items</b> are not permitted. Examples of operations on sets are the addition and removal of <b>data</b> <b>items</b> and searching for a <b>data</b> <b>item</b> in the set. Some languages support sets directly. In others, sets can be implemented by a hash table with dummy values; only the keys are used in representing the set.|$|R
500|$|<b>Data</b> <b>items</b> in COBOL are {{declared}} hierarchically {{through the}} use of level-numbers which indicate if a <b>data</b> <b>item</b> is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level <b>data</b> <b>items,</b> with a level-number of 1, are called [...] Items that have subordinate aggregate data are called those that do not are called [...] Level-numbers used to describe standard <b>data</b> <b>items</b> are between 1 and 49.|$|R
40|$|In {{this paper}} we {{consider}} data freshness and overload handling in embedded systems. The requirements on data management and overload handling {{are derived from}} an engine control software. <b>Data</b> <b>items</b> need to be up-to-date, and to achieve this data dependencies must be considered, i. e., updating a <b>data</b> <b>item</b> requires other <b>data</b> <b>items</b> are upto-date. We also note that a correct result of a calculation can in some cases be calculated using {{a subset of the}} inputs. Hence, data dependencies can be divided into required and not required <b>data</b> <b>items,</b> e. g., only a subset of <b>data</b> <b>items</b> affecting the fuel calculation in an engine control needs to be calculated during a transient overload {{in order to reduce the}} number of calculations. Required <b>data</b> <b>items</b> must always be up-to-date, whereas not required <b>data</b> <b>items</b> can be stale. We describe an algorithm that dynamically determines which <b>data</b> <b>items</b> need to be updated taking workload, data freshness, and data relationships into consideration. Performance results show that the algorithm suppresses transient overloads better than (m, k) - and skipover scheduling combined with established algorithms to update <b>data</b> <b>items.</b> The performance results are collected from an implementation of a real-time database on the realtime operating system µC/OS-II. To investigate whether the system is occasionally overloaded an offline analysis algorithm estimating period times of updates is presented. ...|$|R
50|$|A <b>data</b> <b>item</b> {{describes}} an atomic state {{of a particular}} object concerning a specific property {{at a certain time}} point. A collection of <b>data</b> <b>items</b> for the same object at the same time forms an object instance (or table row). Any type of complex information can be broken down to elementary <b>data</b> <b>items</b> (atomic state). <b>Data</b> <b>items</b> are identified by object (o), property (p) and time (t), while the value (v) is a function of o, p and t: v = F(o,p,t).|$|R
50|$|The {{simplest}} processors are scalar processors. Each instruction {{executed by}} a scalar processor typically manipulates {{one or two}} <b>data</b> <b>items</b> at a time. By contrast, each instruction executed by a vector processor operates simultaneously on many <b>data</b> <b>items.</b> An analogy {{is the difference between}} scalar and vector arithmetic. A superscalar processor is a mixture of the two. Each instruction processes one <b>data</b> <b>item,</b> but there are multiple execution units within each CPU thus multiple instructions can be processing separate <b>data</b> <b>items</b> concurrently.|$|R
3000|$|... {{age of a}} <b>data</b> <b>item,</b> {{calculated}} by taking {{the difference between the}} current time, t_curr, and the measurement time of that <b>data</b> <b>item</b> t(d); [...]...|$|R
50|$|For example, an {{abstract}} stack, {{which is a}} last-in-first-out structure, could be defined by three operations: push, that inserts a <b>data</b> <b>item</b> onto the stack; pop, that removes a <b>data</b> <b>item</b> from it; and peek or top, that accesses a <b>data</b> <b>item</b> {{on top of the}} stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a <b>data</b> <b>item</b> into the queue; dequeue, that removes the first <b>data</b> <b>item</b> from it; and front, that accesses and serves the first <b>data</b> <b>item</b> in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many <b>data</b> <b>items</b> have been pushed into the stack, and that the stack uses a constant amount of storage for each element.|$|R
40|$|Although data {{broadcast}} {{has been}} shown to be an efficient data dissemination technique for mobile computing systems, many issues such as selection of broadcast data and caching strategies at the clients are still active research areas. In this paper, by examining the dynamic properties of the <b>data</b> <b>items</b> in mobile computing systems, we define the validity of a <b>data</b> <b>item</b> by its absolute validity interval (avi). Based on the avi of the <b>data</b> <b>items,</b> we propose different broadcast algorithms in which the selection of <b>data</b> <b>items</b> for broadcast will be based on the avi of the <b>data</b> <b>items</b> and their access frequencies. The purpose of the AVI algorithms is to increase the client cache hit probability so that the access delay for a <b>data</b> <b>item</b> will be much reduced. Simulation experiments have been conducted to compare the AVI algorithms with the algorithm which only considers the popularity of the <b>data</b> <b>items.</b> The results indicate that the AVI algorithms can significantly improve the mean response time and reduce the deadline missing requests. ...|$|R
40|$|In recent years, {{there has}} been much focus on skyline queries that {{incorporate}} and provide more flexible query operators that return <b>data</b> <b>items</b> which are dominating other <b>data</b> <b>items</b> in all attributes (dimensions). Several techniques for skyline have been proposed in the literature. Most of the existing skyline techniques aimed to find the skyline query results by supposing that the values of dimensions are always present for every <b>data</b> <b>item.</b> In this paper we aim to evaluate the skyline preference queries in which some dimension values are missing. We proposed an approach for answering preference queries in a database by utilizing the concept of skyline technique. The skyline set selected for a given query operation is then optimized so that the missing values are replaced with some approximate values that provide a skyline answer with complete data. This will significantly reduce the number of comparisons between <b>data</b> <b>items.</b> Beside that, the number of retrieved skyline <b>data</b> <b>items</b> is reduced and this guides the users to select the most appropriate <b>data</b> <b>items</b> from the several alternative complete skyline <b>data</b> <b>items...</b>|$|R
40|$|Technologies {{such as the}} World Wide Web have {{resulted}} in the access of information over a global network. Data can be text, images, video, or sound. Current network limitations and the large size of <b>data</b> <b>items</b> result in a high response time - the time taken for an image to be transmitted from a remote site to the user. In this paper we focus on reducing this response time for images. Traditionally in an information system exact copies of text <b>data</b> <b>items</b> had to be retrieved for the user but multimedia <b>data</b> <b>items</b> such as video and images can be represented by an equivalent <b>data</b> <b>item</b> which is an approximation of the original <b>data</b> <b>item.</b> Such an equivalent <b>data</b> <b>item</b> might satisfy an application equally well and also have a lower response time. Based on the assumption that such an approximation of a <b>data</b> <b>item</b> is sufficient for several applications, we have developed an architecture where an image equivalent to the original can be retrieved instead of the original image itself. Our architectu [...] ...|$|R
40|$|The {{rapid growth}} of data is inevitable, and {{retrieving}} the best results that meet the user’s preferences is essential. To achieve this, skylines were introduced in which <b>data</b> <b>items</b> that are not dominated by the other <b>data</b> <b>items</b> in the database are retrieved as results (skylines). In most of the existing skyline approaches, the databases {{are assumed to be}} static and complete. However, in real world scenario, databases are not complete especially in multidimensional databases in which some dimensions may have missing values. The databases might also be dynamic in which new <b>data</b> <b>items</b> are inserted while existing <b>data</b> <b>items</b> are deleted or updated. Blindly performing pairwise comparisons on the whole <b>data</b> <b>items</b> after the changes are made is inappropriate as not all <b>data</b> <b>items</b> need to be compared in identifying the skylines. Thus, a novel skyline algorithm, DInSkyline, is proposed in this study which finds the most relevant <b>data</b> <b>items</b> in dynamic and incomplete databases. Several experiments have been conducted and the results show that DInSkyline outperforms the previous works by reducing the number of pairwise comparisons in the range of 52...|$|R
50|$|DataBlitz also {{provides}} higher-layer interfaces for grouping related <b>data</b> <b>items,</b> and performing scans {{as well as}} associative access (via indices) on <b>data</b> <b>items</b> in a group...|$|R
30|$|An {{alternative}} {{data dissemination}} mechanism is the broadcast disks scheme, which permits <b>data</b> <b>items</b> to be broadcast with different frequencies [5]. This algorithm first divides <b>data</b> <b>items</b> {{into a few}} groups (i.e., disks) such that <b>data</b> <b>items</b> with similar popularity are assigned to the same disks. Afterwards, it determines the rotation speed of each disk according to the popularity of <b>data</b> <b>items.</b> In this way, one can construct a broadcast program that adjusts the trade-off between the access time of hot data and that of cold data.|$|R
40|$|Data {{warehousing}} is {{a collection}} of concepts and tools which aim at providing and maintaining a set of integrated data (the data warehouse) for business decision support within an organization. They extract <b>data</b> from different <b>operational</b> <b>data</b> sources, and after some cleansing and transformation procedures data are integrated and loaded into a central repository to enable analysis and mining. Data and metadata lineage are important processes for data analysis. The first allows users to trace warehouse <b>data</b> <b>items</b> back to the original source item from which they were derived and the latter shows which operations have been performed to achieve that target data. This work proposes integrating metadata captured during transformation processes using the CWM metadata standard in order to enable data and metadata lineage. Additionally it presents a tool specially developed for performing this task. ...|$|R
40|$|Applications {{that make}} use of very large {{scientific}} datasets have become an increasingly important subset of scientific applications. In these applications, datasets are often multi-dimensional, i. e., <b>data</b> <b>items</b> are associated with points in a multi-dimensional attribute space, and access to <b>data</b> <b>items</b> is described by range queries. The basic processing involves mapping input <b>data</b> <b>items</b> to output <b>data</b> <b>items,</b> and some form of aggregation of all the input <b>data</b> <b>items</b> that project to the each output <b>data</b> <b>item.</b> We have developed an infrastructure, called the Active Data Repository (ADR), that integrates storage, retrieval and processing of multi-dimensional datasets on distributed-memory parallel architectures with multiple disks attached to each node. In this paper we address efficient execution of range queries on distributed memory parallel machines within ADR framework. We present three potential strategies, and evaluate them under different application scenarios and machine co [...] ...|$|R
5000|$|With read caches, a <b>data</b> <b>item</b> {{must have}} been fetched from its {{residing}} location {{at least once in}} order for subsequent reads of the <b>data</b> <b>item</b> to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a <b>data</b> <b>item</b> may be realized upon the first write of the <b>data</b> <b>item</b> by virtue of the <b>data</b> <b>item</b> immediately being stored in the cache's intermediate storage, deferring the transfer of the <b>data</b> <b>item</b> to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand, ...|$|R
500|$|An 88 level-number {{declares}} a [...] (a so-called 88-level) {{which is}} true when its parent <b>data</b> <b>item</b> contains one of the values specified in its [...] clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the [...] <b>data</b> <b>item.</b> When the <b>data</b> <b>item</b> contains a value of , the condition-name [...] is true, whereas when it contains a value of [...] or , the condition-name [...] is true. If the <b>data</b> <b>item</b> contains some other value, both of the condition-names are false.|$|R
50|$|WinFS models data {{using the}} <b>data</b> <b>items,</b> {{along with their}} relationships, {{extensions}} and rules governing its usage. WinFS needs to understand the type {{and structure of the}} <b>data</b> <b>items,</b> so that the information stored in the <b>data</b> <b>item</b> can be made available to any application that requests it. This is done by the use of schemas. For every type of <b>data</b> <b>item</b> that is to be stored in WinFS, a corresponding schema needs to be provided to define the type, structure and associations of the data. These schemas are defined using XML.|$|R
50|$|A value {{written by}} a process on a <b>data</b> <b>item</b> X will be always {{available}} to a successive read operation performed by the same process on <b>data</b> <b>item</b> X.|$|R
50|$|Monotonic read {{consistency}} {{guarantees that}} after a process reads a value of <b>data</b> <b>item</b> x at time t, it will never see the older value of that <b>data</b> <b>item.</b>|$|R
5000|$|Each <b>data</b> <b>item</b> {{behaviour}} {{is defined}} by the Major Type and Additional Type. The major type is used for selecting the main behaviour or type of each <b>data</b> <b>item.</b>|$|R
5000|$|The quorum-based {{voting for}} replica control {{is due to}} 1979.Each copy of a {{replicated}} <b>data</b> <b>item</b> is assigned a vote. Each operation then has to obtain a read quorum (Vr) or a write quorum (Vw) to read or write a <b>data</b> <b>item,</b> respectively. If a given <b>data</b> <b>item</b> has a total of V votes, the quorums have to obey the following rules: ...|$|R
5000|$|This model {{description}} is sourced from. The HDP {{is a model}} for grouped data. What {{this means is that}} the <b>data</b> <b>items</b> come in multiple distinct groups. For example, in a topic model words are organized into documents, with each document formed by a bag (group) of words (<b>data</b> <b>items).</b> Indexing groups by , suppose each group consist of <b>data</b> <b>items</b> [...]|$|R
40|$|We present {{global and}} local {{algorithms}} for generating traversals of arbitrary trees with the property that {{the maximum number}} of edges between successive nodes in any traversal is three, and that this is the best possible result. We describe the application of such traversals to parallel computing. Keywords Algorithms, concurrent tree traversal, interconnection network, distributed computing, parallel processing. 1 Introduction Many parallel algorithms start with a set of <b>data</b> <b>items</b> distributed across all the nodes of a parallel computer. They repeatedly process the <b>data</b> <b>item</b> at each node in parallel, concurrently transfer each <b>data</b> <b>item</b> from its current node to a new node, and repeat until each <b>data</b> <b>item</b> has been processed at each node. To do this, each <b>data</b> <b>item</b> either follows the same Hamiltonian cycle between the nodes or follows a different Hamiltonian path between the nodes, subject only to the restriction that two different <b>data</b> <b>items</b> never arrive at the same new node at the s [...] ...|$|R
40|$|The {{retention}} of communication data has recently attracted much public interest, {{mostly because of}} the possibility of its misuse. In this paper, we present protocols that address the privacy concerns of the communication partners. Our data retention protocols store streams of encrypted <b>data</b> <b>items,</b> some of which may be flagged as critical (representing misbehavior). The frequent occurrence of critical <b>data</b> <b>items</b> justifies the self-decryption of all recently stored <b>data</b> <b>items,</b> critical or not. Our first protocol allows the party gathering the retained data to decrypt all <b>data</b> <b>items</b> collected within, say, the last half year whenever the number of critical <b>data</b> <b>items</b> reaches some threshold within, say, the last month. The protocol ensures that the senders of data remain anonymous but may reveal that different critical <b>data</b> <b>items</b> came from the same sender. We call this the affiliation of critical data. Our second, computationally more complex scheme obscures the affiliation of critical data with high probability. ...|$|R
