45|31|Public
5000|$|Near-line : Near-line {{storage is}} {{typically}} less accessible {{and less expensive}} than <b>on-line</b> <b>storage,</b> but still useful for backup data storage. A good example would be a tape library with restore times ranging from seconds to a few minutes. A mechanical device is usually used to move media units from storage into a drive where the data can be read or written. Generally it has safety properties similar to <b>on-line</b> <b>storage.</b>|$|E
50|$|SuMS Database and WebCaret {{provided}} <b>on-line</b> <b>storage</b> {{of surface}} and volume-based data along with web-based visualization of the data.|$|E
50|$|TELCOMP {{programs}} were normally input via a paper tape reader on a Teletype Model 33, {{which would be}} connected to a PDP via a modem and acoustic telephone line. Data could be read from the paper tape reader or from the Teletype keyboard. Output was either printed to the Teletype or sent to the paper tape punch. Early versions had no facility for <b>on-line</b> <b>storage</b> of programs or data.|$|E
40|$|An <b>on-line</b> data <b>storage</b> and {{retrieval}} system {{which allows the}} user to extract and process information from stored data bases is described. The capabilities of the system are provided by a general purpose computer program containing several functional modules. The modules contained in MIRADS are briefly described along with user terminal operation procedures and MIRADS commands...|$|R
40|$|A {{mass storage}} {{simulation}} program was written to aid system designers {{in the design}} of a data processing facility. It acts as a tool for measuring the overall effect on the facility of <b>on-line</b> mass <b>storage</b> systems, and it provides the means of measuring and comparing the performance of competing mass storage systems. The performance of the simulation program is demonstrated...|$|R
40|$|In this paper, we {{consider}} a storage server architecture for multimedia information systems. While most other works on multimedia <b>storage</b> servers assume <b>on-line</b> disk <b>storage</b> [12, 11, 15, 8, 2], {{we consider}} a two-tier storage architecture with a robotic tape library as the vast near-line <b>storage</b> and <b>on-line</b> disks as the front-line storage. Magnetic tapes are cheaper, more robust, {{and have a}} larger capacity; hence they are more cost effective for large scale storage systems (e. g., video on demand (VOD) systems [7] may store {{tens of thousands of}} videos). We study in details the design issues of the tape subsystem and propose some novel tape scheduling algorithms which give faster response and require less disk buffering. We also study the disk striping policy and the disk space organization in order to fully utilize the throughput of the robotic tape system and to minimize the size of <b>on-line</b> disk <b>storage.</b> 1 Introduction Recent advances in network technologies make it feasible to provi [...] ...|$|R
5000|$|On-line : On-line {{backup storage}} is {{typically}} the most accessible {{type of data}} storage, which can begin restore in milliseconds of time. A good example is an internal hard disk or a disk array (maybe connected to SAN). This type of storage is very convenient and speedy, but is relatively expensive. <b>On-line</b> <b>storage</b> is quite vulnerable to being deleted or overwritten, either by accident, by intentional malevolent action, or {{in the wake of}} a data-deleting virus payload.|$|E
40|$|With {{the advent}} of hard disk {{microcomputer}} technology, <b>on-line</b> <b>storage</b> of a large surgical pathology data base is now possible. We have developed an inexpensive, microcomputer-based, multi-user, hard disk system for surgical pathology which utilizes a commercially available word processor and data base management system. System functions include word processing with the generation of preliminary and final surgical reports, on-line access to a large surgical pathology data base, printing of daily secretarial and histology log sheets, and semi-automated SNOMED coding. Successful implementation of this system has already resulted in more efficient file management, both in the storage and retrieval of data...|$|E
40|$|AbstractThis {{paper is}} {{concerned}} with {{a new version of}} <b>on-line</b> <b>storage</b> allocation in which the durations of all processes are known at their arrival time. This version of the problem is motivated by applications in communication networks and has not been studied previously. We provide an on-line algorithm for the problem with a competitive ratio of O(min{logΔ,logτ}), where Δ is the ratio between the longest and shortest duration of a process, and τ is the maximum number of concurrent active processes that have different durations. For the special case where all durations are powers of two, the competitive ratio achieved is O(loglogΔ) ...|$|E
40|$|Abstract. Recent {{advances}} in computer technologies {{have made it}} feasible to provide multimedia services, such as news distribution and entertainment, via high-bandwidth networks. The storage and retrieval of large multimedia objects (e. g., video) becomes a major design issue of the multimedia information system. While most other works on multimedia storage servers assume an <b>on-line</b> disk <b>storage</b> system, we consider a two-tier storage architecture with a robotic tape library as the vast near-line <b>storage</b> and an <b>on-line</b> disk system as the front-line storage. Magnetic tapes are cheaper, more robust, and have a larger capacity; hence, they are more cost effective for large scale storage systems (e. g., videoon-demand (VOD) systems may store tens of thousands of videos). We study in detail the design issues of the tape subsystem and propose some novel tape-scheduling algorithms which give faster response and require less disk buffer space. We also study the disk-striping policy and the data layout on the tape cartridge in order to fully utilize the throughput of the robotic tape system and to minimize the <b>on-line</b> disk <b>storage</b> space. Key words: Multimedia storage – Scheduling – Data layout...|$|R
40|$|International Telemetering Conference Proceedings / October 17 - 20, 1994 / Town & Country Hotel and Conference Center, San Diego, CaliforniaThis paper {{describes}} Dryden Flight Research Center's (DFRC's) {{transition from}} a mainframe-oriented post-flight data processing system, heavily dependent upon manual operation and scheduling, to a modern, distributed, highly automated system. After developing requirements and a concept development plan, DFRC replaced one multiple-CPU mainframe with five specialized servers, distributing the processing workload and separating functions. Access to flight data was improved by buying and building client server automated retrieval software that {{takes advantage of the}} local area network, and by providing over 500 gigabytes of <b>on-line</b> archival <b>storage</b> space. Engineering customers see improved access times and continuous availability (7 -days per week, 24 -hours per day) of flight research data. A significant reduction in computer operator workload was achieved, and minimal computer operator intervention is now required for flight data retrieval operations. This new post-flight system architecture was designed and built to provide flexibility, extensibility and cost-effective upgradeability. Almost two years of successful operation have proven the viability of the system. Future improvements will focus on decreasing the elapsed time between raw data capture and engineering unit data archival, increasing the <b>on-line</b> archival <b>storage</b> capacity, and decreasing the automated data retrieval response time...|$|R
50|$|Online {{collaborative}} {{sensor data}} management platforms are on-line database services that allow sensor owners to register and connect their devices to feed data into an <b>on-line</b> database for <b>storage</b> and allow developers {{to connect to}} the database and build their own applications based on that data.|$|R
40|$|High energy {{nuclear physics}} {{experiments}} at the Thomas Jefferson National Accelerator Facility ("Jefferson Lab") {{will have a}} data collection rate of 10 MB/second, generating 1 Terabyte (TB) of raw data per day of accelerator running, and a similar amount after processing. The requirement for on-line disk storage for raw and reduced data sets will exceed 1 TB during 1998. This paper discusses the <b>on-line</b> <b>storage</b> strategy that provides both high performance as well as high capacity, and focuses on the in-house evaluation of RAID (Redundant Arrays of Independent Disks) systems to fulfill the needs of both data acquisition and analysis...|$|E
40|$|Different {{lightweight}} {{methods for}} time series compression {{are discussed in}} the work. An approach to managing this compression is proposed, in the context of which a cascade of compression algorithms is considered; moreover these algorithms are chosen dynamically based on features of the data to be compressed. Implementation of these algorithms on graphics processing units (using OpenCL) allows a speed of operation on the order of 200 Gbit/s to be achieved. The methods proposed in the paper may find wide application in tasks of <b>on-line</b> <b>storage</b> and processing of telemetry data of different complex objects and systems...|$|E
40|$|Uniform storms are {{generally}} applied {{in most of}} the research on sewer systems. This is for modeling simplicity. However, in the real world, these conditions may not be applicable. It is very important to consider the migration behavior of storms not only in the design of combined sewers, but also in controlling them. Therefore, this research was carried out to improve Rathnayake and Tanyimboh’s optimal control algorithm for migrating storms. Promising results were found from the model improvement. Feasible solutions were obtained from the multi-objective optimization and, in addition, the role of <b>on-line</b> <b>storage</b> tanks was well placed...|$|E
40|$|This paper {{presents}} three self-paced, word-by-word reading {{experiments that}} {{test for the}} existence of <b>on-line</b> syntactic <b>storage</b> costs in English. To investigate this issue, we compared reading times for sentence regions in which storage costs varied, keeping other factors constant. Experiment 1 manipulated the number of verbs needed to form a grammatical sentence. Experiment 2 investigated whether filler-gap dependencies incur storage costs, and Experiment 3 investigated whether prepositional phrase arguments of verbs incur storage costs. The results of all three experiments demonstrate the role of online storage costs in sentence comprehension. Taken together with other results in the literature, the results also support a theory of sentence comprehension which includes empty categories mediating filler-gap dependencies. ...|$|R
40|$|Modern I/O {{subsystems}} include {{large storage}} servers which are configured to include multiple <b>on-line</b> and off-line <b>storage</b> media and {{to deal with}} a large number of requests with unpredictable access patterns. The problem of minimizing the cost of accessing data stored in all media is critical for the performance of the system. Given the large storage requirements of modern applications, Tertiary Storage Subsystems have become a crucial component of modern large-scale storage servers. This paper'...|$|R
40|$|Modern {{scientific}} computing involves organizing, moving, visualizing, {{and analyzing}} {{massive amounts of}} data at multiple sites around the world. The technologies, the middleware services, and the architectures {{that are used to}} build useful high-speed, wide area distributed systems, constitute the field of data intensive computing. In this paper the authors describe an architecture for data intensive applications where they use a high-speed distributed data cache as a common element for all of the sources and sinks of data. This cache-based approach provides standard interfaces to a large, application-oriented, distributed, <b>on-line,</b> transient <b>storage</b> system. They describe their implementation of this cache, how they have made it network aware, and how they do dynamic load balancing based on the current network conditions. They also show large increases in application throughput by access to knowledge of the network conditions...|$|R
40|$|General-purpose {{multi-access}} computing {{systems with}} files stored on random-access devices require that these files be protected. If the total <b>on-line</b> <b>storage</b> is inadequate {{there is a}} need for wellorganized off-line storage. This thesis discusses the management problems involved in handling backup and archive copies of files. In Part I we review what a number of systems, including the Edinburgh Multi-Access System (EMAS), have achieved. We also consider the influences of hardware and other forms of computing system. In Part II we return to EMAS and propose a design and an implementation to provide comprehensive facilities, for backup copies of files and recovery of them, and also for archive storage...|$|E
40|$|The declining {{costs of}} {{commodity}} disk drives is rapidly changing {{the economics of}} deploying large amounts of <b>on-line</b> <b>storage.</b> Conventional mass storage systems typically use high performance RAID clusters as a disk cache, often with a file system interface. The disk cache is backed by tape libraries which serve as the final repository for data. In mass storage systems where performance is an issue tape may serve only as a deep archive for disaster recovery purposes. In this case all data is stored on the disk farm. If a high availability system is required, the data is often duplicated on a separate system, with a fail-over mechanism controlling access...|$|E
40|$|The Block 1 {{very long}} {{baseline}} interferometer (VLBI) {{operated by the}} Deep Space Network (DSN) to make weekly measurements of the relative epoch and rate offsets between the time standards in the global network of DSN stations is discussed. The precision of these measurements routinely achieves sub-microsecond levels for epoch offset and accuracies of better than one part in 10 to the 12 th power for rate offset. The implementation of the phase calibrator system permits absolute measurement of epoch offset to better than 10 nanoseconds. With the near-real-time play-back and <b>on-line</b> <b>storage</b> of VLBI data, the Block 1 system typically produces clock parameters within 48 hours {{from the time of}} observation...|$|E
40|$|Abstract — This paper {{presents}} the architecture, algorithm and VLSI hardware of image acquisition, storage and compression on a single-chip CMOS image sensor. The image array {{is based on}} time domain digital pixel sensor technology equipped with non-destructive storage capability using 8 -bit Static-RAM device em-bedded at the pixel level. An adaptive quantization scheme based on Fast Boundary Adaptation Rule (FBAR) and Differential Pulse Code Modulation (DPCM) procedure followed by an <b>on-line,</b> least <b>storage</b> Quadrant Tree Decomposition (QTD) processing is proposed enabling a robust and compact image compression processor. A prototype chip including 64 x 64 pixels, read-out and control circuitry {{as well as the}} compression processor was implemented in 0. 35 µm CMOS technology with a silicon area of 3. 2 × 3. 0 mm 2. Simulation results show compression figures corresponding to 0. 75 Bit-per-Pixel (BPP), while maintaining reasonable PSNR levels. I...|$|R
40|$|Recent {{advances}} in computer technologies {{have made it}} feasible to provide multimedia services, such as news distribution and entertainment, via high bandwidth networks. The storage and retrieval of large multimedia objects (e. g., video) becomes a major design issue of the multimedia information system. While most other works on multimedia storage servers assume an <b>on-line</b> disk <b>storage</b> system [2, 12, 16, 17, 19], we consider a two-tier storage architecture with a robotic tape library as the vast near-line <b>storage</b> and an <b>on-line</b> disk system as the front-line storage. Magnetic tapes are cheaper, more robust, and have a larger capacity; hence they are more cost effective for large scale storage systems (e. g., video on demand (VOD) systems [10] may store tens of thousands of videos). We study in detail the design issues of the tape subsystem and propose some novel tape scheduling algorithms which give faster response and require less disk buffer space. We also study the disk str [...] ...|$|R
40|$|An optical jukebox {{consisting}} of an optical disk repository, a robot arm, {{and a few}} read-write disk drives {{can be used to}} form an <b>on-line</b> tertiary <b>storage</b> system that provides random accesses to a large volume of data. The bottleneck of such devices is the long disk exchange time (time to unload an optical disk from the drive and then load a new disk and get it ready for reading), which amounts to ten or even more seconds. This paper studies three techniques related to efficiently fetching data from such storage systems: properly scheduling incoming requests, implementing a good disk replacement policy and pre-mounting disks while drives are idle. Related algorithms are investigated using a simulation model which results in several interesting observations. Keywords tertiary storage, optical jukebox, request scheduling, disk replacement, pre-fetching. 1 Introduction During the past decade, data volume grows dramatically, especially for certain applications such as multimedia informa [...] ...|$|R
40|$|The {{investigation}} {{is concerned with}} 75 to 100 -kW power networks, power stations. The objective {{of the work is}} to develop and implement generalized mathematical algorithmic solutions for on-line real-time control of the established power system conditions. The author has proposed new models of controlled autotransformer and water-wheel generator operating conditions. The procedures for identification of the parameters of rejection of irregular measurements and common-base optimizations have been developed. The algorithms and complexes of programs have been developed, their characteristics have been investigated, the trends of improvement of computation response speed and <b>on-line</b> <b>storage</b> usage reduction have been proposedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Bibliography: pages [72]- 75. This {{thesis is}} devoted to the study of {{numerical}} comparison with respect to the efficiency, storage requirements, and accuracy of various Triangulation methods for the problem of fitting a function to a set of scattered data. In particular, we propose a Triangulation algorithm that is more efficient than C. L. Lawson's Triangulation Method, one of the bestknown methods available in the literature. An attractive feature of the proposed algorithm is the small <b>on-line</b> <b>storage</b> requirement. Even with a fairly large amount of data, no external reads or writes to disk are required. It is believed that the study, especially the software presented in this thesis, will be of some practical value to the engineering community. M. S. (Master of Science...|$|E
40|$|Abstract: An under–appreciated {{revolution}} in the technology of <b>on–line</b> <b>storage,</b> display, and communications will, by the year 2000, make it economically possible to place the entire contents of a library on–line, in image form, accessible from computer workstations located anywhere, with a hardware storage cost comparable to one year’s operational budget of that library. In this paper we describe a vision in which one can look at any book, journal, paper, thesis, or report in the library without leaving the office, and can follow citations by pointing; the item selected pops up immediately in an adjacent window. To bring this vision to reality, research with special attention to issues of modularity and scale will be needed, on applying the client/server model, on linking data, and on the implications of storage that must persist for decades. ...|$|E
40|$|The {{successful}} implementation of mass storage archives require careful attention to performance optimizations, {{to ensure that the}} system can handle the offered load. However, performance optimizations require an understanding of user access patterns. Since on-line archives and digital libraries are so new, little information is available. The National Space Science Data Center (NSSDC) of NASA Goddard Space Flight Center has run an <b>on-line</b> mass <b>storage</b> archive of space data, the National Data Archive and Distribution Service (NDADS), since November 1991. A large world-wide space research community makes use of NSSDC, requesting more than 20, 000 files per month. Since the initiation of their service, NSSDC has maintained log files which record all accesses the archive. In this report, we present an analysis of the NDADS log files, spanning a four year period (1992 - 1995). We analyze the log files and discuss several issues, including caching, reference patterns, changes in us [...] ...|$|R
40|$|When {{upgrading}} storage systems, {{the key is}} migrating {{data from}} old storage subsystems to the new ones for achieving a data layout able to deliver high performance I/O, increased capacity and strong data availability while preserving the effectiveness of its location method. However, achieving such data layout is not trivial when handling a redundancy scheme because the migration algorithm must guarantee both data and redundancy will not be allocated on the same disk. The Orthogonal redundancy for instance delivers strong data availability for distributed disk arrays but this scheme is basically focused on homogeneous and static environments and a technique that moves overall data layout called re-striping is applied when upgrading it. This paper presents a deterministic placement approach for distributing orthogonal redundancy on distributed heterogeneous disk arrays, which is able to adapt <b>on-line</b> the <b>storage</b> system to the capacity/performance demands by only moving a fraction of data layout. The evaluation reveals that our proposal achieve data layouts delivering an improved performance and increased capacity while keeping {{the effectiveness of the}} redundancy scheme even after several migrations. Finally, it keeps the complexity of the data management at an acceptable level. Postprint (published version...|$|R
40|$|This thesis {{develops}} a general parameterized model that facilitates {{the comparison of}} different file organization techniques for a given multiple key information retrieval system. The model is based on minimizing the expected pro-cessing time of the data base in performing on-line retrieval and updating operations. The decision rules are {{a function of the}} relevant characteristics of the data base, the <b>on-line</b> queries, the <b>storage</b> devices, and the file organization tech-niques, as well as the relative breakdown of the processing requests between retrievals and various types of updating operations. To demonstrate the use of the model, detailed timing formulas are developed for the retrieval and updating opera-tions for three different file organizations: the Multilist system, the Inverted Index system, and the Cellular Seria...|$|R
40|$|The second {{task in the}} Space Station Data System (SSDS) Analysis/Architecture Study is the {{development}} of an information base that will support the conduct of trade studies and provide sufficient data to make key design/programmatic decisions. This volume identifies the preferred options in the technology category and characterizes these options with respect to performance attributes, constraints, cost, and risk. The technology category includes advanced materials, processes, and techniques {{that can be used to}} enhance the implementation of SSDS design structures. The specific areas discussed are mass storage, including space and round <b>on-line</b> <b>storage</b> and off-line storage; man/machine interface; data processing hardware, including flight computers and advanced/fault tolerant computer architectures; and software, including data compression algorithms, on-board high level languages, and software tools. Also discussed are artificial intelligence applications and hard-wire communications...|$|E
40|$|Abstract – Canadian {{engineering}} {{schools must}} {{make the transition to}} outcome-based programming, assessment, and accreditation. The task can be daunting, especially for small schools or programs that cannot rely on extensive information technology support. Cloud-based services are a quick, low-cost option for facilitating many data-management and collaborative tasks required by the process. Cloud services have evolved from mere <b>on-line</b> <b>storage</b> to the "software as a service " paradigm. We report our experience with two services that facilitate collaborative work. Using on-line, specially crafted questionnaires, information may be automatically collected and formatted into spreadsheets, providing a powerful, general purpose data collection engine. This approach was used at various stages in the transition to graduate attributes processing: curriculum mapping, assessment, etc. On-line services have also been used to create a distributed repository of relevant literature for supporting the work of the “attributes” team...|$|E
40|$|We {{describe}} a scenario for the expected usage of COS that incorporates predicted instrumental capabilities, the COS IDT DRM as currently envisioned, community input, {{a study of}} planned and actual usage of the previous HST spectrographs (FOS, GHRS, and STIS), and estimates of {{the time to be}} allocated to COS by future TACs. From this input we derive, for times of normal and stressed usage, estimates of the expected frequency of COS science- and calibration-related exposures (~ 12 /day), average downlink volume (~ 600 Mbits/day), archive volume (TBD), and calibration reference file OPUS <b>on-line</b> <b>storage</b> volume (~ 1 Gbyte). We also provide summaries of COS instrumental capabilities and predicted sensitivities in comparison with those of the other HST spectrographs. __________________________________________________________________________ This report covers a variety of topics. To help you find information, we list the majo...|$|E
40|$|The Control Point Library Building System (CPLBS) is an interactive, {{menu driven}} system which permits a user to {{accurately}} identify features in image data by simultaneously viewing {{a map and}} a vidicon display of the image data through a Zoom Transfer Scope, extract and store features in disk-resident libraries, and perform utility functions necessary to maintain and update the generated libraries. It was developed to permit the generation of control point libraries needed by the Master Data Processor (MDP) to perform highly accurate geometric corrections to Landsat MSS and RBV earth image data. Both CPLBS and MDP were developed by IBM under contract to NASA Goddard Space Flight Center and delivered in January 1979. The CPLBS includes newly developed techniques for accurately locating control points which will perform consistently well in correlation uses, and for storing, updating, and refining sets of control points. It incorporates these techniques in a production system hosted on the MDP hardware, which includes a high-speed arithmetic processor, high-density asynchronous tape drives, and a large (1900 mbyte) <b>on-line</b> disk <b>storage</b> capacity. A Ramtek 9300 Display System and an IBM 3270 Display Station provide image and menu display capability. This paper describes the host computer and special image viewing equipment, {{as well as the}} processing, accuracy, and throughput of the CPLBS, and the structure of the generated libraries...|$|R
40|$|A 16 - bit {{microprocessor}} based {{system has been}} designed and developed for automatic flow field pressure survey and force measurements in the low speed wind tunnel at the Indian Institute of Science. The instrumentation sub systems that are required for the above measurements under the processor control, the system processor or unit, interfaces to standard peripherals, future expansion provisions and communication channel to other computer systems have all been developed in- house. Software {{has been designed to}} provide a powerful friendly and reconfigurable integrated integrated instrumentation. The software also ensures POAL time data acquision. <b>On-line</b> 'pre-processing and <b>storage</b> of the data on a floppy. Software has also been developed on an inexpensive PC for the data presentation in color graphics. The design, development and implementation details, of the system have been presented in this paper...|$|R
40|$|In {{this paper}} we present the system {{design of a}} machine that we have {{constructed}} to study a quadrupedal gallop gait. The gallop gait is the preferred high-speed gait of most cursorial quadrupeds. To gallop, an animal must generate ballistic trajectories with characteristic strong impacts, coordinate leg movements with asymmetric footfall phasing, and effectively use compliant members, all the while maintaining dynamic stability. In this paper we seek to further understand the primary biological features necessary for galloping by building and testing a robotic quadruped similar in size to a large goat or antelope. These features include high-speed actuation, energy <b>storage,</b> <b>on-line</b> learning control, and high-performance attitude sensing. Because body dynamics are primarily influenced by the impulses delivered by the legs, the successful design and control of single leg energetics is {{a major focus of}} this work. The leg stores energy durin...|$|R
