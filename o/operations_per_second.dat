552|10000|Public
5|$|In {{addition}} to reporting active processors, Folding@home determines its computing performance as measured in floating point <b>operations</b> <b>per</b> <b>second</b> (FLOPS) {{based on the}} actual execution time of its calculations. Originally this was reported as native FLOPS: the raw performance from each given type of processing hardware. In March 2009 Folding@home began reporting the performance in native and x86 FLOPS, the latter being an estimation of how many FLOPS the calculation would take on a standard x86 CPU architecture, which is commonly used as a performance reference. Specialized hardware such as GPUs can efficiently perform some complex functions in one floating point operation which otherwise needs multiple operations on the x86 architecture. The x86 measurement attempts to even out these hardware differences. Despite conservative conversions, the GPU clients' x86 FLOPS are consistently greater than their native FLOPS and comprise {{a large majority of}} Folding@home's measured computing performance.|$|E
25|$|On August 29, 2007, IBM {{announced}} the BladeCenter QS21. Generating a measured 1.05 giga–floating point <b>operations</b> <b>per</b> <b>second</b> (gigaFLOPS) per watt, with peak performance of approximately 460GFLOPS {{it is one}} of the most power efficient computing platforms to date. A single BladeCenter chassis can achieve 6.4 tera–floating point <b>operations</b> <b>per</b> <b>second</b> (teraFLOPS) and over 25.8 teraFLOPS in a standard 42U rack.|$|E
25|$|TOP500 {{reports that}} China's Tianhe-2 {{supercomputer}} is the world's most powerful computer, capable of performing over 33 quadrillion floating point <b>operations</b> <b>per</b> <b>second.</b>|$|E
30|$|Input/output <b>operations</b> <b>per</b> <b>seconds</b> (IOPS): IOPS is a {{measurement}} process used {{to characterize the}} storage devices like flash storage memory. The IOPS is not defined independently {{and it is a}} combination of three metrics. Along with IOPS other two metrics, say response time and workload metrics are also defined to characterize the performance of the memory module.|$|R
3000|$|... = 32, and M= 7, the {{computational}} cost {{in terms}} of million floating point <b>operation</b> <b>per</b> <b>second</b> (MFLOPS) results ≃ 198. 8 for SAEC module, ≃ 125. 0 for DTD module, ≃ 2. 7 for decorrelation module, and ≃ 2. 5 for BF module. It can be observed how the computational burden of this latter module is negligible {{in comparison with the}} first two.|$|R
40|$|A #(fog n) {{algorithm}} {{for large}} moduli multiplication for Residue Number System(FtNS) based architectures is proposed. The proposed modulo multiplier is {{much faster than}} previously proposed multipliers and more area efficient. The implementation of the multiplier is modular {{and is based on}} simple cells which leads to efficient VLSI realization. A VLSI implementation using 3 micron CMOS process shows that a pipelined n-bit modulo multiplication scheme can operate with a throughput of 30 M <b>operation</b> <b>per</b> <b>second.</b> 1...|$|R
25|$|Breakthroughs in {{high-performance}} computing, {{including the}} development of novel concepts for massively parallel computing and the design and application of computers that can carry out hundreds of trillions of <b>operations</b> <b>per</b> <b>second.</b>|$|E
25|$|In November 2010 the Air Force Research Laboratory (AFRL) {{created a}} {{powerful}} supercomputer by connecting together 1,760 Sony PS3s which include 168 separate {{graphical processing units}} and 84 coordinating servers in a parallel array capable of performing 500 trillion floating-point <b>operations</b> <b>per</b> <b>second</b> (500TFLOPS). As built the Condor Cluster was the 33rd largest supercomputer {{in the world and}} would be used to analyze high definition satellite imagery.|$|E
25|$|On June 22, 2006, {{researchers}} at LLNL {{announced that they}} had devised a scientific software application that sustained 207.3 trillion <b>operations</b> <b>per</b> <b>second.</b> The record performance was made at LLNL on Blue Gene/L, the world's fastest supercomputer with 131,072 processors. The record was a milestone {{in the evolution of}} predictive science, a field in which researchers use supercomputers to answer questions about such subjects as: materials science simulations, global warming, and reactions to natural disasters.|$|E
3000|$|... denote {{respectively}} {{the total number}} of read and write <b>operations</b> performed <b>per</b> <b>second</b> which are represented by numberOfRead and numberOfWrite attributes of “Logical Unit” class in Section SAN storage, whereas P [...]...|$|R
5000|$|... speed - 3-5 <b>operations</b> (program steps) <b>per</b> <b>second</b> {{on average}} (with xy taking {{the longest time}} of about 3 seconds); ...|$|R
40|$|With {{the current}} {{advances}} in VLSI technology, traditional algorithms for Residue Number System (RNS) based architectures should be reevaluated {{to explore the}} new technology dimensions. In this brief, we introduce A θ(log n) algorithm for large moduli multiplication for RNS based architectures. A systolic array {{has been designed to}} perform the modulo multiplication Algorithm. The proposed modulo multiplier is much faster than previously proposed multipliers and more area efficient. The implementation of this multiplier is modular and is based on using simple cells which leads to efficient VLSI realization. A VLSI implementation using 3 micron CMOS technology shows that a pipelined n-bit modulo multiplication scheme can operate with a throughput of 30 M <b>operation</b> <b>per</b> <b>second.</b> [URL]...|$|R
25|$|The K computer's {{placement}} on the {{top spot}} is seven years after Japan held the title in 2004. NEC's Earth Simulator supercomputer built by NEC at the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) was the fastest {{in the world at}} that time. It used 5,120 NEC SX-6i processors, generating a performance of 28,293,540 MIPS (million instructions per second). It also had a peak performance of 131 TFLOPS (131 trillion floating point <b>operations</b> <b>per</b> <b>second),</b> using proprietary vector processing chips.|$|E
25|$|Today a new supercomputer, L-CSC {{from the}} GSI Helmholtz Center, Made in Germany {{emerged as the}} most {{energy-efficient}} (or greenest) supercomputer in the world. The L-CSC cluster {{was the first and}} only supercomputer on the list to surpass 5 gigaflops/watt (billions of <b>operations</b> <b>per</b> <b>second</b> per watt). L-CSC is a heterogeneous supercomputer that is powered by Dual Intel Xeon E5-260 and GPU accelerators, namely AMD FirePro™ S9150 GPUs. It marks {{the first time that a}} supercomputer using AMD GPUs has held the top spot. Each server has a memory of 256 gigabytes. Connected, the server via an Infiniband FDR network.|$|E
25|$|Many second-generation CPUs {{delegated}} {{peripheral device}} communications to a secondary processor. For example, while the communication processor controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 <b>operations</b> <b>per</b> <b>second)</b> because most operations took {{at least two}} memory cycles; one for the instruction, one for the operand data fetch.|$|E
40|$|This paper {{presents}} a streaming processor specif- ically designed for adaptronic and biomedical engineering applications. The main {{characteristics of the}} streaming processor are the exibility to implement oating-point-based scienti c computations commonly performed in the digital signal processing application. The oating-point operators are connected to dual-port memories through separated 3 operand-buses and 2 resultant-buses. Synthesized with 130 -nm technology, the Spectron can be clocked at 480 MHz. The processor can perform 4 parallel streaming/pipeline oating-point operations using its FPMAC and CORDIC cores, resulting in a performance of about 4 485 = 1 : 94 GFlops (Giga Floating-point <b>operation</b> <b>per</b> <b>second),</b> which is suitable for high performance image processing in biomedical electronic engineering application...|$|R
3000|$|... {{being the}} K-point inverse {{discrete}} Fourier transform (IDFT) of x_(·),m'i̇. Note {{that the authors}} in [6] proposed a similar structure as (22), though P_n,m'i̇ is given by an operation involing two K/ 2 -point IDFT plus post-processing steps which require in total C_OQAM= 5 K _ 2 K+K real <b>operations</b> <b>per</b> T <b>seconds</b> [6].|$|R
40|$|INTERNATIONNAL JOURNAL OF APPLIED BIOMEDICAL ENGINEERING VOL. 6, NO. 1 2013 This paper {{presents}} a streaming processor {{specifically designed for}} adaptronic and biomedical engineering applications. The main characteristics of the streaming processor are the flexibility to implement floating-point-based scientific computations commonly performed in the digital signal processing application. The floating-point operators are connected to dual-port memories through separated 3 operand-buses and 2 resultant-buses. Synthesized with 130 -nm technology, the Spectron can be clocked at 480 MHz. The processor can perform 4 parallel streaming/pipeline floating-point operations us- ing its FPMAC and CORDIC cores, resulting in a performance of about 4 ?? 485 = 1. 94 GFlops (Giga Floating-point <b>operation</b> <b>per</b> <b>second),</b> which is suitable for high performance image processing in biomedical electronic engineering applications...|$|R
25|$|Supercomputing: DRDO's ANURAG {{developed}} the PACE+ Supercomputer for strategic purposes for supporting its various programmes. The initial version, as detailed in 1995, had the following specifications: The system delivered a sustained performance {{of more than}} 960 Mflops (million floating <b>operations</b> <b>per</b> <b>second)</b> for computational fluid dynamics programmes. Pace-Plus included 32 advanced computing nodes, each with 64 megabytes(MB) of memory that can be expanded up to 256MB and a powerful front-end processor which is a hyperSPARC with a speed of 66/90/100 megahertz (MHz). Besides fluid dynamics, these high-speed computer systems were used {{in areas such as}} vision, medical imaging, signal processing, molecular modeling, neural networks and finite element analysis. The latest variant of the PACE series is the PACE ++, a 128 node parallel processing system. With a front-end processor, it has a distributed memory and message passing system. Under Project Chitra, the DRDO is implementing a system with a computational speed of 2-3 Teraflops utilising commercial off the shelf components and the Open Source Linux Operating System.|$|E
2500|$|An {{independent}} {{third-party software}} developer utilised the Python bindings to LMDB in a high-performance environment and published, on the prominent technical news site Slashdot, {{how the system}} managed to successfully sustain 200,000 simultaneous read, write and delete <b>operations</b> <b>per</b> <b>second</b> (a total of 600,000 database <b>operations</b> <b>per</b> <b>second)</b> ...|$|E
2500|$|The work [...] {{denotes the}} number of {{operations}} performed by a given kernel or application. This metric may refer to any type of operation, from number of array points updated per second, to number of integer <b>operations</b> <b>per</b> <b>second,</b> to number of floating point <b>operations</b> <b>per</b> <b>second</b> (FLOPS), and the choice of one or another is driven by convenience. In {{the majority of the}} cases however, [...] is expressed as FLOPS.|$|E
40|$|Complex vision {{systems are}} usually quite slow, {{requiring}} tens of seconds or minutes of computer time for each image. As {{the complexity and}} experimental nature of the system increases, the speed is especially low, since all components of the system must be optimized if the system is to show good performance. The FIDO system, a stereo vision system for controlling a robot vehicle, has existed {{for a number of}} years and has been implemented on a number of different computers. These computers have ranged from a DEC KL 10 to the current implementation on the Warp machine, a 100 Million Floating-point <b>Operations</b> <b>Per</b> <b>Seconds</b> (MFLOPS) systolic army machine. FIDO has shown an enormous range in speed; its ancestor took 15 minutes per step, while the Warp implementation takes less than 5 <b>seconds</b> <b>per</b> step. Moreover, while early versions of FIDO moved in slow, start-and-stop steps, FIDO now runs continuously at 100 ram/second. We review the history of the FIDO system, discuss its implementation on different computers, and concentrate on its current Warp implementation...|$|R
40|$|Large-scale {{information}} processing often relies on subset matching for data classification and routing. Examples are publish/subscribe and stream processing systems, database systems, social media, and information-centric networking. For instance, an advanced Twitter-like messaging service where users might follow specific publishers {{as well as}} specific topics encoded as tag sets must join a stream of published messages with the users and their preferred tag sets so that the user tag set is {{a subset of the}} message tags. Subset matching is an old but also notoriously difficult problem. We present TagMatch, a system that solves this problem by taking advantage of a hybrid CPU/GPU stream processing architecture. TagMatch targets large-scale applications with thousands of matching <b>operations</b> <b>per</b> <b>seconds</b> against hundreds of millions of tag sets. We evaluate Tag- Match on an advanced message streaming application, with very positive results both in absolute terms and in comparison with existing systems. As a notable example, our experiments demonstrate that TagMatch running on a single, commodity machine with two GPUs can easily sustain the traffic throughput of Twitter even augmented with expressive tag-based selection...|$|R
3000|$|... is {{the power}} {{consumption}} of the corresponding hard disk introduced in Equation (21). Since {{it is not possible}} for monitoring systems of data centres to provide accurate information regarding whether the last performed operation is read or write, then we adopt a probabilistic approach in Equation (36) by using the number of read and write <b>operations</b> performed <b>per</b> <b>second</b> in order to guess which operation has more dominance. Note that such a guess is important for RAID protocols since the number of involved hard disks differ from read to write.|$|R
2500|$|The November 2007 {{release of}} the 30th TOP500 list of the 500 most {{powerful}} computer systems in the world, has LLNL’s Blue Gene/L computer in first place for the seventh consecutive time. Five other LLNL computers are in the top 100. [...] The November 2008 {{release of the}} TOP500 list places the Blue Gene/L supercomputer behind the Pleiades supercomputer in NASA/Ames Research Center, the Jaguar supercomputer in Oak Ridge National Laboratory, and the IBM Roadrunner supercomputer in Los Alamos National Laboratory. [...] Currently, the Blue Gene/L computer can sustain 478.2 trillion <b>operations</b> <b>per</b> <b>second,</b> with a peak of 596.4 trillion <b>operations</b> <b>per</b> <b>second.</b>|$|E
2500|$|In [...] "", a Starfleet judge {{rules that}} Data is not Starfleet property. The episode {{establishes}} that Data has a storage capacity of 800 quadrillion bits, (100 PB or 88.81784197 PiB) {{and a total}} linear computational speed of 60 trillion <b>operations</b> <b>per</b> <b>second.</b>|$|E
2500|$|Application Performance Class is a newly defined {{standard}} {{from the}} SD Specification 5.1 and 6.0 {{which not only}} define sequential Reading Speeds but also mandates a minimum IOPS for reading and writing.Class A1 requires a minimum of 1500 reading and 500 writing <b>operations</b> <b>per</b> <b>second,</b> while class A2 requires [...] 4000 and 2000 IOPS.|$|E
40|$|Multigrid methods use a {{combination}} of numerical procedures. Typical components include one or more iterative procedures (e. g., Gau-Seidel relaxation), residual computation, projection, and interpolation. Standard implementations combine the components using a structured programming methodology where each component is programmed separately and called by a multilevel procedure one at a time. Computers today have amultilevel memory hierarchy that include a memory subsystem known as a cache. Parts of main memory are duplicated inside the cache. Re-using the data in cache can speed up a program by a factor of 6; 10 on many computers. In this paper we de ne a collection of cache aware, tiled algorithms that make up standard multigrid methods. We show how to implement cache aware algorithms {{in a manner that is}} straightforward, easy, and portable. While following the de nitions in this paper will not result in getting every last oating point <b>operation</b> <b>per</b> <b>second</b> from any computer, the performance is much better than using standard implementations. The algorithms we propose appear to use completely unstructured, or spaghetti, programming methodology. However, we show that we require a more structured programming methodology than is usually found in well written multigrid codes...|$|R
40|$|The {{primary use}} of {{technical}} computing {{in the oil}} and gas industries is for seismic imaging of the earth's subsurface, driven by the business need for making well-informed drilling decisions during petroleum exploration and production. Since each oil/gas well in exploration areas costs several tens of millions of dollars, producing high-quality seismic images in a reasonable time can significantly reduce the risk of drilling a "dry hole". Similarly, these images are important as they can improve the position of wells in a billion-dollar producing oil field. However seismic imaging is very data- and compute-intensive which needs to process terabytes of data and require Gflop-years of computation (using "flop" to mean floating point <b>operation</b> <b>per</b> <b>second).</b> Due to the data/computing intensive nature of seismic imaging, parallel computing are used to process data to reduce the time compilation. With introducing of Cloud computing, MapReduce programming model has been attracted a lot of attention in parallel and distributed systems [1, 2] to execute massive processing algorithms such as Bioinformatics[3], Astronomy[4], Geology[5] and so on. In this report, we will investigate and discuss current approaches to fit seismic algorithms to MapReduce programming model...|$|R
40|$|Media {{application}} such as 3 D graphic processing, image processing, video decode and encode requires {{high rates}} of arithmetic <b>operation</b> <b>per</b> <b>second.</b> As an outcome of a decade long research, Stream Processor architecture, {{which is designed to}} exploit the characteristic of media processing was proposed. A few architectures of stream processor had been proposed and among the popular streaming processor architecture was Imagine Stream Processor. This project is focusing on implementing Imagine-based stream processor architecture on Altera Cyclone IV GX FPGA specifically for ALU Clusters and Microkernel Controller modules. This project report presents the literature reviews of books, theses and papers regarding stream processor architecture and its related use cases. This report also documents the complete project methodology taken in order to design a stream processor on the FPGA where the design of the Stream Processor is using RTL design methodology and System Verilog Language. This report also detailed the architectural design of the stream processor in Chapter 3. Furthermore, result and discussion of the experimental work involve in this project also reported in Chapter 4. The project report concluded that an FPGA-based stream processor is successfully designed and tested with specific image processing use cases. This project report also described possible enhancements possible on the stream processor desig...|$|R
2500|$|Also, As of October 2016, the Bitcoin Network had {{computing}} power {{claimed to be}} equivalent to 21,247,253.65 PFLOPS (Floating-point <b>Operations</b> <b>Per</b> <b>Second).</b> However, the elements of that network can perform only one specific cryptographic hash computation required by the bitcoin protocol. [...] They cannot perform general floating-point arithmetic operations, therefore their {{computing power}} cannot be measured in FLOPS.|$|E
2500|$|In a press release, {{the company}} claimed {{to make the}} first device capable of {{sustaining}} a billion <b>operations</b> <b>per</b> <b>second</b> (a [...] "BOP"). [...] This feat (remarkable considering that the part ran at 120MHz) was {{due in large part to}} its specialized SIMD MPEG instructions. [...] Similar instructions were eventually added to Intel's MMX x86 instruction set extensions.|$|E
2500|$|Speed and {{complexity}} replies:The {{speed at which}} human brains process information is (by some estimates) 100 billion <b>operations</b> <b>per</b> <b>second.</b> Several critics {{point out that the}} man in the room would probably take millions of years to respond to a simple question, and would require [...] "filing cabinets" [...] of astronomical proportions. This brings the clarity of Searle's intuition into doubt.|$|E
40|$|In a {{web service}} environment, such as Parlay X, {{contracts}} are agreed between the involved participants {{to guarantee a}} reasonable measure of Quality of Service (QoS). A common policy adopted is for contracts to involve only one parameter which places a limit {{on the number of}} incoming operation calls. This constraint is expressed as the maximum number of <b>operation</b> calls <b>per</b> <b>second</b> that a source is allowed to request. In this paper, we investigate the impact that different constraints placed on the incoming traffic have on performance and guarantees. By expressing shaping conditions mathematically, analysis can be performed for a scenario which considers hard time constraints. Simulations will be used to investigate scenarios involving soft time constraints...|$|R
30|$|Each “SAN” {{class has}} the {{following}} energy-relevant attributes: networkTrafficIn and networkTrafficOut {{have the same}} definition as their counterparts trafficIn and trafficOut of Section Network topology modelling. On the other hand, RAIDLevel of “Logical Unit” class shows the level (e.g. RAID 0, 1, 5, 10, etc.) of the RAID being used with the corresponding logical unit. As a matter of fact, each logical unit {{can be considered as}} a separate RAID controller. Furthermore, stripeSize shows the size of the RAID protocol’s stripe. numberOfRead and numberOfWrite denote respectively the number of read and write <b>operations</b> performed <b>per</b> <b>second.</b> All the other attributes of “Logical Unit” class have the same definition as their counterparts of the “Storage Unit” class of Section Server storage.|$|R
40|$|During {{the past}} few years, {{interest}} in convolutional neural networks (CNNs) has risen constantly, thanks to their excellent performance {{on a wide range}} of recognition and classification tasks. However, they suffer from the high level of complexity imposed by the high-dimensional convolutions in convolutional layers. Within scenarios with limited hardware resources and tight power and latency constraints, the high computational complexity of CNNs makes them difficult to be exploited. Hardware solutions have striven to reduce the power consumption using low-power techniques, and to limit the processing time by increasing the number of processing elements (PEs). While most of ASIC designs claim a peak performance of a few hundred giga <b>operations</b> <b>per</b> <b>seconds,</b> their average performance is substantially lower when applied to state-of-the-art CNNs such as AlexNet, VGGNet and ResNet, leading to low resource utilization. Their performance efficiency is limited to less than 55 % on average, which leads to unnecessarily high processing latency and silicon area. In this paper, we propose a dataflow which enables to perform both the fully-connected and convolutional computations for any filter/layer size using the same PEs. We then introduce a multi-mode inference engine (MMIE) based on the proposed dataflow. Finally, we show that the proposed MMIE achieves a performance efficiency of more than 84 % when performing the computations of the three renown CNNs (i. e., AlexNet, VGGNet and ResNet), outperforming the best architecture in the state-of-the-art in terms of energy consumption, processing latency and silicon area...|$|R
