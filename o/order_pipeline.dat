2|59|Public
40|$|Based on over 40 {{years of}} {{experience}} in the field, Ramesh Singh goes beyond corrosion control, providing techniques for addressing present and future integrity issues. Pipeline Integrity Handbook provides pipeline engineers with the tools to evaluate and inspect pipelines, safeguard the life cycle of their pipeline asset and ensure that they are optimizing delivery and capability. Presented in easy-to-use, step-by-step <b>order,</b> <b>Pipeline</b> Integrity Handbook is a quick reference for day-to-day use in identifying key pipeline degradation mechanisms and threats to pipeline integrity. The book begin...|$|E
40|$|Abstract. Simulators play an {{important}} part in computer architecture research. As for specific microarchitecture study, which focuses on the accurate behavior of out-of-order scheduling, ALU contention, and function unit management, an over-simplified abstraction is not sufficient to represent modern processor organizations. Thus cycle-accurate simulators are introduced to describe the accurate behavior in target microarchitecture. In cycle-accurate simulators, the timing feature within function units is simulated. This paper presents PPSim, a cycle-accurate PowerPC instruction set simulator, which models the cache, branch prediction, and out of <b>order</b> <b>pipeline</b> in PowerPC microarchitecture...|$|E
40|$|Abstract. A novel type {{of higher}} <b>order</b> <b>pipelined</b> neural network, the {{polynomial}} pipelined neural network, is presented. The network is constructed {{from a number}} of higher order neural networks concatenated with each other to predict highly nonlinear and nonstationary signals based on the engineering concept of divide and conquer. It is evaluated in financial time series application to predict the exchange rate between the US Dollar and 3 other currencies. The network demonstrates more accurate forecasting and an improvement in the signal to noise ratio over a number of benchmarked neural network...|$|R
3000|$|... 1 A {{preliminary}} {{version of}} β-CUSUM {{has appeared in}} “E. Tsamoura, A. Gounaris and Y. Manolopoulos: Lifting the Burden of History in Adaptive <b>Ordering</b> of <b>Pipelined</b> Stream Filters, Proceedings 7 th IEEE International Conference on Data Engineering, Workshop on Self Managing Database Systems (ICDE-SMDB), 2012 ".|$|R
30|$|We {{observe and}} {{formulate}} harmful <b>ordering</b> under transaction <b>pipeline</b> and propose to rearrange statements in decreasing order of contention to eliminate harmful ordering.|$|R
40|$|Digital filters play a {{key role}} in many DSP {{applications}} and FIR filters are usually selected because of their simplicity and stability against IIR filters. In this thesis eight architectures for multi-stream FIR filtering are studied. Primarily, three kinds of architectures are implemented and evaluated: one-toone mapping, time-multiplexed and pipeline interleaving. During implementation, practical considerations are taken into account such as implementation approach and number representation. Of interest is to see the performance comparison of different architectures, including area and power. The trade-off between area and power is an attractive topic for this work. Furthermore, the impact of the filter <b>order</b> and <b>pipeline</b> interleaving are studied. The result shows that the performance of different architectures differ a lot even with the same sample rate for each stream. It also shows that the performance of different architectures are affected by the filter <b>order</b> differently. <b>Pipeline</b> interleaving improves area utilization at the cost of rapid increment of power. Moreover, it has negative impact on the maximum working frequency. All the FIR filter architectures are synthesized in a 65 nm technology...|$|R
2500|$|In the {{settlement}} of a civil suit, in July 2011 investigators from the U.S. Department of Transportation’s Pipeline and Hazardous Materials Safety Administration determined that the 2006 spills were a result of BPXA’s failure to properly inspect and maintain the pipeline to prevent corrosion. The government issued a Corrective Action Order to BP XA that addressed the <b>pipeline’s</b> risks and <b>ordered</b> <b>pipeline</b> repair or replacement. The U.S. Environmental Protection Agency had investigated {{the extent of the}} oil spills and oversaw BPXA’s cleanup. When BP XA did not fully comply with the terms of the corrective action, a complaint was filed in March 2009 alleging violations of the Clean Water Act, the Clean Air Act and the Pipeline Safety Act. [...] In July 2011, the U.S. District Court for the District of Alaska entered a consent decree between the United States and BPXA resolving the government’s claims. [...] Under the consent decree, BPXA paid a $25 million civil penalty, the largest per-barrel penalty at that time for an oil spill, and agreed to take measures to significantly improve inspection and maintenance of its pipeline infrastructure on the North Slope to reduce the threat of additional oil spills.|$|R
40|$|Software {{systems are}} {{becoming}} heterogeneous: {{instead of a}} small number of large programs from well-established sources, a user’s desktop may now consist of many smaller components that interact in intricate ways. Some components will be downloaded from the network from sources that are only partially trusted. A user would like to know that a number of security properties hold, e. g. that personal data is not leaked to the net, but it is typically infeasible to verify that such components are well-behaved. Instead, they must be executed in a secure environment that provides fine-grain control of the allowable interactions between them, and between components and other system resources. In this paper, we consider the problem of assembling concurrent software systems from untrusted or partially trusted off-the-shelf components, using wrapper programs to encapsulate components and enforce security policies. We introduce a model programming language, the box-π calculus, that supports composition of software components and the enforcement of information flow security policies. Several example wrappers are expressed using the calculus; we explore the delicate security properties they guarantee. We present a novel causal type system that statically captures the allowed flows between wrapped possibly-badly-typed components; we use it to prove that an example <b>ordered</b> <b>pipeline</b> wrapper enforces a causal flow property...|$|R
40|$|In this paper, an instruction-level {{energy model}} is {{proposed}} for the data-path of very long instruction word (VLIW) pipelined processors {{that can be used}} to provide accurate power consumption information during either an instruction-level simulation or power-oriented scheduling at compile time. The analytical model takes into account several software-level parameters (such as instruction <b>ordering,</b> <b>pipeline</b> stall probability, and instruction cache miss probability) as well as microarchitectural-level ones (such as pipeline stage power consumption per instruction) providing an efficient pipeline-aware instruction-level power estimation, whose accuracy is very close to those given by RT or gate-level simulations. The problem of instruction-level power characterization of a K-issue VLIW processor is O(N** 2 K) where N is the number of operations in the ISA and K is the number of parallel instructions composing the very long instruction. One of the advantages of the proposed model consists of reducing the complexity of the characterization problem to O(K x N** 2). The proposed model has been used to characterize a four-issue VLIW core with a six-stage pipeline, and its accuracy and efficiency has been compared with respect to energy estimates derived by gate-level simulation. Experimental results (carried out on a set of embedded DSP benchmarks) have demonstrated an average error in accuracy of 4. 8...|$|R
50|$|The 470 {{embedded}} and customizable core, {{adhering to}} the Power ISA v2.05 Book III-E, was designed by IBM together with LSI and implemented in the PowerPC 476FP in 2009. The 476FP core has 32/32 kB L1 cache, dual integer units and a SIMD capable double precision FPU that handles DSP instructions. Emitting 1.6 W at 1.6 GHz on a 45 nm fabrication process. The 9 stage out of <b>order,</b> 5-issue <b>pipeline</b> handles speeds up to 2 GHz, supports the PLB6 bus, up to 1 MB L2 cache and up to 16 cores in SMP configurations.|$|R
30|$|The image {{processing}} algorithm for WSNs {{has made the}} following findings. Some compression algorithms have been designed for WSNs [7], which include coding algorithm by <b>ordering,</b> <b>pipelined</b> in-network compression, low-complexity video compression, and distributed compression. To decrease the hardware cost and energy consumption, Lu et al. [8] proposed the low-complexity and energy efficient image compression scheme, which reduced the computational complexity and required memory. A hardware solution for user-driven and packet loss tolerant image compression was presented and evaluated [9], {{which was designed to}} enable low power image compression and communication over wireless camera sensor networks. The control architecture was proposed by George Nikolakopoulos et al. [10] according {{to the quality of the}} transmitted images with the traffic load within the network and the level of details contained in an image frame. There are some researchers study the algorithm to reduce the energy consumption in WSNs. It is well known that the data reduction scheme of energy conservation could be used to lower the power consumption. Specially, S. K. Soni et al. [11] implemented the data reduction scheme by the prediction approach based on GM (1, 1) model. According to the importance and priority of data blocks, Kerem Irgan et al. [12] researched a simple encoding scheme at the source sensor. The WSNs would transmit the important or high priority data blocks with reliable paths.|$|R
5000|$|Practically speaking, {{there may}} be {{exceptions}} for which not enough status information about an exception is available, {{in which case the}} processor may raise a special exception, called an [...] "imprecise" [...] exception. Imprecise exceptions cannot occur in non-OoOE implementations, as processor state is changed only in program <b>order</b> (see RISC <b>Pipeline</b> Exceptions).|$|R
40|$|The {{impact of}} {{manufacturing}} flexibility on inventory investments in a distribution network {{consisting of a}} central depot {{and a number of}} local stockpoints is investigated. The lead time of outstanding <b>orders</b> in the <b>pipeline</b> of the central depot can be shortened by the use of flexibility. Stock levels are controlled by a periodic review echelon-order-up-to-policy under service level constraints...|$|R
40|$|International audienceThis paper {{presents}} the modeling, sensing {{and control of}} an autonomous underwater vehicle (AUV) developed in <b>order</b> to perform <b>pipeline</b> following. The pipeline detection and the relative angular position of the vehicle {{with respect to the}} pipeline are obtained by an artificial vision algorithm. The Proportional-Derivative (PD) paradigm with gravity compensation is used for the control of the vehicle. An analysis of performances is presented on Real-time experiments...|$|R
50|$|The {{simplest}} core, e200z0 {{features an}} in <b>order,</b> four stage <b>pipeline.</b> It has no MMU, no cache, and no FPU. It uses the variable bit length (VLE) {{part of the}} Power ISA, which uses 16-bit versions of the otherwise standard 32-bit PowerPC Book E ISA, thus reducing code footprint by up to 30%. It has a single 32-bit AMBA 2.0v6 bus interface. The load/store unit is pipelined, has a 1-cycle load latency and supports throughput of one load or store operation per cycle.|$|R
40|$|Wavelet {{transforms}} {{have been}} one of the important signal processing devel-opments in the last decade, especially for applications such as time-frequency analysis, data compression, segmentation and vision. Although several e cient implementations of wavelet transforms have been derived, their computational burden is still considerable. This paper describes two generic parallel implement-ations of wavelet transforms based on the pipeline processor farming methodology which have thepotential to achieve real-time performance. Results show thatthe parallel implementation of the over-sampled Wavelet Transform achieves virtually linear speedup, while the parallel implementation of the Discrete Wavelet Trans-form (DWT) also out-performs the sequential version, provided that the lter order is large. The DWT parallelisation performance improves with increasing data length and lter order while the frequency domain implementation perform-ance is independent ofwavelet lter <b>order.</b> Parallel <b>pipeline</b> implementations are currently suitable for processing multi-dimensional images with data length at least 512 pixels. 1...|$|R
40|$|The {{increasing}} {{demands for}} multi-media and wireless communication applications have had much impact on design of high-speed and low-power modern DSP systems. Orthogonal IIR digital filters can achieve sharp transition band and have good finite word-length behavior {{and are used}} in many modern DSP applications such as mobile communications. However, Cordic based fine-grain pipelined true orthogonal IIR digital filters have not been developed so far. In this paper, a state-space approach based novel algorithm for designing fine-grain pipelined true orthogonal IIR digital filters is proposed using the matrix lookahead technique. The algorithm only involves applying orthogonal transformations that {{are known to be}} numerically very reliable, and therefore is ideal for VLSI implementations. The proposed filter architecture can be operated at arbitrarily high sample rates and achieves linear increase in hardware complexity with respect to the filter <b>order</b> and <b>pipelining</b> level. It consists of on [...] ...|$|R
40|$|StreaMon is the {{adaptive}} query processing {{engine of the}} STREAM prototype Data Stream Management System (DSMS) [4]. A fundamental challenge in many DSMS applications (e. g., network monitoring, financial monitoring over stock tickers, sensor processing) is that conditions may vary significantly over time. Since queries in these systems are usually long-running, or continuous [4], {{it is important to}} consider adaptive approaches to query processing. Without adaptivity, performance may drop drastically as stream data and arrival characteristics, query loads, and system conditions change over time. StreaMon uses several techniques to support adaptive query processing [1, 2, 3]; we demonstrate three of them: • Reducing run-time memory requirements for continuous queries by exploiting stream data and arrival patterns. • Adaptive join <b>ordering</b> for <b>pipelined</b> multiway stream joins, with strong quality guarantees. • Placing subresult caches adaptively in pipelined multiway stream joins to avoid recomputation of intermediate results. 1...|$|R
500|$|Preliminary reports {{estimated}} that [...] to [...] of oil was {{spilled into the}} ocean through a highway drainage culvert adjacent to the broken pipeline. The amount of oil leaked was later revised to over [...] A Unified Command (ICS) was established consisting of local, state and federal agencies. This included the United States Coast Guard, the U.S. Environmental Protection Agency, California Department of Fish and Wildlife including the Office of Spill Prevention and Response, and the Santa Barbara Office of Emergency Management together with the responsible party, Plains All American Pipeline. The federal Pipeline and Hazardous Materials Safety Administration <b>ordered</b> the <b>pipeline</b> operator {{to provide them with}} the ruptured pipe for metallurgical testing in order to establish the condition of the pipe when it failed. All the oil in the pipeline had to be cleaned out before the section could be removed to determine if corrosion, pressure or a series of failures led to the rupture in the pipeline.|$|R
40|$|Skyline queries help users make {{intelligent}} decisions over complex data, where {{different and}} often conflicting criteria are considered. Current skyline computation methods {{are restricted to}} centralized query processors, limiting scalability and imposing a single point of failure. In this paper, we {{address the problem of}} parallelizing skyline query execution over a large number of machines by leveraging content-based data partitioning. We present a novel distributed skyline query processing algorithm (DSL) that discovers skyline points progressively. We propose two mechanisms, recursive region partitioning and dynamic region encoding, to enforce a partial order on query propagation in <b>order</b> to <b>pipeline</b> query execution. Our analysis shows that DSL is optimal in terms {{of the total number of}} local query invocations across all machines. In addition, simulations and measurements of a deployed system show that our system load balances communication and processing costs across cluster machines, providing incremental scalability and significant performance improvement over alternative distribution mechanisms...|$|R
50|$|Preliminary reports {{estimated}} that 20,000 gal to 21000 gal of oil was {{spilled into the}} ocean through a highway drainage culvert adjacent to the broken pipeline. The amount of oil leaked was later revised to over 105,000 gal. A Unified Command (ICS) was established consisting of local, state and federal agencies. This included the United States Coast Guard, the U.S. Environmental Protection Agency, California Department of Fish and Wildlife including the Office of Spill Prevention and Response, and the Santa Barbara Office of Emergency Management together with the responsible party, Plains All American Pipeline. The federal Pipeline and Hazardous Materials Safety Administration <b>ordered</b> the <b>pipeline</b> operator {{to provide them with}} the ruptured pipe for metallurgical testing in order to establish the condition of the pipe when it failed. All the oil in the pipeline had to be cleaned out before the section could be removed to determine if corrosion, pressure or a series of failures led to the rupture in the pipeline.|$|R
50|$|The broader {{category}} of multi-core processors, by contrast are usually designed to efficiently run both parallel and serial code, and therefore place {{more emphasis on}} high single thread performance (e.g. devoting more silicon to out of <b>order</b> execution, deeper <b>pipelines,</b> more superscalar execution units, and larger, more general caches), and shared memory. These techniques devote runtime resources toward figuring out implicit parallelism in a single thread. They are used in systems where they have evolved continuously (with backward compatibility) from single core processors. They usually have a 'few' cores (e.g. 2,4,8), and may be complemented by a manycore accelerator (such as a GPU) in a heterogeneous system.|$|R
50|$|In an {{automotive}} context, BTO is {{a demand}} driven production approach where {{a product is}} scheduled and built {{in response to a}} confirmed order received for it from a final customer. The final customer refers to a known individual owner and excludes all orders by the original equipment manufacturer (OEM), national sales companies (NSC), dealers or point of sales, bulk orders or other intermediaries in the supply chain. BTO excludes the order amendment function, whereby forecast <b>orders</b> in the <b>pipeline</b> are amended to customer requirements, as this is seen as another level of sophistication for a build to stock (BTS) system (also known as build to forecast (BTF)).|$|R
5000|$|On April 2, PHMSA {{issued a}} {{corrective}} action <b>order</b> to ExxonMobil <b>Pipeline</b> Co. preventing ExxonMobil from restarting operations on the affected {{segment of the}} pipeline until it is satisfied with repairs and all safety concerns have been addressed. According to the order: [...] "continued operation of the Pegasus Pipeline would be hazardous to life, property, and the environment." [...] Arkansas' Attorney General Dustin McDaniel promised a state investigation into the cause and impact of the spill. In a letter to ExxonMobil McDaniel stated: [...] "There are many questions and concerns remaining as to the long-term impacts, environmental or otherwise, from this spill," [...] He asked ExxonMobil to preserve records pending his investigation.|$|R
40|$|Virtual-build-to-order (VBTO) {{is a form}} {{of order}} {{fulfilment}} system in which the producer has the ability to search across the entire pipeline of finished stock, products in production and those in the production plan, in order to find the best product for a customer. It is a system design that is attractive to Mass Customizers, such as those in the automotive sector, whose manufacturing lead time exceeds their customers' tolerable waiting times, and for whom the holding of partly-finished stocks at a fixed decoupling point is unattractive or unworkable. This paper describes and develops the operational concepts that underpin VBTO, in particular the concepts of reconfiguration flexibility and customer aversion to waiting. Reconfiguration is the process of changing a product's specification at any point along the <b>order</b> fulfilment <b>pipeline.</b> The extent to which an order fulfilment system is flexible or inflexible reveals itself in the reconfiguration cost curve, of which there are four basic types. The operational features of the generic VBTO system are described and simulation is used to study its behaviour and performance. The concepts of reconfiguration flexibility and floating decoupling point are introduced and discussed...|$|R
40|$|Modern multi-processors embody up to {{hundreds}} of cores in a single chip, {{in an attempt to}} attain TFlops/sec performance. Many subtle programming frameworks have emerged in order to facilitate the development of parallel, efficient and scalable applications. The MapReduce programming model, after having indisputably, demonstrated its usability and effectiveness in the area of Large-Scale Distributed Systems computation, has been adapted to the needs of shared-memory multi-core and multi-processor systems. The scope of this thesis is to enhance the existing, traditional MapReduce Architecture, by decoupling Map from Combine into two separate phases. These independent phases are interleaved and executed in parallel. We argue that, interleaving Map and Combine computation, leads to more efficient hardware utilization and competent run-time improvements. A high-performance, shared queue data structure has been introduced in <b>order</b> to <b>pipeline</b> intermediate data from Map to Combine phase and allow for concurrent execution. Furthermore, an Inter-thread communication aware thread-to-cpu binding policy has been designed to minimize data exchange overhead. The Pipelined Architecture is evaluated into two inherently diverse multi-core systems and demonstrates execution speedup of up to 5. 34 X compared to a state-of-the art MapReduce Library, Phoenix++. Nevertheless, we observe that not all type of workloads profit from our Pipelined Architecture and reason about the application characteristics that define its suitability to our Runtime...|$|R
40|$|Cathodic {{protection}} (CP) failure due to excursions from safe CP {{levels is}} a challenge for the protection and maintenance of buried energy pipelines. Although research shows that stray current {{is a major factor}} contributing to CP failure, there is little consensus on how 2 ̆ 7 big 2 ̆ 7 the excursions (either in magnitude, length or frequency) need to be in <b>order</b> to cause <b>pipeline</b> corrosion problems. This uncertainty has caused difficulties in selecting suitable parameters in relevant industry standards. This paper provides a brief review of past research on different factors affecting CP efficiency. Preliminary results from new electrochemical cells designed to develop an understanding of how CP excursions away from the 2 ̆ 7 safe 2 ̆ 7 level can lead to corrosion problems are also presented...|$|R
2500|$|From 1939, the base's {{water was}} {{supplied}} by pipelines that drew {{water from the}} Yateras River about [...] northeast of the base. The U.S. government paid a fee for this; in 1964, it was about $14,000 a month for about [...] per day. In 1964, the Cuban government stopped the flow. The base had about [...] of water in storage, and strict water conservation was put into effect immediately. The U.S. first imported water from Jamaica by barge, then relocated a desalination plant from San Diego (Point Loma). When the Cuban government accused the United States of stealing water, base commander John D. Bulkeley <b>ordered</b> that the <b>pipelines</b> be cut and a section removed. A [...] length of the [...] diameter pipe and a [...] length of the [...] diameter pipe were lifted {{from the ground and}} the openings sealed.|$|R
40|$|A {{methodology}} {{is presented}} on assessing the seismic risk of buried steel pipelines crossing active tectonic faults through a comprehensive analysis by incorporating {{the uncertainty of}} the loading resulting from fault movement, soil response and the response of the pipeline itself. The proposed methodology is a two-step process. In the first step Probabilistic Fault Displacement Hazard analysis is implemented to quantify the probabilistic nature of the load, namely the imposed differential displacement on the pipeline due to large permanent fault displacements, incorporating all pertinent uncertainties regarding, for example, seismicity rate, maximum moment magnitude, etc. The second step is the “transition ” from seismological data to pipeline structural response through a vector intensity measure represented by the fault displacement components in 3 D. Advanced pipeline numerical simulations are then carried out in <b>order</b> to form <b>pipeline</b> strain hazard curves as a useful engineering tool for pipeline fault crossing seismic risk assessment...|$|R
50|$|From 1939, the base's {{water was}} {{supplied}} by pipelines that drew {{water from the}} Yateras River about 4.5 mi northeast of the base. The U.S. government paid a fee for this; in 1964, it was about $14,000 a month for about 2500000 gal per day. In 1964, the Cuban government stopped the flow. The base had about 14000000 gal of water in storage, and strict water conservation was put into effect immediately. The U.S. first imported water from Jamaica by barge, then relocated a desalination plant from San Diego (Point Loma). When the Cuban government accused the United States of stealing water, base commander John D. Bulkeley <b>ordered</b> that the <b>pipelines</b> be cut and a section removed. A 38 in length of the 14 in diameter pipe and a 20 in length of the 10 in diameter pipe were lifted {{from the ground and}} the openings sealed.|$|R
40|$|The {{restructuring}} of retail gas services has followed a typical pattern for previously heavily regulated industries: large customers are initially given rights to purchase unbundled services from different entities, {{with the same}} rights dispersed over time to smaller customers. For about ten years now industrial customers in most states {{have been able to}} {open_quotes}play the market{close_quotes}. Since the passage of the Federal Energy Regulatory Commission (FERC) Order 636 in 1992, interest has centered on expanding service unbundling to small retail customers, including residential customers. Importantly, the <b>Order</b> prohibited <b>pipelines</b> from providing bundled sales service. This is not surprising - in the telecommunications industry, for example, the unbundling of wholesale services was a strong stimulant for developing competition in the local exchange market. The push for small-customer service unbundling has derived from the basic but politically attractive idea that all retail customers should directly benefit from competitive forces in the natural gas industry. When one looks at the movement of prices since 1985, {{it is easy to see}} that large retail customers have enjoyed more favorable prices than other retail customers. For example, over the period 1985 to 1994 gas prices to industrial customers and electric utilities fell around 23 percent and 36 percent, respectively. In comparison, gas prices to residential customers increased by around 5 percent while gas prices to commercial customers decreased slightly by about 1 percent. This report examines various aspects of unbundling to small retail gas customers, with special emphasis on residential customers...|$|R
40|$|Scientific data {{analyses}} often apply a pipelined sequence of computational tasks to independent datasets. Each {{task in the}} pipeline captures and processes a dataset element, may be dependent on other tasks in the pipeline, may have a different computational complexity and may be filtered out from progressing in the pipeline. The goal of this work is to develop an efficient scheduler that automatically (i) manages a parallel data reading and an adequate data structure creation, (ii) adaptively defines the most efficient <b>order</b> of <b>pipeline</b> execution of the tasks, considering their inter-dependence and both the filtering out rate and the computational weight, and (iii) manages the parallel execution of the computational tasks in a multicore system, applied to the same or to different dataset elements. A real case study data analysis application from High Energy Physics (HEP) was used to validate the efficiency of this scheduler. Preliminary results show an impressive performance improvement of the pipeline tuning {{when compared to the}} original sequential HEP code (up to a 35 x speedup in a dual 12 -core system), and also show significant performance speedups over conventional parallelization approaches of this case study application (up to 10 x faster in the same system). Project Search-ON 2 (NORTE- 07 - 0162 - FEDER- 000086), co-funded by the North Portugal Regional Operational Programme (ON. 2 - O Novo Norte), under the National Strategic Reference Framework, through the European Regional Development Fund...|$|R
40|$|In {{the last}} decade, the {{importance}} of graphics capabilities have be-come very important in the mobile market. As a result low power embedded solutions for mobile devices {{have been developed to}} run computationally intensive graphics applications, which extensively uses floating point calculations. The work proposed in this thesis target the extension of the Silicon Hive processors capabilities for graphics applications. The Silicon Hive core generation flow that al-lows to introduce a very high degree of parallelism can be efficiently used to generate a processor for graphics. In order to achieve that, in this thesis, we present an hybrid VLIW/SIMD floating point pro-cessor derived from the base Silicon Hive VLIW architecture, Pearl Ray. The hardware implementation of floating point functional units is realized using the Synopsys DesignWare building blocks, which are designed in a way that allows the efficient use of register retiming option in the Design Compiler flow, in <b>order</b> to introduce <b>pipeline</b> stages and improve the timing. Th...|$|R
40|$|Mobil North Sea Ltd. {{operates}} the SAGE Gas Terminal at St. Ferus in Grampian Region, Scotland {{on behalf of}} the SAGE partners. When Phase B is completed in 1994, the gas plant be capable of treating 1150 MMSCFD of sour natural gas from three different North Sea producing areas. Treated pipeline to the Shell NGL fractionation plant at Mossmorran. In <b>order</b> to meet <b>pipeline</b> gas specifications over a wide range of flowrates and feed gas compositions, single-stage turboexpander chilling was selected over Joule-Thomson valve expansion. Four turboexpanders (2 per process train) operate in parallel to achieve ther required performance over the entire flow range of 90 - 575 MMSCHD per process train. Unusual operating condition for the turboexpanders include dense phase inlet gas, expandion near the criondenbar, and high equilibrium liquid content at the exhaust (up to 50 % by weight). The two turboexpanders in each share common suction and discharge facilities as do their associated brake compressors. This paper discusses details of the appliaction including commissioning, start-up, and operation. ...|$|R
40|$|Long range {{ultrasonic}} testing {{is now a}} well established method for examining in-service degradation in <b>pipelines.</b> In <b>order</b> to protect <b>pipelines</b> from the surrounding environment {{it is common for}} viscoelastic coatings to be applied to the outer surface. These coatings are, however, known to impact on the ability of long range ultrasonic techniques to locate degradation, or defects, within a coated pipe. The coating dissipates sound energy travelling along the pipe, attenuating both the incident and reflected signals making responses from defects difficult to detect. This article aims to investigate the influence of a viscoelastic coating on the ability of long range {{ultrasonic testing}} to detect a defect in an axisymmetric pipe. The article focuses on understanding the behaviour of the fundamental torsional mode and quantifying the effect of bitumen coatings on reflection coefficients generated by axisymmetric defects. Reflection coefficients are measured experimentally for coated and uncoated pipes and compared to theoretical predictions generated using numerical mode matching and a hybrid finite element technique. Good agreement between prediction and measurement is observed for uncoated pipes...|$|R
40|$|Abstract—Deduplication {{has become}} one of the hottest topics in the field of data storage. Quite a few methods towards {{reducing}} disk I/O caused by deduplication have been proposed. Some methods also have been studied to accelerate computational subtasks in deduplication. However, the order of computational subtasks can affect overall deduplication throughput significantly, because computational sub-tasks exhibit quite different workload and concurrency in different orders and with different data sets. This paper proposes an adaptive pipelining model for the computational sub-tasks in deduplication. It takes both data type and hardware platform into account. Taking the compression ratio and the duplicate ratio of the data stream, and the compression speed and the fingerprinting speed on different processing units as parameters, it determines the optimal <b>order</b> of the <b>pipeline</b> stages (computational sub-tasks) and assigns each stage to the processing unit which processes it fastest. That is, “adaptive ” refers to both data adaptive and hardware adaptive. Experimental results show that the adaptive pipeline improves the deduplication throughput up to 50 % compared with the plain fixed pipeline, which implies that it is suitable for simultaneous deduplication of various data types on modern heterogeneous multi-core systems. I...|$|R
