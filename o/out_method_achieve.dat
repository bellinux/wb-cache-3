1|10000|Public
40|$|This paper investigates {{differentially}} private {{analysis of}} distance-based outliers. The problem of outlier detection {{is to find}} a small number of instances that are apparently distant from the remaining instances. On the other hand, the objective of differential privacy is to conceal presence (or absence) of any particular instance. Outlier detection and privacy protection are thus intrinsically conflicting tasks. In this paper, instead of reporting outliers detected, we present two types of differentially private queries that help to understand behavior of outliers. One is the query to count outliers, which reports the number of outliers that appear in a given subspace. Our formal analysis on the exact global sensitivity of outlier counts reveals that regular global sensitivity based method can make the outputs too noisy, particularly when the dimensionality of the given subspace is high. Noting that the counts of outliers are typically expected to be relatively small compared to the number of data, we introduce a mechanism based on the smooth upper bound of the local sensitivity. The other is the query to discovery top-$h$ subspaces containing a large number of outliers. This task can be naively achieved by issuing count queries to each subspace in turn. However, the variation of subspaces can grow exponentially in the data dimensionality. This can cause serious consumption of the privacy budget. For this task, we propose an exponential mechanism with a customized score function for subspace discovery. To the best of our knowledge, this study is the first trial to ensure differential privacy for distance-based outlier analysis. We demonstrated our methods with synthesized datasets and real datasets. The experimental results show that <b>out</b> <b>method</b> <b>achieve</b> better utility compared to the global sensitivity based methods...|$|E
30|$|The {{performance}} of this communication protocol {{in terms of}} energy consumption, tracking delay, and trajectory accuracy is evaluated and compared with other similar protocols through simulation. The simulation results show that <b>out</b> new <b>method</b> <b>achieves</b> better communication delay and thus better tracking accuracy while maintaining reasonable energy consumption.|$|R
40|$|This paper {{introduces}} a novel method for cancellable and irrevocable biometric template generation. The proposed method named as complex conjugate phase transform takes {{the main components}} like bit shifted-phase, twin complex conjugate transpose and chaff point generation along with it. The strength of the proposed method is tested in different aspects such as cancelability, irrevocability and security. The performance of the same is also calculated {{in terms of time}} and space complexity; ROC analysis is also carried <b>out.</b> The proposed <b>method</b> <b>achieves</b> higher matching scores; and the experimental results show that the proposed complex conjugate phase transform is better in all the aspects of performance...|$|R
40|$|Part 2 : Security EngineeringInternational audienceAccording to the Basel II Accord {{for banks}} and Solvency II for the {{insurance}} industry, not only should the market and financial risks for the institutions be determined, also the operational risks (opRisk). In recent decades, Value at Risk (VaR) has prevailed for market and financial risks {{as a basis for}} assessing the present risks. Occasionally, there are suggestions as to how the VaR is to be determined in the field of operational risk. However, existing proposals can only be applied to an IT infrastructure to a certain extent, or to parts of them e. g. such as VoIP telephony. In this article, a proposal is discussed to calculate a technical Value at Risk (t-VaR). This proposal is based on risk scenario technology and uses the conditional probability of the Bayes theorem. The vulnerabilities have been determined empirically for an insurance company in 2012. To determine the threats, attack trees and threat actors are used. The attack trees are weighted by a function that is called the criminal energy. To verify this approach the t-VaR was calculated for VoIP telephony for an insurance company. It turns <b>out</b> that this <b>method</b> <b>achieves</b> good and sufficient results for the IT infrastructure as an effective method to meet the Solvency II’s requirements...|$|R
30|$|The {{analysis}} was carried <b>out</b> using a <b>method</b> inspired from the Delphi technique, a widely used and accepted <b>method</b> for <b>achieving</b> convergence of findings concerned with real-world expert knowledge [21].|$|R
30|$|The CWAG <b>method</b> <b>achieves</b> 26.6  % {{more than}} twice the {{incremental}} recovery of WAG.|$|R
30|$|In addition, both <b>methods</b> <b>achieved</b> a {{hit rate}} of 80 % for the testing data.|$|R
3000|$|... 2 <b>method</b> <b>achieves</b> better {{accuracy}} as {{the value}} of α decreases, but its performance is sensitive to α.|$|R
3000|$|... {{show that}} {{proposed}} MA-SA <b>method</b> <b>achieves</b> both significant computational savings and performance improvement {{when compared to}} conventional methods.|$|R
30|$|Extensive {{experiments}} over {{real data}} show that our <b>method</b> <b>achieves</b> better performance compared with the classic co-occurrence-based methods and statistic-based methods.|$|R
50|$|The {{original}} <b>method</b> <b>achieved</b> 50-70% accuracy (depending on the word) on Pride and Prejudice {{and selected}} papers of the Associated Press.|$|R
40|$|This {{contribution}} {{reports the}} performance analysis of SCE 4 “ 5. 3 -test 1 ” and “ 5. 3 -test 2 ” on Color Gamut and Bit-Depth Scalability, {{based on the}} use of 3 D color Look-Up Tables (LUT) to perform inter-layer prediction. It is reported that compared with the SCE 4 anchor, for 5. 3 -test 1 (8 -bit BL, 10 -bit EL) in AI configuration, the proposed <b>method</b> <b>achieves</b> an average BD rate of- 12. 3 %,- 9. 9 %,- 16. 0 % for Y, U, V, respectively. For RA configuration, the proposed <b>method</b> <b>achieves</b> an average BD rate of- 8. 2 %,- 3. 0 %,- 9. 9 % for Y, U, V, respectively. It is reported that compared with the SCE 4 anchor, for 5. 3 -test 2 (10 -bit BL, 10 -bit EL) in AI configuration, the proposed <b>method</b> <b>achieves</b> an average BD rate of- 12. 2 %,- 9. 6 %,- 14. 9 % for Y, U, V, respectively. For RA configuration, the proposed <b>method</b> <b>achieves</b> an average BD rate of- 8. 5 %,- 3. 4 %,- 10. 1 % for Y, U, V, respectively. ...|$|R
30|$|As a conclusion, the {{proposed}} force distribution <b>method</b> <b>achieves</b> suitable torque inputs, taking account of slippage for the leg-grope walk in various environments.|$|R
30|$|The {{proposed}} <b>method</b> and LSQ <b>achieve</b> {{a nearly}} constant detection performance up to 1 / 8 -s audio segments, whereas {{the performance of}} the YSH method drops for audio segments under 2 s. Our <b>method</b> <b>achieves</b> very good performance in the case of high-quality MP 3 files: indeed, for BR 2 equal to 192 kbit/s, our <b>method</b> <b>achieves</b> an almost perfect classification performance irrespective of the audio segment duration. Conversely, For BR 2 equal to 128 kbit/s, even if the performance is only slightly affected by the segment duration, the proposed method remains inferior with respect to the other two methods.|$|R
30|$|Therefore, the {{proposed}} optimization <b>method</b> <b>achieves</b> better performance especially at low and moderate rotor speeds. Systems losses are reduced and system efficiency is improved.|$|R
50|$|While some {{synthesis}} <b>methods</b> <b>achieve</b> sonic complexity {{by using}} many oscillators, distortion methods create a frequency spectrum which has many more components than oscillators.|$|R
50|$|In 1879-80, Hermann Ebbinghaus {{conducted}} research on higher mental processes; he replicated the entire procedure in 1883-4. Ebbinghaus' <b>methods</b> <b>achieved</b> a remarkable set of results.|$|R
50|$|He {{designed}} a fuzzy mathematical method for the SWOT analysis, that surpasses all {{the limitations of}} the previous <b>methods,</b> <b>achieving</b> entrepreneurial precise diagnosises for strategic planning.|$|R
30|$|A simple solution-processed {{dip-coating}} <b>method</b> <b>achieves</b> large-area single-crystalline nanoribbon arrays (5  ×  10  cm 2). High-performance organic field-effect transistors (OFETs) can {{be obtained}} through this method.|$|R
30|$|While {{considering}} {{accuracy and}} macro F-measure, {{it is observed}} that classification using our proposed <b>method</b> <b>achieves</b> a good accuracy (90.21, 89.98 % respectively) against Naïve Bayes and Google prediction API.|$|R
3000|$|..., which {{represents}} {{the size of the}} SCI. In the presence of channel fading and CFO, the time-domain <b>method</b> <b>achieves</b> an improved detection performance over the ML approach when N [...]...|$|R
5000|$|Listeria is a {{bacterial}} disease associated with unpasteurised milk, and can affect some cheeses made in traditional ways. Careful observance {{of the traditional}} cheesemaking <b>methods</b> <b>achieves</b> reasonable protection for the consumer.|$|R
40|$|This paper {{describes}} {{a method for}} integrating separately developed information models. The models may be the schemas of databases, frame systems of knowledge bases, or process models of business operations. The <b>method</b> <b>achieves</b> integration at the semantic level by using an existing global ontology to resolve inconsistencies. The integrated models provide a coherent picture of an enterprise and enable its resources to be accessed and modi ed coherently. The <b>method</b> <b>achieves</b> integration at the semantic level by using an existing global ontology to resolve inconsistencies. Some heuristics are presented {{that may be used}} along with limited input from humans to guide this process...|$|R
5000|$|Rigorous {{calculation}} methods: Bubble point method, sum rates method, numerical methods (Newton-Raphson technique), inside <b>out</b> <b>method,</b> relaxation method, {{other methods}} ...|$|R
40|$|Genomic DNA {{obtained}} from patient whole blood samples {{is a key}} element for genomic research. Advantages and disadvantages, in terms of time-efficiency, cost-effectiveness and laboratory requirements, of procedures available to isolate nucleic acids need to be considered before choosing any particular method. These characteristics have not been fully evaluated for some laboratory techniques, such as the salting <b>out</b> <b>method</b> for DNA extraction, which has been excluded from comparison in different studies published to date. We compared three different protocols (a traditional salting <b>out</b> <b>method,</b> a modified salting <b>out</b> <b>method</b> and a commercially available kit method) to determine the most cost-effective and time-efficient method to extract DNA. We extracted genomic DNA from whole blood samples {{obtained from}} breast cancer patient volunteers and compared the results of the product obtained in terms of quantity (concentration of DNA extracted and DNA obtained per ml of blood used) and quality (260 / 280 ratio and polymerase chain reaction product amplification) of the obtained yield. On average, all three methods showed no statistically significant differences between the final result, but when we accounted for time and cost derived for each method, they showed very significant differences. The modified salting <b>out</b> <b>method</b> resulted in a seven- and twofold reduction in cost compared to the commercial kit and traditional salting <b>out</b> <b>method,</b> respectively and reduced time from 3 days to 1 hour compared to the traditional salting <b>out</b> <b>method.</b> This highlights a modified salting <b>out</b> <b>method</b> as a suitable choice to be used in laboratories and research centres, particularly when dealing with a large number of samples. ...|$|R
30|$|Next, {{we examine}} the average rate {{performance}} as the moving speed and handoff overhead change. Figures 6 and 7 show the average rate versus the actual moving speeds when v 0 = 100 m/s and v 0 = 50 m/s, respectively. The simulation setting {{is the same as}} that in Figures 2 and 3. By always connecting to the RS with the best SNR, the conventional <b>method</b> <b>achieves</b> the highest transmission rate. Both Figures 6 and 7 show that the proposed <b>method</b> <b>achieves</b> almost the same average rate as the conventional one, indicating that the proposed <b>method</b> can <b>achieve</b> the highest reward without sacrificing much transmission rate. The speed <b>method</b> also <b>achieves</b> almost as good transmission rate as the proposed one, but at a price of reduced reward performance. Without considering the moving speed changes, the average rate achieved by the overhead method can change significantly with the actual moving speed, and much worse than the average rates achieved by the other handoff methods when the actual moving speed is much larger or smaller than the original speed.|$|R
3000|$|... the {{subspace}} based {{methods have}} higher accuracy than Capon method from Fig.  10. The MUSIC <b>method</b> <b>achieves</b> slightly better performance than the ESPRIT method. Whereas, the Capon method and MUSIC method involves the spectral peak searching.|$|R
50|$|Using {{large-scale}} English data, NLI <b>methods</b> <b>achieve</b> over 80% {{accuracy in}} predicting the native language of texts written by authors from 11 different L1 backgrounds. This {{can be compared to}} a baseline of 9% for choosing randomly.|$|R
3000|$|... {{which is}} always a {{positive}} value. This indicates that the bit depth of the proposed method is smaller {{than that of the}} baseline method. Thus, we have theoretically shown that the proposed <b>method</b> <b>achieves</b> bit-depth reduction in the enhancement layer.|$|R
40|$|We propose an {{unsupervised}} {{method to}} automatically extract domain-specific prefixes and suffixes from biological corpora {{based on the}} use of PATRICIA tree. The method is evaluated by integrating the extracted affixes into an existing learning-based biological term annotation system. The system based on our <b>method</b> <b>achieves</b> comparable experimental results to the original system in locating biological terms and exact term matching annotation. However, our method improves the system efficiency by significantly reducing the feature set size. Additionally, the <b>method</b> <b>achieves</b> a better performance with a small training data set. Since the affix extraction process is unsupervised, it is assumed that the method can be generalized to extract domain-specific affixes from other domains, thus assisting in domain-specific concept recognition. ...|$|R
40|$|Due to {{resource}} and power constraints, embedded processors often cannot afford dedicated floating-point units. For instance, the IBM PowerPC processor embedded in Xilinx Virtex-II Pro FPGAs only supports emulated floating-point arithmetic, {{which leads to}} slow operation when floatingpoint arithmetic is desired. This paper presents a customizable mathematical library using fixed-point arithmetic for elementary function evaluation. We approximate functions via polynomial or rational approximations depending on the user-defined accuracy requirements. The data representation for the inputs and outputs are compatible with IEEE single-precision and double-precision floating-point formats. Results show that our 32 -bit polynomial <b>method</b> <b>achieves</b> over 80 times speedup over the single-precision mathematical library from Xilinx, while our 64 -bit polynomial <b>method</b> <b>achieves</b> over 30 times speedup...|$|R
40|$|Speaker {{identification}} {{is a key}} component in many practical appli-cations and the need of finding algorithms, which are robust under adverse noisy conditions, is extremely important. In this paper, the problem of text-independent speaker {{identification is}} studied in light of classification based on sparsity representation combined with a discriminative dictionary learning technique. Experimental evalua-tions on a small dataset reveal that the proposed <b>method</b> <b>achieves</b> a superior performance under short training sessions restrictions. In specific, the proposed <b>method</b> <b>achieved</b> high robustness for all the noisy conditions that were examined, when compared with a GMM universal background model (UBM-GMM) and sparse representa-tion classification (SRC) approaches. Index Terms — speaker identification, sparse representation, discriminative dictionary learning, K-SVD 1...|$|R
40|$|Recent methods {{based on}} 3 D {{skeleton}} data have achieved outstanding performance {{due to its}} conciseness, robustness, and view-independent representation. With the development of deep learning, Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) -based learning <b>methods</b> have <b>achieved</b> promising performance for action recognition. However, for CNN-based methods, it is inevitable to loss temporal information when a sequence is encoded into images. In order to capture as much spatial-temporal information as possible, LSTM and CNN are adopted to conduct effective recognition with later score fusion. In addition, experimental {{results show that the}} score fusion between CNN and LSTM performs better than that between LSTM and LSTM for the same feature. Our <b>method</b> <b>achieved</b> state-of-the-art results on NTU RGB+D datasets for 3 D human action analysis. The proposed <b>method</b> <b>achieved</b> 87. 40 % in terms of accuracy and ranked $ 1 ^{st}$ place in Large Scale 3 D Human Activity Analysis Challenge in Depth Videos...|$|R
3000|$|We {{believe that}} {{treatment}} for DDH using a modified medial approach during early childhood {{is an effective}} and reliable method with low AVN rates. As shown here, this <b>method</b> <b>achieves</b> great success in radiological and clinical outcomes after a minimum 3 -year follow-up.|$|R
30|$|In this study, {{we tested}} methods for DNN {{hyperparameter}} optimization. We {{showed that the}} Nelder-Mead <b>method</b> <b>achieved</b> good results in all experiments. Moreover, we achieved state-of-the-art accuracy with age/gender classification using the Adience DB by optimizing the CNN hyperparameters proposed in [26].|$|R
25|$|Wireless {{high power}} {{transmission}} using microwaves is well proven. Experiments {{in the tens}} of kilowatts have been performed at Goldstone in California in 1975 and more recently (1997) at Grand Bassin on Reunion Island. These <b>methods</b> <b>achieve</b> distances on the order of a kilometer.|$|R
