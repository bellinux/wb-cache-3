3|17|Public
40|$|A {{consideration}} of medieval astronomy and {{the views of}} the philosopher William of Occam leads to a {{consideration of}} the ambiguities in the application of the principle of <b>Occam</b> <b>s</b> <b>razor</b> as it applies to problems in science, including the constants applicable to Einstein s Theory of General Relativity and cosmology. The principle can both be used to eliminate unnecessary irrelevancies, but also to constrain the development of imaginative theories. ...|$|E
40|$|We {{present a}} method that assesses the {{theoretical}} detection limit of a Bayesian Markov chain Monte Carlo search for a periodic gravitational wave signal emitted by a neutron star. Inverse probability yields an upper limit estimate for the strength when a signal could not be detected in an observed data set. The proposed method is based on Bayesian model comparison that automatically quantifies <b>Occam</b> <b>s</b> <b>Razor.</b> It limits the complexity of a model by favoring the most parsimonious model that explains the data. By comparing the model with a signal from a pulsar to the null model that assumes solely noise, we derive the detection probability and an estimate for the upper limit that a search, for example, for a narrow-band emission for SN 1987 a, might yield on data at the sensitivity of LIGO data for an observation time of one year. Comment: 11 pages, 3 figures (1, 2 a,b, 3 a,b,c,d...|$|E
40|$|Development of good {{predictive}} {{models for}} jet noise {{has always been}} plagued by the difficulty in obtaining good quality data {{over a wide range}} of conditions in different facilities. We consider such issues very carefully in selecting data to be used in developing our model. Flight effects are of critical importance, and none of the means of determining them are without significant problems. Free-jet flight simulation facilities are very useful, and can provide meaningful data so long as they can be analytically transformed to the flight frame of reference. In this report we show that different methodologies used by NASA and industry to perform this transformation produce very different results, especially in the rear quadrant; this compels us to rely largely on static data to develop our model, but we show reasonable agreement with simulated flight data when these transformation issues are considered. A persistent problem in obtaining good quality data is noise generated in the experimental facility upstream of the test nozzle: valves, elbows, obstructions, and especially the combustor can contribute significant noise, and much of this noise is of a broadband nature, easily confused with jet noise. Muffling of these sources is costly in terms of size as well as expense, and it is particularly difficult in flight simulation facilities, where compactness of hardware is very important, as discussed by Viswanathan (Ref. 13). We feel that the effects of jet density on jet mixing noise may have been somewhat obscured by these problems, leading to the variable density exponent used in most jet noise prediction procedures including our own. We investigate this issue, applying <b>Occam</b> <b>s</b> <b>razor,</b> (e. g., Ref. 14), in a search for the simplest physically meaningful model that adequately describes the observed phenomena. In a similar vein, we see no reason to reject the Lighthill approach; it provides a very solid basis upon which to build a predictive procedure, as we believe we demonstrate in this report. Another feature of our approach is that the analyses are all conducted with lossless spectra, rather than Standard Day spectra, as is often done in industry. We feel {{that it is important to}} isolate the effects of as many physical processes as practical. Atmospheric attenuation can then be included using the relations developed for NASA by Shields and Bass (Ref. 15), which are available in both FOOTPR and ANOPP. The current approach to coannular jet noise prediction used in FOOTPR is reported in Reference 16, which updates the earlier conventional-velocity-profile (CVP, Ref. 17) and inverted-velocity-profile (IVP, Ref. 18) models...|$|E
40|$|In {{the first}} chapter of this paper I focus on the general {{overview}} of Occam 2 ̆ 7 <b>s</b> <b>Razor,</b> and develop several interpretations and adaptations of Occam 2 ̆ 7 <b>s</b> <b>Razor</b> as a principle of simplicity. In the second chapter I apply these different interpretations in the Physicalism/Dualism debate, and critically assess the validity of these implementations of Occam 2 ̆ 7 <b>s</b> <b>Razor</b> in philosophy of mind. In the final chapter I give an overview of my discussion thus far, and make assertions about what my paper means for the usage of Occam 2 ̆ 7 <b>s</b> <b>Razor</b> 2 ̆ 7 s as a whole...|$|R
40|$|Occam 2 ̆ 7 <b>s</b> <b>razor</b> {{states that}} out of {{possible}} explanations, plans, and designs, we should select the simplest one. It turns out that in many practical situations, the simplest explanation indeed {{turns out to be}} the correct one, the simplest plan is often the most successful, etc. But why this happens is not very clear. In this paper, we provide a simple geometric explanation of Occam 2 ̆ 7 <b>s</b> <b>razor...</b>|$|R
40|$|Sociologists {{of science}} {{noticed that the}} results of many {{collaborative}} projects and discoveries are often attributed only to their most famous collaborators, even when the contributions of these famous collaborators were minimal. This phenomenon is known as the Matthew effect, after a famous citation from the Gospel of Matthew. In this article, we show that Occam 2 ̆ 7 <b>s</b> <b>razor</b> provides a possible explanation for the Matthew effect...|$|R
40|$|Many {{classification}} {{methods have}} been proposed to find patterns in text documents. However, according to Occam 2 ̆ 7 <b>s</b> <b>razor</b> principle, "the explanation of any phenomenon should make as few assumptions as possible", short patterns usually have more explainable and meaningful for classifying text documents. In this paper, we propose a depth-first pattern generation algorithm, which can find out short patterns from text document more effectively, comparing with breadth-first algorithm <br /...|$|R
40|$|Value of {{information}} (VOI) {{methods have been}} proposed as a systematic approach to inform optimal research design and prioritization. Four related questions arise that VOI methods could address. (i) Is further research for a health technology assessment (HTA) potentially worthwhile? (ii) Is {{the cost of a}} given research design less than its expected value? (iii) What is the optimal research design for an HTA? (iv) How can research funding be best prioritized across alternative HTAs? Following Occam 2 ̆ 7 <b>s</b> <b>razor,</b> we consider the usefulness of VOI methods in informing questions 1 - 4 relative to their simplicity of use. Expected value of perfect information (EVPI) with current information, while simple to calculate, is shown to provide neither a necessary nor a sufficient condition to address question 1, given that what EVPI needs to exceed varies with the cost of research design, which can vary from very large down to negligible. Hence, for any given HTA, EVPI does not discriminate, as it can be large and further research not worthwhile or small and further research worthwhile. In contrast, each of questions 1 - 4 are shown to be fully addressed (necessary and sufficient) where VOI methods are applied to maximize expected value of sample information (EVSI) minus expected costs across designs. In comparing complexity in use of VOI methods, applying the central limit theorem (CLT) simplifies analysis to enable easy estimation of EVSI and optimal overall research design, and has been shown to outperform bootstrapping, particularly with small samples. Consequently, VOI methods applying the CLT to inform optimal overall research design satisfy Occam 2 ̆ 7 <b>s</b> <b>razor</b> in both improving decision making and reducing complexity. Furthermore, they enable consideration of relevant decision contexts, including option value and opportunity cost of delay, time, imperfect implementation and optimal design across jurisdictions. More complex VOI methods such as bootstrapping of the expected value of partial EVPI may have potential value in refining overall research design. However, Occam 2 ̆ 7 <b>s</b> <b>razor</b> must be seriously considered in application of these VOI methods, given their increased complexity and current limitations in informing decision making, with restriction to EVPI rather than EVSI and not allowing for important decision-making contexts. Initial use of CLT methods to focus these more complex partial VOI methods towards where they may be useful in refining optimal overall trial design is suggested. Integrating CLT methods with such partial VOI methods to allow estimation of partial EVSI is suggested in future research to add value to the current VOI toolkit...|$|R
40|$|With the Fraser 2 ̆ 7 <b>s</b> <b>razors</b> in {{operation}} and the impending collapse of numerous College 2 ̆ 7 s of Advanced Education as independent autonomous {{institutions and the}} unitary vision of the Correy Committee on a teacher training style for New South Wales, certain cautionary warnings should be uttered concerning the direction that teacher training could take if such stupidity is allowed to go unchallenged. The necessity for such warnings is fairly obvious: (1) there may exist in the shuffle for compliance a built in bias towards rigidity in teacher training; (2) political expediency or economy may become confused as good common sense; (3) educational principles might become further subverted {{in the guise of}} administrative efficiency. This paper will examine some of the elements of the teacher training arena, e. g., models of teacher training, decision making and dilemma resolution in the hope that debate will heighten awareness...|$|R
40|$|The {{framework}} of causal Bayes nets, currently influential in several scientific disciplines, provides a rich formalism {{to study the}} connection between causality and probability from an epistemological perspective. This article compares three assumptions in the literature that seem to constrain the connection between causality and probability {{in the style of}} Occam 2 ̆ 7 <b>s</b> <b>razor.</b> The trio includes two minimality assumptions—one formulated by Spirtes, Glymour, and Scheines (SGS) and the other due to Pearl—and the more well-known faithfulness or stability assumption. In terms of logical strength, it is fairly obvious that the three form a sequence of increasingly stronger assumptions. The focus of this article, however, is to investigate the nature of their relative strength. The comparative analysis reveals an important sense in which Pearl 2 ̆ 7 s minimality assumption is as strong as the faithfulness assumption and identifies a useful condition under which it is as safe as SGS 2 ̆ 7 s relatively secure minimality assumption. Both findings have notable implications for the theory and practice of causal inference...|$|R
40|$|Comments {{on certain}} generic points of {{principle}} {{raised by the}} recent decision in Pharmacy Care Systems Ltd v Attorney-General, 16 Aug 2004, CA 198 / 03, in which {{he notes that the}} Court of Appeal enumerated no fewer than seven 2 ̆ 7 elements of duress in New Zealand law today 2 ̆ 7. Points out that hitherto, most authoritative formulations of the duress complaint in Commonwealth contract law have propounded only two, occasionally three, doctrinal criteria comprising the excuse or plea in avoidance. Argues that the Court of Appeal has, by multiplying elements beyond necessity, turned exegesis into excess and offended the basic law of parsimony (or the principle of 2 ̆ 7 Ockham 2 ̆ 7 <b>s</b> <b>razor</b> 2 ̆ 7), and claims that such excess should be 2 ̆ 7 shaved off 2 ̆ 7. Acknowledges that the Court, by recognising the necessity for adjudicators in duress claims to inquire independently into the 2 ̆ 7 gravity 2 ̆ 7 of the threat, paves the way for the evolution of a sophisticated, and normatively defensible, theory and doctrine of contractual duress in NZ...|$|R
40|$|It {{has long}} been {{recognized}} that humans (and possibly other animals) usually break problems down into smaller and more manageable problems using subgoals. Despite a general consensus that subgoaling helps problem solving, it is still unclear what the mechanisms guiding online subgoal selection are during the solution of novel problems for which predefined solutions are not available. Under which conditions does subgoaling lead to optimal behaviour? When is subgoaling better than solving a problem from start to finish? Which is the best number and sequence of subgoals to solve a given problem? How are these subgoals selected during online inference? Here, we present a computational account of subgoaling in problem solving. Following Occam 2 ̆ 7 <b>s</b> <b>razor,</b> we propose that good subgoals are those that permit planning solutions and controlling behaviour using less information resources, thus yielding parsimony in inference and control. We implement this principle using approximate probabilistic inference: subgoals are selected using a sampling method that considers the descriptive complexity of the resulting sub-problems. We validate the proposed method using a standard reinforcement learning benchmark (four-rooms scenario) and show that the proposed method requires less inferential steps and permits selecting more compact control programs compared to an equivalent procedure without subgoaling. Furthermore, we show that the proposed method offers a mechanistic explanation of the neuronal dynamics found in the prefrontal cortex of monkeys that solve planning problems. Our computational framework provides a novel integrative perspective on subgoaling and its adaptive advantages for planning, control and learning, such as for example lowering cognitive effort and working memory load...|$|R
40|$|The Rudd {{government}} 2 ̆ 7 <b>s</b> so-called <b>Razor</b> Gang {{has taken}} the blade {{to some of the}} Howard government 2 ̆ 7 s pre-election promises for the 2007 - 08 fiscal year, including AU 30 million sliced from the federal tech budget. U 6. 4 million will be slashed from the NetAlert program 2 ̆ 7 s education (AU 700, 000) and advertising (AU 5. 7 million) budget. The initial cost of NetAlert, announced by the Coalition government in August last year, was AU 187 million. While AU 84 million was spent on filtering technology, AU 22 million was allocated to an awareness scheme to inform parents and carers of children about online safety issues...|$|R
40|$|Transputer {{compilation}} In {{this section}} we define the compilation to Transputer instructions which still uses abstract auxiliary OCCAM daemon functions. We proceed stepwise, defining for each <b>Occam</b> statement <b>S</b> {{the value of}} compile together with the TRANSPUTER ground rules for {{the execution of the}} code. Each time we show that this implements correctly the semantics of S as compiled to and executed in OCCAM daemon. Declarations The compilation of variable declarations remains the same as in OCCAM daemon. For the channel declarations (see subsection 4. 3.) we have to compile the pseudo instruction init chan for the initialization of channels to nil. This is realized by first loading nil into the register Areg (using the MINT instruction) and then storing it from there to the channel (using the local storing instruction STL) with appropriate address: compile(CHAN id 1; : : :; id r : S; e; m;x) = compile(init chan(~ id); e 0; m;x); compile(S; e 0; m+ r; x) where ~ i [...] ...|$|R
40|$|We give a {{theoretical}} analysis of published {{experimental studies of}} the effects of impurities and disorder on the superconducting transition temperature Tc of the organic molecular crystals kappa-(BEDT-TTF) 2 X (where X = Cu[N(CN) 2]Br and Cu(NCS) 2 and BEDT-TTF is bis(ethylenedithio) tetrathiafulvalene) and beta-(BEDT-TTF) 2 X (for X = I 3 and IBr 2). The Abrikosov-Gorkov (AG) formula describes the suppression of Tc both by magnetic impurities in singlet superconductors, including s-wave superconductors and by nonmagnetic impurities in a non-s-wave superconductor. We show that various sources of disorder (alloying anions, fast electron irradiation, disorder accidentally produced during fabrication, and cooling rate induced disorder) lead to the suppression of Tc as described by the AG formula. This is confirmed by the excellent fit to the data, the fact that these materials are in the clean limit and the excellent agreement between the value of the interlayer hopping integral t[perpendicular] calculated from this fit and the value of t[perpendicular] found from angular-dependent magnetoresistance and quantum oscillation experiments. There are only two scenarios consistent with the current state of experimental knowledge. If the disorder induced by all of the four methods considered in this paper is, as seems most likely, nonmagnetic then the pairing state cannot be s wave. We show that published measurements of the cooling rate dependence of the magnetization are inconsistent with paramagnetic impurities. Triplet pairing is ruled out by NMR and upper critical field experiments. Thus if the disorder is nonmagnetic then this implies that l>= 2, in which case Occam 2 ̆ 7 <b>s</b> <b>razor</b> suggests that d-wave pairing is realized in both beta-(BEDT-TTF) 2 X and kappa-(BEDT-TTF) 2 X. However, particularly given the proximity of these materials to an antiferromagnetic Mott transition, {{it is possible that the}} disorder leads to the formation of local magnetic moments via some atypical mechanism. Thus we conclude that either beta-(BEDT-TTF) 2 X and kappa-(BEDT-TTF) 2 X are d-wave superconductors or else they display an atypical mechanism for the formation of localized moments, possibly related to the competition between the antiferromagnetic and superconducting grounds states. We suggest systematic experiments to differentiate between these two scenarios...|$|R
40|$|The {{present study}} shows that prestructuring based on domain {{knowledge}} leads to statistically significant generalization-performance improvement in artificial neural networks (NNs) of the multilayer perceptron (MLP) type, specifically {{in the case of}} a noisy real-world problem with numerous interacting variables. The prestructuring of MLPs based on knowledge of the structure of a problem domain has previously been shown to improve generalization performance. However, the problem domains for those demonstrations suffered from significant shortcomings: 1) They were purely logical problems, and 2) they contained small numbers of variables in comparison to most data-mining applications today. Two implications of the former were a) the underlying structure of the problem was completely known to the network designer by virtue of having been conceived for the problem at hand, and b) noise was not a significant concern in contrast with real-world conditions. As for the size of the problem, neither computational resources nor mathematical modeling techniques were advanced enough to handle complex relationships among more than a few variables until recently, so such problems were left out of the mainstream of prestructuring investigations. In the present work, domain knowledge is built into the solution through Reconstructability Analysis, a form of information-theoretic modeling, which is used to identify mathematical models that can be transformed into a graphic representation of the problem domain 2 ̆ 7 s underlying structure. Employing the latter as a pattern allows the researcher to prestructure the MLP, for instance, by disallowing certain connections in the network. Prestructuring reduces the set of all possible maps (SAPM) that are realizable by the NN. The reduced SAPM [...] according to the Lendaris-Stanley conjecture, conditional probability, and Occam 2 ̆ 7 <b>s</b> <b>razor</b> [...] enables better generalization performance than with a fully connected MLP that has learned the same I/O mapping to the same extent. In addition to showing statistically significant improvement over the generalization performance of fully connected networks, the prestructured networks in the present study also compared favorably to both the performance of qualified human agents and the generalization rates in classification through Reconstructability Analysis alone, which serves as the alternative algorithm for comparison...|$|R
40|$|In his article, 2 ̆ 2 Atheological Apologetics, 2 ̆ 2 Scott Shalkowski {{argues that}} {{there is no reason to}} believe that the theist {{necessarily}} has the burden of proof in the debate of God 2 ̆ 7 s existence. The strength of his argument lies in his assumptions about facts, knowledge, and justification, positive and negative existence claims, and the relevance of context in a debate. First, Shalkowski argues against Anthony Flew who states in his book, The Presumption of Atheism, that general features about knowledge claims 2 ̆ 2 entail the theist (who is the affirmative side of the debate) to first, introduce and defend his proposed concept of God; and, second, to provide sufficient reason for believing that this concept of his does in fact have application. 2 ̆ 2 Flew uses his claims about knowledge and justification to support what he calls the 2 ̆ 2 presumption of atheism. 2 ̆ 2 Secondly, Shalkowski is concerned with the distinction drawn between positive and negative existence claims. He argues {{that there is}} nothing intrinsic to positive claims that saddles them with the demand for grounds that exempts negative claims from the same demand. He rests this argument on the concept of context relativity and tries to show that in certain contexts the negative existence claims have a burden for grounds that positive ones do not, and therefore concludes {{that there is no}}thing intrinsic to positive claims that suggests that they should bear the burden of argument. I will argue first, that a presumption of atheism is justified but it is not the same presumption of negative atheism that Flew argues for. I do not take Flew 2 ̆ 7 s presumption of atheism because Flew does not believe that context plays a role in the application of this presumption and I feel that this is a necessary ingredient to the presumption. Second, Shalkowski 2 ̆ 7 s parallel between positive and negative existence claims is ill-founded, and his main point about context relativity ignores the relevance of what today 2 ̆ 7 s context is and consequently damages his own position. Further, I will argue that in a scientific era, with the help of rational tools like Ockham 2 ̆ 7 <b>s</b> <b>Razor,</b> the presumption for theism is irrational and a presumption of atheism in the traditional sense is justified...|$|R
40|$|Textos recuperados da Internet por interm´edio de consultas ao Google e Yahoo s ao analisados segundo uma m´etrica simples de avaliac¸ ao de inteligibilidade textual. Tais m´etricas foram criadas para orientar a produc¸ ao textual e recentemente tamb´em foram empregadas em simplificadores textuais autom´aticos experimentais para leitores inexperientes. Nesse trabalho aplicam-se essas m´etricas a texto originais livres, recuperados da Internet, para buscar correlacionar o grau de inteligibilidade textual com a relev ancia que lhes ´e conferida pelos buscadores utilizados. A premissa inicial a estimular a comparac¸ ao entre inteligibilidade e relev ancia ´e o enunciado conhecido como Princ´&# 305;pio de Occam, ou princ´&# 305;pio da economia. Observa-se uma tend encia centralista que ocorre a partir do pequeno afastamento m´edio dos grupos de arquivos melhor colocados no ranking em relac¸ ao `a m´edia da categoria a que pertencem. ´E com a medida do afastamento m´edio que se consegue verificar correlac¸ ao com a posic¸ ao do arquivo no ranking e ´e tamb´em com essa medida que se consegue {{registrar}} diferenc¸as entre o m´etodo de calcular a relev ancia do Google e do Yahoo. Um experimento que decorre do primeiro estudo procura determinar se a medida de inteligibilidade pode ser empregada para auxiliar o usu´ario da Internet a escolher arquivos mais simples ou se a sua indicac¸ ao junto `a listagem de links recuperados ´e ´util e informativa para a escolha e navegac¸ ao do usu´ario. Em um experimento final, embasado no conhecimento previamente obtido, s ao comparadas as enciclop´edias Brit anica eWikip´edia por meio do emprego da m´etrica de inteligibilidade Flesch-Kincaid Grade LevelText {{retrieved from}} the Internet through Google and Yahoo queries are evaluated using Flesch-Kincaid Grade Level, a simple assessment measure of text readability. This kind of metrics {{were created to}} help writers to evaluate their text, and recently in automatic text simplification for undercapable readers. In this work we apply these metrics to documents freely retrieved from the Internet, seeking to find correlations between legibility and relevance acknowledged to then by search engines. The initial premise guiding the comparison between readability and relevance is the statement known as <b>Occam</b> <b>s</b> Principle, or Principle of Economy. This study employs Flesch-Kincaid Grade Level in text documents retrieved from the Internet through search-engines queries and correlate it with the position. It was found a centralist trend in the texts recovered. The centralist tendency mean that the average spacing of groups of files from {{the average of the}} category they belong is meaningfull. With this measure is possible to establish a correlation between relevance and legibility, and also, to detect diferences in the way both search engines derive their relevance calculation. A subsequent experiment seeks to determine whether the measure of legibility can be employed to assist him or her choosing a document combined with original search engine ranking and if it is useful as advance information for choice and user navigation. In a final experiment, based on previously obtained knowledge, a comparison between Wikipedia and Britannica encyclopedias by employing the metric of understandability Flesch-Kincai...|$|R
40|$|This {{observational}} {{field study}} attempted {{to quantify the}} objective task load imposed on emergency department (ED) providers, determine the degree of subjective workload they experience, and to correlate these data with ED operational metrics, mainly ED crowding metrics. Participants were {{a convenience sample of}} 10 emergency care providers; the 3 female and 7 male participants represented a variety of provider levels (6 physicians, 3 physician assistants, and 1 nurse practitioner). Forty-two hours of data were collected. ED variables were obtained from the hospital 2 ̆ 7 s existing information system each hour and included the Emergency Severity Index (ESI), {{number of people in the}} waiting room, patient/doctor ratio, patient/nurse ratio, number of patients assigned, number of providers on duty and crowding variables; Emergency Department Work Index (EDWIN) and occupancy level. Providers were shadowed and observed each hour by a researcher who recorded the type of tasks they performed, the number of tasks they performed, the time they spent on each task and the number of times they were interrupted. Subjective workload ratings (NASA-TLX) were obtained from providers at the end of each hour of observation. Correlations were performed to evaluate the relation of observed, subjective and hospital variables. Overall objective task load was quantified using time-on-task data and task difficulty weightings to achieve a single standardized value for overall objective workload (OTLX). OTLX scores were regressed against ED crowding measures of occupancy and EDWIN score. Structured interviews were conducted with each participant following the observation sessions. Results from the study revealed that providers spent 75 percent of their time performing tasks related to communication with staff, direct patient care, and paperwork. The other 25 percent of their time was spent checking test results, admitting patients to the hospital, taking breaks, looking for supplies, checking the electronic whiteboard, and other job-related tasks. ED occupancy was positively correlated to subjective workload and predicted 30 percent of the variance in subjective workload. The EDWIN score, on the other hand, only predicted 9 percent of the variance in subjective workload. This study revealed no correlation between ED crowding and objective task load and ED crowding predicted less than 4 percent of the variance in OTLX scores. In accordance with 2 ̆ 2 Occam 2 ̆ 7 <b>s</b> <b>razor</b> 2 ̆ 2, ED occupancy may provide an advantage over more complex compound measures of ED crowding such as the EDWIN score in predicting provider subjective workload and may be more useful in making ED staffing and scheduling decisions. In addition to collected and recorded variables, valuable insights were obtained from ED providers regarding issues of ED crowding, time-pressure and workload. It is apparent from their responses that, in the absence of observable changes in task load, the quantity and status of the 2 ̆ 2 unseen 2 ̆ 2 patient weighs heavily on their minds. Future research should assess the number of patients waiting or the number of patients who have left without being seen (LWBS) not only as a metric of ED crowding but as a predictor of ED provider workload...|$|R
40|$|Since {{its formal}} {{independence}} from the former Yugoslavia in 1991, the Republic of Macedo-nia was a subject of different conflict prevention strategies utilized by numerous interna-tional institutions or particular western democracies. Trapped in the constant balancing between regional and internal challenges for peace and stability and periodically supported in its avoidance of violent scenarios, Macedonia was labelled the most successful case of conflict prevention or as a successful case of crisis prevention, later (since 2001), by the numerous international representatives. Different arguments were used {{in order to support}} such assessments. The main argument is that the violence that was experienced in other ex-Yugoslav conflicts did not emerge. Such an argument could be accepted as the essential only if the main objective of specific international institutions preventive strategies were defined as violence avoidance or management thereby supporting only restrictive preventive ap-proach. However, to keep the country on the <b>razor</b> <b>s</b> edge while balancing the stability by means of the instruments for operational conflict prevention could be hazardous approach. Instead, long-term strategy for structural prevention towards the region in general and towards the Republic of Macedonia in particular is required. Key words: Macedonia, accession to the EU, conflict prevention 1...|$|R

