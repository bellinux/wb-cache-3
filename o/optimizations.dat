10000|10000|Public
5|$|Aspect weavers' {{performance}}, {{as well as}} {{the performance}} of the code that they produce, has been a subject of analysis. It is preferable that the improvement in modularity supplied by aspect weaving does not impact run-time performance. Aspect weavers are able to perform aspect-specific <b>optimizations.</b> While traditional <b>optimizations</b> such as the elimination of unused special variables from aspect code can be done at compile-time, some <b>optimizations</b> can only be performed by the aspect weaver. For example, AspectJ contains two similar but distinct keywords, thisJoinPoint, which contains information about this particular instance of woven code, and thisJoinPointStaticPart, which contains information common to all instances of code relevant to that set of advice. The optimization of replacing thisJoinPoint with the more efficient and static keyword thisJoinPointStaticPart can only be done by the aspect weaver. By performing this replacement, the woven program avoids the creation of a join point object on every execution. Studies have shown that the unnecessary creation of join point objects in AspectJ can lead to a performance overhead of 5% at run-time, while performance degradation is only approximately 1% when this object is not created.|$|E
5|$|At compile time, the {{interpreter}} parses Perl code into a syntax tree. At run time, it executes {{the program by}} walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time <b>optimizations</b> on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.|$|E
5|$|Since June 2017, games may be {{promoted}} with additional icons that denote compatibility with hardware enhancements found in newer Xbox One models, including support for high-dynamic-range (HDR) colors (on Xbox One S and Xbox One X), native rendering at 4K resolution (Xbox One X), and specific <b>optimizations</b> for Xbox One X.|$|E
5000|$|Swarm-based <b>optimization</b> {{algorithms}} (e.g., particle swarm <b>optimization,</b> Multi-swarm <b>optimization</b> and ant colony <b>optimization)</b> ...|$|R
5000|$|Derivative free <b>optimization</b> (or derivative-free <b>optimization)</b> is {{a subject}} of {{mathematical}} <b>optimization.</b> It may refer to problems for which derivative information is unavailable, unreliable or impractical to obtain (derivative-free <b>optimization</b> problems), or methods that do not use derivatives (derivative-free <b>optimization</b> methods).Derivative-free <b>optimization</b> {{is closely related to}} blackbox <b>optimization</b> [...]|$|R
30|$|Because of its simple algorithm, {{particle}} swarm <b>optimization</b> {{is easy to}} implement, and no gradient {{information is}} needed. The characteristics of small parameters are good for both continuous <b>optimization</b> problems and discrete <b>optimization</b> problems, especially because of their natural real-coded features are suitable for processing <b>optimization</b> [14]. In recent years, it has been became a hot topic {{in the field of}} intelligent <b>optimization</b> around the world. Particle swarm <b>optimization</b> (PSO) algorithm was first applied to the <b>optimization</b> of nonlinear continuous functions and neural network training and was newly used to solve constrained <b>optimization</b> problems, multi-objective <b>optimization</b> problems, and dynamic <b>optimization</b> problems [8].|$|R
5|$|From March 2007 until November 2012, Folding@home took {{advantage}} of the computing power of PlayStation 3s. At the time of its inception, its main streaming Cell processor delivered a 20x speed increase over PCs for some calculations, processing power which could not be found on other systems such as the Xbox 360. The PS3's high speed and efficiency introduced other opportunities for worthwhile <b>optimizations</b> according to Amdahl's law, and significantly changed the tradeoff between computing efficiency and overall accuracy, allowing the use of more complex molecular models at little added computing cost. This allowed Folding@home to run biomedical calculations that would have been otherwise infeasible computationally.|$|E
5|$|Development for {{multiple}} platforms is profitable, but difficult. <b>Optimizations</b> needed for one platform architecture {{do not necessarily}} translate to others. Individual platforms such as the Sega Genesis and PlayStation 3 are seen as difficult to develop for compared to their competitors, and developers are not yet fully accustomed to new technologies such as multi-core processors and hyper-threading. Multi-platform releases are increasingly common, but not all differences between editions on multiple platforms can be fully explained by hardware alone, and there remain franchise stalwarts that exist solely on one system. Developers for new platforms such as handheld and mobile systems {{do not have to}} operate under the pressure of $20 million budgets and the scrutiny of publishers' marketing experts.|$|E
5|$|The {{development}} {{team for}} Unreal Tournament consisted of around 16 people. Most team members {{had worked on}} Unreal, though Epic hired {{a number of new}} developers to reinforce the team. Programmer Brandon Reinhart was one such hire, joining Epic in August 1998 to help with the support of Unreal and the development of Unreal Tournament. In December 1998, Reinhart discovered an Unreal mod called UBrowser, which provided a new user interface for finding multiplayer matches. After showing it to James Schmalz, the lead designer at Digital Extremes, Schmalz decided to hire the mod's author, Jack Porter. After only a few weeks Porter was already working with the team, replacing the game's existing menu system with his new interface. Epic founder Tim Sweeney worked on improving the networking code along with Steve Polge, who also wrote the original AI code and focused on player physics and general gameplay. Erik de Neve was responsible for the LOD character rendering, and various extra <b>optimizations.</b>|$|E
30|$|In {{order to}} verify the {{advantages}} of the fuzzy stochastic <b>optimization</b> model relative to an existing <b>optimization</b> model, simulation results from fuzzy stochastic <b>optimization</b> are compared with deterministic <b>optimization</b> and stochastic <b>optimization,</b> the latter two being the most commonly used models at present. The deterministic and stochastic <b>optimization</b> models can be derived through appropriate simplification of the fuzzy stochastic <b>optimization</b> model.|$|R
5000|$|The <b>optimization</b> procedure. Either {{continuous}} or discrete <b>optimization</b> is performed. For continuous <b>optimization,</b> gradient-based <b>optimization</b> {{techniques are}} applied to improve the convergence speed.|$|R
30|$|There {{are several}} {{approaches}} to model uncertainty in <b>optimization</b> {{problems such as}} stochastic <b>optimization</b> and fuzzy <b>optimization.</b> Here we consider an <b>optimization</b> problem with interval-valued objective function. Stancu-Minasian and Tigan [6, 7] investigated this kind of <b>optimization</b> problem.|$|R
5|$|The {{specialized}} hardware of graphics processing units (GPU) {{is designed}} to accelerate rendering of 3-Dgraphics applications such as video games and can significantly outperform CPUs for some types of calculations. GPUs {{are one of the}} most powerful and rapidly growing computing platforms, and many scientists and researchers are pursuing general-purpose computing on graphics processing units (GPGPU). However, GPU hardware is difficult to use for non-graphics tasks and usually requires significant algorithm restructuring and an advanced understanding of the underlying architecture. Such customization is challenging, more so to researchers with limited software development resources. Folding@home uses the open source OpenMM library, which uses a bridge design pattern with two application programming interface (API) levels to interface molecular simulation software to an underlying hardware architecture. With the addition of hardware <b>optimizations,</b> OpenMM-based GPU simulations need no significant modification but achieve performance nearly equal to hand-tuned GPU code, and greatly outperform CPU implementations.|$|E
25|$|Today, {{automated}} <b>optimizations</b> {{are almost}} exclusively limited to compiler optimization. However, because compiler <b>optimizations</b> are usually {{limited to a}} fixed set of rather general <b>optimizations,</b> there is considerable demand for optimizers which can accept descriptions of problem and language-specific <b>optimizations,</b> allowing an engineer to specify custom <b>optimizations.</b> Tools that accept descriptions of <b>optimizations</b> are called program transformation systems and are beginning {{to be applied to}} real software systems such as C++.|$|E
25|$|The exact set of GCC <b>optimizations</b> {{varies from}} release to release as it develops, but {{includes}} the standard algorithms, such as loop optimization, jump threading, common subexpression elimination, instruction scheduling, and so forth. The RTL <b>optimizations</b> are of less importance {{with the addition}} of global SSA-based <b>optimizations</b> on GIMPLE trees, as RTL <b>optimizations</b> have a much more limited scope, and have less high-level information.|$|E
40|$|Abstract. Ordinal <b>optimization</b> {{is a tool}} {{to reduce}} the {{computational}} burden in simulation-based <b>optimization</b> problems. So far, the major effort in this field focuses on single-objective <b>optimization.</b> In this paper, we extend this to multiobjective <b>optimization</b> and develop vector ordinal <b>optimization,</b> which {{is different from the}} one introduced in Ref. 1. Alignment probability and ordered performance curve (OPC) are redefined for multiobjective <b>optimization.</b> Our results lead to quantifiable subset selection sizes in the multiobjective case, which supplies guidance in solving practical problems, as demonstrated by the examples in this paper. Key Words. Multiobjective <b>optimization,</b> stochastic <b>optimization,</b> vector ordinal <b>optimization.</b> 1...|$|R
5000|$|The Continuous <b>Optimization</b> Working Group of EURO (also, EUROPT) is {{a working}} group whose {{objective}} is to promote original research in the field of continuous <b>optimization</b> at the European level. EUROPT is the unique European working group on continuous <b>optimization.</b> It {{is one of the largest}} working groups of EURO. It includes most of European researchers working on continuous <b>optimization.</b> Continuous <b>optimization</b> includes linear programming, convex <b>optimization,</b> global <b>optimization</b> ...|$|R
40|$|The {{process of}} {{semantic}} service discovery using an ontology reasoner such as Pellet is time consuming. This restricts {{the usage of}} web services in real time applications having dynamic composition requirements. As performance of semantic service discovery is crucial in service composition, it should be optimized. Various <b>optimization</b> methods are being proposed to improve the performance of semantic discovery. In this work, we investigate the existing <b>optimization</b> methods and broadly classify <b>optimization</b> mechanisms into two categories, namely <b>optimization</b> by efficient reasoning and <b>optimization</b> by efficient matching. <b>Optimization</b> by efficient matching is further classified into subcategories such as <b>optimization</b> by clustering, <b>optimization</b> by inverted indexing, <b>optimization</b> by caching, <b>optimization</b> by hybrid methods, <b>optimization</b> by efficient data structures and <b>optimization</b> by efficient matching algorithms. With a detailed study of different methods, an integrated <b>optimization</b> infrastructure along with matching method has been proposed to improve the performance of semantic matching component. To achieve better <b>optimization</b> the proposed method integrates the effects of caching, clustering and indexing. Theoretical aspects of performance evaluation of the proposed method are discussed...|$|R
25|$|Some {{of these}} <b>optimizations</b> {{performed}} {{at this level}} include dead code elimination, partial redundancy elimination, global value numbering, sparse conditional constant propagation, and scalar replacement of aggregates. Array dependence based <b>optimizations</b> such as automatic vectorization and automatic parallelization are also performed. Profile-guided optimization is also possible.|$|E
25|$|Design and {{implementation}} <b>optimizations</b> {{to decrease the}} size of the shellcode.|$|E
25|$|Another {{application}} of these transformations is in compiler <b>optimizations</b> of nested-loop code, and in parallelizing compiler techniques.|$|E
3000|$|In {{the above}} formulas: x∈R^n_x, [...] y∈R^n_yare the {{decision}} variables {{of the outer}} <b>optimization</b> problem and the inner layer <b>optimization</b> problem, respectively. F,f:R^n_x+n_y→ R are the objective functions of the outer <b>optimization</b> problem and the inner layer <b>optimization</b> problem. [...] g:R^n_x+n_y→R^n_lare rrespectively the Restrictions of the outer <b>optimization</b> problem and the inner layer <b>optimization</b> problem.|$|R
30|$|However, {{since the}} <b>optimization</b> {{of the problem}} {{contains}} some constraints, the penalty function method (sequence unconstrained <b>optimization</b> method (SUMT)) {{is used in the}} research to solve the constraint-constrained <b>optimization</b> problem by changing the constraint <b>optimization</b> problem to the unconstrained <b>optimization</b> problem.|$|R
40|$|This paper {{proposes a}} fuzzy proportional-derivative (PD) {{controller}} <b>optimization</b> engine for engineering <b>optimization</b> problems using an optimality criteria approach. Traditional numerical <b>optimization</b> algorithms treat <b>optimization</b> problems as pure mathematical problems. Engineering {{knowledge about the}} problem is not utilized in the <b>optimization</b> process. The idea of using the fuzzy PD controller in engineering <b>optimization</b> is that, instead of using purely numerical information to obtain the new design point in the next iteration, engineering knowledge and human supervision process can be modeled in the <b>optimization</b> algorithm using fuzzy rules. The fuzzy PD controller <b>optimization</b> engine developed in this work appears to have stable performance in both structural <b>optimization</b> and blow molding parameter <b>optimization</b> examples...|$|R
25|$|GHC is {{also often}} a testbed for {{advanced}} functional programming features and <b>optimizations</b> in other programming languages.|$|E
25|$|JVM {{improvements}} include: synchronization and compiler performance <b>optimizations,</b> new algorithms and upgrades {{to existing}} garbage collection algorithms, and application start-up performance.|$|E
25|$|Many of the {{features}} introduced with Swift also have well-known performance and safety trade-offs. Apple has implemented <b>optimizations</b> that reduce this overhead.|$|E
30|$|The joint <b>optimization</b> {{algorithm}} is proposed to obtain the solutions to the <b>optimization</b> problems, {{which is based on}} the alternating direction <b>optimization</b> and the Dinkelbach <b>optimization.</b>|$|R
40|$|This book {{presents}} efficient metaheuristic algorithms {{for optimal}} design of structures. Many of these algorithms are {{developed by the}} author and his colleagues, consisting of Democratic Particle Swarm <b>Optimization,</b> Charged System Search, Magnetic Charged System Search, Field of Forces <b>Optimization,</b> Dolphin Echolocation <b>Optimization,</b> Colliding Bodies <b>Optimization,</b> Ray <b>Optimization.</b> These are presented together with algorithms which were developed by other authors and have been successfully applied to various <b>optimization</b> problems. These consist of Particle Swarm <b>Optimization,</b> Big Bang-Big Crunch Algorithm, Cuckoo Search <b>Optimization,</b> Imperialist Competitive Algorithm, and Chaos Embedded Metaheuristic Algorithms. Finally a multi-objective <b>optimization</b> method is presented to solve large-scale structural problems based on the Charged System Search algorithm. The concepts and algorithms presented in this book are not only applicable to <b>optimization</b> of skeletal structures and finite element models, but can equally be utilized for optimal design of other systems such as hydraulic and electrical networks. In the second edition seven new chapters are added consisting of the {{new developments in the}} field of <b>optimization.</b> These chapters consist of the Enhanced Colliding Bodies <b>Optimization,</b> Global Sensitivity Analysis, Tug of War <b>Optimization,</b> Water Evaporation <b>Optimization,</b> Vibrating Particle System <b>Optimization</b> and Cyclical Parthenogenesis <b>Optimization</b> algorithms. A chapter is also devoted to optimal design of large scale structures...|$|R
40|$|Parallel {{continuous}} <b>optimization</b> {{methods are}} motivated here by applications {{in science and}} engineering. The key issues are addressed at different computational levels including local and global <b>optimization</b> as well as strategies for large, sparse versus small but expensive problems. Topics covered include global <b>optimization,</b> direct search with and without surrogates, <b>optimization</b> of linked subsystems, and variable and constraint distribution. Finally, there is a discussion of future research directions. Key Words. Parallel <b>optimization,</b> local and global <b>optimization,</b> large-scale <b>optimization,</b> direct search methods, surrogate <b>optimization,</b> <b>optimization</b> of linked subsystems, design <b>optimization,</b> cluster simulation, macromolecular modeling 1 Introduction <b>Optimization</b> has broad applications in engineering, science, and management. Many of these applications either have large numbers of variables or require expensive function evaluations. In some cases, there may be many local minimiz [...] ...|$|R
25|$|In 1979, Frank DeRemer and Tom Pennello {{announced}} {{a series of}} <b>optimizations</b> for the LALR parser that would further improve its memory efficiency. Their work was published in 1982.|$|E
25|$|MOLAP {{generally}} delivers {{better performance}} due to specialized indexing and storage <b>optimizations.</b> MOLAP also needs less storage space compared to ROLAP because the specialized storage typically includes compression techniques.|$|E
25|$|Manual {{optimization}} sometimes has {{the side}} effect of undermining readability. Thus code <b>optimizations</b> should be carefully documented (preferably using in-line comments), and their effect on future development evaluated.|$|E
40|$|Two new {{powerful}} mathematical languages, {{fuzzy set}} theory and possibility theory, {{have led to}} two <b>optimization</b> types that explicitly incorporate data whose values are not real-valued nor probabilistic: 1) flexible <b>optimization</b> and 2) <b>optimization</b> under generalized uncertainty. Our aim is to make clear what these two types are, make distinctions, and show {{how they can be}} applied. Flexible <b>optimization</b> arises when it is necessary to relax the meaning of the mathematical relation of belonging to a set (a constraint set in the context of <b>optimization).</b> The mathematical language of relaxed set belonging is fuzzy set theory. <b>Optimization</b> under generalized uncertainty arises when it is necessary to represent parameters of a model whose values are only known partially or incompletely. A natural mathematical language for the representation of partial or incomplete information about the value of a parameter is possibility theory. Flexible <b>optimization,</b> as delineated here, includes much of what has been called fuzzy <b>optimization</b> whereas <b>optimization</b> under generalized uncertainty includes what has been called possibilistic <b>optimization.</b> We explore why flexible <b>optimization</b> and <b>optimization</b> under generalized uncertainty are distinct and important types of <b>optimization</b> problems. Possibility theory in the context of <b>optimization</b> leads to two distinct types of <b>optimization</b> under generalized uncertainty, single distribution and dual distribution <b>optimization.</b> Dual (possibility/necessity pairs) distribution <b>optimization</b> is new. Mathematical subject classification: 90 C 70, 65 G 40...|$|R
50|$|Conic <b>optimization</b> is a {{subfield}} of convex <b>optimization</b> {{that studies}} {{a class of}} structured convex <b>optimization</b> problems called conic <b>optimization</b> problems. A conic <b>optimization</b> problem consists of minimizing a convex function over the intersection of an affine subspace and a convex cone.|$|R
40|$|In this article, we {{demonstrate}} how to approximate geometric <b>optimization</b> with l p - norm <b>optimization.</b> These {{two categories of}} problems are well known in structured convex <b>optimization.</b> We describe a family of l p -norm <b>optimization</b> problems {{that can be made}} arbitrarily close to a geometric <b>optimization</b> problem, and show that the dual problems for these approximations are also approximating the dual geometric <b>optimization</b> problem. Finally, we use these approximations and the duality theory for l p -norm <b>optimization</b> to derive simple proofs of the weak and strong duality theorems for geometric <b>optimization...</b>|$|R
