0|1567|Public
50|$|Carmel High School {{offers an}} {{extensive}} Advanced Placement curriculum including: AP Biology, AP Calculus AB, AP Calculus BC, AP Chemistry, AP Chinese, AP Computer Science, AP Computer Science Principles, AP Environmental Science, AP French, AP Government & Politics, AP Human Geography, AP Language & Composition, AP Literature & Composition, AP Spanish, AP Statistics, AP Studio Art 2D, AP United States History, and AP World History. Due to an open-enrollment program, Carmel High School has strong {{participation in the}} AP Program with over 396 students (46%) completing at least one AP Exam. Over 72% of students earn a <b>passing</b> <b>score</b> (3 or higher) with a 66% <b>overall</b> <b>pass</b> rate.|$|R
5000|$|To achieve CCNA Routing and Switching certification, {{one must}} earn a <b>passing</b> <b>score</b> on Cisco exam #200-125, or {{combined}} <b>passing</b> <b>scores</b> {{on both the}} [...] "Interconnecting Cisco Network Devices" [...] ICND1 #100-105 and ICND2 #200-105 exams. Passing the ICND1 grants one the Cisco Certified Entry Networking Technician (CCENT) certification. <b>Passing</b> <b>scores</b> are set by using statistical analysis and are subject to change. At {{the completion of the}} exam, candidates receive a score report along with a score breakout by exam section and the <b>passing</b> <b>score</b> for the given exam. Cisco does not publish exam <b>passing</b> <b>scores</b> because exam questions and <b>passing</b> <b>scores</b> are subject to change without notice.|$|R
5000|$|Sarah Carr of The Atlantic said [...] "Compared to Indianola Academy, Gentry High School {{is an open}} book, its {{academic}} struggles {{exposed to}} the world" [...] and that [...] "Gentry has struggled with test scores since the state's accountability system began in the 1990s." [...] In 2011, 56% had <b>passing</b> <b>scores</b> in Algebra, 17% had <b>passing</b> <b>scores</b> in Biology, 51% of Gentry students had <b>passing</b> <b>scores</b> in English, and 42% had <b>passing</b> <b>scores</b> in History.|$|R
40|$|Two {{versions}} of the Nedelsky procedure for setting minimum <b>passing</b> <b>scores</b> are compared. Two groups of judges, one using each version, set <b>passing</b> <b>scores</b> for a classroom test Comparisons of the resulting sets of <b>passing</b> <b>scores</b> are {{made on the basis}} of (1) the raw distributions of <b>passing</b> <b>scores,</b> (2) the consistency of pass-fail decisions between the two versions, (3) the consistency of pass-fail decisions between each version and the <b>passing</b> <b>score</b> established by the test designer, and (4) the mean pairwise agreement between judges across groups. The two {{versions of}} the procedure are found to produce essentially equivalent results. In addition, a significant relationship is observed between the <b>passing</b> <b>score</b> set by a judge and that judge’s level of achievement in the content area of the test...|$|R
50|$|The Structural exam is 16 hours {{long and}} {{administered}} over two days, with two 4-hour sessions and a lunch break per day. Morning breadth sessions consist of 40 multiple-choice questions, while the afternoon depth sessions require essay responses. An examinee must earn a <b>passing</b> <b>score</b> on both days' exams {{in order to}} <b>pass</b> <b>overall,</b> but need not obtain those scores during the same administration of the exam.|$|R
40|$|The {{validity}} of test-based decisions about readiness for a course or a profession {{depends on the}} appropriateness of the <b>passing</b> <b>scores</b> used to make the decisions. The interpretation of <b>passing</b> <b>scores,</b> based on judgments about items or examinees, in terms of some standard of performance depends on two assumptions: (a) that the <b>passing</b> <b>score</b> corresponds to the specified performance standard, in the sense that examinees with <b>scores</b> above the <b>passing</b> <b>score</b> are likely to meet the standard and examinees with <b>scores</b> below the <b>passing</b> <b>score</b> are not likely to meet the standard, and (b) that the specified performance standard is reasonable given the purpose of the decision. These two assumptions can be evaluated in terms of the match between the procedures used to set the <b>passing</b> <b>scores</b> and the purpose of the decision, {{the internal consistency of the}} results, and comparisons to external criteria. The sources of error in the <b>passing</b> <b>score</b> can be identified by examining these two assumptions. This article provides a framework for examining the {{validity of}} performanc...|$|R
5000|$|There {{are three}} ways {{for a student}} to pass the HSAs. One way is to earn the <b>passing</b> <b>score</b> on each exam. The <b>passing</b> <b>scores</b> for the three HSAs are: ...|$|R
40|$|Two {{versions}} of the Nedelsky procedure for setting minimum <b>passing</b> <b>scores</b> are compared. Two groups of judges, one using each version, set pass-ing scores for a classroom test Comparisons of the resulting sets of <b>passing</b> <b>scores</b> are made on the ba-sis of (1) the raw distributions of <b>passing</b> <b>scores,</b> (2) the consistency of pass-fail decisions between the two versions, (3) the consistency of pass-fail deci-sions between each version and the <b>passing</b> <b>score</b> established by the test designer, and (4) the mean pairwise agreement between judges across groups. The two {{versions of}} the procedure are found to pro-duce essentially equivalent results. In addition, a significant relationship is observed between the <b>passing</b> <b>score</b> set by a judge and that judge’s level of achievement in the content area of the test. <b>Passing</b> <b>scores</b> are needed in a broad variety of situations, including (1) entrance examinations, (2) tests for advancement of students from unit to unit in individually prescribed instructional programs, (3) minimum competency testing, and (4) certification or licensing examinations. Though writers such as Glass (1978) charge that <b>passing</b> <b>scores</b> for minimum competency testing are usually selected arbitrarily and frequently used unwisely, others (Hambleton, 1978; Shep-ard, 1976) have documented the need for cutoff scores in such areas as objectives-based pro-grams and individualized instruction. Thi...|$|R
50|$|The minimal <b>passing</b> <b>score</b> for all NBCE exams is 375/800. For parts I and II a <b>passing</b> <b>score</b> of 4 out of 6 {{sectional}} {{tests to}} ensure the applicant has passed the examination as a whole. However, to be licensed full an applicant will need to pass all 6 sections before {{they are able to}} practice. Additionally certain states have high than the NBCE minimum for licensure, they are North Carolina and West Virginia which demand a <b>passing</b> <b>score</b> of 475 on Part IV examination. The state of Wisconsin requires a <b>passing</b> <b>score</b> of 438 on Part III and 475 on Part IV for licensure.|$|R
5000|$|Standard {{setting is}} a process that defines the <b>passing</b> <b>score</b> of the exam. The CFA exam uses the {{modified}} Angoff method which is a commonly used approach to setting standards for certification and licensure examinations. Subject matter experts review the exam and recommend a minimum <b>passing</b> <b>score</b> for the [...] "just-qualified candidate". The minimum <b>passing</b> <b>scores</b> are presented to the Board of Governors in a report. The Board of Governors is not bound by this recommendation, but does recognize it as a very important information.|$|R
40|$|Objective: {{the purpose}} of this study was to explore the value of the {{standard}} error of measurement (SEM) in making decisions about students with examination scores at or below the pass/fail borderline in a new undergraduate medical course with an integrated assessment programme. Methods: an analysis of de-identified, pooled data for borderline candidates was conducted to determine the SEM for each examination and the progress of candidates according to four <b>score</b> bands, from <b>pass</b> <b>score</b> ± 1 SEM, 1 − 2 SEM below the <b>pass</b> <b>score,</b> 2 − 3 SEM below the <b>pass</b> <b>score</b> and > 3 SEM below the <b>pass</b> <b>score.</b> The impact of poor performance in individual subject areas was also measured. Results: data for 1571 candidates were included in the analysis, identifying 132 students with borderline or lower scores, 45...|$|R
40|$|ABSTRACT. TWO {{approaches}} are described for the simultaneous determination of pass-ing scores for subtests when the <b>passing</b> <b>score</b> {{for the total}} test is known. The minimax approach seeks subtest <b>passing</b> <b>scores</b> {{in such a way}} that there is maximum agreement between pass-fail classifications based on each subtest and pass-fail decisions based on the entire test. The Rasch procedure seeks subtest <b>passing</b> <b>scores</b> that occupy approx-imately the same place on the ability scale with the total test <b>passing</b> <b>score.</b> In the context of a basic skills assessment program, the two approaches yield essentially the same set of <b>passing</b> <b>scores.</b> In state-wide or district-wide testing programs such as the South Carolina Basic Skills Assessment Program (BSAP) designed for instructional purposes, test results should be reported with as much detail as possible. For example, when a test is comprised of several subtests, each measuring a separate in-structional objective, an overall description of a student's achievement on the total test is used to determine if the student has adequately mastered the instructional unit covered by the test. When this is the case, the student i...|$|R
50|$|A <b>passing</b> <b>score</b> on the Certified Rehabilitation Counselor Exam.|$|R
50|$|A <b>passing</b> <b>score</b> on the {{associated}} National Counselor Exam (NCE).|$|R
5000|$|<b>Passing</b> <b>score</b> on the National Speech-Language Pathology board exam (Praxis).|$|R
5000|$|... 100% <b>overall</b> <b>pass</b> rate in BTEC {{national}} diplomas and certificates ...|$|R
50|$|School 2012 {{examination}} {{results at}} A-level showed an <b>overall</b> <b>pass</b> rate of 92% shows 30% of pupils achieving the top grades of A- B, up 11% from 2011. Year 13 A2 students at Harrogate High School have achieved an <b>overall</b> <b>pass</b> rate of 96% with 39% of pupils achieving A*- B grades.|$|R
40|$|Establishing {{valid and}} {{reliable}} <b>passing</b> <b>scores</b> is a vital activity for any examination used to make classification decisions. Although {{there are many different}} approaches to setting <b>passing</b> <b>scores,</b> this thesis is focused specifically on the Angoff standard setting method. The Angoff method is a test-centric classical test theory based approach to estimating performance standards. In the Angoff method each judge estimates the proportion of minimally competent examinees who will answer each item correctly. These values are summed across items and averages across judges to arrive at a recommended <b>passing</b> <b>score.</b> Unfortunately, research has shown that the Angoff method has a number of limitations which have the potential to undermine both the validity and reliability of the resulting standard. ^ Many of the limitations of the Angoff method can be linked to its grounding in classical test theory. The {{purpose of this study is}} to determine if the limitations of the Angoff could be mitigated by a transition to an item response theory (IRT) framework. Item response theory is a modern measurement model for relating examinees 2 ̆ 7 latent ability to their observed test performance. Theoretically the transition to an IRT-based Angoff method could result in more accurate, stable, and efficient <b>passing</b> <b>scores.</b> ^ The methodology for the study was divided into three studies designed to assess the potential advantages of using an IRT-based Angoff method. Study one examined the effect of allowing judges to skip unfamiliar items during the ratings process. The goal of this study was to detect if <b>passing</b> <b>scores</b> are artificially biased due to deficits in the content experts 2 ̆ 7 specific item level content knowledge. Study two explored the potential benefit of setting <b>passing</b> <b>scores</b> on an adaptively selected subset of test items. This study attempted to leverage IRT 2 ̆ 7 s score invariance property to more efficiently estimate <b>passing</b> <b>scores.</b> Finally study three compared IRT-based standards to traditional Angoff standards using a simulation study. The goal of this study was to determine if <b>passing</b> <b>scores</b> set using the IRT Angoff method had greater stability and accuracy than those set using the common True Score Angoff method. Together these three studies examined the potential advantages of an IRT-based approach to setting <b>passing</b> <b>scores.</b> ^ The results indicate that the IRT Angoff method does not produce more reliable <b>passing</b> <b>score</b> than the common Angoff method. The transition to the IRT-based approach, however, does effectively ameliorate two sources of systematic error in the common Angoff method. The first source of error is brought on by requiring that all judges rate all items and the second source is introduced during the transition from test to scaled <b>score</b> <b>passing</b> <b>scores.</b> By eliminating these sources of error the IRT-based method allows for accurate and unbiased estimation of the judges 2 ̆ 7 true opinion of the ability of the minimally capable examinee. ^ Although all of the theoretical benefits of the IRT Angoff method could not be demonstrated empirically, the results of this thesis are extremely encouraging. The IRT Angoff method was shown to eliminate two sources of systematic error resulting in more accurate <b>passing</b> <b>scores.</b> In addition this thesis provides a strong foundation for a variety of studies with the potential to aid in the selection, training, and evaluation of content experts. Overall findings from this thesis suggest that the application of IRT to the Angoff standard setting method has the potential to offer significantly more valid <b>passing</b> <b>scores.</b> ...|$|R
50|$|The <b>passing</b> <b>score</b> is 70% correct (28 questions) out of 40 questions.|$|R
5000|$|Obtain a <b>passing</b> <b>score</b> on the Examination for Master Addiction Counselors (EMAC) ...|$|R
5000|$|West Virginia {{increased}} a <b>passing</b> <b>score</b> to 80 effective January 1 2013:http://www.courtswv.gov/legal-community/court-rules/Orders/2012/11-20-2012Admission-Prac-of-Law.pdf ...|$|R
40|$|Cluster {{analysis}} {{can be a}} useful statistical technique for setting minimum <b>passing</b> <b>scores</b> on high-stakes examinations by grouping examinees into homogenous clusters based on their responses to test items. It has been most useful for supplementing data or validating minimum <b>passing</b> <b>scores</b> determined from expert judgment approaches, such as the Ebel and Nedelsky methods. However, {{there is no evidence}} supporting how well cluster analysis converges with the modified Angoff method, which is frequently used in medical creden-tialing. Therefore, {{the purpose of this study}} is to investigate the efficacy of cluster analysis for validating Angoff-derived minimum <b>passing</b> <b>scores.</b> Data are from 652 examinees who took a national credentialing examination based on a content-by-process test blueprint. Results indicate a high degree of consis-tency in minimum <b>passing</b> <b>score</b> estimates derived from the modified Angoff and cluster analysis methods. However, the stability of the estimates from cluster analysis across different samples was modest...|$|R
40|$|Vichai Senthong, 1,* Jarin Chindaprasirt, 1,* Kittisak Sawanyawisuth, 1 Noppadol Aekphachaisawat, 2 Suteeraporn Chaowattanapanit, 1 Panita Limpawattana, 1 Charoen Choonhakarn, 1 Aumkhae Sookprasert 1 1 Department of Medicine, Faculty of Medicine, Khon Kaen University, Khon Kaen, Thailand; 2 Central Library, Silpakorn University, Bangkok, Thailand *These authors contributed {{equally to}} this work Background: The Angoff method {{is one of}} the {{preferred}} methods for setting a passing level in an exam. Normally, group meetings are required, which may be a problem for busy medical educators. Here, we compared a modified Angoff individual method to the conventional group method. Methods: Six clinical instructors were divided into two groups matched by teaching experience: modified Angoff individual method (three persons) and conventional group method (three persons). The <b>passing</b> <b>scores</b> were set by using the Angoff theory. The groups set the scores individually and then met to determine the <b>passing</b> <b>score.</b> In the modified Angoff individual method, <b>passing</b> <b>scores</b> were judged by each instructor and the final <b>passing</b> <b>score</b> was adjusted by the concordance method and reliability index. Results: There were 94 fourth-year medical students who took the test. The mean (standard deviation) test score was 65. 35 (8. 38), with a median of 64 (range 46 – 82). The three individual instructors took 45, 60, and 60 minutes to finish the task, while the group spent 90 minutes in discussion. The final <b>passing</b> <b>score</b> in the modified Angoff individual method was 52. 18 (56. 75 minus 4. 57) or 52 versus 51 from the standard group method. There was not much difference in numbers of failed students by either method (four versus three). Conclusion: The modified Angoff individual method may be a feasible way to set a standard <b>passing</b> <b>score</b> with less time consumed and more independent rather than group work by instructors. Keywords: Angoff, individual, <b>passing</b> <b>score,</b> standard-setting, multiple-choice questions, internal medicin...|$|R
5000|$|Acquire a <b>passing</b> <b>score</b> on the National Clinical Mental Health Counseling Exam (NCMHCE) ...|$|R
5000|$|<b>Overall</b> <b>pass</b> {{rates for}} the {{individual}} Step exams that comprise the USMLE are: ...|$|R
50|$|A <b>passing</b> <b>score</b> is only {{considered}} valid if the browser's {{default settings}} were used.|$|R
5000|$|Class of 2015 - 36% {{participation}} rate, 22% {{of seniors}} with a <b>passing</b> <b>score</b> ...|$|R
40|$|Several {{studies have}} {{compared}} different judgmental methods of setting <b>passing</b> <b>scores</b> by estimating item difficulties for the minimally competent examinee. Usually, a direct {{method of estimating}} item difficulties has been compared with an indirect method suggested by Nedelsky (1954). Nedelsky’s method has usually resulted in a substantially lower cutoff score than that arrived at with a direct method. Two {{studies were carried out}} for the purpose of comparing a direct method of setting <b>passing</b> <b>scores</b> with an indirect method that allowed judges to estimate the probability of the minimally competent examinee eliminating each incorrect alternative. In Study 1 a sample of 52 first-level supervisors used both methods to estimate <b>passing</b> <b>scores</b> on a content-oriented selection test for building maintenance specialists. In Study 2 a sample of 62 first-level supervisors used both methods to estimate <b>passing</b> <b>scores</b> on an entry level auto mechanics test. Results of both studies showed that the variance component for method was relatively small and that for raters was relatively large. Reliability estimates of judgments and correlations between judged difficulties and empirical difficulties showed the Angoff (1971) approach to be slightly superior. Results showed no particular advantage to using an indirect approach for estimating minimal competence. Recently, the problem of setting <b>passing</b> <b>score...</b>|$|R
2500|$|Passing {{the test}} {{requires}} both achieving an <b>overall</b> <b>pass</b> mark {{for the total}} points, and passing each section individually; these {{are based on the}} scaled scores. [...] The sectional scores are to ensure that skills are not unbalanced – so one cannot pass by doing well on the written section but poorly on the listening section, for instance. The <b>overall</b> <b>pass</b> mark depends on the level and varies between 100/180 (55.55%) for the N1 and 80/180 (44.44%) for the N5. The pass marks for individual sections are all 19/60 = 31.67% – equivalently, 38/120 = 19/60 for the large section on the N4 and N5. Note that the sectional pass levels are below the <b>overall</b> <b>pass</b> level, at 31.67% instead of 44.44%–55.55%: one need not achieve the <b>overall</b> <b>pass</b> level on each section. These standards were adopted starting in July 2010, and do not vary from year to year, with the scaling instead varying.|$|R
50|$|Andrade {{is known}} for his {{technical}} ability, <b>passing,</b> <b>scoring</b> threat and ability for quick thinking.|$|R
5000|$|Class of 2016 - 48% {{participation}} rate, anticipated 30% of juniors with a <b>passing</b> <b>score</b> ...|$|R
50|$|Jádson {{is known}} for his {{technical}} ability, <b>passing,</b> <b>scoring</b> threat and ability to use both feet.|$|R
25|$|Ranked {{first in}} the state of Texas for <b>overall</b> <b>passing</b> rates as a school since 2001.|$|R
50|$|In {{the second}} quarter, NMSU went up 14-10 {{on a pair}} of <b>passing</b> <b>scores</b> from Holbrook to Anderson.|$|R
5000|$|Obtain <b>passing</b> <b>scores</b> on USMLE Step 1, USMLE Step 2 Clinical Knowledge and USMLE Step 2 Clinical Skills ...|$|R
5000|$|... 4. Approval: If the {{applicant}} minimally achieves the <b>passing</b> <b>score</b> on the examination, they receive notification of such.|$|R
