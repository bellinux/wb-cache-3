8|20|Public
40|$|Abstract — This paper {{presents}} an asynchronous particle filter algorithm for mobile robot position tracking, {{taking into account}} time considerations when integrating observations being delayed or advanced from the prior estiamate time point. The interest of that filter lies in cooperative environments and in fast vehicles. The paper studies the first case, where a sensor network shares perception data with running robots that receive accurate obeservations with large delays due to acquisition, processing and wireless communications. Promising simulated results comparing a basic particle filter and the proposed one are shown. The paper also investigates a situation where a robot is tracking its position, fusing <b>only</b> <b>odometry</b> and observations from a camera network partially covering the robot path. I...|$|E
40|$|While {{probabilistic}} {{techniques have}} been considered extensively {{in the context of}} metric maps, no general purpose probabilistic methods exist for topological maps. We present the concept of Probabilistic Topological Maps (PTMs), a samplebased representation that approximates the posterior distribution over topologies given the available sensor measurements. The PTM is obtained through the use of MCMC-based Bayesian inference over the space of all possible topologies. It is shown that the space of all topologies is equivalent to the space of set partitions of all available measurements. While the space of possible topologies is intractably large, our use of Markov chain Monte Carlo sampling to infer the approximate histograms overcomes the combinatorial nature of this space and provides a general solution to the correspondence problem in the context of topological mapping. We present experimental results that validate our technique and generate good maps even when using <b>only</b> <b>odometry</b> as the sensor measurements...|$|E
40|$|Abstract—An {{interesting}} recent {{branch of}} SLAM algorithms using vision {{has taken an}} appealing approach which can be characterised as simple, robust and lightweight {{when compared to the}} more established and complex geometrical methods. These lightweight approaches typically comprise mechanical odometry or simple visual odometry for local motion estimation; appearance-basedloopclosuredetectionusingeitherwholeimage statistics or invariant feature matching; and some type of efficient pose graph relaxation. However, such algorithms have so far been proven only as localisation systems, since they have not offered the semantic demarcation of free space and obstacles necessary to guide fully autonomous navigation and exploration. In this paper we investigate how to apply and augment such approaches with other lightweight techniques to permit fully autonomous navigation and exploration around a large and complicated room environment. Our approach uses <b>only</b> <b>odometry</b> and visual sensing, the latter being provided by a rig of multiple standard cameras without the need for precise inter-camera calibration. A particular focus of our work is to investigate the camera configurations which are most valuable to permit the capabilities neededbyautonomousnavigation,andoursolutionneatlyassigns each camera a well-defined specialist role...|$|E
40|$|Abstract: This work {{deals with}} the problem of {{estimating}} the trajectory of an autonomous rover by passive stereo vision <b>only</b> (visual <b>odometry).</b> The proposed method relies on the tracking of pointwise image features and on the estimation of the robot motion by robust bundle adjustment of the stereo matched features, before and after the motion. Preliminary results from the application of the algorithm both to simulated data and to actual image sequences acquired by a mobile platform are presented and discussed...|$|R
30|$|Figure 9 d {{shows the}} result of self-localization based <b>only</b> on <b>odometry.</b> This result can be {{calculated}} by integrating {{the position of the}} robot and the distance covered by the tire. The prior known position and orientation values are needed to initialize the position and orientation coordinates of the robot in the world by utilizing the odometry method. However, the prior known position and orientation values are not needed when using our proposed system. Although the routes estimated on the basis of odometry did not show the aforementioned oscillations, the motor errors accumulate considerably during movement and increase in the case of longer periods of movement.|$|R
40|$|A dead-reckoning scheme {{appropriate}} for skid-steered mobile robots is introduced {{to serve in}} a wider scheme for simultaneous localization and map-building. It is based on internal sensors (inertial data and <b>odometry)</b> <b>only.</b> The information from an experimentally derived kinematic model and an onboard inertial navigation system (INS), after a necessary pre-filtering stage, is fused using a simple and fast modifie...|$|R
40|$|In {{this paper}} {{we present a}} {{navigation}} algorithm that enables mobile robots to retrace routes previously taught {{under the control of}} human operators in outdoor environments. Possible applications include ro-bot couriers, autonomous vehicles, tour guides and robotic patrols. The appearance-based approach presented in the paper is provably convergent, computationally inexpensive compared with map-based approaches and requires <b>only</b> <b>odometry</b> and a monocular omnidi-rectional vision sensor. A sequence of reference images is recorded during the human-guided route-teaching phase. Before starting the autonomous phase, the robot needs to be positioned {{at the beginning of the}} route. During the autonomous phase, the measurement image is compared with reference images using image cross-correlation per-formed in the Fourier domain to recover the difference in relative ori-entation. Route following is achieved by compensating for this orien-tation difference. Over 18 km of experiments performed under varying conditions demonstrate the algorithm’s robustness to lighting varia-tions and partial occlusion. Obstacle avoidance is not included in the current system. KEY WORDS—Mobile robot, navigation, route following, appearance-based, outdoor, panoramic vision, route repeat, re-pla...|$|E
40|$|With most tele-operated robots the operator’s only {{feedback}} {{is the view}} from an onboard camera. Live video lets the operator observe the robot’s immediate surroundings but does not establish the orientation or whereabouts of the robot in its environment. An additional plot of the robot’s trajectory would be helpful for the operator and is sometimes provided, based on GPS. However, indoors where GPS is unavailable, tracking has to rely on dead-reckoning, which is too inaccurate to be useful. Our proposed TelOpTrak method corrects deadreckoning errors even when <b>only</b> <b>odometry</b> and a low-cost (and thus, high-drift) MEMS-class gyro {{are available on the}} robot. TelOpTrak corrects gyro drift by exploiting the structured nature of most buildings, but without having to directly sense building features. This paper explains the TelOpTrak method and provides comprehensive experimental results. Earlier versions of this paper (Borenstein et al., 2010 a), (Borenstein et al., 2010 b) were presented at two conferences. The main difference between the earlier conference papers and the present manuscript is that the latter is more comprehensive, more up-to-date, and it presents an entirely new set of experimental results, including results of a live demonstratio...|$|E
40|$|Trabajo presentado al IROS 2009 celebrado en Saint Louis (EE. UU.) del 10 al 15 de octubre. This paper {{presents}} an asynchronous particle filter algorithm for mobile robot position tracking, {{taking into account}} time considerations when integrating observations being delayed or advanced from the prior estimate time point. The interest of that filter lies in cooperative environments and in fast vehicles. The paper studies the first case, where a sensor network shares perception data with running robots that receive accurate observations with large delays due to acquisition, processing and wireless communications. Promising simulated results comparing a basic particle filter and the proposed one are shown. The paper also investigates a situation where a robot is tracking its position, fusing <b>only</b> <b>odometry</b> and observations from a camera network partially covering the robot path. This work was supported by projects: 'Ubiquitous networking robotics in urban settings' (E- 00938), 'CONSOLIDER-INGENIO 2010 Multimodal interaction in pattern recognition and computer vision' (V- 00069), 'Robotica ubicua para entornos urbanos' (J- 01225). Partially supported by Consolider Ingenio 2010, project CSD 2007 - 00018, CICYT project DPI 2007 - 61452, and IST- 045062 of the European Community Union. Peer Reviewe...|$|E
40|$|Abstract—This paper {{presents}} a multi-robot mapping and localization system. Learning maps and efficient exploration {{of an unknown}} environment is a fundamental problem in mobile robotics usually called SLAM (simultaneous localization and mapping problem). Our approach involves a team of mobile robots that uses Scan-Matching and Fast-SLAM techniques based on scan-laser and odometry information for mapping large environments. The single maps generated for each robot are more accurate than the maps generated <b>only</b> by <b>odometry.</b> When a robot detects another, it sends its processed map and the master robot generates a very accurate global map. This method cuts down the global map building time. Some experimental results and conclusions are presented. Index Terms — Multi-robot SLAM, scan-mathcing, fast-slam...|$|R
40|$|Abstract — We {{present a}} novel {{approach}} which enables a mobile robot to estimate its trajectory in an unknown environment with long-range passive radio-frequency identification (RFID). The estimation is based <b>only</b> on <b>odometry</b> and RFID measurements. The technique requires no prior observation model and makes no assumptions on the RFID setup. In particular, it is adaptive to the power level, the way the RFID antennas are mounted on the robot, and environmental characteristics, which have major impact on long-range RFID measurements. Tag positions need not be known in advance, and only the arbitrary, given infrastructure of RFID tags in the environment is utilized. By a series of experiments with a mobile robot, we show that trajectory estimation is achieved accurately and robustly. I...|$|R
40|$|Date : 2003 - 11 - 18 Where : Univ of Freiburg, {{building}} 079, AIS-Lab Robot-type : Pioneer 2 with 1 LMS-Laser What : 1 robot (magnum with stayton board) {{was driving}} around in building 079 File format : carmen logger format (POS, FLASER), <b>only</b> laser and <b>odometry</b> data recorded, and a scanmatched rec & carmen log file File(s) : fr 079. log. gz fr 079 -sm. log. gz fr 079 -map. tg...|$|R
40|$|How do honeybees use {{visual odometry}} and goal-defining {{landmarks}} to guide food search? In one experiment, bees {{were trained to}} forage in an optic-flow-rich tunnel with a landmark positioned directly above the feeder. Subsequent food-search tests indicated that bees searched much more accurately when both odometric and landmark cues were available than when <b>only</b> <b>odometry</b> was available. When the two cue sources were set in conflict, by shifting {{the position of the}} landmark in the tunnel during test, bees overwhelmingly used landmark cues rather than odometry. In another experiment, odometric cues were removed by training and testing in axially striped tunnels. The data show that bees did not weight landmarks as highly as when odometric cues were available, tending to search {{in the vicinity of the}} landmark for shorter periods. A third experiment, in which bees were trained with odometry but without a landmark, showed that a novel landmark placed anywhere in the tunnel during testing prevented bees from searching beyond the landmark location. Two further experiments, involving training bees to relatively longer distances with a goal-defining landmark, produced similar results to the initial experiment. One caveat was that, with the removal of the familiar landmark, bees tended to overshoot the training location, relative to the case where bees were trained without a landmark. Taken together, the results suggest that bees assign appropriate significance to odometric and landmark cues in a more flexible and dynamic way than previously envisaged...|$|E
40|$|This paper {{presents}} a multi-robot mapping and localization system. Learning maps and efficient exploration {{of an unknown}} environment is a fundamental problem in mobile robotics usually called SLAM (simultaneous localization and mapping problem). Our approach involves a team of mobile robots that uses Scan-Matching and Fast-SLAM techniques based on scan-laser and odometry information for mapping large environments. The single maps generated for each robot are more accurate than the maps generated <b>only</b> by <b>odometry.</b> When a robot detects another, it sends its processed map and the master robot generates a very accurate global map. This method cuts down the global map building time. Some experimental results and conclusions are presented. Comunidad de Madrid and the University of Alcalá, support through the projects RoboCity 2030 (CAM-S- 0505 /DPI/ 000176) and SLAM-MULEX (CCG 07 -UAH/DPI- 1736) ...|$|R
40|$|A {{biomimetic}} {{mobile robot}} called “Shrewbot” {{has been built}} {{as part of a}} neuroethological study of the mammalian facial whisker sensory system. This platform has been used to further evaluate the problem space of whisker based tactile Simultaneous Localisation And Mapping (tSLAM). Shrewbot uses a biomorphic 3 -dimensional array of active whiskers and a model of action selection based on tactile sensory attention to explore a circular walled arena sparsely populated with simple geometric shapes. Datasets taken during this exploration have been used to parameterise an approach to localisation and mapping based on probabilistic occupancy grids. We present the results of this work and conclude that simultaneous localisation and mapping is possible given <b>only</b> noisy <b>odometry</b> and tactile information from a 3 -dimensional array of active biomimetic whiskers and no prior information of features in the environment. ...|$|R
30|$|The {{uncertainty}} in {{the location of the}} robot increases as the interval between emitters becomes larger. In an environment in which only unique ID encoding emitters are used, attempts to improve the accuracy by decreasing the interval between the emitters causes the quantity of required IDs to increase rapidly. The proposed system, which combines unique ID encoding emitters with repeated ID encoding emitters, can reduce the interval without consuming new IDs. We verified the real-time self-localization performance of the proposed system by allowing the robot to move along fixed trajectories. This enabled us to statistically assess the real-time localization performance. The evaluation included four configurations: the combination of 3 unique ID and 8 repeated ID encoding emitters, the use of 11 unique ID encoding emitters, the use of 3 unique ID encoding emitters, and the use of no emitters. The performance assessment of self-localization under the no-emitter configuration is based <b>only</b> on <b>odometry.</b> The robot traversed the fixed trajectory three times at a set speed of 200 mm/s.|$|R
40|$|The {{robotics}} {{field has}} seen indoor robots that are increasingly capable of accurately navigating in buildings and performing service tasks, such as cleaning and transporting items. Given the advances in accurate navigation and robust motion planning, large scale industrial applications become feasible tasks. Two common tasks are the mapping of large unknown structured spaces and using learned maps for coverage planning. In this thesis, methods are presented for robotic mapping of large spaces and coverage planning under finite energy constraints. The thesis {{is presented in}} two parts. Part I focuses on the mapping component. We present a basic Kinect-based Simultaneous Localization And Mapping (SLAM) system for CoBot (mobile service robot in CMU’s Gates-Hillman Center) in predominantly planar environments. The SLAM solution is Kinect-based in the sense that observations come <b>only</b> from <b>odometry</b> measurements and three Kinect sensors. The system is designed for the motivating scenario of map-ping a large room or floor with aisles and shelves for the purposes of a robot in a store or warehouse. We present our feature extraction techniques, describe the graph SLA...|$|R
30|$|Visual <b>odometry</b> <b>only</b> {{provides}} relative positioning, i.e., positioning {{relative to}} an earlier visited reference point. As a consequence, estimation errors are cumulative, and visual odometry methods are therefore susceptible to drift. The greater the distance traveled from the last absolute reference point (e.g., the known GPS coordinates of the starting address), the greater the positional error can become. While this may appear to limit the use of visual odometry to a short distances, the drift error can be bounded by combination with additional passive sensors (e.g., a magnetic compass for dead reckoning) or a priori known information (e.g., the local road map) [2, 3]. As such, visual odometry is still a prime candidate to supplement satellite navigation even for urban scenarios where signal reception may be unreliable for prolonged distances.|$|R
40|$|We present two {{different}} methods based on visual odometry for pose estimation (x, y, Ө) of a robot. The methods proposed are: one appearance based method that computes similarity measures between consecutive images, and one method that computes visual flow of particular features, i. e. spotlights on ceiling. Both techniques {{are used to}} correct the pose (x, y, Ө) of the robot, measuring heading change between consecutive images. A simple Kalman filter, extended Kalman filter and simple averaging filter are used to fuse the estimated heading from visual odometry methods with odometry data from wheel encoders. Both techniques are evaluated on three different datasets of images obtained from a warehouse and {{the results showed that}} both methods are able to minimize the drift in heading compare to using wheel <b>odometry</b> <b>only...</b>|$|R
40|$|For any {{mobile robot}} it {{is a major}} issue that of {{estimating}} its position into the working environment. Although this task is partly carried out through external sensors, incrementally computing the ego-motion of the robot using proprioceptive sensors still is a fundamental step to obtain an estimation of the robot displacement. In this work we deal with the sensor fusion problem for the case of a mobile robot equipped with an odometer and an inertial sensor (a gyroscope). We address this problem rigorously through its formulation as a probabilistic estimation problem, developing an efficient solution {{in the form of an}} Extended Kalman Filter (EKF), which can be easily implemented in the low-level firmware of a real mobile robot. Experimental results reveal a qualitative improvement in the robot pose estimation for our sensor fusion system when compared with <b>odometry</b> <b>only,</b> which is the most wide spread technique in commercial robots. 1...|$|R
40|$|Abstract — Humanoids have {{recently}} {{become a popular}} research platform in the robotics community. Such robots offer various fields for new applications. However, they have several drawbacks compared to wheeled vehicles such as stability problems, limited payload capabilities, violation of the flat world assumption, and they typically provide <b>only</b> very rough <b>odometry</b> information, if at all. In this paper, we investigate the problem of learning accurate grid maps with humanoid robots. We present techniques {{to deal with some}} of the above-mentioned difficulties. We describe how an existing approach to the simultaneous localization and mapping (SLAM) problem can be adapted to robustly learn accurate maps with a humanoid equipped with a laser range finder. We present an experiment in which our mapping system builds a highly accurate map with a size of around 20 m by 20 m using data acquired with a humanoid in our office environment containing two loops. The resulting maps have a similar accuracy as maps built with a wheeled robot. I...|$|R
40|$|The {{problem of}} {{creating}} a map given <b>only</b> the erroneous <b>odometry</b> and feature measurements and locating the own position in this environment is known in the literature as the Simultaneous Localization and Mapping (SLAM) problem. In this paper we investigate how a Nested Dissection Ordering scheme can improve the {{the performance of a}} recently proposed Square Root Information Smoothing (SRIS) approach. As the SRIS does perform smoothing rather than filtering the SLAM problem becomes the Smoothing and Mapping problem (SAM). The computational complexity of the SRIS solution is dominated by the cost of transforming a matrix of all measurements into a square root form through factorization. The factorization of a fully dense measurement matrix has a cubic complexity in the worst case. We show that the computational complexity for the factorization of typical measurement matrices occurring in the SAM problem can be bound tighter under reasonable assumptions. Our work is motivated both from a numerical / linear algebra standpoint as well as by submaps used in EKF solutions to SLAM...|$|R
40|$|A {{key problem}} in the {{deployment}} of sensor networks is that of determining the location of each sensor such that subsequent data gathered can be registered. We would also like the network to provide localization for mobile entities, allowing them to navigate and explore the environment. In this paper, we present a robust decentralized algorithm for mapping the nodes in a sparsely connected sensor network using range- <b>only</b> measurements and <b>odometry</b> from a mobile robot. Our approach utilizes an extended Kalman filter (EKF) in polar space allowing us to model the nonlinearities within the range-only measurements using Gaussian distributions. We also extend this unimodal centralized EKF to a multi-modal decentralized framework enabling us to accurately model the ambiguities in range-based position estimation. Each node within the network estimates its position along with its neighbor 2 ̆ 7 s position and uses a message-passing algorithm to propagate its belief to its neighbors. Thus, the global network localization problem is solved in pieces, by each node independently estimating its local network, greatly reducing the computation done by each node. We demonstrate the effectiveness of our approach using simulated and real-world experiments with little to no prior information about the node locations...|$|R
40|$|Objects {{are rich}} {{information}} sources about the environment. A 3 D {{model of the}} objects, together with their semantic labels, {{can be used for}} camera localization as well as for cognitive reasoning about the environment. However, traditional frameworks for scene reconstruction usually map a cloud of points using structure-from-motion techniques, but do not provide objects representation. On the other side, robotics object-based mapping mainly focus on adding cognitive representations to a metric or topologic map built using traditional SLAM tech-niques. In this work we propose a framework for environment modeling by representing the objects in the scene, detected by an object recognition and seg-mentation technique. The key idea is to incorporate the resulting image segments and labels into a global inference engine in order to build simple geometric models for the objects. For now, we consider the perfect object recognition case, where we know the exact object identities, testing our approach using coarsely hand-annotated images captured by a robot carrying an omnidirectional camera. We found that the resultant object locations and sizes are fully compatible with what is expected, and the inferred robot trajectory is improved when compared to that recovered using <b>odometry</b> <b>only.</b> 1...|$|R
40|$|While {{visual odometry}} has {{unbounded}} error, navigation from pre-existing consistent scene models can generate extremely repeatable position estimates. This paper discusses {{a new approach}} to localization motivated by the fact that many man-made environments constain substantially flat, visually textured surfaces of persistent appearance. For this important class of vision-based navigation problems the scene model can be reduced to a 2 D surface painted with real textures- in other words, an image mosaic. Straightforward techniques from image-based localization and mosaicking are used to produce a field relevant AGV guidance system based on <b>only</b> vision and <b>odometry.</b> The visual tracking and localization aspects of the approach are described. We show that this approach to localization is able to exceed the speed barriers due to distortion and image overlap that are intrinsic to visual tracking and odometry. Speed can, however, become limited by a new mechanism- the inherent instability of visual tracking when operating in the regime beyond the overlap spped limit. Field trials currently demonstrate that the particular simplifications resulting from a downward looking camera configuration produce a guidance system repeatable to 1 mm throughout a 50, 000 square foot facility with an MTBF (corresponding to loss of visual lock) of 10 6 images or five days of operation. ...|$|R
40|$|Presented at the IV Workshop de Visao Computacional (WVC), 17 - 19 November 2008, Bauru, Brazil. Objects {{are rich}} {{information}} sources about the environment. A 3 D {{model of the}} objects, together with their semantic labels, {{can be used for}} camera localization as well as for cognitive reasoning about the environment. However, traditional frameworks for scene reconstruction usually map a cloud of points using structure-from-motion techniques, but do not provide objects representation. On the other side, robotics object-based mapping mainly focus on adding cognitive representations to a metric or topologic map built using traditional SLAM techniques. In this work we propose a framework for environment modeling by representing the objects in the scene, detected by an object recognition and segmentation technique. The key idea is to incorporate the resulting image segments and labels into a global inference engine in order to build simple geometric models for the objects. For now, we consider the perfect object recognition case, where we know the exact object identities, testing our approach using coarsely hand-annotated images captured by a robot carrying an omnidirectional camera. We found that the resultant object locations and sizes are fully compatible with what is expected, and the inferred robot trajectory is improved when compared to that recovered using <b>odometry</b> <b>only...</b>|$|R

