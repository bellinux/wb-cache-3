16|15|Public
40|$|Activity Plan Generator (APGEN), now at version 5. 0, is a {{computer}} program that assists in generating an integrated plan of activities for a spacecraft mission that does not <b>oversubscribe</b> spacecraft and ground resources. APGEN generates an interactive display, through which the user can easily create or modify the plan. The display summarizes the plan {{by means of a}} time line, whereon each activity is represented by a bar stretched between its beginning and ending times. Activities can be added, deleted, and modified via simple mouse and keyboard actions. The use of resources can be viewed on resource graphs. Resource and activity constraints can be checked. Types of activities, resources, and constraints are defined by simple text files, which the user can modify. In one of two modes of operation, APGEN acts as a planning expert assistant, displaying the plan and identifying problems in the plan. The user is in charge of creating and modifying the plan. In the other mode, APGEN automatically creates a plan that does not <b>oversubscribe</b> resources. The user can then manually modify the plan. APGEN is designed to interact with other software that generates sequences of timed commands for implementing details of planned activities...|$|E
40|$|AVA v 2 {{software}} selects {{goals for}} execution from {{a set of}} goals that <b>oversubscribe</b> shared resources. The term goal refers to a science or engineering request to execute a possibly complex command sequence, such as image targets or ground-station downlinks. Developed as an extension to the Virtual Machine Language (VML) execution system, the software enables onboard and remote goal triggering {{through the use of}} an embedded, dynamic goal set that can <b>oversubscribe</b> resources. From the set of conflicting goals, a subset must be chosen that maximizes a given quality metric, which in this case is strict priority selection. A goal can never be pre-empted by a lower priority goal, and high-level goals can be added, removed, or updated at any time, and the "best" goals will be selected for execution. The software addresses the issue of re-planning that must be performed in a short time frame by the embedded system where computational resources are constrained. In particular, the algorithm addresses problems with well-defined goal requests without temporal flexibility that oversubscribes available resources. By using a fast, incremental algorithm, goal selection can be postponed in a "just-in-time" fashion allowing requests to be changed or added at the last minute. Thereby enabling shorter response times and greater autonomy for the system under control...|$|E
40|$|Abstract — In {{this paper}} we use fluid model {{techniques}} to establish two results concerning the throughput of data switches. For an input-queued switch (with no speedup) {{we show that}} a maximum weight algorithm for connecting in-puts and outputs delivers a throughput of 100 %, and for combined input- and output-queued switches that run at a speedup of 2 we show that any maximal matching algorithm delivers a throughput of 100 %. The only assumptions on the input traffic are that it satisfies the strong law of large numbers {{and that it does}} not <b>oversubscribe</b> any input or any output. I...|$|E
40|$|Abstract — The {{potential}} problem of <b>oversubscribing</b> receivers in receiver-driven multicast is addressed. We present a framework based on harmonizing the erasure-resilience properties of video with existing congestion control algorithms. The result offers subscription alternatives for receivers {{in which a}} penalty in terms of visual quality will be experienced by <b>oversubscribing</b> receivers. Thus, we provide an incentive for performing proper congestion control. The presented framework is independent of specific congestion control algorithms. Simulation results show the intended performance. I...|$|R
5000|$|The {{album was}} funded through an online Pledge campaign, {{in which her}} fans {{demonstrated}} their faith <b>oversubscribing</b> it in two days. No doubt aided by a post from Sia Furler directing her fans to Noonan's soundcloud page and saying “My friend Katie sure can sing!”.|$|R
5000|$|System recovery: {{measures}} {{the speed at}} which a DUT recovers from an overload or oversubscription condition. This subtest is performed by temporarily <b>oversubscribing</b> the device under test and then reducing the throughput at normal or low load while measuring frame delay in these two conditions. The different between delay at overloaded condition and the delay and low load conditions represent the recovery time.|$|R
40|$|We {{describe}} an efficient, online goal selection algorithm {{and its use}} for selecting goals at runtime. Our {{focus is on the}} re-planning that must be performed in a timely manner on the embedded system where computational resources are limited. In particular, our algorithm generates near optimal solutions to problems with fully specified goal requests that <b>oversubscribe</b> available resources but have no temporal flexibility. By using a fast, incremental algorithm, goal selection can be postponed in a "just-in-time" fashion allowing requests to be changed or added at the last minute. This enables shorter response cycles and greater autonomy for the system under control...|$|E
40|$|Imagine {{the gleam}} {{in the eye}} of an {{administrator}} who envi-sions many sections or a very large section of an introduc-tory course being staffed by a single instructor and a cadre of student proctors: Faculty salary costs are reduced, in-come remains the same (or increases with the popularity of the course), the proctors are either paying to receive aca-demic credit for their experience or would have been on fi-nancial aid anyway, and another "innovative " course can be included in the next institutional review! Everyone should be happy. Greenspoon (1983) described an experi-ence with which I can readily identify where administrators naively supported the PSI concept without providing ade-quate time and resources for the development of materials, training of proctors, locating testing facilities, and so on. Alternatively, administrators may <b>oversubscribe</b> PS...|$|E
40|$|Operating the Mars Exploration Rovers is a challenging, time-pressured task. Each day, the {{operations}} team must generate {{a new plan}} describing the rover activities for the next day. These plans must abide by resource limitations, safety rules, and temporal constraints. The objective is to achieve as much science as possible, choosing from a set of observation requests that <b>oversubscribe</b> rover resources. In order to accomplish this objective, given the short amount of planning time available, the MAPGEN (Mixed-initiative Activity Plan GENerator) system was made a missioncritical part of the ground operations system. MAPGEN is a mixed-initiative system that employs automated constraint-based planning, scheduling, and temporal reasoning to assist operations staff in generating the daily activity plans. This paper describes the adaptation of constraint-based planning and temporal reasoning to a mixed-initiative setting and the key technical solutions developed for the mission deployment of MAPGEN...|$|E
40|$|Abstract—In {{a typical}} SAN {{solution}} consisting of intelligent storage system {{there will be}} many LUNs with pre-allocated space (thick LUNs) based on the business requirements. In case of thin provisioned LUNs physical space is not allocated upfront; space will be allocated on demand depending on incoming write workload. In thin provisioned LUN implementation some space will be anchored or pre-allocated and in other implementations no space will be anchored (everything is allocated on demand). Most often administrator <b>oversubscribes</b> the space for thick LUN’s. In case of thin LUN’s with anchored space some space will be reserved to accommodate data or incoming writes. There will be many oversubscribed thick and anchored thin LUNs in a storage systems resulting in non optimal usage of storage space. For some of the business needs <b>oversubscribing</b> space for thick and anchored thin LUN’s without considering the utilization doesn’t augur well. There should be a way to optimize storage space in an intelligent storage system based on LUN utilization over a period of time. By determining utilization of a LUN from time to time it’s possible to have dynamic provisioning mechanism for thick and anchored thin LUNs based on the usage over a time. The proposed policy driven thick and anchored thin LUN optimization will help storage admin to optimize space in a storage system...|$|R
40|$|Rising {{trends in}} the number of {{customers}} turning to the cloud for their computing needs has made effective resource allocation imperative for cloud service providers. In order to maximize profits and reduce waste, providers have started to explore the role of <b>oversubscribing</b> cloud resources. However, the benefits of cloud-based oversubscription are not without inherent risks. This paper attempts to unveil the incentives, risks, and techniques behind oversubscription in a cloud infrastructure. Additionally, an overview of the current research that has been completed on this highly relevant topic is reviewed, and suggestions are made regarding potential avenues for future work. Comment: 7 pages, 3 figure...|$|R
40|$|Title: Pricing of IPO:s on the Swedish {{stock market}} Authors: Mikael Gustavsson Martin Kvist Henrik Wannberg Tutor: Anders Hederstierna Problem: There are certain sets of {{problems}} for underwriters {{when they are}} pricing potential stock-exchange companies. The problem partly consists of estimating the demand at an IPO and also to accurately value the business {{in comparison to the}} market. To set the price per share is problematic, since the company carrying out the public offering wants to optimise the amount of capital {{at the same time as}} they want the share to be subscribed in full. If the IPO would not be carried out, it could cause harm to the position of the underwriters. Purpose: To investigate whether underpricing exists associated with IPO:s on the Swedish market during the period 1995 - 1999. Method: The collection of data has been achieved by means of: Internet, E-mail and telephone calls. Conclusions: We have reached the conclusion that <b>oversubscribing</b> in comparison to the return at an IPO has an average growth of 3. 3 per cent the first day, on every occasion of <b>oversubscribing.</b> For the studied population, including 138 initial public offerings, the average market value rise is 15 per cent the first day. When we made our calculations, adjustments have been made for the general index of Affärsvärlden (AFGX). The revenue of 15 per cent indicates that underpricing does exist. 102 IPO:s raise in value the first day and 36 decrease, after adjustmens have been made for AFGX. This interprets that IPO:s are underpriced. Henrik tel. 073 - 62 63 554 Mikael tel. 0702 - 46 28 9...|$|R
40|$|The paper models {{fixed rate}} tenders, where {{a central bank}} offers to lend central bank funds to {{financial}} institutions. Bidders are constrained {{by the amount of}} collateral they have. We focus on the strategic interaction between bidding in the tender and trading in the interbank market after the tender, where short squeezes could occur. We examine how the design of the tender affects equilibrium bidding behavior and the incidence of short squeezes. Important elements in the analysis include the type of policy implemented by the central bank as well as bidders' initial endowments of liquidity and collateral. Three instruments for softening short squeezes are identified: the tender rate, the tender sizes, and admissible collateral. Increasing the tender rate or size tends to decrease the probability and severity of a short squeeze. The possibility of a short squeeze may induce bidders to <b>oversubscribe</b> even if the tender rate is higher than the competitive rate...|$|E
40|$|In {{this paper}} we use fluid model {{techniques}} to establish two results concerning the throughput of data switches. For an input-queued switch (with no speedup) {{we show that}} a maximum weight algorithm for connecting inputs and outputs delivers a throughput of 100 %, and for combined input- and output-queued switches that run at a speedup of 2 we show that any maximal matching algorithm delivers a throughput of 100 %. The only assumptions on the input traffic are that it satisfies the strong law of large numbers {{and that it does}} not <b>oversubscribe</b> any input or any output. I. INTRODUCTION Packet switches based on an input-queued (IQ) crossbar architecture are attractive for use in high speed networks. This is because the buffers which queue packets at the inputs need only run twice as fast the line rates. That is, if time were slotted so that at most one packet arrived at each input of the switch per time slot, then an input buffer potentially needs to make upto two transactions per time slo [...] ...|$|E
40|$|A {{computer}} program for use aboard a scientific-exploration spacecraft autonomously selects among goals specified in high-level requests and generates corresponding sequences of low-level commands, understandable by spacecraft systems. (As used here, 'goals' signifies specific scientific observations.) From a dynamic, onboard set of goals that could <b>oversubscribe</b> spacecraft resources, the program selects a non-oversubscribing subset that maximizes a quality metric. In {{an early version}} of the program, the requested goals are assumed to have fixed starting times and durations. Goals can conflict by exceeding a limit on either the number of separate goals or the number of overlapping goals making demands on the same resource. The quality metric used in this version is chosen to ensure that a goal will never be replaced by another having lower priority. At any time, goals can be added or removed, or their priorities can be changed, and the 'best' goal will be selected. Once a goal has been selected, the program implements a robust, flexible approach to generation of low-level commands: Rather than generate rigid sequences with fixed starting times, the program specifies flexible sequences that can be altered to accommodate run time variations...|$|E
40|$|The CORA (Coallocative, <b>Oversubscribing</b> Resource Allocation) {{architecture}} {{is a market}} based resource reservation system that utilises a trustworthy Vickrey auction to make combinatorial allocations of resources. This paper {{provides an overview of}} several significant components of the CORA architecture. Firstly, CORA utilises a novel combination of techniques to improve utilisation, including oversubscription, coallocation, just-in-time reallocation and a flexible contract structure. Secondly, this paper utilises a new auction architecture that does not require the auctioneers to be trusted. The advantage is that any entity (untrusted or otherwise) can conduct a privacy preserving Vickrey auction, removing the need for a trusted and privileged auction service within the system. CORA demonstrates how a practical, efficient and trustworthy auction scheme can be implemented in a Grid Economy. ...|$|R
40|$|Abstract. Honeybees {{coordinate}} foraging efforts across vast areas {{through a}} complex system of advertising and recruitment. One mechanism for coordination is the waggle dance, a movement pattern which carries positional information about food sources. However, recent {{evidence suggests that}} recruited foragers may not use the dance’s positional information {{to the degree that}} has traditionally been believed. We model bee colony foraging to investigate the value of sharing food source position information in different environments. We find that in several environments, relying solely on private information about previously encountered food sources is more efficient than sharing information. Relying on private information leads to a greater diversity of forage sites and can decrease over-harvesting of sources. This is beneficial in environments with small quantities of nectar per flower, but may be detrimental in nectar-rich environments. Efficiency depends on both the environment and a balance between exploiting high-quality food sources and <b>oversubscribing</b> them. ...|$|R
40|$|Predicated {{execution}} {{enables the}} removal of branches wherein seg-ments of branching code are converted into straight-line segments of conditional operations. An important, but generally ignored side effect of this transformation is that he compiler must assign distinct resources to all the predicated operations {{at a given time}} to ensure that those r sources are available at run-time. However, a resource is only put to productive use when the predicates associated with its operations evaluate to True. We propose predicate-aware schedul-ing to reduce the superfluous commitment of resources to opera-tions whose predicates evaluate to False at run-time. The central idea is to assign multiple operations to the same resource at the same time, thereby <b>oversubscribing</b> itsuse. This assignment is in-telligently performed to ensure that no two operations simultane-ously assigned to the same resource will have both of their predi-cates evaluate to True. Thus, no resource is dynamically oversub-scribed. The overall effect of predicate aware scheduling is to use resources more efficiently, thereby increasing performance when resource constraints are a bottleneck...|$|R
40|$|Abstract. The {{performance}} of multi-tier systems {{is known to}} be significantly degraded by workloads that place bursty service demands on system resources. Burstiness can cause queueing delays, <b>oversubscribe</b> limited threading resources, and even cause dynamic bottleneck switches betweenresources. Thus,thereisneedfor amethodologytocreate benchmarks with controlled burstiness and bottleneck switches to evaluate their impact on system performance. We tackle this problem using a model-based technique for the automatic and controlled generation of bursty benchmarks. Markov models are constructed in an automated manner to model the distribution of service demands placed by sessions of a given system on various system resources. The models are then used to derive session submission policies that result in user-specified levels of service demand burstiness for resources at the different tiers in a system. Our approach can also predict under what conditions these policies can create dynamic bottleneck switching among resources. A case study using a three-tier TPC-W testbed shows that our method is able to control and predict burstiness for session service demands. Further, results from the study demonstrate that our approach was able to inject controlled bottleneck switches. Experiments show that these bottleneck switches cause dramatic latency and throughput degradations that are not shown by the same session mix with non-bursty conditions...|$|E
40|$|A report {{discusses}} an algorithm for an onboard {{planning and}} execution technology {{to support the}} exploration and characterization of geological features by autonomous rovers. A rover {{that is capable of}} deciding which observations are more important relieves the engineering team from much of the burden of attempting to make accurate predictions of what the available rover resources will be in the future. Instead, the science and engineering teams can uplink a set of observation requests that may potentially <b>oversubscribe</b> resources and let the rover use observation priorities and its current assessment of available resources to make decisions about which observations to perform and when to perform them. The algorithm gives the rover the ability to model spatial coverage quality based on data from different scientific instruments, {{to assess the impact of}} terrain on coverage quality, to incorporate user-defined priorities among subregions of the terrain to be covered, and to update coverage quality rankings of observations when terrain knowledge changes. When the rover is exploring large geographical features such as craters, channels, or boundaries between two different regions, an important factor in assessing the quality of a mission plan is how the set of chosen observations spatially cover the area of interest. The algorithm allows the rover to evaluate which observation to perform and to what extent the candidate observation will increase the spatial coverage of the plan...|$|E
40|$|U. S. {{data center}} energy {{consumption}} {{is expected to}} rise past 100 billion kWh in 2013. Approximately 50 % of this energy usage can be attributed to servers, networks, and storage, and the other half goes to power and cooling infrastructures in their support. Servers are so non- energy efficient that they consume 65 % of a fully utilized server's power when only 30 % utilized. Even when 100 % utilized, the server may not be running efficiently. This dissertation improves the energy efficiency of data center systems in three ways. The first method improves server energy efficiency by turning off CPU cores during long- latency memory accesses with no performance penalty to data center applications. This technique leverages peak rush current from on-chip power distribution networks to quickly charge core capacitance, and allows the core to resume execution in as little as 8. 06 ns. Core state is saved through careful use of slave latches and source biasing. A key means to increasing effective CPU utilization and energy efficiency of servers, is to leverage virtual machine and thread migration at minimal performance overhead. Our second technique speeds up software thread migration by up to 2. 5 × compared to Linux, with latencies as small as 933 ns. We leverage this technique to quickly migrate operating system code between asymmetric cores to reduce application energy consumption. The techniques introduced so far assume that I/O devices have sufficient bandwidth to keep the server processor busy. In contrast to this assumption, many data centers <b>oversubscribe</b> their networks to reduce cost and power consumption, sometimes at the expense of overall data center efficiency. Our last contribution is a software top -of-the-rack switch capable of offloading unmodified TCP/ IP traffic onto a prototype, microsecond optical circuit switch within a microsecond. We demonstrate that servers can utilize up to 95. 4 % of optical circuit bandwidth even when switch reconfiguration latency is reduced by three orders of magnitude to 11. 5 microseconds, supporting the introduction of low-latency optics into the data center to radically reduce cost and power consumption of full bisection bandwidth network...|$|E
40|$|With {{the intense}} {{competition}} between cloud providers, oversubscription is increasingly important to maintain profitability. <b>Oversubscribing</b> physical resources {{is not without}} consequences: it {{increases the likelihood of}} overload. Memory overload is particularly damaging. Contrary to traditional views, we analyze current data center logs and realistic Web workloads to show that overload is largely transient: up to 88. 1 % of overloads last for less than 2 minutes. Regarding overload as a continuum that includes both transient and sustained overloads of various durations points us to consider mitigation approaches also as a continuum, complete with tradeoffs with respect to application performance and data center overhead. In particular, heavyweight techniques, like VM migration, are better suited to sustained overloads, whereas lightweight approaches, like network memory, are better suited to transient overloads. We present Overdriver, a system that adaptively takes advantage of these tradeoffs, mitigating all overloads within 8 % of well-provisioned performance. Furthermore, under reasonable oversubscription ratios, where transient overload constitutes the vast majority of overloads, Overdriver requires 15 % of the excess space and generates a factor of four less network traffic than a migration-only approach...|$|R
40|$|The CORA (Coallocative, <b>Oversubscribing</b> Resource Allocation) {{architecture}} is an auction based resource reservation system that makes combinatorial allocations {{of resources to}} clients. The focus {{of this paper is}} on the use of cryptographic tools in CORA to remove the need for trust in the resource auctioneer. One of the nice properties of this approach is that the auctioneers can be drawn from an arbitrary pool of untrusted peers, without the need to establish pre-existing trust or restrict the role of auctioneer to a trusted system service. This approach results in more flexibility in the design of large economic systems, with the potential for wide distribution of load amongst many auctioneers. In addition, only the winners of the auction and the prices they pay are revealed while all other bid values are kept secret. It is our belief that future growth or commercialisation of large scale Grid systems requires the provision of such mechanisms to share the wide pool of Grid brokered resources such as computers, software, licences and peripherals amongst many users and organisations. This paper encapsulates an overview of our design, our experiences of implementing two different secure auction protocols and the performances that we have achieved. ...|$|R
40|$|Interactive services, such as Web search, recommendations, games, and finance, {{must respond}} quickly to satisfy cus-tomers. Achieving this goal {{requires}} optimizing tail (e. g., 99 th+ percentile) latency. Although every server is multi-core, parallelizing individual requests to reduce tail latency is challenging because (1) service demand is unknown when requests arrive; (2) blindly parallelizing all requests quickly <b>oversubscribes</b> hardware resources; and (3) parallelizing the numerous short requests will not improve tail latency. This paper introduces Few-to-Many (FM) incremental parallelization, which dynamically increases parallelism to reduce tail latency. FM uses request service demand pro-files and hardware parallelism in an offline phase to com-pute a policy, represented as an interval table, which spec-ifies {{when and how}} much software parallelism to add. At runtime, FM adds parallelism as specified by the interval ta-ble indexed by dynamic system load and request execution time progress. The longer a request executes, the more paral-lelism FM adds. We evaluate FM in Lucene, an open-source enterprise search engine, and in Bing, a commercial Web search engine. FM improves the 99 th percentile response time up to 32 % in Lucene and up to 26 % in Bing, compared to prior state-of-the-art parallelization. Compared to running requests sequentially in Bing, FM improves tail latency {{by a factor of}} two. These results illustrate that incremental paral-lelism is a powerful tool for reducing tail latency...|$|R
40|$|IQ vs. OQ {{crossbar}} switches z In input-queued (IQ), buffers which queue packets at the inputs {{need only}} run {{twice as fast}} the line rates. z If time were slotted so that at most one packet arrived at each input of the switch per time slot, then an input buffer potentially needs to make up to two transactions per time slot: (1) write in an incoming packet, and (2) copy a buffered packet onto the crossbar fabric z the buffers of an N x N output-queued (OQ) switch are required to run at least N + 1 times the line rate IQ vs. OQ crossbar switches (cont) z IQ switches which maintain a single first-in-first-out (FIFO) buffer at the inputs are known to suffer from the so-called head-of-line (HoL) blocking problem z this problem can limit the throughput of the switch to about 58 % when the input traffic is independent, identically distributed (i. i. d.) Bernoulli and the output destinations are uniform [12] z OQ switches which always deliver 100 % throughput, since no output will idle {{as long as there}} is a packet in the switch destined for it IQ HOL Blocking problem z The low throughput of IQ switches is merely an artifact of HoL blocking caused due to a FIFO organization of the input buffers, and that IQ switches can achieve a throughput of up to 100 % by using “virtual output queuing ” and suitable packet scheduling algorithms. z However, all of these results are shown to hold only when the input traffic is i. i. d. although they allow a non-uniform loading of the switch. ([13], [14], [15]) z It has been believed for some time now that an IQ switch can deliver 100 % throughput for arbitrarily distributed input patterns so long as no input or output is oversubscribed. ([13], [14], [15]) First result of paper z These results ought to be true for a wider class of input distributions and that the i. i. d. assumption is only required by their method of proof. z Theorem 1, provides a proof of this belief using fluid model techniques. More precisely, Theorem 1 proves that an IQ switch using a maximum weight matching algorithm can achieve a throughput of up to 100 % when subjected to arbitrarily distributed input traffic that satisfies the following mild conditions: (i) It obeys the strong law of large numbers, and (ii) it does not <b>oversubscribe</b> any input or output...|$|E
5000|$|A cable hub is a {{building}} or shelter usually {{smaller than a}} headend that takes an already processed video signal from a headend and transmits it to a local community. (or multiple communities) Most cable hubs are {{used in conjunction with}} an HFC Plant. Combined IP video and data enters a hub via a microwave or fiber optic transport circuit and gets routed to QAM devices such as a Cable modem termination system which changes the IP data into an RF QAM to be combined with other services (such as video on demand, switched video) and transmitted to the subscriber. The RF from each service gets combined in the hub to ultimately a single coax cable broken down per node, but right before it leaves the hub to feed customers, gets changed to fiber optic light to feed local cable nodes which may cover a large building, a neighborhood or in rural areas, an entire community. The cable node located in the field in-turn reverses the optical light from the hub and changes the signals back to RF over coaxial cable. This is called a [...] "forward" [...] path (download). The inverse happens on the upload or [...] "return" [...] path as customers transmit data back to a hub. Cable nodes were initially intended to reduce amplifier cascade and improve signal quality to subscribers distant from a hub. Modern cable nodes still serve the same purpose for amplifier cascade reduction, but now are strategically placed in areas of high data density to better allocate bandwidth availability and reduce <b>oversubscribing</b> in a particular area. Cable nodes also allow for multiple channel lineups or public access markets out of the same hub.|$|R
40|$|In Germany, as {{in almost}} all {{industrial}} countries, active pharmaceutical substances can now be found in virtually all water bodies and occasionally also in drinking water. Even though the concentrations in question {{tend to be very}} low, there are initial signs of their impact on aquatic life. There is no evidence as yet of any acute consequences for human health. It is, however, impossible to rule out long-term consequences from these minimal concentrations or unexpected effects from the interaction between various active ingredients (cocktail effect). At special risk here are sensitive segments of the population such as children and the chronically ill. There is thus a need for action on precautionary grounds. The main actors in the health system are largely unaware of the problem posed by drug residues in water. Although knowledge cannot be equated with awareness – given the existence of the ‘not wanting to know' phenomenon – {{the first step is to}} generate a consolidated knowledge base. Only by creating awareness of the problem can further strategies be implemented to ultimately enlighten and bring about behavioural change. At stake here is the overall everyday handling of medications, including prescription, compliance, and drug-free disease prevention down to the doctor-patient relationship. The latter, namely, is often characterised by misunderstandings and a lack of communication about the – supposed – need to prescribe drugs. The first part of the strategy for the general public involves using various channels and media to address three different target groups. These were identified by ISOE in an empirical survey as reacting differently to the problem under review: · ‘The Deniers/Relativists' · ‘The Truth-Seekers' · ‘The Hypersensitives' The intention is to address each target group in the right tone and using the most suitable line of reasoning via specific media and with the proper degree of differentiation. The ‘Truth-Seekers' play an opinion-leading role here. They can be provided with highly differentiated information through sophisticated media which they then pass on to their dialogue partners in an appropriate form. The second part of the strategy for the general public relates to the communication of proper disposal routes for expired drugs. The goal is to confine disposal to pharmacies so that on no account are they flushed down the sink or toilet. Based on an analysis of typical errors in existing communications media on this topic, ISOE prepared recommendations for drafting proper information materials. In addressing pharmacists, the first priority is to convey hard facts: to this end we propose a PR campaign to place articles in the main specialist media. At the same time, the subject should feature in training and continuing education programmes. Another aim is to strengthen the advisory function of the pharmacies. The environmentally sensitive target group would indeed react positively to having their attention drawn to the issue of drug residues in water. For all other customers, the pharmacists can and should act as consultants: they emphasise how important it is to take medication as instructed (compliance) and use suitable pack sizes, and warn older customers in particular about the potential hazards of improper drug intake. The first stage of the communications strategy for doctors likewise revolves around knowledge. Here, however, it is important to take into account their self-image as scientists while in fact having little grasp of this specific area. The line to take is that of ‘discursive selfenlightenment'. This means that the issue of drug residues in water cannot be conveyed to doctors by laymen but must be taken up and imparted via the major media of the medical profession and by medical association officials (top-down). The second stage, namely that of raising doctors’ awareness of the problem, is likely to encounter strong resistance from some of the medical profession. They may fear a threat of interference in treatment plans from an environmental perspective and feel the need to emphasise that doctors are not responsible for environmental issues. As shown in empirical surveys by ISOE, such a defensive reaction is ultimately down to an underlying taboo: people are loath to discuss the over-prescription taking place in countless doctors' surgeries. And it is a fact that this problem cannot be tackled from the environmental perspective, although the goals of water protection are indeed consistent with the economic objectives of restraint in the deployment of drugs. Any communications measure for this target group has to bear in mind that doctors feel restricted by what they see as a ‘perpetual health reform' no matter which government is in power. On no account are they prepared to tolerate any new form of regulation, in this case for environmental reasons. An entirely different view of the problem is taken by ‘critical doctors' such as specialists in environmental health and those with a naturopathic focus. They are interested in the problem because they see a connection between the quality of our environment and our health. What is more, they have patients keen to be prescribed as few drugs as possible and who are instead interested in ‘talking medicine'. So, any communication strategy intent on tackling the difficult problem of <b>oversubscribing</b> drugs needs to look carefully at the experiences of these medical professionals and also at a ‘bottom-up strategy'. Implementation of strategic communications should be entrusted to an agency with experience in ‘issue management'. Knowledge of social marketing and the influencing of behaviour are further prerequisites. All important decisions should be taken by a consensus committee (‘MeriWa' 1 round table), in which the medical profession, pharmacists and consumers are represented. In Deutschland und in fast allen Industrieländern finden sich mittlerweile Medikamentenwirkstoffe in nahezu allen Gewässern und vereinzelt auch im Trinkwasser. Auch wenn die Konzentrationen in der Regel sehr gering sind, lassen sich erste Anzeichen für Auswirkungen auf Wasserlebewesen nachweisen. Akute Folgen für die menschliche Gesundheit sind bisher nicht erwiesen. Es kann allerdings nicht ausgeschlossen werden, dass sich Langzeitfolgen dieser Niedrigstkonzentrationen entwickeln und unerwartete Effekte durch die Wechselwirkung zwischen verschiedenen Wirkstoffen (Cocktaileffekt) entstehen. Besonders gefährdet sind dabei sensible Bevölkerungsgruppen wie Kinder und chronisch Kranke. Es besteht daher nicht zuletzt aus Vorsorgegründen Handlungsbedarf. Das Problem der Medikamentenreste im Wasser ist bei den wichtigsten Akteuren des Gesundheitssystems weitgehend unbekannt. Auch wenn Wissen nicht mit Bewusstsein gleichgesetzt werden kann – denn es gibt auch das Phänomen des Nicht-Wissen-Wollens – geht es in einem ersten Schritt darum, fundiertes Wissen zu erzeugen. Nur auf Basis dieser Sensibilisierung können weitere Strategien umgesetzt und letztendlich Aufklärung und Verhaltensänderungen erreicht werden. Dabei geht es um die gesamte Alltagspraxis im Umgang mit Medikamenten. Diese umfasst Fragen der Verschreibung, der Compliance, der nichtmedikamentösen Krankheitsvorsorge bis hin zum Arzt-Patienten-Verhältnis. Das ist nämlich häufig von Missverständnissen und mangelnder Kommunikation über – vermeintliche – Verschreibungsnotwendigkeiten geprägt. Der erste Teil der Strategie für die Bevölkerung soll über unterschiedliche Kanäle und Medien drei unterschiedliche Zielgruppen ansprechen, die in einer empirischen Untersuchung vom ISOE identifiziert wurden und auf das angesprochene Problem ganz unterschiedlich reagieren: · ‚Die Verleugner/Relativierer‘ · ‚Die Aufklärungsinteressierten‘ · ‚Die Hypersensiblen‘ Jede Zielgruppe soll in der passenden sprachlichen und argumentativen Art und Weise durch spezifische Medien und mit dem richtigen Grad der Differenziertheit angesprochen werden. Dabei spielen „die Aufklärungsinteressierten“ eine Opinionleader-Rolle. Sie können über anspruchsvolle Medien mit sehr differenzierten Informationen versorgt werden und geben dieses Wissen dann in angemessener Form an ihre Gesprächspartner weiter. Der zweite Teil der Strategie für die Bevölkerung bezieht sich auf die Kommunikation richtiger Entsorgungswege für Altmedikamente. Ziel ist es, dass Medikamentenreste nur noch in der Apotheke, keinesfalls aber in der Spüle oder in der Toilette entsorgt werden. Auf Grundlage einer Analyse typischer Fehler in bereits bestehenden Kommunikationsmedien zu diesem Thema hat das ISOE Empfehlungen zur richtigen Konzeption von Infomaterialien erarbeitet. Bei der Ansprache der Apotheker geht es in einem ersten Schritt um die Vermittlung von Faktenwissen: Wir schlagen dazu eine PR-Kampagne vor, die Artikel in den wichtigsten Fachmedien platziert. Gleichzeitig soll das Thema auch Teil der Aus- und Fortbildung werden. Zusätzlich soll die Beraterfunktion der Apotheken gestärkt werden. Die spezielle Zielgruppe der umweltsensiblen Kunden würde durchaus positiv darauf reagieren, wenn sie auf die Problematik der Medikamentenreste im Wasser hingewiesen würde. Bei allen anderen Kunden können und sollen die Apotheker ihre Rolle als Berater wahrnehmen: Sie betonen, wie wichtig die korrekte Einnahme (Compliance) und adäquate Packungsgrößen sind und warnen ihre Kunden, insbesondere die älteren, auch vor potenziellen Fehleinnahmen. Bei der Kommunikationsstrategie für Ärzte geht es im ersten Schritt ebenfalls um Wissen. Dabei muss aber deren Selbstverständnis als Wissenschaftler bei gleichzeitig niedrigem Wissensstand in diesem speziellen Feld berücksichtigt werden. Hier muss der Weg einer ‚diskursiven Selbstaufklärung‘ beschritten werden. Das Thema Medikamentenreste im Wasser kann somit nicht von Laien von außen an die Ärzte herangetragen werden, sondern muss in wichtigen Medien der Ärzteschaft und durch Verbandsfunktionäre angenommen und kommuniziert werden (top-down). Wenn es im zweiten Schritt um eine Problemsensibilisierung geht, muss mit starkem Widerstand eines Teils der Ärzteschaft gerechnet werden. Sie könnten fürchten, dass eine Einmischung in Heilungspläne aus Umweltsicht droht und betonen, dass Ärzte nicht für Umweltfragen zuständig seien. Letztlich steht – das haben empirische Untersuchungen des ISOE gezeigt – hinter dieser Problemabwehr ein Tabu: Es soll nicht darüber gesprochen werden, dass in zahlreichen Praxen zu viel verschrieben wird. Diese Problematik kann tatsächlich nicht aus der Umweltperspektive angegangen werden. Doch decken sich hier die Ziele des Gewässerschutzes mit den ökonomischen Zielen eines sparsamen Umgangs mit Arzneimitteln. Bei jeder Kommunikationsmaßnahme für diese Zielgruppe muss berücksichtigt werden, dass sich die Ärzte von dem, was sie als ‚Dauergesundheitsreform‘ aller Regierungen wahrnehmen, gegängelt fühlen. Sie sind keinesfalls bereit, eine neue Form der Regulierung, diesmal aus Umweltgründen, hinzunehmen. Ganz anders wird das Problem von ‚kritischen Ärzten‘ wie Umweltmedizinern und von Ärzten mit Naturheilschwerpunkt gesehen. Sie interessieren sich für die Problematik, weil sie einen Zusammenhang zwischen Umweltqualität und Gesundheit sehen. Außerdem haben sie Patienten, die an möglichst wenig Medikamentenverschreibungen, dafür aber an einer ‚sprechenden Medizin‘ interessiert sind. Wenn eine Kommunikationsstrategie also auch das schwierige Problem der übermäßigen Verschreibungen angehen will, empfiehlt es sich, die Erfahrungen dieser Mediziner einzubeziehen und zusätzlich auf eine ‚Bottom-up-Strategie‘ abzuzielen. Mit der Umsetzung der strategischen Kommunikation sollte eine Agentur beauftragt werden, die Erfahrungen im ‚Issue Management‘ vorweisen kann. Weiterhin sollte die Agentur Kenntnisse im Social Marketing und der Beeinflussung von Verhalten haben. Alle wichtigen Entscheidungen sollten von einem Konsens-Gremium (Runder Tisch ‚MeriWa‘ 1) verabschiedet werden, in dem die Ärzteschaft, die Apotheker sowie die Verbraucherinnen und Verbraucher angemessen repräsentiert sind...|$|R

