10000|15|Public
25|$|Broadwell, George A. 2001. Optimal {{order and}} pied-piping in San Dionicio Zapotec. in Peter Sells, ed. Formal and Empirical Issues in <b>Optimality</b> Theoretic Syntax, pp.197–123. Stanford: CSLI Publications.|$|E
25|$|All these <b>optimality</b> {{criteria}} are distance based measures, {{and do not}} always correspond to more intuitive notions of closeness, so more visual criteria have been developed {{in response to this}} concern.|$|E
25|$|Neoclassical {{economics}} is sometimes criticized {{for having a}} normative bias. In this view, it does not focus on explaining actual economies, but instead on describing a theoretical world in which Pareto <b>optimality</b> applies.|$|E
25|$|This {{estimator}} {{reaches the}} Cramér–Rao {{bound for the}} model, and thus is optimal {{in the class of}} all unbiased estimators. Note that unlike the Gauss–Markov theorem, this result establishes <b>optimality</b> among both linear and non-linear estimators, but only in the case of normally distributed error terms.|$|E
25|$|Turning his {{attention}} to game theory, Ho in 1965 published the paper Differential Games and Optimal Pursuit-Evasion Strategies, that proved the <b>optimality</b> of a proportional guidance scheme. His paper Nonzero Sum Differential Games was an influential game-theoretic study in systems and control, following earlier work by Samuel Karlin, for example.|$|E
25|$|In the 1950s and 1960s, {{the problem}} became {{increasingly}} popular in scientific circles in Europe and the USA after the RAND Corporation in Santa Monica offered prizes for steps {{in solving the}} problem. Notable contributions were made by George Dantzig, Delbert Ray Fulkerson and Selmer M. Johnson from the RAND Corporation, who expressed the problem as an integer linear program and developed the cutting plane method for its solution. They wrote what is considered the seminal paper on the subject in which with these new methods they solved an instance with 49 cities to <b>optimality</b> by constructing a tour and proving that no other tour could be shorter. Dantzig, Fulkerson and Johnson, however, speculated that given a near optimal solution {{we may be able}} to find <b>optimality</b> or prove <b>optimality</b> by adding a small amount of extra inequalities (cuts). They used this idea to solve their initial 49 city problem using a string model. They found they only needed 26 cuts to come to a solution for their 49 city problem. While this paper did not give an algorithmic approach to TSP problems, the ideas that lay within it were indispensable to later creating exact solution methods for the TSP, though it would take 15 years to find an algorithmic approach in creating these cuts. As well as cutting plane methods, Dantzig, Fulkerson and Johnson used branch and bound algorithms perhaps for the first time.|$|E
25|$|Macroeconomics {{influenced}} the neoclassical synthesis {{from the other}} direction, undermining foundations of classical economic theory such as Say's Law, and assumptions about political economy such as the necessity for a hard-money standard. These developments are reflected in neoclassical theory by {{the search for the}} occurrence in markets of the equilibrium conditions of Pareto <b>optimality</b> and self-sustainability.|$|E
25|$|The {{desirability}} of custom templates for non-compact group is {{in conflict with}} the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex. The <b>optimality</b> of Gabor templates for translations and scales is a possible explanation of this phenomenon.|$|E
25|$|The Pareto optimum {{provision}} {{of a public}} good in a society is at the level where the combined sum of the marginal rate of substitution between private goods and a given public good of all individuals {{is equal to the}} marginal rate of transformation. This contrasts to the Pareto <b>optimality</b> condition of private goods, in which each consumers' marginal rate of substitution is equal; as is the societies marginal rate of transformation.|$|E
25|$|While the {{admissibility}} criterion guarantees {{an optimal}} solution path, {{it also means}} that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of <b>optimality</b> by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + ε) times the optimal solution path. This new guarantee is referred to as ε-admissible.|$|E
25|$|Within lexical semantics, {{especially}} {{as applied to}} computers, modeling word meaning is easier when a given word is {{understood in terms of}} related words; semantic networks are therefore important in computational linguistics. Still, other methods in phonology (e.g. <b>optimality</b> theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as , as well as various 'Net' projects, such as WordNet, VerbNet, and others.|$|E
500|$|Before {{creating}} a tablebase, a programmer must choose a metric of <b>optimality</b> – in other words, he must define {{at what point}} a player has [...] "won" [...] the game. [...] Every position can be defined by its distance (i.e. the number of moves) from the desired endpoint. [...] Two metrics are generally used: ...|$|E
500|$|Research {{into the}} {{theoretical}} phonology of Irish began with , which follows {{the principles and}} practices of The Sound Pattern of English and which {{formed the basis of}} the phonology sections of [...] Dissertations examining Irish phonology from a theoretical point of view include , [...] in <b>optimality</b> theory, and [...] and [...] in government phonology.|$|E
500|$|In mathematics, {{mathematical}} optimization (or optimization or mathematical programming) {{refers to}} {{the selection of a}} best element from some set of available alternatives. In the simplest case, an optimization problem [...] involves maximizing or minimizing a real function by selecting input values of the function and computing the corresponding values of the function. The solution process includes satisfying general necessary and sufficient conditions for <b>optimality.</b> For optimization problems, specialized notation may be used as to the function and its input(s). More generally, optimization includes finding the best available element of some function given a defined domain and may use a variety of different [...] computational optimization techniques.|$|E
2500|$|Application of <b>optimality</b> {{modeling}} {{to elucidate}} {{the degree of}} adaptation ...|$|E
2500|$|In <b>Optimality</b> Theory, grammars are logically {{equivalent}} to antimatroids (...) [...]|$|E
2500|$|... is a paraphrasing of Bellman's famous Principle of <b>Optimality</b> in {{the context}} of the {{shortest}} path problem.|$|E
2500|$|The most {{commonly}} used <b>optimality</b> criterion for selecting a bandwidth matrix is the MISE or mean integrated squared error ...|$|E
2500|$|... optimization, {{finding the}} [...] "best" [...] {{structure}} or solution among several possibilities, {{be it the}} [...] "largest", [...] "smallest" [...] or satisfying some other <b>optimality</b> criterion.|$|E
2500|$|There are {{alternative}} <b>optimality</b> criteria, which {{attempt to}} cover cases where MISE {{is not an}} appropriate measure. The equivalent L1 measure, Mean Integrated Absolute Error, is ...|$|E
2500|$|Broadwell, George A. 2001. [...] "Optimal {{order and}} pied-piping in San Dionicio Zapotec." [...] In Peter Sells, ed. Formal and Empirical Issues in <b>Optimality</b> Theoretic Syntax, pp.197–123. Stanford: CSLI Publications.|$|E
2500|$|... •	"The role {{of money}} in {{supporting}} the Pareto <b>optimality</b> of competitive equilibrium in consumption-loan type models" [...] (with M. Okuno and I. Zilcha). J Econ Theory 20, 41-80 (1979).|$|E
2500|$|Methods {{to obtain}} {{suitable}} ({{in some sense}}) natural extensions of optimization problems that otherwise lack of existence or stability of solutions to obtain problems with guaranteed existence of solutions and their stability in some sense (typically under various perturbation of data) are in general called relaxation. Solutions of such extended (=relaxed) problems in some sense characterizes (at least certain features) of the original problems, e.g. {{as far as their}} optimizing sequences concerns. Relaxed problems may also possesses their own natural linear structure that may yield specific <b>optimality</b> conditions different from <b>optimality</b> conditions for the original problems.|$|E
2500|$|... in {{the sense}} that this is a nonnegative-definite matrix. This theorem {{establishes}} <b>optimality</b> only in the class of linear unbiased estimators, which is quite restrictive. Depending on the distribution of the error terms ε, other, non-linear estimators may provide better results than OLS.|$|E
2500|$|In {{the time}} after this seminal work in GEI, Cass's various papers dealt {{with issues of}} determinacy of {{equilibrium}} (and the closely related issue of existence of sunspot equilibria), and with the <b>optimality</b> of allocations {{in the presence of}} sunspots and incomplete asset markets. [...] These papers include: ...|$|E
2500|$|Amartya Sen offered both {{relaxation}} of transitivity and {{removal of the}} Pareto principle. He demonstrated another interesting impossibility result, known as the [...] "impossibility of the Paretian Liberal" [...] (see liberal paradox for details). Sen went on to argue that this demonstrates the futility of demanding Pareto <b>optimality</b> in relation to voting mechanisms.|$|E
2500|$|A {{statistical}} hypothesis test compares a test statistic (z or t for examples) to a threshold. The test statistic (the formula {{found in the}} table below) is based on <b>optimality.</b> For a fixed level of Type I error rate, use of these statistics minimizes Type II error rates (equivalent to maximizing power). The following terms describe tests in terms of such optimality: ...|$|E
2500|$|The {{two forms}} of {{hypothesis}} testing are based on different problem formulations. The original test is analogous to a true/false question; the Neyman–Pearson test is more like multiple choice. In the view of Tukey the former produces a conclusion {{on the basis of}} only strong evidence while the latter produces a decision on the basis of available evidence. While the two tests seem quite different both mathematically and philosophically, later developments lead to the opposite claim. Consider many tiny radioactive sources. The hypotheses become 0,1,2,3... grains of radioactive sand. There is little distinction between none or some radiation (Fisher) and 0 grains of radioactive sand versus all of the alternatives (Neyman–Pearson). The major Neyman–Pearson paper of 1933 also considered composite hypotheses (ones whose distribution includes an unknown parameter). An example proved the <b>optimality</b> of the (Student's) t-test, [...] "there can be no better test for the hypothesis under consideration" [...] (p 321). Neyman–Pearson theory was proving the <b>optimality</b> of Fisherian methods from its inception.|$|E
2500|$|This {{necessary}} condition for <b>optimality</b> conveys a fairly simple economic principle. [...] In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are [...] "leftovers"), then additional quantities of that resource must have no value. [...] Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no [...] "leftovers").|$|E
2500|$|Proof: Assume [...] and , and [...] [...] We have to {{show that}} [...] and [...] We only need to {{consider}} the case where [...] [...] Since , we obtain , which guarantees that [...] [...] Furthermore, [...] so that [...] [...] If not, [...] which implies (by single crossing differences) that , contradicting the <b>optimality</b> of [...] at [...] To show the necessity of single crossing differences, set , where [...] Then [...] for any [...] guarantees that, if , then [...] Q.E.D.|$|E
2500|$|The Kaldor {{criterion}} is that {{an activity}} moves the economy closer to Pareto <b>optimality</b> if the maximum amount the gainers are prepared to pay to the losers {{to agree to the}} change is greater than the minimum amount losers are prepared to accept; the Hicks criterion {{is that an}} activity moves the economy toward Pareto <b>optimality</b> if the maximum amount the losers would pay the gainers to forgo the change is less than the minimum amount the gainers would accept to so agree. Thus, the Kaldor test supposes that losers could prevent the arrangement and asks whether gainers value their gain so much they would and could pay losers to accept the arrangement, whereas the Hicks test supposes that gainers are able to proceed with the change and asks whether losers consider their loss to be worth less than what it would cost them to pay gainers to agree not to proceed with the change. After several technical problems with each separate criterion were discovered, they were combined into the Scitovsky criterion, more commonly known as the [...] "Kaldor–Hicks criterion", which does not share the same flaws.|$|E
2500|$|... {{where the}} [...] term may be either added or subtracted. If [...] is {{a maximum of}} [...] for the {{original}} constrained problem, then there exists [...] such that [...] is a stationary point for the Lagrange function (stationary points are those points where the partial derivatives of [...] are zero). However, not all stationary points yield a solution of the original problem. Thus, the method of Lagrange multipliers yields {{a necessary condition for}} <b>optimality</b> in constrained problems. Sufficient conditions for a minimum or maximum also exist.|$|E
2500|$|Finding local maxima of a {{function}} [...] {{is done by}} finding all points [...] such that [...] then checking whether all the eigenvalues of the Hessian [...] are negative. (Note that the maxima may not exist even if [...] is continuous because [...] is open, and also note that the conditions checked here are sufficient but not necessary for <b>optimality.)</b> Setting [...] is a non-linear problem and in general, arbitrarily difficult. After finding the critical points, checking the eigenvalues is a linear problem and thus easy.|$|E
2500|$|In {{computer}} science, Hirschberg's algorithm, {{named after}} its inventor, Dan Hirschberg, {{is a dynamic}} programming algorithm that finds the optimal sequence alignment between two strings. <b>Optimality</b> is measured with the Levenshtein distance, defined to be {{the sum of the}} costs of insertions, replacements, deletions, and null actions needed to change one string into the other. [...] Hirschberg's algorithm is simply described as a more space efficient version of the NeedlemanWunsch algorithm that uses divide and conquer. [...] Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.|$|E
2500|$|As there usually exist {{multiple}} Pareto optimal {{solutions for}} multi-objective optimization problems, {{what it means}} to solve such a problem is not as straightforward as it is for a conventional single-objective optimization problem. Therefore, different researchers have defined the term [...] "solving a multi-objective optimization problem" [...] in various ways. This section summarizes some of them and the contexts in which they are used. Many methods convert the original problem with multiple objectives into a single-objective optimization problem. This is called a scalarized problem. If scalarization is done neatly, Pareto <b>optimality</b> of the solutions obtained can be guaranteed.|$|E
2500|$|Allen Newell and Herbert A. Simon {{who used}} what John McCarthy calls an [...] "approximation" [...] in 1958 wrote that alpha–beta [...] "appears {{to have been}} reinvented a number of times". Arthur Samuel had an early version and Richards, Hart, Levine and/or Edwards invented alpha–beta {{independently}} in the United States. McCarthy proposed similar ideas during the Dartmouth Conference in 1956 and suggested it {{to a group of}} his students including Alan Kotok at MIT in 1961. Alexander Brudno independently conceived the alpha–beta algorithm, publishing his results in 1963. Donald Knuth and Ronald W. Moore refined the algorithm in 1975 and Judea Pearl proved its <b>optimality</b> in 1982.|$|E
