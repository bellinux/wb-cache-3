866|877|Public
25|$|Solubility is a {{property}} of interest in many aspects of science, including but not limited to: environmental predictions, biochemistry, pharmacy, drug-design, agrochemical design, and protein ligand binding. Aqueous solubility is of fundamental interest owing to the vital biological and transportation functions played by water. In addition, to this clear scientific interest in water solubility and solvent effects; accurate predictions of solubility are important industrially. The ability to accurately predict a molecules solubility represents potentially large financial savings in many chemical product development processes, such as pharmaceuticals. In the pharmaceutical industry, solubility predictions form part of the early stage lead <b>optimisation</b> <b>process</b> of drug candidates. Solubility remains a concern {{all the way to}} formulation. A number of methods have been applied to such predictions including quantitative structure–activity relationships (QSAR), quantitative structure–property relationships (QSPR) and data mining. These models provide efficient predictions of solubility and represent the current standard. The draw back such models is that they can lack physical insight. A method founded in physical theory, capable of achieving similar levels of accuracy at an sensible cost, would be a powerful tool scientifically and industrially.|$|E
50|$|A {{significant}} {{problem is that}} unfriendly artificial intelligence {{is likely to be}} much easier to create than friendly AI. While both require large advances in recursive <b>optimisation</b> <b>process</b> design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not automatically destroy the human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which {{does not need to be}} invariant under self-modification.|$|E
5000|$|A {{simulation}} {{project of}} a developed field, usually requires [...] "history matching" [...] where historical field production and pressures are compared to calculated values.It was realised {{at an early}} stage that this was essentially an <b>optimisation</b> <b>process,</b> corresponding to Maximum Likelihood. As such, it can be automated, and there are multiple commercial and software packages designed to accomplish just that. The model's parameters are adjusted until a reasonable match is achieved on a field basis and usually for all wells. Commonly, producing water cuts or water-oil ratios and gas-oil ratios are matched.|$|E
3000|$|... 0 of the N {{available}} participating relays {{are active}} {{at any given}} time. The proposed algorithm uses multi-level <b>optimisation</b> <b>processes,</b> and hence the constructed code outperforms the few existing STBC-SM codes.|$|R
50|$|TCS Innovation Lab, Insurance - Chennai: IT <b>Optimisation,</b> Business <b>Process</b> <b>Optimisation,</b> Customer Centricity Enablers, Enterprise Mobility, Telematics, Innovation in Product Development and Management (Product {{lifecycle}} management) in Insurance.|$|R
40|$|This master's {{thesis is}} engaged in <b>optimisation</b> <b>processes</b> of PUQ (as for Purchasing Quality) {{department}}, Robert Bosch České Budějovice. This department is responsible for input control of parts for manufacture as well as check of new parts including their documentation and documentation updates. By optimisation we understand selection of the best variant from group of possibilities...|$|R
5000|$|The {{invasion}} exponent [...] {{is defined}} as the expected growth rate of an initially rare mutant in the environment set by the resident (r), which means the frequency of each phenotype (trait value) whenever this suffices to infer all other aspects of the equilibrium environment, such as the demographic composition and theavailability of resources. For each r, the invasion exponent {{can be thought of as}} the fitness landscape experienced by an initially rare mutant. The landscape changes with each successful invasion, as is the case in evolutionary game theory, but in contrast with the classical view of evolution as an <b>optimisation</b> <b>process</b> towards ever higher fitness.|$|E
5000|$|... {{notes that}} there is no direct {{evolutionary}} motivation for an AI to be friendly to humans. Evolution has no inherent tendency to produce outcomes valued by humans, and there is little reason to expect an arbitrary <b>optimisation</b> <b>process</b> to promote an outcome desired by mankind, rather than inadvertently leading to an AI behaving in a way not intended by its creators (such as Nick Bostrom's whimsical example of an AI which was originally programmed with the goal of manufacturing paper clips, so that when it achieves superintelligence it decides to convert the entire planet into a paper clip manufacturing facility). Anders Sandberg has also elaborated on this scenario, addressing various common counter-arguments. AI researcher Hugo de Garis suggests that artificial intelligences may simply eliminate the human race for access to scarce resources, and humans would be powerless to stop them. Alternatively, AIs developed under evolutionary pressure to promote their own survival could outcompete humanity.|$|E
50|$|Solubility is a {{property}} of interest in many aspects of science, including but not limited to: environmental predictions, biochemistry, pharmacy, drug-design, agrochemical design, and protein ligand binding. Aqueous solubility is of fundamental interest owing to the vital biological and transportation functions played by water. In addition, to this clear scientific interest in water solubility and solvent effects; accurate predictions of solubility are important industrially. The ability to accurately predict a molecules solubility represents potentially large financial savings in many chemical product development processes, such as pharmaceuticals. In the pharmaceutical industry, solubility predictions form part of the early stage lead <b>optimisation</b> <b>process</b> of drug candidates. Solubility remains a concern {{all the way to}} formulation. A number of methods have been applied to such predictions including quantitative structure-activity relationships (QSAR), quantitative structure-property relationships (QSPR) and data mining. These models provide efficient predictions of solubility and represent the current standard. The draw back such models is that they can lack physical insight. A method founded in physical theory, capable of achieving similar levels of accuracy at an sensible cost, would be a powerful tool scientifically and industrially.|$|E
40|$|This study {{represents}} one of the first comprehensive paediatric <b>optimisation</b> <b>processes</b> that were systematically approached via three main objectives. It is expected that the devised optimisation paediatric protocols could have profound implications on clinical practices that use 64 -slice CT or greater. The study provides {{a better understanding of the}} influence of each acquisition reconstruction parameter with regards to image quality and radiation dose during routine paediatric imaging protocols...|$|R
40|$|This paper {{describes}} {{some studies}} in Human-Computer Interaction for Directed Graph Drawing. We {{have developed a}} system where users can help some standard graph drawing algorithms to produce nice drawings of a graph according {{to a set of}} aesthetic criteria. The system follows a general framework for interaction with <b>optimisation</b> <b>processes</b> that can be applied to many optimisation problems. Some discussion about the framework and possible improvements is presented...|$|R
40|$|While design {{processes}} in certain domains have shifted towards early adoption of simulation and virtualisation tech-niques, processes monitoring and reuse is not well-integrated into current development practices. We introduce a frame-work to integrate Multidisciplinary Design <b>Optimisation</b> <b>processes</b> using ontological engineering, where artefact and simulation models are exploited to yield more effec-tive optimisation-driven development. We show how meta-modelling techniques can overcome representational and semantic differences between analysis disciplines and execu-tion environments. 1...|$|R
50|$|Some {{computer}} languages enable (or even require) assertions as to {{the usage}} of parameters, and might further offer the opportunity to declare that variables have their values restricted to some set (for instance, 6 < x ≤ 28) thus providing further grist for the <b>optimisation</b> <b>process</b> to grind through, and also providing worthwhile checks on the coherence of the source code to detect blunders. But this is never enough - only some variables can be given simple constraints, while others would require complex specifications: how might it be specified that variable P {{is to be a}} prime number, and if so, is or is not the value 1 included? Complications are immediate: what are the valid ranges for a day-of-month D given that M is a month number? And are all violations worthy of immediate termination? Even if all that could be handled, what benefit might follow? And at what cost? Full specifications would amount to a re-statement of the program's function in another form and quite aside from the time the compiler would consume in processing them, they would thus be subject to bugs. Instead, only simple specifications are allowed with run-time range checking provided.|$|E
40|$|We {{present an}} {{approach}} for deploying and subsequently managing a virtual network overlay, which is tailored to an end-user’s request. Our approach combines a binary integer <b>optimisation</b> <b>process</b> {{to decide on}} the number and placement of virtual routers, and an autonomic network management system that subsequently manages the configuration of the running virtual network. High-level optimisation policies are used to guide the <b>optimisation</b> <b>process</b> to identify a virtual network that favours lower hosting costs or higher network quality (we use mean delays as a quality metric). Low-level deployment policies are generated and used to govern the deployment and management of the virtual networks. Our results indicate that the binary integer <b>optimisation</b> <b>process</b> produces a virtual network that has lower cost as compared to creating a network based on combined shortest paths. Peer ReviewedPostprint (published version...|$|E
40|$|Abstract. This paper {{presents}} a novel computational visual grouping method, termed pulling, pushing and grouping, or PPG for short. Visual grouping is formulated as a functional <b>optimisation</b> <b>process.</b> Our computational function has three terms, the first pulls similar visual cues together, the second pushes different visual cues apart, {{and the third}} groups spatially adjacent visual cues without regarding their visual properties. An efficient numerical algorithm based on the Hopfield neural model is developed for solving the <b>optimisation</b> <b>process.</b> Experimental results on various intensity, colour and texture images demonstrate {{the effectiveness of the}} new method. 1...|$|E
40|$|The {{objective}} of this master’s thesis is to determine methods for improving a company’s business processes without investing in new technology and whether a relatively small company can benefit from investing in technology. This study determines the meaning of <b>process</b> <b>optimisation</b> and {{how it should be}} conducted. Using existing theory and the case of a logistics company operating in Finland, this research attempts to identify hindrances and find opportunities for the company to develop their <b>processes</b> through <b>process</b> <b>optimisation</b> without technology. Different public bodies in Finland (such as the Finnish government and Statistics Finland) have stated that Finnish logistics requires development and have recommended new technology as a solution to the issue. However, the lack of information on the Finnish logistics business sector makes such statements by public bodies difficult to analyse. <b>Process</b> <b>optimisation</b> has been revealed to be more complex than expected. Many theories available today examine and recommend different technological solutions to execute companies’ work processes. However, a theory is needed on how <b>process</b> <b>optimisation</b> can be carried out at a company lacking technology. <b>Process</b> <b>optimisation</b> consists of <b>process</b> modelling and process analysis. Process modelling appears to be the most significant and crucial aspect of <b>process</b> <b>optimisation.</b> Order-to-delivery <b>processes</b> cannot be optimised within a company if the company does not understand the entirety of such processes. Knowledge of the process has been highlighted as being key to understanding a company’s processes at a high level. The case company in this study showed that <b>process</b> <b>optimisation</b> is possible without implementing new technology; instead, optimisation required additional human capital and a stronger focus on a company’s internal business processes. Technology-based solutions for <b>process</b> <b>optimisation</b> are tempting to implement as doing so may be believed to save time, but no automated solution is able to reveal a company’s critical information if the company does not know what it is looking for and cannot identify its problem areas. This research includes a single case study. The results indicate that whether a relatively small company could benefit from investing in technology is unclear, and the lack of research on <b>process</b> <b>optimisation</b> at Finnish companies resulted in limited findings and analysis. Several different scientific articles presented technology implementation successes and failures, but did not reveal information on the steps taken by the companies...|$|R
40|$|Abstract. While optimisation-driven {{design has}} become {{prevalent}} in many engineering dis-ciplines, support for designers to effectively use {{the results of}} simulation processes has not been addressed satisfactorily, since, processes monitoring and reuse of simulation results are not well-integrated into current development practices. We introduce a framework to integrate Multidisciplinary Design <b>Optimisation</b> <b>processes</b> using ontological engineering, where arte-fact and simulation models are exploited to yield more effective optimisation-driven develop-ment. We show how meta-modelling techniques can overcome representational and semantic differences between analysis disciplines and execution environments...|$|R
50|$|Technical and Financial <b>Optimisation</b> of Welding <b>Processes.</b>|$|R
40|$|The optimum {{functional}} characteristics of suspension components, namely, linear/nonlinear spring and nonlinear damper characteristic functions are determined using simple lumped parameter models. A quarter car model {{is used to}} represent the front independent suspension, and a half car model is used to represent the rear solid axle suspension of a light commercial vehicle. The functional shapes of the suspension characteristics used in the <b>optimisation</b> <b>process</b> are based on typical shapes supplied by a car manufacturer. The complexity of a nonlinear function optimisation problem is reduced by scaling it up or down from the aforementioned shape in the <b>optimisation</b> <b>process.</b> The nonlinear optimised suspension characteristics are first obtained using lower complexity lumped parameter models. Then, {{the performance of the}} optimised suspension units are verified using the higher fidelity and more realistic Carmaker model. An interactive software module is developed to ease the nonlinear suspension <b>optimisation</b> <b>process</b> using the Matlab Graphical User Interface tool...|$|E
40|$|Abstract — In {{this article}} we present a new method for the pruning of {{unnecessary}} connections from neural networks created by an evolutionary algorithm (neuro-evolution). Pruning not only decreases {{the complexity of the}} network but also improves the numerical stability of the parameter <b>optimisation</b> <b>process.</b> We show results from experiments where connection pruning is incorporated into EANT 2, an evolutionary reinforcement learning algorithm for both the topology and parameters of neural networks. By analysing data from the evolutionary <b>optimisation</b> <b>process</b> that determines the network’s parameters, candidate connections for removal are identified without the need for extensive additional calculations. I...|$|E
40|$|Abstract—The paper {{presents}} {{dynamic programming}} based model as a planning {{tool for the}} maintenance of electric power systems. Every distribution component has an exponential age depending reliability function to model the fault risk. In the moment of time when the fault costs exceed the investment costs of the new component the reinvestment of the component should be made. However, in some cases the overhauling of the old component may be more economical than the reinvestment. The comparison between overhauling and reinvestment is made by <b>optimisation</b> <b>process.</b> The goal of the <b>optimisation</b> <b>process</b> is to find the cost minimising maintenance program for electric power distribution system. Keywords—Dynamic programming, Electric distribution system, Maintenance. ...|$|E
5000|$|Technical and {{economical}} <b>optimisation</b> of agricultural <b>processes</b> ...|$|R
40|$|Neural Networks can be {{powerful}} tools for the approximation of complex nonlinear behaviour. After a learning phase neural networks {{are able to}} predict results with high accuracy. While the learning phase might be time consuming, prediction is very fast. After substituting complex simulation programmes by Neural Networks, these {{can be used in}} real-time operation, e. g. for <b>optimisation</b> <b>processes.</b> We demonstrate this approach by modelling the operation of a multi-block power plant. Afterwards operation can be optimised using any mixed integer optimisation algorithm, in our case Simulated Annealing...|$|R
40|$|In the decision‐making process, both single‐ and multi‐criteria {{tasks are}} dealt with. In the {{majority}} of cases, {{the selection of a}} solution comes down to determination of the “best” decision (most often based on the subjective assessment) or to organisation of the set of decisions. The Analytic Hierarchy Process (AHP) is one of the methods used for evaluation of qualitative features in the multi‐criteria <b>optimisation</b> <b>processes.</b> This article discusses the possibilities of using the above‐mentioned method, illustrated with an example of purchasing technical equipment for one of the municipal landfill sites in the Silesian Province...|$|R
40|$|Evolutionary Algorithms are {{equipped}} {{with a range of}} adjustable parameters, such as crossover and mutation rates which significantly influence the performance of the algorithm. Practitioners usually do not have the knowledge and time to investigate the ideal parameter values before the <b>optimisation</b> <b>process.</b> Furthermore, different parameter values may be optimal for different problems, and even problem instances. In this work, we present a parameter control method which adjusts parameter values during the <b>optimisation</b> <b>process</b> using the algorithm’s performance as feedback. The approach is particularly effective with continuous parameter intervals, which are adapted dynamically. Successful parameter ranges are identified using an entropy-based clusterer, a method which outperforms state-of-the-art parameter control algorithms...|$|E
30|$|The {{development}} of automatic methods for phantom image quality assessment (and patient image quality assessment) {{together with the}} use of advanced IT technologies (e.g. large-scale archives, data-mining methods, expert system technique) is required for supporting users in the <b>optimisation</b> <b>process.</b>|$|E
40|$|The present paper {{introduces}} a new optimisation approach for the stator of travelling wave ultrasonic motors. The optimisation approach {{is based on}} a model of the ultrasonic motor that includes stator and rotor models as well as a simplified rotor–stator interface model. The motor model is a parametrical one, thus enabling the selection and optimisation of both the stator geometry and the material parameters. Previous to the <b>optimisation</b> <b>process,</b> the motor model is briefly described and experimentally validated. This validation is performed for both the stator model and the motor model. A set of motor performance indicators (the piezoelectric coupling coefficient, the efficiency, and the output power) are also introduced as objective functions in the <b>optimisation</b> <b>process...</b>|$|E
5000|$|<b>Process</b> <b>optimisation</b> for {{coloured}} laser grooved {{buried contact}} solar cells ...|$|R
40|$|Abstract. The {{process of}} design search and {{optimisation}} is characterised by its computationally intensive operations, which produce a problem {{well suited to}} Grid computing. Here we present a Grid enabled computation toolkit that provides transparent and stable access to Grid compute resources from Matlab, which offers comprehensive support for the design <b>optimisation</b> <b>processes.</b> In particular, the access and integration of the Condor resource management system has been achieved by using the toolkit components that are enabled by Web service and service enhancement technologies. The use of the computation toolkit for a four-dimensional CFD parameter study with Matlab and Condor is considered as an exemplar problem...|$|R
40|$|Genetic Algorithms (GAs) {{following}} a parallel master-slave architecture can be effectively {{used to reduce}} searching time when fitness functions have fixed execution time. This paper presents a parallel GA architecture along with two accelerated GA operators to enhance the performance of master-slave GAs, specially when considering fitness functions with variable execution times. We explore {{the performance of the}} proposed approach, and analyse its effectiveness against the state-of-the-art. The results show a significant improvement in search times and fitness function utilisation, thus potentially enabling the use of this approach as a faster searching tool for timing-sensitive <b>optimisation</b> <b>processes</b> such as those found in dynamic real-time systems...|$|R
30|$|The PWMs for RS and FMKL GLD for {{left and}} right {{censoring}} are given in the Appendix. Based on these results, {{it is now possible}} to find a set of parameters of GLD that minimize the sum of the squared difference between sample and derived PWMs using (14). The problem of choosing initial values for the <b>optimisation</b> <b>process</b> is again solved using the method described in Su (2007 b, 2007 a), which involves an extensive randomised search across the parameters using quasi random number generators such as the Sobol or Halton sequence. The initial values used to start the <b>optimisation</b> <b>process</b> will be a randomised set of parameters that best matches the partial probability weighted moments between the sample and the estimated GLD.|$|E
40|$|A {{method for}} {{optimising}} LPC filters in linear prediction based speech coders is described. The <b>optimisation</b> <b>process</b> compensates for errors incurred through coding the excitation signal, providing {{an improvement in}} the quality of the decoded speech, with no increase in bit rate. Peer ReviewedPostprint (published version...|$|E
40|$|AbstractBy using a {{two-stage}} <b>optimisation</b> <b>process</b> we maximise {{the heat}} rate output of afour-parameter axisymmetric direct steam generation cavity receiver. The model includes radiative and hydrodynamic considerations. We {{show that a}} significant range of geometrical shapes show similar efficiencies while having different wall flux and temperature profiles...|$|E
40|$|The {{process of}} design search and {{optimisation}} is characterised by its computationally intensive operations, which produce a problem {{well suited to}} Grid computing. Here we present a Grid enabled computation toolkit that provides transparent and stable access to Grid compute resources from Matlab, which offers comprehensive support for the design <b>optimisation</b> <b>processes.</b> In particular, the access and integration of the Condor resource management system has been achieved by using the toolkit components that are enabled by Web service and service enhancement technologies. The use of the computation toolkit for a four-dimensional CFD parameter study with Matlab and Condor is considered as an exemplar problem...|$|R
30|$|The {{interchangeability}} {{of vehicles}} and trains {{is an important}} requirement for the <b>optimisation</b> of operational <b>processes.</b>|$|R
5000|$|Device Design and <b>Process</b> <b>Optimisation</b> for LGBC Solar Cells for Use Between 50X and 100X Concentration ...|$|R
