166|1142|Public
25|$|The {{coefficients}} {{that bound}} the inequalities in the primal space {{are used to}} compute the objective in the dual space, input quantities in this example. The coefficients used to compute the objective in the primal space bound the inequalities in the dual space, <b>output</b> <b>unit</b> prices in this example.|$|E
25|$|By 1970, as offset {{printing}} began to replace letterpress printing, the Composer would be adapted as the <b>output</b> <b>unit</b> for a typesetting system. The system included a computer-driven input station {{to capture the}} key strokes on magnetic tape and insert the operator's format commands, and a Composer unit to read the tape and produce the formatted text for photo reproduction.|$|E
2500|$|The primal problem {{deals with}} {{physical}} quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce {{so as to}} maximize total revenue? The dual problem deals with economic values. With floor guarantees on all <b>output</b> <b>unit</b> prices, and assuming the available quantity of all inputs is known, what input unit pricing scheme to set so as to minimize total expenditure? ...|$|E
5000|$|Involve three-layer neural {{networks}} with input, hidden and <b>output</b> <b>units</b> ...|$|R
5000|$|... #Subtitle level 2: Lloyd 600 & Alexander: Combined <b>output</b> (<b>units)</b> ...|$|R
5000|$|Unified Shaders : Texture Mapping <b>Units</b> : Render <b>Output</b> <b>Units</b> ...|$|R
2500|$|... where, [...] is the {{learning}} rate, [...] {{is the cost}} (or loss) function and [...] a stochastic term. The choice of the cost function depends on factors such as {{the learning}} type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as [...] where [...] represents the class probability (output of the unit [...] ) and [...] and [...] represent the total input to units [...] and [...] of the same level respectively. Cross entropy is defined as [...] where [...] represents the target probability for <b>output</b> <b>unit</b> [...] and [...] is the probability output for [...] after applying the activation function.|$|E
5000|$|... 1 Unified shaders : {{texture mapping}} unit : render <b>output</b> <b>unit</b> ...|$|E
5000|$|... #Caption: A simple {{neural network}} with two input units and one <b>output</b> <b>unit</b> ...|$|E
5000|$|... 1 Shader Processors : Texture mapping <b>units</b> : Render <b>output</b> <b>units</b> ...|$|R
5000|$|... 1 Unified shaders : Texture mapping <b>units</b> : Render <b>output</b> <b>units</b> ...|$|R
5000|$|... 1Pixel shaders : Vertex shaders : Texture mapping <b>units</b> : Render <b>output</b> <b>units</b> ...|$|R
50|$|Power is {{calculated}} based on rotational speed x torque x constant, with the constant varying with the <b>output</b> <b>unit</b> desired and the input units used.|$|E
5000|$|DropConnect is the {{generalization}} of dropout {{in which each}} connection, rather than each <b>output</b> <b>unit,</b> can be dropped with probability [...] Each unit thus receives input from a random subset of units in the previous layer.|$|E
5000|$|Three {{to eight}} Compute Units (CUs) {{based on the}} revised GCN 2nd gen microarchitecture; 1 Compute Unit (CU) {{consists}} of 64 Unified Shader Processors : 4 Texture Mapping Units (TMUs) : 1 Render <b>Output</b> <b>Unit</b> (ROP) ...|$|E
5000|$|... 1 SPs - Shader Processors - Unified Shaders : Texture mapping <b>units</b> : Render <b>output</b> <b>units</b> ...|$|R
40|$|The {{principle}} of maximizing mutual information {{is applied to}} learning overcomplete and recurrent representations. The underlying model consists {{of a network of}} input units driving a larger number of <b>output</b> <b>units</b> with recurrent interactions. In the limit of zero noise, the network is deterministic and the mutual information can be related to the entropy of the <b>output</b> <b>units.</b> Maximizing this entropy with respect to both the feedforward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters. The conventional independent components (ICA) learning algorithm can be recovered as a special case where there is an equal number of <b>output</b> <b>units</b> and no recurrent connections. The application o...|$|R
40|$|We {{previously}} {{developed a}} template model of primate visual self-motion processing that proposes a {{specific set of}} projections from MT-like local motion sensors onto <b>output</b> <b>units</b> to estimate heading and relative depth from optic flow. At the time, we showed that that the model <b>output</b> <b>units</b> have emergent properties similar to those of MSTd neurons, although there was little physiological evidence to test the model more directly. We have now systematically examined the properties of the model using stimulus paradigms used by others in recent single-unit studies of MST: 1) 2 -D bell-shaped heading tuning. Most MSTd neurons and model <b>output</b> <b>units</b> show bell-shaped heading tuning. Furthermore, we found that most model <b>output</b> <b>units</b> and the finely-sampled example neuron in the Duffy-Wurtz study are well fit by a 2 D gaussian (sigma approx. 35 deg, r approx. 0. 9). The bandwidth of model and real units can explain why Lappe et al. found apparent sigmoidal tuning using a restricted range of stimuli (+/- 40 deg). 2) Spiral Tuning and Invariance. Graziano et al. found that many MST neurons appear tuned to a specific combination of rotation and expansion (spiral flow) and that this tuning changes little for approx. 10 deg shifts in stimulus placement. Simulations of model <b>output</b> <b>units</b> under the same conditions quantitatively replicate this result. We conclude that a template architecture may underlie MT inputs to MST...|$|R
50|$|The {{coefficients}} {{that bound}} the inequalities in the primal space {{are used to}} compute the objective in the dual space, input quantities in this example. The coefficients used to compute the objective in the primal space bound the inequalities in the dual space, <b>output</b> <b>unit</b> prices in this example.|$|E
5000|$|For example, {{below is}} shown a neural network with two input units (i1 and i2), two hidden units (h1 and h2), and one <b>output</b> <b>unit</b> (o1). It has {{a total of}} six {{connections}} with six corresponding weights represented by the numerals 1-6 (for simplicity, the thresholds are all equal to 1 and are omitted): ...|$|E
50|$|By 1970, as offset {{printing}} began to replace letterpress printing, the Composer would be adapted as the <b>output</b> <b>unit</b> for a typesetting system. The system included a computer-driven input station {{to capture the}} key strokes on magnetic tape and insert the operator's format commands, and a Composer unit to read the tape and produce the formatted text for photo reproduction.|$|E
40|$|A new {{algorithm}} for {{feature selection}} {{based on information}} maximization is derived. This algorithm performs subspace mapping from multi-channel signals, where Network Modules (NM) are used to perform the mapping {{for each of the}} channels. The algorithm is based on maximizing the Mutual Information (MI) between input and <b>output</b> <b>units</b> of each NM and between <b>output</b> <b>units</b> of different NMs. Such formulation leads to substantial redundancy reduction in <b>output</b> <b>units,</b> in addition to extraction of higher order features from input units that exhibit coherence across time and/or space useful in classification problems. We discuss the performance of the proposed algorithm using two scenarios, one dealing with the classification of EEG data while, the second is a speech application dealing with digit classification...|$|R
2500|$|Self-referential RNNs {{with special}} <b>output</b> <b>units</b> for {{addressing}} and rapidly manipulating the RNN's own weights in differentiable fashion (internal storage) ...|$|R
50|$|A {{recurrent}} {{neural network}} for this algorithm consists of some input <b>units,</b> some <b>output</b> <b>units</b> and eventually some hidden units.|$|R
50|$|In {{order to}} perform a {{profitability}} analysis, all costs of an organisation have to be allocated to output units by using intermediate allocation steps and drivers. This process is called costing. When the costs have been allocated, they can be deducted from the revenues per <b>output</b> <b>unit.</b> The remainder shows the unit margin of a product, client, location, channel or transaction.|$|E
5000|$|The {{neural network}} {{is a type}} of parallel-processing {{architecture}} that transforms any stimulus received by the input unit (i.e., stimulus units) to a signal for the <b>output</b> <b>unit</b> (i.e., response units) through a series of mid-level hidden units. Each unit in the input layer is connected to each unit in the hidden layer and, in turn, to each unit in the output layer.|$|E
5000|$|To {{understand}} the mathematical derivation of the backpropagation algorithm, {{it helps to}} first develop some intuitions {{about the relationship between}} the actual output of a neuron and the correct output for a particular training case. Consider a simple neural network with two input units, one <b>output</b> <b>unit</b> and no hidden units. Each neuron uses a linear output that is the weighted sum of its input.|$|E
5000|$|Self-referential RNNs {{with special}} <b>output</b> <b>units</b> for {{addressing}} and rapidly manipulating the RNN's own weights in differentiable fashion (internal storage) ...|$|R
40|$|Conditional {{restricted}} Boltzmann {{machines are}} undirected stochastic neural networks {{with a layer}} of input and <b>output</b> <b>units</b> connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the <b>output</b> <b>units</b> given the states of the input units, parametrized by interaction weights and biases. We address the representational power of these models, proving results on the minimal size of universal approximators of condi...|$|R
50|$|Structurally, a {{neural network}} has three {{different}} classes of units: input units, hidden <b>units,</b> and <b>output</b> <b>units.</b> An activation pattern is {{presented at the}} input units and then spreads in a forward direction from the input units through one or more layers of hidden <b>units</b> to the <b>output</b> <b>units.</b> The activation coming into one unit from other unit is multiplied by the weights on the links over which it spreads. All incoming activation is then added together and the unit becomes activated only if the incoming result is above the unit’s threshold.|$|R
50|$|The Render Output Pipeline is an {{inherited}} term, {{and more often}} referred to as the render <b>output</b> <b>unit.</b> Its job is to control the sampling of pixels (each pixel is a dimensionless point), so it controls antialiasing, when more than one sample is merged into one pixel. All data rendered has to travel through the ROP in order to be written to the framebuffer, from there it can be transmitted to the display.|$|E
50|$|Waydo and Koch {{conducted}} a study where a two layered network was exposed to 40 different face images of ten individuals. Each <b>output</b> <b>unit</b> was constrained to fire to the smallest possible number of inputs. Also, the smallest number of units represented each image. They found {{that most of the}} units responded to a single individual. This suggests that a sparse neuronal representation could emerge in the Medial Temporal Lobe (MTL) while using unsupervised learning.|$|E
5000|$|The primal problem {{deals with}} {{physical}} quantities. With all inputs available in limited quantities, and assuming the unit prices of all outputs is known, what quantities of outputs to produce {{so as to}} maximize total revenue? The dual problem deals with economic values. With floor guarantees on all <b>output</b> <b>unit</b> prices, and assuming the available quantity of all inputs is known, what input unit pricing scheme to set so as to minimize total expenditure? ...|$|E
30|$|In other words, a four-dimensional input {{traffic flow}} vector, {{including}} four time-lagged periods of flow from No. 01 and two <b>output</b> <b>units</b> (representing traffic flow for No. 01 at t[*]+[*] 1 and t[*]+[*] 2 time intervals), {{will be used}} to model the univariate set of data, and a six-dimensional input traffic flow vector, including four time-lagged periods of flow from No. 01, one time-lagged periods of flow from both No. 02 and No. 03, and two <b>output</b> <b>units</b> (representing traffic flow for No. 01 at t[*]+[*] 1 and t[*]+[*] 2 time intervals), {{will be used to}} model the multivariate set of data.|$|R
50|$|For a given set of (input, target) states, {{the network}} is trained to settle into a stable {{activation}} state with the <b>output</b> <b>units</b> in the target state, based on a given input state clamped on the input units.|$|R
50|$|Each C-Bus network {{requires}} a network burden {{if there are}} insufficient C-Bus units on the network. This network burden can be enabled on C-Bus <b>output</b> <b>units</b> through software or a hardware burden can {{be connected to the}} network.|$|R
