20|0|Public
2500|$|The wide {{reporting}} {{of the scandal}} led to criticism of the press for <b>over-coverage.</b> The scandal is {{sometimes referred to as}} [...] "Monicagate," [...] "Lewinskygate," [...] "Tailgate," [...] "Sexgate," [...] and [...] "Zippergate," [...] following the [...] "-gate" [...] nickname construction that has been popular since the Watergate scandal.|$|E
3000|$|Thirdly, the {{registers}} tend to over-cover foreign-born persons. This {{is due to}} {{the fact}} that there are no incentives to report to the tax authorities or municipalities that they are leaving the country. The <b>over-coverage</b> has been estimated at ca. 25 - 50000 persons, ie. around 4 – 8 % of the total foreign-born population in Sweden (Statistics Sweden, 2016) while, in Denmark, it is estimated at about  7500 persons (0.14 % of the total population, or 0.97 % of the total immigrant population) (Statistics Denmark, 2017). <b>Over-coverage</b> is corrected post-hoc when the various administrative bodies identify that persons on their registers have emigrated. 8 [...]...|$|E
40|$|The {{number of}} {{residents}} or population size {{is important for}} all countries. Nowadays in many countries a series of registers have been created, {{which can be used}} for assessing the population size. The residency index is a tool created for estimating the under- and <b>over-coverage</b> of population census and calculation of proper population size. For this aim the concept of a sign of life – a binary variable depending on register i, person j and year k has been introduced showing if the person was active in the register in a given year. The weighted sum of signs of life indicates the probability that the person belongs to the set of residents in a given year. To improve the stability of the index a linear combination of the previous value of the index and the sum of signs of life is used. Necessary parameters were estimated using empirical data...|$|E
40|$|Sensing devices {{generate}} tremendous {{amounts of}} data each day, which include large quantities of multi-dimensional measurements. These data {{are expected to be}} immediately available for real-time analytics as they are streamed into storage. Such scenarios pose challenges to state-of-the-art indexing methods, as they must not only support efficient queries but also frequent updates. We propose here a novel indexing method that ingests multi-dimensional observational data in real time. This method primarily guarantees extremely high throughput for data ingestion, while it can be continuously refined in the background to improve query efficiency. Instead of representing collections of points using Minimal Bounding Boxes as in conventional indexes, we model sets of successive points as line segments in hyperspaces, by exploiting the intrinsic value continuity in observational data. This representation reduces the number of index entries and drastically reduces 2 ̆ 2 <b>over-coverage</b> 2 ̆ 2 by entries. Experimental results show that our approach handles real-world workloads gracefully, providing both low-overhead indexing and excellent query efficiency...|$|E
40|$|Empirical best linear {{unbiased}} prediction (EBLUP) method uses {{a linear}} mixed model in combining information from different sources of information. This method is particularly useful in small area problems. The variability of an EBLUP is traditionally {{measured by the}} mean squared prediction error (MSPE), and interval estimates are generally constructed using estimates of the MSPE. Such methods have shortcomings like under-coverage or <b>over-coverage,</b> excessive length and lack of interpretability. We propose a parametric bootstrap approach to estimate the entire distribution of a suitably centered and scaled EBLUP. The bootstrap histogram is highly accurate, and differs from the true EBLUP distribution by only $O(d^ 3 n^{- 3 / 2 }) $, where $d$ {{is the number of}} parameters and $n$ the number of observations. This result is used to obtain highly accurate prediction intervals. Simulation results demonstrate the superiority of this method over existing techniques of constructing prediction intervals in linear mixed models. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|In {{high energy}} physics, a widely used method to treat {{systematic}} uncertainties in confidence interval calculations {{is based on}} combining a frequentist construction of confidence belts with a Bayesian treatment of systematic uncertainties. In this note we present {{a study of the}} coverage of this method for the standard Likelihood Ratio (aka Feldman & Cousins) construction for a Poisson process with known background and Gaussian or log-Normal distributed uncertainties in the background or signal efficiency. For uncertainties in the signal efficiency of upto 40 % we find <b>over-coverage</b> on the level of 2 to 4 % {{depending on the size of}} uncertainties and the region in signal space. Uncertainties in the background generally have smaller effect on the coverage. A considerable smoothing of the coverage curves is observed. A software package is presented which allows fast calculation of the confidence intervals for a variety of assumptions on shape and size of systematic uncertainties for different nuisance parameters. The calculation speed allows experimenters to test the coverage for their specific conditions. Comment: 19 pages, 7 figures, version to match the one accepted by NI...|$|E
40|$|This {{report is}} of a 44 -year-old immunocompetent woman with septic {{arthritis}} {{of the right}} hand. Scedosporium apiospermum was isolated from the hand. The patient was treated with radical debridement and arthrodesis of the wrist. Itraconazole was given postoperatively as long-term therapy. A girl with known proximal femoral focal deficiency presented with Perthes’ disease at 5 years of age. Her treatment involved a Salter osteotomy. This in conjunction with articular incongruence, due to deformity of the femoral head, resulted in mixed type femoroacetabular impingement when she was 10 years old. Surgical hip dislocation and femoral neck osteochondroplasty successfully relieved her symptoms of impingement. This is the first reported case of Perthes’ disease in a patient with proximal femoral focal deficiency. The case {{highlights the importance of}} thoroughly investigating pain in patients with proximal femoral focal deficiency, a condition which is normally painless. Timely diagnosis of Perthes’ disease and containment procedures can prevent collapse of the femoral head and the resultant sequelae. Acetabular <b>over-coverage</b> should be avoided in pelvic osteotomy to prevent the development of femoroacetabular impingement. link_to_OA_fulltex...|$|E
40|$|In South Africa {{university}} under-preparedness, due to social, {{economic and}} cultural disadvantage, makes black students vulnerable to {{a complex set of}} problems when entering university. This negatively affects retention and graduation rates among non-traditional students. Universities must recognise these students' social, academic and economic struggles and implement interventions to support them. The Humanities Faculty Mentorship Programme (HFMP) provides psychosocial support through mentoring for students likely to be under-prepared {{to meet the demands of}} the University of Cape Town. This paper presents process and outcome evaluations of the HFMP. The process-level evaluation questions are divided into service utilisation, service delivery and organisational support categories. The outcome-level evaluation questions address the programme's intended outcomes; psychosocial adjustment, academic proficiency and university retention. Results indicate that mentor involvement was sufficient, mentees were generally satisfied with their mentoring experience as were mentors with mentor training. Psychosocial adjustment and academic proficiency were achieved. However, <b>over-coverage,</b> poor mentee attendance, and issues with staffing and programme monitoring could have affected the programme's implementation. In addition, the recurrence of academic problems among mentees warrants attention. Suggestions for improving the programme are presented as are recommendations for future evaluations to improve data quality and the assessment of programme effect...|$|E
40|$|Large-sample Wilson-type con fidence {{intervals}} (CIs) {{are derived}} for a parameter {{of interest in}} many clinical trials situations: the log-odds-ratio, in a two sample experiment comparing binomial success proportions, say between cases and controls. The methods cover several scenarios: (i) results embedded in a single 2 x 2 contingency table, (ii) a series of K 2 x 2 tables with common parameter, or (iii) K tables, where the parameter may change across tables {{under the influence of}} a covariate. The calculations of the Wilson CI require only simple numerical assistance, and for example are easily carried out using Excel. The main competitor, the exact CI, has two disadvantages: It requires burdensome search algorithms for the multi-table case and results in strong <b>over-coverage</b> associated with long confidence intervals. All the application cases are illustrated through a well-known example. A simulation study then investigates how the Wilson CI performs among several competing methods. The Wilson interval is shortest, except for very large odds ratios, while maintaining coverage similar to Wald-type intervals. An alternative to the Wald CI is the Agresti-Coull CI, calculated from Wilson and Wald CIs, which has same length as the Wald CI but improved coverage...|$|E
40|$|Confidence {{intervals}} for a binomial parameter or for {{the ratio}} of Poisson means are commonly desired in high energy physics (HEP) applications such as measuring a detection efficiency or branching ratio. Due to the discreteness of the data, {{in both of these}} problems the frequentist coverage probability unfortunately depends on the unknown parameter. Trade-offs among desiderata have led to numerous sets of intervals in the statistics literature, while in HEP one typically encounters only the classic intervals of Clopper-Pearson (central intervals with no undercoverage but substantial <b>over-coverage)</b> or a few approximate methods which perform rather poorly. If strict coverage is relaxed, some sort of averaging is needed to compare intervals. In most of the statistics literature, this averaging is over different values of the unknown parameter, which is conceptually problematic from the frequentist point of view in which the unknown parameter is typically fixed. In contrast, we perform an (unconditional) average over observed data in the ratio-of-Poisson-means problem. If strict conditional coverage is desired, we recommend Clopper-Pearson intervals and intervals from inverting the likelihood ratio test (for central and non-central intervals, respectively). Lancaster’s mid-P modification to either provides excellent unconditional average coverage in the ratio-of-Poisson-means problem. Key words: hypothesis test, confidence interval, binomial parameter, ratio o...|$|E
40|$|International audienceIn this paper, {{we address}} a multi-activity tour {{scheduling}} problem with time varying demand. The {{objective is to}} compute a team schedule for a fixed roster of employees {{in order to minimize}} the <b>over-coverage</b> and the under- coverage of different parallel activity demands along a planning horizon of one week. Numerous complicating constraints are present in our problem: all employees are different and can perform several different activities during the same day-shift, lunch breaks and pauses are flexible, demand is given for 15 minutes periods. Employees have feasibility and legality rules to be satisfied, but the objective function does not account for any quality measure associated with each individual’s schedule. More precisely, the problem mixes simultaneously days-off scheduling, shift scheduling, shift assignment, activity assignment, pause and lunch break assignment. To solve this problem, we developed four methods: a compact Mixed Integer Linear Programming model, a branch-and-price like approach with a nested dynamic program to solve heuristically the subproblems, a diving heuristic and a greedy heuristic based on our subproblem solver. The computational results, based on both real cases and instances derived from real cases, demonstrate that our methods are able to provide good quality solutions in a short computing time. Our algorithms are now embedded in a commercial software, which is already in use in a mini-mart company...|$|E
40|$|The {{reaction}} coe ¢ cients {{of expected}} inations and output {{gaps in the}} forecast-based monetary policy reaction function may be merely weakly identi 8 ̆ 5 ed when the smoothing coe ¢ cient is close to unity and the nominal interest rates are highly persis-tent. In this paper we modify the method of Andrews and Cheng (2012, Econometrica) on inference under weak / semi-strong identi 8 ̆ 5 cation to accommodate the persistence issue. Our modi 8 ̆ 5 cation involves the employment of asymptotic theories for near unit root processes and novel drifting sequence approaches. Large sample properties with a desired smooth transition {{with respect to the}} true values of parameters are devel-oped for the nonlinear least squares (NLS) estimator and its corresponding t / Wald statistics of a general class of models. Despite the not-consistent-estimability, the conservative con 8 ̆ 5 dence sets of weakly-identi ed parameters of interest can be obtained by inverting the t / Wald tests. We show that the null-imposed least-favorable con 8 ̆ 5 dence sets will have correct asymptotic sizes, and the projection-based method may lead to asymptotic <b>over-coverage.</b> Our empirical application suggests that the NLS estimates for the reaction coe ¢ cients in U. S. s forecast-based monetary policy reaction function for 1987 : 32007 : 4 are not accurate su ¢ ciently to rule out the possibility of indeterminacy...|$|E
40|$|The {{combination}} of data from long-baseline and reactor oscillation experiments leads to a preference of the leptonic CP phase δ CP in the range between π and 2 π. We study the statistical significance of this hint by performing a Monte Carlo simulation of the relevant data. We find that {{the distribution of the}} standard test statistic used to derive confidence intervals for δ CP is highly non-Gaussian and depends on the unknown true values of θ 23 and the neutrino mass ordering. Values of δ CP around π/ 2 are disfavored at between 2 σ and 3 σ, depending on the unknown true values of θ 23 and the mass ordering. Typically the standard χ 2 approximation leads to <b>over-coverage</b> of the confidence intervals for δ CP. For the 2 -dimensional confidence region in the (δ CP, θ 23) plane the usual χ 2 approximation is better justified. The 2 -dimensional region does not include the value δ CP = π/ 2 up to the 86. 3 % (89. 2 %) CL assuming a true normal (inverted) mass ordering. Furthermore, we study the sensitivity to δ CP and θ 23 of an increased exposure of the T 2 K experiment, roughly a factor 12 larger than the current exposure and including also anti-neutrino data. Also in this case deviations from Gaussianity may be significant, especially if the mass ordering is unknown...|$|E
40|$|Abstract—We {{provide a}} sweep {{coverage}} algorithm for routing mobile sensors that communicate {{with a central}} data sink. This algorithm improves on its predecessors by {{reducing the number of}} unnecessary scans when different points of interest (POIs) have different requirements for the time interval within which they must be scanned (sweep period). Most sweep coverage algorithms seek to minimize the number of sensors required to cover a given collection of POIs. When POIs have different sweep period requirements, existing algorithms will produce solutions in which sensors visit some POIs much more frequently than is necessary. We define this as the POI <b>Over-Coverage</b> problem. In order to address this problem we develop a Periodic Sweep Coverage (PSC) scheme based on a well-known solution to the Periodic Vehicle Routing Problem (PVRP). Our algorithm seeks a route for the mobile sensors that minimizes the number of unnecessary visits to each POI. To verify and test the proposed scheme we implemented a C++ simulation and ran scenarios with a variety of POI topologies (number and distribution of the POIs) and the speed at which sensors could travel. The simulation results show that the PSC algorithm outperforms other sweep coverage algorithms such as CSweep and Vehicle Routing Problem Sweep Coverage (VRPSC) on both the average number of sensors in a solution and in the computational time required to find a solution. Our results also demonstrate that the PSC scheme is more suitable for the sweep coverage scenarios in which higher speed mobile sensors are used...|$|E
40|$|The primary {{aim of the}} Swedish {{national}} population {{registration system}} is to obtain data that (1) reflect the composition, relationship and identities of the Swedish population and (2) {{can be used as}} the basis for correct decisions and measures by government and other regulatory authorities. For this purpose, Sweden has established two population registers: (1) The Population Register, maintained by the Swedish National Tax Agency ("Folkbokforingsregistret"); and (2) The Total Population Register (TPR) maintained by the government agency Statistics Sweden ("Registret over totalbefolkningen"). The registers contain data on life events including birth, death, name change, marital status, family relationships and migration within Sweden as well as to and from other countries. Updates are transmitted daily from the Tax Agency to the TPR. In this paper we describe the two population registers and analyse their strengths and weaknesses. Virtually 100 % of births and deaths, 95 % of immigrations and 91 % of emigrations are reported to the Population Registers within 30 days and with a higher proportion over time. The <b>over-coverage</b> of the TPR, which is primarily due to underreported emigration data, has been estimated at up to 0. 5 % of the Swedish population. Through the personal identity number, assigned to all residents staying at least 1 year in Sweden, data from the TPR can be used for medical research purposes, including family design studies since each individual can be linked to his or her parents, siblings and offspring. The TPR also allows for identification of general population controls, participants in cohort studies, as well as calculation of follow-up time. NonePublishe...|$|E
40|$|The {{reaction}} {{coefficients of}} expected inflations and output {{gaps in the}} forecast-based monetary policy reaction function may be merely weakly identified when the smoothing coefficient is close to unity, i. e., the nominal interest rates are highly persistent. Using asymptotic theories for near unit root processes and novel drifting sequence approaches, we modify the method of Andrews and Cheng (2012, Econometrica) on inference under weak identification to accommodate the persistence issue. Large sample properties with a desired smooth transition {{with respect to the}} true values of parameters are developed for the nonlinear least squares (NLS) estimator and its corresponding t and Wald statistics of a general class of models. Despite the not-consistent-estimability when the smoothing coefficient is close to unity, the conservative confidence sets of weakly-identified parameters of interest can be obtained by inverting the t or the Wald tests. We show that the null-imposed least-favorable confidence sets will have correct asymptotic sizes while the projection- based and Bonferroni-based methods may lead to asymptotic <b>over-coverage.</b> An identification-category-selection procedure is proposed to select between the standard confidence set and the conservative one under weak identification. Our empirical application suggests that for the model in which the expected inflations and output gaps have a forecast horizon zero, the NLS estimates for the reaction coefficients in U. S. 's forecast-based monetary policy reaction function for 1987 : 3 { 2007 : 4 are not accurate sufficiently to rule out the possibility of indeterminacy. However, for the model with forecast horizon one, the possibility of indeterminacy may be ruled out...|$|E
40|$|Models of weak-scale {{supersymmetry}} offer viable {{dark matter}} (DM) candidates. Their parameter spaces are however rather large and complex, such that pinning down the actual parameter values from experimental data can depend strongly on the employed statistical framework and scanning algorithm. In frequentist parameter estimation, a central requirement for properly constructed confidence intervals {{is that they}} cover true parameter values, preferably at exactly the stated confidence level when experiments are repeated infinitely many times. Since most widely-used scanning techniques are optimised for Bayesian statistics, one needs to assess their abilities in providing correct confidence intervals {{in terms of the}} statistical coverage. Here we investigate this for the Constrained Minimal Supersymmetric Standard Model (CMSSM) when only constrained by data from direct searches for dark matter. We construct confidence intervals from one-dimensional profile likelihoods and study the coverage by generating several pseudo-experiments for a few benchmark sets of pseudo-true parameters. We use nested sampling to scan the parameter space and evaluate the coverage for the benchmarks when either flat or logarithmic priors are imposed on gaugino and scalar mass parameters. The sampling algorithm has been used in the configuration usually adopted for exploration of the Bayesian posterior. We observe both under- and <b>over-coverage,</b> which in some cases vary quite dramatically when benchmarks or priors are modified. We show how most of the variation can be explained as the impact of explicit priors as well as sampling effects, where the latter are indirectly imposed by physicality conditions. For comparison, we also evaluate the coverage for Bayesian credible intervals, and observe significant under-coverage in those cases. Comment: 30 pages, 5 figures; v 2 includes major updates in response to referee's comments; extra scans and tables added, discussion expanded, typos corrected; matches published versio...|$|E
40|$|Hypothesis {{tests for}} the {{presence}} of new sources of Poisson counts amidst background processes are frequently performed in high energy physics (HEP), gamma ray astronomy (GRA), and other branches of science. While there are conceptual issues already when the mean rate of background is precisely known, the issues are even more difficult when the mean background rate has non-negligible uncertainty. After describing a variety of methods {{to be found in the}} HEP and GRA literature, we consider in detail three classes of algorithms and evaluate them over a wide range of parameter space, by the criterion of how close the ensemble-average Type I error rate (rejection of the background-only hypothesis when it is true) compares with the nominal significance level given by the algorithm. We recommend wider use of an algorithm firmly grounded in frequentist tests of the ratio of Poisson means, although for very low counts the <b>over-coverage</b> can be severe due to the effect of discreteness. We extend the studies of Cranmer, who found that a popular Bayesian-frequentist hybrid can undercover severely when taken to high Z values. We also examine the profile likelihood method, which has long been used in GRA and HEP; it provides an excellent approximation in much of the parameter space, as previously studied by Rolke and collaborators. Comment: In v 4, added line to Table 1 so that Z_PL is given separately for L_G and L_P; made numerics in Z_PL identical to C++ code which is now posted at [URL]. In v 3, fixed typos and made other minor changes to v 2, so as to be materially the same as version in NIM. v 2 was a major revision with new materia...|$|E
40|$|In this {{master s}} thesis we present a {{planning}} problem at Maternity Ward West (MWW) at St. Olavs Hospital, concerning the scheduling of 69 employees for a planning horizon of 27 weeks. The scheduling problem involves covering demand for health workers of different skill categories, while respecting employees preferences {{as much as possible}} and ensuring fairness. The goal of the {{master s thesis}} is to create a decision support tool that solves the scheduling problem by producing schedules for MWW of such quality that they are preferable to the manually made schedules produced in the current planning process at MWW. Furthermore, we discuss related literature and provide a theoretical context for our work. This leads to the formulation of the problem scope and the problem description. Subsequently we formulate a general mathematical integer linear programming model and develop it using the commercial optimisation software FICO Xpress Optimisation Suite 7. 8. We refer to it as the 'MWW scheduling model'. It runs successfully, creating a schedule with real-life data for all the employees at MWW. The same data has been used to create a schedule using manual techniques at MWW, making it possible to compare the results of the techniques. The ward manager at MWW states that the MWW scheduling model guarantees the employees influence on the the schedules, because it prioritises employees preferences and lets the ward manager and scheduling group make changes to the produced schedules if needed. The MWW scheduling model respects preferences as it produces schedules allocating shifts in accordance with 89. 8 % of the employees requested shifts. Furthermore, the MWW scheduling model distributes <b>over-coverage</b> more evenly than the manually made schedules, thus securing a more robust schedule. Also, the MWW scheduling model guarantees that schedules always abide by all scheduling rules. Another strength theMWWscheduling model possesses is its lack of bias when allocating shifts, making schedules fair to all employees. The current manual method for scheduling, as opposed to the automatic scheduling provided by the MWW scheduling model, includes a bartering process which the ward manager states is unfair. Removing the need for a bartering process with optimisation-based scheduling is perhaps the single most efficient measure to make the current planning process at MWW more fair. Lastly, the MWW scheduling model creates good schedules very fast. Although the scheduling model likely needs minor adjustments before creating schedules for new planning horizons, optimisation-based scheduling is still remarkably faster than the current scheduling system at MWW. The MWW scheduling model finds good integer solutions within few minutes and is close to reaching optimality within 24 hours, with an optimality gap of 0. 036 % for the full real-life instance. We also use the MWW scheduling model to perform analyses. We perform technical analyses that show how the model is very scalable for different planning horizons and different staff levels. Furthermore, we have shown that the MWW scheduling model {{can be used as a}} management tool for tactical and strategic decisions, by implementing changes in policies and staff levels for different instances and producing feasible schedules for these instances. These policy changes have been chosen after discussions with the ward manager and after receiving input from the board of the Regional Centre of Health Care Development. Most notable is that it seems realistic to open an extra bed unit during weekends without increasing the current staff level, by implementing a policy change that trades extra weekend shifts for extra off-days. Our analyses also show that the MWW can meet coverage requirements with reduced staff levels. Lastly, we perform an analysis that shows that employees can be scheduled to work less than contracted, creating extra off-shifts that serve as a buffer for tackling staffing shortages due to sudden long-term sickness. This policy proves complicated, and the approach needs further development and greater insights into the online operational planning level to be efficient. We have succeeded in reaching our goal of creating a decision support tool that solves the scheduling problem by producing schedules for MWW of such quality that they are preferable to the manually made schedules, and the best testimony to this is that the ward manager at MWW states that she wants to use our model in her work and that she wants a similar model developed for other wards...|$|E

