1|10000|Public
40|$|Dataset bias {{remains a}} {{significant}} barrier towards solving real world computer vi-sion tasks. Though deep convolutional networks {{have proven to}} be a competitive approach for image classification, a question remains: have these models have solved the dataset bias problem? In general, training or fine-tuning a state-of-the-art deep model on a new domain requires a significant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as suscep-tible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (<b>one</b> <b>example</b> <b>per</b> <b>category)</b> or no labeled domain specific data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost. ...|$|E
40|$|Introduction: Quantitative {{measures}} of degree of lumbar spinal stenosis (LSS) such as antero-posterior {{diameter of the}} canal or dural sac cross sectional area vary widely and do not correlate with clinical symptoms or results of surgical decompression. In an effort to improve quantification of stenosis we have developed a grading system based on the morphology of the dural sac and its contents as seen on T 2 axial images. The grading comprises seven categories ranging form normal to the most severe stenosis and {{takes into account the}} ratio of rootlet/CSF content. Material and methods: Fifty T 2 axial MRI images taken at disc level from twenty seven symptomatic lumbar spinal stenosis patients who underwent decompressive surgery were classified into seven categories by five observers and reclassified 2 weeks later by the same investigators. Intra- and inter-observer reliability of the classification were assessed using Cohen's and Fleiss' kappa statistics, respectively. Results: Generally, the morphology grading system itself was well adopted by the observers. Its success in application is strongly influenced by the identification of the dural sac. The average intraobserver Cohen's kappa was 0. 53 ± 0. 2. The inter-observer Fleiss' kappa was 0. 38 ± 0. 02 in the first rating and 0. 3 ± 0. 03 in the second rating repeated after two weeks. Discussion: In this attempt, the teaching of the observers was limited to an introduction to the general idea of the morphology grading system and <b>one</b> <b>example</b> MRI image <b>per</b> <b>category.</b> The identification of the dimension of the dural sac may be a difficult issue in absence of complete T 1 T 2 MRI image series as it was the case here. The similarity of the CSF to possibly present fat on T 2 images was the main reason of mismatch in the assignment of the cases to a category. The Fleiss correlation factors of the five observers are fair and the proposed morphology grading system is promising...|$|R
50|$|Details from Rae Featherstone's first Builder's Payment Certificate Book stub {{of private}} work, Certificates 1 - 147 dated 16 August 1940 to 13 June 1956, in the {{possession}} of his family. Transcription in progress; <b>one</b> <b>example</b> <b>per</b> property.|$|R
40|$|Some {{applications}} require classifiers {{that can}} learn classification information from very few <b>examples</b> <b>per</b> a decision class. In such applications, {{it is difficult}} or expensive to gather more than <b>one</b> <b>example</b> <b>per</b> a class. This paper provides {{an analysis of the}} performance of the radial-basis neural networks when trained on <b>one</b> <b>example</b> <b>per</b> decision class. Radial-basis neural networks are widely used in many applications. The empirical analysis investigates the recognition accuracy of these classifiers in surrounding area of known examples. This is done by changing number of attribute values, or changing a single value with different margins. The results show a strong relationship between the distance of the testing records to the training ones and the predictive accuracy. Similar distances caused by changing different attributes provide same error rate. The results indicate that distance-base classifiers (such as k-nearest neighbor) are much better classifiers than radial-basis networks with very few training data. 1...|$|R
40|$|Deep Neural Networks (DNNs) often {{struggle}} with one-shot learning {{where we have}} only {{one or a few}} labeled training <b>examples</b> <b>per</b> <b>category.</b> In this paper, we argue that by using side information, we may compensate the missing information across classes. We introduce two statistical approaches for fusing side information into data representation learning to improve one-shot learning. First, we propose to enforce the statistical dependency between data representations and multiple types of side information. Second, we introduce an attention mechanism to efficiently treat examples belonging to the 'lots-of-examples' classes as quasi-samples (additional training samples) for 'one-example' classes. We empirically show that our learning architecture improves over traditional softmax regression networks as well as state-of-the-art attentional regression networks on one-shot recognition tasks...|$|R
40|$|Classifying nouns into {{semantic}} categories (e. g., animals, food) is {{an important}} line of research in both cognitive science and natural language processing. We present a minimally supervised model for noun classification, which uses symmetric patterns (e. g., “X and Y”) and an iterative variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a predefined set of symmetric patterns, but extract them automatically from plain text, in an unsu-pervised manner. We experiment with four semantic categories and show that symmetric patterns constitute much better classification features compared to leading word embedding methods. We further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-the-art label propagation alternatives for this task. In experiments, our model obtains 82 %- 94 % accuracy using as few as four labeled <b>examples</b> <b>per</b> <b>category,</b> emphasizing the effectiveness of simple search and representation techniques for this task. ...|$|R
40|$|In this paper, Fisherface is {{extended}} for face recognition from <b>one</b> <b>example</b> image <b>per</b> person. Fisherface {{is one of}} the most successful face recognition methods. However, Fisherface requires several training images for each face, so it cannot be applied to face recognition applications where only <b>one</b> <b>example</b> image <b>per</b> person is available for training. To tackle this problem, Fisherface method {{is extended}} by utilizing 3 D morphable model to derive multiple images of a face from one single image. Experimental results on ORL face database and real time face database show that face recognition method proposed in this paper makes impressive performance improvement compared with conventional Eigenface methods. Keywords...|$|R
40|$|In this paper, we extend Fisherface for face {{recognition}} from <b>one</b> <b>example</b> image <b>per</b> person. Fisherface {{is one of}} the most successful {{face recognition}} methods. However, Fisherface requires several training images for each face, so it cannot be applied to the face recognition applications where only <b>one</b> <b>example</b> image <b>per</b> person is available for training. To tackle this problem, we extended the Fisherface method by proposing a method to derive multiple images of a face from one single image. Fisherface is then trained on these derived images. Experimental results on Bern face database and our 350 subjects database show that our method makes impressive performance improvement compared with the conventional Eigenfaces and template matching techniques. 1...|$|R
40|$|This report {{presents}} {{the findings of}} a study undertaken to identify and describe the protected area governance models in use in Western Australia. The overall aim {{of the study was to}} identify and describe these governance models and provide <b>one</b> detailed <b>example</b> <b>per</b> model. This report includes a condensed account of the governance models from the full study (available as Shields 2013), and also describes a Western Australian example of each model...|$|R
40|$|Previous {{studies have}} shown that spike-timing-dependent {{plasticity}} (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated - using rate-based neural networks trained with back-propagation - that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of <b>examples</b> <b>per</b> <b>category</b> were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH- 80, and MNIST databases. We also demonstrate the superiority of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken together, our results suggest that the combination of STDP with latency coding may be a key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption...|$|R
40|$|In this paper, we are {{interested}} in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of <b>examples</b> <b>per</b> novel <b>category</b> is very limited, e. g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods...|$|R
2500|$|A once common {{programming}} optimization, used {{especially in}} 3D graphics, was to pre-calculate {{a table of}} sine values, for <b>example</b> <b>one</b> value <b>per</b> degree. This allowed results to be looked up from a table rather than being calculated in real time. With modern CPU architectures this method may offer no advantage.|$|R
40|$|This thesis {{defines the}} {{essentials}} of activism and gives examples of online technology that can assist grassroots organizations in promoting positive change within their communities. I discuss existing online technologies that {{make it possible for}} grassroots organizations to enhance the traditional (non-Internet-based) approaches to activism. Online activism is a growing trend among non-profit organizations. Several online awards are given to organizations that have noticeably pursued online pursuit of electronic advocacy. From making telephone calls to organizing demonstrations, grassroots groups can begin saving money, time, and human resources. The Internet frees people from physical barriers and borders. I have investigated current online technologies {{that can be used to}} supplement traditional activism services. I give <b>one</b> <b>example</b> <b>per</b> approach that can be used online. The examples include self-education, promoting your organization 2 ̆ 7 s message, successful online actions, information distribution, corporate utilization, and the future of online activism. Examples of activism in this report focus on non-violent advocacy or civil disobedience. I have researched online activism through the use of periodicals, books, World Wide Web Internet searches, and interviews with advocacy specialists. This report offers evidence that the Internet has changed, and will continue to change, the paradigm of political and social activism. By providing increased access using new, fast, and efficient technology, more people are able to have a greater say in shaping their worlds...|$|R
40|$|If we are {{provided}} a face database with only <b>one</b> <b>example</b> view <b>per</b> person, {{is it possible}} to recognize new views of them under a variety of different poses, especially views rotated in depth from the original example view? We investigate using prior knowledge about faces plus each single example view to generate virtual views of each person, or views of the face as seen from different poses. Prior knowledge of faces is represented in an example-based way, using 2 D views of a prototype face seen rotating in depth. The synthesized virtual views are evaluated as example views in a view-based approach to pose-invariant face recognition. They are shown to improve the recognition rate over the scenario where only the single real view is used...|$|R
40|$|Classification {{with only}} <b>one</b> labeled <b>example</b> <b>per</b> class is a {{challenging}} problem in machine learning and pattern recognition. While {{there have been}} some attempts to address this problem in the context of specific applications, very little work has been done so far on the problem under more general object classification settings. In this paper, we propose a graph-based approach to the problem. Based on a robust path-based similarity measure proposed recently, we construct a weighted graph using the robust path-based similarities as edge weights. A kernel matrix, called graph Laplacian kernel, is then defined based on the graph Laplacian. With the kernel matrix, in principle any kernel-based classifier can be used for classification. In particular, we demonstrate the use of a kernel nearest neighbor classifier on some synthetic data and real-world image sets, showing that our method can successfully solve some difficult classification tasks with only very few labeled examples. 1...|$|R
50|$|Many parents {{believe that}} such walkers teach a child to walk faster. However, they may {{actually}} delay walking by {{two to three weeks}} for a typical child. The amount of use matters; for every 24 hours babies spend in a baby walker (for <b>example,</b> <b>one</b> hour <b>per</b> day for 24 days), they learn to walk three days later and to stand four days later than they would have.|$|R
40|$|We {{propose a}} novel {{framework}} for contour based object detection and recognition, which we formulate as a joint contour fragment grouping and labeling problem. For a given set of contours of model shapes, we simultaneously perform selection of relevant contour fragments in edge images, grouping {{of the selected}} contour fragments, and their matching to the model contours. The inference in all these steps is performed using particle filters (PF) but with static observations. Our approach needs <b>one</b> <b>example</b> shape <b>per</b> class as training data. The PF framework combined with decomposition of model contour fragments to part bundles allows us to implement an intuitive search strategy for the target contour in a clutter of edge fragments. First a rough sketch of the model shape is identified, followed by fine tuning of shape details. We show that this framework yields not only accurate object detections but also localizations in real cluttered images. 1...|$|R
40|$|Abstract Object {{identification}} is a specialized type of recognition {{in which the}} category (e. g. cars) is known and {{the goal is to}} recognize an object’s exact identity (e. g. Bob’s BMW). Two special challenges characterize object identification. First, inter-object variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. Second, there may be many different instances of the category but few or just one positive “training ” <b>examples</b> <b>per</b> object instance. Because variation among object instances may be small, a solution must locate possibly subtle object-specific salient features, like a door handle, while avoiding distracting ones such as specular highlights. With just <b>one</b> training <b>example</b> <b>per</b> object instance, however, standard modeling and feature selection techniques cannot be used. We describe an on-line algorithm that takes one image from a known category and builds an efficient “same” versus “different ” classification cascade by predicting the most discriminative features for that object instance. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered sequence of discriminative features specific to the given image. Learned stopping thresholds make the identifier very efficient. To make this possible, category-specific characteristics are learne...|$|R
40|$|Object {{identification}} is a specialized type of recognition {{in which the}} category (e. g. cars) is known and {{the goal is to}} recognize an object’s exact identity (e. g. Bob’s BMW). Two special challenges characterize object identification. First, inter-object variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. Second, there may be many different instances of the category but few or just one positive “training ” <b>examples</b> <b>per</b> object instance. Because variation among object instances may be small, a solution must locate possibly subtle object-specific salient features, like a door handle, while avoiding distracting ones such as specular highlights. With just <b>one</b> training <b>example</b> <b>per</b> object instance, however, standard modeling and feature selection techniques cannot be used. We describe an on-line algorithm that takes one image from a known category and builds an efficient “same ” versus “different ” classification cascade by predicting the most discriminative features for that object instance. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered sequence of discriminative features specific to the given image. Learned stopping thresholds make the identifier very efficient. To make this possible, category-specific characteristics are learned automatically in an off-line training procedure from labeled image pairs of the category. Our method, using the same algorithm for both cars and faces, outperforms a wide variety of other methods. 1...|$|R
40|$|In many tasks {{the goal}} is to learn a {{function}} that varies predictably with transformations of the input. Examples: • Pose-invariant classification. Recognize an object category regardless of the object translation, rotation, and scale. • Pose regression. Detect an object and estimate its translation, rotation, and scale. • Detection. Find an object location (center), without estimating its orientation and scale. Toy Example: Rotation-invariant classification Learn three classes of 2 D points (red o, green +, blue x). starting from just <b>one</b> <b>example</b> point <b>per</b> class and gradually enforcing invariance to larger rotations. 0 π/ 8 π/ 4 π A function f: X! Y is equivariant if its output varies with its input in a predictable way for given transformations T: 8 t 2 T: f(tx; w) ⇡ tf(x; w) The effect of t on the input and output spaces X and Y can be chosen arbitrarily. Invariance is obtained when t acts as the identity on the output (ty = y). Example (co-variance). Let y = f(x;w) be the location of a butterfly in an image x. If tx is the rotated image, then the butterfly location ty = f(tx;w) should track the motion. x t...|$|R
40|$|Molenberghs, Verbeke, and Demétrio (2007) and Molenberghs et al. (2010) {{proposed}} a general framework to model hierarchical data subject to within-unit correlation and/or overdispersion. The framework extends classical overdispersion models {{as well as}} generalized linear mixed models. Subsequent work has examined various aspects {{that lead to the}} formulation of several extensions. A unified treatment of the model framework and key extensions is provided. Particular extensions discussed are: explicit calculation of correlation and other moment-based functions, joint modelling of several hierarchical sequences, versions with direct marginally interpretable parameters, zero-inflation in the count case, and influence diagnostics. The basic models and several extensions are illustrated using a set of key <b>examples,</b> <b>one</b> <b>per</b> data type (count, binary, multinomial, ordinal, and time-to-event) ...|$|R
50|$|Second, if the {{poisonous}} or deleterious substance is unavoidable and is within an established tolerance, regulatory limit, or action level, the food {{will not be}} deemed to be adulterated. Tolerances and regulatory limits are thresholds above which a food will be considered adulterated. They are binding on FDA, the food industry, and the courts. Action levels are limits at or above which FDA may regard food as adulterated. They are not binding on FDA. FDA has established numerous action levels (for <b>example,</b> <b>one</b> part <b>per</b> million methylmercury in fish), which are set forth in its booklet Action Levels for Poisonous or Deleterious Substances in Human Food and Animal Feed.|$|R
5000|$|There {{are five}} {{different}} categories of tests with three activities <b>per</b> <b>category.</b> The categories are (followed by their activities): ...|$|R
5000|$|The {{population}} of 2142 is about 15 billion people. Technologically advanced combines have instituted rigid population control measures. Reproduction {{is limited to}} self-replacement, for <b>example,</b> <b>one</b> child <b>per</b> person. This has been named [...] "The One Life/One Birth Law". Theoretically this will stabilize the population, but actually it reduces it through early death of the child or premature death of an adult before {{he or she has}} reproduced. This will guarantee a gradual decline in the population. But it also requires mandatory sterilization immediately after the permitted off-spring is born. A citizen who goes beyond the One Life/One Birth limit will be forcibly sterilized and risks having all assets seized in order to support the extra child.|$|R
40|$|AggPro predicts {{baseball}} statistics by {{utilizing a}} weighted average of predictions provided by several other statistics projection systems. The aggregate projection that is generated is {{more accurate than}} any of the constituent systems individually. We explored the granularity at which weights should be assigned by considering four possibilities: a single weight for each projection system, one weight <b>per</b> <b>category</b> <b>per</b> system, one weight per player per system, and one weight <b>per</b> player <b>per</b> <b>category</b> <b>per</b> system. We found that assigning one weight <b>per</b> <b>category</b> <b>per</b> system provides better results than the other options. Additionally, we projected raw statistics directly and compared the results to projecting rate statistics scaled by predicted player usage. We found that predicting rate statistics and scaling by predicted player usage produces better results. We also discuss implementation challenges that we faced in producing the AggPro projections. ...|$|R
5000|$|The {{winners and}} nominees <b>per</b> <b>category</b> were (Winners are listed first and {{highlighted}} in boldface): ...|$|R
30|$|EPFL: {{randomly}} selecting 99 {{images for}} testing (11 images <b>per</b> <b>category)</b> {{and the rest}} for training.|$|R
50|$|There {{were not}} more than 5 nominees <b>per</b> <b>category</b> and there were 10 {{categories}} in total.|$|R
40|$|Splits of {{train set}} and test set {{given in the}} dataset. In MIT Indoor 67 {{experiment}} [6], the training size is 100 images <b>per</b> <b>category.</b> Experiment is ran on 1 split of train set and test set given in the dataset. On the 10 splits randomly generated by ourself, the classification accuracy is 69. 10 %± 1. 62 % In the Scene 15 experiment [3], the training size is 50 images <b>per</b> <b>category.</b> Experiments are ran on 10 random splits of train set and test set. In the SUN Attribute experiment [5], the training size is 150 images per attribute. The report result is average precision. The splits of train set and test set are given in the paper. In Caltech 101 and Caltech 256 experiment [1, 2], the training size is 30 images <b>per</b> <b>category.</b> The experiments are ran on 10 random splits of train set and test set. In Stanford Action 40 experiment [8], the training size is 100 images <b>per</b> <b>category.</b> Experiments are ran on 10 random splits of train set and test set. The reported result is classification accuracy...|$|R
30|$|Table 1 {{gives the}} number of {{segmented}} gestures <b>per</b> <b>category,</b> with {{the number of}} resulting sequences (Nseq) and symbolic labels assigned (L).|$|R
2500|$|A Mac {{dashboard}} widget {{was released}} in December, listing [...] "top 10 stuff currently on TPB, either <b>per</b> <b>category</b> or the full list".|$|R
50|$|The key {{enabling}} technologies {{can be categorized}} in machine components, machine design methods & tools, and machine control. A few examples are listed below <b>per</b> <b>category.</b>|$|R
50|$|Plants can be {{classified}} as one of several sizes based on its above-ground diameter. Although the size <b>per</b> <b>category</b> is not set in stone, these are a general guideline.|$|R
50|$|This {{would be}} the final {{ceremony}} to have five nominees <b>per</b> <b>category,</b> most major categories (acting and programs) were expanded to include at least six slots the following year.|$|R
5000|$|IGN {{reviewed}} 13th Skull [...] "Good" [...] with a 7.0 {{rating for}} its Presentation, Gameplay, Graphics, Sound and Lasting Appeal for an approximately of 6.5 to 7.5 <b>per</b> <b>Category.</b>|$|R
50|$|Voting for César Awards is {{conducted}} through two ballots by mail: {{the first to}} establish nominations <b>per</b> <b>category</b> (three to five, depending on the discipline), and the second to decide the winner.|$|R
