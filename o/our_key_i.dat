0|8130|Public
40|$|In {{this note}} we reply to Fabio Canova’s (2006) {{comments}} on our paper ‘VAR Analysis and the Great Moderation’. We show that Canova’s comments do not affect <b>our</b> <b>key</b> conclusions: (<b>i)</b> existing VAR {{evidence on the}} Great Moderation does not provide decisive support for the good luck hypothesis; and (ii) such evidence {{is consistent with the}} notion that good policy has {{played a significant role in}} fostering the greater stability of the most recent years...|$|R
40|$|We examine {{vertical}} backward {{integration in}} a reduced-form model of successive oligopolies. <b>Our</b> <b>key</b> findings are: (<b>i)</b> There may be asymmetric equilibria where some firms integrate and others remain separated, even if firms are symmetric initially; (ii) Efficient firms {{are more likely}} to integrate vertically. As a result, integrated firms also tend to have a large market share. The driving force behind these findings are demand/mark-up complementarities in the product market. We also identify countervailing forces resulting from strong vertical foreclosure, upstream sales and endogenous acquisition cost...|$|R
40|$|We {{present a}} {{systematic}} {{investigation of the}} effect of spin-orbit interaction on optical conductivity in monolayer graphene. <b>Our</b> <b>key</b> findings are: (<b>i)</b> level splitting at various crystal symmetry points caused by true spin as well as pseudospin of the electrons gives rise to a resonant current response; (ii) under heavy doping, the spin-orbit interaction leads to a re-entrance of finite conductivity at very low frequency which was strictly forbidden in the absence of spin-orbit coupling; (iii) deformation of band structure and the topological properties of trigonal warping are analytically identified in a low-energy conical-like approximation...|$|R
40|$|Workloads on {{general-purpose}} computing {{systems have}} changed {{dramatically over the}} past few years, with greater emphasis on emerging compute-intensive applications such as media processing and databases. However, until recently, most high performance computing studies have primarily focused on scientific and engineering workloads, potentially leading to designs not suitable for these emerging workloads. This dissertation addresses this limitation. <b>Our</b> <b>key</b> contributions include (<b>i)</b> the first detailed quantitative simulation-based studies of the performance of media processing and database workloads on systems using state-of-the-art processors, and (ii) cost-effective architectural solutions targeted at achieving the higher performance requirements of future systems running these workloads. The first par...|$|R
40|$|Building on {{research}} previously reported at AAMAS conferences, {{this paper describes}} an innovative application of a novel gametheoretic approach for a national scale security deployment. Working with the United States Transportation Security Administration (TSA), we have developed a new application called GUARDS to assist in resource allocation tasks for airport protection at over 400 United States airports. In contrast with previous efforts such as AR-MOR and IRIS, which focused on one-off tailored applications and one security activity (e. g. canine patrol or checkpoints) per application, GUARDS faces three <b>key</b> issues: (<b>i)</b> reasoning about hundreds of heterogeneous security activities; (ii) reasoning over diverse potential threats; (iii) developing a system designed for hundreds of end-users. Since a national deployment precludes tailoring to specific airports, <b>our</b> <b>key</b> ideas are: (<b>i)</b> creating a new game-theoreti...|$|R
40|$|This paper fills what {{we believe}} to be a lacuna in the {{existing}} literature concerning upper bounds on exponential sums. Although it has always been evident that many of the known estimates can be made explicit, it is a non-trivial problem to actually do so. In particular so that the constants involved do not render the explicit estimates useless in practical applications. We have used the practical bounds that are needed to prove Theorem 1 as motivation for our results here, though we hope that this work will be applicable to a variety of other problems which routinely apply these or related exponential sum estimates. In particular our results here can be used to say something about the questions of estimating the number of integers free of large prime factors in short intervals (see [FL]), and of the largest prime factor of an integer in an interval (see [J]). <b>Our</b> <b>key</b> result <b>i...</b>|$|R
40|$|L-dopa, {{which is}} a {{precursor}} for dopamine, acts to amplify strong signals, and dampen weak signals as suggested by previous studies. The effect of L-dopa has been demonstrated in language studies, suggesting restriction of the semantic network. In this study, we aimed to {{examine the effect of}} L-dopa on language processing with fMRI using Independent Component Analysis (ICA). Two types of language tasks (phonological and semantic categorization tasks) were tested under two drug conditions (placebo and L-dopa) in 16 healthy subjects. Probabilistic ICA (PICA), part of FSL, was implemented to generate Independent Components (IC) for each subject for the four conditions and the ICs were classified into taskrelevant source groups by a correlation threshold criterion. <b>Our</b> <b>key</b> findings include: (<b>i)</b> The highly task-relevant brai...|$|R
40|$|We examine two of {{the most}} {{fascinating}} fraud cases that rocked the hedge fund industry: Bernard L. Madoff Investment Securities and Bayou Management. From academic research and news reports, we distill important lessons for the hedge fund investor. <b>Our</b> <b>key</b> conclusions are (<b>i)</b> conflicts of interests are often symptomatic of more serious problems at hedge funds, (ii) auditing statements from obscure accounting firms should provide little assurance to investors, (iii) a comparison between the actual fund risk exposures based on realized returns with the theoretical risk exposures from the trading strategy serves as a useful cross-check for investors, and (iv) for funds with a master-feeder structure, it is important to conduct due diligence on both the master and the feeder fund...|$|R
40|$|We {{administer}} the Allais paradox questions to both {{a representative sample}} of the Dutch population and to student subjects. Three treatments are implemented: one with the original high hypothetical payoffs, one with low hypothetical payoffs and a third with low real payoffs. <b>Our</b> <b>key</b> findings are: (<b>i)</b> violations in the non-lab sample are systematic and a large bulk of violations is likely to stem from non-familiarity with large payoffs, (ii) we can identify groups of the general population that have much higher than average violation rates; this concerns mainly the lowly educated and unemployed, and (iii) the relative treatment differences in the population at large are accurately predicted by the lab sample, but violation rates in all lab treatments are about 15 percentage points lower than in the corresponding non-lab treatments...|$|R
40|$|International audienceThis paper {{addresses}} the static analysis {{of an important}} class of X 10 programs, namely those with finish/async parallelism, and affine loops and array reference structure as in the polyhedral model. For such programs our analysis can certify whenever a program is deterministic or flags races. <b>Our</b> <b>key</b> contributions are (<b>i)</b> adaptation of array dataflow analysis from the polyhedral model to programs with finish/async parallelism, and (ii) use of the array dataflow analysis result to certify determinacy. We distinguish our work from previous approaches by combining the precise statement instance-wise and array element-wise analysis capability of the polyhedral model with finish/async programs that are more expressive than doall parallelism commonly considered in the polyhedral literature. We show that our approach is exact (no false negative/positives) and more precise than previous approaches, but is limited to programs that fit the polyhedral model...|$|R
40|$|Abstract. Team {{formation}} {{is a critical}} step in deploying a multi-agent team. In some scenarios, agents coordinate by voting continuously. When forming such teams, should {{we focus on the}} diversity of the team or on the strength of each member? Can a team of diverse (and weak) agents outperform a uniform team of strong agents? We propose a new model to address these questions. <b>Our</b> <b>key</b> contributions include: (<b>i)</b> we show that a diverse team can overcome a uniform team and we give the necessary conditions for it to happen; (ii) we present optimal voting rules for a diverse team; (iii) we perform synthetic experiments that demonstrate that both diversity and strength contribute to the performance of a team; (iv) we show experiments that demonstrate the usefulness of our model {{in one of the most}} difficult challenges for Artificial Intelligence: Computer Go...|$|R
40|$|This paper {{addresses}} the static analysis {{of an important}} class of X 10 programs, namely those with finish/async parallelism, and affine loops and array reference structure as in the polyhedral model. For such programs our analysis can certify whenever a program is deterministic or flags races. <b>Our</b> <b>key</b> contributions are (<b>i)</b> adaptation of array dataflow anal-ysis from the polyhedral model to programs with finish/async par-allelism, and (ii) use of the array dataflow analysis result to certify determinacy. We distinguish our work from previous approaches by combining the precise statement instance-wise and array element-wise analysis capability of the polyhedral model with finish/async programs that are more expressive than doall parallelism com-monly considered in the polyhedral literature. We show that our approach is exact (no false negative/positives) and more precise than previous approaches, but is limited to programs that fit the polyhedral model...|$|R
40|$|Team {{formation}} {{is a critical}} step in deploying a multi-agent team. In some scenarios, agents coordinate by voting continuously. When forming such teams, should {{we focus on the}} diversity of the team or on the strength of each member? Can a team of diverse (and weak) agents outperform a uniform team of strong agents? We propose a new model to address these questions. <b>Our</b> <b>key</b> contributions include: (<b>i)</b> we show that a diverse team can overcome a uniform team and we give the necessary conditions for it to happen; (ii) we present optimal voting rules for a diverse team; (iii) we perform synthetic experiments that demonstrate that both diversity and strength contribute to the performance of a team; (iv) we show experiments that demonstrate the usefulness of our model {{in one of the most}} difficult challenges for Artificial Intelligence: Computer Go. ...|$|R
40|$|Motivated {{by several}} {{examples}} from industry, {{such as the}} introduction of a biotechnology-based process innovation in nylon manufacturing, we consider a technology provider that develops and introduces innovations to a market of industrial customers [...] original equipment manufacturers (OEMs). The technology employed by these OEMs determines the performance quality of the end product they manufacture, which in turn forms the basis of competition among them. Within this context of downstream competition, we examine the technology provider's introduction strategies when improving technologies are introduced sequentially. We develop a two-period game-theoretic framework to account for the strategic considerations of the parties involved (i. e., the technology provider and the OEMs). Our main result indicates that the technology provider may find it beneficial to induce partial adoption of the new technology, depending on the technological progress the provider intends to offer in the future. We analyze many technology-specific and market-related characteristics [...] such as volume-based pricing for new component technologies, upgrade prices, and OEMs with differing capabilities [...] that correspond to various business settings. <b>Our</b> <b>key</b> result (<b>i.</b> e., partial adoption) proves to be a robust phenomenon. We also develop additional insights regarding the interactions between adoption and OEM capabilities. technology introduction, technology adoption, game theory, industrial markets, industrial customers, business-to-business, multistage game...|$|R
40|$|Recent {{occurrences of}} mobile worms like Cabir, Mabir and CommWarrior have created growing {{concerns}} over the security of data stored on mobile devices such as cell phones and PDAs. These worms have in common that they all use Bluetooth communication as their infection channel. In order to prepare effective defense strategies against such worms, we study the nature, characteristics, and spreading dynamics of Bluetooth worms in the safe environment of simulation. <b>Our</b> <b>key</b> findings are: (<b>i)</b> Mobility may not boost the Bluetooth worm propagation; instead, link instability owing to it has {{negative impact on the}} worm spreading speed; (ii) The inherent capacity constraints imposed by the wireless channel (e. g. interference) and the specifics of the Bluetooth protocol can significantly slow down the Bluetooth worm propagation; (iii) Intelligently designed worms can improve their propagation speed to a noticeable degree by strategically selecting worm model parameters or exploiting out-of-band propagation capabilities. ...|$|R
40|$|Abstract—In this paper, {{we focus}} on various {{experiments}} conducted to analyze the performance of Transmission Control Protocol (TCP) and Universal Datagram Protocol (UDP) based applications in a IEEE 802. 16 deployed network. We analyze the effect of Medium Access Control (MAC) and Physical layer characteristics {{on the performance of}} TCP and UDP-based applications. <b>Our</b> <b>key</b> findings are: (<b>i)</b> throughput achieved by TCPbased applications is lower than that of UDP-based applications, (ii) slot utilization of TCP-based applications is lower than that of UDP-based applications and (iii) throughput of TCP-based applicationssuffersinthepresenceof UDP-basedapplicationsfor similar channel states. We also observe that throughput achieved byboth TCPand UDP-basedapplicationswith Automatic Repeat ReQuest (ARQ) set in system are higher as compared to that of without ARQ. The findings of these experiments can be adopted while designing efficient scheduling schemes for IEEE 802. 16 based network, such that higher throughput, utilization and better delay performance can be achieved. I...|$|R
40|$|Understanding the {{evolution}} of human society, as a complex adaptive system, is a task that has been looked upon from various angles. In this paper, we simulate an agent-based model with a high enough population tractably. To do this, we characterize an entity called society, which helps us reduce the complexity of each step from O(n^ 2) to O(n). We propose a very realistic setting, where we design a joint alternate maximization step algorithm to maximize a certain fitness function, which we believe simulates the way societies develop. <b>Our</b> <b>key</b> contributions include (<b>i)</b> proposing a novel protocol for simulating {{the evolution}} of a society with cheap, non-optimal joint alternate maximization steps (ii) providing a framework for carrying out experiments that adhere to this joint-optimization simulation framework (iii) carrying out experiments to show that it makes sense empirically (iv) providing an alternate justification for the use of society in the simulations...|$|R
40|$|We {{develop a}} model in which hadron {{production}} in the true asymptotic region proceeds via the exchange of a factorizable singularity at J= 1, which implies a sensible meson spectrum. The rise of the hadronic total cross section and the inclusive plateau are ascribed to threshold effects of this mechanism, which is estimated to take effect at Fermilab energies. In the true asymptotic region the total cross section decreases like a small power of the rapidity, while fireball structure appears in the one-particle distribution. Both the exclusive (multiperipheral) and inclusive (Mueller) approaches are exploited. The discussion is {{in the language of}} statistical mechanics and <b>our</b> <b>key</b> assumptions are (<b>i)</b> existence of sensible thermodynamic limit, (ii) Koba-Nielsen-Olesen scaling, and (iii) factorization. We show that the nearest-neighbor interaction implied in the Feynman-Wilson ""gas"" by our factorizable singularity is responsible for its critical behavior at infinite rapidity. © 1978 The American Physical Society...|$|R
40|$|We {{assess the}} effect of female {{bargaining}} power on the share of educational expenditures in the household budget in India. We augment the collective household model by endogenizing female bargaining power and use a three-stage least squares approach to simultaneously estimate female bargaining power, per capita household expenditure and budget share of education. <b>Our</b> <b>key</b> results are: (<b>i)</b> female bargaining power has a positive and {{significant effect on the}} household budget share of educational spending; (ii) this bargaining power is associated positively (negatively) with education spending in urban (rural) areas; (iii) female bargaining power has a uniformly positive effect on educational expenditure of girls in urban areas among all caste groups, but the observed negative association in rural areas appears to be driven by one of the lower caste groups; and (iv) a pro-male bias exists in educational spending for all age groups, with some differentiation by location and caste...|$|R
40|$|We study {{fundamental}} aspects {{related to}} the efficient processing of the SPARQL query language for RDF, proposed by the W 3 C to encode machine-readable information in the Semantic Web. <b>Our</b> <b>key</b> contributions are (<b>i)</b> a complete complexity analysis for all operator fragments of the SPARQL query language, which – as a central result – shows that the SPARQL operator OPTIONAL alone {{is responsible for the}} PSPACE-completeness of the evaluation problem, (ii) a study of equivalences over SPARQL algebra, including both rewriting rules like filter and projection pushing that are wellknown from relational algebra optimization as well as SPARQLspecific rewriting schemes, and (iii) an approach to the semantic optimization of SPARQL queries, built on top of the classical chase algorithm. While studied {{in the context of a}} theoretically motivated set semantics, almost all results carry over to the official, bag-based semantics and therefore are of immediate practical relevance...|$|R
40|$|The {{drivers of}} CO 2 {{emissions}} are a widely studied subject {{of great importance}} to both individual countries and the global community. However, {{the inclusion of a}} quantitative measure of political uncertainty, national and global, has until now been largely overlooked. We investigate how geopolitical uncertainty (GPU) and income interact with CO 2 emissions using a panel quantile regression approach for a set of 63 nations over the period 1985 - 2014. <b>Our</b> <b>key</b> findings are; (<b>i)</b> a consistent negative (positive) relation between global (local) uncertainty and the different CO 2 emission distribution levels, (ii) the relation between uncertainty and emissions is heterogeneous across different income groups, (iii) clear and consistent evidence for the Environmental Kuztnet Curve hypothesis with respect to uncertainty, (iiii) when deciding on environmental policy, it is of great importance to consider political uncertainty and whether to use a local or global measure...|$|R
40|$|Abstract. In an {{open system}} {{we can have}} many {{different}} kinds of agents. How-ever, it is a challenge to decide which agents to pick when forming multi-agent teams. In some scenarios, agents coordinate by voting continuously. When form-ing such teams, should we focus on the diversity of the team or on the strength of each member? Can a team of diverse (and weak) agents outperform a uniform team of strong agents? We propose a new model to address these questions. <b>Our</b> <b>key</b> contributions include: (<b>i)</b> we show that a diverse team can overcome a uni-form team and we give the necessary conditions for it to happen; (ii) we present optimal voting rules for a diverse team; (iii) we perform synthetic experiments that demonstrate that both diversity and strength contribute to the performance of a team; (iv) we show experiments that demonstrate the usefulness of our model {{in one of the most}} difficult challenges for Artificial Intelligence: Computer Go 1...|$|R
40|$|This paper {{contributes}} to automatic classification and localization of human actions in video. Whereas motion {{is the key}} ingredient in modern approaches, we assess the benefits of having objects in the video representation. Rather than considering a handful of carefully selected and localized objects, we conduct an empirical study on the benefit of encoding 15, 000 object categories for action using 6 datasets totaling more than 200 hours of video and covering 180 action classes. <b>Our</b> <b>key</b> contributions are <b>i)</b> the first in-depth study of encoding objects for actions, ii) we show that objects matter for actions, and are often semantically relevant as well. iii) We establish that actions have object preferences. Rather than using all objects, selection is advantageous for action recognition. iv) We reveal that object-action relations are generic, which allows to transferring these relationships from the one domain to the other. And, v) objects, when combined with motion, improve the state-of-the-art for both action classification and localization...|$|R
40|$|We analyse the {{validity}} of common optimisations on multi-threaded programs in two memory models—the DRF guarantee and the Java Memory Model. Unlike in the single-threaded world, even simple program transformations, such as com-mon subexpression elimination, can introduce new behaviours in shared-memory multi-threading with an interleaved semantics. To validate such optimisations, most current programming languages define weaker semantics, called memory models, that aim to allow such transformations while providing reasonable guar-antees. In this thesis, we consider common program transformations and analyse their safety in the two most widely used language memory models: (i) the DRF guarantee, which promises sequentially consistent behaviours for data race free programs, and (ii) the Java Memory Model, which is the semantics of multi-threaded Java. The DRF guarantee is the semantics of Ada {{and it has been}} proposed as the semantics of the upcoming revision of C++. <b>Our</b> <b>key</b> results are: (<b>i)</b> we prove that a large class of elimination and reordering transforma...|$|R
40|$|The Author(s) 2012. This {{article is}} {{published}} with open access at Springerlink. com Abstract We administer the Allais paradox questions to both {{a representative sample}} of the Dutch population and to student subjects. Three treatments are implemented: one with the original high hypothetical payoffs, one with low hypothetical payoffs and a third with low real payoffs. <b>Our</b> <b>key</b> findings are: (<b>i)</b> violations in the non-lab sample are systematic and a large bulk of violations is likely to stem from non-familiarity with large payoffs, (ii) we can identify groups of the general population that have much higher than average violation rates; this concerns mainly the lowly educated and unemployed, and (iii) the relative treatment differences in the population at large are accurately predicted by the lab sample, but violation rates in all lab treatments are about 15 percentage points lower than in the corresponding non-lab treatments. This paper was originally entitled “Allais for all: revisiting the paradox”...|$|R
40|$|The goal of {{this paper}} is to study {{approaches}} to bridge the gap between first-order and second-order type methods for composite convex programs. <b>Our</b> <b>key</b> observations are: <b>i)</b> Many well-known operator splitting methods, such as forward-backward splitting (FBS) and Douglas-Rachford splitting (DRS), actually define a fixed-point mapping; ii) The optimal solutions of the composite convex program and the solutions of a system of nonlinear equations derived from the fixed-point mapping are equivalent. Solving this kind of system of nonlinear equations enables us to develop second-order type methods. Although these nonlinear equations may be non-differentiable, they are often semi-smooth and their generalized Jacobian matrix is positive semidefinite due to monotonicity. By combining with a regularization approach and a known hyperplane projection technique, we propose an adaptive semi-smooth Newton method and establish its convergence to global optimality. Preliminary numerical results on ℓ_ 1 -minimization problems demonstrate that our second-order type algorithms are able to achieve superlinear or quadratic convergence. Comment: 25 pages, 4 figure...|$|R
40|$|We {{describe}} an innovative {{application of a}} novel game-theoretic approach for a national scale security deployment. Working with the United States Transportation Security Administration (TSA), we have developed a new application called GUARDS to allocate the TSA’s limited resources across hundreds of security activities to provide protection at over 400 United States airports. Similar security applications (e. g., ARMOR and IRIS) have focused on one-off tailored applications and one security activity (e. g. checkpoints) per application, GUARDS {{on the other hand}} faces three new <b>key</b> issues: (<b>i)</b> reasoning about hundreds of heterogeneous security activities; (ii) reasoning over diverse potential threats; (iii) developing a system designed for hundreds of end-users. Since a national deployment precludes tailoring to specific airports, <b>our</b> <b>key</b> ideas are: (<b>i)</b> creating a new game-theoretic framework that allows for heterogeneous defender activities and compact modeling {{of a large number of}} threats; (ii) developing an efficient solution technique based on general purpose Stackelberg game solvers; (iii) taking a partially centralized approach for knowledge acquisition. The scheduling assistant has been delivered to the TSA and is currently undergoing evaluation for scheduling practices at an undisclosed airport. If successful, the TSA intends to incorporate the system into their unpredictable scheduling practices nationwide...|$|R
40|$|In {{this thesis}} I examine the {{multiplex}} correlations between art, activism, and identity using {{data collected from}} my Participatory Action Research in Art Collective, a group of socially conscious UCSD student-artists. I draw from my experiences dating from January of 2010 to the present. I begin by mapping out the student mobilizations that occurred between February 15 th and March 4 th of 2010 at UCSD {{in response to the}} UC budget crisis and the underrepresentation and under-servicing of black students on campus. This context provides {{a deeper understanding of the}} conflicts that Art Collective grappled with in our performances and activism. Next, I uncover some of the historical issues of racial and economic elitism that underscore UCSD's history and how this context informs the current power dynamics in the university's spaces. Using spatial and postcolonial theory, in addition to my personal reflections as a member of Art Collective, I assert that our public music making is a means through which we bodily reclaim institutional space and rearticulate our voices and identities on our own terms. Finally, after outlining some of <b>our</b> <b>key</b> performances, <b>I</b> situate Art Collective within a realm of processes: reflection, coalition building, and sharing subjectivities and experiences. These processes make the collective's performance style distinct, as it cultivates common bonds that strengthen community and foster personal growth. In essence, this thesis is a representation and an extension of my praxis as an artist-activist engaging in reflexive community wor...|$|R
40|$|In this paper, {{we report}} on {{experience}} in building and deploying an operational Internet broadcast system based on Overlay Multicast. In over a year, the system has been providing a cost-e#ective alternative for Internet broadcast, used by over 3600 users spread across multiple continents in home, academic and commercial environments. Technical conferences and special interest groups are the early adopters. Our experience confirms that Overlay Multicast can be easily deployed and can provide reasonably good application performance. The experience has led us to identify first-order issues that are guiding our future e#orts and are of importance to any Overlay Multicast protocol or system. <b>Our</b> <b>key</b> contributions are (<b>i)</b> enabling a real Overlay Multicast application and strengthening the case for overlays as a viable architecture for enabling group communication applications on the Internet, (ii) the details in engineering and operating a fully functional streaming system, addressing {{a wide range of}} real-world issues that are not typically considered in protocol design studies, and (iii) the data, analysis methodology, and experience that we are able to report given our unique standpoint...|$|R
40|$|This paper {{contributes}} to recent research on price dynamics using micro-price data sets. We emphasize a previously neglected aspect, {{the role of}} retailer heterogeneity. <b>Our</b> <b>key</b> findings are: (<b>i)</b> the frequency of price adjustment and the implied duration of prices varies considerably across retailers; (ii) price promotions (sales) also vary across retailers with some retailers seldom using sales, while for others sales are a common feature of pricing; (iii) the duration of reference prices is-at most- 26 weeks but the duration of reference prices is around 16 weeks for some retailers; (iv) branded products have shorter durations than private label products; (v) decomposition analysis suggests price adjustment is evenly split between sales and reference prices but, for some retailers, reference prices are {{the main source of}} price changes; (vi) there is low correlation between the frequency of price and costs changes across both products and retailers. Taken together, while confirming the significance of price stickiness after accounting for sales, price dynamics vary considerably across retailers. In turn, retailer heterogeneity has important implications for interpreting aggregate price dynamics in both theoretical and empirical research...|$|R
40|$|Spin-Hall {{oscillators}} (SHO) {{are promising}} sources of spin-wave signals for magnonics applications, and {{can serve as}} building blocks for magnonic logic in ultralow power computation devices. Thin magnetic layers used as "free" layers in SHO are in contact with heavy metals having large spin-orbital interaction, and, therefore, could {{be subject to the}} spin-Hall effect (SHE) and the interfacial Dzyaloshinskii-Moriya interaction (i-DMI), which may lead to the nonreciprocity of the excited spin waves and other unusual effects. Here, we analytically and micromagnetically study magnetization dynamics excited in an SHO with oblique magnetization when the SHE and i-DMI act simultaneously. <b>Our</b> <b>key</b> results are: (<b>i)</b> excitation of nonreciprocal spin-waves propagating perpendicularly to the in-plane projection of the static magnetization; (ii) skyrmions generation by pure spin-current; (iii) excitation of a new spin-wave mode with a spiral spatial profile originating from a gyrotropic rotation of a dynamical skyrmion. These results demonstrate that SHOs can be used as generators of magnetic skyrmions and different types of propagating spin-waves for magnetic data storage and signal processing applications...|$|R
40|$|It {{is widely}} held that {{debugging}} cyber-physical systems (CPS) is challenging. However, few empirical studies quantitatively and qualitatively capture {{the state of}} the art and {{the state of the}} practice in debugging CPS and analyze what major re-search gaps remain. This paper presents an empirical study of verification and validation in CPS through three com-plementary methods: a structured on-line survey of CPS developers and researchers, semi-structured interviews with professional CPS developers from various backgrounds, and a qualitative analysis of state of the art in research related to CPS testing. We find that traditional verification and val-idation methodologies are not sufficient for cyber-physical systems, and we identify several potential avenues for future work. <b>Our</b> <b>key</b> findings include: (<b>i)</b> many CPS developers do not use traditional verification and validation methodolo-gies and rely heavily on trial and error; (ii) simulation alone is not enough to capture dangerous bugs in CPS; (iii) it is widely acknowledged that the main challenges in CPS de-bugging are related to models of software systems, models of physics, and integration of cyber and physics models. These findings aid in identifying research directions to address the identified key challenges in CPS verification and validation...|$|R
40|$|Abstract: 2 ̆ 2 In this paper, {{we report}} on {{experience}} in building and deploying an operational Internet broadcast system based on Overlay Multicast. In over a year, the system has been providing a cost-effective alternative for Internet broadcast, used by over 3600 users spread across multiple continents in home, academic and commercial environments. Technical conferences and special interest groups are the early adopters. Our experience confirms that Overlay Multicast can be easily deployed and can provide reasonably good application performance. The experience has led us to identify first-order issues that are guiding our future efforts and are of importance to any Overlay Multicast protocol or system. <b>Our</b> <b>key</b> contributions are (<b>i)</b> enabling a real Overlay Multicast application and strengthening the case for overlays as a viable architecture for enabling group communication applications on the Internet, (ii) the details in engineering and operating a fully functional streaming system, addressing {{a wide range of}} real-world issues that are not typically considered in protocol design studies, and (iii) the data, analysis methodology, and experience that we are able to report given our unique standpoint. 2 ̆...|$|R
40|$|Consumer reports (CR) and J. D. Power % Associates (JDP) produce {{annual reports}} on cars made by US and Japanese auto-makers that {{are widely used}} by {{consumers}} in the US in making their purchase decisions. In the mainstream media, US and Japanese cars have been compared, but no systematic statistical analysis of this exists {{to the best of}} our knowledge. Further, the two sources, CR and JDP, have also not been compared for any potential correlation. In this paper, we carry out statistical tests to determine whether: i) strong correlation exists between these two sources; ii) whether US and Japanese cars have undergone any trends in the last 10 years. <b>Our</b> <b>key</b> findings are: <b>i)</b> CR and JDP are not strongly correlated; ii) although Japanese cars still outrank US cars, the latter are fast closing the gap in perceived quality, while the former are fast losing ground on brand image. automotive surveys; consumer satisfaction; quality management; advertisements; consumer perceptions; US cars; Japanese cars; consumer reports; USA; United States; Japan; automobile industry; purchase decisions; J. D. Power % Associates; JDP annual reports; statistical analysis; perceived quality; brand image...|$|R
3000|$|Infrastructure: Who are <b>our</b> <b>key</b> partners? What are <b>our</b> <b>key</b> activities? Where are <b>our</b> {{available}} <b>key</b> resources? [...]...|$|R
40|$|In {{a recent}} paper we {{introduced}} and analyzed a deterministic GIt/M/st + GI fluid model {{that can be}} used to show how queue lengths and waiting times depend on model parameters in a large-scale queueing system that experiences periods of overloading. The main feature of the model is time-varying arrival rate and staffing, but the model also includes the realistic feature of a non-exponential patience distribution. <b>Our</b> <b>key</b> assumptions were (<b>i)</b> that the scale is large (there are many servers), (ii) that the system alternates between overloaded intervals and underloaded intervals, and (iii) that the service-time distribution is exponential. Here we extend the analysis to a large class of non-exponential service distributions. To do so, we express the service content density in an overloaded interval as the solution of a fixed point equation. We apply the Banach contraction fixed point theorem to show that the equation has a unique solution and to develop an efficient algorithm. Given the service content density, all other performance measures can be calculated by previous methods. We also show how the staffing can be chosen to stabilize delays at any target value. Keywords: Large-scaleservicesystems; queueswithtime-varyingarrivals; nonstationaryqueues; many-server queues; deterministic fluid model; fluid approximation; queues with abandonment; non-Markovian queues; stabilizing delays. ...|$|R
