1|24|Public
40|$|There is a {{beautiful}} theory of integral closure of ideals in regular local rings of dimension two, due to Zariski, several aspects of which were later extended to modules. Our goal is to study integral closures of modules over normal domains by attaching divisors/determinantal ideals to them. They will be of two kinds: the <b>ordinary</b> <b>Fitting</b> ideal and its divisor, and another ‘determinantal’ ideal obtained through Noether normalization. They are useful to describe the integral closure of some class of modules and to study the completeness of the modules of Kähler differentials...|$|E
2500|$|These last {{paragraphs}} {{introduce a}} new tone, after the [...] "teasing oscillations of mood" [...] {{in the first part}} and the [...] "dry repetitions of the court documents," [...] the novella's conclusion is [...] "terse, rapid, taut with detail," [...] and for Berthoff an admirable example of [...] "Melville's <b>ordinary</b> boldness in <b>fitting</b> his performance to the whole developing occasion." ...|$|R
40|$|AbstractThe {{residual}} {{processes of}} a stationary AR(p) process and of polynomial regression are considered. The residuals are obtained from <b>ordinary</b> least squares <b>fitting.</b> In the AR case, the partial sums converge to Brownian motion. In the polynomial case, they converge to generalized Brownian bridges. Other {{uses of the}} residuals are considered. Parameter estimation based on approximate log likelihood function of the residuals is considered...|$|R
40|$|The {{residual}} {{processes of}} a stationary AR(p) process and of polynomial regression are considered. The residuals are obtained from <b>ordinary</b> least squares <b>fitting.</b> In the AR case, the partial sums converge to Brownian motion. In the polynomial case, they converge to generalized Brownian bridges. Other {{uses of the}} residuals are considered. Parameter estimation based on approximate log likelihood function of the residuals is considered. auto-regression polynomial regression weak convergence approximate likelihood least squares residuals estimation...|$|R
40|$|Law {{and lawyers}} play {{an ever more}} {{important}} role in the development and use of science. Often, the relations between law and science are the stuff of <b>ordinary</b> business, <b>fitting</b> scientific activity into an existing infrastructure of intellectual property, con-tract, products liability, and tort law. At other times lawmakers may use science to identify harms and take regulatory action, which then lead regulated industries to use law to bend or distort science in their favor. 1 One of the most important interactions between law and science is use of the legal system to create a favorable environment for innovation. 2 Especially at early stages of research, law can encourage, facilitate, or retard the development of new science and technology and the benefits it might bring. A paradigmatic case of that interplay has occurred during the past decade in the reaction to discoverie...|$|R
40|$|The spatial {{distribution}} of recent mean temperature trends over Spain during the period 1961 – 2006 at monthly, seasonal and annual time scale is carried out in this study by applying various statistical tools to data from 473 weather stations. The magnitude of trends {{was derived from the}} slopes of the linear trends using <b>ordinary</b> least-square <b>fitting.</b> The non-parametric Mann–Kendall test was used to determine the statistical significance of trends. Maps of surface temperature trends were generated by applying a geostatistical interpolation technique to visualize the detected tendencies. This study reveals that temperature has generally increased during all months and seasons of the year over the last four decades. More than 60 % of whole Spain has evidenced significant positive trends in March, June, August, spring and summer. This percentage diminishes around 40 % in April, May and December. Annual temperature has significantly risen in 100 % of Spain of around 0. 1 – 0. 2 °C/decade according to the Fourth Assessment Report of the IPCC...|$|R
40|$|Abstract. This paper {{introduces}} a Multi-Branch Genetic Programming (MB-GP) encoding applied for modelling dynamic systems. In this particular problem of modelling, Genetic Programming {{has proved to}} be an efficient alternative and has produced interesting results. However, it has been showed that, based on traditional GP representation (Koza-style), solutions tend to be complex in their structure and translation into problem domain turns difficult. Thus, the aim of the MB-GP is to reduce the complexity, producing simple models and estimating coefficients by means of an <b>ordinary</b> least squares <b>fitting</b> method. This approach is then tested on recorded data of a gas turbine engine systems, comparing its performance and complexity against previous work...|$|R
40|$|Abstract. In {{order to}} reduce the {{vibration}} load transmitted from the launch vehicle to the spacecraft during the launching stage, a whole-spacecraft vibration isolating method is advanced by modifying <b>ordinary</b> payload attach <b>fitting</b> (PAF). The core component of the PAF is a kind of damping rod produced by composite material encapsulating metal rubber (MR). In this paper, nonlinear equivalent damping model of the damping rod is built up. By using the model, the stiffness, damping and the transmissibility of the damping rod is studied attentively. Simulation results demonstrate that there exist nonlinear relationships between vibration parameters and the stiffness and damping characteristics of the damping rod. The damping rod can isolate high-frequency vibration as well as restrain the resonant peak effectively...|$|R
40|$|A {{method is}} {{described}} for {{the extraction of}} isotopic information from attenuated gamma ray spectra using the gross-count material basis set (GC-MBS) model. This method solves for the isotopic composition of an unknown mixture of isotopes attenuated through an absorber of unknown material. For binary isotopic combinations the problem is nonlinear in only one variable and is easily solved using standard line optimization techniques. Results are presented for NaI spectrum analyses of various binary combinations of enriched uranium, depleted uranium, low burnup Pu, {sup 137 }Cs, and {sup 133 }Ba attenuated through a suite of absorbers ranging in Z from polyethylene through lead. The GC-MBS method results are compared to those computed using <b>ordinary</b> response function <b>fitting</b> and with a simple net peak area method. The GC-MBS method {{was found to be}} significantly more accurate than the other methods over the range of absorbers and isotopic blends studied...|$|R
40|$|Abstract. Established the {{circular}} saw blade visual measurement system and developed geometry parameters measurement software. Calculated {{the inner and outer}} circle diameters and the roundness error, the tooth angle and the radial rake angle, the radial direction clearance angle on the basis of image processing. Calibrated the measured part using the standard gauge block. Improved the circle and the line sub-pixel methods. Experiment results showed that the fitting error of improved least–square linear fitting method was quarter of <b>ordinary</b> least–square linear <b>fitting</b> error under small difference of orientation time. The diameters of the inner circle and the outer circle,the roundness error,the tooth cusp angle and the radial rake angle, the radial direction clearance angle were respectively 25. 204 mm, 193. 624 mm, 0. 005 mm, 59. 999 °± 0. 00695 °, 15. 004 °± 0. 0104 °, 14. 997 °± 0. 0137 ° while taken sub-pixel method...|$|R
2500|$|At the {{beginning}} of the American Civil War, the Confederate government sought to counter the United States Navy in part by appealing to private enterprise world-wide to engage in privateering against United States shipping. Privateering was the practice of <b>fitting</b> <b>ordinary</b> private merchant vessels with modest armament, then sending them to sea to capture other merchant vessels in return for monetary reward. The captured vessels and cargo fell under customary prize rules at sea. [...] Prizes would be taken to the jurisdiction of a competent court, which could be in the sponsoring country or theoretically in any neutral port. If the court found that the capture was legal, the ship and cargo would be forfeited and sold at a prize auction. The proceeds would be distributed among owners and crew according to a contractual arrangement. Privateers were also authorized to attack an enemy's navy warships and then apply to the sponsoring government for direct monetary reward, usually gold or gold specie (coins).|$|R
40|$|Whether or not {{the initial}} star cluster mass {{function}} is established through a universal, galactocentric-distance-independent stochastic process, on the scales of individual galaxies, remains an unsolved problem. This debate has recently gained new impetus through {{the publication of a}} study that concluded that the maximum cluster mass in a given population is not solely determined by size-of-sample effects. Here, we revisit the evidence in favor and against stochastic cluster formation by examining the young (≲ a few × 10 ^ 8 yr-old) star cluster mass [...] galactocentric radius relation in M 33, M 51, M 83, and the Large Magellanic Cloud. To eliminate size-of-sample effects, we first adopt radial bin sizes containing constant numbers of clusters, which we use to quantify the radial distribution of the first- to fifth-ranked most massive clusters using <b>ordinary</b> least-squares <b>fitting.</b> We supplement this analysis with an application of quantile regression, a binless approach to rank-based regression taking an absolute-value-distance penalty. Both methods yield, within the 1 σ to 3 σ uncertainties, near-zero slopes in the diagnostic plane, largely irrespective of the maximum age or minimum mass imposed on our sample selection, or of the radial bin size adopted. We conclude that, at least in our four well-studied sample galaxies, star cluster formation does not necessarily require an environment-dependent cluster formation scenario, which thus supports the notion of stochastic star cluster formation as the dominant star cluster-formation process within a given galaxy. Comment: ApJ, in press, 39 pages in AAS preprint format, 10 multi-panel figures (some reduced in size to match arXiv compilation routines...|$|R
30|$|The main {{aim of this}} {{research}} was to test if fractional-order differential equation models could give better fits than integer-order models to continuous glucose monitoring (CGM) data from subjects with type 1 diabetes. In {{this research}}, real continuous glucose monitoring (CGM) data was analyzed by three mathematical models, namely, a deterministic first-order differential equation model, a stochastic first-order differential equation model with Brownian motion, and a deterministic fractional-order model. CGM data was analyzed to find optimal values of parameters by using <b>ordinary</b> least squares <b>fitting</b> or maximum likelihood estimation using a kernel-density approximation. Matlab and R programs have been developed for each model to find optimal values of the parameters to fit observed data and to test the usefulness of each model. The fractional-order model giving the best fit has been estimated for each subject. Although our results show that fractional-order models can give better fits to the data than integer-order models in some cases, {{it is clear that the}} models need further improvement before they can give satisfactory fits.|$|R
40|$|The formula {{based on}} keratometric {{readings}} {{which is generally}} used is unsatisfactory for fitting contact lenses after penetrating keratoplasty, possibly owing to lack {{of information on the}} peripheral corneal toricity. In these studies a photokeratoscope was used to examine the entire graft topography. In all cases the corneal configuration became more normal after suture removal, but a considerable toricity still remained, especially near the graft-host junction. One month after removal of sutures spherical hard contact lenses (polymethylmethacrylate, PMMA) were fitted to 30 patients in accordance with data obtained by computerised analysis of the photokeratograms. Of the 30 patients (contact lens wearers) 27 (90 %) obtained a stable vision of better than 20 / 30 for eight hours daily, and 24 (80 %) achieved a stable vision of 20 / 20 for their full waking hours. After one year the contact lens wearers showed a significant decrease in the extent of astigmatism when compared with the non-contact-lens wearers (10 patients). These results suggest that the photokeratoscope can be more useful than <b>ordinary</b> keratometers in <b>fitting</b> contact lenses after keratoplasty, and that hard contact lenses have moulding effects on the graft topography...|$|R
40|$|Prediction {{models for}} {{continuous}} bounded outcomes are often developed by <b>fitting</b> <b>ordinary</b> least square regression. However, predicted values from such method may lie outside {{the range of}} the outcome as it is bounded within a fixed range, with non-linear expectation due to the ceiling and floor effects of the bounds. Thus, regular regression models such as normal linear or nonlinear models, are inadequate for prediction purposes for bounded response variable and the use of distributions that can model different shapes are essential. Beta regression, apart from modeling different shapes and constraining predictions to an admissible range, {{has been shown to be}} superior to alternative methods for data fitting but not for prediction purposes. We take data structures into account and compared various penalized beta regression method on predictive accuracy for bounded outcome variables using optimism corrected measures. Contrary to results obtained under many regression contexts, the classical maximum likelihood method produced good predictive accuracy in terms of R 2 and RMSE. The ridge penalized beta regression performed better in terms of g-index, which is a measure of performance of the methods in external data sets. We restricted attention to prespecified models throughout and as such variable selection methods are not evaluated...|$|R
40|$|The Minimum Evolution (ME) {{approach}} to phylogeny estimation {{has been shown}} to be statistically consistent when it is used in conjunction with <b>ordinary</b> least-squares (OLS) <b>fitting</b> of a metric to a tree structure. The traditional {{approach to}} using ME has been to start with the Neighbor Joining (NJ) topology for a given matrix and then do a topological search from that starting point. The first stage requires O(n³) time, where n is the number of taxa, while the current implementations of the second are in O(p n³) or more, where p is the number of swaps performed by the program. In this paper, we examine a greedy approach to minimum evolution which produces a starting topology in O(n²) time. Moreover, we provide an algorithm that searches for the best topology using nearest neighbor interchanges (NNIs), where the cost of doing p NNIs is O(n² C p n), i. e., O(n²) in practice because p is always much smaller than n. The Greedy Minimum Evolution (GME) algorithm, when used in combination with NNIs, produces trees which are fairly close to NJ trees in terms of topological accuracy. We also examine ME under a balanced weighting scheme, where sibling subtrees have equal weight, as opposed to the standard “unweighted ” OLS, wher...|$|R
40|$|We {{present an}} {{extensive}} {{analysis of the}} relationship between star formation rate surface density () and molecular gas surface density () at sub-kpc scale in the elliptical galaxy Centaurus A (also known as NGC 5128) at the distance 3. 8 [*]Mpc. 12 CO (= 2 - 1) data from Atacama Large Millimetre/Sub-Millimetre Array SV data with very high resolution (2. 9 ′′, 0. 84 ′′), as well as 24 [*]μm data from the Spitzer Space Telescope, were used. This {{is one of the first}} studies of the SF law on Centaurus A at this very high spatial resolution. The results showed a breakdown in star formation law with a index relating and at 185 [*]pc. A significant correlation exists between surface densities of molecular gas and SFR with very long depletion time (68 [*]Gy). In addition we examined the spatially resolved relationship between velocity dispersion and star formation rate surface density for the outer disk of this galaxy and we found that the average velocity dispersion is equal to 11. 78 [*]km/s. The velocity dispersion of the molecular ISM for the outer disk is found to follow a power relation with the star formation rate surface density, where β is the slope from the <b>ordinary</b> least square <b>fitting.</b> The value of β is about and is the power law index of the star formation law...|$|R
40|$|One of the {{variables}} {{to be considered in}} developing models of vulnerability to groundwater contamination is the distance from the water table in relation to the land surface. This factor influences the time required for a contaminant to reach the aquifer, integrating itself to this analysis, the physicochemical characteristics of the overlay material and the contaminant load. Using GIS functions, data obtained directly from the registration forms of drilling wells, databases and pre-existing data from SRTM, which underwent exploratory analysis, which led to the use of <b>ordinary</b> kriging estimator, <b>fitting</b> the model with the experimental theoretical exponential model. These procedures were derived from four outcomes: (a) depth of water table, (b) groundwater flow model, (c) potentiometric curves and (d) 3 D model of the potentiometric surface. The distance of the water table to the surface indicates that there is large area of the unconfined aquifer whose depth is less than 10 meters, from which, considering the GOD method, it represents extremely vulnerable to this parameter. Another relevant analysis is the recognition that these are the most used areas by farming and by identifying the existence of two preferential directions of the groundwater flow, and the contaminant loads that may be inserted in this sub-basin which can reach the second and the third largest river of the state : Contas and Paraguaçu river respectively. Pages: 5592 - 559...|$|R
40|$|In {{this thesis}} new wave {{function}} based electron correlation methods for {{the calculation of}} different magnetic properties of large molecules are presented, namely NMR shielding tensors, magnetizability tensors, and rotational g tensors. These new methods were developed {{at the level of}} local second-order Møller-Plesset perturbation theory (LMP 2) and employ gauge-including atomic orbitals (GIAOs) to overcome the gauge origin problem. The short-range nature of dynamic electron correlation is exploited by local approximations. Density <b>fitting</b> (DF) with <b>ordinary</b> Gaussians as <b>fitting</b> functions is used to decompose the computationally expensive four-index electron-repulsion integrals. The GIAO-DF-LMP 2 program was implemented in the MOLPRO quantum chemistry package. The accuracy of the density fitting approximation and of the local approach was investigated in depth by calculations on small and medium-sized molecules. The error introduced by density fitting turns out to be negligibly small for all properties. The local approximation causes deviations from canonical calculations which are typically much smaller than the method error of MP 2 itself, particularly for NMR shielding tensors and rotational g tensors. Carbon- 13 NMR chemical shieldings differ in the range of 1 ppm from results of canonical calculations. The performance of the program was demonstrated by calculations on larger molecular systems with up to 100 atoms. The here presented methods {{open the door to the}} calculation of magnetic properties for many potentially interesting molecules which were out of reach of wave function based electron correlation methods so far...|$|R
40|$|We {{developed}} a hierarchical Bayesian model (HBM) to investigate how {{the presence of}} Seyfert activity relates to their environment, herein represented by the galaxy cluster mass, $M_{ 200 }$, and the normalized cluster-centric distance, $r/r_{ 200 }$. We achieved this by constructing an unbiased sample of galaxies from the Sloan Digital Sky Survey, with morphological classifications provided by the Galaxy Zoo Project. A propensity score matching approach is introduced to control {{for the effects of}} confounding variables: stellar mass, galaxy colour, and star formation rate. The connection between Seyfert-activity and environmental properties in the de-biased sample is modelled within an HBM framework using the so-called logistic regression technique, suitable for the analysis of binary data (e. g., whether or not a galaxy hosts an AGN). Unlike standard <b>ordinary</b> least square <b>fitting</b> methods, our methodology naturally allows modelling the probability of Seyfert-AGN activity in galaxies on their natural scale, i. e. as a binary variable. Furthermore, we demonstrate how an HBM can incorporate information of each particular galaxy morphological type in a unified framework. In elliptical galaxies, our analysis indicates a strong correlation of Seyfert-AGN activity with $r/r_{ 200 }$, and a weaker correlation with the mass of the host. In spiral galaxies these trends do not appear, suggesting that the link between Seyfert activity and the properties of spiral galaxies are independent of the environment. Comment: 11 pages, 6 figures, accepted in MNRA...|$|R
40|$|We use {{hierarchical}} Bayesian {{regression analysis}} to investigate star formation {{laws in the}} Andromeda galaxy (M 31) in both local (30, 155, and 750 pc) and global cases. We study and compare the well-known Kennicutt-Schmidt law, the extended Schmidt law and the metallicity/star formation correlation. Using a combination of Hα and 24 μm emission, a combination of far-ultraviolet and 24 μm, and the total infrared emission, we estimate the total star formation rate (SFR) in M 31 to be between 0. 35 ± 0. 04 M_yr^- 1 and 0. 4 ± 0. 04 M_yr^- 1. We produce a stellar mass surface density map using IRAC 3. 6 μm emission and measured the total stellar mass to be 6. 9 × 10 ^ 10 M_. For the Kennicutt-Schmidt law in M 31, we find the power-law index N to be between 0. 49 and 1. 18, for all the laws, the power-law index varies more with changing gas tracer than with SFR tracer. The power-law index also changes with distance from {{the centre of the}} galaxy. We also applied the commonly-used <b>ordinary</b> least squares <b>fitting</b> method and showed that using different fitting methods leads to different power-law indices. There is a correlation between the surface density of SFR and the stellar mass surface density, which confirms that the Kennicutt-Schmidt law needs to be extended to consider the other physical properties of galaxies. We found a weak correlation between metallicity, the SFR and the stellar mass surface density. Comment: 20 pages, 12 figures, 4 tables; Accepted for publication in MNR...|$|R
40|$|LISA Pathfinder (LPF), the {{precursor}} mission to a gravitational wave observatory of the European Space Agency, will measure {{the degree to}} which two test masses can be put into free fall, aiming to demonstrate a suppression of disturbance forces corresponding to a residual relative acceleration with a power spectral density (PSD) below (30 fm/sq s/Hz) (sup 2) around 1 mHz. In LPF data analysis, the disturbance forces are obtained as the difference between the acceleration data and a linear combination of other measured data series. In many circumstances, the coefficients for this linear combination are obtained by fitting these data series to the acceleration, and the disturbance forces appear then as the data series of the residuals of the fit. Thus the background noise or, more precisely, its PSD, whose knowledge is needed to build up the likelihood function in <b>ordinary</b> maximum likelihood <b>fitting,</b> is here unknown, and its estimate constitutes instead {{one of the goals of}} the fit. In this paper we present a fitting method that does not require the knowledge of the PSD of the background noise. The method is based on the analytical marginalization of the posterior parameter probability density with respect to the background noise PSD, and returns an estimate both for the fitting parameters and for the PSD. We show that both these estimates are unbiased, and that, when using averaged Welchs periodograms for the residuals, the estimate of the PSD is consistent, as its error tends to zero with the inverse square root of the number of averaged periodograms. Additionally, we find that the method is equivalent to some implementations of iteratively reweighted least-squares fitting. We have tested the method both on simulated data of known PSD and on data from several experiments performed with the LISA Pathfinder end-to-end mission simulator...|$|R
40|$|The {{objectives}} {{of this study}} were to assess the construct validity and predictive validity of a previously published comorbidity classification scheme designed for use with administrative data. The scheme groups non-primary discharge diagnoses into a set of thirty comorbidity indicators, which may be used to describe and compare populations with respect to burden of comorbid illness. The scheme was developed on a large population of hospitalized patients in California in 1992 (training population) and the predictive effect of the indicators estimated with respect to the outcomes length of stay, hospital charges, and in-hospital death. ^ The current study drew data from the Massachusetts Hospital Case Mix Data Base of 1992 (testing population). The effect of the comorbidity indicators on each outcome was estimated by <b>fitting</b> <b>ordinary</b> least squares regression (OLSR) models of length of stay and hospital charges, as well as logistic regression models of in-hospital mortality, to the testing population. The estimated effect of the comorbidity indicators on each outcome, adjusted for demographics and characteristics of index hospitalization, was compared between the training and testing populations. ^ The characteristics of the testing population were largely {{similar to those of the}} training population. The relationship between burden of comorbid illness (as measured by the number of comorbidity indicators per patient) and the outcomes was comparable in the two populations. The estimated adjusted effect of the comorbidity indicators and the predictive ability of the OLSR models were comparable in the training and testing population with respect to the outcomes length of stay and charges. The estimated adjusted effect of the comorbidity indicators on in-hospital death was not comparable in the two populations. ^ The results support construct validity and predictive validity of the comorbidity classification in Massachusetts discharge data in 1992. Other aspects of baseline risk must be accounted for separately. The estimated adjusted effect of the indicators in the training population on the outcomes length of stay and charges, but not in-hospital death, is generalizable to Massachusetts 2 ̆ 7 discharge data and may be further generalizable. Practical application of the comorbidity indicators for comorbidity adjustment in epidemiological research should be further explored. ...|$|R

