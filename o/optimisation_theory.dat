70|32|Public
5000|$|Whittle {{was elected}} a Fellow of the Royal Society in 1978, and an Honorary Fellow of the Royal Society of New Zealand in 1981. The Royal Society awarded him their Sylvester Medal in 1994 in {{recognition}} of his [...] "major distinctive contributions to time series analysis, to <b>optimisation</b> <b>theory,</b> and {{to a wide range}} of topics in applied probability theory and the mathematics of operational research". In 1986, the Institute for Operations Research and the Management Sciences awarded Whittle the Lanchester Prize for his book Systems in Stochastic Equilibrium (...) and the John von Neumann Theory Prize in 1997 for his [...] "outstanding contributions to the theory of operations research and management science".|$|E
40|$|Scheduling and multicriteria <b>optimisation</b> <b>theory</b> {{have been}} subject, separately, to {{numerous}} studies. Since {{the last twenty}} years, multicriteria scheduling problems {{have been subject to}} a growing interest. However, a gap between multicriteria scheduling approaches and multicriteria optimisation field exits. This book is an attempt to collect the elementary of multicriteria <b>optimisation</b> <b>theory</b> and the basic models and algorithms of multicriteria scheduling. It is composed of numerous illustrations, algorithms and examples which may help the reader in understanding the presented concepts. This book covers general concepts such as Pareto optimality, complexity theory, and general method for multicriteria optimisation, as well as dedicated scheduling problems and algorithms: just-in-time scheduling, flexibility and robustness, single machine problems, parallel machine problems, shop problems, etc. The second edition contains revisions and new material...|$|E
40|$|Summary: [n {{this paper}} {{many of the}} factors {{involved}} {{in the application of}} <b>optimisation</b> <b>theory</b> to the design of communication systemsare discussed. As m illustrationof the relevance of theoretical results to practical communication system design, discrete-timeand continuous-time linearfilteringtheory is brieflyreviewed and applications of the theory are discussed...|$|E
40|$|Today's {{more dynamic}} {{business}} environment increases {{the need for}} greater agility in supply chains, which increases both the importance and frequency of partner selection decision-making. Previous {{research has suggested that}} the application of the Dempster-Shafer and <b>optimisation</b> <b>theories</b> offers a way of solving this problem under conditions of resource constraints. This paper advances this approach by offering a simplified yet thorough, rigorous yet still practical method for formulating criteria to use in partner selection decision-making in agile supply chains. An empirical illustrative example is used to demonstrate the approach, obtain insights into its application and identify issues for future research. Agile supply chain Partner selection criteria Dempster-Shafer <b>theory</b> Belief acceptability <b>Optimisation...</b>|$|R
40|$|A four-phase {{conceptual}} model for supplier selection in agile supply chains (ASCs) is presented. The use of ASCs {{has become more}} common in today’s increasingly dynamic markets. However, supplier selection decisions are inherently more complex and difficult under the conditions of uncertainty and ambiguity created as supply chains form and re-form. The four phases of the model comprise Supplier selection preparation, Pre-classification, Final selection and Application feedback. It draws {{on a range of}} quantitative and qualitative techniques. These include application of the Dempster-Shafer and <b>optimisation</b> <b>theories,</b> radial basis function artificial neural networks (RBF-ANN), analytic network process-mixed integer multi-objective programming (ANP-MIMOP), Kraljic’s supplier classification matrix and the principles of continuous improvement. The resulting model offers a comprehensive and systematic approach to tackling this increasingly important task...|$|R
40|$|Empirical thesis. Bibliography: pages 374 - 380. 1. Introduction [...] 2. Naïve vs. {{sophisticated}} static optimisation models [...] 3. Optimisation rules united : harvesting {{the power}} of a syndicate [...] 4. Solving dynamic multi-period portfolio choice problems [...] 5. Conclusion [...] Appendices [...] References. Optimisation models developed since the development of modern portfolio theory diverge mainly in their approaches to the estimation mechanics of asset returns and risk parameters. The original structure of modern portfolio theory employs the estimates of the first two moments of returns, namely the mean and variance, in the optimisation algorithm. Critics of modern portfolio theory base their arguments on the fact that statistical estimates of the input parameters, namely return and risk, are subject to estimation error. As a result, the output of the modern portfolio <b>theory</b> <b>optimisation</b> model is usually unstable with counterintuitive portfolio weights. This has provoked ample research fixated on addressing those limitations by using various econometric and mathematical techniques. This research addresses two issues that constitute the crux of deliberations on portfolio choice problems. The first is the moments estimation mechanics and its effect on portfolio optimisation in a static framework. The other is the asset returns predictability and its employment in dynamic asset allocation strategies. In both cases, the performance of the portfolio optimisation rules is compared with the performance of the naïve allocation as a benchmark. This is because recent empirical evidence has highlighted the supremacy of naïve allocation over statistically driven optimisation strategies. This thesis forms an empirical investigation of the asset allocation models that appeared in the financial literature throughout the past few decades. This thesis first delves into the weaknesses and strengths of each of the <b>optimisation</b> <b>theories</b> suggestedin the financial literature, and then extends some of the <b>optimisation</b> <b>theories</b> into novel approaches. In essence, the core of this research is an investigation of the effcient market hypothesis and the effectiveness of asset returns predictability. This research endeavor is significant, first because of its empirical scrutiny of asset allocation strategies, vigorous analysis and robustness checks of those asset allocation strategies under several risk and returns configurations, and second because of its consideration of static optimisation models versus dynamic allocation techniques. Mode of access: World wide web 1 online resource (xiv, 380 pages) table...|$|R
40|$|In {{this article}} we {{establish}} a connection between semi-supervised learning and compressive sampling. We show that sparsity and compressibility of the learning function {{can be obtained from}} heavy-tailed distributions of filter responses or coefficients in spectral decompositions. In many cases the NP-hard problems of finding sparsest solutions can be replaced by l 1 -problems from convex <b>optimisation</b> <b>theory,</b> which provide effective tools for semi-supervised learning. We present several conjectures and examples...|$|E
40|$|In {{this paper}} we {{consider}} the design of jointly optimal filter-banks for transmission over frequency selective MIMO relay channels. Utilising block processing we present designs that maximise {{signal to noise ratio}} under the zero forcing constraint and minimise the mean square error. Both designs targetted in this paper assume limited power resource at the relaying stage of the network and we resort to techniques from convex <b>optimisation</b> <b>theory</b> to obtain the solutions to the constrained design problems...|$|E
40|$|This book {{provides}} a comprehensive {{treatment of the}} principles underlying optimal constrained control and estimation. The contents progress from <b>optimisation</b> <b>theory,</b> fixed-horizon discrete optimal control, receding-horizon implementations and stability conditions to explicit solutions and numerical algorithms, moving horizon estimation, and connections between constrained estimation and control. Several case studies and further developments illustrate and expand the core principles. Specific topics covered include: • An overview of <b>optimisation</b> <b>theory.</b> • Links to optimal control theory, including the discrete-minimum principle. • Linear and nonlinear receding-horizon constrained control including stability. • Constrained control solutions having a finite parameterisation for specific classes of problems. • Numerical procedures for solving constrained optimisation problems. • Output feedback optimal constrained control. • Constrained state estimation. • Duality between constrained estimation and control. • Applications to finite alphabet control and estimation problems, cross-directional control, rudder-roll stabilisation of ships, and control over communication networks. Constrained Control and Estimation is a self-contained treatment assuming that the reader has a basic background in systems theory, including linear control, stability and state-space methods. It is suitable for use in senior-level courses and as material for reference and self-study. A companion website is continually updated by the authors...|$|E
40|$|Purpose - The {{purpose of}} this paper is to present a four-phase dynamic {{feedback}} model for supply partner selection in agile supply chains (ASCs). ASCs are commonly used as a response to increasingly dynamic markets. However, partner selection in ASCs is inherently more complex and difficult under conditions of uncertainty and ambiguity as supply chains form and re-form. Design/methodology/approach - The model draws on both quantitative and qualitative techniques, including the Dempster-Shafer and <b>optimisation</b> <b>theories,</b> radial basis function artificial neural net:works (RBF-ANN), analytic network process-mixed integer multi-objective programming (ANP-MIMOP), Kraljic's supplier classification matrix and principles of continuous improvement. It incorporates modern computer programming techniques to overcome the information processing difficulties inherent in selecting from amongst large numbers of potential suppliers against multiple criteria in conditions of uncertainty. Findings - The model enables decision makers to make efficient and effective use of the vastly increased amount of data that is available in today's information-driven society and it offers a comprehensive, systematic and rigorous approach to a complex problem. Research limitations/implications - The model has two main drawbacks. First, practitioners may find it difficult to match supplier evaluation criteria with the strategic objectives for an ASC. Second, they may perceive the model to be too complex for use when speed is of the essence. Originality/value - The main contribution of this paper is that, for the first time, it draws together work from previous articles that have described each of the four stages of the model in detail to present a comprehensive overview of the model...|$|R
40|$|Whenever a {{visualization}} researcher {{is asked}} about the purpose of visualization, the phrase "gaining insight" by and large pops out instinctively. However, it is not absolutely factual that all uses of visualization are for gaining a deep understanding, unless the term insight is broadened to encompass all types of thought. Even when insight {{is the focus of}} a visualization task, it is rather difficult to know what insight is gained, how much, or how accurate. In this paper, we propose that "saving time" in accomplishing a user's task is the most fundamental objective. By giving emphasis to saving time, we can establish a concrete metric, alleviate unnecessary contention caused by different interpretations of insight, and stimulate new research efforts in some aspects of visualization, such as empirical studies, design <b>optimisation</b> and <b>theories</b> of visualization...|$|R
40|$|Abstract. It is both {{difficult}} and expensive {{to design and}} implement a software system that excels in all its functional and non-functional properties. Instead, we could envisage a software system which {{is made up of}} several software components having complementary attributes. These components are obtained from a library of software components, each having a specific functional and non-functional profile, and they are selected by a decision maker to satisfy the system requirements and optimise the quality of service under cost constraints. This paper specifically considers the problem of the optimal selection of software components with respect to their non-functional attributes, and describes a game theoretic solution by formulating the problem as a bargaining game. In addition to the game theoretic solution, the paper also discusses other alternative approaches for the optimal selection of software components. Keywords. Component-based software engineering, multi-attribute <b>optimisation,</b> game <b>theory,</b> value trade-offs, non-functional requirements, utility functions. 1...|$|R
40|$|Energy {{efficient}} {{designs of}} communication systems are receiving great attention in both academia and industry. This letter investigates the energy efficient resource allocation schemes with Quality of Service (QoS) guarantee towards green wireless communication systems. We utilise the convex <b>optimisation</b> <b>theory</b> {{to obtain the}} optimal joint subcarrier and power allocation strategy. A new solution methodology is proposed to achieve the resolutions of transcendental equations. The simulation results demonstrate that our scheme outperforms other related approaches {{in terms of the}} energy efficiency performance, QoS guarantee and implementation complexity...|$|E
40|$|Plants {{change their}} shapes, {{depending}} on their environment, for example, plant height increases with increasing population density. We examined the density-dependent plasticity in shoot morphology of herbs by analysing a mathematical model which identifies {{a number of key}} factors that influence shoot morphology, namely (i) solar radiation captured by leaves; (ii) shading from neighbouring plants; and (iii) utilisation efficiency of resource by leaves, stems and veins. An <b>optimisation</b> <b>theory</b> was used to obtain optimal shoot morphology in relation to maximal light capture by leaves, under trade-offs of resource partition among organs. We first evaluated the solar radiation flux per unit leaf area per day for different shoot forms. Our model predicts that the optimal internodal length of the stem that brings about the maximal light capture by leaves increases with plant population density, and this is consistent with experimental data. Moreover, our simple model can also be extended to explain the morphological plasticity in other herbs (i. e. stemless plants) that are different from our model plants with a stem. These findings illustrate how <b>optimisation</b> <b>theory</b> can be used for the analysis of plasticity in shoot morphology of plants in response to environmental changes, as well as the analysis of diversity in morphology. (C) 2000 Elsevier Science B. V...|$|E
40|$|A {{method for}} the {{reconstruction}} of the primordial density fluctuation field is presented. Various previous approaches to this problem rendered non-unique solutions. Here, it is demonstrated that the initial positions of dark matter fluid elements, under the hypothesis that their displacement is the gradient of a convex potential, can be reconstructed uniquely. In our approach, the cosmological reconstruction problem is reformulated as an assignment problem in <b>optimisation</b> <b>theory.</b> When tested against numerical simulations, our scheme yields excellent reconstruction on scales larger than a few megaparsecs. Comment: 14 pages, 10 figure...|$|E
40|$|The {{problem of}} expressing a {{specific}} polynomial as the determinant of a square matrix of affine-linear forms arises from algebraic geometry, <b>optimisation,</b> complexity <b>theory,</b> and scientific computing. Motivated by {{recent developments in}} this last area, we introduce {{the notion of a}} uniform determinantal representation, not of a single polynomial but rather of all polynomials in a given number of variables and of a given maximal degree. We derive a lower bound {{on the size of the}} matrix, and present a construction achieving that lower bound up to a constant factor as the number of variables is fixed and the degree grows. This construction marks an improvement upon a recent construction due to Plestenjak-Hochstenbach, and we investigate the performance of new representations in their root-finding technique for bivariate systems. Furthermore, we relate uniform determinantal representations to vector spaces of singular matrices, and we conclude with a number of future research directions. Comment: 23 pages, 3 figures, 4 table...|$|R
40|$|International audienceDecision {{theory has}} been long applied to project {{management}} for risk and uncertainty reduction. Among the foundations, the manager is considered following axioms describing his rationality the most prominent ones being transitivity and independence. The order in preferences is not supposed be reversed yet unknowns events of nature, seen as exogenous, may perturb {{our understanding of the}} given situation and may require designing new decisions going against decision theories, hence increasing uncertainty. In this paper we show that in an innovation project management, traditional decision making is not able to grasp expansion and generativity phenomena as a manager senses the unknown and endogenises it. To highlight this phenomenon we use Bayesian Nets with Wald's foundations to sense the reordering preferences in an industrial case and the benefits of designing one's playground and being intransitive. The purpose to contribute to the idea theories studying generative processes (design theory) by opposition to <b>optimisation</b> (decision <b>theory)</b> can help extend the underlying logics of innovation management and untangle the tipping point, the necessity to explore/exploit...|$|R
40|$|Biological {{and other}} natural {{processes}} {{have always been a}} source of inspiration for computer science and information technology. Many emerging problem solving techniques integrate advanced evolution and cooperation strategies, encompassing a range of spatio-temporal scales for visionary conceptualization of evolutionary computation. This book is a collection of research works presented in the VI International Workshop on Nature Inspired Cooperative Strategies for Optimization (NICSO) held in Canterbury, UK. Previous editions of NICSO were held in Granada, Spain (2006 & 2010), Acireale, Italy (2007), Tenerife, Spain (2008), and Cluj-Napoca, Romania (2011). NICSO 2013 and this book provides a place where state-of-the-art research, latest ideas and emerging areas of nature inspired cooperative strategies for problem solving are vigorously discussed and exchanged among the scientific community. The breadth and variety of articles in this book report on nature inspired methods and applications such as Swarm Intelligence, Hyper-heuristics, Evolutionary Algorithms, Cellular Automata, Artificial Bee Colony, Dynamic Optimization, Support Vector Machines, Multi-Agent Systems, Ant Clustering, Evolutionary Design <b>Optimisation,</b> Game <b>Theory</b> and other several Cooperation Models...|$|R
40|$|AbstractA basic {{result of}} <b>optimisation</b> <b>theory</b> {{is that a}} saddle-point of the Lagrangian is an optimum of the {{associated}} programming problem, independently of any concavity assumptions. It is also well known that under concavity assumptions the two are equivalent; i. e., an optimum is always a saddle-point. It is demonstrated that this basic equivalence of saddle-points and optima in fact holds for a much larger class of problems, which are not necessarily concave, but are equivalent to concave programmes up to a diffeomorphism. This class generalises the class of geometric programmes...|$|E
40|$|A {{common problem}} faced by fire safety {{engineers}} {{in the field}} of evacuation analysis concerns the optimal design of an arbitrarily complex structure in order to minimise evacuation times. How does the engineer determine the best solution? In this study we introduce the concept of numerical optimisation techniques to address this problem. The study makes user of the buildingEXODUS evacuation model coupled with classical <b>optimisation</b> <b>theory</b> including Design of Experiments (DoE) and Response Surface Models (RSM). We demonstrate the technique using a relatively simple problem of determining the optimal location for a single exit in a square room...|$|E
40|$|A novel {{optimising}} controller {{is designed}} that leads {{a slow process}} from a sub-optimal operational condition to the steady-state optimum in a continuous way based on dynamic information. Using standard results from <b>optimisation</b> <b>theory</b> and discrete optimal control, the solution of a steady-state optimisation problem is achieved by solving a receding-horizon optimal control problem which uses derivative and state information from the plant via a shadow model and a state-space identifier. The paper analyzes the steady-state optimality of the procedure, develops algorithms with and without control rate constraints and applies the procedure to a high fidelity simulation study of a distillation column optimisation...|$|E
40|$|Abstract: In this note, {{a direct}} proof is given of the NP-completeness of {{a variant of}} Graph Coloring, i. e., a generic proof similar to the proof of Cook of the NPcompleteness of Satisfiability. Then, transformations from this variant of Graph Coloring to Independent Set and to Satisfiability are shown. These proofs could be useful in an {{educational}} setting, where basics {{of the theory of}} NP-completeness must be explained to students whose background in combinatorial <b>optimisation</b> and/or graph <b>theory</b> is stronger than their background in logic. In addition, I believe that the proof given here is slightly easier than older generic proofs of NP-completeness. Keywords: NP-completeness, computational complexity, graphs, educatio...|$|R
40|$|In this note, {{a direct}} proof is given of the NP-completeness of {{a variant of}} GRAPH COLORING, i. e., a generic proof is given, similar to the proof of Cook of the NP-completeness of SATISFIABILITY. Then, transformations from this variant of GRAPH COLORING to INDEPENDENT SET and to SATISFIABILITY are given. These proofs could be useful in an {{educational}} setting, where basics {{of the theory of}} NP-completeness must be explained to students whose background in combinatorial <b>optimisation</b> and/or graph <b>theory</b> is stronger than their background in logic. In addition, I believe that the proof given here is slightly easier than older generic proofs of NP-completeness...|$|R
40|$|This {{document}} {{serves as}} introduction to two important methodologies {{that have been}} developed in PSO (Product and System <b>Optimisation)</b> : Game <b>Theory</b> is a new approach for multi-objective optimisation problems, based on combination of classic mono-objective algorithms with a competitive Game whose goal is the search of the Nash equilibrium, as best compromise of contrasting objectives; Robust Design is a methodology of optimisation whose goal is to find a solution not optimal only in a single design point, but that conserves good properties also for slight variations of some operative variables. The definition of these methodologies has been required in the last FENET workshop held on Marc...|$|R
40|$|By {{applying}} recent {{results in}} <b>optimisation</b> <b>theory</b> variously known as optimisation transfer or majorise/minimise algorithms, an algorithm for binary, kernel, Fisher discriminant analysis is introduced that {{makes use of}} a non-smooth penalty on the coefficients to provide a parsimonious solution. The problem is converted into a smooth optimisation that can be solved iteratively with no greater overhead than iteratively re-weighted least-squares. The result is simple, easily programmed and is shown to perform, {{in terms of both}} accuracy and parsimony, as well as or better than a number of leading machine learning algorithms on two well-studied and substantial benchmarks...|$|E
40|$|During {{the last}} two decades, many {{heuristic}} procedures for the joint replenishment problem {{have appeared in the}} literature. The only available optimal solution procedure was based on an enumerative approach and was computationally prohibitive. In this paper we present an alternative optimal approach based on global <b>optimisation</b> <b>theory.</b> By applying Lipschitz optimisation one can find a solution with an arbitrarily small deviation from an optimal value. An efficient procedure is presented which uses a dynamic Lipschitz constant and generates a solution in little time. The running time of this procedure grows only linearly in the number of items. inventory;joint replenishment;Lipschitz optimisation;global optimisation;multi-item...|$|E
40|$|This {{paper is}} {{concerned}} with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provide arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical <b>optimisation</b> <b>theory.</b> Simulation examples demonstrate the efficacy of our proposed solution...|$|E
40|$|Exergy Analysis {{has been}} {{identified}} in the literature as a powerful tool to benchmark the resource efficiency of thermal systems. The exergy approach provides a rational basis for process <b>optimisation,</b> where, in <b>theory,</b> the processes with the greatest exergy destruction represent the greatest energy efficiency opportunities. Exergy analysis of a Waste Water Treatment Plant (WWTP) has been performed. In addition, two separate reference environments for WWTPs are defined based on plant location. Biological oxygen demand was identified as the most useful parameter when calculating the chemical exergy of organic matter in waste water. The {{results of this study}} indicate that organic matter is the principal contributor to chemical exergy values and that exergy analysis is a useful approach to identify inefficient processes within a WWTP...|$|R
40|$|The {{applicability}} of complex systems theory in economics is evaluated and compared with standard approaches to economic theorising based upon constrained optimisation. A complex system {{is defined in}} the economic context and differentiated from complex systems in physio-chemical and biological settings. It is explained why {{it is necessary to}} approach economic analysis from a network, rather than a production and utility function perspective, when we are dealing with complex systems. It is argued that much of heterodox thought, particularly in neo-Schumpeterian and neo-Austrian evolutionary economics, can be placed within a complex systems perspective upon the economy. The challenge is to replace prevailing ‘simplistic’ theories, based in constrained <b>optimisation,</b> with ‘simple’ <b>theories,</b> derived from network representations in which value is created through the establishment of new connections between elements...|$|R
40|$|Inpainting-based image {{compression}} is an emerging paradigm for compressing visual {{data in a}} completely different way than popular transform-based methods such as JPEG. The underlying idea sounds very simple: One stores only a small, carefully selected subset of the data, which results in a substantial reduction of the file size. In the decoding phase, one interpolates the missing data by means of a suitable inpainting process. It propagates information from the known data into the areas where nothing has been stored, e. g. by solving a partial differential equation or by clever copy-and-paste mechanisms. Inpainting-based codecs (coders and decoders) are more intuitive than transform-based ones, they are closer to biological mechanisms in our brain, and first results show that they may offer promising performance for high compression rates. However, before these ideas become practically viable, a number of difficult fundamental problems must be solved first. They involve e. g. the selection of the data and the inpainting operator, coding strategies, and the search for highly efficient numerical algorithms. This requires a collaborative effort of experts in data compression, inpainting, <b>optimisation,</b> approximation <b>theory,</b> numerical algorithms, and biological vision. In this Dagstuhl seminar we have brought together leading researcher from all these fields for the first time. It enabled a very fruitful and inspiring interaction which will form the basis for future progress...|$|R
40|$|The {{relative}} entropy S(ρ||σ) is frequently {{used as a}} distance, or distinguishability measure between two states ρ and σ. In this paper we study the relation between this measure {{and a number of}} other measures used for that purpose, including the trace norm distance. More precisely, we derive lower and upper bounds on the {{relative entropy}} S(ρ||σ) in terms of various distance measures for ρ-σ based on unitarily invariant norms. The upper bounds can be considered as statements of continuity of the relative entropy distance in the sense of Fannes. We employ <b>optimisation</b> <b>theory</b> to obtain bounds that are as sharp as possible...|$|E
40|$|In {{this note}} we {{put forward a}} {{conjecture}} on the average optimal length for bipartite matching with {{a finite number of}} elements where the different lengths are independent one from the others and have an exponential distribution. The problem of random bipartite matching (or assignment) is interesting both {{from the point of view}} of <b>optimisation</b> <b>theory</b> and of statistical mechanics [1, 2, 3, 4]. Each instance of the problem is characterised by a N cross N matrix d; sometimes d(i, k) represents the distance between i and k. We are interested to compute the length of the optimal bipartite matching defined as: L(d) = min Π i= 1,...|$|E
40|$|Enhanced {{understanding}} of the factors determining transnational companies’ localisation decisions is important for regulators and other stakeholders concerned about maintaining current activity levels in a petroleum producing country. This article discusses localisation decisions {{in the context of}} theories of industrial clusters and real portfolio <b>optimisation</b> <b>theory</b> (materiality), which we argue are two fruitful lines of explanation for transnational companies’ behaviour. The industrial cluster literature is concerned about the level of positive externalities associated with geographic clustering of related production activities. The concept of materiality, implying that investment projects in an oil province must be of a certain minimum size in order to be interesting for oil companies, is evaluated empirically and compared to predictions of mainstream economic theory...|$|E
40|$|International audienceDensity {{functional}} <b>theory</b> <b>optimisations</b> with GGA functionals {{that include}} an empirical correction {{term for the}} dispersion energy (denoted as -D) have been performed for the van der Waals dimer of porphine. Interaction energies have been also obtained with a recently developed double-hybrid functional (B 2 -PLYP-D) and at the SCS-MP 2 level of theory. The preferred conformer is a parallel displaced complex (1. 72 A lateral displacement) with an interplanar distance of 3. 27 A. The orientation (rotation) of the two monomers has only minor effects (< 1 kcal/mol) on the binding energy ΔE, implying a certain flexibility of mutual movement around the monomer principal axes. The best estimate for ΔE is - 25 kcal/mol with B 2 -PLYP-D. The also investigated T-shaped structures are much higher (about 15 kcal/mol) in energy. While dispersion contributions are absolutely essential for the binding of all investigated structures, the electrostatic contributions mainly determine the preferred conformations (e. g. displacement vs. rotation) ...|$|R
40|$|International audienceIn the {{so-called}} ‘patch problem', {{at any given}} moment, the forager must decide whether to leave the current patch or to remain there and continue foraging. Optimal foraging theory and subsequent theoretical works have identified theoretical optimal policies governing this decision. In a stochastic environment, the Bayesian framework {{has proved to be}} effective. A set of mechanistic proximal mechanisms explaining how parasitoid wasps may take decisions has been proposed. These mechanisms are based in on changes in the degree of motivation to continue foraging during a particular foraging episode. Using a simple, straightforward model, we show here that the psychological mechanism proposed mimics precisely the theoretical Bayesian solution, provided that motivation displays exponential decay, rather than the linear pattern of decay initially assumed. Changes in motivation thus function as a sort of analogue computer, and may be seen as more than purely heuristic rules of thumb. This link between psychological processes and ultimate <b>optimisation</b> places foraging <b>theory</b> in the domain of neuroeconomics...|$|R
40|$|Readership: broad, from {{graduate}} students with calculus and mathematical statistics prerequisites to researchers to instructors. Somehow, I {{had missed the}} first edition of this book (Lange, 1999) and thus I read it with a newcomers eyes. (Obviously, I will not comment on the differences with the first edition, sketched by the author in the Preface). Past the initial surprise of discovering Numerical Analysis for Statisticians was a mathematics book rather than an algorithmic book, I became quickly engrossed into my reading, so fascinating a book it is! Numerical Analysis for Statisticians provides most of the necessary background in calculus and some algebra to conduct rigorous numerical analyses of statistical problems and models. As indicated above by the table of contents, the coverage includes expansions, eigen-analysis, <b>optimisation,</b> integration, approximation <b>theory,</b> and simulation, in exactly 600 pages. The quality of the coverage is uniformly superb, including the simulation chapters that extend all the way to functional distances for MCMC convergence assessment and to Rao...|$|R
