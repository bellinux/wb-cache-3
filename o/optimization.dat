10000|10000|Public
5|$|TuneUp Utilities 2013: Mainly {{improved}} {{in the area}} of disk cleanup and performance <b>optimization</b> via the Program Deactivator and the Live <b>Optimization.</b> It supports Windows 8.|$|E
25|$|Multi-objective <b>optimization</b> (also {{known as}} multi-objective programming, vector <b>optimization,</b> multicriteria <b>optimization,</b> multiattribute <b>optimization</b> or Pareto <b>optimization)</b> {{is an area}} of {{multiple}} criteria decision making, that is concerned with mathematical <b>optimization</b> problems involving more than one objective function to be optimized simultaneously. Multi-objective <b>optimization</b> has been applied in many fields of science, including engineering, economics and logistics where optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives. Minimizing cost while maximizing comfort while buying a car, and maximizing performance whilst minimizing fuel consumption and emission of pollutants of a vehicle are examples of multi-objective <b>optimization</b> problems involving two and three objectives, respectively. In practical problems, there can be more than three objectives.|$|E
25|$|In microeconomics, {{the utility}} {{maximization}} problem and its dual problem, the expenditure minimization problem, are economic <b>optimization</b> problems. Insofar as they behave consistently, consumers {{are assumed to}} maximize their utility, while firms are usually assumed to maximize their profit. Also, agents are often modeled as being risk-averse, thereby preferring to avoid risk. Asset prices are also modeled using <b>optimization</b> theory, though the underlying mathematics relies on optimizing stochastic processes rather than on static <b>optimization.</b> International trade theory also uses <b>optimization</b> to explain trade patterns between nations. The <b>optimization</b> of portfolios {{is an example of}} multi-objective <b>optimization</b> in economics.|$|E
40|$|Although code <b>optimizations</b> are {{necessary}} to parallelize code, few guidelines exist for determining when and where to apply <b>optimizations</b> to produce the most efficient code. The order of applying <b>optimizations</b> can also {{have an impact on}} the efficiency of the final target code. However, determining the appropriate <b>optimizations</b> is difficult due to the complex interactions among the <b>optimizations,</b> scheduler and architecture. To aid in selecting appropriate <b>optimizations,</b> an optimizer generator (Genesis) is presented that produces an optimizer from specifications of <b>optimizations.</b> This paper describes the design and implementation of Genesis and demonstrates how such a generator could be used by optimizer designers. Some experiences with the generator are also described. key words: Parallelizing compiler <b>optimizations</b> Automatic generation Transformation...|$|R
40|$|Though {{verification}} {{tools are}} finding industrial use, {{the utility of}} engineering <b>optimizations</b> that make them scalable and usable is not widely known. Despite the fact that several <b>optimizations</b> are part of folklore in the communities that develop these tools, no rigorous evaluation of these <b>optimizations</b> has been done before. We describe and evaluate several engineering <b>optimizations</b> implemented in the Yogi property checking tool, including techniques to pick an initial abstraction, heuristics to pick predicates for refinement, <b>optimizations</b> for interprocedural analysis, and <b>optimizations</b> for testing. We believe that our empirical evaluation gives the verification community useful information about which <b>optimizations</b> they could implement in their tools, and what gains they can realistically expect from these <b>optimizations...</b>|$|R
40|$|In {{this paper}} we analyze {{the effect of}} {{compiler}} <b>optimizations</b> on fine grain parallelism in scalar programs. We characterize three levels of optimization: classical, superscalar, and multiprocessor. We show that classical <b>optimizations</b> not only improve a program's efficiency but also its parallelism. Superscalar <b>optimizations</b> further improve the parallelism for moderately parallel machines. For highly parallel machines, however, they actually constrain available parallelism. The multiprocessor <b>optimizations</b> we consider are memory renaming and data migration. Introduction Compiler <b>optimizations</b> are designed to reduce a program's execution time. Traditionally, these <b>optimizations</b> are customized for a given machine model. Classical <b>optimizations</b> are designed to improve the program's efficiency for a machine model which has one thread of execution and can issue one instruction per cycle. Superscalar <b>optimizations</b> are designed for a machine model with a single thread of execution and a lim [...] ...|$|R
25|$|<b>Optimization</b> can be {{automated}} by compilers or {{performed by}} programmers. Gains are usually limited for local <b>optimization,</b> and larger for global optimizations. Usually, {{the most powerful}} <b>optimization</b> {{is to find a}} superior algorithm.|$|E
25|$|Many design {{problems}} {{can also be}} expressed as <b>optimization</b> programs. This application is called design <b>optimization.</b> One subset is the engineering <b>optimization,</b> and another recent and growing subset of this field is multidisciplinary design <b>optimization,</b> which, while useful in many problems, has in particular been applied to aerospace engineering problems.|$|E
25|$|Post {{placement}} <b>optimization</b> before CTS performs netlist <b>optimization</b> with ideal clocks. It can fix setup, hold, max trans/cap violations. It can do placement <b>optimization</b> {{based on}} global routing. It re does HFN synthesis.|$|E
40|$|Understanding the {{performance}} impact of compiler <b>optimizations</b> on superscalar processors is complicated because compiler <b>optimizations</b> {{interact with the}} microarchitecture in complex ways. This paper analyzes this interaction using interval analysis, an analytical processor model that allows for breaking total execution time into cycle components. By studying the impact of compiler <b>optimizations</b> on the various cycle components, one can gain insight into how compiler <b>optimizations</b> affect out-of-order processor performance. The analysis provided in this paper reveals various interesting insights and suggestions for future work on compiler <b>optimizations</b> for out-of-order processors. In addition, we contrast the effect compiler <b>optimizations</b> have on out-of-order versus in-order processors. ...|$|R
5000|$|The {{compiler}} {{provides a}} large set of high-level <b>optimizations</b> {{as well as}} target-specific <b>optimizations</b> to produce faster or smaller code. It is also able to optimize across functions and modules. Target-independent <b>optimizations</b> includes: ...|$|R
40|$|There are {{two kinds}} of {{compiler}} optimizations: machine-dependent <b>optimizations</b> and machine-independent <b>optimizations.</b> For specialized architectures such as a DSP architecture, machine-dependent <b>optimizations</b> play a very important role, because these <b>optimizations</b> seek to exploit the special hardware features of the target processor to maximize performance. However, using these machine-dependent <b>optimizations</b> alone in a compiler requires that programmers write their programs in a specific style. It is proposed that certain machine-independent <b>optimizations</b> will allow programmers to realize the benefits of machine-dependent <b>optimizations</b> without imposing overly restrictive rules on the programming style. An existing SUIF-based C compiler is retargeted to the U of T DSP architecture and extended to include several machine-independent <b>optimizations</b> such as induction variable creation, loop-invariant code motion and dependence analysis. Two versions of several existing DSP benchmarks are written in a natural C-style. The augmented compiler is used to compile these versions and the performance of the generated code is analyzed an...|$|R
25|$|<b>Optimization</b> {{has been}} widely used in civil {{engineering}}. The most common civil engineering problems that are solved by <b>optimization</b> are cut and fill of roads, life-cycle analysis of structures and infrastructures, resource leveling and schedule <b>optimization.</b>|$|E
25|$|Mathematical <b>optimization</b> {{is used in}} much modern {{controller}} design. High-level controllers such as model {{predictive control}} (MPC) or real-time <b>optimization</b> (RTO) employ mathematical <b>optimization.</b> These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical <b>optimization</b> problem including constraints and {{a model of the}} system to be controlled.|$|E
25|$|The {{method of}} Lagrange {{multipliers}} {{can be used}} to reduce <b>optimization</b> problems with constraints to unconstrained <b>optimization</b> problems.|$|E
40|$|Partial {{redundancy}} elimination (PRE) techniques play {{an important}} role in optimizing compilers. Many <b>optimizations,</b> such as elimination of redundant expressions, communication <b>optimizations,</b> and load-reuse <b>optimizations,</b> employ PRE as an underlying technique for improving the efficiency of a program. Classical approache...|$|R
40|$|Abstract. Effective <b>optimizations</b> for {{concurrent}} programs {{require the}} compiler to have detailed {{knowledge about the}} scheduling of parallel tasks at runtime. Currently, <b>optimizations</b> for parallel programs must define their own models and analyses of the parallel constructs used in the source programs. This makes developing new <b>optimizations</b> more difficult and complicates their integration into a single optimizing compiler. We investigate an approach that separates the static analysis of the dynamic runtime schedule from subsequent <b>optimizations.</b> We present three <b>optimizations</b> {{that are based on}} the information gathered during the schedule analysis. Variants of those <b>optimizations</b> have been described in the literature before but each work is built upon its own highly specialized analysis. In contrast, our independent schedule analysis shows synergistic effects where previously incompatible <b>optimizations</b> can now share parts of their implementation and all be applied to the same program. ...|$|R
40|$|Traditional {{compiler}} <b>optimizations</b> such as {{loop invariant}} removal and common sub-expression elimination are standard in all optimizing compilers. The {{purpose of this}} paper is to present new versions of these <b>optimizations</b> that apply to programs using dynamicallyallocated data structures, and to show the effect of these <b>optimizations</b> on the performance of multithreaded programs. In this paper we show how heap pointer analyses can be used to support better dependence testing, new applications of the above traditional <b>optimizations,</b> and high-quality code generation for multithreaded architectures. We have implemented these analyses and <b>optimizations</b> in the EARTH-C compiler to study their impact on the performance of generated multithreaded code. We provide both static and dynamic measurements showing the effect of the <b>optimizations</b> applied individually, and together. We note several general trends, and discuss the performance tradeoffs, and suggest when specific <b>optimizations</b> are general [...] ...|$|R
25|$|Another {{field that}} uses <b>optimization</b> {{techniques}} extensively is operations research. Operations research also uses stochastic {{modeling and simulation}} to support improved decision-making. Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale <b>optimization</b> and stochastic <b>optimization</b> methods.|$|E
25|$|Ant colony <b>optimization</b> (ACO) based <b>optimization</b> of 45nm CMOS-based sense {{amplifier}} circuit could converge to optimal solutions in very minimal time.|$|E
25|$|Black-Litterman model <b>optimization</b> is an {{extension}} of unconstrained Markowitz <b>optimization</b> that incorporates relative and absolute 'views' on inputs of risk and returns.|$|E
50|$|AOT compilers {{can perform}} complex and {{advanced}} code <b>optimizations,</b> which {{in most cases}} of JITing will be considered much too costly. In contrast, AOT usually cannot perform some <b>optimizations</b> possible in JIT, like runtime profile-guided <b>optimizations,</b> pseudo-constant propagation, or indirect-virtual function inlining.|$|R
5000|$|The middle end {{performs}} <b>optimizations</b> on {{the intermediate}} representation {{in order to}} improve the performance {{and the quality of the}} produced machine code. The middle end contains those <b>optimizations</b> that are independent of the CPU architecture being targeted. Common <b>optimizations</b> phases include: ...|$|R
40|$|Energy {{consumption}} and power dissipation are increasingly becoming important design constraints in high performance microprocessors. Compilers traditionally are {{not exposed to}} the energy details of the processor. However, with the increasing power/energy problem, {{it is important to}} evaluate how the existing compiler <b>optimizations</b> influenceenergy {{consumption and}} power dissipation in the processor. In this paper we present a quantitative study wherein we examine the effect of the standard <b>optimizations</b> levels -O 1 to -O 4 of DEC Alpha's cc compiler on power and energy of the processor. We also evaluate the effect of four individual <b>optimizations</b> on power/energy and attempt to classify them as "low energy" or "low power" <b>optimizations.</b> In our experiments we find that <b>optimizations</b> that improve performance by reducing the number of instructions are optimized for energy. Such <b>optimizations</b> reduce the total amount of work done by the program. This is in contrast to <b>optimizations</b> that improve [...] ...|$|R
25|$|LP solvers are in {{widespread}} use for <b>optimization</b> of various problems in industry, such as <b>optimization</b> of flow in transportation networks.|$|E
25|$|Multi-objective <b>optimization</b> {{problems}} have been generalized further into vector <b>optimization</b> problems where the (partial) ordering is no longer given by the Pareto ordering.|$|E
25|$|Figure 1 {{depicts a}} {{geometry}} <b>optimization</b> of the atoms in a carbon nanotube {{in the presence}} of an external electrostatic field. In this <b>optimization,</b> the atoms on the left have their positions frozen. Their interaction with the other atoms in the system are still calculated, but alteration the atoms' position during the <b>optimization</b> is prevented.|$|E
40|$|Code {{space is}} a {{critical}} issue facing designers of software for embedded systems. Many traditional compiler <b>optimizations</b> are designed to reduce the execution time of compiled code, but not necessarily {{the size of the}} compiled code. Further, di#erent results can be achieved by running some <b>optimizations</b> more than once and changing the order in which <b>optimizations</b> are applied. Register allocation only complicates matters, as the interactions between di#erent <b>optimizations</b> can cause more spill code to be generated. The compiler for embedded systems, then, must take care to use the best sequence of <b>optimizations</b> to minimize code space...|$|R
40|$|In {{this paper}} we analyze the e ect of {{compiler}} <b>optimizations</b> on ne grain parallelism in scalar programs. We characterize {{three levels of}} optimization: classical, superscalar, and multiprocessor. We show that classical <b>optimizations</b> not only improve a program's e ciency but also its parallelism. Superscalar <b>optimizations</b> further improve the parallelism for moderately parallel machines. For highly parallel machines, however, they actually constrain available parallelism. The multiprocessor <b>optimizations</b> we consider are memory renaming and data migration...|$|R
50|$|Such <b>optimizations</b> become hard to spot {{by humans}} when the code {{is more complex}} and other <b>optimizations,</b> like inlining, take place.|$|R
25|$|Fractional {{programming}} studies <b>optimization</b> of {{ratios of}} two nonlinear functions. The special class of concave fractional {{programs can be}} transformed to a convex <b>optimization</b> problem.|$|E
25|$|Particle swarm <b>optimization</b> is an {{algorithm}} modelled on swarm {{intelligence that}} finds {{a solution to}} an <b>optimization</b> problem in a search space, or model and predict social behavior {{in the presence of}} objectives.|$|E
25|$|In-placement <b>optimization</b> re-optimizes {{the logic}} based on VR. This can perform cell sizing, cell moving, cell bypassing, net splitting, gate duplication, buffer insertion, area recovery. <b>Optimization</b> {{performs}} iteration of setup fixing, incremental timing and congestion driven placement.|$|E
40|$|Class-specific <b>optimizations</b> are {{compiler}} <b>optimizations</b> {{specified by}} the class implementor to the compiler. They allow the compiler {{to take advantage of}} the semantics of the particular class so as to produce better code. <b>Optimizations</b> of interest include the strength reduction of class:: array address calculations, elimination of large temporaries, and the placement of asynchronous send/recv calls so as to achieve computation/communication overlap. We will outline our progress towards the implementation of a C++ compiler capable of incorporating class-specific <b>optimizations...</b>|$|R
5000|$|Cone tweeters {{have the}} same basic design and form as a woofer with <b>optimizations</b> to operate at higher frequencies. The <b>optimizations</b> usually are: ...|$|R
40|$|Scalar {{analyses}} and <b>optimizations</b> are indispensable and challenging in parallelizing and optimizing compilers. Scalar analyses statically collect {{information on how}} programs manipulate variables. Scalar <b>optimizations,</b> on the other hand, improve {{the efficiency of the}} compiled codes and the precision of the analyzed results. Performing scalar {{analyses and}} <b>optimizations</b> is challenging, especially when the programming languages being considered support general pointer usage. This is because many existing pointer analysis algorithms are either imprecise or inefficient, and the interactions between analyses and <b>optimizations</b> raise the issue of ordering the analyses and the <b>optimizations.</b> In this dissertation, we propose a new approach to perform scalar analyses and <b>optimizations.</b> We first analyze both pointer and non-pointer variables interprocedurally. The analyzed results are represented by an extended version of the traditional static single assignment (SSA) form. We then perform the opt [...] ...|$|R
