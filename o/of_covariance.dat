6845|10000|Public
25|$|The {{choice of}} {{how to deal with}} an outlier should depend on the cause. Some estimators are highly {{sensitive}} to outliers, notably estimation <b>of</b> <b>covariance</b> matrices.|$|E
25|$|For {{continuous}} outcome data, analysis <b>of</b> <b>covariance</b> (e.g., {{for changes}} in blood lipid levels after receipt of atorvastatin after acute coronary syndrome) tests the effects of predictor variables.|$|E
2500|$|... for [...] (otherwise [...] is singular, but {{substantially}} {{the same}} result holds for [...] ). Here, [...] denotes the likelihood of [...] from a multivariate normal distribution with zero mean and covariance matrix [...] Therefore, for [...] and , [...] is the above maximum-likelihood estimator. See estimation <b>of</b> <b>covariance</b> matrices for details on the derivation.|$|E
5000|$|... where [...] is {{the sill}} {{variation}} and [...] is the vector <b>of</b> <b>covariances</b> <b>of</b> residuals at the unvisited location.|$|R
40|$|We {{consider}} asymptotic {{distributions of}} maximum deviations <b>of</b> sample <b>covariance</b> matrices, a fundamental problem in high-dimensional inference <b>of</b> <b>covariances.</b> Under mild dependence {{conditions on the}} entries of the data matrices, we establish the Gumbel convergence of the maximum deviations. Our result substantially generalizes earlier ones where the entries {{are assumed to be}} independent and identically distributed, and it provides a theoretical foundation for high-dimensional simultaneous inference <b>of</b> <b>covariances.</b> Comment: 21 pages, one supplementary fil...|$|R
5000|$|Covariance matrix, {{a matrix}} <b>of</b> <b>covariances</b> between {{a number of}} {{variables}} ...|$|R
2500|$|The general {{formulation}} <b>of</b> <b>covariance</b> and contravariance {{refer to}} how the components of a coordinate vector transform under a change of basis (passive transformation). [...] Thus let V be a vector space of dimension n over the field of scalars S, and let each of [...] and [...] be a basis of V. to V. [...] Regarding f as a row vector whose entries are {{the elements of the}} basis, the associated linear isomorphism is then [...] Also, let the change of basis from f to f′ be given by ...|$|E
2500|$|The {{works of}} {{physicists}} such as James Clerk Maxwell, and [...] mathematicians Gregorio Ricci-Curbastro and Tullio Levi-Civita {{led to the}} development of tensor analysis and the notion <b>of</b> <b>covariance,</b> which identifies an intrinsic geometric property as one that is invariant with respect to [...] coordinate transformations. [...] These ideas found a key application in Einstein's theory of [...] general relativity and its underlying equivalence principle. A modern definition of a 2-dimensional manifold was given by Hermann Weyl in his 1913 book on Riemann surfaces. [...] The widely accepted general definition of a manifold in terms of an atlas is due to Hassler Whitney.|$|E
2500|$|Here [...] is the covariance, {{which is}} zero for {{independent}} random variables (if it exists). The formula states that the variance of a sum {{is equal to the}} sum of all elements in the covariance matrix of the components. The next expression states equivalently that the variance of the sum is the sum of the diagonal <b>of</b> <b>covariance</b> matrix plus two times the sum of its upper triangular elements (or its lower triangular elements); this emphasizes that the covariance matrix is symmetric. This formula is used in the theory of Cronbach's alpha in classical test theory.|$|E
40|$|We {{present the}} NNDC-BNL {{methodology}} for estimating neutron cross section covariances in thermal, resolved resonance, unresolved resonance and fast neutron regions. The three {{key elements of}} the methodology are Atlas of Neutron Resonances, nuclear reaction code EMPIRE, and the Bayesian code implementing Kalman filter concept. The covariance data processing, visualization and distribution capabilities are integral components of the NNDC methodology. We illustrate its application on examples including relatively detailed evaluation <b>of</b> <b>covariances</b> for two individual nuclei and massive production <b>of</b> simple <b>covariance</b> estimates for 307 materials. Certain peculiarities regarding evaluation <b>of</b> <b>covariances</b> for resolved resonances and the consistency between resonance parameter uncertainties and thermal cross section uncertainties are also discussed...|$|R
50|$|Thus, in {{an equally}} {{weighted}} portfolio, the portfolio variance tends {{to the average}} <b>of</b> <b>covariances</b> between securities {{as the number of}} securities becomes arbitrarily large.|$|R
5000|$|... {{where we}} have now made used <b>of</b> {{translational}} <b>covariance,</b> which is part <b>of</b> the Poincaré <b>covariance.</b> Thus: ...|$|R
2500|$|In {{hierarchical}} architectures, {{one layer}} {{is not necessarily}} invariant to all transformations that are handled by the hierarchy as a whole. Some transformations may pass through that layer to upper layers, {{as in the case}} of non-group transformations described in the previous section. For other transformations, an element of the layer may produce invariant representations only within small range of transformations. For instance, elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation. For such transformations, the layer should provide covariant rather than invariant, signatures. The property <b>of</b> <b>covariance</b> can be written as , where [...] is a layer, [...] is the signature of image on that layer, and [...] stands for [...] "distribution of values of the expression for all [...] ".|$|E
2500|$|One natural outcome {{from the}} {{covariance}} between environment and competition {{is that the}} species with very low densities will have more fluctuation in its recruitment rates than species with normal densities. [...] This occurs because in good environments, species with high densities will often experience large amount of crowding {{by members of the}} same species, thus limiting the benefits of good years/patches, and making good years/patches more similar to bad years/patches. [...] Low-density species are rarely able to cause crowding, thus allowing significantly increased fitness in good years/patches. [...] Since the fluctuation in recruitment rate is an indicator <b>of</b> <b>covariance</b> between environment and competition, and since species-specific environmental response and buffered population growth can normally be assumed in nature, finding much stronger fluctuation in recruitment rates in rare and low-density species provides a strong indication that the storage effect is operating within a community.|$|E
5000|$|... #Subtitle level 2: Parametric {{families}} <b>of</b> <b>covariance</b> functions ...|$|E
40|$|We {{propose a}} test of {{equality}} <b>of</b> two <b>covariance</b> matrices based on the maximum standardized di erence <b>of</b> scalar <b>covariances</b> <b>of</b> two sample <b>covariance</b> matrices. We derive the tail probability of the asymptotic null distribution of the test statistic by the tube method. However the usual formal tube formula has to be suitably modi ed,because {{in this case the}} index set, around which the tube s formed,has zero critical radius. ...|$|R
40|$|The differentiability of {{a random}} field {{has a direct}} {{relationship}} with the differentiability <b>of</b> its <b>covariance</b> function. We review the concept of differentiability <b>of</b> space–time <b>covariance</b> models and random fields, and its implications on predictions. We analyze the change of behavior <b>of</b> the <b>covariance</b> function at the origin and at different space–time lags away from the origin, by using the concept of smoothness which can be considered the geometrical view of the differentiability. We propose a way to measure the smoothness <b>of</b> any <b>covariance</b> function, {{and apply it to}} purely spatial and space–time covariance functions...|$|R
30|$|Another {{effective}} usage <b>of</b> the <b>covariance</b> matrix for sensing is the eigenvalue based detection (EBD) [20 – 25], {{which uses}} the eigenvalues <b>of</b> the <b>covariance</b> matrix as test statistics.|$|R
5000|$|Notice the {{definition}} <b>of</b> <b>covariance,</b> {{it can also}} be written as ...|$|E
50|$|By {{specifying the}} X's appropriately, one can obtain any pattern <b>of</b> <b>covariance</b> {{over time and}} alternatives.|$|E
5000|$|Scott Wise, former Chief Investment Officer at Rice {{and current}} President and Chief Investment Office <b>of</b> <b>Covariance</b> Capital Management.|$|E
5000|$|... (where {{the order}} of the matrix-vector product is not commutative), in terms <b>of</b> the <b>covariance</b> <b>of</b> the {{weighted}} mean: ...|$|R
40|$|AbstractWe {{report a}} matrix {{expression}} for the <b>covariance</b> matrix <b>of</b> MLEs of factor loadings in factor analysis. We then derive the analytical formula for <b>covariance</b> matrix <b>of</b> the <b>covariance</b> estimators <b>of</b> MLEs of factor loadings by obtaining the matrix of partial derivatives, which maps the differential <b>of</b> sample <b>covariance</b> matrix (in vector form) into the differential <b>of</b> the <b>covariance</b> estimators...|$|R
40|$|The {{problem of}} {{simultaneous}} {{estimation of the}} eigenvalues <b>of</b> a <b>covariance</b> matrix is considered under a sum of squared errors loss. A new class of estimators which is a generalization of Dey's (1988) class of estimators is given. As an immediate consequence, {{a new class of}} estimators of trace <b>of</b> the <b>covariance</b> matrix is obtained. Wishart identity estimation <b>of</b> eigenvalues <b>covariance</b> matrix...|$|R
50|$|The one-way ANOVA can be {{generalized}} to the factorial and multivariate layouts, {{as well as to}} the analysis <b>of</b> <b>covariance.</b>|$|E
5000|$|... {{from the}} {{definition}} <b>of</b> <b>covariance.</b> Then we apply {{the law of}} total expectation by conditioning on the random variable Z: ...|$|E
50|$|In multivariate statistics, random {{matrices}} {{were introduced}} by John Wishart for {{statistical analysis of}} large samples; see estimation <b>of</b> <b>covariance</b> matrices.|$|E
40|$|The {{determinant}} <b>of</b> the <b>covariance</b> matrix for high-dimensional data {{plays an}} important role in statistical inference and decision. It has many real applications including statistical tests and information theory. Due to the statistical and computational challenges with high dimensionality, little work has been proposed in the literature for estimating the determinant <b>of</b> high-dimensional <b>covariance</b> matrix. In this paper, we estimate the determinant <b>of</b> the <b>covariance</b> matrix using some recent proposals for estimating high-dimensional covariance matrix. Specifically, we consider a total <b>of</b> eight <b>covariance</b> matrix estimation methods for comparison. Through extensive simulation studies, we explore and summarize some interesting comparison results among all compared methods. We also provide practical guidelines based on the sample size, the dimension, and the correlation of the data set for estimating the determinant <b>of</b> high-dimensional <b>covariance</b> matrix. Finally, from a perspective of the loss function, the comparison study in this paper may also serve as a proxy to assess the performance <b>of</b> the <b>covariance</b> matrix estimation...|$|R
40|$|In {{this paper}} we propose a new {{regression}} interpretation of the Cholesky factor <b>of</b> the <b>covariance</b> matrix, as opposed to the well known regression interpretation of the Cholesky factor <b>of</b> the inverse <b>covariance,</b> which leads to a new class <b>of</b> regularized <b>covariance</b> estimators suitable for high-dimensional problems. Regularizing the Cholesky factor <b>of</b> the <b>covariance</b> via this regression interpretation always results in a positive definite estimator. In particular, one can obtain a positive definite banded estimator <b>of</b> the <b>covariance</b> matrix at the same computational cost as the popular banded estimator proposed by Bickel and Levina (2008 b), which is not guaranteed to be positive definite. We also establish theoretical connections between banding Cholesky factors <b>of</b> the <b>covariance</b> matrix and its inverse and constrained maximum likelihood estimation under the banding constraint, and compare the numerical performance of several methods in simulations and on a sonar data example. ...|$|R
40|$|In {{the present}} work some topics {{in the context}} of {{principal}} compo-nents are studied when the matrix <b>of</b> <b>covariances</b> is singular. In par-ticular, the maximum likelihood estimates of the eigenvalues and the eigenvectors <b>of</b> the sample <b>covariance</b> matrix are obtained. Also, the likelihood ratio statistics for proving the hypothesis of sphericity an...|$|R
50|$|The {{graphical}} lasso is an algorithm {{to estimate}} the precision matrix (inverse <b>of</b> <b>covariance</b> matrix) from the observations from multivariate Gaussian distribution.|$|E
50|$|If {{one takes}} k=p, the problem reduces to the {{ordinary}} PCA, and the optimal value becomes the largest eigenvalue <b>of</b> <b>covariance</b> matrix Σ.|$|E
5000|$|Many of the {{properties}} <b>of</b> <b>covariance</b> can be extracted elegantly by observing that it satisfies similar properties to those of an inner product: ...|$|E
5000|$|Compared to the Shapiro-Wilk test {{statistic}} , the Shapiro-Francia {{test statistic}} [...] {{is easier to}} compute, {{because it does not}} require that we form and invert the matrix <b>of</b> <b>covariances</b> between order statistics.|$|R
5000|$|... where [...] is {{the vector}} of solved weights, [...] are the Lagrange multipliers, [...] is the {{extended}} <b>covariance</b> matrix <b>of</b> residuals and [...] is the extended vector <b>of</b> <b>covariances</b> at new location.|$|R
40|$|Massively {{parallel}} {{recordings of}} spiking activity in cortical circuits show large variability <b>of</b> <b>covariances</b> across pairs <b>of</b> neurons [Ecker et al., Science (2010) ]. In {{contrast to the}} low average, the wide distribution <b>of</b> <b>covariances</b> {{and its relation to}} the structural variability of connections between neurons is still elusive. Here, we derive the formal relation between the statistics of connections and the statistics <b>of</b> integral pairwise <b>covariances</b> in networks <b>of</b> Ornstein-Uhlenbeck processes that capture the fluctuations in leaky integrate-and-fire and binary networks [Grytskyy et al., Front. Comput. Neurosci. (2013) ]. Spin-glass mean-field techniques [Sompolinsky and Zippelius, Phys. Rev. B (1982) ] applied to a generating function representing the joint probability distribution of network activity [Chow and Buice, J. Math. Neurosci. (2015) ] yield expressions that explain the divergence <b>of</b> mean <b>covariances</b> and their width when the coupling in the linear network approaches a critical value. Using these relations, distributions of correlations provide insights into the properties of the structure and the operational regime of the network...|$|R
