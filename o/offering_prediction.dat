2|40|Public
40|$|Though {{serving as}} an {{effective}} means for damage identification, the capability of an artificial neural network (ANN) for quantitative prediction is substantially dependent {{on the amount of}} training data. In virtue of a concept of “Digital Damage Fingerprints” (DDF), a hierarchical approach for the development of training databases was proposed for ANN-based damage identification. With the object of exploiting the capability of ANN to address the key questions: “Is there damage?” and “Where is the damage?”, the amount of training data (damage cases) was increased progressively. Mutuality was established between the quantity of training data and the accuracy of answers to the two questions of interest, and was experimentally validated by identifying the position of actual damage in carbon fibre-reinforced composite laminates. The results demonstrate that such a hierarchical approach is capable of <b>offering</b> <b>prediction</b> as to the presence and location of damage individually, with substantially reduced computational cost and effort {{in the development of the}} ANN training database. <br /...|$|E
40|$|AbstractSurface {{acoustic}} wave (SAW) resonators built on Langasite (LGS) are capable to withstand temperature {{in excess of}} 900 ∘ C and demonstration of wireless interrogation of packaged sensors up to 700 ∘ C has been achieved for several tens of hours. These promising results emphasize {{the need for an}} accurate characterization of the raw material in order to design SAW resonators {{with a high level of}} confidence in the prediction, particularly concerning the temperature coefficient of frequency (TCF). Several data set have been published for LGS, <b>offering</b> <b>prediction</b> capabilities but also a significant level of data dispersion. Therefore, the evaluation of the effective thermal properties of SAW under periodic gratings turns out less robust than expected. Based also on published data and on measurements achieved within the SAWHOT project, harmonic admittance calculations have been achieved for deriving the evolution of mixed matrix parameters allowing for accurate SAW device simulation at any temperature. Adjusting the temperature coefficients then yield improved sets of material coefficients for design purpose. Using these data, we have demonstrated the possibility to develop a differential temperature sensor operating at temperature up to 600 °C...|$|E
40|$|We {{propose a}} {{theoretical}} model for branching instabilities in 2 -dimensional fracture, <b>offering</b> <b>predictions</b> for when crack branching occurs, how multiple cracks develop, {{and what is}} the geometry of multiple branches. The model is based on equations of motion for crack tips which depend only on the time dependent stress intensity factors. The latter are obtained by invoking an approximate relation between static and dynamic stress intensity factors, together with an essentially exact calculation of the static ones. The results of this model are in good agreement with a sizeable quantity of experimental data. Comment: 9 pages, 11 figure...|$|R
5000|$|In 2012, 2013, and 2014 Sundeep [...] "OrangeShirtGuy (OSG)" [...] Rao {{performed}} live [...] "fortune telling" [...] segments, <b>offering</b> humorous <b>predictions</b> about viewers' future {{lives when}} they sent in donations to Child's Play Charity.|$|R
40|$|We {{describe}} a novel on-line file access predictor, Recent Popularity, capable of rapid adaptation to workload changes while simultaneously predicting more events with greater accuracy than prior efforts. We distinguish {{the goal of}} predicting the most events accurately from the goal of offering the most accurate predictions (when declining to offer a prediction is acceptable). For this purpose we present two distinct measures of accuracy, general and specific accuracy, corresponding to these goals. We describe how our new predictor and an earlier effort, Noah, can trade the number of events predicted for prediction accuracy by modifying simple parameters. When prediction accuracy is strictly {{more important than the}} number of predictions offered, trace-based evaluation demonstrates error rates as low as 2 %, while <b>offering</b> <b>predictions</b> for more than 60 % of all file access events...|$|R
40|$|This {{paper was}} {{presented}} at the 10 th International Conference on Manufacturing Research (ICMR 2012). Aston University, Birmingham, 11 - 13 September. The conference proceedings are available at: [URL] is an emerging trend of offering combined products and services to customers as integrated solutions. These are implemented by contracts such as availability contracts. Uncertainties may arise due to the novelty of the process of designing and managing such <b>offerings,</b> <b>prediction</b> of equipment failure and multiple stakeholders involvement in addition to the long-term nature of the contract. Understanding through-life uncertainties and their impact on cost is critical to ensure sustainability and profitability of the companies offering such solutions. The focus {{of this paper is to}} (i) evaluate existing uncertainty classifications and (ii) propose essential considerations for characterising the uncertainties in availability contracts. Appropriate classification of uncertainties should improve the quality of cost estimation by stimulating an understanding and awareness of uncertainties and their characteristics...|$|R
50|$|In September 2013, ESPN {{signed a}} deal with BETEGY, <b>offering</b> its users <b>prediction</b> and tips for soccer and {{football}} based on the startup's data. It is deeply integrated in the ESPN Pick Center.|$|R
40|$|File {{prefetching}} {{based on}} previous file access patterns {{has been shown to}} be an effective means of reducing file system latency by implicitly loading caches with files that are likely to be needed in the near future. Mistaken prefetching requests can be very costly in terms of added performance overheads including increased latency and bandwidth consumption. Such costs of mispredictions are easily overlooked when considering access prediction algorithms only in terms of their accuracy, but we describe a novel algorithm that uses machine learning to not only improve overall prediction accuracy, but as a means to avoid these costly mispredictions. Our algorithm is fully adaptive to changing workloads, and is fully automated in its ability to refrain from <b>offering</b> <b>predictions</b> when they are likely to be mistaken. Our trace-based simulations show that our algorithm produces prediction accuracies of up to 98 %. While this appears to be at the expense of a very slight reduction in cache hit ratios, application of this algorithm actually results in substantial reductions in unnecessary (and costly) I/O operations. 1...|$|R
30|$|In this section, we {{consider}} results of studies investigating the perceived utility of negotiated learner models or learning benefits demonstrated. Because learner model negotiation involves interactions such as challenging the model or (dis)agreeing with it, we include systems {{that do not}} have the interaction symmetry of fully negotiated learner models. In addition, while not negotiated models, some studies have suggested that allowing learners to directly change information in the learner model (where errors had been deliberately introduced by the system to determine whether students would correct their model) can lead to a more accurate learner model (Bull et al. 2008), and <b>offering</b> <b>predictions</b> of a learner’s expertise can motivate them to perform self-assessments for their learner model (Hochmeister et al. 2012). While the latter did not measure self-assessment accuracy, OLMs have indeed been found to increase the self-assessment of at least the weaker students at university level (Mitrovic and Martin 2007) and also of schoolchildren (Kerly and Bull 2008). This provides a positive starting point for negotiated learner models, since learners do appear able to recognise information they regard as an inaccurate representation of their understanding, as well as being able to propose updates. This is essential for negotiating a learner model.|$|R
40|$|High {{throughput}} sequencing {{techniques have}} highly impactedon modern biology, widening {{the gap between}} sequenced andannotated data. Automatic annotation tools are thereforeof the foremost importance to guide biologists' experiments. However, most of the state-of-the-art methods rely on annotation transfer, <b>offering</b> reliable <b>predictions</b> only in homology settings. In this work we present a novel appraoch to protein feature prediction, which exploits the Semanti Based Regularization to inject prior knowledge in the learning process. The experimental results conducted on the yeast genome show that {{the introduction of the}} constraints positively impacts on the overall prediction quality...|$|R
40|$|Evidence from macaque monkey tracing studies {{suggests}} connectivity-based subdivisions {{within the}} precuneus, <b>offering</b> <b>predictions</b> for similar subdivisions in the human. Here we present functional connectivity analyses {{of this region}} using resting-state functional MRI data collected from both humans and macaque monkeys. Three distinct patterns of functional connectivity were demonstrated within the precuneus of both species, with each subdivision suggesting a discrete functional role: (i) the anterior precuneus, functionally connected with the superior parietal cortex, paracentral lobule, and motor cortex, suggesting a sensorimotor region; (ii) the central precuneus, functionally connected to the dorsolateral prefrontal, dorsomedial prefrontal, and multimodal lateral inferior parietal cortex, suggesting a cognitive/associative region; and (iii) the posterior precuneus, displaying functional connectivity with adjacent visual cortical regions. These functional connectivity patterns were differentiated from the more ventral networks associated with the posterior cingulate, which connected with limbic structures such as the medial temporal cortex, dorsal and ventromedial prefrontal regions, posterior lateral inferior parietal regions, and the lateral temporal cortex. Our {{findings are consistent with}} predictions from anatomical tracer studies in the monkey, and provide support that resting-state functional connectivity (RSFC) may in part reflect underlying anatomy. These subdivisions within the precuneus suggest that neuroimaging studies will benefit from treating this region as anatomically (and thus functionally) heterogeneous. Furthermore, the consistency between functional connectivity networks in monkeys and humans provides support for RSFC as a viable tool for addressing cross-species comparisons of functional neuroanatomy...|$|R
40|$|BACKGROUND Breast {{cancer is}} a {{heterogeneous}} disease with variable morphological features, clinical outcomes and response to different therapeutic options. Gene expression profiling studies on breast carcinomas have revolutionised the classification of breast carcinomas into molecular subtypes enhancing the treatment protocols and <b>offering</b> better <b>prediction</b> of outcomes. MATERIALS AND METHODS The present study aims to discuss the special histological variants with special emphasis on their molecular phenotype using surrogate immunohistochemical markers. RESULTS Special variants of breast carcinomas diagnosed {{during the study period}} of two years from August 2013 and July 2015 were classified into molecular subtypes using immunohistochemical expression of ER, PR and HER- 2. CONCLUSIONS Special types of breast carcinomas are very rare and have different clinicopathologic behaviours. It is important to know their characteristics to make proper management decisions and to predict the prognosis...|$|R
40|$|How {{power is}} {{extended}} vertically through hierarchies and horizontally through industrial networks and markets {{is a classic}} issue in sociology that was once extensively studied by Weber. Yet it is little studied today. Definitions of power exclude power exercise beyond the single relation, as does research in exchange networks. In contrast, research in organizations recognizes the extension of power, but offers no theory to explain how it is produced or to identify the conditions which might further or impede it. Here the extension of power beyond the single relationship is called power-at-a-distance. New theory <b>offering</b> metric <b>predictions</b> is applied to power-ata-distance in exchange networks. That theory identifies the conditions that extend power beyond the dyad and the conditions that tend to block that extension. Five experiments on contrasting structures broadly support those predictions. Relations between power-at-a-distance and power centralization are theorized. Practical problems of extending power are addressed and further research is proposed. Unlike earlier, simpler societies where the exercise of power was always direc...|$|R
40|$|This book {{examines}} {{in detail}} the entire process involved in implementing geotechnical projects, from a well-defined initial stress and deformation state, to {{the completion of the}} installation process.   The individual chapters provide the fundamental knowledge needed to effectively improve soil-structure interaction models. Further, they present the results of theoretical fundamental research on suitable constitutive models, contact formulations, and efficient numerical implementations and algorithms. Applications of fundamental research on boundary value problems are also considered in order to improve the implementation of the theoretical models developed. Subsequent chapters highlight parametric studies of the respective geotechnical installation process, as well as elementary and large-scale model tests under well-defined conditions, in order to identify the most essential parameters for optimizing the process. The book provides suitable methods for simulating boundary value problems in connection with geote chnical installation processes, <b>offering</b> reliable <b>predictions</b> for the deformation behavior of structures in static contexts or dynamic interaction with the soil...|$|R
40|$|The {{future of}} satellite-based optical {{lightning}} detection, beyond {{the end of}} the current TRMM mission, is discussed. Opportunities for new low-earth orbit missions are reviewed. The potential for geostationary observations is significant; such observations provide order-of-magnitude gains in sampling and data efficiency over existing satellite convective observations. The feasibility and performance (resolution, sensitivity) of geostationary measurements using current technology is discussed. In addition to direct and continuous hemispheric observation of lighting, geostationary measurements have the potential (through data assimilation) to dramatically improve short and medium range forecasts, <b>offering</b> benefits to <b>prediction</b> of NOx productions and/or vertical transport...|$|R
40|$|We {{present a}} {{comprehensive}} black-hole event generator, BlackMax, which simulates the experimental signatures of microscopic and Planckian black-hole production and evolution at the LHC {{in the context}} of brane world models with low-scale quantum gravity. The generator is based on phenomenologically realistic models free of serious problems that plague low-scale gravity, thus <b>offering</b> more realistic <b>predictions</b> for hadron-hadron colliders. The generator includes all of the black-hole graybody factors known to date and incorporates the effects of black-hole rotation, splitting between the fermions, non-zero brane tension and black-hole recoil due to Hawking radiation (although not all simultaneously). The generator can be interfaced with Herwig and Pythia. Comment: 32 pages, 61 figures, webpage [URL]...|$|R
40|$|Most professionals, {{actively}} engaged in design, {{live in a world}} of trade-offs. The most typical compromise is that reducing the cost of design causes quality to suffer, but there are many others as well. This paper summarizes current use of one of the most popular approaches to improving the new offering development process: design reuse. In the present study 42 companies were surveyed, of which 23 were in man-ufacturing and 19 were in services—but all were {{actively engaged}} in technology and design reuse in new offerings. It was hypothesized that policies for design reuse and internal sourcing would promote the complexity and breadth of reuse (here the combination of modular and architectural substitution), which, in turn would dampen the percentage of substitution and reduce the negative impact on innova-tiveness of new <b>offerings.</b> These <b>predictions</b> were generally supported. Adoption of policies for encouragement or to mandate design reuse were significantly correlated with the extent of reuse (application of both architectural and modular design vs. just one or the other) among manufacturers but not services firms in the sample. Internal sourcing of ideas for design reuse was significantly correlated with extent o...|$|R
40|$|This chapter {{examines}} the colonial origins and present-day scope of those legal and social structures {{that seek to}} marginalise queer Singaporean life, including the recently reaffirmed criminalisation of male homosexual sex within the Singapore Penal Code. It contextualises the Singapore government’s resistance to leading change {{in this area and}} identifies the sources of current pressures for reform. The government’s hesitancy over the likelihood and timing of any potential liberalisation is revealed as all the more incongruous given the existence of a large, confident and visible gay and lesbian community within contemporary Singapore. The chapter explores how Singapore’s enthusiastic embrace of global economic integration and its attempts to reshape itself as an ideal destination and competitive hub for transnational flows of commerce, finance, tourism, expatriate labour, and knowledge-based creative industries has served to colour contemporary discourses of homosexual law reform and queer social visibility and acceptance. It also points to how state managers have regarded many of the outcomes of such globalizing processes as conflicting with approved narratives of postcolonial Singaporean nationalism and state sovereignty. The chapter concludes by <b>offering</b> some <b>predictions</b> about the likelihood and extent of future legal and political reform...|$|R
40|$|This Article {{is meant}} {{to serve as a}} guide to the Uniform Prudent Investor Act. I point to the main reforms and explain what {{motivated}} them. I also attempt a look into the future, <b>offering</b> some <b>predictions</b> about how trust-investment practice is likely to change as the principles embodied in the Restatement and the Uniform Act take effect. Among the changes foreseen are greater use of equities; of pooled investment vehicles, such as mutual funds; and of relatively unconventional investments, such as foreign securities and derivatives. I also speak of the tendency to break up trusteeship and allocate its functions among specialized service providers. I suggest that, even though the Uniform Prudent Investor Act is default law that the settlor of the trust can alter or oust, the Act is likely to limit the settlor 2 ̆ 7 s power to impose manifestly uneconomic investment restrictions. I also explain why the new trust-investment law is likely to have unsettling effects upon the seemingly quite distinct subject of principal-and-income law, that is, upon the rules that govern the allocations that trustees are commonly obliged to make between current and future beneficiaries of the trust...|$|R
40|$|In {{an attempt}} to {{eliminate}} the Landau Pole from QED by " borrowing " asymptotic freedom from QCD, I was successful in uniting the coupling constants of the two, respectively, in the " Electrostrong relation ". The Landau pole, estimated at around 10 ^ 286 eV, leads to a value of α^- 1 (Q) = 43. 06 (33) in the Electrostrong Relation, instead of infinity {{as was the case}} before. In addition, the Strong CP problem is also solved proving that {{there is no need for}} fine tuning in QCD. The first step however, was improving the measurement for the running of the strong coupling constant αs(Q) that can be tested experimentally for value of αS(ΛQCD) for the energy scale, QCD integration parameter ΛQ <b>offering</b> such a <b>prediction</b> for the first time...|$|R
40|$|The binary A(8) B phase (prototype Pt(8) Ti) {{has been}} {{experimentally}} observed in 11 systems. A high-throughput search {{over all the}} binary transition intermetallics, however, reveals 59 occurrences of the A(8) B phase: Au(8) Zn(dagger), Cd(8) Sc(dagger), Cu(8) Ni(dagger), Cu(8) Zn(dagger), Hg(8) La, Ir(8) Os(dagger), Ir(8) Re, Ir(8) Ru(dagger), Ir(8) Tc, Ir(8) W(dagger), Nb(8) Os(dagger), Nb(8) Rh(dagger), Nb(8) Ru(dagger), Nb(8) Ta(dagger), Ni(8) Fe, Ni(8) Mo(dagger) *, Ni(8) Nb(dagger) *, Ni(8) Ta*, Ni(8) V*, Ni(8) W, Pd(8) Al(dagger), Pd(8) Fe, Pd(8) Hf, Pd(8) Mn, Pd(8) Mo*, Pd(8) Nb, Pd(8) Sc, Pd(8) Ta, Pd(8) Ti, Pd(8) V*, Pd(8) W*, Pd(8) Zn, Pd(8) Zr, Pt(8) Al(dagger), Pt(8) Cr*, Pt(8) Hf, Pt(8) Mn, Pt(8) Mo, Pt(8) Nb, Pt(8) Rh(dagger), Pt(8) Sc, Pt(8) Ta, Pt(8) Ti*, Pt(8) V*, Pt(8) W, Pt(8) Zr*, Rh(8) Mo, Rh(8) W, Ta(8) Pd, Ta(8) Pt, Ta(8) Rh, V(8) Cr(dagger), V(8) Fe(dagger), V(8) Ir(dagger), V(8) Ni(dagger), V(8) Pd, V(8) Pt, V(8) Rh, and V(8) Ru(dagger) ((dagger) = metastable, * = experimentally observed). This is surprising for the wealth of new occurrences that are predicted, especially in well-characterized systems (e. g., Cu-Zn). By verifying all experimental results while <b>offering</b> additional <b>predictions,</b> our study serves as a striking demonstration {{of the power of}} the high-throughput approach. The practicality of the method is demonstrated in the Rh-W system. A cluster-expansion-based Monte Carlo model reveals a relatively high order-disorder transition temperature...|$|R
40|$|A {{bow wave}} {{breaking}} {{is one of}} the most prominent factors to be considered regarding the nonlinearity of added resistance for a ship. Considering the stability of the bow wave breaking, which is mostly influenced by the ship speed and the waterline entrance angle, can enhance understanding of the nonlinearity. Understanding of the nonlinearity can be improved by considering the stability of the bow wave breaking, which is mostly influenced by the ship speed and the waterline entrance angle. New transfer function containing the ship speed is proposed to make a better representative of the nonlinearity. This method is evaluated with the model test data of the Fast Displacement Ship (FDS) under the short waves condition (λ/L= 0. 4). This study has shown that new transfer function can be an efficient analysis method of the ship performance <b>prediction</b> <b>offering</b> intuitive consistency with proposed residual resistance concept. The findings lead to better understanding the nonlinearity considering bow wave breaking. Ship Hydromechanics and Structure...|$|R
40|$|By nature, people's {{tastes and}} {{preferences}} are unique and diverse {{so that a}} constant coefficient of each housing attribute produced by ordinary least squares (OLS) {{is not able to}} fully describe the behaviour of homebuyers of different classes. To complement the least squares, quantile regression is used to identify how real estate prices respond differently to a change in one unit of housing attribute at different quantiles. Theoretically, quantile regression can be utilized to estimate the implicit price for each housing attribute across the distribution of real estate prices, allowing specific percentiles of prices to be more influenced by certain housing attributes when compared to other percentiles. Empirical results demonstrate that most housing attributes, such as apartment size, age and floor level, command different prices at different quantiles. With the use of this approach, the efficiency of the mortgage markets is enhanced by <b>offering</b> more accurate <b>prediction</b> of real estate prices at the lower and upper price distribution. © 2012 Copyright Taylor and Francis Group, LLC. Link_to_subscribed_fulltex...|$|R
40|$|Kinna-Uganda (K-U) {{is a form}} {{of cinema}} that has emerged in Uganda {{following}} decades of the totalitarian political regime of Idi Amin, which in turn followed almost a century of colonial rule. The concept of national cinema is evaluated in this thesis as a tool for analysis of K-U. The national cinema concept is compared with four other models [...] regional, continental, Pan-African, and transnational [...] that {{can also be used to}} analyze and deconstruct films and the academic space within which they reside. This study examines Uganda 2 ̆ 7 s pre-colonial, colonial, and post-colonial times and their impact on the production of K-U and on Uganda 2 ̆ 7 s film industry. This research also presents the dissemination of Africa 2 ̆ 7 s cinema in the context of the African Diaspora and provides a guide to K-U and the Ugandan film industry from its inception until early 2010. In analyzing these findings of this study, it has been determined that the concept of national cinema is useful when dissecting K-U. The strength and utility of national cinema as a concept lies in four different factors: it aids in identifying K-U; it helps to explain the origins and characteristics of the local Ugandan film industry; it identifies the common constraints to expansion of the Ugandan film industry and, lastly, it helps in <b>offering</b> a <b>prediction</b> for the future of the Ugandan film industry. However, adhering to a strict definition of national cinema is shown to be less useful than previously in evaluating K-U, as it has propagated through Africa and increasingly involves co-productions between Ugandan and non-Ugandan entities...|$|R
40|$|International audienceThe {{characteristics}} of the flow, combustion and temperature in a 300 MWe tangentially fired pulverized-coal furnace are numerically studied using computational fluid dynamics. The mathematical model {{is based on a}} Eulerian description for the continuum phase and a Lagrangian description for coal particles. The combustion reaction scheme was modeled using eddy dissipation concept. The application of a proper turbulence model is mandatory to generate accurate predictions of flow and heat transfer during combustion. The current work presents a comparative study to identify the suitable turbulence model for tangentially fired furnace problem. Three turbulence models including the standard k-epsilon model, the RNG k-epsilon model and the Reynolds Stress model, RSM are examined. The predictions are compared with the published experimental data of Zheng et al. (Proc Combust Inst 29 : 811 - 818, 2002). The RNG k-epsilon model proves to be the most suitable turbulence model, <b>offering</b> a satisfactory <b>prediction</b> of the velocity, temperature and species fields. The detailed results presented in this paper may enhance the understanding of complex flow patterns and combustion processes in tangentially fired pulverized-coal furnaces...|$|R
40|$|This {{article is}} {{simultaneously}} an international comparative law piece about prisoner disenfranchisement in various countries, a transnational work of legal theory providing {{a framework for}} the use of foreign law in domestic constitutional courts, and a domestic analysis of the constitutional underpinnings of felon disenfranchisement. The article begins with a comprehensive comparative analysis of the recent prisoner disenfranchisement decisions in Canada, South Africa, and Europe. It notes that the over-arching theme of these decisions is to view the acceptability of prisoner disenfranchisement along a continuum, where it becomes more acceptable the more serious the offense committed. The article then examines the growing phenomenon of a “transnational judicial discourse” between domestic, foreign, and international courts, distinguishing the more controversial universalist and genealogical interpretations of the transnational judicial discourse from the less controversial dialogical interpretation of the discourse which has been separately endorsed by six justices of the US Supreme Court. Against this background, the article then examines the transnational judicial discourse on felon disenfranchisement, suggesting that the concept of a continuum of applicability taken from the international cases can inform the domestic debate on this issue. Specifically, it questions the current understanding that the framers of section 2 of the Fourteenth Amendment were sanctioning disenfranchisement for every kind of crime along the continuum of applicability. After an examination of the legislative history of the Fourteenth Amendment, it concludes that this was not the framers’ intent; rather, they only intended the disenfranchisement of those committing crimes of rebellion or disloyalty to the State, such as treason. With the textual barrier removed and the door open to a more nuanced constitutional examination of felon disenfranchisement for various crimes, the article concludes by <b>offering</b> some <b>predictions</b> on what a continuum of applicability of felon disenfranchisement would look like under strict scrutiny analysis...|$|R
50|$|Globalization {{theory was}} popularized in the 1990s {{as a model}} for {{understanding}} global communication. The concept of globalization inspired a number of theories from various schools of thought in communication studies that each emphasize different aspects of globalization. Many globalization theories highlight actors in the business sector as leaders in the processes of global integration. Transnationalizing business is often celebrated as progression toward a more interconnected world. Globalization theories are often associated with theories of modernity. Some scholars view globalization as the social, political, economic, and cultural integration of societies into a capitalist system; Others see globalization as a successor to modernity, while some see it as an iteration of imperialism. Some question the usefulness and legitimacy of globalization theory, arguing that it does not adequately conceptualize current international relations or function as a lens through which to examine everyday events. Many scholars criticize globalization theories as overzealous toward and unrealistic about the extent of global integration. Some scholars criticize social theorists for <b>offering</b> opinions and <b>predictions</b> based on theory with little practical evidence. In contrast, some scholars work to dispute the pessimistic views of globalization theory.|$|R
40|$|International audienceMachining {{time is a}} major {{performance}} criterion {{when it comes to}} high-speed machining. CAM software can help in estimating that time for a given strategy. But in practice, CAM-programmed feed rates are rarely achieved, especially where complex surface finishing is concerned. This means that machining time forecasts are often more than one step removed from reality. The reason behind this is that CAM routines do not take either the dynamic performances of the machines or their specific machining tolerances into account. The present article seeks to improve simulation of high-speed NC machine dynamic behaviour and machining time <b>prediction,</b> <b>offering</b> two models. The first contributes through enhanced simulation of three-axis paths in linear and circular interpolation, taking high-speed machine accelerations and jerks into account. The second model allows transition passages between blocks to be integrated in the simulation by adding in a polynomial transition path that caters for the true machining environment tolerances. Models are based on respect for path monitoring. Experimental validation shows the contribution of polynomial modelling of the transition passage due to the absence of a leap in acceleration. Simulation error on the machining time prediction remains below 1 %...|$|R
40|$|Abstract. Recently, novel {{learning}} algorithms such as Support Vector Regression (SVR) and Neural Networks (NN) {{have received}} increasing attention in forecasting and time series <b>prediction,</b> <b>offering</b> attractive theoretical properties and successful applications in several real world problem domains. Commonly, time series {{are composed of}} the combination of regular and irregular patterns such as trends and cycles, seasonal variations, level shifts, outliers or pulses and structural breaks, among others. Conventional parametric statistical methods are capable of forecasting a particular combination of patterns through ex ante selection of an adequate model form and specific data preprocessing. Thus, the capability of semi-parametric methods from computational intelligence to predict basic time series patterns without model selection and preprocessing is of particular relevance in evaluating their contribution to forecasting. This paper proposes an empirical comparison between NN and SVR models using radial basis function (RBF) and linear kernel functions, by analyzing their predictive power on five artificial time series: stationary, additive seasonality, linear trend, linear trend with additive seasonality, and linear trend with multiplicative seasonality. Results obtained show that RBF SVR models have problems in extrapolating trends, while NN and linear SVR models without data preprocessing provide robust accuracy across all patterns and clearly outperform the commonly used RBF SVR on trended time series. ...|$|R
40|$|Computational {{identification}} of putative microRNA (miRNA) targets {{is an important}} step towards elucidating miRNA functions. Several miRNA target-prediction algorithms have been developed followed by publicly available databases of these predictions. Here we present a new database <b>offering</b> miRNA target <b>predictions</b> of several binding types, identified by our recently developed modular algorithm RepTar. RepTar is based on {{identification of}} repetitive elements in 3 ′-UTRs and is independent of both evolutionary conservation and conventional binding patterns (i. e. Watson–Crick pairing of ‘seed’ regions). The modularity of RepTar enables the prediction of targets with conventional seed sites as well as rarer targets with non-conventional sites, such as sites with seed wobbles (G-U pairing in the seed region), 3 ′-compensatory sites and the newly discovered centered sites. Furthermore, RepTar’s independence of conservation enables the prediction of cellular targets of the less evolutionarily conserved viral miRNAs. Thus, the RepTar database contains genome-wide predictions of human and mouse miRNAs as well as predictions of cellular targets of human and mouse viral miRNAs. These predictions are presented in a user-friendly database, which allows browsing through the putative sites as well as conducting simple and advanced queries including data intersections of various types. The RepTar database is available at [URL]...|$|R
40|$|Learning {{strategies}} are traditionally {{divided into two}} categories: unsupervised learning and supervised learning. In contrast, for feature selection, there are four different categories of training scenarios: 1) unsupervised; 2) (regular) supervised; 3) self-supervised (SS); and 4) doubly supervised. Many genomic applications naturally arise in either (regular) supervised or self-supervised formulation. The distinction of these two supervised scenarios lies in whether the class labels are assigned to the samples versus the features. This paper explains how to convert an SS formulation into a symmetric doubly supervised (SDS) formulation by a pairwise approach. The SDS formulation offers more explicit information for effective feature selection than the SS formulation. To harness this information, the paper adopts a selection scheme called vector-index-adaptive SVM (VIA-SVM), {{which is based on}} the fact that the support vectors can be subdivided into different groups each <b>offering</b> quite distinct <b>prediction</b> performance. Simulation studies validate that VIA-SVM performs very well for time-course microarray data. This paper further proposes a fusion strategy to integrate the diversified information embedded in the SDS formulation. Simulation studies on protein sequence data for subcelluar localization confirm that the prediction can be significantly improved by combining VIA-SVM with relevance scores (e. g., SNR) and redundancy metrics (e. g., Euclidean distance). Department of Electronic and Information Engineerin...|$|R
40|$|The {{objective}} {{of this paper is}} to investigate the subscription price decision in a common stock rights <b>offering.</b> The competing <b>prediction</b> of the Heinkel and Schwartz model, in which a higher subscription price signals a higher quality firm, and the Myers and Majluf model in which the firm sets a low subscription price in order to secure the financing for a positive NPV project from the firm's existing stockholders are examined. For a sample of 69 industrial firms, the two-day offering period cumulative abnormal return is estimated using standard event study methodology. The relative subscription price is defined as the ratio of the subscription price to the mean of closing prices of the five trading days immediately preceding the two-day offering period. The sample was divided into two subsamples according to high and low relative subscription prices. The results indicate that industrial firms conform to the theoretical predictions of the Heinkel and Schwartz model. Higher quality firms use the subscription price in a rights offering as a signalling mechanism to distinguish themselves from lower quality firms. Relative subscription price is positively and significantly associated with the CAR. Apparently for industrials, securing the financing of a project is not a primary consideration. ...|$|R
40|$|Rationale Numerous {{cross-sectional}} studies {{investigated the}} link between marital status and BMI {{in the context of}} competing social science theories (marriage market, marriage selection, marriage protection and social obligation), frequently <b>offering</b> conflicting theoretical <b>predictions</b> and conflicting empirical findings. Objective This study analysed the effects of marriage, divorce, pregnancy, and parenthood on male BMI in a longitudinal setting, avoiding the estimation bias of cross-sectional studies and allowing for an analysis of BMI fluctuation over time and the dynamic effects of these events. Method Using the Panel Study of Income Dynamics 1999 – 2013 dataset (N = 8729), this study was the first to employ a dynamic panel-data estimation to examine the static and dynamic effects of marriage, divorce, and fatherhood on male BMI. Results The study showed that married men have higher BMI, but marital status changes largely drove this static effect, namely, an increase in BMI in the period following marriage, and a decrease in BMI preceding and following divorce. Conclusions Thus, this study found marked evidence in support of the marriage market and social obligation theories' predictions about male BMI, and supports neither marriage protection theory nor marriage selection theory. Wives’ pregnancies had no significant effect on BMI; instead, men tend to have higher BMI in the periods following childbirth. Finally, analyses showed marked contemporaneous correlations between husband and wife BMI over the course of marriage...|$|R
40|$|Recently, novel {{learning}} algorithms such as Support Vector Regression (SVR) and Neural Networks (NN) {{have received}} increasing attention in forecasting and time series <b>prediction,</b> <b>offering</b> attractive theoretical properties and successful applications in several real world problem domains. Commonly, time series {{are composed of}} the combination of regular and irregular patterns such as trends and cycles, seasonal variations, level shifts, outliers or pulses and structural breaks, among others. Conventional parametric statistical methods are capable of forecasting a particular combination of patterns through ex ante selection of an adequate model form and specific data preprocessing. Thus, the capability of semi-parametric methods from computational intelligence to predict basic time series patterns without model selection and preprocessing is of particular relevance in evaluating their contribution to forecasting. This paper proposes an empirical comparison between NN and SVR models using radial basis function (RBF) and linear kernel functions, by analyzing their predictive power on five artificial time series: stationary, additive seasonality, linear trend, linear trend with additive seasonality, and linear trend with multiplicative seasonality. Results obtained show that RBF SVR models have problems in extrapolating trends, while NN and linear SVR models without data preprocessing provide robust accuracy across all patterns and clearly outperform the commonly used RBF SVR on trended time series. IFIP International Conference on Artificial Intelligence in Theory and Practice - Neural Net...|$|R
60|$|On these {{problems}} light is thrown by a successor of Mr. Spencer's authority, Mr. Duff Macdonald, in the Blantyre Mission. This gentleman, the Rev. David Clement Scott, has published 'A Cyclopaedic Dictionary of the Mang'anja Language in British Central Africa.'[13] Looking at ancestral spirits first, we find Mzimu, 'spirits of the departed, {{supposed to come}} in dreams.' Though abiding in the spirit world, they also haunt thickets, they inspire Mlauli, prophets, and make them rave and utter <b>predictions.</b> <b>Offerings</b> are made to them. Here is a prayer: 'Watch over me, my ancestor, who died long ago; tell the great spirit {{at the head of}} my race from whom my mother came.' There are little hut-temples, and the chief directs the sacrifices of food, or of animals. There are religious pilgrimages, with sacrifice, to mountains. God, like men in this region, has various names, as Chiuta, 'God in space and the rainbow sign across;' Mpambe, 'God Almighty' (or rather 'pre-excellent'); Mlezi, 'God the Sustainer,' and Mulungu, 'God who is spirit.' Mulungu = God, 'not spirits or fetish.' 'You can't put the plural, as God is One,' say the natives. 'There are no idols called gods, and spirits are spirits of people who have died, not gods.' Idols are Zitunzi-zitunzi. 'Spirits are supposed to be with Mulungu.' God made the world and man. Our author says 'when the chief or people sacrifice it is to God,' but he also says that they sacrifice to ancestral spirits. There is some confusion of ideas here: Mr. Macdonald says nothing of sacrifice to Mtanga.|$|R
40|$|Large-Eddy Simulation (LES) {{becomes a}} {{more and more}} demanded tool to improve the design of aero-engines. The main reason for this request stems from the {{constraints}} imposed on the next generation low-emission engines at the industrial development level and the ability for LES to provide information on the instantaneous turbulent flow field which greatly contributes to improving the prediction of mixing and combustion thereby <b>offering</b> an improved <b>prediction</b> of the exhaust emission. The work presented in this thesis discusses two recurring issues of LES. For one, numerical schemes for LES require certain properties, i. e. low-diffusion schemes of high order of accuracy {{so as not to}} interfere with the turbulence models. To meet this purpose in the context of fully unstructured solvers, a new family of high-order time-integration schemes is proposed. With this class of schemes, the diffusion implied by the numerical scheme become adjustable and built-in. Second, since fully unsteady by nature, LES is very consuming in terms of CPU time. Even with today's supercomputers complex problems require long simulation times. Due to the low flow velocities often occurring in industrial applications, the use of a low-Mach number solver seems suitable and can lead to large reductions in CPU time if comparable to fully compressible solvers. The impact of the incompressibility assumption and the different nature of the numerical algorithms are rarely discussed. To partly answer the question, detailed comparisons are proposed for an experimental swirled configuration representative of a real burner that is simulated by LES using a fully explicit compressible solver and an incompressible solution developed at CORIA...|$|R
