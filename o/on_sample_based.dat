4|10000|Public
40|$|Abstract—This paper {{describes}} the modeling of jitter in clock-and-data recovery (CDR) systems using an event-driven model that accurately includes {{the effects of}} power-supply noise, the finite bandwidth (aperture window) in the phase detector’s front-end sampler, and intersymbol interference in the system’s channel. These continuous-time jitter sources are captured in the model through their discrete-time influence <b>on</b> <b>sample</b> <b>based</b> phase detectors. Modeling parameters for these disturbances are di-rectly extracted from the circuit implementation. The event-driven model, implemented in Simulink, has a simulation accuracy within 12 % of an Hspice simulation—but with a simulation speed that is 1800 times higher. Index Terms—Clock jitter, clock-and-data recovery (CDR) mod-eling, discrete-time simulation, event driven. I...|$|E
40|$|The article {{presents}} {{an analysis of}} cross-linking reaction using measurements of time dependencies of torque at constant temperature from the range 100 up to 200 degrees C. The measured results obtained <b>on</b> <b>sample</b> <b>based</b> on styrene - butadiene (SBS) prepared in the laboratory show the behaviour which can be well described by equations of chemical reactions of first-order kinetics. It is possible mathematically describe significant constants of the kinetics of networking reaction (induction period, reaction rate coefficient) by the solution of differential equations and by mathematical approximation. Constants are exponentially dependent on the temperature of vulcanization, while dependencies are Arrhenius like. Math description allows describe {{the progress of the}} vulcanization reaction also in the temperature range outside of the monitoring interval, i. e. in the area of extremely long times required for the realization of the cross-linking reaction...|$|E
40|$|The {{development}} of mechanistic models of biological systems {{is a central}} part of Systems Biology. One major task in developing these models is the inference of the correct model parameters. Due to the size of most realistic models and their possibly complex dynamical behaviour one must usually rely <b>on</b> <b>sample</b> <b>based</b> methods. In this paper we present a novel algorithm that reliably estimates model parameters for deterministic as well as stochastic models from trajectory data. Our algorithm samples iteratively independent particles from the level sets of the likelihood and recovers the posterior from these level sets. The presented approach is easily parallelizable and, by utilizing density estimation through Dirichlet Process Gaussian Mixture Models, can deal with high dimensional parameter spaces. We illustrate that our algorithm is applicable to large, realistic deterministic and stochastic models and succeeds in inferring the correct posterior from a given number of observed trajectories. This algorithm presents a novel, computationally feasible approach to identify parameters of large biochemical reaction models based on sample path data...|$|E
5000|$|Data {{omission}} (<b>based</b> <b>on</b> the <b>sample</b> quality, <b>based</b> <b>on</b> the <b>sample</b> value, <b>based</b> <b>on</b> the count).|$|R
30|$|When {{the sample}} {{preparation}} was completed, it was gripped by membrane after placing the filter paper and porous stone both sides, top and bottom. The {{height of the}} membrane was maintained two {{times as much as}} the sample so that it can cover the top sample cap and <b>sample</b> <b>base</b> grooves. Afterwards, the gripped sample with filter papers and porous stones were placed <b>on</b> <b>sample</b> <b>base.</b> The <b>sample</b> cap with screwed piston rod was kept <b>on</b> the <b>sample.</b> Next, unfolding of extra portion of membrane at top and bottom were done and o-rings were placed in the grooves which were already covered by membrane. Once the cell chamber was assembled, it was filled with water. To confirm the close contact between the load sensor and piston rod a negligible amount of force was applied.|$|R
40|$|This paper {{examines}} Probabilistic Sensitivity Analysis (PSA) {{methods and}} tools {{in an effort}} to understand their utility in vehicle loads and dynamic analysis. Specifically, this study addresses how these methods may be used to establish limits on payload mass and cg location and requirements on adaptor stiffnesses while maintaining vehicle loads and frequencies within established bounds. To this end, PSA methods and tools are applied to a realistic, but manageable, integrated launch vehicle analysis where payload and payload adaptor parameters are modeled as random variables. This analysis is used to study both Regional Response PSA (RRPSA) and Global Response PSA (GRPSA) methods, with a primary focus <b>on</b> <b>sampling</b> <b>based</b> techniques. For contrast, some MPP based approaches are also examined...|$|R
40|$|LeBel and Paunonen (2011) {{highlight}} {{that despite}} their importance and popularity in both theoretical and applied research, many implicit measures {{continue to be}} plagued by a persistent and troublesome issue – low reliability. In their paper, they offer a conceptual analysis {{of the relationship between}} reliability, power and replicability, and then provide a series of recommendations for researchers interested in using implicit measures in an experimental setting. At the core of their account is the idea that reliability can be equated with statistical power, such that lower levels of reliability are associated with decreasing probabilities of detecting a statistically significant effect, given one exists in the population (p. 573). They also take the additional step of equating reliability and replicability. In our commentary, we draw {{attention to the fact that}} there is no direct, fixed or one-to-one relation between reliability and power or replicability. In our commentary, we draw attention to the fact that there is no direct, fixed or one-to-one relation between reliability and power or replicability. More specifically, we argue that when adopting an experimental (rather than a correlational) approach, researchers strive to minimize inter-individual variation, which has a direct impact <b>on</b> <b>sample</b> <b>based</b> reliability estimates. We evaluate the strengths and weaknesses of the LeBel and Paunonen’s recommendations and refine them where appropriate...|$|E
50|$|This method can process {{larger number}} of {{elements}} than traditional BEMD method. Also, it can shorten the time consuming for the process. Depended <b>on</b> using nonparametric <b>sampling</b> <b>based</b> texture synthesis, the BPBEMD could obtain better result after decomposing and extracting.|$|R
40|$|In this paper, we {{consider}} the problem of placing networked sensors {{in a way that}} guarantees coverage and connectivity. We focus <b>on</b> <b>sampling</b> <b>based</b> deployment and present algorithms that guarantee coverage and connectivity with a small number of sensors. We consider two different scenarios based on the flexibility of deployment. If deployment has to be accomplished in one step, like airborne deployment, then the main question becomes how many sensors are needed. If deployment can be implemented in multiple steps, then awareness of coverage and connectivity can be updated. For this case, we present incremental deployment algorithms which consider the current placement to adjust the sampling domain. The algorithms are simple, easy to implement, and require a small number of sensors. We believe the concepts and algorithms presented in this paper will provide a unifying framework for existing and future deployment algorithms which consider many practical issues not considered in the present work...|$|R
50|$|Mushet {{carried out}} many {{experiments}} with the metal, discovering {{that a small}} amount added during the manufacture of steel rendered it more workable when heated. It was not until 1856, however, that he realised the true potential of this property when his friend Thomas Brown brought him a piece of steel, made using the Bessemer Process, asking if he could improve its poor quality. Mushet carried out experiments <b>on</b> the <b>sample,</b> <b>based</b> <b>on</b> those he had previously carried out with spiegeleisen.|$|R
40|$|Data {{collected}} on a rectangular lattice occur frequently {{in many areas}} such as field trials, geostatistics, remotely sensed data, and image analysis. Models for the spatial process often make simplifying assumptions, including axial symmetry and separability. We consider methods for testing these assumptions and compare tests <b>based</b> <b>on</b> <b>sample</b> covariances, tests <b>based</b> <b>on</b> the <b>sample</b> spectrum, and model-based tests...|$|R
40|$|We focus <b>on</b> row <b>sampling</b> <b>based</b> approximations for matrix algorithms, in {{particular}} matrix multipication, sparse matrix reconstruction, and ℓ_ 2 regression. For ∈^m× d (m points in d≪ m dimensions), and appropriate row-sampling probabilities, which typically {{depend on the}} norms of the rows of the m× d left singular matrix of (the leverage scores), we give row-sampling algorithms with linear (up to polylog factors) dependence on the stable rank of. This result is achieved {{through the application of}} non-commutative Bernstein bounds. Keywords: row-sampling; matrix multiplication; matrix reconstruction; estimating spectral norm; linear regression; randomizedComment: Working pape...|$|R
40|$|Cataloged from PDF {{version of}} article. A method for verifying {{knowledge}} bases {{that is based}} on the unification of rules is discussed. One characteristic that distinguishes this approach from other verification tools is that it infers some of the rules that are not explicitly given in the rule base and considers their effect on the verification process. The method can determine conflicting, redundant, subsumed, circular, and dead-end rules, redundant if conditions in rules, and cycles and contradictions within rules. The method has been implemented in a computer program called UVT (for unification-based verification tool) and tested <b>on</b> <b>sample</b> knowledge <b>bases...</b>|$|R
40|$|Two-photon {{quantum well}} {{infrared}} photodetectors (QWIPs) are nonlinear detectors for the mid-infrared and terahertz regimes optimized for resonant two-photon absorption. Here we present first results <b>on</b> two-photon QWIP <b>samples</b> <b>based</b> <b>on</b> the GaAs/AIGaAs material system with intersubband energies between 25 and 12 meV (6 - 3 THz) confirmed by photocurrent spectra. The dark current showed large discontinuities, presumably caused by impact ionization. We performed interferometric autocorrelation experiments at the free-electron laser FELBE and observed nonlinear interferograms for all samples...|$|R
40|$|A {{theoretical}} {{description of}} scanning tunneling potentoimetry (STP) measurement {{is presented to}} address the increasing need for a basis to interpret experiments <b>on</b> macrscopic <b>samples.</b> <b>Based</b> <b>on</b> a heuristic understanding of STP provided to facilitate theoretical understanding, the total tunneling current related to the density matrix of the sample is derived within the general framework of quantum transport. The measured potentiometric voltage is determined implicitly as the voltage necessary to null the tunneling current. Explicit expressions of measured voltages are presented under certain assumptions, and limiting cases are discussed to connect to previous results. The need to go forward and formulate the theory {{in terms of a}} local density matrix is also discussed. Comment: 14 pages, 7 figure...|$|R
40|$|We {{study the}} problem of {{approximately}} answering aggregation queries using sampling. We observe that uniform sampling performs poorly when {{the distribution of the}} aggregated attribute is skewed. To address this issue, we introduce a technique called outlier-indexing. Uniform sampling is also inefective for queries with low selectivity. We rely <b>on</b> weighted <b>sampling</b> <b>based</b> <b>on</b> workload information to overcome this shortcoming. We demonstrate that a combination of outlier-indexing with weighted sampling can be used to answer aggregation queries with significantly reduced approximation error compared to either uniform sampling or weighted sampling alone. We discuss the implementation of these techniques on Microsoft's SQL Server; and present experimental results that demonstrate the merits of our techniques...|$|R
50|$|Some Assembly Required {{is a sound}} collage {{radio program}} in the United States, {{produced}} in Minneapolis, Minnesota. It is the first radio show known to focus exclusively <b>on</b> works of <b>sample</b> <b>based</b> music, and appropriation in audio art. The nationally syndicated program features work by artists {{from a variety of}} genres, including plunderphonics, hip hop turntablism, musique concrète, noise, bastard pop, sound art and more. The program celebrated its tenth anniversary on January 27, 2009. The final episode originally aired in 2011.|$|R
40|$|Mechanical {{properties}} at elevated temperature, {{in modern}} alloys based on intermetallic phase Ni 3 Al are connected with phase composition, especially with proportion of ordered phase γ′ (L 12) and disordered phase γ (A 1). In this paper, analysis of one key systems for mentioned alloys - Ni-Al-Cr, is presented. A series of alloys with chemical composition originated from Ni-rich part of Ni-Al-Cr system was prepared. DTA thermal {{analysis was performed}} <b>on</b> all <b>samples.</b> <b>Based</b> <b>on</b> shape of obtained curves, characteristic for continuous order-disorder transition, places of course of phase boundaries γ′+γ / γ were determined. Moreover, temperature of melting and freezing of alloys were obtained. Results of DTA analysis concerning phase boundary γ′+γ / γ indicated agreement with results obtained by authors using calorimetric solution method...|$|R
40|$|An {{original}} high-performance liquid chromatographic method with {{fluorescence detection}} is presented for the simultaneous {{determination of the}} three antiepileptic drugs gabapentin, vigabatrin and topiramate in human plasma. After pre-column derivatisation with dansyl chloride, the analytes were separated on a Hydro-RP column with a mobile phase composed of phosphate buffer (55 %) and acetonitrile (45 %) and detected at lambda(em) = 500 nm, exciting at 300 nm. An original pre-treatment procedure <b>on</b> biological <b>samples,</b> <b>based</b> <b>on</b> solid-phase extraction with MCX cartridges for gabapentin and vigabatrin, and with Plexa cartridges for topiramate, gave high extraction yields (> 91 %), satisfactory precision (RSD 91 %). Therefore, the method seems to be suitable for the therapeutic drug monitoring (TDM) of patients treated with gabapentin, vigabatrin and topiramat...|$|R
40|$|Abstract: Text-To-Visual speech (TTVS) {{synthesis}} {{by computer}} {{can increase the}} speech intelligibility and make the human-computer interaction interfaces more friendly. This paper describes a Chinese text-to-visual speech synthesis system <b>based</b> <b>on</b> data-driven (<b>sample</b> <b>based)</b> approach, which is realized by short video segments concatenation. An effective method to construct two visual confusion trees for Chinese initials and finals is developed. A co-articulation model based on visual distance and hardness factor is proposed, {{which can be used}} in the recording corpus sentence selection in analysis phase and the unit selection in synthesis phase. The obvious difference between boundary images of the concatenation video segments is smoothed by image morphing technique. By combining with the acoustic Text-To-Speech (TTS) synthesis, a Chinese text-to-visual speech synthesis system is realized...|$|R
5000|$|Hodgkin {{collaborated with}} Joseph Jackson Lister, who in 1830 enunciated design {{principles}} for the achromatic microscope. By that time Hodgkin and Lister had already published research <b>on</b> tissue <b>samples,</b> <b>based</b> <b>on</b> observations made with Lister's innovative microscope, in particular on the [...] "globule hypothesis" [...] {{of the time}} which was held in particular by Henri Milne Edwards. They denied the existence of globules in tissue; Ernst Heinrich Weber in 1830 contradicted them, and the debate continued for a decade. For a while microscopy suffered in its reputation, but by 1840 histology was a recognised discipline, and in time the view of Hodgkin and Lister that [...] "globules" [...] were optical artefacts became accepted. The 1827 paper they published has been called [...] "the foundation of modern histology".|$|R
40|$|Two {{approaches}} to {{the interpretation of the}} data of magnetic force microscopy are considered. The first approach involves the reconstruction of the magnetization distribution in the researched <b>samples</b> <b>on</b> the <b>base</b> of an assumption about the magnetic state and the subsequent numerical magnetic force microscopy experiment. The second is related to an experimental data processing...|$|R
40|$|International audienceThe aim of {{this review}} is to discuss the {{situation}} of alveolar echinococcosis in France, {{in the light of}} the current knowledge on its transmission patterns in the world, especially Europe. An important risk of higher contamination of the rural environment may be suspected from newly reported cases of infected foxes or voles in several countries where the disease was not found before. Furthermore, the increase of prevalence rate in foxes in Bade-Würtemberg (Germany) and Franche-Comté (France), traditionally endemic, is also in support of this new trend. Urban foxes and the spreading of infected foxes to cities may also be the cause of the extension to urban and suburban areas of a disease which was believed to be limited to rural areas. The setting up of an epidemiosurveillance system for fox (dog, cat) populations is currently in progress. It will be grounded on coprotests (ELISA, PCR) for the detection of E. multilocularis material in definitive hosts faeces and <b>on</b> <b>sampling</b> <b>based</b> <b>on</b> the multiscale analysis of the characteristics of landscapes favourable to transmission. A register of human cases has been established at the European level (WHO collaborative centre of the University of Franche-Comté and University of Ulm and Hohenheim). Unfortunately, the evolution of the parasite distribution and of the risks of exposure in France is unrecognised. Progress in surveillance and prevention could be expected if health authorities would attribute higher priority to this disease...|$|R
40|$|Abstract- Fuzzy {{systems are}} {{extensively}} {{used in both}} applied and experimental medicine and {{are one of the}} most prevalent subjects of today’s Medical Informatics. Despite some limitations of their use due to information, economical, educational and other reasons these systems are widely acknowledged in medical institutions operating in all levels of healthcare. The accuracy of some up-to-date fuzzy systems today matches the diagnostic abilities of physicians that are why it holds an important role in risk-assessment and diagnostics in pharmocology. In this paper, I have done work on the fever symptoms that help in the diagnosis of dengue fever symptoms with the help of some doctors on the basis of fuzzy rule system I tried to give a better identification of dengue fever <b>on</b> the <b>sample</b> <b>based,</b> <b>on</b> the lab features and clinical symptoms...|$|R
40|$|The {{purpose of}} this {{document}} (RE- 239) is to analyze {{the way in which}} the Bank has been supporting sector reform policies in various countries in recent years. It is <b>based</b> <b>on</b> a <b>sample</b> <b>based</b> <b>on</b> the principal evaluation results from 14 sector loans that were approved between 1990 and 1995, in eight member countries of the Bank. Drawing on elements both of theory and of empirical evidence, the document establishes a set of features that are necessary for achieving sustained growth of the kind that will contribute to reducing poverty. The sector operation evaluations analyzed here focus on the impact of sector lending in supporting policies and institutions in those areas where major efforts are being made to consolidate stabilization and improve growth prospects, and thereby to reduce poverty...|$|R
40|$|Manifestations of {{anomalous}} conductivity in polar dielectric films {{is demonstrated}} {{for the first}} time <b>on</b> <b>samples</b> of copolymer of vinylene and vinyl chloride, as was previously observed <b>on</b> modified PVC <b>samples.</b> <b>On</b> the <b>base</b> of these experimental results important new insights into the physical sense of traditionally used in these conditions specific resistivity indicators, both volume and surface, should be replaced by transverse and longitudinal resistance (according to polymer film surface) respectively, because the specific resistivity of polymer composite does not permits to calculate as usual the resistance of sample of arbitrary form [...] Comment: 5 pages, in russion, 1 tabl...|$|R
40|$|Voluminous spatio-temporal data make {{multimedia}} analysis tasks extremely {{inefficient and}} lack of adaptability. We present a novel experience <b>based</b> <b>sampling</b> technique which {{has the ability to}} focus on the analysis’s task by making use of the contextual information and past experiences. <b>Based</b> <b>on</b> this, a <b>sampling</b> <b>based</b> dynamic attention model is built by sensing the experiential environment. Sensor samples are used to gather information about the current environment and attention samples are used to represent the current state of attention. In our framework, the taskattended samples are inferred from experiences and maintained by a <b>sampling</b> <b>based</b> dynamical system. The multimedia analysis task can then focus <b>on</b> the attention <b>samples</b> only. Moreover, past experiences and the current environment can be used to adaptively correct and tune the attention. As a prototypical multimedia analysis task, we tackle the face detection problem in videos. Face detection is only performed <b>on</b> the attended <b>samples</b> to achieve robust real time processing. Experimental results have been presented to demonstrate the efficacy of our technique. This experience <b>based</b> <b>sampling</b> <b>based</b> analysis method appears to be a promising technique for general multimedia analysis problems. The generality stems from the power of the sampling method which makes no assumptions about the form of distribution of attention which is usually multimodal in nature...|$|R
40|$|This paper {{presents}} the experimental {{results of a}} study, which {{has been carried out}} to analyze the insulation characteristics of coconut oil in order to check the suitability as the liquid insulation in power transformers. In this study dielectric response measurements were carried out in Frequency domain at temperatures of 25 °C, 50 °C, 80 2 ̆ 7 C. In addition, results of mineral oil samples were taken for comparison. Further, Breakdown Voltage was also measured <b>on</b> <b>samples.</b> <b>Based</b> <b>on</b> the results, relative permittivity and conductivity values were obtained at room temperature (25 2 ̆ 7 C) and elevated temperatures. Finally, the activation energy values were calculated <b>on</b> each <b>sample.</b> The dielectric breakdown voltages of the five coconut oil samples were in the range of 9. 3 kV to 18. 4 kV, which should be above 50 kV according to the IEC 60296 standard. Conductivity values of coconut oil at room temperature were in a range from 10 - 11 to 10 - 12 S, which is considerably higher than that of new mineral oil. These results imply that further purification is required to bring coconut oil to the required standard to use as an insulating medium in transformers. Effort was done to identify the effect of temperature on dielectric response measurements of oil insulation system and correlation of it with the conductivity. The activation energy of five coconut oil samples was around 0. 1, which is less than that of mineral oil, i. e. 0. 4. This implies that the increase of conductivity with temperature is considerably low in coconut oil, when comparing with mineral oil...|$|R
40|$|Abstract. In this research, the {{time-dependent}} changes induced {{in charge}} characteristics of phosphoric acid and lime treated quartz-rich kaolinitic soil were investigated. Also, {{in order to}} study {{the relationship between the}} exchange capacity and the pore water chemistry, pH measurements was performed <b>on</b> cured <b>samples.</b> <b>Based</b> <b>on</b> the collected data, {{it was found that the}} pH of stabilized soils showed a tendency for reaching soil’s natural pH with increasing curing time. In addition, the increase in number of broken bonds around the edges of soil particles and also the formation of cementitious compounds that acquired negative charges contributed to achieving higher CECp values at longer curing periods. From engineering point of view, the lime treated samples revealed the highest degree of improvement with an approximately 16 -fold strength increase in comparison to the natural soil over an 8 months curing period...|$|R
40|$|The {{purpose of}} this study was to {{determine}} the factors that affect consumers in the considered of a motorcycle brand Bajaj Pulsar P 220 DTS-i in Malang. This type of research conducted <b>on</b> 54 <b>samples</b> <b>based</b> <b>on</b> sales data Bajaj dealers in Malang. Collecting data using questionaires distributed to 54 respondents who had purchased a motorcycle brand Bajaj Pulsar P 220 DTS-i in Malang The analysis tools used in this research is factors analysis. The results of the analysis of the product attributes that affect consumers in the considered of a motorcycle brand Bajaj Pulsar P 220 DTS-i in Malang, it was known that there were three factor that affect consumers in the considered of a motorcycle brand Bajaj Pulsar P 220 DTS-i in Malang, they were: 1) superiority of product, 2) quality of product, 3) complementary services...|$|R
30|$|Finally, {{this work}} {{turned into an}} {{exploratory}} study on 17 texts that {{became part of the}} research on TEALE published between 1960 and 2015. It is true that the sampling system did not provide us with subsequent texts that possibly are currently available, but the explanatory nature of the study should focus on whether to use resources in a similar research <b>on</b> a larger <b>sample</b> <b>based</b> <b>on</b> the results obtained. Moreover, if we also understand {{that the vast majority of}} this scientific production based on experiences that has subsequently impacted on other published studies, concerns to higher education.|$|R
40|$|Background Compliance {{in cancer}} {{screening}} among socially disadvantaged persons {{is known to}} be lower than among more socially advantaged persons. However, most of the studies regarding compliance proceed via a questionnaire and are thus limited by self-reported measures of participation and by participation bias. This study aimed at investigating the influence of socioeconomic characteristics on compliance to an organised colorectal cancer screening programme <b>on</b> an unbiased <b>sample</b> <b>based</b> <b>on</b> data from the entire target population within a French geographical department, Calvados (n 180 045). Methods Individual data of participation and aggregate socioeconomic data, from the structure responsible fo...|$|R
40|$|Abstract. Several {{authors have}} {{theoretically}} determined distribution-free bounds <b>on</b> <b>sample</b> complexity. Formulas <b>based</b> <b>on</b> several learning paradigms have been presented. However, {{little is known}} on how these formulas perform and compare {{with each other in}} practice. To our knowledge, controlled experimental results using these formulas, and comparing of their behavior, have not so far been presented. The present paper represents a contribution to filling up this gap, providing experimentally controlled results on how simple perceptrons trained by gradient descent or by the support vector approach comply with these bounds in practice. ...|$|R
40|$|Here, in {{this paper}} it has been {{considered}} a sub family of exponential family. Maximum likelihood estimations (MLE) for the parameter of this family, probability density function, and cumulative density function <b>based</b> <b>on</b> a <b>sample</b> and <b>based</b> <b>on</b> lower record values have been obtained. It has been considered Mean Square Error (MSE) as a criterion for determining which is better in different situations. Additionally, it has been proved some theories about the relations between MLE based on lower record values and <b>based</b> <b>on</b> a random <b>sample.</b> Also, some interesting asymptotically properties for these estimations have been shown during some theories...|$|R
30|$|It is {{necessary}} to determine the compressibility parameters of soils such as the compression index (Cc) and the recompression index (Cr) for safe and economic design of civil engineering structures. In order to calculate the consolidation settlement of normally consolidated and over-consolidated saturated fine-grained soils, the compressibility parameters are determined by means of laboratory oedometer test <b>on</b> undisturbed <b>samples</b> <b>based</b> <b>on</b> Terzaghi’s consolidation theory. These parameters can be influenced from the quality of samples used in the tests. Although the compressibility parameters must be obtained from careful oedometer test measurements based on good quality undisturbed samples, conventional oedometer test comprises major disadvantages such as costliness, unwieldiness and time-consuming. In addition, the other important disadvantage of the estimation of the compressibility parameters is that the graphical method directly depends on the personal experience. Because of these factors, many researchers have been tried to develop practical and fast solutions.|$|R
40|$|Abstract:- <b>Based</b> <b>on</b> <b>sampling</b> {{likelihood}} {{and feature}} intensity, in this paper, a feature-preserving denoising algorithm for point-sampled surfaces is proposed. In terms of moving least squares surface, the sampling likelihood for each point on point-sampled surfaces is computed, which measures {{the probability that}} a 3 D point is located <b>on</b> the <b>sampled</b> surface. <b>Based</b> <b>on</b> the normal tensor voting, the feature intensity of sample point is evaluated. By applying the modified bilateral filtering to each normal, and in combination with sampling likelihood and feature intensity, the filtered point-sampled surfaces are obtained. Experimental results demonstrate that the algorithm is robust, and can denoise the noise efficiently while preserving the surface features. Key-Words:-Moving least squares surface; sampling likelihood; normal voting tensor; feature intensity; bilateral filtering; point-sampled surfaces denoising...|$|R
