0|3152|Public
5000|$|Rainfall average ~250 mm (till year 2000), <b>{{temperature}}</b> <b>Min,</b> <b>Max</b> (-4 Winter)-(38 Summer) C temperature average (4.4 Winter)-(30 Summer) C ...|$|R
5000|$|... #Caption: Average <b>temperatures</b> (<b>min.,</b> avrg., <b>max.)</b> and precipitations.|$|R
30|$|While modeling, {{the monthly}} <b>temperature</b> (<b>Min.,</b> <b>Max.,</b> and Avg.), {{relative}} humidity (<b>Min.</b> and <b>Max.),</b> average wind speed, sunshine hours and rainfall data from 1980 to 2013 (i.e. 408 datasets) {{were divided into}} calibration period 1980 – 2004 (300 datasets) and validation period 2005 – 2013 (108 datasets).|$|R
30|$|A few {{researchers}} proposed hybrid neural {{models for}} weather forecasting. Kaur et al. (2011) and Maqsood et al. (2004) presented a hybrid neural model for twenty-four hours ahead weather forecasting. The hybrid model is fusion of several neural architectures including MLP, ERNN and the Hopfield Model. The authors compared forecasting results of individual networks with hybrid model and confirmed that hybrid model efficiency {{was better than}} individuals. Bustami et al. (2007) proposed a fusion of evolutionary neural networks (back propagation learning) and generic algorithms to forecast highest temperature/day. The input parameters included month, day, daily precipitation, <b>max</b> <b>temperature,</b> <b>min</b> <b>temperature,</b> <b>max</b> soil <b>temperature,</b> <b>min</b> soil <b>temperature,</b> <b>max</b> relative humidity, <b>min</b> relative humidity, solar radiation, and wind speed. Author claim an accuracy is 79.49 (20 error bound).|$|R
5000|$|Air <b>Temperatures.</b> The <b>max,</b> <b>min,</b> {{and mean}} <b>temperatures</b> for each month are entered into this table.|$|R
3000|$|The {{connections}} between the input layer and the middle or hidden layer contain weights [Eq. (2)], which are usually determined through training the system. The hidden layer sums the weighted inputs and uses the transfer function to create an output value. The transfer function is {{a relationship between the}} internal activation level of the neuron (called activation function) and the outputs. A typical transfer function is the sigmoid function, which varies from 0 to 1 for a range of inputs (Melesse et al. 2011). The 408 dataset was divided into training and validation data (300 training and 108 validations) for single hidden layer in three-layer feed forward back propagation algorithm. The feed forward back propagation algorithms was applied using different transfer function and best transfer function (logsig) is chosen on the basis of performance of model. The supervised learning of model is done using reference evapotranspiration (ET 0) as output and <b>temperature</b> (<b>Min.,</b> <b>Max.,</b> and Avg.), relative humidity (<b>Min.</b> and <b>Max.),</b> average wind speed, and sunshine hours as input parameters.|$|R
30|$|Correct {{estimation}} of evapotranspiration is prerequisite for the precise {{planning and management}} of irrigation schemes, hydrological modeling, groundwater recharge, crop performance, and land use planning, etc., and it became much more important in regions of arid and semiarid zones. However, {{it is difficult to}} obtain precise field measurement due to complex nature of the process itself, spatial variability, atmospheric instability, etc. There are numerous methods of computing ET, but the Penman–Monteith (PM) equation which provides the most reliable results for the estimates of reference evapotranspiration (ET 0) in most of the regions and for varied weather condition, has been adopted by Food and Agriculture Organization (FAO). In the present study, performance evaluation and comparison of simulated ET 0 by FAO 56 PM and an ANN model with single layer feed forward back propagation algorithm has been carried out. <b>Temperature</b> (<b>Min.,</b> <b>Max.,</b> and Avg.), relative humidity (<b>Min.</b> and <b>Max.),</b> average wind speed, sunshine hours and rainfall data from the meteorological monitoring station located in FRI, Dehradun on monthly basis for a period of 34  years (1980 – 2013) were used as inputs to predict ET 0.|$|R
5000|$|Numerical data {{handling}} and elementary statistical processing (sum, avg, <b>min,</b> <b>max)</b> ...|$|R
5000|$|Winter Average (day) <b>Min</b> temp=22.5⁰, <b>Max</b> Temp=25.1⁰Winter Average (night) <b>Min</b> temp=6.5⁰, <b>Max</b> Temp=7.9⁰ ...|$|R
40|$|Congeneric species occupying {{different}} habitats {{might be}} expected to have different seed dormancy strategies and germination requirements while those growing in the same habitats may be more similar. I tested this hypothesis with a broad survey of the germination of 51 Carex species from mesic deciduous forests, wet deciduous forests, and wetland or seasonally flooded areas in response to different controlled environmental conditions. A canonical discriminant analysis based on the responses of 29 species to various treatments showed clear differences among seeds from each habitat with respect to germination behaviour. Germination of seeds from mesic deciduous forest species was generally faster after moist stratification than after either submersion or dry storage. In seeds from most wet areas of deciduous forest, wetland, or seasonally flooded habitats, germination was similar or greater after submerged as compared to moist stratification. Total germination was significantly increased by light in all species except Carex pedunculata, an ant-dispersed species. Germination was not significantly different on a poorly drained peat soil than on a well-drained sandy loam. Seeds of most species were at least physiologically dormant at maturity and germination of fresh seeds was generally low. Spring germination was similar among species from all habitats and generally began in late May or early June, at fairly high <b>temperatures</b> (<b>min</b> 7 °C, <b>max</b> 17 °C), which is unusual for forest species but consistent among Carex species...|$|R
5000|$|EJB query {{language}} (EJB-QL) additions: ORDER BY, AVG, <b>MIN,</b> <b>MAX,</b> SUM, COUNT, and MOD.|$|R
5000|$|Summer Average (day) <b>Min</b> temp=35.8⁰, <b>Max</b> temp=37.1⁰ Summer Average (night) <b>Min</b> temp=21.9⁰, <b>Max</b> temp=22.9⁰ ...|$|R
50|$|Low Order Moments: Includes {{computing}} <b>min,</b> <b>max,</b> mean, standard deviation, variance, etc. for a dataset.|$|R
5000|$|Calculation: the data-logger {{processes}} {{most of the}} {{meteorological data}} for the users (avg, <b>min,</b> <b>max...).</b>|$|R
50|$|Statistics = <b>Min,</b> <b>Max,</b> Quartiles, Mean, St Dev, Missing, Medium, Sum, Variance, Skewness, Kurtosis, chi square.|$|R
5000|$|... 288-500 Trucks (<b>min,</b> <b>max</b> {{based on}} 24k HMMWV and 15k trucks in theater, and 750-1300 losses) ...|$|R
5000|$|Cluster-wide {{analytics}} aggregation with <b>MIN</b> (...) , <b>MAX</b> (...) , COUNT (...) , AVG (...) like in SQL SELECT ... GROUP BY ..., ORDER BY ... statements ...|$|R
40|$|We {{characterize}} the <b>min</b> <b>max</b> {{values of a}} class of repeated games with imperfect monitoring. Our result relies on the optimal trade-off for the team formed by punishing players between optimization of stage-payoffs and generation of signals for future correlation. Amounts of correlation are measured through the entropy function. Our theorem on <b>min</b> <b>max</b> values stems from a more general characterization of optimal strategies for a class of optimization problems. ...|$|R
30|$|In {{order to}} {{describe}} how the continual warming trend has affected {{and will continue to}} affect Michigan’s wine grape industry, it was necessary to use data from historical sources in conjunction with future projections. First, <b>temperatures</b> (<b>max,</b> <b>min</b> and mean) were calculated for the growing season (1 April– 31 October) for both regions considered in the study. The NEXDCP 30 dataset (future projections) has data obtained from 32 model runs plus one ensemble mean of all models run in the RCP 4.5 and RCP 8.5 GHG emission scenarios. However, these model simulations begin in the year 1950. The historical data from these models was developed by incorporating PRISM temperature and precipitation data when the creators were using the BCSD method of downscaling (Wood et al. 2004). This downscaled data was used as historical climate in southwest and northwest Michigan in this study from 1950 to 2005. From 2006 to 2099, there were 32 different model runs for each RCP scenario, and there was one ensemble mean for all models. This historical and future projection data was then used as the input for the analysis in this paper.|$|R
5000|$|Relaxing our {{demand on}} [...] by {{demanding}} , i.e. [...] where [...] is {{the set of}} positive semi-definite matrices, and changing {{the order of the}} <b>min</b> <b>max</b> to <b>max</b> <b>min</b> (see the references for more details), the optimization problem can be formulated as: ...|$|R
40|$|Abstract—this paper {{discusses}} about unibiometric systems, multibiometric systems, product rule, <b>max</b> {{rule and}} <b>min</b> rule of score level fusion. Score level fusion {{is used to}} generate scores of a person. <b>Min</b> <b>max</b> normalization scheme is used for normalization which normalizes scores between 0 and 1. The proposed method also evaluates the results between product rule, <b>min</b> rule and <b>max</b> rule. Keywords-multimodal biometrics, score level fusion, product rule, <b>min</b> rule, <b>max</b> rule. I...|$|R
40|$|Environmental {{conditions}} provide significant {{constraints and}} opportunities for human behavior. Paleoenvironmental studies are thus {{an important component of}} archaeological investigations seeking to understand the organization, development and decline of past societies. This dissertation develops a method of paleoenvironmental research that uses stable isotope analysis of jackrabbit and cottontail (Leporidae) bones to quantify aspects of the landscapes in which they lived. The project then applies the technique to a sample of bones excavated from four archaeological sites in desert ecosystems of North America to obtain information about human-leporid and human environmental relationships through time. Site locations include Pueblo Grande (Arizona, USA), La Ferrería (Durango, Mexico), La Quemada (Zacatecas, Mexico), and Teotihuacan (Mexico, Mexico). Results are explored across multiple temporal and spatial scales to refine the method and improve our understanding of the long-term dynamics of social-environmental systems within arid and semi-arid landscapes. Results of baseline analyses on modern bones demonstrate significant correlations between stable oxygen isotope values and moisture variables (precipitation and humidity), and between stable carbon and nitrogen isotope values and <b>temperature</b> variables (<b>min,</b> <b>max,</b> and mean <b>temperature),</b> indicating the utility of leporids as paleoenvironmental proxies. Two primary conclusions result from analyses on archaeological specimens. First, results from La Ferrería leporids demonstrate changes through time suggesting a shift towards wetter conditions during the Las Joyas (AD 850 - 1000) phase, a period characterized by increasing population and new architectural construction. Importantly, these findings link social development in the region with environmental change. Secondly, results from Teotihuacan demonstrate rising carbon isotope values concomitant with the growth of the city, likely reflecting human modifications to the landscape, and falling carbon isotope values following its sociopolitical collapse. Notably, during the peak of population and complexity of Teotihuacan leporids from the residential complex of Oztoyahualco demonstrate significantly higher stable carbon isotope values than other contexts, supporting previous assumptions that residents there specialized in rabbit management or breeding. By using stable isotope values from jackrabbit and cottontail bones from modern and archaeological contexts, this dissertation improves our understanding of the baseline factors that influence bone isotope ratios and provides new information on the social-environment dynamics of the ancient New Worl...|$|R
50|$|<b>Oil</b> <b>Temperature</b> Gauge - Indicates {{the engine}} <b>oil</b> <b>temperature.</b>|$|R
30|$|Statistical {{quantities}} {{extracted from}} the IF, IB, and IA of the LF, MF, and HF components: mean, standard deviation, median, RMS, <b>min,</b> <b>max,</b> coefficient of variation, skewness, and kurtosis.|$|R
30|$|Q 1 and Q 3 values {{can also}} be {{estimated}} from box plot diagram shown in Fig.  12. In this type of plot <b>min,</b> <b>max,</b> median of data are depicted.|$|R
40|$|In this paper, we {{introduce}} a <b>min</b> <b>max</b> approach {{for addressing the}} generalization problem in Reinforcement Learning. The <b>min</b> <b>max</b> approach works by determining a sequence of actions that maximizes the worst return that could possibly be obtained considering any dynamics and reward function compatible with the sample of trajectories and some prior knowledge on the environment. We consider the particular case of deterministic Lipschitz continuous environments over continuous state spaces, nite action spaces, and a nite optimization horizon. We discuss the non-triviality of computing an exact solution of the <b>min</b> <b>max</b> problem even after reformulating it {{so as to avoid}} search in function spaces. For addressing this problem, we propose to replace, inside this <b>min</b> <b>max</b> problem, the search for the worst environment given a sequence of actions by an expression that lower bounds the worst return that can be obtained for a given sequence of actions. This lower bound has a tightness that depends on the sample sparsity. From there, we propose an algorithm of polynomial complexity that returns a sequence of actions leading to the maximization of this lower bound. We give a condition on the sample sparsity ensuring that, for a given initial state, the proposed algorithm produces an optimal sequence of actions in open-loop. Our experiments show that this algorithm can lead to more cautious policies than algorithms combining dynamic programming with function approximators. Peer reviewe...|$|R
30|$|From two <b>min</b> and <b>max</b> problems, {{we notice}} {{that it is}} not an easy task to {{determine}} the result of the <b>min</b> and <b>max</b> problems, so we propose the following method.|$|R
50|$|CUBRID {{provides}} {{support for}} window functions {{as defined in}} the SQL:2003 standard. The implemented functions are ROW_NUMBER, COUNT, <b>MIN,</b> <b>MAX,</b> SUM, AVG, STDDEV_POP, STDDEV_SAMP, VAR_POP, VAR_SAMP, RANK, DENSE_RANK, LEAD, LAG and NTILE.|$|R
40|$|Abstract—in {{working process}} of power transformer, which {{directly}} affects the safe {{operation of transformer}} <b>oil</b> <b>temperature</b> {{as well as the}} stability of the network, so vital to transformer <b>oil</b> <b>temperature</b> detection and control. Based on single chip and chip design of digital <b>temperature</b> measurement transformer <b>oil</b> <b>temperature</b> of an intelligent control system. The system uses a digital temperature sensor DS 18 B 20 collection transformer <b>oil</b> <b>temperature,</b> improves the accuracy of the system. The low power consumption, strong anti-jamming ability of the SCM STC 89 C 51 as the main controller to achieve control and real-time monitoring of transformer <b>oil</b> <b>temperature,</b> and input control module is designed for different transformer <b>oil</b> <b>temperature</b> preset control during normal operation, improving system usability and human-computer interaction. Keywords—transformer <b>oil</b> temperature; <b>temperature</b> control; STC 89 C 51; DS 18 B 20 I...|$|R
50|$|Bejan and Lorente (2010) {{claimed that}} all the {{proposed}} optimality statements (<b>min,</b> <b>max,</b> opt, and design) have limited ad-hoc applicability, and are unified under their own proposed Constructal law of design and evolution in nature.|$|R
5000|$|Aggregation {{capability}} in SQL {{limited to}} COUNT, SUM, <b>MIN,</b> <b>MAX,</b> AVG functions. No support for GROUP BY or other aggregation functionality found in database systems. However, stored procedures {{can be used}} to implement in-the-database aggregation capability.|$|R
30|$|We {{measured}} the processing time when one humming file is matched with 48 MIDI {{data on a}} desktop computer consisting of Intel Core 2 Quad 2.33 GHz CPU, 4 GB RAM, and Windows XP OS. Experimental {{results showed that the}} processing time of each score level fusion method (<b>MIN,</b> <b>MAX,</b> SUM, Weighted SUM, and PRODUCT rules) was same as 0 ms. Another results on the desktop computer of slower speed (Intel Core 2 Duo 2.1 GHz CPU, 2 GB RAM, and Windows XP OS) showed that the processing time of all methods (<b>MIN,</b> <b>MAX,</b> SUM, Weighted SUM, and PRODUCT rules) were same as 0 ms, also.|$|R
40|$|The basic {{conceptions of}} the model „entity-relationship” as entities, relationships, {{structural}} constraints of the relationships (index cardinality, participation degree, and structural constraints of kind (<b>min,</b> <b>max))</b> are considered and formalized in terms of relations theory. For the binary relations two operators (<b>min</b> and <b>max)</b> are introduced; structural constraints are determined {{in terms of the}} operators; the main theorem about compatibility of these operators’ values on the source relation and inversion to it is given here...|$|R
40|$|Lukasiewicz {{logic is}} a "fuzzy" logic in which truth value can be real {{numbers in the}} unit interval. There are connectives for <b>min,</b> <b>max,</b> {{addition}} and complement (1 -x). The "value" of a closed formula in a fuzzy (relational model) is defined in the natural way. A formula is called valid iff it has value 1 in every fuzzy model. We show that the set of valid formulas in Lukasiewicz predicate logic is a complete Pi^ 0 _ 2 set. We also show that if we restrict {{our attention to the}} classical language (<b>min,</b> <b>max,</b> complement) then the classically valid formulas are exactly those formulas whose fuzzy value is 1 / 2...|$|R
3000|$|The {{first step}} of {{discarding}} the interior points locating inside the quadrilateral formed by four extreme points is to find those points with the <b>min</b> or <b>max</b> x/y coordinates. In sequential programming pattern, a loop over all input points needs {{to be carried out}} to find the <b>min</b> or <b>max</b> values. In parallel programming pattern, the finding of <b>min</b> or <b>max</b> values in a vector can be efficiently achieved by performing a parallel reduction. Thrust provides such common data-parallel primitive and several easy-to-use interface functions. two functions, i.e., thrust::min_element (...) and thrust::max_element (...), are used to efficiently find the <b>min</b> and <b>max</b> coordinates of all points in parallel; see lines 11 – 14 in Fig.  9.|$|R
40|$|The top <b>oil</b> <b>temperature</b> for {{transformer}} has a {{great influence}} on transformer’s operational life and load capacity, therefore, {{it is important to}} predict the top <b>oil</b> <b>temperature.</b> On the basis of analyzing and summarizing the main impacts on the top <b>oil</b> <b>temperature,</b> an idea is proposed to predict the top <b>oil</b> <b>temperature</b> by means of Bayesian network, and Bayesian network model is established. The model takes active power, reactive power, load current, ambient temperature and previous time <b>oil</b> <b>temperature</b> as its quantitative indicators, and trains the sample data to find out the probability distribution between various factors. The model is verified according to data collected from the transformer of SSZ 11 - 50 kV/ 220. The results show that the relative error between predictive value and measured value is small, which can be accepted completely in engineering. Therefore, Bayesian network is reasonable and can be widely applied to forecast the top <b>oil</b> <b>temperature...</b>|$|R
