1217|502|Public
25|$|By {{assigning}} a softmax activation function, a {{generalization of}} the logistic function, on the <b>output</b> <b>layer</b> of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs {{can be interpreted}} as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.|$|E
25|$|Igor Aizenberg {{and colleagues}} {{introduced}} it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes {{the learning of}} a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large <b>output</b> <b>layer.</b>|$|E
2500|$|Researchers {{demonstrated}} (2010) {{that deep}} neural networks interfaced with a hidden Markov model with context-dependent states {{that define the}} neural network <b>output</b> <b>layer</b> can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search [...]|$|E
50|$|The United Kingdom Census 2011 {{considered}} the village as five relevant <b>output</b> <b>layers,</b> approximately {{a third of}} the ward.|$|R
30|$|However, as {{the hidden}} <b>layer</b> <b>outputs</b> can be collinear, the {{modelling}} performance would be poor by using least square solution {{to find the}} weights between the hidden and <b>output</b> <b>layers.</b> This would be especially true for ELM as they have randomly assigned hidden layer weights and typically large number of hidden neurons are required. This paper proposes using PCR to obtain the weights between the hidden and <b>output</b> <b>layers</b> to overcome the multicollinearity problems. Instead of regressing H and T directly, the principal components of H matrix are used as regressors.|$|R
3000|$|There are two <b>output</b> <b>layers</b> in the {{detection}} model: one output {{is used to}} describe the probability distribution of each RoI, p[*]=[*](p 0,[*]p 1,[*]…,[*]p [...]...|$|R
2500|$|Typically, neurons are {{organized}} in layers. Different layers may perform {{different kinds of}} transformations on their inputs. Signals travel from the first (input), to the last (<b>output)</b> <b>layer,</b> possibly after traversing the layers multiple times. [...] In artificial networks with multiple hidden layers, the initial layers might detect primitives (e.g. the pupil in an eye, the iris, eyelashes, etc..) and their output is fed forward to deeper layers who perform more abstract generalizations (e.g. [...] eye, mouth).... and so on until the final layers perform the complex object recognition (e.g. face).|$|E
2500|$|Each block {{consists}} of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the <b>output</b> <b>layer</b> has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is [...] Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: ...|$|E
50|$|Outstar is an {{output from}} the neurodes {{of the hidden}} layer of the neural network {{architecture}} which works as an input for <b>output</b> <b>layer.</b> Neurode of hidden layer provides input to neurode of the <b>output</b> <b>layer.</b>|$|E
50|$|In 2010, {{researchers}} extended {{deep learning}} from TIMIT to large vocabulary speech recognition, by adopting large <b>output</b> <b>layers</b> of the DNN based on context-dependent HMM states constructed by decision trees.|$|R
30|$|In (7) and (8), ωHO is a N[*]×[*]K vector indicting {{the weights}} between the hidden <b>layers</b> and <b>output</b> <b>layers,</b> and ωIH is a J[*]×[*]N vector indicting the weights between the input layers and hidden layers, where N {{is the number}} of hidden nodes. hi is the hidden <b>layer’s</b> <b>output</b> vector and g is the {{activation}} function in hidden nodes [27].|$|R
50|$|Display of {{step-by-step}} translation layers {{gives an}} increased {{level of confidence}} to the end-user, as he can trace back to the source and get clarity regarding translated text by analysis of the <b>output</b> <b>layers</b> and some reference to context.|$|R
50|$|This {{depends on}} the change in weights of the th nodes, which {{represent}} the <b>output</b> <b>layer.</b> So to change the hidden layer weights, the <b>output</b> <b>layer</b> weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.|$|E
5000|$|It {{consists}} of one input layer, one hidden layer and one <b>output</b> <b>layer.</b> The number of neurons in the <b>output</b> <b>layer</b> {{depends on the}} number of hidden units K. Each hidden neuron has N binary input neurons: The weights between input and hidden neurons are also binary: ...|$|E
5000|$|Do a full forward {{pass with}} the input {{through to the}} <b>output</b> <b>layer</b> ...|$|E
5000|$|The [...] "POPFNN" [...] {{architecture}} is a five-layer neural network where the layers from 1 to 5 are called: input linguistic layer, condition layer, rule <b>layer,</b> consequent <b>layer,</b> <b>output</b> linguistic <b>layer.</b> The fuzzification of the inputs and the defuzzification of the outputs are respectively {{performed by the}} input linguistic and <b>output</b> linguistic <b>layers</b> while the fuzzy inference is collectively performed by the rule, condition and consequence layers.|$|R
50|$|The United Kingdom Census 2011 {{considered}} the village as three relevant <b>output</b> <b>layers,</b> approximately {{a quarter of}} the ward Shere, the latter being used for elections to Guildford Borough Council. All parts featured their single largest proportion of housing as single family homes with gardens.|$|R
30|$|We {{propose a}} {{hierarchical}} classification system {{that consists of}} a hybrid MLP/HMM, where the neural network architecture performs phone classification with a hierarchical set of broad class phonetic classifiers. The number of <b>output</b> <b>layers</b> in the MLP {{is the same as}} the cut places of the clustering tree, with the same order. Each cluster is characterized by the set of phones grouped by the tree cut and is called broad class. The broad class predictions from earlier classifiers are fed to the next ones in order to enhance the class discrimination in the current classifier. The last layer performs a 1 -to- 61 classification of the set of phones. All layers are trained concurrently so that, in training mode, targets are presented at all <b>output</b> <b>layers.</b>|$|R
5000|$|The {{output from}} the Kohonen layer, {{which is the}} winning neuron, feeds into a hidden layer and finally into an <b>output</b> <b>layer.</b> In other words, the Kohonen SOM is the front-end, while the hidden and <b>output</b> <b>layer</b> of a {{multilayer}} perceptron is the back-end of thehybrid Kohonen SOM. The hybrid Kohonen SOM was first applied to machine vision systems for image classification and recognition.|$|E
5000|$|The {{first factor}} is {{straightforward}} to evaluate if the neuron {{is in the}} <b>output</b> <b>layer,</b> because then [...] and ...|$|E
5000|$|Do a {{feed-forward}} pass {{to compute}} activations at all hidden layers, {{then at the}} <b>output</b> <b>layer</b> to obtain an output ...|$|E
2500|$|The United Kingdom Census 2011 {{considered}} the village as three relevant <b>output</b> <b>layers,</b> approximately {{a quarter of}} the ward Shere, the latter being used for elections to Guildford Borough Council. [...] All parts featured their single largest proportion of housing as single family homes with gardens.|$|R
30|$|There are 52 {{characters}} (26 {{for small}} and 26 for capital alphabets) to be classified. Hence 6 bit variable {{is required for}} the output. Thus the number of output neurons is six. Log sigmoidal (logsig) is the activation function for neurons in hidden and <b>output</b> <b>layers.</b>|$|R
50|$|The United Kingdom Census 2011 {{considered}} the village as four relevant <b>output</b> <b>layers,</b> approximately {{a third of}} the ward Shere, the latter being used for elections to Guildford Borough Council. All parts featured their single largest proportion of housing as either semi-detached or single-family homes and most of these with gardens.|$|R
5000|$|Architecturally, the {{simplest}} {{form of an}} autoencoder is a feedforward, non-recurrent neural network {{very similar to the}} multilayer perceptron (MLP) - having an input layer, an <b>output</b> <b>layer</b> and one or more hidden layers connecting them -, but with the <b>output</b> <b>layer</b> having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value [...] given inputs [...] ). Therefore, autoencoders are unsupervised learning models.|$|E
50|$|A CNN {{consists}} of an input and an <b>output</b> <b>layer,</b> as well as multiple hidden layers. The hidden layers are either convolutional, pooling or fully connected.|$|E
50|$|The <b>output</b> <b>layer</b> {{compares the}} {{weighted}} votes for each target category accumulated {{in the pattern}} layer and uses the largest vote to predict the target category.|$|E
3000|$|As {{described}} in Section 3, our MLP neural net performs {{a couple of}} forward and backward processes at every iteration m. First, the whole set of connected layers propagate the degraded image y from the input to the <b>output</b> <b>layers</b> by means of Equation 14. Afterwards, the new synaptic weigh matrixes W [...]...|$|R
40|$|Feed-forward {{neural network}} models and linear {{regression}} models {{were developed to}} estimate the net weight of rice grain after drying and sorting process. A three-layer (one input, one hidden, and one <b>output</b> <b>layers)</b> feed- forward neural netw本研究利用前饋式類神經網路與線性回歸等兩種模式預測濕穀乾燥後之稻穀淨重 ，並比較兩者之特點。前饋式類神經網路模型由輸入層、隱藏層、與輸出層等神經層所構成 ，其中 輸入層有 2 估神經元(輸入數據為濕穀含水率與容積 重) ，隱藏層有 1 個神經元，輸 出層則為 1 個神經元(輸出數據為乾穀淨重) ，利用倒傳遞演算法訓練網路，使預測總誤差收 斂至可接受之範圍。線性回歸模式，則分別以一 次與二次方程式表示。本研究所建立乾穀 淨重預測模式，預測稻穀淨重與實際量測結果比較，不論類神經網路模式或線性回歸模式， 所得之平...|$|R
50|$|Instantaneously trained neural {{networks}} (ITNNs) {{were inspired by}} the phenomenon of short-term learning that seems to occur instantaneously. In these networks the weights of the hidden and the <b>output</b> <b>layers</b> are mapped directly from the training vector data. Ordinarily, they work on binary data, but versions for continuous data that require small additional processing exist.|$|R
50|$|An autoencoder, autoassociator or Diabolo {{network is}} similar to the {{multilayer}} perceptron (MLP) - with an input layer, an <b>output</b> <b>layer</b> and one or more hidden layers connecting them. However, the <b>output</b> <b>layer</b> has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value. Therefore, autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings, typically for the purpose of dimensionality reduction and for learning generative models of data.|$|E
50|$|Radial basis {{functions}} are functions {{that have a}} distance criterion {{with respect to a}} center. Radial basis functions have been applied as a replacement for the sigmoidal hidden layer transfer characteristic in multi-layer perceptrons. RBF networks have two layers: In the first, input is mapped onto each RBF in the 'hidden' layer. The RBF chosen is usually a Gaussian. In regression problems the <b>output</b> <b>layer</b> is a linear combination of hidden layer values representing mean predicted output. The interpretation of this <b>output</b> <b>layer</b> value is the same as a regression model in statistics. In classification problems the <b>output</b> <b>layer</b> is typically a sigmoid function of a linear combination of hidden layer values, representing a posterior probability. Performance in both cases is often improved by shrinkage techniques, known as ridge regression in classical statistics. This corresponds to a prior belief in small parameter values (and therefore smooth output functions) in a Bayesian framework.|$|E
50|$|The {{optimization}} algorithm repeats a two phase cycle, propagation and weight update. When an input vector {{is presented to}} the network, it is propagated forward through the network, layer by layer, until it reaches the <b>output</b> <b>layer.</b> The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated {{for each of the}} neurons in the <b>output</b> <b>layer.</b> The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.|$|E
30|$|In which l is {{the number}} of nodes in hidden layer, n and m are the number of nodes in the input and <b>output</b> <b>layers,</b> respectively, and a is {{constant}} ranging from 0 to 10. In this paper, the actual number of nodes in hidden layer is computed to be √(6 + 1) + 10 = 13.|$|R
3000|$|In Eq. (1), {{the generic}} form of {{nonlinear}} singular Lane–Emden equation is given. While in Eqs. (3 – 5) continuous mapping of neural networks models for approximate solution ŷ([...] x [...]) and its derivatives {{are presented in}} term of single input, hidden and <b>output</b> <b>layers.</b> Additionally, in the hidden layers log-sigmoid activation function and its derivatives are used for y(t) and its derivatives, respectively.|$|R
40|$|This paper {{proposes a}} {{minimization}} learning method for neural networks with one hidden layer. We treat {{two types of}} networks without and with thresholds in their <b>output</b> <b>layers.</b> Both of them are learnt by minimizing error functions whenever one unit {{is added to the}} hidden layer. Our learning method is applied to design a neural network with thresholds for image recognition...|$|R
