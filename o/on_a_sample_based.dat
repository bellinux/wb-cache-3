2|10000|Public
40|$|The {{purpose of}} this {{document}} (RE- 239) is to analyze {{the way in which}} the Bank has been supporting sector reform policies in various countries in recent years. It is based <b>on</b> <b>a</b> <b>sample</b> <b>based</b> on the principal evaluation results from 14 sector loans that were approved between 1990 and 1995, in eight member countries of the Bank. Drawing on elements both of theory and of empirical evidence, the document establishes a set of features that are necessary for achieving sustained growth of the kind that will contribute to reducing poverty. The sector operation evaluations analyzed here focus on the impact of sector lending in supporting policies and institutions in those areas where major efforts are being made to consolidate stabilization and improve growth prospects, and thereby to reduce poverty...|$|E
40|$|International audienceIn {{this work}} we present some {{parameters}} {{that are being}} studied to perform a purely morphological analysis of reconstructed images of extended objects, particularly galaxies, {{in the context of}} the ESA-Gaia mission. Those parameters, known as Concentration, Asymmetry, Clumpiness, Gini's coefficient and the Momentum of the brightest 20 % of the galaxy, form a set that is becoming commonly used when a limited number of pixels is available to analyse, such as will be the case for Gaia reconstructed images. We comment about small modifications on those parameters that are planned to be performed. We also report tests with a preliminar version of the code that is being written to analyse Gaia images <b>on</b> <b>a</b> <b>sample</b> <b>based</b> on the Frei catalog of galaxies. Finally, we comment on the possibility of using Support Vector Machines to perform the morphological classification based on those measured parameters, and conclude that a very good level of segregation can be obtained for a two-class discrimination...|$|E
30|$|Decide <b>on</b> <b>an</b> {{adequate}} <b>sample</b> size <b>based</b> <b>on</b> {{the expected}} differences {{that should be}} observed between control and disease/treatment groups and the intra-animal variability of the SPECT/PET measurements.|$|R
40|$|Here, in {{this paper}} it has been {{considered}} a sub family of exponential family. Maximum likelihood estimations (MLE) for the parameter of this family, probability density function, and cumulative density function <b>based</b> <b>on</b> <b>a</b> <b>sample</b> and <b>based</b> <b>on</b> lower record values have been obtained. It has been considered Mean Square Error (MSE) as a criterion for determining which is better in different situations. Additionally, it has been proved some theories about the relations between MLE based on lower record values and <b>based</b> <b>on</b> <b>a</b> random <b>sample.</b> Also, some interesting asymptotically properties for these estimations have been shown during some theories...|$|R
40|$|This book {{presents}} {{the details of}} the BONUS algorithm and its real world applications in areas like sensor placement in large scale drinking water networks, sensor placement in advanced power systems, water management in power systems, and capacity expansion of energy systems. A generalized method for stochastic nonlinear programming <b>based</b> <b>on</b> <b>a</b> <b>sampling</b> <b>based</b> approach for uncertainty analysis and statistical reweighting to obtain probability information is demonstrated in this book. Stochastic optimization problems are difficult to solve since they involve dealing with optimization and uncertainty loops. There are two fundamental approaches used to solve such problems. The first being the decomposition techniques and the second method identifies problem specific structures and transforms the problem into a deterministic nonlinear programming problem. These techniques have significant limitations on either the objective function type or the underlying distributions for the uncertain variables. Moreover, these methods assume that there are a small number of scenarios to be evaluated for calculation of the probabilistic objective function and constraints. This book begins to tackle these issues by describing a generalized method for stochastic nonlinear programming problems. This title is best suited for practitioners, researchers and students in engineering, operations research, and management science who desire a complete understanding of the BONUS algorithm and its applications to the real world...|$|R
40|$|Background Compliance {{in cancer}} {{screening}} among socially disadvantaged persons {{is known to}} be lower than among more socially advantaged persons. However, most of the studies regarding compliance proceed via a questionnaire and are thus limited by self-reported measures of participation and by participation bias. This study aimed at investigating the influence of socioeconomic characteristics <b>on</b> compliance to <b>an</b> organised colorectal cancer screening programme <b>on</b> <b>an</b> unbiased <b>sample</b> <b>based</b> <b>on</b> data from the entire target population within a French geographical department, Calvados (n 180 045). Methods Individual data of participation and aggregate socioeconomic data, from the structure responsible fo...|$|R
30|$|Finally, {{this work}} turned into <b>an</b> {{exploratory}} study <b>on</b> 17 texts that {{became part of}} the research on TEALE published between 1960 and 2015. It is true that the sampling system did not provide us with subsequent texts that possibly are currently available, but the explanatory nature of the study should focus on whether to use resources in <b>a</b> similar research <b>on</b> <b>a</b> larger <b>sample</b> <b>based</b> <b>on</b> the results obtained. Moreover, if we also understand {{that the vast majority of}} this scientific production based on experiences that has subsequently impacted on other published studies, concerns to higher education.|$|R
40|$|Abstract User {{participation}} {{and involvement in}} software development {{are considered to be}} essential for a successful software system. Three research areas, human aspects of software engineering, requirements engineering, and information systems, study these topics from various perspectives. We {{think it is important to}} analyze user {{participation and}} involvement in software engineering comprehensively to encourage further research in this area. We investigate the evidence on effects of user participation and involvement on system success and we explore which methods are available in literature. A systematic mapping study was conducted. The systematic search yielded 3, 698 hits, from which we identified 289 unique papers. These papers were reviewed by the first author based on inclusion and exclusion criteria. The second author validated the selection of papers by reviewing the reasons for exclusion and inclusion and the corresponding papers <b>on</b> <b>a</b> <b>sample</b> <b>base.</b> 58 of the 289 papers were selected (22 statistical survey and meta-study papers and 36 methods papers). Based on the empirical evidence of the surveys and meta-studies, we developed a meta-analysis of structural equation models. This overview demonstrates that most papers showed positive correlations between aspects of development processes (including user participation) and human aspects (including user involvement) and system success. The analysis of the proposed solutions from the method papers revealed a wide variety of user participation and involvement practices for most activities within software development...|$|R
40|$|This paper {{examines}} {{the impact of}} internationalization on the product, process and organizational innovations of Korean service firms. Despite the increasing importance of the service sector and the discrepancies in the natures of the manufacturing and service industries, the internationalization-innovation link {{in the context of}} service firms has rarely been examined empirically <b>on</b> <b>a</b> large <b>sample.</b> <b>Based</b> <b>on</b> the results of the logistic regressions using the 2006 Korean Innovation Survey data, we found that Korean service firms' international expansion is significantly and positively associated with their product and organizational innovations. In addition, the magnitude of the estimates in our models revealed that internationalization has <b>a</b> greater impact <b>on</b> product innovation than on process or organizational innovation...|$|R
40|$|An {{important}} structuring {{mechanism for}} knowledge bases is building an inheritance hierarchy of classes {{based on the}} content of their knowledge objects. The hierarchy can be used to handle several query processing tasks more efficiently. Building and maintaining this hierarchy is a difficult task for the knowledge engineer. The notion of knowledge space has been previously proposed to help automate such a task. In this paper an incremental algorithm for building the knowledge space is proposed and tested <b>on</b> <b>a</b> <b>sample</b> knowledge <b>base.</b> The empirical behavior shows that after a point the knowledge space can be updated in close to constant time on the average. The knowledge space structure is presented in the framework of the theory of concept lattices. The presentation suggests that the notion of a complete knowledge lattice might be a worthwhile alternative because of the additional richness of the structure. However, this richness induces a large amount of additional processing and storage [...] ...|$|R
40|$|Voluminous spatio-temporal data make {{multimedia}} analysis tasks extremely {{inefficient and}} lack of adaptability. We present <b>a</b> novel experience <b>based</b> <b>sampling</b> technique which {{has the ability to}} focus on the analysisâ€™s task by making use of the contextual information and past experiences. <b>Based</b> <b>on</b> this, <b>a</b> <b>sampling</b> <b>based</b> dynamic attention model is built by sensing the experiential environment. Sensor samples are used to gather information about the current environment and attention samples are used to represent the current state of attention. In our framework, the taskattended samples are inferred from experiences and maintained by <b>a</b> <b>sampling</b> <b>based</b> dynamical system. The multimedia analysis task can then focus on the attention samples only. Moreover, past experiences and the current environment can be used to adaptively correct and tune the attention. As a prototypical multimedia analysis task, we tackle the face detection problem in videos. Face detection is only performed on the attended samples to achieve robust real time processing. Experimental results have been presented to demonstrate the efficacy of our technique. This experience <b>based</b> <b>sampling</b> <b>based</b> analysis method appears to be a promising technique for general multimedia analysis problems. The generality stems from the power of the sampling method which makes no assumptions about the form of distribution of attention which is usually multimodal in nature...|$|R
40|$|Ensuring {{conformance}} {{as well as}} establishing quality, {{software testing}} {{is an integral part}} of software engineering lifecycle. However, due to resource and time-to-market constraints, testing all exhaustive possibilities is impossible in nearly all practical testing problems. Considering the aforementioned constraints, much research is now focusing <b>on</b> <b>a</b> <b>sampling</b> technique <b>based</b> <b>on</b> interaction testing (termed t-way strategy). Although helpful, most existing t-way strategies (e. g. AETG, IPOG and GTWay) assume that all parameters have uniform interaction. However, in reality, the interaction between parameters is rarely uniform. Some parameters may not even interact rendering wasted testing efforts. As a result, a number of newly developed t-way strategies that considers variable strength interaction based on input-output relationships have been developed in the literature e. g. Union, ParaOrder and Density. Although useful, these strategies often lack in optimality i. e. in term of the generated test size. Furthermore, no single strategy appears to be dominant as the optimal generation of t-way interaction test suite is considered NP hard problem. Motivated by the abovementioned challenges, this paper proposes and implements a new strategy, called General Variable Strength (GVS). It is demonstrated that GVS, in some cases, produces better results than other competing strategies...|$|R
40|$|This paper {{examines}} {{the relationship between}} hedge fund activism and target firm performance, executive compensation, and executive wealth. It introduces a theoretical framework that describes the activism process as a sequence of discrete decisions. The methodology uses regression analysis <b>on</b> <b>a</b> matched <b>sample</b> <b>based</b> <b>on</b> firm size, industry, and market-to-book ratio. All regressions control for industry and year fixed effects. Schedule 13 D Securities and Exchange Commission (SEC) filings are the source for the statistical sample of hedge fund target firms. I supplement that data with target firm financial, operating, and share price information from the CRSP-COMPUSTAT merged database. Activist hedge funds target undervalued or underperforming firms with high profitability and cash flows. They do not avoid firms with powerful CEOs. Leverage, executive compensation, pay for performance and CEO turnover increase at target firms after {{the arrival of the}} activist hedge fund. Target firm executivesâ€™ wealth is more sensitive to changes in share price after hedge fund activism events suggesting that the executive team experiences changes to their compensation structure that provides incentive to take action to improve returns to shareholders. The top executives reap rewards for increasing firm value but not for increased risk taking...|$|R
40|$|In this paper, a {{subspace}} {{identification solution}} is provided for Active Noise Control (ANC) problems. The solution {{is related to}} so-called block updating methods, where instead of updating the (feedforward) controller <b>on</b> <b>a</b> <b>sample</b> by <b>sample</b> <b>base,</b> it is updated each time based <b>on</b> <b>a</b> block of N samples. The use of the subspace identification based ANC methods enables non-iterative derivation and updating of MIMO compact state space models for the controller. The robustness property of subspace identification methods forms {{the basis of an}} accurate model updating mechanism, using small size data batches. The design of a feedforward controller via the proposed approach is illustrated for an acoustic duct benchmark problem, supplied by TNO Institute of Applied Physics (TNO-TPD), the Netherlands. We also show how to cope with intrinsic feedback. A comparison study with various ANC schemes, such as block Filtered-U demonstrates the increased robustness of a subspace derived controller. Keyword [...] ...|$|R
40|$|An {{important}} {{class of}} structural econometric models (nonlinear rational expectations, option pricing, auction models, [...] .) characterize observable variables as highly nonlinear transforma- tions of some latent variables. These transformations are one-to-one, but {{they depend on}} the unknown distribution of the latent variables through the equilibrium of the game and/or the learning process. Therefore numerical complexity of the equilibrium definition generates sub- stantial obstacles for the direct implementation of maximum likelihood inference. This is a particular case of argmax estimators <b>based</b> <b>on</b> <b>a</b> untractable <b>sample</b> <b>based</b> criterion Q[exp. T][thÃªta,lambda(thÃªta) ] contaminated by the occurences of [thÃªta] in a â€˜nuisance functionâ€™ [lambda(thÃªta) ] to which corresponds a simple criterion Q[exp. T][thÃªta,lambda(thÃªta. exp 0) ] with [thÃªta. exp. 0] the true, unknown value of the parameter. The natural idea is to replace the unknown value [lambda(thÃªta. exp 0) ] by some â€™good proxyâ€™, say [lambda(thÃªta. exp 1) ], to maximize Q[exp. T][thÃªta,lambda(thÃªta. exp 1) ] with respect to [thÃªta] {{and to get a}} new, updated estimate which, in turn, can be used for approxi- mating [lambda(thÃªta. exp 0) ], [...] . Such steps can be considered for <b>a</b> fixed <b>sample</b> size (iterative M-estimators), or each time new data arrive (recursive estimators). In this paper we present and analyze several iterative and recursive (Robbins-Monro type) estimation procedures of this kind which, at the end, are applied to the class of structural econometric models which motivated this work. Iterative procedures, M-estimators, Robbins-Monro procedures, structural models, latent variables...|$|R
40|$|Some films {{based on}} electromechanically active polymer {{composites}} have been prepared. Polydimethylsiloxane-Î±,Ï‰-diols (PDMSs) having different molecular masses (Mv = 60 700 and Mv = 44 200) {{were used as}} matrix in which two different active fillers were incorporated: titanium dioxide in situ generated from its titanium isopropoxide precursor and silica particles functionalized with polar aminopropyl groups <b>on</b> surface. <b>A</b> reference <b>sample</b> <b>based</b> <b>on</b> simple crosslinked PDMS was also prepared. The composites processed as films were investigated to evaluate their ability to act as efficient electromechanical actuators for potential biomedical application. Thus, the surface morphology of interest for electrodes compliance was analysed by atomic force microscopy. Mechanical and dielectric characteristics were evaluated by tensile tests and dielectric spectroscopy, respectively. Electromechanical actuation responses were measured by interferometry. The biocompatibility of the obtained materials has been verified through tests in vitro and, for valuable films, in vivo. The experimental, clinical and anatomopathological evaluation of the in vivo tested samples did not reveal significant pathological modifications...|$|R
40|$|AbstractHigher {{proportion}} of non-seatbelt usage rates in crashes occurring during nighttime shows that daytime seatbelt usage alone may not indicate the overall usage patterns. These findings have prompted various agencies to estimate seatbelt usage rates during nighttime. These agencies {{developed their own}} methodology for data collection and data analyses. In spite of all these recent developments, collecting representative sample at nighttime remains an issue which {{requires a lot of}} effort. This paper is an attempt to develop a methodology to collect nighttime seatbelt usage data more efficiently and accurately <b>based</b> <b>on</b> <b>a</b> mathematical <b>sampling</b> theory. <b>Based</b> <b>on</b> this methodology, two sets of data collection per site are recommended. Duration of data collection varies depending on vehicle miles traveled at the site of interest. The authors hope that this methodology could be used in other transportation related data collection efforts, where identifying critical time and time duration for collecting representative data samples are important...|$|R
40|$|In {{this paper}} we provide a novel {{approach}} for breaking a significant class of block ciphers, the so-called SPN ciphers, using the process of gene assembly in ciliates. Our proposed scheme utilizes, for the first time, the Turing-powerful potential of gene assembly procedure of ciliated protozoa {{into the real world}} computations and has a fewer number of steps than the other proposed schemes to break a cipher. We elaborate notions of formal language theory based on AIR systems, which {{can be thought of as}} a modified version of intramolecular scheme to model the ciliate bio-operations, for construction of building blocks necessary for breaking the cipher, and based on these nature-inspired constructions which are as powerful as Turing machines, we propose a theoretical approach for breaking SPN ciphers. Then, we simulate our proposed plan for breaking these ciphers <b>on</b> <b>a</b> <b>sample</b> block cipher <b>based</b> <b>on</b> this structure. Our results show that the proposed scheme has 51. 5 percent improvement over the best previously proposed nature-inspired scheme for breaking a cipher...|$|R
40|$|This study {{investigated}} the impact of task-based writing on EFL learnersâ€™ writing performance and creativity. For this purpose, 56 female intermediate Iranian EFL learners were chosen from a total number of 89 through their performance <b>on</b> <b>a</b> <b>sample</b> piloted PET. <b>Based</b> <b>on</b> the result, the students {{were randomly assigned to}} one control and one experimental group with 28 participants in each. Prior to the treatment, students took part in a writing test (part of PET) and the Abedi-Schumacher Creativity Test (ACT) as pretests. Both groups underwent the same amount of teaching and the same writing topics during 18 sessions of treatment. The only difference was that the experimental group was engaged in doing task-based writing activities while the control group was not asked to do any kind of tasks. At the end of the treatment, a writing test (another PET) and the ACT were administered to both groups. The results of the statistical analysis demonstrated that learners benefited significantly from task-based writing in terms of both their writing and creativity...|$|R
40|$|In {{this letter}} we report the {{discovery}} of a z= 4. 88 radio galaxy discovered with a new technique which does not rely <b>on</b> pre-selection of <b>a</b> <b>sample</b> <b>based</b> <b>on</b> radio properties such as steep-spectral index or small angular size. This radio galaxy was discovered in the Elais-N 2 field and has a spectral index of alpha = 0. 75, i. e. not ultra-steep spectrum. It also has a luminosity consistent with being drawn from the break of the radio luminosity function and can therefore be considered as a typical radio galaxy. Using the Spitzer-SWIRE data over this field we find that the host galaxy is consistent with being similarly massive to the lower redshift powerful radio galaxies (~ 1 - 3 L*). We note however, that at z= 4. 88 the H-alpha line is redshifted into the IRAC 3. 6 micron filter and some of the flux in this band may be due to this rather than stellar continuum emission. The discovery of such a distant radio source from our initial spectroscopic observations demonstrate the promise of our survey for finding the most distant radio sources. Comment: 5 pages, 3 figures, accepted for publication in MNRAS Letter...|$|R
25|$|Reflecting on {{previous}} mtDNA studies {{carried out by}} Behar, Atzmon et al. conclude that all major Jewish population groups are showing evidence for founder females of Middle Eastern origin with coalescence times >2000 years. A 2013 study, <b>based</b> <b>on</b> <b>a</b> much larger <b>sample</b> <b>base,</b> drew differing conclusions, namely, that the Mt-DNA of Ashkenazi Jews originated among southern European women, where Diaspora communities had been established centuries before {{the fall of the}} Second Temple in 70 CE. A 2014 study by Fernandez et al. found that Ashkenazi Jews display a frequency of haplogroup K which suggests an ancient Near Eastern origin, stating that this observation clearly contradicts {{the results of the study}} led by Richards which suggested a predominantly European origin for the Ashkenazi community's maternal lines. However, the authors of the 2014 study also state that definitively answering the question of whether this group was of Jewish origin rather than the result of a Neolithic migration to Europe would require the genotyping of the complete mtDNA in ancient Near Eastern populations.|$|R
40|$|International audienceBACKGROUND: Compliance {{in cancer}} {{screening}} among socially disadvantaged persons {{is known to}} be lower than among more socially advantaged persons. However, most of the studies regarding compliance proceed via a questionnaire and are thus limited by self reported measures of participation and by participation bias. This study aimed at investigating the influence of socioeconomic characteristics <b>on</b> compliance to <b>an</b> organised colorectal cancer screening programme <b>on</b> <b>an</b> unbiased <b>sample</b> <b>based</b> <b>on</b> data from the entire target population within a French geographical department, Calvados (N= 180, 045). METHODS: Individual data of participation and aggregate socioeconomic data, from respectively the structure responsible for organising screening and the French census, were analysed simultaneously by a multilevel model. RESULTS: Uptake was significantly higher in women than in men; odds ratio (OR= 1. 33; 95 %CI: 1. 21 - 1. 45); and significantly lower in the youngest (50 - 59 years) and in the oldest (70 - 74 years) persons, compared with intermediate ages 60 - 69 years with respectively OR= 0. 70 (95 %CI: 0. 63 - 0. 77) and OR= 0. 82 (95 %CI: 0. 72 - 0. 93). Uptake fell with increasing level of deprivation, {{there was a significant difference}} of uptake probability between the least deprived and the most deprived areas (OR= 0. 68; 95 %CI: 0. 59 - 0. 79). No significant influence of the general practitioners density was found. CONCLUSION: Multilevel analysis allowed to detect areas of weak uptake linked to areas of strong deprivation. These results suggest that targeting populations with a risk of low compliance, as identified both socially and geographically in our study, could be adopted to minimise inequalities in screening...|$|R
40|$|In large networks, the {{connected}} triples {{are useful}} for solving various tasks including link prediction, community detection, and spam filtering. Existing works in this direction concern mostly with the exact or approximate counting of connected triples that are closed (aka, triangles). Evidently, the task of triple sampling has not been explored in depth, although <b>sampling</b> is <b>a</b> more fundamental task than counting, and the former is useful for solving various other tasks, including counting. In recent years, some works on triple sampling have been proposed that are <b>based</b> on direct <b>sampling,</b> solely {{for the purpose of}} triangle count approximation. They <b>sample</b> only from <b>a</b> uniform distribution, and are not effective for <b>sampling</b> triples from <b>an</b> arbitrary user-defined distribution. In this work we present two indirect triple sampling methods that are based on Markov Chain Monte Carlo (MCMC) sampling strategy. Both of the above methods are highly efficient compared to a direct sampling-based method, specifically for the task of <b>sampling</b> from <b>a</b> non-uniform probability distribution. Another significant advantage of the proposed methods is that they can sample triples from networks that have restricted access, <b>on</b> which <b>a</b> direct <b>sampling</b> <b>based</b> method is simply not applicable...|$|R
40|$|The {{purpose of}} this in vivo, single-blind, {{randomized}} study was to compare fluoride concentrations in saliva of patients treated with oral hygiene products containing different fluoride salts. The study involved 104 students attending the University of Sassari. Participants were subdivided: group A used a sodium monofluorophosphate (NaMFP) toothpaste; groups B and C used an amine fluoride (AmF) toothpaste; group D used a toothpaste and a mouthwash both based on AmF, and group E used a toothpaste and <b>a</b> varnish both <b>on</b> <b>an</b> NaMFP <b>base.</b> <b>Samples</b> of unstimulated saliva were collected at baseline (t 0), {{at the end of}} the 20 days' treatment phase (t 1) and after 24 h, during which the volunteers refrained from any oral hygiene measure (t 2). Saliva fluoride concentrations were measured using an ion-specific electrode. All measurements were made in triplicate and analysed statistically using ANOVA. In saliva, the mean fluoride concentration increased significantly in each treatment group. In conclusion, the fluoride concentration in saliva can be maintained to an optimal therapeutic level with the regular use of fluoridated products...|$|R
40|$|We {{introduce}} the first index {{that can be}} built in <b>o(n)</b> time for <b>a</b> text of length n, and also queried in o(m) time for a pattern of length m. <b>On</b> <b>a</b> constant-size alphabet, for example, our index uses O(n^ 1 / 2 +Îµn) bits, is built in O(n/^ 1 / 2 -Îµ n) deterministic time, and finds the occ pattern occurrences in time O(m/ n + âˆš(n) n + occ), where Îµ> 0 is an arbitrarily small constant. As a comparison, the most recent classical text index uses O(n n) bits, is built in O(n) time, and searches in time O(m/ n + n + occ). We build <b>on</b> <b>a</b> novel text <b>sampling</b> <b>based</b> <b>on</b> difference covers, which enjoys properties that allow us efficiently computing longest common prefixes in constant time. We extend our results to the secondary memory model as well, where we give the first construction in o(Sort(n)) time of a data structure with suffix array functionality, which can search for patterns in the almost optimal time, with an additive penalty of O(âˆš(_M/B n) n), where M {{is the size of}} main memory available and B is the disk block size...|$|R
40|$|Family is {{the basic}} aspect in the {{development}} of childâ€™s well-being. A positive parent behavior in daily routines and social support will lead to a better and safe environment for the family. This study aimed to investigate how this positive behavior in a family have a role in childrenâ€™s well-being. Childrenâ€™s Subjective Well-Being and Family Positive Behavior questionnaire were administered in two cities, Bandung and Sumedang, with proportionate stratified <b>sampling</b> <b>on</b> <b>a</b> school <b>based</b> <b>sample</b> of children and adolescence aged 9 to 13 years in 2017. Data from 367 children was analyzed with Partial Least Square using Smart PLS 2. 0, with subjective well-being was employed in order to control the effect of positive family behavior (eat, connection, movement, play, learn, give, and religious). Findings showed that {{the amount of time spent}} with the child during mealtime, play, learn, sport, or worship, had no direct effect on children subjective well-being. We should be considered about the role of environmental experiences that affect wellbeing, not only about the quantities but the qualities. Children should take the meaning of positive family behavior such as safety, love, care and support to improve their self-esteem, self-confidence, aspiration and sense of secure, as the construct of children subjective wellbeing. Keywords: subjective well-being, positive family behavior, parent-child relationship...|$|R
40|$|Water {{quality data}} banks {{are needed to}} {{document}} the status and trends of water pollution in a country. Examples of such systems are the STORET system in the U. S., the NAQUADAT system in Canada and the EIS system in Scandinavia. All of these systems require trained personnel {{to help in the}} formulation of the inquiry and the actual querying of the system. By contrast, what is described in this paper is an on-line, interactive data management and analysis system which allows the user the direct search, update, retrieval and analysis of the data from a computer terminal. The user addresses the system in a high level language closely resembling English and has complete control over building, updating and querying the individual data banks. Almost all statistical operations can be performed on the data starting from histograms, distributions, correlations to regression, discriminant, component and spectral analysis. Commands for producing camera-ready graphs on graphic terminals are available. The system is implemented on The University of Michigan Computer System and can be accessed through local telephone numbers in more than 100 cities in the U. S. and Canada and from the major European capitals via the TELENET system. The operation of the system is illustrated <b>on</b> <b>a</b> small <b>sample</b> data <b>base</b> <b>on</b> the Ohio river provided by the Ohio River Sanitation Commission (ORSANCO) ...|$|R
40|$|Development of {{quadtree}} as hierarchical data structuring {{technique for}} representing spatial data (like points, regions, surfaces, lines, curves, volumes, etc.) has been motivated {{to a large}} extent by storage requirements of images, maps, and other multidimensional (spatially structured) data. For many spatial algorithms, time-efficiency of quadtrees in terms of execution may be as important as their space-efficiency concerning storage conditions. Briefly, the quadtree is a class of hierarchical data structures which is based on the recursive partition of a square region into quadrants and sub-quadrants until a predefined limit. Beyond the wide applicability of quadtrees in image processing, spatial information analysis, and building digital databases (processes becoming ordinary for the astronomical community), there may be numerous further applications in astronomy. Some of these practicable applications based on quadtree representation of astronomical data are presented and suggested for further considerations. Examples are shown for use of point as well as region quadtrees. Statistics of different leaf and non-leaf nodes (homogeneous and heterogeneous sub-quadrants respectively) at different levels may provide useful information on spatial structure of astronomical data in question. By altering the principle guiding the decomposition process, different types of spatial data may be focused <b>on.</b> Finally, <b>a</b> <b>sampling</b> method <b>based</b> <b>on</b> quadtree representation of an image is proposed which may prove to be efficient in the elaboration of <b>sampling</b> strategy in <b>a</b> region where observations were carried out previously either with different resolution or/and in different bands...|$|R
40|$|The {{literature}} of ELT is perhaps overwhelmed by attempts to enhance learnersâ€™ writing {{through the application}} of different methodologies. One such methodology is critical discourse analysis which is founded upon stressing not only the decoding of the propositional meaning of a text but also its ideological assumptions. Accordingly, this study was an attempt to investigate the impact of critical discourse analysis-based (CDA) instruction on EFL learnersâ€™ writing complexity, accuracy, and fluency (CAF). To fulfill the purpose of this study, 60 female intermediate EFL learners were selected from among a total number of 100 through their performance <b>on</b> <b>a</b> piloted <b>sample</b> PET. <b>Based</b> <b>on</b> the results, the students were randomly assigned to a control and an experimental group with 30 participants in each. Both groups underwent the same amount of teaching time during 17 sessions which included a treatment of CDA instruction for the experimental group. A writing posttest was administered {{at the end of the}} instruction to both groups and their mean scores on the test were compared through a MANOVA. The results led to the rejection of the three null hypotheses, thereby demonstrating that the learners in the experimental group benefited significantly more than those in the control group in terms of improving their writing CAF. To this end, it is recommended that CDA instruction be incorporated more frequently in writing classes following of course adequate syllabus design and materials development...|$|R
40|$|Demand for Croatia as a {{nautical}} country constantly {{maintains the}} upward trend. The {{reason for this}} is the fact that Croatia is {{on the way to the}} EU integration as a future equal member state. This fact significantly contributes to Croatiaâ€™s openness to the generating market in both economical and in terms of tourism. Nautical tourism of Croatia is profitable which has been recognized by foreign entrepreneurs by directing their fleets to the Adriatic and developing successful and today frequently leading charter companies. The paper defines the charter activity and points out its characteristics. The purpose of the paper is to research, understand and set forth the attitudes of nautical tourists within the charter domain as an activity with the highest annual growth rate in nautical tourism. Methodology used in this paper was questionnaire based on three sections of questions. First section of questions refers to structure of charter companies, the second section of questions refers to the features of demand and season of 2009, and the third section of the questionnaire shows the expectations of charter companies for the season of 2010. Further on, the authors have additionally researched Slovenian charter companies and made the correlation of Slovenian and Croatian charter market. The research in this paper is descriptive, conducted <b>on</b> <b>a</b> one-time basis and <b>on</b> <b>an</b> intentionally selected <b>sample.</b> <b>Based</b> <b>on</b> the results, appropriate conclusions and attitudes have been made also incorporating specific views regarding the quality improvement of a nautical tourist product and charter offer...|$|R
40|$|For {{two or more}} classes (or types) of points, nearest {{neighbor}} contingency tables (NNCTs) are constructed using {{nearest neighbor}} (NN) frequencies and are used in testing spatial segregation of the classes. Pielouâ€™s test of independence, Dixonâ€™s cell-specific, class-specific, and overall tests are the tests based on NNCTs (i. e., they are NNCT-tests). These tests are designed and intended for use under the null pattern of random labeling (RL) of completely mapped data. However, {{it has been shown}} that Pielouâ€™s test is not appropriate for testing segregation against the RL pattern while Dixonâ€™s tests are. In this article, we compare Pielouâ€™s and Dixonâ€™s NNCT-tests; introduce the one-sided versions of Pielouâ€™s test; extend the use of NNCT-tests for testing complete spatial randomness (CSR) of points from two or more classes (which is called CSR independence, henceforth). We assess the finite sample performance of the tests by an extensive Monte Carlo simulation study and demonstrate that Dixonâ€™s tests are also appropriate for testing CSR independence; but Pielouâ€™s test and the corresponding one-sided versions are liberal for testing CSR independence or RL. Furthermore, we show that Pielouâ€™s tests are only appropriate when the NNCT is <b>based</b> <b>on</b> <b>a</b> random <b>sample</b> of (<b>base,</b> NN) pairs. We also prove the consistency of the tests under their appropriate null hypotheses. Moreover, we investigate the edge (or boundary) effects on the NNCT-tests and compare the buffer zone and toroidal edge correction methods for these tests. We illustrate the tests <b>on</b> <b>a</b> real life and an artificial data set...|$|R
40|$|Aims: We present new {{extraction}} {{and identification}} techniques for supernova (SN) spectra developed within the Supernova Legacy Survey (SNLS) collaboration. Methods: The new spectral extraction method takes {{full advantage of}} photometric information from the Canada-France-HawaÃ¯ telescope (CFHT) discovery and reference images by tracing the exact position of the supernova and the host signals on the spectrogram. When present, the host spatial profile is measured on deep multi-band reference images and is used to model the host contribution to the full (supernova + host) signal. The supernova is modelled as a Gaussian function of width equal to the seeing. A Ï‡ 2 minimisation provides the flux of each component in each pixel of the 2 D spectrogram. For a host-supernova separation greater than âª† 1 pixel, the two components are recovered separately {{and we do not}} use a spectral template in contrast to more standard analyses. This new procedure permits a clean extraction of the supernova separately from the host in about 70 % of the 3 rd year ESO/VLT spectra of the SNLS. A new supernova identification method is also proposed. It uses the SALT 2 spectrophotometric template to combine the photometric and spectral data. A galaxy template is allowed for spectra for which a separate extraction of the supernova and the host was not possible. Results: These new techniques have been tested against more standard extraction and identification procedures. They permit a secure type and redshift determination in about 80 % of cases. The present paper illustrates their performances <b>on</b> <b>a</b> few <b>sample</b> spectra. <b>Based</b> <b>on</b> observations obtained with FORS 1 at the Very Large Telescope on the Cerro Paranal, operated by the European Southern Observatory, Chile (ESO Large Programmes 171. A- 0486 and 176. A- 0589) ...|$|R
40|$|We {{motivate the}} {{importance}} of understanding the kinematics and dynamics of the Milky Way stellar halo both in unravelling the formation history and evolution of our host Galaxy and in the more general context of galaxy dynamics. We present a cleaned picture of the kinematics of the smooth component of the stellar halo: we develop a method to quantify the average distance error <b>on</b> <b>a</b> <b>sample</b> of stars <b>based</b> <b>on</b> the idea of Schoenrich et al. (2012), but adapted so that it uses velocity information only on average. We use this scheme to construct an analytic distance calibration for Blue Horizontal Branch (BHB) field halo stars in Sloan colours and demonstrate that our calibration is a) more accurate than the ones available and b) unbiased w. r. t. metallicity and colour. We measure {{the rotation of the}} smooth component of the stellar halo with a tool-set of four estimators that use either only the l. o. s. velocities or the full 3 D motion. From two samples of BHB stars from the Sloan Digital Sky Survey, we favour a non-rotating single halo. We critique conflicting results in the literature <b>based</b> on similar <b>samples</b> and trace back the disagreement (either in the sign of rotation or in the morphology of the halo) to sample contaminations and/or neglect account of the halo geometry. We propose a scheme that generalizes any isotropic spherical model to a model where the potential is axisymmetric and the distribution function {{is a function of the}} three actions. The idea is to approximate the Hamiltonian as a function of the actions with a library of quadratic fits to surfaces of constant energy in action space and then make explicit the dependence of the energy on the three actions in the ergodic distribution function. The transparency of the physics implied by the model we achieve, should make it possible to combine our spheroidal models to the f(J) -models of Binney (2010) for the disks and of Pontzen & Governato (2013) for the dark-matter halo, and obtain a complete actions-defined dynamical model of the Milky Way Galaxy. This thesis is not currently available in ORA...|$|R
40|$|The {{influence}} of several environmental and antropic parameters were evaluated on productivity, diversity and distribution {{patterns of the}} Gasteromycetes, in Parque de Natureza de Noudar (Alentejo, Portugal). The study area is dominate by holm oak stands with and without shrubs (montado) and areas of pastures and grasses. In a minor scale, there are some cork oak stands, shrub lands and stream margins vegetation. The experimental design consisted <b>on</b> <b>a</b> stratified <b>sampling</b> procedure, <b>based</b> in the total area and land use of each biotope and the maximum sampling effort. Therefore, 45 circular plots (250 mÂ² each) were distributed randomly and proportionally among the six biotopes present in the study area. All the plots were sampled monthly, from November of 2006 to April of 2007. During this period all the carpophores were identified, collected to evaluate the species productivity (number of carpophores) and some specimens were preserved for the reference collection (Herbarium of the University of Ã‰vora - UEVH). After the environmental characterization {{of the study area}} (climate, vegetation and soil), all data was analyzed using a Principal components analysis (PCA) followed by a hierarchical grouping of "clusters", using as aggregation method the "complete linkage. " The results suggest that the presence of this heterogenic group of fungi is partly conditioned by vegetation type and management practices. On the other hand, productivity and phenology {{are more likely to be}} influenced by climatic conditions, particularly precipitation. The holm oak biotopes present the highest biodiversity of this group of fungi, whereas areas dominated by shrubs reveal the highest productivity. Within the holm oak biotopes, the montado biotope registers the highest number of Gasteromycetes trophic groups (mycorrhizal, humicolous, lignicolous and coprophilous), whereas in the pastures and grasses and in the streams biotopes no presences were registered. Ten of the total species, namely Bovista delicata, B. dermoxantha, Crucibulum laeve, Cyathus olla, C. stercoreus, Geastrum elegans, Handkea excipuliformis, Ileodictyon gracile, Lycoperdon atropurpureum and Vascellum pratense, are reported for the first time to the province of Baixo Alentejo. The species potential distribution maps may constitute a valuable tool for identifying and preserve areas of special mycological interest on the Parque de Natureza de Noudar...|$|R
50|$|This list is <b>a</b> <b>sample</b> <b>based</b> <b>on</b> {{the highest}} ratings at Gay Erotic Video Index.|$|R
40|$|This thesis {{contributes}} {{to the field of}} applied statistics and financial modeling by analyzing mathematical models used in retail credit underwriting processes. Specifically, it has three goals. First, the thesis aims to challenge the performance criteria used by established statistical approaches and propose focusing on predictive power instead. Secondly, it compares the analytical leverage of the established and other suggested methods according to the newly proposed criteria. Third, the thesis seeks to develop and specify a new comprehensive profitability-based underwriting model and critically reflect on its strengths and weaknesses. In the first chapter I look into the area of probability of default modeling and argue for comparing the predictive power of the models in time rather than focusing on the random testing sample only, as typically suggested in the scholarly literature. For this purpose I use the concept of survival analysis and the Cox model in particular, and apply it to a real Czech banking data sample alongside the commonly used logistic regression model to compare the results using the Gini coefficient and lift characteristics. The Cox model performs comparably on the randomly chosen validation sample and clearly outperforms the logistic regression approach in the predictive power. In the second chapter, in the area of loss given default modeling I introduce two Cox-based models, and compare their predictive power with the standard approaches using the linear and logistic regression <b>on</b> <b>a</b> real data <b>sample.</b> <b>Based</b> <b>on</b> the modified coefficient of determination, the Cox model shows better predictions. Third chapter focuses on estimating the expected profit {{as an alternative to the}} risk estimation itself and building on the probability of default and loss given default models, I construct a comprehensive profitability model for fix-term retail loans underwriting. The model also incorporates various related risk-adjusted revenues and costs, allowing more precise results. Moreover, I propose four measures of profitability, including the risk-adjusted expected internal rate of return and return on equity and simulate the impact of the model on each of the measures. Finally, I discuss some weaknesses of these approaches and solve the problem of finding default or fraud concentrations in the portfolio. For this purpose, I introduce a new statistical measure based <b>on</b> <b>a</b> pre-defined expert critical default rate and compare the GUHA method with the classification tree method <b>on</b> <b>a</b> real data <b>sample.</b> While drawing on the comparison of different methods, this work {{contributes to}} the debates about survival analysis models used in financial modeling and profitability models used in credit underwriting...|$|R
