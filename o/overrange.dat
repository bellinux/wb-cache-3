13|8|Public
50|$|Control {{grouping}} poses difficult problems, {{especially in}} systems where a given operating parameter {{may be affected}} by multiple control groups. For example, in a stereophonic multiway sound system, the gain of the left-channel high-frequency amplifier may be affected by settings of master controls for (a) overall high-frequency level, (b) left-channel level, and (c) overall level of the entire system. In such systems, machine intelligence is required to manage cumulative settings effects that lead to <b>overrange</b> or underrange parameter values. The AES70 grouping mechanism provides a basis for such management, for one or many devices.|$|E
40|$|This thesis {{describes}} {{the feasibility of}} an analog-to-digital converter (ADC) with a sample-rate of 1 - 2 GS/s, a resolution of 8 - 10 bits, and a state-of-the-art power efficiency of less than 1 pJ/conversion step. The time-interleaved architecture exploits parallelism to increase the sample-rate while maintaining good power efficiency, and therefore {{it is the most}} suitable architecture. Chapter 2 {{describes the}} Track and Hold (T&H) architecture for such a time-interleaved ADC, and the importance of channel matching is discussed. The use of a frontend sampler has the advantage of good timing alignment between channels, it limits however the input bandwidth and the achievable resolution. By placing a switch between the T&H buffer and its capacitive load, a buffer with a limited bandwidth can be used to save power, without introducing distortion. Chapter 3 discusses the architecture of the sub-ADCs, which are used in the time-interleaved ADC. A Successive Approximation ADC (SA-ADC) can have a very good power efficiency, its sample-rate is however limited. To increase the sample-rate, <b>overrange</b> techniques can be used to reduce the required DAC settling time. A new <b>overrange</b> technique is presented called the single-sided <b>overrange</b> technique. Compared to a conventional 6 bits SA-ADC, it saves 58...|$|E
30|$|Since the DLP {{was taken}} from the scanner dose report, <b>overrange</b> was taken into consideration. We studied local {{protocols}} for given clinical indications. In these local protocols, the kV could be variable per scan phase. Tube current modulation techniques were employed as defined in the local scan protocols.|$|E
40|$|<b>Overranging</b> or {{overscanning}} {{increases the}} dose delivered to patients undergoing helical Computed Tomography examinations. In {{order to reduce}} it, nowadays most of the multidetector tomographs close the X-ray beam aperture at the scan extremes. This technical innovation, usually referred to as dynamic or adaptive collimation, also influences the <b>overranging</b> assessment methods. In particular, the film free approach proposed in previous studies is not suitable for these modern tomographs. The present study aims to introduce a new method of estimating <b>overranging</b> with real time dosimetry, even suitable for tomographs equipped with adaptive collimation. The approach proposed {{is very easy to}} implement and time saving because only a pencil chamber is required. It is also equivalent in precision and in accuracy to the film based one, considered an absolute benchmark. Comment: 5 figures, 3 table...|$|R
30|$|The {{similarity}} {{in the data}} of this survey compared to its predecessor [5] indicate, however, that during that period the more liberal use of CT and expansion of CT indications have been balanced with newer technology for dose optimisation such as volumetric tube current modulation and interactive collimation to reduce <b>overranging.</b>|$|R
30|$|For {{clinical}} indications {{in which}} contrast media timing is not critical (lymphoma staging), hospitals {{have decided to}} split long image ranges into two shorter helical acquisitions. The dose penalty of this practice is limited but variable (Table  2): For imaging of the neck, thorax, and abdomen, the DLP is 1, 104  mGy cm when the neck and thorax-abdomen are scanned separately versus 968  mGy cm for the neck-thorax-abdomen scanned in one run. In contrast, for imaging of the thorax and abdomen, the difference is only 790  mGy cm versus 769  mGy cm. Most differences lie in differences in the extra dose from <b>overranging</b> [12, 13]. In contrast, most of the dose increase is seen in imaging of the thorax and liver CT for lung carcinoma staging, with 557  mGy cm for the lung and liver scanned separately versus 383  mGy cm for the thorax-liver in one run. This {{is due to the}} fact that for the thorax a lower technique can be chosen.|$|R
40|$|To {{reconstruct}} {{the first and}} last sections of a helical computed tomographic (CT) scan, the scan length is automatically extended beyond the planned image boundaries, a phenomenon known as overranging. With common 16 -section CT scanning protocols, the <b>overrange</b> length is between 3 and 6 cm. For scanners with 64 or more sections, this length will be much greater, since overranging increases as pitch or detector collimation increases. Manufacturers have equipped the latest generation of CT scanners (128 sections or more) with <b>overrange</b> dose-reducing innovations that reduce overranging by typically up to 50 %, which in the best cases reduces overranging to that of the previous scanner models (64 sections). To reduce the impact of overranging on radiosensitive organs just outside the planned scan region, it is best to use an axial protocol rather than a helical protocol. If this is not an option, lowering the pitch or the detector collimation will significantly reduce overranging. Finally, CT examinations should be planned {{in such a way that}} radiosensitive organs are as far as possible from the imaged volume...|$|E
40|$|No {{pipeline}} delays with SAR ADC Excellent dc accuracy performance 2 {{parallel interface}} modes Low power: 90 mW (full power) and 2. 5 mW (nap mode) Standby mode: 2 µA maximum Single 5 V supply operation Internal 2. 5 V reference Full-scale <b>overrange</b> mode (using 13 th bit) System offset removal via user access offset register Nominal 0 V to 2. 5 V input with shifted range capability 14 -bit pin compatible upgrade AD 7484 available REFSE...|$|E
40|$|A time-interleaved ADC is {{presented}} with 16 channels, each {{consisting of a}} track-and-hold (T&H) and two successive approximation (SA) ADCs in a pipeline configuration to combine a high sample rate with good power efficiency. The single-sided <b>overrange</b> architecture achieves a 25 % higher power efficiency of the SA-ADC compared with the conventional <b>overrange</b> architecture, and look-ahead logic is used to minimize logic delay in the SA-ADC. For the T&H, three techniques are presented enabling a high bandwidth and linearity and good timing alignment. Single channel performance of the ADC is 6. 9 ENOB at an input frequency of 4 GHz. Multichannel performance is 7. 7 ENOB at 1. 35 GS/s with an ERBW of 1 GHz. The FoM of the complete ADC including T&H is 0. 6 pJ per conversion step. An improved version {{is presented}} as well and achieves an SNDR of 8. 6 ENOB for low sample rates, and, with increased supply voltage, it reaches a sample rate of 1. 8 GS/s with 7. 9 ENOB at low input frequencies and an ERBW of 1 GHz. At fin = 3. 6 GHz, the SNDR is still 6. 5 ENOB, and total timing error including jitter is 0. 4 ps rms...|$|E
40|$|The TEMAG (Tether Magnetometer) {{on board}} the Shuttle TSS-I {{satellite}} was devoted to magnetic field measurements. During the Shuttle flight in early August 92 the tethered Satellite was partially deployed up {{to a maximum of}} 256 m only from the Shuttle and a orbited for about 30 h. Therefore the induced voltage was only a few 10 V. The TEMAG experiment was switched on during the whole 30 h period measuring the flight dynamics (the attitude control of the s/c was partially <b>overranged</b> due to strong dynamic oscillations). The attitude data from NASA/ASI are still pending in order to evaluate the magnetic field data, especially to investigate the equatorial electrojet and possible induced fields in the earth's crust. The evaluation is being continued, the experiment will be recalibrated in December 93, January 94, a reflight is discussed for 1996. (orig.) Available from TIB Hannover: DtF QN 1 (23, 34) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEBundesministerium fuer Forschung und Technologie (BMFT), Bonn (Germany); Deutsche Agentur fuer Raumfahrtangelegenheiten (DARA) GmbH, Bonn (Germany) DEGerman...|$|R
40|$|The Thesis submits the {{dilemmas}} of nursing competencies in critical care. In the theoretical Section the author concentrates first on nursing competencies given by legal regulation, secondly on education system, and lastly on gaining the nurse qualification. The author mapped various elements influencing {{the scope of}} powers of nurses, more precisely the regulation of their profession beginning at legal liability of medical staff and ending on their code of ethics. The author describes interpersonal relations at the workplace with focus on the relations between nurses and doctors. The last few chapters of the theoretical Section relate to some concrete nursing inverventions - how they are provided and on their stipulation in law. Practical Section hereof is focused on analyses of research regarding the above mentioned topic. The author used the quantitative method of research and structured questionnaire collecting data. Practical Section hereof aims to identify the grounds for the breach of or the compliance with the competencies of nurses in critical care. This Section finds the possible purposes why the competences are <b>overranged</b> {{from the point of}} view both of nurses and doctors...|$|R
40|$|Abstract A rangesum query to {{an array}} A {{is a pair}} (`, r) of range endpoints, which should be {{answered}} by P`<=i<r A[i]. To compress A, we consider representing an array A loss-ily by a histogram, a function that is constant on each of {{a small number of}} buckets. We then answer range queries from H instead of from A, i. e., as P`<=i<r H[i]. An optimal rangesum histogram H for this purpose isone whose bucket boundaries and constant heights within buckets are chosen to minimize the expected square error, E`,r ^iP`<=i<r A[i]- P`<=i<r H[i]j 2 *, assuming each rangesum query is equally likely. Rangesum histograms findmany applications in database systems. In a degenerate variation, all rangesum queries are <b>overranges</b> of size one, namely, individual points; histograms optimal for this special case are called pointwise optimal his-tograms. Pointwise optimal histogram is a classical notion in statistics and approximation theory, but rangesum opti-mal histogram appears to be novel in these areas. While optimal pointwise histograms can be constructed efficientlyby simple dynamic programming, no efficient (even approximate) general rangesum histogram construction algorithmswere previously known. In practice, all commercial database systems use heuristically built histograms for pointwise andrangesum queries...|$|R
40|$|Silicon {{micromachining}} etching {{techniques were}} utilized to batch-fabricate hundreds of general purpose microaccelerometers {{on a single}} silicon substrate. Piezoresistive sensing elements were aligned to the back-side patterns using an IR mask aligner and then diffused into the areas of maximum stress. Capping of the two-arm cantilever beam structure was achieved {{using a combination of}} electrostatic bonding and low temperature glass films. <b>Overrange</b> protection, critical damping, and overall protection from the outside environment are achieved by controlling the cavity depths of the top and bottom covers. Temperature compensation, amplification, and filtering are performed by a companion LSI chip that is interfaced to the accelerometer by conventional wire-bonding techniques...|$|E
40|$|International audienceThe {{localization}} of {{an acoustic}} {{source in the}} oceanicwaveguide is a difficult task because the oceanic environmentis often poorly known. Uncertainty in the environment resultsin uncertainty in the source position and poor localizationresults. Hence, localization methods dealing with environmentaluncertainty are required. In this paper, a Bayesian approach tosource localization is introduced {{in order to improve}} robustnessand obtain quantitative measures of localization uncertainty. The Green’s function of the waveguide is considered as anuncertain random variable whose probability density accountsfor environmental uncertainty. The uncertain distribution <b>overrange</b> and depth is then obtained through the integration of theposterior probability density (PPD) over the Green’s functionprobability density. An efficient integration technique makes thewhole localization process computationally efficient. Some resultsare presented for a simple uncertain Green’s function model toshow the ability of the proposed method to give reliable PPDs...|$|E
40|$|Macroalgae biomass {{has been}} {{considered}} as a promising feedstock for biogas production. In order to improve the efficiency of anaerobic digestion (AD) of macroalgae, semi-continuous fermentation was conducted {{to examine the effects}} of organic loading rate (OLR) on biogas production from Macrocystis pyrifer. Results showed that, under OLRs of 1. 37, 2. 74, 4. 12 and 6. 85 kg VSsubstrate/(m(3). d), the average unit biogas yields were 438. 9, 477. 3, 480. 1 and 188. 7 mL/(g VSsubstrate d), respectively. It indicated that biogas production was promoted by the increased OLR in an appropriate range while inhibited by the OLR beyond the appropriate range. The investigation on physical-chemical parameters revealed that unfavorable VFAs concentration, pH and salinity might be the main causes for system failure due to the <b>overrange</b> OLR, while the total phenols failed to reach the inhibitory concentration. Microbial community analysis demonstrated that several bacterial and archaeal phyla altered with increase in OLR apparently. (C) 2017 Elsevier Ltd. All rights reserved...|$|E
40|$|The {{purpose of}} the present study was to explore the {{correlation}} between the contrast-enhanced ultrasound (CEUS) and acoustic radiation force impulse (ARFI) characteristics of breast cancers and the expression of human epidermal growth factor receptor 2 (HER- 2). HER- 2 expression levels in the tumor masses of 167 clearly diagnosed cases of breast cancer were measured and analyzed. The enhancement features and time intensity curve (TIC) of CEUS and virtual touch tissue imaging (VTI) and virtual touch tissue quantification (VTQ) technology of ARFI were employed to analyze the relationship between HER- 2 expression and the CEUS and ARFI characteristics of breast cancer. (1) Statistically significant differences in the distribution of the contrast agent, perforator blood flow, the <b>overranging</b> phenomenon and perfusion defects between the study groups with different HER- 2 expression levels (P 0. 05). (2) Statistically significant differences in the VTQ results between the groups with different HER- 2 expression levels were found (P 0. 05). A correlation was found between the CEUS and ARFI characteristics of breast cancer and HER- 2 expression levels. This correlation was principally reflected in perfusion defects, perforator blood flow, PI, PT, K and VTQ...|$|R
40|$|Noninvasive {{coronary}} angiography with multislice computed tomography (CT) scanners is feasible with high sensitivity and negative predictive value. The radiation exposure {{associated with this}} technique, however, is high and concerns in {{the widespread use of}} CT have arisen. We evaluated the diagnostic accuracy of {{coronary angiography}} using 320 -row CT, which avoids exposure-intensive overscanning and <b>overranging.</b> We prospectively studied 118 unselected consecutive patients with suspected coronary artery disease (CAD) referred for invasive coronary angiography (ICA). All patients had 320 -row CT within 1 week of ICA, which, together with quantitative analysis, served as the reference standard. Of the 65 out of 118 patients who were diagnosed as having CAD by ICA, 64 (98 %) were correctly identified at 320 -row CT. Noteworthy, 320 -row CT correctly detected CAD in 3 patients with atrial fibrillation and ruled out the disease in the other 8 patients. From 151 significant coronary stenoses detected on ICA, 137 (91 %) were correctly identified with 320 -row CT. In the per-patient analysis, sensitivity and specificity of 320 -row CT were 98 and 91 %, respectively. In the per-vessel analysis, sensitivity and specificity of 320 -row CT were 93 and 95 %, respectively. In the per segment analysis, sensitivity and specificity of 320 -row CT were 91 and 99 %, respectively. Diameter stenosis determined with the use of CT showed good correlation with ICA (P < 0. 001, R = 0. 81) without significant underestimation or overestimation (− 3. 1 ± 24. 4 %; P = 0. 08). Comparison of CT with ICA revealed a significantly smaller effective radiation dose (3. 1 ± 2. 3 vs. 6. 5 ± 4. 2 mSv; P < 0. 05) and amount of contrast agent required (99 ± 51 vs. 65 ± 42 ml, P < 0. 05) for 320 row CT. The present study in an unselected population including patients with atrial fibrillation demonstrates that 320 -row CT may significantly reduce the radiation dose and amount of contrast agent required compared with ICA while maintaining a very high diagnostic accuracy...|$|R
40|$|Eberline's new microoomputer-based {{radiation}} survey instrument, Model ESP-I, {{is designed to}} correct for coincidence loss and thus extend the range of each detector probe. It provides an "OVERRAN(G " display when the ooincidence correction factor emceeds 5 or when the count rate exceeds 2. 5 X 106 cpm. With Eberline's HP- 290 probe, this indication should occur at about 80 R/h, and it is triggered when the count rate from the GM tube exceeds 2. 5 X 106 cpF. We have discovered {{that some of the}} (24 tubes which can be used in the HP- 290 probe may not reach 2. 5 X 106 cpm. In such cases, the detector probe can be in a radiation field considerably above 80 R/h and still provide a reading below 80 R/h. Replacement tubes that have not been selected in accordance with Eberline's <b>overrange</b> criterion my create this problem even if the original GM tube functioned properly. We recommend that you take {{one or more of the}} following precautions if you are using a HP- 290 probe with an ESP-l...|$|E
40|$|Full-featured {{evaluation}} {{board for}} the AD 5420 On-board reference Link options Direct hook-up to USB port of PC PC software for control EVALUATION BOARD DESCRIPTION The EVAL-AD 5420 is a full-featured evaluation board, designed to allow the user to easily evaluate all features of the AD 5420 current source, 16 -bit DAC. All of the AD 5420 pins are accessible at on-board connectors for external connection. The board can be controlled by two means, via the on-board connector (J 8) or via the USB port of a Windows ® 2000, NT®, XP®-based PC using the AD 5420 evaluation software. The default setup is for control via the USB port. DEVICE DESCRIPTION The AD 5420 is a low cost, precision, fully integrated 16 -bit converter, offering a programmable current source output {{designed to meet the}} requirements of industrial process control applications. The output current range is programmable to 4 mA to 20 mA, 0 mA to 20 mA, or an <b>overrange</b> function of 0 mA to 24 mA. The output is open-circuit protected. The device is specified to operate with a power supply range from 10. 8 V to 40 V. Output loop compliance is 0 V to AVDD – 2. 5 V. Complete specifications for the AD 5420 are available in the AD 5420 data sheet available from Analog Devices, Inc., and should be consulted in conjunction with this data sheet when using the evaluation board...|$|E
40|$|DESKToe cALCULAToRS is {{provided}} by a new, self-contained, rack-mounted unit. By adding 17 times the programmable memory to the Model 9100 A, and 8 times the programmable memory to the Model 91008, this Model 910 lA Extended Memory, Fig. 1, geatly extends the range of their problem solving abilities. The calculators are still the controlling and calculating elements of the system; the extended memory is operated by Format (FMT) commands from the calculators. It has a 20, 832 bit memory capable of storing up to 3472 program steps, or 248 fourteen-digit registers for data storage. When used with other peripherals, the Model 91014 becomes the main data and program storage unit. With appropriate interface cards in the Model 25704 Coupler, 1 the calculators are able to feed data to, or accept inputs from teleprinters and other equipment. To simplify programming, register addressing and program selection are in decimal numbers rather than binary, octal or hexadecimal systems. The programmer need not be concerned with internal addressing. He needs only to remember corresponding program numbers and their functions. Address information important to the programmer is displayed when programs are transferred. Diagnostic codes are shown whenever there is a program error related to the 9101 A operation. Basic Features The HP 9101, { extended memory is divided into 248 fourteen-digit registers (ten digits displayed, two digits <b>overrange,</b> and two digits exponent). The registers of the 91014 ' are numbered decimally from 0 through 247, and are always addressed from the 9100 A/B X-register. Programs stored in the 9 101 A are transferred from the 9100,{/ 8, starting at location 00 in the 9100, { and f 00 in the 91008; and stopping when an END statement is encountered in the program. To identify the program, a two-digit number (decimal number 00 through 99) {{is placed in the}} 9100 A/B X-register prior to the transfer. To assisthe programmer in making maximum use of the 91014 storage, any new program added in storage is stored starting with the first available register, incrementing registers sequentially (14 program steps per register as in the 9100 A/B) until the END statement is reached. Cover: Marked cards, lower right, are used to enter programs and data in the Mode...|$|E
40|$|This book {{describes}} the research {{carried out by}} our PhD student Simon Louwsma at the University of Twente, The Netherlands {{in the field of}} high-speed Analogto- Digital (AD) converters. AD converters are crucial circuits for modern systems where information is stored or processed in digital form. Due to increasing data rates and further digitization of systems, the demands on the AD converters are increasing in both sample-rate and number of bits. A fast and accurate AD converter combined with digital signal processing offers an attractive alternative for the analog signal chain still present in many actual receivers. This book offers an exploration of fundamental and practical limits of high speed AD conversion, aiming at a step forward in number of bits and sample-rate, while keeping the power consumption low. To achieve high performance, a technique called time interleaving is used. Time interleaving is the analog equivalent of parallel processing in the digital domain. To implement this, instead of a single Track-and- Hold (T&H), we use a whole series of them, each sampling a bit later than the previous one. In the design example in this book we use 16 T&H circuits, followed by 16 sub-AD converters. The timing alignment of these T&H circuits needs to be extremely accurate, and conventionally, complex timing calibration is used to achieve this. Here however, it is shown that even better performance can be achieved by a compact and good design of the timing circuit without requiring any timing calibration. The circuits use a minimum of transistors that cause timing inaccuracies and special layout techniques are the finishing touch. Thanks to the absence of a control range for the timing, the amount of jitter is also reduced. To save power and to keep the input capacitance low, small sized transistors are used in the time-interleaving T&H circuitry. Only simple DC calibrations are needed to make the 16 paths behave equally over the whole input frequency range. An extensive analysis of accuracy and timing requirements is given and circuit solutions are described in detail. After the input signal is sampled by a T&H section, a sub-ADC finalizes the conversion. Pipeline AD converters are popular for conversion rates around 100 MS/s, but they suffer from the fact that even in the first stage of the pipeline the full accuracy for settling is required. This makes the design of high speed in combination with a high accuracy quite a challenge. Instead of that, we use sub-ADCs based on Successive Approximation (SA). As explained in this book, this has quite some advantages: A SAR ADC contains less critical analog blocks, and its power consumption can be ten times less than a comparable pipeline ADC. A potential disadvantage of Successive Approximation converters is the relatively low maximum sample-rate. This problem is tackled with a new <b>overrange</b> technique that greatly reduces the demands on settling time per conversion step and that postpones the critical decision to the last conversion step. This offers great advantage over a Pipeline ADC, where the first residue amplifier must settle to full accuracy to avoid unrecoverable analog errors in the conversion process. The work described in this book shows state-of-the art performance and describes techniques, which gain popularity among today’s AD converter designers. We enjoyed carrying out the research with Simon and we hope you will enjoy reading the results...|$|E

