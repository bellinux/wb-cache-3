5108|10000|Public
5|$|Following Murray's strong end to 2014 and {{reaching}} {{the final of}} the 2015 Australian Open, he moved into the top four in the ATP rankings {{for the first time in}} over year, meaning that the Big Four held the top four places in the rankings for the first time since early 2013, slowing the idea <b>of</b> <b>the</b> <b>regression</b> of the quartet. However, Federer and Nadal both lost in the third round and in the quarterfinals respectively, the first time in 12 appearances that Federer had lost before the semi-finals at the first Grand Slam of the season. Djokovic won the title, as well as the first three Masters titles of the year in Indian Wells (where the Big Four were the top four seeds in ATP Masters 1000 event for the time since 2012), Miami and Monte-Carlo, defeating Murray in the final in Miami (his seventh straight victory against the Brit) and Federer in the final of Indian Wells respectively. With these victories, Djokovic became the only man to win the Indian Wells-Miami sweep on three separate occasions.|$|E
25|$|When this {{assumption}} is violated the regressors are called linearly dependent or perfectly multicollinear. In such case the value <b>of</b> <b>the</b> <b>regression</b> coefficient β cannot be learned, although prediction of y values {{is still possible}} for new values of the regressors that lie in the same linearly dependent subspace.|$|E
25|$|R-squared is the {{coefficient}} of determination indicating goodness-of-fit <b>of</b> <b>the</b> <b>regression.</b> This statistic will be equal to one if fit is perfect, and to zero when regressors X have no explanatory power whatsoever. This is a biased estimate of the population R-squared, and will never decrease if additional regressors are added, {{even if they are}} irrelevant.|$|E
30|$|In general, {{statistical}} significance <b>of</b> <b>the</b> <b>regressions</b> <b>of</b> females is {{low compared to}} that <b>of</b> males, but <b>the</b> signs coincide.|$|R
30|$|For the sectors {{for which}} no cointegration {{relationship}} is found, we report <b>the</b> results <b>of</b> <b>the</b> <b>regressions</b> <b>of</b> <b>the</b> first differences <b>of</b> <b>the</b> variables <b>of</b> interest.|$|R
30|$|Obtain <b>the</b> <b>regression</b> {{value as}} <b>the</b> average <b>of</b> <b>the</b> N <b>regression</b> values from N trees.|$|R
25|$|While the δD of {{source water}} is the biggest {{influence}} on the δD of lipids, discrepancies between fractionation factor values obtained from the slope and from the intercept <b>of</b> <b>the</b> <b>regression</b> suggest that the relationship {{is more complex than}} a two-pool fractionation. In other words, there are multiple fractionation steps that {{must be taken into account}} in understanding the isotopic composition of lipids.|$|E
25|$|Another {{variation}} <b>of</b> <b>the</b> <b>regression</b> {{hypothesis is}} the best learned-last-forgotten hypothesis, which emphasizes the intensity {{and quality of the}} acquired knowledge, not the order in which it is learned. Therefore, the better something is learned, the longer it will remain. Because the language component is repeated again and again, it becomes automated and increases the probability that it will last in the memory (Schöpper-Grabe 1998: 241).|$|E
25|$|The numerator, n−p, is the {{statistical}} degrees of freedom. The first quantity, s2, is the OLS estimate for σ2, whereas the second, , is the MLE estimate for σ2. The two estimators are quite similar in large samples; {{the first one}} is always unbiased, while the second is biased but minimizes the mean squared error of the estimator. In practice s2 is used more often, since it is more convenient for the hypothesis testing. The square root of s2 is called the standard error <b>of</b> <b>the</b> <b>regression</b> (SER), or standard error of the equation (SEE).|$|E
40|$|This study {{examines}} <b>the</b> relative importance <b>of</b> {{political and economic}} variables in <b>the</b> determination <b>of</b> a country’s standing in credit ratings provided by commercial rating agencies. It finds that creditworthiness appears to be determined primarily by economic variables. While including political events can improve <b>the</b> explanatory power <b>of</b> <b>the</b> <b>regressions,</b> <b>the</b> exclusion <b>of</b> political variables does not bias the parameter estimates for <b>the</b> effects <b>of</b> economic variables. ...|$|R
40|$|In {{the paper}} <b>the</b> median {{estimator}} <b>of</b> <b>the</b> logistic <b>regression</b> parameters employing smoothed {{data in the}} discrete case is considered. Sensitivity of this estimator to contaminations <b>of</b> <b>the</b> logistic <b>regression</b> data is studied by simulations and compared with <b>the</b> sensitivity <b>of</b> some robust estimators previously introduced to logistic regression...|$|R
40|$|AbstractUsing a few {{very basic}} observations, we take full {{advantage}} <b>of</b> <b>the</b> special structure <b>of</b> <b>the</b> l∞ <b>regression</b> problem to obtain a direct and finite algorithm for <b>the</b> computation <b>of</b> <b>the</b> l∞ <b>regression</b> line. <b>The</b> complexity <b>of</b> <b>the</b> algorithm and the computational results showing <b>the</b> effectiveness <b>of</b> <b>the</b> algorithm will also be presented...|$|R
25|$|As {{an attempt}} to correct some of the error due to a non-zero , the usage of local linear {{weighted}} regression with ABC to reduce the variance of the posterior estimates has been suggested. The method assigns weights to the parameters according to how well simulated summaries adhere to the observed ones and performs linear regression between the summaries and the weighted parameters {{in the vicinity of}} observed summaries. The obtained regression coefficients are used to correct sampled parameters in the direction of observed summaries. An improvement was suggested in the form of nonlinear regression using a feed-forward neural network model. However, {{it has been shown that}} the posterior distributions obtained with these approaches are not always consistent with the prior distribution, which did lead to a reformulation <b>of</b> <b>the</b> <b>regression</b> adjustment that respects the prior distribution.|$|E
500|$|There {{is clear}} {{evidence}} that sea levels fell in {{the final stage of}} the Cretaceous by more {{than at any other time}} in the Mesozoic era. In some Maastrichtian stage rock layers from various parts of the world, the later layers are terrestrial; earlier layers represent shorelines and the earliest layers represent seabeds. These layers do not show the tilting and distortion associated with mountain building, therefore, the likeliest explanation is a [...] "regression", that is, a drop in sea level. There is no direct evidence for the cause <b>of</b> <b>the</b> <b>regression,</b> but the explanation currently accepted as most likely, is that the mid-ocean ridges became less active and therefore sank under their own weight.|$|E
2500|$|... <b>of</b> <b>the</b> <b>regression</b> {{coefficient}} of the lagged dependent variable, provided ...|$|E
5000|$|<b>The</b> center <b>of</b> <b>the</b> inverse <b>regression</b> {{curve is}} located at [...] Therefore, <b>the</b> {{centered}} inverse <b>regression</b> curve is ...|$|R
5000|$|... #Caption: Visualization <b>of</b> <b>the</b> Quantile <b>Regression</b> Averaging (QRA) {{probabilistic}} forecasting technique.|$|R
40|$|SUMMARY Regression {{problems}} {{with a number of}} related response variables are typically analyzed by separate multiple regressions. This paper shows how these regressions can be visualized jointly in a biplot based on reduced-rank regression. Reduced-rank regression combines multiple regression and principal components analysis and can therefore be carried out with standard statistical packages. The proposed biplot highlights <b>the</b> major aspects <b>of</b> <b>the</b> <b>regressions</b> by displaying <b>the</b> least-squares approximation <b>of</b> fitted values, regression coefficients and associated t-ratio's. The utility and interpretation <b>of</b> <b>the</b> reduced-rank <b>regression</b> biplot is demonstrated with an example using public health data that were previously analyzed by separate multiple regressions...|$|R
2500|$|Galton {{invented the}} use <b>of</b> <b>the</b> <b>regression</b> line [...] {{and for the}} choice of r (for {{reversion}} or regression) to represent the correlation coefficient.|$|E
2500|$|If {{the design}} matrix [...] <b>of</b> <b>the</b> <b>regression</b> is known, exact {{critical}} {{values for the}} distribution of [...] under the null hypothesis of no serial correlation can be calculated. Under the null hypothesis [...] is distributed as ...|$|E
2500|$|Adjusted R-squared is a {{slightly}} {{modified version of}} , designed to penalize for the excess number of regressors which do not add to the explanatory power <b>of</b> <b>the</b> <b>regression.</b> This statistic is always smaller than , can decrease as new regressors are added, and even be negative for poorly fitting models: ...|$|E
50|$|<b>The</b> value <b>of</b> σg determines <b>the</b> slope <b>of</b> <b>the</b> least-squares <b>regression</b> curve.|$|R
40|$|The {{asymptotic}} variance matrix <b>of</b> <b>the</b> quantile <b>regression</b> estimator {{depends on}} <b>the</b> density <b>of</b> <b>the</b> error. For both deterministic and random regressors, the bootstrap distribution {{is shown to}} converge weakly to <b>the</b> limit distribution <b>of</b> <b>the</b> quantile <b>regression</b> estimator in probability. Thus, the confidence intervals constructed by the bootstrap percentile method have asymptotically correct coverage probabilities. ...|$|R
50|$|A Method <b>of</b> Determining <b>the</b> <b>Regression</b> Curve When <b>the</b> Marginal Distribution is <b>of</b> <b>the</b> Normal Logarithmic Type, Annals of Mathematical Statistics, 7:196-201, 1936.|$|R
2500|$|Although serial {{correlation}} {{does not affect}} the consistency of [...] the estimated regression coefficients, it does affect our ability to conduct valid statistical tests. First, the F-statistic to test for overall significance <b>of</b> <b>the</b> <b>regression</b> may be inflated under positive {{serial correlation}} because the mean squared error (MSE) will tend to underestimate the population error variance. Second, positive serial correlation typically causes the ordinary least squares (OLS) standard errors for the regression coefficients to underestimate the true standard errors. As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small standard errors for the regression coefficient. These small standard errors will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters <b>of</b> <b>the</b> <b>regression</b> model more often than we would if the standard errors were correctly estimated.|$|E
2500|$|Given {{the success}} of ROC curves for the {{assessment}} of classification models, the extension of ROC curves for other supervised tasks has also been investigated. Notable proposals for regression problems are the so-called regression error characteristic (REC) Curves [...] and the Regression ROC (RROC) curves. In the latter, RROC curves become extremely similar to ROC curves for classification, with the notions of asymmetry, dominance and convex hull. Also, the area under RROC curves is proportional to the error variance <b>of</b> <b>the</b> <b>regression</b> model.|$|E
2500|$|If the {{relationship}} is estimated with nonparametric regression then it produces {{a version of the}} curve which has a [...] "hinge"– i.e. a kink in {{the relationship}} where the slope <b>of</b> <b>the</b> <b>regression</b> equation falls off significantly. This point occurs around the per capita income level of $2,045 (data for the year 2000) which is about the per capita income level of India. This level of income is generally associated with a crossing of a [...] "epidemiological transition", where countries change from having most of their mortality occur due to infant mortality to that due to old age mortality, and from prevalence of infectious diseases to that of chronic diseases.|$|E
30|$|Several {{variants}} <b>of</b> <b>the</b> <b>regressions</b> were tested, many {{of these}} were rejected because of containing variables were there were logical problems with causality or were variables essentially were describing the same phenomena. The simpler model {{we ended up with}} contains systems variables that typically could be linked to the RIS framework. We note that the ‘RIS variable’ ‘number of knowledge development institutions’ not is significant when we remove the five largest city regions from <b>the</b> <b>regression,</b> but that <b>the</b> ‘innovation infrastructure’ is even when we remove the five largest city regions from <b>the</b> <b>regression.</b>|$|R
40|$|ADF, ADFRESID, DF, DFRESID] = UNITROOT (SERIES) tests <b>the</b> null {{hypothesis}} <b>of</b> <b>the</b> existence <b>of</b> a unit root in SERIES and returns matrix ADF {{with the results}} for <b>the</b> Augmented Dickey-Fuller <b>regression</b> with <b>the</b> highest number <b>of</b> augmented terms (dlags), if any, significant at the 10 % level, vector ADFRESID with <b>the</b> residuals <b>of</b> <b>the</b> ADF <b>regression,</b> matrix DF with the results for <b>the</b> Dickey-Fuller <b>regression,</b> and vector DFRESID with <b>the</b> residuals <b>of</b> <b>the</b> DF <b>regression.</b> ...|$|R
40|$|Most {{automated}} essay scoring programs use {{a linear}} regression model to predict an essay score from several essay features. This article applied a cumulative logit model instead <b>of</b> <b>the</b> linear <b>regression</b> model to automated essay scoring. Comparison <b>of</b> <b>the</b> performances <b>of</b> <b>the</b> linear <b>regression</b> model and <b>the</b> cumulative logit model {{was performed on}} a large variety of data sets. It appears that the cumulative logit model performed somewhat better than did <b>the</b> linear <b>regression</b> model...|$|R
2500|$|Bayesian linear {{regression}} applies {{the framework of}} Bayesian statistics to {{linear regression}}. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β {{are assumed to be}} random variables with a specified prior distribution. [...] The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression. [...] In addition, the Bayesian estimation process produces not a single point estimate for the [...] "best" [...] values <b>of</b> <b>the</b> <b>regression</b> coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity. [...] This can be used to estimate the [...] "best" [...] coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.|$|E
5000|$|... where MSE is {{the mean}} square error <b>of</b> <b>the</b> <b>regression,</b> α and β are the {{constant}} and slope <b>of</b> <b>the</b> <b>regression</b> respectively, sβ2 is the variance of the slope <b>of</b> <b>the</b> <b>regression,</b> N {{is the number of}} points in the regression, n is the number of sample units and p is the mean value of p0 in the regression. The parameters a and b are estimated from Taylor's law: ...|$|E
5000|$|... <b>of</b> <b>the</b> <b>regression</b> {{coefficient}} of the lagged dependent variable, provided ...|$|E
30|$|Upon {{comparing}} <b>the</b> R 2 <b>of</b> <b>the</b> first linear <b>regression</b> model (Table[*] 3) {{with that}} <b>of</b> <b>the</b> second linear <b>regression</b> model (Table[*] 5), {{we can then}} report that the second model provides a better prediction <b>of</b> <b>the</b> sentence in years. In addition, because the error variance in <b>the</b> first <b>regression</b> model, S 2 = 1.90, is larger than that <b>of</b> <b>the</b> second <b>regression</b> model, S 2 = 1.62, the second model generates errors that are less dispersed. This implies that using <b>the</b> additional set <b>of</b> proposed indicators generates a better prediction.|$|R
50|$|A {{trigonometric}} representation <b>of</b> <b>the</b> orthogonal <b>regression</b> {{line was}} given by Coolidge in 1913.|$|R
50|$|Note {{that this}} is {{different}} from <b>the</b> likelihood function <b>of</b> <b>the</b> truncated <b>regression</b> model.|$|R
