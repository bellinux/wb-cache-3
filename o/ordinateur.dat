46|18|Public
25|$|Ordinator: A {{computer}} (from {{the same}} root as <b>ordinateur</b> (French) and ordenador (Spanish)).|$|E
25|$|Roland Perry {{also kept}} his {{freelance}} journalism going, and this peaked when he covered three US Presidential Campaigns (1976-Jimmy Carter; 1980-Ronald Reagan; 1984-Ronald Reagan). He wrote primarily for leading {{newspapers and magazines}} in the UK, including The Times and The Sunday Times. This led {{to a series of}} political articles for Penthouse Magazine UK, and a documentary on the election of Ronald Reagan in 1980 The Programming of the President. Perry further wrote a book, Hidden Power: The Programming of the President, which was published by Aurum Press in 1984 in the UK, and Beaufort Books in the US. At this time, he wrote another political book for French publishers Robert Laffont and Bonnel. Despite having the dreary title Elections sur <b>Ordinateur</b> (Elections on Computer) it was well received in France. It covered the marketing of political candidates in Europe.|$|E
5000|$|Groupe de {{recherche}} en conception assistée par <b>ordinateur</b> (GRCAO) ...|$|E
5000|$|TIMA : Technique de l'Informatique, de la Microélectronique pour l'Architecture des <b>ordinateurs</b> (Technics of Computer Science, Microelectronics for Computer Architecture).|$|R
2500|$|Laguerre, Christian École, {{informatique}} et nouveaux comportements préf. de Ségolène Royal (Paris Montréal (Québec) : Éd. l'Harmattan, 1999) , subject(s): Informatique—Aspect social Éducation et informatique <b>Ordinateurs</b> et enfants.|$|R
40|$|Computers are {{becoming}} more widely accepted for applications in the construction industry. Architects, engineers, builders and building operators unfamiliar with computer technology may be confused by the myriad of tools available for computer-aided design and for construction automation. Computer technology has many useful applications in construction practice. Aussi disponible en fran 7 ais: Les <b>ordinateurs</b> et la constructionPeer reviewed: NoNRC publication: Ye...|$|R
5000|$|Inside Looking Out (1992), {{ensemble}} instrumental, <b>ordinateur</b> et bande ...|$|E
5000|$|... 2014 - Aléas: Une oeuvre intégralement écrite par un <b>ordinateur</b> ...|$|E
50|$|L'opéra de glace, pour chœur d'adolescents, vidéo et <b>ordinateur,</b> icarEditions, 2008.|$|E
40|$|P. Deshayes & V. Gautheron—Small Computers Play Mankala. In {{this study}} of awele no optimal {{strategy}} could be defined—which keeps the game interesting. However a number of 'advantageous attitudes' were defined as statistically preferable to a random playing of the game. Deshayes Patrick, Gautheron Véronique. Les petits <b>ordinateurs</b> jouent au mankala [...] In: Cahiers d'études africaines, vol. 16, n° 63 - 64, 1976. pp. 489 - 493...|$|R
50|$|Computers For Schools (CFS) (<b>Ordinateurs</b> pour les écoles) (OPÉ) is a Canadian not-for-profit organization, {{founded in}} 1993 by both Industry Canada and the TelecomPioneers. The Computers for Schools (CFS) Program is a national, Industry Canada-led {{initiative}} that has offshoots in all provinces and territories. The different workshops collect, repair and refurbish donated surplus computers from {{both public and}} private sector sources, and redistribute them to schools, public libraries, not-for-profit learning organizations and Aboriginal communities throughout Canada.|$|R
50|$|The Canadian Society for Digital Humanities is a Canadian {{scholarly}} association. Its {{full name}} is CSDH-SCHN or Society for Digital Humanities/Société canadienne des humanités numériques. The CSDH-SCHN was founded as the COCH/COSH or Consortium for Computers in the Humanities/ Consortium pour <b>ordinateurs</b> en sciences humaines in 1986. The organization {{changed its name}} to the Society for Digital Humanities/Société pour l'étude des médias interactifs (SDH/SEMI), but became CSDH-SCHN after 2007, when it was enfranchised by the Alliance of Digital Humanities Organizations.|$|R
50|$|TAO is the French {{acronym for}} Testing Assisté par <b>Ordinateur</b> (Computer Based Testing).|$|E
5000|$|Ordinator: A {{computer}} (from {{the same}} root as <b>ordinateur</b> (French) and ordenador (Spanish)).|$|E
5000|$|... 1997, X, Langue - <b>Ordinateur</b> - Mentalité (réd. Toshio Ishiwata & ANdré Włodarczyk) ...|$|E
5000|$|Oulipo {{member and}} ALAMO (Atelier de Littérature Assistée par la Mathématique et les <b>Ordinateurs)</b> founder Paul Braffort {{explained}} that Calvino initially published a shorter {{version of the}} story in the Italian edition of Playboy magazine in 1973. Calvino later expanded the text with the view to turning it into a novel titled, L’ordre dans le crime ( [...] literally [...] "The Order in Crime" [...] and L’ordine del delitto in Italian). Braffort was asked by Calvino to write an editing and filtering program. This collaboration resulted in a presentation made on 15 June 1977 at the [...] "Atelier de Recherches Avancées" [...] at the Pompidou Centre (Centre for Art and Culture) in Paris.|$|R
40|$|Am 19. November 1997 wurde in der Universitätsbibliothek der Rijksuniversiteit Groningen (Niederlande) (RUG) die Elektronische Bibliothek (EB) eröffnet. Mit über 130 Rechnern werden den Studenten und Mitarbeitern der Universität während 77 Stunden pro Woche (über alle Wochentage verteilt) elektronische Daten mit entsprechenden Suchmöglichkeiten zur Verfügung gestellt. Die Gründe für die Einrichtung dieser Elektronischen Bibliothek werden in diesem Beitrag erläutert. The {{electronic}} library of the University Library of Groningen The {{electronic library}} of the University Library of the Rijksuniversiteit Groningen (Netherlands) was {{opened on the}} 19 th of November 1997. With some 130 computers, the students {{and members of the}} University can access electronic data and searching possibilities during 77 hours per week (distributed over all days of the week). The contribution relates the reasons of the installation of the electronic library. La bibliothèque électronique de la Bibliothèque Universitaire de Groningen La bibliothèque électronique de la Bibliothèque Universitaire de la Rijksuniversiteit Groningen (Pays-Bas) fut ouverte le 19 novembre 1997. Les étudiants et les membres de l’université y disposent de 130 <b>ordinateurs</b> donnant accès aux ressources électroniques avec les moyens de recherches appropriés et ceci pendant 77 heures par semaine (réparties sur tous les jours). Cette contribution a pour but d’expliquer les raisons de l’installation de la bibliothèque électronique. ...|$|R
40|$|The {{provision}} of public washroom facilities is usually based on requirements in various building codes, {{and health and}} labour standards. These requirements are traditional and often do not correspond to the actual usage of washrooms. Without information {{on the extent to}} which washrooms are used it is difficult, if not impossible, to make a rational decision about the number of plumbing fixtures to install in a building. As plumbing fixtures are expensive the over- {{provision of}} fixtures can result in a needless waste of money. On the other hand, the underprovision of fixtures results in user dissatisfaction, and perhaps increased maintenance costs and health hazards. To enable designers to make reasonable decisions about the provision of washroom facilities and to enable the various agencies that are responsible for plumbing fixture requirements to stipulate an appropriate number of fixtures, information must be developed to describe the likelihood of various fixtures being used and the consequences of providing a certain number and arrangement of fixtures. The National Research Council's Division of Building Research has been conducting studies of washroom use over the last few years in order to provide this kind of information. Les r 9 sultats de plusieurs 9 tudes sur l'utilisation des toilettes sont d 9 crits de m qu'une vari 9 t 9 d'exigences dans divers codes et normes. Des renseignements sur le pourcentage de personnes utilisant les toilettes, la dur 9 e d'utilisation et les r 9 sultats de simulations par <b>ordinateurs</b> sont d 9 crits pour des centres d'achat, des th 92 tres, des stades et des stades couverts. Peer reviewed: YesNRC publication: Ye...|$|R
50|$|Her debut {{solo album}} in 1983, Coeur <b>Ordinateur,</b> would feature songs written by Plamondon.|$|E
5000|$|Sous la {{direction}} de María Klonáris et Katerína Thomadáki, Technologies et imaginaires: Art cinéma, art vidéo, art <b>ordinateur,</b> Paris, Dis Voir, 1990.|$|E
50|$|The Matra & Hachette <b>Ordinateur</b> Alice was a home {{computer}} sold in France beginning in 1983. It was a clone of the TRS-80 MC-10, produced through {{a collaboration between}} Matra and Hachette in France and Tandy Corporation in the United States.|$|E
50|$|Britain, {{rather than}} the United States, opened Japan to Western trade, {{in part because the}} United States became fragmented, due to {{interference}} from a Britain which foresaw the implications of a unified United States on the world stage. Counterpart successor states to our world's United States include: a (truncated) United States; the Confederate States of America; the Republic of Texas; the Republic of California; a Communist Manhattan Island commune (with Karl Marx as a leading light); British North America (analogous to Canada, albeit slightly larger in this world); Russian America (Alaska); and terra nullius. Napoleon III's French Empire holds an entente with the British and Napoleon is even married to a British woman. In the world of The Difference Engine, it occupies Mexico, as it did briefly in reality during the American Civil War. Like Great Britain, it has its own analytical/difference engines (<b>ordinateurs),</b> especially used in the context of domestic surveillance within its police force and intelligence agencies. As for the other world powers, Germany remains fragmented, with no suggestion that Prussia will eventually form the core of a unified nation as it did in our own world in 1871, which may be due to French sabotage analogous to that pursued {{in the case of the}} fragmentation of the United States noted above. Japan is awakening after the British ended its isolation, and looks, as in our world, set to become one of this world's leading industrial and economic powers from the twentieth century onward. Due to the intervention of Lords Byron and Babbage, providing famine relief with grain confiscated from the landed aristocracy, the Irish potato famine never occurred, and as a result there is no agitation for Irish home rule or Irish independence, and the Irish instead become enthusiastic supporters of the Radical regime.|$|R
40|$|So far, {{following}} {{the works of}} A. M. Turing, the algorithms were considered as the mathematical abstraction from which we could write programs for computers whose principle {{was based on the}} theoretical concept of Turing machine. We start here from the observation that natural algorithms or rather algorithms of the nature which are massively parallel, autoadaptative and reproductible, and for which {{we do not know how}} they really work, nor why, are not easily specified by the current theoretical model of Universal Turing machine, or Universal Computer. In particular the aspects of communications, evolutionary rules (rulers), random (unpredictable) events, just like the genetic code, are taken into account only by subtleties which oblige to break the theory. We shall propose one universal model of algorithm called machine-alpha which contains and generalizes the existing models. [...] - Jusqu'ici, suite aux travaux de A. M. Turing [Turing, 1936], les algorithmes ont été vus comme l'abstraction à partir de laquelle on pouvait écrire des programmes pour des <b>ordinateurs</b> dont le principe était lui-même issu du concept théorique de machine de Turing. Nous partons ici du constat que les algorithmes naturels ou plutôt les algorithmes de la nature, massivement parallèles, autoadaptatifs et auto reproductibles, dont on ne sait pas comment ils fonctionnent réellement, ni pourquoi, ne sont pas aisément spécifiés par le modèle théorique actuel de Machine de Turing Universelle, ou de Calculateur Universel; en particulier les aspects de communications, de règles évolutives, d' événements aléatoires, à l'image du code génétique, ne sont pris en compte que par ajout d'artifices à la théorie. Nous nous proposons ici de montrer comment aborder ces problèmes en repensant le modèle théorique. Nous proposerons un modèle d'algorithme, appelé ici machine-α qui contient et généralise les modèles existants. Comment: 19 page...|$|R
40|$|This {{repository}} {{contains the}} original Perl scripts and datasets, {{that were used}} in the paper: - Jean-Baptiste Camps & Florian Cafiero, ``Genealogical variant locations and simplified stemma: a test case'', in Analysis of Ancient and Medieval Texts and Manuscripts: Digital Approaches, ed. Tara Andrews & Caroline Macé, Turnhout, 2015 (Lectio, 1), p. 69 ‑ 93. The scripts and data are offered in the state in which they were used for the original version of the paper. We do not advise use of these scripts, but encourage using the actualised version of the software, provided {{in the form of a}} package for the statistical software _R_, available on Github, [URL] In this repository, you will find: 	the original version of the three scripts (root of the repository); 	a folder for each of the data-sets (only Parzival and Fournival were actually used in the paper), containing 	 		*. csv, the numeric encoded format, to use with the scripts; 		*. ods, the spreadsheet in which variants were labelled and selected. 	 	 The scripts implement a revised version of the algorithm that was invented by: 	Eric Poole, ``The Computer in Determining Stemmatic Relationships'', Computers and the Humanities, 8 - 4 (1974), p. 207 ‑ 16; 	Eric Poole, ``L’analyse stemmatique des textes documentaires'', in La pratique des <b>ordinateurs</b> dans la critique des textes, Paris: CNRS Éditions, 1979, p. 151 ‑ 61; and then revised and extend by Camps & Cafiero 2015. The script `EliminationdePoole` is for identifying conflicting Variant locations, while `AgregationdePoole` is to be used to group manuscripts, and `Reconstruction` to reconstruct the virtual model of a given group of manuscripts. The two datasets that were used for the paper, Parzival and Fournival, come from, 	Parzival: M. Spencer et al., `Phylogenetics of artificial manuscripts', Journal of theoretical biology, 227 - 4 (2004), p. 503 – 511; 	Fournival: Richard de Fournival, Li bestiaires d’amours di maistre Richart de Fornival e li response du bestiaire, ed. Cesare Segre, Naples, 1957. The other datasets are partial and in an unfinished state, they have their source in: 	Heinrichi: Roos, Teemu, and Heikkilä, Tuomas, ``Evaluating methods for computer-assisted stemmatology using artificial benchmark data sets'', Literary and Linguistic Computing, 24 - 4 (2009), p. 417 – 433. 	Notre-Besoin: Baret, Philippe V., Robinson, P., and Macé, C., ``Testing methods on an artificially created textual tradition'', Linguistica computazionale, 24 (2004), p. 1000 – 1029. The fulls sources for most of these datasets can be obtained through the site of the 2007 CASC: 	Roos, Teemu, Heikkilä, Tuomas, and Myllymäki, Petri, Computer-Assisted Stemmatology Challenge, 2007, [URL]...|$|R
5000|$|I {{sat down}} at my {{computer}} at 11 o'clock, and, at noon, I had a text that was so interesting that the Americans wanted to publish it. It was on that day that I realized I was truly a federalist. (Je me suis assis devant mon <b>ordinateur</b> à 11 h et, à midi, j'avais un texte tellement intéressant que les Américains ont voulu le publier. C'est ce jour-là que je me suis rendu compte que j'étais vraiment fédéraliste.) ...|$|E
50|$|In 1984 a standard, ASCII-based {{transliteration}} {{system was}} proposed {{by an international}} group of Egyptologists at the first Table ronde informatique et égyptologie and published in 1988 (see Buurman, Grimal, et al., 1988). This {{has come to be}} known as the Manuel de Codage (or MdC) system, based on the title of the publication, Inventaire des signes hiéroglyphiques en vue de leur saisie informatique: Manuel de codage des textes hiéroglyphiques en vue de leur saisie sur <b>ordinateur.</b> It is widely used in e-mail discussion lists and internet forums catering to professional Egyptologists and the interested public.|$|E
5000|$|... 1- Architecture: Architecture DES Architecture, Master2- Arts Décoratifs: Section Arts Graphiques et Publicité: [...] Creation Publicitaire License, Master Graphisme/ Création Assistée par <b>Ordinateur</b> License [...] Illustration - Bande Dessinée License, Master [...] Animation 2D/3D License, Master [...] Graphisme-Multimédia-Réseaux Sociaux License, Master [...] Photographie License, Master [...] Direction Artistique Master [...] Section Architecture d'intérieur et Design produit: [...] Architecture d'intérieur License, Master [...] Design produit Master [...] 3- Arts Plastiques et Appliqués: Arts Plastiques License, Master [...] Arts Appliqués Master [...] Nouveaux Médias Master [...] 4- Cinema et Réalisation Audiovisuelle: [...] Realisation Audiovisuelle License [...] Réalisation Cinema Master [...] Production Master Réalisation Master [...] 5- Urbanisme: [...] Urbanisme Master [...] Aménagement du Paysage Master [...] Majors {{offered at}} ALBA-:Balamand Campus (English Programs)/ Mr. George Fiani: 1- Graphic Design BFA, MFA ...|$|E
40|$|The growing {{needs in}} {{computing}} performance imply more complex computer architectures. The lack of good programming environments for these machines must be filled. The goal {{to be reached}} {{is to find a}} compromise solution between portability and performance. The subject of this thesis is studying the problem of static allocation of task graphs onto distributed memory parallel computers. This work takes part of the project INRIA-IMAG APACHE and of the european one SEPP-COPERNICUS (Software Engineering for Parallel Processing). The undirected task graph is the chosen programming model. A survey of the existing solutions for scheduling and for mapping problems is given. The possibility of using directed task graphs after a clustering phase is underlined. An original solution is designed and implemented; this solution is implemented within a working programming environment. Three kinds of mapping algorithms are used: greedy, iterative and exact ones. Most developments have been done for tabu search and simulated annealing. These algorithms improve various objective functions (from most simple and portable to the most complex and architecturaly dependant). The weigths of the task graphs can be tuned using a post-mortem analysis of traces. The use of tracing tools leads to a validation of the cost function and of the mapping algorithms. A benchmark protocol is defined and used. The tests are runned on the Meganode (a 128 transputer machine) using VCR from the university of Southampton as a router, synthetic task graphs generation with ANDES of the ALPES project (developped by the performance evaluation team of the LGI-IMAG) and the Dominant Sequence Clustering of PYRROS (developped by Tao Yang and Apostolos Gerasoulis). La demande croissante de puissance de calcul est telle que des <b>ordinateurs</b> de plus en plus performants sont fabriques. Afin que ces machines puissent etre facilement exploitees, les lacunes actuelles en terme d'environnements de programmation doivent etre comblees. Le but a atteindre est de trouver un compromis entre recherche de performances et portabilite. Cette these s'interesse plus particulierement au placement statique de graphes de taches sur architectures paralleles a memoire distribuee. Ce travail s'inscrit dans le cadre du projet INRIA-IMAG APACHE et du projet europeen SEPP-COPERNICUS (Software Engineering for Parallel Processing). Le graphe de taches sans precedence est le modele de representation de programmes paralleles utilise dans cette these. Un tour d'horizon des solutions apportees dans la litterature au probleme de l'ordonnancement et du placement est fourni. La possibilite d'utilisation des algorithmes de placement sur des graphes de precedence, apres une phase de regroupement, est soulignee. Une solution originale est proposee, cette solution est interfacee avec un environnement de programmation complet. Trois types d'algorithmes (gloutons, iteratifs et exacts) ont ete concus et implementes. Parmi ceux-ci, on retrouve plus particulierement un recuit simule et une recherche tabu. Ces algorithmes optimisent differentes fonctions objectives (des plus simples et universelles aux plus complexes et ciblees). Les differents parametres caracterisant le graphe de taches peuvent etre affines suite a un releve de traces. Des outils de prise de traces permettent de valider les differentes fonctions de cout et les differents algorithmes d'optimisation. Un jeu de tests est defini et utilise. Les tests sont effectue sur le Meganode (machine a 128 transputers), en utilisant comme routeur VCR de l'universite de Southampton, les outils de generation de graphes synthetiques ANDES du projet ALPES (developpe par l'equipe d'evaluation de performances du LGI-IMAG) et l'algorithme de regroupement DSC (Dominant Sequence Clustering) de PYRROS (developpe par Tao Yang et Apostolos Gerasoulis). Mapping task graphs on distributed memory parallel computer...|$|R
40|$|La {{simulation}} numérique est actuellement très utilisée pour étudier les systèmes physiques. Elle nécessite un programme de calcul scientifique constitué d'un modèle mathématique représentatif du problème étudié et des méthodes numériques de résolution associées. Elle fournit des résultats numériques censés représenter le phénomène physique. Pour pouvoir valider la simulation, il est absolument indispensable, d'une part, d'estimer la propagation des erreurs d'arrondi due à l'arithmétique approchée des <b>ordinateurs</b> et, d'autre part, d'évaluer l'influence des erreurs de données sur les résultats fournis. Nous présentons, dans cet article, le logiciel CADNA qui {{permet de}} valider les logiciels numériques. Nous l'appliquons à un logiciel de simulation d'analyse de la combustion dans les moteurs à allumage commandé et en montrons son efficacité. For analyzing physical phenomena, numerical simulation is used more and more frequently. Starting with a mathematical model describing the phenomenon being analyzed, this simulation consists in creating a scientific computing program expressing this model by implementing the numerical methods required for solving it. Simulation {{is considered to be}} valid when the results its provides are in agreement with the results issuing from experimenting with the phenomenon. However, to conclude in the possible validity of the simulation, the numerical results provided by the computer must be previously validated. Yet, these results contain a computing error resulting from the propagation of round-off errors caused by the floating-point arithmetic used by the computer. They also contain an error coming from the uncertainties concerning the data of the problem. Hence it is first indispensable to assess the influence of these errors. This article is made up of two parts. The first part concerns the validation of numerical software results. After making a brief review of the floating-point arithmetic and highlighting the serious consequences it may have on the results obtained, we describe a probabilistic approach to the analysis of round-off errors, the CESTAC (Contrôle et Estimation STochastique des Arrondis de Calculs) method, from the standpoint of both its theoritical bases and its practical implementation. This method has given rise to a new arithmetic, called stochastic arithmetic, the principal properties of which are summed up. Likewise, a probabilistic approach estimating the influence of data errors is described. A software called CADNA (Control of Accuracy and Debugging for Numerical Applications) able to automaticaly implement stochastic arithmetic in any Fortran program, is described in this paper. When used in programs implementing the three classes of numerical computing methods (finite, iterative and approximate methods), it can detect numerical instabilities, control branchings and provide accuracy of the results considering the propagation of round-off errors and data errors. It is an efficient tool for validating the results of numerical software. The second part is devoted {{to the use of the}} CADNA software for qualifying the simulation software, ANALCO (ANALyse de COmbustion) which analyses combustion in spark-ingnition engines. After a description of the normal model of the phenomenon being analyzed and after mathematical model has been deduced, the ANALCO simulation software is described. The results obtained with ANALCO, not using CADNA, reveal the disagreement between the simulation results and the experimental results. The use of the CADNA software eliminates the numerical instabilties, controls the execution of the program and demonstrates that the disagreement between the simulation results and the results observed is due only to numerical problems. Likewise, the CADNA software brings out both the validity range of the model in the light of the data errors and the data that make the mathematical model the most sensitive. From this analysis, we propose to improve the accuracy of the most influential data so as to widen the validity range of the model. This study shows that:(a) The ANALCO software not associated with CADNA;-does not detect the end of the combustion;-leads to results that are contrary to physical reality (oscillations of the mass of burned gases at the end of the combustion);- provides values of the CA 50 and HLC without giving their accuracy. (b) The ANALCO software associated with CADNA can be used :- to analyse the influence of data uncertainty. Hence with current sensors, the CA 50 and HLC are respectively provided with three and two significant decimal figure. It is impossible to provide significant results beyond a burned fraction of 90 %. By improving the accuracy of some sensors, we could reach 96 %;- to determine the three data making the model the most sensitive, i. e. the air flow rate, the angular position of the crankshaft and the heat transfer coefficient. In short, by the use of the CADNA sofware, the simulation software, ANALCO, has been qualified...|$|R
40|$|In this PhD thesis, {{we present}} a study of {{distributed}} asynchronous algorithms of control. Distributed algorithms are algorithms operating on distributed systems. These systems consist in networks of sites, where each site can be either simple (when reduced to a single processor) or complex (when expanded to a whole computer or a Local Area Network). In this study, we only consider networks of sites sharing neither memory nor global clock. Sites work in parallel, asynchronously and each computation is only performed by message exchange. In such a context, distributed algorithms are called ``message-driven''. We try to limit waiting states by not introducing synchronization mechanisms. Generally speaking, we make no particular assumption on the way algorithms start, namely, any non-empty subset of sites may start an algorithm. We try to remain as general as possible but in this work, we limit our considerations to determinist algorithms. Our assumptions are supporting the essential properties of distributed algorithms~: that is essentially the local behaviour. A control algorithm establishes a virtual structure over the whole network in which each site can distinguish some of its neighbors to play special roles. More particularly, we have chosen to study structures which are similar to spanning trees. We recall that numerous problems in distributed computing, such as distributed termination and leader election, {{can be reduced to}} spanning tree construction. In order to construct such a structure or to elect a leader, most of known distributed algorithms transform this problem into an extrema-finding problem. In fact, they elect the site which has the greatest (or the lowest) identity and construct a spanning tree at the same time. We study two kinds of algorithms~: phase-based algorithms and what we call anarchic algorithms. The latter algorithms are designed to behave without any kind of synchronization. Of course their study is difficult and they often need more message exchange, but this kind of un-foreseeable behaviour is a rather good way for improving the fault tolerance. We present a new algorithm of such a kind, moreover it is associated to a leader election which is not an extrema-finding. Its analysis leads us to show worst-case examples, but these examples are rare and our empirical average-case analysis show that this algorithm is almost as good as the best known algorithm for spanning tree construction. This latter algorithm uses token-based methods and therefore behave more sequentially. Other algorithms such as constructing constrained spanning trees, are studied. The most popular constraint is the minimum total weight, which represents an economical criterion. To our knowledge, the Minimum Diameter Spanning Tree is a problem which had never been addressed in the field of distributed research. We consider ``weighted'' diameter~: viz. the diameter $D$ of a graph is the sum of the edges' weights along the longest shortest path. If we consider time complexity, this constraint is obviously of great interest, since it always exists a couple of sites needing at least $D$ units of time to exchange information. We present a method for constructing a MDST and from this method, we give different algorithms, whether fault tolerance is needed or not. Finally, the practical study of distributed algorithm on large networks leads to build a simulator. Compared to the others simulators, ours offer the advantage of being simple and easily adaptable. In order to have faster and more realistic simulations, we parallelize this algorithm. The same code can be performed on a personal computer, a parallel computer or even a distributed machine. Nous présentons dans cette thèse une étude sur des algorithmes distribués asynchrones et déterministes de contröle. Un système distribué consiste en un réseau de sites (processeurs, <b>ordinateurs</b> ou réseaux locaux). Dans cette thèse, nous ne considérons que des réseaux de sites communicants n'ayant ni mémoire partagée ni horloge globale. De nombreux problèmes de l'algorithmique distribuée sont réductibles à la construction d'un Arbre Couvrant qui est la structure de contrôle qui nous intéresse. Nous étudions deux types d'algorithmes~: ceux utilisant la notion de phase logique et les autres qui ne considèrent aucun mécanisme de synchronisation. Ces derniers ont des comportements imprévisibles améliorant la tolérance aux fautes. Nous présentons un nouvel algorithme de ce type associé à une élection qui n'est pas une recherche d'extremum contrairement à l'usage. Cet algorithme est comparable au meilleur algorithme connu qui utilise des jetons et des phases logiques induisant un comportement plus "séquentiel". D'autres algorithmes, construisant des AC contraints, sont considérés. En particulier l'AC de Diamètre Minimum qui est, à notre connaissance, un problème qui n'a jamais été étudié dans ce domaine. Le diamètre d'un graphe est la somme des poids des arêtes du plus long des plus courts chemins. Si nous considérons la complexité temporelle, cette contrainte est d'un intérêt &vident. Nous proposons différents algorithmes suivant que la tolérance aux fautes est nécessaire ou non. Finalement, l'étude pratique des algorithmes distribués sur des réseaux de grande taille nous a conduit à la construction d'un simulateur. Il permet l'exécution d'un même code source sur des machines séquentielles ou parallèles...|$|R
50|$|Roland Perry {{also kept}} his {{freelance}} journalism going, and this peaked when he covered three US Presidential Campaigns (1976-Jimmy Carter; 1980-Ronald Reagan; 1984-Ronald Reagan). He wrote primarily for leading {{newspapers and magazines}} in the UK, including The Times and The Sunday Times. This led {{to a series of}} political articles for Penthouse Magazine UK, and a documentary on the election of Ronald Reagan in 1980 The Programming of the President. Perry further wrote a book, Hidden Power: The Programming of the President, which was published by Aurum Press in 1984 in the UK, and Beaufort Books in the US. At this time, he wrote another political book for French publishers Robert Laffont and Bonnel. Despite having the dreary title Elections sur <b>Ordinateur</b> (Elections on Computer) it was well received in France. It covered the marketing of political candidates in Europe.|$|E
5000|$|The {{foundation}} of the Académie française (French Academy) in 1634 by Cardinal Richelieu created an official body whose goal has been the purification and preservation of the French language. This group of 40 members {{is known as the}} Immortals, not, as some erroneously believe, because they are chosen to serve for the extent of their lives (which they are), but because of the inscription engraved on the official seal given to them by their founder Richelieu—"À l'immortalité" [...] ("to the Immortality the French language"). The foundation still exists and contributes to the policing of the language and the adaptation of foreign words and expressions. Some recent modifications include the change from software to logiciel, packet-boat to paquebot, and riding-coat to redingote. The word <b>ordinateur</b> for computer was however not created by the Académie, but by a linguist appointed by IBM (see :fr:ordinateur).|$|E
40|$|International audienceWe present {{here for}} the first time the tele-robotics system for hot-cells MT 200 -TAO (TAO stands for Teleoperation Assistee par <b>Ordinateur</b> or Computer Assisted Tele-robotics), able to replace a {{conventional}} telescopic medium tele-manipulator (extension 4 m; capacity 20 kg). The system is currently under evaluation in COGEMA/AREVA- La Hague hot-cells...|$|E
40|$|The {{world is}} {{day by day}} more computerized. There {{is more and more}} {{software}} running everywhere, from personal computers to data servers, and inside most of the new popularized inventions such as connected watches or intelligent washing machines. All of those technologies use software applications to perform the services they are designed for. Unfortunately, the number of software errors grows with the number of software applications. In isolation, software errors are often annoyances, perhaps costing one person a few hours of work when their accounting application crashes. Multiply this loss across millions of people and consider that even scientific progress is delayed or derailed by software error: in aggregate, these errors are now costly to society as a whole. There exists two techniques to deal with those errors. Bug fixing consists in repairing errors. Resilience consists in giving an application the capability to remain functional despite the errors. This thesis focuses on bug fixing and resilience in the context of exceptions. Exceptions are programming language constructs for handling errors. They are implemented in most mainstream programming languages and widely used in practice. We specifically target two problems:Problem # 1 : There is a lack of debug information for the bugs related to exceptions. This hinders the bug fixing process. To make bug fixing of exceptions easier, we will propose techniques to enrich the debug information. Those techniques are fully automated and provide information about the cause and the handling possibilities of exceptions. Problem # 2 : There are unexpected exceptions at runtime {{for which there is no}} error-handling code. In other words, the resilience mechanisms against exceptions in the currently existing (and running) applications is insufficient. We propose resilience capabilities which correctly handle exceptions that were never foreseen at specification time neither encountered during development or testing. In this thesis, we aim at finding solutions to those problems. We present four contributions to address the two presented problems. In Contribution # 1, we lies the foundation to address both problems. To improve the available information about exceptions, we present a characterization of the exceptions (expected or not, anticipated or not), and of their corresponding resilience mechanisms. We provide definitions about what is a bug when facing exceptions and what are the already-in-place corresponding resilience mechanisms. We formalize two formal resilience properties: source-independence and pure-resilience as well as an algorithm to verify them. We also present a code transformation that uses this knowledge to enhance the resilience of the application. Contribution # 2 aims at addressing the limitations of Contribution # 1. The limitations is that there are undecidable cases, for which we lack information to characterize them in the conceptual framework of Contribution # 1. We focus on the approaches that use the test suite as their main source of information as in the case of Contribution # 1. In this contribution, we propose a technique to split test cases into small fragments in order to increase the efficiency of dynamic program analysis. Applied to Contribution # 1, this solution improves the knowledge acquired by providing more information on more cases. Applied to other dynamic analysis techniques which also use test suites, we show that it improve the quality of the results. For example, one case study presented is the use of this technique on Nopol, an automatic repair tool. Contribution # 1 and # 2 are generic, they target any kind of exceptions. In order to further contribute to bug fixing and resilience, we need to focus on specific types of exceptions. This focus enables us to exploit the knowledge we have about them and further improve bug fixing and resilience. Hence, in the rest of this thesis, we focus on a more specific kind of exception: the null pointer dereference exceptions (NullPointerException in Java). Contribution # 3 focuses on Problem # 1, it presents an approach to make bug fixing easier by providing information about the origin of null pointer dereferences. We present an approach to automatically provide information about the origin of the null pointer dereferences which happen in production mode (i. e. those for which no efficient resilience mechanisms already exists). The information provided by our approach is evaluated w. r. t. its ability to help the bug fixing process. This contribution is evaluated other 14 real-world null dereference bugs from large-scale open-source projects. Contribution # 4 addresses Problem # 2, we present a way to tolerate the same kind of errors as Contribution # 3 : null pointer dereference. We first use dynamic analysis to detect harmful null dereferences, skipping the non-problematic ones. Then we propose a set of strategies able to tolerate this error. We define code transformations to 1) detect harmful null dereferences at runtime; 2) allow a runtime ehavior modification to execute strategies; 3) assess the correspondance between the modified behavior and the specifications. This contribution is evaluated other 11 real-world null dereference bugs from large-scale open-source projects. To sum up, this thesis studies the exceptions, their behaviors, the information one can gathered from them, the problems they may cause and the applicable solutions to those problems. Le monde est de plus en plus informatisé. Il y a de plus en plus de logiciels en cours d'exécution partout, depuis les <b>ordinateurs</b> personnels aux serveurs de données, et à l'intérieur de la plupart des nouvelles inventions connectées telles que les montres ou les machines à laver intelligentes. Toutes ces technologies utilisent des applications logicielles pour effectuer les taches pour lesquelles elles sont conçus. Malheureusement, le nombre d'erreurs de logiciels croît avec le nombre d'applications logicielles. Localement, une erreur logicielle est embêtante, elle peut coûter quelques heures de travail à une personne lorsque l'application crashe. Multipliez cette perte sur des millions de personnes et considérez que même les avancées scientifiques sont retardées ou empêchées par des erreurs de ce type: dans l'ensemble, ces erreurs sont coûteuses pour la société dans son ensemble. Il existe deux techniques pour faire face à ces erreurs. La réparation logicielle consiste à réparer ces erreurs manuellement. La résilience consiste à donner à une application la capacité de rester fonctionnelle malgré les erreurs. Cette thèse porte sur la correction des bugs et la résilience dans le contexte des exceptions. L'Exception est un mécanisme de gestion d'erreurs. Il est intégré dans la plupart des langages de programmation et largement utilisé dans la pratique. Dans cette thèse, nous ciblons spécifiquement deux problèmes:Problème n° 1 : Il ya un manque d'informations de débogage pour les bugs liés à des exceptions. Cela entrave le processus de correction de bogues. Pour rendre la correction des bugs liées aux exceptions plus facile, nous allons proposer des techniques pour enrichir les informations de débogage. Ces techniques sont entièrement automatisées et fournissent des informations sur la cause et les possibilités de gestion des exceptions. Problème n ° 2 : Il ya des exceptions inattendues lors de l'exécution pour lesquelles il n'y a pas de code pour gérer l'erreur. En d'autres termes, les mécanismes de résilience actuels contre les exceptions ne sont pas suffisamment efficaces. Nous proposons de nouvelles capacités de résilience qui gérent correctement les exceptions qui n'ont jamais été rencontrées avant. Dans cette thèse, nous nous efforçons de trouver des solutions à ces problèmes. Nous présentons quatre contributions pour résoudre les deux problèmes présentés. En résumé, cette thèse étudie les exceptions, leurs comportements, l'information qu'on peut recueillir auprès d'elles, les problèmes qu'elles peuvent causer et les solutions applicables à ces problèmes...|$|R
40|$|Guillaume Alléon EADS-CCR (France) invité Åke Björck Linköping University (Sweden) {{rapporteur}} Iain S. Duff CERFACS (France) & RAL (UK) membre du jury Luc Giraud CERFACS (France) directeur de thèse Gene H. Golub Stanford University (CA, USA) membre du jury Gérard Meurant CEA (France) rapporteur Chris C. Paige Mc Gill University (Canada) rapporteur Yousef Saad University of Minnesota (USA) invitéThe {{starting point}} of this thesis is a problem posed by the electromagnetism group at EADS-CCR: How to solve several linear systems with the same coefficient matrix but various right-hand sides ? For the targeted application, the matrices are complex, dense and huge (of order of a few millions). Because such matrices cannot be computed nor stored in numerical simulations involved in a design process, {{the use of an}} iterative scheme with an approximate matrix-vector product is the only alternative. The matrix-vector product is performed using the Fast Multipole Method. In this context, the goal of this thesis is to adapt Krylov solvers so that they handle efficiently multiple right-hand sides. Some preliminary works dealing with one right hand side show that GMRES was an efficient and robust solver for that application. Consequently, we mainly focus, in this thesis, on variants of GMRES. The orthogonalization schemes that we implemented for GMRES are some variants of the Gram-Schmidt algorithm. In a first part, we have investigated the effect of rounding errors in the Gram-Schmidt algorithms. Our results answer questions that have been around for the last 25 years. We give theoretical explanations of what was currently observed and accepted, namely: * modified Gram-Schmidt algorithm generates a well-conditioned set of vectors, * and Gram-Schmidt algorithm iterated twice gives an orthogonal set of vectors. These two sentences holds when the initial matrix is ``not-too-ill-conditioned'' in a sense that is clearly defined. Furthermore, when the Gram-Schmidt algorithm is iterated with selective reorthogonalizations then, in a third part, we give a new criterion. We prove that the resulting algorithm is robust while the most commonly used criterion might have some weakness. In a fourth part, we generalize standard results for modified Gram-Schmidt from norms to singular values. This enables us to propose an a-posteriori reorthogonalization procedure for modified Gram-Schmidt algorithm based on a low rank update. These results have several direct applications, we give examples for Krylov methods for solving linear systems and for eigenvalue computations. Finally, instead of using the Euclidean scalar product, we have derived our results on Gram-Schmidt algorithm with the A-scalar product, where A is an Hermitian definite positive matrix. The relevance of such a study is illustrated in a collaboration with the Global Change team at CERFACS, the Lanczos algorithm with reorthogonalization is used for data assimilation in a climate modeling problem. In a second part, we have implemented variants of the GMRES algorithm for both real and complex, single and double precision arithmetics suitable for serial, shared memory and distributed memory computers. This software implementation, that complies with scientific library standard quality, is described in detail. For the sake of the simplicity, flexibility and efficiency the GMRES solvers have been implemented using the reverse communication mechanism for the matrix-vector product, the preconditioning step and the dot product computations. Several orthogonalization procedures have been implemented to reduce the cost of the dot product calculation, that is a well known bottleneck for the Krylov method efficiency in a parallel distributed environment. The implemented stopping criterion is based on a normwise backward error. The variants available are GMRES-DR, seed-GMRES and block-GMRES (that adds to standard GMRES, flexible GMRES and SQMR). An LU-matrix-vector product step is performed in GMRES-DR in order to store the approximate eigenvectors on the first Krylov vectors. Implicit restart and implicit preconditioning is done in seed-GMRES to avoid a matrix-vector product and a preconditioning step per right-hand side and per GMRES cycle. The block-GMRES version allows the user to select either no deflation or deflation based on the singular value decomposition of the Krylov vectors. Finally we extend existing theoretical result that relates the norm of the residual to the smallest singular value of the space constructed by the Krylov solver from GMRES to block-GMRES. The third part is dedicated to the improvement of these standard methods for the solution of linear systems arising in electromagnetic applications. After a deep presentation of the code we use, we study the influence of the level of non-symmetry in the SQMR algorithm and illustrate the behaviour of the GMRES-DR method in our example. Then we mainly focus on the multiple right-hand sides solves. First of all, we examined in details techniques to adapt single right-hand side method in the multiple right-hand sides case: increase the quality of the preconditioner, initial guess strategy or gathering strategy. In the context of the monostatic radar cross section calculation, we prove that the number of independent right-hand sides is finite. The dimension of the space of right-hand sides given by our theory and the dimension numerically observed corresponds well each other. This property enables us to reduce considerably the number of linear systems to solve. In this context, a particular implementation of the block-GMRES method is given. Then, some specific issues about the seed and the block methods are discussed. Finally more prospective results are given. First, different strategies to extract and add spectral informations in a GMRES cycle are presented and compared. Then, we exploit the fact that the Fast Multipole Method is an inexact matrix-vector product for which the accuracy can be tuned. The less accurate the matrix-vector product is, the fastest the computation. We show how {{to take advantage of this}} by using relaxed schemes (inexact Krylov methods) or inner-outer schemes (flexible GMRES). Finally we study the relevance of the normwise backward error as a stopping criterion for the iterative solvers in the monostatic radar cross section problem. Le point de départ de cette thèse est un problème posé par le groupe électromagnétisme de EADS-CCR : comment résoudre plusieurs systèmes linéaires avec la même matrice mais différents seconds membres ? Pour l'application voulue, les matrices sont complexes, denses et de grande taille. Un problème standard comporte environ quelques millions d'inconnues. Comme de telles matrices ne peuvent être ni calculées, ni stockées dans un processus industriel, l'utilisation d'un produit matrice-vecteur approché est la seule alternative. En l'occurrence, le produit matrice-vecteur est effectué en utilisant la méthode multipôle rapide. Dans ce contexte, le but de cette thèse est d'adapter les méthodes itératives de type Krylov de telle sorte qu'elles traitent efficacement les nombreux seconds membres. Des travaux préliminaires avec un seul second membre ont montré que la méthode GMRES est particulièrement efficace et robuste pour cette application. En conséquence dans cette thèse nous abordons uniquement les variantes de GMRES. Les schémas d'orthogonalisation que nous avons implantés dans GMRES sont des variantes de l'algorithme de Gram-Schmidt. Dans une première partie, nous nous intéressons à l'influence des erreurs d'arrondi dans les algorithmes de Gram-Schmidt. Nos résultats répondent à des questions vieilles de vingt-cinq ans. Nous donnons l'explication théorique de ce qui était communément observé et accepté : - l'algorithme de Gram-Schmidt modifié génère un ensemble de vecteurs bien conditionné; - l'algorithme de Gram-Schmidt itéré deux fois fabrique un ensemble de vecteurs orthonormé. Ces deux propositions reposent sur l'hypothèse que la matrice de départ est "numériquement non singulière" en un sens qui est clairement défini. D'autre part, quand l'algorithme de Gram-Schmidt est itéré avec un critère de réorthogonalisation, nous proposons un nouveau critère. Nous montrons que l'algorithme obtenu est robuste alors que le critère communément utilisé est mis en défaut dans certains cas. Finalement, nous généralisons des résultats standards sur les normes en terme de valeurs singulières pour l'algorithme de Gram-Schmidt modifié. Ceci nous permet de dériver un schéma de réorthogonalisation a posteriori utilisant une matrice de rang faible. Ces résultats ont plusieurs applications directes. Nous en donnons des exemples avec les méthodes de Krylov pour résoudre des problèmes linéaires avec plusieurs seconds membres. Dans la deuxième partie, nous avons implémenté des variantes de la méthode GMRES pour les arithmétiques réelle et complexe, simple et double précisions. Cette implémentation convient pour des <b>ordinateurs</b> classiques, à mémoire partagée ou distribuée. Le code en résultant satisfait aux critères de qualité des librairies standards et son implémentation est largement détaillée. Pour des besoins de simplicité, flexibilité et efficacité, les solveurs utilisent un mécanisme de reverse communication pour les produits matrice-vecteur, les étapes de préconditionnement et les produits scalaires. Différents schémas d'orthogonalisation sont implémentés pour réduire le coût de calcul des produits scalaires, un point particulièrement important pour l'efficacité des méthodes de Krylov dans un environnement parallèle distribué. Le critère d'arrêt implémenté est basé sur l'erreur inverse normalisée. Les variantes disponibles sont GMRES-DR, seed-GMRES et block-GMRES. Ces codes s'ajoutent aux variantes déjà existantes (GMRES, flexible GMRES et SQMR). Un produit matrice-vecteur avec une décomposition LU est utilisé dans GMRES-DR de telle sorte que le stockage des approximations des vecteurs propres se fasse sur les premiers vecteurs de l'espace de Krylov. Un restart implicite et une étape de préconditionnement implicite ont été implémentés dans seed-GMRES. Nous supprimons ainsi un produit matrice-vecteur et une étape de préconditionnement par second membre et par cycle de GMRES. La version de block-GMRES permet à l'utilisateur de sélectionner différents modes de déflation. Pour terminer, des résultats reliant la norme du résidu de GMRES à la plus petite valeur singulière de l'espace construit par la méthode de Krylov ont été généralisés à la méthode block-GMRES. La troisième partie est consacrée à l'amélioration des techniques standards pour la résolution des systèmes linéaires dans le cadre des problèmes électromagnétiques. Après une présentation approfondie du code, nous étudions l'influence de la non-symétrie sur la convergence de l'algorithme SQMR. Nous étudions aussi le comportement de GMRES-DR sur nos problèmes. Ceci correspond à deux méthodes avec un seul second membre, le reste de cette partie concerne les cas comportant plusieurs seconds membres. Tout d'abord, nous examinons en détail les techniques qui permettent d'adapter les méthodes utilisées pour un second membre unique aux cas comportant plusieurs seconds membres. Par exemple, on peut améliorer la qualité du préconditionneur, avoir une stratégie de solution initiale, grouper les opérations de plusieurs résolutions ou encore paralléliser plusieurs résolutions. Dans le contexte du calcul de surface équivalente radar monostatique, nous avons montré que l'espace des seconds membres du problème continu était de dimension finie. La dimension donnée par notre théorie est proche de celle que nous observons en pratique. Cette propriété nous permet de réduire considérablement le nombre de systèmes linéaires à résoudre. Dans ce contexte, une version de la méthode block-GMRES est donnée. Ensuite, nous abordons certains problèmes spécifiques des méthodes seed-GMRES et block-GMRES pour lesquels nous proposons des solutions. Pour finir, des résultats plus prospectifs sont donnés. Plusieurs stratégies pour extraire et ajouter de l'information spectrale d'un cycle de GMRES à l'autre sont proposées et comparées. Puis nous utilisons le fait que la méthode multipôle rapide est un produit matrice-vecteur inexact dont la précision est réglable. Moins précis est le produit matrice-vecteur, plus rapide il est. Nous montrons comment tirer partie de cette propriété en utilisant un schéma relâché (méthode de Krylov inexacte) ou des itérations emboîtées (flexible GMRES). Enfin, le critère d'arrêt basé sur l'erreur inverse normalisée dans le cadre du calcul d'une surface équivalente radar est remis en question...|$|R
40|$| émulsifiant. Les {{particules}} de polymère ainsi produites sont suspendues dans l'eau, grâce à l'émulsifiant. Le milieu résultant est donc appelé 'latex'. Les propriétés du latex dépendent de beaucoup de paramètres, tels que la distribution de la masse molaire de polymère, la distribution des tailles de particules, la température de transition vitreuse, la morphologie, et la composition du polymère, si la réaction fait intervenir plusieurs monomères. Ces propriétés sont influencées par plusieurs variables, {{la nature}} et la quantité des additifs tels que, l'amorceur, le tensioactif les agents de transfert éventuels, la façon dont ces produits et les monomères sont introduits dans le réacteur, la température de la réaction, l'agitation, et le type de réacteur utilisé. Ces nombreuses variables sont alors nos variables de commande, que nous devons faire varier pour obtenir les propriétés désirées. En réalité, afin d'obtenir des propriétés spécifiques, tout en assurant la sécurité du procédé, plusieurs paramètres sont fixés a priori, tels que la quantité initiale de réactifs et la température de la réaction. Ceci dit, plusieurs paramètres restent à faire varier en ligne, pendant la réaction, comme la température de la double enveloppe, pour maintenir la réaction à la température prévue, et le débit d'ajout des monomères, qui nous permettront de contrôler les propriétés du latex (la {{taille des particules}} et la composition du polymère). Ce genre d'intervention en ligne pour améliorer la qualité et la sécurité du procédé, le contrôle en ligne du procédé de polymérisation en émulsion, sont l'objet but principal de ce travail. Du point de vue du contrôle, le procédé est un système dynamique (représenté par des équations différentielles) possédant des entrées, des états (les variables décrivant l'évolution du procédé), et finalement des sorties (qui sont les variables mesurées en ligne, et sont en général, une combinaison des états du système). Pour contrôler un procédé, un modèle représentatif de ce procédé est nécessaire. Faute de trouver un modèle parfait du procédé, des informations en ligne provenant des sorties du système, s'avèrent nécessaires pour accomplir la stratégie de contrôle. En réalité, les raisons pour lesquelles, les méthodes de contrôle avancées n'ont pas été utilisées lors des procédés de polymérisation en émulsion sont le manque de capteurs en ligne capables de donner des mesures pour la plupart des propriétés des polymères, la rapidité de la réaction, la sensibilité de la réaction à la présence de petites quantités d'additifs, la nonlinéarité du modèle du procédé et le grand nombre de paramètres inconnus dans le modèle et l'interaction entre ces paramètres. Une des propriétés intéressantes à contrôler lors des procédés de polymérisation en émulsion, est la composition du polymère, pour les procédés faisant intervenir plusieurs monomères à une composition non azéotropique. La composition du polymère intervient au niveau de la détermination de la température de transition vitreuse, des propriétés mécaniques, ainsi que de la morphologie du polymère. Il a été trouvé dans la littérature que l'ajout du monomère le plus réactif à des débits variables est la méthode la plus efficace pour contrôler la composition tout en assurant une vitesse de réaction relativement élevée. En utilisant une méthode avancée de contrôle, nous pouvons alors maintenir la composition à une valeur désirée. Ceci nécessite une connaissance approfondie du procédé ainsi que la mesure en ligne de la composition de polymère. Les capteurs en ligne utilisés pour suivre les procédés de polymérisation en émulsion peuvent être divisés en deux catégories: des capteurs nécessitant une boucle de circulation externe, ou un système de prise d'échantillons afin d'effectuer l'analyse (Chromatographie en phase gazeuse, densimétrie); et des capteurs in situ qui effectuent les analyses dans le réacteur (Spectroscopie Infrarouge, sondes ultrasons). La chromatographie en phase gazeuse peut être utilisée pour mesurer, en ligne, la composition du mélange de monomère résiduel, si le réacteur est équipé d'un système automatique de prise d'échantillons, de dilution, dans certains cas, et d'injection dans le chromatogramme. La densimétrie a été également utilisée pour la mesure en ligne de la composition du polymère. Une boucle de circulation du latex dans l'appareil est donc indispensable pour effectuer l'analyse. Cependant ces deux méthodes présentent quelques difficultés expérimentales, telles que la floculation du latex dans les appareils et le retard des analyses, qui sont dues, pour partie, au transfert du latex. C'est pourquoi, les expériences sont préférablement suivies avec les capteurs in situ, où l'analyse est effectuée dans le réacteur. Cependant, ces capteurs, tels que l'infrarouge, l'ultrason et la spectroscopie Raman sont en phase de développement, et nous ne possédons pas encore de modèles complets liant ces mesures avec les propriétés du latex. Le manque de capteurs en ligne qui donnent des mesures réelles des procédés est un problème fréquent dans plusieurs domaines. Dans les procédés de polymérisation en émulsion, ce manque est d'abord dû à la nature hétérogène et visqueuse du latex qui rend la mesure directe des propriétés du polymère souvent difficile. Ensuite, l'équipement d'un procédé avec plusieurs capteurs serait économiquement impossible. A cause du manque de capteurs, et du manque de modèles exacts des procédés, des efforts ont été faits dans le domaine de l'estimation logicielle des états, non mesurés expérimentalement, en se basant sur des mesures réelles et le modèle du procédé. Ces observateurs, ou estimateurs, ou encore capteurs logiciels, sont conçus à partir du modèle du procédé en utilisant les sorties réelles du procédé. Si le système est observable, l'observateur nous permettra d'obtenir des informations sur les états non mesurés du procédé. Les observateurs sont souvent utilisés pour le suivi des procédés, mais sont également très utiles pour le filtrage, la détection de panne et le contrôle des procédés. Les méthodes d'estimation et de contrôle linéaire ont souvent été appliquées dans le domaine de la polymérisation en émulsion, et ce malgré la nonlinéarité des modèles représentant ces procédés. Ceci est dû premièrement à la difficulté de manipuler les outils non linéaires, au temps nécessaire à l'ordinateur pour résoudre ces observateurs et au fait que les observateurs non linéaires proposés dans la littérature étaient souvent limités à un groupe de systèmes non linéaires. Ces sujets ne posent aucune difficulté aujourd'hui, avec le développement de la théorie non linéaire, et l'évolution de la rapidité et de la capacité des <b>ordinateurs.</b> L'objectif de ce travail est le contrôle de la composition des polymères, tout en assurant une vitesse de réaction élevée, ainsi que la sécurité de l'opération. Puisque les modèles représentant ce procédé sont nonlinéaires, nous allons utiliser des méthodes d'estimation et de contrôle nonlinéaires adaptées au procédé. La stratégie d'estimation est constituée de trois parties. Dans la première partie, nous allons développer un capteur qui fournit des informations précises et rapides sur le procédé. La deuxième partie est constituée de l'estimation de la composition du polymère lors des procédés de co- et terpolymérisations en émulsion. La dernière étape est la construction de lois de commande adéquates qui nous permettent d'obtenir la composition et la vitesse de réaction désirées. Dans le premier chapitre, une introduction générale du sujet ainsi que la stratégie de recherche sont proposées. Le deuxième chapitre contient les théories d'estimation et de contrôle nonlinéaires que nous allons utiliser tout au long de ce travail. Le troisième chapitre traite des capteurs en ligne pour les procédés de polymérisation en émulsion. D'après certains critères, nous choisissons d'utiliser la calorimétrie pour suivre le procédé. Cependant, ce capteur ne nous donne pas directement une mesure de la vitesse de la réaction, car plusieurs paramètres dans le bilan thermique restent inconnus. Pour contourner ce problème, nous allons utiliser une méthode d'optimisation de la conversion globale du monomère, en corrigeant les paramètres inconnus. Pour ce faire, les mesures de la température du réacteur, de la double enveloppe, et quelques mesures expérimentales de la conversion sont nécessaires. Dans le quatrième chapitre nous utilisons les informations obtenues par calorimétrie, pour estimer la composition du polymère lors des procédé de copolymérisation. Nous traitons le cas d'un observateur nonlinéaire à grand gain qui tient compte de la réaction dans la phase aqueuse et un autre où on néglige l'effet de la phase aqueuse. Il s'avère que, pour les monomères étudiés, la composition du polymère n'est pas sensible à la réaction dans la phase aqueuse. Ce phénomène est peut être dû au fait que la plupart des radicaux sont dans les particules, surtout pour un taux de solide élevé. La deuxième raison est que les monomères utilisés ici sont seulement partiellement solubles dans l'eau, ce qui fait que la quantité de monomère dans les particules est plus importante que dans la phase aqueuse. Dans le cinquième chapitre, un observateur nonlinéaire à grand gain est construit pour suivre la composition du polymère lors des procédés de terpolymérisation. Puisque le quatrième chapitre montre clairement que nous n'avons pas besoin de tenir compte de la phase aqueuse pour l'estimation de la composition, un modèle de monomères hydrophobes est choisi pour construire l'observateur. Le chapitre 6 expose les lois de commande développées pour maintenir la composition de co- et de terpolymères sur une trajectoire prédéfinie. Des lois de commande nonlinéaires avec une linéarisation entrée-sortie sont utilisées. Dans ce chapitre, nous établissons un contrôleur local de la pompe, qui assure l'exécution des débits envoyés par la commande de la composition. Dans le dernier chapitre nous évoquons le concept de maximisation de productivité. Notre objectif est de maintenir la composition à une valeur prédéfinie, et en même temps maximiser la vitesse de la réaction. Les variables de commande sont les débits d'ajout de monomères. La variable contrôlée est la concentration de monomère dans les particules. Cependant, pendant le contrôle de la concentration de monomère dans les particules il faut faire attention à ce que la chaleur produite par la réaction soit inférieure à la chaleur maximale que la double enveloppe est capable d'évacuer...|$|R
