6016|0|Public
5|$|In the infinite-dimensional case, an <b>orthonormal</b> basis {{will not}} be a basis in the sense of linear algebra; to {{distinguish}} the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique.|$|E
25|$|In mathematics, the Stiefel {{manifold}} V'k(Rn) is {{the set of}} all <b>orthonormal</b> k-frames in Rn. That is, it is the set of ordered k-tuples of <b>orthonormal</b> vectors in Rn. It {{is named}} after Swiss mathematician Eduard Stiefel. Likewise one can define the complex Stiefel manifold V'k(Cn) of <b>orthonormal</b> k-frames in Cn and the quaternionic Stiefel manifold V'k(Hn) of <b>orthonormal</b> k-frames in Hn. More generally, the construction applies to any real, complex, or quaternionic inner product space.|$|E
25|$|Every <b>orthonormal</b> {{basis in}} a {{separable}} Hilbert space is a Schauder basis. Every countable <b>orthonormal</b> basis {{is equivalent to}} the standard unit vector basis in ℓ2.|$|E
25|$|The theorem {{that every}} Hilbert space has an <b>orthonormal</b> basis.|$|E
25|$|Consider a 60° (6-fold) {{rotation}} matrix {{with respect}} to an <b>orthonormal</b> basis in 2D.|$|E
25|$|For a given {{inertial}} frame, an <b>orthonormal</b> {{basis in}} space, combined {{by the unit}} time vector, forms an <b>orthonormal</b> basis in Minkowski space. The number {{of positive and negative}} unit vectors in any such basis is a fixed pair of numbers, equal to the signature of the bilinear form associated with the inner product. This is Sylvester's law of inertia.|$|E
25|$|Physicists {{commonly}} regard a density operator {{as being}} {{represented by a}} (possibly infinite) density matrix relative to some <b>orthonormal</b> basis.|$|E
25|$|A {{frame is}} a tight frame if A = B; in other words, the frame {{satisfies}} a generalized version of Parseval's identity. For example, {{the union of}} k <b>orthonormal</b> bases of a vector space is a tight frame with A = B = k. A tight frame is a Parseval frame (sometimes called a normalized frame) if A = B = 1. Each <b>orthonormal</b> basis is a Parseval frame, but the converse is not always true.|$|E
25|$|In other words, {{the space}} of <b>orthonormal</b> bases is like the {{orthogonal}} group, but without a choice of base point: given an orthogonal space, there is no natural choice of <b>orthonormal</b> basis, but once one is given one, there is a one-to-one correspondence between bases and the orthogonal group. Concretely, a linear map is determined by where it sends a basis: just as an invertible map can take any basis to any other basis, an orthogonal map can take any orthogonal basis to any other orthogonal basis.|$|E
25|$|The {{completeness}} of the Hermite functions {{follows from}} the fact that the Bargmann transform is unitary and carries the <b>orthonormal</b> basis e'n(z) of holomorphic Fock space onto the H'n(x).|$|E
25|$|In linear algebra, an {{orthogonal}} matrix or real orthogonal matrix is {{a square}} matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., <b>orthonormal</b> vectors), i.e.|$|E
25|$|The {{standard}} {{bases of}} the sequence spaces c0 and ℓp for 1≤ p< ∞, as well as every <b>orthonormal</b> basis in a Hilbert space, are unconditional. These bases are also symmetric.|$|E
25|$|If T {{satisfies}} TT* = T*T, we call T normal. It {{turns out}} that normal matrices are precisely the matrices that have an <b>orthonormal</b> system of eigenvectors that span V.|$|E
25|$|The {{singularities}} are avoided {{when considering}} and manipulating the rotation matrix as <b>orthonormal</b> row vectors (in 3D applications often named the right-vector, up-vector and out-vector) instead of as angles. The singularities are also avoided {{when working with}} quaternions.|$|E
25|$|The spherical {{harmonic}} functions form {{a complete}} <b>orthonormal</b> {{set of functions}} {{in the sense of}} Fourier series. It should be noted that workers in the fields of geodesy, geomagnetism and spectral analysis use a different phase and normalization factor than given here (see spherical harmonics).|$|E
25|$|Taking an <b>orthonormal</b> basis e'n of H, S(H) can {{be written}} as an {{infinite}} tensor product of the S(C e'n). The irreducibility of W {{on each of these}} spaces implies the irreducibility of W on the whole of S(H). W is called the complex wave representation.|$|E
25|$|In formal terms, this {{representation}} is a wavelet series {{representation of a}} square-integrable function with respect to either a complete, <b>orthonormal</b> set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square integrable functions. This is accomplished through coherent states.|$|E
25|$|The two {{previous}} theorems {{raise the question}} of whether all inner product spaces have an <b>orthonormal</b> basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's A Hilbert Space Problem Book (see the references).|$|E
25|$|An <b>orthonormal</b> {{basis for}} Minkowski space {{necessarily}} consists of one timelike and three spacelike unit vectors. If {{one wishes to}} work with non-orthonormal bases {{it is possible to}} have other combinations of vectors. For example, one can easily construct a (non-orthonormal) basis consisting entirely of null vectors, called a null basis.|$|E
25|$|Every Hilbert space admits an <b>orthonormal</b> basis, and any {{two such}} bases for a {{particular}} space have the same cardinality. This cardinality is called the dimension of the Hilbert space. This dimension is finite {{if and only if}} the space's Hamel dimension is finite, and in this case the two dimensions coincide.|$|E
25|$|The orbispace {{associated}} to an orbihedron has a canonical metric structure, coming locally from the length metric {{in the standard}} geometric realization in Euclidean space, with vertices mapped to an <b>orthonormal</b> basis. Other metric structures are also used, involving length metrics obtained by realizing the simplices in hyperbolic space, with simplices identified isometrically along common boundaries.|$|E
25|$|Hilbert {{spaces are}} central to many applications, from quantum {{mechanics}} to stochastic calculus. The spaces L2 and ℓ2 are both Hilbert spaces. In fact, by choosing a Hilbert basis (i.e., a maximal <b>orthonormal</b> subset of L2 or any Hilbert space), one sees that all Hilbert spaces are isometric to ℓ2(E), where E is a set with an appropriate cardinality.|$|E
25|$|To {{define a}} {{particular}} cross product, an <b>orthonormal</b> basis {ej} may be selected and a multiplication table provided that determines all the products {ei × ej}. One possible multiplication table {{is described in}} the Example section, {{but it is not}} unique. Unlike three dimensions, there are many tables because every pair of unit vectors is perpendicular to five other unit vectors, allowing many choices for each cross product.|$|E
25|$|This {{projection}} has {{the structure}} of a principal G-bundle where G is the associated classical group of degree k. Take the real case for concreteness. There is a natural right action of O(k) on V'k(Rn) which rotates a k-frame in the space it spans. This action is free but not transitive. The orbits of this action are precisely the <b>orthonormal</b> k-frames spanning a given k-dimensional subspace; that is, they are the fibers of the map p. Similar arguments hold in the complex and quaternionic cases.|$|E
25|$|The {{determinant}} can {{be thought}} of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an <b>orthonormal</b> basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.|$|E
25|$|Two {{types of}} tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them {{decomposes}} a tensor into a sum of rank-1 tensors, {{which is called}} a tensor rank decomposition. The second type of decomposition computes the <b>orthonormal</b> subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is {{referred to in the}} literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal component analysis in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.|$|E
25|$|The {{approach}} of Cartan and Weyl, using connection 1-forms {{on the frame}} bundle of , gives a third way to understand the Riemannian connection. They noticed that parallel transport dictates that a path in the surface be lifted to a path in the frame bundle so that its tangent vectors lie in a special subspace of codimension one in the three-dimensional tangent space of the frame bundle. The projection onto this subspace is defined by a differential 1-form on the <b>orthonormal</b> frame bundle, the connection form. This enabled the curvature properties of the surface to be encoded in differential forms on the frame bundle and formulas involving their exterior derivatives.|$|E
25|$|In this {{construction}} {{the representation}} of the Clifford algebra , the Lie algebra , and the Spin group , all depend on the choice of the <b>orthonormal</b> basis and the choice of the gamma matrices. This can cause confusion over conventions, but invariants like traces are independent of choices. In particular, all physically observable quantities must be independent of such choices. In this construction a spinor can be represented as a vector of 2k complex numbers and is denoted with spinor indices (usually α, β, γ). In the physics literature, abstract spinor indices are often used to denote spinors even when an abstract spinor construction is used.|$|E
25|$|According to quantum {{mechanics}} (particularly quantum indeterminacy), no possible measurement distinguishes between the 4 different polarization states, {{as they are}} not all orthogonal. The only possible measurement is between any two orthogonal states (an <b>orthonormal</b> basis). So, for example, measuring in the rectilinear basis gives a result of horizontal or vertical. If the photon was created as horizontal or vertical (as a rectilinear eigenstate) then this measures the correct state, {{but if it was}} created as 45° or 135° (diagonal eigenstates) then the rectilinear measurement instead returns either horizontal or vertical at random. Furthermore, after this measurement the photon is polarized in the state it was measured in (horizontal or vertical), with all information about its initial polarization lost.|$|E
25|$|In any {{discretised}} wavelet transform, {{there are}} only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the father wavelet φ in L2(R), and that a is an integer. A typical choice is a = 2 and b = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every <b>orthonormal</b> discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.|$|E
25|$|Numerical {{analysis}} {{takes advantage}} {{of many of the}} properties of orthogonal matrices for numerical linear algebra, and they arise naturally. For example, it is often desirable to compute an <b>orthonormal</b> basis for a space, or an orthogonal change of bases; both take the form of orthogonal matrices. Having determinant ±1 and all eigenvalues of magnitude 1 is of great benefit for numeric stability. One implication is that the condition number is 1 (which is the minimum), so errors are not magnified when multiplying with an orthogonal matrix. Many algorithms use orthogonal matrices like Householder reflections and Givens rotations for this reason. It is also helpful that, not only is an orthogonal matrix invertible, but its inverse is available essentially free, by exchanging indices.|$|E
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an <b>orthonormal</b> eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.|$|E
500|$|A {{system of}} vectors {{satisfying}} {{the first two}} conditions basis is called an <b>orthonormal</b> system or an <b>orthonormal</b> set (or an <b>orthonormal</b> sequence if [...] is countable). Such a system is always linearly independent. Completeness of an <b>orthonormal</b> system of vectors of a Hilbert space can be equivalently restated as: ...|$|E
500|$|Conversely, if [...] is an <b>orthonormal</b> set {{such that}} Parseval's {{identity}} holds for every , then [...] is an <b>orthonormal</b> basis.|$|E
500|$|As a {{consequence}} of Zorn's lemma, every Hilbert space admits an <b>orthonormal</b> basis; furthermore, any two <b>orthonormal</b> bases of the same space have the same cardinality, called the Hilbert dimension of the space. For instance, since [...] has an <b>orthonormal</b> basis indexed by , its Hilbert dimension is the cardinality of [...] (which may be a finite integer, or a countable or uncountable cardinal number).|$|E
500|$|The {{notion of}} an <b>orthonormal</b> basis from linear algebra generalizes over {{to the case of}} Hilbert spaces. In a Hilbert space , an <b>orthonormal</b> basis is a family [...] of {{elements}} of [...] satisfying the conditions: ...|$|E
