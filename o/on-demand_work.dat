5|19|Public
2500|$|Rolf {{advocates for}} {{innovation}} within the labor union movement, including calling for labor {{to use its}} existing resources to reinvent itself by developing organizations that “have economic power to improve lives,” that can “scale” to ultimately help millions of workers and that are financially sustainable so they can survive without relying on foundation grants. Rolf has also recently supported new responses {{to the rise of}} contingent and <b>on-demand</b> <b>work,</b> including portable benefits and a basic income. A Fast Company article in September 2017 said that Rolf is [...] "widely seen {{as one of the most}} far-thinking leaders in labor." ...|$|E
30|$|According to NIST (National Institute of Standards and Technology, USA), “Cloud Computing is a {{model for}} {{enabling}} convenient, <b>on-demand</b> <b>work</b> access to a shared pool of configurable resources (e.g., networks, servers, storage, applications and services) that can rapidly be provisioned and released with minimal management effort or service provider interaction” [3].|$|E
40|$|Data-driven {{modeling}} of biological {{systems such as}} proteinprotein interaction networks is data-intensive and combinatorially challenging. Backtracking can constrain a combinatorial search space. Yet, its recursive nature, exacerbated by data-intensity, limits its applicability for large-scale systems. Parallel, scalable, and memory-efficient backtracking is a promising approach. Parallel backtracking suffers from unbalanced loads. Load rebalancing via synchronization and data movement is prohibitively expensive. Balancing these discrepancies, while minimizing end-to-end execution time and memory requirements, is desirable. This paper introduces such a framework. Its scalability and efficiency, demonstrated on the maximal clique enumeration problem, are attributed to the proposed: (a) representation of search tree decomposition to enable parallelization; (b) depth-first parallel search to minimize memory requirement; (c) least stringent synchronization to minimize data movement; and (d) <b>on-demand</b> <b>work</b> stealing with stack splitting to minimize processors ’ idle time. The applications of this framework to real biological problems related to bioethanol production are discussed. 1...|$|E
50|$|In 2009, Avail Media (an IPTV provider) and TVN Entertainment Corporation merged {{to create}} Avail-TVN. In May 2010, the company {{announced}} that it would add support for 3D television to its roster of services. The company’s 3D <b>on-demand</b> offering <b>works</b> with operators' existing infrastructure and will work with existing set-top boxes after minor software updates.|$|R
40|$|This work {{deals with}} the {{suitability}} of the enterprise information system implementation in on-demand model for small businesses. Describes the benefits and risks of this operation forthe implementation and further development of small businesses. Compares the features, advantages and shortcomings of modelson-premise and <b>on-demand.</b> The <b>work</b> included analysis, folds to define and compare products available corporate informationsystem, solved on-demand model in the Czech and world market...|$|R
5000|$|UN Radio and UN Television were {{transformed}} to better meet the programming and delivery needs of global media partners. Fawzi’s team began using internet and satellite feeds to transmit rough cuts and soundbites {{that members of}} the media could access live and <b>on-demand.</b> <b>Working</b> with APTN and UNICEF, they developed a new series of daily news video-clips called UNifeed. They also created the magazine-style television programme 21st Century about the work and issues on the UN’s agenda. Produced in a way that allows broadcasters to use segments in their own programming, 21st Century is today aired around the world and shown on airline in-flight programming. [...] Fawzi also served as an advisor for the UN’s senior management. As a member of the strategic group that met regularly to make recommendations to the Secretary-General and his team, Fawzi contributed to formulating UN public information strategies and promoting interaction with the press. He was called upon by areas of the UN and organizations in the wider UN family to review resources and needs, identify priority issues and develop concrete proposals for improving outreach.|$|R
40|$|Digital work {{platforms}} are transforming {{labor markets}} around the world. Firms that own, manage and deploy these work platforms have reframed employer–worker relations by defining their core business as {{the provision of}} the technology that enables certain services to be provided rather than the provision of those services, and offering their workers independent contractor arrangements rather than employee contracts. This has significant consequences in terms of wages, jobs security and other working conditions. Digital work platforms also increase worker welfare by offering unparalleled flexibility in setting work hours and most permit a workday to be segmented, allowing {{certain parts of the}} population who otherwise {{would not be able to}} work (due to other commitments or constraints) to have some source of income. At the same time, they pose significant challenges in the labor market. Companies replace employees with contract workers to control costs but this may lead to lower pay, benefits, and job security. Therefore, there is an urgent need for a policy debate on how to best prepare workers for this new reality. This document describes three main concerns: the issue of worker misclassification in digital work platforms, the lack of social security systems for workers in the gig economy that are not considered employees, and the problems that the isolating nature of <b>on-demand</b> <b>work</b> presents with respect to worker organization and the right to collective bargaining. Abstract. [...] Introduction. [...] I. Labor issues regarding the on-demand economy. [...] A. Non-standard employment and employee misclassification. B. Social security in the on-demand economy. C. Collective bargaining. [...] II. Looking forward...|$|E
40|$|As {{the number}} of cores in manycore systems grows exponentially, {{the number of}} {{failures}} is also predicted to grow exponentially. Hence massively parallel computations {{must be able to}} tolerate faults. Moreover new approaches to language design and system architecture are needed to address the resilience of massively parallel heterogeneous architectures. Symbolic computation has underpinned key advances in Mathematics and Computer Science, for example in number theory, cryptography, and coding theory. Computer algebra software systems facilitate symbolic mathematics. Developing these at scale has its own distinctive set of challenges, as symbolic algorithms tend to employ complex irregular data and control structures. SymGridParII is a middleware for parallel symbolic computing on massively parallel High Performance Computing platforms. A key element of SymGridParII is a domain specific language (DSL) called Haskell Distributed Parallel Haskell (HdpH). It is explicitly designed for scalable distributed-memory parallelism, and employs work stealing to load balance dynamically generated irregular task sizes. To investigate providing scalable fault tolerant symbolic computation we design, implement and evaluate a reliable version of HdpH, HdpH-RS. Its reliable scheduler detects and handles faults, using task replication as a key recovery strategy. The scheduler supports load balancing with a fault tolerant work stealing protocol. The reliable scheduler is invoked with two fault tolerance primitives for implicit and explicit work placement, and 10 fault tolerant parallel skeletons that encapsulate common parallel programming patterns. The user is oblivious to many failures, they are instead handled by the scheduler. An operational semantics describes small-step reductions on states. A simple abstract machine for scheduling transitions and task evaluation is presented. It defines the semantics of supervised futures, and the transition rules for recovering tasks in the presence of failure. The transition rules are demonstrated with a fault-free execution, and three executions that recover from faults. The fault tolerant work stealing has been abstracted in to a Promela model. The SPIN model checker is used to exhaustively search the intersection of states in this automaton to validate a key resiliency property of the protocol. It asserts that an initially empty supervised future on the supervisor node will eventually be full in the presence of all possible combinations of failures. The performance of HdpH-RS is measured using five benchmarks. Supervised scheduling achieves a speedup of 757 with explicit task placement and 340 with lazy work stealing when executing Summatory Liouville up to 1400 cores of a HPC architecture. Moreover, supervision overheads are consistently low scaling up to 1400 cores. Low recovery overheads are observed in the presence of frequent failure when lazy <b>on-demand</b> <b>work</b> stealing is used. A Chaos Monkey mechanism has been developed for stress testing resiliency with random failure combinations. All unit tests pass in the presence of random failure, terminating with the expected results...|$|E
50|$|Relaunched in 2006 after {{a ten-year}} hiatus, {{the press was}} noted for its unique all-digital platform. Rice's digital press {{operated}} just as a traditional press, up to a point. Manuscripts were solicited, reviewed, edited and resubmitted for final approval by an editorial board of prominent scholars. But rather than waiting for months for a printer to make a bound book, Rice University Press's digital files were instead run through Connexions, an open-source e-publishing platform. The technology offered authors {{a way to use}} multimedia—audio files, live hyperlinks or moving images—to craft dynamic scholarly arguments, and to publish <b>on-demand</b> original <b>works</b> in fields of study that were increasingly constrained by print publishing.|$|R
5000|$|GEGL is {{modelled}} after a directed acyclic graph, {{where each}} node represents an image operation (called [...] "operators" [...] or [...] "ops"), and each edge represents an image. Operations can in general take several input images and give several output images, which corresponds to having several incoming edges (images) and several outgoing edges (images) {{at a given}} node (operation). The system uses an <b>on-demand</b> model, doing <b>work</b> only as required. This allows features such as having very quick previews while editing, and once the user has finished making changes, GEGL will repeat the same operations in full resolution for the final image in the background.|$|R
40|$|Hyper/J {{supports}} {{a new approach}} to constructing, integrating and evolving software, called multi-dimensional separation of concerns. Developers can decompose and organize code and other artifacts according to multiple, arbitrary criteria (concerns) simultaneously—even after the software has been implemented—and synthesize or integrate the pieces into larger-scale components and systems. Hyper/J facilitates several common development and evolution activities non-invasively, including: adaptation and customization, mix-and-match of features, reconciliation and integration of multiple domain models, reuse, product line management, extraction or replacement of existing parts of software, and <b>on-demand</b> remodularization. Hyper/J <b>works</b> with standard Java software, not requiring special compilers or environments. This demonstration will show it in action in a number of software engineering scenarios at different stages of the software lifecycle...|$|R
40|$|The {{launch of}} a new Engineering Technology {{undergraduate}} degree at a research intensive university prompted collaboration from six different disciplines within the College of Technology. With a flexible curriculum designed to meet existing and future workforce needs, the program of study incorporated both new and revised courses. One of the new courses is a gateway Introduction to Engineering Technology course designed {{to attract and retain}} both traditional and nontraditional students. In this introductory course, engineering technology is defined based on the skill set needed for the current and future economy. The gateway course employs a reverse course-content-delivery design whereby students engage traditional lecture-based subject matter in a user-friendly manner that encourages students to revisit lectures <b>on-demand.</b> Students <b>work</b> through a series of at-home assignments in a linear manner, labeled simply as read, watch, and do. These assignments build upon each other to develop both depth and breadth through repeated exposure and analysis of core concepts. This is consistent with learning theory literature, which is replete with studies showing that when students experience expectation failure, followed by a time of thorough and investigative feedback loops, learning gains are increased almost fourfold, from 20 – 30...|$|R
40|$|International audienceVehicular ad hoc {{networks}} represent one of {{the most}} important applications of wireless ad hoc networks. In highways, where different networks exist, frequent and sometimes unnecessary switches from one network to another may occur, which degrade network performance and affect applications, especially those that require high quality networks such as multimedia applications (Video conferencing, Video <b>On-Demand,</b> etc.). This <b>work</b> presents an analytical model and a Vertical Handover decision method for Highways called VHH. It is based on position, velocity, jitter, and density as mandatory inputs which aim to both minimize Vertical handover frequency and avoid unnecessary handoffs and ping pong effect between different networks, in the goal of enhancing multimedia streaming services in highways. A simulation is provided to prove the performance of the algorithm...|$|R
40|$|Purpose—Little {{is known}} about the current state of {{industry}} standards subscriptions in U. S. libraries. In this age of electronic access and tightening budgets, many libraries are re-examining whether or not to alter paper subscriptions of standards to electronic versions and/or switch to on-demand delivery. Design/Methodology—Two surveys were conducted in an attempt to gauge the extent to which other libraries are currently collecting standards in electronic format, or providing on-demand purchasing for industry standards. Findings—The number of libraries purchasing electronic standards or providing on-demand purchasing in 2003 appears to be incongruous with comments from both surveys. In the 2001 survey, librarians could not find ways to fund on-demand purchasing and in the 2003 survey, a number of libraries were purchasing some sets on an irregular basis to save money. A little over half (51 %) of the responding libraries provide electronic versions of standards and sixty percent indicated they provide on-demand purchasing of individual standards. Originality/Value—Survey responses resulted in several local changes to improve patron success rates at acquiring industry standard including: created a publicly-searchable database of locally-available standards, initiated a purchase <b>on-demand</b> process, <b>worked</b> with Interlibrary Loan (ILL) staff to determine which standards would be better to purchase versus request on ILL, and became more adept at reference interviews involving requests for standards...|$|R
50|$|She self-publishes {{her work}} through her Montoursville, Pennsylvania-based TW Designworks {{business}} (launched in late 1998), {{although many of}} her earlier cross-stitch designs are available through Leisure Arts, Inc. Additionally, the Janlynn Corporation has manufactured her designs in kit form for several years. Prior to self publishing, she was {{a regular contributor to}} Just CrossStitch magazine. She followed Cathy Livingston and was succeeded by Marie Barber. In July 2005, Teresa announced that she would taking a leave of absence from cross stitch design to focus more on artwork with the first fruits of this new focus being a selection of art prints of pen and ink fantasy designs, also sold through TW Designworks. In early October 2007, Teresa announced that her temporary leave of absence was ending, having found a local <b>on-demand</b> publisher to <b>work</b> with.|$|R
40|$|In {{any large}} scale {{distribution}} architecture, considerable thought {{needs to be}} given to resource management, particularly in the case of high quality TV <b>on-demand.</b> This <b>work</b> presents a globally accessible network storage architecture operating over a shared infrastructure, termed Video Content Distribution Network (VCDN). The goal of which is to store all TV content broadcast {{over a period of time}} within the network and make it available to clients in an on-demand fashion. This paper evaluates a number of content placement approaches in terms of their ability to efficiently manage system resources. Due to the dynamic viewing patterns associated with TV viewing, the effectiveness of content placement is expected to change over time, therefore so too should the content placement. The placement of content within such a system is the single most influential factor in resource usage. Intuitively, the further content is placed from a requesting client, the higher the total bandwidth requirements are. Likewise, the more replicas of an object that are distributed throughout the network, the higher the storage costs will be. Ideally, the placement algorithm should consider both these resources when making placement decisions. Another desirable property of the placement algorithm, is that it should be able to converge on a placement solution quickly. A number of placement algorithms are examined, each with different properties, such as minimizing delivery path. There are a large number of variables in such a system, which are examined and their impact on the algorithms performance is shown. 1...|$|R
40|$|Adaptive {{streaming}} over HTTP {{is largely}} used to deliver live and <b>on-demand</b> video. It <b>works</b> by adjusting video quality according to network conditions. While QoE for different streaming services has been studied, {{it is still}} unclear how access line capacity impacts QoE of broadband users in video sessions. We make a first step {{to answer this question}} by characterizing parameters influencing QoE, such as frequency of video adaptations. We take a passive point of view, and analyze a dataset summarizing video sessions of a large population for one year. We first split customers based on their estimated access line capacity. Then, we quantify how the latter affects QoE metrics by parsing HTTP requests of Microsoft Smooth Streaming (MSS) services. For selected services, we observe that at least 3 ~Mbps of downstream capacity is needed to let the player select the best bitrate, while at least 6 ~Mbps are required to minimize delays to retrieve initial fragments. Surprisingly, customers with faster access lines obtain limited benefits, hinting to restrictions on the design of services...|$|R
40|$|Dense {{cellular}} networks (DenseNets) {{are fast}} becoming a reality {{with the rapid}} deployment of base stations (BSs) aimed at meeting the explosive data traffic demand. In legacy systems however this comes with the penalties of higher network interference and energy consumption. In order to support network densification in a sustainable manner, the system behavior should be made 'load-proportional' thus allowing certain portions of the network to activate <b>on-demand.</b> In this <b>work,</b> we develop an analytical framework using tools from stochastic geometry theory for the performance analysis of DenseNets where load-awareness is explicitly embedded in the design. The model leverages on a flexible cellular network architecture {{where there is a}} complete separation of the data and signaling communication functionalities. Using the proposed model, we identify the most energy- efficient deployment solution for meeting certain minimum service criteria and analyze the corresponding power savings through dynamic sleep modes. Based on state-of-the-art system parameters, a homogeneous pico deployment for the data plane with a separate layer of signaling macro-cells is revealed to be the most energy-efficient solution in future dense urban environments...|$|R
40|$|The network {{virtualization}} allows new on-demand management capabilities, in {{this work}} we demonstrate such a service, namely, on-demand efficient monitoring or anonymity. The proposed service is based on network virtualization of expanders or sparsifiers over the physical network. The defined virtual (or overlay) communication graphs coupled with a multi-hop extension of Valiant randomization based routing lets us monitor the entire traffic in the network, with a very few monitoring nodes. In particular, we show that using overlay network with expansion properties and Valiant randomized load balancing {{it is enough to}} place $O(m) $ monitor nodes when the length of the overlay path (number of intermediate nodes chosen by Valiant's routing procedure) is $O(n/m) $. We propose two randomized routing methods to implement policies for sending messages, and we show that they facilitate efficient monitoring of the entire traffic, such that the traffic is distributed uniformly in the network, and each monitor has equiprobable view of the network flow. In terms of complex networks, our result can be interpreted as a way to enforce the same betweenness centrality to all nodes in the network. Additionally, we show that our results are useful in employing anonymity services. Thus, we propose monitoring or anonymity services, which can be deployed and shut down <b>on-demand.</b> Our <b>work</b> is the first, as far as we know, to bring such on-demand infrastructure structuring using the cloud network virtualization capability to existing monitoring or anonymity networks. We propose methods to theoretically improve services provided by existing anonymity networks, and optimize the degree of anonymity, in addition providing robustness and reliability to system usage and security. We believe that, our constructions of overlay expanders and sparsifiers weighted network are of independent interest...|$|R
40|$|Green WLAN is a {{promising}} technique for accessing future indoor Internet services. It is designed {{not only for}} high-speed data communication purposes but also for energy efficiency. The basic strategy of green WLAN {{is that all the}} access points are not always powered on, but rather <b>work</b> <b>on-demand.</b> Though powering off idle access points does not affect data communication, a serious asymmetric matching problem will arise in a WLAN indoor positioning system due to the fact the received signal strength (RSS) readings from the available access points are different in their offline and online phases. This asymmetry problem will no doubt invalidate the fingerprint algorithm used to estimate the mobile device location. Therefore, in this paper we propose a green WLAN indoor positioning system, which can recover RSS readings and achieve good localization performance based on singular value thresholding (SVT) theory. By solving the nuclear norm minimization problem, SVT recovers not only the radio map, but also online RSS readings from a sparse matrix by sensing {{only a fraction of the}} RSS readings. We have implemented the method in our lab and evaluated its performances. The experimental results indicate the proposed system could recover the RSS readings and achieve good localization performance...|$|R
40|$|In 1959, in {{his famous}} talk ‘There is plenty of room at the bottom’, {{physicist}} Richard Feynman had envisaged {{a new era of}} science where one could build electronic systems which would sense and interact with a world only a few atoms in size. To build such systems we not only need new materials but also new transduction strategies. The hunt for new materials has led us back to carbon, a material known since antiquity. Carbon nanotube and graphene-two allotropes of carbon, possess structural, electronic, optical and mechanical properties perfect for building fast, robust and sensitive nano-systems. However, the available sensing technologies are still incapable of high fidelity detection critical for studying nanoscale events in complex environments like ligand-receptor binding, molecular adsorption/desorption, π-π stacking, catalysis, etc. In this thesis, I first introduce a fundamentally new nanoelectronic sensing technology based on heterodyne mixing to investigate the interaction between charge density fluctuations in a nanoelectronic sensor caused by oscillating dipole moment of molecule and an alternating current drive voltage which excites it. By detecting molecular dipole instead of associated charge, we address the limitations of conventional charge-detection based nanoelectronic sensing techniques. In particular, using a carbon nanotube heterodyne platform, I demonstrate for the first time, biological detection in high ionic background solutions where conventional charge-detection based techniques fail due to fundamental Debye screening effect. Next, we report the first graphene nanoelectronic heterodyne vapor sensors which can detect a plethora of vapor molecules with high speed (~ 0. 1 second) and high sensitivity (< 1 part per billion) simultaneously; recording orders-of-magnitude improvement over existing nanoelectronic sensors which suffer from fundamental speed-sensitivity tradeoff issue. Finally, we use heterodyne detection as a probe to quantify the fundamental non-covalent binding interaction between small molecules and graphene by analyzing the real-time molecular desorption kinetics. More importantly, we demonstrate for the first time, electrical tuning of molecule-graphene binding kinetics by electrostatic control of graphene work function signifying the ability to tailor chemical interactions <b>on-demand.</b> Our <b>work</b> not only lays a foundation for next-generation of rapid and sensitive nanoelectronic detectors, but also provides an insight into the fundamental molecule-nanomaterial interaction...|$|R
40|$|During its 3 {{years of}} operations, the {{institutional}} repository at the University of Nebraska–Lincoln has gathered and posted over 19, 000 documents, which, together with its hosted ETD collection, {{make it the}} country’s 3 rd largest IR. Usage records also indicate {{it is one of}} the busiest, as well, furnishing almost 120, 000 downloads during the most recent month (October 2008). The IR has also become the cornerstone for a bundle of electronic publishing services developed and provided by the UNL Libraries. Indeed, these services may be seen as the enablers or preconditions of the IR’s successful track record. To facilitate and accelerate faculty uptake and participation in the IR, services are provided to supplement the “traditional” self-archiving model. We call it our “Do It For Me” model, and the terms are basically: send us your publications list, and we will do the rest. The IR staff does article collection, permissions clearing, scanning, preparation of author versions (including typesetting and proofing), and ultimately mediated deposit. More than 90 % of the faculty articles in the IR have been acquired via this model. Mediated deposit also allows for greater consistency in metadata, file sizes and data integrity, quality control, and copyright/permissions compliance. We also seek and solicit previously unpublished works—of any length—to be issued as original publications. Works that demonstrate strong demand in electronic form, where eligible, can be re-purposed as <b>on-demand</b> printed <b>works,</b> available for sale through a 3 rd-party vendor. The IR also promotes itself as an archival hosting service for document collections, periodical series, and conference proceedings. Finally, there are services that provide a “running re-sale” to stimulate interest and increase satisfaction among depositors. Our IR provides regular monthly usage reports and usage analysis to depositors; and this has proved immensely useful in increasing submissions from authors and co-authors, who are impressed with the numbers of downloads their articles receive. In order to maintain the relatively high levels of usage (the current average is over 7 downloads per month per article), the IR practices search engine optimization—by creating full and search-term-laden abstracts and by placing links on appropriate external websites. The presentation discusses best practices for content recruitment and document preparation, policies and implementation, staffing requirements, and software customization and design. Finally, the program considers the central role of the library’s IR in an overall campus strategy for scholarly communication and publication...|$|R
40|$|Mobile Ad hoc Networks (MANETs) are {{becoming}} popular {{as a means}} of providing communication among a group of people. Because of self-configuring and self-organizing characteristics, MANETs can be deployed quickly. There is no infrastructure defined in the network, therefore all of the participating nodes relay packets for other nodes and perform routing if necessary. Because of the limitations in wireless transmission range, communication links could be multi-hop. Routing protocol is the most important element of MANET. Routing protocols for MANET can broadly be classified as proactive routing protocol and reactive routing protocol. In proactive routing protocols like Destination Sequence Distance Vector (DSDV), mobile nodes periodically exchange routing information among themselves. Hence proactive routing protocols generate high overhead messages in the network. On the other hand, reactive routing protocols like Ad hoc On-demand Distance Vector (AODV) and Dynamic Source Routing (DSR) <b>work</b> <b>on-demand.</b> Hence reactive routing protocols generate fewer number of overhead messages in the network compared to proactive routing protocols. But reactive routing protocols use a global search mechanism called 2 ̆ 2 flooding 2 ̆ 2 during the route discovery process. By 2 ̆ 2 flooding 2 ̆ 2 mechanism a source node can discover multiple routes to a destination. 2 ̆ 2 Flooding 2 ̆ 2 generates a large number of overhead packets in the network and is the root cause of scaling problem of reactive routing protocols. Hierarchical Dynamic Source Routing (HDSR) protocol has been proposed in this dissertation to solve that scaling problem. The DSR protocol has been modified and optimized to implement HDSR protocol. HDSR protocol reduces the 2 ̆ 2 flooding 2 ̆ 2 problem of reactive routing protocols by introducing hierarchy among nodes. Two game theoretic models, Forwarding Dilemma Game (FDG) and Forwarding Game Routing Protocol (FGRP), is proposed to minimize the 2 ̆ 7 flooding 2 ̆ 7 effect by restricting nodes that should participate in route discovery process based on their status. Both FDG and FGRP protocols reduce overhead packet and improve network performances in terms of delay packet delivery ratio and throughput. Both protocols were implemented in AODV and the resulting protocol outperformed AODV in our NS- 2 simulations. A thorough connectivity analysis was also performed for FDG and FGRP to ensure that these protocols do not introduce disconnectivity. Surprisingly, both FDG and FGRP showed better connectivity compared to AODV in moderate to high node density networks...|$|R
40|$|The {{work-life balance}} is a {{fundamental}} issue for all the workers and it is linked to many others, such as quality of work, female participation in the labour market, gender equality and active ageing. Among the new challenges of contemporary society, a key role is played by the introduction in the workplaces of new digital and computer technologies, which may change working-time arrangement and, at the same time, guarantee more flexibility in working-time. On the one hand, this phenomenon {{can lead to a}} better management of working-time; nevertheless, on the other hand, it can determine an increase in working hours, since digital technologies and working-time flexibility are associate with a different evaluation of workers’ performance no more in reference to the working time, but {{on the basis of the}} results. It may result in an increase of the working time. Indeed, working hours’ flexibility does not always ascribe more freedom to the workers, but, as it is often the case, it can lead to an increase in working time, which encroach on private and family life. Technologies offer the possibility to “anytime-anyplace” jobs, but it can not result in a demand of working “always-everywhere”. The working times allowed by new technologies risks to result in new sweating practices: even though digital technologies provide easier way to shape working time patterns, actually it challenges the fundamental right of workers to have a day of rest. The distinction between working and non-working time becomes less and less visible and the border line between personal and professional life is more and more confused. In order to avoid these criticisms, it was theorized the “right to disconnect”, which is the last frontiers of right to privacy in the 21 st century. The “ghost of ubiquity” is appearing, since workers are requested to be connected always and everywhere. At the moment, the tendency is to request a broader participation of workers in the life of the enterprise. It leads to actual difficulties in distinguishing working time and personal and family life. It is essential an alternation between work connection and disconnection. It is a matter of wisely managing working time patterns and connection/disconnection times. In order to guarantee this aim, social partners through social dialogue can play an important role. It seems to be a possible way forward to reach a compromise between flexible working hours and the private life of workers. Another problematic aspect regards the introduction of information and communications technology (ITC) in workplaces. The potentiality of such new technologies is the basis of a new revolution concerning intelligent production systems and new way of working, called “sharing economy” or “collaborative economy”. Such new economies involve promises of a new great development, but also many challenges which require a protective intervention involving governments, enterprises, workers and individuals. It is not only a structural change, but also and above all a functional change, in the sense that the way of providing work is profoundly changing. It implies a new labour law conception, not only because of a substantial change of its protective capacity, but also because it involves a profound change of its scope and of the extent of its protection area; it also implies a profound reflexion about labour law rules at national, European and international level. In this sense, we can talk about a new labour law dimension. Work is a key factor in this transformation and in the digital revolution. We are used to talk about “work 4. 0 ” and about the heterogeneous phenomenon of the sharing economy. In particular, the latter includes crowdwork, work on digital platform and <b>work</b> <b>on-demand</b> via apps. The great change of work does not involve only industry 4. 0. Obviously, it does not exist a homogeneous and monolithic concept relating to this type of work, since methods and ways of working may change on the basis of the service required by the customer and in relation to the complexity and the quality of the work. If any active policy for women will not be taken, in the future women will miss best career opportunities, risking to get worse the already marker gender inequality. In the light of that, it is essential to adopt a holistic approach to cope with this situation and to encourage a greater female participation in the labour market, through real integration policy. Work 4. 0 is an opportunity without precedent to reach the same participation of women in the labour market through focusing the attention on the gender equality issue within the digital labour market...|$|R

