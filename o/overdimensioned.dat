34|31|Public
50|$|However, as {{it entered}} service, {{it proved to}} be rather <b>overdimensioned</b> for its armament, slower than {{intended}} (the intended speed of 20 kn while surfaced was never achieved) and with poor maneuverability, both surfaced and underwater; its huge cost and the rather poor endurance meant that plans to build more boats to the same designs were shelved.|$|E
5000|$|When {{building}} the Norwegian Trunk Railway (1850-1854), Robert Stephenson built {{the line in}} accordance with British standards of standard gauge and <b>overdimensioned</b> bridges and curves. This line was very expensive; Pihl argued that narrow-gauged railways would be less expensive to construct. After studying foreign designs, C. W. Bergh initially concluded that [...] would be suitable, but Pihl argued for extra width and opted for [...] Through his influential position in the department he convinced the politicians that all new railways should be built on the narrow gauge—except those that would connect with the Swedish system, where standard gauge had become the norm. During the railway construction boom of the 1870s and 1880s all but the Kongsvinger Line, the Meråker Line and the Østfold Line were built with narrow gauge, leaving Norway with two incompatible systems.|$|E
50|$|With {{its origins}} in wound rotor {{induction}} motors with multiphase winding sets on the rotor and stator, respectively, that was invented by Nikola Tesla in 1888, the rotor winding set of the doubly-fed electric machine is connected to a selection of resistors via multiphase slip rings for starting. However, the slip power {{was lost in the}} resistors. Thus means to increase the efficiency in variable speed operation by recovering the slip power were developed. In Krämer (or Kraemer) drives the rotor was connected to an AC and DC machine set that fed a DC machine connected to the shaft of the slip ring machine. Thus the slip power was returned as mechanical power and the drive could be controlled by the excitation currents of the DC machines. The drawback of the Krämer drive is that the machines need to be <b>overdimensioned</b> in order to cope with the extra circulating power. This drawback was corrected in the Scherbius drive where the slip power is fed back to the AC grid by motor generator sets.|$|E
40|$|Abstract—Grid users may {{experience}} inconsistent performance due to specific characteristics of grids, such as fluctuating workloads, high failure rates, and high resource heterogeneity. Although extensive {{research has been}} done in grids, providing consistent performance remains largely an unsolved problem. In this study we use <b>overdimensioning,</b> a simple but cost-ineffective solution, to solve the performance inconsistency problem in grids. To this end, we propose several <b>overdimensioning</b> strategies, and we evaluate these strategies through simulations with workloads consisting of Bag-of-Tasks. We find that although <b>overdimensioning</b> is a simple solution, it is a viable solution to provide consistent performance in grids. I...|$|R
40|$|Abstract. This Paper {{deals with}} the {{performance}} limitations of B 6 inverters when operated under unsymmetrical voltage sags. Three different strategies the IARC, the ICPS and the PNSC will be investigated and conclusions will be drawn as of the required <b>overdimensioning</b> of the converter, optimum control strategy and theoretical compliance of this inverter topology with current Fault-Ride-Through norms. Key words Fault-Ride-Through, grid inverter, distributed generation...|$|R
40|$|Waste {{of energy}} due to over-provisioning and <b>overdimensioning</b> of network {{infrastructures}} has recently stimulated {{the interest on}} energy consumption reduction by Internet Service Providers (ISPs). By means of resource consolidation, network virtualization based architectures will enable energy saving. In this letter, we extend the well-known virtual network embedding problem (VNE) to energy awareness and propose a mixed integer program (MIP) which provides optimal energy efficient embeddings. Simulation results show the energy gains of the proposed MIP over the existing cost-based VNE approach. Peer ReviewedPostprint (published version...|$|R
30|$|Algorithms are {{presented}} {{to reduce the}} number of access points in <b>overdimensioned</b> networks without affecting coverage (access point selection algorithm) and to achieve a certain throughput with the least amount of access points possible (network optimization algorithm).|$|E
40|$|The {{efficiency}} {{consequence of}} keeping <b>overdimensioned</b> Induction Motors (IM) in Heating Ventilation and Air Conditioning (HVAC) applications, when modernizing such systems, using the potential that modern power electronic drives gives, is investigated in this paper. Five different Eff 1 IM ratings {{in the range}} 1. 1 - 4 kW have {{been the target of}} the analysis. It is a common assumption that the motor efficiency drops rapidly as the load decreases below 75 % its rated operation, which is the case for grid connected motors or motors operated with constant V/Hz ratio by an inverter. As a result, overdimensioning of the IM will result in increased energy cost compared to a smaller rating. However, by using a variable V/Hz ratio, it is shown here that the efficiency is in general highest for the largest IM regardless of the loading, despite the increase of the mechanical losses caused by increased cooling and friction. A conclusion found is that it is in principle always worth to keep an <b>overdimensioned</b> existing motor instead of replacing it with a smaller motor having a better efficiency class...|$|E
40|$|The {{credit risk}} {{is one of}} the most {{dangerous}} category of banking risks because it covers a wide range of products and services. In the last years we were the witnesses to an intensification of the negative impact of this kind of risk at the international level. In the transition economies, the credit riskï¿½s potential was <b>overdimensioned</b> by the various evolutions of the companies and their debt level. credit risk; bank lending;...|$|E
40|$|Abstract: The {{design of}} splined joints used in {{industrial}} applications frequently re-sults in maximum flank pressure. This {{often leads to}} <b>overdimensioning</b> and in cer-tain cases to failure of the shaft-hub connection. The existing german standard DIN 5466 for the calculation of load capacity, when coupled with increasing power den-sities and weight savings, is an unsatisfactory basis for dimensioning. The principles behind the calculations for the new draft standard, developed by the Institut für Maschinenwesen TU Clausthal, are presented here {{as well as the}} main geometric pa-rameters which have an influence: the number of teeth and the width of the splined joint. 1...|$|R
40|$|As {{the number}} of {{computing}} and storage nodes keeps in-creasing, the interconnection network is becoming {{a key element of}} many computing and communication systems, where the overall performance directly depends on network performance. This performance may dramatically drop during congestion situations. Although congestion may be avoided by <b>overdimensioning</b> the network, the current trend is to reduce overall cost and power consumption by reduc-ing {{the number of}} network components. Thus, the network will be prone to congestion, thereby becoming mandatory the use of congestion management techniques. In that sense, the technique known as Regional Explicit Congestion Notification (RECN) completely eliminates the Head-of-Line (HOL) blocking produced by congested pack...|$|R
40|$|This paper {{reports on}} some {{of the results of the}} working group WG 34. 02, {{established}} by the Study Committee 34 to study and investigate co-ordination of relays and conventional current transformers (CT). The objective of the WG 34. 02 work is to suggest common recommendations on how the manufacturers should specify the CT requirements and also suggest a guideline for co-ordination of relays and current transformers. The paper will show and discuss a more realistic and correct method of calculating the <b>overdimensioning</b> factor of current transformers when co-ordinating fast numerical relays and current transformers, than what has normally been the common practice. 1...|$|R
40|$|Abstract: Governmental {{processes}} {{are complex and}} knowledge-intensive. Most pro-cess management systems fail to support them in an adequate way. Semantic tech-nologies like Semantic Web Services allow incorporating knowledge in process. But often these techniques are <b>overdimensioned</b> {{and can not be}} executed properly. Here we propose a more practical motivated approach where knowledge-intensive parts of a process are controlled, enacted and supported by business rules on top of ontolo-gies. With the help of a case study we demonstrate how to model knowledge-intensive processes. ...|$|E
40|$|Many {{pulp mill}} hot and warm water systems are <b>overdimensioned</b> and produce more hot and warm water than demanded. This {{overproduction}} {{could be used}} for other applications, if heat could be released at high enough temperatures. In this paper a new method to release as much excess heat of a high temperature as possible in the secondary heat system has been developed. A case study of a Swedish kraft pulp mill showed that 5 - 6 MW excess heat with a temperature above 90 oC could be released if the system were rebuilt. Two different alternatives for using the excess heat have been investigated...|$|E
40|$|To ensure {{quality and}} {{performance}} of soft realtime embedded systems, the evaluation of their properties is needed from early phases of the design. These systems typically allow a certain rate of deadline misses. However, as they are often analysed using hard real-time techniques, which determine hard bounds of their performance properties, they are <b>overdimensioned</b> and thus expensive. Using a case study inspired by industrial practice, we present how to compose a suitable model for soft real-time systems based on the formally defined modelling language POOSL. By means of simulations for different usage scenarios, evaluation of the timing properties {{of the system is}} provided. Furthermore, we compare our results with two other performance modelling techniques, which are based on analytical computation, showing that our approach leads to a more appropriate dimensioning of soft real-time systems. ...|$|E
3000|$|This {{development}} ultimately {{can lead}} to the realisation of [...] "smart" [...] grids, where {{there is no longer a}} parallel and isolated operation of different remote metering and control applications but an integrated system basing on a common communication infrastructure that allows a well-coordinated, energy- and cost-efficient grid operation. The key challenge for power grids around the world is or will soon be to accommodate substantial amounts of fluctuating generation from distributed generators. The smart grid concept offers a solution in which the existing grid infrastructure can be used more to its limits. Online feedback from the system gives access to additional reserves resulting from original systematic <b>overdimensioning</b> due to uncertainty of the actual strain.|$|R
40|$|Network {{performance}} engineering can {{verify the}} design and dimensioning of large size control networks like CSMA based building automation networks. It combines performance analysis with diagnosis methods to evaluate the network utilization and detect design errors before installation and can therewith save the expenses of <b>overdimensioning</b> and redesign. This paper will develop a diagnosis model based on fault trees that is {{able to use the}} huge amount of performance analysis results to identify design errors and analyze their coherences. This enables not only a fast backtracking of fault causes and the derivation of solutions; it can also visualize the fault coherence to the user and help him to understand his design. 1...|$|R
40|$|Abstract—One way {{for optimum}} loading of <b>overdimensioning</b> conveyers is speed (capacity) decrement, with {{attention}} for production capabilities and demands. At conveyers which drives with three phase slip-ring induction motor, technically reasonable solution for conveyer (driving motors) speed regulation is using constant torque subsynchronous cascade with static semiconductor converter and transformer for energy reversion {{to the power}} network. In the paper is described mathematical model for parameter calculation of two-motors 6 kV subsynchronous cascade. It is also demonstrated that applying of this cascade gave several good properties, foremost in electrical energy saving, also in improving of other energy indexes, and finally that results in cost reduction of complete electrical motor drive. Keywords—Conveyer with rubber belt, electrical motor drive, sub synchronous cascade S I...|$|R
40|$|This paper {{deals with}} {{adaptive}} traffic routing in telephone networks. At present di mens i oni ng and traffi c routi ng are determi ned {{once a year}} {{in order to meet}} some predefi ned grade of servi ce requirements. For technological reasons, i. e. electromechanical switching centres, call routing is now defined according to some fixed static procedure. In order to provide protection against important di sturbances, the network has then to be <b>overdimensioned</b> at the planning stage. The emergence of stored program control networks consisting of electronic switching centres inter-connected by common channel signalling links is making possible a new strategy- adaptive traffic routing-, which allows real-time reaction to changes of the network state due to overloads or failures. Several adaptive traffic routing algo-ri thms are therefore descri bed and compared on a testbed network in order to improve network performances...|$|E
40|$|The railway, {{that brought}} the {{industrial}} expansion to the medieval walls of Prague in the 19 th century, left there a hundred years later immense, undivided lucrative spaces, that are <b>overdimensioned</b> for {{the requirements of the}} current railway transportation. The railway brownfields now offer their potential to integrate themselves into the city organism, connect the already existing housing areas and create new, multifunctional city quarters. This thesis introduces the issue of the railway brownfield transformation in both historical context, and the context of nowadays approach to spatial planning. The case study presents the brownfield of the Praha-Bubny railway station and the organisations, that are creating the image of the brownfield's future transformation, as well as their partial goals and ideas about the newly forming space. KEY WORDS: railway brownfields, Bubny railway station, transformation, urbanism, spatial plannin...|$|E
30|$|In this article, an {{algorithm}} for indoor {{path loss}} prediction at 2.4 GHz is proposed, avoiding {{the problems of}} both methods mentioned above. It {{is based on the}} calculation of the dominant path between transmitter and receiver [10]. Measurements have been performed in four buildings in Belgium for constructing and validating the model. A comparison with ray-tracing simulations is executed. The applicability to an actual wireless testbed network is investigated. Furthermore, an algorithm for the reduction of the number of access points of a network is presented. Since networks are often <b>overdimensioned,</b> especially in office environments, this algorithm could aid in reducing operating costs. Then, a network optimization algorithm is discussed. This algorithm can be of great interest to anyone who wants to set up a new WiFi or sensor network in either home or professional environments. It allows meeting a certain throughput requirement with a minimum number of transmit nodes.|$|E
40|$|Corporate journal Alcatel, [URL] audienceData traffic now {{represents}} {{the majority of}} traffic carried in transport networks. Due to the very bursty nature of users' data traffic, the aggregated traffic will be also bursty in metro networks {{as well as in}} the core. If mean data rates alone are considered in network design, the bursty nature of the traffic leads to underdimensioning the networks and could result in congestion. However, ignoring statistical multiplexing, which reduces the impact of bursts by only considering peak data rates, leads to <b>overdimensioning</b> and excessive costs. Guérin's equivalent bandwidth model takes into account the statistical multiplexing of aggregated variable traffic flows in an accurate and very simple way, enabling the realistic dimensioning and simulation of data-aware transport networks...|$|R
40|$|Today, {{backbone}} {{networks of}} telecom operators deploy {{a large number}} of devices and links. This is mainly due to both redundancy purposes for network service reliability and resource <b>overdimensioning</b> for maintaining quality of service during rush hours. Unfortunately, current network devices do not have power management primitives, and have constant energy consumption independent of their actual workloads. Starting from these considerations, we propose a viable approach to introduce and support standby modes in backbone network devices. This approach can be effectively used to almost halve the energy requirements of the whole telecom core network. Our main idea consists of periodically reconfiguring nodes and links to meet incoming traffic volumes and operational constraints of real-world networks, such as reliability, stability, quality of service, and reconvergence times. To this purpose, the approach we propose directly exploits the main features of both backbone device architectures and the network protocol stack...|$|R
40|$|Abstract Modern {{multimedia}} applications {{usually have}} real-time constraints {{and they are}} implemented using application-domain specific embedded processors. Dimensioning a system requires accurate estimations of resources needed by the applications. Overestimation leads to <b>overdimensioning.</b> For a good resource estimation, all the cases in which an application can run must be considered. To avoid {{an explosion in the}} number of different cases, those that are similar with respect to required resources are combined into, so called, application scenarios. This paper presents a methodology and a tool that can automatically detect the most important variables from an application and use them to select and dynamically predict scenarios, with respect to the necessary time budget, for soft real-time multimedia applications. The tool was tested for three multimedia applications. Using a proactive scenario-based dynamic voltage scheduler based on the scenarios and the runtime predictor generated by our tool, the energy consumption decreases with up to 19 %, while guaranteeing a frame deadline miss ratio close to zero...|$|R
40|$|This paper {{addresses}} {{the potential for}} energy reduction obtained by using dynamic line tensioning in thruster assisted position mooring systems. Traditionally, mooring systems have been designed {{in such a way}} that thruster assistance has not been neccessary under normal environmental conditions. However, as oil production moves into deeper waters, such <b>overdimensioned</b> mooring systems are no longer feasible. Thus, new "hybrid" solutions must be developed, in which increased thruster action compensates for fewer, and lighter, anchor lines. In this paper, controlling the line tensions dynamically is suggested as an additional means of station keeping, and a control law is derived based on passivity. A model consisting of a rigid-body submodel for the vessel, and a finite element submodel for the mooring system is presented and used for simulations. The simulations show the performance of the proposed control system. Keywords [...] - Ship control, position mooring systems. I. Introduction P [...] ...|$|E
40|$|Complex Application Specific Instruction-set Processors (ASIPs) {{expose to}} the {{designer}} {{a large number}} of degrees of freedom, posing the need for highly accurate and rapid simulation environments. FPGA-based emulators represent an alternative to software cycle-accurate simulators, preserving maximum accuracy and reasonable simulation times. The work presented in this paper aims at exploiting FPGA emulation within technology aware design space exploration of ASIPs. The potential speedup provided by reconfigurable logic is reduced by the overhead of RTL synthesis/implementation. This overhead can be mitigated by reducing the number of FPGA implementation processes, through the adoption of binary-level translation. Hereby we present a prototyping method that, given a set of candidate ASIP configurations, defines an <b>overdimensioned</b> ASIP architecture, capable of emulating all the design space points under evaluation. This approach is then evaluated with a design space exploration case study. Along with execution time, by coupling FPGA emulation with activity-based physical modeling, we can extract area/power/energy figure...|$|E
40|$|In {{these pages}} a General Theory of Discourse, GTD, will be described, some {{elements}} {{of which have been}} developed and used also in the IDEAL Project. Initially we were oriented towards a taxonomic approach to the construction of such a theory; however this approach allowed us only to identify a certain number of features of explanation dialogues; to use such features it became necessary to develop an alternative approach which gave account of a structure plausible to them. We focused on the relationship between explanation dialogues and discourse theory to emphasize the characteristics the former must have to be reasonably representative of the latter ones. It is obvious that a full theory of dialogic communication requires a highly structured and rich system of categories, which {{may turn out to be}} <b>overdimensioned</b> for the decription of dialogues in the field of explanation. The model presented in the following sections is, then, a reduced and domain oriented version of a more articulated theory of dialogue and discourse...|$|E
40|$|Providing {{guaranteed}} QoS, be it statistical or deterministic, necessarily requires {{allocation of}} scarce resources. This might happen on a session or on an aggregate basis, nevertheless, {{it is conceivable}} that at least at system edges scarcity of resources, exposed in the form of non-negligible (virtual) costs, will prevail to necessitate explicit allocation of resources as opposed to pure <b>overdimensioning.</b> An example of this logic is constituted by the Differentiated Services (DiffServ) architecture which is largely based on explicit bilateral Service Level Agreements (SLA) between peering providers. Often such resource allocation decisions are done on a multi-period basis because resource allocation decisions at a certain point in time may depend on earlier decisions and thus it can turn out sub-optimal to look at decisions in an isolated fashion. In earlier papers we discussed the general class of optimization problems that are applicable in these scenarios. We call the class MPRASE (Multi-Period Resource Allocation at System Edges). In this paper we present a taxonomy for all the MPRASE problems. 4...|$|R
3000|$|The {{increase}} in indoor WiFi deployments {{leads to the}} appearance of a lot of access points in (professional) environments. These access points are often placed in a more or less arbitrary way, not always leading to an optimal network layout. <b>Overdimensioning</b> the network not only increases the installation and operational costs, it also causes an increasing amount of interference and a sub-optimal use of resources. Moreover, different companies mostly have their own WiFi network, installed and operating independently from other networks. Sharing wireless network infrastructure and resources between different wireless networks could increase QoS, spectrum use efficiency, energy efficiency, [...]..., this way providing benefits for all participating networks. A part of the access points could e.g., be switched off, without affecting connectivity. This section presents an access point selection algorithm: it selects a minimal number of access points out of a larger set, while still meeting a certain throughput requirement in the rooms to be covered. The calculations are based on the path loss prediction algorithm discussed {{in the first part of}} this article.|$|R
40|$|Providing {{guaranteed}} QoS necessarily requires {{allocation of}} scarce resources. It {{is conceivable that}} at least at system edges scarcity of resources, exposed {{in the form of}} non-negligible (virtual) costs, will prevail to necessitate explicit allocation of resources as opposed to pure <b>overdimensioning.</b> An example of this logic is constituted by the Differentiated Services (DiffServ) architecture. Often such resource allocation decisions are done on a multi-period basis because resource allocation decisions at a certain point in time may depend on earlier decisions and thus it can turn out sub-optimal to look at decisions in an isolated fashion. Therefore, in this paper, we investigate a fairly large and diverse set of (network) QoS problems all of which deal with the problem of multi-period resource allocation at system edges. We devise a taxonomy for the classification of these problems and introduce a common mathematical framework under which these problems can be tackled. The ultimate goal of our work is to strive for solution techniques towards the generalized class of problems such that these are applicable in a number of scenarios which have so far not been regarded in an integrated fashion. 1...|$|R
40|$|The {{difficulty}} {{in evaluating the}} residual real risk in sites characterized by previous activities of uncontrolled MSW disposal {{can lead to the}} application of inappropriate or <b>overdimensioned</b> intervention techniques. An alternative approach can be used instead: creating systems able to accompany, and possibly accelerate, the phase of complete stabilization with active issue control systems. The present paper shows {{the results of the study}} carried out on a landfill used by the Town of Matera (Italy) from 1975 to 1990. The environmental conditions have been restored by means of a characterization plan based on geognostic knowledge, geophysical investigations and on the analysis of the waste contained in the landfill. The achieved results form the scientific base of the area recovery plan through actions of in site residual issue control. The main element of the proposed action is the creation of a bio-active coverage made up of a mixture of compost and vegetable ground able to check the gaseous substance and to reset the leaching towards the subsoil...|$|E
40|$|Research PaperThe {{report is}} an {{analysis}} of the resource rent in the Swedish fishery, produced as a contribution to the new strategic plan for the Swedish fishery in the years 2007 - 2013 according to the draft EU regulation for the EFF (European Fishery Fund). A model has been produced based on linear programming. The study originates from the situation in 2004 and with this basis different scenarios are calculated for the coming years. Special emphasis has been put on the optimal resource rent and the maximum number of employed persons within the sector. The actual current situation points to great deficits, and also to a fishery overcapacity. This is due to too many vessels sharing a resource too small. The fixed costs of the <b>overdimensioned</b> fishing fleet turn potential profits into great losses. The effects of restructuring the fishing fleet could be large. An optimal economic scenario suggests that more that 50...|$|E
40|$|Today’s Internet-scale {{computing}} systems often run {{at a low}} {{average load}} with only occasional peak performance demands. Consequently, computing resources are often <b>overdimensioned,</b> leading to high costs. While load control techniques between clients and servers can help to better utilize a given system, these techniques can place a significant communication and computation load on servers. To improve on these issues, we contribute with scalable techniques for client-request rate control, achieved through integration of (i) a scalable distributed feedback channel to transmit control information from the server to the clients with (ii) decoupling strategies that allow to constrain and filter client requests directly at the client, illustrated {{in the area of}} first-price sealed-bid online auctions, and (iii) a PID (Proportional-Integral-Derivative) controller that adaptively controls the input parameters of those decoupling strategies to facilitate an optimal server utilization. In contrast to related work, we can hence optimize server load directly at the source through rate control of the clients. Our evaluations show that this setup supports large sets of clients before the controller becomes unstable...|$|E
40|$|Real-time {{multimedia}} traffic {{requires some}} minimum delivery guarantees to be effectively transmitted over packet-switched networks. This {{is even more}} necessary when interactive sessions (e. g., Voice-over-IP applications) are involved. Effective and timely packet delivery is sometimes achieved by <b>overdimensioning</b> network capacity. However, {{this is not the}} most practical and economic solution. Instead, providing a specific service to different traffic types may achieve better results without wasting network resources. Quality of Service (QoS) be implemented at different layers of the protocol stack, but tight control at the link layer is essential when the physical medium is shared among a number of devices. Wireless LANs are a typical example of that. The IEEE 802. 11 e amendment enhances the original version of the WiFi standard with new QoS functionalities. It maintains backward compatibility with legacy hardware, and this may cause the priority scheme to be ineffective in those deployments where non-QoS enabled hardware is present. In this paper, we analyze some widespread commercial 802. 11 products to check the effectiveness of the QoS mechanisms. We show that all hardware under examination fails in providing effective QoS guarantees when legacy stations are present...|$|R
40|$|Abstract. With {{the trend}} of moving towards 10 – 20 MW turbines, rotor diameters are growing beyond {{the size of the}} largest {{turbulent}} structures in the atmospheric boundary layer. As a consequence, the fully uniform transients that are commonly used to predict extreme gust loads are losing their connection to reality and may lead to gross <b>overdimensioning.</b> More suiting would be to represent gusts by advecting air parcels and posing certain physical constraints on size and position. However, this would introduce several new degrees of freedom that significantly increase the computational burden of extreme load prediction. In an attempt to elaborate on {{the costs and benefits of}} such an approach, load calculations were done on the DTU 10 MW reference turbine where a single uniform gust shape was given various spatial dimensions with the transverse wavelength ranging up to twice the rotor diameter (357 m). The resulting loads displayed a very high spread, but remained well under the level of a uniform gust. Moving towards spatially constrained gust models would therefore yield far less conservative, though more realistic predictions at the cost of higher computation time. 1...|$|R
40|$|During the {{operation}} of {{a nuclear power plant}} many of its components, systems and structures become radioactive. After the final shutdown of a nuclear power plant is will be decommissioned, i. e. the radioactive substances will be removed from the plant. The decommissioning is a very labour-intensive project, and e. g. the cost estimate for the decommissioning of the Loviisa nuclear power plant amounts 360 M€, comprising a significant part of its nuclear waste management costs. The decommissioning comprises several consecutive work phases, such as dismantling, cutting, characterization, packaging, transport and final disposal of the components, systems and structures of the power plant. A smooth completion of the decommissioning project requires these operations to be arranged taking into account the variability in the work performance in an appropriate way. Besides the management of the work performance, availability of buffer storage space between the work phases can reduce the effect of delays in one work phase {{on the rest of the}} work phases. The thesis considers some waste types according to the decommissioning plan of the Loviisa nuclear power plant, and determines optimal buffer storage size considering the cost of storage and the costs resulting from a deficiency of buffer storage. The variation in the work performance is determined based on literature from comparable applications, and an optimization model is constructed using queueing theory to describe the material flow. The optimization model results in an optimal buffer storage size and the related costs. Several sensitivity analysis cases with regard to model parameters are included in the analysis, and they indicate the material flow variation being one of the dominant factors to determine the optimal buffer storage size. The sensitivity of the result to storage and delay cost parameters is lower. The recommended buffer storage space represents 15 % of the total waste volume focused on in the thesis. One of the key findings is that in such a labour-intensive project as the decommissioning, underdimensioning of the buffer storage may result in significantly higher costs than <b>overdimensioning.</b> Therefore a slight <b>overdimensioning</b> is recommended due to uncertainties in the work variability data. Furthermore, the analysis carried out in the thesis reveals the need of a management system which is able to respond fast to anomalies in the work performance as well as an enterprise resource planning system which meets the requirements for workforce allocation and material flow bookkeeping, including the special requirements relating to radioactive materials. Based on the analysis, the thesis gives recommendations for further decommissioning planning as well as for the execution of the decommissioning project...|$|R
