111|68|Public
25|$|In free-operant {{avoidance}} {{a subject}} periodically receives an aversive stimulus (often an electric shock) unless an <b>operant</b> <b>response</b> is made; the response delays {{the onset of}} the shock. In this situation, unlike discriminated avoidance, no prior stimulus signals the shock. Two crucial time intervals determine the rate of avoidance learning. This first is the S-S (shock-shock) interval. This is time between successive shocks {{in the absence of a}} response. The second interval is the R-S (response-shock) interval. This specifies the time by which an <b>operant</b> <b>response</b> delays {{the onset of the}} next shock. Note that each time the subject performs the <b>operant</b> <b>response,</b> the R-S interval without shock begins anew.|$|E
2500|$|A {{discriminated}} avoidance experiment {{involves a}} series of trials in which a neutral stimulus such as a light is followed by an aversive stimulus such as a shock. After the neutral stimulus appears an <b>operant</b> <b>response</b> such as a lever press prevents or terminate the aversive stimulus. In early trials the subject {{does not make the}} response until the aversive stimulus has come on, so these early trials are called [...] "escape" [...] trials. As learning progresses, the subject begins to respond during the neutral stimulus and thus prevents the aversive stimulus from occurring. Such trials are called [...] "avoidance trials." [...] This experiment is said to involve classical conditioning, because a neutral CS is paired with an aversive US; this idea underlies the two-factor theory of avoidance learning described below.|$|E
2500|$|... a) Classical {{conditioning}} of fear. Initially {{the organism}} experiences the pairing of a CS (conditioned stimulus) with an aversive US (unconditioned stimulus). The theory assumes that this pairing creates {{an association between}} the CS and the US through classical conditioning and, because of the aversive nature of the US, the CS comes to elicit a conditioned emotional reaction (CER) – [...] "fear." [...] b) Reinforcement of the <b>operant</b> <b>response</b> by fear-reduction. As {{a result of the}} first process, the CS now signals fear; this unpleasant emotional reaction serves to motivate operant responses, and responses that terminate the CS are reinforced by fear termination. Note that the theory does not say that the organism [...] "avoids" [...] the US in the sense of anticipating it, but rather that the organism [...] "escapes" [...] an aversive internal state that is caused by the CS.|$|E
50|$|Operant {{studies using}} {{vertebrates}} {{have been conducted}} for many years. In such studies, an animal operates or changes {{some part of the}} environment to gain a positive reinforcement or avoid a negative one. In this way, animals learn from the consequence of their own actions, i.e. they use an internal predictor. <b>Operant</b> <b>responses</b> indicate a voluntary act; the animal exerts control over the frequency or intensity of its responses, making these distinct from reflexes and complex fixed action patterns. A number of studies have revealed surprising similarities between vertebrates and invertebrates in their capacity to use <b>operant</b> <b>responses</b> to gain positive reinforcements, but also to avoid negative reinforcement that in vertebrates would be described as 'pain'.|$|R
5000|$|The {{idea that}} {{behavior}} is strengthened or weakened by its consequences raises several questions. Among {{the most important}} are these: (1) <b>Operant</b> <b>responses</b> are strengthened by reinforcement, but where do they come from in the first place? (2) Once {{it is in the}} organism's repertoire, how is a response directed or controlled? (3) How can very complex and seemingly novel behaviors be explained? ...|$|R
40|$|The horse (Equus caballus) {{provides}} a useful model where the study of repetitious behaviour is concerned as they perform three distinct stereotypies including one oral (crib-biting) and two locomotor equivalents (weaving and box-walking). Whilst several preliminary investigations have been performed into the neuro-aetiology of crib-biting, no studies to date have sought to elucidate the brain mechanisms underlying locomotor stereotypy in this species. As such, the primary aim {{of this investigation was}} to probe the neural basis of locomotor stereotypy (weaving) and extend current knowledge with regards to the crib-biting response. In this regard, behavioural probes have proved useful in identifying altered striatal functioning in a number of species without the use of invasive methods. Consequently spontaneous blink rate (SBR), behavioural initiation and an extinction-devaluation paradigm were conducted on a sample of crib-biting (n= 8), weaving (n= 8) and control (n= 8) horses to investigate striatal output patterns. Crib-biting horses demonstrated significantly lower SBR when compared to the control (p< 0. 05) and the weaving (p< 0. 01) animals. Behaviour initiation was significantly increased for the crib-biting (p< 0. 01) and the weaving (p< 0. 05) horses when compared to control equivalents. During the extinction paradigm, the control horses required significantly more trials to reach learning criterion when compared to both crib-biting (p< 0. 001) and weaving (p< 0. 001) animals. The crib-biting horses performed significantly more <b>operant</b> <b>responses</b> during extinction 1 and extinction 2 compared to weaving (p< 0. 001 and p< 0. 01 respectively) and control horses (p< 0. 001 and p< 0. 001 respectively). The crib-biting sample conducted significantly more <b>operant</b> <b>responses</b> during extinction 1 when compared to extinction 2 (p< 0. 005), though no difference was observed for the control or weaving group. Finally, crib-biting horses required significantly more trials to reach total extinction criterion when compared to their control (p< 0. 001) and weaving (p< 0. 01) equivalents. This data suggests that there is an initial acceleration of ventral-dorsal activity within the striatum of crib-biting horses. However, the significant reduction of <b>operant</b> <b>responses</b> during extinction 2 compared to extinction 1 is indicative of a return to action outcome monitoring {{in the final stages of}} the extinction experiment. It is possible that this reduction of <b>operant</b> <b>responses</b> in extinction 2 is resultant of dopamine receptor saturation following devaluation. As the number of <b>operant</b> <b>responses</b> during extinction 2 is significantly higher for the crib-biting horses, the crib-biting horse is therefore responding habitually during extinction 2 in response to the conditioned stimulus as motivation in terms of reward acquisition has ceased. On the other hand the weaving horses did not transit towards stimulus-response learning at any stage of the extinction paradigm. Rather the weaving horse data suggests enhanced motivation as a result of increased phasic dopamine release highlighted by significantly reduced trials to attain learning criterion compared to control animals...|$|R
2500|$|Operant {{conditioning}} uses several {{consequences to}} modify a voluntary behavior. Recent studies by Rabin et al. {{have examined the}} ability of rats to perform an operant order to obtain food reinforcement using an ascending fixed ratio (FR) schedule. They found that 56Fe-ion doses that are above 2 Gy affect the appropriate responses of rats to increasing work requirements. [...] NCRP Report No. 153 [...] notes that [...] "The disruption of <b>operant</b> <b>response</b> in rats was tested 5 and 8 months after exposure, but maintaining the rats on a diet containing strawberry, but not blueberry, extract were shown to prevent the disruption. When tested 13 and 18 months after irradiation, {{there were no differences}} in performance between the irradiated rats maintained on control, strawberry or blueberry diets. [...] These observations suggest that the beneficial effects of antioxidant diets may be age dependent." ...|$|E
50|$|In free-operant {{avoidance}} {{a subject}} periodically receives an aversive stimulus (often an electric shock) unless an <b>operant</b> <b>response</b> is made; the response delays {{the onset of}} the shock. In this situation, unlike discriminated avoidance, no prior stimulus signals the shock. Two crucial time intervals determine the rate of avoidance learning. This first is the S-S (shock-shock) interval. This is time between successive shocks {{in the absence of a}} response. The second interval is the R-S (response-shock) interval. This specifies the time by which an <b>operant</b> <b>response</b> delays {{the onset of the}} next shock. Note that each time the subject performs the <b>operant</b> <b>response,</b> the R-S interval without shock begins anew.|$|E
5000|$|... #Caption: Figure 6-5.jpg High-LET {{radiation}} effects on <b>operant</b> <b>response.</b> This figure shows {{the relationship between}} the exposure to different energies of 56Fe and 28Si particles and the threshold dose for the disruption of performance on a food-reinforced <b>operant</b> <b>response.</b> Only a single energy of 48Ti particles was tested. The threshold dose (cGy) for the disruption of the response is plotted against particle LET (keV/μm).|$|E
2500|$|The {{idea that}} {{behavior}} is strengthened or weakened by its consequences raises several questions. [...] Among {{the most important}} are these: (1) <b>Operant</b> <b>responses</b> are strengthened by reinforcement, but where do they come from in the first place? [...] (2) Once {{it is in the}} organism's repertoire, how is a response directed or controlled? [...] (3) How can very complex and seemingly novel behaviors be explained? ...|$|R
40|$|<b>Operant</b> <b>responses</b> {{of human}} {{subjects}} were conditioned {{according to a}} variable-interval schedule of positive reinforcement. A brief noise was delivered as punishment {{for each of the}} responses. The noise suppressed the punished responses more when an alternative unpunished response was concurrently available than when only a single punished response was available. This finding extends the generality of a previous study that had used a period of extinction rather than the brief noise as the punishing stimulus...|$|R
5000|$|Shaping {{is used in}} {{training}} <b>operant</b> <b>responses</b> in lab animals, and in applied behavior analysis to change human or animal behaviors considered to be maladaptive or dysfunctional. It also {{plays an important role}} in commercial animal training. Shaping assists in [...] "discrimination", which is the ability to tell the difference between stimuli that are and are not reinforced, and in [...] "generalization", which is the application of a response learned in one situation to a different but similar situation.|$|R
50|$|Specific PIT {{and general}} PIT also occur with {{aversive}} stimuli and are defined analogously. Specific PIT with an aversive stimulus {{occurs when a}} CS is paired with an aversive stimulus and subsequent exposure to the CS enhances an <b>operant</b> <b>response</b> that is directed away from the aversive stimulus with which it was paired (i.e., it promotes escape and avoidance behavior). General PIT with an aversive stimulus occurs when a CS is paired with one aversive stimulus and it enhances an <b>operant</b> <b>response</b> that is directed away from a different aversive stimulus.|$|E
5000|$|Operant {{conditioning}} technique: infants {{are placed}} in a crib and a ribbon that is connected to a mobile overhead is tied {{to one of their}} feet. Infants notice that when they kick their foot the mobile moves - the rate of kicking increases dramatically within minutes. Studies using this technique have revealed that infants' memory substantially improves over the first 18-months. Whereas 2- to 3-month-olds can retain an <b>operant</b> <b>response</b> (such as activating the mobile by kicking their foot) for a week, 6-month-olds can retain it for two weeks, and 18-month-olds can retain a similar <b>operant</b> <b>response</b> for as long as 13 weeks.|$|E
5000|$|Consequences can {{consist of}} {{reinforcing}} stimuli (S) or punishing stimuli (S) which follow and modify an <b>operant</b> <b>response.</b> Reinforcing stimuli are often classified as positively (S) or negatively reinforcing (S). Reinforcement may {{be governed by}} a schedule of reinforcement, that is, a rule that specifies when or how often a response is reinforced. (See operant conditioning).|$|E
5000|$|Extinction {{involves}} the discontinuation {{of a particular}} reinforcer in <b>response</b> to <b>operant</b> behavior, such as replacing a reinforcing drug infusion with a saline vehicle. When the reinforcing element of the operant paradigm is no longer present, a gradual reduction in <b>operant</b> <b>responses</b> results in eventual cessation or “extinction” of the operant behavior.Reinstatement is the restoration of operant behavior to acquire a reinforcer, often triggered by external events/cues or exposure to the original reinforcer itself. Reinstatement can be broken into a few broad categories: ...|$|R
40|$|We {{examined}} the effects of teaching overt precurrent behaviors on the current operant of solving multiplication and division word problems. Two students were taught four precurrent behaviors (identification of label, operation, larger numbers, and smaller numbers) in a different order, {{in the context of a}} multiple baseline design. After meeting criterion on three of the four precurrent skills, the students demonstrated the current operant of correct problem solutions. These skills generalized to novel problems. Correct current <b>operant</b> <b>responses</b> (solutions that matched answers revealed by coloring over the space with a special marker) maintained the precurrent behaviors in the absence of any other programmed reinforcement...|$|R
40|$|The {{present study}} was {{designed}} to explore the relationship between the cannabinoid and opioid receptors in animal models of opioid-induced reinforcement. The acute administration of SR 141716 A, a selective central cannabinoid CB 1 receptor antagonist, blocked heroin self-administration in rats, as well as morphine-induced place preference and morphine selfadministration in mice. Morphine-dependent animals injected with SR 141716 A exhibited a partial opiate-like withdrawal syndrome that had limited consequences on <b>operant</b> <b>responses</b> for food and induced place aversion. These effects were associated with morphine-induced changes in the expression of CB 1 receptor mRNA in specific nuclei of the reward circuit, including dorsal caudate putamen, nucleus accumbens, and septum. Additionally, the opioid antagonist naloxone precipitated a mild cannabinoid-like withdrawal syndrome in cannabinoiddependen...|$|R
5000|$|In {{relation}} to rewarding stimuli, specific PIT {{occurs when a}} CS {{is associated with a}} specific rewarding stimulus through classical conditioning and subsequent exposure to the CS enhances an <b>operant</b> <b>response</b> that is directed toward the same reward with which it was paired (i.e., it promotes approach behavior). General PIT occurs when a CS is paired with one reward and it enhances an <b>operant</b> <b>response</b> that is directed toward a different rewarding stimulus. Neurobiological state factors (e.g., appetite and satiety states, stress level, drug states such as intoxication and withdrawal, etc.), and particularly the motivational state of an animal, strongly affect the amount of appetitive motivational salience (i.e., incentive salience) that a reward cue confers to an associated rewarding stimulus via Pavlovian-instrumental transfer. [...] Acute stress amplifies the motivational salience that reward cues confer to rewarding stimuli through both specific and general PIT; however, chronic stress reduces the motivational impact reward cues.|$|E
50|$|Estes and {{his mentor}} B.F. Skinner {{presented}} {{their analysis of}} anxiety, introducing the conditioned emotional response (CER)/conditioned fear response (CFR) paradigm, where rats were trained to respond on an operant schedule that produced a steady response rate, after which they were tested with an electric shock stimulus that was conditioned as a fear signal. The fear signal suppressed the <b>operant</b> <b>response,</b> and the magnitude of suppression {{was used as a}} mesure of anxiety. The CER/CFR became widely used to study Pavlovian conditioning in a variety of organisms.|$|E
50|$|Studies {{using an}} operant {{framework}} {{have indicated that}} humans can influence the behavior of dogs through food, petting and voice. Food and 20-30 seconds of petting maintained operant responding in dogs. Some dogs will show a preference for petting once food is readily available, and dogs will remain in proximity to a person providing petting and show no satiation to that stimulus. Petting alone was sufficient to maintain the <b>operant</b> <b>response</b> of military dogs to voice commands, and responses to basic obedience commands in all dogs increased when only vocal praise was provided for correct responses.|$|E
40|$|SummaryLearning and {{motivation}} are integral in shaping an organism's adaptive behavior. The dopamine {{system has been}} implicated in both processes; however, dissociating the two, both experimentally and conceptually, has posed significant challenges. We have developed an animal model that dissociates expression or scaling of a learned behavior from learning itself. An inducible dopamine transporter (DAT) knockdown mouse line has been generated, which exhibits significantly slower reuptake of released dopamine and increased tonic firing of dopamine neurons without altering phasic burst firing. Mice were trained in experimental tasks prior to inducing a hyperdopaminergic tone and then retested. Elevated dopamine enhanced performance in goal-directed <b>operant</b> <b>responses.</b> These data demonstrate that alterations in dopaminergic tone can scale {{the performance of a}} previously learned behavior in the absence of new learning...|$|R
40|$|Integrative Comp. PhysioZ. 41) : R 682 -R 690, 1997. -Rats {{subjected}} to total sleep deprivation (TSD) by the disk-over-water method {{were provided with}} a continuously available operant by which they could increase ambient temperature (Tamb). TSD rats progressively increased <b>operant</b> <b>responses</b> for heat to 700 % of baseline levels. During {{the last quarter of}} sleep deprivation, they maintained mean Tamb at 9 °C above baseline and held Tamb over 40 °C for 35 % of the day. In contrast, yoked control rats (TSC) did not change mean Tamb. Although both TSD and TSC rats showed a progressive decline in intraperitoneal temperature (Tip), TSD rats main-tained an elevated Tamb even during periods when Tip and brain temperatures (Tbr) were above baseline levels. Thus these results confirm and extend earlier findings that rats {{subjected to}} TSD show an increase in temperature set poin...|$|R
40|$|Self-stimulatory {{behavior}} is repetitive, stereotyped, functionally autonomous behavior seen in both normal and developmentally disabled populations, yet no satisfactory theory of its development and major characteristics {{has previously been}} offered. We present here a detailed hypothesis of the acquisition and maintenance of self-stimulatory behavior, proposing that the behaviors are <b>operant</b> <b>responses</b> whose reinforcers are automatically produced interoceptive and exteroceptive perceptual consequences. The concept of perceptual stimuli and reinforcers, the durability of self-stimulatory behaviors, the sensory extinction effect, the inverse relationship between self-stimulatory and other behaviors, the blocking effect of self-stimulatory behavior on new learning, and response substitution effects are {{discussed in terms of}} the hypothesis. Support for the hypothesis from the areas of sensory reinforcement and sensory deprivation is also reviewed. Limitations of major alternative theories are discussed, along with implications of the perceptual reinforcement hypothesis for the treatment of excessive self-stimulatory behavior and for theoretical conceptualizations of functionally related normal and pathological behaviors...|$|R
50|$|Continuous reinforcement: A single <b>operant</b> <b>response</b> {{triggers}} the dispense {{of a single}} dose of reinforcer. A time-out period may follow each <b>operant</b> <b>response</b> that successfully yields a dose of reinforcer; during this period the lever used in training may be retracted preventing the animal from making further responses. Alternatively operant responses will fail to produce drug administration allowing previous injections to take effect. Moreover, time-outs also help prevent subjects from overdosing during self-administration experiments. Fixed-ratio studies require a predefined number of operant responses to dispense one unit of reinforcer. Standard fixed ratio reinforcement schedules include FR5 and FR10, requiring 5 and 10 operant responses to dispense a unit of reinforcer, respectively.Progressive ratio reinforcement schedules utilize a multiplicative {{increase in the number}} of operant responses required to dispense a unit of reinforcer. For example, successive trials might require 5 operant responses per unit of reward, then 10 responses per unit of reward, then 15, and so on. The number of operant responses required per unit of reinforcer may be altered after each trial, each session, or any other time period as defined by the experimenter. Progressive ratio reinforcement schedules provide information about the extent that a pharmacological agent is reinforcing through the breakpoint. The breakpoint is the number of operant responses at which the subject ceases engaging in self-administration, defined by some period of time between operant responses (generally up to an hour).Fixed interval (FI) schedules require that a set amount of time pass between drug infusions, regardless of the number of times that the desired response is performed. This “refractory” period can prevent the animal from overdosing on a drug.Variable interval (VI) schedules of reinforcement are identical to FI schedules, except that the amount of time between reinforced operant responses varies, making it more difficult for the animal to predict when the drug will be delivered.|$|E
5000|$|A {{discriminated}} avoidance experiment {{involves a}} series of trials in which a neutral stimulus such as a light is followed by an aversive stimulus such as a shock. After the neutral stimulus appears an <b>operant</b> <b>response</b> such as a lever press prevents or terminate the aversive stimulus. In early trials the subject {{does not make the}} response until the aversive stimulus has come on, so these early trials are called [...] "escape" [...] trials. As learning progresses, the subject begins to respond during the neutral stimulus and thus prevents the aversive stimulus from occurring. Such trials are called [...] "avoidance trials." [...] This experiment is said to involve classical conditioning, because a neutral CS is paired with an aversive US; this idea underlies the two-factor theory of avoidance learning described below.|$|E
5000|$|Operant {{conditioning}} uses several {{consequences to}} modify a voluntary behavior. Recent studies by Rabin et al. {{have examined the}} ability of rats to perform an operant order to obtain food reinforcement using an ascending fixed ratio (FR) schedule. They found that 56Fe-ion doses that are above 2 Gy affect the appropriate responses of rats to increasing work requirements. NCRP Report No. 153 [...] notes that [...] "The disruption of <b>operant</b> <b>response</b> in rats was tested 5 and 8 months after exposure, but maintaining the rats on a diet containing strawberry, but not blueberry, extract were shown to prevent the disruption. When tested 13 and 18 months after irradiation, {{there were no differences}} in performance between the irradiated rats maintained on control, strawberry or blueberry diets. These observations suggest that the beneficial effects of antioxidant diets may be age dependent." ...|$|E
40|$|OBJECTIVE: To {{determine}} whether indorenate, a serotonin-receptor agonist, can exert discriminative control over <b>operant</b> <b>responses,</b> {{to establish the}} temporal course of discriminative control and to compare its stimulus properties to a (5 -HT) IA receptor agonist. [3 H]- 8 -hydroxy- 2 -(di-N-propylamino) tetralin (8 -OH-DPAT). DESIGN: Prospective animal study. ANIMALS: Ten male Wistar rats. INTERVENTIONS: Rats were trained to press either of 2 levers for sucrose solution according to a fixed ratio schedule, which was gradually increased. Rats were given injections of either indorenate or saline solution during discrimination training. Once they had achieved an 83 % accuracy rate, rats underwent generalization tests after having received a different dose of indorenate, the training dose of indorenate at various intervals before the test, various doses of 8 -OH-DPT, or NAN- 190 administered before indorenate or 8 -OH-DPAT. OUTCOME MEASURES: Distribution of responses between the 2 levers before the first reinforcer of the session, response rate for all the responses in the session, and a discrimination index that expressed the drug-appropriate responses {{as a proportion of}} the total responses. RESULTS: Indorenate administration resulted in discriminative control over <b>operant</b> <b>responses,</b> maintained at fixed ratio 10, at a dose of 10. 0 mg/kg (but not 3. 0 mg/kg). When the interval between the administration of indorenate and the start of the session was varied, the time course of its cue properties followed that of its described effects on 5 -HT turnover. In generalization tests, the discrimination index was a function of the dose of indorenate employed; moreover, administration of 8 -OH-DPAT (from 0. 1 to 1. 0 mg/kg) fully mimicked the stimulus properties of indorenate in a dose-dependent way. The (5 -HT) IA antagonist NAN- 190 prevented the stimulus generalization from indorenate to 8 -OH-DPAT. Also, NAN- 190 antagonized the stimulus control of indorenate when administered 45 minutes before the session, but not when administered 105 minutes before the session (i. e., 15 minutes before the administration of indorenate). CONCLUSION: (5 -HT) IA receptors are of relevance to the stimulus function of indorenate. However, other receptor subtypes may also be involved. Hence, other agonists and specific antagonists should be studied before definite conclusions are drawn...|$|R
40|$|<b>Operant</b> <b>responses</b> of 16 {{children}} (mean age 6 {{years and}} 1 month) were reinforced according to different fixed-interval schedules (with interreinforcer intervals of 20, 30, or 40 s) {{in which the}} reinforcers were either 20 -s or 40 -s presentations of a cartoon. In another procedure, they received training on a self-control paradigm in which both reinforcer delay (0. 5 s or 40 s) and reinforcer duration (20 s or 40 s of cartoons) varied, and subjects were offered a choice between various combinations of delay and duration. Individual differences in behavior under the self-control procedure were precisely mirrored by individual differences under the fixed-interval schedule. Children who chose the smaller immediate reinforcer on the self-control procedure (impulsive) produced short postreinforcement pauses and high response rates in the fixed-interval conditions, and both measures changed little with changes in fixed-interval value. Conversely, children who chose the larger delayed reinforcer in the self-control condition (the self-controlled subjects) exhibited lower response rates and long postreinforcement pauses, which changed systematically with changes in the interval, in their fixed-interval performances...|$|R
40|$|Some {{studies have}} found that {{extinction}} leaves response structures unaltered; others have found that response variability is increased. Responding by Long-Evans rats was extinguished after 3 schedules. In one, reinforcement depended on repetitions of a particular response sequence across 3 operanda. In another, sequences were reinforced only if they varied. In the third, reinforcement was yoked: not contingent upon repetitions or variations. In all cases, rare sequences increased during extinction— variability increased—but the ordering of sequence probabilities was generally unchanged, the most common sequences during reinforcement continuing to be most frequent in extinction. The rats' combination of generally doing what worked before but occasionally doing something very different may maximize the possibility of reinforcement from a previously bountiful source while providing necessary variations for new learning. The main goal of the present study is to reconcile two competing accounts of extinction. One is that variability increases during extinction of <b>operant</b> <b>responses.</b> Another is that operant structures, manifest in response topographies, sequences, and distributions, are unaffected by extinction and remain intact. How can structure...|$|R
50|$|This {{essentially}} philosophical position gained {{strength from}} {{the success of}} Skinner's early experimental work with rats and pigeons, summarized in his books The Behavior of Organisms and Schedules of Reinforcement. Of particular importance was his concept of the <b>operant</b> <b>response,</b> of which the canonical example was the rat's lever-press. In contrast {{with the idea of}} a physiological or reflex response, an operant is a class of structurally distinct but functionally equivalent responses. For example, while a rat might press a lever with its left paw or its right paw or its tail, all of these responses operate on the world in the same way and have a common consequence. Operants are often thought of as species of responses, where the individuals differ but the class coheres in its function-shared consequences with operants and reproductive success with species. This is a clear distinction between Skinner's theory and S-R theory.|$|E
50|$|Extinction {{refers to}} the loss of {{performance}} after a conditioned stimulus is no longer paired with an unconditioned stimulus. It can also refer {{to the loss of}} an <b>operant</b> <b>response</b> when it is no longer reinforced.Research done by Bouton (2002) has shown that extinction is not an example of unlearning, but a new type of learning where the performance of the individual depends on the context. The renewal effect is seen when a participant is first conditioned in a context (context A) and then shows extinction in another context (B). Returning to context A may renew the conditioned response. This evidence demonstrates that appropriate responses underlying extinction may be linked to contextual information. Hence, someone who is in the context in which they initially learned the material is likely to be cued to act as they were initially conditioned to act. If they are in the extinction context, then that context will likely prompt them not to respond. After the extinction of learned fear, Maren and colleagues have shown that the context-dependence of extinction is mediated by hippocampal and prefrontal cortical neurons projecting to the amygdala.|$|E
5000|$|In the operant {{conditioning}} paradigm, extinction {{refers to the}} process of no longer providing the reinforcement that has been maintaining a behavior. Operant extinction differs from forgetting in that the latter refers to a decrease in the strength of a behavior over time when it has not been emitted. For example, a child who climbs under his desk, a response which has been reinforced by attention, is subsequently ignored until the attention-seeking behavior no longer occurs. In his autobiography, B.F. Skinner noted how he accidentally discovered the extinction of an <b>operant</b> <b>response</b> due to the malfunction of his laboratory equipment: My first extinction curve showed up by accident. A rat was pressing the lever in an experiment on satiation when the pellet dispenser jammed. I was not there at the time, and when I returned I found a beautiful curve. The rat had gone on pressing although no pellets were received. ... The change was more orderly than the extinction of a salivary reflex in Pavlov's setting, and I was terribly excited. It was a Friday afternoon {{and there was no one}} in the laboratory who I could tell. All that weekend I crossed streets with particular care and avoided all unnecessary risks to protect my discovery from loss through my accidental death.|$|E
40|$|Previously, we {{demonstrated}} that {{the action of the}} natural alkaloid, ibogaine, to reduce alcohol (ethanol) consumption is mediated by the glial cell line-derived neurotrophic factor (GDNF) in the ventral tegmental area (VTA). Here we set out to test the actions of GDNF in the VTA on ethanol-drinking behaviors. We found that GDNF infusion very rapidly and dose-dependently reduced rat ethanol, but not sucrose, operant self-administration. A GDNF-mediated decrease in ethanol consumption was also observed in rats with a history of high voluntary ethanol intake. We found that the action of GDNF on ethanol consumption was specific to the VTA as infusion of the growth factor into the neighboring substantia nigra did not affect <b>operant</b> <b>responses</b> for ethanol. We further show that intra-VTA GDNF administration rapidly activated the MAPK signaling pathway in the VTA and that inhibition of the MAPK pathway in the VTA blocked the reduction of ethanol self-administration by GDNF. Importantly, we demonstrate that GDNF infused into the VTA alters rats' responses in a model of relapse. Specifically, GDNF application blocked reacquisition of ethanol self-administration after extinction. Together, these results suggest that GDNF, via activation of the MAPK pathway, is a fast-acting selective agent to reduce the motivation to consume and seek alcohol...|$|R
40|$|Skinner (1957) attested {{that the}} {{acquisition}} of one type of verbal operant will not necessarily occasion the emergence of another type of verbal response topography. In contrast, several {{studies have shown that}} multiple exemplar training (MET) is a mechanism that can facilitate the emergence of untrained operants, and it has been considered a powerful tool for establishing generalized <b>operant</b> <b>responses</b> also known as derived relational responses in the language of Relational Frame Theory (RFT). Using a multiple probe design across participants, the current study evaluated the effects of two training protocols in the emergence of untaught intraverbal responses (listing and vocal spelling of words). In Experiment 1, four participants diagnosed with intellectual disability were trained in taking dictation responses and tested for the emergence of intraverbal responses in the form of vocal spelling of words. In Experiment 2, three out of the four participants were trained to relate three sets of three synonyms each using a conditional discrimination training. The results demonstrated that the training procedures used during both experiments were effective in occasioning the emergence of untrained intraverbal responses. It was suggested that participants should have had a history of MET through the course of their academic life which facilitated the emergence of different intraverbal responses in this study...|$|R
40|$|The key pecking of pigeons {{maintained}} on a variable-interval {{schedule of}} food reinforcement was suppressed during occasional presentations of a warning stimulus paired with electric shock. On alternate sessions, a co-actor pigeon was visible in an adjoining chamber where it emitted the same food-reinforced key peck during the warning stimulus that signalled shock for the subject. With no shock and at low shock intensities, where the subject's responding was not suppressed or suppressed only slightly, the co-actor had little effect. At the higher shock intensities, where the subject's responding was reduced {{by at least}} 40 %, the response rate during the warning stimulus was consistently higher when the co-actor was present. One explanation of these results assumes a special relationship between social stimuli and aversive stimuli in which the presence of another animal reduces emotional reactions and thereby allows <b>operant</b> <b>responses</b> to increase. This {{was not the case}} here because the mere presence of the co-actor did not maintain social facilitation. Rather, the present results, taken in conjunction with previous findings, suggest that changes in social and non-social variables which affect the rate of food-reinforced responding may produce proportionately larger changes in responding when that responding is suppressed by aversive stimulation than when it is not...|$|R
