3|2421|Public
5000|$|However, {{most modern}} devices will report write {{operations}} as complete once {{the data is}} stored in its <b>onboard</b> <b>cache</b> <b>memory,</b> before the data is written to the (slow) magnetic storage. This allows commands {{to be sent to}} the other device on the cable, reducing the impact of the [...] "one operation at a time" [...] limit.|$|E
50|$|During this initialization, writable memory {{may not be}} available, so all {{computations}} have to {{be performed}} within the processor registers. For this reason, first stage boot loaders tend to be written in assembler language and only do the minimum to provide a normal execution environment for the next program. Some processors either embed a small amount of SRAM in the chip itself, or allow using the <b>onboard</b> <b>cache</b> <b>memory</b> as RAM, to make this first stage boot loader easier to write using high-level language.|$|E
50|$|The PowerBook 500 {{series was}} {{released}} as Apple was already moving its desktop machines to the PowerPC processor range, and a future upgrade was promised from the start. This came in 1995, as an Apple Motherboard containing a 100 MHz 603e processor and 8 MB of RAM (which snapped {{into a slot}} containing the previous 25 or 33 MHz 68040 processor and the 4 MB of RAM on the previous daughterboard). At the same time Newer Technology offered an Apple-authorized 117 MHz Motherboard, which was more popular than the Apple product, and optionally came without any RAM. The company later offered 167 MHz and 183 MHz upgrades containing more memory and <b>onboard</b> <b>cache</b> <b>memory</b> to improve performance. However, the internal architecture of the 500 series meant that the speed increase provided by the 100 and 117 MHz upgrades was, for most users, relatively small.|$|E
40|$|We {{describe}} <b>cache</b> <b>memory</b> design {{suitable for}} use in FPGA-based cache controller and processor. Cache systems are on-chip memory element used to store data. Cache serves as a buffer between a CPU and its main <b>memory.</b> <b>Cache</b> <b>memory</b> is used to synchronize the data transfer rate between CPU and main <b>memory.</b> As <b>cache</b> <b>memory</b> closer to the micro processor, it is faster than the RAM and main memory. The advantage of storing data on cache, as compared to RAM, {{is that it has}} faster retrieval times, but it has disadvantage of on-chip energy consumption. In term of detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption, The efficient <b>cache</b> <b>memory</b> will proposed by this research work, by implementation of <b>cache</b> <b>memory</b> on FPGA. We believe that our implementation achieves low complexity and low energy consumption in terms of FPGA resource usage. present in <b>cache</b> <b>memory</b> then the term is called „cache hit‟. The advantage of storing data on cache, as compared to RAM, is that it has faster retrieval times, but it has disadvantage of on-chip energy consumption. This paper deals with the design of efficient <b>cache</b> <b>memory</b> for detecting miss rate in <b>cache</b> <b>memory</b> and less power consumption. This <b>cache</b> <b>memory</b> may used in future work to design FPGA based cache controller...|$|R
5000|$|<b>Cache</b> <b>Memory</b> - Number of cache modules 1-32, Module {{capacity}} 8 or 16GB, Maximum <b>cache</b> <b>memory</b> 512GB ...|$|R
40|$|The {{purpose of}} this study is to explore the {{relationship}} between hit ratio of <b>cache</b> <b>memory</b> and design parameters. <b>Cache</b> <b>memories</b> are widely used in the design of computer system architectures to match relatively slow memories against fast CPUs. Caches hold the active segments of a program which are currently in use. Since instructions and data in <b>cache</b> <b>memories</b> can be referenced much faster than the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit the execution rate of the machine to be substantially increased. In order to function effectively, <b>cache</b> <b>memories</b> must be carefully designed and implemented. In this study, a trace-driven simulation study of direct mapped, associative mapped and set-associative mapped <b>cache</b> <b>memories</b> is made. In the simulation, cache fetch algorithm, placement policy, cache size and various parameters related to cache design and the resulting effect on system performance is investigated. The <b>cache</b> <b>memories</b> are simulated using the C language and the simulation results are analyzed for the design and implementation of <b>cache</b> <b>memories.</b> Department of Physics and AstronomyThesis (M. S. ...|$|R
40|$|In multitask, {{preemptive}} real-time systems, {{the use of}} <b>cache</b> <b>memories</b> make difficult {{the estimation}} of the response time of tasks, due to the dynamic, adaptive and nonpredictable behaviour of <b>cache</b> <b>memories.</b> But many embedded and critical applications need the increase of performance provided by <b>cache</b> <b>memories...</b>|$|R
40|$|<b>Cache</b> <b>memories</b> {{are used}} in modern, medium and {{high-speed}} CPUs to hold temporarily those portions {{of the contents of}} main memory which are {believed to be) currently in use. Since instructions and data in <b>cache</b> <b>memories</b> can usually be referenced in 10 to 25 percent of the time required to access main <b>memory,</b> <b>cache</b> <b>memories</b> permit th...|$|R
40|$|Degraded K-user {{broadcast}} channels (BC) are studied when receivers are facilitated with <b>cache</b> <b>memories.</b> Lower {{and upper}} bounds are derived on the capacity-memory tradeoff, i. e., on the largest rate of reliable communication over the BC {{as a function}} of the receivers' cache sizes, and the bounds are shown to match for some special cases. The lower bounds are achieved by two new coding schemes that benefit from non-uniform cache assignment. Lower and upper bounds are also established on the global capacity-memory tradeoff, i. e., on the largest capacity-memory tradeoff that can be attained by optimizing the receivers' cache sizes subject to a total <b>cache</b> <b>memory</b> budget. The bounds coincide when the total <b>cache</b> <b>memory</b> budget is sufficiently small or sufficiently large, characterized in terms of the BC statistics. For small <b>cache</b> <b>memories,</b> it is optimal to assign all the <b>cache</b> <b>memory</b> to the weakest receiver. In this regime, the global capacity-memory tradeoff grows as the total <b>cache</b> <b>memory</b> budget divided by the number of files in the system. In other words, a perfect global caching gain is achievable in this regime and the performance corresponds to a system where all cache contents in the network are available to all receivers. For large <b>cache</b> <b>memories,</b> it is optimal to assign a positive <b>cache</b> <b>memory</b> to every receiver such that the weaker receivers are assigned larger <b>cache</b> <b>memories</b> compared to the stronger receivers. In this regime, the growth rate of the global capacity-memory tradeoff is further divided by the number of users, which corresponds to a local caching gain. Numerical indicate suggest that a uniform cache-assignment of the total <b>cache</b> <b>memory</b> is suboptimal in all regimes unless the BC is completely symmetric. For erasure BCs, this claim is proved analytically in the regime of small cache-sizes. Comment: Submitted to IEEE Transactions on Information Theor...|$|R
40|$|The first {{methods to}} bound {{execution}} time in computer systems with <b>cache</b> <b>memories</b> {{were presented in}} the late eighties [...] - twenty {{years after the first}} <b>cache</b> <b>memories</b> designed. Today, fifteen years later, methods has been developed to bound execution time with <b>cache</b> <b>memories</b> [...] . that were state-of-the-art twenty years ago. This report presents <b>cache</b> <b>memories</b> and real-time from the very basics to the state-of-the-art of <b>cache</b> <b>memory</b> design, methods to use <b>cache</b> <b>memories</b> in real-time systems and the limitations of current technology. Methods to handle intrinsic and extrinsic behavior on instruction and data caches will be presented and discussed, but also close issues like pipelining, DMA and other unpredictable hardware components will be briefly presented. No method is today able to automatically calculate a safe and tight Worst-Case Execution Time WCETc for any arbitrary program that runs on a modern high-performance system [...] - there are always cases where the method will cross into problems. Many of the methods can although give very tight WCETc or reduce the related problems under specified circumstances...|$|R
40|$|Abstract- <b>Cache</b> <b>memory</b> {{performance}} analysis is a challenging topic upon first introduction. Students must synthesize {{a significant amount}} of computer architecture knowledge, comprehend reasonably complex replacement strategies, and analyze performance. We propose a programming exercise that has students develop a visual <b>cache</b> <b>memory</b> simulator and then use the simulator to analyze several memory reference trace files. Our student learning assessment measured the quality of each team programming exercise solution and each individual’s own cache {{performance analysis}}. In addition, the final exam has several questions related to <b>cache</b> <b>memory.</b> Early results indicate students achieve a better understanding of <b>cache</b> <b>memory</b> and its impact on performance...|$|R
30|$|No <b>cache</b> <b>memory</b> is used.|$|R
40|$|High {{performance}} {{is the major}} concern {{in the area of}} VLSI Design. <b>Cache</b> <b>memory</b> consumes the half of total power in various systems. Thus, the architecture behavior of the cache governs both high performance and low power consumption. Simulator simulates <b>cache</b> <b>memory</b> design in various formats with help of various simulators like simple scalar, Xilinx etc. This paper explores the issue and consideration involved in designing efficient <b>cache</b> <b>memory</b> and we have discussed the <b>cache</b> <b>memory</b> simulation behavior on various simulators. Memory design concept is becoming dominant; memory level parallism is one of the critical issues concerning its performance. We have to propose high performance cache simulation behavior for performance improvement for future mobile processors design and customize mobile devices...|$|R
40|$|Abstract—High {{performance}} {{is the major}} concern in VLSI Design. Thus, the architecture behavior of the cache governs both high performance and low power consumption. High performance simulator simulates <b>cache</b> <b>memory</b> design in various formats with help of various simulators like simplescalar, Xilinx, Top spice 8 etc. This paper explores the issue and consideration involved in designing the efficient <b>cache</b> <b>memory.</b> We have discussed the <b>cache</b> <b>memory</b> simulation behavior on various simulators. We propose high performance cache simulation behavior issues for future mobile processors design and customize mobile devices...|$|R
40|$|Embedded {{microprocessor}} <b>cache</b> <b>memories</b> {{suffer from}} limited observability and controllability creating problems during in-system test. The application of test algorithms for SRAM <b>memories</b> to <b>cache</b> <b>memories</b> thus requires opportune transformations. In this paper {{we present a}} procedure to adapt traditional march tests to testing the data and the directory array of k-way set-associative <b>cache</b> <b>memories</b> with LRU replacement. The basic idea is to translate each march test operation into an equivalent sequence of cache operations able to reproduce the desired marching sequence into the data and the directory array of the cach...|$|R
40|$|Abstract—In this paper, an {{efficient}} technique is proposed {{to manage the}} <b>cache</b> <b>memory.</b> The proposed technique introduces some modifications on the well-known set associative mapping technique. This modification requires a little alteration {{in the structure of}} the <b>cache</b> <b>memory</b> and on the way by which it can be referenced. The proposed alteration leads to increase the set size virtually and consequently to improve the performance and the utilization of the <b>cache</b> <b>memory.</b> The current mapping techniques have accomplished good results. In fact, there are still different cases in which <b>cache</b> <b>memory</b> lines are left empty and not used, whereas two or more processes overwrite the lines of each other, instead of using those empty lines. The proposed algorithm aims at finding {{an efficient}} way to deal with such problem...|$|R
40|$|For high {{performance}} processor design, <b>cache</b> <b>memory</b> size {{is an important}} parameter which directly affects performance and the chip area. Modeling performance and area is required for design tradeoff of <b>cache</b> <b>memory.</b> This paper describes a tool which calculates <b>cache</b> <b>memory</b> performance and area. A designer can try variety of cache parameters to complete the specification of a <b>cache</b> <b>memory.</b> Data examples calculated using this tool are shown. Key Words and Phrases: <b>Cache</b> <b>memory,</b> Design tradeoff, Memory modeling, CAD ii Copyright 1994 by Osamu Okuzawa and Michael J. Flynn iii Contents 1. Introduction 1 2. Related Work 1 3. Performance/Area Workbench 2 3. 1 Tool target 2 3. 2 Input 2 3. 3 Output 3 3. 4 Model & Calculation method 3 3. 5 Tool Implementation 6 3. 6 Using Manual 9 4. Results 10 4. 1 Tool Performance 10 4. 2 Calculation result examples and analysis 11 5. Conclusions 16 iv List of Figures 1 CPI and Area Calculation Program Configuration 7 2 Calculation Process 8 3 32 KB spl [...] ...|$|R
40|$|Recently, energy {{dissipation}} by microprocessors is getting larger, {{which leads to}} a serious problem in terms of allowable temperature and performance improvement for future microprocessors. <b>Cache</b> <b>memory</b> is effective in bridging a growing speed gap between a processor and relatively slow external main memory, and has increased in its size. Almost all of today's commercial processors, not only highperformance microprocessors but embedded ones, have onchip <b>cache</b> <b>memories.</b> However, {{energy dissipation}} in the <b>cache</b> <b>memory</b> will approach or exceed 50 % of the increasing total energy dissipation by processors. An important point to note is that, in the near future, static (leakage) energy will dominate the total energy consumption in deep sub-micron processes. This paper describes <b>cache</b> <b>memory</b> architecture, especially for on-chip multiprocessors, that achieves efficient reduction of leakage energy in <b>cache</b> <b>memories</b> by exploiting gated-Vdd control, software selfinvalidation for L 1 cache, and dynamic data compression for L 2 cache. The simulation results show that our techniques can reduce a substantial amount of leakage energy without large performance degradation...|$|R
50|$|In {{addition}} to the macro-instruction <b>cache</b> <b>memory</b> {{also found in the}} ND-100, the ND-110 had a unique implementation of <b>cache</b> <b>memory</b> on the micro-instruction level. The step known as mapping in the ND-100 was then avoided because the first micro-instruction word of a macro-instruction was written into the control store cache.|$|R
40|$|<b>Cache</b> <b>memories</b> {{bridge the}} growing access-time {{gap between the}} {{processor}} and the main <b>memory.</b> <b>Cache</b> <b>memories</b> use randomisation functions for two purposes: (i) to {{limit the amount of}} search when looking up an address in the cache and (ii) to interleave the access stream over multiple independent banks, allowing multiple simultaneous accesses...|$|R
40|$|The Web-based {{information}} systems, as {{the network}} traffic and slow remote servers {{can lead to}} long delays in the answer delivery. Client memory is largely used to cache data and minimize future interaction with the servers. In this paper, we propose an extended <b>cache</b> <b>memory</b> to store the frequently used data. We observe that the frequently used data contain the images in a Web site. In this paper we propose that all the data which is frequently used must be stored at user end in an extended <b>cache</b> <b>memory.</b> The extended <b>cache</b> <b>memory</b> {{may be in the}} form of pen drive, CD, DVD or it could be saved at user’s machine. The only difference to access the Web between traditional and in our way is that the user have to provide the path of the data which resides at the extended <b>cache</b> <b>memory...</b>|$|R
40|$|This {{document}} {{is a short}} review about the MESI protocol simulator. This simulator is a tool {{which is used to}} teach the <b>cache</b> <b>memory</b> coherence on the computer systems with hierarchical memory system. The MESI protocol is a well known method to the maintenance of the information coherence in the memory system. The MESI simulator is also used to explain the process of the <b>cache</b> <b>memory</b> location (in multilevel <b>cache</b> <b>memory</b> systems). In this paper, we explain the MESI protocol and we show how the simulator works. Besides, we present some classroom practices carried out by using the MESI simulator...|$|R
30|$|Data (D$): Read/Write Hits and Misses, <b>cache</b> <b>memory</b> {{hits and}} misses.|$|R
50|$|These Seagate {{models were}} fitted with 2 MB of <b>cache</b> <b>memory.</b>|$|R
40|$|We {{developed}} a new algorithm for route discovery, nodes management, and mobility handling for on-demand cache routing on mobile Ad-Hoc networks (MANET). We used Ad-Hoc On demand Distance Vector (AODV) protocol as the better known reactive protocol, as well as using Link State algorithm of the Optimize Link State Routing (OLSR) protocol together. We used two levels of <b>caches</b> <b>memory</b> L- 1 and L- 2 along with link state routing table for each node. Which maintaining by using the algorithm of OLSR, which working under the AODV protocol. For mobility handling, we used link state algorithm working under AODV to manage node addition, deletion and movement in the network efficiently. We used the Network simulator NS- 2 version 2. 29 to show the results comparing with the AODV used just <b>cache</b> <b>memory,</b> and comparing with AODV without <b>cache</b> <b>memory.</b> The results shows that our algorithms outperform comparing with AODV without <b>cache</b> <b>memories,</b> and AODV with two levels of <b>cache</b> <b>memory</b> on packet delivery rate, where the link state routing protocol is used to distribute and maintain routing information among various nodes within a domain by using two messages which are Hello messages and Topology Control messages (TC) ...|$|R
50|$|IBM and Hitachi {{models were}} fitted with 128 KB of <b>cache</b> <b>memory.</b>|$|R
40|$|AbstractCompact {{numerical}} schemes provide high-order {{solution of}} PDEs with low dissipation and dispersion. Computer implementation of these schemes requires numerous passes of data through <b>cache</b> <b>memory</b> that considerably reduces performance of these schemes. To reduce this difficulty, a novel {{algorithm is proposed}} here. This algorithm {{is based on a}} wavefront approach and sweeps through <b>cache</b> <b>memory</b> only twice...|$|R
5000|$|<b>Cache</b> <b>memory</b> error-correction {{of up to}} 4 errors per tag or 32-bit word ...|$|R
40|$|Nowadays, the {{computational}} systems (multi and uniprocessors) need {{to avoid}} the cache coherence problem. There are some techniques to solve this problem. The MESI cache coherence protocol is one of them. This paper presents a simulator of the MESI protocol which is used for teaching the <b>cache</b> <b>memory</b> coherence on the computer systems with hierarchical memory system and for explaining {{the process of the}} <b>cache</b> <b>memory</b> location in multilevel <b>cache</b> <b>memory</b> systems. The paper shows a description of the course in which the simulator is used, a short explanation about the MESI protocol and how the simulator works. Then, some experimental results in a real teaching environment are described...|$|R
2500|$|Capacities of main <b>memory</b> and <b>cache</b> <b>memory</b> {{are usually}} {{expressed}} with customary binary prefixes ...|$|R
5000|$|DISKCACHE (OS/2 only, ignored under DR-DOS 7.02 and higher) : Configures <b>cache</b> <b>memory</b> sizes.|$|R
5000|$|<b>Cache</b> <b>memory</b> (often static RAM) - this {{operates}} at speeds comparable with the CPU.|$|R
40|$|Computer {{performance}} has increased rapidly {{for a number}} of years. In addition to the improvements in architecture and implementation techniques, <b>cache</b> <b>memories</b> have been utilized in memory hierarches to reduce memory latency and bus tra c. A large amount of research has been done to increase the performance of <b>cache</b> <b>memories.</b> This work is unique in that it optimizes the memor...|$|R
40|$|Abstract — An {{architectural}} level {{technique for}} a high per-formance and low energy <b>cache</b> <b>memory</b> is proposed in this paper. The key idea of our approach is to divide a <b>cache</b> <b>memory</b> into several number of cache blocks and to acti-vate a few parts of the cache blocks. The threshold voltage of each cache block is dynamically changed according to an utilization of each block. Frequently accessed cache blocks are woken up and others are put to sleep by controlling the threshold voltage. Since time overhead to change the threshold voltage can not be neglected, predicting a cache block which will be accessed in next cycle is important. History based prediction technique to predict cache blocks which should be woken up is also proposed. Experimental results demonstrated that the leakage energy dissipation in <b>cache</b> <b>memories</b> optimized by our approach can be less than 5 % of energy dissipation in a <b>cache</b> <b>memory</b> which does not employ our approach. ...|$|R
40|$|Cosmic-ray induced soft {{errors in}} <b>cache</b> <b>memories</b> are {{becoming}} {{a major threat to}} the reliability of microprocessor-based systems. In this paper, we present a new method to accurately estimate the reliability of <b>cache</b> <b>memories.</b> We have measured the MTTF (Mean-Time-To-Failure) of unprotected first-level (L 1) caches for twenty programs taken from SPEC 2000 benchmark suite. Our results show that a 16 KB first-level cache possesses a MTTF of at least 400 years (for a raw error rate of 0. 002 FIT/bit.) However, this MTTF is significantly reduced for higher error rates and larger cache sizes. Our results show that for selected programs, a 64 KB first-level cache is more than 10 times as vulnerable to soft errors versus a 16 KB <b>cache</b> <b>memory.</b> Our work also illustrates that the reliability of <b>cache</b> <b>memories</b> is highly application-dependent. Finally, we present three different techniques to reduce the susceptibility of first-level caches to soft errors by two orders of magnitude. Our analysis shows how to achieve a balanc...|$|R
40|$|Some {{algorithms}} {{running with}} compromised data select <b>cache</b> <b>memory</b> {{as a type}} of secure memory where data is confined and not transferred to main memory. However, cold-boot attacks that target <b>cache</b> <b>memories</b> exploit the data remanence. Thus, a sudden power shutdown may not delete data entirely, giving the opportunity to steal data. The biggest challenge for any technique aiming to secure the <b>cache</b> <b>memory</b> is performance penalty. Techniques based on data scrambling have demonstrated that security can be improved with a limited reduction in performance. However, they still cannot resist side-channel attacks like power or electromagnetic analysis. This paper presents a review of known attacks on memories and countermeasures proposed so far and an improved scrambling technique named random masking interleaved scrambling technique (RM-ISTe). This method is designed to protect the <b>cache</b> <b>memory</b> against cold-boot attacks, even if these are boosted by side-channel techniques like power or electromagnetic analysis. Postprint (author's final draft...|$|R
