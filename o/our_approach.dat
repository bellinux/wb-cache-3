10000|10000|Public
500|$|Liverpool manager Bob Paisley was {{critical}} of his team's performance in the first leg: [...] "We threw it away, our attitude was wrong and we were careless. Anderlecht are a great team going forward, but we never attacked them as we should. <b>Our</b> <b>approach</b> seems to have gone a bit wrong and we've lost our scoring touch where {{earlier in the season}} our finishing was great." ...|$|E
2500|$|Smith and King {{pose the}} further points in their 2009 peer {{reviewed}} paper Naturism and Sexuality:broadening <b>our</b> <b>approach</b> to sexual wellbeing ...|$|E
2500|$|According to the committee, {{its purpose}} was, [...] "primarily to reach those who, {{contrary}} to the teachings of Christianity and the principles of democracy, are taking part, unfortunately, in spreading race and minority hatreds in the United States. <b>Our</b> <b>approach</b> will be positive and dignified, {{and there will be}} no personal attacks against anyone." ...|$|E
40|$|To {{improve the}} ability of {{predicting}} the impact scope of a given change, we present two approaches applicable {{to the maintenance of}} object-oriented software systems. <b>Our</b> first <b>approach</b> exclusively uses a logical model extracted from UML relations among classes, and <b>our</b> other, hybrid <b>approach</b> additionally considers information mined from version histories. Using the open source Hadoop system, we evaluate <b>our</b> <b>approaches</b> by comparing <b>our</b> impact predictions with predictions generated using existing data mining techniques, and with actual change sets obtained from bug reports. We show that both <b>our</b> <b>approaches</b> produce better predictions when the system is immature and the version history is not well-established, and <b>our</b> hybrid <b>approach</b> produces comparable results with data mining as the system evolves. 1...|$|R
40|$|Abstractâ€”Moving object {{detection}} {{is essential}} for real-time surveillance; however, {{it is challenging to}} support moving object detection in a timely fashion due to the compute-intensive nature. In this paper, we tackle the challenge by developing new techniques to substantially expedite moving object detection. We have implemented <b>our</b> <b>approaches</b> using a low-end webcam in a commodity laptop with no special hardware for high speed image processing. We have compared the performance of <b>our</b> <b>approaches</b> to the well-known background modeling technique. <b>Our</b> <b>approaches</b> reduce the average delay for moving object detection by up to 45. 5 % and decrease the memory consumption by up to approximately 14 %, while supporting equally accurate detection...|$|R
30|$|Sections 6.1 and 6.2 {{describe}} <b>our</b> <b>approaches</b> {{in solving}} these {{problems in the}} context of iris matching.|$|R
2500|$|So, {{do we need}} {{a radical}} rethink on how we move unwanted, still useable items to the {{second-hand}} market place? Is there a case for changing <b>our</b> <b>approach</b> to producer responsibility and insisting that producers finance collection for reuse, and additionally, drive consumer choices for reuse, repair and remanufacture; whilst addressing the costs of recycling and disposal? ...|$|E
2500|$|It [...] was {{my feeling}} ... {{that we were}} better off telling all the story for half the characters, rather than half the story for all the characters. Cutting the novel in half would have {{produced}} two half-novels; <b>our</b> <b>approach</b> will produce two novels taking place simultaneously, but set hundreds or even thousands of miles apart, and involving different casts [...] of characters (with some overlap).|$|E
60|$|This is {{intended}} to proclaim <b>our</b> <b>approach,</b> and warn all strangers from our track.|$|E
60|$|We {{had a very}} {{pleasant}} day, though {{we were all in}} a tender state about <b>our</b> <b>approaching</b> separation.|$|R
40|$|Abstract. This paper proposes novel {{exclusive}} {{and continuous}} approaches {{to guide the}} search and the retrieval in fingerprint image databases. Both approaches are useful to perform a coarse level classification of fingerprint images before fingerprint authentication tasks. <b>Our</b> <b>approaches</b> are characterized by: (1) texture image descriptors based on pairs of multi-resolution decomposition methods that encode effectively global and local fingerprint information, with similarity measures used for fingerprint matching purposes, and (2) a novel multi-class object recognition method based on the Optimum Path Forest classifier. Experiments were carried out on the standard NIST- 4 dataset aiming to study the discriminative and scalability capabilities of <b>our</b> <b>approaches.</b> The high classification rates allow us demonstrate the feasibility and validity of <b>our</b> <b>approaches</b> for characterizing fingerprint images accurately. ...|$|R
30|$|In this section, we {{describe}} our experimental methodology and {{present the results}} that {{evaluate the effectiveness of}} <b>our</b> <b>approaches.</b>|$|R
60|$|Hect. Go you before. Tell him of <b>our</b> <b>approach,</b> Which will, I fear, be much unwelcome to him.|$|E
60|$|I found Bianca {{awaiting}} me in {{the gallery}} above the courtyard, drawn thither by the sounds of <b>our</b> <b>approach.</b>|$|E
60|$|The General, {{buried in}} a book before him, noticed neither the {{movements}} of his own men nor <b>our</b> <b>approach.</b>|$|E
6000|$|... "You {{look too}} cheerful, Miss Hannay. I find {{that we are}} {{expected}} to wear sad countenances at <b>our</b> <b>approaching</b> banishment." ...|$|R
30|$|On the {{two major}} data sets for action recognition, <b>our</b> <b>approaches</b> {{outperformed}} {{those found in the}} literature, both in the unsupervised and supervised case.|$|R
40|$|With {{shrinking}} VLSI feature {{sizes and}} increasing overall chip areas, buffering {{has emerged as}} an effective {{solution to the problem}} of growing interconnect delays in modern designs. The problem of buffer insertion in a single net has been the focus of most previous researches. However, efficient algorithms for buffer insertion in whole circuits are generally needed. In this paper, we relate the timing constrained minimal buffer insertion problem to the min-cost flow dual problem, and propose two algorithms based on min-cost flow and min-cut techniques, respectively, to solve it in combinational circuits. We compare <b>our</b> <b>approaches</b> to a traditional approach based on Lagrangian relaxation. Experimental results demonstrate that <b>our</b> <b>approaches</b> are efficient and effective. On the average, <b>our</b> <b>approaches</b> achieve 45 % and 39 % reduction, respecitively, on the number of buffers inserted in comparison to the traditional approach. ...|$|R
6000|$|... 'Oh, I hope you'll {{die in a}} garret, starved to death!' {{said the}} boy, mistaking <b>our</b> <b>approach</b> for that of his negligent attendant.|$|E
60|$|As we {{drew near}} to the cottage, I saw a lady {{standing}} by the gate, watching <b>our</b> <b>approach.</b> It was the Widow Canby.|$|E
60|$|FOURTH. But {{this will}} yet be further {{manifest}} {{by what we}} have yet to say of the manner of <b>our</b> <b>approach</b> unto the throne of grace.|$|E
3000|$|... in {{the second}} stage. Note that <b>our</b> <b>approaches</b> can still be used for other {{possible}} implementations {{as long as the}} assumption of orthogonal transmissions is satisfied.|$|R
40|$|This paper {{describes}} <b>our</b> <b>approaches</b> {{to raise}} the level of abstraction at which hardware suitable for accelerating computationally-intensive applications can be specified. Field-Programmable Gate Arrays (FPGAs) are becoming adopted as a computational platform by the high-performance computing community, but there are challenges to extract maximum performance from these devices. Unlike other <b>approaches,</b> <b>our</b> focus is on data memory organisation and inputoutput bandwidth considerations, which are the typical stumbling block of existing hardware compilation schemes. We describe <b>our</b> <b>approaches,</b> which are based on formal optimization techniques, and present some results showing the advantage of exposing the interaction between data memory system design and parallelism extraction to the compiler. 1...|$|R
40|$|Abstract. The Border Gateway Protocol (BGP) is the {{de facto}} inter-domain routing {{protocol}} that connects autonomous systems (ASes). Despite its importance for the Internet infrastructure, BGP is vulnerable {{to a variety of}} attacks due to lack of security mechanisms in place. Many BGP security mechanisms have been proposed, however, none of them has been deployed because of either high cost or high complexity. The right trade-o between eciency and security has been ever challenging. In this paper, we attempt to trade-o between eciency and security by giving a little dose of trust to BGP routers. We present a new exible threat model that assumes for any path of length h, at least one BGP router is trustworthy, where h is a parameter that can be tuned according to security requirements. Based on this threat model, we present two new symmetric key approaches to securing BGP: the centralized key distribution approach and the distributed key distribution <b>approach.</b> Comparing <b>our</b> <b>approaches</b> to the previous SBGP scheme, <b>our</b> centralized <b>approach</b> has a 98 % improvement in signature veri cation. <b>Our</b> distributed <b>approach</b> has equivalent signature generation cost as in SBGP and an improvement of 98 % in signature verication. Comparing <b>our</b> <b>approaches</b> to the previous SPV scheme, <b>our</b> centralized <b>approach</b> has a 42 % improvement in signature generation and a 96 % improvement in signature verication. <b>Our</b> distributed <b>approach</b> has a 90 % improvement on signature generation cost and a 95 % improvement in signature verication verication. By combining <b>our</b> <b>approaches</b> with previous public key approaches, it is possible to simultaneously provide an increased level of security and reduced computation cost. ...|$|R
60|$|Mrs Bowater {{had already}} come sauntering {{back to our}} breakfast table, and with gaze impassively fixed on the horizon, pretended not {{to be aware of}} <b>our</b> <b>approach.</b>|$|E
60|$|The latter, perceiving <b>our</b> <b>approach,</b> {{came down}} from his post of {{observation}} and walked quietly in our direction, with his head sunk upon his breast, like one who is absorbed in thought.|$|E
60|$|Breaking in {{upon the}} party tumultuously, as we did, we always created a sensation. Sometimes, {{we found the}} animal still alive and struggling; in which case, it was {{generally}} dropped at <b>our</b> <b>approach.</b>|$|E
30|$|TFI also {{includes}} all the computational {{cost for the}} operations needed to prepare the residual information for <b>our</b> <b>approaches,</b> such as the mean, the variance and so on.|$|R
40|$|Previous PAN {{workshops}} have afforded {{evaluation of}} <b>our</b> <b>approaches</b> to author verification/identification based on stopword cooccurrence patterns. Problems {{have tended to}} involve comparing one document to a small set of documents (n<= 5) of known authorship. This paper discusses the adaptation of one of <b>our</b> <b>approaches</b> to a PAN 2016 problem of author clustering, which involves generating clusters within larger sets of documents (n<= 100) for {{an unknown number of}} distinct authors, where each set is in English, Dutch or Greek. We describe <b>our</b> previous <b>approaches</b> as the background to the approach taken to this task and briefly overview the results that were achieved, which are not expected to be particularly remarkable due to substantial limitations on our time around the task...|$|R
40|$|We {{describe}} our {{participation in}} the TREC 2005 Question Answering track; our main focus this year was on improving <b>our</b> multi-stream <b>approach</b> to question answering and on making a first step towards a question answering-as-XML retrieval strategy. We provide {{a detailed account of}} the ideas underlying <b>our</b> <b>approaches</b> to the QA task, report on our results, and give a summary of our findings...|$|R
60|$|As {{he spoke}} the last words, we drew {{up in front}} of the fine old house. A lady in a stout tweed skirt, who was bending over a flower bed, {{straightened}} herself at <b>our</b> <b>approach.</b>|$|E
60|$|Thus {{it was by}} mere {{chance that}} <b>our</b> <b>approach</b> was almost noiseless, {{and when we were}} come within view of the dwelling, from whence we could hear the hum of voices, none of the inmates were aware of our presence.|$|E
60|$|From my post of observation, {{under the}} canvas screens {{of one of}} the waggons, I could make out turbaned heads popping up {{to have a look at}} us from among the rocks, and an {{occasional}} scout hurrying northward with the news of <b>our</b> <b>approach.</b>|$|E
30|$|In {{this section}} we present <b>our</b> <b>approaches</b> for solving optimal {{protection}} problem, which formulates protecting the state variables in a smart grid with {{minimum number of}} measures against FDI attacks.|$|R
40|$|Abstractâ€”The Border Gateway Protocol (BGP) is the {{de facto}} inter-domain routing {{protocol}} that connects autonomous systems (ASes). Despite its importance for the Internet infrastructure, BGP is vulnerable {{to a variety of}} attacks due to lack of security mechanisms in place. Many BGP security mechanisms have been proposed. However, none of them has been deployed because of either high cost or high complexity. The right trade-off between efficiency and security has been ever challenging. In this paper, we attempt to trade-off between efficiency and security by giving a little dose of trust to BGP routers. We present a new flexible threat model that assumes for any path of length h, at least one BGP router is trustworthy, where h is a parameter that can be tuned according to security requirements. Based on this threat model, we present two new symmetric key approaches to securing BGP: the centralized key distribution approach and the distributed key distribution <b>approach.</b> Comparing <b>our</b> <b>approaches</b> to the previous SBGP scheme, <b>our</b> centralized <b>approach</b> has a 98 % improvement in signature verification. <b>Our</b> distributed <b>approach</b> has equivalent signature generation cost as in SBGP and an improvement of 98 % in signature verification. Comparing <b>our</b> <b>approaches</b> to the previous SPV scheme, <b>our</b> centralized <b>approach</b> has a 42 % improvement in signature generation and a 96 % improvement in signature verification. <b>Our</b> distributed <b>approach</b> has a 90 % improvement on signature generation cost and a 95 % improvement in signature verification verification. We also describe practical techniques for increasing the long term security and collusion resistance of our key distribution protocols without increasing the signature generation and verification cost. By combining <b>our</b> <b>approaches</b> with previous public key approaches, it is possible to simultaneously provide an increased level of security and reduced computation cost...|$|R
40|$|With {{increasing}} globalisation {{of software}} production, {{widespread use of}} software components, {{and the need to}} maintain software systems over long periods of time, there has been a recognition that better support for collaborative working is needed by software engineers. In this paper, two approaches to developing improved system support for collaborative software engineering are described: GENE-SIS and OPHELIA. As both project are moving towards industrial trials and eventual public releases of their systems, this exercise of comparing and contrasting <b>our</b> <b>approaches</b> has provided the basis for future collaboration between our projects particularly in carrying out comparative studies of <b>our</b> <b>approaches</b> in practical use. ...|$|R
