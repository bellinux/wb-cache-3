0|53|Public
40|$|DNA {{sequence}} analysis {{depends on the}} accurate assembly of fragment reads for the determination of a consensus sequence. Genomic sequences frequently contain repeat elements that may confound the fragment assembly process, and errors in fragment assembly may seriously impact the biological interpretation of the sequence data. Validating the fidelity of sequence assembly by experimental means is desirable. This report examines the use of restriction digest analysis as a method for testing the fidelity of sequence assembly. Restriction digest fingerprint matching is an established technology for high resolution physical map construction, but the requirements for <b>assembly</b> <b>validation</b> {{differ from those of}} fingerprint mapping. Fingerprint matching is a statistical process that is robust to the presence of errors in the data and independent of absolute fragment mass determination. <b>Assembly</b> <b>validation</b> depends on the recognition of a small number of discrepant fragments and is very sensitive to [...] ...|$|R
40|$|We {{present the}} first {{collection}} of tools aimed at automated genome <b>assembly</b> <b>validation.</b> This work formalizes several mechanisms for detecting mis-assemblies, and describes their implementation in our automated validation pipeline, called amosvalidate. We demonstrate {{the application of}} our pipeline in both bacterial and eukaryotic genome assemblies, and highlight several assembly errors in both draft and finished genomes. The software described is compatible with common assembly formats and is released, open-source, at [URL] © 2008 Phillippy et al.; licensee BioMed Central Ltd...|$|R
40|$|Comparing {{complete}} animal mitochondrial genome sequences {{is becoming}} increasingly common for phylogenetic reconstruction and {{as a model for}} genome evolution. Not only are they much more informative than shorter sequences of individual genes for inferring evolutionary relatedness, but these data also provide sets of genome-level characters, such as the relative arrangements of genes, that can be especially powerful. We describe here the protocols commonly used for physically isolating mtDNA, for amplifying these by PCR or RCA, for cloning, sequencing, <b>assembly,</b> <b>validation,</b> and gene annotation, and for comparing both sequences and gene arrangements. On several topics, we offer general observations based on our experiences to date with determining and comparing complete mtDNA sequences...|$|R
40|$|The MT 2 or "s-transverse mass" {{statistic}} {{was developed}} to associate a parent mass scale to a missing transverse energy signature, given that escaping particles are generally expected in pairs, while collider experiments are sensitive to just a single transverse momentum vector sum. This document focuses on the generalized extension of that statistic to asymmetric one- and two-step decay chains, with arbitrary child particle masses and upstream missing transverse momentum. It provides a unified theoretical formulation, complete solution classification, taxonomy of critical points, and technical algorithmic prescription for treatment of the MT 2 event scale. An implementation of the described algorithm is available for download, and is also a deployable component of the author's selection cut software package AEACuS (Algorithmic Event Arbiter and Cut Selector). Appendices address combinatoric event <b>assembly,</b> algorithm <b>validation,</b> and a complete pseudocode. Comment: As published in JHEP; appendices address combinatoric event <b>assembly,</b> algorithm <b>validation,</b> and a complete pseudocode; program revision included as ancillary file; 32 pages; 6 illustrated event case studies; 7 footnoted event case studie...|$|R
40|$|A multi-institution {{collaborative}} team of Australian chemistry education researchers, {{teaching a}} total of over 3000 first year chemistry students annually, has explored a tool for diagnosing students 2 ̆ 7 prior conceptions as they enter tertiary chemistry courses. Five core topics were selected and clusters of diagnostic items were assembled linking related concepts in each topic together. An ordered multiple choice assessment strategy was adopted to enable provision of formative feedback to students through combination of the specific distractors that they chose. Concept items were either sourced from existing research instruments or developed by the project team. The outcome is a diagnostic tool consisting of five topic clusters of five concept items that has been delivered in large introductory chemistry classes at five Australian institutions. Statistical analysis of data has enabled exploration of the composition {{and validity of the}} instrument including a comparison between delivery of the complete 25 item instrument with subsets of five items, clustered by topic. This analysis revealed that most items retained their validity when delivered in small clusters. Tensions between the <b>assembly,</b> <b>validation</b> and delivery of diagnostic instruments for the purposes of acquiring robust psychometric research data versus their pragmatic use are considered in this study...|$|R
40|$|With the {{expansion}} of next-generation sequencing technology and advanced bioinformatics, {{there has been a}} rapid growth of genome sequencing projects. However, while this technology enables the rapid and cost-effective assembly of draft genomes, the quality of these assemblies usually falls short of gold standard genome assemblies produced using the more traditional BAC by BAC and Sanger sequencing approaches. <b>Assembly</b> <b>validation</b> is often performed by the physical anchoring of genetically mapped markers, but this is prone to errors and the resolution is usually low, especially towards centromeric regions where recombination is limited. New approaches are required to validate reference genome assemblies. The ability to isolate individual chromosomes combined with next-generation sequencing permits the <b>validation</b> of genome <b>assemblies</b> at the chromosome level. We demonstrate this approach by the assessment of the recently published chickpea kabuli and desi genomes. While previous genetic analysis suggests that these genomes should be very similar, a comparison of their chromosome sizes and published assemblies highlights significant differences. Our chromosomal genomics analysis highlights short defined regions that appear to have been misassembled in the kabuli genome and identifies large-scale misassembly in the draft desi genome. The integration of chromosomal genomics tools within genome sequencing projects has the potential to significantly improve the construction and <b>validation</b> of genome <b>assemblies.</b> The approach could be applied both for new genome assemblies as well as published assemblies, and complements currently applied genome assembly strategies...|$|R
40|$|A NASA {{study for}} the {{preliminary}} definition of a teleoperated robotic device has been recently completed. The Fligt Telerobotic Servicer (FTS) {{will be used to}} assist astronauts in many of the <b>on-board</b> tasks of <b>assembly,</b> maintenance, servicing, and inspection of the Space Station. The role of artificial intelligence (AI) in furthering the FTS automation capabilities and, hence, extending its capacity for growth and evolution is discussed. Relevant system engineering issues are identified, and an approach for insertion of AI technology is presented in terms of the NASA/NBS Standard Reference Model control architecture NASREM...|$|R
40|$|AbstractPlanning and {{development}} of technical systems under consideration of the whole system lifecycle is a state-of-the-art procedure, which is well-established in practical use. For system engineers, the systematic inclusion of system lifecycle properties becomes increasingly important, because agile system are required, which work optimally under current boundary conditions and can easily adapt to future requirements. Manifold system properties with specific dependencies within the observed system necessitate a systematic analysis to facilitate a targeted consideration in the early phase of development. Therefore we present a generic approach for modularizing systems depending on system lifecycle properties. Within {{a case study of}} automotive <b>assembly</b> a <b>validation</b> of this method was done and first implications were derived...|$|R
5000|$|This step covers many {{engineering}} disciplines including: mechanical, electrical, electronic, software (embedded), and domain-specific, such as architectural, aerospace, automotive, ... Along {{with the}} actual creation of geometry there is {{the analysis of the}} components and product <b>assemblies.</b> Simulation, <b>validation</b> and optimization tasks are carried out using CAE (computer-aided engineering) software either integrated in the CAD package or stand-alone. These are used to perform tasks such as:- Stress analysis, FEA (finite element analysis); kinematics; computational fluid dynamics (CFD); and mechanical event simulation (MES). CAQ (computer-aided quality) is used for tasks such as Dimensional tolerance (engineering) analysis.Another task performed at this stage is the sourcing of bought out components, possibly with the aid of procurement systems.|$|R
40|$|Educational {{research}} shows that self-beliefs can have profound influences on learning behavior and achievement. It follows, then, that beliefs {{about the nature of}} programming aptitude (mindset) and the way individuals perceive themselves as programmers (self-concept) could have a salient impact on their programming practice. As such, new teaching methods could be used to support student self-beliefs and thereby encourage practice. However, valid measurement is needed to test this hypothesis. This paper presents the <b>assembly</b> and <b>validation</b> of a measurement instrument to support research into self-enrichment within the introductory programming context. An evaluation shows that the reliability and construct validity of the instrument are adequate, while the concurrent validity of the evaluation framework is satisfactory in the higher education context. However, future validation is required to establish cross-context validity...|$|R
40|$|The International Standardization Organization {{introduced}} standard 12189 for the preclinical {{evaluation of}} the mechanical reliability of posterior stabilization devices. The well-known vertebrectomy model formalized in standard F 1717 by the American Society for Testing and Materials was modified {{with the introduction of}} a modular anterior support made up of three calibrated springs, which allows to describe a more realistic scenario, closer to the effective clinical use, as well to test even very flexible and dynamic posterior stabilization implants. Despite these important improvements, ISO 12189 received very little attention in the literature. The aim of the work is to provide a systematic procedure for the <b>assembly</b> and <b>validation</b> of a finite element model capable of describing the experimental test according to ISO 12189. The validated finite element model is able to catch very well the effective stiffness of the unassembled and assembled constructs (percentage differences...|$|R
40|$|The M T 2, or “s-transverse mass”, {{statistic}} {{was developed}} to associate a parent mass scale to a missing transverse energy signature, given that escaping particles are generally expected in pairs, while collider experiments are sensitive to just a single transverse momentum vector sum. This document focuses on the generalized M ˜ T 2 M̃_T 2 extension of that statistic to asymmetric one- and two-step decay chains, with arbitrary child particle masses and upstream missing transverse momentum. It provides a unified theoretical formulation, complete solution classification, taxonomy of critical points, and technical algorithmic prescription for treatment of the M ˜ T 2 M̃_T 2 event scale. An implementation of the described algorithm is available for download, and is also a deployable component of the author’s selection cut software package AEAC u S (Algorithmic Event Arbiter and C u t Selector). appendices address combinatoric event <b>assembly,</b> algorithm <b>validation,</b> and a complete pseudocode...|$|R
40|$|International audienceThis paper {{details the}} {{implementation}} process of an embedded structural health monitoring (SHM) system enabling condition-based {{maintenance of aircraft}} nacelles. One critical issue before {{being able to make}} use of such system is to ensure the effective bonding of the chosen actuators and sensors with their host structure, especially as the latter will be exposed to harsh environments and wide operational variability. In this work, we are concerned with the composite components of the nacelle and we use piezoelectric elements as both sensors and actuators. We propose an integrated approach that allows to validate a combination "Substrate [...] Glue [...] Piezoelectric" (SGP) and thus provides criteria to choose and size these <b>assemblies.</b> This <b>validation</b> scheme is based on the observation of the variations of the static capacity of the piezoelectric element after enduring various temperature and stress conditions when bonded to its host structure. Based on those SGP combinations, an active SHM strategy interrogating the structure by means of elastic wave propagation is currently being developed and preliminary results on samples representative of the nacelle are presented and discussed...|$|R
40|$|AbstractCatfish is {{the major}} {{aquaculture}} species in the United States. To enhance its genome studies involving genetic linkage and comparative mapping, a bacterial artificial chromosome (BAC) contig-based physical map of the channel catfish (Ictalurus punctatus) genome was generated using four-color fluorescence-based fingerprints. Fingerprints of 34, 580 BAC clones (5. 6 × genome coverage) were generated for the FPC assembly of the BAC contigs. A total of 3307 contigs were assembled using a cutoff value of 1 × 10 − 20. Each contig contains an average of 9. 25 clones with an average size of 292  kb. The combined contig size for all contigs was 0. 965  Gb, approximately the genome size of the channel catfish. The reliability of the contig assembly was assessed by both hybridization of gene probes to BAC clones contained in the fingerprinted <b>assembly</b> and <b>validation</b> of randomly selected contigs using overgo probes designed from BAC end sequences. The presented physical map should greatly enhance genome research in the catfish, particularly aiding {{in the identification of}} genomic regions containing genes underlying important performance traits...|$|R
40|$|The recent Zika virus {{outbreak}} {{highlights the}} need for low-cost diagnostics that can be rapidly developed for distribution and use in pandemic regions. Here, we report a pipeline for the rapid design, <b>assembly,</b> and <b>validation</b> of cell-free, paper-based sensors {{for the detection of}} the Zika virus RNA genome. By linking isothermal RNA amplification to toehold switch RNA sensors, we detect clinically relevant concentrations of Zika virus sequences and demonstrate specificity against closely related Dengue virus sequences. When coupled with a novel CRISPR/Cas 9 -based module, our sensors can discriminate between viral strains with single-base resolution. We successfully demonstrate a simple, field-ready sample-processing workflow and detect Zika virus from the plasma of a viremic macaque. Our freeze-dried biomolecular platform resolves important practical limitations to the deployment of molecular diagnostics in the field and demonstrates how synthetic biology can be used to develop diagnostic tools for confronting global health crises. Defense Threat Reduction Agency (DTRA) (HDTRA 1 - 14 - 1 - 0006) United States. National Institutes of Health (NIH AI 100190...|$|R
40|$|This paper {{details the}} {{implementation}} process of an embedded structural health monitoring (SHM) system enabling condition-based {{maintenance of aircraft}} nacelles. One critical issue before {{being able to make}} use of such system is to ensure the effective bonding of the chosen actuators and sensors with their host structure, especially as the latter will be exposed to harsh environments and wide operational variability. In this work, we are concerned with the composite components of the nacelle and we use piezoelectric elements as both sensors and actuators. We propose an integrated approach that allows to validate a combination “Substrate—Glue—Piezoelectric” (SGP) and thus provides criteria to choose and size these <b>assemblies.</b> This <b>validation</b> scheme is based on the observation of the variations of the static capacity of the piezoelectric element after enduring various temperature and stress conditions when bonded to its host structure. Based on those SGP combinations, an active SHM strategy interrogating the structure by means of elastic wave propagation is currently being developed and preliminary results on samples representative of the nacelle are presented and discussed. Projet AIRCELLE (EPICE/CORALIE...|$|R
40|$|Mountain bike wheels may be {{evaluated}} for durability and {{mode of failure}} using bump drum test machines that subject wheels to simulated rider loads and uneven terrain. These machines may operate at high speeds in order to accelerate failures, particularly during the design and prototype phase of wheel development. This paper describes a mathematical model {{of the dynamics of}} a wheel test drum <b>assembly,</b> and the <b>validation</b> of the model using data recorded during a wheel test. This model will subsequently be applied to better understand the key test operating parameters that must be controlled in order to produce repeatable durability data during accelerated testing. © 2012 Published by Elsevier Ltd...|$|R
40|$|AbstractMountain bike wheels may be {{evaluated}} for durability and {{mode of failure}} using bump drum test machines that subject wheels to simulated rider loads and uneven terrain. These machines may operate at high speeds in order to accelerate failures, particularly during the design and prototype phase of wheel development. This paper describes a mathematical model {{of the dynamics of}} a wheel test drum <b>assembly,</b> and the <b>validation</b> of the model using data recorded during a wheel test. This model will subsequently be applied to better understand the key test operating parameters that must be controlled in order to produce repeatable durability data during accelerated testing...|$|R
40|$|This paper {{presents}} the first {{results of a}} research work, which purposes were to develop the knowledge base and the architecture of computer tool to assist the design of welded products. The most important contributions of this research work were the definition of knowledge structure and its components, formal and heuristic knowledge components construction, {{and the development of}} a design process model for manufacturing and <b>assembly</b> and the <b>validations</b> of contributions on projects of local metal-mechanical industry. The knowledge base includes principles and rules to design, information about costs and welding times, selection factors for welding processes, welding standards, definition of welding construction types, pre-welding and post-welding processes and welding geometry among others aspects. ...|$|R
40|$|Global {{spectral}} medium range weather forecasti ng {{model on}} PARAM This paper describes the experience gained while parallelizing the global medium range weather forecasting application called TSO on a PARAM machine {{based on a}} distributed memory parallel architecture. The TSO code employs the spectral method for horizontal directions and finite differencing in the vertical direction and time marching. The parallel implementation {{takes care of the}} easy portability of parallel code across various platforms and environments. The parallel code is optimized using iS 60 based <b>assembly</b> routines. The <b>validation</b> of the parallel code has been accomplished by comparing the parallel TSO results with those of the Cray for 2 nd February 1993 initial data...|$|R
30|$|Recently, several {{research}} groups {{have focused on}} the problem of non-destructive measurement of the actual heat transfer conditions of building assemblies using thermal images. Madding's work (Madding [2008]) is one of the earliest studies that propose a method for analyzing thermal images to estimate the actual heat transfer conditions for building envelopes. Based on the environmental assumption of a steady-state heat transfer condition of building environments, this work measured the R-value of drywall assemblies using the indoor surface temperature data obtained from thermal images. The measurements were experimentally conducted in controlled lab environments, and then they were compared with the theoretical expected properties. Likewise, based on the similar environmental assumptions, methods proposed in (Albatici and Tonelli [2010]; Dall'O' et al. [2013]; Fokaides and Kalogirou [2011]) measured the overall heat transfer coefficient of the building <b>assemblies.</b> For <b>validation,</b> several experiments were conducted for a few standard wall assemblies. In these studies, the difference between the measured values using thermal images and the notional values was reported to be in the range of ~ 10 % (Madding [2008]), 10 - 20 % (Fokaides and Kalogirou [2011]), and around 15 % for cavity walls (Dall'O' et al. [2013]).|$|R
40|$|The {{traditional}} spacecraft {{system is}} a monolithic structure with a single mission focused design and lengthy production and qualification schedules coupled with enormous cost. Additionally, there rarely, if ever, is any designed preventive maintenance plan or re-fueling capability. There has been much research in recent years into alternative options. One alternative option involves autonomous on-orbit servicing of current or future monolithic spacecraft systems. The U. S. Department of Defense (DoD) embarked on a highly successful venture to prove out such a concept with the Defense Advanced Research Projects Agency’s (DARPA’s) Orbital Express program. Orbital Express demonstrated all of the enabling technologies required for autonomous on-orbit servicing to include refueling, component transfer, autonomous satellite grappling and berthing, rendezvous, inspection, proximity operations, docking and undocking, and autonomous fault recognition and anomaly handling (Kennedy, 2008). Another potential option involves a paradigm shift from the monolithic spacecraft system to one involving multiple interacting spacecraft that can autonomously assemble and reconfigure. Numerous benefits are associated with autonomous spacecraft assemblies, ranging from a removal of significant intra-modular reliance that provides for parallel design, fabrication, <b>assembly</b> and <b>validation</b> processes to the inherent smaller nature of fractionated systems which allows for each module to be placed into orbit separately on more affordable launch platforms (Mathieu, 2005) ...|$|R
40|$|A {{previous}} work of ours found the best agreement between EUV light curves observed in an active region core (with evidence of super-hot plasma) and those predicted from a {{model with a}} random combination of many pulse-heated strands with a power-law energy distribution. We extend that work by including spatially resolved strand modeling and by studying the evolution of emission along the loops in the EUV 94 A and 335 A channels of the Atmospheric Imaging <b>Assembly</b> <b>on-board</b> the Solar Dynamics Observatory. Using the best parameters of the {{previous work}} as the input of the present one, {{we find that the}} amplitude of the random fluctuations driven by the random heat pulses increases from the bottom {{to the top of the}} loop in the 94 A channel and, viceversa, from the top to the bottom in the 335 A channel. This prediction is confirmed by the observation of a set of aligned neighbouring pixels along a bright arc of an active region core. Maps of pixel fluctuations may therefore provide easy diagnostics of nano-flaring regions...|$|R
40|$|Background: With the {{increasing}} use of complex quantitative models in applications throughout the financial world, model risk has become a major concern. The credit crisis of 2008 – 2009 provoked added concern about the use of models in finance. Measuring and managing model risk has subsequently come under scrutiny from regulators, supervisors, banks and other financial institutions. Regulatory guidance indicates that meticulous monitoring of all phases of model development and implementation is required to mitigate this risk. Considerable resources must be mobilised for this purpose. The exercise must embrace model development, <b>assembly,</b> implementation, <b>validation</b> and effective governance. Setting: Model validation practices are generally patchy, disparate and sometimes contradictory, and although the Basel Accord and some regulatory authorities have attempted to establish guiding principles, no definite set of global standards exists. Aim: Assessing the available literature for the best validation practices. Methods: This comprehensive literature study provided a background to the complexities of effective model management and focussed on model validation as a component of model risk management. Results: We propose a coherent ‘best practice’ framework for model validation. Scorecard tools are also presented to evaluate if the proposed best practice model validation framework has been adequately assembled and implemented. Conclusion: The proposed best practice model validation framework is designed to assist firms in the construction of an effective, robust and fully compliant model validation programme and comprises three principal elements: model validation governance, policy and process...|$|R
40|$|Since {{launch in}} May 2002, Aqua MODIS has {{successfully}} operated for nearly 10 years, continuously collecting global datasets for scientific studies of key {{parameters of the}} earth's land, ocean, and atmospheric properties and their changes over time. The quality of these geophysical parameters relies on the input quality of sensor calibrated radiances. MODIS observations are made in 36 spectral bands with wavelengths ranging from visible (VIS) to longwave infrared (LWIR). Its reflective solar bands (RSB) are calibrated using data collected from its on-board solar diffuser and regularly scheduled lunar views. The thermal emissive bands (TEB) are calibrated using an on-board blackbody (BB). The changes in the sensor's spectral and spatial characteristics are monitored by an <b>on-board</b> spectroradiometric calibration <b>assembly</b> (SRCA). This paper presents an overview of Aqua MODIS 10 -year on-orbit operation and calibration activities, from launch to present, and summarizes its on-orbit radiometric, spectral, and spatial calibration and characterization performance. In addition, it will illustrate and discuss on-orbit changes in sensor characteristics and corrections applied to continuously maintain the sensor level 1 B (L 1 B) data quality, as well as lessons learned that could benefit future calibration efforts...|$|R
40|$|Sierra Nevada Corporations Space Systems {{performed}} {{bearing life}} testing for the Scan Mirror Motor/Encoder Assembly (SMMA), {{part of the}} Scan Mirror <b>Assembly</b> <b>on-board</b> the Aerosol Polarimetry Sensor (APS) on the NASA Glory Spacecraft. The baseline bearing life test duration extended beyond the launch date for the Glory Spacecraft; a risk that the program was willing to undertake {{with the understanding that}} if any anomalies or failures occurred before the required life was achieved, then the mission objectives or operating profile could be modified on orbit to take those results into account. Even though the Glory Spacecraft failed to reach orbit during its launch in March of 2011, the bearing life testing was continued through a mutual understanding of value between Sierra Nevada Corporation and our customer; with a revised goal of testing to failure rather than completing a required number of life cycles. Life testing thus far has not only exceeded the original mission required life, but has also exceeded the published test data for Cumulative Degradation Factor (CDF) from NASA/CR- 2009 - 215681. Many lessons were learned along the way regarding long life testing. The bearing life test has been temporarily suspended due to test support equipment issues...|$|R
40|$|An {{evacuation}} {{model validation}} data-set collected {{as part of}} the EU FP 7 project SAFEGUARD is presented. The data was collected from a cruise ship operated by Royal Caribbean International (CS). The trial was a semi-unannounced assembly trial conducted at sea and involved some 2500 passengers. The trial took place at an unspecified time however, passengers were aware that on their voyage an assembly exercise would take place. The validation data-set consists of passenger; response times, starting locations, end locations and arrival times in the <b>assembly</b> stations. The <b>validation</b> data were collected using a novel data acquisition system consisting of ship-mounted beacons, each emitting unique Infra-Red (IR) signals and IR data logging tags worn by each passenger. The results from blind simulations using maritimeEXODUS for the assembly trial are presented and compared with the measured data. Three objective measures are proposed to assess the goodness of fit between the predicted model data and the measured data...|$|R
40|$|Two {{evacuation}} {{model validation}} data-sets collected {{as part of}} the EU FP 7 project SAFEGUARD are presented. The data was collected from a RO-PAX ferry operated by ColorLine (RP 1) and a cruise ship operated by Royal Caribbean (CS). The trials were semi-unannounced assembly trials at sea and involved some 1349 and 2500 passengers respectively. The trials took place at an unspecified time however, passengers were aware that on their voyage an assembly exercise would take place. The validation data-sets consist of passenger; response times, starting locations, end locations and arrival times in the <b>assembly</b> stations. The <b>validation</b> data were collected using a novel data acquisition system consisting of ship-mounted beacons, each emitting unique Infra-Red (IR) signals and IR data logging tags worn by each passenger. The results from blind simulations using maritimeEXODUS for these assembly trials are presented and compared with the measured data. Three objective measures are proposed to assess the goodness of fit between the predicted model data and the measured data...|$|R
40|$|The authors {{describe}} tools {{developed by}} the Computational Nuclear Physics group for testing the quality of internally developed nuclear data and the fidelity of translations from ENDF formatted data to ENDL formatted data used by Livermore. These tests include S{sub n} calculations for the effective k value characterizing critical assemblies and for replacement coefficients of different materials embedded in the Godiva and Jezebel critical assemblies. For those assemblies and replacement materials for which reliable experimental information is available, these calculations provide an integral check {{on the quality of}} data. Because members of the ENDF and reactor communities use calculations for these same <b>assemblies</b> in their <b>validation</b> process, a comparison between their results with ENDF formatted data and their results with data translated into the ENDL format provides a strong check on the accuracy of translations. As a first application of the test suite they present a study comparing ENDL 99 and ENDF/B-V. They also consider the quality of the ENDF/B-V translation previously done by the Computational Nuclear Physics group. No significant errors are found...|$|R
40|$|Within this work, an {{enhanced}} simulation strategy for predicting process-induced distortions and residual stresses of composite structures is developed, while {{taking into account}} probabilistic variations. The simulation strategy is demonstrated for a carbon fibre reinforced frame of an aircraft fuselage. Based on a sequentially coupled temperature-displacement analysis temperature and degree of cure distributions as well as stresses, strains and distortions are determined. For {{the description of the}} reaction kinetic a phenomenological based model considering chemical and diffusion-controlled reactions is introduced. The mechanical material behavior is described by a viscoelastic material model depending on temperature, time and degree of cure. Corresponding process-induced distortions and occurring residual stresses are calculated and analyzed. The results of the process-induced distortions are used to derive a compensated tooling design. In order to analyze the effect of statistical and systematic deviations of underlying process and material parameters a procedure for a probabilistic process simulation is created. Based on a first sensitivity analysis a design of experiment is performed over a specified parameter range. From this surrogate models are derived to enable a full but efficient probabilistic analysis of spring-in angles and residual stresses at multiple frame positions. The effect of process parameter variations and fluctuation of inherent material properties on process-induced distortions is assessed. Moreover, the influence of different process cycles is shown. This allows to determine the expected range of the final part distortion and to derive the resulting tolerances during <b>assembly.</b> For <b>validation</b> of the probabilistic process simulation several frames are manufactured at different process conditions, and frame geometries are measured by digital image correlation technique. The resulting part thicknesses, spring-in angles and global frame distortions are determined. Finally, a comparison of measurements and probabilistic predictions is given, and results are evaluated with respect to main sensitivities...|$|R
40|$|Radiative fluxes and cloud forcings for {{the ocean}} {{areas of the}} Arctic are {{computed}} from the monthly cloud product of the International Satellite Cloud Climatology Project (ISCCP) for 1983 - 90. Spatially averaged short-wave fluxes are compared well with climatological values, while downwelling longwave fluxes are significantly lower. This is {{probably due to the}} fact that the ISCCP cloud amounts are underestimates. Top-of-the-atmosphere radiative fluxes are in excellent agreement with measurements from the Earth Radiation Budget Experiment (ERBE). Computed cloud forcings indicate that clouds have a warming effect at the surface and at the top of the atmosphere during winter and a cooling effect during summer. The net radiative effect of clouds is larger at the surface during winter but greater at the top of the atmosphere during summer. Overall the net radiative effect of clouds at the top of the atmosphere is one of cooling. This is in contrast to a previous result from ERBE data showing arctic cloud forcings have a net warming effect. Sensitivities to errors in input parameters are generally greater during winter with cloud amount being the most important paarameter. During summer the surface radiation balance is most sensitive to errors in the measurements of surface reflectance. The results are encouraging, but the estimated error of 20 W/sq m in surface net radiative fluxes is too large, given that estimates of the net radiative warming effect due to a doubling of CO 2 are on the order of 4 W/sq m. Because it is difficult to determine the accuracy of results with existing in situ observations, it is recommended that the development of improved algorithms for the retrieval of surface radiative properties be accompanied by the simultaneous <b>assembly</b> of <b>validation</b> datasets...|$|R
40|$|Background The {{evolution}} of Next-Generation Sequencing (NGS) has considerably reduced {{the cost per}} sequenced-base, allowing a significant rise of sequencing projects, mainly in prokaryotes. However, the range of available NGS platforms requires different strategies and software to correctly assemble genomes. Different strategies are necessary to properly complete an assembly project, {{in addition to the}} installation or modification of various software. This requires users to have significant expertise in these software and command line scripting experience on Unix platforms, besides possessing the basic expertise on methodologies and techniques for genome assembly. These difficulties often delay the complete genome assembly projects. Results In order to overcome this, we developed SIMBA (SImple Manager for Bacterial Assemblies), a freely available web tool that integrates several component tools for assembling and finishing bacterial genomes. SIMBA provides a friendly and intuitive user interface so bioinformaticians, even with low computational expertise, can work under a centralized administrative control system of assemblies managed by the assembly center head. SIMBA guides the users to execute assembly process through simple and interactive pages. SIMBA workflow was divided in three modules: (i) projects: allows a general vision of genome sequencing projects, in addition to data quality analysis and data format conversions; (ii) assemblies: allows de novo assemblies with the software Mira, Minia, Newbler and SPAdes, also <b>assembly</b> quality <b>validations</b> using QUAST software; and (iii) curation: presents methods to finishing assemblies through tools for scaffolding contigs and close gaps. We also presented a case study that validated the efficacy of SIMBA to manage bacterial assemblies projects sequenced using Ion Torrent PGM. Conclusion Besides to be a web tool for genome assembly, SIMBA is a complete genome assemblies project management system, which can be useful for managing of several projects in laboratories. SIMBA source code is available to download and install in local webservers at [URL]...|$|R
40|$|Next-generation {{sequencing}} (NGS) {{has become}} a powerful sequencing tool, applied {{in a wide range}} of biological studies. However, the traditional sample preparation protocol for NGS is non-strand-specific (NSS), leading to biased estimates of expression for transcripts overlapped at the antisense strand. Strand-specific (SS) protocols have recently been developed. In this study, we prepared the same RNA sample by using the SS and NSS protocols, followed by sequencing with Illumina HiSeq platform. Using real-time quantitative PCR as a standard, we first proved that the SS protocol more precisely estimates gene expressions compared with the NSS protocol, particularly for those overlapped at the antisense strand. In addition, we also showed that the sequence reads from the SS protocol are comparable with those from conventional NSS protocols in many aspects. Finally, we also mapped a fraction of sequence reads back to the antisense strand of the known genes, originally without annotated genes located. Using sequence <b>assembly</b> and PCR <b>validation,</b> we succeeded in identifying and characterizing the novel antisense genes. Our results show that the SS protocol performs more accurately than the traditional NSS protocol and can be applied in future studies...|$|R
40|$|It {{is still}} not common in {{research}} on software quality to delve into non-technical issues. In the particular case of implementing customized information systems software (CISS), the field is also not completely {{aware of the importance}} of managing customers with a formal and objective set of measures that account for their responsibility in projects. CISS products – whose source code is ultimately developed according to each customer’s demands on core business processes – ask developers to pay unique attention to issues like competencies, culture, strategy, and resources of the client organization, as well as to the industry’ critical success factors, best practices, and prospects. The present research adds to software engineering and to organizational theory by introducing a conceptual framework (rationale) and a set of seven indicators, 27 metrics and 88 measures for improving the knowledge and the managerial practices regarding the participation of customers in CISS development. The focus is on managing the customer team (CuTe) – professionals from the client organization that contracts CISS projects, who are assigned special business and information technology (IT) roles for interacting with outsourced developers in such projects, since both customer and external (outsourced) developer teams share project authority and responsibility. Research insights and assumptions were developed throughout a six-year professional interaction with companies in a major Brazilian IT cluster; and a three-year case study within a landmark enterprise resource planning (ERP) implementation in a Brazilian university, supported by indepth interviews with key CuTe professionals in the project, provided the research with compelling data for the <b>assembly</b> and <b>validation</b> of findings. The resultant framework – formed by the rationale and the measurement instruments – is called Model for Eliciting Team Resources and Improving Competence Structures (METRICS), and it is to be used in the industry by customers and external developers to help plan, control, assess, and make historical records of CuTe design and performance in CISS projects. Academicians also benefit from the incorporation of a new perspective with which to deal with the customersupplier interaction in IT endeavors...|$|R
40|$|Copyright © 2015 Kuo-Wang Tsai et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Next-generation sequencing (NGS) has become a powerful sequencing tool, applied {{in a wide range}} of biological studies. However, the traditional sample preparation protocol for NGS is non-strand-specific (NSS), leading to biased estimates of expression for transcripts overlapped at the antisense strand. Strand-specific (SS) protocols have recently been developed. In this study, we prepared the same RNA sample by using the SS and NSS protocols, followed by sequencing with Illumina HiSeq platform. Using real-time quantitative PCR as a standard, we first proved that the SS protocol more precisely estimates gene expressions compared with the NSS protocol, particularly for those overlapped at the antisense strand. In addition, we also showed that the sequence reads from the SS protocol are comparable with those from conventional NSS protocols in many aspects. Finally, we also mapped a fraction of sequence reads back to the antisense strand of the known genes, originally without annotated genes located. Using sequence <b>assembly</b> and PCR <b>validation,</b> we succeeded in identifying and characterizing the novel antisense genes. Our results sho...|$|R
40|$|Microvibrations of a {{satellite}} reaction wheel assembly are commonly analysed in either hard-mounted or coupled boundary conditions, though coupled wheel-to-structure disturbance models are {{more representative of}} the real {{environment in which the}} wheel operates. This article investigates the coupled microvibration dynamics of a cantilever configured reaction wheel assembly mounted on either a stiff or flexible platform. Here a method is presented to cope with modern project necessities: (i) need of a model which gives accurate estimates covering a wide frequency range; (ii) reduce the personnel and time costs derived from the test campaign, (iii) reduce the computational effort without affecting the quality of the results. The method involves measurements of the disturbances induced by the reaction wheel assembly in a hard-mounted configuration and of the frequency and speed dependent dynamic mass of the reaction wheel. In addition, it corrects the approximation due to missing speed dependent dynamic mass in conventional reaction wheel assembly microvibration analysis. The former was evaluated experimentally using a previously designed and validated platform. The latter, on the other hand, was estimated analytically using a finite element model of the wheel <b>assembly.</b> Finally, the <b>validation</b> of the coupled wheel-structure disturbance model is presented, giving indication of the level of accuracy that can be achieved with this type of analyses...|$|R
