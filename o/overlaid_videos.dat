1|47|Public
40|$|Tese de mestrado, Arte e Multiméia - Audiovisuais, Universidade de Lisboa, Faculdade de Belas Artes, 2012 This work concludes a proof in a Master degree on Multimedia Art in {{the audio}} visual field. The idea of memory, with the {{possibility}} of its immateriality and dissolution, and reminiscence are the subject matter of this project. Human memory, to which attention is devoted here for its decisive importance to the gathering of knowledge, is the ability to store information. Immaterial, it has the quality of that which is intangible, without substance, possibly spiritual or supernatural; frail and perishable, from it, records are created which seek to ensure their conservation. Often, through wear and transformation by imagination, the memory, in its immateriality, undergoes a process of dissolution. Reminiscence stems from a vague, not very consistent memory, it is but a process that enables the reconstitution of an idea, by differentiation, selection and recovery of memories. This seemingly simple and familiar theme, proved more complex throughout this project’s process of reflection and creation, requiring a vast and profound research. Several different approaches and artistic works were observed and analysed: from Anton Giulio Bragaglia (1890 - 1960), from José Luís Neto (1966 -), from Alain Resnais (1922 -), from Michel Gondry (1963 -), from Christopher Nolan (1970 -), from Ridley Scott (1937 -) and from David Clearbout (1969 -), on which memory is also addressed (albeit subtly) as something elusive and difficult to fix. The created installation consists in a set of projections comprised by two <b>overlaid</b> <b>videos</b> on the same, discontinuous translucent surface, which can be observed from several different angles. In it the two different facets of this project are synthetized, theory and practice. It aims to trigger a process of reminiscence by establishing a relationship between the observation of images and visual memorie...|$|E
2500|$|The 2006 shows {{included}} an 'all new video light show' by Sterile Cowboys & Co. (a.k.a. Nicolas Jenkins) – three screens of heavily <b>overlaid</b> <b>video</b> {{with the middle}} screen overlaid by yet another layer of [...] "analog" [...] projections including moiréd overlays and liquid/oil effects performed by [...] "something human" [...] a.k.a. Caleigh Fisher a friend from the TOPY years. Videos from the upcoming album and DVDs were previewed as works in process. Much of the video work revolves around Breyer P-Orridges exploration of the 'pandrogyne'.|$|R
5000|$|By 2011, augmenting people, objects, {{and landscapes}} {{had become a}} {{recognized}} art style. For example, in 2011, artist Amir Bardaran's work, [...] "Frenchising the Mona Lisa" [...] <b>overlaid</b> <b>video</b> on Da Vinci's painting using an AR mobile application called Junaio. The AR app allowed the user to train his or her smartphone on Da Vinci's Mona Lisa and watch the lady loosen her hair and wrap a French flag around her visage in the form an Islamic hijab. The wearing of a hijab was controversial in France at the time.|$|R
5000|$|Videotapes {{that are}} {{recorded}} with timecode numbers <b>overlaid</b> on the <b>video</b> {{are referred to}} as window dubs, named after the [...] "window" [...] that displays the burnt-in timecode on-screen.|$|R
50|$|Cognitech, an {{independent}} video laboratory, superimposed video imagery {{taken of the}} Phoenix Lights onto video imagery it shot during daytime from the same location. In the composite image, the lights are seen to extinguish at the moment they reach the Estrella mountain range, which is visible in the daytime, but invisible in the footage shot at night. A broadcast by local Fox Broadcasting Company affiliate KSAZ-TV claimed to have performed a similar test that showed the lights were {{in front of the}} mountain range and suggested that the Cognitech data might have been altered. Dr. Paul Scowen, visiting professor of Astronomy at Arizona State University, performed a third analysis using daytime imagery <b>overlaid</b> with <b>video</b> shot of the lights and his findings were consistent with Cognitech. The Phoenix New Times subsequently reported the television station had simply <b>overlaid</b> two <b>video</b> tracks on a video editing machine without using a computer to match the zoom and scale of the two images.|$|R
30|$|These {{systems do}} not aim {{to detect the}} {{fingering}} and handing which a player is actually using (a pair of gloves are tracked in [1], and graphics information is <b>overlaid</b> on captured <b>video</b> in [2] and [3]). We have developed a different approach from most of these researches.|$|R
2500|$|Resolution: 256×192 (16 colours). In reality, {{there are}} just 15 colour tints available, because, just like Sinclair Spectrum there are two codes for black. Unlike the Spectrum, however, one of the blacks is {{actually}} [...] "transparent", so the MSX video picture could be <b>overlaid</b> on another <b>video</b> signal, for example one from a video disk.|$|R
40|$|Abstract—Community {{streaming}} is {{an enhanced}} {{form of joint}} content viewing where {{a sense of community}} is reinforced by the addition of interactive visual overlays, controlled in real-time by viewers, on top of a shared video stream. As a concrete example, we describe a community video system called ECHO, where personalized avatars are overlaid on top of a real-time encoded video stream of an Internet game for multicast consumption. Recognizing that only the visual overlays are generated live, we propose schemes that encode and schedule the live and non-live portions of the <b>overlaid</b> <b>video</b> separately in order to exploit the difference in delay sensitivity of the two, leading to video streams that contain two sub-streams with different delay constraints. We show that, in the known channel case, a low complexity “earliest deadline first ” packet scheduling algorithm minimizes receiver buffer delay. We also analyze the case where multiple streams are multiplexed, which allows us to quantify the potential gains of allowing different delay constraints for different sub-streams. We show that a “water filling ” strategy maximizes the total number of streams that can be supported. Simulation results show that the bandwidth necessary to maintain low-latency for visual overlays is reduced by about 40 % when our proposed sub-stream approach is used. For multiplexing of multiple streams, our approach can increase the number of supported streams (e. g., a 30 % increase when around ten streams are multiplexed). Index Terms—Video coding, video streaming. I...|$|R
5000|$|Initially, a lyric {{video for}} [...] "Fuck You" [...] was {{released}} to YouTube on August 19, 2010, featuring kinetic typography, with {{the lyrics of}} the song appearing on different colored backgrounds with film grain <b>overlaid</b> on the <b>video.</b> The same thing {{was done in the}} German and Spanish versions of the video, translating the lyrics, although the vocals remained in English.|$|R
5000|$|<b>Overlaid</b> {{comments}} on <b>videos</b> (弹幕dàn mù): Only signed up users {{are able to}} {{comments on}} the videos. If users do not want see the comments overlaid while watching, they can hide the comments. When users reach level 5, the advanced comment functions would be unblock. Otherwise, the users can only send 30 comments and 80 words each comment per day. The users can block the unwanted wordings while watching videos.|$|R
50|$|Clock idents are {{typically}} displayed as an analogue clock, although some broadcasters have experimented with digital clocks. In particular, during the 1970s and 1980s, many ITV {{regions in the}} UK adopted digital clock designs, which are overlaid onto a colored card using CSO. The backgrounds were generally static, but some clocks had movement. For example, Associated-Rediffusion had a spinning Adastral. The final clocks from 1995-1998, used by RTÉ One and RTÉ Two, were <b>overlaid</b> onto a <b>video</b> background.|$|R
40|$|The UPC {{system works}} by extracting monomodal signal {{segments}} (face tracks, speech segments) that overlap {{with the person}} names <b>overlaid</b> in the <b>video</b> signal. These segments are assigned directly {{with the name of}} the person and used as a reference to compare against the non-overlapping (unassigned) signal segments. This process is performed independently both on the speech and video signals. A simple fusion scheme is used to combine both monomodal annotations into a single one. Postprint (published version...|$|R
40|$|Virtual Interaction {{refers to}} a {{technique}} for interacting with computer generated graphics. Graphical objects are <b>overlaid</b> on live <b>video</b> of the user. A chromakey separates the user (foreground) from the background resulting in a silhouette of user. The computer causes the graphical objects to move {{in relation to the}} silhouette so that they appear to interact with the user. This paper presents the implementations of the system, some techniques for interaction and discusses using the system as a tool for physical therapy. 1...|$|R
40|$|This thesis {{describes}} {{research into}} developing a client/server ar- chitecture for a mobile Augmented Reality (AR) application. Following the earthquakes that have rocked Christchurch {{the city is}} now changed forever. CityViewAR is an existing mobile AR application designed {{to show how the}} city used to look before the earthquakes. In CityViewAR 3 D virtual building models are <b>overlaid</b> onto <b>video</b> captured by a smartphone camera. However the current version of CityViewAR only allows users to browse information stored on the mobile device. In this research the author extends the CityViewAR application to a client-server model so that anyone can upload models and annotations to a server and have this information viewable on any smartphone running the application. In this thesis we describe related work on AR browser architectures, the system we developed, a user evaluation of the prototype system and directions for future work...|$|R
40|$|Recently modern non-Euclidean {{structure}} and motion estimation {{methods have been}} incorporated into augmented reality scene tracking and virtual object registration. We present a study of how the choice of projective, affine or Euclidean scene viewing geometry and similarity, affine or homography based object registration affects how accurately a virtual object can be <b>overlaid</b> in scene <b>video</b> from varying viewpoints. We found that projective and affine methods gave accurate overlay to a few pixels, while Euclidean geometry obtained by auto calibrating the camera was not as accurate and gave about 15 pixel overlay error...|$|R
50|$|The channel {{featured}} shortened trailers {{to preview}} movies and events expected to {{air on the}} provider's pay-per-view services, which were provided to cable and satellite operators regularly both on laserdisc as well as via a continuous satellite feed. These short trailers, whether sourced locally or delivered via satellite, would feature listings information {{at the bottom of}} the screen, which was locally <b>overlaid</b> into their <b>video</b> feed to provide system-specific scheduling information, with countdowns to the next telecast of the program appearing at the top left of the screen.|$|R
40|$|The {{submitted}} {{portfolio of}} work {{emerges from a}} focus on treating musicking bodies as compositional material. The work explores aspects of awkwardness in performance, slow motion movement, confrontation, simultaneous and multiple forms of intersubjective identity, public presentations of private activities, and dialogic relationships with performance. Because of these interests, and their grounding in performance, my practice has involved developing compositional approaches and strategies for working with documented forms of performance. The accompanying written commentary reflects {{on the findings of}} this investigation by focusing primarily on techniques of working with documents of performances. By considering Nicholas Cook’s notion of scores-as-scripts, by which musical scores are expanded from being isolated and autonomous texts of musical work to existing in relationship with instances of performance, I propose the notion of documents-as-scores. Reflecting on the capacity for documentation to transform representations and manifestations of performance, I suggest that chirographic and/or typographic representations of musical notation inscribed in the document-form of sheet music have the potential to function as documentation of performance. Expanding on this potential, and drawing from various definitions of the word “document,” I suggest that other document-forms such as audio/video files or human bodies can be musically inscribed to function as scores for performance. These scores are made of document-forms inscribed with information that I treat as material subject to compositional protocols of manipulation, which include protraction, expansion, situation, distortion, effacement, dislocation, isolation, and contextualization, among others. To narrow the scope of this research, I focus on ways in which musicking bodies are intellectually/physically engaged with, represented in, and embodiments of these documents-as-scores. Integrating examples from the portfolio, the commentary introduces the notion of documents-as-scores and proceeds to examine ways of working with different document-forms. In Chapter 1, physical and digital forms of notation are effaced to articulate facets of awkwardness and integrative destruction in music. In Chapter 2, distended, incomplete, and <b>overlaid</b> <b>video</b> and audio recordings are reflected in performance by looking and listening for representations and indices of physical action. In Chapter 3, humans/persons become formally constitutive embodied documents whose verbal, physical, and musical memories are situated within performative reading contexts...|$|R
5000|$|The NASA X-38 {{was flown}} using a Hybrid Synthetic Vision system that {{overlaid}} map data on video to provide enhanced navigation for the spacecraft during flight tests from 1998 to 2002. It used the LandForm software and was useful for times of limited visibility, including an instance when {{the video camera}} window frosted over leaving astronauts {{to rely on the}} map overlays. [...] The LandForm software was also test flown at the Army Yuma Proving Ground in 1999. In the photo at right one can see the map markers indicating runways, air traffic control tower, taxiways, and hangars <b>overlaid</b> on the <b>video.</b>|$|R
500|$|Ryo {{chose to}} {{distribute}} his music on Nico Nico Douga {{because he liked}} the website along with its response system, which enables user's comments to appear <b>overlaid</b> on the <b>video</b> screen. Ryo did not originally intend to use Hatsune Miku as the vocalist for his songs, and most members of Supercell {{did not even know}} of Miku until after [...] "Melt" [...] gained popularity. [...] Since Ryo did not know any singers, he was recommended by his friends to use Miku, which he thought was a good idea since videos using Miku were already widely being distributed on Nico Nico Douga at the time.|$|R
40|$|A {{new device}} that {{combined}} high-resolution (1080 p) wide-angle video and three channels of high-frequency acoustic recordings (at 500 kHz per channel) in a portable underwater housing {{was designed and}} tested with wild bottlenose and spotted dolphins in the Bahamas. It consisted of three hydrophones, a GoPro camera, a small Fit PC, a set of custom preamplifiers and a high-frequency data acquisition board. Recordings were obtained to identify individual vocalizing animals through time-delay-of-arrival localizing in post-processing. The calculated source positions were then <b>overlaid</b> onto the <b>video</b> – providing the ability to identify the vocalizing animal on the recorded video. The new tool allowed for much clearer analysis of the acoustic behavior of cetaceans than was possible before...|$|R
5000|$|The album's lead single [...] "Fuck You" [...] was {{released}} on August 19, 2010, and charted at number two on the Billboard Hot 100. It also became an international hit and peaked within the top-10 of charts in several countries, including {{number one in the}} Netherlands and the United Kingdom. The radio edit of the song was entitled [...] "Forget You", while another edit is simply entitled [...] "FU". A music video for the song {{was released}} on YouTube on August 19, 2010, featuring the lyrics of the song appearing on different colored backgrounds with film grain <b>overlaid</b> on the <b>video.</b> The video went viral, receiving over two million views within a week of its release. The official music video {{was released on}} September 1, 2010.|$|R
30|$|Barnum et al. [19] took {{a similar}} approach. In their method, an {{acquired}} background image {{is divided into}} two planes related to a moving object and a non-moving object behind the moving object. Then, they replaced the background image with the pre-observed image. It should be noted as well that they used a relay camera for the main and background observer cameras to create a common region between the two, and therefore, these two cameras can be placed far from each other [19]. Sugimoto et al. switched multi-view RGB-D cameras to cover a wide range of the real-time background observation, and still, unobservable areas are compensated from the past frames [45]. Mei et al. used a 3 D map constructed using vSLAM beforehand to localize the current camera and <b>overlaid</b> a real-time <b>video</b> of a surveillance camera in the current view [27].|$|R
40|$|We {{present a}} {{framework}} for tangible user interfaces on handheld devices {{through the use of}} augmented reality, where virtual objects can be manipulated and arranged using physical objects in the real world. Visual feedback is provided in the high-resolution handheld display, where virtual objects are <b>overlaid</b> onto live <b>video</b> from the camera. The virtual objects are registered and tracked in real-time relative to the physical environment using the device’s camera. Realistic lighting and rendering of high-resolution virtual models is achieved through hardwareaccelerated graphics and shadow mapping. The user can interact with the virtual objects and system parameters both through an overlaid menu interface and through direct touch-screen interaction. We describe our framework, our adaptation of the ARToolKitPlus tracking library for a mobile platform, and a number of interaction techniques that we implemented for a prototype urban planning application...|$|R
40|$|Human {{language}} {{content in}} video is typically manifested either as spoken audio that accompanies the visual content, or as text that is <b>overlaid</b> on the <b>video</b> or {{contained within the}} video scene itself. The bulk of research and engineering in language extraction from video thus far has focused on spoken language content. More recently, researchers have also developed technologies capable of detecting and recognizing text content in video. Anecdotal evidence indicates {{that in the case}} of rich multi-media sources such as Broadcast News, spoken and textual content provide complementary information. Here we present the results of a recent BBN study in which we compared named entities, a critically important type of language content, between aligned speech and videotext tracks. These new results show that videotext content provides significant additional information that does not appear in the speech stream...|$|R
40|$|The space-time bag-of-features (BoF) {{approach}} {{is the most popular}} pipeline for challenging human action data [5, 9], however classification performance diminishes with dataset difficulty (e. g. HMDB [4]). ◮ State-of-the-art methods [8, 2, 4, 6] derive action representations from an entire video clip, even though this may contain motion and scene patterns pertaining to multiple action classes. ◮ Different actions that have similar motions may lead to confusion between classes. Our approach ◮ Human actions may naturally be described as a collection of parts. Figure: A training video sequence taken from the KTH dataset [7] plotted in space and time. <b>Overlaid</b> on the <b>video</b> are discriminative cubic action subvolumes learned in a max-margin multiple instance learning framework, with colour indicating their class membership strength. ◮ In our framework, action models are derived from smaller portions of th...|$|R
5000|$|Besides hosting video content, Bilibili's core {{feature is}} a {{real-time}} commentary subtitle system that displays user comments as streams of moving subtitles <b>overlaid</b> on the <b>video</b> playback screen, visually resembling a danmaku shooter game. These subtitles are called [...] "danmu" [...] (literally [...] "bullets"). Such subtitles are simultaneously broadcast to all viewers in real-time, creating a chat room experience in which users feel like watching and playing together with others. This system offers users various subtitle controls, including style, format, and movement. Users are also fond of creating translated and soramimi subtitles, or special effects with carefully formed subtitles. The site {{also offers a}} feature called [...] "advanced subtitles", where users can use ECMAScript-based API to control video playback, dynamically change danmu subtitles and draw shapes onto the screen. This functionality is only available with the video poster's permission.|$|R
40|$|We {{present in}} this article a video OCR system that detects and recognizes <b>overlaid</b> texts in <b>video</b> {{as well as its}} {{application}} to person identification in video documents. We proceed in several steps. First, text detection and temporal tracking are performed. After adaptation of images to a standard OCR system, a final post-processing combines multiple transcriptions of the same text box. The semi-supervised adaptation of this system to a particular video type (video broadcast from a French TV) is proposed and evaluated. The system is efficient as it runs 3 times faster than real time (including the OCR step) on a desktop Linux box. Both text detection and recognition are evaluated individually and through a person recognition task where it is shown that the combination of OCR and audio (speaker) information can greatly improve the performances of a state of the art audio based person identification system. Index Terms — Video OCR, text detection, text recognition, semi-supervised parametrization, person identification. 1...|$|R
40|$|Abstract. We {{created an}} exhibit {{based on a}} new {{locomotion}} interface for swimming in a virtual reality ocean environment {{as part of our}} Swimming Across the Pacific art project. In our exhibit we suspend the swimmer using a hand gliding and leg harness with pulleys and ropes in an 8 ft-cubic swimming apparatus. The virtual reality ocean world has sky, sea waves, splashes, ocean floor and an avatar representing the swimmer who wears a tracked headmounted display so he can watch himself swim. The audience sees the swimmer hanging in the apparatus <b>overlaid</b> on a <b>video</b> projection of his ocean swimming avatar. The avatar mimics the real swimmer’s movements sensed by eight magnetic position trackers attached to the swimmer. Over 500 people tried swimming and thousands watched during two exhibitions. We report our observations of swimmers and audiences engaged in and enjoying the experience leading us to identify design strategies for interactive exhibitions...|$|R
40|$|AbstractContact analog head-up {{displays}} (cHUDs) {{enable the}} presentation of augmented reality (AR) information in the driver's primary field of view and are a promising display innovation {{in light of the}} increasing degree of assistance and automation in modern cars. As cHUD technology still faces several limitations, robust and error-tolerant display concepts are required. Four design variations (cut-off, no cut-off, tilt, 2 D) of two display concepts (boomerang, arrow) for contact analog navigation were realized in a mock-up setting in which the virtual cHUD image was <b>overlaid</b> on a <b>video</b> recording of a real driving scene. Thirty participants (within-subjects design) rated attractiveness, positional accuracy, functionality, clearness/unambiguousness, distraction, quality of 3 D representation, interpretability, and intuitiveness. The results suggest that the variation tilt cannot be recommended for application in an automotive cHUD. The boomerang concept was preferred over the arrow concept. The results have important implications for the design of contact analog information in an automotive cHUD...|$|R
40|$|Technical session OH 5 : Multimedia Content Analysis, {{understanding}} and Retrieval VInternational audienceWe present {{in this article}} a video OCR system that detects and recognizes <b>overlaid</b> texts in <b>video</b> {{as well as its}} application to person identification in video documents. We proceed in several steps. First, text detection and temporal tracking are performed. After adaptation of images to a standard OCR system, a final post-processing combines multiple transcriptions of the same text box. The semi-supervised adaptation of this system to a particular video type (video broadcast from a French TV) is proposed and evaluated. The system is efficient as it runs 3 times faster than real time (including the OCR step) on a desktop Linux box. Both text detection and recognition are evaluated individually and through a person recognition task where it is shown that the combination of OCR and audio (speaker) information can greatly improve the performances of a state of the art audio based person identification system...|$|R
5000|$|By late 1993, Prevue Guide was rebranded as [...] "Prevue Channel," [...] and {{an updated}} channel logo was {{unveiled}} to match. Beginning in early 1994 and up until its {{first couple of}} years as the TV Guide Channel, the network licensed production music (first at one-minute lengths, later at 15- and 30-second lengths) from several music libraries for use as interstitial music. In 1996, the Prevue Channel logo was given a new eye-like design, and two years later, the classic Dodger-style typeface its logo had incorporated since 1988 was replaced with an italicized lower-case Univers, though Sneak Prevue continued to use the original logo font until it shut down in 2002. In 1997, Prevue Channel became the first electronic program guide to show formalized TV ratings symbols for Canada and the United States, which appeared alongside program titles within the listings grid, {{as well as in the}} supplementary scheduling information <b>overlaid</b> accompanying promo <b>videos</b> in the top half of the screen.|$|R
40|$|Figure 1 : Left to right: a frame {{from the}} input video sequence, a partial tracing of the model, {{the final model}} <b>overlaid</b> on the <b>video,</b> {{and the result of}} {{rendering}} the final model back into the original sequence. VideoTrace is a system for interactively generating realistic 3 D models of objects from video—models that might be inserted into a video game, a simulation environment, or another video sequence. The user interacts with VideoTrace by tracing the shape of the object to be modelled over one or more frames of the video. By interpreting the sketch drawn by the user in light of 3 D information obtained from computer vision techniques, a small number of simple 2 D interactions can be used to generate a realistic 3 D model. Each of the sketching operations in VideoTrace provides an intuitive and powerful means of modelling shape from video, and executes quickly enough to be used interactively. Immediate feedback allows the user to model rapidly those parts of the scene which ar...|$|R
40|$|We {{created an}} exhibit {{based on a}} new {{locomotion}} interface for swimming in a virtual reality ocean environment {{as part of our}} Swimming Across the Pacific art project. In our exhibit we suspend the swimmer using a hand gliding and leg harness with pulleys and ropes in an 8 ft-cubic swimming apparatus. The virtual reality ocean world has sky, sea waves, splashes, ocean floor and an avatar representing the swimmer. The swimmer wears a tracked head-mounted display so he can watch himself swim. The audience sees the swimmer hanging in the apparatus <b>overlaid</b> on a <b>video</b> projection of his ocean swimming avatar. The avatar mimics the real swimmer’s movements sensed by eight magnetic position trackers attached to the swimmer. Over 400 people tried swimming and thousands watched during a five-day exhibition. Our observations of swimmers and audiences engaged in and enjoying the experience lead us to identify design strategies for interactive exhibitions. Elements of the interface design of the whole system impacted the following four distinct user groups: individual swimmer, swimmer as a group of swimmers, attendants and audience, providing valuable insight to create new interactive installation experiences. Author Keywords Virtual swimming, virtual reality, Swimming Across th...|$|R
40|$|AbstractBiomechanical {{feedback}} in water-based rowing {{is traditionally}} presented as paper reports or <b>video</b> <b>overlaid</b> with data once a session has been completed. Research into {{the provision of}} extrinsic feedback in sport suggests that real-time feedback can lead to skill acquisition and, when appropriately applied, lead to skill retention during competition and therefore a positive performance outcome. This paper presents a novel system architecture that delivers real-time feedback using commercially available off-the-shelf components. The development of a rowing specific system to test a range of feedback strategies is presented, including fading feedback, mixing feedback modalities and varying of the frequency and timing of feedback. MoSync, a cross-platform smartphone development language, was used to write the client application while the server was written as an embedded application in C and Lua that ran {{on top of the}} OpenWrt open-source router operating system. Data was transmitted wirelessly across a Wi-Fi network. A human-centred design process was led by a group of high-performance athletes and coaches and the system was shown to deliver data to up to 10 clients simultaneously. Future research will investigate the efficacy of a variety of different feedback strategies to rowers...|$|R
40|$|Videos {{combined}} with quizzes and polls {{are used for}} educational purposes in many settings, including Massive Open Online Courses (MOOCs) and university lectures. For the authors of these resources, this often involves splitting videos up into sections with a poll or quiz after each section to gauge understanding. This approach requires the author to have video editing skills, and can be time-consuming. It would be useful if quizzes could be created and included directly into videos {{without the need for}} video editing. The resulting media could be analysed to discover how best to present information for learning. This paper describes the development of three main tools to Enhance Synote, the web based video annotation System. The Quiz Authoring Tool allows users to specify: sets of questions and polls to appear in the video, the time at which question sets should appear, and actions to be taken when questions are answered (e. g. skip back in the video if the answer is incorrect). The Questions Overlay library to allow quizzes and polls to be <b>overlaid</b> on Web <b>videos</b> in any of the main video formats, playable in the major browsers on different platforms. The Video and Quiz Analytics records and displays metrics of user behavio...|$|R
40|$|An {{important}} {{recent development}} in the simulation techniques was {{the changes in the}} mode of presentation: from passive mode to active one. It is now possible to present an image according to the observeris voluntary movement of body and head by means of a head-mounted display. Such interactive simulation system, which allows people to observe what they like to see, is suitable to study environmental perception, because active attention is essential to manipulate enormous information in the environment. The present paper reports two case studies in which an interactive simulation system was developed to test psychological impact of interior and exterior spaces: the case study 1 intended to clarify the effect of the disposition of transparent and opaque surfaces of a room on the occupantsi “sense of enclosurei, the case study 2 intended to make clear some physical features along a street which are influential for changing atmosphere. In addition to the empirical research, an attempt to develop a new simulation system which uses both analogue and digital images is briefly reported, and a preliminary experiment was conducted to test the performance of the simulation system in which such movable elements as pedestrians and cars generated by real-time CG were <b>overlaid</b> on the <b>video</b> image of a scale model street...|$|R
