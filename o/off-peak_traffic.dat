26|13|Public
25|$|Like the Gulf Freeway, the North Freeway {{soon became}} congested. The oil {{boom of the}} 1970s {{resulted}} in large-scale residential development along the highway, most notably The Woodlands. Since the corridor was strongly directional, with 65% of peak-hour traffic going in the peak direction, a 9.6-mile (15.4km) contraflow lane for buses and other high-occupancy vehicles (HOV) was implemented later that decade, opening on August 28, 1979 between downtown and Shepherd Drive (exit 56B). The facility, operating during both rush hour periods, occupied the leftmost lane of the other direction, and was separated from the other lanes with a movable pylon every 40 feet (12 m). In 1980, the existing center breakdown lanes were restriped for HOV traffic for about two miles (3km) from {{the north end of}} the contraflow lane. However, <b>off-peak</b> <b>traffic</b> was increasing, and construction began in 1983 on a more permanent reversible transitway in the median. This, the second transitway in Houston (a month after the one on the Katy Freeway), opened on November 23, 1984, replacing the contraflow lane.|$|E
50|$|There is {{a bus to}} Shoin University {{which takes}} 15 minutes during <b>off-peak</b> <b>traffic.</b>|$|E
5000|$|Gig Harbor is one {{of several}} cities and towns that claim to be [...] "the gateway to the Olympic Peninsula". Due to its close access to several state and city parks, and {{historic}} waterfront that includes boutiques and fine dining, it has become a popular tourist destination. Gig Harbor is located along State Route 16, about six miles (10 km) from its origin at Interstate 5, over the Tacoma Narrows Bridge. A $1.2 billion project to add a second span to the bridge was completed in 2007. During <b>off-peak</b> <b>traffic</b> times, Tacoma can be reached in five minutes and Seattle in just under an hour.|$|E
30|$|Using vehicle {{movement}} {{data from}} RFID for Nanjing {{for the month}} of May 2014, the study employed SEM approach to investigate the effects of weekday and weekend vehicle movements, road type choice and type of car (private cars and taxis) on peak and off-peak period movement. The model results postulated that weekday, weekend, road choice type and car type (taxis and private cars) influence peak and <b>off-peak</b> period <b>traffic</b> in different ways.|$|R
50|$|Capital Cymru {{broadcasts}} hourly {{local news}} updates from 6am-7pm on weekdays and 6am-12pm at weekends with headlines on the half-hour during Capital Breakfast on weekdays. On weekdays, breakfast and drivetime bulletins are broadcast in Welsh only while <b>off-peak</b> bulletins and <b>traffic</b> updates are in English with a bilingual bulletin broadcast at 6am.|$|R
5000|$|Ramp meters are {{claimed to}} reduce {{congestion}} (increase speed and volume) on freeways by reducing demand and by breaking up platoons of cars. Two variations of demand reduction are commonly cited; one being access rate, the other diversion. [...] Some ramp meters are designed and programmed to operate only {{at times of}} peak travel demand; such meters are turned off during <b>off-peak</b> times allowing <b>traffic</b> to merge onto the freeway without stopping. Other ramp meters are designed to operate continuously, only being turned off for maintenance or repairs.|$|R
50|$|Hahn cites {{her efforts}} {{to clean up the}} Port of Los Angeles as one of her main accomplishments while on the City Council. The 2006 Clean Air Action Plan, which she and Mayor Antonio Villaraigosa pushed forward, set a goal of {{reducing}} pollution by 45 percent within five years and shifted the movement of goods at the ports to <b>off-peak</b> <b>traffic</b> hours. Hahn supported the addition of the Clean Trucks Program that requires the 16,000 diesel trucks serving the ports meet 2007 EPA emission standards within five years. She has noted that the ports have been Southern California's largest emitter of greenhouse gasses and diesel emissions and that the Clean Trucks Program also provides for improved working conditions, wages and benefits for port truckers. Prior to the Clean Air Action Plan, she had already shifted about 35% of goods to be moved during off-peak hours. Hahn also helped advance redevelopment projects at the Port of Los Angeles in both San Pedro and Wilmington.|$|E
50|$|Like the Gulf Freeway, the North Freeway {{soon became}} congested. The oil {{boom of the}} 1970s {{resulted}} in large-scale residential development along the highway, most notably The Woodlands. Since the corridor was strongly directional, with 65% of peak-hour traffic going in the peak direction, a 9.6-mile (15.4 km) contraflow lane for buses and other high-occupancy vehicles (HOV) was implemented later that decade, opening on August 28, 1979 between downtown and Shepherd Drive (exit 56B). The facility, operating during both rush hour periods, occupied the leftmost lane of the other direction, and was separated from the other lanes with a movable pylon every 40 feet (12 m). In 1980, the existing center breakdown lanes were restriped for HOV traffic for about two miles (3 km) from {{the north end of}} the contraflow lane. However, <b>off-peak</b> <b>traffic</b> was increasing, and construction began in 1983 on a more permanent reversible transitway in the median. This, the second transitway in Houston (a month after the one on the Katy Freeway), opened on November 23, 1984, replacing the contraflow lane.|$|E
5000|$|A {{limitation}} of implementing a reversible flow design {{is that it}} cannot serve congestion that may {{be present in the}} <b>off-peak</b> <b>traffic</b> direction. If such is the case, then some users, such as deadheading transit buses that need trip reliability to make a second peak direction run during the commute period, will be adversely impacted. All freeway reversible lanes must be separated by [...] "Jersey" [...] barriers in a high-speed roadway setting (which is not the case on arterial treatments). They are typically constructed in the median of freeway facilities and may be one, two or more lanes wide. These characteristics have several associated advantages and disadvantages. A facility that changes direction to serve morning and afternoon traffic can be an efficient solution since it allocates capacity specifically to the most congested direction of travel. Reversible lanes offer a much higher guaranteed level of service for transit since side friction from adjacent traffic is removed. Some locales, notably Houston, implemented reversible lanes to address the peak direction alone since width was not available to address both directions of travel. Adapting a reversible flow lane or roadway into a freeway typically requires rebuilding most bridges with center columns. A disadvantage of reversible lanes is the ongoing cost of daily surveillance and lane/ramp reversal activities. These treatments must be designed to prevent wrong way movements, requiring extensive and redundant ITS and traffic control device treatments for each opening, plus a staff compliment who must visually inspect the roadway prior to each opening period. Tolling and enforcement is made easier by the barrier environment in which a single field location can be identified to monitor and/or toll all traffic flow.|$|E
40|$|Mobile {{communication}} {{networks are}} usually planned {{to provide some}} minimum service quality level during peak traffic hours. Consequently, in <b>off-peak</b> hours, when <b>traffic</b> loads are lower, the network is characterised by over-capacity, {{in the sense that}} same service quality targets can typically be satisfied with a reduced set of network resources, e. g. sites, carriers, etc. In this paper, we propose a procedure for deriving the potential of energy-oriented network optimisation, and apply this procedure to the case of a UMTS/HSDPA network. Depending on the desired performance target and the energy consumption model, energy savings from energy-oriented network optimisation of up to about 40 % are shown to be attainable. © 2010 IEEE...|$|R
40|$|This study {{demonstrates}} {{through a}} case study that detailed analyses, even after {{the construction of a}} project, are feasible using current technologies and available data. A case study of highway 25 is used to illustrate the method and verify the levels of air contaminants from additionally induced traffic during and after the construction of highway. Natural traffic growth was removed from the effect of observed gas emissions by comparing observed levels on other further locations in the same metropolitan area. This study estimates air pollution from the additional traffic during and after the construction of A- 25 extension project. NO 2 levels were spatially interpolated during peak and <b>off-peak</b> hour <b>traffic</b> and traffic density simulated on the road network for four scenarios. Comparing the four scenarios, it was found that levels of NO 2 concentrations were reduced at neighbor areas due to less traffic during the construction period. Levels of NO 2 after the construction were higher than those in 2008. The simulated traffic density for four scenarios revealed that traffic density was significantly increased on both arterial and access roads within the close vicinity of the extension project during and after its construction...|$|R
40|$|Traffic signal assigns the {{right-of-way}} {{to various}} conflicting traffic movements at an intersection. However, when a driver’s approaches a signalized intersection {{at the onset}} of amber, he/she is forced to make decisions about whether to pass or stop during a very short time period. This can be a difficult decision when the vehicle is located within the dilemma zone. It may result in a rear-end crash due to a sudden stop or red-light violation due to insufficient time to stop safely. In the present study six intersections, two with countdown timer, two with no-countdown timer and two with vehicle actuated system were analyzed to study the effect of various traffic signal systems on red light violation. The data of <b>off-peak</b> hour <b>traffic</b> was collected to minimize the influence of congestion on driver’s behavior by using video-recording technique. The data collected are those pertaining to the analysis of vehicles’ approaching speed distance from the stop line, the decision made by driver (i. e. stop abruptly, accelerate through amber and run red light) at onset amber as well as the types of vehicles driven. The finding of the study indicates that relatively large proportion of drivers did not willing to stop at onset of amber signal. The study suggests that a vehicle-actuated traffic signal system has resulted a higher rate of redlight violation with 55. 56 percent compared to the others types of signal system studied. However, more data are required to validate this finding...|$|R
30|$|The {{results showed}} that eCall has the {{greatest}} potential to save lives {{in cases where the}} emergency call would, with no eCall, be made more than 5  min after the accident. Consequently, eCall (equipped with accurate location information such as Global Positioning System, GPS) is expected to have the most substantial effects on minor rural roads, at night-time, and in <b>off-peak</b> <b>traffic.</b>|$|E
40|$|Abstract—Power {{consumption}} of ICT {{is becoming more}} and more a sensible problem, which is of interest for both the research community, for ISPs and for the general public. In this paper we consider a real IP backbone network and a real traffic profile. We evaluate the energy cost of running it, and, speculating on the possibility of selectively turning off spare devices whose capacity is not required to transport <b>off-peak</b> <b>traffic,</b> we show that it is possible to easily achieve more than 23 % of energy saving per year, i. e., to save about 3 GWh/year considering today’s power footprint of real network devices. I...|$|E
30|$|Estimating {{the stream}} travel {{time for an}} entire stream from limited Bluetooth data is a {{challenge}} and not many studies have addressed this problem until now. In this study, the data obtained from 2 W and LMV were extrapolated to estimate travel times of other classes of vehicles such as three wheelers and heavy motor vehicles to get a true estimate of stream travel time. The analysis was carried out separately for peak and <b>off-peak</b> <b>traffic</b> flow conditions. The study established linear relationships between speeds of different classes of vehicles through weighted linear regression analysis and estimated stream travel time as distance divided by stream speed. This technique was successful in estimating stream travel time accurately with an average MAPE of 2  %.|$|E
30|$|Other {{studies have}} also found that {{measures}} of turbulence (particularly variance of speed) within the traffic stream {{can be linked to}} crash likelihood [5, 7, 14]. External factors including road geometry, time of day and environmental conditions, was also included in the model, since these factors alone can affect driver behavior. Lee et al. [10 – 13] found that freeway segments with merging or diverging traffic contribute more to crash potential than straight freeway segments with no changes in lane configuration. Time of day refers to peak and <b>off-peak</b> periods. Typically, <b>traffic</b> volumes and congestion are higher during peak periods and drivers, particularly commuters, may react more aggressively to maintain their schedules. These factors are likely to increase the likelihood of a crash occurrence during the peak periods. Lastly, environmental conditions include such factors as local weather, road surface quality, and lighting. Due to the limited amount of available environmental data, the effect of these factors can be difficult to capture.|$|R
30|$|In the {{original}} TUC strategy [5, 11], {{for the control}} of green splits, a multivariable regulator is used that modifies given fixed-time plans based on the currently observed traffic loads in the network links. In case of oversaturated conditions, {{the sensitivity of the}} regulator to the particular utilized fixed plans is minor. When demands and queueing, however, are low, TUC’s split decisions are close to the utilized fixed plans. Thus, its performance depends on the quality of these plans, which need to be the appropriate for the considered traffic load. To circumvent the need for good fixed-time plans, the hybrid variant of the TUC strategy was developed, whereby signalized junctions are controlled by a real-time Webster-type demand-driven strategy as long as traffic conditions are undersaturated; while a switching to {{the original}} TUC is effectuated when traffic conditions are close to saturation [13]. For the needs of the present study, the hybrid variant of TUC is used, which {{has been found to be}} efficient during both <b>off-peak</b> and congested <b>traffic</b> conditions.|$|R
40|$|During <b>off-peak</b> {{nighttime}} periods, <b>traffic</b> {{signals are}} often placed on flashing operation (flashing yellow on one approach and flashing red on the other) to reduce delay {{on the major}} street approaches to intersections. This report {{presents the results of}} analyses to determine: 1. The effect of flashing operation on accident experience, 2. The levels of accident experience that can be expected under different conditions and signal operations, and 3. Appropriate criteria for the development of signal operation procedures during off-peak nighttime hours. The relative accident impacts of flashing versus stop-and-go (i. e., standard green-yellow-red cycle) signal operation in Oakland County (Michigan) were evaluated. The impetus for this study involved both safety and liability questions, The study was conducted in two stages. The first stage involved a beforeand-after analysis of accidents at six 4 -Iegged intersections where the hours of flashing operation had been either curtailed or eliminated. The next step consisted of a comparative analysis of accidents at intersections categorized by signal operation, by intersection type, and by functional classification of the intersecting roadways...|$|R
40|$|This paper {{proposes a}} {{real-time}} adaptive control model for signalized intersections that decides optimal control parameters {{commonly found in}} modern actuated controllers, aiming to exploit the adaptive functionality of traffic-actuated control and to improve the performance of traffic-actuated signal system. This model incorporates a flow prediction process that estimates the future arrival rates and turning proportions at target intersection based on the available detector information and signal timing plan. Signal control parameters are optimized dynamically cycle-by-cycle to satisfy these estimated demands. The proposed adaptive control strategy is tested on a network consisting of thirty-eight actuated signals using microscopic simulation. Simulation results show the proposed adaptive model is able to improve {{the performance of the}} study network, especially under <b>off-peak</b> <b>traffic</b> conditions...|$|E
40|$|This paper investigates {{differentiated}} {{design standards}} {{as a source}} of capacity additions that are more affordable and have smaller aesthetic and environmental impacts than expressways. We consider several tradeoffs, including narrow versus wide lanes and shoulders on an expressway of a given total width, and high-speed expressway versus lower-speed arterial. We quantify the situations in which <b>off-peak</b> <b>traffic</b> is sufficiently great to make it worthwhile to spend more on construction, or to give up some capacity, in order to provide very high off-peak speeds even if peak speeds are limited by congestion. We also consider the implications of differing accident rates. The results support expanding the range of highway designs that are considered when adding capacity to ameliorate urban road congestion. Highway design; Capacity; Free-flow speed; Parkway...|$|E
40|$|A cache-aided erasure {{broadcast}} channel is studied. The receivers {{are divided into}} two sets: the weak and strong receivers, where the receivers in the same set {{all have the same}} erasure probability. The weak receivers, in order to compensate for the high erasure probability, are equipped with cache mem- ories of equal size, while the receivers in the strong set have no caches. Data can be pre-delivered to weak receivers??? caches over the <b>off-peak</b> <b>traffic</b> period before the receivers reveal their demands. A joint caching and channel coding scheme is proposed such that all the receivers, even the receivers without any cache memories, benefit from the presence of caches across the network. The trade-off between the cache size and the achievable rate is studied, and it is shown that the proposed scheme significantly improves the achievable trade-off upon the state-of-the-art...|$|E
40|$|Traditional {{signal timing}} {{policies}} have typically prioritized vehicles over pedestrians at intersections, leading to undesirable consequences such as large delays and risky crossing behaviors. The {{objective of this}} paper is to explore signal timing control strategies to reduce pedestrian delay at signalized intersections. The impacts of change in signal controller mode of operation (coordinated vs. free) at intersections were studied using the micro-simulation software VISSIM. A base model was developed and calibrated for an existing pedestrian active corridor. A hypothetical network of three intersections was used to explore the effects of mode of operation and measures of delay for pedestrians and all users. From a pedestrian perspective, free operation was found to be more beneficial due to lower delays. However, from a system wide (all user) perspective, coordinated operation showed the greatest benefits with lowest system delay under heavy traffic conditions (v/c 3 ̆e 0. 7). In the <b>off-peak</b> conditions when <b>traffic</b> volumes are lower, free operation resulted in lowest system delay (v/c 3 ̆c 0. 7). During coordination, lower cycle lengths were beneficial for pedestrians, due to smaller delays. The results revealed that volume to capacity (v/c) ratios for the major street volumes coupled with pedestrian actuation frequency for the side street phases, could be used to determine the signal controller mode of operation that produces the lowest system delay. The results were used to create a guidance matrix for controller mode based on pedestrian and vehicle volumes. To demonstrate application, the matrix is applied to another corridor in a case study approach...|$|R
40|$|Traffic {{performance}} measures {{under conditions of}} increasing demand have been assessed on five networks of differing characteristics using the traffic assignment model CONTRAM. The time-dependent nature of the modelling has enabled the circumstances to be identified when the network is effectively saturated. Monitoring of queue lengths in each time interval has also indicated the levels of traffic demand at which queues fail to recover (or reduce) {{by the end of}} the modelling period. This indicates queueing instability and, with extended modelling to include subsequent <b>off-peak</b> periods, maximum <b>traffic</b> growth consistent with longer term queue stability could be identified. While the above criteria are useful in practice, the limit of traffic growth in a network is more likely to depend on behavioural responses to traffic conditions and the availability of alternatives. These considerations have led to a hybrid approach to saturation level prediction being developed in this study. This consists of initially stopping traffic growth on Origin-Destination (O-D) movements when average travel speed on set routes on those movements fall below minimum tolerable values. A methodology is described on three networks, and is shown to be practicable. Using 15 kph minimum speeds for illustrative purposes, the gradual redistribution of traffic demands in the network with increasing growth caused network saturation to be predicted a few years later in the evaluation period. This study has highlighted a number of areas of uncertainty and further research is recommended into: (1) methodology application; (2) sensitivity testing; (3) peak spreading; (4) the use of alternative methodologies such as SCOOT or the use of matrix manipulation methods; and (5) behavioural responses. <br/...|$|R
40|$|With {{the advent}} of Advanced Traveler Information Systems (ATIS), {{short-term}} travel time prediction is becoming increasingly important. Travel time can be obtained directly from instrumented test vehicles, license plate matching, probe vehicles etc., or from indirect methods such as loop detectors. Because of their wide spread deployment, travel time estimation from loop detector data {{is one of the}} most widely used methods. However, the major criticism about loop detector data is the high probability of error due to the prevalence of equipment malfunctions. This dissertation presents methodologies for estimating and predicting travel time from the loop detector data after correcting for errors. The methodology is a multi-stage process, and includes the correction of data, estimation of travel time and prediction of travel time, and each stage involves the judicious use of suitable techniques. The various techniques selected for each of these stages are detailed below. The test sites are from the freeways in San Antonio, Texas, which are equipped with dual inductance loop detectors and AVI. ??	Constrained non-linear optimization approach by Generalized Reduced Gradient (GRG) method for data reduction and quality control, which included a check for the accuracy of data from a series of detectors for conservation of vehicles, in addition to the commonly adopted checks. ??	A theoretical model based on traffic flow theory for travel time estimation for both <b>off-peak</b> and peak <b>traffic</b> conditions using flow, occupancy and speed values obtained from detectors. ??	Application of a recently developed technique called Support Vector Machines (SVM) for travel time prediction. An Artificial Neural Network (ANN) method is also developed for comparison. Thus, a complete system for the estimation and prediction of travel time from loop detector data is detailed in this dissertation. Simulated data from CORSIM simulation software is used for the validation of the results...|$|R
40|$|Decentralized coded caching is {{studied for}} a content server with $N$ files, each of size $F$ bits, serving $K$ active users, each {{equipped}} with a cache of distinct capacity. It is assumed that the users' caches are filled in advance during the <b>off-peak</b> <b>traffic</b> period without {{the knowledge of the}} number of active users, their identities, or the particular demands. User demands are revealed during the peak traffic period, and are served simultaneously through an error-free shared link. A new decentralized coded caching scheme is proposed for this scenario, and it is shown to improve upon the state-of-the-art in terms of the required delivery rate over the shared link, when there are more users in the system than the number of files. Numerical results indicate that the improvement becomes more significant as the cache capacities of the users become more skewed. Comment: To be presented in ASILOMAR conference, 201...|$|E
40|$|Integrated {{networks}} {{of the near}} future are expected to provide {{a wide variety of}} services, which could consume widely differing amounts of resources. We present a framework for pricing services in integrated networks, and study the effect of pricing on user behavior and network performance. We first describe a network model that is simple, yet models details such as the wealth distribution in society, different classes of service, peak and <b>off-peak</b> <b>traffic,</b> elasticity of user's demand, and call blocking due to budgetary constraints. We then perform experiments to study the effect of setup, per packet and peak load prices on the blocking probability of two classes of calls passing through a single node enforcing admission control. Some selected results are that a) increasing prices first increases the net revenue to a provider, then causes a decrease b) peak-load pricing spreads network utilization more evenly, raising revenue while simultaneously reducing call blocking probability. Fin [...] ...|$|E
40|$|Decentralized {{proactive}} caching {{and coded}} delivery is studied in a content delivery network, where each user {{is equipped with}} a cache memory, not necessarily of equal capacity. Cache memories are filled in advance during the <b>off-peak</b> <b>traffic</b> period in a decentralized manner, i. e., without the knowledge of the number of active users, their identities, or their particular demands. User demands are revealed during the peak traffic period, and are served simultaneously through an error-free shared link. The goal is to find the minimum delivery rate during the peak traffic period that is sufficient to satisfy all possible demand combinations. A group-based decentralized caching and coded delivery scheme is proposed, and it is shown to improve upon the state-of-the-art in terms of the minimum required delivery rate when there are more users in the system than files. Numerical results indicate that the improvement is more significant as the cache capacities of the users become more skewed. A new lower bound on the delivery rate is also presented, which provides a tighter bound than the classical cut-set bound. Comment: to appear, IEEE Transactions on Communication...|$|E
40|$|The {{reduction}} of power consumption in communication networks {{has become a}} key issue for both the Internet Service Providers (ISP) and the research community. Ac- cording to different studies, the power consumption of Information and Communication Technologies (ICT) varies from 2 % to 10 % of the worldwide power consumption [1, 2]. Moreover, the expected trends for the future predict a notably increase of the ICT power consumption, doubling its value by 2020 [2] and growing to around 30 % of the worldwide electricity demand by 2030 according to business-as-usual evaluation scenarios [15]. It is therefore not surprising that researchers, manufacturers and network providers are spending significant efforts to reduce the power consumption of ICT systems from dif- ferent angles. To this extent, networking devices waste {{a considerable amount of}} power. In partic- ular, their power consumption has always been increased in the last years, coupled with the increase of the offered performance [16]. Actually, power consumption of network- ing devices scales with the installed capacity, rather than the current load [17]. Thus, for an ISP the network power consumption is practically constant, unrespectively to traffic fluctuations. However, actual traffic is subject to strong day/night oscillations [3]. Thus, many devices are underutilized, especially during <b>off-peak</b> hours when <b>traffic</b> is low. This represents a clear opportunity for saving energy, since many resources (i. e., routers and links) are powered on without being fully utilized. In this context, resource consolidation is a known paradigm for the {{reduction of}} the power consumption. It consists in having a carefully selected subset of network devices entering a low power state, and use the rest to transport the required amount of traffic. This is possible without disrupting the Quality of Service (QoS) offered by the network infrastructure, since communication networks are designed over the peak foreseen traffic request, and with redundancy and over-provisioning in mind. In this thesis work, we present different techniques to perform resource consolida- tion in backbone IP-based networks, ranging from centralized solutions, where a central entity computes a global solution based on an omniscient vision of the network, to dis- tributed solutions, where single nodes take independent decisions on the local power- state, based solely on local knowledge. Moreover, different technological assumptions are made, to account for different possible directions of the network devices evolutions, ranging from the possibility to switch off linecard ports, to whole network nodes, and taking into account different power consumption profiles...|$|R
40|$|A cache-aided {{broadcast}} network is studied, {{in which a}} server delivers contents {{to a group of}} receivers over a packet erasure broadcast channel (BC). The receivers are divided into two sets with regards to their channel qualities: the weak and strong receivers, where all the weak receivers have statistically worse channel qualities than all the strong receivers. The weak receivers, in order to compensate for the high erasure probability they encounter over the channel, are equipped with cache memories of equal size, while the receivers in the strong set have no caches. Data can be pre-delivered to weak receivers' caches over the <b>off-peak</b> <b>traffic</b> period before the receivers reveal their demands. Allowing arbitrary erasure probabilities for the weak and strong receivers, a joint caching and channel coding scheme, which divides each file into several subfiles, and applies a different caching and delivery scheme for each subfile, is proposed. It is shown that all the receivers, even those without any cache memories, benefit from the presence of caches across the network. An information theoretic trade-off between the cache size and the achievable rate is formulated. It is shown that the proposed scheme improves upon the state-of-the-art in terms of the achievable trade-off...|$|E
40|$|We {{define and}} propose a {{resource}} allocation architecture for cellular networks. The architecture combines content-aware, time-aware and location-aware resource allocation for next generation broadband wireless systems. The architecture ensures content-aware resource allocation by prioritizing real-time applications users over delay-tolerant applications users when allocating resources. It enables time-aware resource allocation via traffic-dependent pricing that varies during different hours of day (e. g. peak and <b>off-peak</b> <b>traffic</b> hours). Additionally, location-aware resource allocation is integrable in this architecture by including carrier aggregation of various frequency bands. The context-aware resource allocation is an optimal and flexible architecture {{that can be}} easily implemented in practical cellular networks. We highlight the advantages of the proposed network architecture with a discussion on the future research directions for context-aware resource allocation architecture. We also provide experimental results to illustrate a general proof of concept for this new architecture. Comment: (c) 2015 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other work...|$|E
40|$|Small basestations (SBs) {{equipped}} with caching units have potential {{to handle the}} unprecedented demand growth in heterogeneous networks. Through low-rate, backhaul connections with the backbone, SBs can prefetch popular files during <b>off-peak</b> <b>traffic</b> hours, and service them to the edge at peak periods. To intelligently prefetch, each SB must learn what and when to cache, while taking into account SB memory limitations, the massive number of available contents, the unknown popularity profiles, {{as well as the}} space-time popularity dynamics of user file requests. In this work, local and global Markov processes model user requests, and a reinforcement learning (RL) framework is put forth for finding the optimal caching policy when the transition probabilities involved are unknown. Joint consideration of global and local popularity demands along with cache-refreshing costs allow for a simple, yet practical asynchronous caching approach. The novel RL-based caching relies on a Q-learning algorithm to implement the optimal policy in an online fashion, thus enabling the cache control unit at the SB to learn, track, and possibly adapt to the underlying dynamics. To endow the algorithm with scalability, a linear function approximation of the proposed Q-learning scheme is introduced, offering faster convergence as well as reduced complexity and memory requirements. Numerical tests corroborate the merits of the proposed approach in various realistic settings...|$|E
40|$|Abstract—Data centers consume {{significant}} amounts of energy. As severs become more energy efficient with various energy saving techniques, the data center network (DCN) has been accounting for 20 % {{or more of the}} energy consumed by the entire data center. While DCNs are typically provisioned with full bisection bandwidth, DCN traffic demonstrates fluctuating patterns. The objective of this work is to improve the energy efficiency of DCNs during <b>off-peak</b> <b>traffic</b> time by powering off idle devices. Although there exist a number of energy optimization solutions for DCNs, they consider only either the hosts or network, but not both. In this paper, we propose a joint optimization scheme that simultaneously optimizes virtual machine (VM) placement and network flow routing to maximize energy savings, and we also build an OpenFlow based prototype to experimentally demonstrate the effectiveness of our design. First, we formulate the joint optimization problem as an integer linear program, {{but it is not a}} practical solution due to high complexity. To practically and effectively combine host and network based optimization, we present a unified representation method that converts the VM placement problem to a routing problem. In addition, to accelerate processing the large number of servers and an even larger number of VMs, we describe a parallelization approach that divides the DCN into clusters for parallel processing. Further, to quickly find efficient paths for flows, we propose a fast topology oriented multipath routing algorithm that uses depth-first search to quickly traverse between hierarchical switch layers and uses the best-fit criterion to maximize flow consolidation. Finally, we have conducted extensive simulations and experiments to compare our design with existing ones. The simulation and experiment results fully demonstrate that our design outperforms existing host- or network-only optimization solutions, and well approximates the ideal linear program. Index Terms—Data center networks; virtual machine migra-tion; multipath routing; energy efficiency. I...|$|E
40|$|Pan Cao, Wnjia Liu, John S. Thompson, Chenyang Yang, and Eduard A. Jorswieck, 'Semidynamic Green Resource Management in Downlink Heterogeneous Networks by Group Sparse Power Control', IEEE Journal on Selected Areas in Communications, Vol. 34 (5) : 1250 - 1266, May 2016, doi: [URL] paper {{addresses}} an energy-saving {{problem for}} the downlink of a cloud-assisted heterogeneous network (HetNet) using a time-division duplex (TDD) model, which aims to minimize the base stations (BSs) sum power consumption while meeting the rate requirement of each user equipment (UE). The basic idea of this work is {{to make use of}} the scalability of system configurations such that green resource management can be employed by flexibly switching off some unnecessary hardware components, especially for <b>off-peak</b> <b>traffic</b> scenarios. This motivates us to utilize a flexible BS power consumption formulation to jointly model its signal processing and circuit power, transmit power, and backhaul transmission power. Instead of using the integer variables [1, 0] to control the ???on/off??? two status of a BS in most previous work, we employ the group sparsity of a transmit power vector to denote the activity of each frequency carrier (FC) such that the signal processing and circuit power can be scaled with the effective bandwidth, thereby leading to multiple sleep modes for a BS in multi-FC systems. Based on this BS power model and the group sparsity concept, a simplified resource allocation scheme for joint BS-UE association, FC assignment, downlink power allocation, and BS sleep modes determination is presented, which is based on the average channel statistics computed over the coherence time of the large scale fading (LSF). This semidynamic green resource management mechanism can be formulated as a NP-hard optimization problem. In order to make it tractable, the successive convex approximation (SCA) -based algorithm is applied to efficiently find a stationary solution using a cloud-based centralized optimization. Simulation results also verify the effectiveness of the proposed mechanism under the developed BS power consumption model...|$|E
40|$|AbstractThe current Oslo toll {{ring system}} was {{implemented}} in 1990 to generate funds for road {{investments in the}} larger Oslo area. Currently, about 60 % of the toll income is being used for investments and maintenance of public transport. The use of {{a large share of}} toll income on public transport may be seen as strategy to induce people to use public transport. However, congestion in peak periods is still a major problem in the Oslo area and has attracted public attention. Congestion imposes a major social cost. All else equal, congested traffic produces more air pollution, increases travel time and consumes more energy than smooth traffic flow. Given that the Oslo toll ring with flat rates is already in place, planners have started to address {{the question of whether the}} system in place can be converted to a marginal cost-based scheme to reduce the prevalent congestion. In this paper we examine the benefits of converting the already existing Oslo toll ring with flat rates to a congestion-based charging scheme. To do so, we first develop a theoretical model along the lines of Arnott et al. (1993). The theoretical model is then combined with a more practical and empirical model for estimating the optimal tolls and assessing the effects of a congestion scheme. The findings show that transforming the current toll ring system into a congestion-pricing system, where peak traffic is charged a higher toll than <b>off-peak</b> <b>traffic,</b> holds great potential for easing traffic congestion and improving the environment. Further, it will improve the efficiency of both public and private transport while at the same time it can raise more revenues compared to the current situation. Thus, such a move will not be in conflict with the current road financing toll. These results have direct policy implications in that they should appeal to decision-makers and hence are relevant for the marketing of congestion pricing...|$|E
40|$|Modern {{data centers}} host {{hundreds}} of thousands of servers to achieve economies of scale. Such a huge number of servers create challenges for the data center network (DCN) to provide proportionally large bandwidth. In addition, the deployment of virtual machines (VMs) in data centers raises the requirements for efficient resource allocation and find-grained resource sharing. Further, the large number of servers and switches in the data center consume significant amounts of energy. Even though servers become more energy efficient with various energy saving techniques, DCN still accounts for 20 % to 50 % of the energy consumed by the entire data center. ^ The objective of this dissertation is to enhance DCN performance as well as its energy efficiency by conducting optimizations on both host and network sides. First, as the DCN demands huge bisection bandwidth to interconnect all the servers, we propose a parallel packet switch (PPS) architecture that directly processes variable length packets without segmentation-and-reassembly (SAR). The proposed PPS achieves large bandwidth by combining switching capacities of multiple fabrics, and it further improves the switch throughput by avoiding padding bits in SAR. Second, since certain resource demands of the VM are bursty and demonstrate stochastic nature, to satisfy both deterministic and stochastic demands in VM placement, we propose the Max-Min Multidimensional Stochastic Bin Packing (M 3 SBP) algorithm. M 3 SBP calculates an equivalent deterministic value for the stochastic demands, and maximizes the minimum resource utilization ratio of each server. Third, to provide necessary traffic isolation for VMs that share the same physical network adapter, we propose the Flow-level Bandwidth Provisioning (FBP) algorithm. By reducing the flow scheduling problem to multiple stages of packet queuing problems, FBP guarantees the provisioned bandwidth and delay performance for each flow. Finally, while DCNs are typically provisioned with full bisection bandwidth, DCN traffic demonstrates fluctuating patterns, we propose a joint host-network optimization scheme to enhance the energy efficiency of DCNs during <b>off-peak</b> <b>traffic</b> hours. The proposed scheme utilizes a unified representation method that converts the VM placement problem to a routing problem and employs depth-first and best-fit search to find efficient paths for flows. ...|$|E
40|$|The {{ability to}} predict NO 2 {{concentrations}} ([NO¬ 2]) within urban street networks {{is important for}} the evaluation of strategies to reduce exposure to NO 2. However, models aiming to make such predictions involve the coupling of several complex processes: traffic emissions under different levels of congestion; dispersion via turbulent mixing; chemical processes of relevance at the street-scale. Parameterisations of these processes are challenging to quantify with precision. Predictions are therefore subject to uncertainties which should be taken into account when using models within decision making. This paper presents an analysis of mean [NO¬ 2] predictions from such a complex modelling system applied to a street canyon within the city of York, UK including the treatment of model uncertainties and their causes. The model system consists of a micro-scale traffic simulation and emissions model, a Reynolds Averaged turbulent flow model coupled to a reactive Lagrangian particle dispersion model. The analysis focuses on the sensitivity of predicted in-street increments of [NO¬ 2] at different locations in the street to uncertainties in the model inputs. These include physical characteristics such as background wind direction, temperature and background ozone concentrations; traffic parameters such as overall demand and primary NO 2 fraction; as well as model parameterisations such as roughness lengths, turbulent time- and length-scales and chemical reaction rate coefficients. Predicted [NO¬ 2] is shown to be relatively robust with respect to model parameterisations, although there are significant sensitivities to the activation energy for the reaction NO+O 3 as well as the canyon wall roughness length. Under <b>off-peak</b> <b>traffic</b> conditions, demand is the key traffic parameter. Under peak conditions where the network saturates, road-side [NO¬ 2] is relatively insensitive to changes in demand and more sensitive to the primary NO 2 fraction. The most important physical parameter was found to be the background wind direction. The study highlights the key parameters required for reliable [NO¬ 2] estimations suggesting that accurate reference measurements for wind direction should be a critical part of air quality assessments for in-street locations. It also highlights the importance of street scale chemical processes in forming road-side [NO¬ 2], particularly for regions of high NOx emissions such as close to traffic queues...|$|E
