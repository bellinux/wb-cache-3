10000|10000|Public
25|$|Formally, tree {{alignment}} is {{the following}} <b>optimization</b> <b>problem.</b>|$|E
25|$|The minimum vertex cover {{problem is}} the <b>optimization</b> <b>problem</b> of finding a {{smallest}} vertex cover in a given graph.|$|E
25|$|The {{problem of}} finding a minimum vertex cover is a {{classical}} <b>optimization</b> <b>problem</b> in computer science and is a typical example of an NP-hard <b>optimization</b> <b>problem</b> that has an approximation algorithm. Its decision version, the vertex cover problem, was one of Karp's 21 NP-complete problems and is therefore a classical NP-complete problem in computational complexity theory. Furthermore, the vertex cover problem is fixed-parameter tractable and a central problem in parameterized complexity theory.|$|E
50|$|The {{class of}} conic <b>optimization</b> <b>problems</b> is a {{subclass}} of convex <b>optimization</b> <b>problems</b> {{and it includes}} {{some of the most}} well known classes of convex <b>optimization</b> <b>problems,</b> namely linear and semidefinite programming.|$|R
40|$|This paper studies robust {{solutions}} and semidefinite linear programming (SDP) relaxations {{of a class}} of convex polynomial <b>optimization</b> <b>problems</b> {{in the face of}} data uncertainty. The class of convex <b>optimization</b> <b>problems,</b> called robust SOS-convex polynomial <b>optimization</b> <b>problems,</b> includes robust quadratically constrained convex <b>optimization</b> <b>problems</b> and robust separable convex polynomial <b>optimization</b> <b>problems.</b> It establishes sums-of-squares polynomial representations characterizing robust {{solutions and}} exact SDP-relaxations of robust SOS-convex polynomial <b>optimization</b> <b>problems</b> under various commonly used uncertainty sets. In particular, the results show that the polytopic and ellipsoidal uncertainty sets, that allow second-order cone re-formulations of robust quadratically constrained <b>optimization</b> <b>problems,</b> continue to permit exact SDP-relaxations for a broad class of robust SOS-convex polynomial <b>optimization</b> <b>problems.</b> Research was partially {{supported by a grant from}} the Australian Research Council. J. Vicente-PÃ©rez has been partially supported by the MICINN of Spain, Grant MTM 2011 - 29064 -C 03 - 02...|$|R
30|$|In this work, a new {{generalized}} gradient projection algorithm for minimax <b>optimization</b> <b>problems</b> with inequality constraints is presented. As further work, {{we think}} the ideas can be extended to minimax <b>optimization</b> <b>problems</b> with equality and inequality constraints and other <b>optimization</b> <b>problems.</b>|$|R
25|$|Minimizing (2) can be rewritten as a {{constrained}} <b>optimization</b> <b>problem</b> with a differentiable {{objective function}} in the following way.|$|E
25|$|Solving a multi-objective <b>optimization</b> <b>problem</b> is {{sometimes}} understood as approximating or computing all or a representative set of Pareto optimal solutions.|$|E
25|$|Fractional {{programming}} studies optimization of {{ratios of}} two nonlinear functions. The special class of concave fractional {{programs can be}} transformed to a convex <b>optimization</b> <b>problem.</b>|$|E
3000|$|It is {{well known}} that in dealing with {{symmetric}} cone <b>optimization</b> <b>problems,</b> such as second-order cone <b>optimization</b> <b>problems</b> and positive semi-definite <b>optimization</b> <b>problems,</b> this type of vector-valued functions plays an essential role. Inspired by this, we study the properties of [...]...|$|R
40|$|AbstractThe {{paper was}} {{motivated}} by solution methods suggested in the literature for solving linear <b>optimization</b> <b>problems</b> over (max,+) - or (max,min) -algebras and certain class of so called max-separable <b>optimization</b> <b>problems.</b> General features of these <b>optimization</b> <b>problems,</b> which {{play a crucial role}} in the optimization methods were used to formulate a general class of <b>optimization</b> <b>problems</b> with disjunctive constraints and a max-separable objective function and suggest a solution procedure for solving such problems. Linear problems over (max,+) -algebras and the max-separable problems are contained in this general class of <b>optimization</b> <b>problems</b> as special cases...|$|R
25|$|The {{method of}} Lagrange {{multipliers}} {{can be used}} to reduce <b>optimization</b> <b>problems</b> with constraints to unconstrained <b>optimization</b> <b>problems.</b>|$|R
25|$|Radio {{resource}} management is often solved by scalarization; that is, {{selection of a}} network utility function that tries to balance throughput and user fairness. The choice of utility function has a large impact on the computational complexity of the resulting single-objective <b>optimization</b> <b>problem.</b> For example, the common utility of weighted sum rate gives an NP-hard problem with a complexity that scales exponentially {{with the number of}} users, while the weighted max-min fairness utility results in a quasi-convex <b>optimization</b> <b>problem</b> with only a polynomial scaling with the number of users.|$|E
25|$|Crammer and Singer {{proposed}} a multiclass SVM method which casts the multiclass classification problem {{into a single}} <b>optimization</b> <b>problem,</b> rather than decomposing it into multiple binary classification problems. See also Lee, Lin and Wahba.|$|E
25|$|Particle swarm {{optimization}} is an algorithm modelled on swarm {{intelligence that}} finds {{a solution to}} an <b>optimization</b> <b>problem</b> in a search space, or model and predict social behavior {{in the presence of}} objectives.|$|E
50|$|Bilevel <b>optimization</b> <b>problems</b> {{are hard}} to solve. One {{solution}} method is to reformulate bilevel <b>optimization</b> <b>problems</b> to <b>optimization</b> <b>problems</b> for which robust solution algorithms are available. Extended Mathematical Programming (EMP) is an extension to mathematical programming languages that provides several keywords for bilevel <b>optimization</b> <b>problems.</b> These annotations facilitate the automatic reformulation to Mathematical Programs with Equilibrium Constraints (MPECs) for which mature solver technology exists. EMP is available within GAMS.|$|R
40|$|Dealing {{with the}} {{simulation}} based <b>optimization</b> <b>problems,</b> this title presents the systematic {{development of the}} methods and algorithms. It covers the time dependent <b>optimization</b> <b>problems</b> with applications in environmental engineering, and also deals with steady state <b>optimization</b> <b>problems,</b> in which the PDEs are solve...|$|R
30|$|Because of its simple algorithm, {{particle}} swarm optimization is easy to implement, and no gradient {{information is}} needed. The characteristics of small parameters are good for both continuous <b>optimization</b> <b>problems</b> and discrete <b>optimization</b> <b>problems,</b> especially because of their natural real-coded features are suitable for processing optimization [14]. In recent years, it has been became a hot topic {{in the field of}} intelligent optimization around the world. Particle swarm optimization (PSO) algorithm was first applied to the optimization of nonlinear continuous functions and neural network training and was newly used to solve constrained <b>optimization</b> <b>problems,</b> multi-objective <b>optimization</b> <b>problems,</b> and dynamic <b>optimization</b> <b>problems</b> [8].|$|R
25|$|In {{order to}} solve this problem with a {{numerical}} optimization technique, we must first transform this problem such that the critical points occur at local minima. This is done by computing the magnitude of the gradient of the unconstrained <b>optimization</b> <b>problem.</b>|$|E
25|$|Unlike other deep architectures, such as DBNs, {{the goal}} is not to {{discover}} the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode <b>optimization</b> <b>problem.</b> In purely discriminative tasks, DSNs perform better than conventional DBNs.|$|E
25|$|As the <b>optimization</b> <b>problem</b> {{described}} above {{can be solved}} as a convex problem with respect to either dictionary or sparse coding while the other {{one of the two}} is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.|$|E
40|$|In {{this paper}} we {{consider}} certain <b>optimization</b> <b>problems</b> which are described by inequalities in partially ordered vector space. Using the scalarization procedure developed in [6, 7] we derive optimality conditions for <b>optimization</b> <b>problems</b> of maximum type and for vector <b>optimization</b> <b>problems.</b> As applications we obtain various optimality conditions including an alternation theorem for the Chebyshev approximation with certain side-conditions and a scalarization for vector <b>optimization</b> <b>problems</b> where efficiency {{is defined by}} a cone...|$|R
40|$|We {{introduce}} {{the framework of}} qualitative <b>optimization</b> <b>problems</b> (or, simply, <b>optimization</b> <b>problems)</b> to represent preference theories. The formalism uses separate modules to describe the space of outcomes to be compared (the generator) and the preferences on outcomes (the selector). We consider two types of <b>optimization</b> <b>problems.</b> They differ {{in the way the}} generator, which we model by a propositional theory, is interpreted: by the standard propositional logic semantics, and by the equilibrium-model (answer-set) semantics. Under the latter interpretation of generators, <b>optimization</b> <b>problems</b> directly generalize answer-set optimization programs proposed previously. We study strong equivalence of <b>optimization</b> <b>problems,</b> which guarantees their interchangeability within any larger context. We characterize several versions of strong equivalence obtained by restricting the class of <b>optimization</b> <b>problems</b> that can be used as extensions and establish the complexity of associated reasoning tasks. Understanding strong equivalence is essential for modular representation of <b>optimization</b> <b>problems</b> and rewriting techniques to simplify them without changing their inherent properties. 1...|$|R
30|$|In this paper, we are {{concerned}} with the convex composite <b>optimizations</b> <b>problem.</b> Many problems in mathematical programming such as convex inclusion problems, minimax problems, penalization methods, goal programming <b>problems,</b> constrained <b>optimization</b> <b>problems,</b> and other problems can be formulated like composite <b>optimization</b> <b>problems</b> (see, for example, [1 â 6]).|$|R
25|$|A {{particularly}} fast {{method for}} exact BN learning is {{to cast the}} problem as an <b>optimization</b> <b>problem,</b> and solve it using integer programming. Acyclicity constraints {{are added to the}} integer program (IP) during solving in the form of cutting planes. Such method can handle problems with up to 100 variables.|$|E
25|$|While the {{decision}} problem is NP-complete, the <b>optimization</b> <b>problem</b> is NP-hard, its resolution {{is at least}} as difficult as {{the decision}} problem, and there is no known polynomial algorithm which can tell, given a solution, whether it is optimal (which would mean that there is no solution with a larger V, thus solving the NP-complete decision problem).|$|E
25|$|Mathematical {{optimization}} {{is used in}} much modern controller design. High-level controllers such as model {{predictive control}} (MPC) or real-time optimization (RTO) employ mathematical optimization. These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical <b>optimization</b> <b>problem</b> including constraints and {{a model of the}} system to be controlled.|$|E
50|$|In {{computer}} science, a polynomial-time approximation scheme (PTAS) {{is a type}} of approximation algorithm for <b>optimization</b> <b>problems</b> (most often, NP-hard <b>optimization</b> <b>problems).</b>|$|R
40|$|This {{tutorial}} {{concerns a}} method for solving a variety of circuit sizing and <b>optimization</b> <b>problems,</b> {{which is based on}} formulating the problem as a geometric program (GP), or a generalized geometric program (GGP). These nonlinear, constrained <b>optimization</b> <b>problems</b> can be transformed to convex <b>optimization</b> <b>problems,</b> and then solved (globally) very efficiently...|$|R
50|$|Optimization {{algorithms}} {{implemented in}} pSeven allow solving single and multiobjective constrained <b>optimization</b> <b>problems</b> {{as well as}} robust and reliability based design <b>optimization</b> <b>problems.</b> Users can solve both engineering <b>optimization</b> <b>problems</b> with cheap to evaluate semi-analytical models and the problems with expensive (in terms of CPU time) objective functions and constraints.|$|R
25|$|When {{decision}} making is emphasized, the objective of solving a multi-objective <b>optimization</b> <b>problem</b> is referred to supporting a decision maker in finding the most preferred Pareto optimal solution according to his/her subjective preferences. The underlying assumption is that one {{solution to the problem}} must be identified to be implemented in practice. Here, a human decision maker (DM) plays an important role. The DM is expected to be an expert in the problem domain.|$|E
25|$|A a deep {{stacking}} network (DSN) (deep convex network) {{is based}} on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong. It formulates the learning as a convex <b>optimization</b> <b>problem</b> with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.|$|E
25|$|Sometimes {{it is of}} {{interest}} {{to find out what}} is the immediate effect of a perturbation or knockout, since it takes time for regulatory changes to occur and for the organism to re-organize fluxes to optimally utilize a different carbon source or circumvent the effect of the knockout. MOMA predicts the immediate sub-optimal flux distribution following the perturbation by minimizing the distance (Euclidean) between the wild-type FBA flux distribution and the mutant flux distribution using quadratic programming. This yields an <b>optimization</b> <b>problem</b> of the form.|$|E
25|$|Multi-objective <b>optimization</b> <b>problems</b> {{have been}} {{generalized}} further into vector <b>optimization</b> <b>problems</b> where the (partial) ordering {{is no longer}} given by the Pareto ordering.|$|R
30|$|In {{order to}} get an insight {{to the issue of}} set-valued <b>optimization</b> <b>problems,</b> we give two {{examples}} (see Kuroiwa [32]) of set-valued <b>optimization</b> <b>problems.</b>|$|R
40|$|Many real-life <b>optimization</b> <b>problems</b> require {{taking into}} account {{multiple}} points of view corresponding to multiple objectives. In recent years, the demand for new applications and the increasing power of computers resulted in the growing interest in computationally hard multiple objective <b>optimization</b> (MOO <b>problems),</b> e. g. non-linear and combinatorial <b>optimization</b> <b>problems.</b> Problems of thi...|$|R
