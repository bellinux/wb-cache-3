33|24|Public
50|$|The {{operational}} {{implementation and}} diagnostic measurement technologies needed to perform SHM produce more data than traditional uses of structural dynamics information. A condensation {{of the data}} is advantageous and necessary when comparisons of many feature sets obtained over the lifetime of the structure are envisioned. Also, because data will be acquired from a structure {{over an extended period}} of time and in an operational environment, robust data reduction techniques must be developed to retain feature sensitivity to the structural changes of interest in the presence of environmental and <b>operational</b> <b>variability.</b> To further aid in the extraction and recording of quality data needed to perform SHM, the statistical significance of the features should be characterized and used in the condensation process.|$|E
50|$|Because {{data can}} be {{measured}} under varying conditions, the ability to normalize the data becomes {{very important to the}} damage identification process. As it applies to SHM, data normalization is the process of separating changes in sensor reading caused by damage from those caused by varying operational and environmental conditions. One of the most common procedures is to normalize the measured responses by the measured inputs. When environmental or <b>operational</b> <b>variability</b> is an issue, the need can arise to normalize the data in some temporal fashion to facilitate the comparison of data measured at similar times of an environmental or operational cycle. Sources of variability in the data acquisition process and with the system being monitored need to be identified and minimized to the extent possible. In general, not all sources of variability can be eliminated. Therefore, it is necessary to make the appropriate measurements such that these sources can be statistically quantified. Variability can arise from changing environmental and test conditions, changes in the data reduction process, and unit-to-unit inconsistencies.|$|E
50|$|Load {{following}} {{power plants}} run {{during the day}} and early evening. They either shut down or greatly curtail output during the night and early morning, when the demand for electricity is the lowest. The exact hours of operation depend on numerous factors. One of the most important factors for a particular plant is how efficiently it can convert fuel into electricity. The most efficient plants, which are almost invariably the least costly to run per kilowatt-hour produced, are brought online first. As demand increases, the next most efficient plants are brought on line and so on. The status of the electrical grid in that region, especially how much base load generating capacity it has, and the variation in demand are also very important. An additional factor for <b>operational</b> <b>variability</b> is that demand does not vary just between night and day. There are also significant variations in the time of year and day of the week. A region that has large variations in demand will require a large load following or peaking power plant capacity because base load power plants can only cover the capacity equal to that needed during times of lowest demand.|$|E
40|$|International audienceIn this {{contribution}} {{the first}} {{results in the}} development of a SHM approach for the foundations of an offshore wind turbine will be presented. Key problems are the <b>operational</b> and environmental <b>variability</b> of the resonance frequencies of the turbine. This paper suggests a (non-) linear regression model to perform data normalization. Real-life data obtained from an offshore turbine on a monopile is used to validate the used model and to demonstrate the performance of the presented approach...|$|R
40|$|International audienceThe {{effects of}} ambient <b>operational</b> {{temperature}} <b>variability</b> on the measured dynamics response of structures {{have been addressed}} in several studies. It is intuitive that temperature variation may change the material/geometric properties or boundary conditions of a structure and therefore may affect the damage detection performance. Then we consider {{the ability of a}} Fuzzy similarity classifier as a feature when the temperature is changing, it will be shown that temperature change might have more significant effect rather than the simulated damage on this feature, which leads to false positive decisions. Therefore, it is vital to compensate the effect of temperature to achieve a desirable result. To do this, the temperature effect is compensated and it is shown the compensation increases the performance of damage detection using the Fuzzy similarity index. To support claims mentioned above, this work involves experiments with composite plate equipped with PZT transducers. To simulate the effect of temperature the specimen is subjected to temperature change between - 25 C and 60 C...|$|R
40|$|This is {{the final}} version of the article. Available from IWA Publishing via the DOI in this record. End-of-pipe {{permitting}} is a widely practised approach to control effluent discharges from wastewater treatment plants. However, the effectiveness of the traditional regulation paradigm is being challenged by increasingly complex environmental issues, ever growing public expectations on water quality and pressures to reduce operational costs and greenhouse gas emissions. To minimise overall environmental impacts from urban wastewater treatment, an operational strategy-based permitting approach is proposed and a four-step decision framework is established: 1) define performance indicators to represent stakeholders’ interests, 2) optimise operational strategies of urban wastewater systems in accordance to the indicators, 3) screen high performance solutions, and 4) derive permits of operational strategies of the wastewater treatment plant. Results from a case study show that <b>operational</b> cost, <b>variability</b> of wastewater treatment efficiency and environmental risk can be simultaneously reduced by at least 7...|$|R
40|$|ABSTRACT: Accelerometer {{data were}} {{acquired}} from a simulated three-story building {{driven by an}} electro-dynamic shaker attached {{to the base of}} the structure. Data were collected on the undamaged structure and after multiple damaged cases had been introduced to the structure. <b>Operational</b> <b>variability</b> was introduced by changing the shaker input levels. A statistical damage detection and localization method was implemented and applied to these data. The algorithm was shown to be insensitive to the <b>operational</b> <b>variability</b> and other sources of variability. This investigation was conducted as part of a conceptual study to demonstrate the feasibility of detecting damage in structural joints caused by seismic excitation. NOMENCLATURE: X = acceleration time history value α = auto-regressive model coefficients ε = residual error m = number of data points n = auto-regressive model order 1...|$|E
40|$|Wind-induced <b>operational</b> <b>variability</b> {{is one of}} {{the major}} {{challenges}} for structural health monitoring of slender engineering structures like aircraft wings or wind turbine blades. Damage sensitive features often show an even bigger sensitivity to <b>operational</b> <b>variability.</b> In this study a composite cantilever was subjected to multiple mass configurations, velocities and angles of attack in a controlled wind tunnel environment. A small-scale impact damage was introduced to the specimen and the structural response measurements were repeated. The proposed damage detection methodology is based on automated operational modal analysis. A novel baseline preparation procedure is described that reduces the amount of user interaction to the provision of a single consistency threshold. The procedure starts with an indeterminate number of operational modal analysis identifications from a large number of datasets and returns a complete baseline matrix of natural frequencies and damping ratios that is suitable for subsequent anomaly detection. Mahalanobis distance-based anomaly detection is then applied to successfully detect the damage under varying severities of <b>operational</b> <b>variability</b> and with various degrees of knowledge about the present operational conditions. The damage detection capabilities of the proposed methodology were found to be excellent under varying velocities and angles of attack. Damage detection was less successful under joint mass and wind variability but could be significantly improved through the provision of the currently encountered operational conditions...|$|E
40|$|Water {{transport}} in the ionomeric membrane, typically Nafion®, has profound {{influence on}} the performance of the polymer electrolyte fuel cell, in terms of internal resistance and overall water balance. In this work, the response of Nafion ® membrane water content and the resulting water gradient to fuel cell <b>operational</b> <b>variability</b> are investigated and the results from a theoretical modeling study are furnished along with reference to neutron imaging observations...|$|E
40|$|<b>Operational</b> time <b>variability</b> {{is one of}} the key {{parameters}} determining theaverage {{cycle time}} of lots. Many different sources of variability can beidentified such as machine breakdowns, setup, and operator availability. However, an appropriate measure to quantify variability is missing. Measuressuch as Overall Equipment Effectiveness (OEE) used in the semiconductorindustry are entirely based on mean value analysis and do not includevariances. 3 ̆cp 3 ̆eThe main contribution of this paper is the development of a new algorithm thatenables estimation of the mean effective process time t_e and the coefficientof variation c_e^ 2 of a multiple machine workstation from real fab data. Thealgorithm formalizes the effective process time definitions as known in theliterature. The algorithm quantifies the claims of machine capacity by lots,which include time losses due to down time, setup time, and otherirregularities. The estimated t_e and c_e^ 2 values can be interpreted inaccordance with the well-known G/G/m queueing relations. Some test examples aswell as an elaborate case from the semiconductor industry show the potential ofthe new effective process time algorithm for cycle time reduction programs. 3 ̆c/p 3 ̆...|$|R
50|$|The SHM {{problem can}} be {{addressed}} {{in the context of}} a statistical pattern recognition paradigm. This paradigm can be broken down into four parts: (1) Operational Evaluation, (2) Data Acquisition and Cleansing, (3) Feature Extraction and Data Compression, and (4) Statistical Model Development for Feature Discrimination. When one attempts to apply this paradigm to data from real world structures, it quickly becomes apparent that the ability to cleanse, compress, normalize and fuse data to account for <b>operational</b> and environmental <b>variability</b> is a key implementation issue when addressing Parts 2-4 of this paradigm. These processes can be implemented through hardware or software and, in general, some combination of these two approaches will be used.|$|R
40|$|<b>Operational</b> time <b>variability</b> {{is one of}} the key {{parameters}} {{determining the}} average cycle time of lots. Many different sources of variability can be identified such as equipment breakdowns, set-up, and operator availability. However, an appropriate measure to quantify variability is missing. Measures such as the overall equipment efficiency (OEE) in the semiconductor industry are entirely based on mean value analysis and do not include variances. The main contribution of this paper is the development of a new algorithm that enables estimation of the mean effective process time te and the coefficient of variation ce 2 of a multiple machine equipment family from real fab data. The algorithm formalizes the effective process time definitions as given by Hopp and Spearman (2000), and Sattler (1996). The algorithm quantifies the claims of machine capacity by lots, which includes time losses due to down time, set-up time, or other irregularities. The estimated te and ce 2 values can be interpreted in accordance with the well-known G/G/m queueing relations. A test example as well as an elaborate case from the semiconductor industry show the potential of the new effective process time (EPT) algorithm for cycle time reduction program...|$|R
40|$|Quantifying the <b>operational</b> <b>variability</b> of extravehicular {{activity}} (EVA) execution {{is critical to}} help design and build future support systems to enable astronauts to monitor and manage operations in deep-space, where ground support operators {{will no longer be}} able to react instantly and manage execution deviations due to the significant communication latency. This study quantifies the <b>operational</b> <b>variability</b> exhibited during Apollo 14 - 17 lunar surface EVA operations to better understand the challenges and natural tendencies of timeline execution and life support system performance involved in surface operations. Each EVA (11 in total) is individually summarized as well as aggregated to provide descriptive trends exhibited throughout the Apollo missions. This work extends previous EVA task analyses by calculating deviations between planned and as-performed timelines as well as examining metabolic rate and consumables usage throughout the execution of each EVA. The intent of this work is to convey the natural variability of EVA operations and to provide operational context for coping with the variability inherent to EVA execution as a means to support future concepts of operations...|$|E
40|$|The {{most widely}} known form of {{multifunctional}} aircraft structure is smart structures for structural health monitoring (SHM). The {{aim is to}} provide automated systems whose purposes are to identify and to characterize possible damage within structures by using a network of actuators and sensors. Unfortunately, environmental and <b>operational</b> <b>variability</b> render many of the proposed damage detection methods difficult to successfully be applied. In this paper, an original robust damage detection approach using output-only vibration data is proposed. It is based on independent component analysis and matrix perturbation analysis, where an analytical threshold is proposed {{to get rid of}} statistical assumptions usually performed in damage detection approach. The effectiveness of the proposed SHM method is demonstrated numerically using finite element simulations and experimentally through a conformal load-bearing antenna structure and composite plates instrumented with piezoelectric ceramic materials. FUI MSIE (Pole Astech...|$|E
40|$|Recent {{advances}} in hardware and instrumentation technology {{have allowed the}} possibility of deploying very large sensor arrays on structures. Exploiting the huge amount of data that can result in order to perform vibration-based structural health monitoring (SHM) is not a trivial task and requires research {{into a number of}} specific problems. In terms of pressing problems of interest, this paper discusses: the design and optimisation of appropriate sensor networks, efficient data reduction techniques, efficient and automated feature extraction methods, reliable methods to deal with environmental and <b>operational</b> <b>variability,</b> efficient training of machine learning techniques and multi-scale approaches for dealing with very local damage. The paper {{is a result of the}} ESF-S 3 T Eurocores project "Smart Sensing For Structural Health Monitoring" (S 3 HM) in which a consortium of academic partners from across Europe are attempting to address issues in the design of automated vibration-based SHM systems for structures. status: publishe...|$|E
40|$|This thesis investigates {{innovative}} effluent point-source permitting approaches from {{an integrated}} urban wastewater system (UWWS) perspective, and demonstrates that three proposed permitting approaches based on optimal operational or control {{strategies of the}} wastewater system are effective in delivering multiple and balanced environmental benefits (water quality, GHG emissions) in a cost-efficient manner. Traditional permitting policy and current flexible permitting practices are first reviewed, and opportunities for permitting from an integrated UWWS perspective are identified. An operational strategy-based permitting approach is first developed by a four-step permitting framework. Based on integrated UWWS modelling, operational strategies are optimised with objectives including minimisation of <b>operational</b> cost, <b>variability</b> of treatment efficiency and environmental risk, subject to compliance of environmental water quality standards. As trade-offs exist between the three objectives, the optimal solutions are screened according to the decision-makers’ preference and permits are derived based on the selected solutions. The advantages of this permitting approach over the traditional regulatory method are: a) cost-effectiveness is considered in decision-making, and b) permitting based on operational strategies is more reliable in delivering desirable environmental outcomes. In the studied case, the selected operational strategies achieve over 78...|$|R
40|$|NASA's has {{established}} long term goals for access-to-space. The third generation launch systems {{are to be}} fully reusable and operational around 2025. The goals for the third generation launch system are to significantly reduce cost and improve safety over current systems. The Advanced Space Transportation Program Office (ASTP) at the NASA's Marshall Space Flight Center in Huntsville, AL has the agency lead to develop space transportation technologies. Within ASTP, under the Spaceliner Investment Area, third generation technologies are being pursued. The Spaceliner Investment Area's primary objective is to mature vehicle technologies to enable substantial increases {{in the design and}} operating margins of third generation RLVs (current Space Shuttle is considered the first generation RLV) by incorporating advanced propulsion systems, materials, structures, thermal protection systems, power, and avionics technologies. Advancements in design tools and better characterization of the operational environment will result in reduced design and <b>operational</b> <b>variabilities</b> leading to improvements in margins. Improvements in operational efficiencies will be obtained through the introduction of integrated vehicle health management, operations and range technologies. Investments in these technologies will enable the reduction in the high operational costs associated with today's vehicles by allowing components to operate well below their design points resulting in improved component operating life, reliability, and safety which in turn reduces both maintenance and refurbishment costs. The introduction of advanced technologies may enable horizontal takeoff by significantly reducing the takeoff weight and allowing use of existing infrastructure. This would be a major step toward the goal of airline-like operation. These factors in conjunction with increased flight rates, resulting from reductions in transportation costs, will result in significant improvements of future vehicles. The real-world problem is that resources are limited and technologies need to be prioritized to assure the resources are spent on technologies that provide the highest system level benefits. Toward that end, a systems approach is being taken to determine the benefits of technologies for the Spaceliner Investment Area. Technologies identified to be enabling will be funded. However, the other technologies will be funded based on their system's benefits. Since the final launch system concept will not be decided for many years, several vehicle concepts are being evaluated to determine technology benefits. Not only performance, but also cost and operability are being assessed. This will become an annual process to assess these technologies against their goals and the benefits to various launch systems concepts. The paper describes the system process, tools and concepts used to determine the technology benefits. Preliminary results will be presented along with the current technology investments that are being made by ASTP's Spaceliner Investment Area...|$|R
40|$|Monitoring for drug-induced liver injury (DILI) via serial {{transaminase}} measurements {{in patients}} on potentially hepatotoxic medications (e. g., for HIV and tuberculosis) is routine in resource-rich nations, but often unavailable in resource-limited settings. Towards enabling universal access to affordable point-of-care (POC) screening for DILI, we have performed the first field {{evaluation of a}} paper-based, microfluidic fingerstick test for rapid, semi-quantitative, visual measurement of blood alanine aminotransferase (ALT). Our objectives were to assess <b>operational</b> feasibility, inter-operator <b>variability,</b> lot variability, device failure rate, and accuracy, to inform device modification for further field testing. The paper-based ALT test was performed at POC on fingerstick samples from 600 outpatients receiving HIV treatment in Vietnam. Results, read independently by two clinic nurses, were compared with gold-standard automated (Roche Cobas) results fro...|$|R
40|$|Complex {{constraints}} generally {{define the}} performance of air transportation systems. These constraints include aircraft operational characteristics, airline operating procedures, and Air Traffic Control (ATC) requirements. The <b>operational</b> <b>variability</b> that is present in complex air transportation systems and their components typically demands a Monte Carlo approach when modeling system performance metrics. However, the inherent variability is generally not known a priori. This calls for a separate model validation approach that yields estimates of system variability and validates baseline model performance. This paper reports on an integrated aviation modeling platform that was developed for comparing and evaluating proposed aircraft flight operations and ATC procedures. It integrates both an agent-based Monte Carlo modeling environment and a data-driven model validation capability. The capabilities are outlined, the validation approach is described, and examples are presented of performance metrics quantifying operational benefits of air navigation procedures that are currently being implemented at major U. S. airports...|$|E
40|$|ABSTRACT: This {{paper is}} a {{continuation}} of a study entitled "Damage Detection in Building Joints by Statistical Analysis " [1] in which accelerometer data were acquired from a simulated three-story building driven by an electro-dynamic shaker attached {{to the base of the}} structure. Joint damage and environmental conditions were simulated and data were collected systematically for comparison. <b>Operational</b> <b>variability</b> was introduced by changing the shaker input amplitudes and frequency range. An Auto-Regressive model with Exogenous Inputs (ARX) was fit to the collected data and the standard deviations of the residual errors between ARX predictions and the measured data were used as the damage sensitive features. A Sequential Probability Ratio Test (SPRT) was used to make damage detection decisions. The test produced promising results, but was shown to be sensitive to the operational and environmental variability. This investigation was conducted as part of a conceptual study to demonstrate the feasibility of detecting damage in structural joints caused by seismic excitation...|$|E
40|$|An {{approach}} to error estimation and measurement {{control in the}} analysis of the balance measurements of mass standards on the in-cell electronic mass balances of the Fuel Conditioning Facility is presented. In light of measurement data from one year of operation, the algorithms proposed are evaluated. The need {{to take into account the}} effects of facility operations on the estimates of measurement uncertainty is demonstrated. In the case of a newly installed balance, where no historical data exists, an ad hoc procedure of adding a term which takes into account the <b>operational</b> <b>variability</b> is proposed. This procedure allows a sufficiently long operation so as to collect data for the estimate of the contribution of operational effects to the uncertainty estimate. An algorithm for systematically taking into account historical data is developed and demonstrated for two balances over two calibration periods. The algorithm, both asymptotically and in the two samples cases, has the necessary desirable properties for estimating the uncertainty in the measurements of the balances...|$|E
40|$|One major {{obstacle}} {{to the implementation of}} structural health monitoring (SHM) is the effect of <b>operational</b> and environmental <b>variabilities,</b> which may corrupt the signal of structural degradation. Recently, an approach inspired from the community of econometrics, called cointegration, has been employed to eliminate the adverse influence from operational and environmental changes and still maintain sensitivity to structural damage. However, the linear nature of cointegration may limit its application when confronting nonlinear relations between system responses. This paper proposes a nonlinear cointegration method based on Gaussian process regression (GPR); the method is constructed under the Engle-Granger framework, and tests for unit root processes are conducted {{both before and after the}} GPR is applied. The proposed approach is examined with real engineering data from the monitoring of the Z 24 Bridge...|$|R
40|$|Stratospheric {{ozone profile}} {{measurements}} from the Stratospheric Aerosol and Gas Experiment~(SAGE) II satellite instrument (1984 – 2005) are combined {{with those from}} the Optical Spectrograph and InfraRed Imager System (OSIRIS) instrument on the Odin satellite (2001 –Present) to quantify interannual variability and decadal trends in stratospheric ozone between 60 ° S and 60 ° N. These data are merged into a multi-instrument, long-term stratospheric ozone record (1984 –present) by analyzing the measurements during the overlap period of 2002 – 2005 when both satellite instruments were <b>operational.</b> The <b>variability</b> in the deseasonalized time series is fit using multiple linear regression with predictor basis functions including the quasi-biennial oscillation, El Niño–Southern Oscillation index, solar activity proxy, and the pressure at the tropical tropopause, in addition to two linear trends (one before and one after 1997), from which the decadal trends in ozone are derived. From 1984 to 1997, there are statistically significant negative trends of 5 – 10 % per decade throughout the stratosphere between approximately 30 and 50 km. From 1997 to present, a statistically significant recovery of 3 – 8 % per decade has taken place {{throughout most of the}} stratosphere with the notable exception between 40 ° S and 40 ° N below approximately 22 km where the negative trend continues. The recovery is not significant between 25 and 35 km altitudes when accounting for a conservative estimate of instrument drift...|$|R
50|$|Critical process {{parameters}} (CPP) in {{pharmaceutical manufacturing}} are key variables affecting the production process. CPPs are attributes that are monitored to detect deviations in standardized production operations and product output quality or changes in Critical Quality Attributes. Those attributes {{with a higher}} impact on CQAs should be prioritized and held in a stricter state of control. The manufacturer should conduct test to set acceptable range limits of the determined CPPs and define acceptable process variable <b>variability.</b> <b>Operational</b> conditions within this range are considered acceptable operational standards. Any deviation from the acceptable range will be indicative of issues within {{the process and the}} subsequent production of substandard products. Data relating to CPP should be recorded, stored, and analyzed by the manufacturer. CPP variables and ranges should be reevaluated after careful analysis of historical CPP data. Identifying CPPs is done in stage one of Process Validation: Process design and ???are an essential part of a manufacturing control strategy.|$|R
40|$|This paper {{reviews the}} {{relevant}} literature for designing a vaccine supply chain. We show that vaccines {{are not like}} commodities and introduce the typical characteristics of a vaccine supply chain. We study the decisions at strategic, tactical and operational levels that are integrated in the research field on supply chain network design. We {{provide an overview of}} how uncertainty is incorporated in the reviewed literature by distinguishing strategic uncertainty and <b>operational</b> <b>variability.</b> Our future vaccine supply chain network needs to be sustainable, hereby taking the preferences of different stakeholders into account for obtaining a set of economical, technological and value key performance indicators that need to be satisfied by the design. This review emphasizes the need of these three types of performance criteria and shows the criteria that are described in the literature. Finally, we identify the most frequently used modeling and solution techniques and discuss the real-life applicability of the research up to now. nrpages: 73 status: publishe...|$|E
40|$|International audienceThis paper {{details the}} {{implementation}} process of an embedded structural health monitoring (SHM) system enabling condition-based {{maintenance of aircraft}} nacelles. One critical issue before {{being able to make}} use of such system is to ensure the effective bonding of the chosen actuators and sensors with their host structure, especially as the latter will be exposed to harsh environments and wide <b>operational</b> <b>variability.</b> In this work, we are concerned with the composite components of the nacelle and we use piezoelectric elements as both sensors and actuators. We propose an integrated approach that allows to validate a combination "Substrate [...] Glue [...] Piezoelectric" (SGP) and thus provides criteria to choose and size these assemblies. This validation scheme is based on the observation of the variations of the static capacity of the piezoelectric element after enduring various temperature and stress conditions when bonded to its host structure. Based on those SGP combinations, an active SHM strategy interrogating the structure by means of elastic wave propagation is currently being developed and preliminary results on samples representative of the nacelle are presented and discussed...|$|E
40|$|The paper {{presents}} a reliable methodology - based on nonlinear acoustics - for impact damage detection in composite materials. The nonlinear vibro-acoustic wave modulation technique {{is used to}} detect damage. The problem of <b>operational</b> <b>variability</b> of the method {{with respect to the}} selection of frequency and amplitude of low-frequency (LF) modal excitation is investigated. This problem is addressed using the concept of stationarity of time series of vibro-acoustic data. Cointegration analysis is employed to compensate for the effect of variable operational conditions associated with LF modal (or vibration) excitation in nonlinear vibro-acoustic wave modulations. Analysis of stationary statistical characteristics of vibro-acoustic responses - after cointegration analysis - are used for damage detection. The proposed method is validated using vibro-acoustic data from laminated composite plates and composite sandwich panels. The results demonstrate that the proposed approach can effectively compensate for the effect of LF modal excitation on nonlinear vibro-acoustic wave modulations and detect the damage more accurately and robustly than the existing nonlinear acoustics based on the analysis of modulation sidebands...|$|E
40|$|We {{consider}} {{a firm that}} invests in capacity under demand uncertainty and thus faces two related but distinct types of risk: mismatch between capacity and demand and profit variability. Whereas mismatch risk can be mitigated with greater <b>operational</b> flexibility, profit <b>variability</b> can be reduced through financial hedging. We show {{that the relationship between}} these two risk mitigating strategies depends on the type of flexibility: Product flexibility and financial hedging tend to be complements (substitutes) [...] i. e., product flexibility tends to increase (decrease) the value of financial hedging, and, vice versa, financial hedging tends to increase (decrease) the value of product flexibility [...] when product demands are positively (negatively) correlated. In contrast to product flexibility, postponement flexibility is a substitute to financial hedging as intuitively expected. Although our analytical results assume perfect flexibility and perfect hedging and rely on a linear approximation of the value of hedging, we validate their robustness in an extensive numerical study. financial hedging, postponement, flexibility, risk management...|$|R
40|$|International audienceApproximately 70 % of {{occupational}} accidents involving lost work time, which occurred in France in 2008, involved bumps, sprains, jams, falls, pain during movement or effort. About 5 % of these accidents were falls from high places and 34 % occurred during manual handling operations. Surprisingly, {{one of the}} immediate causes of these injuries was not any disturbance identified as inherently harmful and external to the victim, but rather disturbances associated with {{the motion of the}} victim. In addition, these occurrences often concerned a hazard termed "circumstantial" since it was, a priori, compatible with human presence in a familiar situation. Virtually all work activity involves a potential risk of this kind of accident, which is frequently considered commonplace, minor and of little interest in the prevention field, despite the statistics. Issues raised by the prevention of this type {{of occupational}} accident, the variety of socio-technical systems, the risks they are required to confront and the multiplicity of objectives in developing a model all justify the current interest in intermediate accident models. We discuss accident model characteristics, which are specifically linked to both the type of accident and model development objective: the model’s systemic nature, its level of generality, the type of accident factors involved and the links between them, the difference between the no-accident work situation and the accident situation as reflected by the model, the model’s limits and its ability to predict occurrence of similar events. Special attention is given to modeling accidents involving movement disturbance. Differences between these accidents and major accidents are confirmed but, paradoxically, a number of aspects are similar. In particular, the difference between modeled accident and no-accident work situations is comparable for a collision with equipment, for example, and a major accident. The latter is described by Hollnagel (2004) as the product of normal <b>operational</b> <b>variabilities.</b> We subsequently describe the model of accidents involving movement disturbance. This model was developed to transform existing representations of this kind of accident. Our model is systemic, taking into account factors considered "subjective," and using a unique combination of common factors to represent deviation from a no-accident work situation. The concept of "circumstantial hazard" in the model allows us to highlight the impossibility of implementing the safest prevention barriers for this type of accident and the difficulties of a risk assessment. This paper confirms the advantage of modeling and analyzing occupational accidents. Accidents models have become a tool for identifying accident-prone variabilities in work situations. Individuals, groups and organizations can all benefit from knowing these variabilities in order to prevent accidents of a similar type. Globalement les caractéristiques des accidents du travail ont évolué au cours des cinquante dernières années. Les accidents les plus nombreux voient leur fréquence persister et font encore l’objet de nombreux préjugés. Il s’agit des accidents initiés par une perturbation du mouvement de la victime. Parallèlement, l’étude des évolutions concomitantes de la prévention des accidents du travail et des caractéristiques des modèles d’accidents oriente clairement l’intérêt pratique vers des modèles intermédiaires répondant à des besoins locaux. Certains caractères des modèles d’accidents sont examinés avec une attention toute spéciale accordée à la modélisation des accidents avec perturbation du mouvement. À propos de ceux-ci on relève curieusement, sur certains aspects, une proximité avec les accidents technologiques majeurs. Enfin, le modèle d’accident développé par les auteurs est présenté. L’objectif de son élaboration, ses spécificités et les conséquences de celles-ci, en termes de contribution à une meilleure prévention sont discutés...|$|R
40|$|Two perfluoroalkyl {{chemicals}} (PFCs), perfluoroctanoic acid (PFOA) and perfluorooctanesulfonate (PFOS), {{have been}} detected widely at low levels in biota and humans. Secondary sewage treatment plants (STPs) do not effectively remove these compounds; hence STP effluents are recognized point sources to the aquatic environment. Here we sampled water from various stages of an advanced water treatment plant, utilizing microfiltration, reverse osmosis and advanced oxidation, and assessed the removal of 15 PFCs in the system. The plant studied takes effluent from several STPs, producing up to 66 ML/day of purified recycled water. Grab samples were collected at various stages of the treatment train. Analysis showed PFCs {{to be present in}} the influent at concentrations of 78. 9 – 151. 2 ng/L (Σ 15 PFCs), but almost completely removed by reverse osmosis. PFCs were detected above reporting limits at 4 of the 7 sampling points. This work provides a measure of PFCs in Australian recycled water, contributes data on the fate of these compounds in advanced water treatment, and illustrates their efficient removal by RO under <b>operational</b> conditions. Analytical <b>variability</b> appeared to be less than variability between sampling events, indicating in future composite samples or flow proportional sampling will improve data quality...|$|R
40|$|Fractal {{dimension}} {{analysis of}} mode shapes has been actively {{studied in the}} area of structural damage detection. The most prominent features of fractal dimension analysis are high sensitivity to damage and instant determination of damage location. However, an intrinsic deficiency is its susceptibility to measurement noise, likely obscuring the features of damage. To address this deficiency, this study develops a novel damage detection method, scale fractal dimension (SFD) analysis of mode shapes, based on combining the complementary merits of a stationary wavelet transform (SWT) and Katz's fractal dimension in damage characterization. With this method, the SWT is used to decompose a mode shape into a set of scale mode shapes at scale levels, with damage information and noise separated into distinct scale mode shapes because of their dissimilar scale characteristics; the Katz's fractal dimension individually runs on every scale mode shape in the noise-adaptive condition provided by the SWT to canvass damage. Proof of concept for the SFD analysis is performed on cracked beams simulated by the spectral finite element method; the reliability of the method is assessed using Monte Carlo simulation to mimic the <b>operational</b> <b>variability</b> in realistic damage diagnosis. The proposed method is further experimentally validated on a cracked aluminum beam with mode shapes acquired by a scanning laser vibrometer. The results show that the SFD analysis of mode shapes provides a new strategy for damage identification in noisy conditions. Department of Mechanical Engineerin...|$|E
40|$|The {{workforce}} {{size and}} the overtime budget have an important impact on the total personnel costs of an organisation. Since the personnel costs represent a significant fraction of the operating costs, {{it is important to}} define an appropriate hiring and overtime policy. Overtime is defined {{as an extension of the}} daily working time or the total working time over the planning period. In this paper, we make the distinction between scheduled and unscheduled overtime when we define the overtime policy. Scheduled overtime is proactively assigned in the baseline personnel roster whereas unscheduled overtime is allocated as a reactive strategy to overcome operational disruptions. The hiring and overtime policy undoubtedly influence the robustness included in a personnel roster, i. e., the capability of an organisation to deal with roster disruptions at an acceptable cost. In this paper, we investigate the trade-off between the hiring budget and the overtime budget and the way overtime should be allocated in the personnel management process. The latter comprehends a trade-off between the proactive scheduling of overtime and the reservation of overtime to balance supply and demand in response to <b>operational</b> <b>variability.</b> Insights are obtained by exploring three different strategies to compose a personnel shift baseline roster. We verify the robustness of each of these strategies by applying a three-step methodology that thrives on optimisation and simulation and we formulate some managerial guidelines to define an appropriate hiring and overtime policy...|$|E
40|$|A {{design study}} was {{completed}} {{to explore the}} theoretical physical capacity (TPC) of the John F. Kennedy International Airport (KJFK) runway system for a northflow configuration assuming impedance-free (to throughput) air traffic control functionality. Individual runways were modeled using an agent-based, airspace simulation tool, the Airspace Concept Evaluation System (ACES), with all runways conducting both departures and arrivals on a first-come first-served (FCFS) scheduling basis. A realistic future flight schedule was expanded to 3. 5 times the traffic level of a selected baseline day, September 26, 2006, to provide a steady overdemand state for KJFK runways. Rules constraining departure and arrival operations were defined to reflect physical limits beyond which safe operations {{could no longer be}} assumed. Safety buffers to account for all sources of <b>operational</b> <b>variability</b> {{were not included in the}} TPC estimate. Visual approaches were assumed for all arrivals to minimize inter-arrival spacing. Parallel runway operations were assumed to be independent based on lateral spacing distances. Resulting time intervals between successive airport operations were primarily constrained by same-runway and then by intersecting-runway spacing requirements. The resulting physical runway capacity approximates a theoretical limit that cannot be exceeded without modifying runway interaction assumptions. Comparison with current KJFK operational limits for a north-flow runway configuration indicates a substantial throughput gap of approximately 48 %. This gap may be further analyzed to determine which part may be feasibly bridged through the deployment of advanced systems and procedures, and which part cannot, because it is either impossible or not cost-effective to control. Advanced systems for bridging the throughput gap may be conceptualized and simulated using this same experimental setup to estimate the level of gap closure achieved...|$|E
40|$|AbstractEnd-of-pipe {{permitting}} is {{a widely}} practised approach to control effluent discharges from wastewater treatment plants. However, {{the effectiveness of the}} traditional regulation paradigm is being challenged by increasingly complex environmental issues, ever growing public expectations on water quality and pressures to reduce operational costs and greenhouse gas emissions. To minimise overall environmental impacts from urban wastewater treatment, an operational strategy-based permitting approach is proposed and a four-step decision framework is established: 1) define performance indicators to represent stakeholders’ interests, 2) optimise operational strategies of urban wastewater systems in accordance to the indicators, 3) screen high performance solutions, and 4) derive permits of operational strategies of the wastewater treatment plant. Results from a case study show that <b>operational</b> cost, <b>variability</b> of wastewater treatment efficiency and environmental risk can be simultaneously reduced by at least 7 %, 70 % and 78 % respectively using an optimal integrated operational strategy compared to the baseline scenario. However, trade-offs exist between the objectives thus highlighting the need of expansion of the prevailing wastewater management paradigm beyond the narrow focus on effluent water quality of wastewater treatment plants. Rather, systems thinking should be embraced by integrated control of all forms of urban wastewater discharges and coordinated regulation of environmental risk and treatment cost effectiveness. It is also demonstrated through the case study that permitting operational strategies could yield more environmentally protective solutions without entailing more cost than the conventional end-of-pipe permitting approach. The proposed four-step permitting framework builds on the latest computational techniques (e. g. integrated modelling, multi-objective optimisation, visual analytics) to efficiently optimise and interactively identify high performance solutions. It could facilitate transparent decision making on water quality management as stakeholders are involved in the entire process and their interests are explicitly evaluated using quantitative metrics and trade-offs considered in the decision making process. We conclude that the operational strategy-based permitting shows promising for regulators and water service providers alike...|$|R
40|$|In {{the present}} fierce global competition, poor responsiveness, low {{flexibility}} {{to meet the}} uncertainty of demand, and the low efficiency of traditional assembly lines are adequate motives to persuade manufacturers to adopt highly flexible production tools such as cross-trained workers who move along the assembly line while carrying out their planned jobs at different stations [1]. Cross-trained workers can be applied in various models in assembly lines. A novel model which taken into consideration in many industries nowadays is called the linear walking worker assembly line and employs workers who travel along the line and fully assemble the product {{from beginning to end}} [2]. However, these flexible assembly lines consistently endure imbalance in their stations which causes a significant loss in the efficiency of the lines. The <b>operational</b> time <b>variability</b> {{is one of the main}} sources of this imbalance [3] and is the focus of this study which investigated the possibility of decreasing the mentioned loss by arranging workers with different variability in a special order in walking worker assembly lines. The problem motivation comes from the literature of unbalanced lines which is focused on bowl phenomenon. Hillier and Boling [4] indicated that unbalancing a line in a bowl shape could reach the optimal production rate and called it bowl phenomenon.  This study chose a conceptual design proposed by a local automotive company as a case study and a discrete event simulation study as the research method to inspect the questions and hypotheses of this research.   The results showed an improvement of about 2. 4 % in the throughput due to arranging workers in a specific order, which is significant compared to the fixed line one which had 1 to 2 percent improvement. In addition, analysis of the results concluded that having the most improvement requires grouping all low skill workers together. However, the pattern of imbalance is significantly effective in this improvement concerning validity and magnitude...|$|R
40|$|Quantum-dot {{cellular}} automata (QCA) is a field-coupled computing paradigm. States of a {{cell change}} due to mutual interactions of either electrostatic or magnetic fields. Due to their small sizes, power is an important design parameter. In this paper, we derive an upper bound for power loss that will occur with input change, even with the circuit staying at respective ground states {{before and after the}} change. This bound is computationally efficient to compute for large QCA circuits since it just requires the knowledge of the before and after ground states due to input change. We categorize power loss in clocked QCA circuits into two types that are commonly used in circuit theory: switching power and leakage power. Leakage power loss is independent of input states and occurs when the clock energy is raised or lowered to depolarize or polarize a cell. Switching power is dependent on input combinations and occurs at the instant when the cell actually changes state. Total power loss is controlled by changing the rate of change of transitions in the clocking function. Our model provides an estimate of power loss in a QCA circuit for clocks with sharp transitions, which result in nonadiabatic operations and gives us the upper bound of power expended. We derive expressions for upper bounds of switching and leakage power that are easy to compute. Upper bounds obviously are pessimistic estimates, but are necessary to design robust circuits, leaving room for <b>operational</b> manufacturing <b>variability.</b> Given that thermal issues are critical to QCA designs, we show how our model can be valuable for QCA design automation in multiple ways. It can be used to quickly locate potential thermal hot spots in a QCA circuit. The model {{can also be used to}} correlate power loss with different input vector switching; power loss is dependent on the input vector. We can study the tradeoff between switching and leakage power in QCA circuits. And, we can use the model to vet different designs of the - - same logic, which we demonstrate for the full adder...|$|R
