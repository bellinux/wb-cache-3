19|10000|Public
5000|$|The SIGCOMM Award, for {{outstanding}} lifetime technical {{achievement in the}} fields <b>of</b> <b>data</b> <b>and</b> <b>computer</b> communications ...|$|E
50|$|Optimization {{provides}} the technical basis for targeting decisions. Whilst mathematical optimization {{theory has been}} in existence since the 1950s, its application to marketing only began in the 1970s, and lack <b>of</b> <b>data</b> <b>and</b> <b>computer</b> power were limiting factors until the 1990s.|$|E
50|$|The {{promise of}} Peer To Patent also {{draws on the}} success of various other {{movements}} that have created effective, productive communities on the Internet from far-flung individuals: free software and open-source software development, peer-to-peer systems for the collaborative sharing <b>of</b> <b>data</b> <b>and</b> <b>computer</b> processing, and Wikipedia.|$|E
40|$|Atmospheric {{simulation}} is {{an important}} means of understanding the environment around us. Through the collection of large amounts <b>of</b> atmospheric <b>data</b> <b>and</b> <b>computer</b> modeling one can predict how variuos particulates such as dirt, smog, and fire can affect our cities and overall public health. However, gleaning insight from numerica...|$|R
50|$|The Telecommunications Industry Association's Telecommunications Infrastructure Standard for Data Centers {{specifies}} {{the minimum}} requirements for telecommunications infrastructure <b>of</b> <b>data</b> centers <b>and</b> <b>computer</b> rooms including single tenant enterprise <b>data</b> centers <b>and</b> multi-tenant Internet hosting data centers. The topology proposed {{in this document}} {{is intended to be}} applicable to any size data center.|$|R
40|$|The {{comparison}} of theoretical and instrument response functions {{and its use}} as a procedure for determining the transfer function of the COPE correlation interferometer are summarized. Data show qualitative agreement can be obtained when discrepancies between theory and instrument are investigated and instrument components are analyzed in detail. Data were obtained using a set <b>of</b> calibration <b>data</b> <b>and</b> <b>computer</b> algorithms...|$|R
30|$|Xijie Yin {{was born}} in Shandong, China, in 1965. She {{received}} the Master degree from Shandong University, China. Now, she works in the School <b>of</b> <b>Data</b> <b>and</b> <b>Computer</b> Science, Shandong Women’s University, Her research interests include cloud security and image processing.|$|E
40|$|The field <b>of</b> <b>data</b> <b>and</b> <b>computer</b> {{communications}} networking uses {{an array}} of abstract concepts such as encapsulation, protocol data units, virtual circuits etc. to describe and explain the underlying processes. Various studies, together with our own observations, strongly indicate that students often find these concepts difficult to learn, as they cannot easily be demonstrated...|$|E
40|$|This course {{introduces}} {{not only}} the fundamental principles, technology, and current development <b>of</b> <b>data</b> <b>and</b> <b>computer</b> communications; it also exposes students {{to most of the}} current research areas in the field of computer communications. Such as protocol advancements, security issues, networking, routing, wireless and mobility. It lays down many topics of research and prepares students to do good quality research in the area...|$|E
50|$|The Telecommunications Industry Association (TIA) ANSI/TIA-942-A Telecommunications Infrastructure Standard for Data Centers is an American National Standard (ANS) that {{specifies}} {{the minimum}} requirements for telecommunications infrastructure <b>of</b> <b>data</b> centers <b>and</b> <b>computer</b> rooms including single tenant enterprise <b>data</b> centers <b>and</b> multi-tenant Internet hosting data centers. The topology {{proposed in the}} standard {{was intended to be}} applicable to any size data center.|$|R
40|$|In {{accelerator}} applications we need photon <b>and</b> electron <b>data,</b> as well {{as computer}} codes that utilize this data, in order to predict results inexpensively and safely. In this paper I will first cover {{the current status of}} available photon <b>and</b> electron <b>data,</b> with emphasize on the improved detailed that has only recently been added to our data bases. Next I will cover the availability <b>of</b> this <b>data</b> <b>and</b> <b>computer</b> codes that use it...|$|R
5000|$|... collectd is a Unix daemon that collects, {{transfers}} <b>and</b> stores performance <b>data</b> <b>of</b> <b>computers</b> <b>and</b> network equipment. The acquired data {{is meant}} to help system administrators maintain an overview over available resources to detect existing or looming bottlenecks.|$|R
30|$|He is {{currently}} with the School <b>of</b> <b>Data</b> <b>and</b> <b>Computer</b> Science, Sun Yat-sen University, China. He received his B.S. degree from Nanjing University of Science and Technology, China, in 1995, and the M.S. and Ph.D. degrees from Huazhong University of Science and Technology, China, in 2002 and 2005, respectively. From June of 2009 to June of 2010, {{he was a}} Postdoctoral Researcher at New Jersey Institute of Technology, NJ, USA. From August of 2013 to August of 2014, he was a Korea Foundation for Advanced Studies (KFAS) scholar at Korea University, Seoul, Korea. His research interests include reversible data hiding, steganography, steganalysis, and digital forensics.|$|E
40|$|AbstractFood {{composition}} {{tables are}} largely a twentieth century product. The early tables in America developed by W. O. Atwater in the 1890 's at the Storrs Connecticut Agricultural Experiment Station grew exponentially {{during the first}} half of the 20 th century as one after another vitamin and mineral was found to be essential for life. As tables became more complex the user required better tools for calculating food composition. The rapid growth of computer applications in the mid to late 20 th century allowed nutritionists to access these complex tables. This confluence <b>of</b> <b>data</b> <b>and</b> <b>computer</b> applications spawned an organization created specifically to study food composition databases...|$|E
40|$|Thesis for {{the degree}} of Licentiate of Engineering, a degree that falls between M. Sc. and Ph. D. With the ever {{increased}} use of computers for critical systems, computer security— the protection <b>of</b> <b>data</b> <b>and</b> <b>computer</b> systems from intentional, malicious intervention—is attracting increasing attention. Many methods of defence already exist, of which one is the strong perimeter defence. This thesis is concerned with one such method of defence, the automated computer security intrusion detection system, or intrusion detection system (IDS) for short. The field has existed for some years, but this thesis demonstrates that several fundamental factors {{in the application of}} intrusion detection systems still remain unaddressed. Two of the main factors are effectiveness—how to make the intrusion detection system classify malign and benign activity correctly—and efficiency—how to run the intrusion detection system in as cost effective a manner as possible...|$|E
40|$|The Energy Information Administration (EIA) makes {{available}} for public use a series <b>of</b> machine-readable <b>data</b> files <b>and</b> <b>computer</b> models. The <b>data</b> files <b>and</b> models are {{made available to}} the public on magnetic tapes. In addition, selected data files/models are available on diskette for IBM-compatible personal computers...|$|R
50|$|Brown {{has stated}} {{that it is more}} likely that one or more non-companion stars, passing near the Sun billions of years ago, could have pulled Sedna out into its current orbit. In 2004, Kenyon {{forwarded}} this explanation after analysis <b>of</b> Sedna's orbital <b>data</b> <b>and</b> <b>computer</b> modeling <b>of</b> possible ancient non-companion star passes.|$|R
40|$|Abstract. With {{the rapid}} {{development}} <b>of</b> <b>data</b> science <b>and</b> <b>computer</b> technology, hierarchical storage {{system based on}} Hadoop is being research. In this paper, we propose a novel methodology of hierarchical storage system based on Hadoop framework. This way the Hadoop cluster will not reach the stage where the NameNode becomes irresponsive due to excessive JVM garbage collection as the HDFS will not be heavily loaded. The experimental result illustrates the effectiveness and feasibility of proposed framework, further modification areas of research are proposed in the end...|$|R
40|$|This paper aims to {{research}} various data mining techniques applied to solve intrusion detection problems. In general, intrusion detection techniques {{can be divided}} into two major categories: misuse detection and anomaly detection. Taking into consideration effectiveness of the anomaly detection technique not only against known types of attacks (like misuse detection does by exploiting signature database) but also against new ones, it has become a topical issue in majority <b>of</b> <b>data</b> <b>and</b> <b>computer</b> security researches. The techniques discussed in the paper include the Hidden Markov Model (HMM) method for modelling and evaluating invisible events based on system calls, further development of Stephanie Forrest’s idea of the fixed-length audit trail patterns, the principle component analysis based method for anomaly intrusion detection with less computation efforts, algorithm based on k-nearest neighbour method, as well as applying association rule algorithm to audit data...|$|E
40|$|The use of {{specialized}} techniques for recovery, authentication, {{and analysis of}} electronic data. • Used for reconstruction of computer usage, examination of residual data, and authentication of data by technical analysis or explanation of technical features <b>of</b> <b>data</b> <b>and</b> <b>computer</b> usage. 2 Who Uses It? • FBI • Private companies • It’s services are used by many corporations. Why is it Used? • To view the amount of damage caused by an intruder. • To find evidence of terrorism, child pornography, crimes of violence, theft or destruction of intellectual property, Internet crimes, and fraud. Computer Crime • The use of computers in crime falls into two general categories: – They can be the instrument of an offence. • Tool to commit theft, extortion, fraud, attacking other systems – The can contain evidence pertaining to an offence. • Communication with victims or accomplices, hiding evidence...|$|E
40|$|The {{scale and}} {{diversity}} of networked sources <b>of</b> <b>data</b> <b>and</b> <b>computer</b> programs is rapidly swamping human abilities to digest and even locate relevant information. The high speed of computing has compounded this problem by the generation of even larger amounts of data, derived {{in ways that are}} generally opaque to human users. The result is an increasing gulf between human and computer abilities. Society's ever wider-scale dependence on rapidly growing networked sources of software threatens severe breakdowns if machine intelligibility issues are not given high priority. In this paper we argue that lack of machine intelligibility in humancomputer interactions can be traced directly to present approaches to software design. According to the duality principle in this paper, software involved in human-computer interaction should contain two distinct layers: a declarative knowledge-level layer and a lower-level functional or procedural-knowledge layer. This extends the formal methods separation [...] ...|$|E
40|$|In {{this text}} we compile the grounds and {{reasons why the}} Virtual Private Networks are being {{strongly}} introduced in the context <b>of</b> <b>data</b> communication <b>and</b> <b>computer</b> security. It is also reasoned out {{the election of a}} specic Virtual Private Network OpenVPN), in order to be implemented and tested. We deeply analyze this kind of Virtual Network, the different scenarios in which it can be applied, as well as its conguration and installation. Finally we analyze the achieved results and the possible security problems that may aris...|$|R
40|$|This {{course will}} focus on imparting {{knowledge}} about the aspects <b>of</b> <b>data</b> communication <b>and</b> <b>computer</b> network systems with the required basic principles behind them. This course provides essential knowledge about the OSI model and TCP/IP model. It creates a good foundation covering the physical, data link, network, transport, and application layers. Course Outcomes: a. To understand the communication basics. b. To have the knowledge of different networks. c. To know about different protocols. d. To understand {{how to find the}} routes by using different routing algorithms...|$|R
50|$|IT general {{controls}} (ITGC) are {{controls that}} apply to all systems components, processes, <b>and</b> <b>data</b> for a given organization or information technology (IT) environment. The objectives of ITGCs are to ensure the proper {{development and implementation of}} applications, as well as the integrity <b>of</b> programs, <b>data</b> files, <b>and</b> <b>computer</b> operations.|$|R
40|$|The digital society demands greater {{institutional}} and political transparency, inuenced by the openness and accessibility {{characteristics of the}} Internet. The election campaign {{is the time for}} political parties to show how they want to act in government. This research aims to analyze what the parties say about their commitment to transparency and how transparent they are, in fact, during the electoral campaign. Thus, it tries to measure the level of transparency of Partido Popular and Ciudadanos during the election campaigns of 2015 and 2016 for the General Elections. For this, a quantitative and semi- qualitative content analysis was approached from the project of Dader, Campos and Quintana (2011) on the transparency of the organization, the transparency of the campaign activity, the transparency of the participation with the citizens, Transparency <b>of</b> <b>data</b> <b>and</b> <b>computer</b> tools and the transparency portal. The results allow us to observe an improvement between 2015 and 2016, as well as the dierences between what they proclaim in their electoral programs and the reality of their portals of transparency. ...|$|E
40|$|BACKGROUND Experienced {{judgement}} and specialist {{knowledge are}} essential to the proper specification, understanding and interpretation <b>of</b> <b>data</b> <b>and</b> <b>computer</b> analyses. The human expert has traditionally supplied this knowledge and judgement with the computer doing the necessary number-crunching. However, artificial intelligence (AI) research provides ways of embodying this knowledge and judgement within computer programs. Despite an early lead in the field, UK research and developmnent into AI techniques was held back in the 1970 s when the then Science Research Council took the view that the 'combinatorial explosion' of possibilities would be an insurmountable obstacle to AI developent. But in America and Japan research continued, and the surge of interest in the 1980 s has been a consequence of the 'Fifth Generation Computer' research programme initiated by Japan (Feigenbaum and McCorduck; 1984). This led in Europe to the ESPRIT programme of advanced technology research, and in the UK to the Alvey programme (Department of Industry, 1982). As a result, all sectors of industry have been encouraged to consider how such advanced technology can be applied, and the transport industry is no exception. This paper sets out to explain some of the relevant techniques in simple terms, and to describe a number of situations in which transport planning and operations might be helped through their use, illustrating this by reference to the pioneering work going on in transport applications in the USA, Britain and Australia...|$|E
40|$|Abstract Tree {{identification}} {{is a very}} important to support almost all activities in the forest sector. Unfortunately, the inavailability <b>of</b> <b>data</b> <b>and</b> <b>computer</b> programs that is user friendly have caused ineficiency in tree identification. This research tries to make an expert system to identify trees by using the leaf images. To store the data in the knowledge base one must choose one of the some leaf images that are in the data base available in the program according the characteristic of the leaf. Each leaf image has a code and the accumulation of all codes build a tree code then this code is saved in the knowledge base. The tree code is used to identify a tree by making the comparison between input chosen by user and the tree code in the knowledge base using forward chaining. User who has information about a tree can add to the knowledge base but this information must be validated by an expert before it is used in the system. Another task of an expert is to give a CF (certainty factor) for each tree. The result of this research shows that no more errors are found due to input mistakes and the program is more user friendly. Another advantage is that the knowledge base is more flexible, dynamic and well organized Validation of knowledge base by experts can increase the quality and accuracy of using the knowledge base system. Keywords : expert system, leaf image, knowledge base, forward chaining, C...|$|E
5000|$|This stock-market course pulled Smith {{away from}} {{macroeconomics}} towards finance, {{and the use}} and misuse of statistics in finance pulled Smith towards a lifelong interest in the abuse <b>of</b> <b>data</b> <b>and</b> statistical analysis.Tobin once wryly observed that {{the bad old days}} when researchers had to do calculations by hand were actually a blessing. In today’s language, it was a feature, not a flaw The calculations were so hard that people thought hard before they calculated. Today, with terabytes <b>of</b> <b>data</b> <b>and</b> lightning-fast <b>computers,</b> it is too easy to calculate first, think later. Smith argues that it is better to think hard before calculating ...|$|R
40|$|Abstract- The {{accurate}} {{prediction of}} storms {{is vital to}} the oil and gas sector for the management of their operations. An overview of research exploring the prediction of storms by ensemble prediction systems is presented and its application to the oil and gas sector is discussed. The analysis method used requires larger amounts <b>of</b> <b>data</b> storage <b>and</b> <b>computer</b> processing time than other more conventional analysis methods. To overcome these difficulties eScience techniques have been utilised. These techniques potentially have applications to the oil and gas sector to help incorporate environmental data into their information systems...|$|R
40|$|The main {{objective}} is to conduct <b>data</b> analyses <b>of</b> SEPAC <b>data</b> <b>and</b> <b>computer</b> modeling to investigate spacecraft environment effects associated with injection of electron beam, plasma clouds, and neutral gas clouds from the Shuttle Orbiter. To understand the dependence of spacecraft charging potential on beam density and other plasma parameters, a two dimensional electrostatic particle code was used to simulate the injection of electron beams from an infinite conductor into a plasma. The ionization effects on spacecraft charging are examined by including interactions of electrons with neutral gases. A survey of the simulation results is presented and discussed...|$|R
40|$|The {{second edition}} of An Introduction to Efficiency and Productivity Analysis is {{designed}} to be a general introduction for those who wish to study efficiency and productivity analysis. The book provides an accessible, well-written introduction to the four principal methods involved: econometric estimation of average response models; index numbers, data envelopment analysis (DEA); and stochastic frontier analysis (SFA). For each method, a detailed introduction to the basic concepts is presented, numerical examples are provided, and some of the more important extensions to the basic methods are discussed. Of special interest is the systematic use of detailed empirical applications using real-world data throughout the book. In recent years, {{there have been a number}} of excellent advance-level books published on performance measurement. This book, however, is the first systematic survey of performance measurement with the express purpose of introducing the field to a wide audience of students, researchers, and practitioners. Indeed, the 2 nd Edition maintains its uniqueness: (1) It is a well-written introduction to the field. (2) It outlines, discusses and compares the four principal methods for efficiency and productivity analysis in a well-motivated presentation. (3) It provides detailed advice on computer programs that can be used to implement these performance measurement methods. The book contains computer instructions and output listings for the SHAZAM, LIMDEP, TFPIP, DEAP and FRONTIER computer programs. More extensive listings <b>of</b> <b>data</b> <b>and</b> <b>computer</b> instruction files are available on the book 2 ̆ 7 s website: (www. uq. edu. au/economics/cepa/crob 2005) ...|$|E
40|$|The field <b>of</b> <b>data</b> <b>and</b> <b>computer</b> {{communications}} networking uses {{an array}} of abstract concepts such as encapsulation, protocol data units, virtual circuits etc. to describe and explain the underlying processes. Various studies, together with our own observations, strongly indicate that students often find these concepts difficult to learn, as they cannot easily be demonstrated. A number of academics have described the animation tools they have developed to illustrate such concepts and almost invariably they comment on the favourable reactions their efforts receive from their students. However, {{it is difficult to}} find examples that anchor the design of their animation offerings in the principles of good instructional design and few conduct rigorous evaluations to see if there has been a genuine and measurable improvement in student understanding of the basic concepts being illustrated. Our work does both. We have designed an animation tool on virtual circuits which takes cognizance of contemporary work on instructional design, particularly the work of Mayer (2003). Two versions of the animation were produced, one with narration and the other with narration and additional on-screen text. We randomly assigned 110 first year undergraduate students to view a version of the animation. A pre and post test was used to determine if, in fact, improved learning actually occurred and which version of the animation produced the better outcome. Initial analysis of results indicates no statistical difference between the scores for the two versions and suggests that animations, by themselves, do not necessarily improve student understanding...|$|E
40|$|With {{the ever}} {{increasing}} use of computers for critical systems, computer security, the protection <b>of</b> <b>data</b> <b>and</b> <b>computer</b> systems from intentional, malicious intervention, is attracting much attention. Among the methods for defence, intrusion detection, i. e. {{the application of a}} tool to help the operator identify ongoing or already perpetrated attacks {{has been the subject of}} considerable research in the past ten years. A key problem with current intrusion detection systems is the high number of false alarms they produce. This thesis presents research into why false alarms are and will remain a problem and proposes to apply results from the field of information visualisation to the problem of intrusion detection. This was thought to enable the operator to correctly identify false (and true) alarms, and also aid the operator in identifying other operational characteristics of intrusion detection systems. Four different visualisation approaches were tried, mainly on data from web server access logs. Two direct approaches were tried; where the system puts the onus of identifying the malicious access requests on the operator by way of the visualisation. Two indirect approaches were also tried; where the state of two self learning automated intrusion detection systems were visualised to enable the operator to examine their inner workings. This with the hope that in doing so, the operator would gain an understanding of how the intrusion detections systems operated and whether that level of operation, and the quality of the output, was satisfactory. Several experiments were performed and many different attacks in web access data from publicly available web servers were found. The visualisation helped the operator either detect the attacks herself and more importantly the false alarms. It also helped her determine whether other aspects of the operation of the self learning intrusion detection systems were satisfactory...|$|E
40|$|This paper {{describes}} a Bayesian technique for unsupervised classification <b>of</b> <b>data</b> <b>and</b> its <b>computer</b> implementation, Autoclass. Given real valued or discrete data, AutoClass automatically determines the most probable num-ber of classes {{present in the}} data, the most probable descriptions of those classes, and each object's probability of membership in each class. The program performs as well as or better than existing automatic classifica-tion systems when run on the same <b>data,</b> <b>and</b> contains no ad hoc similarity measures or stopping criteria. Researchers have also applied AutoClass to several large databases where it has discovered classes corresponding to new phenomena which were previously unsuspected...|$|R
40|$|EIA makes {{available}} for public use a series <b>of</b> machine-readable <b>data</b> files <b>and</b> <b>computer</b> models on magnetic tapes. Selected data files/models {{are also available}} on diskette for IBM-compatible personal computers. For each product listed in this directory, a detailed abstract is provided which describes the data published. Ordering information is given in the preface. Indexes are included...|$|R
40|$|Research is {{reported}} dealing with problems <b>of</b> digital <b>data</b> transmission <b>and</b> <b>computer</b> communications networks. The results of four individual studies are presented which include: (1) signal processing with finite state machines, (2) signal parameter estimation from discrete-time observations, (3) digital filtering for radar signal processing applications, and (4) multiple server queues where all servers are not identical...|$|R
