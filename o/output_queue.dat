95|303|Public
5000|$|... if JCL {{validation}} is unsuccessful, job {{is moved}} into <b>output</b> <b>queue</b> ...|$|E
5000|$|Push 3 to the <b>output</b> <b>queue</b> (whenever {{a number}} is read it is {{pushed to the}} output) ...|$|E
5000|$|When a job {{is moved}} into <b>output</b> <b>queue,</b> the {{submitter}} will receive a [...] "job ended" [...] notification (via topic).|$|E
40|$|In {{this paper}} we propose an {{architecture}} for an <b>output</b> <b>queued</b> switch based on the mesh of trees topology. After establishing the equivalence of our proposal with the <b>output</b> <b>queued</b> model, we analyze its features, showing that it merges positive features of the input queued switches (specially their implementability) with all the characteristics typical of <b>output</b> <b>queued</b> ones. Moreover, such an architecture is able to easily and efficiently manage multicast traffic, which is becoming extremely important in networks with traditional communication services integrated in...|$|R
40|$|Programmable routers {{extend the}} {{traditional}} store-and-forward paradigm of a router to a store-process-and-forward paradigm to perform custom application processing of packets. Building programmable routers which are scalable and can provide Quality of Service (QoS) guarantees to individual network flows poses significant challenges. Firstly, most QoS mechanisms {{are designed to}} work on <b>output</b> <b>queued</b> routers whereas scalable routers always employ some form of both input and <b>output</b> <b>queuing.</b> Also, even in <b>output</b> <b>queued</b> routers {{most of the research}} work has been directed at link scheduling. Scheduling processing resources in such routers poses new challenges due to the inherent unpredictability of execution times of arbitrary application codes. In this thesi...|$|R
40|$|Fixed-size packet {{switches}} � Operates in a time-slotted manner � The slot duration {{is equal to}} the cell transmission time Contention occurs when multiple inputs have arrivals destined to the same output Buffering is needed to avoid packet loss Buffering schemes in a packet switch � <b>Output</b> <b>queueing</b> (IQ) � Input queueing (OQ) � Virtual <b>output</b> <b>queueing</b> (VOQ) / combined input-output-queuein...|$|R
50|$|The {{shunting}} yard algorithm {{can also be}} applied to produce prefix notation (also known as Polish notation). To do this one would simply start {{from the end of}} a string of tokens to be parsed and work backwards, reverse the <b>output</b> <b>queue</b> (therefore making the <b>output</b> <b>queue</b> an output stack), and flip the left and right parenthesis behavior (remembering that the now-left parenthesis behavior should pop until it finds a now-right parenthesis).|$|E
5000|$|... {{while there}} are tokens to be read: read a token. if the token is a number, then push it to the <b>output</b> <b>queue.</b> if the token is an {{operator}}, then: while there is an operator {{at the top of}} the operator stack with greater than or equal to precedence and the operator is left associative: pop operators from the operator stack, onto the <b>output</b> <b>queue.</b> push the read operator onto the operator stack. if the token is a left bracket (i.e. [...] "("), then: push it onto the operator stack. if the token is a right bracket (i.e. [...] ")"), then: while the operator {{at the top of the}} operator stack is not a left bracket: pop operators from the operator stack onto the <b>output</b> <b>queue.</b> pop the left bracket from the stack. /* if the stack runs out without finding a left bracket, then there are mismatched parentheses. */if there are no more tokens to read: {{while there are}} still operator tokens on the stack: /* if the operator token on the top of the stack is a bracket, then there are mismatched parentheses. */ pop the operator onto the output queue.exit.|$|E
50|$|Control {{symbols are}} used to delimit packets (Start of Packet, End of Packet, Stomp), to {{acknowledge}} packets (Packet Acknowledge, Packet Not Acknowledged), reset (Reset Device, Reset Port) and to distribute events within the RapidIO system (Multicast Event Control Symbol). Control symbols are also used for flow control (Retry, Buffer Status, Virtual <b>Output</b> <b>Queue</b> Backpressure) and for error recovery.|$|E
40|$|Traditionally, <b>Output</b> <b>Queued</b> switch {{architectures}} {{have been}} proposed to implement Quality of Service schemes such as Weighted Fair <b>Queueing.</b> <b>Output</b> <b>Queued</b> switches with N input and output ports require up to N serial memory operations per time slot (taken to be the time between packet arrivals at an input). Given the high and increasing processor/memory gap, {{it is important to}} shift the bottleneck from memory to processor in order to obtain scalable architectures. It has recently been demonstrated that most <b>Output</b> <b>Queued</b> switches can be emulated using Combined Input <b>Output</b> <b>Queued</b> Switches which require O(N) processor operations and a small, constant number of memory operations, thus moving the performance bottleneck from memory to processor. These bounds hold against all, even adversarial, traÆc patterns. In this paper we analyze the scheduling algorithms used in [2, 10] to obtain the above results when the input traÆc is stochastic. We prove that if the queue size at each output por [...] ...|$|R
40|$|Abstract — This paper {{presents}} {{an analysis of}} <b>output</b> <b>queued</b> cell switches which are introduced with generic non-uniformly distributed traffic. Random arbitration is employed whereby non-empty queues compete equally for service within each switching interval. In particular, we study the case of two-state Markov-modulated arrivals in which input ports generate bursty streams that are non-uniformly distributed. Under the assumption of a memoryless server, the probability generating function of the interarrival process is utilized to derive closedform expressions for the queue size distribution. The methodology established in this paper forms a flexible tool in determining bounds on the behavior and expected performance of <b>output</b> <b>queued</b> switches under a range of traffic scenarios. The validity of the analytical inference is established through simulation results. Keywords- <b>output</b> <b>queued</b> switches; ON/OFF arrivals; packet scheduling; performance analysis I...|$|R
40|$|We {{describe}} a novel method for scheduling high-speed network switches. The targeted architecture is an input-buffered switch with a non-blocking switch fabric. The input buffers are organized as virtual <b>output</b> <b>queues</b> to avoid head-of-line blocking. The {{task of the}} scheduler is to decide when the input ports can forward packets from the virtual <b>output</b> <b>queues</b> to the corresponding output ports. Our Least Choice First (LCF) scheduling method selects the input and output ports to be matched by prioritizing the input ports according {{to the number of}} virtual <b>output</b> <b>queues</b> that contain packets: The fewer virtual <b>output</b> <b>queues</b> with packets, the higher the scheduling priority of the input port. This way, the number of switch connections and, with it, switch throughput is maximized. Fairness is provided through the addition of a round-robin algorithm. We present two alternative implementations: A central implementation intended for narrow switches and a distributed implementation based on an iterative algorithm intended for wide switches. The simulation results show that the LCF scheduler outperforms other scheduling methods such as the parallel iterative matcher [1], iSLIP [12], and the wave front arbiter [16]...|$|R
50|$|Each {{input line}} card spreads its packets evenly to the N buffers, {{something}} it can clearly do without contention. Each buffer writes these packets {{into a single}} buffer-local memory at a combined rate of R. Simultaneously, each buffer sends packets {{at the head of}} each virtual <b>output</b> <b>queue</b> to each output line card, again at rate R/N to each card. The output line card can clearly forward these packets out the line with no contention.|$|E
5000|$|Like the {{evaluation}} of RPN, the shunting yard algorithm is stack-based. Infix expressions are the form of mathematical notation most people are used to, for instance [...] "3 + 4" [...] or [...] "3 + 4 × (2 − 1)". For the conversion there are two text variables (strings), the input and the output. There is also a stack that holds operators not yet added to the <b>output</b> <b>queue.</b> To convert, the program reads each symbol in order and does something based on that symbol. The result for the above examples would be [...] "3 4 +" [...] or [...] "3 4 2 1 − × +".|$|E
40|$|This paper proposes an {{innovative}} concept, called virtual <b>output</b> <b>queue,</b> to support available bit rate (ABR) traffic on an input-buffered, per virtual circuit (VC) queued switch. This technique allows ABR models developed for output-buffered systems to be migrated to an input-buffered system. In order t...|$|E
40|$|Abstract — Virtual <b>output</b> <b>queueing</b> {{is known}} to {{overcome}} the head of line blocking problem of input queueing. This type of buffering is widely used in ATM networks. In order to deliver desired performance, virtual <b>output</b> <b>queueing</b> requires efficient and effective scheduling algorithm with low operating complexity. For large scale switches this might be difficult to achieve, as algorithm complexity increases together {{with the size of}} the switch fabric. It is possible to resolve this problem by using interconnection network architectures with distributed buffers. In this case, each network node is a 2 × 2 switching element that employs virtual <b>output</b> <b>queueing</b> and dedicated selection policy that operates locally. Thus, large scale switches can be achieved without the expense of complex scheduling algorithms. In this paper, performance characteristics of the longest queue first, oldest cell first and random selection policies in the Banyan lik...|$|R
40|$|Abstract — Virtual <b>output</b> <b>queueing</b> is {{commonly}} deployed as a buffering technique in high-performance input-queued switch architectures. This paper presents analysis for discrete-time virtual <b>output</b> <b>queued</b> switches with incoming traffic {{governed by a}} uniformly distributed Markov modulated ON/OFF process, and geometrically distributed service times. We utilize the k-step first-passage time probability matrix to derive the probability generating function of the inter-arrival times distribution. Based on the latter, closed-form expressions for the queue size distribution and mean delay are obtained. The validity of the analysis is established through computer simulations...|$|R
40|$|In {{this paper}} we propose an <b>output</b> <b>queued</b> switch {{architecture}} based on the mesh of trees topology. We establish the equivalence of our proposal with the <b>output</b> <b>queued</b> model, {{with respect to a}} large variety of queuing techniques, and we analyze its features, showing that in principle it could be a practical solution to guarantee quality of service. Moreover, such an architecture is able to easily and efficiently manage multicast traffic, which is becoming extremely important in order to offer multimedia (specially voice and video) services over the network...|$|R
40|$|Congestion control {{plays an}} {{important}} role in network resource management in very large networks with heavy traffic. Active Queue Management (AQM) algorithms are one of the approaches to resolve the congestion problems. Majority of AQM algorithms mainly focus on single queued links. The input queued switches are limited in throughput and output queued switches require a large speedup factor so our attention is directed towards Combined Input and Output Queued (CIOQ) switches. A simple modification to the RED AQM algorithm is proposed in this paper in order to account for the presence of both input and output queues in the switch. The weighted sum of input and <b>output</b> <b>queue</b> lengths are specifically used as the congestion measure instead of just the <b>output</b> <b>queue</b> length. The average backlog in the switch is significantly reduced in the low speedup region by using our modified algorithm as compared to RED without this modification. Simulations show that the loss rate in the modified RED slightly larger than that in traditional RED but the <b>output</b> <b>queue</b> length in modified RED is tremendously reduced. The congestion measure which is computed using weighting factor results in reduction of the average backlog. Using our modified algorithm simulations indicate improvement i...|$|E
30|$|User Data Path (UDP): Includes blocks Input Arbiter, VLAN Remover, Watchdog, Output Port Lookup, VLAN Adder and <b>Output</b> <b>Queue.</b> This block {{function}} is to process and forward packets. It collects the packets from 8 inputs (4 CPU queue, and 4 DMA queue) (Moore et al. 2010) and transfers packets to 8 outputs respectively.|$|E
40|$|In this paper, {{we propose}} a novel {{design of a}} {{multicast}} ATM switch using input and output link sharing. Switch inputs and outputs are grouped into small modules called Input Shared Blocks #ISBs# and Output Shared blocks #OSBs#, respectively. Link sharing among grouped inputs can solve output contention and avoid link starvation. Output link sharing eliminates the speedup requirement for the central switch fabric and provides optimal bu#er utilization. We propose two scheduling algorithms : Individual Virtual <b>Output</b> <b>Queue</b> Round Robin #IVOQ Round Robin#, and Grouped Virtual <b>Output</b> <b>Queue</b> Round Robin#GVOQ Round Robin#. Both schemes are based on group mapping from an ISB to an OSB, and reduce the scheduling complexity dramatically. This switch architecture can be easily extended to a large scale, high speed ATM switch enjoying Terabits capacity. Toevaluate the switch performance, we experiment a simulation for a 256 x 256 switch under various tra#c conditions. Simulations indicate [...] ...|$|E
40|$|<b>Output</b> <b>queued</b> {{switches}} {{are appealing}} {{because they have}} better latency and throughput than input queued switches. However, they are difficult to build: a direct implementation of an N output-queued switch requires the switching fabric and the packet memories at the outputs to run at N times the line rate. Attempts {{have been made to}} implement <b>output</b> <b>queuing</b> with slow components, e. g., by having memories at both inputs and outputs running at twice the line rate. In these approaches, even though the packet memory speed is reduced, the scheduler time complexity is high [...] - at least #(N). We show that idealized <b>output</b> <b>queuing</b> can be simulated in a shared memory architecture with (3 N 2) packet memories running at the line rate, using a scheduling algorithm access machine (PRAM). The number of processing elements and memory cells used by the PRAM are a small multiple {{of the size of the}} idealized switch...|$|R
40|$|Abstract. In this paper, {{we study}} the {{performance}} of high-speed packet switches, where the switch fabric operates at a slightly higher speed than the links, i. e., a speeded-up switch. Such structures {{are by no means}} new and there are two well studied architectures in the literature for such packet switches: pure input queueing (no speedup) and pure <b>output</b> <b>queueing</b> (speedup of N, the number of links), with <b>output</b> <b>queueing</b> switches offering substantial performance benefits. However, as link speeds keep increasing, the speedup of N needed for pure <b>output</b> <b>queueing</b> becomes a significant technical challenge. This {{is one of the main}} reasons for the renewed interest in moderately speeded-up switch fabrics. The aim of this paper is to highlight the result that only a moderate speed-up factor (less than two) is sufficient to achieve full input link utilization. In particular, we emphasize that this holds, even without relying on a central switch controller making intelligent decisions on which packets t...|$|R
40|$|Architectures {{based on}} a {{non-blocking}} fabric, such as a crosspoint switch, are attractive for use in high-speed LAN switches, IP routers, and ATM switches. These fabrics, coupled with memory bandwidth limitations, dictate that queues be placed at the input of the switch. But {{it is well known}} that input-queueing can lead to low throughput, and does not allow the control of latency through the switch. This is in contrast to output-queueing, which maximizes throughput, and permits the accurate control of packet latency through scheduling. We ask the question: Can a switch with combined input and <b>output</b> <b>queueing</b> be designed to behave identically to an output-queued switch? In this paper, we prove that if the switch uses virtual <b>output</b> <b>queueing,</b> and has an internal speedup of just four, it is possible for it to behave identically to an <b>output</b> <b>queued</b> switch, regardless of the nature of the arriving traffic. Our proof is {{based on a}} novel scheduling algorithm, known as Most Urgent Cel [...] ...|$|R
40|$|This paper {{presents}} a simple prioritytagging filtering mechanism, called SAP (Shrew Attack Protection), which protects well-behaved TCP flows against low-rate TCP-targeted Shrew attacks. In this scheme, a router maintains a simple set of counters and {{keeps track of}} the drop rate for each potential victim. If the monitored drop rates are low, all packets are treated as normal and equally complete {{to be admitted to}} the <b>output</b> <b>queue</b> and only dropped based on the AQM (Active Queue Management) policy when the <b>output</b> <b>queue</b> is full. SAP keeps tagging victim packets as high priority until their drop rate is below the fair drop rate. By preferentially dropping normal packets to protect high-priority packets, SAP can prevent low rate TCP-targeted Shrew attacks from causing a well-behaved TCP flow to lose multiple consecutive packets repeatedly. This simple strategy protects wellbehaved TCP flows away from near zero throughputs (due to slow start) under an attack...|$|E
40|$|This paper studies {{scheduling}} algorithms and {{evaluates the}} performance of high-speed switching systems. A novel architecture for three-dimensional Virtual <b>Output</b> <b>Queue</b> (3 D-VOQ) switches is proposed with a suitable scheduling algorithm to improve the competitive transfer of service. This 3 D-VOQ switch, which exactly emulates an output-queued switch with a broad class of service scheduling algorithms, requires no speedup, independently of its incoming traffic pattern and switch size. First, an N×N 3 D-VOQ switch is proposed. In this contentionfree architecture, the head-of-line problems are eliminated using a few virtual output queues (VOQ) from input ports and the output sides were arranged using sufficient separate queues. Next, a Small Time-to-leave Cell First (STCF) algorithm is proposed to generate a stable many-to-many assignment. Finally, analysis and simulation confirm {{the performance of}} 3 D-VOQ and its satisfying high/low QoS requirements. Key words: QoS guarantees, Virtual <b>Output</b> <b>Queue</b> (VOQ), switching system, packet switching, 3 D-VOQ...|$|E
40|$|This paper proposes an {{innovative}} concept, called virtual <b>output</b> <b>queue,</b> to support available bit rate (ABR) traffic on an input-buffered, per virtual circuit (VC) queued switch. This technique allows ABR models developed for output-buffered systems to be migrated to an inputbuffered system. In order {{to evaluate the}} virtual <b>output</b> <b>queue</b> and to compare different ABR algorithms, a simulator of the ATM testbed at the University of Illinois has been enhanced with ABR functions. This paper provides simulation results for the input-buffered variation of the ERICA+ algorithm. INTRODUCTION ABR is designed for computer data transmission in ATM networks. For an ABR connection, the source receives feedback from the network and adjusts its rate according to this information. The feedback information is carried within special cells, called Resource Management cells (RM cells). RM cells are periodically inserted into the stream of ABR data cells. As {{a consequence of the}} feedback mechanism, cell los [...] ...|$|E
40|$|Virtual <b>output</b> <b>queueing</b> {{is known}} to {{overcome}} the head of line blocking problem of input queueing. This type of buffering is widely used in ATM networks. In order to deliver desired performance, virtual <b>output</b> <b>queueing</b> requires efficient and effective scheduling algorithm with low operating complexity. For large scale switches this might be difficult to achieve, as algorithm complexity increases together {{with the size of}} the switch fabric. It is possible to resolve this problem by using interconnection network architectures with distributed buffers. In this case, each network node is a 2 × 2 switching element that employs virtual <b>output</b> <b>queueing</b> and dedicated selection policy that operates locally. Thus, large scale switches can be achieved without the expense of complex scheduling algorithms. In this paper, performance characteristics of the longest queue first, oldest cell first and random selection policies in the Banyan like interconnection network are studied. Results show that the longest queue first selection policy outperforms others in terms of packet loss performance, whereas random selection policy achieves low throughput-delay ratio performance...|$|R
40|$|Input {{buffered}} switches most efficiently use {{memory and}} crossbar bandwidth. Virtual <b>Output</b> <b>Queueing</b> (VOQ) is re-quired {{to circumvent the}} head-of-line blocking limiting the throughput to 58. 6 %. For the slotted access control to the switch fabric, weighted arbitration algorithms achieve 100 % throughput with lowest delays under all admissible traffic. Following the arbitration decision, distributed QoS-aware cell schedulers decide locally in each input port upon the next cell to forward. In this paper we treat cell scheduling algorithms employed in VOQ switches, in contrast to <b>output</b> <b>queueing</b> (OQ). We show that typical scheduling properties also hold under the VOQ architecture and give representa-tive quantitative performance results. ...|$|R
40|$|ABSTRACT: ATLAS I is a single-chip ATM switch with {{optional}} credit-based (backpressure) flow control. This 4 -million-transistor 0. 35 -micron CMOS chip, {{which is}} currently under development, offers 20 Gbit/s aggregate I/O throughput, sub-microsecond cut-through latency, 256 -cell shared buffer containing multiple logical <b>output</b> <b>queues,</b> priorities, multicasting, and load monitoring. This paper discusses the use of backpressure inside networks based on ATLAS I chips: in switching fabrics of large ATM switches, or in wormhole-style workstation cluster LANs. We explain and we show bysimulation that the ATLAS I backpressure provides a switching fabric with high performance, comparable to an <b>output</b> <b>queued</b> switch, at low cost, comparable to an input buffered switch. 1...|$|R
40|$|Abstract: The paper {{discusses}} {{the different approaches}} in the architecture and design of cell-switches. The main ideas explored are the Input Queue (IQ) and the <b>Output</b> <b>Queue</b> (OQ) cell switches. Both architectures are described and compared under different sets of assumptions in terms of arrival distribution and packet classification. Results show that the efficiency of these architectures is generally comparable and depends on the assumptions made...|$|E
40|$|Abstract — In {{very large}} {{networks}} with heavy traffic, congestion control {{plays an important}} role in network resource management. One approach to this is the Active Queue Management (AQM) algorithms. Many AQM algorithms have been proposed and analyzed but they mainly focus on single queued links. Recognizing the fact that input queued switches are limited in throughput and output queued switches require a large speedup factor, we direct our attention to Combined Input and Output Queued(CIOQ) switches. We propose a simple modification to the RED AQM algorithm in order to account for the presence of both input and output queues in the switch. Specifically we use the weighted sum of input and <b>output</b> <b>queue</b> lengths as the congestion measure instead of just the <b>output</b> <b>queue</b> length. Simulations show that with such a simple modification, the average backlog in the switch is significantly reduced in the low speedup region as compared to RED without this modification. Unlike the traditional dynamic of having the loss rate grow with the length of the queue, simulations show that for a loss rate in the modified RED slightly larger than that in RED, the <b>output</b> <b>queue</b> length in modified RED is tremendously reduced. The weighting factor used in the computation of the congestion measure provides a means to balance the reduction in the average backlog on the one hand, and the increase in the loss rate on the other hand. Finally, simulations show that the improvement gained in terms of the queue length does not compromise in any way the utilization of the switch as compared to RED and Droptail. I...|$|E
40|$|Abstract—This paper {{proposes a}} new {{switching}} scheme for switch fabric, which is named Multicast-enabled Protocol Agnostic Forwarding Engine (M-PAFE). M-PAFE {{is a central}} scheduling crossbar-based switch fabric with embedded Combined Input and <b>Output</b> <b>Queue</b> from the queuing views. An extra switching path is dedicated for multicast cell processing in M-PAFE. And the simulation results are provided to evaluate the performance of M-PAFE in comparison with ESLIP, which is a well-known central scheduling implemented in Cisco 12000 series ’ routers...|$|E
5000|$|... iDocs - The central {{processing}} engine (administrator) that runs with iSeries - System i - Power Systems Accounting/ERP software applications. iDocs monitors <b>Output</b> <b>Queues</b> for spool files, database files or XML data to process and route based on user-defined business rules.|$|R
40|$|Input queuing switch {{architectures}} must {{be controlled}} by a scheduling algorithm, which solves contentions in the trans-fer of data units from inputs to outputs. Several scheduling algorithms were proposed in the literature for switches op-erating on fixed-size data units. In this paper we {{consider the case of}} packet switches, i. e., devices operating on variable-size data units at their interfaces, but internally operating on fixed-size data units, and we propose novel extensions of known scheduling algorithms. We show that, in the case of packet switches, input queuing architectures can provide advantages over <b>output</b> <b>queuing</b> architectures. 1 Input vs. <b>Output</b> <b>Queueing</b> In the recent past, significant research efforts were de-voted by both the academic and the industrial commu...|$|R
40|$|<b>Output</b> <b>Queuing</b> (OQ) and Input Queuing (IQ) are the {{two basic}} queuing {{strategies}} implemented in routers. IQ {{has been identified as}} the simplest and the most scalable. However, IQ achieves only 58. 6 % throughput due to the Head Of Line (HOL) blocking effect. The Virtual <b>Output</b> <b>Queuing</b> (VOQ) strategy is a proffered solution to the HOL blocking. It has been shown that VOQ can achieve a 100 % throughput with an effective scheduling algorithm. This paper proposes a Multi stage Queuing and Scheduling strategy which implements VOQ at the input and OQ at the output of the router. The scheduling algorithm for the VOQ proposed in this paper is an Iterative Probabilistic Scheduling. 1...|$|R
