10|37|Public
60|$|This was {{a slight}} that at another time Captain Blood would not have borne for a moment. But at present, in his <b>odd</b> <b>frame</b> of mind, and its divorcement from piracy, he was content to smile his utter {{contempt}} of the French General. Not so, however, his captains, and still less his men. Resentment smouldered amongst them for a while, to flame out violently {{at the end of}} that week in Cartagena. It was only by undertaking to voice their grievance to the Baron that their captain was able for the moment to pacify them. That done, he went at once in quest of M. de Rivarol.|$|E
50|$|FFmpeg {{linked with}} the VP8/VP9 {{reference}} codec library libvpx can extract VP8 key frames from WebM media and a script can then add the WebP RIFF header and the NUL pad byte for <b>odd</b> <b>frame</b> lengths. Meanwhile, FFmpeg supports libwebp directly.|$|E
50|$|Mick {{recently}} {{delved into}} Chinese 8 Ball entering their 2016 World Championships in Yushun, China. Mick borrowed a cue and had one week of practice {{before reaching the}} Final, only succumbing to China's Hanqing Shi by the <b>odd</b> <b>frame</b> of an epic battle. Another British player, Darren Appleton, also reached the latter stages, losing to Shi in the semi-finals.|$|E
50|$|Shots {{were viewed}} “with {{an eye to}} {{vertical}} frames and horizontal <b>frames,</b> <b>odd</b> <b>frames,</b> small frames and large frames.” It was important to also shoot material that would be of limited interest, {{so as not to}} confuse the viewer with too many visual stimuli. Editing required close attention to where the various images would direct the viewer's gaze.|$|R
30|$|In this section, we {{describe}} various experiments conducted for performance {{comparisons between the}} proposed MCFI algorithm and existing algorithms (Han and Woods 1997; Kim and Sunwoo 2014; Choi et al. 2007; Kang et al. 2010). The search range in each experiment was set to ±  16 pixels for both horizontal and vertical directions, but only the proposed MCFI used the search range of ±  4 because of MPS. All test sequences used for the experiments are in the standard CIF (352  ×  288), 720 p (1280  ×  720) and 1080 p (1920  ×  1080) formats. To quantitatively measure {{the quality of the}} interpolated <b>frames,</b> 50 <b>odd</b> <b>frames</b> were removed from 30 fps test sequences and new 50 <b>odd</b> <b>frames</b> were reconstructed using 51 even frames. Then, PSNR performances of the reconstructed frames were compared with the original <b>odd</b> <b>frames.</b> Since most of the experimental literature on MCFI (Kim and Sunwoo 2014; Wang et al. 2010; Ha et al. 2004; Choi et al. 2007) uses 50 frames to evaluate PSNR performance, we follow suit.|$|R
30|$|The {{evaluation}} {{assumed the}} same rate and quality for the <b>odd</b> <b>frames</b> of all the schemes, and therefore the rate-distortion performances of <b>odd</b> <b>frames</b> {{were not included in}} the plots. It is evident from Fig.  11 that when highly reliable side information (MC-I) is used, the TDWZ codec is 7 – 8  dB better than the PDWZ codec. On the other hand, using less reliable side information (MC-E), the TDWZ codec yields a PSNR gain of 1 – 3  dB against DCT-based intraframe coding. It is also observed that compression efficiency loss is approximately 5  dB higher since the motion and occlusions in the foreman sequence make it more difficult to extrapolate the succeeding frames.|$|R
50|$|Higgins was not {{extended}} {{until the}} semi-final when Rex Williams led by six frames before the Irishman {{won by the}} <b>odd</b> <b>frame</b> with a 61 break. Williams had beaten Ray Reardon 25-23, in a match contested in five different club venues in Scotland. John Spencer beat Eddie Charlton in the second semi-final played in Bolton from 10 to 15 January.|$|E
50|$|Joe Davis {{was given}} a harsher {{handicap}} than previously. He won his first two matches but then lost heavily to Sidney Smith (giving 30 point per frame) and Alec Brown (giving 35). Giving 20 points per frame, he lost the final match of the tournament against Horace Lindrum. Davis led 35-33 but lost the last three to lose the match by the <b>odd</b> <b>frame.</b>|$|E
5000|$|The {{introduction}} of digital television formats has changed things somewhat. Most digital TV formats, including the popular DVD format, record NTSC originated video with the even field {{first in the}} recorded frame (the development of DVD took place in regions that traditionally utilize NTSC). However, this frame sequence has migrated through to the so-called PAL format (actually a technically incorrect description) of digital video {{with the result that}} the even field is often recorded first in the frame (the European 625 line system is specified as <b>odd</b> <b>frame</b> first). This is no longer a matter of convention because a frame of digital video is a distinct entity on the recorded medium. This means that when reproducing many non NTSC based digital formats (including DVD) it is necessary to reverse the field order otherwise an unacceptable shuddering [...] "comb" [...] effect occurs on moving objects as they are shown ahead in one field and then jump back in the next.|$|E
30|$|The {{selected}} video sequences {{were originally}} in progressive format. To generate interlaced content, the even {{lines of the}} even <b>frames</b> and the <b>odd</b> lines of the <b>odd</b> <b>frames</b> were removed, as shown in Fig. 6. This way, objective quality measurements could be done, using the original sequences—progressive frames—as references.|$|R
60|$|May was bidding June good-morrow, and {{the roses}} were just dreaming {{that it was}} almost time to wake, when John came again into the quiet room which now seemed the Eden that {{contained}} his Eve. Of course there was a jubilee; but something seemed to have befallen the whole group, for never had they appeared in such <b>odd</b> <b>frames</b> of mind. John was restless, and wore an excited look, most unlike his usual serenity of aspect.|$|R
5000|$|Alternate Frame Rendering (AFR): One {{graphics}} {{processing unit}} (GPU) computes all the <b>odd</b> video <b>frames,</b> the other renders the even frames. (i.e. time division) ...|$|R
5000|$|Modern video formats utilize {{a variety}} of frame rates. Due to the mains {{frequency}} of electric grids, analog television broadcast was developed with frame rates of 50 Hz or 60 Hz, sometimes with video being interlaced so more motion information could be sent on the same available broadcast bandwidth, and sometimes with video being broadcast at 25 or 30 fps with each frame doubled. Film, which was almost universally shot at 24 frames per second, could not be displayed at its native frame rate, which required pulldown conversion, often leading to [...] "judder": to convert 24 frames per second into 60 frames per second, every <b>odd</b> <b>frame</b> is doubled and every even frame is tripled, which creates uneven motion. Other conversions have similar uneven frame doubling. Newer video standards support 120, 240, or 300 frames per second, allowing frames to be evenly multiplied for common frame rates such as 24 fps film and 30 fps video, as well as 25 and 50 fps video {{in the case of}} 300 fps displays. These standards also support video that's natively in higher frame rates, and video with interpolated frames between its native frames. Some modern films are experimenting with frame rates higher than 24 fps, such as 48 and 60 fps.|$|E
40|$|In this paper, {{we propose}} an MDC schemes for {{stereoscopic}} 3 D video. In the literature, MDC {{has previously been}} applied in 2 D video {{but not so much}} in 3 D video. The proposed algorithm enhances the error resilience of the 3 D video using the combination of even and <b>odd</b> <b>frame</b> based MDC while retaining good temporal prediction efficiency for video over error-prone networks. Improvements are made to the original even and <b>odd</b> <b>frame</b> MDC scheme by adding a controllable amount of side information to improve frame interpolation at the decoder. The side information is also sent according to the video sequence motion for further improvement. The performance of the proposed algorithms is evaluated in error free and error prone environments especially for wireless channels. Simulation results show improved performance using the proposed MDC at high error rates compared to the single description coding (SDC) and the original even and <b>odd</b> <b>frame</b> MDC...|$|E
40|$|Abstract. In this paper, {{we propose}} an MDC schemes for {{stereoscopic}} 3 D video. In the literature, MDC {{has previously been}} applied in 2 D video {{but not so much}} in 3 D video. The proposed algorithm enhances the error resilience of the 3 D video using the combination of even and <b>odd</b> <b>frame</b> based MDC while retaining good temporal prediction efficiency for video over error-prone networks. Improvements are made to the original even and <b>odd</b> <b>frame</b> MDC scheme by adding a controllable amount of side information to improve frame interpolation at the decoder. The side information is also sent according to the video sequence motion for further improvement. The performance of the proposed algorithms is evaluated in error free and error prone environments especially for wireless channels. Simulation results show improved performance using the proposed MDC at high error rates compared to the single description coding (SDC) and the original even and <b>odd</b> <b>frame</b> MDC. 1 Index Terms — 3 D video, multiple description video coding, side information, error-resilience. I...|$|E
2500|$|Such HDR imaging is used {{in extreme}} dynamic range {{applications}} like welding or automotive work. Some other cameras designed for use in security applications can automatically provide two or more images for each frame, with changing exposure [...] For example, a sensor for 30fps video will give out 60fps with the <b>odd</b> <b>frames</b> at a short exposure time and the even frames at a longer exposure time. Some of the sensor may even combine the two images on-chip so that a wider dynamic range without in-pixel compression is directly available to the user for display or processing.|$|R
40|$|Framing {{effects have}} a {{significant}} influence on the finitely repeated matching pennies game. The combination of being labelled "a guesser," and having the objective of matching the opponent's action, appears to be advantageous. We find that being a player who aims to match the opponent's action is advantageous irrespective of whether the player moves first or second. We examine alternative explanations for our results and relate them to Edgar Allan Poe's "The Purloined Letter. " We propose a behavioral model which generates the observed asymmetry in the players' performance. Matching pennies Even and <b>odd</b> <b>Framing</b> effects Edgar Allan Poe...|$|R
40|$|We compare {{two types}} of sampled motion stimuli: {{ordinary}} periodic displays with modulation amplitude mo=e that translate 90 ° between successive frames and amplifier sandwich displays. In sandwich displays, even-numbered frames are of one type, odd-numbered frames are of the same or different type, and (1) both types have the same period, (2) translate in a consistent direction 90 ° between frames, and (3) even frames have modulation amplitude me, <b>odd</b> <b>frames</b> have modulation amplitude mo. In both first-order motion (van Santen, J. P. H. & Sperling, G. (1984). Temporal covariance model of human motion perception...|$|R
40|$|Scalable {{multiple}} description video coding provides adaptability to bandwidth variations {{and receiving}} device characteristics {{and at the}} same time improves error robustness in multimedia networks. One promising application includes error resilient scalable video conferencing in a virtual collaboration system. In this paper, a scalable multiple description video coding (MDC) is proposed for stereoscopic 3 D video. Scalable MDC has been applied to 2 D video for error resilience but not much on 3 D video. The proposed algorithm enhances the error resilience of the base layer of H. 264 /SVC using even and <b>odd</b> <b>frame</b> based MDC. The performance of the algorithm is examined in error free environment and in mobile WiMax (IEEE 802. 16 e) error prone environments. Simulation results show improved objective and 2 D/ 3 D subjective performance using the proposed scalable MDC in an IEEE 802. 16 e network at high error rates compared to single description coding (SDC) ...|$|E
50|$|Such HDR imaging is used {{in extreme}} dynamic range {{applications}} like welding or automotive work. Some other cameras designed for use in security applications can automatically provide two or more images for each frame, with changing exposure. For example, a sensor for 30fps video will give out 60fps with the <b>odd</b> <b>frames</b> at a short exposure time and the even frames at a longer exposure time. Some of the sensor may even combine the two images on-chip so that a wider dynamic range without in-pixel compression is directly available to the user for display or processing.|$|R
3000|$|..., and {{the set of}} <b>odd</b> {{numbered}} <b>frames</b> X_ 2 i + 1 termed as key {{frames are}} available at decoder, where i ∈{ 0, 1, [...]...,N - 1 / 2 }. Therefore, steps for the compression of even numbered frames X [...]...|$|R
40|$|Abstract—By {{transmitting}} {{texture and}} depth videos from two adjacent captured viewpoints, a client can synthesize via depth-image-based rendering (DIBR) any intermediate virtual {{view of the}} scene, determined by the dynamic movement of the client’s head. In so doing, depth perception of the 3 D scene will be created through motion parallax. Due to the stringent playback deadline of interactive free viewpoint video, burst packet losses in the texture and depth video streams caused by transmission over unreliable channels are difficult to overcome and can severely degrade the synthesized view quality at the client. We propose a multiple description coding (MDC) of free viewpoint video in texture-plus-depth format that will be transmitted on two disjoint network paths. Specifically, we encode even frames of the left view and <b>odd</b> <b>frames</b> of the right view separately as one description and transmit it on path one. Similarly, we encode <b>odd</b> <b>frames</b> of the left view and even frames of the right view as the second description and transmit it on path two. Appropriate quantization parameters (QP) are selected for each description, such that its data rate matches optimally the available transmission bandwidth {{on each of the}} two paths. If the receiver receives one description but not the other due to burst loss on one of the paths, it can still partially reconstruct the missing frames in the loss-corrupted description using a computationally efficient DIBR-based recovery scheme that we design. Extensive experimental results show that our MDC streaming system can outperform the traditional single-path single-description transmission scheme by up to 7 dB in Peak Signal-to-Noise Ratio (PSNR) of the synthesized intermediate view at the receiving client. I...|$|R
50|$|Each GPU renders entire frames in sequence. For example, in a Two-Way setup, one GPU {{renders the}} <b>odd</b> <b>frames,</b> {{the other the}} even frames, one after the other. Finished outputs are sent to the master for display. Ideally, this {{would result in the}} {{rendering}} time being cut by the number of GPUs available. In their advertising, Nvidia claims up to 1.9x the performance of one card with the Two-Way setup. While AFR may produce higher overall framerates than SFR, it also exhibits the temporal artifact known as Micro stuttering, which may affect frame rate perception. It is noteworthy that while the frequency at which frames arrive may be doubled, the time to produce the frame is not reduced - which means that AFR is not a viable method of reducing input lag.|$|R
40|$|International audienceMultiple {{description}} coding (MDC) is {{a framework}} {{designed to improve}} the robustness of video content transmission in lossy environments. In this work, we propose an MDC technique using a legacy coder to produce two descriptions, based on separation of even and <b>odd</b> <b>frames.</b> If only one description is received, the missing frames are reconstructed using temporal high-order motion interpolation (HOMI), a technique originally proposed for distributed video coding. If both descriptions are received, the frames are reconstructed as a block-wise linear {{combination of the two}} descriptions, with the coefﬁcient computed at the encoder in a RD-optimised fashion, encoded with a contextadaptive arithmetic coder, and sent as side information. We integrated the proposed technique in a mobile ad-hoc streaming protocol, and tested it using a group mobility model. The results show a non-negligible gain for the expected video quality, with respect to the reference technique...|$|R
5000|$|To correct this, {{drop frame}} SMPTE {{timecode}} was invented. In {{spite of what}} the name implies, no video frames are dropped (skipped) using drop-frame timecode. Rather, some of the timecodes are dropped. In order to make an hour of timecode match {{an hour on the}} clock, drop-frame timecode skips frame numbers 0 and 1 of the first second of every minute, except when the number of minutes is divisible by ten (i.e. when minutes mod 10 equals zero). (Because editors making cuts must be aware of the difference in color subcarrier phase between even and <b>odd</b> <b>frames,</b> it is helpful to skip pairs of frame numbers.) This achieves an [...] "easy-to-track" [...] drop frame rate of 18 frames each ten minutes (18,000 frames @ 30 frame/s) and almost perfectly compensates for the difference in rate, leaving a residual timing error of only 1.0 ppm, roughly 2.6 frames (86.4 milliseconds) per day.|$|R
30|$|We have {{proposed}} a method to obtain 62.5 % of k-space data at <b>odd</b> time <b>frames</b> and estimate the sampling trajectory at even time frames. In ECG-gated cardiac MRI, k-space data for each cardiac phase (odd and even time frames) are obtained by collecting k-space data across several heart beats. Since even and <b>odd</b> time <b>frames</b> from a prospective gating are synthesized ones, they are not obtained in sequential order. Thus, the proposed algorithm may seem infeasible. However, {{it should be noted}} that when we do not fill k-space for even time points, we use the time to acquire more k-space data of the odd time points. Therefore, the number of heart bits (acquisition time) required for sampling of k-spaces of odd time points reduces. The acquisition time of even time frames, which is done afterwards, is reduced in the same way. Thus, the proposed method decreases the total time required for data acquisition. Details are presented in Additional file 1.|$|R
40|$|Abstract—Multiple {{description}} coding (MDC) is {{a framework}} {{designed to improve}} the robustness of video content transmission in lossy environments. In this work, we propose an MDC technique using a legacy coder to produce two descriptions, based on separation of even and <b>odd</b> <b>frames.</b> If only one description is received, the missing frames are reconstructed using temporal high-order motion interpolation (HOMI), a technique originally proposed for distributed video coding. If both descriptions are received, the frames are reconstructed as a block-wise linear com-bination of the two descriptions, with the coefficient computed at the encoder in a RD-optimised fashion, encoded with a context-adaptive arithmetic coder, and sent as side information. We integrated the proposed technique in a mobile ad-hoc streaming protocol, and tested it using a group mobility model. The results show a non-negligible gain for the expected video quality, {{with respect to the}} reference technique. Index Terms—Video coding, multiple description, legacy coder, image interpolation, robust coding, mobile ad-hoc networks. I...|$|R
40|$|In {{the near}} future, 3 D video {{is likely to}} be used to enhance video applications, as it offers a greater sense of immersion. When 3 D video is {{compressed}} and transmitted over error prone channels, the associated packet loss leads to poor visual quality. Hence, error resilience techniques for 3 D video are needed. This thesis aims to improve the error robustness of the compressed 3 D video in error prone transmission scenarios. Firstly, this thesis describes how 3 D video can be represented using 2 D video information, and depth information. This format can be compressed using tools available in some video coding standards, including Multiple Auxiliary Component (MAC) tool in MPEG- 4 version 2, and the use of reduced resolution coding for depth compression. It is observed that the reduced resolution depth compression provides improved 2 D video performance. However, the quality of the depth information is limited at high bit rates due to the distortion introduced by down-sampling and up-sampling (DSUS). Secondly, Multiple Description Coding (MDC), based on even and <b>odd</b> <b>frames</b> is proposed for error resilient 3 D video. Improvements are made to the original scheme by adding a controllable amount of side information to improve frame interpolation at the decoder and compression efficiency. The side information is also sent according to the video sequence motion for further improvement. The performances of the proposed MDC algorithms are found to be better than single description coding (SDC) and the original scheme at high error rates with reduced error free coding efficiency. Finally, the combination of Scalable Video Coding (SVC) and MDC (scalable MDC) for 3 D video is investigated for error robustness and scalability. A scalable MDC scheme based on even and <b>odd</b> <b>frames</b> is proposed for H. 264 based SVC. Reduced resolution depth compression is then applied to improve the performance. The proposed algorithms provide better 3 D video performance than the original SVC in error prone environments and for low bit-rate video. Key words: stereoscopic 3 D video coding, 2 D and depth, error resilience, multiple description video coding, scalable multiple description video coding...|$|R
40|$|Streaming video over a {{wireless}} network faces several challenges {{such as high}} packet error rates, bandwidth variations, and delays, which could have negative effects on the video streaming and the viewer will perceive a frozen picture for certain durations due to loss of frames. In this study, we propose a Time Interleaving Robust Streaming (TIRS) technique to significantly reduce the frozen video problem and provide a satisfactory quality for the mobile viewer. This is done by reordering the streaming video frames as groups of even and <b>odd</b> <b>frames.</b> The objective of streaming the video {{in this way is}} to avoid the losses of a sequence of neighbouring frames in case of a long sequence interruption. We evaluate our approach by using a user panel and mean opinion score (MOS) measurements; where the users observe three levels of frame losses. The results show that our technique significantly improves the smoothness of the video on the mobile device in the presence of frame losses, while the transmitted data are only increased by almost 9 % (due to reduced time locality) ...|$|R
5000|$|Alternate Frame Rendering (AFR) is a {{technique}} of graphics rendering in personal computers which combines the work output {{of two or more}} graphics processing units (GPU) for a single monitor, in order to improve image quality, or to accelerate the rendering performance. The technique is that one graphics processing unit computes all the <b>odd</b> video <b>frames,</b> the other renders the even frames. This technique is useful for generating 3D video sequences in real time, improving or filtering textured polygons and performing other computationally intensive tasks, typically associated with computer gaming, CAD and 3D modeling.|$|R
50|$|In {{this game}} {{there is only}} one ball thrown per frame. If the pinfall is an even number, the frame is scored as a strike. If the pinfall is an <b>odd</b> number, the <b>frame</b> is scored as a spare where the first score of the frame is the pinfall number.|$|R
40|$|Single Display Groupware (SDG) {{describes}} the interaction technique in which multiple users interact {{with a single}} shared display. This is a good technique for collaboration, but it introduces several challenges. One important challenge is lack of screen space; no matter how large the display is, space will quickly run out when it is being used simultaneously by several users. Another concern is information overload to the users; when working collaboratively, {{there are certain things}} that should be seen by all users, but allowing every user to see everything will clutter the screen and make it difficult to focus on the task. Single Display Privacyware (SDP) [10] is a technique for interaction in which private information is displayed to individual users through a single public display. This technique has been explored by Shoemaker and Inkpen, and should help address many of the issues with SDG. Shoemaker and Inkpen implemented a demonstration of SDP with modified shutter glasses. Shutter glasses are usually used to present 3 D stereo graphics to users. The glasses have LCD images, which can either be clear or opaque. When used with 3 D stereo, the shutters alternate eyes at the same speed as the refresh rate of the display, allowing one eye to see only the even numbered frames and the other to see only the <b>odd</b> numbered <b>frames.</b> With proper graphics, this can create a very compelling 3 D effect. Shoemaker and Inkpen used two pairs of glasses that were modified so that both eyes of one user see the even frames and both eyes of the second user see the <b>odd</b> <b>frames.</b> Using these glasses, Shoemaker and Inkpen created a software environment for building structures with virtual LEGO-like blocks. Both users could see the main work area, but each could only see her own cursor and context menus. Additionally, each user was able to look at a different set of on screen instructions for building specific figures. A user study showed that the SDP version of this program yielded strong improvement over an SDG version where both users saw the same image. Our Application...|$|R
50|$|A match usually {{consists}} of a fixed, <b>odd</b> number of <b>frames.</b> A frame begins with setting up the balls as described above. A frame ends when all balls are potted, or {{when one of the}} players concedes defeat because he is too far behind in score to equal or beat the score of the other player.|$|R
30|$|Second {{group is}} made of a linear-response sensors, such as a Dual-pixel sensor and a Linear sensor using {{exposure}} bracketing. Dual-pixel sensor {{is made of}} two interlaced arrays of pixels with different responsiveness (high and low). It produces two images acquired at the same time, which are then combined in a higher-dynamic-range image. In some cases a single sensitive element has two (or more) storage nodes to store the multiple images. Linear pixel and exposure bracketing is a standard approach in which two (or more) images with different exposure (integration time) of the sensor are taken after each other and afterwards merged. In video applications, there are two general possibilities for this action. If we can sacrifice the frame rate and halve it, then we can consecutively take long-exposure image in <b>odd</b> <b>frames</b> and short-exposure image during the even frames (or the other way around). Otherwise, to keep the frame rate, {{we have to take}} two images after each other during the same frame. To prevent disturbances, long-exposure image has to be obtained during the active video period, and short-exposure image should be recorded during the vertical blanking period. (Some new CMOS sensor architectures allow taking the short-exposure image during the active video period, which can reduce image blur.) This immediately poses a restriction on the duration of the short exposure image, which has to be obtained {{before the end of the}} frame.|$|R
40|$|The {{perceived}} {{direction of}} motion (e. g., up/down) in an ambiguous third-order motion stimulus can be changed by instructions to attend to a particular color (Blaser et al., 1999) or by prior practice in a color-search task (Tseng et al., 2000). In these experiments, subjects performed thousands of consecutive trials attending to only one color. Tseng found that sensitization to that color survived for a month. Is there attentional sensitization when observers shift attention between colors every N trials, N = [1, 200]. Procedure. In our third-order ambiguous-motion paradigm, even frames contain red/green isoluminant gratings, <b>odd</b> <b>frames</b> contain high/low contrast texture gratings. Apparent motion is determined by figure-ground, i. e., the movement of salient areas. Salience {{is determined by the}} difference from the gray background - areas of high contrast or of high color saturation have greater salience. Attention to a color produces a change in motion-direction perception that is equivalent to an increase in saturation, i. e., an increase in salience. Three different attend cues for each trial were used: letters, color patchs, and spoken color names. After observers attended to red stripes, attention was switched to green every N trials, and vs vs. The results were compared to the no-instruction condition. Results. Most observers failed to perceive motion above 8 Hz. Below 4 Hz, all modes of instruction and all values of N produced shifts in observers' psychometric functions equivalent to increasing the saturation of the attended color by 10 - 20 %. This effect of "fast," voluntary attention, while highly significant, is half of what we measured with prolonged attention to the same color. link_to_subscribed_fulltex...|$|R
3000|$|The encoder works {{independently}} on each video frame, performing thus {{a so-called}} intra coding of the frames. The even indexed frames, that {{are referred to}} as key-frames (KF), are traditionally encoded using, for example, an H. 264 /AVC encoder operating in intra mode (i.e., without using any inter-frame prediction). The <b>odd</b> indexed <b>frames</b> instead, called Wyner-Ziv (WZ) frames, are encoded using the principles of distributed source coding. More specifically, these frames are first transformed, with a block based DCT, and then quantized thanks to proper quantization matrices. Homologous coefficients are then encoded, bit plane by bit plane, using a suitable turbo code. In particular, each bit plane of each frequency band is fed into a turbo encoder with rate [...]...|$|R
