1085|3767|Public
25|$|In 2015, Zhou et al. {{suggested}} to apply <b>naive</b> <b>Bayes</b> <b>classifier</b> to detect pathological brains.|$|E
5000|$|Classification: Perceptron, SGD classifier, <b>Naive</b> <b>bayes</b> <b>classifier.</b>|$|E
5000|$|... on document-level: Searching, clustering, and <b>Naive</b> <b>Bayes</b> <b>classifier</b> ...|$|E
40|$|<b>Naïve</b> <b>Bayes</b> <b>classifiers</b> are a {{very simple}} tool for {{classification}} problems, although {{they are based on}} independence assumptions that do not hold in most cases. Extended <b>naïve</b> <b>Bayes</b> <b>classifiers</b> also rely on independence assumption, but break them down to artificial subclasses, in this way becoming more powerful than ordinary <b>naïve</b> <b>Bayes</b> <b>classifiers.</b> Since the involved computations for <b>Bayes</b> <b>classifier</b> are basically generalised mean value calculations, they easily render themselves to incremental and online learning. However, for the extended <b>naïve</b> <b>Bayes</b> <b>classifiers</b> it is necessary, to choose and construct the subclasses, a problem whose answer is not obvious, especially in for online learning. In this paper we propose an evolving extended <b>naïve</b> <b>Bayes</b> <b>classifiers</b> that can learn and evolve in an online manner. 1...|$|R
50|$|Additive {{smoothing}} {{is commonly}} {{a component of}} <b>naive</b> <b>Bayes</b> <b>classifiers.</b>|$|R
40|$|We {{address the}} problem of {{efficiently}} learning <b>Naive</b> <b>Bayes</b> <b>classifiers</b> under classconditional classification noise (CCCN). <b>Naive</b> <b>Bayes</b> <b>classifiers</b> rely on the hypothesis that the distributions associated to each class are product distributions. When data is subject to CCC-noise, these conditional distributions are themselves mixtures of product distributions. We give analytical formulas which makes it possible to identify them from data subject to CCCN. Then, we design a learning algorithm based on these formulas able to learn <b>Naive</b> <b>Bayes</b> <b>classifiers</b> under CCCN. We present results on artificial datasets and datasets extracted from the UCI repository database. These results show that CCCN can be efficiently and successfully handled. 1...|$|R
5000|$|<b>Naive</b> <b>Bayes</b> <b>classifier</b> with multinomial or multivariate Bernoulli event models.|$|E
5000|$|The multinomial <b>naive</b> <b>Bayes</b> <b>{{classifier}}</b> {{becomes a}} linear classifier when expressed in log-space: ...|$|E
50|$|In 2015, Zhou et al. {{suggested}} to apply <b>naive</b> <b>Bayes</b> <b>classifier</b> to detect pathological brains.|$|E
5000|$|VFDTc (2006) extends VFDT for {{continuous}} data, concept drift, {{and application of}} <b>Naive</b> <b>Bayes</b> <b>classifiers</b> in the leaves.|$|R
50|$|Despite their naive {{design and}} {{apparently}} oversimplified assumptions, <b>naive</b> <b>Bayes</b> <b>classifiers</b> have worked quite well in many complex real-world situations. In 2004, {{an analysis of}} the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of <b>naive</b> <b>Bayes</b> <b>classifiers.</b> Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.|$|R
40|$|When {{classifying}} {{objects with}} <b>Naïve</b> <b>Bayes</b> <b>classifiers,</b> {{we are faced}} with {{the problem of how to}} handle continuous attributes. Common solutions to this problem are discretizing, or assuming the data to be normally distributed. In this paper we take a different approach and instead model the class-specific attribute distributions of <b>Naïve</b> <b>Bayes</b> <b>classifiers</b> with MDL-optimal histogram density functions. We present experimental results, comparing MDL-optimal histograms to Gaussian distributions and histograms learned with other methods. 1...|$|R
50|$|Since the <b>Naïve</b> <b>Bayes</b> <b>classifier</b> {{is simple}} yet effective, {{it is usually}} used as a {{baseline}} method for comparison.|$|E
50|$|Contextual {{classification}} of image data {{is based on}} the Bayes minimum error classifier (also known as a <b>naive</b> <b>Bayes</b> <b>classifier).</b>|$|E
5000|$|Given a {{collection}} [...] of labeled samples [...] and unlabeled samples , start by training a <b>naive</b> <b>Bayes</b> <b>classifier</b> on [...]|$|E
40|$|Abstract. Although {{at first}} sight {{probabilistic}} networks and fuzzy clustering seem to be disparate areas of research, a closer look reveals that they can both be seen as generalizations of <b>naive</b> <b>Bayes</b> <b>classifiers.</b> If all attributes are numeric (except the class attribute, of course), <b>naive</b> <b>Bayes</b> <b>classifiers</b> often assume an axis-parallel multidimensional normal distribution for each class as the underlying model. Probabilistic networks remove the requirement that the distributions must be axis-parallel by taking the covariance of the attributes into account, where this is necessary. Fuzzy clustering is an unsupervised method that tries to find general or axis-parallel distributions to cluster the data. Although {{it does not take}} into account the class information, it can be used to improve the result of <b>naive</b> <b>Bayes</b> <b>classifiers</b> and probabilistic networks by removing the restriction that there can be only one distribution per class. ...|$|R
50|$|Instead of {{decision}} trees, linear {{models have been}} proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and <b>naive</b> <b>Bayes</b> <b>classifiers.</b>|$|R
50|$|In machine learning, <b>naive</b> <b>Bayes</b> <b>classifiers</b> are {{a family}} of simple {{probabilistic}} classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.|$|R
50|$|Later {{supervised}} learning usually works much better when the raw input data is first translated {{into such a}} factorial code. For example, suppose the final goal is to classify images with highly redundant pixels. A <b>naive</b> <b>Bayes</b> <b>classifier</b> will assume the pixels are statistically independent random variables and therefore fail to produce good results. If the data are first encoded in a factorial way, however, then the <b>naive</b> <b>Bayes</b> <b>classifier</b> will achieve its optimal performance (compare Schmidhuber et al. 1996).|$|E
50|$|Classification: Building a {{model to}} assign items into {{different}} labeled groups. DAAL provides multiple algorithms in this area, including <b>Naïve</b> <b>Bayes</b> <b>classifier,</b> Support Vector Machine, and multi-class classifiers.|$|E
5000|$|Maximum {{conditional}} independence: if {{the hypothesis}} can be {{cast in a}} Bayesian framework, try to maximize conditional independence. This is the bias used in the <b>Naive</b> <b>Bayes</b> <b>classifier.</b>|$|E
50|$|<b>Naive</b> <b>Bayes</b> <b>classifiers</b> are {{a popular}} {{statistical}} technique of e-mail filtering. They typically use {{bag of words}} features to identify spam e-mail, an approach commonly used in text classification.|$|R
40|$|The Tree Augmented <b>Naïve</b> <b>Bayes</b> (TAN) <b>classifier</b> relaxes the {{sweeping}} independence {{assumptions of the}} <b>Naïve</b> <b>Bayes</b> approach by taking account of conditional probabilities. It does this in a limited sense, by incorporating the conditional probability of each attribute given the class and (at most) one other attribute. The method of boosting has previously proven very effective in improving the performance of <b>Naïve</b> <b>Bayes</b> <b>classifiers</b> and in this paper, we investigate its effectiveness on application to the TAN classifier...|$|R
40|$|Abstract. Recent work in {{supervised}} learning {{has shown that}} a surprisingly simple Bayesian <b>classifier</b> called <b>naïve</b> <b>Bayes</b> is competitive with {{state of the art}} classifiers. This simple approach stands from assumptions of conditional independence among features given the class. Improvements in accuracy of <b>naïve</b> <b>Bayes</b> has been demonstrated by a number of approaches, collectively named semi <b>naïve</b> <b>Bayes</b> <b>classifiers.</b> Semi <b>naïve</b> <b>Bayes</b> <b>classifiers</b> are usually based on the search of specific values or structures. The learning process of these classifiers is usually based on greedy search algorithms. In this paper we propose to learn these semi <b>naïve</b> <b>Bayes</b> structures through estimation of distribution algorithms, which are non-deterministic, stochastic heuristic search strategies. Experimental tests have been done with 21 data sets from the UCI repository...|$|R
50|$|Despite {{the fact}} that the {{far-reaching}} independence assumptions are often inaccurate, the <b>naive</b> <b>Bayes</b> <b>classifier</b> has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the <b>naive</b> <b>Bayes</b> <b>classifier</b> will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the <b>naive</b> <b>Bayes</b> <b>classifier</b> are discussed in the literature cited below.|$|E
5000|$|Given a way {{to train}} a <b>naive</b> <b>Bayes</b> <b>classifier</b> from labeled data, it's {{possible}} to construct a semi-supervised training algorithm that can learn {{from a combination of}} labeled and unlabeled data by running the supervised learning algorithm in a loop: ...|$|E
50|$|In natural {{language}} processing, multinomial LR classifiers {{are commonly used}} {{as an alternative to}} naive Bayes classifiers because they do not assume statistical independence of the random variables (commonly known as features) that serve as predictors. However, learning in such a model is slower than for a <b>naive</b> <b>Bayes</b> <b>classifier,</b> and thus may not be appropriate given {{a very large number of}} classes to learn. In particular, learning in a <b>Naive</b> <b>Bayes</b> <b>classifier</b> is a simple matter of counting up the number of co-occurrences of features and classes, while in a maximum entropy classifier the weights, which are typically maximized using maximum a posteriori (MAP) estimation, must be learned using an iterative procedure; see #Estimating the coefficients.|$|E
40|$|Abstract — Although {{probabilistic}} {{networks and}} fuzzy clustering {{may seem to}} be disparate areas of research, they can both be seen as generalizations of <b>naive</b> <b>Bayes</b> <b>classifiers.</b> If all descriptive attributes are numeric, <b>naive</b> <b>Bayes</b> <b>classifiers</b> often assume an axis-parallel multidimensional normal distribution for each class. Probabilistic networks remove the requirement that the distributions must be axis-parallel by taking covariances into account where this is necessary. Fuzzy clustering tries to find general or axis-parallel distributions to cluster the data. Although it neglects the class information, {{it can be used to}} improve the result of the abovementioned methods by removing the restriction to only one distribution per class. I...|$|R
50|$|<b>Naive</b> <b>Bayes</b> <b>classifiers</b> work by {{correlating}} {{the use of}} tokens (typically words, {{or sometimes}} other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email {{is or is not}} spam.|$|R
50|$|For {{some types}} of {{probability}} models, <b>naive</b> <b>Bayes</b> <b>classifiers</b> can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for <b>naive</b> <b>Bayes</b> models uses the method of maximum likelihood; in other words, one can work with the <b>naive</b> <b>Bayes</b> model without accepting Bayesian probability or using any Bayesian methods.|$|R
5000|$|The {{simplest}} one is <b>Naïve</b> <b>Bayes</b> <b>classifier.</b> Using {{the language}} of graphical models, the <b>Naïve</b> <b>Bayes</b> <b>classifier</b> is described by the equation below. The basic idea (or assumption) of this model is that each category has its own distribution over the codebooks, and that the distributions of each category are observably different. Take a face category and a car category for an example. The face category may emphasize the codewords which represent [...] "nose", [...] "eye" [...] and [...] "mouth", while the car category may emphasize the codewords which represent [...] "wheel" [...] and [...] "window". Given a collection of training examples, the classifier learns different distributions for different categories. The categorization decision is made by ...|$|E
50|$|Averaged one-dependence estimators (AODE) is a {{probabilistic}} classification learning technique. It {{was developed}} to address the attribute-independence problem of the popular <b>naive</b> <b>Bayes</b> <b>classifier.</b> It frequently develops substantially more accurate classifiers than naive Bayes {{at the cost of}} a modest increase in the amount of computation.|$|E
50|$|The {{assumptions}} on {{distributions of}} features {{are called the}} event model of the <b>Naive</b> <b>Bayes</b> <b>classifier.</b> For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused.|$|E
40|$|<b>Naïve</b> <b>Bayes</b> <b>classifiers,</b> {{a popular}} tool for {{predicting}} the labels of query instances, are typically {{learned from a}} training set. However, since many training sets contain noisy data, a classifier user {{may be reluctant to}} blindly trust a predicted label. We present a novel graphical explanation facility for <b>Naïve</b> <b>Bayes</b> <b>classifiers</b> that serves three purposes. First, it transparently explains the reasoning used by the classifier to foster user confidence in the prediction. Second, it enhances the user's understanding of the complex relationships between the features and the labels. Third, it can help the user to identify suspicious training data. We demonstrate these ideas in the context of our implemented web-based system, which uses examples from molecular biology. 1...|$|R
40|$|Though <b>naive</b> <b>Bayes</b> text <b>classifiers</b> {{are widely}} used because of its simplicity, the {{techniques}} for improving performances of these classifiers have been rarely studied. In this paper, we propose and evaluate some general and e ective techniques for improving performance of the <b>naive</b> <b>Bayes</b> text <b>classifier.</b> We suggest document model based parameter estimation and document length normalization to alleviate {{the problems in the}} traditional multinomial approach for text classification. In addition, Mutual-Information-weighted <b>naive</b> <b>Bayes</b> text <b>classifier</b> is proposed to increase the effect of highly informative words. Our techniques are evaluated on the Reuters 21578 and 20 Newsgroups collections, and significant improvements are obtained over the existing multinomial <b>naive</b> <b>Bayes</b> approach...|$|R
50|$|Many {{networking}} and security companies claim {{to detect and}} control Skype's protocol for enterprise and carrier applications. While the specific detection methods used by these companies are often proprietary, Pearson's chi-squared test and stochastic characterization with <b>Naive</b> <b>Bayes</b> <b>classifiers</b> are two approaches that were published in 2007.|$|R
