4|17|Public
5000|$|Sampling {{requires}} some careful consideration. If the organization elects to use 100% inspection on a process, {{the production rate}} determines an appropriate sampling rate which in turn determines the sample size. If the organization elects to only inspect a fraction of units produced, the sample size should be chosen large enough so that the chance of finding at least one <b>nonconforming</b> <b>unit</b> in a sample is high—otherwise the false alarm rate is too high. One technique is to fix sample size {{so that there is}} a 50% chance of detecting a process shift of a given amount (for example, from 1% defective to 5% defective). If δ is the size of the shift to detect, then the sample size should be set to [...] Another technique is to choose the sample size large enough so that the p-chart has a positive lower control limit or [...]|$|E
40|$|We {{consider}} the case of a batch of discrete units produced by a process subject to failures under a known probability distribution function, and apply information theory to the problem of finding the first <b>nonconforming</b> <b>unit</b> in the batch at minimum cost. Two distinct but related aspects of this problem were treated: determining which units should be inspected, and determining how many units should be sent for inspection at the same time. The solution is based on the principles of inspecting the product units that maximize the reduction in the uncertainty regarding the location of the first <b>nonconforming</b> <b>unit,</b> and of minimizing the cost per unit of uncertainty reduced. These principles are formalized by means of a series of theorems leading to an easy-to-implement algorithm for managing parallel inspection. This approach is successfully compared with the optimal solution obtained with dynamic programming and with other heuristics. inspection planning, information theory...|$|E
40|$|An online group testingmethod {{to search}} for a hidden object in a {{discrete}} search space is proposed. A relevant example is a search after a <b>nonconforming</b> <b>unit</b> in a batch, while many other applications can be related. A probability mass function is defined over the search space to represent the probability of an object (e. g., a <b>nonconforming</b> <b>unit)</b> to be located at some point or subspace. The suggested method follows a stochastic local search procedure and {{can be viewed as a}} generalization of the Learning Real-Time A ∗ (LRTA∗) search algorithm, while using informational distance measures over the searched space. It is proved that the proposed Informational LRTA ∗ (ILRTA∗) algorithm converges and always terminates. Moreover, it is shown that under relevant assumptions, the proposed algorithm generalizes known optimal information-theoretic search procedures, such as the offline Huffman search or the generalized optimum testing algorithm. However, the ILRTA ∗ can be applied to new situations, such as a search with side information or an online search where the probability distribution changes. The obtained results can help to bridge the gap between different search procedures that are related to quality control, artificial intelligence, and information theory...|$|E
50|$|In {{statistical}} quality control, the p-chart {{is a type}} of control chart used to monitor the proportion of <b>nonconforming</b> <b>units</b> in a sample, where the sample proportion nonconforming is defined as the ratio of the number of <b>nonconforming</b> <b>units</b> to the sample size, n.|$|R
40|$|This paper {{presents}} a statistical analysis control chart for <b>nonconforming</b> <b>units</b> in quality control. In many situations the Shewhart control charts for <b>nonconforming</b> <b>units</b> {{may not be}} suitable or cannot be used, as for many processes, the assumptions of binomial distribution may deviate or may provide inadequate model. In this Study we propose a new control chart based on regression estimator of proportion based on single auxiliary variable, namely the Pr chart and compared its performance with P and Q chart with probability to signal as a performance measure. It has been observed that the proposed chart is superior to the P and Q chart. This study will help quality practitioners to choose an efficient alternative to the classical P and Q charts for monitoring <b>nonconforming</b> <b>units</b> in industrial process...|$|R
40|$|International audienceIn this paper, {{we develop}} a joint quality control and {{preventive}} maintenance policy for a production system producing conforming and <b>nonconforming</b> <b>units.</b> The considered system consists of one machine which must supply another production line {{operating on a}} just-in-time basis. Each lot produced by the machine is subject to a quality control. According to the proportion l of <b>nonconforming</b> <b>units</b> observed and compared to a threshold value l m, one decides to undertake or not maintenance actions on the system. In order to palliate perturbations caused by the stopping of the machine for preventive and corrective maintenance actions of random durations, a buffer stock h is built up to ensure the continuous supply of the subsequent production line. The proposed strategy is modelled using simulation and experimental design. This approach allows to generate a second order response surface allowing to easily determine the optimal rate, lm*{l_{m}^*}, of <b>nonconforming</b> <b>units</b> {{on the basis of}} which preventive maintenance actions should be performed, and the optimal size, h*, of the buffer stock to be built. These values minimize the total cost per time unit which includes the costs related to maintenance, quality and inventory...|$|R
40|$|Single {{sampling}} plans are investigated for variables indexed by acceptable quality level (AQL) and {{average outgoing quality}} limit (AOQL) under measurement error. Procedures and tables are provided for selection of single {{sampling plans}} for variables for given AQL and AOQL when rejected lots are 100 % inspected for replacement of a <b>nonconforming</b> <b>unit.</b> For a particular sampling plan in operation for an observed measurement, a method for determining true operating characteristic (OC) functions and average outgoing quality (AOQ) is described for various error sizes...|$|E
50|$|In {{statistical}} quality control, the np-chart {{is a type}} of control chart used to monitor the number of <b>nonconforming</b> <b>units</b> in a sample. It is an adaptation of the p-chart and used in situations where personnel find it easier to interpret process performance in terms of concrete numbers of units rather than the somewhat more abstract proportion.|$|R
40|$|Hypergeometric Attribute Sampling System Based on Risk and Fraction Defective (HYPERSAMP) {{computer}} program demonstrates attribute sampling system developed to determine {{minimum sample size}} required for any preselected value for consumer's risk and fraction of <b>nonconforming</b> <b>units.</b> Used in place of MIL-STD- 105 E sampling plans when minimum sample size desirable, such as when tests are destructive or expensive. Written for IBM PC-compatible computers...|$|R
40|$|Abstract. The study models multi-characteristics {{inspection}} {{for inspection}} allocation problems with workstations of attribute data in serial production systems. Either 100 % or 0 % inspection is performed and Type I and Type II errors are considered. In addition, this study considers three possibilities of treatment of detected <b>nonconforming</b> <b>units,</b> namely, repair, rework and scrap. With the above considerations, a profit model is developed for optimally allocating inspections. Moreover, a genetic algorithm {{is used to}} solve the problem and it is proved to have much less computation time, compared with an optimization method based on complete enumeration, especially when number of workstations and characteristics becomes more...|$|R
40|$|This article {{proposes a}} new np control chart, called the npx chart, that employs an {{attribute}} inspection (inspecting whether a unit is conforming or nonconforming) {{to monitor the}} mean value of a variable x. The distinctive feature of the npx chart is using the statistical warning limits to replace the specification limits for the classification of conforming or <b>nonconforming</b> <b>units.</b> By optimizing the warning limits, the npx chart usually outperforms the X¯ chart {{on the basis of}} same inspection cost. In addition, the npx chart often works as a leading indicator of trouble and allows operators to take corrective action before any defective is actually produced...|$|R
40|$|This article studies {{a unique}} {{feature of the}} {{binomial}} CUSUM chart in which the difference (d t −d 0) is replaced by (d t −d 0) 2 {{in the formulation of}} the cumulative sum C t (where d t and d 0 are the actual and in-control numbers of <b>nonconforming</b> <b>units,</b> respectively, in a sample). Performance studies are reported and the results reveal that this new feature is able to increase the detection effectiveness when fraction nonconforming p becomes three to four times as large as the in-control value p 0. The design of the new binomial CUSUM chart is presented along with the calculation of the in-control and out-of-control Average Run Lengths (ARL 0 and ARL 1) ...|$|R
40|$|The {{performance}} of attributes control charts (such as c and np charts) is usually evaluated {{under the assumption}} of known process parameters (i. e., the nominal proportion of <b>nonconforming</b> <b>units</b> or the nominal number of nonconformities). However, in practice, these process parameters are rarely known {{and have to be}} estimated from an in-control phase I data set. In this paper, we derive the run length properties of the phase II synthetic c and np charts with estimated parameters, and we investigate the numbermof phase I samples that would be necessary for these charts in order to obtain similar in-control average run lengths as in the known parameters case. We also propose new specific chart parameters that allow these charts to have approximately the same in-control average run lengths as the ones obtained in the known parameter case. China Scholarship Council No [2011] 6032. SARChI chair at the University of Pretoria, South Africa. [URL]...|$|R
40|$|The {{application}} of protective gel, {{which is a}} subprocess of the electronic assembly of the exhaust gas recirculation sensor, is a highly capable process with the fraction of <b>nonconforming</b> <b>units</b> as low as 200 ppm. Every unit is inspected immediately after gel application. The conventional Shewhart chart is of no use here, and {{the approach based on}} the Bernoulli process is therefore considered. The number of conforming items in a row until the occurrence of first or the r-th nonconforming is determined and CCC-r, CCC-r EWMA, and CCC CUSUM charts are applied. The aim of the control is to detect the process deterioration, and so the one-sided charts are used. So that the charts based on the geometric or negative binomial distribution can be compared, their performance is assessed through the average number of inspected units until a signal (ANOS). Our study confirmed that CCC-r EWMA and CCC CUSUM are able to detect the process shift more quickly than the CCC-r chart. Of the two charts, the first is easier to construct. Web of Science 7 art. no. 1...|$|R
5000|$|Design for Six Sigma {{emerged from}} the Six Sigma and the Define-Measure-Analyze-Improve-Control (DMAIC) quality methodologies, which were {{originally}} developed by Motorola to systematically improve processes by eliminating defects. Unlike its traditional Six Sigma/DMAIC predecessors, which are usually focused on solving existing manufacturing issues (i.e., [...] "fire fighting"), DFSS aims at avoiding manufacturing problems by taking a more proactive approach to problem solving and engaging the company efforts {{at an early stage}} to reduce problems that could occur (i.e., [...] "fire prevention"). The primary goal of DFSS is to achieve a significant {{reduction in the number of}} <b>nonconforming</b> <b>units</b> and production variation. It starts from an understanding of the customer expectations, needs and Critical to Quality issues (CTQs) before a design can be completed. Typically in a DFSS program, only a small portion of the CTQs are reliability-related (CTR), and therefore, reliability does not get center stage attention in DFSS. DFSS rarely looks at the long-term (after manufacturing) issues that might arise in the product (e.g. complex fatigue issues or electrical wear-out, chemical issues, cascade effects of failures, system level interactions).|$|R
40|$|On {{standard}} control charts, {{the hypothesis}} of normality is usually assumed without any additional verification. Nevertheless, in some cases this assumption is not accurate and might cause errors in process quality monitoring. In particular, for {{the control of the}} proportion of <b>nonconforming</b> <b>units</b> (p-chart) the normality is doubtful when p is small and consequently, lower control limit {{less than or equal to}} zero are frequent. Some authors have proposed new techniques to define limits in the p-chart. Others have proposed transformations to improve the detection of special causes. In X-bar charts, the mean of a critical to quality (CTQ) characteristic is monitored. When the variable is far from normality, then parametric, or even nonparametric, control charts might be used. Recent works suggest applying transformations to make the data quasi-normal. This kind of lack of normality is usually present when in those analyses CTQ is a part of a composition. Our proposal is to highlight how above mentioned problems can be treated from a compositional point of view. New strategies are proposed and illustrated trough a case study where a proportion has the role of a CT...|$|R
40|$|In {{this paper}} the author {{analyses}} {{the reduction of}} product nonconformity obtainable {{through the use of}} Shewhart control charts during the operative phase of a productive process. This result can be called "filter effect" of control charts. The author supplies a quantification of this effect by defining, in correspondence with a nonconformity input of the process, an approximate measure of the expected proportion of <b>nonconforming</b> <b>units</b> resulting from the use of control charts. The study is conducted both under the classic hypothesis of a single shift,which is constant over time, in the observed process parameter, and under the hypothesis of a uniformly progressive shift. The result obtained show that, at a given level of nonconformity input, the filter effect is more pronounced {{in the case of a}} progressive shift, and above all, that it is possible to quantify this effect at assigned levels of risk. It can be observed that the filter effect of a control chart increases when the nonconformity level of the process decreases. Therefore the filter effect is particularly remarkable when more time is needed to detect the process parameter and, consequently, there is a longer exposure to the resulting nonconformity...|$|R
40|$|The {{industrial}} revolution was mark {{the beginning of}} the rise of industrial in the world. Moreover, in this globalization era, a lot of industry popping up especially those industries in Indonesia with many of those industries would emerge also thight competition. Each company must be trying to superior to that of its products so that each company will always improve the quality of their products in various ways so that the product can deportment in the market. One way of improving the quality of by doing quality control on each of its products. There are many method of conducting control quality. One method used is multivariate np chart. Multivariate np chart usually used for <b>nonconforming</b> <b>units.</b> Based on the results of this research, it is found that the production process in phase I namely from January to February in a state of controlled so that the parameters in the production process phase I can be used in the production process phase II, while to the process of the production phase II there are several observations that are out-of-control so that the production phase II in a state of uncontrolled. Keywords: multivariate np chart, nonconforming, out-of-control, phase I, phase II...|$|R
40|$|Purpose: The {{purpose of}} this paper is to present the {{application}} of a procedure for the quality control of stainless steel tubes produced for automotive exhaust systems from a leading company in the steel sector, based on the Delphi method in accordance with the ISO/TS 16949 : 2009 and the ISO 9000 : 2008. Using Delphi methodology, it was possible to identify the main problems in the production lines object of the study, the main defects and their causes. Statistical methods were used to monitor process compliance and capacity. The panel of experts involved in Delphi method was able to identify causes of non-compliance and suggest corrective actions. Design/methodology/approach: The quality procedure implemented involves the application of the Delphi method and the ISO/TS 16949 : 2009 standard in conjunction with ISO 9000 : 2008 to the production line of welded tubes for exhaust systems. The statistical methods used to monitor the process were mainly control charts. Capability index, Cp and Cpk, were used to measure the process attitude to produce compliant outputs. Dimensional data were acquired by non-destructive testing on diameters and X-R charts were used to graphically represent the process state of control. Destructive tests were performed to monitor the welding quality and P-chart were used to assess the proportion of <b>nonconforming</b> <b>units.</b> Findings: In this work, a procedure was developed in order to characterize the production process of TXM tubes realized in the line 31 of the leader company plant. The use of Delphi methodology, in order to incorporate experts opinions in the quality control of stainless steel tubes, was one of the main points of this work. The panel of experts worked together to identify process issues, define their causes and propose corrective actions. The paper provides an overview about the quality approach of one of the world's largest companies in the production of steel and shows also how the statistical tools are used in order to manage process behavior. Originality/value: The value of this paper is to illustrate an innovative approach to a real life quality problem; it demonstrates how the application of qualitative and quantitative quality instruments in accordance with technical specification can help in increasing and maintaining product compliance and in optimizing the management of resources...|$|R
40|$|The paper {{develops}} {{a model to}} determine the optimal product reliability and production rate that achieves the biggest total integrated profit for an imperfect manufacturing process. The basic assumption of the classical Economic Manufacturing Quantity (EMQ) model is that all manufacturing items are of perfect quality. The assumption is not true in practice. Most of the production system produces perfect and imperfect quality items. In some cases the imperfect quality (non conforming) items are reworked {{at a cost to}} restore its quality to the original one. Rework cost may be reduced by improvements in product reliability (i. e., decreasing in product reliability parameter). Lower value of product reliability parameter results in increase development cost of production and also smaller quantity of <b>nonconforming</b> products. The <b>unit</b> production cost is a function of product reliability parameter and production rate. As a result, higher development cost increases unit production cost. The problem of optimal planning work and rework processes belongs to the broad field of production-inventory model which deals with all kinds of reuse processes in supply chains. These processes aim to recover defective product items {{in such a way that}} they meet the quality level of 'good item'. The benefits from imperfect quality items are: regaining the material and value added on defective items and improving the environment protection. In this point of view, a model is introduced here to guide a firm/industry in addressing variable product reliability factor, variable unit production cost and dynamic production rate for time-varying demand. The paper provides an optimal control formulation of the problem and develops necessary and sufficient conditions for optimality of the dynamic variables. In this purpose, the Euler-Lagrange method is used to obtain optimal solutions for product reliability parameter and dynamic production rate. Finally, numerical examples are given to illustrate the proposed model. Time-varying demand Imperfect production Inventory Product reliability...|$|R
40|$|Binary {{measurement}} systems (BMS) {{are widely}} used in both manufacturing industry and medicine. In industry, a BMS {{is often used to}} measure various characteristics of parts and then classify them as pass or fail, according to some quality standards. Good measurement systems are essential both for problem solving (i. e., reducing the rate of defectives) and to protect customers from receiving defective products. As a result, it is desirable to assess the performance of the BMS as well as to separate the effects of the measurement system and the production process on the observed classifications. In medicine, BMSs are known as diagnostic or screening tests, and are used to detect a target condition in subjects, thus classifying them as positive or negative. Assessing the performance of a medical test is essential in quantifying the costs due to misclassification of patients, and in the future prevention of these errors. In both industry and medicine, the most commonly used characteristics to quantify the performance a BMS are the two misclassification rates, defined as the chance of passing a <b>nonconforming</b> (non-diseased) <b>unit,</b> called the consumer's risk (false positive), and the chance of failing a conforming (diseased) unit, called the producer's risk (false negative). In most assessment studies, it is also of interest to estimate the conforming (prevalence) rate, i. e. probability that a randomly selected unit is conforming (diseased). There are two main approaches for assessing the performance of a BMS. Both approaches involve measuring a number of units one or more times with the BMS. The first one, called the "gold standard" approach, requires the use of a gold-standard measurement system that can determine the state of units with no classification errors. When a gold standard does not exist, is too expensive or time-consuming, another option is to repeatedly measure units with the BMS, and then use a latent class approach to estimate the parameters of interest. In industry, for both approaches, the standard sampling plan involves randomly selecting parts from the population of manufactured parts. In this thesis, we focus on a specific context commonly found in the manufacturing industry. First, the BMS under study is nondestructive. Second, the BMS is used for 100 % inspection or any kind of systematic inspection of the production yield. In this context, we are likely to have available a large number of previously passed and failed parts. Furthermore, the inspection system typically tracks the number of parts passed and failed; that is, we often have baseline data about the current pass rate, separate from the assessment study. Finally, we assume that {{during the time of the}} evaluation, the process is under statistical control and the BMS is stable. Our main goal is to investigate the effect of using sampling plans that involve random selection of parts from the available populations of previously passed and failed parts, i. e. conditional selection, on the estimation procedure and the main characteristics of the estimators. Also, we demonstrate the value of combining the additional information provided by the baseline data with those collected in the assessment study, in improving the overall estimation procedure. We also examine how the availability of baseline data and using a conditional selection sampling plan affect recommendations on the design of the assessment study. In Chapter 2, we give a summary of the existing estimation methods and sampling plans for a BMS assessment study in both industrial and medical settings, that are relevant in our context. In Chapters 3 and 4, we investigate the assessment of a BMS in the case where we assume that the misclassification rates are common for all conforming/nonconforming parts and that repeated measurements on the same part are independent, conditional on the true state of the part, i. e. conditional independence. We call models using these assumptions fixed-effects models. In Chapter 3, we look at the case where a gold standard is available, whereas in Chapter 4, we investigate the "no gold standard" case. In both cases, we show that using a conditional selection plan, along with the baseline information, substantially improves the accuracy and precision of the estimators, compared to the standard sampling plan. In Chapters 5 and 6, we investigate the case where we allow for possible variation in the misclassification rates within conforming and nonconforming parts, by proposing some new random-effects models. These models relax the fixed-effects model assumptions regarding constant misclassification rates and conditional independence. As in the previous chapters, we focus on investigating the effect of using conditional selection and baseline information on the properties of the estimators, and give study design recommendations based on our findings. In Chapter 7, we discuss other potential applications of the conditional selection plan, where the study data are augmented with the baseline information on the pass rate, especially in the context where there are multiple BMSs under investigation...|$|R

