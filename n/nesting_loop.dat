1|1474|Public
40|$|Power-gating is a {{technique}} investigated widely for reducing leakage energy in the functional units of microprocessors at the architectural level. Effective power-gating involves deactivating idle functional units for sustained periods incurring little or no performance degradation. Accurate prediction of long idle periods is essential, which, in turn, depends on the application program characteristics. In this thesis, we propose a compiler-based leakage reduction technique for embedded architectures by exploiting the well-known attributes of embedded applications, namely, small code size and intensive loops. From the control flow graph (CFG) representation of the source program, we construct a forest of loop hierarchy trees (LHTs), which capture the <b>nesting</b> <b>loop</b> properties of the program. As an LHT satisfies the partial ordering on the loop nesting, we exploit this property to identify maximal subgraphs (of functional unit idleness) in the original program. For each subgraph so found, a sleep instruction is introduced at the entry point of the corresponding code segement, thus optimizing the number of sleep instructions. The sleep instruction has one operand, a bit-vector comprised of ON/OFF controlbits for all functional units in the data path. Our target architecture is a modified ARM processor model comprising of functional units with power-gating ability. We obtained an average leakage energy reduction of 34. 1 % for 12 benchmarks chosen from the MiBench suite, with range of 19. 5 % and standard deviation of 6. 5 %...|$|E
40|$|Loop transformations are {{critical}} for compiling high-performance code for modern computers. Existing work has focused on transformations for perfectly <b>nested</b> <b>loops</b> (that is, loops in which all assignment statements are contained within the innermost loop of a <b>loop</b> <b>nest).</b> In practice, most <b>loop</b> <b>nests,</b> {{such as those in}} matrix factorization codes, are imperfectly nested. In some programs, imperfectly <b>nested</b> <b>loops</b> can be converted into perfectly <b>nested</b> <b>loops</b> by loop distribution, but this is not always legal. In this paper, we present an approach to transforming imperfectly <b>nested</b> <b>loops</b> directly. Our approac...|$|R
40|$|Executing {{a program}} in {{parallel}} machines needs not only to find sufficient parallelism in a program, {{but it is also}} important that we minimize the synchronization and communication overheads in the parallelized program. This yields to improve the performance by reducing the time needed for executing the program. Parallelizing and partitioning of <b>nested</b> <b>loops</b> requires efficient iteration dependence analysis. Although many loop transformations techniques exist for <b>nested</b> <b>loop</b> partitioning, most of these transformation techniques perform poorly when parallelizing <b>nested</b> <b>loops</b> with non-uniform (irregular) dependences. In this paper the affine and unimodular transformations are applied {{to solve the problem of}} parallelism in <b>nested</b> <b>loops</b> with non-uniform dependence vectors. To solve these problem few researchers converted the non-uniform <b>nested</b> <b>loops</b> to uniform <b>nested</b> <b>loops</b> and then find the parallelism. We propose applying directly the two approaches affine and unimodular transformations to extract and improve the parallelism in <b>nested</b> <b>loops</b> with non-uniform dependences. The study shows that unimodular transformation is better than affine transformation when the dependences in <b>nested</b> <b>loops</b> exist only in one statement. While affine transformation is more effective when the <b>nested</b> <b>loops</b> have a sequence of statements and the dependence exists between these different statements...|$|R
50|$|Hash join {{is similar}} to <b>nested</b> <b>loop</b> join but faster than <b>nested</b> <b>loop</b> join and hash join is used for equi join.|$|R
50|$|A <b>nested</b> <b>loop</b> join is a naive {{algorithm}} that joins {{two sets}} by using two <b>nested</b> <b>loops.</b> Join operations {{are important to}} database management.|$|R
40|$|In {{this paper}} we {{address the problem of}} {{partitioning}} <b>nested</b> <b>loops</b> with non-uniform (irregular) dependence vectors. Parallelizing and partitioning of <b>nested</b> <b>loops</b> requires efficient inter-iteration dependence analysis. Although many methods exist for <b>nested</b> <b>loop</b> partitioning, most of these perform poorly when parallelizing <b>nested</b> <b>loops</b> with irregular dependences. Unlike the case of <b>nested</b> <b>loops</b> with uniform dependences these will have a complicated dependence pattern which forms a non-uniform dependence vector set. We apply the results of classical convex theory and principles of linear programming to iteration spaces and show the correspondence between minimum dependence distance computation and iteration space tiling. Cross-iteration dependences are analyzed by forming an Integer Dependence Convex Hull (IDCH). Every integer point in this IDCH corresponds to a dependence vector in the iteration space of the <b>nested</b> <b>loops.</b> A simple way to compute minimum dependence distances from the dep [...] ...|$|R
40|$|This paper {{addresses}} {{the problems of}} communication -free partitions of statement-iterations of <b>nested</b> <b>loops</b> and data accessed by these statement-iterations. Communication-free hyperplane partitions of disjoint subsets of data and statement-iterations are considered. This approach is more possible than existing methods in finding the data and program distribution patterns that can cause the processor fully-parallel execution without any interprocessor communication. In addition, this approach {{can be applied to}} the more general loop models such as the imperfectly <b>nested</b> <b>loop</b> or multiple imperfectly <b>nested</b> <b>loops,</b> not like most of the existing methods that can be applied only to the perfectly <b>nested</b> <b>loop</b> or multiple perfectly <b>nested</b> <b>loops.</b> Due to the limitation of space, this paper only proposes the compilation technique of statement-level communication-free hyperplane partitioning for a perfectly <b>nested</b> <b>loop.</b> Keywords: Communication-free partitioning, data distribution, data locality, hy [...] ...|$|R
40|$|Usually {{the most}} {{computationally}} intensive {{part of a}} program is attributed to the <b>nested</b> <b>loops</b> it contains. It is therefore of interest to try to parallelize <b>nested</b> <b>loops</b> {{in order to reduce}} the overall computation time. A special category of FOR(DO) <b>nested</b> <b>loops</b> are the uniform dependence loops, which are the focus of this paper. The primary goals in thi...|$|R
5000|$|The block <b>nested</b> <b>loop</b> join {{algorithm}} is a generalization {{of the simple}} <b>nested</b> <b>loops</b> algorithm that takes advantage of additional memory {{to reduce the number}} of times that the [...] relation is scanned.|$|R
30|$|Hypo 2. The {{function}} that requests {{an answer to}} a question based on the characteristics of the <b>nested</b> <b>loop</b> will promote an understanding of the relationships between the implementation and behavior of a <b>nested</b> <b>loop.</b>|$|R
40|$|Most {{existing}} {{solutions to}} pipelining <b>nested</b> <b>loops</b> are developed for general purpose processors, {{and may not}} work efficiently for field-programmable gate arrays due to loop control overhead. This is especially true when the <b>nested</b> <b>loops</b> have nonrectangular iteration spaces (IS). Thus we propose a novel method that can transform triangular IS-the most frequently found type of nonrectangular IS-into rectangular ones, so that other loop transformations can be effectively applied and the overall performance of <b>nested</b> <b>loops</b> can be maximized. Our evaluation results using the state-of-the-art Vivado high-level synthesis tool demonstrate that our technique can improve the performance of <b>nested</b> <b>loops</b> with nonrectangular IS significantly. clos...|$|R
40|$|There {{are many}} methods for <b>nested</b> <b>loop</b> {{partitioning}} exist; however, {{most of them}} perform poorly when they partition loops with non-uniform dependences. This paper proposes a generalized and optimized loop partitioning mechanism which can exploit parallelism in <b>nested</b> <b>loops</b> with non-uniform dependences. Our approach based on the region partitioning technique divides the loop into variable size partitions. Furthermore, the proposed algorithm partitions a <b>nested</b> <b>loop</b> using the copy-renaming and optimized partitioning techniques so as to minimize the serial part of the iteration space. Thus, it out performs previous partition mechanisms for <b>nested</b> <b>loops</b> with non-uniform dependences. Compared with other popular techniques, our scheme shows dramatic improvement in preliminary performance results...|$|R
40|$|Chain-based {{scheduling}} [1] is an e cient partitioning {{and scheduling}} scheme for <b>nested</b> <b>loops</b> on distributed-memory multicomputers. The {{idea is to}} take advantage of the regular data dependence structure of a <b>nested</b> <b>loop</b> to overlap and pipeline the communication and computation. Most partitioning and scheduling algorithms proposed for <b>nested</b> <b>loops</b> on multicomputers [1, 2, 3] are graph algorithms on the iteration space of the <b>nested</b> <b>loop.</b> The graph algorithms for partitioning and scheduling are too expensive (at least O(N), where N is the total number of iterations) to be implemented in parallelizing compilers. Graph algorithms also need large data structures to store the result of the partitioning and scheduling. In this paper, we propose compiler loop transformations and the code generation to generate chain-based parallel codes for <b>nested</b> <b>loops</b> on multicomputers. The cost of the loop transformations is O(nd), where n is the number of <b>nesting</b> <b>loops</b> and d is the number of data dependences. Both n and d are very small in real programs. The loop transformations and code generation for chain-based partitioning and scheduling enable parallelizing compilers to generate parallel codes which contain all partitioning and scheduling information that the parallel processors need at run time...|$|R
40|$|This paper {{presents}} {{an approach to}} software pipelining of <b>nested</b> <b>loops.</b> While several papers have addressed software pipelining of inner loops, little {{work has been done}} in the area of extending it to <b>nested</b> <b>loops.</b> This paper solves the problem of finding the minimum iteration initiation interval (in the absence of resource constraints) for each level of a <b>nested</b> <b>loop.</b> The problem is formulated as one of finding a rational quasi-affine schedule for each statement in the body of a perfectly <b>nested</b> <b>loop</b> which is then solved using linear programming. This allows us to treat iteration-dependent statement reordering and multidimensional loop unrolling in the same framework. Unlike most work in scheduling <b>nested</b> <b>loops,</b> we treat each statement in the body as a unit of scheduling. Thus, the schedules derived allow for instances of statements from different iterations to be scheduled at the same time. Optimal schedules derived here subsume extant work on software pipelining of inner loops, in t [...] ...|$|R
40|$|Microsoft SQL Server was {{successful}} {{for many years}} for transaction processing and decision support workloads with neither merge join nor hash join, relying entirely on <b>nested</b> <b>loops</b> and index <b>nested</b> <b>loops</b> join. How much difference do additional join algorithms really make, and how much system performance do they actually add? In a pure OLTP workload that requires only record-to-record navigation, intuition agrees that index <b>nested</b> <b>loops</b> join is sufficient. For a DSS workload, however, the question is much more complex. To answer this question, we have analyzed TPC-D query performance using an internal build of SQL Server with merge-join and hash-join enabled and disabled. It shows that merge join and hash join are both required to achieve the best performance for decision support workloads. 1. 0 Introduction For a long time, most relational database systems employed only <b>nested</b> <b>loops</b> join, in particular {{in the form of}} index <b>nested</b> <b>loops</b> join, and merge join. The general rule of thumb, [...] ...|$|R
40|$|Multi-dimensional systems {{containing}} <b>nested</b> <b>loops</b> {{are widely}} used to model scientific applications such as image processing, geophysical signal processing and fluid dynamics. However, branches within these loops may degrade the performance of pipelined architectures. This paper presents the theory, supporting hardware and experiments of a novel technique, based on multi-dimensional retiming, for reducing pipeline hazards caused by branches within <b>nested</b> <b>loops.</b> This technique, called Multi-Dimensional Branch Anticipation Scheduling, is able to achieve nearoptimal schedule length for <b>nested</b> <b>loops</b> containing branch instructions. 1...|$|R
30|$|For {{the control}} practice, {{students}} were first {{given the same}} 1 -h lecture on <b>nested</b> <b>loops</b> as the experimental group, including {{an explanation of the}} characteristics of <b>nested</b> <b>loops.</b> We subsequently conducted the 15 -min pre-test. Then, they studied <b>nested</b> <b>loops</b> using textbooks and the lecture material, without using LEPA 2. Before studying, the teacher explained that the aim {{of the study was to}} understand <b>nested</b> <b>loops</b> using a sample program. The sample program was identical to the one in the experimental practice. The teacher also explained that, in particular, they should study tracing the program. After a 1 -h study period, we conducted the 15 -min post-test. Both pre- and post-tests were the same as those given to the experimental group.|$|R
5000|$|The block <b>nested</b> <b>loop</b> join {{algorithm}} improves on {{the simple}} <b>nested</b> <b>loop</b> join by only scanning [...] once for every group of [...] tuples. For example, one {{variant of the}} block <b>nested</b> <b>loop</b> join reads an entire page of [...] tuples into memory and loads them into a hash table. It then scans , and probes the hash table to find [...] tuples that match any of the tuples in the current page of [...] This reduces the number of scans of [...] that are necessary.|$|R
30|$|For the {{experimental}} practice, {{the teacher who}} regularly taught the course lectured on single and <b>nested</b> <b>loops</b> for 1  h as a review in the first class. The lecture included {{an explanation of the}} characteristics of <b>nested</b> <b>loops,</b> as described in the “Two characteristics in <b>nested</b> <b>loop</b> code” section. At the end of the class, we conducted a 15 -min pre-test to evaluate the students’ understanding of <b>nested</b> <b>loops</b> before using LEPA 2. In the second class, we allowed the students to use LEPA 2 to learn <b>nested</b> <b>loops.</b> The program for this exercise displayed a five-step pyramid with spaces, asterisks, and new lines. Before the exercise in the class, the teacher described the aims of the exercise and the environment provided by LEPA 2. During the exercise, neither the teacher nor our team provided assistance to the students for understanding the program. We recorded screen videos of the student’s interactions with the environment. After the 60 -min exercise, we conducted a 15 -min post-test to evaluate the students’ understanding of <b>nested</b> <b>loops</b> after using LEPA 2. In the beginning of the first class, we informed that the students would have pre- and post-tests but the resultant scores of both test would not affect the course grades.|$|R
40|$|International audienceLoop {{pipelining}} {{is a key}} {{transformation in}} high-level synthesis tools as it helps maximizing both computational throughput and hardware utilization. Nevertheless, it somewhat looses its efficiency when dealing with small trip-count inner loops, as the pipeline latency overhead quickly limits its efficiency. Even if {{it is possible to}} overcome this limitation by pipelining the execution of a whole <b>loop</b> <b>nest,</b> the applicability of <b>nested</b> <b>loop</b> pipelining has so far been limited to a very narrow subset of <b>loops,</b> namely perfectly <b>nested</b> <b>loops</b> with constant bounds. In this work we propose to extend the applicability of nested-loop pipelining to imperfectly <b>nested</b> <b>loops</b> with affine dependencies by leveraging on the so-called polyhedral model. We show how such <b>loop</b> <b>nest</b> can be analyzed, and under certain conditions, how one can modify the source code in order to allow <b>nested</b> <b>loop</b> pipeline to be applied using a method called polyhedral bubble insertion. We also discuss the implementation of our method in a source-to-source compiler specifically targeted at High-Level Synthesis tools...|$|R
5000|$|The block <b>nested</b> <b>loop</b> runs in [...] I/Os where [...] is {{the number}} of {{available}} pages of internal memory and [...] and [...] is size of [...] and [...] respectively in pages. Notethat block <b>nested</b> <b>loop</b> runs in [...] I/Os if [...] fits in the available internal memory.|$|R
40|$|Loop {{unwinding}} is {{a well-known}} technique for reducing loop overhead, exposing parallelism, and increasing the efficiency of pipelining. Traditional loop unwinding {{is limited to the}} innermost loop of a set of <b>nested</b> <b>loops</b> and the amount of unwinding is either fixed or must be specified by the user. In this paper we present a general technique, loop quantization, for unwinding multiple <b>nested</b> <b>loops,</b> explain its advantages over other transformations, and illustrate its practical effectiveness. An abstraction of <b>nested</b> <b>loops</b> is presented which leads to results about the complexity of computing quantizations and an algorithm...|$|R
40|$|<b>Nested</b> <b>loops</b> {{are widely}} used in {{scientific}} applications such as image processing, geophysical signal processing and fluid dynamics. However, branches within these loops may degrade the performance on the underlying pipelined architecture. This paper presents the theory, supporting hardware and experiments of a novel technique, based on multi-dimensional retiming, for reducing pipeline hazards caused by branches within <b>nested</b> <b>loops.</b> This technique, called Multi-Dimensional Branch Anticipation Scheduling, is able to achieve optimal or near-optimal schedule length for <b>nested</b> <b>loops</b> containing branch instructions in polynomial time. It transforms a multi-dimensional conditional data flow graph representing the <b>nested</b> <b>loop,</b> carries out conditional resource sharing, breaks sharing-prevention cycles and minimizes additional hardware requirements incurred by propagation of branch control signals along the schedule. Such propagation is accomplished by Branch Anticipation Bits (babits). We furth [...] ...|$|R
30|$|In this paper, we {{describe}} a code-reading support environment and practical classroom applications using this environment to understand <b>nested</b> <b>loops.</b> Previously, {{we developed a}} code-reading support system based on visualization of the relationships among the program code, target domain world, and operations. We implemented the proposed system in exercises with <b>nested</b> <b>loops.</b> The evaluation results suggested that students could frequently fulfill learning objectives using the proposed system. However, we also discovered that some students experienced a learning impasse in the classroom. We attempted to address these students with two supporting approaches: bridging {{the gap between the}} generalization structures in the program code and their corresponding operations and enabling learners to predict the behavior of the <b>nested</b> <b>loops.</b> In this paper, we extend our previous system with new functions based on our two supporting approaches. Further, we implement the system in another classroom for <b>nested</b> <b>loops.</b> We describe a correlation between the proposed system and an understanding of <b>nested</b> <b>loops</b> using pre-/post-test comparisons. We discuss how code reading using the proposed system allows learners to cultivate a superior understanding of the program code.|$|R
40|$|Embedded {{systems have}} strict timing and code size requirements. Software {{pipelining}} {{is one of}} the most important optimization techniques to improve the execution time of loops by increasing the parallelism among successive loop iterations. However, little research has been done for the software pipelining problem on <b>nested</b> <b>loops.</b> The existing software pipelining techniques for single loops can only explore the innermost loop parallelism of a <b>nested</b> <b>loop,</b> so the final timing performance is inferior. While multi-dimensional (MD) retiming can explore the outer loop parallelism, it introduces large overheads in loop index generation and code size due to transformation. In this paper, we propose theory and algorithms of software pipelining for <b>nested</b> <b>loops</b> with minimal overheads based on the fundamental understanding of the properties of software pipelining for <b>nested</b> <b>loops.</b> We show the relationship among execution sequence, execution time of loop body, and software pipelining degree of a <b>nested</b> <b>loop</b> using retiming concepts. Two algorithms of Software PIpelining for <b>NEsted</b> <b>loops</b> (SPINE) are proposed: The SPINE-FULL algorithm generates fully parallelized loops with computation and code size overheads as small as possible. The SPINE-ROW-WISE algorithm generates the best possible parallelized loops with the minimal overheads. Our technique can be directly applied to imperfect <b>nested</b> <b>loops.</b> The experimental results show that the average improvement on the execution time of the pipelined loop generated by SPINE is 71. 7 % compared with that generated by the standard software pipelining approach. The average code size is reduced by 69. 5 % compared with that generated by MD retiming...|$|R
5000|$|... #Subtitle level 3: Multiple early exit/exit from <b>nested</b> <b>loops</b> ...|$|R
40|$|Relational {{database}} systems use join queries {{to retrieve}} data from two relations. Several join methods {{can be used}} to execute these queries. This study investigated the effect of varying join selectivity factors on the performance of the join methods. Experiments using the ORACLE environment were set up to measure the performance of three join methods: <b>nested</b> <b>loop</b> join, sort merge join and hash join. The performance was measured in terms of total elapsed time, CPU time and the number of I/O reads. The study found that the hash join performs better than the <b>nested</b> <b>loop</b> and the sort merge under all varying conditions. The <b>nested</b> <b>loop</b> competes with the hash join at low join selectivity factor. The results also showed that the sort merge join method performs better than the <b>nested</b> <b>loop</b> when a predicate is applied to the inner table...|$|R
40|$|Software {{pipelining}} {{is one of}} {{the most}} important optimization techniques to increase the parallelism among successive loop iterations. However, little research has been done for the software pipelining problem on <b>nested</b> <b>loops.</b> The existing software pipelining techniques for single loops can only explore the innermost loop parallelism of a <b>nested</b> <b>loop,</b> so the final timing performance is inferior. While multi-dimensional (MD) retiming can explore the outer loop parallelism, it introduces large computation overhead and code size expansion due to loop transformation. Therefore, the generated code is inefficient for optimizing <b>nested</b> <b>loops.</b> In this paper, we propose theory and algorithms of software pipelining on <b>nested</b> <b>loops</b> (SPINE) with minimal computation overhead and code size, based on the fundamental understanding of the properties of software pipelining for <b>nested</b> <b>loops.</b> Different from the previous studies in this area, we show the relationship among execution sequence, execution time of loop body, and software pipelining degree using retiming concepts. We find that the execution sequence plays an important role in generating code with small computation overhead and economic code size. Two software pipelining algorithms for <b>nested</b> <b>loops</b> are proposed: The SPINE-FULL algorithm generates fully parallelized loops with computation and code size overheads as small as possible. The SPINE-ROW-WISE algorithm generates the best possible parallelized loops with the minimal overheads. The experimental results show that The average improvement on the execution time of the pipelined loop generated by SPINE is 71. 7 % compared with that generated by the standard software pipelining approach. The average code size reduction is 69. 5 % compared with that generated by MD retiming. Our approach can be applied to either perfect or imperfect <b>nested</b> <b>loops...</b>|$|R
50|$|The Art and Science of <b>Nested</b> <b>Loops,</b> {{seminar in}} Orlando, Florida.|$|R
40|$|In this thesis, we {{consider}} the complexity of computing the optimal join order sequences for star queries and general queries. We consider the following join methods in our thesis - indexed <b>nested</b> <b>loop</b> joins, sort-merge joins, hash joins and block <b>nested</b> <b>loop</b> joins. The use of cartesian products is avoided and only linear trees are considered for query execution...|$|R
40|$|On modern computers, the {{performance}} of programs is often limited by memory latency rather than by processor cycle time. To reduce the impact of memory latency, the restructuring compiler community has developed localityenhancing program transformations, the most well-known of which is loop tiling. Tiling is restricted to perfectly <b>nested</b> <b>loops,</b> but many imperfectly <b>nested</b> <b>loops</b> can be transformed into perfectly <b>nested</b> <b>loops</b> that can then be tiled. Recently, we proposed an alternative approach to locality enhancement called data shackling. Data shackling reasons about data traversals rather than iteration space traversals, and can be applied directly to imperfectly <b>nested</b> <b>loops.</b> We have implemented shackling in the SGI MIPSPro compiler which already has a sophisticated implementation of tiling. Our experiments on the SGI Octane workstation with dense numerical linear algebra programs show that shackled code obtains double {{the performance}} of tiled code for most of these programs, and ob [...] ...|$|R
30|$|<b>Nested</b> <b>loops</b> are a {{learning}} target with which novice learners frequently have an initial difficulty. This is because to fully comprehend this concept {{requires that the}} learner understands {{all three of the}} abovementioned fundamentals. Koppelman and van Dijk (2010) emphasized the importance of <b>nested</b> <b>loops</b> as one of the targets required to understand the concept of abstraction. However, limited exposure in programming courses constrains the efforts of learners to develop a thorough understanding of these fundamental concepts. The purpose of our study is to encourage students to learn these concepts efficiently. We have introduced learning support systems into classroom exercises in <b>nested</b> <b>loops</b> for several years (Kogure et al. 2013).|$|R
30|$|In this paper, we {{described}} our {{learning support}} system and classroom practices with the proposed system for an improved understanding of <b>nested</b> <b>loops.</b> <b>Nested</b> <b>loops</b> are an appropriate target for learning the fundamental skills of programming. We believe that learning support systems efficiently and effectively {{contribute to an}} understanding of fundamental programming concepts and the acquisition of the skills required to utilize them.|$|R
30|$|A 4. The student {{used the}} {{function}} {{to observe the}} characteristics of <b>nested</b> <b>loops</b> (Ex 4).|$|R
3000|$|... {{specified}} in (19). These calculations require four <b>nested</b> <b>loops,</b> but note {{that we can}} decrease the number of <b>loops</b> to three <b>nested</b> <b>loops</b> by considering only the word pairs that {{are present in the}} training documents instead of all word pairs. Thus the time complexity in the M-step is O(KNB) where B is the average number of the word pairs in the training documents.|$|R
40|$|Abstract — In modern {{multiprocessor}} systems, proces-sors can be stalled by inter-task communication when read-ing from {{a remote}} buffer. This paper presents {{a solution for}} the inter-task communication, that has a minimal im-pact {{on the performance of}} the system, hides the inter-task communication latency without requiring additional hard-ware. The solution applies to jobs, represented as task graphs, where the tasks are <b>nested</b> <b>loop</b> programs. Buffers are allocated in scratch-pad memories of the consuming tasks to provide low latency read access. For the <b>nested</b> <b>loop</b> programs, minimal buffer sizes can be determined to cover all possible communication patterns. The added computational complexity is low, as the solution adds only a few operations to the <b>nested</b> <b>loop</b> programs...|$|R
