99|3|Public
6000|$|... "February 5th, 1839.--The Carnival began yesterday. It is {{a curious}} example of the trifling things which will heartily amuse tens of {{thousands}} of grown people, precisely because they are trifling, and therefore a relief from serious business, cares and labors. The Corso is a street about a mile long, and about as broad as Jermyn Street; but bordered by much loftier houses, with many palaces and churches, and has two or three small squares opening into it. Carriages, mostly open, drove up and down it for two or three hours; and the contents were shot at with handfuls of comfits from the windows,--in the hope of making them as <b>non-content</b> as possible,--while they returned the fire {{to the best of their}} inferior ability. The populace, among whom was I, walked about; perhaps one in fifty were masked in character; but there was little in the masquerade either of splendor of costume or liveliness of mimicry. However, the whole scene was very gay; there were a good many troops about, and some of them heavy dragoons, who flourished their swords with the magnanimity of our Life-Guards, to repel the encroachments of too ambitious little boys. Most of the windows and balconies were hung with colored drapery; and there were flags, trumpets, nosegays and flirtations of all shapes and sizes. The best of all was, that there was laughter enough to have frightened Cassius out of his thin carcass, could the lean old homicide have been present, otherwise than as a fleshless ghost;--in which capacity I thought I had a glimpse of him looking over the shoulder of a particolored clown, in a carriage full of London Cockneys driving towards the Capitol. This good-humored foolery will go on for several days to come, ending always with the celebrated Horse-race, of horses without riders. The long street is cleared in the centre by troops, and half a dozen quadrupeds, ornamented like Grimaldi in a London pantomime, scamper away, with the mob closing and roaring at their heels." ...|$|E
50|$|This method {{involves}} {{generation of}} a compressed video capsule of a lecture by recognizing activities and segmenting them {{based on the}} parameters of talking head, writing hand and slideshow using Hidden Markov model. The process separates the content frames of writing hand and slideshow from the <b>non-content</b> frames of talking head and a capsule is generated by selecting suitable content and <b>non-content</b> frames.|$|E
50|$|<b>Non-content</b> COMINT {{is usually}} used to deduce {{information}} about the user of a certain transmitter, such as locations, contacts, activity volume, routine and its exceptions.|$|E
50|$|A Jewish candidate, Lionel de Rothschild, {{was elected}} {{as one of}} the four members of Parliament for the City of London in 1847 but could not take his seat without taking an oath of office, and the bill that was {{introduced}} on 16 December that year was intended to carry out the wishes of a definite English constituency. This passed its third reading in the Commons on 4 May 1848, by a majority of 62 votes, but was rejected in the Lords by 163 <b>non-contents</b> to 128 contents. The same thing happened in 1849 when Lionel de Rothschild was again elected, but in the following year the struggle took on another and more dramatic form.|$|R
2500|$|The {{winning streak}} reached five games, before losses to <b>non-contenting</b> {{teams in the}} Oilers, Blue Jackets, Wild – the latter two in a shootout – and to the Avalanche, whom the Flames were battling for a playoff spot, left the players {{disappointed}} in their performance. [...] Speaking about the team's chances of making the post-season with just over two weeks remaining in the regular season, Alex Tanguay stated that [...] "we're just giving it away." [...] An overtime loss to the Canucks officially eliminated the Flames from playoff contention with two games left in the season. The team won the final two games, 3–2 over Vancouver and 5–2 over Anaheim. [...] Both victories were sparked by Akim Aliu, a rookie forward recalled from Abbotsford to evaluate his play at the NHL level. He served as an agitator, scored two goals and added an assist. The final game also marked the 350th consecutive sell-out for the Flames at the Saddledome.|$|R
5000|$|The {{real-time}} interception {{of contents}} of electronic communication is prohibited under the wiretap act, while the Pen Register Act [...] provides protection for the interception of the <b>non-content</b> part of the electronic communication. The [...] "From" [...] and [...] "To" [...] fields along with the IP address of the sender/receiver have been considered as <b>non-content</b> information, while the subject has been considered as the content. Once the email is stored on a computer (email server/user computer), it is protected from unauthorized access under the Stored Communications Act (Title II of Electronic Communications Privacy Act).|$|E
5000|$|Section 2703 (18 U.S.C. § 2703) of the SCA {{describes}} {{the conditions under}} which the government is able to compel an ISP to disclose [...] "customer or subscriber" [...] content and <b>non-content</b> information for each of these types of service: ...|$|E
5000|$|Everything2 {{is not a}} wiki, {{and there}} is no direct way for <b>non-content</b> editors to make {{corrections}} or amendments to another author's article. Avenues for correction involve discussing the writeup with its author; petitioning a content editor; adding a note in a special [...] "broken nodes" [...] section; or superseding the original writeup with an original, stand-alone follow-up.|$|E
5000|$|In 1986, the United States Congress {{passed the}} Stored Communications Act (codified at 18 U.S.C. Chapter 121 §§ 2701-2712) which governs {{the privacy of}} stored Internet {{communications}} in the United States. Section 2703 (18 U.S.C. § 2703) provides the rules that the government must follow to compel a third-party service provider to disclose [...] "customer or subscriber" [...] content and <b>non-content</b> information.|$|E
50|$|The LCA and the LIV also criticised s 187A(6), which {{introduces}} {{the requirement that}} telecommunication service providers create data not currently captured through their services. In particular, the LCA was concerned that it was unclear in the Memorandum and Bill how the content and substance of communications would be separated and filtered from the <b>non-content</b> by service providers {{in the course of}} meeting their data retention obligations.|$|E
5000|$|The content also doesn't {{determine}} whether the email was either unsolicited or bulk, the two key features of spam. So, if a friend sends you a joke that mentions [...] "viagra", content filters can easily mark it as being spam {{even though it is}} neither unsolicited nor sent in bulk. <b>Non-content</b> base statistical means can help lower false positives because it looks at statistical means vs. blocking based on content/keywords. Therefore, {{you will be able to}} receive a joke that mentions [...] "viagra" [...] from a friend.|$|E
5000|$|A {{national}} security letter (NSL) is an administrative subpoena {{issued by the}} United States government to gather information for {{national security}} purposes. NSLs do not require prior approval from a judge. The Stored Communications Act, Fair Credit Reporting Act, and Right to Financial Privacy Act authorize the United States government to seek such information that is [...] "relevant" [...] to authorized national security investigations. By law, NSLs can request only <b>non-content</b> information, for example, transactional records and phone numbers dialed, but never the content of telephone calls or e-mails.|$|E
50|$|Systematic {{processing}} involves {{comprehensive and}} analytic, cognitive processing of judgment-relevant information. The systematic approach values source reliability and message content, which may exert stronger impact on persuasion, when determining message validity. Judgments developed from systematic processing {{rely heavily on}} in-depth treatment of judgment-relevant information and respond accordingly to the semantic content of the message. Recipients developing attitudes from a systematic basis exert considerable cognitive effort and actively attempt to comprehend and evaluate the message's arguments. Systematic recipients also attempt to assess their validity {{as it relates to}} the message’s conclusion. Systematic views of persuasion emphasize detailed processing of message content and the role of message-based cognitions in mediating opinion change. While recipients utilizing systematic processing rely heavily on message content, source characteristics and other <b>non-content</b> may supplement the recipients’ assessment of validity in the persuasion message.|$|E
5000|$|Section 214, which {{amended the}} parts of FISA that deal with pen {{registers}} and trap and trace devices, {{because they believe that}} originally under FISA, court orders brought before the court were limited to the investigation of foreign threats to national security. They believe that the amendment broadens this to include U.S. citizens and [...] "there's no way for citizens to know how often FISA pen-traps are authorized, whether and to what extent they're being used to spy on Internet communications, or how the court interprets the distinction between communications content and <b>non-content</b> when it comes to Internet communications." [...] They also criticize the amendment for being too vague in specifying what can and can't be trapped (trap and trace and pen registers are only meant to determine data about the nature of communications, not the contents of the communications themselves).|$|E
50|$|The CCLS, LCA and the APF all {{submitted}} {{that both}} thresholds for access were too low, for several reasons. First, ‘reasonably necessary’ was not {{defined in the}} Bill {{and according to the}} CCLS, this could be interpreted in several ways and would be better altered to simply use the word ‘necessary’. The APF recommended a higher threshold be applied to access of both real-time communications and stored content, and that it be required that such access relate to investigations of serious criminal offences, punishable by an imprisonment term of at least 7 years. Additionally, the APF submitted that the procedural safeguards for access to data under Chapter 4 of the TIA Act were inadequate. It recommended that safeguards be introduced to regulate access to <b>non-content</b> telecommunications data, which could involve a decision of an independent body required to balance the objectives of access against the intrusion of privacy.|$|E
5000|$|Detecting spam {{based on}} the content of the email, either by {{detecting}} keywords such as [...] "viagra" [...] or by statistical means (content or <b>non-content</b> based), is very popular. Content based statistical means or detecting keywords can be very accurate when they are correctly tuned to the types of legitimate email that an individual gets, but they can also make mistakes such as detecting the keyword [...] "cialis" [...] in the word [...] "specialist" [...] (see also Internet censorship: Over- and under-blocking). Spam originators frequently seek to defeat such measures by employing typographical techniques such as replacing letters with accented variants or alternative characters which appear identical to the intended characters but are internally distinct (e.g., replacing a Roman 'A' with a Cyrillic 'A'), or inserting other characters such as whitespace, nonprinting characters, or bullets into a term to block pattern matching. This introduces an arms race which demands increasingly complex keyword-detection methods.|$|E
50|$|Heuristic {{processing}} uses judgmental rules {{known as}} knowledge structures that are learned {{and stored in}} memory. The heuristic approach offers an economic advantage by requiring minimal cognitive {{effort on the part}} of the recipient. Heuristic processing is governed by availability, accessibility, and applicability. Availability refers to the knowledge structure, or heuristic, being stored in memory for future use. Accessibility of the heuristic applies to the ability to retrieve the memory for use. Applicability of the heuristic refers to the relevancy of the memory to the judgmental task. Due to the use of knowledge structures, heuristic information processors are likely to agree with messages delivered by experts, or messages that are endorsed by others, without fully processing the semantic content of the message. In comparison to systematic processors, heuristic processors judge the validity of messages by relying more on accessible context information, such as the identity of the source or other <b>non-content</b> cues, which are more persuasive to them than the message characteristics. Heuristic views de-emphasize detailed information processing and focuses on the role of simple rules or cognitive heuristics in mediating persuasion.|$|E
5000|$|The Electronic Communications Privacy Act restricts {{government}} and private access to computer records. Thus, in order to unmask the author of an anonymous post through the legal process, the individual seeking the information must comply with ECPA. There is no provision within ECPA, other than voluntary disclosure or with consent, that allows civil litigants to force an ISP or website to reveal {{the contents of a}} user's emails via a subpoena. However, a private party in a lawsuit may force an ISP to disclose <b>non-content</b> records (e.g. the name of the owner of an account, a list of email addresses to whom emails were sent, access times, etc.) through a subpoena. In addition, the government can obtain the records needed to identify the person behind an IP address using a subpoena. In order to obtain more detailed transactional records, the government would be required to obtain a court order by setting forth [...] "specific and articulable facts show that there are reasonable grounds to believe...the records...are relevant and material to an ongoing criminal investigation." ...|$|E
5000|$|Section 2702 (18 U.S.C. § 2702) of the SCA targets {{two types}} of online service, [...] "electronic {{communication}} services" [...] and [...] "remote computing services." [...] The statute defines an electronic communication service as [...] "any service which provides to users thereof the ability to send or receive wire or electronic communications." [...] A remote computing service is defined as [...] "the provision to the public of computer storage or processing services {{by means of an}} electronic communications system." [...] Also describes conditions under which a public ISP can voluntarily disclose customer communications or records. In general, ISPs are forbidden to [...] "divulge to any person or entity the contents of any communication which is carried or maintained on that service." [...] However, ISPs are allowed to share [...] "non-content" [...] information, such as log data and the name and email address of the recipient, with anyone other than a governmental entity. In addition, ISPs that do not offer services to the public, such as businesses and universities, can freely disclose content and <b>non-content</b> information. An ISP can disclose the contents of a subscriber's communications authorized by that subscriber.|$|E
40|$|As {{technology}} {{continues to}} make information and facts readily accessible, the importance of understanding {{the context of the}} information and demonstrating how to use it appropriately will provide better indications of learning than factual recall. This chapter examines the manner in which curriculum and assessment reforms are moving toward promotion of student skill development beyond traditional content knowledge recall. A discussion of the current state of <b>non-content</b> skill assessment in chemistry is presented noting in particular that instructor interest in <b>non-content</b> aspects of learning appears to outpace the measurement of them. Additionally, the chapter presents data from a national survey. These data were used to understand the relative importance of <b>non-content</b> goals and skills in the general chemistry classroom. How these data will inform future efforts to create appropriate formative and summative assessments of goals and skills beyond content knowledge is also discussed...|$|E
40|$|We investigates {{language}} {{models for}} informational and navigational web search. Retrieval {{on the web}} is a task that differs substantially from ordinary ad hoc retrieval. We perform an analysis of prior probability of relevance {{for a wide range}} of <b>non-content</b> features, shedding further light on the importance of <b>non-content</b> features for web retrieval. Language models can naturally incorporate multiple document representations, as well as <b>non-content</b> information. For the former, we employ mixture language models based on document full-text, incoming anchor-text, and document titles. For the latter, we study a range of priors based on document length, URL structure, and link topology. We look at three types of topics—distillation, home page, and named page— as well as for a mixed query set. We find that the mixture models lead to considerable improvement of retrieval effectiveness for all topic types. The web-centric priors generally lead to further improvement of retrieval effectiveness...|$|E
40|$|Abstract. In this work, {{we present}} an {{application}} of the recently pro-posed unsupervised keyword extraction algorithm RAKE to a corpus of Polish legal texts {{from the field of}} public procurement. RAKE is essen-tially a language and domain independent method. Its only language-specific input is a stoplist containing a set of <b>non-content</b> words. The performance of the method heavily depends on the choice of such a stoplist, which should be domain adopted. Therefore, we complement RAKE algorithm with an automatic approach to selecting <b>non-content</b> words, which is based on the statistical properties of term distribution...|$|E
40|$|An {{important}} {{class of}} searches on the world-wide-web has the goal {{to find an}} entry page (homepage) of an organisation. Entry page search {{is quite different from}} Ad Hoc search. Indeed a plain Ad Hoc system performs disappointingly. We explored three <b>non-content</b> features of web pages: page length, number of incoming links and URL form. Especially the URL form proved to be a good predictor. Using URL form priors we found over 70 % of all entry pages at rank 1, and up to 89 % in the top 10. <b>Non-content</b> features can easily be embedded in a language model framework as a prior probability...|$|E
40|$|Abstract- User {{search for}} the {{required}} information using search engines. Search engines crawl and index web pages according to their informative content. User is interested only in the informative contents and not in non-informative content blocks. Web pages often contain navigation sidebars, advertisements, search blocks, copyright notices, etc which are not content blocks. The information contained in these <b>non-content</b> blocks can harm web mining. So {{it is important to}} separate the informative primary content blocks from non-informative blocks. In this paper three different algorithms for separating content blocks from <b>non-content</b> blocks developed by different authors are discussed. Removing non-informative content blocks from web pages can achieve significant storage and timing saving. Index Terms- Primary content, Entropy, noisy blocks, Web mining, Web blocks I...|$|E
40|$|The BlogVox system {{retrieves}} opinionated blog posts {{specified by}} ad hoc queries. BlogVox {{was developed for}} the 2006 TREC blog track by the University of Maryland, Baltimore County and the Johns Hopkins University Applied Physics Laboratory using a novel system to recognize legitimate posts and discriminate against spam blogs. It also processes posts to eliminate extraneous <b>non-content,</b> including blog-rolls, link-rolls, advertisements and sidebars. After retrieving posts relevant to a topic query, the system processes them to produce a set of independent features estimating {{the likelihood that a}} post expresses an opinion about the topic. These are combined using an SVM-based system and integrated with the relevancy score to rank the results. We evaluate BlogVox’s performance against human assessors. We also evaluate the individual splog filtering and <b>non-content</b> removal components of BlogVox...|$|E
40|$|Introduction As the ubiquitousness of {{multimedia}} {{information has been}} encouraging the diversification of the user's need for information, each user may have his own individual requirements regarding both content issues and <b>non-content</b> issues. Examples of the first can be requirements with respect to accuracy, completeness and up-to-dateness. Examples of the latter are requirements regarding data format, processing time and cost. Not only is the information environment heterogeneous and distributed, there are also price tags attached to the {{different parts of the}} information chain. The price may depend on many different aspects, such as information quality, connect time, processing time, network usage, etc. Most research thus far has focused on content-based issues. The importance of <b>non-content</b> issues is a direct consequence of the use {{of multimedia}} data, and is only slowly being acknowledged. We advocate that an advanced multimedia information retrieval service shou...|$|E
40|$|Abstract. The {{research}} presented {{investigates the}} use of <b>non-content</b> features for effective information retrieval. We use the expression noncontent features {{to refer to the}} structural markup within a document or a collection and the document’s surface features, i. e. document’s (derived) metadata (e. g. size). Our main hypothesis is that the best use information retrieval systems can make of this type of information will be determined by the different types of search tasks and contextual factors. We focus our investigation on three main aspects: (1) The analysis of existing and the creation of new retrieval strategies on {{the use of}} noncontent features, (2) the use of relevance feedback techniques to refine the <b>non-content</b> information given a user need, and (3) the study of the relationships between user search tasks and contextual factors and the structural characteristics of the relevant information. ...|$|E
40|$|Abstract. Topic {{distillation}} aims {{at finding}} key resources which are high-quality pages for certain topics. With analysis in <b>non-content</b> features of key resources, a pre-selection method is introduced in topic distillation research. A decision tree is constructed to locate key resource pages using query-independent <b>non-content</b> features including in-degree, document length, URL-type and two new features {{we found out}} involving site’s self-link structure analysis. Although the result page set contains only about 20 % pages of the whole collection, it covers more than 70 % of key resources. Furthermore, information retrieval on this page set makes more than 60 % improvement with respect to that on all pages. These results were achieved using TREC 2002 web track topic distillation task for training and TREC 2003 corresponding task for testing. It shows an effective way of getting better performance in topic distillation with a dataset significantly smaller in size. 1...|$|E
40|$|We {{propose to}} score phrase {{translation}} pairs for {{statistical machine translation}} using term weight based models. These models employ tf. idf to encode the weights of content and <b>non-content</b> words in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Two similarity functions are compared. Using these models in a statistical machine translation task shows significant improvements. ...|$|E
40|$|We {{describe}} {{a method to}} extract content text from diverse Web pages by using the HTML document’s Text-To-Tag Ratio rather than specific HTML cues {{that may not be}} constant across various Web pages. We describe how to compute the Text-To-Tag Ratio on a line-by-line basis and then cluster the results into content and <b>non-content</b> areas. With this approach we then show surprisingly high levels of recall for all levels of precision, and a large space savings. 1...|$|E
40|$|This {{thesis is}} a {{conceptual}} replication of VanPatten (1990) and Bouden, Greenslade, and Sanz (1999). It is a quantitative study {{about the effects}} that consciously focusing on form while reading for meaning will have on a L 2 learner?s comprehension due to the limited attentional capacity of working memory. Data was gathered from 101 subjects at a private Mexican institution, 52 at the intermediate L 2 English level and 49 at the advanced L 2 English level, by way of brief exercises to measure text comprehension while focusing on a lexical content item, a bound morpheme, or a <b>non-content</b> lexical item. Subject?s reading comprehension was measured using {{a modified version of}} Carrell?s (1985) idea unit analysis, after which a statistical analysis was used to obtain the overall comprehension scores for each task group. The statistical analysis revealed that L 2 learners of English have difficulty focusing on a bound morpheme and a <b>non-content</b> lexical item while reading for comprehension in the L 2. It also demonstrated that focusing on a content lexical item does not adversely affect L 2 comprehension. The implications for this study include providing evidence that L 2 learners at the intermediate and advanced stages have difficulty consciously focusing on form while reading for meaning in the L 2. It also provides evidence that working memory is a limited capacity processing system...|$|E
40|$|Blog posts {{are often}} informally written, poorly structured, rife with {{spelling}} and grammatical errors, and feature non-traditional content. These characteristics make them difficult to process with standard language analysis tools. Performing linguistic analysis on blogs is plagued by two additional problems: (i) {{the presence of}} spam blogs and spam comments and (ii) extraneous <b>non-content</b> including blog-rolls, link-rolls, advertisements and sidebars. We describe techniques designed to eliminate noisy blog data developed {{as part of the}} BlogVox system- a blog analytics engine we developed for the 2006 TREC Blog Track. The findings in this paper underscore the importance of removing spurious content from blog collections. ...|$|E
30|$|The {{creation}} of new coordinator positions was not done in a vacuum. The DAT expended considerable effort developing the messaging and mission of this structure so as to develop attitudes, beliefs, and values (i.e., symbols) to optimize how the coordination system was taken up and sustained. They also developed a framework around student learning with three components: (1) content knowledge, (2) critical thinking, and (3) professional skills. This particular framework was developed to be responsive to the current ways of thinking in the department but also provide greater attention to <b>non-content</b> skills {{as a way to}} gradually shift the thinking (i.e., symbols) in the department.|$|E
40|$|Abstract. We present two {{approaches}} to the Amharic- English bilin-gual track in CLEF 2004. Both experiments use a dictionary based ap-proach to translate the Amharic queries into English Bags-of-words, but while one approach removes <b>non-content</b> bearing words from the Amharic queries based on their IDF value, the other uses a list of English stop words to perform the same task. The resulting translated (English) terms are then submitted to a retrieval engine that supports the Boolean and vector-space models. In our experiments, the second approach (based {{on a list of}} English stop words) performs slightly better than the one based on IDF values for the Amharic terms. ...|$|E
40|$|Abstract. There is {{significant}} value in having predictions for an item before {{deciding whether to}} invest time or money in consuming that item. In a web based scenario where the items are multimedia items such as audio, recommendations {{can be made to}} users based on an understanding of their previous consumption or their indications of likes and dislikes. We examine two types of recommendation: content based and <b>non-content</b> or collaborative recommendation. We then apply out thinking to the area of new internet services such as online radio, and propose an architecture for an intelligent music radio system. We then suggest the efficacy of using conceptual clustering techniques in such a paradigm. 1...|$|E
40|$|Fourth Amendment doctrines {{created in}} the 1970 s and 1980 s no longer reflect how the world works. The formal legal distinctions on which they rely—(a) private versus public space, (b) {{personal}} information versus third party data, (c) content versus <b>non-content,</b> and (d) domestic versus international—are failing to protect the privacy interests at stake. Simultaneously, reduced resource constraints are accelerating the loss of rights. The doctrine has yet {{to catch up with}} the world in which we live. A necessary first step for the Court is to reconsider the theoretical underpinning of the Fourth Amendment, to allow for the evolution of a more effective normative framing. Failure to do so will mean the continued retraction of Fourth Amendment protections...|$|E
