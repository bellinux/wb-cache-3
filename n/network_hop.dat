17|338|Public
50|$|The {{algorithm}} is independently computed at each <b>network</b> <b>hop.</b> The algorithm operates over an interval, initially 100 milliseconds. Per-packet queuing delay is monitored through the hop. As each packet is dequeued for forwarding, the queuing delay (amount {{of time the}} packet spent waiting in the queue) is calculated. The lowest queuing delay for the interval is stored. When the last packet of the interval is dequeued, if the lowest queuing delay for the interval is greater than 5 milliseconds, this single packet is dropped and the interval used for the next group of packets is shortened. If the lowest queuing delay for the interval is less than 5 milliseconds, the packet is forwarded and the interval is reset to 100 milliseconds.|$|E
50|$|Website {{monitoring}} companies provide organizations {{the ability}} to consistently monitor a website, or server function, and observe how it responds. The monitoring is often conducted from several locations {{around the world to}} a specific website, or server, in order to detect issues related to general Internet latency, <b>network</b> <b>hop</b> issues, and to pinpoint errors. Monitoring companies generally report on these tests in a variety of reports, charts and graphs. When an error is detected monitoring services send out alerts via email, SMS, phone, SNMP trap, pager that may include diagnostic information, such as a network trace route, code capture of a web page's HTML file, a screen shot of a webpage, and even a video of a website failing. These diagnostics allow network administrators and webmasters to correct issues faster.|$|E
30|$|This global, dynamic, and {{physically}} distributed resource allocation algorithm ensures low per-hop latency under no-loaded network conditions and manageable growth in latency under loaded network conditions. The agent hardware monitors the PE load conditions and <b>network</b> <b>hop</b> count between PEs, and uses these as parameters based {{on which the}} algorithm dynamically finds a route between each possible pair of communicating nodes. The algorithm can be applied in other MPSoC-based architectures with inherent redundancy due to presence of several identical components in an MPSoC.|$|E
5000|$|Guesstimates from {{adjacent}} Class C range and/or {{gleaned from}} <b>network</b> <b>hops.</b>|$|R
5000|$|By {{comparing}} {{the sequence of}} <b>network</b> <b>hops</b> reported by a tool such as traceroute for a proxied protocol such as http (port 80) with that for a non proxied protocol such as SMTP (port 25).|$|R
5000|$|Butterfly {{networks}} have lower diameter than other toplogies like linear array,ring and 2-D mesh. This implies that in butterfly network, a message sent from one processor would reach its destination in {{lower number of}} <b>network</b> <b>hops.</b>|$|R
40|$|In this paper, we {{describe}} an {{implementation of a}} ad-hoc, distributed sensor platform that provides synchronized time to its users. By abstracting the time synchronization layer away, we allow developers {{to focus on the}} core challenges of their applications (e. g., signal processing, aggregation, routing) rather than dealing with the algorithmic and systems issues that inevitably arise when integrating sensing with distributed synchronization. Through a variety of techniques, notably the use of Reference-Broadcast Synchronization (RBS) [5], our platform offers better than 5 µsec precision when comparing streams of audio data sampled at nodes separated by one <b>network</b> <b>hop.</b> 1...|$|E
40|$|Abstract — Traditional {{approaches}} to mirroring, caching, and content distribution have an underlying assumption that minimizing <b>network</b> <b>hop</b> count minimizes client latency. However, with uncongested backbones and potentially high-latency service times for dynamic content, such techniques are of limited effectiveness. In this paper we present an architecture in which dispatchers at an overloaded Internet Data Center (IDC) redirect requests for dynamic content to a geographically remote but less loaded IDC. We show with both analytical modeling {{as well as}} testbed experiments that the delay savings of redirecting requests to a lightly loaded IDC can far outweigh the overhead in inter-IDC network latency. Consequently, client end-to-end delays are significantly reduced without requiring modifications to clients, servers, or DNS...|$|E
40|$|ManyWorld-Wide Web clients {{today are}} {{connected}} to the Internet via low-speed computer links. Examples are home users accessing the Internet through telephone lines, and wireless computer users. This form of communication is characterized by a client being attached to a high-bandwidth network through a poor link. This thesis examines techniques that can help bring Web performance for such poorlyconnected clients more in line with others that are well-connected. A prefetching scheme that is realized at a proxy server one <b>network</b> <b>hop</b> away from the clientis proposed. The client {{does not need to be}} aware that prefetching is being performed# it is only aware of a perceived reduction in latency. The prefetching scheme considered takes full advantage of characteristics unique to the World-Wide Web, and it adapts dynamically to changing user behavior. A wireless network is emulated and used to help validate the ideas presented...|$|E
50|$|Users {{can define}} their own {{partitioning}} schemes. This allows developers to add “distribution awareness” to applications by partitioning {{based on a}} sub-key that is common to all rows being accessed by high running transactions. This ensures that data used to complete transactions is localized on the same shard, thereby reducing <b>network</b> <b>hops.</b>|$|R
50|$|The {{command-line}} {{options of}} the ping utility and its output vary between the numerous implementations. Options may include {{the size of}} the payload, count of tests, limits for the number of <b>network</b> <b>hops</b> (TTL) that probes traverse, and interval between the requests. Many systems provide a companion utility ping6, for testing on Internet Protocol version 6 (IPv6) networks.|$|R
50|$|In June 2011 the BFD {{protocol}} standardization process {{entered the}} Proposed Standard stage. RFC 5880 defines the BFD protocol, detecting MPLS LSP failure, using BFD to monitor connectivity across multiple <b>network</b> <b>hops,</b> and using BFD for IPv4 and IPv6. BFD's operation {{in conjunction with}} Open Shortest Path First (OSPF) and IS-IS protocols has also been outlined in RFC 5881.|$|R
40|$|Server {{replication}} {{is commonly}} used to improve the performance experienced by a large, globally-distributed client base. Based on traces from 193 real mirror servers, {{we find that the}} servers with the lowest transfer times for a file maintain very good performance over long periods of time; the average mirror server does not. Servers o ering top service comprised {{only a small fraction of}} the total number of servers hosting the file. We offer a server selection technique that identifies the best performing server from a large collection of servers; Without further testing, the server selected achieves performance that is very close to selecting the best possible server for at least a month of downloads. Such a technique yields lower average file transfer times in almost all cases than policies that perform tests on a per-download basis and select servers with the lowest <b>network</b> <b>hop</b> count or lowest roundtrip time...|$|E
40|$|Many World-Wide Web clients {{today are}} {{connected}} to the Internet via low-speed computer links. Examples are home users accessing the Internet through telephone lines, and wireless computer users. This form of communication is characterized by a client being attached to a high-bandwidth network through a poor link. This thesis examines techniques that can help bring Web performance for such poorlyconnected clients more in line with others that are well-connected. A prefetching scheme that is realized at a proxy server one <b>network</b> <b>hop</b> away from the client is proposed. The client {{does not need to be}} aware that prefetching is being performed; it is only aware of a perceived reduction in latency. The prefetching scheme considered takes full advantage of characteristics unique to the World-Wide Web, and it adapts dynamically to changing user behavior. A wireless network is emulated and used to help validate the ideas presented. iv Acknowledgements I would like to express gratitude to my su [...] ...|$|E
40|$|Abstract—We {{study the}} problem of {{maintaining}} group communication between m mobile agents, tracked and helped by n static networked sensors. We develop algorithms to maintain a O(lg n) -approximation to the minimum Steiner tree of the mobile agents such that the maintenance message cost is on average O(lg n) per each hop an agent moves. The key idea is to extract a ‘hierarchical well-separated tree (HST) ’ on the sensor nodes such that the tree distance approximates the sensor <b>network</b> <b>hop</b> distance {{by a factor of}} O(lg n). We then prove that maintaining the subtree of the mobile agents on the HST uses logarithmic messages per hop movement. With the HST we can also maintain O(lg n) approximate k-center for the mobile agents with the same message cost. Both the minimum Steiner tree and the k-center problems are NP-hard and our algorithms are the first efficient algorithms for maintaining approximate solutions in a distributed setting. I...|$|E
40|$|Scalable Flat Cache Only Memory Architectures (Flat COMA) are {{designed}} for reduced memory access latencies while minimizing programmer and operating system involvement. Indeed, to keep memory access latencies low, neither the programmer needs to perform clever data placement nor the operating system needs to perform page migration. The hardware automatically replicates the data and migrates it to the attraction memories of the nodes that use it. Unfortunately, part of the latency of memory accesses is superfluous. In particular, reads often perform unnecessary attraction memory accesses, require too many <b>network</b> <b>hops,</b> or perform necessary attraction memory accesses inefficiently. In this paper, we propose relatively inexpensive schemes that address these three problems. To eliminate unnecessary attraction memory accesses, we propose a small direct-mapped cache called Invalidation Cache (IVC). To {{reduce the number of}} <b>network</b> <b>hops,</b> the IVC is augmented with hint pointers to processors. T [...] ...|$|R
40|$|The {{nature of}} Wireless Sensor Networks and their de-ployments in real environments prevent the {{application}} of classic real-time methods to guarantee timeliness properties, which is further complicated by the pivotal importance of low energy consumption. In this paper we present a method to estimate probabil-ities of meeting the end-to-end delivery deadlines and extend the concept of routing path, thus providing addi-tional knowledge to the <b>network</b> <b>hops.</b> ...|$|R
50|$|Key-based routing (KBR) is a lookup {{method used}} in {{conjunction}} with distributed hash tables (DHTs) and certain other overlay networks. While DHTs provide a method to find a host responsible for a certain piece of data, KBR provides a method to find the closest host for that data, according to some defined metric. This may not necessarily be defined as physical distance, but rather the number of <b>network</b> <b>hops.</b>|$|R
40|$|Though the WWW {{has come}} a long way since when it was monikered the World Wide Wait, it is still not {{reliable}} during heavy workload conditions. Overloads due to flash arrival of users or diurnal workload patterns are known to exponentially increase download times. More recently, online banks and portals have been the target of Distributed Denial-of-Service (DDoS) attacks, which send a deluge of requests and drive away the legitimate users. This dissertation proposes a web hosting architecture consisting of a grid of clusters, to provide high-performance in the presence of standard overload conditions as well as resilience during attacks. The architecture’s high-performance component is provided by a server selection framework, Wide-Area ReDirection (WARD), which efficiently multiplexes resources across the cluster grid. Traditional approaches assume that minimizing <b>network</b> <b>hop</b> count minimizes client latency. In contrast, WARD’s server selection algorithm for-wards requests to the server that minimizes the total of estimated network and server delays. WARD is better-suited to handling overload conditions in dynamic web con...|$|E
40|$|Middleboxes such as firewalls, NAT, proxies, or Deep Packet Inspection play an {{increasingly}} important role in various types of IP networks, including enterprise and cellular networks. Recent studies have shed the light on their impact on real traffic and the complexity of managing them. Network operators and researchers have few tools to understand the impact of those boxes on any path. In this paper, we propose tracebox, an extension to the widely used traceroute tool, {{that is capable of}} detecting various types of middlebox interference over almost any path. tracebox sends IP packets containing TCP segments with different TTL values and analyses the packet encapsulated in the returned ICMP message. Further, as recent routers quote, in the ICMP message, the entire IP packet that they received, is able to detect any modification performed by upstream middleboxes. In addition, tracebox can often pinpoint the <b>network</b> <b>hop</b> where the middlebox interference occurs. We evaluate tracebox with measurements performed on PlanetLab nodes. Our analysis reveals various types of middleboxes that were not expected on such an experimental testbed supposed to be connected to the Internet without any restriction. Peer reviewe...|$|E
40|$|International audienceA {{distribution}} network {{is a system}} aiming to transfer {{a certain type of}} resource from feeders to customers. Feeders are producers of a resource and customers have a certain demand in this resource that must be satisfied. Distribution networks can be represented on graphs and be subject to constraints that limit the number of intermediate nodes between some elements of the <b>network</b> (<b>hop</b> constraints) because of physical constraints. This paper uses layered graphs for hop constrained problems to build extended formulations. Preprocessing techniques are also presented {{to reduce the size of}} the layered graphs used. The presented model is studied on the hop-constrained minimum margin problem in an electricity network. This problem consists of designing a connected electricity {{distribution network}}, and to assign customers to electricity feeders at a maximum number of hops H so as to maximize the minimum capacity margin over the feeders to avoid an overload for any feeder. Numerical results of our model are compared with those of state-of-the-art solution techniques of the minimum margin problem form Rossi et al. [20]. Variations of the initial problem are also presented, considering losses due to transportation or by replacing hop constraints by distance constraints, a variation arising in the context of multicast transmission in telecommunications...|$|E
5000|$|For remote {{detection}} and diagnosis, the Internet Control Message Protocol provides an [...] "echo" [...] functionality, where a special packet is transmitted that always produces a reply {{after a certain}} number of <b>network</b> <b>hops,</b> from whichever node has just received it. Tools such as ping, traceroute, and MTR use this protocol to provide a visual representation of the path packets are taking, and to measure packet loss at each hop.|$|R
5000|$|... #Caption: An {{illustration}} of <b>hops</b> in a <b>network.</b> The <b>hop</b> count between the computers {{in this case}} is 2.|$|R
30|$|While this {{addresses}} {{the issue of}} network bandwidth, an application may also have network latency constraints between pairs of servers. In this case, then Admission Control must ensure that the servers are allocated “close” to one another by some metric, e.g., the number of <b>network</b> <b>hops.</b> While using a graph structure to represent sets of latency requirements is most general, latency requirements could be addressed by evaluating such a distance metric for simple pair-wise allocations.|$|R
40|$|Mobile Ad-Hoc {{networks}} (MANETs) {{have become}} very popular for military applications, disaster recovery operations in which the fixed network infrastructure might not be available due to wars, natural disasters, and the like. One of the main research challenges in mobile ad hoc networks is designing adaptive, scalable and low-cost routing protocols for these highly dynamic environments. In this thesis, we propose a new metric called hop change metric in order to represent {{the changes in the}} network topology due to mobility. Hop change metric represents the changes in the number of hops in the routing table. It is believed that the change in the hop count is a good representative of the mobility. The high number of change in the hop count can be a sign of high mobility. This metric is implemented in two popular and main routing protocols. Hop change metric is firstly employed to the most popular reac-tive protocol AODV (Ad hoc On-Demand Distance Vector Routing). This approach called LA-AODV (Lightweight Adaptive AODV). The the main goal of LA-AODV is selecting a route with a low degree of mobility. LA-AODV uses the hop change metric for selecting better routes among valid route reply packets. Due to reflecting the change in the <b>network,</b> <b>hop</b> change metric helps to select a stable route to the destination. The results show that, LA...|$|E
40|$|Traditional Network-on-Chip (NoC) systems {{comprised}} of many cores suffer from debilitating bottlenecks of latency and significant power dissipation {{due to the}} overhead inherent in multi-hop communication. In addition, these systems remain vulnerable to malicious circuitry incorporated into the design by untrustworthy vendors {{in a world where}} complex multi-stage design and manufacturing processes require the collective specialized services of a variety of contractors. This thesis proposes a novel small-world tree-based network-on-chip (SWTNoC) structure designed for high throughput, acceptable energy consumption, and resiliency to attacks and node failures resulting from the insertion of hardware Trojans. This tree-based implementation was devised as a means of reducing average <b>network</b> <b>hop</b> count, providing a large degree of local connectivity, and effective long-range connectivity by means of a novel wireless link approach based on carbon nanotube (CNT) antenna design. Network resiliency is achieved by means of a devised adaptive routing algorithm implemented to work with TRAIN (Tree-based Routing Architecture for Irregular Networks). Comparisons are drawn with benchmark architectures with optimized wireless link placement by means of the simulated annealing (SA) metaheuristic. Experimental results demonstrate a 21 % throughput improvement and a 23 % reduction in dissipated energy per packet over the closest competing architecture. Similar trends are observed at increasing system sizes. In addition, the SWTNoC maintains this throughput and energy advantage {{in the presence of a}} fault introduced into the system. By designing a hierarchical topology and designating a higher level of importance on a subset of the nodes, much higher network throughput can be attained while simultaneously guaranteeing deadlock freedom as well as a high degree of resiliency and fault-tolerance...|$|E
40|$|The next {{generation}} of computing systems will be embedded, in a virtually unbounded number, and dynamically connected. Although these systems will penetrate every possible domain of our daily life, the expectation {{is that they will}} operate outside our normal cognizance, requiring far less attention from the human users than the desktop computers today. The networked embedded computing era will challenge our ways of thinking and computing far more than the PC revolution did in the past. The current software and network architectures and their associated programming models were not designed for these scenarios. Traditional parallel and distributed computing models are based on a distribution of tasks across a stable cluster of similar processing units. In networks of embedded systems however, nodes have properties such as location or functionality that make them only partially substitutable in a specific task. Tasks need to execute on specific nodes to achieve prescribed objectives, necessitating the location of target nodes in a manner that allows partial substitution. We propose a computing model and a system architecture for distributed embedded systems where nodes "cooperate" by providing their computing and communication resources to distributed tasks. The system architecture for cooperative computing is based on Smart Messages (SM), which can be viewed as intelligent carriers of data in a network. Smart Messages are collections of code and mobile data that migrate through the network, a single <b>network</b> <b>hop</b> at a time, executing at each step. Smart Messages are responsible for their own routing. To validate the model we have implemented two previously proposed applications, Directed Diffusion and SPIN, for data collection and data dissemination in sens [...] ...|$|E
50|$|Desperate to {{continue}} using, Charles turns to Reed who {{has found a}} way {{to continue}} getting Fantasites through an underground <b>network.</b> <b>Hoping</b> to continue to use worms and wanting to provide for June, who turns out is a fellow Fantasite addict suffering from withdrawals, Charles commits himself to a dangerous group of Fantasite-masked men. The group is run by Stephen, who {{has found a way}} to reclaim worms by smashing the skulls of users and re-selling the worms extracted from their brains.|$|R
50|$|Although {{applications}} perceive no functional differences, HiperSockets {{require less}} processing overhead {{on either side}} of the connections, improving performance. Since they are memory-based, they operate at memory speeds, reducing network latency and improving end-user performance, especially for complex applications which would otherwise require multiple <b>network</b> <b>hops</b> to fulfill requests. HiperSockets also provide security benefits, especially on the memory key-protected mainframe, even without encryption, because there is no opportunity to intercept a network connection. Moreover, HiperSockets improve reliability and availability because there are no network hubs, routers, adapters, or wires to break.|$|R
5000|$|Diameter: This is {{the worst}} case latency (between two nodes) {{possible}} in the system. It can be calculated in terms of <b>network</b> <b>hops,</b> which {{is the number of}} links a message must travel in order to reach the destination node. In the 8 node butterfly network, it appears that N(0,0) and N(3,7) are farthest away, but upon inspecting carefully, it is apparent that due to the symmetric nature of the network, traversing from any rank 0 node to any rank 3 node requires only 3 hops. Therefore the diameter of this system is 3.|$|R
40|$|Though the WWW {{has come}} a long way since when it was monikered the World Wide Wait, it is still not {{reliable}} during heavy workload conditions. Overloads due to flash arrival of users or diurnal workload patterns are known to exponentially increase download times. More recently, online banks and portals have been the target of Distributed Denial-of-Service (DDoS) attacks, which send a deluge of requests and drive away the legitimate users. This dissertation proposes a web hosting architecture consisting of a grid of clusters, to provide high-performance in the presence of standard overload conditions as well as resilience during attacks. The architecture's high-performance component is provided by a server selection framework, W&barbelow;ide-A&barbelow;rea R&barbelow;eD&barbelow;irection (WARD), which efficiently multiplexes resources across the cluster grid. Traditional approaches assume that minimizing <b>network</b> <b>hop</b> count minimizes client latency. In contrast, WARD's server selection algorithm forwards requests to the server that minimizes the total of estimated network and server delays. WARD is better-suited to handling overload conditions in dynamic web content, which are known to stress compute resources more than the network. Using a combination of analytical modeling and testbed experiments, it's shown that delay savings by redirecting requests to an under-loaded cluster can far outweigh the overhead in inter-cluster network latency. For instance, for an e-commerce site with 300 concurrent clients, redirection reduces download times from 5 to 2. 3 seconds. The architecture's DDoS-resilience is provided by DDoS-Shield, consisting of a suspicion assignment mechanism and a scheduler. Assuming sophisticated attackers, the possible attacks are characterized as either request flooding, asymmetric or repeated one-shot, on the basis of the application workload parameters exploited. In contrast to prior work, the suspicion mechanism assigns a continuous valued vs. binary suspicion measure to each client session, and the scheduler utilizes these values to determine if and when to schedule a session's requests. Testbed-driven experiments demonstrate the potency of these resource attacks as well as evaluate the efficacy of the counter-mechanism. For instance, an asymmetric attack effected to overwhelm the database CPU, increases download times from 0. 15 to 10 seconds, while DDoS-Shield is shown to improve performance to 0. 8 seconds...|$|E
50|$|Many mobile {{services}} {{split the}} application into a front-end client {{program and a}} back-end server program following the traditional client-server model. The front-end mobile application offloads its functionality to the back-end servers for various reasons such as speeding up processing. With the advent of cloud computing, the back-end server is typically hosted at the cloud datacenter. Though {{the use of a}} cloud datacenter offers various benefits such as scalability and elasticity, its consolidation and centralizion lead to a large separation between a mobile device and its associated datacenter. End-to-end communication then involves many <b>network</b> <b>hops</b> and results in high latencies and low bandwidth.|$|R
50|$|Most video emails do {{not include}} the actual video file as an {{attachment}} to the email because attachment size limits. The most common technique is to have the video file uploaded to a video hosting service. The uploaded file can have metadata attached to it identifying the creator, video codec used, channel and other tags. The video hosting service often encodes the video in to multiple formats to permit efficient display on variety of devices like desktop browser or lower resolution mobile devices. Video hosting services offer low latency, high bandwidth, minimum <b>network</b> <b>hops,</b> multiple encoding formats, and backup.|$|R
30|$|These {{findings}} {{provide a}} holistic {{view of how}} people share political news. The first finding suggests that the formation of echo-chambers resulting from subscriptions to traditional media outlets is countered by the more serendipitous news sharing happening among users. Also, the fact that followers hold a certain influence over a user is not surprising, if one considers that people are influenced by peers who are up to three (social <b>network)</b> <b>hops</b> away from them [14]. Based on these four generic factors that motivate political news sharing, we demonstrate {{a new way of}} visualizing news articles that gives users a fine control over the PoNS’ four dimensions.|$|R
40|$|Abstract—This {{document}} {{provides a}} problem statement for Store, Carry and Forward (SCF) network, a network consisting of non-realtime communication between {{systems that are}} gener-ally disconnected, which require multiple <b>network</b> <b>hops</b> between source and destination, and which may never be fully connected end-to-end at any given time. Included {{as part of this}} problem statement are a number of use cases that motivate having a standard method to communicate between such systems, as multi-organization and multi-vendor support and interoperability is highly desirable. This document also describes the requirements for a SCF protocol, and the expectations placed upon the SCF agents and SCF applications as well as guidelines and requirements fo...|$|R
40|$|Consistent hashing is at {{the core}} of many P 2 P proto-cols. It evenly {{distributes}} the keys over the nodes, thereby enabling logarithmic routing effort ‘with high probability’. However, consistent hashing incurs unnecessary overhead as shown in this paper. By removing consistent hashing from Chord, we derived a protocol that has the same favorable logarithmic routing performance but needs less <b>network</b> <b>hops</b> for updating its routing table. Additionally, our Chord # protocol supports range queries, which are not possible with Chord. Our empirical results indicate that Chord # outperforms Chord even under high churn, that is, when nodes frequently join and leave the system. 1...|$|R
