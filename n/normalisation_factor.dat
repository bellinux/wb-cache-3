57|40|Public
5000|$|For SID {{according}} to Daniel, sort by SID {{and apply the}} relevant <b>normalisation</b> <b>factor</b> as specified in the file metadata.|$|E
5000|$|... is a <b>normalisation</b> <b>factor</b> (see below), and [...] is the {{associated}} Legendre polynomial of degree [...] and order [...]The azimuth angle [...] is zero straight ahead and increases counter-clockwise. The elevation angle [...] is zero on the horizontal plane and positive {{in the upper}} hemisphere.|$|E
3000|$|... t method. For normalisation, two {{reference}} genes {{were used}} and <b>normalisation</b> <b>factor</b> {{was calculated by}} geNorm software (Vandesompele et al. 2002). The normalised expression level of gene of interest (GOI) was calculated by dividing the raw GOI quantities with the appropriate <b>normalisation</b> <b>factor.</b> Next, the standard deviation (SD) and standard error on the normalised gene of interest (GOInorm) expression levels were calculated by applying the error propagation rules for independent variables.|$|E
40|$|In {{the context}} of Life Cycle Assessment (LCA), {{according}} to ISO 14044 (ISO 2006), normalisation is an optional step of Life Cycle Impact Assessment (LCIA) which allows the practitioner expressing results after characterization using a common reference impact. This supports the comparison between alternatives using reference numerical scores. The <b>normalisation</b> <b>factors</b> express the total impact of a reference region for a certain impact category (e. g. climate change, eutrophication, etc.) in a reference year. The same applies to Environmental Footprint. This document provides <b>normalisation</b> <b>factors</b> (NFs) {{for the implementation of}} the EU Environmental Footprint (EC - European Commission, 2013). The calculation of <b>normalisation</b> <b>factors</b> is based on a refinement and update of the ‘Resource Life Cycle indicators’ dataset (EC - JRC, 2012 b), used as inventory. These indicators were developed within the Life Cycle Indicators framework (EC - JRC, 2012 a) in {{the context of}} the Roadmap to a resource efficient Europe, within the Flagship initiative - A resource-efficient Europe of the Europe 2020 Strategy. The aim of the Life Cycle Indicators is to monitor the environmental impacts associated with European production and consumption, as well as waste management within the EU, by including also impacts from trade (imports and exports). The Life Cycle Indicators are based on the collection of data related to territorial emission (domestic inventory) complemented with process based LCA for representative traded goods. In fact, the indicators have been designed to provide information on the environmental impacts linked to European consumption and production. The ‘apparent consumption’ approach is adopted by accounting for both the domestic extractions of resources and emissions in the EU 27 as well as the impacts due to international trade (both imports and exports). For the domestic inventory, the data gaps related to emissions and resource use have been overcome adopting a series of estimation strategies (details on estimation strategies are reported in Sala et al., 2014). Both for the domestic inventory and for those resulting from modelling the trade, the ILCD set of impact assessment methods and related characterisation factors (EC- JRC, 2011) have been applied for calculating <b>normalisation</b> <b>factors.</b> The elementary flows adopted for the calculation of the <b>normalisation</b> <b>factors</b> are also derived in particular from the Life Cycle indicators for Resources (EC - JRC, 2012 b). Compared to the original report, updated data for 2010 at EU 27 level and at country level has been used. In this report, the main methodological steps towards the calculation of the <b>normalisation</b> <b>factors</b> are described and discussed providing an overview of the improvements of current figures compared to existing database as well as limitations due to data gaps and extrapolations. The original goal of the study was to develop <b>normalisation</b> <b>factors</b> that are based on an apparent consumption approach as developed in the prototype life cycle indicators work. The impacts related to imported goods should be added and the impacts related to exported goods should be deducted from the domestic (territorial) figures for EU 27. The consideration of international trade in <b>normalisation</b> <b>factors</b> would allow getting a more comprehensive picture of the actual environmental impacts due to EU production and consumption processes. However, the study has indicated that at present the level of methodological development and data availability, are deemed not sufficiently mature for the results of impacts associated with trade to be recommended for use as normalisation values in the context of Environmental Footprint or Life Cycle Assessments. The main reasons are: i) significant variability in the results applying different methods for selection and up-scaling of products; ii) ratio import to domestic seems to be underestimated. JRC. H. 8 -Sustainability Assessmen...|$|R
40|$|Purpose. Assessing {{comprehensively}} {{the overall}} {{environmental impacts of}} a region remains a major challenge. Within life cycle assessment (LCA), this evaluation is performed calculating <b>normalisation</b> <b>factors</b> at different scales. Normalisation represents an optional step of LCA according to ISO 14040 / 44 which may help in understanding the relative magnitude of the impact associated to a product when compared to a reference value. In order to enhance the robustness and comprehensiveness of <b>normalisation</b> <b>factors</b> for Europe in 2010, this paper present a methodology for building an extended domestic inventory of emission and resources {{to be used in}} the context of Product Environmental Footprint Material and methods. The <b>normalisation</b> <b>factors</b> (NFs) for EU 27 in 2010 are based on extensive data collection and the application of extrapolation strategies for data gaps filling. The inventory is based on domestic emissions into air, water and soil and on resource extracted in EU, adopting a production based approach. A hierarchy hasebeen developed for data sources selection based on their robustness and data quality. Data gap filling has been based on proxy indicators, capitalizing existing statistics on pressure indicators. To calculate NFs, the inventory has been multiplied by the characterization factors at midpoint as recommended in International reference Life Cycle Data System (ILCD) Handbook (EC-JRC, 2011). Results and discussion. The resulting NFs presents several added values compared to prior normalization exercises, namely: more complete inventory; robustness evaluation of the data sources; more comprehensive coverage of the flows within each impact category; overall evaluation of the robustness of the final figures. Few flows (NOx, SOx, NH 4 etc) are driving the impacts of several impact categories, and the choice of the data sources is particularly crucial, as this may lead to differences in the NFs. The adoption of domestic NFs may results in overestimating the relative magnitude of certain impacts, especially when those impacts are associated with traded goods from or to outside the EU 27. Conclusion. <b>Normalisation</b> <b>factors</b> may help identification of the relative magnitude of the impact. Nonetheless, several limitations still exist both at the inventory and at the impact assessment level. Those limitations should be clearly reported and understood by the users of <b>normalisation</b> <b>factors</b> in order to correctly interpret the results of their study. Indeed, the efforts towards more robust normalization reference are needed both at the inventory and at the impact assessment side, including more robust impact assessment methods as well as better coverage of substances for which an inventory data is available but the characterization is missing. Strenghts and limitations of the current exercize have, then, implications also in other application context where integrated assessment of impacts is needed and were data gap filling and estimation of potential environemntal impacts is needed. JRC. H. 8 -Sustainability Assessmen...|$|R
40|$|Purpose. The aim of {{this study}} was twofold: firstly, to {{determine}} whether the European Association of Nuclear Medicine (EANM) dosage card results in weight-independent effective doses or weight-independent count rates; secondly, to determine whether one dosage card is sufficient for 95 different radiopharmaceuticals, and, if not, how many cards we reasonably need to take into account inter-tracer variability. Methods. <b>Normalisation</b> <b>factors</b> for count rate and effective dose were calculated as a function of body weight, with 70 kg as standard. Calculations were performed, using whole-body absorption fractions and MIRDOSE 3 software, for seven anthropomorphic phantoms and ten radionuclides. An analytic function for both relations was proposed. <b>Normalisation</b> <b>factors</b> for effective dose for 95 radiopharmaceuticals were investigated using cluster analysis. Results. <b>Normalisation</b> <b>factors</b> for count rate and effective dose can be estimated accurately as a function of body weight W by (W/ 70) a holding only one parameter, called the a value. The a values for 95 radiopharmaceuticals were classified into three clusters (n A = 7, nB = 76, nC = 12). Cluster A contains tracers for renal studies. Cluster B contains all remaining tracers, except iodine-labelled tracers for thyroid studies and 89 Sr for therapy, which belong to cluster C. Conclusion. Correction factors proposed by the EANM task group mainly correct for effective dose. They are very similar to the factors obtained for cluster A. Using the EANM factors for tracers belonging to clusters B and C results in significantly higher effective doses to children. We suggest using three tracer-dependent dosage cards for which the correction factors have been calculated to obtain weight-independent effective doses. © Springer-Verlag 2004. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
3000|$|To {{make these}} {{features}} also scale invariant, they additionally {{need to be}} divided by a <b>normalisation</b> <b>factor,</b> e.g. I [...]...|$|E
3000|$|... is a <b>normalisation</b> <b>factor.</b> Thus, {{incorrectly}} classified {{examples are}} more likely to be included in the next bootstrap data set. The final model is a mixture with [...]...|$|E
40|$|It was {{recently}} suggested by Blythe and Evans that a properly defined steady state <b>normalisation</b> <b>factor</b> {{can be seen}} as a partition function of a fictitious statistical ensemble in which the transition rates of the stochastic process play the role of fugacities. In analogy with the Lee-Yang description of phase transition of equilibrium systems, they studied the zeroes in the complex plane of the <b>normalisation</b> <b>factor</b> in order to find phase transitions in nonequilibrium steady states. We show that like for equilibrium systems, the ``densities'' associated to the rates are non-decreasing functions of the rates and therefore one can obtain the location and nature of phase transitions directly from the analytical properties of the ``densities''. We illustrate this phenomenon for the asymmetric exclusion process. We actually show that its <b>normalisation</b> <b>factor</b> coincides with an equilibrium partition function of a walk model in which the ``densities'' have a simple physical interpretation. Comment: LaTeX, 23 pages, 3 EPS figure...|$|E
40|$|N° 1 Many PET {{scanners}} nowadays {{have the}} possibility to record event-by-event information, known as list mode data. This {{has the advantage of}} keeping the data in the highest possible resolution (both temporal and spatial). In most cases, list mode data are then binned into sinogram format before reconstruction. In this paper, we discuss at which stage <b>normalisation</b> <b>factors</b> should be introduced. It is shown that noise is greatly reduced by performing the normalisation after the binning. We illustrate this with acquired and simulated data for the quad HiDAC camera...|$|R
30|$|Emissions are {{aggregated}} into 14 {{environmental and}} health impact categories. Aggregated results were normalised {{in order to}} allow for a comparative analysis of the relative importance of each impact category (Table A. 6 in the online accompanying appendix). The global warming potential and particulate matter emissions from recycling 1 tonne of food waste are, for example, scaled relative to the per capita greenhouse gas and particulate matter emissions in the year 2010 (and are reported in units of milli-Person equivalents, mPE). <b>Normalisation</b> <b>factors</b> were derived from the PROSUITE project which was developed specifically for the ILCD method (Blok et al. 2013).|$|R
40|$|The {{accurate}} {{determination of}} personal dose equivalent requires {{the proper use}} of appropriate radiological quantities and units, knowledge of the dose equivalent response of the personal dosemeters used and detailed information on the fluence as well as dose equivalent spectra at the workplaces. This information can then be used to select the appropriate dosemeters, to set up the optimum calibration conditions and to introduce, in case of need, <b>normalisation</b> <b>factors</b> for application in specific radiation fields. High-energy neutrons contribute significantly to the radiation fields around high-energy particle accelerators. Examples for procedures and methods to determine personal dose equivalent at accelerator centres are give...|$|R
30|$|Rotation {{and start}} point shift {{invariance}} is obtained because absolute values are used, and translation invariance results from discarding k[*]=[*] 0. For scale invariance, they additionally {{need to be}} divided by a <b>normalisation</b> <b>factor,</b> e.g.|$|E
40|$|One of the {{fundamental}} problems in Stereophonic Acoustic Echo Cancellation (SAEC) lies in the misadjustment in the filter coefficients due to the two strongly correlated channel-inputs. In this paper, we study {{the effect of the}} <b>normalisation</b> <b>factor,</b> ffl, within the twochannel NLMS (ffl-NLMS) algorithm for subband SAEC and show that the optimal choice may be close to the variance of the channel-input data. In the simulation study with real speech datasets, it is observed that subband stereo echo cancellers using the Fast Least Squares (FLS) algorithm in lower bands and the ffl-NLMS algorithm, with the optimal <b>normalisation</b> <b>factor</b> setting, in higher frequency bands can improve misalignment performance significantly compared with using only the FLS algorithm in all subbands. In addition, around 23 dB further misalignment performance improvement is obtained by applying smoothly time-varying allpass filters in the lower frequency bands, while introducing no perceptible auditory degradation...|$|E
30|$|The {{denominator}} on {{the right}} hand side represents a <b>normalisation</b> <b>factor</b> accounting for the average net income and population size. The index is also divided by two because all income differentials are counted twice in the double summation over all households. Table  8 presents low-income rates and gaps {{as well as the}} value of Gini coefficients by family type in the base run.|$|E
40|$|TThe {{results of}} tagged photon {{measurements}} of the 16 O(γ, pn) and 16 O(γ, pp) reactions, carried out with photons of energies of 80 – 131 MeV, are presented. Missing energy spectra for both reactions, with an energy resolution of 7 MeV have been obtained. The 16 O(γ, pn) missing energy spectrum {{is very similar to}} that recently measured for the 12 C(γ, pN) reaction. In both cases the recoil momentum distributions are quantitavely described by a quasideuteron mechanism. Using <b>normalisation</b> <b>factors</b> based on this mechanism the average cross section for the 16 O(γ, pn) reaction, for nucleons ejected from the 1 p shell, is 510 ± 95 μb. The corresponding cross section for the 16 O(γ, pp) reaction is 10. 0 ± 3. 0 μb...|$|R
40|$|Normalisation is an {{optional}} step of Life Cycle Assessment (LCA) {{that can help}} appraising the magnitude of a certain impact category relative {{to that of a}} reference system. In order to develop <b>normalisation</b> <b>factors</b> for Europe, data on emission into air, water and soil as well on resource use has been collected. An ‘apparent consumption’ perspective in accounting had been adopted by adding impacts occurring within domestic boundaries, adding impacts associated to imported products and subtracting impacts associated to exports, consistently with a ‘footprint’ perspective. The purpose of this set of indicators is to tracking the overall environmental impact of the European Union (limited to EU 27) and ultimately of each Member State (of the EU 27), while taking into account also the burdens associated with trade. A set of LCA-based indicators of environmental impacts had been developed at country scale for the EU 27 member states for the year 2010 integrating data available from monitoring efforts with extrapolation strategies for filling data gaps. In fact, the ‘domestic’ inventory of environmental interventions had been developed by making use of existing national statistics and through estimation techniques, whereas the ‘trade’ inventory, composed of both import and export, had been estimated through bottom-up modeling of life cycle inventories of representative products {{on the basis of a}} commercial software (Ecoinvent 3. 0). The impact assessment had been done by applying the set of 15 (+ 1) impact categories and underlying indicators of environmental impact as recommended in the ILCD and then consistent with the LCIA methods used within the Product Environmental Footprint. The current set of <b>normalisation</b> <b>factors</b> is significantly more complete compared to previous normalisation exercise. Nonetheless, high uncertainty characterize some impact categories more than others because of the underlying data sources and modeling. Further on, the role of trade is highly variable across impact categories, however in its current formulation it is considered as a prototypal assessment needing additional refinements. The presentation will illustrate the results, discussing challenges and uncertainties. JRC. H. 8 -Sustainability Assessmen...|$|R
30|$|For pixels in the tumour ROI, {{a tissue}} to blood (T:B) ratio ≥[*] 1.2 was {{considered}} to indicate significant hypoxia as first described by Rajendran et al. [43]. Thereafter, various authors have used T:B ratio ≥[*] 1.2, 1.3 or 1.4 to indicate hypoxia. There is currently no consensus on which of these values to use. Since [18 F]FMISO concentrations in blood and muscle are nearly identical by {{a few minutes after}} tracer injection, T:M ratios were considered equivalent to T:B ratios with the benefit that no blood sampling is required. Since T:M SUVmax represented SUVmax of tumour divided by SUVmax of muscle, and since the <b>normalisation</b> <b>factors</b> in numerator and denominator cancel each other, the numerical outcome of T:M SUVmax is mathematically equivalent to the T:M ratio which is approximately equivalent to the T:B ratio (as evident from results in Fig.  1 a, b). Therefore, we thought it was appropriate to use both T:M SUVmax and T:B SUVmax to define tumour hypoxia volumes.|$|R
40|$|We {{show that}} the known matrix {{representations}} of the stationary state algebra of the Asymmetric Simple Exclusion Process (ASEP) can be interpreted combinatorially as various weighted lattice paths. This interpretation enables us to use the constant term method (CTM) and bijective combinatorial methods to express many forms of the ASEP <b>normalisation</b> <b>factor</b> in terms of Ballot numbers. One particular lattice path representation shows that the coefficients in the recurrence relation for the ASEP correlation functions are also Ballot numbers. Additionally, the CTM has a strong combinatorial connection {{which leads to a}} new ``canonical'' lattice path representation and to the ``omega-expansion'' which provides a uniform approach to computing the asymptotic behaviour in the various phases of the ASEP. The path representations enable the ASEP <b>normalisation</b> <b>factor</b> to be seen as the partition function of a more general polymer chain model having a two parameter interaction with a surface. Comment: 41 pages, 24 figure...|$|E
3000|$|The first {{equation}} is Chapman-Kolomogrov prediction equation, {{the second is}} Bayes rule update, while the last {{equation is}} <b>normalisation</b> <b>factor</b> [76]. This model is labelled doubly stochastic as it accounts for measurement error in xt− 1 by defining the observation series yt− 1, while xt− 1 {{is defined as the}} latent variable series. The latent state model is defined by the non-linear function [x [...]...|$|E
30|$|The <b>normalisation</b> <b>factor</b> can {{be related}} to body surface area, lean body mass and body weight. Here, the SUVs were {{normalised}} to body weight. SUVs were computed for every reconstructed time frame. For this study of uptake heterogeneity, images summed over the last 30  min were used. In future analyses, heterogeneity in the specific binding of the radiotracers might be further analysed by using other calculated macroparameters such as the binding potential or distribution volumes.|$|E
40|$|<b>Normalisation</b> <b>factors</b> are {{calculated}} as results of regional/global inventories of emission and resources characterised trough impact assessment methods. Several methodological assumptions {{are needed for}} building the inventory. Sala et al 2015 presented a set of <b>normalisation</b> <b>factors</b> for the EU 2010 defining a methodological approach for sources selection and for building proxy indicators. Qualitative and quantitative uncertainty evaluation is needed for assessing the robustness of final figures. Five sources of uncertainty have been analysed in this work: (F 1) {{the selection of the}} sources of data; (F 2) the classification of data as life cycle inventory (LCI) elementary flows; (F 3) the classification of substances for characterization; (F 4) the specification of the emission compartments; and (F 5) the use of spatially differentiated characterization factors. The sensitivity of the normalization factors to such uncertainties were assessed through a global sensitivity method, for the impact categories acidification (ACID), terrestrial eutrophication (ET), marine eutrophication (EM), photochemical ozone formation (POF), respiratory inorganics/particulate matter (RIPM) and water depletion (WD). The uncertainty associated with the methodological choices made for calculating normalization factors (Sala et al 2015) was assessed. Generally the value calculated by Sala et al (2015) compare well against average and median values estimated in this analysis for ACID, ET, EM and POF. Instead, the impact categories RIPM and WD show different patterns, for the former, although the average value is very similar, the median value is far lower than the normalization factor reported by Sala et al. (2015). For what concerns WD, the median value is much higher. Future improvements of the normalization factors should therefore prioritize the development of more detailed inventories of emissions by including: higher substance resolution, height of emission as well as the use of spatially differentiated characterization factors. The authors recommend that the normalization factors from Sala et al. (2015) are applied together with two additional sets of normalization factors i. e. the 'median values' and the set of 'average + standard deviation' values, so to better capture their uncertainty. Similarly, the interpretation of the results should build on the qualitative estimates of robustness provided by Sala et al. (2015). JRC. H. 8 -Sustainability Assessmen...|$|R
40|$|AbstractMixing {{effects in}} the MSSM Higgs sector can {{give rise to a}} {{sizeable}} interference between the neutral Higgs bosons. On the other hand, factorising a more complicated process into production and decay parts by means of the narrow-width approximation (NWA) simplifies the calculation. The standard NWA, however, does not account for interference terms. Therefore, we introduce a generalisation of the NWA (gNWA) which allows for a consistent treatment of interference effects between nearly mass-degenerate particles. Furthermore, we apply the gNWA at the tree and 1 -loop level to an example process where the neutral Higgs bosons h and H are produced in the decay of a heavy neutralino and subsequently decay into a fermion pair. The h – H propagator mixing is found to agree well with the approximation of Breit-Wigner propagators times finite wave-function <b>normalisation</b> <b>factors,</b> both leading to a significant interference contribution. The factorisation of the interference term based on on-shell matrix elements reproduces the full interference result within a precision of better than 1 % for the considered process. The gNWA also enables the inclusion of contributions beyond the 1 -loop order into the most precise prediction...|$|R
40|$|This paper {{deals with}} {{problems}} related to determination of customer costs of reliability – {{the total amount of}} costs related with power supply interruptions (loss of production, overtime costs to personnel (stuff), etc.) and expenses related with activities for diminishing negative effect of power supply interruptions (UPS devices, generators, additional insurance, etc.). The paper gives description of different cost estimation methods based on review and analysis of proposals for customer cost estimation and customer cost estimation surveys from different countries. On the basis of analysis of different customer cost estimation surveys and questionnaires, there is proposed new design of questionnaire for surveys with the aim to use it for performing customer cost evaluation study in the frames of “Government action plan” of Latvian government for year 2012. Problem of survey data normalization is discussed and proposal of appropriate <b>normalisation</b> <b>factors,</b> from authors` point of view, is given. Results of the work presented in the paper could be used when: • trying to get knowledge of customer valuation of reliability; • developing financial incentives for performance-based regulation of utilities (system operators); • developing guaranteed reliability standards; • etc...|$|R
40|$|Measurements are {{presented}} of the inclusive distributions of theJ/Ψ meson produced by muons of energy 200 GeV from an ammonia target. The gluon {{distribution of the}} nucleon has been derived from the data in the range 0. 04 <x< 0. 36 using a technique based on the colour singlet model. An arbitrary <b>normalisation</b> <b>factor</b> is required to obtain a reasonable integral of the gluon distribution. Some comments are made on the use ofJ/Ψ productionby virtual photons to extract the gluon distribution at HERA...|$|E
40|$|We {{consider}} the integer QH state on Riemann surfaces with conical singularities, {{with the main}} objective of detecting {{the effect of the}} gravitational anomaly directly from the form of the wave function on a singular geometry. We suggest the formula expressing the <b>normalisation</b> <b>factor</b> of the holomorphic state in terms of the regularized zeta determinant on conical surfaces and check this relation for some model geometries. We also comment on possible extensions of this result to the fractional QH states. Comment: 15 page...|$|E
40|$|Abstract. Through BM 25, the {{asymptotic}} term frequency quantification TF = tf/(tf+K), where tf is the within-document term {{frequency and}} K is a <b>normalisation</b> <b>factor,</b> became popular. This paper reports a finding regarding {{the meaning of}} the TF quantification: in the triangle of independence and subsumption, the TF quantification forms the altitude, that is, the middle between independent and subsumed events. We refer to this new assumption as semi-subsumed. While this finding of a well-defined probabilistic assumption solves the probabilistic interpretation of the BM 25 TF quantification, it is also of wider impact regarding probability theory. ...|$|E
40|$|A {{computation}} of inclusive {{cross sections}} for neutral Higgs boson production through gluon fusion and bottom-quark annihilation {{is presented in}} the MSSM with complex parameters. The predictions for the gluon-fusion process are based on an explicit calculation of the leading-order cross section for arbitrary complex parameters which is supplemented by higher-order corrections: massive top- and bottom-quark contributions at NLO QCD, in the heavy top-quark effective theory the top-quark contribution up to N^ 3 LO QCD including a soft expansion for the CP-even component of the light Higgs boson. For its CP-odd component and the heavy Higgs bosons the contributions are incorporated up to NNLO QCD. Two-loop electroweak effects are also incorporated, and SUSY QCD corrections at NLO are interpolated from the MSSM with real parameters. Finite wave function <b>normalisation</b> <b>factors</b> ensuring correct on-shell properties of the external Higgs bosons are incorporated from the code FeynHiggs. For the typical case of a strong admixture of the two heavy Higgs bosons it is demonstrated that squark effects are strongly dependent on the phases of the complex parameters. The remaining theoretical uncertainties for cross sections are discussed. The results have been implemented into an extension of the numerical code SusHi called SusHiMi. Comment: 37 pages, 12 figures, minor clarifications, updated references, matches published versio...|$|R
40|$|Mixing {{effects in}} the MSSM Higgs sector can {{give rise to a}} {{sizeable}} interference between the neutral Higgs bosons. On the other hand, factorising a more complicated process into production and decay parts by means of the narrow-width approximation (NWA) simplifies the calculation. The standard NWA, however, does not account for interference terms. Therefore, we introduce a generalisation of the NWA (gNWA) which allows for a consistent treatment of interference effects between nearly mass-degenerate particles. Furthermore, we apply the gNWA at the tree and 1 -loop level to an example process where the neutral Higgs bosons $h$ and $H$ are produced in the decay of a heavy neutralino and subsequently decay into a fermion pair. The $h-H$ propagator mixing is found to agree well with the approximation of Breit-Wigner propagators times finite wave-function <b>normalisation</b> <b>factors,</b> both leading to a significant interference contribution. The factorisation of the interference term based on on-shell matrix elements reproduces the full interference result within a precision of better than 1 % for the considered process. The gNWA also enables the inclusion of contributions beyond the 1 -loop order into the most precise prediction. Comment: 7 pages, 7 figures, Contribution to the proceedings of ICHEP 201...|$|R
40|$|Environmental {{life cycle}} {{assessment}} (LCA) is a method for the quantitative assessment of {{the environmental impacts of}} products. A number of impact categories are related to toxic effects of chemicals. Multimedia models for substance fate, supplemented with models for human exposure, have been developed in the context of human and environmental risk assessment (HERA). Different authors have adapted such models for use in LCA, largely on a continental level. It has sometimes been suggested to merge LCA toxicity assessment and HERA into one common tool. Here, it is demonstrated that LCA and HERA cannot be merged, due to a fundamental difference concerning their respective goals. Subsequently, adaptations to existing multimedia models are proposed to make it possible to extend multimedia models with a module for metals. The core of the thesis is formed by the GLOBOX model: a global, regionally differentiated fate, intake and effect model for LCA toxicity assessment. For emissions of any organic chemical or metal to any compartment in any country or at any sea, this model calculates region-specific characterisation factors. Finally, an updated set of LCA <b>normalisation</b> <b>factors</b> is provided, with which the relative contributions of a product to the different impact categories can be evaluated. Promotor: H. A. Udo de Haes, Co-promotor: G. HuppesWith summary in DutchUnileve...|$|R
40|$|In this paper, {{we study}} {{a method to}} sample from a target {{distribution}} π over R^d having a positive density {{with respect to the}} Lebesgue measure, known up to a <b>normalisation</b> <b>factor.</b> This method is based on the Euler discretization of the overdamped Langevin stochastic differential equation associated with π. For both constant and decreasing step sizes in the Euler discretization, we obtain non-asymptotic bounds for the convergence to the target distribution π in total variation distance. A particular attention is paid to the dependency on the dimension d, to demonstrate the applicability of this method in the high dimensional setting. These bounds improve and extend the results of (Dalalyan 2014) ...|$|E
40|$|We {{test the}} {{recently}} introduced radiation model against the gravity {{model for the}} system composed of England and Wales, both for commuting patterns and for public transportation flows. The analysis is performed both at macroscopic scales, i. e. at the national scale, and at microscopic scales, i. e. at the city level. It is shown that the thermodynamic limit assumption for the original radiation model significantly underestimates the commuting flows for large cities. We then generalize the radiation model, introducing the correct <b>normalisation</b> <b>factor</b> for finite systems. We show that even if the gravity model has a better overall performance the parameter-free radiation model gives competitive results, especially for large scales. Comment: in press Phys. Rev. E, 201...|$|E
40|$|H 1 has {{measured}} the diffractive DIS cross section ep → eXY {{using data from}} both of the HERA data-taking periods. Using new measurements of the diffractive cross section at different centre-of-mass energies, the diffractive longitudinal structure function FDL has been extracted at low and medium Q 2. The results are in agreement with NLO QCD predictions based on fits to inclusive data. New high statistics measurements of the diffractive reduced cross section s Dr have been combined with previous H 1 data to produce one coherent diffractive dataset measured over the accessible kinematic range. This precise dataset agrees well with pubished ZEUS data up to a <b>normalisation</b> <b>factor,</b> as well with QCD-based predictions...|$|E
40|$|For {{systems of}} {{unstable}} particles that mix with each other, an approximation of the fully momentum-dependent propagator matrix {{is presented in}} terms of a sum of simple Breit-Wigner propagators that are multiplied with finite on-shell wave function <b>normalisation</b> <b>factors.</b> The latter are evaluated at the complex poles of the propagators. The pole structure of general propagator matrices is carefully analysed, and it is demonstrated that in the proposed approximation imaginary parts arising from absorptive parts of loop integrals are properly taken into account. Applying the formalism to the neutral MSSM Higgs sector with complex parameters, very good numerical agreement is found between cross sections based on the full propagators and the corresponding cross sections based on the described approximation. The proposed approach does not only technically simplify the treatment of propagators with non-vanishing off-diagonal contributions, it is shown that it can also facilitate an improved theoretical prediction of the considered observables via a more precise implementation of the total widths of the involved particles. It is also well-suited for the incorporation of interference effects arising from overlapping resonances. Comment: 44 pages, 13 figures; v 2 : matches the version to appear in JHEP, extended discussion of uncertainty estimate and gauge dependenc...|$|R
40|$|The {{activity}} of the colon is regulated by chemical signalling, of which serotonin (5 - HT) is a key transmitter. Monitoring of mucosal 5 -HT overflow has been achieved to date using microelectrodes on small segment of colonic tissue, however little is known if such measurements are reflective with regards to 5 -HT signalling from the entire colon. This study focused on developing an electrochemical array device that could be utilised to conduct multi-site measurements of 5 -HT overflow from the entire colon. A 3 D printed mould was fabricated that could house 6 multi-wall carbon nanotube composite electrodes and provide a fixed distance between the electrodes and the tissue along {{the entire length of}} the colon. The electrodes were assessed for sensitivity, stability and crosstalk before conducting in vitro measurements using colons obtained from 6 and 24 month old mice. As composite electrodes can have a high degree of variability, <b>normalisation</b> <b>factors</b> were required between electrodes for a given array. The device had the sensitivity and stability required for 5 -HT measurements from intestinal tissue. Regio-specific changes in 5 -HT overflow were observed with age, where increases in 5 -HT overflow were observed in the distal colon due to an impairment/loss in the serotonin transporter (SERT). Our strategy can be utilised to develop arrays of varying sizes and geometries which can offerpractical solutions for large scale tissue measurements...|$|R
40|$|The QQT is a quasi-quantum {{mechanical}} {{treatment of the}} collision between molecules. Instead of a partial wave expansion approach, it uses a kind of Feynman path integral method that exploits the path length differences originating from the different orientations of an anisotropic molecule. As a result, the QQT provides valuable physical insight while requiring very little computational effort. The current paper gives a systematic derivation of the QQT and explains its underlying principles. The expression for the scattering amplitude is shown to be self-consistent, without any <b>normalisation</b> <b>factors,</b> when the rotational energy level spacing is negligible. The constant curvature approximation that is presented makes the QQT conceptually even more simple, {{and its effect on}} the calculated differential cross-sections (DCSs) turns out to be small. As examples we present QQT calculations of the DCSs for Ne-CO(1) and He-NO(2), at collision energies of, respectively, 511 cm- 1 and 514 cm- 1. The anisotropy of the hard shell potential energy surface for Ne-CO in terms of the incoming de Broglie wavelength is about twice as large as for He-NO. This leads to state-to-state DCSs that have up to three maxima of comparable amplitude, instead of only one large maximum as is found for He-NO. The QQT results for these two applications are compared with results from close coupling calculations...|$|R
