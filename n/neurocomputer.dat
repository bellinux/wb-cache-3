54|33|Public
50|$|Proponents {{hope that}} neurobiotics {{will replace the}} {{computer}} age with the <b>neurocomputer</b> age.|$|E
50|$|In 1991-97, Siemens {{developed}} the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 <b>Neurocomputer.</b> The MA-16 is a fast matrix-matrix multiplier {{that can be}} combined to form systolic arrays. It can process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) {{at a rate of}} 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contains 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.|$|E
50|$|A wetware {{computer}} is an organic computer (also {{known as an}} artificial organic brain or a <b>neurocomputer)</b> built from living neurons. Professor Bill Ditto, at the Georgia Institute of Technology, is the primary researcher driving the creation of these artificially constructed, but still organic brains. One prototype is constructed from leech neurons, and is capable of performing simple arithmetic operations. The concepts are still being researched and prototyped, but in the near future, {{it is expected that}} artificially constructed organic brains, even though they are still considerably simpler in design than animal brains, should be capable of simple pattern recognition tasks such as handwriting recognition.|$|E
40|$|This paper {{presents}} {{an introduction to}} <b>neurocomputers</b> and {{an overview of the}} history of <b>neurocomputers.</b> Direct implementation methods of <b>neurocomputers</b> using techniques from microelectronics and photonics are discussed. Emulation methods using special-purpose hardware are highlighted. The role of parallel computing systems for improved performance is introduced. Some commercially available <b>neurocomputers</b> and performance issues of such systems are also presented...|$|R
40|$|The {{purpose of}} the work: the {{theoretical}} generalization {{and the development of}} methods, algorithms and the ways of constructing the effective structure, units and assemblies of the high-capacity digital neurocomputers; the development of the technical decisions of its hardware realization; the creation of the series-fit items. The new approaches and methods of building of the <b>neurocomputers</b> have been offered, the effective structure and algorithms of the assemblies and units functioning have been developed, first, the unified number of the country <b>neurocomputers,</b> series HERCULES has been created. The developed <b>neurocomputers</b> have been introduced in service. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
30|$|High-performance fast {{calculations}} {{are developed}} quite well and have many applied aspects, helping solve complex problems. <b>Neurocomputers</b> have many advantages {{as compared to}} regular Harvard structure computers, {{but they also have}} one significant drawback: their price; as a result, some alternatives should be sought.|$|R
40|$|Based on the {{introduction}} of the traditional mathematical models of neurons in general-purpose <b>neurocomputer,</b> a novel all-purpose mathematical model-Double synaptic weight neuron (DSWN) is presented, which can simulate all kinds of neuron architectures, including Radial-Basis-Function (RBF) and Back-propagation (BP) models, etc. At the same time, this new model is realized using hardware and implemented in the new CASSANN-II <b>neurocomputer</b> {{that can be used to}} form various types of neural networks with multiple mathematical models of neurons. In this paper, the flexibility of the new model has also been described in constructing neural networks and based on the theory of Biomimetic pattern recognition (BPR) and high-dimensional space covering, a recognition system of omni directionally oriented rigid objects on the horizontal surface and a face recognition system had been implemented on CASSANN-H <b>neurocomputer.</b> The result showed DSWN neural network has great potential in pattern recognition...|$|E
40|$|We {{present the}} {{requirements}} for a <b>neurocomputer</b> for spike-processing neural networks. In a simulation study we investigated the performance of available hardware and showed, {{that there is still}} a need for a specific <b>neurocomputer</b> dedicated to the simulation of spike-processing networks. On the basis of our simulation study and an investigation of the features of spike-processing networks we analyses {{the requirements for}} the design of dedicated hardware. An efficient hardware architecture should contain an event-list module, a sender-oriented connection module and a number of fixed-point processing units...|$|E
40|$|An {{electronic}} <b>neurocomputer</b> which implements a radial basis function {{neural network}} (RBFNN) is described. The RBFNN is {{a network that}} utilizes a radial basis function as the transfer function. The key advantages of RBFNNs over existing neural network architectures include reduced learning time and the ease of VLSI implementation. This <b>neurocomputer</b> {{is based on an}} analog/digital hybrid design and has been constructed with both custom analog VLSI circuits and a commercially available digital signal processor. The hybrid architecture is selected because it offers high computational performance while compensating for analog inaccuracies, and it features the ability to model large problems...|$|E
40|$|As FPGAs have {{increasingly}} become denser and faster, {{they are being}} utilized for many applications, including the implementation of neural networks. Ideally, FPGA implementations, being directly in hardware and having parallelism, will have performance advantages over software on conventional machines. But {{there is a great}} deal to be done to make the most of FPGAs and to prove their worth in implementing neural networks, especially in view of past failures in the implementation of <b>neurocomputers.</b> This paper looks at some of the relevant issues...|$|R
40|$|Abstract- Digital {{implementations}} of <b>neurocomputers</b> {{are presently}} quite expensive, they require excessive power, they {{suffer from a}} number of issues that cause performance characteristics to differ from the theoretical model of the system, and they are relatively intolerant of fault conditions. The inherent advantages of the massively parallel structure of these systems are also lost in the common practice of executing algorithms sequentially on a conventional computer. The paper presents nonlinear analog signal methodology where for nonlinear processing nonlinear characteristics o f MOS transistors are used. I...|$|R
40|$|Recent {{advances}} in the technology and applications of spatial light modulators (SLMs) are discussed in review essays by leading experts. Topics addressed include materials for SLMs, SLM devices and device technology, applications to optical data processing, and applications to artificial neural networks. Particular attention is given to nonlinear optical polymers, liquid crystals, magnetooptic SLMs, multiple-quantum-well SLMs, deformable-mirror SLMs, three-dimensional optical memories, applications of photorefractive devices to optical computing, photonic <b>neurocomputers</b> and learning machines, holographic associative memories, SLMs as parallel memories for optoelectronic neural networks, and coherent-optics implementations of neural-network models...|$|R
40|$|Abstract—We combine {{here two}} {{well-known}} and established concepts: microelectromechanical systems (MEMS) and neurocomputing. First, we consider MEMS oscillators having low amplitude activity and we derive a {{simple mathematical model}} that describes nonlinear phase-locking dynamics in them. Then, we investigate a theoretical possibility of using MEMS oscillators to build an oscillatory <b>neurocomputer</b> having autocorrelative associative memory. The <b>neurocomputer</b> stores and retrieves complex oscillatory patterns {{in the form of}} synchronized states with appropriate phase relations between the oscillators. Thus, we show that MEMS alone can be used to build a sophisticated information processing system (U. S. provisional patent 60 / 178, 654). Index Terms—Andronov–Hopf bifurcation, resonantors, neural networks, oscillatory associative memory, smart matter...|$|E
40|$|A novel analog-digital hybrid {{architecture}} {{based on the}} utilization of high density digital random access memories for the storage of the synaptic weights of a neural network, and high speed analog hardware to perform neural computation is described. An electronic <b>neurocomputer</b> based on such an architecture is ideally suited for investigating the dynamics, associative recall properties, and computational capabilities of neural networks and provides significant speed improvement in comparison to conventional software based neural network simulations. As {{a demonstration of the}} feasibility of the hybrid architectural concept, a prototype breadboard hybrid <b>neurocomputer</b> system with 32 neurons has been designed and fabricated with off-the-shelf hardware components. The performance of the breadboard system has been tested for variety of applications including associative memory and combinatorial problem solving such as Graph Coloring, and is discussed in this paper...|$|E
40|$|In this paper, a novel {{mathematical}} model of neuron-Double Synaptic Weight Neuron (DSWN) (l) is presented. The DSWN can simulate {{many kinds of}} neuron architectures, including Radial-Basis-Function (RBF), Hyper Sausage and Hyper Ellipsoid models, etc. Moreover, this new model has been implemented in the new CASSANN-II <b>neurocomputer</b> {{that can be used}} to form various types of neural networks with multiple {{mathematical model}}s of neurons. The flexibility of the DSWN has also been described in constructing neural networks. Based on the theory of Biomimetic Pattern Recognition (BPR) and high-dimensional space covering, a recognition system of omni directionally oriented rigid objects on the horizontal surface and a face recognition system had been implemented on CASSANN-II <b>neurocomputer.</b> In these two special cases, the result showed DSWN neural network had great potential in pattern recognition...|$|E
40|$|The MEMS (Micro Electro Mechanical Sys-tems) {{oscillator}} has a {{high quality}} factor Q and it is en-abled to accumulate on the silicon wafer. Therefore the os-cillator works with low power and miniaturization of the device size is realized. So researches of micromechanical oscillators have been active in recent years. The MEMS oscillators can be coupled to each other, leading to poten-tial applications such as <b>neurocomputers,</b> filters, and means of clock distribution in microprocessors. In this pa-per, our purpose is to confirm that ring coupled MEMS os-cillators are synchronized with different phase relation-ships...|$|R
40|$|Human non-conscious {{reasoning}} {{is one of}} the most successful procedures evolved for the purposes of solving everyday problems in an efficient way. This is why the field of artificial intelligence should analyze, formalize and emulate the multiple ways of non-conscious reasoning with the purpose of applying them in human problem solving tasks, like medical diagnostics and treatments, educational diagnostics and intervention, organizational and political decision making, artificial intelligence knowledge based systems and <b>neurocomputers,</b> automatic control systems and similar devices for aiding people in the problem-solving process. In this paper, a heuristic framework for those non-conscious ways of {{reasoning is}} presented based on neurocognitive representations, heuristics, and fuzzy sets...|$|R
40|$|THE ARRAYS OF NUMBERS In this paper, {{the authors}} {{developed}} principles of realization for associative processing with logical-time representation {{of information for}} sorting the arrays of numbers. Also the authors developed the organization methods of opticalmemory pages, <b>neurocomputers</b> with processors based on optical and optoelectronic elements. The examples of realization of sorting on increase of array of five elements by method of a pair exchange and on associative processor, aimed at application of the newest achievements in domain of creation of optoelectroniс laminated structures are given. Key words: associative processor, logical integrated circuit, optoelectronic integrated circuit, and data array sorting and optoelectronic processor. 1...|$|R
40|$|We combine {{here two}} {{well known and}} {{established}} concepts: Microelectromechanical systems (MEMS) and neurocomputing. First, we consider MEMS oscillators having low amplitude activity and we derive a simple mathematical model that describes nonlinear phase-locking dynamics in them. Then, we investigate a theoretical possibility of using MEMS oscillators to build an oscillatory <b>neurocomputer</b> having autocorrelative associative memory. The <b>neurocomputer</b> stores and retrieves complex oscillatory patterns {{in the form of}} synchronized states with appropriate phase relations between the oscillators. Thus, we show that MEMS alone can be used to build a sophisticated information processing system (U. S. patent is applied for). Keywords [...] -resonators, Andronov-Hopf bifurcation, oscillatory associative memory, neural networks, smart matter I. Introduction. M ICROELECTROMECHANICAL systems (MEMS) are used to create miniature, highly accurate sensors and actuators which can gather non-electronic inform [...] ...|$|E
40|$|We {{present the}} {{implementation}} of on-line Hebbian learning for NESPINN, the <b>Neurocomputer</b> for the simulation of spiking neurons. In order to support various forms of Hebbian learning we developed a programmable weight unit for the NESPINN-system. On-line weight modifications are performed event-controlled in parallel to the computation of basic neuron functions. According to our VHDL-simulations, the system will offer a performance of up to 50 MCUPS. 1 Introduction This paper presents {{the implementation of}} on-line Hebbian learning for NESPINN, the <b>Neurocomputer</b> for the simulation of spiking neurons [1]. The NESPINN-system will enable real-time simulations of large-scale spike-processing neural networks. Learning for spiking neurons refers usually to Hebbian learning. Hebbian Learning is efficient and plausible for these biological-inspired model neurons. Furthermore, it is hardware-friendly and can be implemented as on-line learning with only local operations. Various formulations of [...] ...|$|E
40|$|We {{present the}} {{requirements}} for a <b>neurocomputer</b> for spike-processing neural networks. In a simulation study we investigated the performance of available hardware and showed, {{that there is still}} a need for a specific <b>neurocomputer</b> dedicated to the simulation of spike-processing networks. On the basis of our simulation study and an investigation of the features of spike-processing networks we analyses {{the requirements for}} the design of dedicated hardware. An efficient hardware architecture should contain an event-list module, a sender-oriented connection module and a number of fixed-point processing units. 1 Introduction Experimental results [1] [2] together with theoretical studies [3] [4] suggest that the time structure of neuronal spike trains is relevant in neuronal signal processing. The synchronized firing of neuronal assemblies could serve as a versatile and general mechanism for feature binding, pattern segmentation and figure/ground separation. This mechanism could also be u [...] ...|$|E
40|$|Neural network {{algorithms}} {{are excellent}} candidates for parallel implementation {{thanks to their}} high regularity. This is often supplemented by computation locality, low precision requirements and some tolerance to algorithmic modifications. These features make dedicated digital systems appropriate to support efficiently neural network applications. Many designs have been presented in the technical literature and a few commercial systems have appeared on the market. A few representative systems are presented and discussed to illustrate the different architectural solutions adopted. Designers of systems based on custom processing units often claim that these machines display supercomputer performances at prices comparable to top-range workstations. A framework for correct performance evaluation is reported to emphasize critical performance problems and to produce results that match the expectations of potential users. 1 Introduction This paper discusses some aspects of <b>neurocomputers,</b> a [...] ...|$|R
40|$|The article {{demonstrates}} the way artificial intelligence science {{has passed since}} the 1950 s up to now. The work concerns the important development stages of AI in Ukraine and the world. In particular, the work highlights the history of AI as the history of <b>neurocomputers,</b> heuristic programs and chess programs; history of biological cybernetics and evolution {{of the theory of}} pattern recognition and speech recognition. The article illustrates scientific contribution to this problem by native scientists N. M. Amosov, V. A. Kovalevsky, M. I. Schlesinger, T. K. Vintsiuk, M. M. Botvinnik, D. A. Pospelov and T. A. Taran. There are provided materials showing that the domestic reading machine «CHARS» was not inferior to similare of Western standards, while remaining economical. The author examines the development of «softwareinterlocutors » and expert systems...|$|R
40|$|A new {{methodology}} for {{the generation of}} efficient parallel programs from high-level neural network specifications is presented. All possible mappings of the neural network onto the parallel processors are generated and evaluated by using {{a description of the}} parallel target architecture. Thus the optimal mapping can be determined at compile-time and efficient parallel simulation code can be generated easily by transforming the neural network specification accordingly. Introduction Many neural network learning algorithms require a large number of iterations to converge towards good results. Often also a lot of critical parameters that are not known in advance (e. g. learning rate, number of neurons, initializations) must be selected experimentally. So all together an enourmous computation time is used and fast implementations on universal parallel computers (especially SIMD architectures like MasPar [1]) or <b>neurocomputers</b> (like CNAPS [2]) are required. Unfortunately, the programming o [...] ...|$|R
40|$|We {{take a new}} {{approach}} to the generation of Jacobi theta function identities. It is complementary to the procedure which makes use of the evaluation of Parseval-like identities for elementary cylindrically-symmetric functions on computer holograms. Our method is more simple and explicit than this one, which was an outcome of the construction of <b>neurocomputer</b> architectures through the Heisenberg model...|$|E
40|$|Neurons of the {{cortical}} tissue in a mammalian brain are connected {{in an extremely}} sparse and random fashion. The paper presents efficient methods for a parallel simulation of neural networks modeling this connection scheme on a CNAPS SIMD <b>neurocomputer.</b> Appropriate algorithms and data structures are introduced that allow for a minimal loss of parallelism during the computation of input scalar products. A `greedy' optimization procedure applied to the neuron [...] processor assignment is shown to gain a further reduction of computation time getting close to the lower bound. Using these methods, a considerable speedup in comparison to sequential computation is achieved. Keywords: cortical models, <b>neurocomputer,</b> parallel simulation, SIMD parallelism, sparse random networks, speed optimization 1. Introduction The complete (or at least regular) interconnection between or inside groups of neurons is characteristic {{for the vast majority}} of artificial neural networks. In real nervous systems, o [...] ...|$|E
40|$|A {{methodology}} for deriving neural network simulation programs on <b>neurocomputer</b> architectures with integer arithmetic units is presented. Special {{emphasis is placed}} on the analysis of fixed-point variables. It is shown how good fixed-point encodings of neural network variables can be determined automatically. 1 Introduction A lot of <b>neurocomputer</b> architectures have been proposed in the last 15 years. Many of them have been realized as research prototypes, some of them are also commercially available. They all offer the possibility for a high-speed simulation of neural networks. Especially the training of large neural networks can be accelerated by one or two orders of magnitude compared to the simulation on a workstation. However only {{a relatively small number of}} such systems has been installed. Besides of the high price this is also due to the lack of appropriate machine-independent software environments for the simulation of neural networks [1]. The user must perform the following [...] ...|$|E
40|$|Neural {{hardware}} {{has undergone}} rapid development {{during the last}} few years. This paper presents an overview of neural hardware projects within industries and academia. It describes digital, analog, and hybrid neurochips and accelerator boards as well as large-scale <b>neurocomputers</b> built from general purpose processors and communication elements. Special attention is given to multiprocessor projects that focus on scalability, flexibility, and adaptivity of the design and thus seem suitable for brain-style (cognitive) processing. The sources used for this overview are taken from journal papers, conference proceedings, data sheets, and ftp-sites and present an up-to-date overview of current state-of-the-art neural hardware implementations. 1 Categorization of neural hardware This paper presents an overview of time-multiplexed hardware designs, some of which are already commercially available, others representing design studies being carried out by research groups. A large number of design [...] ...|$|R
40|$|Recent {{developments}} in the interfacing of neurons with silicon chips may {{pave the way for}} progress in constructing scalable <b>neurocomputers.</b> The assembly of synthetic neuronal networks with predefined synaptic connections and controlled geometric structure has been realized experimentally within the last decade. Furthermore, when such neuronal networks are interfaced with semiconductors, action potentials in neurons of the network can be elicited by capacitative stimulators, and voltage measurements can be made by transistors incorporated into the associated silicon chip. Despite the impressive progress, such preliminary devices have not yet demonstrated the performance of useful computations, and constructing larger devices can be both expensive and time-consuming. Accordingly, an appropriate modeling framework with the capability to simulate current experimental results in such devices may be used to make useful predictions regarding their potential computational power. A proposed modeling framework for functional neuronal network...|$|R
40|$|Synchronization of {{oscillations}} is {{a phenomenon}} prevalent in natural, social, and engineering systems. Controlling synchronization of oscillating systems is motivated by {{a wide range of}} applications from neurological treatment of Parkinson's disease to the design of <b>neurocomputers.</b> In this article, we study the control of an ensemble of uncoupled neuron oscillators described by phase models. We examine controllability of such a neuron ensemble for various phase models and, furthermore, study the related optimal control problems. In particular, by employing Pontryagin's maximum principle, we analytically derive optimal controls for spiking single- and two-neuron systems, and analyze the applicability of the latter to an ensemble system. Finally, we present a robust computational method for optimal control of spiking neurons based on pseudospectral approximations. The methodology developed here is universal to the control of general nonlinear phase oscillators. Comment: 29 pages, 6 figure...|$|R
40|$|This paper {{presents}} {{a new concept}} for a parallel <b>neurocomputer</b> architecture {{which is based on}} a configurable neuroprocessor design. The neuroprocessor adapts its internal parallelism dynamically to the required data precision for achieving an optimal utilization of the available hardware resources. This is realized by encoding a variable number of p different data elements in one very long data word of b bits. All components of the neuroproccessor (multiplier, accumulator, adder, : : :) support the parallel execution of p operations on all data elements of one very long data word. 1 Introduction In recent years many <b>neurocomputer</b> architectures have been proposed [3]. Many of them are based on special-purpose processors (so-called neuroprocessors) which have been developped for the simulation of neural networks (e. g. [1], [4], [7]). In principle, all such systems can support many neural network models but due to several decisions of the processor design they are optimized only for o [...] ...|$|E
40|$|Abstract: We {{present the}} {{implementation}} of on-line Hebbian learning for NESPINN, the <b>Neurocomputer</b> for the simulation of spiking neurons. In order to support various forms of Hebbian learning we developed a programmable weight unit for the NESPINN-system. On-line weight modifications are performed event-controlled in parallel to the computation of basic neuron functions. According to our VHDL-simulations, the system will offer a performance of up to 50 MCUPS. ...|$|E
40|$|The {{capacity}} of classical neurocomputers {{is limited by}} the number of classical degrees of freedom which is roughly proportional {{to the size of the}} computer. By Contrast, a Hypothetical quantum <b>neurocomputer</b> can implement an exponentially large number of the degrees of freedom within the same size. In this paper an attempt is made to reconcile linear reversible structure of quantum evolution with nonlinear irreversible dynamics for neural nets...|$|E
40|$|This paper {{describes}} some of {{our recent}} work {{in the development of}} computer architectures for efficient execution of artificial neural network algorithms. Our earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme. We have used the RAP to simulate variable precision arithmetic and guide us in the design of higher performance <b>neurocomputers</b> based on custom VLSI. The RAP system {{played a critical role in}} this study, enabling us to experiment with much larger networks than would otherwise be possible. Our study shows that back-propagation training algorithms only require moderate precision. Specifically, 16 b weight values and 8 b output values are sufficient to achieve training and classification results comparable to 32 b floating point. Although these results were gathered for frame classification in continuous speech, we expect that they will extend to many other connectionist calculations. We have used [...] ...|$|R
40|$|The {{labelling}} {{of features}} by synchronization of spikes {{seems to be}} a very efficient encoding scheme for a visual system. Simulation of a vision system with millions of pulse-coded model neurons, however, is almost impossible on the base of available processors including parallel processors and <b>neurocomputers.</b> A "one-to-one" silicon implementation of pulse-coded model neurons suffers from communication problems and low flexibility. On the other hand, acceleration of the simulation algorithm of pulse-coded leaky integrator neurons has proved to be straightforward, flexible and very efficient. Thus we decided to develop an accelerator for a special version of the French and Stein neurons with modulatory inputs which are advantageous for simulation of synchronization mechanisms. Moreover, our accelerator also provides a Hebb'ian-like learning rule and supports adaptivity. Up to 128 K neurons with a total number of 16 M freely allocatable synapses are simulated within one system. The size of [...] ...|$|R
40|$|Substantial {{evidence}} indicates that the time structure of neuronal spike trains is relevant in neuronal signal processing. Bio-inspired spiking neural networks are taking these results into account. Applications of these networks to low vision problems, e. g. segmentation, requires that the simulation of large-scale networks must be performed in a reasonable time. On this basis, we investigated the achievable performance of existing hardware platforms for the simulation of spiking neural networks with sizes from 8 k neurons up to 512 k neurons/ 50 M synapses. We present results for workstations (Sparc-Ultra), digital signal processors (TMS-C 8 x), <b>neurocomputers</b> (CNAPS, SYNAPSE), small- and large-scale parallel-computers (4 xPentium, CM- 2, SP 2) and discuss the specific implementation issues. According to our investigation, only supercomputers like CM- 2 can match the performance requirements for the simulation of very large-scale spiking neural networks. Therefore, {{there is still the}} need for [...] ...|$|R
