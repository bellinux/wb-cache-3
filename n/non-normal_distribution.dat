266|366|Public
50|$|<b>Non-normal</b> <b>distribution</b> for errors: in the {{simplest}} cases a generalised linear model might be applicable.|$|E
5000|$|The {{constant}} factor 3 in {{the definition}} of the Z-factor is motivated by the normal distribution, for which more than 99% of values occur within 3 standard deviations of the mean. If the data follow a strongly <b>non-normal</b> <b>distribution,</b> the reference points (e.g. the meaning of a negative value) may be misleading. Another issue is that the usual estimates of the mean and standard deviation are not robust; accordingly many users in the high-throughput screening community prefer [...] "Robust Z-prime". Extreme values (outliers) in either the positive or negative controls can adversely affect the Z-factor, potentially leading to an apparently unfavorable Z-factor even when the assay would perform well in actual screening.In addition, the application of the single Z-factor-based criterion to two or more positive controls with different strengths in the same assay will lead to misleading results [...] The absolute sign in the Z-factor makes it inconvenient to derive the statistical inference of Z-factor mathematically ...|$|E
5000|$|It is {{important}} to distinguish between downside and upside risk because security distributions are non-normal and non-symmetrical. This {{is in contrast to}} what the capital asset pricing model (CAPM) assumes: that security distributions are symmetrical, and thus that downside and upside betas for an asset are the same. Since investment returns tend to have a <b>non-normal</b> <b>distribution,</b> however, there in fact tend to be different probabilities for losses than for gains. The probability of losses is reflected in the downside risk of an investment, or the lower portion of the distribution of returns. The CAPM, however, includes both halves of a distribution in its calculation of risk. Because of this {{it has been argued that}} it is crucial to not simply rely upon the CAPM, but rather to distinguish between the downside risk, which is the risk concerning the extent of losses, and upside risk, or risk concerning the extent of gains. Studies indicate that [...] "around two-thirds of the time standard beta would underestimate the downside risk." ...|$|E
40|$| for a {{wide variety}} of <b>non-normal</b> <b>distributions,</b> {{especially}} skewed|$|R
5000|$|For <b>non-normal</b> <b>distributions</b> an {{approximate}} (up to O(n−1) terms) formula for the unbiased estimator {{of the standard}} deviation is ...|$|R
40|$|Power method polynomials {{are used}} for {{simulating}} <b>non-normal</b> <b>distributions</b> with specified product moments or L-moments. The power method is capable of producing distributions with extreme values of skew (L-skew) and kurtosis (L-kurtosis). However, these distributions can be extremely peaked and thus not representative of real-world data. To obviate this problem, two families of distributions are introduced based on a doubling technique with symmetric standard normal and logistic power method distributions. The primary focus of the methodology is {{in the context of}} L-moment theory. As such, L-moment based systems of equations are derived for simulating univariate and multivariate <b>non-normal</b> <b>distributions</b> with specified values of L-skew, L-kurtosis, and L-correlation. Evaluation of the proposed doubling technique indicates that estimates of L-skew, L-kurtosis, and L-correlation are superior to conventional product-moments in terms of relative bias and relative efficiency when extreme <b>non-normal</b> <b>distributions</b> are of concern...|$|R
30|$|Descriptive {{statistics}} {{are presented as}} mean ± standard deviation (SD) for normally distributed values and as median (with interquartile range) in case of <b>non-normal</b> <b>distribution.</b> Categorical variables were compared using the chi-squared test, while continuous variables were compared using Student's t test or Mann Whitney U test in case of <b>non-normal</b> <b>distribution.</b>|$|E
40|$|This study compares {{parametric}} and nonparametric quantile regression methods using Monte Carlo simulations. Simulation {{results indicate}} that the nonparametric quantile regression approach is more appropriate, particularly when the underlying model is nonlinear or the error term follows a <b>non-normal</b> <b>distribution.</b> ...|$|E
30|$|All {{statistical}} {{analyses were performed}} using MedCalc Statistical Software version 12.7. 2 (MedCalc Software, Ostend, Belgium). Considering the small number of included patients, <b>non-normal</b> <b>distribution</b> of the results was assumed. All results are given as median [25 th and 75 th percentile].|$|E
40|$|Many {{distributions}} of multivariate {{data in the}} real world follow a <b>non-normal</b> model with <b>distributions</b> being skewed and/or heavy tailed. In studies in which multivariate <b>non-normal</b> <b>distributions</b> are needed, it is important for simulations of those variables to provide data that is close to the desired parameters while also being fast and easy to perform. Three algorithms for generating multivariate <b>non-normal</b> <b>distributions</b> are reviewed for accuracy, speed and simplicity. They are the Fleishman Power Method, the Fifth-Order Polynomial Transformation Method, and the Generalized Lambda Distribution Method. Simulations were run in order to compare the three methods by how well they generate bivariate distributions with the desired means, variances, skewness, kurtoses, and correlation, simplicity of the algorithms, and how quickly the desired distributions were calculated...|$|R
30|$|GLMs are {{extensions}} of linear regression models that {{can deal with}} dependent variables that follow <b>non-normal</b> <b>distributions</b> (McCullagh and Nelder 1989). For binomial distributions, logistic regression is most frequently used, and probit regression is also often used in natural hazard modeling.|$|R
30|$|Lachenbruch et al. (1977) {{studied the}} {{performance}} of the QDF under non-normality. They generated random samples from <b>non-normal</b> <b>distributions</b> and the samples were transformed into components by using Johnson’s system of transformation. Among their findings, they found that, the overall sample standard deviation, the between sample variability of the individual error rates of the function (QDF) under normal or <b>non-normal</b> <b>distributions</b> was quite large. In the computation of the overall sample standard deviation, the between sample variability of the individual error rates in the QDF on normal or <b>non-normal</b> <b>distributions</b> was quite large and for that instability of QDF is pronounced. Also the actual error rates were considerably larger than the optimal rates in the case of zero mean difference (this is a very difficult problem in assignment). The QDF for non-normal samples generally did not do substantially worse than when the QDF was derived under normal samples which were obtained after transformation. Lachenbruch et al. (1977) compared the re-substitution method and the leave-one-out method. The re-substitution method had an unacceptably high bias. The leave-one-out method was far superior in respect of generally having a far lesser bias.|$|R
40|$|For the {{presence}} of <b>non-normal</b> <b>distribution</b> characteristics in the financial assets returns, the model of AR(1) -GJR(1, 1) is used to characterize the marginal distribution of the style assets in China stock market. The Copula function is introduced to analyze the dependency structure between the six style assets, combined with the marginal distributed residual sequences. And the joint return distribution of the style portfolios is simulated, combined with extreme value theory and Monte Carlo simulation method. Then the market risks (VaR and CVaR) of the style portfolios in China stock markets are obtained. The {{results of the study}} show that the generalized Pareto distribution Model can well fit the <b>non-normal</b> <b>distribution</b> characteristics such as peak and fat tail in the style assets returns...|$|E
40|$|A {{new method}} {{to conduct a}} right-tailed test for the {{correlation}} on bivariate <b>non-normal</b> <b>distribution</b> is proposed. The comparative simulation study shows that the new test controls the type I error rates well for all the distributions considered. An investigation of the power performance is also provided...|$|E
40|$|This paper derives {{the exact}} {{distribution}} and {{moments of the}} Stein-ruleestimator in a model where the disturbances follow a <b>non-normal</b> <b>distribution</b> of the Edgeworth or Gram-Charlier type. The results are achieved by combining the approach of Davis [4] for examining non-normality with the fractional calculus techniques of Phillips [11]. ...|$|E
40|$|Simulation {{techniques}} {{must be able}} {{to generate}} the types of distributions most commonly encountered in real data, for example, <b>non-normal</b> <b>distributions.</b> Two recognized procedures for generating non-normal data are Fleishman's linear transformation method and the method proposed by Ramberg et al. that is based on generalization of the Tukey lambda distribution. This study compares these procedures in terms {{of the extent to which}} the distributions they generate fit their respective theoretical models, and it also examines the number of simulations needed to achieve this fit. To this end, the paper considers, in addition to the normal distribution, a series of <b>non-normal</b> <b>distributions</b> that are commonly found in real data, and then analyses fit according to the extent to which normality is violated and the number of simulations performed. The results show that the two data generation procedures behave similarly. As the degree of contamination of the theoretical distribution increases, so does the number of simulations required to ensure a good fit to the generated data. The two procedures generate more accurate normal and <b>non-normal</b> <b>distributions</b> when at least 7000 simulations are performed, although when the degree of contamination is severe (with values of skewness and kurtosis of 2 and 6, respectively) it is advisable to perform 15000 simulations...|$|R
50|$|Bartlett's test is {{sensitive}} to departures from normality. That is, if the samples come from <b>non-normal</b> <b>distributions,</b> then Bartlett's test may simply be testing for non-normality. Levene's test and the Brown-Forsythe test are alternatives to the Bartlett test that are less sensitive to departures from normality.|$|R
40|$|Analysis of {{covariance}} techniques {{have been developed}} primarily for normally distributed errors. We give solutions when the errors have <b>non-normal</b> <b>distributions.</b> We show that our solutions are efficient and robust. We provide a real-life example. Copyright (c) 2009 The Authors. Journal compilation (c) 2009 International Statistical Institute. ...|$|R
30|$|Once {{the number}} of factors for {{extraction}} had been decided, the factor structure was explored using a forced three-factor Principal Axis Factoring (PAF) with oblique rotation (to allow for potential correlation between the factors). PAF was chosen as an extraction method due to the <b>non-normal</b> <b>distribution</b> of the data [27].|$|E
30|$|SPSS version 21.0 {{was used}} for {{statistical}} analysis. Due to <b>non-normal</b> <b>distribution</b> of the data, all numerical data were expressed as median (25 th - 75 th percentile). Mann-Whitney U test, Wilcoxon’s signed-rank test, Chi square test and partial correlation were used as needed, and p < 0.05 was considered significant.|$|E
30|$|The Kolmogorov-Smirnov {{statistical}} {{test was used}} to determine the <b>non-normal</b> <b>distribution</b> of the mean imprecision, using the median as a measure of central tendency and the interquartile interval as an expression of its distribution. The Kruskal-Wallis H test (p <  0.05) was applied in cases of an imprecision of tooth/movement combination whose mean was different to the others.|$|E
40|$|This article obtains {{a general}} formula {{to find the}} {{correlation}} coefficient between the sample mean and variance. Several particular results for major <b>non-normal</b> <b>distributions</b> are extracted to help students in classroom, clients during statistical consulting service. Key words: Skewness, kurtosis, non-normal data, count and continuous distributions...|$|R
40|$|Reaction {{times were}} {{simulated}} for examining {{the power of}} six methods for multiple testing, {{as a function of}} sample size and departures from normality. Power estimates were low for all methods for <b>non-normal</b> <b>distributions.</b> With normal distributions, even for small sample sizes, satisfactory power estimates were observed, especially for FDR-based procedures...|$|R
40|$|The {{conventional}} power method transformation is a moment-matching technique that simulates <b>non-normal</b> <b>distributions</b> with controlled measures of skew and kurtosis. The percentile-based power method {{is an alternative}} that uses the percentiles of a distribution in lieu of moments. This article presents a SAS/IML macro that implements the percentile-based power method...|$|R
30|$|Descriptive {{data are}} {{presented}} as means and percentages. Given the small sample size, a <b>non-normal</b> <b>distribution</b> was assumed. Means were compared using the Mann-Whitney U test, while categorical data was compared using chi-squared analysis with Fischer’s exact P values calculated. Epi Info™ 3.5. 3 (Center for Disease Control, Atlanta, GA, USA) {{was used in the}} data analysis.|$|E
40|$|Abstract: Laha {{distribution}} {{has been}} introduced in 1958 {{as an example of}} a <b>non-normal</b> <b>distribution</b> where the quotient follows the Cauchy law. In this paper we present two procedures for the computer generation of this distribution and we discuss its applications to life time modelling. Key Words: Computer generation, composition procedure, ratio of uniforms method, life time...|$|E
30|$|Statistical {{analysis}} was performed using SigmaStat software. Results are presented as the mean[*]±[*]standard deviation (SD) for variables with a normal distribution, or the median (interquartile range) for variables with a <b>non-normal</b> <b>distribution.</b> Comparisons were performed using Student’s t test or Mann–Whitney’s U test, as appropriate. Categorical variables were compared with Fisher’s exact test. A p value less than 0.05 was considered statistically significant.|$|E
30|$|Pleasure, arousal, and {{dominance}} scores {{across the}} situations were analyzed with three separate maximum likelihood mixed effects linear regression models as implemented through SPSS’s Generalized Linear Mixed Models (GLMM; α[*]=[*]. 017). GLMM represents a special class of regression model, which allows both random and fixed effects to be modeled; and can model outcome variables with <b>non-normal</b> <b>distributions.</b>|$|R
50|$|Disadvantages: Critics voice {{concerns}} {{that such a}} standardization can be misleading. Since standardizing a variable removes the unit of measurement from its value, a standardized coefficient for a given relationship only represents its strength relative to the variation in the distributions. This invites bias due to sampling error when one standardizes variables using {{means and standard deviations}} based on small samples. Furthermore, a change of one standard deviation in one variable is only equivalent to a change of one standard deviation in another predictor insofar as the shapes of the two variables' distributions resemble one another. The meaning of a standard deviation may vary markedly between <b>non-normal</b> <b>distributions</b> (e.g., when skewed or otherwise asymmetrical). This underscores the importance of normality assumptions in parametric statistics, and poses an additional problem when interpreting standardized coefficient estimates that even nonparametric regression does not solve when dealing with <b>non-normal</b> <b>distributions.</b>|$|R
40|$|Many {{inferential}} statistical tests {{require that}} the observed variables have a normal distribution. Monte Carlo simulations are used to investigate the effect of violating this assumption and require an algorithm that generates samples from <b>non-normal</b> <b>distributions,</b> thereby controlling correlations among random variables, the marginal distributions, and the multivariate distribution. Most previously used algorithms only allow control over the correlations and the marginals, but recent {{results show that the}} robustness of certain methods depends on the multivariate distribution as well. In my thesis, I suggest a new method to generate samples from <b>non-normal</b> <b>distributions</b> that allows manipulations of all three parameters simultaneously. The algorithm jointly controls the correlation matrix, central moments of the marginals, and the multivariate distribution. Additionally, I also show that the multivariate distribution has a distinct impact on the robustness of a structural equation model, whereas extraction criteria for exploratory factor analysis are unaffected by the underlying distribution...|$|R
40|$|In the {{analysis}} of compositional data, zero components or a <b>non-normal</b> <b>distribution</b> of the log-ratios severely limits {{the analysis}} that can be done. Ranking across both cases and components, before scaling, is suggested as a possible new approach. The idea is illustrated with a data set which compares the results with other methods including replacement of Os by small positive quantities and other ranking methods. link_to_subscribed_fulltex...|$|E
40|$|Statistical {{analysis}} {{is an important}} and responsible stage in ecological research, often posing a challenge. One of the main problems is that the statistical samples typical of ecological research frequently include data that violate the original assumptions of standard analytical tools. A large array of tests has been designed for data of different charac-teristics (<b>non-normal</b> <b>distribution,</b> unequal variance). Yet, recommendations found in textbooks (Snedecor and Co...|$|E
30|$|To {{show the}} status of {{correlations}} between the variables in this study, we used Chi-square test. Here, Chi-square as a nonparametric test was used instead of parametric tests such as independent t test. One {{reason for this is}} the fact that the variances between the two comparison groups were different. Also in this study we were dealing with <b>non-normal</b> <b>distribution</b> of data, and more importantly the variables were all nominal.|$|E
40|$|Non-normal {{variation}} across repeated measurements {{leads to}} nonlinear and heteroskedastic regression to the mean unlike the simple linear and homoskedastic regression to the mean found in normal models. This paper investigates {{the nature of}} the regression to the mean phenomenon in non-normal settings using (a) small variance approximations and (b) exact results obtained using normal mixtures to approximate <b>non-normal</b> <b>distributions.</b> ...|$|R
40|$|This article {{examines}} the effects of non-normality as measured by skewness and provides an alternative method of designing individuals control chart with <b>non-normal</b> <b>distributions.</b> A skewness correction method for constructing the individuals control chart is provided. An example of thickness of biscuit process is presented to illustrate the individuals control chart limits. Key words: Quality control, shewhart control charts (S), skewness correction (SC) ...|$|R
40|$|As is well known, process {{capability}} analysis {{for more than}} one quality variables is a complicated and sometimes contentious area with several quality measures vying for recognition. When these variables exhibit non-normal characteristics, the situation becomes even more complex. The aim {{of this paper is to}} measure Process Capability Indices (PCIs) for bivariate non-normal process using the bivariate Burr distribution. The univariate Burr distribution has been shown to improve the accuracy of estimates of PCIs for univariate <b>non-normal</b> <b>distributions</b> (see for example, [7] and [16]). Here, we will estimate the PCIs of bivariate <b>non-normal</b> <b>distributions</b> using the bivariate Burr distribution. The process of obtaining these PCIs will be accomplished in a series of steps involving estimating the unknown parameters of the process using maximum likelihood estimation coupled with simulated annealing. Finally, the Proportion of Non-Conformance (PNC) obtained using this method will be compared with those obtained from variables distributed under the bivariate Beta, Weibull, Gamma and Weibull-Gamma distributions...|$|R
