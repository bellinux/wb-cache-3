175|815|Public
5000|$|Therefore, the {{minimizing}} vector [...] is {{a solution}} of the <b>normal</b> <b>equation</b> ...|$|E
50|$|The Unified WGS Solution, {{as stated}} above, was a {{solution}} for geodetic positions and associated parameters of the gravitational field based on an optimum combination of available data. The WGS 72 ellipsoid parameters, datum shifts and other associated constants were derived separately. For the unified solution, a <b>normal</b> <b>equation</b> matrix was formed based {{on each of the}} mentioned data sets. Then, the individual <b>normal</b> <b>equation</b> matrices were combined and the resultant matrix solved to obtain the positions and the parameters.|$|E
5000|$|This {{expression}} {{is known as}} the <b>Normal</b> <b>Equation</b> and gives us a possible solution to the inverse problem. It is equivalent to Ordinary Least Squares ...|$|E
50|$|The {{conjugate}} gradient method {{can be applied}} to an arbitrary n-by-m matrix by applying it to <b>normal</b> <b>equations</b> ATA and right-hand side vector ATb, since ATA is a symmetric positive-semidefinite matrix for any A. The result is {{conjugate gradient}} on the <b>normal</b> <b>equations</b> (CGNR).|$|R
40|$|SOLVE II program {{combines}} {{any number}} of sets of <b>normal</b> <b>equations</b> and obtains solution vector and related statistics. <b>Normal</b> <b>equations</b> of square, nonnegative definite matrix form. Program utilizes only upper symmetric portion of matrix. Program uses partitioned Cholesky decomposition method for matrix inversion to accommodate large parameter systems...|$|R
5000|$|... #Subtitle level 2: Conjugate {{gradient}} on the <b>normal</b> <b>equations</b> ...|$|R
5000|$|In Germany, the {{notation}} of a <b>normal</b> <b>equation</b> {{is used for}} dividend, divisor and quotient (cf. first section of Latin American countries above, where it's done virtually the same way): ...|$|E
50|$|If {{the problem}} is linear we can use some type of matrix inverse method—often {{the problem is}} ill-posed or {{unstable}} so {{we will need to}} regularize it: good, simple methods include the <b>normal</b> <b>equation</b> or singular value decomposition. If the problem is weakly nonlinear, an iterative method such Newton-Raphson may be appropriate.|$|E
5000|$|An {{alternative}} to fitting m data points {{by a simple}} polynomial in the subsidiary variable, z, is to use orthogonal polynomials. where P0 [...]. Pk {{is a set of}} mutually orthogonal polynomials of degree 0 [...]. k. Full details on how to obtain expressions for the orthogonal polynomials and the relationship between the coefficients b and a are given by Guest. [...] Expressions for the convolution coefficients are easily obtained because the normal equations matrix, JTJ, is a diagonal matrix as the product of any two orthogonal polynomials is zero by virtue of their mutual orthogonality. Therefore, each non-zero element of its inverse is simply the reciprocal the corresponding element in the <b>normal</b> <b>equation</b> matrix. The calculation is further simplified by using recursion to build orthogonal Gram polynomials. The whole calculation can be coded in a few lines of PASCAL, a computer language well-adapted for calculations involving recursion.|$|E
5000|$|... #Subtitle level 3: Inverting {{the matrix}} of the <b>normal</b> <b>equations</b> ...|$|R
40|$|AbstractWe {{describe}} a direct method for solving sparse linear least squares problems. The storage {{required for the}} method {{is no more than}} that needed for the conventional <b>normal</b> <b>equations</b> approach. However, the <b>normal</b> <b>equations</b> are not computed; orthogonal transformations are applied to the coefficient matrix, thus avoiding the potential numerical instability associated with computing the <b>normal</b> <b>equations.</b> Our approach allows full exploitation of sparsity, and permits the use of a fixed (static) data structure during the numerical computation. Finally, the method processes the coefficient matrix one row at a time, allowing for the convenient use of auxiliary storage and updating operations...|$|R
5000|$|The {{algebraic}} {{solution of}} the <b>normal</b> <b>equations</b> can be written as ...|$|R
40|$|AbstractSelecting {{the optimal}} {{reference}} satellite {{is an important}} component of high-precision relative positioning because the reference satellite directly influences the strength of the <b>normal</b> <b>equation.</b> The reference satellite selection methods based on elevation and positional dilution of precision (PDOP) value were compared. Results show that all the above methods cannot select the optimal reference satellite. We introduce condition number of the design matrix in the reference satellite selection method to improve structure of the <b>normal</b> <b>equation,</b> because condition number can indicate the ill condition of the <b>normal</b> <b>equation.</b> The experimental results show that the new method can improve positioning accuracy and reliability in precise relative positioning...|$|E
40|$|Practical {{measurement}} schemes require {{redundant observations}} for quality control and errors checking. This led to inconsistent solution where every subset (minimum required data) gives different results. Least Square Estimation (LSE) {{is a method}} to provide a unique solution (of the <b>normal</b> <b>equation)</b> from redundant observations by minimizing the sum of squares of the residuals. Analysis of LSE also provide estimate quality of parameters, observations and residuals, assessment of network’s reliability and precision, detection of gross errors etc. Many methods {{can be applied to}} solve <b>normal</b> <b>equation,</b> e. g. Gauss-Doolittle, Gauss-Jordan Elimination, Singular Value Decomposition, Iterative Jacoby etc. Cholesky Decomposition is an efficient method to solve <b>normal</b> <b>equation</b> with positive definite and symmetric coefficient matrix. It is also capable of detecting weak condition 1 of the system. Solving large <b>normal</b> <b>equation</b> will require a lot of times and computer memory. Implementation of sparse matrix in Cholesky Decomposition will speed up the execution times and minimize the memory usage by exploiting the zeros and symmetrical of coefficient matrix. This paper discusses the procedures and benefits of implementing sparse matrix in Cholesky Decomposition. Some preliminary results are also included...|$|E
40|$|Abstract. Solving {{a system}} of linear {{equations}} by its <b>normal</b> <b>equation</b> usually is highly unrecommended because this approach worsens the condition number and inflates the computational cost. For linear systems whose unknowns are matrices, such as the Sylvester equation, Lyapunov equation, Stein equation, and a variate of their generalizations, the formulation of the corresponding <b>normal</b> <b>equation</b> {{in the sense of}} tensor operators offers a common structure via gradient dynamics. This paper explains the setting of this framework and demonstrates its versatility by one simple ODE integrator that can handle almost all these types of problems...|$|E
5000|$|... the {{solution}} {{of which can be}} written with the <b>normal</b> <b>equations,</b> ...|$|R
5000|$|... which, on rearrangement, become n {{simultaneous}} linear <b>equations,</b> the <b>normal</b> <b>equations</b> ...|$|R
2500|$|... and {{the best}} fit can be found by solving the <b>normal</b> <b>equations.</b>|$|R
40|$|We study {{numerical}} stability for interior-point methods {{applied to}} Linear Programming, LP, and Semidefinite Programming, SDP. We analyze the difficulties inherent in current methods and present robust algorithms. We {{start with the}} error bound analysis of the search directions for the <b>normal</b> <b>equation</b> approach for LP. Our error analysis explains the surprising fact that the ill-conditioning is not a significant problem for the <b>normal</b> <b>equation</b> system. We also explain why most of the popular LP solvers have a default stop tolerance of only 10 - 8 when the machine precision on a 32 -bit computer is approximately 10 - 16. We then propose a simple alternative approach for the <b>normal</b> <b>equation</b> based interior-point method. This approach has better numerical stability than the <b>normal</b> <b>equation</b> based method. Although, our approach is not competitive in terms of CPU time for the NETLIB problem set, we do obtain higher accuracy. In addition, we obtain significantly smaller CPU times compared to the <b>normal</b> <b>equation</b> based direct solver, when we solve well-conditioned, huge, and sparse problems by using our iterative based linear solver. Additional techniques discussed are: crossover; purification step; and no backtracking. Finally, we present an algorithm to construct SDP problem instances with prescribed strict complementarity gaps. We then introduce two measures of strict complementarity gaps. We empirically show that: (i) these measures can be evaluated accurately; (ii) {{the size of the}} strict complementarity gaps correlate well with the number of iteration for the SDPT 3 solver, {{as well as with the}} local asymptotic convergence rate; and (iii) large strict complementarity gaps, coupled with the failure of Slater's condition, correlate well with loss of accuracy in the solutions. In addition, the numerical tests show that there is no correlation between the strict complementarity gaps and the geometrical measure used in [31], or with Renegar's condition number...|$|E
40|$|AbstractThis {{paper is}} {{concerned}} with the solution of systems of linear equations TNXN = bN, where ∗TN∗NϵN denotes a sequence of nonsingular nonsymmetric Toeplitz matrices arising from a generating function of the Wiener class. We present a technique for the fast construction of optimal trigonometric preconditioners MN = MN(T′NTN) of the corresponding <b>normal</b> <b>equation</b> which can be extended to Toeplitz least squares problems in a straightforward way. Moreover, we prove that the spectrum of the preconditioned matrix MN 1 T′NTN is clustered at 1 such that the PCG-method applied to the <b>normal</b> <b>equation</b> converges superlinearly. Numerical tests confirm the theoretical expectations...|$|E
40|$|This {{paper is}} {{concerned}} with the solution of systems of linear equations T N xN = bN, where fT N gN 2 IN denotes a sequence of nonsingular nonsymmetricToeplitz matrices arising from a generating function of the Wiener class. We present a technique for the fast construction of optimal trigonometric preconditioners M N = M N (T 0 N T N) of the corresponding <b>normal</b> <b>equation</b> which can be extended to Toeplitz least squares problems in a straightforward way. Moreover, we prove that the spectrum of the preconditioned matrix M Γ 1 N T 0 N T N is clustered at 1 such that the PCG-method applied to the <b>normal</b> <b>equation</b> converges superlinearly. Numerical tests confirm the theoretical expectations. 1991 Mathematics Subject Classification. 65 F 10, 65 F 15, 65 T 10. Key words and phrases. Toeplitz matrix, Krylov space methods, CG-method, preconditioners, <b>normal</b> <b>equation,</b> clusters of eigenvalues. 1 Introduction Consider the system of linear equations T N xN = bN; (1. 1) where T N 2 IR N;N [...] ...|$|E
2500|$|... which, in {{a linear}} least squares system give the {{modified}} <b>normal</b> <b>equations,</b> ...|$|R
5000|$|... and {{the best}} fit can be found by solving the <b>normal</b> <b>equations.</b>|$|R
30|$|In active set methods, {{solving the}} core {{unconstrained}} least-squares problem is independent from {{the construction of}} the active set. To accelerate the core unconstrained least-squares solver, Myre et al. (2018) developed the TNT algorithm. The TNT algorithm implements the Cholesky factor of the <b>normal</b> <b>equations</b> as a preconditioner to a left-preconditioned conjugate gradient normal residual (PCGNR) method (Saad 2003). PCGNR {{can be thought of as}} a computationally cheap mechanism that iteratively improves the solution. The <b>normal</b> <b>equations</b> are explicitly formed to create the preconditioner for CGNR. The numerical issues typically associated with the <b>normal</b> <b>equations</b> and the condition number of the problem are thereby avoided.|$|R
40|$|A {{generalized}} numerical wave-front reconstruction {{method is}} proposed that {{is suitable for}} diversified irregular pupil shapes of optical systems to be measured. That is, to make a generalized and regular <b>normal</b> <b>equation</b> set, the test domain is extended to a regular square shape. The compatibility of this method is discussed in detail, and efficient algorithms (such as the Cholesky method) for solving this <b>normal</b> <b>equation</b> set are given. In addition, the authors give strict analyses of not only the error propagation in the wave-front estimate {{but also of the}} discretization errors of this domain extension algorithm. Finally, some application examples are given to demonstrate this algorithm...|$|E
40|$|The {{purpose of}} this paper is to develop a new method of {{estimating}} productive inefficiency of frontier production functions. A composed additive disturbance term, as the sum of symmetric and non-positive random variables, is specified. <b>Normal</b> <b>equation</b> is derived to estimate the coefficient and inefficiency measures...|$|E
30|$|It {{has been}} shown that a very similar network {{configuration}} can make a huge difference in the estimated parameters. For many regional networks, either the latitude or the longitude of the Euler pole is nearly collinear with the magnitude of the rotation rate, such that an iterative solution does not converge in the direct estimation of the Euler pole parameters. In the given example, the latitude of the Euler pole is highly correlated with the rotation rate since the Euler pole is almost in the south of the Anatolian plate. The <b>normal</b> <b>equation</b> matrix {{is a function of the}} geometry of the distribution of the network, and the condition number of the <b>normal</b> <b>equation</b> matrix is an indication of how well-posed the problem is. Ill-posed problems are generally identified by the high condition numbers of the <b>normal</b> <b>equation</b> matrix (Hansen, 2010). For instance in the given examples, while the condition number of the <b>normal</b> <b>equation</b> matrix in a direct estimation of the Euler pole parameters is about ~ 1014, it decreases to ~ 103 in estimating the Cartesian Euler vector, which is still very large and poses a weakly multicollinear problem. The contribution of the proposed method depends on the quality of the network distribution and the a priori values. For a nearly ideal network, the contribution could be negligible. The distribution of errors for the sites, and the geometry of the sites, determine the performance of the proposed method. The performance of the proposed method, when using different a priori values, is also dependent on the network geometry. The same a priori values used in three networks affected the performance differently due to the different network configurations and error distribution. However, using the regularization constant, the proposed method can always be tuned to give better MSE, or at least, the same MSE with the standard least-squares.|$|E
5000|$|The {{solution}} of the <b>normal</b> <b>equations</b> yields the vector [...] of the optimal parameter values.|$|R
30|$|Solution from <b>normal</b> <b>equations</b> {{can have}} round-off errors so QR {{decomposition}} of matrix X is done.|$|R
5000|$|The {{least squares}} {{parameter}} estimates are obtained from [...] <b>normal</b> <b>equations.</b> The residual {{can be written}} as ...|$|R
40|$|In this paper, the {{conjugate}} gradient (CG) algorithm is modified using the RLS <b>normal</b> <b>equation</b> and new data windowing scheme. It {{is known that}} CG algorithm has fast convergence rate and numerical stability. However, the existing CG algorithms still suffer from either slow convergence or high misadjustment compared with the RLS algorithm. In this paper, the parameter beta for CG algorithm is redesigned from the RLS <b>normal</b> <b>equation</b> and a general data windowing scheme reusing the data inputs is presented to solve these problems. The optimal property of parameter alpha is also analyzed using the control Lyapunov function (CLF) of the square deviation of weight error vector. The superior performance of the proposed algorithms over the RLS algorithm and the other existing CG algorithms is demonstrated by computer simulations...|$|E
40|$|In {{this paper}} we propose a fast {{algorithm}} for the least squares solution of overdetermined Toeplitz linear systems. The resolvent operator of <b>normal</b> <b>equation</b> is approximated as the Schur complement of a suitable augmented matrix. Numerical results are reported to highlight the performance of our algorithm with respect to other direct methods...|$|E
40|$|Bundle {{adjustment}} {{with additional}} parameters {{is identified as}} a critical step for precise orthoimage generation and 3 D reconstruction of Dunhuang wall paintings. Due {{to the introduction of}} self-calibration parameters and quasi-planar constraints, the structure of coefficient matrix of the reduced <b>normal</b> <b>equation</b> is banded-bordered, making the solving process of bundle adjustment complex. In this paper, Conjugate Gradient Bundle Adjustment (CGBA) method is deduced by calculus of variations. A preconditioning method based on improved incomplete Cholesky factorization is adopt to reduce the condition number of coefficient matrix, as well as to accelerate the iteration rate of CGBA. Both theoretical analysis and experimental results comparison with conventional method indicate that, the proposed method can effectively conquer the ill-conditioned problem of <b>normal</b> <b>equation</b> and improve the calculation efficiency of bundle adjustment with additional parameters considerably, while maintaining the actual accuracy...|$|E
25|$|For {{non-linear}} {{least squares}} systems a similar argument {{shows that the}} <b>normal</b> <b>equations</b> should be modified as follows.|$|R
2500|$|For {{non-linear}} systems similar reasoning {{shows that}} the <b>normal</b> <b>equations</b> for an iteration cycle can be written as ...|$|R
50|$|For {{non-linear}} {{least squares}} systems a similar argument {{shows that the}} <b>normal</b> <b>equations</b> should be modified as follows.|$|R
