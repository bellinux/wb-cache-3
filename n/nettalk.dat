60|1|Public
25|$|Cellular and {{landline}} {{phone service}} suffered major disruptions {{in the affected}} area. Immediately after the earthquake cellular communication was jammed across much of Japan due to a surge of network activity. On {{the day of the}} quake, American broadcaster NPR was unable to reach anyone in Sendai with working phone or Internet. Internet services were largely unaffected in areas where basic infrastructure remained, despite the earthquake having damaged portions of several undersea cable systems landing in the affected regions; these systems were able to reroute around affected segments onto redundant links. Within Japan, only a few websites were initially unreachable. Several Wi-Fi hotspot providers reacted to the quake by providing free access to their networks, and some American telecommunications and VoIP companies such as AT, Sprint, Verizon, T-Mobile and VoIP companies such as <b>netTALK</b> and Vonage have offered free calls to (and in some cases, from) Japan for a limited time, as did Germany's Deutsche Telekom.|$|E
50|$|<b>Nettalk</b> {{can manage}} {{multiple}} connections, {{the preferences of}} which are stored individually. This includes NickServ identities and visited channels. <b>Nettalk</b> completes commands, nicks and parameters automatically. Command syntax for typed in commands are displayed to the user like in modern software development environments. Commands can be searched for by their function, {{as well as by}} name. Also, there is the ability to show private messages in full-screen applications, and a spell checker for German and English. <b>Nettalk</b> supports mouse gestures since version 6.5.|$|E
5000|$|Anastasios [...] "Takis" [...] Kyriakides (born December 17, 1946) is a Greek-American businessman, {{inventor}} {{and founder}} {{and chief executive}} officer of <b>netTALK</b> Inc. Kyriakides holds several patents [...] and has founded a number of companies, including Lexicon Corporation, Mylex Corporation, Regency Cruise Lines [...] and <b>netTALK.</b>|$|E
50|$|The {{program also}} {{contains}} a script function. <b>Nettalk's</b> scripting uses a BASIC dialect, which is supported by syntax highlighting.|$|R
50|$|<b>NETtalk</b> is an {{artificial}} neural network. It {{is the result}} of research carried out in the mid-1980s by Terrence Sejnowski and Charles Rosenberg. The intent behind <b>NETtalk</b> was to construct simplified models that might shed light on the complexity of learning human level cognitive tasks, and their implementation as a connectionist model that could also learn to perform a comparable task.|$|E
5000|$|On September 21, 2012, NetTalk.com, Inc. filed a {{complaint}} in United States District Court For the Southern District of Florida, Civil Action No.: 9:12-cv-81022-CIV-MIDDLEBROOK/BRANNON, against MAGICJACK VOCALTECLTD, MAGIJACK HOLDINGS CORPORATION f/k/a YMAX HOLDINGS CORPORATION and DANIEL BORISLOW. In the complaint, <b>netTalk</b> alleges patent infringement by the defendants, seeking injunctive relief and damages of two hundred million dollars ($200,000,000) {{as a result of}} the alleged patent infringement by defendants. As a result of this action, MAGICJACK/ VOCALTEC LTD filed a document with the United States Patent Office (“USPTO”) requesting reexamination of <b>netTALK</b> patent 8,243,722. The USPTO granted the reexamination petition and the claim against MAGICJACK/VOVALTEC LTD was stayed pending the outcome of the USPTO reexamination. In December 2013 <b>netTALK</b> received a USPTO Notice of Intent to Issue Ex Parte Reexamination Certificate (“NIRC”) for netTALK’s U.S. Patent Number 8,243,722. [...] In January 2014, <b>netTALK</b> petitioned the courts to restart the aforementioned lawsuit against MAGICJACK VOCALTECLTD, MAGIJACK HOLDINGS CORPORATION f/k/a YMAX HOLDINGS CORPORATION and DANIEL BORISLOW. The case is set for trial in early 2015. On February 27, 2014, <b>netTALK</b> received the reexamination certificate which restated that all three claims of the ‘722 Patent are deemed allowable by the USPTO. The three claims of the ‘722 Patent were minimally amended during the proceeding.|$|E
50|$|<b>NETtalk</b> {{was created}} to explore the {{mechanisms}} of learning to correctly pronounce English text. The authors note that learning to read involves a complex mechanism involving {{many parts of the}} human brain. <b>NETtalk</b> does not specifically model the image processing stages and letter recognition of the visual cortex. Rather, it assumes that the letters have been pre-classified and recognized, and these letter sequences comprising words are then shown to the neural network during training and during performance testing. It is NETtalk's task to learn proper associations between the correct pronunciation with a given sequence of letters based on {{the context in which the}} letters appear. In other words, <b>NETtalk</b> learns to use the letters around the currently pronounced phoneme that provide cues as to its intended phonemic mapping.|$|E
5000|$|In its {{review of}} <b>Nettalk</b> 6.6.5, NetzWelt.de stated, [...] "the {{graphical}} interface {{makes it easy}} to get started with this powerful ... software".|$|E
50|$|<b>NETtalk</b> is {{a program}} that learns to {{pronounce}} written English text by being shown text as input and matching phonetic transcriptions for comparison.|$|E
50|$|MagicJack sued <b>netTalk</b> in April 2012 for patent infringement. The {{federal court}} has {{dismissed}} the entire case with prejudice, including all claims, counterclaims, defenses and causes of action.|$|E
50|$|Kyriakides {{founded and}} {{presently}} serves {{as chief executive}} officer of netTALK.com, Inc., and is the co-inventor of the <b>netTALK</b> DUO, a consumer electronics VoIP device for making telephone calls over an internet connection.|$|E
50|$|Many cable TV service {{providers}} also offer VoIP-based telephone service via the cable infrastructure (PacketCable). High-speed Internet access service subscriber may use VoIP telephony by subscribing to a third-party service, such as Vonage, MagicJack+ and <b>NetTALK.</b>|$|E
50|$|<b>Nettalk</b> is a {{free and}} open source IRC Client for Windows. It is a {{fully-fledged}} IRC client with UTF-8 and DCC file transfer support. The program is available in Dutch, English, French, German, Hungarian, Italian, Russian, Simplified Chinese, Spanish and Swedish as of version 6.7.13.|$|E
50|$|More {{typically}} {{applications that}} benefit from low latency, such as Skype or SIP or older VoIP using USB devices (<b>NetTalk,</b> MagicJack) may benefit. As these increasingly use Ethernet directly to routers, however, which would generally {{be a much}} lower latency than using a PC and USB connection.|$|E
5000|$|He co-invented the Boltzmann {{machine with}} Geoffrey Hinton and pioneered the {{application}} of learning algorithms to difficult problems in speech (<b>NETtalk)</b> and vision. [...] His infomax algorithm for Independent Component Analysis (ICA) with Tony Bell has been widely adopted in machine learning, signal processing and data mining.|$|E
50|$|Cellular and {{landline}} {{phone service}} suffered major disruptions {{in the affected}} area. Immediately after the earthquake cellular communication was jammed across much of Japan due to a surge of network activity. On {{the day of the}} quake, American broadcaster NPR was unable to reach anyone in Sendai with working phone or Internet. Internet services were largely unaffected in areas where basic infrastructure remained, despite the earthquake having damaged portions of several undersea cable systems landing in the affected regions; these systems were able to reroute around affected segments onto redundant links. Within Japan, only a few websites were initially unreachable. Several Wi-Fi hotspot providers reacted to the quake by providing free access to their networks, and some American telecommunications and VoIP companies such as AT&T, Sprint, Verizon, T-Mobile and VoIP companies such as <b>netTALK</b> and Vonage have offered free calls to (and in some cases, from) Japan for a limited time, as did Germany's Deutsche Telekom.|$|E
40|$|This paper {{reports on}} a {{comparison}} to the well-known <b>NetTalk</b> implementation of Engl!sh text-to-speech translation via neural networks. A distributed representation scheme for encoding is investigated opposed to the classic localist representation scheme used in the original <b>NetTalk.</b> The paper discusses a modem re-implementation based on Elman’s Simple Recurrent Network...|$|E
40|$|<b>NETtalk</b> is a massively-parallel {{network that}} learns to convert English text to phonemes. In <b>NETtalk,</b> the memory {{representations}} are shared among many processing units, and these representations are learned by practice. In humans, distributed practice {{is more effective}} for longterm retention than massed practice, and we wondered whether learning in <b>NETtalk</b> had similar properties. <b>NETtalk</b> was tested on cued paired-associate recall using nonwords as stimuli. Retention of these target items was measured {{as a function of}} spacing, or the number of interspersed items between successive repetitions of the target. A significant advantage for spaced or distributed items was found for spacings of up to forty intervening items when tested at a retention interval of 64 items. Conversely, a significant advantage for massed items was found if testing immediately followed study. These results'are strikingly similar to the results of many experiments using human subjects and suggest an explanation based on distributed representation...|$|E
40|$|In 1987, Sejnowski and Rosenberg {{developed}} their famous <b>NETtalk</b> system for English textto -speech. This chapter describes a machine learning approach to text-to-speech that builds upon and extends the initial <b>NETtalk</b> work. Among the many extensions to the <b>NETtalk</b> system were the following: a different learning algorithm, a wider input "window", error-correcting output coding, a right-to-left {{scan of the}} word to be pronounced (with the results of each decision influencing subsequent decisions), {{and the addition of}} several useful input features. These changes yielded a system that performs much better than the original <b>NETtalk</b> system. After training on 19, 002 words, the system achieves 93. 7 % correct pronunciation of individual phonemes and 64. 8 % correct pronunciation of whole words (where the pronunciation must exactly match the dictionary pronunciation to be correct). Based on the judgements of three human participants in a blind assessment study, our system was estimated to have a seri [...] ...|$|E
40|$|Abstract. This paper {{describes}} <b>NETtalk,</b> a {{class of}} massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of <b>NETtalk</b> has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to <b>NETtalk</b> reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations {{may be found in}} assemblies of neurons intermediate in size between highly localized and completely distributed representations. 1...|$|E
40|$|During {{the last}} years, several neuro{{computers}} have been developed, but still general purpose computers are {{an alternative to}} these special purpose computers. This paper describes a mapping of the backpropagation learning algorithm onto a large 2 -D torus architecture. The parallel algorithm was implemented on a 512 processor AP 1000 and evaluated using <b>NETtalk</b> and other applications. To obtain high speedup, we have suggested an approach to combine the multiple parallel degrees (training set parallelism, node parallelism and pipelining of the training patterns) of the algorithm. For {{a large number of}} processors, we obtained a performance of 81 million weight updates per second using 512 processors, when running the <b>NETtalk</b> network. Our results show that to obtain the best performance on a large number of processors, a combination of multiple degrees of parallelism in the backpropagation algorithm ought to be considered. INTRODUCTION One of the most popular artificial neural networks (ANN [...] ...|$|E
40|$|Recently, we {{described}} a two-step self-learning approach for grapheme-to-phoneme (G 2 P) conversion [1]. In the first step, grapheme and phoneme strings {{in the training}} data are aligned via an iterative Viterbi procedure that may insert graphemic and phonemic nulls where required. In the second step, a Trie structure encoding pronunciation rules is generated. In this paper we describe the alignment module, and give alignment accuracies on the <b>NETtalk</b> database. We also compare transcription accuracies for two approaches to the second step on three databases: the <b>NETtalk</b> database, the CMU dictionary and the French part of the ONOMASTICA lexicon. The two transcription approaches applied in this research are a Trie approach [1] and an approach based on binary decision trees grown {{by means of the}} Gelfand-RavishankarDelp algorithm [2, 3, 4]. We discuss the choice of questions for these decision trees - {{it may be possible to}} formulate questions about groups of characters (e. g., "is the next lette [...] ...|$|E
40|$|This paper {{proposes a}} {{knowledge}} representation of rhythmic patterns, and a neural model to segment musical pieces {{in accordance with}} three cases of rhythmic segmentation. The neural model has a topology which is identical to that of <b>NETtalk</b> (Sejnowski & Rosenberg, 1987). It is trained on sets of contrived patterns, and evaluated on two two-part inventions, two three-part inventions, and two fugues of Bach (Bach, 1970, 1989). ...|$|E
40|$|This paper {{describes}} an improved input coding method for a text-to-phoneme (TTP) {{neural network model}} for speaker independent speech recognition systems. The code-book is self-organizing and is jointly optimized with the TTP model ensuring that the coding is optimal in terms of overall performance. The code-book {{is based on a}} set of single layer neural networks with shared weights. Experiments show that performance is increased com-pared to the <b>NETTalk</b> and NETSpeak models. 1...|$|E
40|$|We {{define a}} novel g-to-p {{prediction}} algorithm that utilises {{the concept of}} a ‘default phoneme’: a grapheme which is realised as a specific phoneme significantly more often than as any other phoneme. We find that this approach results in an algorithm that performs well across a range from very small to large data sets. We evaluate the algorithm on two benchmarked databases (Fonilex and <b>NETtalk)</b> and find highly competitive performance in asymptotic accuracy, initial learning speed, and model compactness...|$|E
40|$|We propose an attention-enabled encoder-decoder {{model for}} the problem of grapheme-to-phoneme conversion. Most {{previous}} work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and <b>NetTalk).</b> Comment: Accepted in SLT 201...|$|E
40|$|This paper {{describes}} a Norwegian <b>NETtalk</b> program, a neural network program which learns to convert Norwegian text to phonemes. The original database consists of about 60000 Norwegian words and their transcriptions. This phoneme database is {{developed by the}} Norwegian Telecom and is further developed {{to be used in}} this program. The Sampa notation for Norwegian is used for the phoneme transcriptions. The experiments have been performed on a PC-platform and on only a sample of words from this database...|$|E
40|$|Abstract—The {{conventional}} linear backpropagation {{algorithm is}} replaced by a nonlinear version, which avoids the necessity for calculating the derivative of the activation function. This may be exploited in hardware realizations of neural processors. In this paper we derive the nonlinear backpropagation algorithms in the framework of recurrent backpropagation and present some numerical simulations of feedforward networks on the <b>NetTalk</b> problem. A discussion of implementation in analog very large scale integration (VLSI) electronics concludes the paper. Index Terms — Backpropagation, neural-network implementation, neural networks, recurrent backpropagation. I...|$|E
40|$|The {{relationship}} between written and spoken words is convoluted in languages {{with a deep}} orthography such as English and therefore {{it is difficult to}} devise explicit rules for generating the pronunciations for unseen words. Pronunciation by analogy (PbA) is a data-driven method of constructing pronunciations for novel words from concatenated segments of known words and their pronunciations. PbA performs relatively well with English and outperforms several other proposed methods. However, the best published word accuracy of 65. 5 % (for the 20, 000 word <b>NETtalk</b> corpus) suggests there is much room for improvement in it. Previous PbA algorithms have used several different scoring strategies such as the product of the frequencies of the component pronunciations of the segments, or the number of different segmentations that yield the same pronunciation, and different combinations of these methods, to evaluate the candidate pronunciations. In this article, we instead propose to use a probabilistically justified scoring rule. We show that this principled approach alone yields better accuracy (66. 21 % for the <b>NETtalk</b> corpus) than any previously published PbA algorithm. Furthermore, combined with certain ad hoc modifications motivated by earlier algorithms, the performance climbs up to 66. 6 %, and further improvements are possible by combining this method with other methods...|$|E
40|$|We {{present in}} this paper a {{statistical}} model for languageindependent bi-directional conversion between spelling and pronunciation, based on joint grapheme/phoneme units 1 extracted from automatically aligned data. The model is evaluated on spelling-to-pronunciation and pronunciation-tospelling conversion on the <b>NetTalk</b> database and the CMU dictionary. We also study the effect of including lexical stress in the pronunciation. Although a direct comparison is difficult to make, our model's performance appears {{to be as good}} or better than that of other data-driven approaches that have been applied to the same tasks. 1...|$|E
40|$|Unrestricted English text can be {{converted}} to speech by applying phonological rules and handling exceptions with a look-up table. However, this approach is highly labor intensive since each entry and rule must be hand-crafted. <b>NETtalk</b> is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units. ~fter ' training on a corpus of informal continuous speech, it achieves good performance and generalizes to novel words. The distributed internal representations of the phonological regularities discovered by the network are damage resistant...|$|E
40|$|This paper {{presents}} the results of simulations of a new class of artificial neural network models of reading. Unlike previous models, they are not restricted to mono-syllabic words, require no complicated input-output representations such as Wickelfeatures and, although based on the <b>NETtalk</b> system of Sejnowski & Rosenberg (1987), require no pre-processing to align the letters and phonemes in the training data. The best cases are able to achieve 100 % performance on the Seidenberg & McClelland (1989) training corpus, in excess of 90 % on pronounceable non-words and on damage exhibit symptoms similar to acquired surface dyslexia...|$|E
40|$|Abstract: 2 ̆ 2 We {{describe}} a fast-propagation algorithm for a linear array of processors. Results of an {{implementation of this}} algorithm on Warp, a ten processor, programmable systolic array computer, are reviewed and compared with back-propagation implementations on other machines. Our current Warp simulator is about 8 times faster at simulating the <b>NETtalk</b> text-to-speech network than the fastest back-propagation simulator reported in the literature. This fast simulator on Warp is being used routinely in a road recognition experiment for robot navigation at Carnegie Mellon. Our results indicate that linear systolic array machines can be efficient neural network simulators. Planned extensions and improvements to our current algorithm are discussed. 2 ̆...|$|E
40|$|Amulti-layer {{perceptron}} (MLP) {{similar to}} that used in the <b>NETtalk</b> system is used to form a mapping between sequences of allophones and corresponding frames of LPC synthesizer control parameters. Three parameter sets equivalent to the LPC coe cients, line spectral pair (LSP), PARCOR and log area ratio, are evaluated. In addition to training a standard MLP, networks which havebeen decomposed according to phonetic class and by allophone, are trained. Decomposition is found to reduce training time and produce greater accuracy on the training set, however the network decomposed by allophone is found to receive to few training patterns too generalise properly on new data...|$|E
40|$|Text-to-phoneme mapping {{is a very}} {{important}} preliminary step in any text-to-speech synthesis system. In this paper, we study the performances of the multilayer perceptron (MLP) neural network for the problem of text-to-phoneme mapping. Specifically, we study the influence of the input letter encoding in the conversion accuracy of such system. We show, that for large network complexities the orthogonal binary codes (as introduced in <b>NetTalk)</b> gives better performance. On the other hand in applications that require very small memory load and computational complexity other compact codes may be more suitable. This study is a first step toward implementation a neural network based text-to-phoneme mapping in mobile devices. 1...|$|E
40|$|Previous {{attempts}} to derive connectionist models for text-tophoneme conversion [...] such as <b>NETtalk</b> and NETspeak [...] have generally used pre-aligned training data and purely feedforward networks, {{both of which}} represent simplifications of the problem. In this work, we explore the potential of recurrent networks to perform the conversion task when trained on non-aligned data. Initially, our use of a single recurrent network produced disappointing results. This led {{to the definition of}} a two-phase model in which the hidden-unit representation of an autoassociative network was fed forward to a recurrent network. Although this model currently does not perform as well as NETspeak, it is solving a harder problem. Also, we propose several possible avenues for improvement...|$|E
