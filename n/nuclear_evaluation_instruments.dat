0|1780|Public
40|$|Evidence-based <b>evaluation</b> <b>instruments</b> for course units offer {{academics}} quality data, {{useful for}} continual improvement. Placement units are not traditional course units and {{may require a}} specific <b>evaluation</b> <b>instrument,</b> of which none seem available. This study commenced {{the development of a}} placement <b>evaluation</b> <b>instrument</b> supported by a relevant learning framework. The qualitative study was conducted (n= 56) and examined the views of placement stakeholders regarding what attributes of the placement experience should be evaluated. Data were thematically analyzed, leading to a 25 item <b>evaluation</b> <b>instrument.</b> This instrument can be completed by students to evaluate and improve placement experiences and outcomes. (Asia-Pacific Journal of Cooperative Education, 2012, 13 (4), 225 - 238...|$|R
40|$|This study made a {{critical}} analysis of summative teacher performance <b>evaluation</b> <b>instruments</b> used nationally in public school systems. The investigation involved identifying in the literature the types of instruments used, the types of rating scales and response modes used, and {{the merits of the}} comment section that may be found on the evaluation document. The following purposes provided the framework for the investigation: (1) To determine the degree to which school systems were using summative teacher performance <b>evaluation</b> <b>instruments.</b> (2) To determine the current status of teacher performance <b>evaluation</b> <b>instruments</b> used nationally by ascertaining: (a) the kinds of instruments being used, and (b) the types of criterion found on the instruments. (3) To compare summative teacher performance <b>evaluation</b> <b>instruments</b> by district size and purpose for which the district has said the instrument is used;The findings support the following conclusions: (a) little difference was noted regarding the summative teacher performance <b>evaluation</b> <b>instrument</b> {{as it relates to the}} purposes for which districts have said they use the instrument, (b) the category/rating scale <b>evaluation</b> <b>instrument</b> is used most frequently by districts, (c) districts have identified growth as the major purpose for which teachers are evaluated, (d) <b>evaluation</b> <b>instruments</b> usually have criteria that are embedded rather than in a discrete position, and (e) the verbal rating scale is used with a high degree of frequency by the school districts studied...|$|R
5000|$|Sensual <b>Evaluation</b> <b>Instrument</b> (snapshot non-verbal self-report) ...|$|R
40|$|During {{the first}} round of the study panel members were asked to comment on the kinds of <b>evaluation</b> <b>instruments</b> they {{considered}} appropriate. The responses did not indicate clearly their preferences except that no one was in favour of a detailed <b>evaluation</b> <b>instrument</b> which indicated weights for each criterion...|$|R
40|$|The {{present study}} aims at {{developing}} <b>evaluation</b> <b>instruments</b> on softball skills {{on the basis}} of authentic assessments. This research and development were conducted in Yogyakarta State University. This study used the research and development design which was conducted to develop <b>evaluation</b> <b>instruments</b> on softball skills {{on the basis of}} authentic assessments. The results were <b>evaluation</b> <b>instruments</b> on softball skills on the basis of authentic assessments for the students of physical, health, and recreation education. The instruments were developed based on the authentic assessments which were conducted through observation...|$|R
30|$|Our results {{indicate}} that both the SF- 36 and WOMAC are valid and complementary <b>evaluation</b> <b>instruments.</b> The WOMAC assesses physical outcomes, while the SF- 36 is mostly dedicated to the social and psychological arena [21]. Utilization of these QoL indicators alongside more traditional <b>evaluation</b> <b>instruments,</b> such as the Harris hip score, continues to increase in the literature [14, 20].|$|R
40|$|The {{requirement}} for <b>evaluation</b> <b>instruments</b> learning outcomes authentically {{need to be}} done to get description totality about the student competence, when carry out biology experiment activity on katalase enzyme subject matter. This sudy aims to produce the final product {{in the form of an}} <b>evaluation</b> <b>instrument</b> for catalase enzyme experiment test. Development model adopted in this study is ADDIE model. Based on the results of expert judgement, the instrument was valid and can be tested in classroom. The results of the expert validation, <b>evaluation</b> <b>instrument</b> found that the enzyme catalase experiment test has appropriated entirety, in terms of the content aspect, constructs, linguistic, and practicalities. The test results were showed that the instrument has high realiability and validity for all student competency. Therefore, the conclusion is this <b>evaluation</b> <b>instrument</b> theority and rasionally that good and valid to use for measuring and considering all student competency for catalase enzyme experiment test...|$|R
50|$|Course <b>evaluation</b> <b>instruments</b> {{generally}} include {{variables such}} as communication skills, organizational skills, enthusiasm, flexibility, attitude toward the student, teacher - student interaction, encouragement of the student, knowledge of the subject, clarity of presentation, course difficulty, fairness of grading and exams, and global student rating. Examples of standardized course <b>evaluation</b> <b>instruments</b> are provided by evaluation tools such as TrainingCheck, and CE-Gen.|$|R
40|$|The work {{is devoted}} to the {{analysis}} of the properties of the <b>nuclear</b> <b>evaluations</b> in the half-Markovian process stay time distribution densities in the given state and densities of distributing intervals between restoration process events. The consistency and asymptotic normality of the <b>nuclear</b> <b>evaluation</b> on the density of distributing intervals between events of the simple restoration process have been proven, the asymptotic representations at T approaches infinity for mean-root-square error and covariation of this evaluation at experiment plan [1 BT] have been obtained. The consistency of the <b>nuclear</b> <b>evaluation</b> on the density of distributing stay time and fixed state for half-Markovian processes has been proven. The consistency and asymptotic normality of the <b>nuclear</b> <b>evaluation</b> on the density of distributing two-dimensional random vector component in the diagram of the stopped consequences have been proven. The obtained results can find application at design of the real information processing systems, at creation of the statistical data processing software packages. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|The paper {{seeks to}} explore and {{critically}} evaluate extant research on <b>evaluation</b> <b>instruments</b> for e-learning and to ask whether they are aligned to {{what we know about}} good teaching. Several online instruments are investigated by examining the criteria they apply to the evaluation of the student experience of e-learning. A related issue explored in the paper is the application of current research and theory underpinning effective teaching and learning to the design of <b>evaluation</b> <b>instruments.</b> The question asked is whether commonly used evaluation criteria are aligned with state of the art theoretical knowledge about teaching and learning. It is proposed that more learner–centred and constructivist <b>evaluation</b> <b>instruments</b> are needed that reflect what we know about learning online...|$|R
40|$|Abstract: The {{objective}} {{of the study was}} to develop the <b>evaluation</b> <b>Instrument</b> of reading four test. The development of such instrument is a valuable effort to measure the students’ mastery of the reading skill and to improve the quality of the <b>evaluation</b> <b>instrument</b> which is produced by the teacher. Before the teacher develop the reading <b>evaluation</b> <b>instrument,</b> he had cultivated the previous test instrument to analyze the weakness and the strength of the implemented test instrument. After that, the teacher started to develop a prototype which has been modified by the teacher himself. Then, he administered the first try out to the English students of semester VII and collecting some inputs and critics of the modified reading test instrument after the students finished doing the test. The result of the test together with the students’ critics and inputs were used to rebuild the reading test intrument better. At last, the teacher has developed the <b>evaluation</b> <b>instrument</b> of reading test for students of semester VII in Muhammadiyah University of Metro...|$|R
40|$|Abstract Background There {{are many}} {{instruments}} available freely for evaluating obstetric care quality in low-resource settings. However, this profusion can be confusing; moreover, <b>evaluation</b> <b>instruments</b> {{need to be}} adapted to local issues. In this article, we present tools we developed to guide the choice of instruments and describe how we used them in Burkina Faso to facilitate the participative development of a locally adapted instrument. Methods Based on a literature review, we developed two tools: a conceptual framework and an analysis grid of existing <b>evaluation</b> <b>instruments.</b> Subsequently, we facilitated several sessions with evaluation stakeholders in Burkina Faso. They used the tools to develop a locally adapted <b>evaluation</b> <b>instrument</b> that was subsequently tested in six healthcare facilities. Results Three outputs emerged from this process: 1) A comprehensive conceptual framework {{for the quality of}} obstetric care, each component of which is a potential criterion for evaluation. 2) A grid analyzing 37 instruments for evaluating the quality of obstetric care in low-resource settings. We highlight their key characteristics and describe how the grid can be used to prepare a new evaluation. 3) An <b>evaluation</b> <b>instrument</b> adapted to Burkina Faso. We describe the experience of the Burkinabé stakeholders in developing this instrument using the conceptual framework and the analysis grid, while taking into account local realities. Conclusions This experience demonstrates how drawing upon existing instruments can inspire and rationalize the process of developing a new, tailor-made instrument. Two tools that came out of this experience can be useful to other teams: a conceptual framework for the quality of obstetric care and an analysis grid of existing <b>evaluation</b> <b>instruments.</b> These provide an easily accessible synthesis of the literature and are useful in integrating it with the context-specific knowledge of local actors, resulting in <b>evaluation</b> <b>instruments</b> that have both scientific and local legitimacy. </p...|$|R
40|$|This {{dissertation}} explored Tennessee practitioner {{perceptions of}} the construct, content and utility of Exemplary Practices in Alternative Education: Indicators of Quality Programming (Exemplary Practices) for use as an <b>evaluation</b> <b>instrument</b> (National Alternative Education Association, 2009). The general {{purposes of this study}} were to (1) examine the legitimacy of the ten constructs (i. e., standards) and corresponding content (i. e., indicators of success) of best practice as presented in the Exemplary Practices and (2) investigate the utility of the Exemplary Practices when transformed into an <b>evaluation</b> <b>instrument</b> for alternative schools and programs. The study entailed a two-phased sequential, mixed-model research design (Cameron, 2009). Phase One involved a concurrent embedded strategy (Creswell, 2009) to obtain quantitative and qualitative data related to the constructs and content found in the Exemplary Practices. With the exception of four indicators, findings provide evidence of construct and content validity as perceived by Tennessee practitioners. Phase Two involved a sequential, explanatory research strategy (Creswell, 2009) aimed at collecting data related to the utility of the Exemplary Practices when transformed into an <b>evaluation</b> <b>instrument.</b> Findings indicate that the majority of constructs and content were not observable during utility testing. Additionally, findings point to the need for enhancements to the instrument. During utility testing, simple observations were not enough to fully ascertain whether or not the alternative school or program was implementing the Exemplary Practices with fidelity. Research participants overwhelming noted that the <b>evaluation</b> <b>instrument</b> should incorporate evidence categories for observations, interviews and artifacts. Following Phase One and Phase Two of the study, the researcher developed an <b>evaluation</b> <b>instrument</b> for designing, delivering, evaluating and improving alternative education programming. The instrument was constructed from the Exemplary Practices but adapted based upon practitioner perceptions of construct and content validity, as well as overall utility. The culminating <b>evaluation</b> <b>instrument</b> is presented as a product of the research study...|$|R
40|$|Objective: To {{evaluate}} {{the reliability of}} a peer <b>evaluation</b> <b>instrument</b> in a longitudinal team-based learning setting. Methods: Student pharmacists were instructed to {{evaluate the}} contributions of their peers. Evaluations were analyzed for the variance of the scores by identifying low, medium, and high scores. Agreement between performance ratings within each group of students was assessed via intra-class correlation coefficient (ICC). Results: We found little variation in the standard deviation (SD) based on the score means among the high, medium, and low scores within each group. The lack of variation in SD of results between groups suggests that the peer <b>evaluation</b> <b>instrument</b> produces precise results. The ICC showed strong concordance among raters. Conclusions: Findings suggest that our student peer <b>evaluation</b> <b>instrument</b> provides a reliable method for peer assessment in team-based learning settings...|$|R
40|$|This study {{aimed at}} elaborating and validating an ESL/ EFL {{software}} <b>evaluation</b> <b>instrument</b> that encompassed {{the principles of}} Communicative Language Teaching and an interactive approach to computer use for language learning. Once the instrument was elaborated, it was tested for its internal consistency and inter-item and inter-rater reliability. The results of the Pearson Coefficient and the ICC Coefficient measures indicated high levels of inter-rater reliability for the group of 26 teachers. The Cronbach’s Alpha Coefficients for the three programs indicated that the ESL/EFL software <b>evaluation</b> <b>instrument</b> had adequate levels of inter-item reliability. These {{results suggest that the}} ESL/EFL software <b>evaluation</b> <b>instrument</b> has high levels of internal consistency. These results indicate that the <b>evaluation</b> <b>instrument</b> has a high degree of reliability. The positive indicators of reliability obtained from the procedures used to assess the inter-rater reliability and the internal consistency and the face and content validity attributed to the instrument suggest that the ESL/EFL software evaluation 306 Vládia M. C. Borges instrument is potentially valid for assessing the degree to which ESL/EFL software programs develop language skills according to the Communicative Language Teaching principles and an interactive approach to computer use in language learning...|$|R
40|$|The {{purpose of}} the study was to {{determine}} if teacher appraisal instruments used in Indiana public schools contained teacher performance criteria identified by teaching effectiveness researchers. Student scores from the 1986 Indiana Basic Competency Skills Test were examined to determine the relationship to the use of teaching effectiveness criteria on teacher <b>evaluation</b> <b>instruments,</b> per pupil expenditure, and other selected variables. A questionnaire was mailed to each Indiana public school superintendent along with a request to return the teacher <b>evaluation</b> <b>instrument</b> currently being used. Each <b>evaluation</b> <b>instrument</b> was judged to determine the percent of teacher evaluation criteria matching the twenty effective teaching criteria identified by Manatt and Stow. Data obtained from the questionnaire and teacher <b>evaluation</b> <b>instruments</b> were analyzed using frequency tabulations, percentages, and statistical treatment. The Spearman Rank Order of Correlation and the Kruskal-Wallis one-way analysis of variance were used to determine significance between student test scores, effective teaching criteria and selected variables. P=<. 05 was used as the statistical measure to determine the level of statistical significance. Among others, the following conclusions were drawn: 1. There is no significant difference between student achievement scores on the Indiana Basic Competency Skills Test and school corporations having a higher or lower percentage of research-based criteria on teacher <b>evaluation</b> <b>instruments,</b> except in 6 th grade Math. 2. There is no significant difference between student achievement scores and school corporations spending more or fewer dollars per pupil. 3. Students from school corporations with larger student enrollments score significantly lower on the Indiana Basic Competency Skills Test. Department of Educational Administration and SupervisionThesis (D. Ed. ...|$|R
40|$|Evaluating {{quality of}} obstetric care in low-resource settings: Building on the {{literature}} to design tailor-made evaluation instruments- an illustration in Burkina Faso Florence Morestin 1 *, Abel Bicaba 2, Jean de Dieu Sermé 2, Pierre Fournier 1, 3 Background: There are many instruments available freely for evaluating obstetric care quality in low-resource settings. However, this profusion can be confusing; moreover, <b>evaluation</b> <b>instruments</b> {{need to be}} adapted to local issues. In this article, we present tools we developed to guide the choice of instruments and describe how we used them in Burkina Faso to facilitate the participative development of a locally adapted instrument. Methods: Based on a literature review, we developed two tools: a conceptual framework and an analysis grid of existing <b>evaluation</b> <b>instruments.</b> Subsequently, we facilitated several sessions with evaluation stakeholders in Burkina Faso. They used the tools to develop a locally adapted <b>evaluation</b> <b>instrument</b> that was subsequently tested in six healthcare facilities. Results: Three outputs emerged from this process: 1) A comprehensive conceptual framework {{for the quality of}} obstetric care, each component of which is a potential criterion for evaluation. 2) A grid analyzing 37 instruments for evaluating the quality of obstetric care in low-resource settings. We highlight their key characteristics and describe how the grid can be used to prepare a new evaluation. 3) An <b>evaluation</b> <b>instrument</b> adapted to Burkina Faso. We describe the experience of the Burkinabé stakeholders in developing this instrument using the conceptual framework and the analysis grid, while taking into account local realities. Conclusions: This experience demonstrates how drawing upon existing instruments can inspire and rationalize the process of developing a new, tailor-made instrument. Two tools that came out of this experience can be useful to other teams: a conceptual framework for the quality of obstetric care and an analysis grid of existing <b>evaluation</b> <b>instruments.</b> These provide an easily accessible synthesis of the literature and are useful in integrating it with the context-specific knowledge of local actors, resulting in <b>evaluation</b> <b>instruments</b> that have both scientific and local legitimacy...|$|R
40|$|The aims of this {{research}} is to develop an affective evaluation models and to produce its <b>evaluation</b> <b>instrument</b> in the course of Education on Islam religion for students of D-II Elementary School Teachers Education so {{that there will be a}} qualified standard of non-test instrument. This research is a development research study with population consisting of students of D-II elementary School Teachers Education of Yogyakarta State University of 2004 / 2005 academic year who took the course on education of Islam religion. The sample was selected purposively involving classes N. 15 and D. 15 class. The development research followed the research steps: 1) Preparation; 2) Construction of an affective evaluation; 3) Models try out; 4) Model analysis; 5) Evaluation and reflection. Analyze data involve the use of statistic parametric with factor analysis. The data analysis employing the SPSS Program shows that, from 50 items in the effective <b>evaluation</b> <b>instrument</b> on education of Islam religion, 8 items are invalid because their F value are smaller than 0. 3 and 4 items are invalid because F values are negative. From the eigenvalue with F values greater than 1 as F values that can be used as a factor of a traits,the affective <b>evaluation</b> <b>instrument</b> for Education of islam comprised 28 faktors with a cumulative percentage 90. 27 %. from the statistical figure, it can be conclude that the affective <b>evaluation</b> <b>instrument</b> for Education instrument for Education of Islam religion can be used as an affective <b>evaluation</b> <b>instrument.</b> The Cronbach alpha reliability coefficient is 0. 7. therefore, this instrument is reliable enough to measure an affective aspect in Education of islam...|$|R
40|$|Abstract: This paper {{presents}} {{the process of}} developing an <b>evaluation</b> <b>instrument</b> specifically for the evaluation of e-Commerce Web sites from the first-time buyer’s viewpoint. The development process is based on theoretical discussions of the Web evaluation and Web user satisfaction literature. A draft <b>evaluation</b> <b>instrument</b> was developed. To enhance its reliability and validity, several iterative trials on e-Commerce Web sites were conducted. Some modifications were made to the instrument. The final version is capable of evaluating e-Commerce Web sites effectively. The instrument provides implications to both Web evaluation practitioners and academics...|$|R
40|$|AIM The Creighton Competency <b>Evaluation</b> <b>Instrument</b> (CCEI) was {{modified}} from an existing instrument, the Creighton Simulation <b>Evaluation</b> <b>Instrument,</b> {{for use in}} the National Council of States Boards of Nursing National Simulation Study (NCSBN NSS). BACKGROUND The CCEI was developed for the NCSBN NSS for use as the <b>evaluation</b> <b>instrument</b> for both simulation and traditional clinical experiences in associate and baccalaureate nursing programs. METHOD Five nursing programs assisted with reliability and validity testing of the CCEI. Using a standardized validation questionnaire, faculty rated the CCEI on its ability to accurately measure student performance and clinical competency. Videos scripted at three levels of performance were used to test reliability. RESULTS Content validity ranged from 3. 78 to 3. 89 on a four-point Likert-like scale. Cronbach 2 ̆ 7 s alpha was 3 ̆e. 90 when used to score three different levels of simulation performance. CONCLUSION The CCEI is useful for evaluating both the simulation and traditional clinical environments...|$|R
40|$|In {{the state}} of West Virginia, the {{educator}} evaluation system was implemented in 2010 {{as part of a}} comprehensive system of support to increase teacher effectiveness and student learning. As part of the system, the Educator <b>Evaluation</b> <b>Instrument</b> was developed to measure teachers 2 ̆ 7 effectiveness. This study was conducted to determine whether the Educator <b>Evaluation</b> <b>Instrument</b> was valid for use in measuring effectiveness. A hierarchical confirmatory factor analysis (HCFA) was conducted on the scores from the demonstration year. The data were not normal, nor was good model fit established based on the current model. Because good model fit could not be established, an exploratory factor analysis was conducted, which through principal component analysis, four components were extracted and utilized as the first-order factors in the HCFA. With the new model, good fit was established, and therefore redesigning the Educator <b>Evaluation</b> <b>Instrument</b> to align with the new components is recommended to ensure validity of use...|$|R
40|$|This study {{reports on}} the {{development}} of a teacher <b>evaluation</b> <b>instrument,</b> based on students’ observations, which exhibits cumulative ordering in terms of the complexity of teaching acts. The study integrates theory on teacher development with theory on teacher effectiveness and applies a cross-validation procedure to verify whether teaching acts have a cumulative order. The resulting teacher <b>evaluation</b> <b>instrument</b> comprises 32 teaching acts with cumulative ordering in terms of complexity. This ordering aligns with prior teacher development research. It also represents a valuable extension in that the instrument can provide feedback about a teacher’s current phase of development and advice for improvement...|$|R
40|$|This paper {{discusses}} six {{factors which}} can be used to succe fully examine, evaluate, and select reading instructional mater als; it also examines the appropriate characteristics of an obje tive <b>evaluation</b> <b>instrument.</b> <b>Evaluation</b> of reading instructional mat rials should be based on a sound philosophy of reading in truction as practiced in the school district; take into c sideration the intellectual, social, oral, and legal standards of mmunity and state; utilize all available information regarding the onstruction and effectiveness of materials being considered; be both //subjective and objective; and deal primarily withinstructional design characteristics, and only secondarily with phy-sical characteristics. <b>Evaluation</b> <b>instruments</b> should be objective, contain descriptions to reinforce ratings, utilize Multi-point rating scales, and provide mechanisms for comparison with subjective ratings...|$|R
40|$|The Unified Modelling Language (UML) as {{delivered}} in September 1997 offers {{the structure and}} dynamics of its modelling constructs developed in order to standardise different object oriented (OO) development practices. Represented as a language, UML covers some aspects addressed by any methodology {{and is expected to}} be accompanied by OO CASE tools through notation and implementation of the UML philosophy. This paper discusses the problem of OO CASE tools as methodology companions that encourage or enforce methodology support. The basis for an <b>evaluation</b> <b>instrument</b> has been developed in order to analyse how commercially available OO CASE tools support the UML. The <b>evaluation</b> <b>instrument</b> is based on extraction of a set of rules that are supposed to be followed in order to claim that the UML itself is being followed. The rules are extracted from the current UML Semantics document and its well-formedness rules. The <b>evaluation</b> <b>instrument</b> is tested against a few OO CASE tools in order to analyse how it can be used on a larger scale for assessing the level of automation and UML support embedded in the tools...|$|R
40|$|This paper {{describes}} {{the nature of}} my proficiencies-based economics courses and the “teaching/learning ” <b>evaluation</b> <b>instrument</b> I developed to help students assess what they learned and to help me determine how I could help students enhance not only their learning but also their ability to learn. The paper begins with {{a critique of the}} traditional course/instructor evaluation questionnaires, elaborates my proficiencies-based approach to teaching and learning, {{describes the}} <b>evaluation</b> <b>instrument</b> I developed, and then presents the evaluation results based on using this approach in a general, one-semester introductory economics course. The paper concludes with a discussion of the lessons I learned from using this new <b>evaluation</b> <b>instrument</b> with its explicit attention to teaching and learning. A Mistake One of my worst professional mistakes occurred in the late 1960 s. That is when I joined several colleagues to devise a more effective course evaluation system for the Department of Economics. We were distressed by the sloppy course evaluation system recently instituted by the student government organization, the Wisconsin Student Association (WSA). Its system suffere...|$|R
40|$|Sorry, {{the full}} text of this article is not {{available}} in Huskie Commons. Please click on the alternative location to access it. 189 p. In a wave of reform movements, our schools need to respond to the increasing use of technology in order to prepare learners to become productive members of society. The use of technology has entered schools slowly and recently skyrocketed with more microcomputers and software being purchased every year by schools. For the microcomputer to become an effective classroom aid, the software implemented must be dynamic. This has not been the case. Software has been poor in relation to technical capabilities, matching curricular objectives for the classroom, and instructional capabilities. There exists a definite need to have a software evaluation model reliable in assessing the technical capabilities, meeting of curricular objectives, and instructional capabilities of microcomputer software. This study analyzed the instructional component of the software evaluation model by examining models of instruction based on teacher effectiveness research of Barak Rosenshine, Madeline Hunter, David Ausubel, and Robert Gagne as well as instructional criteria with low-inferential implications from extant software evaluations to develop a microcomputer software <b>evaluation</b> <b>instrument.</b> The microcomputer software <b>evaluation</b> <b>instrument</b> based on models of instruction was field-tested and found reliable for evaluating high school mathematics software (KR- 20 =. 8884), science software (KR- 20 =. 8989), and mathematics and science software combined (KR- 20 =. 8924). Based on the study's findings, it was concluded that the microcomputer software <b>evaluation</b> <b>instrument</b> developed for this study was a valid and reliable tool for assessing the instructional qualities of high school mathematics and science software. Implications of the microcomputer software <b>evaluation</b> <b>instrument</b> included recommendations for educational software writers to acquire an educational background and utilize the microcomputer software <b>evaluation</b> <b>instrument</b> to provide instructional guidance in developing software. In addition, it was recommended that the instrument become entirely computerized so that the practitioner can easily and reliably assess high school mathematics and science software by the total utilization of the microcomputer...|$|R
40|$|AbstractThis study's aim is {{to develop}} a valid and {{reliable}} <b>evaluation</b> <b>instrument</b> to define the believes according to using technology in mathematical teaching. The sample is formed with 216 candidate teachers. While forming a scale those steps are followed having item tool, taking expert opinion, implementation, computing validity and reliability, finalizing the scale. At the end of the analysis scale is formed with 31 items, four factors and four factors’ variance ratios are % 44. Cronbach Alpha internal integrity coefficient is calculated as 0. 895 for the whole scale. In line with findings “the belief scale according to using computer technology in mathematics teaching” is improved and it is decided as a valid and reliable <b>evaluation</b> <b>instrument...</b>|$|R
40|$|This article gathers {{the results}} of a {{research}} aiming to investigate if course coordinators consider Sinaes course <b>evaluation</b> <b>instruments</b> true quality indicators and if they can likewise function as tools of college administration. This survey consisted of an electronic questionnaire including open-ended and closed-ended questions in which 90 coordinators from private higher education institutions from the state of Sao Paulo took part. Answers were grouped into three categories: a) aspects of the instrument concerning its quality indicators; b) aspects of external evaluation process, and c) impacts of evaluation on HEI. Coordinators’ replies were conducive to consider that Sinaes course <b>evaluation</b> <b>instruments</b> can be employed as administration tools since they forward subsidies for the planning of actions to improve course quality...|$|R
40|$|The study {{described}} in this paper examines the procedures used {{in the identification of}} the broad and complex factors influencing science curriculum delivery in francophone‐minority settings where the teaching of science is the responsibility of non‐specialist science teachers. Furthermore, it describes the processes involved in the development and validation of an <b>evaluation</b> <b>instrument,</b> the Science Delivery <b>Evaluation</b> <b>Instrument</b> for Francophone‐minority Settings (SDEIFMS), used to identify factors influencing science program delivery. The study begins by exploring the themes generated from several qualitative studies pertaining to the phenomenon of science delivery in francophone‐minority settings in Saskatchewan and Manitoba. Subsequent to this, quantitative procedures used to develop and validate the SDEIFMS are presented. Finally, practical applications of the SDEIFMS as a part of an ongoing CRYSTAL initiative are also discussed...|$|R
40|$|The {{problem of}} this study were (1) how the model {{construction}} and <b>evaluation</b> <b>instruments</b> (2) whether the instrument can be used to evaluate the results of the implementation of learning English curriculum in Biological Science Education Program UNNES lecturer in terms of performance indicators. Development of research aims to produce an <b>evaluation</b> <b>instrument</b> and whether the instrument can be used to find English-language classes are conducted in accordance with the expected success criteria I-MHERE program for faculty performance indicators. Methods of model development to take some model development procedure Borg & Gall. Model validation is done through focus group discussions and request a written opinion of an expert. The data were analyzed descriptively and used to produce products in the form of <b>evaluation</b> <b>instruments</b> for use in model evaluation context, input, prosses, and product or the Stufflebeam CIPP. The results component in the form of instruments to evaluate faculty input, the input component of the curriculum, and the components of the curriculum implementation process of learning English I-MHERE Prodi Education in Biology, Mathematics and Natural Sciences, in particular to review the indicators UNNES lecturer. The instrument has been developed to serve the purpose of such instruments are developed until the early trials...|$|R
40|$|The {{purpose of}} this study was to develop and {{validate}} an <b>evaluation</b> <b>instrument</b> that would provide information to sites and schools about the costs and consequences of participating in a pharmacy clerkship program. The <b>evaluation</b> <b>instrument</b> for estimating the learning opportunities at the sites was based on Kolb's theory of experience as a source for learning and development and Bandura's social cognitive theory of thought and action. The <b>evaluation</b> <b>instrument</b> for estimating impact of student training on practice sites was based on the Nonemployee and Employee Models of the student-preceptor relationship and was adapted from an earlier study. Instrument development was an iterative process involving theoretical and empirical components resulting in algorithms, guidelines, and worksheets. Student activities were the unit of analysis for all instruments. Learning opportunity was characterized by level of learning cycle completeness, ranging from no opportunity to completion of all four steps. Student activities were defined by characteristics that were under the control of the site or school and independent of the individual student, thereby removing confounding factors in the estimation process. The impact sustained by a practice site and the student learning opportunities present at the site could be estimated and compared to negotiate a placement that would minimize potential negative impact and maximize the learning opportunities for the clerkship student...|$|R
40|$|The study aims to make <b>evaluation</b> <b>instrument</b> of {{implementation}} science authentic assessment, were: 1) to determine Aiken’s V score at <b>evaluation</b> <b>instrument</b> validity, and 2) to determine reliability score at <b>evaluation</b> <b>instrument.</b> The study {{was part of}} evaluation research using the CIPP Stufflebeam evaluation model. The research instruments were teacher’s questionnaires, students’ questionnaires, document check list, observation sheets, and interview guideline. The instruments were validated using Aiken’s V and limited trial. The Aiken’s V analysis result is valid if has Aiken’s V score more than 0. 82 with seven validator (rater) and number of rating categories 5. The results of the limited trial were to know the instrument reliability. The item reliabilities were analyzed with alpha Cronbach estimate. The reliability was done on 250 student respondents. The result of reliability is reliable if has the alpha Cronbach score 0, 7. The results of this research are (1) the score Aiken’s V teacher ’s questionnaires obtained is 0. 997, students’ questionnaires 0. 997, document check list 0. 999, observation sheets 0. 999, and interview guideline 0. 999. (2) The reliability score of teacher’s questionnaires is 0. 807, students’ questionnaires is 0. 79, and observation sheets 0. 968. The instruments are valid and reliable to take the research data...|$|R
50|$|Program M {{seeks to}} promote the health and {{empowerment}} of young women through critical reflections about gender, rights and health. It consists of educational workshops, community campaigns and innovative <b>evaluation</b> <b>instruments</b> for assessing the program’s impact on young women’s gender-related attitudes and perceived self-efficacy in interpersonal relationships.|$|R
40|$|The {{purposes}} of this study were to identify the procedures and criteria used for conducting teacher performance evaluation in Texas public schools, to determine the degree to which teacher performance <b>evaluation</b> procedures and <b>instruments</b> reflect the stated evaluation policies of Texas public schools, and to determine the degree to which teacher performance <b>evaluation</b> <b>instruments</b> used in Texas public schools reflect presage criteria (teacher characteristics) as opposed to process criteria (teacher behavior) as opposed to product criteria (student change or gain) as opposed to general job performance requirements (job expectations). The main findings include the following. (1) Teacher performance evaluation is required in all Texas public school districts and is often performed several times a year by more than one observer. The building principal is the key person involved in this process. (2) Although all school districts stated the supervisory function of the improvement of instruction as the major purpose of their teacher performance evaluation policy, a large number of school districts utilize teacher performance evaluation for the administration functions of serving as a basis for retention or dismissal. (3) If in reality teacher performance evaluation were construed as the improvement of instruction or teaching performance, it should be predictable that process criteria (teacher behavior) would account for the majority of items in the <b>evaluation</b> <b>instruments.</b> However, these items accounted for only about one-fourth of the total number. At the same time, items relating to general job requirements accounted for over 50 percent of the items. This exhibits a maintenance rather than teaching thrust. (4) The data gathered on current teacher performance <b>evaluation</b> <b>instruments</b> appear to be highly pertinent to maintaining the school as an organization and appear to be helpful in making personnel decisions. (5) Teacher performance <b>evaluation</b> <b>instruments</b> in Texas public schools are much more heavily weighted toward assessing teachers in their multiple roles rather than the many aspects of teaching...|$|R
40|$|FORTRAN {{computer}} algorithm containing various image-processing {{analysis and}} enhancement functions developed. Algorithm developed specifically to process images of developmental heat-engine materials obtained with sophisticated nondestructive <b>evaluation</b> <b>instruments.</b> Applications of program include scientific, industrial, and biomedical imaging for studies of flaws in materials, analyses {{of steel and}} ores, and pathology...|$|R
40|$|This study {{presents}} the procedures and {{results of an}} experiment conducted in Sweden to assess the relative effectiveness of two methods of teaching the grammatical structures of English as a foreign language to adults. The main objective {{of the study is}} to find indications as to which of the two theories (the audiolingual habit theory or the cognitive code-learning theory) provides a better basis for teaching foreign language grammar to adults. Chapters examine experimental objectives, existing research, teaching methods, comparative studies, experimental design, <b>evaluation</b> <b>instruments,</b> and project results. Appendixes contain sample instructional materials, charts illustrating distribution of lesson time, <b>evaluation</b> <b>instruments,</b> and information about the Swedish school system. Lists of tables and figures are provided. (RL),i 2 r,:T! 4; 4 Of. f. tiri EDUCAUN r. WEITARE Oi"! iDUCATiO...|$|R
