148|141|Public
25|$|The {{fact that}} Horn clauses {{can be given}} a {{procedural}} interpretation and, vice versa, that goal-reduction procedures {{can be understood as}} Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of <b>non-monotonic</b> <b>logic.</b>|$|E
25|$|In the {{simplest}} {{case in which}} H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, {{the most important one}} being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a <b>non-monotonic</b> <b>logic.</b>|$|E
5000|$|Relevance logic, linear logic, and <b>non-monotonic</b> <b>logic</b> reject {{monotonicity}} of entailment; ...|$|E
5000|$|... #Subtitle level 2: Proof-theoretic versus model-theoretic formalizations of <b>non-monotonic</b> <b>logics</b> ...|$|R
50|$|These {{principles}} do {{not hold}} in all logics, however. Obviously they {{do not hold}} in <b>non-monotonic</b> <b>logics,</b> nor do they hold in relevance logics.|$|R
40|$|This paper {{reports on}} {{systematic}} research {{which aims to}} classify <b>non-monotonic</b> <b>logics</b> by their expressive power. The classication is based on translation functions that satisfy three important criteria: polynomiality, faithfulness and modularity (PFM for short). The basic method for classication is to prove that PFM translation functions exist (or do not exist) between certain logics. As a result, <b>non-monotonic</b> <b>logics</b> can be arranged to form a hierarchy. This paper gives {{an overview of the}} current expressive power hierarchy (EPH) and investigates semi-normal default logic as well as prerequisite-free and semi-normal default logic in order to locate their exact positions in the hierarchy. ...|$|R
50|$|Default {{logic is}} a <b>non-monotonic</b> <b>logic</b> {{proposed}} by Raymond Reiter to formalize reasoning with default assumptions.|$|E
50|$|He is {{currently}} working on integrating <b>non-monotonic</b> <b>logic</b> with social network analysis in the context of sociological theories.|$|E
5000|$|Defeasible {{logic is}} a <b>non-monotonic</b> <b>logic</b> {{proposed}} by Donald Nute to formalize defeasible reasoning. In defeasible logic, {{there are three}} different types of propositions: ...|$|E
40|$|The {{reference}} class {{problem in}} probability {{theory and the}} multiple inheritances (extensions) problem in <b>non-monotonic</b> <b>logics</b> {{can be referred to}} as special cases of con icting beliefs. The current solution accepted in the two domains is the speci city priority principle. By analyzing an example, several factors (ignored by the principle) are found to be relevant to the priority of a reference class. A new approach, Non-Axiomatic Reasoning System (NARS), is discussed, where these factors are all taken into account. It is argued that the solution provided by NARS is better than the solutions provided by probability theory and <b>non-monotonic</b> <b>logics.</b> ...|$|R
40|$|Among non-monotonic {{systems of}} reasoning, <b>non-monotonic</b> modal <b>logics,</b> and autoepistemic logic in particular, have had {{considerable}} success. The presence of explicit modal operators allows {{flexibility in the}} embedding of other approaches. Also several theoretical results of interest have been established concerning these logics. In this paper we introduce <b>non-monotonic</b> modal <b>logics</b> based on many-valued logics, rather than on classical logic. This extends earlier work of ours on many-valued modal logics. Intended applications are to situations involving several reasoners, not just one as in the standard development. 1 Introduction Several kinds of <b>non-monotonic</b> <b>logics</b> have been considered over the past dozen years. Among these, <b>non-monotonic</b> modal <b>logics</b> have been particularly interesting. These, of course, have explicitly occurring modal operators, and this allows for fine control when translating from other formalisms; see [13, 5], for instance. Following the ideas of McDermott and Do [...] ...|$|R
50|$|The idea of {{comparing}} {{the size of}} proofs {{can be used for}} any automated reasoning procedure that generates a proof. Some research has been done about the size of proofs for propositional non-classical logics, in particular, intuitionistic, modal, and <b>non-monotonic</b> <b>logics.</b>|$|R
50|$|Probabilist doctrines {{continue}} to be debated {{in the context of}} artificial general intelligence, as a counterpoint to the use of <b>non-monotonic</b> <b>logic,</b> as the proper form for knowledge representation remains unclear.|$|E
50|$|Preferential {{entailment}} is a <b>non-monotonic</b> <b>logic</b> {{based on}} selecting only models {{that are considered}} the most plausible. The plausibility of models is expressed by an ordering among models called a preference relation, hence the name preference entailment.|$|E
50|$|Clark's key {{contributions}} {{have been}} in the field of logic programming. His 1978 paper on negation as failure was arguably the first formalisation of a <b>non-monotonic</b> <b>logic.</b> His 1981 paper on a relational language for parallel programming introduced concurrent logic programming.|$|E
50|$|In any {{consideration}} {{of the use of}} logic to model law it needs to be borne in mind that law is inherently non-monotonic, as is shown by the rights of appeal enshrined in all legal systems, {{and the way in which}} interpretations of the law change over time. Moreover, in the drafting of law exceptions abound, and, in the application of law, precedents are overturned as well as followed. In logic programming approaches, negation of failure is often used to handle non-monotonicity, but specific <b>non-monotonic</b> <b>logics</b> such as defeasible logic have also been used. Following the development of abstract argumentation, however, these concerns have been addressed through argumentation rather than through the use of <b>non-monotonic</b> <b>logics.</b>|$|R
40|$|Distributed ontologies {{expressed}} as description logics may define repeated information. To reason about concepts that these ontologies express, a possible {{option is to}} generate unique concept definitions in a different terminology or TBox. The {{creation of a new}} terminology from different ontologies need to be consistent, and expressed with <b>non-monotonic</b> <b>logics</b> to be further updated with new distributed ontologies. The model AGM of theory change seems to be an interesting framework to be studied in conjunction with description logics and generate a new <b>non-monotonic</b> description <b>logics</b> model. Eje: Agentes y Sistemas Inteligente...|$|R
40|$|Abstract. In {{the field}} of <b>non-monotonic</b> <b>logics,</b> the lexicographic closure is {{acknowledged}} as a a powerful and logically well-characterized approach; {{we are going to}} see that such a construction can be applied in {{the field of}} Description Logics, an important knowledge representation formalism, and we shall provide a simple decision procedure. ...|$|R
50|$|His main {{research}} {{interest is}} integrating logical and psychological accounts of reasoning. Recent work includes investigations of interpretative processes in reasoning and, with Michiel van Lambalgen at the Institute for Logic, Language and Computation in Amsterdam, {{the use of}} <b>non-monotonic</b> <b>logic</b> and neural network implementations to model reasoning.|$|E
50|$|The {{fact that}} Horn clauses {{can be given}} a {{procedural}} interpretation and, vice versa, that goal-reduction procedures {{can be understood as}} Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of <b>non-monotonic</b> <b>logic.</b>|$|E
50|$|An {{expert in}} pure and applied logic, his {{research}} largely focused on issues in defeasible reasoning and <b>non-monotonic</b> <b>logic.</b> His more recent work in philosophy of logic was concerned with  applications of generalized quantifier theory and abstraction principles to {{the foundations of}} arithmetic in the more general context of Fregean foundations, as well as making contributions to Frege scholarship.|$|E
5|$|The {{frame and}} {{qualification}} problems. AI researchers (like John McCarthy) who used logic {{discovered that they}} could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new <b>logics</b> (like <b>non-monotonic</b> <b>logics</b> and modal logics) to try to solve the problems.|$|R
40|$|Non-monotonic {{reasoning}} {{has attracted}} interest {{for at least}} two reasons. First, although human beings are only provided with incomplete knowledge about the real world, this does not significantly restrict our reasoning activities. We regard it as natural to revise previously drawn conclusions in the presence of new information. Second, computational resources are bounded. A data or knowledge base cannot be stuffed with explicit information about all {{that is not the case}} in the world. <b>Non-monotonic</b> <b>logics</b> promise to model such common-sense reasoning and to combine declarative representation of knowledge with reasoning about implicitly represented information. During the last decade intensive research efforts have been spent on different formalizations of non-monotonic reasoning. It turns out that <b>non-monotonic</b> <b>logics</b> have to cope with two problems: first the consequence relation of the underlying monotonic logic and second the additional machinery which leads to non-monotonicity. Unfort [...] ...|$|R
40|$|We {{present a}} {{succession}} of presentations of an argumentation-theoretic proof procedure that applies uniformly {{to a wide variety}} of logics for default reasoning, including Theorist, default logic, logic programming, autoepistemic <b>logic,</b> <b>non-monotonic</b> modal <b>logic</b> and certain instances of circumscription...|$|R
50|$|Third, commonsense {{reasoning}} involves plausible reasoning. It requires {{coming to a}} reasonable conclusion given what is already known. Plausible reasoning has been studied {{for many years and}} there are a lot of theories developed that include probabilistic reasoning and <b>non-monotonic</b> <b>logic.</b> It takes different forms that include using unreliable data and rules, whose conclusions are not certain sometimes.|$|E
50|$|In the {{simplest}} {{case in which}} H, B1, …, Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there exist many extensions of this simple case, {{the most important one}} being the case in which conditions in the body of a clause can also be negations of atomic formulae. Logic programming languages that include this extension have the knowledge representation capabilities of a <b>non-monotonic</b> <b>logic.</b>|$|E
50|$|Proof-theoretic {{formalization}} of a <b>non-monotonic</b> <b>logic</b> {{begins with}} adoption of certain non-monotonic rules of inference, and then prescribes {{contexts in which}} these non-monotonic rules may be applied in admissible deductions. This typically is accomplished by means of fixed-point equations that relate the sets of premises and the sets of their non-monotonic conclusions. Defaults logics and autoepistemic logic {{are the most common}} examples of non-monotonic logics that have been formalized that way.|$|E
25|$|Default <b>logics,</b> <b>non-monotonic</b> <b>logics</b> and {{circumscription}} are {{forms of}} logic {{designed to help}} with default reasoning and the qualification problem. Several extensions of logic {{have been designed to}} handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus; and modal logics.|$|R
40|$|In this paper, we {{introduce}} a sequent calculus CIRC for propositional Circumscription. This work {{is part of}} a larger project, aiming at a uniform proof-theoretic reconstruction of the major families of <b>non-monotonic</b> <b>logics.</b> Among the novelties of the calculus, we mention that CIRC is analytic and comprises an axiomatic rejection method, which allows for a fully detailed formalization of the nonmonotonic aspects of inference. 1 Introduction <b>Non-monotonic</b> <b>logics</b> play a fundamental role in knowledge representation and commonsense reasoning, as well as in the theory of programming languages. The semantic and algorithmic aspects of non-monotonic reasoning have been extensively investigated (e. g. see [21, 25, 12, 16, 17, 8, 28, 33, 24] and [29, 26, 2, 5, 35, 1]). On the other hand, the proof-theoretic aspects are not yet completely understood. The fundamental papers by Gabbay [13], Makinson [23] and Kraus, Lehmann and Magidor [18], focus their attention on general properties of [...] ...|$|R
40|$|This paper {{argues that}} the goals people have when {{reasoning}} determine their own norms of reasoning. A radical descriptivism which avoids norms never worked for any science; nor can it work for the psychology of reasoning. Norms as we understand them are illustrated with examples from categorical syllogistic reasoning and the `new paradigm' of subjective probabilities. We argue that many formal systems are required for psychology: classical <b>logic,</b> <b>non-monotonic</b> <b>logics,</b> probability logics, relevance logic, and others. One of the hardest challenges is working out what goals reasoners have and choosing and tailoring the appropriate logics to model the norms those goals imply...|$|R
5000|$|One {{controversial}} theoretical {{issue is}} the identification of an appropriate competence model, or a standard against which to compare human reasoning. Initially classical logic was chosen as a competence model. Subsequently some researchers opted for <b>non-monotonic</b> <b>logic</b> and Bayesian probability. Research on mental models and reasoning {{has led to the}} suggestion that people are rational in principle but err in practice. [...] Connectionist approaches towards reasoning have also been proposed.|$|E
50|$|Efforts {{have been}} made {{within the field of}} {{artificial}} intelligence to perform and analyze the act of argumentation with computers. Argumentation has been used to provide a proof-theoretic semantics for <b>non-monotonic</b> <b>logic,</b> starting with the influential work of Dung (1995). Computational argumentation systems have found particular application in domains where formal logic and classical decision theory are unable to capture the richness of reasoning, domains such as law and medicine. In Elements of Argumentation, Philippe Besnard and Anthony Hunter introduce techniques for formalizing deductive argumentation in artificial intelligence, emphasizing emerging formalizations for practical argumentation.|$|E
50|$|Circumscription is a <b>non-monotonic</b> <b>logic</b> {{created by}} John McCarthy to formalize {{the common sense}} {{assumption}} that things are as expected unless otherwise specified. Circumscription was later used by McCarthy {{in an attempt to}} solve the frame problem. To implement circumscription in his initial formulation, McCarthy augmented first-order logic to allow the minimization of the extension of some predicates, where the extension of a predicate is the set of tuples of values the predicate is true on. This minimization is similar to the closed world assumption that what is not known to be true is false.|$|E
40|$|We {{study the}} {{transformation}} of "predicate introduction" in <b>non-monotonic</b> <b>logics.</b> By this, we mean the act of replacing a complex formula by a newly defined predicate. From a knowledge representation perspective, such transformations {{can be used to}} eliminate redundancy or to simplify a theory. From a more practical point of view, they {{can also be used to}} transform a theory into a normal form imposed by certain inference programs or theorems. In this paper, we study predicate introduction in the algebraic framework of "approximation theory"; this is a fixpoint theory for non-monotone operators that generalizes all main semantics of various <b>non-monotonic</b> <b>logics,</b> including logic programming, default logic and autoepistemic logic. We prove an abstract, algebraic equivalence result in this framework. This can then be used to show that, in logic programming, certain transformations are equivalence preserving under, among others, both the stable and well-founded semantics. Based on this result, we develop a general method of eliminating universal quantifiers in the bodies of rules. Our work is, however, also applicable beyond logic programming. In a companion paper, we demonstrate this, by using the same algebraic results to derive a transformation which reduces the nesting depth of the modal operator K in autoepistemic logic. status: publishe...|$|R
40|$|One of {{the most}} {{important}} developments over the last twenty years both in logic and in Artificial Intelligence is the emergence of so-called <b>non-monotonic</b> <b>logics.</b> These logics were initially developed by McCarthy [10], McDermott & Doyle [13], and Reiter [17]. Part of the original motivation was to provide a formal framework within which to model cognitive phenomena such as defeasible inference and defeasible knowledge representation, i. e., to provide a formal account of the fact that reasoners can reach conclusions tentatively, reserving the right to retract them in the light of further information. This initial intuition has given rise to a wealth of formal frameworks, from circumscription (McCarthy [11]) to default logic (Reiter [17]) to autoepistemic logic (Moore [14]), to cite only a few. All these frameworks, however, single out a notion of inferential consequence which is either computationally hard, or mathematically not well-behaved (essentially because of {{what has come to be}} known as the “multiple-extensions ” problem), or both. In this paper, we will claim that such computational complexity and mathematical behavior turn out to be problematic if we want to adhere to the original motivation for the development of <b>non-monotonic</b> <b>logics,</b> and that they might ultimately make non-monotonic formalisms not viable as cognitive models...|$|R
40|$|In {{this article}} I use the {{distinction}} between hard and soft information from the dynamic epistemic logic tradition to extend prior work on informational conceptions of <b>logic</b> to include <b>non-monotonic</b> consequence-relations. In particular, I defend the claim {{that at least some}} <b>non-monotonic</b> <b>logics</b> can be understood on the basis of soft or ‘belief-like’ logical information, and thereby question the orthodox view that all logical information is hard, ‘knowledge-like’, information...|$|R
