1|10000|Public
30|$|The {{starting}} point in system behavior recognition is generating a static reservoir model. Generally, in simulation and modeling, the <b>number</b> <b>of</b> <b>known</b> <b>parameters</b> {{is less than}} that of the unknown ones. Therefore, applying a suitable estimation method for solving the problem is essential. In addition to all estimation techniques, simulations based on geostatistical methods, such as Sequential Gaussian simulation (SGS), seem to be very efficient. In the SGS method, different realizations can be produced from a data series with the same probability. This method is an appropriate technique for generating data with constant spatial variability of statistical parameters. In this study, after generating the heterogeneity factors with the SGS method, a 5 -spot standard model has been applied for the basic wells’ arrangement. Then heterogeneities in five different correlation ranges (drawn from the SGS model results) were applied in the basic model.|$|E
3000|$|How much information, i.e., <b>number</b> <b>of</b> <b>known</b> SSL/TLS <b>parameters</b> and pairings to HTTP headers, {{do we need}} {{to analyze}} a {{significant}} portion of network traffic? [...]...|$|R
50|$|C {{allows for}} {{functions}} {{to accept a}} variable <b>number</b> <b>of</b> <b>parameters,</b> <b>known</b> as variadic functions.|$|R
40|$|The {{positive}} semidefinite minimum rank of {{a simple}} graph G is defined to be the smallest possible rank over all positive semidefinite real symmetric matrices whose ijth entry (for i = j) is nonzero whenever {i, j} is an edge in G and is zero otherwise. The computation of this parameter directly is difficult. However, there are a <b>number</b> <b>of</b> <b>known</b> bounding <b>parameters</b> and techniques which can be calculated and performed on a computer. We programmed an implementation of these bounds and techniques in the open-source mathematical software Sage. The program, {{in conjunction with the}} orthogonal representation method, establishes the positive semidefinite minimum rank for all graphs of order 7 or less...|$|R
40|$|We {{report on}} a {{systematic}} radial velocity survey for double degenerate (DD) binaries as potential progenitors of type Ia supernovae: SPY (ESO Supernovae Ia Progenitor surveY). More than 1000 white dwarfs and pre-white dwarfs were observed with the VLT. Our aim is to perform a statistically significant test of the DD scenario. SPY detected more than 100 new binary white dwarfs, dramatically increasing the <b>number</b> <b>of</b> <b>known</b> DDs. System <b>parameters</b> are determined from ongoing follow-up observations. Our sample includes systems with masses close to the Chandrasekhar limit and a probable SN Ia progenitor candidate...|$|R
40|$|Usually when {{determining}} parameters with {{an inverse}} method, {{it is assumed}} that parameters or properties, other than those being sought, are known exactly. When such <b>known</b> <b>parameters</b> are uncertain, the inverse solution can be very sensitive to the degree of uncertainty. The stochastic regularization method can be modi ed to reduce this sensitivity. This paper presents such a modi cation. In addition, the relationship between Tikhonov-Phillips regularization and stochastic regularization is described. Because of this relationship, it is possible to modify the usual Tikhonov-Phillips regularization to account for such uncertainties. Nomenclature A discretized form of F () b uncertain <b>known</b> <b>parameters</b> E[] expected value F () system model response H discretized form of 8 () I Identity matrix K <b>number</b> <b>of</b> uncertain <b>known</b> <b>parameters</b> L quantity to be minimized M <b>number</b> <b>of</b> sought parameters N <b>number</b> <b>of</b> data points t time V extended covariance matrix W weighting matrix x spatial location z measurements regularizing parameter rms value of stochastic variable standard deviation 6 covariance matrix parameters ^ estimated parameter 8 () penalty function subscript...|$|R
40|$|Original paper can {{be found}} at: [URL] Astronomical Society of the PacificWe report on a {{systematic}} radial velocity survey for double degenerate (DD) binaries as potential progenitors of type Ia supernovae: SPY (ESO Supernovae Ia Progenitor surveY). More than 1000 white dwarfs and pre-white dwarfs were observed with the VLT. Our aim is to perform a statistically significant test of the DD scenario. SPY detected more than 100 new binary white dwarfs, dramatically increasing the <b>number</b> <b>of</b> <b>known</b> DDs. System <b>parameters</b> are determined from ongoing follow-up observations. Our sample includes systems with masses close to the Chandrasekhar limit and a probable SN Ia progenitor candidate...|$|R
40|$|Abstract: This paper {{addresses}} {{the problem of}} joint multi-source localization and environment perception in wireless sensor networks (WSNs) based on received signal strength (RSS). Contrary to the traditional RSS approaches which assume <b>known</b> <b>number</b> <b>of</b> sources and <b>known</b> <b>parameter</b> <b>of</b> environments, we propose to estimate the sources with unknown number and the unknown environmental parameter simultaneously. By assuming that the sources are sparse in the sensing field, a non-convex ℓ 1 regularized least squares problem is formulated. This non-convex problem is then decomposed to two simple subproblems, one for multi-source localization and another for environment perception. A joint multi-source localization and environment perception algorithm is proposed, with extensive simulations to validate its effectiveness...|$|R
40|$|The {{luminosity}} {{function of}} active galactic nuclei (AGNs) has been measured down to luminosities ~ 10 ^(42) ergs s^(- 1) {{in the soft}} and hard X-rays. Some fraction of this activity {{is associated with the}} accretion of the material liberated by the tidal disruption of stars by massive black holes. We estimate the contribution to the X-ray luminosity function from the tidal disruption process. While the contribution depends on a <b>number</b> <b>of</b> poorly <b>known</b> <b>parameters,</b> it appears that it can account for the majority of X-ray-selected AGNs with soft or hard X-ray luminosities ≲ 10 ^(43) - 10 ^(44) ergs s^(- 1). If this is correct, {{a significant portion of the}} X-ray luminosity function of AGNs is comprised of sources powered by tidal disruption at the faint end, while the sources at the bright end are powered by nonstellar accretion. Black holes with masses ≲ 2 × 10 ^ 6 M_☉ could have acquired most of their present mass by an accretion of tidal debris. In view of the considerable theoretical uncertainty concerning the detailed shape of the light curves of tidal disruption events, we focus on power-law luminosity decay (as identified in candidate tidal disruption events), but we also discuss constant accretion rate models. We expect that the sources associated with thin-disk accretion of circularized tidal debris have peak luminosities of ≲ 10 ^(39) - 10 ^(41) ergs s^(- 1), below the luminosity range probed in present surveys...|$|R
40|$|The current {{attempt is}} aimed to outline the {{geometrical}} framework <b>of</b> a well <b>known</b> statistical problem, concerning the explicit {{expression of the}} arithmetic mean standard deviation distribution. To this respect, after a short exposition, three steps are performed as 1) formulation of the arithmetic mean standard deviation {{as a function of}} the errors which, by themselves, are statistically independent; 2) formulation of the arithmetic mean standard deviation distribution {{as a function of the}} errors; 3) formulation of the arithmetic mean standard deviation distribution as a function of the arithmetic mean standard deviation and the arithmetic mean rms error. The integration domain can be expressed in canonical form after a change of reference frame in the n-space, which is recognized as an infinitely thin n-cylindrical corona where the symmetry axis coincides with a coordinate axis. Finally, the solution is presented and a <b>number</b> <b>of</b> (well <b>known)</b> related <b>parameters</b> are inferred for sake of completeness...|$|R
40|$|The Yarkovsky effect, a non-gravitational {{acceleration}} {{produced by}} the anisotropic emission of thermal energy (Öpik, 1951, Proc. Roy. Irish Acad. 54, 165 – 199), {{plays an important role}} in the dynamical evolution of asteroids. Current theoretical models of the Yarkovsky effect, however, rely on a <b>number</b> <b>of</b> poorly <b>known</b> <b>parameters</b> that can only approximate how real asteroids respond to solar heating. To improve this situation, we investigated whether the orbital distribution of the Karin cluster, a 5. 8 ± 0. 2 Myr old S-type asteroid family (Nesvorný et al., 2002 a, Nature 417, 720 – 722), could be used to determine the rate at which multikilometer main-belt asteroids spread in semimajor axis due to the Yarkovsky effect. Our results indicate that the orbital histories of individual Karin cluster members bear clear signatures of having drifted in semimajor axis drift since their formation. Using numerical methods, we determined the drift speed of ≈ 70 Karin cluster members (asteroids 1 – 6 km in diameter). This is the first time the speed that main-belt asteroids evolve in the semimajor axis due to the non-gravitational effects have been measured. The magnitude of measured speeds is similar to those predicted by theoretical models of the Yarkovsky force. Taken together, our results represent the first direct detection of the Yarkovsky effect for main-belt asteroids, and they validate in significant ways the asteroid thermal models described in the recent literature (e. g., Vokrouhlický, 1999, Astron. Astrophys. 344, 362 – 366). By comparing the measured drift speeds to those calculated from theoretical models of the Yarkovsky effect, we determined that Karin cluster members do not have surface thermal conductivities K in excess of ∼ 0. 1 W m− 1 K− 1. Instead, their derived K values are consistent with the presence of regolith over most/all of their ∼ 5. 8 Myr lifetimes. This low-conductive regolith layer may be thin becaus...|$|R
40|$|Increasingly, science {{relies on}} complex {{numerical}} models to aid understanding of physical phenomena. Often the equations in such models contain a high <b>number</b> <b>of</b> poorly <b>known</b> <b>parameters</b> {{so that the}} resulting output encodes much uncertainty. A 'computer simulator', which comprises the model equations together with a solver routine, produces a solution for a given choice of these 'input' parameters. In cases where the dimension of the input parameter space is high, {{we can only hope}} to obtain a thin coverage of the space by running the simulator. Building a representation of the simulator output {{as a function of the}} input, then, is a statistical problem in which we observe output at a collection of input choices and, based on these observations, infer output values for unseen inputs about which we are uncertain. In a Bayesian context, this representation, termed the 'emulator', encodes our beliefs about the relationships between inputs and outputs. Our interest is in exploiting the structure of compartmental models to aid in this process. Compartmental models are widely applied to model systems in the absence of fundamental equations to describe the processes of interest. We show that the structure of such models enables us to efficiently generate additional function information, in the form of input derivatives, each time we run the simulator and we adapt the emulator methodology to allow for derivatives. We show that considering derivatives offers a range of natural ways to aid assessment of prior beliefs and that updating based on derivatives can lead to substantial reduction in emulator uncertainty. We show that, in addition, the model structure allows us to derive estimates of increased costs of generating derivatives which we can compare against the corresponding reduction in uncertainties. We are motivated throughout by the problem of calibrating a compartmental model of plankton cycles at multiple locations in the sea, and we show that a knock on effect of reduction of uncertainty by derivatives is an improvement in our ability to perform this calibration. The search for a model which could accurately reproduce plankton cycles at various physical locations, if successful, is thought to have significant ramifications for understanding climate change. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|A Geographic Information System (GIS) {{and fuzzy}} {{modelling}} {{were used to}} develop a habitat suitability index for the noble crayfish, Astacus astacus. The model is based on crayfish distribution data for the federal state Hesse, Germany, which had been recorded between 1988 and 1996. It includes 185 sites with noble crayfish in 126 watercourses. Official data on the morphological quality of surface waters recorded between 1996 and 1998 by order of the Ministry of Hesse for Environment, Rural Areas and Consumers Protection was {{used to describe the}} habitat features. One third of the crayfish sites was selected by chance to determine habitat properties significantly associated with crayfish occurrence by means of the Kolmogorov-Smirnoff test and frequency analysis. Meaningless associations were excluded based on expert knowledge. Five parameters related to the structure of the riverbed and the bankside were incorporated in the fuzzy model. The model was complemented by a <b>number</b> <b>of</b> <b>parameters</b> <b>known</b> to exclude the occurrence of the noble crayfish (e. g. pipes, concrete embankments). Finally, a habitat suitability index for every stretch of water in Hesse was calculated. The predictive power of the fuzzy model was tested on the remaining distribution data set for Astacus astacus. The habitat suitability index differed slightly but significantly (p < 0. 001) between sites with and sites without crayfish occurrence. Fuzzy modelling proved to be useful for assessing habitat suitability with respect to crayfish, though further improvements of the model appeared to be necessary for a more reliable prediction of noble crayfish sites...|$|R
40|$|The {{purpose of}} this study was to {{determine}} whether baseline patients' self reported health-related quality of life (HRQOL) parameters could predict survival beyond key biomedical prognostic factors in patients with metastatic colorectal cancer. The analysis was conducted on 299 patients. HRQOL baseline scores were assessed using the European Organisation for Research and Treatment of Cancer, Quality of Life Questionnaire-Core 30 (EORTC QLQ-C 30). The Cox proportional hazards regression model was used for both univariate and multivariate analyses of survival. In addition, a bootstrap resampling technique was used to assess the stability of the outcomes. The final multivariate Cox regression model retained four variables as independent prognostic factors for survival: white blood cell (WBC) count with a hazard ratio (HR) of 1. 961 (95 % CI, 1. 439 - 2. 672; P < 0. 001), alkaline phosphatase with HR = 1. 509 (95 % CI, 1. 126 - 2. 022; P = 0. 005), <b>number</b> <b>of</b> sites involved with HR = 1. 108 (95 % CI, 1. 024 - 1. 198; P = 0. 01) and the patient's score on the social functioning scale with HR = 0. 991 (95 % CI, 0. 987 - 0. 996; P < 0. 001) which translates into a 9 % decrease in the patient's hazard of death for any 10 point increase. The independent prognostic importance of social functioning and the stability of the final Cox regression model were also confirmed by the additional bootstrap model averaging analysis, based on 1000 bootstrap-generated samples. The results suggest that social functioning, acts as a prognostic measure of survival beyond a <b>number</b> <b>of</b> previously <b>known</b> biomedical <b>parameters.</b> (c) 2005 Elsevier Ltd. All rights reserved...|$|R
40|$|Land surface models (LSMs) are {{increasingly}} {{called upon to}} represent not only the exchanges of energy, water and momentum across the land-atmosphere interface (their original purpose in climate models), but also how ecosystems and water resources 5 respond to climate and atmospheric environment, and how these responses in turn in-fluence land-atmosphere fluxes of carbon dioxide (CO 2), trace gases and other species that affect the composition and chemistry of the atmosphere. However, the LSMs embedded in state-of-the-art climate models differ in how they represent fundamental aspects of the hydrological and carbon cycles, resulting in large inter-model differences 10 and sometimes faulty predictions. These “third-generation” LSMs respect the close coupling of the carbon and water cycles through plants, but otherwise tend to be underconstrained, and have not taken full advantage of robust hydrological parameterizations that were independently developed in offline models. Benchmarking, combining multiple sources of atmospheric, biospheric and hydrological data, should be a required 15 component of LSM development, but this field has been relatively poorly supported and intermittently pursued. Moreover, benchmarking alone {{is not sufficient to}} ensure that models improve. Increasing complexity may increase realism but decrease reliability and robustness, by increasing the <b>number</b> <b>of</b> poorly <b>known</b> model <b>parameters.</b> In contrast, simplifying the representation of complex processes by stochastic param- 20 eterization (the representation of unresolved processes by statistical distributions of values) has been shown to improve model reliability and realism in both atmospheric and land-surface modelling contexts. We provide examples for important processes in hydrology (the generation of runoff and flow routing in heterogeneous catchments) and biology (carbon uptake by species-diverse ecosystems). We propose that the way 25 forward for next-generation complex LSMs will include: (a) representations of biological and hydrological processes based on the implementation of multiple internal constraints; (b) systematic application of benchmarking and data assimilation techniques to optimize parameter values and thereby test the structural adequacy of models; and (c) stochastic parameterization of unresolved variability, applied in both the hydrological and the biological domains...|$|R
50|$|For {{train and}} road traffic noise, the {{description}} of the sources is usually made in terms <b>of</b> <b>known</b> <b>parameters,</b> such as speed, <b>number</b> <b>of</b> vehicles etc. Measurements are used for the validation of results.|$|R
3000|$|... where, A :R^MR^N is {{a matrix}} <b>of</b> <b>known</b> <b>parameters</b> that {{represents}} a rendering process. Equation (4) can be solved by linear least squares in principle if we have sufficient observations.|$|R
40|$|Five {{different}} shrinkage formulas {{were compared}} to see which most accurately reduced the positive bias in sample R 2 values as estimators of the squared population multiple correlation coefficient (&rho; 2). Artificial populations <b>of</b> <b>known</b> <b>parameters</b> were constructed and "Monte Carlo " sampling was done with a computer. The fol-lowing parameters were examined: <b>number</b> <b>of</b> independent variable...|$|R
40|$|In {{this article}} {{discriminant}} analysis is extended to directional statistics. Optimal identification rules are developed for distinguishing different distributions defined on a circle or on a sphere. Details are given for Dimroth-Watson's, Selby's, Arnold's and angular central Gaussian distributions {{in the case}} <b>of</b> <b>known</b> <b>parameters...</b>|$|R
40|$|We {{describe}} a thought experiment using an isolated system <b>of</b> <b>known</b> <b>parameters</b> and assuming the correctness of Clausius and Boltzmann descriptions of entropy. The experiment produced an astronomical {{increase in the}} <b>number</b> <b>of</b> possible ways the system can be arranged and inescapably in achieving this, {{an increase in the}} compartment volume of the system. The result may possibly be of significance to cosmology. Comment: 8 pages, no figure...|$|R
40|$|Abstract. In many {{scenarios}} we {{are interested}} in examining the spectral content of a measured signal. In general the methods of estimating the spectral content are either parametric or non-parametric. Parametric methods are those which take advantage <b>of</b> <b>known</b> <b>parameters</b> <b>of</b> the signal, such the <b>number</b> <b>of</b> tones it contains. Non-parametric methods do not make such assumptions apriori. In this document we examine two parametric algorithms: MUSIC and ESPRIT, both of which assume a <b>known</b> <b>number</b> <b>of</b> tones in the measured signal. 1...|$|R
40|$|A chiral {{invariant}} effective Lagrangian {{may be used}} {{to calculate}} the three-body interactions among low-energy pions and nucleons in terms <b>of</b> <b>known</b> <b>parameters.</b> This method is illustrated by the calculation of the pion-nucleus scattering length. Comment: 16 pages plus four figures, LATEX, Texas preprint UTTG- 11 - 9...|$|R
40|$| Universe. However, the <b>number</b> <b>of</b> <b>known</b>|$|R
40|$|The major {{objective}} of this thesis is to develop instrumentation and parameter estimation algorithms for nondestructive measurement of non-homogeneous material property profiles with fringing electric field dielectrometry sensors. The instrumentation includes interdigital sensors and sensor arrays, other types of fringing field sensors, electronic circuit boards for measurement of sensor signals, and mechanical setups for specific applications. The parameter estimation algorithms require solving forward and inverse problems of material property estimation. The forward problem implies calculation of the sensor admittance matrix {{as a function of}} geometry and material properties. The inverse problem, inherently more difficult than the forward problem, implies estimation of unknown geometry and material properties based on known properties and measured entries of the sensor admittance matrix. The developed instrumentation and algorithms are applied to practical problems which include mo++nitoring of moisture dynamics in transformer pressboard, evaluation of the saturation state of chemical garments, detection of flaws in fiberglass flywheels, and detection of buried metal and plastic landmines. The design strategy and fabrication practices are described for multiple penetration depth interdigital sensors designed for measurement of conductivity and permittivity of electrical insulation of power transformers. An extensive overview of interdigital electrode technology in other fields is given. A <b>number</b> <b>of</b> disturbance parameters that affect interdigital dielectrometry measurements is characterized and either eliminated or accounted for using empirical, analytical, and numerical simulation approaches. A new type of fringing field sensor has been developed to improve the cross-correlation between different fringing field patterns. In most cases, the forward problem has been solved using commercial finite-element software "Maxwell" by Ansoft Corp. Other methods, such as a co++ntinuum model, analytical expressions, and direct calibration were used for comparison and to achieve greater accuracy in simple cases. A family of algorithms for solving inverse problems has been developed to address different applications. (cont.) No single algorithm provides the most accurate and reliable results in all cases. The most appropriate algorithm for each given application should be chosen on the basis of required speed and accuracy, <b>number</b> <b>of</b> <b>known</b> and unknown <b>parameters,</b> type of distribution of material properties, contact conditions between the sensor head and the material, and a specific type of sensor selected for the task. Major types of property estimation algorithms include direct calibration; use of empirically and numerically determined approximations; use of pre-computed lookup tables; iterative guesses at dielectric and geometry properties while solving the minimization problem of matching theoretical and measured entries of sensor admittance matrix; direct mapping between the sensor output and the physical variable of interest (not necessarily a dielectric property);(cont.) pattern recognition in the dielectric spectroscopy signature; and search for signal characteristics in the sensor output due to material property variations. Each of these major types of algorithms has been implemented in one or more forms to achieve the desired results for each specific problem. One of the algorithmic approaches has been generalized to other types of problemsby implementing it as a generic optimization tool. Moisture dynamics in transformer pressboard has been studied extensively with numerical simulations of the forward and inverse problem. The developed algorithm has been applied to experimental dataobtained by another graduate student, Yanqing Du, in a concurrent Ph. D. thesis. It has been demonstrated that a three-wavelength interdigital sensor can be used to measure time-dependent continuous smoothly varying moisture profiles in oil-impregnated power transformer pressboard. Ultimately, this technology is capable of preventing partial discharges and transformer failures due to flow electrification and static charging of the oil-pressboard interface. Preliminary measurements also demonstrated adequate sensitivity and selectivity of fringing field sensors for the detection of flaws in fiberglass flywheels and detection and discrimination of buried plastic and metal landmines. The saturation state of chemical protective garments hasbeen determined for relatively high levels of saturation. Additional work is needed to improve sensitivity in the low saturation region. by Alexander V. Mamishev. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1999. Vita. Includes bibliographical references (v. 2, p. 677 - 704). This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections...|$|R
25|$|Nevertheless, {{sequence}} similarity is {{the most}} commonly used form of evidence to infer relatedness, since the <b>number</b> <b>of</b> <b>known</b> sequences vastly outnumbers the <b>number</b> <b>of</b> <b>known</b> tertiary structures. In the absence of structural information, sequence similarity constrains the limits of which proteins can be assigned to a superfamily.|$|R
50|$|In a {{simplified}} view of US digital-TV distribution, the cable/sat operator receives a programing feed (producer) from a network station, repackages it for carriage on a data-network or distribution channel (satellite, digital-cable) <b>of</b> <b>known</b> <b>parameters,</b> then re-transmits the modified bitstream to the customer-site (viewer.) The viewer's set-top-box decoder decompresses the delivered bitstream, and displays the program on-screen.|$|R
50|$|The <b>number</b> <b>of</b> <b>known</b> non-isoprenoid indole {{alkaloids}} {{is small}} compared to the <b>number</b> <b>of</b> indole alkaloids.|$|R
40|$|Global {{energy use}} is increasing. As {{societies}} advance, {{they will continue}} to need energy to power residential and commercial buildings, in the industrial sector, for transportation and other vital services. To satisfy this rising demand, liquid, natural gas, coal, nuclear power and renewable fuel sources are extensively developed. Particularly fossil fuels (i. e. oil, natural gas and coal) remain the largest source of energy for the world. Petroleum exploration and production companies continuously develop new and enhance current production technologies to increase recovery from the existing fields. These companies rely on various tools to support their production and development decisions. Reservoir modeling is a standard tool used in the decision making process allowing analysis and prediction of the reservoir flow behavior, identification of beneficial production strategies and evaluation of the associated risks. The models used for reservoir simulation contain a large <b>number</b> <b>of</b> imperfectly <b>known</b> <b>parameters</b> characterizing the reservoir flow, e. g. permeability and porosity of the reservoir rock. Therefore the predictive value of such models is limited and tends to deteriorate in time. History matching is employed to update the values <b>of</b> poorly <b>known</b> model <b>parameters</b> in time {{with the help of the}} production data which become available during the production life of the reservoir, i. e. to adapt parameters such that simulated results are consistent with measured production data. Such an approach generally improves estimates of the model parameters and the predictive capability of the model. Remarkably, the information extracted from the measurements in the history matching phase is repeatedly found as not enough to provide well-calibrated model with a high predictive value. Hence, consideration of additional data can be of particular help. To optimize the costs and effort associated with collection of new data and computations, up-front selection of the most influential measurements and their locations is desirable. Methods to assess the impact of measurements on model parameter updating are therefore needed. The research objective of this thesis was to develop efficient tools for quantifying the impact of measured data on the outcome of history matching of reservoir models, i. e. tools that provide a meaningful quantification of the impact of observations, while requiring limited time and effort to be incorporated in the history matching algorithms. This research addressed history matching two-dimensional two-phase reservoir model representing water flood with production data (bottom hole pressure at injection well and oil and water flow rates at production wells). First, the applicability and implementation <b>of</b> a <b>number</b> <b>of</b> history matching algorithms were investigated. The representer method (RM) has been considered as an example of variational techniques. The algorithm’s key feature is the computation of a set of so-called representers describing the influence of a certain measurement on an estimation of the state and/or parameter. The RM was found to provide a reasonable parameter estimate, although it is computationally inefficient for dealing with large data sets. This fact yielded testing of the accelerated representer method (ARM), where direct computation of representers is avoided. The results indicate that the accuracy of the ARM can be controlled to provide an outcome of the same accuracy as the RM, and that the ARM outperforms the classical RM in terms of computational speed when the <b>number</b> <b>of</b> assimilated measurements increases. In this thesis we developed a strategy to evaluate the <b>number</b> <b>of</b> operations performed by the methods to assess the amount of data for which the ARM becomes beneficial to use. The RM and the ARM require the model adjoint and are not intended for continuous (sequential) history matching, namely for incorporating obtained data in the model on the fly. Instead they perform history matching over a rather long time window using all available observations. The ensemble Kalman filter (EnKF) has been discussed as it is the algorithm for continuous history matching. The EnKF schemes do not require the model adjoint, which makes them very attractive for data assimilation with complex non-linear models. The use of the EnKF in reservoir engineering however is prone to producing physically unreasonable values of the state variables. The problem can be overcome by including a so-called confirmation step in the algorithm. The EnKF, particularly with a confirmation step, is often computationally demanding for large-scale applications. The asynchronous EnKF (AEnKF) is a modification of the EnKF which offers a practical way to perform history matching in such cases by updating the system with batches of measurements collected at the times different to the time of the update. Hence, all observations collected during a certain time-window can be history-matched at once at the end of observational period. This allows for comparison of the influence of the observations collected at different times. Furthermore, it does not rely on an adjoint model, though it resembles the approach usually followed in variational methods. Both the EnKF and the AEnKF demonstrated considerable improvement of the model parameter estimates compared to the prior and gave acceptable history matches. Since the AEnKF allows for history matching all the data gathered throughout the observational period at once, it permits comparison of the effect of observations collected at different time instances. The equivalence of the AEnKF to variational techniques (e. g. the RM) yields the possibility to evaluate if ensemble Kalman filtering and variational methods utilize the observations in a similar manner. The representer method and the AEnKF were selected to be used as platforms for quantification of the measurements impact on history matching. Secondly, in this thesis we developed a tool to quantify the impact of measured data on the outcome of history matching. The method has been inspired by the recent advancements in meteorology and oceanography, and is based on a so-called sensitivity matrix. This matrix can be used to evaluate the amount of information extracted from available data during the data assimilation phase and identify the observations that have contributed to the parameter update the most. In particular, we used the diagonal elements <b>of</b> the matrix, <b>known</b> as self-sensitivities, as a quantitative measure of the influence of observed measurements on predicted measurements. Additionally, we have proposed a way to use the norm of the sensitivity matrix for assessing the magnitude of possible change in the accuracy of the model due to the respective change in the accuracy of collected observations. The observation sensitivity matrix is fast and easy to compute both for adjoint-based and EnKF types of history matching algorithms. The analysis performed with the aid of the observation sensitivity matrix has confirmed that the RM and the AEnKF utilize the data with comparable effectiveness. Remarkably, for a simple test case the global averaged influence of the observed measurements is only 4 %. This is a rather low value compared to the 96 % global averaged influence of the prior. The observation sensitivity matrix can be also used to investigate the dependency between the measurement location / type and its importance to history matching. Applied mathematicsElectrical Engineering, Mathematics and Computer Scienc...|$|R
30|$|Equal-sized {{facilities}} (machines) {{are assigned}} to the same <b>number</b> <b>of</b> <b>known</b> locations.|$|R
5000|$|Endemics - <b>Number</b> <b>of</b> <b>known</b> {{endemic species}} per million square {{kilometer}} land area ...|$|R
5000|$|The set {{of direct}} factors (of an effect) has a <b>number</b> <b>of</b> <b>known</b> properties: ...|$|R
40|$|Figure 6 - Ratio {{between the}} <b>number</b> <b>of</b> <b>known</b> and {{expected}} species richness in French Guiana. Ratios {{are based on}} the <b>number</b> <b>of</b> <b>known</b> species in French Guiana and the number described at the world level (Zhang 2013). Other orders on the right of the histogram with no known species in French Guiana comprise: Mecoptera, Archaeognatha, Zygentoma, Embioptera, Grylloblattodea, Mantophasmatodea, Zoraptera, Phthiraptera, Rhaphidioptera...|$|R
3000|$|... c, d is zero. Throughput of flow Bb can be {{computed}} {{in terms}} <b>of</b> all <b>known</b> <b>parameters</b> using Eq. 10, provided {{the value of}} τ [...]...|$|R
2500|$|These are the <b>number</b> <b>of</b> <b>known</b> {{appearances}} by White Conduit Club players (M = matches played): ...|$|R
50|$|Nepenthes mirabilis has the {{greatest}} <b>number</b> <b>of</b> <b>known</b> natural hybrids <b>of</b> any species in the genus.|$|R
5000|$|Emergency {{software}} and hardware fixes, normally containing the corrections to a small <b>number</b> <b>of</b> <b>known</b> problems.|$|R
