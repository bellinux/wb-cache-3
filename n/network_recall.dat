9|79|Public
3000|$|... e, {{and builds}} a remote {{connection}} for the Hitch Hiker <b>network.</b> <b>Recall</b> that a remote connection is formally a tuple 〈N [...]...|$|E
30|$|A {{large number}} of {{research}} has been conducted on the analysis of social networks, based on CDRs. As it appears from the different publications on this topic, there exist some common features but also many differences {{in the structure of the}} constructed <b>network.</b> <b>Recall</b> as simplest example the degree distributions, which show different functional forms for most datasets.|$|E
30|$|In this section, {{we study}} the {{multiplicity}} of the lost packets in the network {{and its impact on}} the decoding process. By multiplicity, we mean the number of copies of the packet that exist in the <b>network.</b> <b>Recall</b> that, when a packet is coded with others, the coded message is broadcasted into the network creating duplicates of these coded packets. When a packet is lost, other copies of that packet still exist in the network creating undecoding opportunities at the receiver.|$|E
3000|$|Here we will {{specify the}} {{classification}} of protection scheme in multiplex <b>networks.</b> <b>Recall</b> that due to the interconnected properties of a multiplex network, the epidemic spreading can occur along the link of all corresponding layer of the network. Hence, the node protection scheme can be classified into two basic classes: [...]...|$|R
50|$|This {{demonstrates}} {{the capability of}} autoassociative <b>networks</b> to <b>recall</b> the whole by using some of its parts.|$|R
3000|$|... {{users in}} the <b>network</b> [32]. <b>Recall</b> {{that the use of}} several tens of {{thousands}} of sensors is required for the considered application case [1].|$|R
30|$|For each {{received}} bundle, several {{measures are}} performed. First, the delay, that is, the elapsed time {{from generation to}} delivery, is recorded. Delay, which is usually imposed by an application, is a meaningful parameter for discriminating forwarding schemes. Similarly, a cost parameter, intended as how much of network resources a routing scheme consumes, will also be considered. In our case, we define as cost of a routing scheme {{the average number of}} network nodes that receive the bundle, apart from the intended destination. Although simplistic, this serves as an indication of how much extra traffic is generated in the <b>network</b> (<b>recall</b> that we neglect signaling traffic by assuming that nodes have perfect knowledge of the logical connectivity), how intensively the buffers are employed, and it is also related to the amount of overhead introduced at lower layers.|$|E
40|$|AbstractHow does an individual's sex {{influence}} their recall of social relations? Extensive {{research has shown}} that social networks differ by sex and has attempted to explain these differences either through structural availability or individual preferences. Addressing the limitations of these explanations, we build on an increasing body of research emphasizing the role of cognition in the formation and maintenance of networks to argue that males and females may exhibit different strategies for encoding and recalling social information in memory. Further, because activating sex roles can alter cognitive performance, we propose that differences in recall may only or primarily appear when respondents are made aware of their sex. We explore differences in male and female network memory using a laboratory experiment asking respondents to memorize and recall a novel social network after receiving either a sex prime or a control prime. We find that sex significantly impacts social <b>network</b> <b>recall,</b> however being made aware of one's sex does not. Our results provide evidence that differences in male and female networks may be partly due to sex-based differences in network cognition...|$|E
40|$|Gyula Kovács and Mihály Racsmány contributed {{equally to}} this study. The testing effect {{refers to the}} {{phenomenon}} that repeated retrieval of memories promotes better long-term retention than repeated study. To investigate the neural correlates of the testing effect, we used event-related {{functional magnetic resonance imaging}} methods while participants performed a cued recall task. Prior to the neuroi-maging experiment, participants learned Swahili–German word pairs, then half of the word pairs were repeatedly studied, whereas the other half were repeatedly tested. For half of the participants, the neuroimaging experiment was performed immediately after the learning phase; a 1 -week retention interval was inserted for {{the other half of the}} participants. We found that a large network of areas identified in a separate 2 -back functional localizer scan were active during the final recall of the word pair associations. Importantly, the learning strategy (retest or restudy) of the word pairs determined the manner in which the retention interval affected the activations within this <b>network.</b> <b>Recall</b> of previously restudied memories was accompanied by reduced activation within this network at long retention intervals, but no reduction was observed for previously retested memories. We suggest that retrieval promotes learning via stabilizing cue-related activation patterns in a network of areas usually associated with cognitive and attentional control functions...|$|E
3000|$|The simple dynamics-based {{measures}} of early meme diffusion defined above, while potentially useful, do not characterize {{the manner in}} which a meme propagates over the underlying social or information <b>networks.</b> <b>Recall</b> that the predictability assessment summarized in Section 2.3 suggests that both early dispersion of diffusion activity across network communities and early diffusion activity within the network core ought to be predictive of meme success. The insights offered by this theoretical analysis motivate the definition of two network dynamics-based features for meme prediction: [...]...|$|R
30|$|In this section, {{we compare}} HBA against the tree-based {{algorithm}} [7] for always-on <b>networks.</b> <b>Recall</b> that the tree-based algorithm [7] {{is the first}} centralized method designed for always-on networks under SINR-based interference model. In this respect, we note that HBA is the first distributed algorithm designed for duty-cycle networks under the SINR-based interference model. In order to compare HBA faithfully against the tree-based algorithm [7], the scheduling period used by HBA is set to 1, i.e., T= 1, meaning HBA also works in the always-on mode.|$|R
30|$|Given the {{considerations}} above, {{we build}} on the stub-matching model (Bender and Canfield 1978), which generates random networks with a particular fixed in and outdegree sequence, by definition also preserving {{the exact number of}} nodes. Furthermore, to ensure that degree sequences are fixed for all edge types, each combination of edge types is modeled separately, fixing the node degrees for each (combination of) link type(s). Thus, we model in total 2 |J|− 1 different <b>networks</b> (<b>recall</b> that J is the set of link types). This is a mere three network models in our case, namely for the ownership links, the board interlocks and the combined “multiplex link”.|$|R
40|$|This paper {{establishes}} a new constrained combinatorial optimization {{approach to the}} design of cellular neural networks with sparse connectivity. This strategy is applicable to cases where maintaining links between neurons incurs a cost, which could possibly vary between these links. The cellular neural network’s interconnection topology is diluted without significantly degrading its performance, the network quantified by the average recall probability for the desired patterns engraved into its associative memory. The dilution process selectively removes the links that contribute the least to a metric related to the size of system’s desired memory pattern attraction regions. The metric used here is the magnitude of the network’s nodes’ stability parameters, which have been proposed as a measure for the quality of memorization. Further, the efficiency of the method is justified by comparing it with an alternative dilution approach based on probability theory and randomized algorithms. We demonstrate by means of an example that this method of network dilution based on combinatorial optimization produces cheaper associative memories that in general trade off performance for cost, and in many cases the performance of the diluted network is on par with the original system. Also the randomized algorithm based method results in same network performance in terms of <b>network</b> <b>recall</b> probability. This work is supported by NSF-IIS award # 0822845...|$|E
40|$|Disclaimer: These notes {{have not}} been {{subjected}} to the usual scrutiny reserved for formal publications. They may be distributed outside this class only {{with the permission of}} the Instructor. Last lecture saw the introduction of Groebner bases as a powerful tool – when used with the simple division algorithm – for membership testing in multivariate polynomial ideals. In particular we proved the Hilbert Basis Theorem for multivariate ideals, that every such ideal has a finite (Groebner) basis. We also explored construction of Groebner bases through Buchberger’s algorithm. In this lecture we complete the present discussion of systems of polynomials and polynomial equations with reduced Groebner bases and a convenient back-substitution method for solving systems of polynomial equations using Groebner bases. We then proceed to sorting networks, which consist of a sequence of layers of parallel pairwise comparisons {{that can be used for}} efficient sorting. After focusing on Batcher networks – sorting networks that are Θ(log 2 n) deep – we introduce the AKS sorting <b>network.</b> <b>Recall</b> that a Groebner basis of an ideal I ⊆ K[x 1, [...] ., xn] is a basis {g 1, [...] ., gt} of I such that LT (I) = 〈LT (g 1), [...] ., LT (gt) 〉 where for any f ∈ K[x 1, [...] ., xn] the leading term (taken with respect to the chosen lexicographic ordering on {x 1, [...] ., xn}) of f is denoted by LT (f), and LT (I) = {LT (f) | f ∈ I}. 14. 1 Reduced Groebner Base...|$|E
40|$|Tanner, Herbert G. A new network {{topology}} optimization approach to {{cellular neural network}} design, as a method for realizing associative memories using sparser networks is conceptualized. This type of optimization allows recurrent neural networks to be implemented in a spatially distributed fashion, that is, with components of the network residing in different physical locations. This could find application in addressing the problem of dynamic allocation {{of a team of}} robots to a collection of spatially distributed tasks which is relevant for large scale environmental monitoring and surveillance. Spatially distributed sensing allows for greater coverage of the environment than a single large vehicle with multiple sensors would permit in many cases. In this work, we try to answer the question of how could the design process be different if the {{network topology}} was also part of the design. A sparser cellular neural network topology can be achieved without significantly degrading the performance of the network, by selectively deleting those weights from the optimized network which contribute the least to ability of the network to recall the desired patterns. This approach is particularly useful where neural links incur varying costs, such as implementation of associative memories over wireless sensor networks. The cellular neural networks interconnection topology is diluted, without significantly degrading its performance, where performance is quantified by the average recall probability of the patterns engraved into the networks associative memory. The average recall probability is a measure of performance of the designed network in presence of noise and is defined as the ratio of number of recovered memory patterns (perturbed initial condition vectors which result in same output as the stored memory vector) to the total number of perturbed initial condition vectors. Since the average recall probability cannot be assessed prior to testing, the optimization algorithm uses the networks stability parameters as a measure of quality of memorization, and optimization proceeds by selectively removing costly links that contribute the least to the magnitude of these parameters. Two different approaches to implementing the optimization of the networks topology are implemented and compared. The first one is a sequential process in which a single link is removed each time, specifically the one the removal of which incurs the least performance cost compared to all other existing high-cost links. This method ignores the possibility that a non-obvious combination of links may produce better results through the links simultaneous removal. This phenomenon has been observed in simulation studies which validated the proposed method. To validate further the optimization, but more importantly, to ensure that the overall approach does not depend on the particular method used for the combinatorial optimization we also implemented an alternative approach which is based on the randomized optimization. In this approach a random sample of a sufficient number of i. i. d possible topology is generated. In other words, each random topology in the sample has the same probability distribution as the others and all are mutually independent. An example is used to demonstrate that irrespectively of the combinatorial algorithm used, the approach yields sparser associative memories that in general trade off performance for cost, and in many cases the performance of the diluted network is on par with the original system. In our numerical tests, the two methods yield comparable results, which do not differ significantly in terms of resulting network performance. Performance is quantified in terms of the <b>network</b> <b>recall</b> probability, and in the proposed optimization algorithm approach is captured by the neural networks stability parameters. Further, we apply the ideas developed so far to control network communication in actual robots to experimentally verify our simulation results. Experimental testing has shown that spatially distributed implementations of cnn on CoroBots are indeed feasible, and that for some cases, the communication delays related to the communication between the different components of the network are not significant enough to affect the performance and stability properties of the dynamical system. It is shown that the error between simulation of the discrete-time dynamics and experimental results practically coincide, with a maximum error difference of the order of 10 - 4. Thus the proposed combinatorial optimization methods performed almost equally well in practice as in simulations. University of Delaware, Department of Mechanical EngineeringM. S...|$|E
40|$|AbstractA {{growing number}} of studies {{indicate}} that aspects of psychology and cognition influence network structure, but {{much remains to be}} learned about how network information is stored and retrieved from memory. Are <b>networks</b> <b>recalled</b> as dyads, as triads, or more generally as sub-groups? We employ an experimental design coupled with exponential random graph models to address this issue. We find that respondents flexibly encode social information as triads or groups, depending on the network, but not as dyads. This supports prior research showing that networks are stored using “compression heuristics”, but also provides evidence of cognitive flexibility in the process of encoding relational information...|$|R
5000|$|Early in {{his career}} at CBS, Golden's work drew the {{attention}} of Frank Stanton, {{who was then the}} newly named head of the Research Department, and who eventually became the president of CBS. John Cowden, vice president of the CBS Television <b>Network,</b> <b>recalled</b> that Stanton and Golden [...] "shared a common philosophy about their work and in particular about advertising. They were both perfectionists... animated by the conviction that the only possible way for advertising to command attention and be remembered was to present each message so distinctively that it would stand out in relief from all others" [...] (Golden, Weihs, and Strunsky, 130). The two men built a friendship on their shared belief in the effectiveness of good visual form and their ambition for excellence.|$|R
30|$|In recent years, {{a lot of}} {{research}} has been done in order to use Big Data to help monitor and prevent epidemics of infectious diseases. If one can model information spreading in mobile phone <b>networks</b> (<b>recall</b> Section  6), then the same theory could also be used to model the spreading of real infectious diseases. As mobile phone data can help follow the movements of people (recall Section  5), these movements can also provide information about how a disease could travel and spread across a country. The dynamics at hand usually depend on the type of disease and how it can be transmitted, hence many articles, of which we will review a few here, propose different models based on the mobility of people to predict the spread of an epidemic.|$|R
40|$|In this paper, {{implementation}} of a genetic algorithm has been described to store and later, recall of some prototype patterns in Hopfield neural network associative memory. Various operators of genetic algorithm (mutation, cross-over, elitism etc) are used to evolve the population of optimal weight matrices {{for the purpose of}} storing the patterns and then recalling of the patterns with induced noise was made, again using a genetic algorithm. The optimal weight matrices obtained during the training are used as seed for starting the GA in recalling, instead starting with random weight matrix. A detailed study of the comparison ofresults thus obtained with the earlier results has been done. It has been observed that for Hopfield neural <b>networks,</b> <b>recall</b> of patterns is more successful if evolution of weight matrices is applied for training purpose also...|$|R
40|$|We {{examine the}} {{performance}} of Hebbian-like attractor neural <b>networks,</b> <b>recalling</b> stored memory patterns from their distorted versions. Searching for an activation (firing-rate) function that maximizes the performance in sparsely-connected low-activity networks, we show that the optimal activation function is a Threshold-Sigmoid of the neuron's input field. This function is shown to be in close correspondence with the dependence of the firing rate of cortical neurons on their integrated input current, as described by neurophysiological recordings and conduction-based models. It also accounts for the decreasing-density shape of firing rates that {{has been reported in}} the literature. 1 Introduction The reduction of detailed conduction-based models of neuronal firing into simpler, lowerdimensional descriptions has recently received considerable attention (e. g., [Doya and Selverston, 1994, Ermentrout, 1994]). Such reductions lead to frequency-current curves similar to the curve obtained [...] ...|$|R
40|$|Photograph {{used for}} {{a story in the}} Daily Oklahoman newspaper. Caption: "Good old days when {{telegraph}} operators were the key figures in the nation's communications <b>network</b> are <b>recalled</b> Saturday for Jerry Thomas as he particpates in an observance of the anniversary of Samuel F. B Morse's birthday at the Western Union office. ...|$|R
40|$|An {{analysis}} of the storage capacity of a sparsely connected associative memory is presented. In these <b>networks,</b> <b>recall</b> performance is constrained by the mechanism for setting global threshold values. Recall performance improves if stored patterns are gradually reconstructed using a sequence of thresholds (progressive recall). We present a method for predicting the performance of progressive recall, {{which is used to}} test all possible threshold sequences, and to find the optimal sequence. We show that previous progressive recall strategies overestimated expected performance, and do not result in optimal storage capacity. Computer simulations studies are presented to support the theory. keywords: associative memory, progressive recall, hippocampus scientific conference Proceedings of International Conference on Artificial Neural Networks (ICANN' 95), Vol. 2, pp 509 - 514 Performance {{analysis of}} progressive recall in partially connected recurrent networks Hajime Hirase and Michael Recce [...] ...|$|R
5000|$|In March 1998, Vučić was {{appointed}} Minister of Information {{in the government}} of Mirko Marjanović. Following rising resentment against Milošević, Vučić introduced fines for journalists who criticized the government and banned foreign TV <b>networks.</b> He <b>recalled</b> in 2014 that he was wrong and had changed, stating [...] "I was not ashamed to confess all my political mistakes".|$|R
40|$|Abstract: In neural network, the {{associative}} memory {{is one in}} which applying some input pattern leads to the response of a corresponding stored pattern. During the learning phase the memory is fed with a number of input vectors and in the recall phase when some known input is presented to it, the <b>network</b> <b>recalls</b> and reproduces the output vector. Here, we improve and increase the storing ability of the memory model proposed in [1]. We show that there are certain instances where their algorithm can not produce the desired performance by retrieving exactly the correct vector. That is, in their algorithm, a number of output vectors can become activated from the stimulus of an input vector while the desired output is just a single vector. Our proposed solution overcomes this and uniquely determines the output vector as some input vector is applied. Thus we provide a more general scenario of this neural network memory model consisting of Competitive Cooperative Neurons (CCNs) ...|$|R
30|$|According to {{relation}} (3), a gene {{connected to}} many genes who themselves {{have a low}} degree gets a high Shapley value (in other words, the relevance of a gene increases {{with the number of}} its neighbours having a low degree). Relation (3) also suggests that genes with a high Shapley value would be able to interact directly with the maximum number of other nodes in the network and its removal would split the network in a maximum number of connected components with few genes, or eventually constituted by isolated genes. As shown in the paper (Aadithya 2010), relation (3) can be calculated via an O(|N| + |E|) procedure, which makes possible its computation on very large <b>networks</b> (<b>recall</b> that on a network 〈N,E〉, calculating betweenness centrality takes O(|N| |E|) time using Brandes’ algorithm (Brandes 2001). We conclude this section showing the results of relation (3) applied to the motivating example in “A motivating example” section.|$|R
40|$|Brain {{networks}} memorize previous {{performance to}} adjust their output in light of past experience. These activity-dependent modifications generally result from changes in synaptic strengths or ionic conductances, and ion pumps have only rarely been demonstrated to play a dynamic role [1, 2, 3, 4]. Locomotor behavior is produced by central pattern generator (CPG) networks and modified by sensory and descending signals to allow for changes in movement frequency, intensity, and duration [5, 6, 7], but whether or how the CPG <b>networks</b> <b>recall</b> recent activity is largely unknown. In Xenopus frog tadpoles, swim bout duration correlates linearly with interswim interval, suggesting that the locomotor network retains a short-term memory of previous output. We discovered an ultraslow, minute-long afterhyperpolarization (usAHP) in network neurons following locomotor episodes. The usAHP is mediated by an activity- and sodium spike-dependent enhancement of electrogenic Na+/K+ pump function. By integrating spike frequency over time and linking the membrane potential of spinal neurons to network performance, the usAHP plays a dynamic role in short-term motor memory. Because Na+/K+ pumps are ubiquitously expressed in neurons of all animals and because sodium spikes inevitably accompany network activity, the usAHP may represent a phylogenetically conserved but largely overlooked mechanism for short-term memory of neural network function. Publisher PDFPeer reviewe...|$|R
40|$|Optoelectronic {{apparatus}} acts as artificial neural <b>network</b> performing associative <b>recall</b> of binary images. Recall {{process is}} iterative one involving optical computation of inner products between binary input vector {{and one or}} more reference binary vectors in memory. Inner-product method requires far less memory space than matrix-vector method...|$|R
40|$|The {{purpose of}} this {{assignment}} is to test and possibly expand your knowledge about learning Bayesian <b>networks</b> from data. <b>Recall</b> that learning Bayesian networks involves both structure learning, i. e., learning the graph topology from data, and parameter learning, i. e., learning the actual, local probability distributions from data...|$|R
3000|$|In this section, we compute {{resistance}} {{distances between}} any pair of vertices in the complete multipartite graph K_n_ 1, n_ 2,..., n_k via electrical <b>network</b> approach. <b>Recall</b> that G {{is a complete}} k-partite graph if the vertex set V can be partitioned into k parts V_ 1, V_ 2, [...]...,V_k such that uv∈ E(G) {{if and only if}} u and v are in different parts. If [...] V_i =n_i (i= 1, 2,...,k), then G is denoted by K_n_ 1, n_ 2,..., n_k. The following two lemmas play essential roles further.|$|R
40|$|This paper {{describes}} {{experiments on}} on the robustness of tensor product networks using distributed representations, for recall tasks. The results of the experiments indicate, among other things, {{that the degree of}} robustness increases with the number of binding units and decreases with the fraction of the space of possible facts that have been taught to the <b>network.</b> Mean <b>recall</b> scores decrease linearly with the proportion of binding units inactivated, and recall score variance depends linearly on number of binding units and on number of facts taught to the network...|$|R
50|$|In addition, {{cashback}} offers {{may help}} to combat fraud on high value contracts. Where up-front inducements are used to incentivise sales, such as instant cashback or free merchandise, criminals may use fraud {{to gain access to}} the incentives. When the customer defaults on the contract, the <b>network</b> will <b>recall</b> the commission paid to the retailer and the retailer is out-of-pocket. Cashback deals paid over time allow the retailer to provide an incentive on condition that the consumer sticks to the terms of the contract, thereby securing the retailers' commission.|$|R
50|$|This input then {{reaches the}} {{olfactory}} cortex. Here, Hebbian learning <b>networks</b> allow for <b>recall</b> with partial or weak stimuli, indicating {{the first stage}} of conscious perception. Here, connections with the hypothalamus and hippocampus indicate that olfaction stimuli affect emotion, decision making, and learning only after significant processing and rudimentary identification.|$|R
30|$|In this paper, we {{will give}} the maximum flow of chance {{distribution}} of an uncertain random network. The remainder {{of this paper is}} organized as follows. In the section ‘Preliminaries’, some basic concepts and properties of uncertainty theory and chance theory used throughout this paper are introduced. In the section ‘Uncertain random network’, uncertain random <b>network</b> is <b>recalled.</b> In the section ‘Maximum flow of uncertain random network’, chance distribution of maximum flow is proved. The section ‘Expected value of maximum flow’ proposes the expected value the maximum flow in an uncertain random network. The section ‘Conclusions’ gives a brief summary to this paper.|$|R
40|$|International audienceIt {{has been}} proved, for several classes of {{continuous}} and discrete dynamical systems, {{that the presence}} of a positive (resp. negative) circuit in the interaction graph of a system is a necessary condition for the presence of multiple stable states (resp. a cyclic attractor). A positive (resp. negative) circuit is said to be functional when it “generates” several stable states (resp. a cyclic attractor). However, there are no definite mathematical frameworks translating the underlying meaning of “generates. ” Focusing on Boolean <b>networks,</b> we <b>recall</b> and propose some definitions concerning the notion of functionality along with associated mathematical results...|$|R
3000|$|..., both schemes (the {{proposed}} scheme {{and the one}} in[10]) do not perform well once the secondary achieves very low throughput and degrades the primary <b>network</b> performance. We <b>recall</b> that as discussed above, the {{proposed scheme}} considerably outperforms the method introduced in[10], and even larger gains can be obtained if {{a larger number of}} relays is available.|$|R
50|$|Allen {{was drafted}} with the 10th overall {{selection}} of the 1982 NFL Draft by the Los Angeles Raiders. In a recent ad on NFL <b>Network,</b> Allen <b>recalls</b> shortly before being drafted that the Raiders asked him his weight at the time (he answered 200 or 212) and then drafted him soon after. Though his rookie season was shortened by a league strike, Allen rushed for 697 yards and led the Raiders to {{the best record in}} the AFC at 8-1. He was voted the NFL Offensive Rookie of the Year. The Raiders would lose to the New York Jets in the AFC Divisional Playoffs.|$|R
