10000|0|Public
25|$|Estimation of such {{models is}} usually done via parametric, semi-parametric and <b>non-parametric</b> maximum {{likelihood}} methods.|$|E
25|$|One can compute {{prediction}} intervals {{without any}} assumptions on the population; formally, {{this is a}} <b>non-parametric</b> method.|$|E
25|$|Evolutionary methods, gene {{expression}} programming, simulated annealing, expectation-maximization, <b>non-parametric</b> methods and {{particle swarm optimization}} are other methods for training neural networks.|$|E
25|$|Linkage {{analysis}} may {{be either}} parametric (if {{we know the}} relationship between phenotypic and genetic similarity) or <b>non-parametric.</b> Parametric linkage analysis is the traditional approach, whereby {{the probability that a}} gene important for a disease is linked to a genetic marker is studied through the LOD score, which assesses the probability that a given pedigree, where the disease and the marker are cosegregating, is due to the existence of linkage (with a given linkage value) or to chance. <b>Non-parametric</b> linkage analysis, in turn, studies the probability of an allele being identical by descent with itself.|$|E
25|$|It is {{possible}} to make statistical inferences without assuming a particular parametric family of probability distributions. In that case, one speaks of <b>non-parametric</b> statistics {{as opposed to the}} parametric statistics just described. For example, a test based on Spearman's rank correlation coefficient would be called <b>non-parametric</b> since the statistic is computed from the rank-order of the data disregarding their actual values (and thus regardless of the distribution they were sampled from), whereas those based on the Pearson product-moment correlation coefficient are parametric tests since it is computed directly from the data values and thus estimates the parameter known as the population correlation.|$|E
25|$|Though <b>non-parametric,</b> L-estimators are {{frequently}} used for parameter estimation, {{as indicated by}} the name, though they must often be adjusted to yield an unbiased consistent estimator. The choice of L-estimator and adjustment depend on the distribution whose parameter is being estimated.|$|E
25|$|However, the {{simplicity}} of L-estimators means that they are easily interpreted and visualized, and makes them suited for descriptive statistics and statistics education; many can even be computed mentally from a five-number summary or seven-number summary, or visualized from a box plot. L-estimators play {{a fundamental role in}} many approaches to <b>non-parametric</b> statistics.|$|E
25|$|A single {{parasite}} species {{usually has}} an aggregated distribution across host individuals, {{which means that}} most hosts harbor few parasites, while a few hosts carry {{the vast majority of}} parasite individuals. This poses considerable problems for students of parasite ecology, as it renders parametric statistics invalid. Log-transformation of data before the application of parametric test, or the use of <b>non-parametric</b> statistics is recommended by several authors, but this can give rise to further problems, so quantitative parasitology is based on more advanced biostatistical methods.|$|E
2500|$|The sample maximum {{and minimum}} provide a <b>non-parametric</b> {{prediction}} interval: ...|$|E
2500|$|Wolfowitz's main {{contributions}} were in {{the fields}} of statistical decision theory, [...] <b>non-parametric</b> statistics, sequential analysis, and information theory.|$|E
2500|$|Eliminating the {{parameter}} [...] {{from these}} parametric equations will yield the <b>non-parametric</b> equation of the Mohr circle. This {{can be achieved}} by rearranging the equations for [...] and , first transposing the first term in the first equation and squaring both sides of each of the equations then adding them. Thus we have ...|$|E
2500|$|... {{were not}} present in the {{original}} paper on HM and are not needed to present the method. None of the inference rules below will take care or even note them. The same holds for the <b>non-parametric</b> [...] "primitive types" [...] in said paper. All the machinery for polymorphic type inference can be defined without them. They have been included here for sake of examples {{but also because the}} nature of HM is all about parametric types. This comes from the function type , hard-wired in the inference rules, below, which already has two parameters and has been presented here as only a special case.|$|E
2500|$|If the {{parameters}} vary at roughly twice the natural {{frequency of the}} oscillator (defined below), the oscillator phase-locks to the parametric variation and absorbs energy at a rate proportional to the energy it already has. [...] Without a compensating energy-loss mechanism provided by , the oscillation amplitude grows exponentially. (This phenomenon is called parametric excitation, parametric resonance or parametric pumping.) [...] However, if the initial amplitude is zero, it will remain so; this distinguishes it from the <b>non-parametric</b> resonance of driven simple harmonic oscillators, in which the amplitude grows linearly in time regardless of the initial state.|$|E
2500|$|When the {{normality}} assumption {{does not}} hold, a <b>non-parametric</b> {{alternative to the}} t-test can often have better statistical power. Similarly, {{in the presence of}} an outlier, the t-test is not robust. For example, for two independent samples when the data distributions are asymmetric [...] (that is, the distributions are skewed) or the distributions have large tails, then the Wilcoxon rank-sum test (also known as the Mann–Whitney U test) can have three to four times higher power than the t-test. The nonparametric counterpart to the paired samples t-test is the Wilcoxon signed-rank test for paired samples. For a discussion on choosing between the t-test and nonparametric alternatives, see Sawilowsky (2005).|$|E
2500|$|Compound hierarchical-deep models compose deep {{networks}} with <b>non-parametric</b> Bayesian models. Features can {{be learned}} using deep architectures such as DBNs, DBMs, deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse feature learning, RNNs, conditional DBNs, de-noising auto encoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a [...] ) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples, for example for computer vision, statistics and cognitive science.|$|E
2500|$|A Q–Q plot is used {{to compare}} the shapes of distributions, {{providing}} a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions. [...] Q–Q plots can be used to compare collections of data, or theoretical distributions. [...] The use of Q–Q plots to compare two samples of data {{can be viewed as a}} <b>non-parametric</b> approach to comparing their underlying distributions. [...] A Q–Q plot is generally a more powerful approach to do this than the common technique of comparing histograms of the two samples, but requires more skill to interpret. [...] Q–Q plots are commonly used to compare a data set to a theoretical model. [...] This can provide an assessment of [...] "goodness of fit" [...] that is graphical, rather than reducing to a numerical summary. [...] Q–Q plots are also used to compare two theoretical distributions to each other. [...] Since Q–Q plots compare distributions, there is no need for the values to be observed as pairs, as in a scatter plot, or even for the numbers of values in the two groups being compared to be equal.|$|E
50|$|The {{first two}} types {{are known as}} analytic, or <b>non-parametric,</b> {{representations}} of curves; when compared to parametric representations for use in CAD applications, <b>non-parametric</b> representations have shortcomings. In particular, the <b>non-parametric</b> representation depends on {{the choice of the}} coordinate system and does not lend itself well to geometric transformations, such as rotations, translations, and scaling; <b>non-parametric</b> representations therefore {{make it more difficult to}} generate points on a curve. These problems can be addressed by rewriting the <b>non-parametric</b> equations in parametric form.|$|E
5000|$|<b>Non-parametric</b> {{estimation}} of the variance function and its importance, has been discussed widely in the literatureIn <b>non-parametric</b> regression analysis, {{the goal is to}} express the expected value of your response variable(y) as a function of your predictors (X). That is we are looking to estimate a mean function, [...] without assuming a parametric form. There are many forms of <b>non-parametric</b> smoothing methods to help estimate the function [...] An interesting approach is to also look at a <b>non-parametric</b> variance function, [...] A <b>non-parametric</b> variance function allows one to look at the mean function {{as it relates to the}} variance function and notice patterns in the data.|$|E
50|$|Another {{justification}} {{for the use of}} <b>non-parametric</b> methods is simplicity. In certain cases, even when the use of parametric methods is justified, <b>non-parametric</b> methods may be easier to use. Due both to this simplicity and to their greater robustness, <b>non-parametric</b> methods are seen by some statisticians as leaving less room for improper use and misunderstanding.|$|E
5000|$|<b>Non-parametric</b> {{methods are}} widely used for {{studying}} populations that take on a ranked order (such as movie reviews receiving one to four stars). The use of <b>non-parametric</b> methods may be necessary when data have a ranking but no clear numerical interpretation, such as when assessing preferences. In terms of levels of measurement, <b>non-parametric</b> methods result in [...] "ordinal" [...] data.|$|E
5000|$|... {{confidence}} intervals are more flexible {{and can be}} used practically in more situations than credible intervals: one area where credible intervals suffer in comparison is in dealing with <b>non-parametric</b> models (see <b>non-parametric</b> statistics).|$|E
50|$|<b>Non-parametric</b> robot {{calibration}} circumvents the parameter identification. Used with serial robots, it {{is based}} on the direct compensation of mapped errors in the work space. Used with parallel robots, <b>non-parametric</b> calibration can be performed by the transformation of the configuration space.|$|E
5000|$|... #Subtitle level 2: Parametric versus <b>non-parametric</b> {{processes}} ...|$|E
50|$|As <b>non-parametric</b> methods make fewer assumptions, their {{applicability}} is much {{wider than the}} corresponding parametric methods. In particular, they may be applied in situations where less {{is known about the}} application in question. Also, due to the reliance on fewer assumptions, <b>non-parametric</b> methods are more robust.|$|E
50|$|The wider {{applicability}} {{and increased}} robustness of <b>non-parametric</b> tests {{comes at a}} cost: in cases where a parametric test would be appropriate, <b>non-parametric</b> tests have less power. In other words, a larger sample size can be required to draw conclusions with {{the same degree of}} confidence.|$|E
50|$|The {{choice of}} &epsilon; is usually user-defined. Like most line fitting / {{polygonal}} approximation / dominant point detection methods, {{it can be}} made <b>non-parametric</b> by using the error bound due to digitization / quantization as a termination condition. MATLAB code for such a <b>non-parametric</b> RDP algorithm is available here.|$|E
50|$|The {{statistical}} {{approaches to}} novelty detection may be classified into parametric and <b>non-parametric</b> approaches. Parametric approaches assume a specific statistical distribution (such as a Gaussian distribution) {{of data and}} statistical modeling based on data mean and covariance, whereas <b>non-parametric</b> approaches do not make any assumption on the statistical properties of data.|$|E
50|$|Analysis of data {{obtained}} by ranking commonly requires <b>non-parametric</b> statistics.|$|E
5000|$|Efros-Leung in 1999 - [...] "Texture Synthesis by <b>Non-parametric</b> Sampling".|$|E
5000|$|Hollander and Wolfe, <b>Non-parametric</b> {{statistical}} methods (Section 8.7), 1999. Wiley.|$|E
5000|$|Probabilistic model (<b>non-parametric,</b> e.g. Kernel Smoothing; parametric, e.g. Normal, Beta) ...|$|E
5000|$|... #Subtitle level 3: Robbins method : <b>non-parametric</b> {{empirical}} Bayes (NPEB) ...|$|E
50|$|<b>Non-{{parametric}}</b> models {{differ from}} parametric models {{in that the}} model structure is not specified a priori but is instead determined from data. The term <b>non-parametric</b> {{is not meant to}} imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.|$|E
5000|$|Following is {{a partial}} list of <b>non-parametric</b> {{spectral}} density estimation techniques: ...|$|E
50|$|Commonly, kernel widths {{must also}} be {{specified}} when running a <b>non-parametric</b> estimation.|$|E
5000|$|Alternatively, <b>non-parametric</b> {{processes}} often involve loss (or gain) {{and give}} rise to: ...|$|E
