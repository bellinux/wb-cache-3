126|199|Public
50|$|Oyster Harbors {{was first}} {{inhabited}} by the Wampanoag tribe {{when they first}} settled on Cape Cod. A legend states Captain Kidd presumably had buried treasure at <b>Noisy</b> <b>Point</b> during this time, guarded by the witch Hannah Screecham. Around 1658, the island was reserved for Native Americans. However, after an expensive lawsuit in 1737, they sold the island to the Lovell family. For the next two centuries, the island would remain uninhabited, and was used only for salt works and pastureland. Because of this, the island's oldest house, the Nymphas Marston House which was built in 1680, was actually moved there from {{the main part of}} town.|$|E
30|$|Our {{algorithm}} is robust {{with respect to}} different sampling densities and the typical noise introduced by laser scanner acquisition. This is achieved by employing suitable point sampling approaches described in Section 5, and by using feature points and their descriptors to effectively constrain rigid transformations on <b>noisy</b> <b>point</b> sets.|$|E
40|$|The Chow {{form of the}} {{essential}} variety in computer vision is calculated. Our derivation uses secant varieties, Ulrich sheaves and representation theory. Numerical experiments show that our formula can detect <b>noisy</b> <b>point</b> correspondences between two images. Comment: 27 pages, 1 figure, 6 Macaulay 2 ancillary files. v 2 : edits to Theorem 1. 1, references, acknowledgement...|$|E
40|$|<b>Noisy</b> <b>points</b> in {{training}} data maybe due to incorrect class labels or erroneous recording of attribute values. These points greatly influence {{the orientation of}} the classification boundary. In this paper, we formalize two notions of noisy points: intrusive outliers and hard-to-classify points. We adapt two well-known distance-based notions of outliers in unlabeled data to formalize intrusive outliers and adapt the corresponding algorithms to detect them in labeled data. We propose a boosting-based algorithm to identify hard-toclassify points in labeled data. We empirically compare these two notions of <b>noisy</b> <b>points</b> and their influence on classification accuracy. Finally we experimentally prove that removal of <b>noisy</b> <b>points</b> improves the robustness of the classification boundary against future noisy data. 1...|$|R
30|$|Noisy-points elimination. <b>Noisy</b> <b>points</b> {{still existed}} in a disorderly fashion {{in the image}} after applied multi-threshold segmentation. To remove the <b>noisy</b> <b>points,</b> we applied the {{following}} measures to the preprocessed images. Define L as a length threshold. By detecting each portion with the same gray value in the image, the projected length (L 1 and L 2) in the X and Y directions of the portion were obtained. If both L 1 and L 2 were less than L, the color of that portion was replaced by its surrounding color.|$|R
40|$|Boosting is {{a machine}} {{learning}} technique widely used across many disciplines. Boosting enables one {{to learn from}} labeled data in order to predict the labels of unlabeled data. A central property of boosting instrumental to its popularity is its resistance to overfitting. Previous experiments provide a margin-based explanation for this resistance to overfitting. In this thesis, the main finding is that boosting's resistance to overfitting can {{be understood in terms}} of how it handles <b>noisy</b> (mislabeled) <b>points.</b> Confirming experimental evidence emerged from experiments using the Wisconsin Diagnostic Breast Cancer(WDBC) dataset commonly used in machine learning experiments. A majority vote ensemble filter identified on average that 2. 5 % of the points in the dataset as noisy. The experiments chiefly investigated boosting's treatment of <b>noisy</b> <b>points</b> from a volume-based perspective. While the cell volume surrounding <b>noisy</b> <b>points</b> did not show a significant difference from other points, the decision volume surrounding <b>noisy</b> <b>points</b> was two to three times less than that of non-noisy points. Additional findings showed that decision volume not only provides insight into boosting's resistance to overfitting in the context of <b>noisy</b> <b>points,</b> but also serves as a suitable metric for identifying which points in a dataset are likely to be mislabeled. by Jeffrey Chan. Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student-submitted PDF version of thesis. Includes bibliographical references (pages 53 - 56) ...|$|R
40|$|We {{present a}} novel, efficient, {{initialization}} free {{approach to the}} problem of epipolar geometry estimation, by formulating it as one of hyperplane inference from a sparse and <b>noisy</b> <b>point</b> set in an 8 D space. Given a set of <b>noisy</b> <b>point</b> correspondences in two imagesas obtained from two views of a static scene without correspondences, even in the presence of moving objects, our method pulls out inlier matches while rejecting outliers. Unlike most methods which optimize certain objective function, our approach does not involve initialization or any search in the parameter space, and therefore is free of the problem of local optima or poor convergence. Since no search is involved, it is unnecessary to impose simplifying assumption (such as affine camera or local planar homography) to the scene being analyzed for reducing the search complexity. Subject to the general epipolar constraint only, we detect wrong matches by establishing salient "extremalities" via a novel approach, 8 D Tensor Votin [...] ...|$|E
40|$|We {{propose a}} new {{definition}} of the implicit surface for a <b>noisy</b> <b>point</b> cloud that allows for high-quality reconstruction of the surface in all cases. It is based on proximity graphs that provide a more topology-based measure for proximity of points. The new definition can be evaluated very fast, but, unlike other definitions based on the moving least squares (MLS) approach, it does not suffer from artifacts. 1 Moving Least Squares An appealing {{definition of the}} surface over a <b>noisy</b> <b>point</b> cloud P ∈ R 3 is the zero set S = {x | f (x) = 0 } of the implicit function f (x) = n(x) · (x − a(x)) [Alexa et al. 2003], where a(x) is the weighted average of all points P, and n(x) is determined by moving least squares. Usually, a Gaussian kernel (weighting function) θ(d) = e−d 2 /h 2,d = �x − p�, is used, but other kernels work, too. There are several variations of this simple definition, but for sak...|$|E
40|$|Surface {{reconstruction}} from cross cuts {{usually requires}} curve reconstruction from planar <b>noisy</b> <b>point</b> samples [...] The output curves must form a possibly disconnected 1 manifold for the surface reconstruction to proceed [...] This article describes an implemented algorithm for {{the reconstruction of}} planar curves (1 manifolds) out of <b>noisy</b> <b>point</b> samples of a sel-fintersecting or nearly sel-fintersecting planar curve C [...] C:[a,b]⊂R→R is self-intersecting if C(u) =C(v), u≠v, u,v∈(a,b) (C(u) is the self-intersection point) [...] We consider only transversal self-intersections, i. e. those for which the tangents of the intersecting branches at the intersection point do not coincide (C′(u) ≠C′(v)) [...] In the presence of noise, curves which self-intersect cannot be distinguished from curves which nearly sel fintersect [...] Existing algorithms for curve reconstruction out of either <b>noisy</b> <b>point</b> samples or pixel data, do not produce a (possibly disconnected) Piecewise Linear 1 manifold approaching the whole point sample [...] The algorithm implemented in this work uses Principal Component Analysis (PCA) with elliptic support regions near the selfintersections [...] The algorithm was successful in recovering contours out of noisy slice samples of a surface, for the Hand, Pelvis and Skull data sets [...] As a test for the correctness of the obtained curves in the slice levels, they were input into an algorithm of surface reconstruction, leading to a reconstructed surface which reproduces the topological and geometrical properties of the original object [...] The algorithm robustly reacts not only to statistical noncorrelation at the self-intersections(nonmanifold neighborhoods) but also to occasional high noise at the nonselfintersecting (1 manifold) neighborhood...|$|E
40|$|The {{tracking}} {{abilities of}} 1 st generation Kinect sensors {{have been tested}} over common trajectories of folk dances. Trajectories related errors, including offset, curve shape, <b>noisy</b> <b>points</b> are investigated and mitigated using well-known signal processing filters. Low cost depth trackers can contribute towards the remote tutoring of folk dances, by providing adequate data to instructors and explicit details to the trainees which segments of their dance trajectories need more work...|$|R
40|$|This paper {{addresses}} {{the problem of}} removing the noise from <b>noisy</b> <b>points</b> clouds {{in the context of}} surface reconstruction. We introduce a new smoothing operator Q inspired by the moving least-squares method and robust statistics theory. Our method can be seen as an improvement of the moving least-squares method to preserve sharp features. We also present effective numerical optimization algorithms to compute Q and some theoretical results on their convergence...|$|R
40|$|Scientific data {{is often}} {{in the form of}} a finite set of <b>noisy</b> <b>points,</b> sampled from an unknown space, and {{embedded}} in a high-dimensional space. Topological data analysis focuses on recovering the topology of the sampled space. In this chapter, we look at methods for constructing combinatorial representations of point sets, as well as theories and algorithms for effective computation of robust topological invariants. Throughout, we maintain a computational view by applying our techniques to a dataset representing the conformation space of a small molecule...|$|R
40|$|We {{introduce}} {{and study}} {{a type of}} (one-dimensional) wave equations with <b>noisy</b> <b>point</b> sources. We first study the existence and uniqueness problem of the equations. Then, {{we assume that the}} locations of point sources are unknown but we can observe the solution at some other location continuously in time. We propose an estimator to identify the point source locations and prove the convergence of our estimator...|$|E
30|$|In this case, {{the test}} field is quite flat. Therefore, {{the model of}} a planar surface was fitted into the pointset. The result showed that the average {{distance}} was about 5 times bigger than NURBS models. It might be caused by the noise {{as well as the}} shallow pockets on the wall. It proves that simple geometric objects cannot be used to model a <b>noisy</b> <b>point</b> cloud with high accuracy, even in simple cases.|$|E
40|$|We {{describe}} a new method for surface reconstruction and smoothing based on unorganized <b>noisy</b> <b>point</b> clouds without normals. The {{output of the}} method is a refined triangular mesh that approximates the original point cloud while preserving the fine details present in the underlying surface. The method has five steps: noise removal, clustering, data reduction, initial reconstruction, and mesh refinement. All these steps contain novel features. We also present theoretical justifications for the heuristics used in the reconstruction step...|$|E
40|$|Various glaciological topics require {{observations}} of horizontal velocities over vast areas, e. g., detecting acceleration of glaciers, {{as well as}} for estimating basal parameters of ice sheets using inverse modelling approaches. The quality of the velocity is of high importance; hence, methods to remove <b>noisy</b> <b>points</b> in remote sensing derived data are required. We present a three-step filtering process and assess its performance for velocity fields in Greenland and Antarctica. The filtering uses the detection of smooth segments, removal of outliers using the median and constraints on the variability of the flow direction over short distances. The applied filter preserves the structures in the velocity fields well (e. g., shear margins) and removes <b>noisy</b> data <b>points</b> successfully, while keeping 72 – 96 % of the data. In slow flowing regions, which are particularly challenging, the standard deviation is reduced by up to 96 %, an improvement that affects vast areas of the ice sheets...|$|R
40|$|A {{method to}} extract a visual {{continuous}} curve using a genetic {{algorithm is proposed}} in an image including a discontinuous curve and <b>noisy</b> <b>points.</b> The fitness of an individual in the population {{is defined by the}} global shape of a curve that is formed by connecting all points extracted by the individual based on proximity and continuity of points in perceptive grouping factors. A final shape of a curve that is formed by connecting points included in the individual with the maximum fitness is extracted after evolution of a genetic algorithm. Experimental results show that the proposed method extracted visual perceptive curves in noisy images. 1...|$|R
40|$|When seeking {{for small}} local {{patterns}} {{it is very}} intricate to distinguish between incidental agglomeration of <b>noisy</b> <b>points</b> and true local patterns. We propose a new approach that addresses this problem by exploiting temporal information which is contained in most business data sets. The algorithm enables the detection of local patterns in noisy data sets more reliable compared to the case when the temporal information is ignored. This is achieved by making use {{of the fact that}} noise does not reproduce its incidental structure but even small patterns do. In particular, we developed a method to track clusters over time based on an optimal match of data partitions between time periods...|$|R
40|$|Many {{real world}} data sets {{can be viewed}} as points in a higherdimensional space that lie {{concentrated}} around a lower-dimensional manifold structure. We propose a new multiscale representation for such point clouds based on lifting and perfect matching. The result is an adaptive wavelet transform that decomposes a point cloud into manifold approximations and details at multiple scales. We illustrate with several examples that the transform can extract an unknown smooth manifold from <b>noisy</b> <b>point</b> cloud samples using simple wavelet thresholding ideas. 1...|$|E
30|$|It was {{proposed}} {{in this work}} a method that incrementally estimated geometric primitives from the sparse maps generated by visual SLAM. This approach used the history information from every reconstruction to select the correct type of shape and their parameters so that the result was stable even when dealing with <b>noisy</b> <b>point</b> clouds. The evaluation showed {{that the use of}} temporal information contributed to more precise and more stable modeling and presents better results regarding precision F 1 -score and, in most cases, recall.|$|E
40|$|Abstract—We {{present a}} {{cooperative}} grasping approach {{based on a}} topological representation of objects. Using point cloud data we extract loops on objects suitable for generating entanglement. We use the Gauss Linking Integral to derive controllers for multi-agent systems that generate hooking grasps on such loops while minimizing the entanglement between robots. The approach copes well with <b>noisy</b> <b>point</b> cloud data, it is computationally simple and robust. We demonstrate the method for performing object grasping and transportation, through a hooking maneuver, with two coordinated NAO robots. I...|$|E
40|$|Ensemble {{methods have}} proved to be highly {{effective}} in improving the performance of base learners under most circumstances. In this paper, we propose a new algorithm that combines the merits of some existing techniques, namely bagging, arcing and stacking. The basic structure of the algorithm resembles bagging, using a linear support vector machine (SVM). However, the misclassification cost of each training point is repeatedly adjusted according to its observed out-of-bag vote margin. In this way, the method gains the advantage of arcing – building the classifier the ensemble needs – without fixating on potentially <b>noisy</b> <b>points.</b> Computational experiments show that this algorithm performs consistently better than bagging and arcing. ...|$|R
40|$|We {{present a}} method for {{constructing}} a surface approximation scheme whose input {{is a set of}} unorganized <b>noisy</b> <b>points</b> in space and whose output is a set of quadric patches. The local surface properties, necessary for the subsequent segmentation, are estimated directly from the data using a simple and efficient data structure- the neighborhood graph. Our segmentation scheme, based on principal curvatures, constructs initial point sub-sets, which may be enlarged or further subdivided based on associated approximation error estimates obtained through approximation of the initial segments by quadric surfaces. Our method is highly efficient and produces a high-quality piecewise quadric surface approximation, which we demonstrate for several simple and complex example data sets. 1...|$|R
40|$|We {{present a}} reverse {{engineering}} method for constructing a surface approximation scheme whose input {{is a set}} of unorganized <b>noisy</b> <b>points</b> in space and whose output {{is a set of}} quadric patches. The local surface properties, necessary for the subsequent segmentation, are estimated directly from the data using a simple and efficient data structure- the neighborhood graph. Our segmentation scheme, based on principal curvatures, constructs initial point sub-sets, which may be enlarged or further subdivided based on associated approximation error estimates obtained through approximation of the initial segments by quadric surfaces. Our method is highly efficient and produces a high-quality piecewise quadric surface approximation of engineering objects, which we demonstrate for several simple and complex example data sets. 1...|$|R
40|$|This paper {{proposes a}} general {{framework}} for overfitting control in surface reconstruction from <b>noisy</b> <b>point</b> data. The problem {{we deal with}} is {{how to create a}} model that will capture as much detail as possible and simultaneously avoid reproducing the noise of the input points. The proposed framework is based on extra-sample validation. It is fully automatic and can work in conjunction with any surface reconstruction algorithm. We test the framework with a Radial Basis Function algorithm, Multi-level Partition of Unity implicits, and the Power Crust algorithm...|$|E
40|$|The {{problem of}} {{generating}} a surface triangulation from {{a set of}} points with normal information arises in several mesh processing tasks like surface reconstruction or surface resampling. In this paper we present a surface triangulation approach {{which is based on}} local 2 d delaunay triangulations in tangent space. Our contribution is the extension of this method to surfaces with sharp corners and creases. We demonstrate the robustness of the method on difficult meshing problems that include nearby sheets, self intersecting non manifold surfaces and <b>noisy</b> <b>point</b> samples...|$|E
40|$|Abstract − 3 D point data {{acquired}} from laser scan or stereo vision {{can be quite}} noisy. A preprocessing step is often needed before a surface reconstruction algorithm can be applied. In this paper, we propose a nonparametric approach for <b>noisy</b> <b>point</b> data preprocessing. In particular, we proposed an anisotropic kernel based nonparametric density estimation method for outlier removal, and a hill-climbing line search approach for projecting data points onto the real surface boundary. Our approach is simple, robust and efficient. We demonstrate our method on both real and synthetic point datasets...|$|E
30|$|Noise Noisy data {{refers to}} data that is {{irrelevant}} or meaningless. This {{can lead to}} models that suffer from overfitting. Clustering or similarity measures can help identify <b>noisy</b> data <b>points,</b> but toolkits lack algorithms which are specifically optimized for this task [162].|$|R
40|$|It is {{difficult}} for existed polygon medial axis (MA) extracting methods to address complex polygon {{and to make sure}} of the accuracy and connectivity of MA. A new curved-polygon MA extracting method is introduced. Firstly, concepts of curved-polygon together with its MA are given. Secondly, the mean distance transformation (MDT) of the nearest border point set (NBPS) is put forward. Thirdly, a new MA point judgment regulations is constructed, and the method of greed points-growth and detection are used to extract MA. Fourthly, various complex polygons were used to inspect this new method. It was found that the impacts of border <b>noisy</b> <b>points</b> are eliminated effectively and both the accuracy and connectivity of MA are satisfactory which overcome the shortcomings of traditional methods...|$|R
40|$|We propose {{methods for}} outlier {{handling}} and noise reduction using weighted local linear smoothing {{for a set}} of <b>noisy</b> <b>points</b> sampled from a nonlinear manifold. The methods can be used by manifold learning methods such as Isomap, LLE and LTSA as a preprocessing step to obtain a more accurate reconstruction of the underlying nonlinear manifolds. Weighted PCA is used as a building block for our methods and we suggest an iterative weight selection scheme for robust local linear fitting. We also develop an efficient and effective bias-reduction method to deal with the “trim the peak and fill the valley ” phenomenon in local linear smoothing. Synthetic examples along with several image data sets are presented to show that manifold learning methods combined with weighted local linear smoothing give more accurate results. ...|$|R
40|$|Abstract — We {{present a}} new {{approach}} to perform robust proximity queries between <b>noisy</b> <b>point</b> cloud data. Our approach takes into account the uncertainty that arises due to discretization error and noise, and formulates contact computation as a twoclass classification problem. We use appropriate techniques from machine learning to compute the collision probability for each point in the input data and accelerate the computation using stochastic traversal of bounding volume hierarchies. We highlight the performance of our algorithm on point clouds captured using laser range finders, active stereo cameras and synthetic datasets. I...|$|E
40|$|We {{propose a}} generic method for obtaining {{non-parametric}} image warps from <b>noisy</b> <b>point</b> correspondences. Our formulation integrates a huber function into a motion coherence framework. This makes our fitting function es-pecially robust to piecewise correspondence noise (where an image section is consistently mismatched). By utilizing over parameterized curves, we can generate realistic non-parametric image warps from very noisy correspondence. We also demonstrate how our algorithm {{can be used}} to help stitch images taken from a panning camera by warping the images onto a virtual push-broom camera imaging plane. 1...|$|E
40|$|We {{consider}} {{the problem of}} denoising a noisily sampled submanifold M in R d, where the submanifold M is a priori unknown and we are only given a <b>noisy</b> <b>point</b> sample. The presented denoising algorithm {{is based on a}} graph-based diffusion process of the point sample. We analyze this diffusion process using recent results about the convergence of graph Laplacians. In the experiments we show that our method is capable of dealing with non-trivial high-dimensional noise. Moreover using the denoising algorithm as pre-processing method we can improve the results of a semi-supervised learning algorithm. ...|$|E
40|$|Point cloud {{source data}} for surface {{reconstruction}} is usually contaminated with noise and outliers. To overcome this deficiency, a density-based point cloud denoising method {{is presented to}} remove outliers and <b>noisy</b> <b>points.</b> First, particle-swam optimization technique is employed for automatically approximating optimal bandwidth of multivariate kernel density estimation to ensure the robust performance of density estimation. Then, mean-shift based clustering technique is used to remove outliers through a thresholding scheme. After removing outliers from the point cloud, bilateral mesh filtering is applied to smooth the remaining points. The experimental results show that this approach, comparably, is robust and efficient. Comment: 9 pages, 5 figures, to be appeared in the Proceeding of 9 th International Conference on Robotics, Vision, Signal Processing & Power Applications (ROVISP), 2 - 3 Feb 2016, Penang, Malaysi...|$|R
40|$|Abstract [...] - Detecting {{the edges}} of objects within images is a {{critical}} task for quality image processing. This paper proposes an edge detection operator based on the combination of fuzzy gradient morphology and Sobel operator. When we use traditional detection operators to detect {{the edge of an}} object in an image, we get lots of <b>noisy</b> <b>Points.</b> In this paper, we demonstrate a technique in which we preprocess an image with Sobel operator and then apply gradient morphology. This method effectively removes the noise and gives good detail image edge detection. We evaluate the method quantitatively and compare it to classical morphological method. Our fuzzy based edge segmentation method performs better than the classical edge detectors. Since the proposed methods are based on fuzzy morphological operations, these are efficient and enhanced...|$|R
40|$|In {{panorama}} images {{captured by}} omni-directional cameras during video conferencing, the image sizes {{of the people}} around the conference table are not uniform due to the varying distances to the camera. Spatiallyvarying-uniform (SVU) scaling functions have been proposed to warp a panorama image smoothly such that the participants have similar sizes on the image. To generate the SVU function, one needs to segment the table boundaries, which was generated manually in the previous work. In this paper, we propose a robust algorithm to automatically segment the table boundaries. To ensure the robustness, we apply a symmetry voting scheme to filter out <b>noisy</b> <b>points</b> on the edge map. Trigonometry and quadratic fitting methods are developed to fit a continuous curve to the remaining edge points. We report experimental results on both synthetic and real images. 1...|$|R
