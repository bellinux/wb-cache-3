67|105|Public
2500|$|When it is {{necessary}} to capture audio covering the entire 20–20,000Hz range of human hearing, such as when recording music or many types of acoustic events, audio waveforms are typically sampled at 44.1kHz (CD), 48kHz, 88.2kHz, or 96kHz. The approximately double-rate requirement is a consequence of the <b>Nyquist</b> <b>theorem.</b> [...] Sampling rates higher than about 50kHz to 60kHz cannot supply more usable information for human listeners. Early professional audio equipment manufacturers chose sampling rates in the region of 50kHz for this reason.|$|E
5000|$|... for all {{integers}} , where [...] is {{the symbol}} period. The <b>Nyquist</b> <b>theorem</b> {{says that this}} is equivalent to: ...|$|E
5000|$|Since [...] and [...] are {{conjugate}} variables (with {{respect to}} the Fourier transform) {{we can use the}} <b>Nyquist</b> <b>theorem</b> to show that the step in k-space determines the field of view of the image (maximum frequency that is correctly sampled) and the maximum value of k sampled determines the resolution; i.e., ...|$|E
5000|$|... the 8-bit, 8 kHz {{standard}} is developed; <b>Nyquist's</b> <b>theorem</b> {{and the standard}} 3.5 kHz telephony bandwidth ...|$|R
40|$|The present {{investigation}} is essentially {{concerned with the}} extension of results presented by Lehtomaki et al. (1981) on the robustness of multivariable linear time invariant feedback control systems. The work reported by Lehtomaki et al. {{is based on a}} multivariable version of <b>Nyquist's</b> <b>theorem</b> from which several robustness theorems were derived. In connection with the current investigation a slightly more general approach based on <b>Nyquist's</b> <b>theorem</b> is given in a fundamental robustness theorem from which various robustness tests may be obtained. A fundamental characterization of robustness is considered, and important tools from matrix theory are introduced. Attention is given to robustness tests and unstructured model error, and a robustness analysis for linear systems with structured model error...|$|R
40|$|Compressed sensing (CS) {{is an area}} {{of signal}} {{processing}} and statistics that emerged in the late 1990 ’s and then exploded in the mid- 2000 ’s [1, 2, 3, 4, 5]. The main idea is that many signals can be reconstructed and/or processed using many fewer measurements than predicted by a naive application of <b>Nyquist’s</b> <b>theorem.</b> CS originated with the observation that many signal processing systems first sample a larg...|$|R
5000|$|If {{a proper}} (no clipping/saturation) analog signal is {{converted}} to digital via A/D with sufficient samples, and then reconverted to analog via D/A then <b>Nyquist</b> <b>theorem</b> guarantees {{that there will be}} no problem in the analog domain due to [...] "peak" [...] issues because the restored analog signal will be an exact copy of the original analog signal.|$|E
50|$|When {{consecutive}} {{symbols are}} transmitted over a channel by a linear modulation (such as ASK, QAM, etc.), the impulse response (or equivalently the frequency response) {{of the channel}} causes a transmitted symbol to be spread in the time domain. This causes intersymbol interference because the previously transmitted symbols affect the currently received symbol, thus reducing tolerance for noise. The <b>Nyquist</b> <b>theorem</b> relates this time-domain condition to an equivalent frequency-domain condition.|$|E
50|$|When it is {{necessary}} to capture audio covering the entire 20-20,000 Hz range of human hearing, such as when recording music or many types of acoustic events, audio waveforms are typically sampled at 44.1 kHz (CD), 48 kHz, 88.2 kHz, or 96 kHz. The approximately double-rate requirement is a consequence of the <b>Nyquist</b> <b>theorem.</b> Sampling rates higher than about 50 kHz to 60 kHz cannot supply more usable information for human listeners. Early professional audio equipment manufacturers chose sampling rates in the region of 50 kHz for this reason.|$|E
30|$|The {{explosive}} growth of information {{has brought a}} great burden for signal processing and storage. In some application scenarios with resource strain on computing and bandwidth, the sampling frequency required in the tradition <b>Nyquist</b> sampling <b>theorem</b> makes signal acquisition, processing, storage, and transmission {{under the pressure of}} massive data. Particularly, the <b>Nyquist</b> sampling <b>theorem</b> increases the cost and lowers the effectiveness of data acquisition and processing equipment in the transmission and processing of large-scale image data [1, 2].|$|R
50|$|Because the superregenerative {{detectors}} tend {{to receive}} the strongest signal and ignore other signals in the nearby spectrum, the superregen works best with bands that are relatively free of interfering signals. Due to <b>Nyquist's</b> <b>theorem,</b> its quenching frequency {{must be at least}} twice the signal bandwidth. But quenching with overtones acts further as a heterodyne receiver mixing additional unneeded signals from those bands into the working frequency. Thus the overall bandwidth of superregenerator cannot be less than 4 times that of the quench frequency, assuming the quenching oscillator produces an ideal sine wave.|$|R
40|$|Abstract—Traditionally, {{the channel}} used for {{differential}} multiple-input-multiple-output (MIMO) systems is constant dur-ing one frame and changes randomly from one frame to another. This channel behavior is too simple to be realistic. In this paper, we propose a new time selective channel model for differential space-time modulation (DSTM) schemes. A {{sufficient number of}} Rayleigh channel matrices are randomly generated, and the other channel matrices are sinc interpolated according to the <b>Nyquist’s</b> sampling <b>theorem.</b> The performance of DSTM schemes with two, four and eight transmit antennas are evaluated over this time selective channel model. Simulation results show slightly degraded but more realistic performance when this new channel model is used. Keywords—MIMO, Differential Space-Time Modulation, non-coherent, channel model, <b>Nyquist’s</b> sampling <b>theorem,</b> sinc inter-polation. I...|$|R
5000|$|Many domains contain {{multiple}} displays, {{and require}} {{more than a simple}} discrete yes/no response time measurement. A critical question for these situations may be [...] "How much time will operators spend looking at X relative to Y?" [...] or [...] "What is the likelihood that the operator will completely miss seeing a critical event?" [...] Visual sampling is the primary means of obtaining information from the world. An early model in this domain is Sender's (1964, 1983) based upon operators monitoring of multiple dials, each with different rates of change. Operators try, as best as they can, to reconstruct the original set of dials based on discrete sampling. This relies on the mathematical <b>Nyquist</b> <b>theorem</b> stating that a signal at W Hz can be reconstructed by sampling every 1/W seconds. This was combined with a measure of the information generation rate for each signal, to predict the optimal sampling rate and dwell time for each dial. Human limitations prevent human performance from matching optimal performance, but the predictive power of the model influenced future work in this area, such as Sheridan's (1970) extension of the model with considerations of access cost and information sample value.|$|E
50|$|Taking {{the example}} of a current high {{definition}} (HD) video system, with 1920 by 1080 pixels, the <b>Nyquist</b> <b>theorem</b> states that it should be possible, in a perfect system, to resolve fully (with true black to white transitions) a total of 1920 black and white alternating lines combined, otherwise referred to as a spatial frequency of 1920/2=960 line pairs per picture width, or 960 cycles per picture width, (definitions in terms of cycles per unit angle or per mm are also possible but generally less clear when dealing with cameras and more appropriate to telescopes etc.). In practice, this is far from the case, and spatial frequencies that approach the Nyquist rate will generally be reproduced with decreasing amplitude, so that fine detail, though it can be seen, is greatly reduced in contrast. This gives rise to the interesting observation that, for example, a standard definition television picture derived from a film scanner that uses oversampling, as described later, may appear sharper than a high definition picture shot on a camera with a poor modulation transfer function. The two pictures show an interesting difference that is often missed, the former having full contrast on detail up to a certain point but then no really fine detail, while the latter does contain finer detail, but with such reduced contrast as to appear inferior overall.|$|E
30|$|Finally, {{sufficient}} rotational images need to {{be obtained}} to produce reconstruction that reflects equal spatial resolution as the transmission images. According to the <b>Nyquist</b> <b>theorem,</b> the minimum number of radiographs needed to achieve this effect within 180 ° of rotation is Nπ/ 2, where N is width of the sample in pixels. In practice, little difference is observed for image numbers >Nπ/ 4.|$|E
40|$|Lesion {{detectability}} {{is related}} to the spatial resolution of a radiological imaging system. Thus an understanding of a system’s spatial resolution is important to the overall clinical value of a system. Spatial resolution in MRI imager is dominated by the pixel size via the <b>Nyquist</b> sampling <b>theorem</b> 4. Resolution of an MR imager can be measured i...|$|R
40|$|International audienceIt is well {{proved that}} the signals {{transmitted}} through a mutipath channel experience Rayleigh fading. The channel also varies due to the moving of user end or the scatters and Doppler shift effect is generated. Researchers have to consider these two effects in their studies of wireless communications. Some researchers use step channel model which is too idealized to simulate MIMO systems. Besides, sum-of-sinusoid based simulators are widely used among which Jakes' simulator is the most well-known. However this kind of simulator has to strive for the deficiencies in higher-order statistics due to the approximation of random variables by determinants. In this paper, we propose a time selective channel model used for laboratory simulations from a new sight which can avoid the above drawbacks. The channel is reconstructed by sinc function based on <b>Nyquist's</b> sampling <b>theorem.</b> The performance of DSTM scheme is evaluated in this new channel model. Keywords—time-selective channel model, Rayleigh fading, sinc interpolation, <b>Nyquist's</b> sampling <b>theorem,</b> DSTM...|$|R
40|$|Estimate {{the maximum}} number C of bits of {{information}} that can be transmitted per second in a communications channel of bandwidth B, meaning that this channel acts as a low-pass filter with cutoff frequency B. 2 Solution The solution proceeds by taking the channel capacity C to be the product of {{the maximum number}} of pulses per second that can be transmitted times the maximum amount of {{information that can be}} encoded onto a single pulse. 2. 1 Maximum Number of Pulses per Second A simple model of a pulse is that it is one-half period of a sine wave. The shortest period wave that can be transmitted down a channel of bandwidth B is 1 /B. Hence, the shortest pulse that can be transmitted has period 1 / 2 B, and the maximum number of (distinct) pulses per second that can be transmitted is The estimate (1) is sometimes called <b>Nyquist’s</b> <b>theorem</b> [1]. 2. 2 Maximum Information per Pulse Npulse = 2 B. (1...|$|R
40|$|The {{connection}} of the Callen-Welton and Nyquist fluctuation-dissipation relations is considered for plasma-like classical and quantum systems. The conditions for {{appearance of the}} dissipative parameters in the equilibrium current-current correlation function are investigated. The paper presents the arguments for the restrictions of the <b>Nyquist</b> <b>theorem</b> and against violation of the Callen-Welton theorem in the quantum case. Comment: 14 pages, 0 figures, sumitted in Physica...|$|E
40|$|The paper {{discusses}} {{circumstances that}} lead to improper measurements of temperature: without appropriate consideration to the context of physical phenomena occurring in nature or without appropriate awareness of mathematical principles; all of which results in data misinterpretation. Based on the conducted discussion, it can be stated that measurements and analyses of thermal phenomena, which are time-varying processes, should be performed using the same principles and tools as {{in the identification of}} dynamic processes, such as the <b>Nyquist</b> <b>theorem...</b>|$|E
40|$|Data {{acquisition}} unit is {{the basic}} building block of any power system protection and monitoring device. Operating principle of data acquisition unit heavily depends on the Nyquist theory. As per <b>Nyquist</b> <b>theorem,</b> the sampling rate of analog-to-digital converter (ADC) should be {{greater than or equal}} to the twice of signal bandwidth. Situation of sub-Nyquist rate ADC sampling can arise in relays/PMUs if there is no anti-aliasing filter or if the frequency specification of anti-aliasing filter violates the <b>Nyquist</b> <b>theorem.</b> Traditionally, sub-Nyquist rate sampling has been considered unusable due to aliasing. Digital relays and PMUs always use anti-aliasing filter before ADC to band-limit the analog signals. Compressive Sampling (CS) theory has opened up a new possibility of signal reconstruction from sub-Nyquist rate samples if the signal is `sparse' in nature. This paper discusses the possible use of sub-Nyquist rate ADC sampling in digital relays and PMUs. This concept is comparatively new in the area of power system signal processing. Both advantages and implementation challenges of sub-Nyquist rate sampling are discussed in the paper. Results are also presented to demonstrate the phasor estimation performance with sub-Nyquist rate ADC sampling...|$|E
30|$|Before {{the second}} time-frequency {{analysis}} is executed, we extracted the centroid curve, which {{is close to}} the speed signal of the human body to a certain extent. Down-sampling is adopted to obtain a sampling frequency down to 10 Hz, which also satisfies the <b>Nyquist</b> sampling <b>theorem.</b> If the sampling frequency of the second time-frequency analysis does not undergo down-sampling, the final time-frequency diagram will exhibit serious distortion.|$|R
40|$|Abstract: According to the {{principle}} of equivalent surface area and <b>Nyquist</b> sampling <b>theorem,</b> this paper introduces the method which takes PIC 18 F 2431 SCM as the control center in details, and it forms Single-phase sine inverter power supply by adopting {{the principle}} of equivalent surface area directly. This method that can be widely used in controlling Single-phase AC motor speed is simple and cost-effective. ...|$|R
50|$|In a sense, any {{spectrum}} analyzer that has vector signal analyzer capability is a realtime analyzer. It samples data {{fast enough to}} satisfy <b>Nyquist</b> Sampling <b>theorem</b> and stores the data in memory for later processing. This kind of analyser is only realtime {{for the amount of}} data / capture time it can store in memory and still produces gaps in the spectrum and results during processing time.|$|R
40|$|The paper uses a {{frequency}} domain method for boundary control of hyperbolic conservation laws. We {{show that the}} transfer function of the hyperbolic system belongs to the Callier-Desoer algebra, for which the <b>Nyquist</b> <b>theorem</b> provides necessary and sufficient conditions for input-output closed-loop stability. We examine the link between input-output stability and exponential stability of the state. Specific results are then derived for the case of proportional boundary controllers. The results are illustrated {{in the case of}} boundary control of open-channel flow...|$|E
30|$|The receiver, in general, {{is based}} on the <b>Nyquist</b> <b>theorem</b> [3] to avoid {{aliasing}} with redundancy. The channelized receivers use multichannel parallel alternating sampling in the time [4] or frequency [5] domain. The channelized receivers have a high intercept probability and sensitivity and wide instantaneous bandwidth and dynamic range. But, the multichannel structure is large in the size, weight, and power. With the development of the compressive sensing (CS) theory, the sparse signals can be recovered exactly by solving a convex optimization problem [6].|$|E
40|$|This paper {{investigates the}} {{performance}} of decentralized multi-channel feedback analog control systems, which are flexible and economic for practical applications of active noise control. The generalized <b>Nyquist</b> <b>theorem</b> and Gerschgorin circle theorem are used to derive a sufficient stability condition {{in terms of the}} predefined maximum noise amplification and the geometrical configuration of the independent controllers, and the noise reduction performance of the multi-channel system is predicted with the design and geometrical configuration of the independent controllers. Simulation and experimental results are presented to illustrate the effectiveness of the proposed analysi...|$|E
50|$|Image scaling can be {{interpreted}} as a form of image resampling FIX or image reconstruction from the view of the <b>Nyquist</b> sampling <b>theorem.</b> According to the theorem, down sampling to a smaller image from a higher-resolution original can only be carried out only after applying a suitable 2D anti-aliasing filter to prevent aliasing artifacts. The image is reduced to the information that can be carried by the smaller image.|$|R
30|$|To {{quantify}} the image resolution, we determine two parameters experimentally, the localization precision and the localization density. The localization precision of Alexa- 647 in retinal tissue, measured as {{the spread of}} repetitive localizations of the same dye molecules, {{was found to be}} 10 [*]nm in xy and 23 [*]nm in z. These values correspond to image resolutions of 23 [*]nm in xy and 54 [*]nm in z. In addition, we found the average distance between neighboring localizations along the boundary between two cells to be 30 [*]nm, corresponding to a resolution limit of 60 [*]nm according to the <b>Nyquist</b> sample <b>theorem.</b> If we were to achieve the same, 50 [*]nm z resolution through ultra-thin sectioning, the section thickness should be no more than 25 [*]nm according to the <b>Nyquist</b> sampling <b>theorem,</b> which is 4 times smaller than the 100 [*]nm thickness used here. To reconstruct a large volume of tissue would require 4 times as many sections, which not only requires substantially longer total imaging time but is also subject to more sectioning loss and section alignment errors.|$|R
40|$|Abstract Aims To {{develop and}} {{implement}} an automated virtual slide screening system that distinguishes normal histological findings and several tissue – based crude (texture – based) diagnoses. Theoretical considerations Virtual slide technology has to handle and transfer images of GB Bytes in size. The performance of tissue based diagnosis can be separated into a) a sampling procedure to allocate the slide area containing the most significant diagnostic information, and b) {{the evaluation of the}} diagnosis obtained from the information present in the selected area. <b>Nyquist's</b> <b>theorem</b> that is broadly applied in acoustics, can also serve for quality assurance in image information analysis, especially to preset the accuracy of sampling. Texture – based diagnosis can be performed with recursive formulas that do not require a detailed segmentation procedure. The obtained results will then be transferred into a "self-learning" discrimination system that adjusts itself to changes of image parameters such as brightness, shading, or contrast. Methods Non-overlapping compartments of the original virtual slide (image) will be chosen at random and according to <b>Nyquist's</b> <b>theorem</b> (predefined error-rate). The compartments will be standardized by local filter operations, and are subject for texture analysis. The texture analysis is performed {{on the basis of a}} recursive formula that computes the median gray value and the local noise distribution. The computations will be performed at different magnifications that are adjusted to the most frequently used objectives (* 2, * 4. 5, * 10, * 20, * 40). The obtained data are statistically analyzed in a hierarchical sequence, and in relation to the clinical significance of the diagnosis. Results The system has been tested with a total of 896 lung cancer cases that include the diagnoses groups: cohort (1) normal lung – cancer; cancer subdivided: cohort (2) small cell lung cancer – non small cell lung cancer; non small cell lung cancer subdivided: cohort (3) squamous cell carcinoma – adenocarcinoma – large cell carcinoma. The system can classify all diagnoses of the cohorts (1) and (2) correctly in 100 %, those of cohort (3) in more than 95 %. The percentage of the selected area can be limited to only 10 % of the original image without any increased error rate. Conclusion The developed system is a fast and reliable procedure to fulfill all requirements for an automated "pre-screening" of virtual slides in lung pathology. </p...|$|R
30|$|Recently, a leap in signal sensing realm {{is made by}} the {{promising}} approach of compressive sensing (CS), which is mainly supported by the positive theoretical and experimental results in [2 – 6]. It enables sampling at a rate comparable to signal’s information rate. In CS, an incoherent linear projection is employed to acquire an accurate representation of compressible signal directly using few measurements, much lesser than the number prescribed by the <b>Nyquist</b> <b>theorem.</b> The signal is then recovered from undersampled measurements by solving an inverse problem either through a linear program or a greedy pursuit [7].|$|E
30|$|Reconnaissance {{receiver}} as a channelized receiver [3, 4], in general, {{is based}} on the <b>Nyquist</b> <b>theorem</b> for design of the data acquisition of wideband signals [5]. And Nyquist rate is only a necessary but not sufficient condition for signals recovered accurately [6]. For another, nonuniform sampling exists extensively in the practical system of nonideal and compressed sensing (CS) theory as a typical example of nonuniform sampling. The research in analog-to-information (A 2 I) conversion is still limited in prototype and numerical simulation [7]. And there are some requirements for the sparse characteristic of the received signals based on CS [8 – 11].|$|E
30|$|Compressed sensing (CS) is a {{sampling}} paradigm that provides the signal compression at a rate significantly below the Nyquist rate [1 – 3]. It is based on that a sparse or compressible signal can be represented by {{the fewer number of}} bases than the one required by <b>Nyquist</b> <b>theorem,</b> when it is mapped to the space with bases incoherent to the bases of the sparse space. The incoherent bases are called the measurement vectors. CS has a wide range of applications including radar imaging [4], DNA microarrays [5], image reconstruction and compression [6 – 14], etc.|$|E
25|$|Exactly how, when, or why Harry Nyquist had {{his name}} {{attached}} to the sampling theorem remains obscure. The term <b>Nyquist</b> Sampling <b>Theorem</b> (capitalized thus) appeared as early as 1959 in a book from his former employer, Bell Labs, and appeared again in 1963, and not capitalized in 1965. It had been called the Shannon Sampling Theorem as early as 1954, but also just the sampling theorem by several other books in the early 1950s.|$|R
30|$|In recent years, {{compressed}} sensing (CS)[1 – 4] {{has been}} a new and popular paradigm of signal acquisition and compression in applied science and engineering such as image processing, wireless communication, magnetic resonance imaging (MRI) and so on. In contrast with the conventional <b>Nyquist</b> sampling <b>theorem,</b> CS theory demonstrates that a sparse signal can be exactly recovered through far fewer projections, providing that the sensing matrix is highly incoherent with the sparsifying matrix.|$|R
50|$|Exactly how, when, or why Harry Nyquist had {{his name}} {{attached}} to the sampling theorem remains obscure. The term <b>Nyquist</b> Sampling <b>Theorem</b> (capitalized thus) appeared as early as 1959 in a book from his former employer, Bell Labs, and appeared again in 1963, and not capitalized in 1965. It had been called the Shannon Sampling Theorem as early as 1954, but also just the sampling theorem by several other books in the early 1950s.|$|R
