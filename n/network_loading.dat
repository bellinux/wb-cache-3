236|3971|Public
50|$|As another example, {{enabling}} {{fault reporting}} for Internet network packet delivery failure will increase <b>network</b> <b>loading</b> when {{the network is}} already busy, and that will cause total network outage.|$|E
50|$|The {{deterministic}} {{nature of}} the routing makes offline prediction/computation/experimentation of the <b>network</b> <b>loading</b> much simpler since actual routes are not dependent on {{the contents of the}} packet headers {{with the exception of the}} VLAN identifier.|$|E
5000|$|... 1986 - Will Recker and Michael McNally {{proposed}} the theoretical {{background for the}} first operational activity-based model, STARCHILD 1986 - Will Recker and Michael McNally presented part II of their STARCHILD model 1987 - Thomas Golob, Will Recker, and John Leonard studied truck-related accidents using log-linear models and found that durations of accidents were log-normally distributed 1992 - Kenneth Small proposed a revenue distribution model to make congestion pricing practical and politically viable 1992 - Genevieve Giuliano reviewed congestion pricing policies and suggested politically acceptable alternatives 1994 - Charles Lave and Patrick Elias examined the 65 mph speed limit systematically at the statewide level and showed that the change from 55 mph reduced fatality rates 1995 - Kelvin Cheu and Stephen Ritchie developed an incident detection algorithm based on inductive loop data and an artificial neural network method 1995 - Kenneth Small measured the costs of air pollution in Los Angeles from motor vehicles 1995 - Jayakrishnan, Wei Tsai, and Anthony Chen developed a dynamic traffic assignment model with <b>network</b> <b>loading</b> that made use of DYNASMART, the first mesoscopic traffic simulation model 1996 - Michael Zhang, Stephen Ritchie, and Will Recker formulated the ramp metering control as a dynamic optimal control problem and presented solution methods and validation 1997 - Thomas Golob and Michael McNally used a structural model to explain interactions between household members 1998 - Randall Crane empirically tested the hypothesis that urban design can influence travel, showing that no such evidence could be found from local household travel survey and GIS data 1999 - Carlos Sun, Stephen Ritchie, Kevin Tsai, and Jayakrishnan formulated the vehicle reidentification problem as a lexicographic optimization problem and demonstrated robust performance 2000 - David Brownstone explored the advantages of merging stated preference and revealed preference data in an empirical study to evaluate alternative fuel vehicle market penetration 2000 - Thomas Golob developed a model that jointly generates activity participation, travel time, and trip generation ...|$|E
40|$|Multiconstraints optimal <b>network</b> <b>load</b> {{balancing}} is an NP-hard {{problem and}} it {{is an important part of}} traffic engineering. In this research we balance the <b>network</b> <b>load</b> using classical method (brute force approach and dynamic programming is used but result shows the limitation of this method) but at a certain level we recognized that the optimization of balanced <b>network</b> <b>load</b> with increased number of nodes and demands is intractable using the classical method because the solution set increases exponentially. In such case the optimization techniques like evolutionary techniques can employ for optimizing <b>network</b> <b>load</b> balance. In this paper we analyzed proposed classical algorithm and evolutionary based genetic approach is devise as well as proposed in this paper for optimizing the balance <b>network</b> <b>load...</b>|$|R
40|$|In this paper, a backoff mechanism, Exponential Linear Backoff Algorithm (ELBA), is {{proposed}} to improve system performance over contention-based wireless networks. In the ELBA, the variation of contention window size is combined both exponentially and linearly, dependent on the <b>network</b> <b>load,</b> {{as indicated by the}} number of consecutive collisions. In the ELBA, a threshold is set to determine the <b>network</b> <b>load.</b> If the contention window size is smaller than the threshold, a light <b>network</b> <b>load,</b> the contention window is tuned exponentially. Conversely, if the contention window size is larger than the threshold, a heavy <b>network</b> <b>load,</b> the contention window size is tuned linearly. The numerical results show that the ELBA provides a better system throughput and collision rate in both light and heavy <b>network</b> <b>loads</b> than the related backoff schemes, including binary exponential backoff (BEB), exponential increase exponential decrease (EIED) and linear increase linear decrease (LILD). Categories and Subject Descriptor...|$|R
40|$|Abstract — This paper studies {{impact of}} {{changing}} mobility {{speed on the}} performance of a reactive routing protocol AODV with reference to varying <b>network</b> <b>load.</b> For experimental purposes, initially we observed the performance of AODV with increasing <b>Network</b> <b>Load</b> from 4 packets to 24 packets at the maximum mobility speed of 10 m/s. In another scenario we observed the performance of AODV with increasing <b>Network</b> <b>Load</b> from 4 packets to 24 packets at maximum mobility speed of 20 m/s. The performance of AODV is observed across Packet Delivery Ratio, Loss Packet Ratio and Routing overhead parameters. Our simulation results show that AODV is performing better with higher mobility speed at higher <b>network</b> <b>load.</b> ...|$|R
40|$|Abstract. <b>Network</b> <b>loading</b> {{problems}} {{occur in}} the design of telecommunication networks, in many different settings. For instance, bifurcated or non-bifurcated routing (also called splittable and unsplittable) can be considered. In most settings, the same polyhedral structures return. A better understanding of these structures therefore can {{have a major impact on}} the tractability of polyhedral-guided solution methods. In this paper, we investigate the polytopes of the problem restricted to one arc/edge of the network (the undirected/directed edge capacity problem) for the non-bifurcated routing case. As an example, one of the basic variants of <b>network</b> <b>loading</b> is described, including an integer linear programming formulation. As the edge capacity problems are relaxations of this <b>network</b> <b>loading</b> problem, their polytopes are intimately related. We give conditions under which the inequalities of the edge capacity polytopes define facets of the <b>network</b> <b>loading</b> polytope. We describe classes of strong valid inequalities for the edge capacity polytopes, and we derive conditions under which these constraints define facets. For the diverse classes the complexity of lifting projected variables is stated. The derived inequalities are tested on (i) the edge capacity problem itself and (ii) the described variant of the <b>network</b> <b>loading</b> problem. The results show that the inequalities substantially reduce the number of nodes needed in a branch-and-cut approach. Moreover, they show the importance of the edge subproblem for solving <b>network</b> <b>loading</b> problems. Key words. network design – mixed integer programming – cutting planes – knapsack with integer capacity 1...|$|E
40|$|<b>Network</b> <b>loading</b> {{problems}} {{occur in}} the design of telecommunication networks, in many different settings. The polyhedral structure of this problem is important in developing solution methods for the problem. In this paper we investigate the polytope of the problem restricted to one edge of the network (the edge capacity problem). We describe classes of strong valid inequalities for the edge capacity polytope, and we derive conditions under which these constraints define facets. As the edge capacity problem is a relaxation of the <b>network</b> <b>loading</b> problem, their polytopes are intimately related. We, therefore, also give conditions under which the inequalities of the edge capacity polytope define facets of the <b>network</b> <b>loading</b> polytope. Furthermore, some structural properties are derived, such as the relation of the edge capacity polytope to the knapsack polytope. We conclude the theoretical part of this paper with some lifting theorems, where we show that this problem is polynomially solvable for most of our classes of valid inequalities. The derived inequalities are tested on (i) the edge capacity problem itself and (ii) a variant of the <b>network</b> <b>loading</b> problem. The results show that the inequalities substantially reduce the number of nodes needed in a branch-and-cut approach. Moreover, they show the importance of the edge subproblem for solving <b>network</b> <b>loading</b> problem...|$|E
40|$|We {{consider}} the <b>Network</b> <b>Loading</b> Problem under a polyhedral uncertainty {{description of traffic}} demands. After giving a compact multi-commodity formulation of the problem, we prove an unexpected decomposition property obtained from projecting out the flow variables, considerably simplifying the resulting polyhedral analysis and computations by doing away with metric inequalities, an attendant feature of most successful algorithms on the <b>Network</b> <b>Loading</b> Problem. Under a specific choice of the uncertainty description (hose model), we study the polyhedral aspects of <b>Network</b> <b>Loading</b> Problems, {{used as the basis}} of an efficient Branch-and-Cut algorithm supported by a simple heuristic for generating upper bounds. The results of extensive computational experiments on well-known network design instances are reported...|$|E
30|$|For AT&T, if the <b>network</b> <b>load</b> is low with |Q|= 50, Offline-R has {{an average}} link {{utilization}} of 2.9 % against 3.23 % for Offline-MO. In high <b>network</b> <b>load</b> scenarios, i.e., |Q|= 2000, the average link utilization of Offline-R reaches 56.13 % versus 62.71 % for Offline-MO; this indicates {{an increase of}} 1935.5 and 1941.4 %, respectively. These results are also consistent for Abilene. Under low <b>network</b> <b>load,</b> i.e., |Q|= 50, Offline-R has an average link utilization of 12.64 % and Offline-MO 18.18 %; when the <b>network</b> <b>load</b> is increased to |Q|= 2000, the average link utilization rises to 86.01 and 93.15 %, respectively, which means a rate of increase of 680.45 % for Offline-R and 512 % for Offline-MO. The average link utilization of Abilene is consistently 2.1 times that of AT&T under the same <b>network</b> <b>load.</b> This is because Abilene has fewer links, or smaller network capacity. This difference in link utilization has {{a direct impact on}} the final number of active links.|$|R
40|$|Bonneville {{will provide}} Network Integration Transmission Service {{pursuant}} to the terms and conditions contained in this Tariff and Service Agreement. The service that Bonneville will provide under this Tariff allows a Transmission Customer to integrate, economically dispatch and regulate its current and planned Network Resources to serve its <b>Network</b> <b>Load.</b> <b>Network</b> Integration Transmission Service also may {{be used by the}} Transmission Customer to deliver nonfirm energy purchases to its <b>Network</b> <b>Load</b> without additional charge. To the extent that the transmission path for moving power from a Network Resource to a <b>Network</b> <b>Load</b> includes the Eastern and Southern Interties, the terms and conditions for service over such intertie facilities are provided under Part 2 of this Tariff. Also, transmission service for third-party sales which are not designated as <b>Network</b> <b>Load</b> will be provided under Bonneville's Point-to-Point Transmission Service (Part 2 of this Tariff) ...|$|R
30|$|Voice traffic {{throughput}} for EDCA and {{the proposed}} mechanism closely follow each other, both at the uplink and the downlink when the <b>network</b> <b>load</b> is low (4 – 5 transmitting stations). However, when the <b>network</b> <b>load</b> is moderate (6 – 7 transmitting stations), the throughput of the proposed mechanism at the uplink closely follows that of EDCA, but at the downlink, the proposed mechanism faces a slight degradation in throughput as compared to EDCA, this slight depreciation {{is because of the}} relatively higher medium occupancy by other applications at that instant. At high <b>network</b> <b>loads</b> (8 – 9 transmitting stations), sharp depreciation in the throughput of EDCA is seen both at the uplink and downlink while the proposed mechanism offers a gradual depreciation in throughput as depicted in Figure  3 a. Video traffic's throughput for EDCA {{and the proposed}} mechanism closely follow each other both at the uplink and downlink as shown in Figure  3 b. But as the <b>network</b> <b>load</b> increases, abrupt depreciation is experienced by EDCA, and the proposed mechanism betters EDCA at each value of the <b>network</b> <b>load</b> because of the better adaptiveness of the proposed mechanism for video traffic. Web-browsing traffic observes less throughput at the uplink for the proposed mechanism than EDCA when the <b>network</b> <b>load</b> is low, but when the <b>network</b> <b>load</b> gets higher (6 – 9 transmitting stations), the proposed mechanism experiences higher throughput than EDCA and the throughput depreciation with increasing <b>network</b> <b>load</b> is also gradual {{in the case of the}} proposed mechanism as shown in Figure  3 c. At the downlink, the throughput of the web-browsing traffic for the proposed mechanism outperforms EDCA for every value of the <b>network</b> <b>load,</b> conforming to the proposed mechanism's design of awarding more priority to the downlink traffic because of the asymmetric traffic volume at the uplink and the downlink. File-sharing traffic experiences better throughput for the proposed mechanism as compared to EDCA both at the uplink and the downlink as illustrated in Figure  3 d. The poor performance of EDCA for non-real-time traffic supports the argument that EDCA neglects non-real-time applications in favour of real-time applications. The benefits of the proposed mechanism are more prominent at higher <b>network</b> <b>loads</b> but are visible at lower <b>network</b> <b>loads</b> as well. EDCA actually provides overwhelming share of bandwidth to voice traffic which suppresses the throughput of other applications. The proposed mechanism on the other hand provides adequate bandwidth to voice and all other applications. At higher <b>network</b> <b>loads,</b> EDCA is unable to provide such excess of bandwidth to real-time applications either, that is where the performance of the proposed mechanism is maximum because of the better fairness and more efficient bandwidth utilization features of the proposed mechanism.|$|R
30|$|Thus, the {{proposed}} QoS-DFFR scheme reduces co-channel interference and optimizes the sector, per-user, and system throughput by satisfying <b>network</b> <b>loading</b> conditions.|$|E
40|$|AbstractIn {{the design}} of {{telecommunication}} networks, decisions concerning capacity installation and routing of commodities {{have to be taken}} simultaneously. <b>Network</b> <b>Loading</b> problems formalize these decisions in mathematical optimization models. Several variants of the problem exist: bifurcated or non-bifurcated routing, bidirected or unidirected capacity installation, and symmetric versus non-symmetric routing restrictions. Moreover, different concepts of reliability can be considered. In this paper, we study the polyhedral structure of two basic problems for non-bifurcated routing: <b>network</b> <b>loading</b> with bidirected and unidirected capacity installation. We show that strong valid inequalities for the substructure restricted to a single edge, are also strong valid inequalities for the overall models. In a computational study, several classes of inequalities, both for the substructure and the overall problem, are compared on real-life instances for both variants of <b>network</b> <b>loading...</b>|$|E
40|$|This article {{presents}} a computationally ecient technique for the macroscopic (equationbased) <b>network</b> <b>loading</b> of microscopic (simulation based) travel demand. It combines the expressive power of microsim-ulations as sampling tools for highly heterogeneous demand segments with mathematically wellunderstood macroscopic <b>network</b> <b>loading</b> procedures. The presented experimental results demonstrate the ef-ciency and precision {{of the approach}} even for large and complex scenarios. This work is intended {{to contribute to the}} understand-ing and application of microsimulation techniques in the context of mathematically motivated dynamic trac assignment procedures...|$|E
40|$|This paper {{presents}} {{a study of}} throughput performance of voice and video traffic based on simulative and analytical methods in IPv 4 /IPv 6 networks. This research aims {{to find out what}} Internet protocol performs better in terms of throughput under congested circumstances in IPv 4 /IPv 6 networks. In doing so three different <b>network</b> <b>loads</b> in both scenarios are considered. In the context of <b>network</b> <b>load,</b> the importance of varying <b>network</b> <b>load</b> is realized while configuring and simulating the network models. For instance, a medium <b>network</b> <b>load,</b> high <b>network</b> <b>load</b> then a worst possible <b>network</b> <b>load</b> are considered to understand the impact on the performance of throughput in IPv 4 /IPv 6 networks. The network topology scenarios are partially meshed on the implication with a small ISP domain as this is an ideal choice of IP domain corresponded to a realistic network topology. Two network models are defined which allow us to compare the obtained results. In addition, IPv 4 network model is used and extended in terms of configuring IPv 6 network model as. The simulation has been carried out using Optimized Network Engineering Tool (OPNET). This paper shows that analytical and simulative approaches produce same results in terms of throughput performance for video/voice traffic. From Internet Protocol performance perspective, IPv 6 experiences more throughput than IPv 4...|$|R
30|$|From Figure 3 we {{can also}} infer {{that one of the}} major sources of {{perturbation}} upon the RT communication is the increase of the external <b>network</b> <b>load</b> and not only the error-prone characteristics of the wireless medium. It is interesting to compare the impact upon the average queue size between an error-free and an error-prone channel in the following cases: (a) undisturbed versus disturbed scenarios; (b) increase of the <b>network</b> <b>load</b> from 10 % to 50 %. In both cases, the impact of an error-prone channel is smaller than the increase of the external <b>network</b> <b>load.</b>|$|R
30|$|We {{observe the}} smaller {{rate for the}} optimal scheme when the <b>network</b> <b>load</b> is low {{as a result of}} {{changing}} retransmission times m to ensure the delay to be below the threshold required by the user. However, when the <b>network</b> <b>load</b> is high, the optimal scheme can achieve a much higher throughput than the non-optimal scheme.|$|R
30|$|To {{solve the}} issues of QoS and <b>network</b> <b>loading</b> conditions, we propose a QoS-based dynamic FFR (QoS-DFFR) scheme for {{frequency}} band allocation to fulfill users' QoS requirements and to maximize per-user and per-sector throughput. In the proposed QoS-DFFR scheme, the concept of bonus bandwidth (BBW) is introduced to dynamically allocate the frequency bands to the PR zones according to QoS requirements and service priorities of the users. In this way, the QoS of the users can be guaranteed, and the per-user and per-sector throughput can be optimized, according to <b>network</b> <b>loading</b> conditions.|$|E
40|$|AbstractGiven {{a simple}} graph G(V,E) {{and a set}} of traffic demands between the nodes of G, the <b>Network</b> <b>Loading</b> Problem {{consists}} of installing minimum cost integer capacities on the edges of G allowing routing of traffic demands. In this paper we study the Capacity Formulation of the <b>Network</b> <b>Loading</b> Problem, introducing the new class of Tight Metric Inequalities, that completely characterize the convex hull of the integer feasible solutions of the problem. We present separation algorithms for Tight Metric Inequalities and a cutting plane algorithm, reporting on computational experience...|$|E
40|$|In {{this paper}} a new {{formulation}} of within-day dynamic traffic assignment is presented, where a dynamic user equilibrium is {{expressed as a}} fixed-point problem in terms of arc flow temporal profiles. Specifically, it is shown that, by extending to the dynamic case the concept of <b>network</b> <b>loading</b> map, one need not introduce the continuous <b>network</b> <b>loading</b> problem {{in order to ensure}} the temporal consistency of the supply model. On this basis it is possible to devise efficient assignment algorithms, based on piece-wise linear or piece-wise constant approximation of temporal profiles over predefined time intervals covering the period of analysis, whose complexity is equal to the one resulting in the static case multiplied by the number of time intervals. With specific reference to a Logit path choice model, an implicit path enumeration <b>network</b> <b>loading</b> procedure is obtained as an extension of Dial's algorithm; then, the fixed-point problem is solved through the Bather's method. (C) 2004 Elsevier Ltd. All rights reserved...|$|E
30|$|As the <b>network</b> <b>load</b> increases, buffers {{start to}} fill up and {{messages}} are dropped or are not forwarded in a useful timeframe, leading to a decreased delivery ratio. Latency also shows a slightly downward trend with increasing <b>network</b> <b>load,</b> as when buffers are full, messages generated closer to the sink {{are more likely to}} be delivered.|$|R
40|$|International audienceThe paper {{considers}} {{the impact of}} changing code parameters on the <b>network</b> <b>load,</b> for some given storage-flexible Data Center Network (DCN), i. e. such DCN in which the reliability and the storage volume can be modified during the storage life of the DCN data. Two regimes of the <b>network</b> <b>load</b> are considered: transition (during the migration process) and stationary (at {{the end of the}} migration process). Our main result is the derivation of the link between reliability and <b>network</b> <b>load</b> via code parameters; clearly, this link is code-dependent. Two different erasure-coding families are considered as examples (MDS and LDPC codes), to illustrate the dependence in two different coding cases...|$|R
5000|$|Monitoring of server resources: system <b>load,</b> <b>network</b> <b>load,</b> and disk usage, {{using an}} agent.|$|R
30|$|Hence, in the {{conventional}} FFR scheme, {{a significant amount of}} BW is wasted because the scheme does not consider the dynamic <b>network</b> <b>loading</b> conditions. This BW wastage results in a less spectrally efficient system.|$|E
40|$|In {{this paper}} the Dynamic User Equilibrium is {{formulated}} and solved as a fixed point problem {{in terms of}} time-continuous real valued temporal profiles of arc inflows and arc performances, where travel demand is specified both through a deterministic and a stochastic implicit path choice model. By extending to the dynamic case the concept of <b>Network</b> <b>Loading</b> Map, yielding arc flows for given demand flows consistently with certain arc performances, the equilibrium is formulated without introducing the Dynamic <b>Network</b> <b>Loading</b> as a subproblem. The proposed assignment algorithm reflects this approach as the temporal consistency of the flow pattern through the arc performance model is attained only jointly with the equilibrium. In addiction, a new dynamic shortest path algorithm is presented to evaluate the <b>Network</b> <b>Loading</b> Map in the deterministic case, which {{can also be utilized}} within a Montecarlo simulation to evaluate it in the stochastic case. The main feature of this algorithm is its capability to handle a continuous travel time pattern by approximating the temporal profiles with piece-wise linear functions of time defined on long time intervals...|$|E
40|$|Previous {{works on}} transit {{assignment}} focused on addressing transit <b>network</b> <b>loading</b> with implicit enumeration {{of the travel}} alternatives, where users waiting at the line stops are assumed to make adaptive choices, depending on given line frequencies and other information concerning the forthcoming vehicles...|$|E
40|$|The {{proliferation}} of information providers {{has led to}} an increased number of users browsing the World Wide Web. ‘lime-consuming interaction and increased <b>network</b> <b>loads</b> are some of the adverse effects of browsing activities. This paper presents a design based on agent technology and the use of structured information, that significantly lowers <b>network</b> <b>load</b> and hits user-interaction by automating the task of browsing. ...|$|R
40|$|Abstract — In {{this paper}} we analyze more than 100 hours of packet traces from Planet-Lab {{measurements}} {{to study the}} correlation of Internet packet losses. We first apply statistical tests to identify the correlation timescale of the binary loss data. We find that in half of the traces packet losses are far from independent. More significantly, the correlation timescale of packet losses is correlated with the <b>network</b> <b>load.</b> We then examine the loss runs and the success runs of packets. The loss runs are typically short, regardless of the <b>network</b> <b>load.</b> We find that the success runs {{in the majority of}} our traces are also uncorrelated. Furthermore, their correlation timescale also does not depend on the <b>network</b> <b>load.</b> All of these results show that the impact of <b>network</b> <b>load</b> on the correlation of packet losses is nontrivial and that loss runs and success runs are better modelled as being independent than the binary losses themselves. I...|$|R
40|$|Replicating or caching popular {{content in}} {{memories}} distributed across {{the network is}} a technique to reduce peak <b>network</b> <b>loads.</b> Conventionally, the performance gain of caching was thought to result from making part of the requested data available closer to end users. Recently, {{it has been shown}} that by using a carefully designed technique to store the contents in the cache and coding across data streams a much more significant gain can be achieved in reducing the <b>network</b> <b>load.</b> Inner and outer bounds on the <b>network</b> <b>load</b> v/s cache memory tradeoff were obtained in (Maddah-Ali and Niesen, 2012). We give an improved outer bound on the <b>network</b> <b>load</b> v/s cache memory tradeoff. We address the question of to what extent caching is effective in reducing the server load when the number of files becomes large as compared to the number of users. We show that the effectiveness of caching become small when the number of files becomes comparable to the square of the number of users...|$|R
40|$|This paper {{presents}} {{a class of}} algorithms that solves the <b>network</b> <b>loading</b> problem within a dynamic traffic assignment. The different design parameters of the link transmission models are discussed and one specific implementation is presented. The selected approach provides computational efficient solutions for regional mobility planners. status: accepte...|$|E
40|$|A common {{drawback}} {{to most of}} the DTA models is that they do not represent properly spill-back of congestion. Simulation has been suggested as an approach to overcome the problem. The solution approach to DTA would then consist of two components, an analytical method to determine the path dependent flow rates and a simulation based <b>network</b> <b>loading</b> to determine arc volumes. The improvements in software and hardware technologies have made possible to simulate microscopically real networks of sensitive size. This paper describes a heuristic approach to DTA in which two alternative analytical components are used to determine the path flow rates, one based on a stochastic route choice method, and another one based on an approximation to dynamic user equilibrium conditions, while the <b>network</b> <b>loading</b> is done by a microscopic simulation model...|$|E
40|$|In {{studies on}} the {{influence}} of incidents on travel time researchers rely on Monte Carlo simulation. This procedure is very demanding computational-wise, which limits the research scope. This paper presents a highly efficient method for approximately quantifying congestion spillback due to incidents: Marginal Incident Computation (MIC). MIC superimposes the effect of an incident on a single base simulation run (without incidents) instead of carrying out a complete dynamic <b>network</b> <b>loading</b> with the incident, which would involve many calculations identical to the base simulation (e. g. prior to or far away from the incident). Whereas the results obtained with MIC vary only slightly from the outcome of a complete dynamic <b>network</b> <b>loading,</b> the gain in computation time is significant: a factor > 1100 for a case study of the Sioux Falls benchmark network. status: publishe...|$|E
5000|$|Computation bundles - {{converts}} computational client/server round {{trips to}} relocatable data bundles, reducing <b>network</b> <b>load.</b>|$|R
30|$|In the {{following}} sections, note that low <b>network</b> <b>load</b> refers to scenarios {{with no more}} than 300 LSP requests and their max requested bandwidth is {{less than or equal to}} 200 Mb. Conversely, we use the term high <b>network</b> <b>load</b> for scenarios where the number of LSP requests is at least 1000 and their max requested bandwidth is greater than or equal to 600 Mb.|$|R
40|$|The use of high {{performance}} networking technologies, e. g., ATM networks, demands much from both operating systems and processors. During high <b>network</b> <b>loads,</b> user threads may be starved because the processor spends all its time handling interrupts. To alleviate this problem, we propose using a two level network interface servicing scheme that uses interrupts during low <b>network</b> <b>loads</b> to provide low latency, and polling threads during high <b>network</b> <b>loads</b> to avoid user thread starvation. In this paper, {{we examine the}} use of such a scheme on dual-processor workstations running Windows NT connected by an ATM network. Performance evaluation of our multiprocessor prototype implementation shows that using our two level scheme can improve performance when used carefully...|$|R
