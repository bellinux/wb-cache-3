22|5|Public
2500|$|When Shannon stated {{and proved}} the {{sampling}} theorem in his 1949 paper, according to Meijering [...] "he {{referred to the}} critical sampling interval T = 1/(2W) as the <b>Nyquist</b> <b>interval</b> corresponding to the band W, in recognition of Nyquist’s discovery of the fundamental importance of this interval in connection with telegraphy." [...] This explains Nyquist's name on the critical interval, {{but not on the}} theorem.|$|E
5000|$|Long before Harry Nyquist had {{his name}} {{associated}} with sampling, the term Nyquist rate was used differently, with a meaning {{closer to what}} Nyquist actually studied. Quoting Harold S. Black's 1953 book Modulation Theory, in the section <b>Nyquist</b> <b>Interval</b> of the opening chapter Historical Background: ...|$|E
5000|$|When Shannon stated {{and proved}} the {{sampling}} theorem in his 1949 paper, according to Meijering [...] "he {{referred to the}} critical sampling interval T = 1/(2W) as the <b>Nyquist</b> <b>interval</b> corresponding to the band W, in recognition of Nyquist’s discovery of the fundamental importance of this interval in connection with telegraphy." [...] This explains Nyquist's name on the critical interval, {{but not on the}} theorem.|$|E
40|$|PURPOSE: Zero echo time (ZTE) imaging is {{a robust}} and silent 3 D radial {{technique}} suitable for direct MRI of tissues with very rapid transverse relaxation. Given its successful application on micro- and animal MRI systems, {{the purpose of}} this work is to enable and demonstrate ZTE imaging in humans using a whole-body magnet. METHODS: A commercial 7 T MRI scanner was complemented by rapid high-power transmit-receive switches, a custom-built spectrometer, and a proton-free detector coil. With this setup, transmit-receive switching is achieved within 1 µs, radiofrequency (RF) excitation is performed in 3 µs, and digital bandpass filtering takes 5. 3 µs, resulting in an effective dead time of only 5 µs. RESULTS: ZTE imaging was performed at 250 and 500 kHz bandwidth with central k-space gaps of 1. 2 and 2. 5 <b>Nyquist</b> <b>intervals</b> and repetition times of 739 and 471 µs. The technique was applied for silent 3 D imaging of the head and joints of human volunteers at an isotropic resolution down to 0. 83 mm. A sound pressure level of 41 dB(A) was measured, which is a reduction of more than 40 dB(A) compared to gradient-switched MRI. CONCLUSION: ZTE imaging in humans was demonstrated for the first time, enabled by dedicated, high-performing RF hardware...|$|R
40|$|Abstract — One of {{the main}} goals in sensor {{networks}} is to measure some physical phenomena by sensors in an area. Usually the physical data is distributed smoothly, that is, the data is usually a band-limited signal in the frequency domain. But {{when we consider the}} deployment of sensor nodes, we are not always allowed to put our nodes on or near critical sampling points (ergo, <b>Nyquist</b> <b>intervals)</b> in the spatial area because of many other considerations like connectivity, coverage, reliability and so on. This limits the amount of data that we can actually measure. However, under some mild conditions, we can perfectly reconstruct unknown data points using results from irregular sampling theory. In fact, using these results means that some measurements in the network are unnecessary and we can recover the physical data using only a subset of nodes. Using this observation, we propose a new sleep scheduling strategy. Every time slot we only have to wake up some nodes but still guarantee the reconstruction of physical data in this area. By doing this, only some nodes need to be active in a time slot and we can save a lot of energy in the network. We can also store less data in every node and transmit less data to the sink. This reduces the buffer size we need in every node and the data traffic in the whole network. It also saves the energy and reduces the time delay in transmission of data to the sink. I...|$|R
40|$|A new dealiasing scheme {{uses the}} full four-dimensionality {{available}} in an operational Doppler radar data stream. It examines one tilt angle at a time, {{beginning at the}} highest elevation where clutter is minimal and gate-to-gate shear is typically low compared to the Nyquist velocity. It then dealiases each tilt in descending order until the entire radial velocity volume is corrected. In each tilt, the algorithm performs six simple steps. In the first two steps, a reflectivity threshold and filter are applied to the radial velocity field to remove unwanted noise. The third step initializes dealiasing by attempting to adjust the value of each gate by <b>Nyquist</b> <b>intervals</b> such that it agrees with both the nearest gate in the next higher tilt and the nearest gate in the previous volume. The gates that pass the third step at a high confidence level become the initial values for step four, which consist of correcting the neighboring gates within the scan, while preserving environmental shear as much as possible. In step five, remaining gates are compared {{to an average of}} neighboring corrected gates, and anomalous gates are deleted. As a last resort, step six uses a velocity azimuth display (VAD) analysis of the wind field to interpret and correct any remaining isolated echoes. This scheme uses all available data dimensions to interpret and dealias each tilt and is efficient enough to operate on a continuous data stream. It performs reliably even in difficult dealiasing situations and at low Nyquist velocity. During two complex events observed by low-Nyquist Doppler radar in the European Alps, 93 % of 4300 tilts were dealiased without error. When errors did occur, they were usually confined to small regions and most frequently resulted from the occurrence of gate-to-gate shear that was impossible to resolve by the Nyquist velocity...|$|R
3000|$|... [*]kHz, lie in {{the lower}} half of the <b>Nyquist</b> <b>interval,</b> that is, between 0 and 11025 [*]Hz (corresponding to the angular {{frequency}} range from 0 to [...]...|$|E
3000|$|The {{achievable}} rate {{in case of}} Nyquist rate sampling {{is limited}} by the quantization resolution of the analog-to-digital converter (ADC). In this regard, a flash converter consisting of NComp comparators limits the maximum achievable rate to [...] _ 2 (N_Comp+ 1) bits per <b>Nyquist</b> <b>interval</b> [1]. Differently, by time interleaving NComp comparator operations per <b>Nyquist</b> <b>interval,</b> 2 ^N_Compi̇ quantization regions exist, which enhances the limit of the achievable rate to NComp bits per <b>Nyquist</b> <b>interval.</b> In this regard, employing 1 -bit quantization and oversampling at the receiver is promising in terms of the achievable rate. Moreover, a 1 -bit ADC at the receiver is robust against amplitude uncertainties such that the automatic gain control can be simplified, and linearity requirements of the analog frontend are relaxed. Last but not least, a 1 -bit ADC requires only simple circuitry and does not need much headroom for amplitude processing, which makes it appropriate for low supply voltages and with this low energy consumption. All these motivate us to study the achievable rate of channels with 1 -bit output quantization and oversampling at the receiver.|$|E
3000|$|... -plane [22, 27]. A second consequence, {{which to}} our {{knowledge}} has not been recognized up till now, is that the estimated poles tend to be equally distributed around the unit circle when noise is present, even at high signal-to-noise ratios and for low-AR model orders. From this observation, it follows that signals with tonal components that are approximately equally distributed in the <b>Nyquist</b> <b>interval</b> can be better represented with an all-pole model than signals that have their tonal components concentrated in a selected region of the <b>Nyquist</b> <b>interval.</b> Unfortunately, audio signals tend {{to belong to the}} latter class of signals, since they are typically sampled at a sampling frequency that is much higher than the frequency of their dominating tonal components.|$|E
40|$|This paper {{describes}} {{the implementation of}} an improved clutter suppression method for the multiple pulse repetition time (PRT) technique based on simulated radar data. The suppression method is constructed using maximum likelihood methodology in time domain and is called parametric time domain method (PTDM). The procedure relies {{on the assumption that}} precipitation and clutter signal spectra follow a Gaussian functional form. The multiple interleaved pulse repetition frequencies (PRFs) that are used in this work are set to four PRFs (952, 833, 667, and 513 [*]Hz). Based on radar simulation, it is shown that the new method can provide accurate retrieval of Doppler velocity even in the case of strong clutter contamination. The obtained velocity is nearly unbiased for all the range of <b>Nyquist</b> velocity <b>interval.</b> Also, the performance of the method is illustrated on simulated radar data for plan position indicator (PPI) scan. Compared with staggered 2 -PRT transmission schemes with PTDM, the proposed method presents better estimation accuracy under certain clutter situations...|$|R
40|$|Currently, {{there is}} no {{standard}} nomenclature and procedure to systematically identify the scale and magnitude of bedforms such as bars, dunes and ripples that are commonly present in many sedimentary environments. This thesis proposes a standardization of the nomenclature and symbolic representation of bedforms, and details the combined application of robust spline filters and continuous wavelet transforms to discriminate these morphodynamic features, namely bedform hierarchies (BHs). The proposed methodology for bedform discrimination is applied to synthetic bedform signals, which are sampled at a <b>Nyquist</b> ratio <b>interval</b> of 5 to 100 and a signal-to-noise ratio interval of 1 to 20, and to a detailed 3 D bed survey of the Rio Parana, Argentina, which exhibits large-scale dune bedforms with superimposed, smaller bedforms. After discriminating the synthetic bedform signals into 3 BHs that represent bars, dunes and ripples, {{the accuracy of the}} methodology is quantified by estimating the reproducibility, the cross correlation and the standard deviation ratio of the actual and retrieved signals. For the case of the field measurements, the proposed method is used to discriminate small and large dunes; and subsequently, obtain and statistically analyze the common morphological descriptors such as wavelength, slope, and amplitude for both stoss and lee sides of these different size bedforms. The analysis of the synthetic signals demonstrates that the Morlet wavelet function is the most efficient in retrieving smaller periodicities such as ripples and that the proposed methodology effectively discriminate the waves of different periodicities scales for Nyquist ratios higher than 50 and signal-to-noise ratios. The analysis of the bedforms of the Parana River reveals that in most cases, a Gamma probability distribution (with a positive skewness) best describes the dimensionless wavelength and amplitude for both the lee and stoss sides of large dunes. For the case of the smaller superimposed dunes, the dimensionless wavelength shows a discrete behavior governed by the sampling frequency of the data, and the dimensionless amplitude better fits the Gamma probability distribution, again with a positive skewness...|$|R
40|$|A {{new type}} of second-order digital {{parametric}} equalizer is proposed whose frequency response matches closely that of its analog counterpart throughout the <b>Nyquist</b> <b>interval</b> and does not suffer from the prewarping effect of the bilinear transformation near the Nyquist frequency. Closed-form design equations and direct-form and lattice realizations are derived. 1...|$|E
30|$|In this section, {{we present}} five {{existing}} alternative LP models, and we illustrate how all these models attempt {{to compensate for}} the shortcomings of the conventional LP model, described in Section 3, when the input signal tonal components are concentrated in {{the lower half of the}} <b>Nyquist</b> <b>interval.</b> In the first three alternative LP models, namely, the constrained pole-zero LP (PZLP) model, the high-order LP (HOLP) model, and the pitch prediction (PLP) model, the influence of the input signal frequency distribution is decreased by using a model different from the conventional low-order all-pole model. In the last two alternative LP models, namely, the warped LP (WLP) model and the selective LP (SLP) model, the performance of the conventional low-order all-pole model is increased by first transforming the input signal such that its tonal components are spread in the entire <b>Nyquist</b> <b>interval.</b> As stated earlier, we will mainly focus on the alternative LP models, and not on how the model parameters can be estimated.|$|E
3000|$|..., the warping {{operation}} {{tends to}} spread out the tonal components in the observed signal over the entire <b>Nyquist</b> <b>interval.</b> From the conventional LP analysis in Section 3, it can hence be expected that applying a conventional, that is, low-order all-pole LP model to the warped signal will yield a better prediction than a conventional LP model of the original signal. The optimal prediction is obtained when the frequency transformation produces a uniform spreading of the tonal components in the <b>Nyquist</b> <b>interval.</b> For monophonic audio signals, this is never the case, since the bilinear frequency warping in (51)-(52) disturbs the harmonicity of the signal. For this class of signals, the frequency transformation of the selective LP model described in Section 4.5 appears to be better suited. However, for polyphonic audio signals, the above bilinear frequency warping may be a near-optimal mapping, since {{in this case the}} different fundamental frequencies are approximately related to each other according to the Bark scale (see also the simulation results in Section 5.3).|$|E
40|$|This work {{deals with}} the {{properties}} of a long slot array fed by an array of periodically located feeds spaced at a <b>Nyquist</b> <b>interval.</b> The study begins with the rigorous Green's function (GF) of a single long slot which was extended to an infinite array structure. After this, the input impedance properties of the array when fed by an array of δ sources are described by simple formulas, which explicitly show a theoretically unlimited bandwidth {{when there is no}} backplane...|$|E
40|$|We {{present a}} new proof of an Ergodic theorem for Wide-Sense Stationary Random Processes {{added with a}} new {{canonical}} sampling theorem of mine for finite time duration signals in the frequency domain (periodograms) which is free from the <b>Nyquist</b> <b>interval</b> restriction. We point out the usefulness of such theorem {{in the context of}} a model of random vibrations transmission (pressure-explosive fluctuations). Replacement due to inadvertent and wrong acknowledgmentsComment: 13 pages-In order to protest against that stupid and ruthless decision of arxiv moderation in not allow me to submit unplished articles in arxi...|$|E
40|$|This work {{deals with}} the {{properties}} of an array of long slots fed by periodically located feed elements spaced at <b>Nyquist</b> <b>interval.</b> The presentation is structured in two parts. This first part introduces the theoretical formulation {{of the problem and}} the input impedance properties of the array when fed by an array of 6 -sources. Analytical formulas show a theoretically unlimited bandwidth when there is no back plane. The second part presents the proof of concepts and test results of a prototype UHF long slot array in the next companion paper [1]...|$|E
3000|$|In this paper, we have {{analyzed}} {{the performance of}} the conventional LP model when applied to tonal audio signals, and illustrated how the quality of this model depends on the distribution of the signal tonal components in the <b>Nyquist</b> <b>interval.</b> It was shown that the conventional LP model, with a model order equal to two times the number of tonal components, and calculated by minimizing an LS criterion, produces a PEF that features a tradeoff between cancelling the tonal components and keeping the residual spectrum as flat as possible. This tradeoff occurs since the tonal components in an audio signal, sampled at [...]...|$|E
40|$|A 5 -bit 2 GS/s current-steering D/A {{converter}} for ultra-wideband (UWB) transceivers {{is presented}} in this paper. It {{is based on a}} full-binary weighted architecture and achieves better than 10 -bit static linearity without calibration. The DAC occupies 0. 5 mm × 0. 75 mm in a standard 90 nm CMOS technology. A spurious-free dynamic range (SFDR) of more than 30 dB has been measured over the complete <b>Nyquist</b> <b>interval</b> at sampling frequencies of 2 GS/s. The power consumption at a 2 GHz clock frequency for a near-Nyquist sinusoidal output signal equals only 12 mW. For UWB signals, which have about 500 MHz bandwidth, the DAC consumes even less than 8 mW. status: publishe...|$|E
40|$|We have {{systematically}} {{studied the}} optimal real-space sampling of atomic pair distribution data by comparing refinement results from oversampled and resampled data. Based on nickel and a complex perovskite system, we {{demonstrate that the}} optimal sampling is bounded by the <b>Nyquist</b> <b>interval</b> described by the Nyquist-Shannon sampling theorem. Near this sampling interval, the data points in the PDF are minimally correlated, which results in more reliable uncertainty prediction. Furthermore, refinements using sparsely sampled data may run many times faster than using oversampled data. This investigation establishes a theoretically sound limit {{on the amount of}} information contained in the PDF, which has ramifications towards how PDF data are modeled. Comment: 10 pages, 4 figure...|$|E
40|$|This is {{the final}} report of a three-year, Laboratory-Directed Research and Development (LDRD) project at the Los Alamos National Laboratory (LANL). The work funded by this grant {{addresses}} theoretical issues pertaining to potential applications of higher-order spectra (polyspectra) for communications and measurement. Although the sensitivity to additive Gaussian noise provided a motive for our interest in the bispectrum, the research done here focused {{on the ability of}} the bispectrum to see outside the <b>Nyquist</b> <b>interval.</b> The main contribution of this work is a new understanding of the everyday phenomenon of aliasing that is described below. The second contribution this work makes is significant, though limited, progress in understanding the joint realizability, conditions for spectra and bispectra. A third contribution, which arose in efforts to broaden our results on joint realizability, is the awareness that a very common inequality, used to normalize the bispectrum, is invalid. This result has considerable significance for the practical use of the bispectrum...|$|E
40|$|The {{properties}} of the Gabor and Morlet transforms are examined {{with respect to the}} Fourier analysis of discretely sampled data. Forward and inverse transform pairs based on a fixed window with uniform sampling of the frequency axis can satisfy numerically the energy and reconstruction theorems; however, transform pairs based on a variable window or nonuniform frequency sampling in general do not. Instead of selecting the shape of the window as some function of the central frequency, we propose constructing a single window with unit energy from an arbitrary set of windows that is applied over the entire frequency axis. By virtue of using a fixed window with uniform frequency sampling, such a transform satisfies the energy and reconstruction theorems. The shape of the window can be tailored to meet the requirements of the investigator in terms of time/frequency resolution. The algorithm extends naturally to the case of nonuniform signal sampling without modification beyond identification of the <b>Nyquist</b> <b>interval...</b>|$|E
40|$|Abstract only given, {{substantially}} as follows. The properties {{derived by}} G. Polya and E. C. Titchmarsh for band-limited deterministic signals are extended to bandlimited processes. It is {{shown that the}} number of zeros /b n/(/b T/) (complex and real) of the process with a magnitude <or=T is such that the limit /b n/(/b T/) / 2 /b T / for /b T/ rarr infinity equals 2 /b W/, where /b W/ is the signal bandwidth. It is also shown that these zeros determine the process in the mean-square-sense. Real zero signals (e. g. with no complex zeros) have a zero-crossing rate equal to 2 /b W/ and are defined in the mean-square-sense (except for a scale factor) by their zero-crossings. Special attention is given to the class of real-zero signals whose successive samples alternate in sign. This class of signals has one zero-crossing in each <b>Nyquist</b> <b>interval</b> with probability one; it is the basis for a communication system described in another paper. The characteristic function and correlation function of this type of signal are derived. Anglai...|$|E
30|$|While linear {{prediction}} (LP) {{has become}} immensely popular in speech modeling, {{it does not}} seem to provide a good approach for modeling audio signals. This is somewhat surprising, since a tonal signal consisting of a number of sinusoids can be perfectly predicted based on an (all-pole) LP model with a model order that is twice the number of sinusoids. We provide an explanation why this result cannot simply be extrapolated to LP of audio signals. If noise is taken into account in the tonal signal model, a low-order all-pole model appears to be only appropriate when the tonal components are uniformly distributed in the <b>Nyquist</b> <b>interval.</b> Based on this observation, different alternatives to the conventional LP model can be suggested. Either the model should be changed to a pole-zero, a high-order all-pole, or a pitch prediction model, or the conventional LP model should be preceded by an appropriate frequency transform, such as a frequency warping or downsampling. By comparing these alternative LP models to the conventional LP model in terms of frequency estimation accuracy, residual spectral flatness, and perceptual frequency resolution, we obtain several new and promising approaches to LP-based audio modeling.|$|E
40|$|This study explores four {{techniques}} for simulating analog filters by recursive digital filters. A uniform basis for comparing these methods {{is provided by}} requiring each digital filter to maintain the D. C. gain and order of the given analog filter. Three of these techniques are modified versions of existing methods found in the literature. The fourth is a proposed compensation scheme for the bilinear transformation which preserves the finite critical frequency locations of the original analog filter. For band-limited filters and for sampling intervals approaching zero, it is demonstrated that the D. C. gain requirement with the standard Z-transformation of the analog filter transfer function results in the so-called impulse-invariant method. The nature of the mappings induced by the bilinear transformation is investigated and some contours generated by this mapping are given. While under the most general conditions, none of these methods minimizes the mean-squared error at the sampling instants, it is shown that the frequency responses of these digital filters (in the <b>Nyquist</b> <b>interval)</b> and their unit-step responses are often acceptable approximations {{to those of the}} given analog filter and warrant their use as simulators. Some numerical examples are given which illustrate implementation of these filters. Roundoff accumulation errors are encountered and bounds which show the effect of filter order and sampling rate are discussed...|$|E
40|$|International audienceThe {{exploitation}} of Doppler radars for weather observations is strongly {{constrained by the}} well-known range-velocity dilemma. To overcome the range and velocity ambiguities, dual and triple staggered pulse-repetition time (PRT) techniques are commonly used in Doppler radar systems. Today, a triple-PRT (3 -PRT) scheme is operational in France. These techniques imply nonuniform sampling of the weather signal, inducing multiple replicas in the Doppler spectrum. The situation is particularly complicated for short-wavelength radars, where larger extension factors of the unambiguous <b>Nyquist</b> <b>interval</b> are needed. To overcome these difficulties, a novel technique called OptM-PRT is proposed. It mainly consists in optimizing the transmission scheme based on multiple pulse repetition time, so that the corresponding autocorrelation function is well filled. The Doppler spectrum is therefore reconstructed with much less ambiguities, from the computation of the autocorrelation function of radar signal and its Fourier transform. Considering both 3 -PRT and Opt 9 -PRT schemes, the magnitude and Doppler velocity of radar returns in rain are simulated for different spectral widths, with and without elimination of the spectral lines of ground clutter. When the ground clutter is filtered out, the 3 -PRT is found to better reproduce the Doppler velocity, whereas the Opt 9 -PRT better restitutes {{the magnitude of the}} signal. In the presence of noise, the Opt 9 -PRT scheme produces the best result for both the magnitude and velocity. The 3 - and Opt 9 -PRT techniques have been applied to the C-band Doppler radar operating in Bourges, France. The experimental results show that Opt 9 -PRT efficiently reconstructs the Doppler spectrum of rain echoes...|$|E

