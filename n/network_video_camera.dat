2|10000|Public
40|$|Abstract: The aim of {{the work}} was to develop {{telematics}} aplications {{to be used in}} an e-teaching context. A web based application has been developed, which links a programmable logic controller (PLC) with a computer for programming via Internet. The main requirement of our application is to provide a friendly user interface that allows PLC application developers to control a Flexible Manufacturing System (FMS) from anywhere in the world via a thin client that does not need the installation of any additional software on the user side. The user has possibility to see the environment (FMS) via images from a <b>network</b> <b>video</b> <b>camera...</b>|$|E
40|$|This paper {{discusses}} {{aspects of}} technological change and the path towards institutionalization of new innovations, {{with a focus on}} how users matter and are part of the co-construction of such change. An empirical example from the security industry is used to illustrate how two distinct phases in the analog-to-digital shift in video surveillance technology have had very different outcomes with respect to the pace and nature of user adoption. The notions of actor- networks and communities of practice are used to analyze the different user and industry reactions to the introduction of the digital video recorder (DVR) and the <b>network</b> <b>video</b> <b>camera</b> respectively. The analysis focus on how established industry socio-technical actor- networks encompassing users incumbent industry and technological artifacts can resist attempts of disruptive innovation technological change from powerful outside forces...|$|E
50|$|The {{album is}} a live {{recording}} of Deep Purple's infamous {{appearance at the}} California Jam Festival on 6 April 1974, which was televised by ABC-TV in prime time. It {{was one of the}} first to feature their third line-up, which included vocalist David Coverdale and bassist/vocalist Glenn Hughes. At the end of the show, guitarist Ritchie Blackmore first attacked one of the <b>network's</b> <b>video</b> <b>cameras</b> (which had been getting between him and the audience) with his guitar, and then had his amplifiers doused with gasoline and set on fire, which caused an explosion.|$|R
50|$|The HALO Program is a {{collaborative}} effort between Denver Police, community groups and local businesses. Established in 2006, the program utilizes <b>networked</b> <b>video</b> <b>cameras</b> to deter crime and enhance public safety through faster response to incidents. Monitored locations include high-traffic intersections, areas where drug activity and street crime are prevalent, and public facilities and parks. Cameras are also used to protect tourist sites, healthcare facilities and areas with homeland security importance. Mobile cameras are used to help manage crime hotspots. Police point to successes, including HALO's help in controlling major drug and street crime issues on notorious East Colfax Avenue.|$|R
40|$|In {{this paper}} we present an indoor {{micronavigation}} system for enclosed parking garages. It builds on car-to-infrastructure communication to provide layout information {{of the car}} park, the coordinates of the destination parking lot, as well as external positioning information to vehicles. In our approach we use customary <b>network</b> <b>video</b> <b>cameras</b> to detect and locate vehicles within the car park. Once a vehicle is detected, the system correlates {{the position of the}} vehicle to the car park layout and transmits this information to the appropriate vehicle to substitute the internal positioning system. With this information the vehicle is guided from the car park entrance to a destination parking lot...|$|R
5000|$|In 2005 {{the school}} {{establish}} a [...] "wireless bully button" [...] system which alerts teachers by SMS when students {{push the button}} and records incidents via a <b>network</b> of 20 <b>video</b> <b>cameras,</b> which {{we have been told}} do not actually work.|$|R
5000|$|Ayers Island is {{the site}} of the Striar Textile Mill, which closed in 1996. The land is {{polluted}} due to past manufacturing activity. The town of Orono took possession of the island in 1999. Later it was purchased by Ayers Island LLC, which announced plans to fit it with an experimental surveillance system consisting of [...] "a comprehensive <b>network</b> of <b>video</b> <b>cameras,</b> motion detectors and sensors" [...] monitored by computer.|$|R
40|$|With the {{proliferation}} of inexpensive cameras {{and the availability of}} high-speed wired and wireless networks, networks of distributed cameras are becoming an enabling technology for a broad range of interdisciplinary applications in domains such as public safety and security, manufacturing, transportation, and healthcare. Today’s live video processing systems on networks of distributed cameras, however, are designed for specific classes of applications. To provide a generic query processing platform for applications of distributed camera networks, we designed and implemented a new class of general purpose database management systems, the live video database management system (LVDBMS). We view <b>networked</b> <b>video</b> <b>cameras</b> as a special class of interconnected storage devices, and allow the user to formulate ad hoc queries expressed over real-time live video feeds. In this paper we introduce our system and present the live video data model, the query language, and the query processing and optimization technique. 1...|$|R
40|$|One {{important}} goal of surveillance systems is {{to collect information}} about the behavior and position of interested tar-gets in the sensing environment. Traditional video surveil-lance systems usually cannot provide complete information of interested targets since they are limited by single and fixed monitoring directions. Recently, surveillance systems com-bining wireless sensor <b>networks</b> with <b>video</b> <b>cameras</b> have be-come more and more popular. In this demonstration 1, we show a multi-view surveillance system consisting of several rotatable <b>video</b> <b>cameras</b> and objects. By implementing the proposed visual sensor selection algorithm, our system can provide more meaningful information from multiple rotat-able cameras. ...|$|R
40|$|This paper {{proposes a}} {{practical}} calibration solution for estimating the boresight and lever-arm {{parameters of the}} sensors mounted on a Mobile Mapping System (MMS). On our MMS devised for conducting the calibration experiment, three <b>network</b> <b>video</b> <b>cameras,</b> one mobile laser scanner, and one Global Navigation Satellite System (GNSS) /Inertial Navigation System (INS) were mounted. The geometric relationships between three sensors were solved by the proposed calibration, considering the GNSS/INS as one unit sensor. Our solution basically uses the point cloud generated by a 3 -dimensional (3 D) terrestrial laser scanner rather than using conventionally obtained 3 D ground control features. With the terrestrial laser scanner, accurate and precise reference data could be produced and the plane features corresponding with the sparse mobile laser scanning data could be determined with high precision. Furthermore, corresponding point features could be extracted from the dense terrestrial laser scanning data and the images captured by the <b>video</b> <b>cameras.</b> The parameters of the boresight and the lever-arm were calculated based on the least squares approach and the precision of the boresight and lever-arm could be achieved by 0. 1 degrees and 10 mm, respectively...|$|R
40|$|Realityflythrough is a telepresence/tele-reality {{system that}} {{works in the}} dynamic, uncalibrated environments {{typically}} associated with ubiquitous computing. By harnessing <b>networked</b> mobile <b>video</b> <b>cameras,</b> it allows a user to remotely and immersively explore a physical space. RealityFlythrough creates the illusion of complete live camera coverage in a physical environment. This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: (1) its support of the abstractions for infinite camera coverage, (2) its scalability, and (3) its robustness to changing user requirements. ...|$|R
40|$|As <b>networks</b> of <b>video</b> <b>cameras</b> are {{installed}} in many applications like security and surveillance, environmental monitoring, disaster response, and assisted living facilities, among others, image understanding in camera networks {{is becoming an}} important area of research and technology development. There are many challenges {{that need to be}} addressed in the process. Some of them are listed below: - Traditional computer vision challenges in tracking and recognition, robustness to pose, illumination, occlusion, clutter, recognition of objects, and activities; - Aggregating local information for wid...|$|R
40|$|<b>Network</b> <b>video</b> <b>cameras,</b> {{invented in}} the last decade or so, permit today pervasive, wide-area visual surveillance. However, due to the vast amounts of visual data that such cameras produce human-operator {{monitoring}} is not possible and automatic algorithms are needed. One monitoring task of particular interest is the detection of suspicious behavior, i. e., identification of individuals or objects whose behavior differs from behavior usually observed. Many methods based on object path analysis have been developed to date (motion detection followed by tracking and inferencing) but they are sensitive to motion detection and tracking errors and are also computationally complex. We propose a new surveillance method capable of abnormal behavior detection without explicit estimation of object paths. Our method is based on a simple model of video dynamics. We propose one practical implementation of this general model via temporal aggregation of motion detection labels. Our method requires little processing power and memory, is robust to motion segmentation errors, and general enough to monitor humans, cars or any other moving objects in uncluttered as well as highly-cluttered scenes. Furthermore, on account of its simplicity, our method can provide performance guarantees. It is also robust in harsh environments (jittery cameras, rain/snow/fog) ...|$|R
40|$|We {{are rapidly}} {{moving toward a}} world where {{personal}} <b>networked</b> <b>video</b> <b>cameras</b> are ubiquitous. Already, camera-equipped cell phones are becoming commonplace. Imagine being able to tap into all of these live video feeds to remotely explore the world in real-time. We introduce RealityFlythrough, a telepresence system that makes this vision possible. By situating live 2 d video feeds in a 3 d model of the world, RealityFlythrough allows any space to be explored remotely. No special cameras, tripods, rigs, scaffolding, or lighting is required to create the model, and no lengthy preprocessing of images is necessary. Rather than try to achieve photorealism at every point in space, we instead focus on providing the user {{with a sense of}} how the video streams relate to one another spatially. By providing cues in the form of dynamic transitions, we can approximate photorealistic telepresence while harnessing cameras “in the wild. ” This paper describes the RealityFlythrough system, and reports on a live flythrough experience. We find that telepresence can work in the wild using only commodity hardware and off-the-shelf software, and that imperfect transitions are sensible and provide a compelling user experience. ...|$|R
40|$|Realityflythrough is a telepresence/tele-reality {{system that}} {{works in the}} dynamic, uncalibrated environments {{typically}} associated with ubiquitous computing. By opportunistically harnessing <b>networked</b> mobile <b>video</b> <b>cameras,</b> it allows a user to remotely and immersively explore a physical space. This paper describes {{the architecture of the}} system, motivated by the real-time and dynamic requirements imposed by one application domain: SWAT team command and control support. RealityFlythrough is analogous to an operating system in that it provides abstractions to the user that hide inherent limitations in the underlying system. Just as an operating system provides the illusion of infinite processors and infinite memory, RealityFlythrough provides the illusion of complete live camera coverage in a physical environment. ...|$|R
40|$|We {{present an}} {{embedded}} software application for the real-time estimation of building occupancy using a <b>network</b> of <b>video</b> <b>cameras.</b> We analyze {{a series of}} alternative decompositions of the main application tasks and profile each of them by running the corresponding embedded software on three different processors. Based on the profiling measures, we build various alternative embedded platforms by combining different embedded processors, memory modules and network interfaces. In particular, we consider the choice of two possible network technologies: ARCnet and Ethernet. After deriving an analytical model of the network costs, we use it to complete {{an exploration of the}} design space as we scale the number of <b>video</b> <b>cameras</b> in an hypothetical building. We compare our results with those obtained for two real buildings of different characteristics. We conclude discussing the results of our case study in the broader context of other camera-network applications...|$|R
40|$|Realityflythrough is a telepresence/tele-reality {{system that}} {{works in the}} dynamic, uncalibrated environments {{typically}} associated with ubiquitous computing. By opportunistically harnessing <b>networked</b> mobile <b>video</b> <b>cameras,</b> it allows a user to remotely and immersively explore a physical space. Live 2 d video feeds are situated in a 3 d representation of the world. Rather than try to achieve photorealism at every point in space, we instead focus on providing the user {{with a sense of}} how the video streams relate to one another spatially. By providing cues in the form of dynamic transitions, we can approximate photorealistic telepresence while harnessing cameras “in the wild. ” This paper shows that transitions between situated 2 d images are sensible and provide a compelling telepresence experience...|$|R
40|$|Given an uncalibrated <b>network</b> of <b>video</b> <b>cameras,</b> we are {{tasked with}} the problem of {{building}} a 3 D model of every person's face as they move within the network. The process of doing so requires overcoming many challenges including person detection, tracking, cross-camera correspondence (how to determine if a person in one camera is the same person in another), and the final 3 D model reconstruction from multiple views in real-time or at near real-time speeds (efficient modeling and data fusion from multiple hardware sources). Towards this goal, a wireless camera network was designed and built from the ground up, a new tracking algorithm and a cross-camera human signature method was developed, and face modeling using multiple cameras in a real-world setting was performed...|$|R
50|$|Deep Purple's {{performance}} {{was one of}} the first with their third line-up, which included the vocalist David Coverdale and the vocalist/bassist Glenn Hughes. Deep Purple was given the choice of when to go on stage, and chose to go on during sunset, thus pushing Emerson, Lake & Palmer to the last performance. Assuming that, as with all festivals, the show would run late anyway, they stalled when the festival was actually ahead of schedule. Angry organizers tried to force the band to go on and then threatened to cancel their performance but a quick thinking announcer told the crowds that Deep Purple would be coming on. The band made everyone wait nearly an hour until near dusk before they went on stage. In spite of this, the show did not end up running late. At the end of the show, guitarist Ritchie Blackmore threw a number of guitars, amplifier and speaker cabinets out into the audience and attacked one of the <b>network's</b> <b>video</b> <b>cameras</b> (which had been getting between him and the audience) with a guitar. Later on, a mishap with a pyrotechnic effect caused one of Blackmore's amplifiers to explode, which briefly set the stage on fire. The group had to leave the concert by helicopter to avoid a possibly ugly confrontation with furious fire marshals and ABC-TV executives (and potential arrest for the pyrotechnics). The damage to the camera was estimated to be $10,000, later settled by the managers.|$|R
40|$|This paper {{describes}} a surveillance system {{that uses a}} network of sensors of different kind for localizing and tracking people in an office environment. The sensor <b>network</b> consists of <b>video</b> <b>cameras,</b> infrared tag readers, a fingerprint reader and a PTZ camera. The system implements a Bayesian framework that uses noisy, but redundant data from multiple sensor streams and incorporates it with the contextual and domain knowledge. The paper {{describes a}}pproaches to camera specification, dynamic background modeling, object modeling and probabilistic inference. The preliminary experimental results are presented and discussed. 1...|$|R
40|$|Abstract For troop and {{military}} installation protection, modern computer vision methods must be harnessed to enable {{a comprehensive approach}} to con-textual awareness. In this chapter we present a collection of intelligent video technologies currently under development at the General Electric Global Research Center, which {{can be applied to}} this challenging prob-lem. These technologies include: aerial analysis for object detection and tracking, site-wide tracking from <b>networks</b> of fixed <b>video</b> <b>cameras,</b> person detection from moving platforms, biometrics at a distance and facial analysis for the purposes of inferring intent. We hypothesize that a robust approach to troop protection will require the synthesis of all of these technologies into a comprehensive system of systems...|$|R
40|$|Real-time {{three-dimensional}} {{tracking of}} people is an important requirement for {{a growing number of}} applications. In this paper we describe two trackers; both of them use a <b>network</b> of <b>video</b> <b>cameras</b> for person tracking. These trackers are called a rectilinear video array tracker (R-VAT) and an omnidirectional video array tracker (O-VAT), indicating the two different ways of video capture. The specific objectives of this paper are twofold: (i) to present a systematic comparison of these two trackers using an extensive series of experiments conducted in an `intelligent' room; (ii) to develop a real-time system for tracking the head and face of a person, {{as an extension of the}} O-VAT approach. The comparative research indicates that O-VAT is more robust to the number of people, less complex and runs faster, needs manual camera calibration, and the integrated omnidirectional <b>video</b> <b>network</b> has better reconfigurability. The person head and face tracker study shows that such a system can serve as a most effective input stage for face recognition and facial expression analysis modules...|$|R
30|$|In {{the article}} {{entitled}} ‘Temporal Scalable Mobile Video Communications {{based on an}} Improved WZ-to-SVC transcoder’ by Alberto Corrales-García, José Luis Martínez, Gerardo Fernández-Escribano, and Francisco José Quiles, the authors present a WynerZiv (WZ) coding approach to SVC transcoding. Applications such as low-power sensor <b>networks,</b> <b>video</b> surveillance <b>cameras,</b> or mobile communications present a different framework in which low-cost senders transmit video bit streams to a central receiver. In order to manage this kind of application efficiently, WZ coding proposes a solution in {{which most of the}} complexity is moved from the encoder to the decoder. Despite the advantages of the video transcoding framework, the transcoder accumulates high complexity and must be reduced in order to avoid excessive delays in communication. Thus, in order to reduce that delay, the information generated during the WZ stage is reused during the SVC stage. Consequently, the time taken by the transcoding is reduced by around 77.77 %, with a negligible rate-distortion penalty.|$|R
40|$|Camera {{assignment}} and hand-off {{are some of}} the key image processing problems in a <b>video</b> <b>network.</b> In this paper, we propose a new approach for camera {{assignment and}} hand-off in a <b>video</b> <b>network.</b> The <b>camera</b> assignment problem is modeled as a weakly acyclic game which allows the design of utility functions based on different user-supplied criteria. A theoretical and experimental comparison of the proposed approach with the two recently proposed approaches based on potential game theory and constraint satisfaction problem is provided. This comparison shows that the proposed approach is theoretically more general and computationally more efficient than the other approaches...|$|R
40|$|In recent years, with {{emerging}} {{applications such}} as wireless video surveillance, multimedia sensor <b>networks,</b> disposable <b>video</b> <b>cameras,</b> medical applications and mobile camera phones, the traditional video coding architecture is being challenged. For these emerging applications, Distributed Video Coding (DVC) seems {{to be able to}} offer efficient and low-complexity encoding video compression. In this paper, we present a novel transform domain distributed video coding algorithm based on Turbo Trellis Coded Modulation (TTCM). As in the conventional turbo based Wyner-Ziv encoder, transform quantized coefficients are applied to the TTCM encoder and parity bits are generated from both constituent encoders. However, TTCM symbols are not generated at the encoder since they are not sent to the decoder. Parity bits produced by the TTCM encoder are stored in a buffer and transmitted to the decoder upon request. TTCM symbols are generated at the decoder and these symbols are passed to the TTCM decoder for demodulation. Experimental results show that significant rate-distortion (RD) gains compared to the state-of-the-art results available in the literature can be obtained...|$|R
40|$|<b>Networks</b> of {{multiple}} <b>video</b> <b>cameras</b> are being deployed in several scenarios like surveillance, traffic enforcement, human motion anal-ysis and sports telecast. The unmanageable {{size of the}} raw video sequences necessitates the use of compression schemes to efficiently encode these videos. Traditional video compression schemes ac-count for spatial and temporal redundancy in a video sequence. In this paper, we present a compression technique that also leverages the information redundancy between video sequences across differ-ent cameras with overlapping fields of view. An algorithm based on homography for efficient compression {{of multiple}} video sequences is presented that performs significantly better than current schemes. We also derive an efficient distributed version of the algorithm that can be implemented {{on a large scale}} network of cameras. This dis-tributed algorithm minimizes both the communication costs and the compression costs simultaneously...|$|R
40|$|We {{present a}} novel stochastic, {{adaptive}} strategy for tracking multiple {{people in a}} large <b>network</b> of <b>video</b> <b>cameras.</b> Similarities between features (appearance and biometrics) observed at different cameras are continuously adapted and the stochastically optimal path for each person computed. The following are the major contributions of the proposed approach. First, we consider situations where the feature similarities are uncertain and treat them as random variables. We show how the distributions of these random variables can be learned and how to compute the tracks in a stochastically optimal manner. Second, we consider the possibility of long-term interdependence of the features over space and time. This allows us to adaptively evolve the feature correspondences by observing the system performance over a time window, and correct for errors in the similarity computations. Third, we show that the above two conditions can be addressed by treating the issue of tracking in a camera network as an optimization problem in a stochastic adaptive system. We show results on data collected by a large camera network. The proposed approach is particularly suitable for distributed processing over the entire network...|$|R
40|$|Abstract—We {{propose a}} methodology, and its {{embodiment}} into a design flow, to realize execution platforms for high-performance building applications. This {{is an example}} of a class of cyberphysical systems where a network of sensors, controllers, and actuators must be designed under physical spatial constraints to implement various types of signal processing and control tasks. In our approach, the applications are specified using the dataflow model of computation while the building dictates the physical constraints, including the position of sensors and actuators. We present a rigorous formulation of the design-space exploration problem and we propose to solve it by progressing through a sequence of refinement steps from specification to detailed implementation. Two key steps are the synthesis of the computation platform and the synthesis of the communication network. Combined, they allow us to automatically derive an optimal implementation through the selection and composition of processing and networking elements from given technology libraries. We demonstrate the applicability of our approach by comparing it to the manual design of a given case study: the real-time estimation of building occupancy using a <b>network</b> of <b>video</b> <b>cameras.</b> Index Terms—Cyber–Physical Systems; CAD; Sythesis; I...|$|R
40|$|<b>Networks</b> of <b>video</b> <b>cameras,</b> {{meteorological}} sensors, and ancillary {{electronic equipment}} are under development in collaboration among NASA Ames Research Center, the Federal Aviation Administration (FAA), and the National Oceanic Atmospheric Administration (NOAA). These networks {{are to be}} established at and near airports to provide real-time information on local weather conditions that affect aircraft approaches and landings. The prototype network is an airport-approach-zone camera system (AAZCS), which has been deployed at San Francisco International Airport (SFO) and San Carlos Airport (SQL). The AAZCS includes remotely controlled color <b>video</b> <b>cameras</b> located on top of SFO and SQL air-traffic control towers. The cameras are controlled by the NOAA Center Weather Service Unit located at the Oakland Air Route Traffic Control Center and are accessible via a secure Web site. The AAZCS cameras can be zoomed and can be panned and tilted to cover a field of view 220 wide. The NOAA observer can see the sky condition as it is changing, thereby making possible a real-time evaluation of the conditions along the approach zones of SFO and SQL. The next-generation network, denoted a remote tower sensor system (RTSS), will soon be deployed at the Half Moon Bay Airport and a version of it will eventually be deployed at Los Angeles International Airport. In addition to remote control of <b>video</b> <b>cameras</b> via secure Web links, the RTSS offers realtime weather observations, remote sensing, portability, and a capability for deployment at remote and uninhabited sites. The RTSS can be used at airports that lack control towers, {{as well as at}} major airport hubs, to provide synthetic augmentation of vision for both local and remote operations under what would otherwise be conditions of low or even zero visibility...|$|R
40|$|We {{are rapidly}} {{moving toward a}} world of {{ubiquitous}} <b>video</b> where personal <b>networked</b> <b>video</b> <b>cameras</b> are everywhere. Already, camera-equipped cell phones are becoming commonplace. Imagine being able to tap into all of these live video feeds to remotely explore the world in real- time. We introduce RealityFlythrough, a telepresence system that makes this vision possible. By situating live 2 d video feeds in a 3 d model of the world, RealityFlythrough allows any space to be explored remotely. No special cameras, tripods, rigs, scaffolding, or lighting is required to create the model, and no lengthy preprocessing of images is necessary. Rather than try to achieve photorealism at every point in space, we instead focus on providing the user {{with a sense of}} how the video streams relate to one another spatially. By providing cues in the form of dynamic transitions and by stitching together live and archived views of the scene, we can approximate photorealistic telepresence while harnessing cameras "in the wild. " This dissertation describes how to construct a system like RealityFlythrough and explores the issues with deploying such a system in a real-world setting where limits in wireless bandwidth place constraints on the quality and number of mobile video feeds. The primary contribution of this dissertation is a demonstration that with the appropriate division of labor between the human brain and the computer, some computationally intractable problems can be solved. In particular, this dissertation demonstrates that the computationally intensive task of stitching multiple video feeds into a cohesive whole can be avoided by having the computer perform a relatively simple rough stitch that is accurate enough to allow the human brain to complete the process. This dissertation also makes the following contributions to the field of telepresence: 1) A functioning system is presented and studied in a series of user experiments, 2) a robust system architecture is described that has successfully adapted to a series of unanticipated changes, 3) a novel visualization technique is presented that reduces the computational requirements that have so far posed a barrier to live, real-time image synthesis in real-world environments, and 4) this same visualization technique is shown to reduce the negative effects of low-frame-rate vide...|$|R
40|$|Lockheed Martin and the University of Central Florida (UCF) are jointly {{investigating}} {{the use of}} a <b>network</b> of COTS <b>video</b> <b>cameras</b> and computers for a variety of security and surveillance operations. The detection and tracking of humans as well as vehicles is of interest. The three main novel aspects of the work presented in this paper are i) the integration of automatic target detection and recognition techniques with tracking ii) the handover and seamless tracking of objects across a network, and iii) the development of real-time communication and messaging protocols using COTS networking components. The approach leverages the previously developed KNIGHT human detection and tracking system developed at UCF, and Lockheed Martin's automatic target detection and recognition (ATD/R) algorithms. The work presented in this paper builds on these capabilities for surveillance using stationary sensors, with the goal of subsequently addressing the problem of moving platforms...|$|R
30|$|Traditionally, video communications, such as {{television}} broadcasting, {{have been based}} on a down-link model, where the information is encoded once and then transmitted to many terminal devices. These applications are supported by traditional video codecs, such as those adopted by all MPEG and ITU-T video coding standards[1], which are characterized by architectures where most of the complexity is present in the encoding part. However, in the last few years, with the ever-increasing development of mobile devices and wireless networks, a growing number of emerging multimedia applications have required an up-link model. These end-user devices are able to capture, record, encode, and transmit video with low-constraint requirements. Applications, such as low-power sensor <b>networks,</b> <b>video</b> surveillance <b>cameras,</b> or mobile communications, present a different framework in which low-cost senders transmit video bitstreams to a central receiver. In order to manage this kind of applications efficiently, distributed video coding[2], and specially Wyner–Ziv coding (WZ)[3], proposed a solution in which most of the complexity is moved from the encoder to the decoder. In particular, the WZ codec used in this study is based on the architecture proposed by VISNET-II[4], which is an improved version of the architecture proposed by the project DISCOVER[5]. More detail about WZ coding can be found in[3].|$|R
30|$|The above design, {{which implies}} complex {{encoders}} and lightweight decoders, {{is well suited}} for broadcasting-like applications, where a single sender is transmitting data to many receivers. In contrast to this downstream model, {{a growing number of}} emerging applications, such as low-power sensor <b>networks,</b> wireless <b>video</b> surveillance <b>cameras,</b> and mobile communication devices, are rather relying on an upstream model. In this case, many clients, often mobile, low-power, and with limited computing resources, are transmitting data to a central server. In the context of this upstream model, it is usually advantageous to have lightweight encoding with high compression efficiency and resilience to transmission errors. Thanks to the improved performance and reducing cost of cameras, another trend is towards multiview systems where a dense network of cameras captures many correlated views of the same scene.|$|R
40|$|Distributed Video Coding (DVC) {{is a new}} coding {{paradigm}} for video compression, based on Slepian- Wolf (lossless coding) and Wyner-Ziv (lossy coding) information theoretic results. DVC is useful for emerging applications such as wireless <b>video</b> <b>cameras,</b> wireless low-power surveillance <b>networks</b> and disposable <b>video</b> <b>cameras</b> for medical applications etc. The primary objective of DVC is low-complexity video encoding, where bulk of computation is shifted to the decoder, as opposed to low-complexity decoder in conventional video compression standards such as H. 264 and MPEG etc. There are couple of early architectures and implementations of DVC from Stanford University[2][3] in 2002, Berkeley University PRISM (Power-efficient, Robust, hIgh-compression, Syndrome-based Multimedia coding) [4][5] in 2002 and European project DISCOVER (DIStributed COding for Video SERvices) [6] in 2007. Primarily {{there are two types}} of DVC techniques namely pixel domain and transform domain based. Transform domain design will have better rate-distortion (RD) performance as it exploits spatial correlation between neighbouring samples and compacts the block energy into as few transform coefficients as possible (aka energy compaction). In this paper, architecture, implementation details and "C" model results of our transform domain DVC are presented. Comment: Signal & Image Processing : An International Journal(SIPIJ) Vol. 2, No. 1, March 201...|$|R
40|$|Recent {{advances}} in technology have allowed for Small Unmanned Aerial Vehicles (SUAVs) to employ miniaturized smart payloads such as gimbaled cameras, deployable mechanisms, and <b>network</b> sensors. Gimbaled <b>video</b> <b>camera</b> systems, designed at NPS, use two servo actuators to command line of sight orientation via serial controller while tracking a target and is termed Visual Based Target Tracking (VBTT). Several Tactical Network Topology (TNT) experiments have shown high value of this new payload but also revealed inherent delays that exist between command and actuation of the pan-tilt servo actuators controlling the camera. Preliminary analysis shows that these delays are due to a communication lag between the ground control station and the onboard serial controller, a data processing delay within that controller, and the mechanical delays of the gimbal. This thesis applies system identification techniques to the servo controller system and considers {{the implementation of a}} Smith Predictor into the camera control algorithm {{in order to reduce the}} overall effect of the lag on the system performance...|$|R
40|$|Abstract—This paper {{introduces}} mobile camera robots with camera rotation capabilities. The motivation behind mobile camera robots {{is that as}} they rotate their camera {{their real}} sensing range moves from a FoV coverage to a disk coverage, therefore allowing neighboring nodes to decrease their activity level, thus their energy consumption. As a sensing node’s activity {{is based on a}} criticality or risk approach, we proposed 2 interaction behaviors between fixed image sensors and mobile camera robots to dynamically adapt the activity level without decrease the surveillance quality. The performance of the interaction models is evaluated through simulation. The results show that mobile camera robots can successfully help to increase the network lifetime but, depending on the number of deployed mobile camera robots, care must be taken with the interaction behaviors to not decrease the detection quality. Keywords-Sensor <b>networks,</b> <b>video</b> surveillance, pannable <b>camera,</b> mission-critical applications I...|$|R
