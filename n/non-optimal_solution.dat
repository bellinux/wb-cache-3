39|67|Public
25|$|In {{optimization}} problems, heuristic algorithms {{can be used}} to find {{a solution}} close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting {{closer and closer to the}} optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the <b>non-optimal</b> <b>solution</b> is known, the algorithm is further categorized as an approximation algorithm.|$|E
2500|$|For example, in {{the coin}} change problem {{of finding the}} minimum number of coins of given {{denominations}} {{needed to make a}} given amount, a dynamic programming algorithm would find an optimal solution for each amount by first finding an optimal solution for each smaller amount and then using these solutions to construct an optimal solution for the larger amount. In contrast, a greedy algorithm might treat the solution as a sequence of coins, starting from the given amount and at each step subtracting the largest possible coin denomination that is less than the current remaining amount. If the coin denominations are 1,4,5,15,20 and the given amount is 23, this greedy algorithm gives a <b>non-optimal</b> <b>solution</b> of 20+1+1+1, while the optimal solution is 15+4+4.|$|E
50|$|H_MCOP was {{designed}} to offer a (<b>non-optimal)</b> <b>solution</b> to the RSP and MCP problems which are otherwise NP-complete for an optimal solution algorithm. H_MCOP defining feature is that it uses heuristics to short-cut some of the complexity.|$|E
40|$|The biases of {{individual}} algorithms for non-parametric document clustering {{can lead to}} <b>non-optimal</b> <b>solutions.</b> Ensemble clustering methods may overcome this limitation, but have not been applied to document collections. This paper presents a comparison of strategies for non-parametric document ensemble clustering. Peer ReviewedPostprint (published version...|$|R
40|$|The {{scope of}} this work covers a real case of {{elective}} surgery planning in a Lisbon hospital. The aim is to employ more efficiently the resources installed in the surgical suite of the hospital in question besides improving the functioning of its surgical service. Such a planning sets out to schedule elective surgeries from the waiting list on a weekly time horizon {{with the objective of}} maximizing the use of the surgical suite. For this purpose, the authors develop an integer linear programming model. The model is tested using real data obtained from the hospital’s record. The <b>non-optimal</b> <b>solutions</b> are further improved by developing a custom-made, simple and efficient improvement heuristic. Application of this heuristic effectively improves almost all <b>non-optimal</b> <b>solutions.</b> The results are analyzed and compared with the actual performance of the surgical suite. This analysis reveals that the solutions obtained using this approach comply with the conditions imposed by the hospital and improve the use of the surgical suite. It also shows that in this case study the plans obtained from the proposed approach may be implemented in real life...|$|R
40|$|Abstract. This paper {{summarizes}} {{our recent}} development of algorithms that construct feasible trajectories for problems that involve both differential constraints (typically {{in the form}} of an underactuated nonlinear system), and global constraints (typically arising from robot collisions). Dynamic programming approaches are described that produce approximately-optimal solutions for low-dimensional problems. Rapidly-exploring Random Tree (RRT) approaches are described that can find feasible, <b>non-optimal</b> <b>solutions</b> for higher-dimensional problems. Several key issues for future research are discussed. ...|$|R
50|$|In {{optimization}} problems, heuristic algorithms {{can be used}} to find {{a solution}} close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting {{closer and closer to the}} optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the <b>non-optimal</b> <b>solution</b> is known, the algorithm is further categorized as an approximation algorithm.|$|E
5000|$|For example, in {{the coin}} change problem {{of finding the}} minimum number of coins of given {{denominations}} {{needed to make a}} given amount, a dynamic programming algorithm would find an optimal solution for each amount by first finding an optimal solution for each smaller amount and then using these solutions to construct an optimal solution for the larger amount. In contrast, a greedy algorithm might treat the solution as a sequence of coins, starting from the given amount and at each step subtracting the largest possible coin denomination that is less than the current remaining amount. If the coin denominations are 1,4,5,15,20 and the given amount is 23, this greedy algorithm gives a <b>non-optimal</b> <b>solution</b> of 20+1+1+1, while the optimal solution is 15+4+4.|$|E
50|$|Despite {{the fact}} that the bin packing problem has an NP-hard {{computational}} complexity, optimal solutions to very large instances of the problem can be produced with sophisticated algorithms. In addition, many heuristics have been developed: for example, the first fit algorithm provides a fast but often <b>non-optimal</b> <b>solution,</b> involving placing each item into the first bin in which it will fit. It requires Θ(n log n) time, where n is the number of elements to be packed. The algorithm can be made much more effective by first sorting the list of elements into decreasing order (sometimes known as the first-fit decreasing algorithm), although this still does not guarantee an optimal solution, and for longer lists may increase the running time of the algorithm. It is known, however, that there always exists at least one ordering of items that allows first-fit to produce an optimal solution.|$|E
40|$|In {{previous}} work the authors consider the dynamic assignment problem, which involves solving sequences of assignment problems {{over time in}} the presence of uncertain information about the future. The algorithm proposed by the authors provides generally high-quality but <b>non-optimal</b> <b>solutions.</b> In this work, though, the authors prove that if the optimal solution to a dynamic assign-ment problem in one of two problem classes is unique, then the optimal solution is a fixed point under the algorithm...|$|R
40|$|Abstract—This paper {{presents}} {{a new version}} of Tabu Search (TS) based on Cuckoo Search (CS) called (Tabu-Cuckoo Search TCS) to reduce the effect of the TS problems. The proposed algorithm provides a more diversity to candidate solutions of TS. Two case studies have been solved using the proposed algorithm, 4 -Color Map and Traveling Salesman Problem. The proposed algorithm gives a good result compare with the original, the iteration numbers are less and the local minimum or <b>non-optimal</b> <b>solutions</b> are less...|$|R
40|$|Although several {{researchers}} have integrated methods for reinforcement learning (RL) with case-based reasoning (CBR) to model continuous action spaces, existing integrations typically employ discrete approximations of these models. This limits {{the set of}} actions that can be modeled, and may lead to <b>non-optimal</b> <b>solutions.</b> We introduce the Continuous Action and State Space Learner (CASSL), an integrated RL/CBR algorithm that uses continuous models directly. Our empirical study shows that CASSL significantly outperforms two baseline approaches for selecting actions on a task from a real-time strategy gaming environment. 1...|$|R
3000|$|Note {{that the}} {{equation}} in (18) gives for q̅(θ [...]) [...] a solution that, due to (15)–(16), violates constraint (10) on [θ 1, θ 2]. Hence first order conditions alone would lead {{in this case}} to a <b>non-optimal</b> <b>solution</b> to Problem  2.|$|E
40|$|How {{does the}} {{knowledge}} of experts affect their behaviour in situations that require unusual methods of dealing? One possibility, loosely originating in research on creativity and skill acquisition, is that an increase in expertise can lead to inflexibility of thought due to automation of procedures. Yet another possibility, based on expertise research, is that experts ’ knowledge leads to flexibility of thought. We tested these two possibilities {{in a series of}} experiments using the Einstellung (set) effect paradigm. Chess players tried to solve problems that had both a familiar but <b>non-optimal</b> <b>solution</b> and a better but less familiar one. The more familiar solution induced the Einstellung (set) effect even in experts, preventing them from finding the optimal solution. The presence of the <b>non-optimal</b> <b>solution</b> reduced experts ' problem solving ability was reduced to about that of players three standard deviations lower in skill level by the presence of the <b>non-optimal</b> <b>solution.</b> Inflexibility of thought induced by prior knowledge (i. e., the blocking effect of the familiar solution) was shown by experts but the more expert they were, the less prone they were to the effect. Inflexibility of experts is both reality and myth. But the greater the level of expertise, the more of a myth it becomes...|$|E
30|$|Best Fit Decreasing (BFD): Like FFD, BFD also sorts {{items in}} non-increasing order. It then chooses a bin such that minimum empty space {{will be left}} after the item is packed. In {{most of the cases}} BFD could find an optimal {{solution}} while FFD gives a <b>non-optimal</b> <b>solution</b> as reported in [20].|$|E
40|$|Abstract. This paper {{studies the}} effects of the {{substrate}} thermal gradients on the buffer insertion techniques. Using a non-uniform temperaturedependent distributed RC interconnect delay model, the buffer insertion problem is analyzed and design guidelines are provided to ensure the nearoptimality of the signal performance {{in the presence of the}} thermal gradients. In addition, the effect of temperature-dependent driver resistance on the buffer insertion is studied. Experimental results show that neglecting thermal gradients in the substrate and the interconnect lines can result in <b>non-optimal</b> <b>solutions</b> when using standard buffer insertion techniques and that these effects intensify with technology scaling. ...|$|R
40|$|Abstract. We {{present the}} first optimally resilient, bounded, wait-free {{implementation}} of a distributed atomic register, tolerating Byzantine readers and (up to one-third of) Byzantine servers, {{without the use of}} unproven cryptographic primitives or requiring communication among servers. Unlike previous (<b>non-optimal)</b> <b>solutions,</b> the sizes of messages sent to writers depend only on the actual number of active readers and not on the total number of readers in the system. With a novel use of secret sharing techniques combined with write back throttling we present the first solution to tolerate Byzantine readers information theoretically, without the use of cryptographic techniques based on unproven numbertheoretic assumptions. ...|$|R
40|$|This paper {{studies the}} effects of the {{substrate}} thermal gradients on the buffer insertion techniques. Using a non-uniform temperaturedependent distributed RC interconnect delay model, the buffer insertion problem is analyzed and design guidelines are provided to ensure the nearoptimality of the signal performance {{in the presence of the}} thermal gradients. In addition, the effect of temperature-dependent driver resistance on the buffer insertion is studied. Experimental results show that neglecting thermal gradients in the substrate and the interconnect lines can result in <b>non-optimal</b> <b>solutions</b> when using standard buffer insertion techniques and that these effects intensify with technology scaling. ...|$|R
40|$|In {{this paper}} a new {{calibration}} scheme for recovering Euclidian camera parameters from their affine of projective primitives is presented. It {{is based on}} a contraction mapping implying that the obtained solution is unique, i. e. no local minimas threaten to yield a <b>non-optimal</b> <b>solution.</b> The approach unifies Euclidian calibration from affine and projective configurations and fewer cameras (m >= 2) need to be available than in traditional schemes. The algorithm is validated on synthetic and real data...|$|E
3000|$|..., coded ALOHA has {{a higher}} packet {{injection}} rate from outer nodes to the center node than dose ALOHA. This rate results in more queued packets Enc[m] at the center node, a higher network coding opportunity, and increases the throughput of coded ALOHA accordingly. However, the packet delay {{is dominated by the}} queueing delay, and becomes larger than that of ALOHA. This is the trade-off between throughput and delay in coded ALOHA for the optimal throughput point. In the following section, we show that this trade-off property is not valid for <b>non-optimal</b> <b>solution</b> cases.|$|E
40|$|In {{the present}} work I study the {{attributes}} of permutation puzzles {{and try to find}} the algorithms usable for solving these puzzles. The task of this project is to implement the algorithm for <b>non-optimal</b> <b>solution</b> of permutation puzzles by decomposition to sub problems and invent a suitable form of puzzle definition. The result of this project is a program with graphic interface, which allows the user to create custom permutation puzzle. With this puzzle, the user {{will be able to do}} the predefined moves, make custom positions and search the result of the position...|$|E
30|$|Note {{that these}} information-centric {{approaches}} require dozens of information-related software functionalities. From the service-centric point of view, such functionalities {{are no longer}} {{different from any other}} service. On the other hand, the service-centric approaches require dozens of specific information, such as descriptors, identifiers, names, contracts, goals, etc. It is clear that both approaches are complementary. Thus, what could we expect from the current panorama of research? More synergy among them! However, this is not the case. Apparently, there is very little interaction between the two proposals, leading to establishment of <b>non-optimal</b> <b>solutions.</b> Every side is reinventing the wheel when covering aspects from the other side.|$|R
40|$|Determining {{the optimal}} {{size of the}} supply base has haunted {{managers}} for years. A small supply base {{gives rise to the}} risk of supply disruption, whereas a large supply base increases the fixed cost. In this paper, we consider the risks of supply disruption due to occurrence of super, semi-super, and unique events in order to formulate a model to determine the optimal size of supply base. We depict the model in a decision tree-like structure and forward a tabular method of solution that obviates the need avoids evaluation of a majority of <b>non-optimal</b> <b>solutions</b> and thus overcomes the problem of dimensionality. Supply base Supply risk Supply disruption...|$|R
40|$|We {{present the}} first optimally resilient, bounded, wait-free {{implementation}} of a replicated register providing atomic semantics in {{a system in which}} readers can be Byzantine, up to f servers (n ≥ (3 f + 1)) are subject to Byzantine failures and servers do not communicate with each other. Unlike previous (<b>non-optimal)</b> <b>solutions,</b> the sizes of messages sent to writers depend only on the actual number of active readers and not on the total number of readers in the system. With a novel use of secret sharing techniques combined with write back throttling we present the first solution to tolerate Byzantine readers, information theoretically, without the use of cryptographic techniques based on unproven number-theoretic assumptions. 1...|$|R
40|$|Sequential pattern mining is {{the process}} of finding the {{relationships}} between occurrences of sequential events, to find if there exists any specific order of the occurrences. The extraction of sequential pattern is not polynomial in time of execution. The algorithms for performing sequential pattern mining can assure optimum solutions but they do not take into consideration the time taken to reach such solutions. In this paper we propose a new algorithm based on genetic concepts which gives, may be a <b>non-optimal</b> <b>solution</b> but in a reasonable time (polynomial) of execution...|$|E
40|$|The {{morphological}} {{design of}} Discrete-Time Cellular Neural Networks (DTCNNs) {{has been presented}} in a companion paper [1]. DTCNN templates have been given for the elemental morphological operators. One way to obtain realizations for more complex operators is cascading the DTCNN equivalences of the constituent elemental operators. Here it is shown that this straightforward mapping mostly yields a <b>non-optimal</b> <b>solution</b> {{with respect to the}} required amount of hardware. A hardware reduction scheme of morphologically designed DTCNNs is proposed, which includes the introduction of time variant templates and the identification of non-elementary expressions for which a single layer DTCNN exists...|$|E
40|$|Patients {{with motor}} control {{difficulties}} often "type" {{on a computer}} using a switch keyboard to guide a scanning cursor to text elements. We show how to optimize {{some parts of the}} design of switch keyboards by casting the design problem as mixed integer programming. A new algorithm to find an optimized design solution is approximately 3600 times faster than a previous algorithm, which was also susceptible to finding a <b>non-optimal</b> <b>solution.</b> The optimization requires a model of the probability of an entry error, and we show how to build such a model from experimental data. Example optimized keyboards are demonstrated. Comment: In Proceedings of the 15 th International ACM SIGACCESS Conference on Computers and Accessibility 201...|$|E
40|$|The paper {{presents}} a club theoretic {{model of a}} city. In the model welfare in a city depends on its size because {{of positive and negative}} externalities (or agglomeration economies and diseconomies) generated by close proximity of people and economic activities. Technological externalities appear in people’s utility functions and pecuniary externalities are due to shortcomings of the price mechanism. Both types of externalities are internalised in the city and act as centripetal and centrifugal forces that explain migration and thus the formation and development of the city. The issue of optimal city size is studied and it is found that the individually originated formation mechanism of the city may lead to <b>non-optimal</b> <b>solutions...</b>|$|R
5000|$|If [...] has a Gaussian distribution, {{for some}} {{values of the}} {{preference}} parameter [...] a <b>non-optimal</b> nonlinear <b>solution</b> for the control laws is given which gives a lower value for the expected cost function than does the best linear pair of control laws (Theorem 2).|$|R
40|$|To {{cope with}} the {{complexity}} of the ever changing internet architecture, network virtualization services are vowed {{to play an important role}} in the future. To provide such solutions effectively, internet providers face the problem of optimizing the allocation of virtual networks on their physical resources. Since this problem is known to be NP-hard, heuristic based online solutions tend to provide better response time, however they lead to <b>non-optimal</b> <b>solutions.</b> This paper shows how a periodic live migration of virtual networks, using a state provided by offline optimization, can help an internet provider increase its virtual network load by up to 20 %. Due to better packing of virtual load, some physical resources can also be shut down to save energy...|$|R
3000|$|To {{elaborate}} on the above heuristics, let us compare the NMF to a search process. If the NMF factoring process {{is seen as a}} search, the act of initialising W with a Tone-model is analogous to starting the search near the global optimum. When the search begins, fixing W biases the search to a certain direction. If the basis vector matrix W characterises the Tone-model of the input instrument and the value of active pitches r are determined correctly, then, {{it is likely that the}} obtained solution would be of good quality. If the value of r is wrongly determined, then the search might be guided to any <b>non-optimal</b> <b>solution.</b> Allowing the W to vary should lower the magnitude of inactive w [...]...|$|E
40|$|In this paper, {{we present}} a tabu-search based {{algorithm}} that optimizes routing for packet switching networks. The problem of routing optimization {{can be seen as}} the search of the shortest path in a graph, where the bandwidths of connections, together with their traffic, can be considered as weights. This kind of optimization is usually carried out by means of the well-known Dijkstra algorithm or its various implementations. However, an exhaustive research tends to be very heavy, from a computational point of view, when the number of nodes gets high. For this reason, we opt for a meta-heuristic algorithm, particularly tabu search, capable of finding a <b>non-optimal</b> <b>solution,</b> that can be considered quite good, even without the need of an exhaustive research...|$|E
40|$|Abstract:- In this paper, {{we present}} a tabu-search based {{algorithm}} that optimizes routing for packet switching networks. The problem of routing optimization {{can be seen as}} the search of the shortest path in a graph, where the bandwidths of connections, together with their traffic, can be considered as weights. This kind of optimization is usually carried out by means of the well-known Dijkstra algorithm or its various implementations. However, an exhaustive research tends to be very heavy, from a computational point of view, when the number of nodes gets high. For this reason, we opt for a meta-heuristic algorithm, particularly tabu search, capable of finding a <b>non-optimal</b> <b>solution,</b> that can be considered quite good, even without the need of an exhaustive research. Key-Words:- Tabu search, optimization, routing, communication networks...|$|E
30|$|DALP-TS {{demonstrates}} its efficacy against 100 distinct problem instances {{for each}} of six realistic scenarios. At {{the time of this}} research, no previously developed baseline method or solution methodology to the DALP existed. Thus, the DALP-TS results are compared against a computed lower bound on the number of aircraft trips required. The technique of comparing computed results to <b>non-optimal</b> <b>solutions</b> is not without precedent in airlift loading; [23, 25, 29], and [41] compared their results to manually computed values from a human subject matter expert (i.e., “loadmaster”) rather than computed optimal solutions. The DALP-TS initial solution extends the greedy algorithm used by AALPS to include temporal constraints. Thus, any reduction in the required number of trips from the initial solution to the final reported solution is an improvement over the current methodology.|$|R
30|$|The {{general idea}} in active {{learning}} is {{to estimate the}} value of labeling one unlabeled instance. Query-By-Committee [38], for example, uses a set of classifiers to identify the instance with the highest disagreement. Schohn et al. [37] worked on active learning for Support Vector Machines (SVM) selecting queries—instances to be labeled—by their proximity to the dividing hyperplane. Their results are, in some cases, better than if all available data are used to train. Cohn et al. [14] describe an optimal solution for pool-based active learning that selects the instance that, once labeled and added to the training set, produces the minimum expected error. This approach, however, requires high computational effort. Previous active learning approaches (providing <b>non-optimal</b> <b>solutions)</b> aim at reducing uncertainty by selecting queries as the unlabeled instances on which the classifier is less confident [29].|$|R
40|$|Information {{technology}} (IT) {{service availability}} {{is at the}} core of customer satisfaction and business success for today's organisations. Many medium- to large-size organisations outsource part of their IT services to external providers, with service-level agreements describing the agreed availability of outsourced service components. Availability management of partially outsourced IT services is a non-trivial task since classic approaches for calculating availability are not applicable, and IT managers can only rely on their expertise to fulfil it. This often leads to the adoption of <b>non-optimal</b> <b>solutions.</b> In this paper we present $A^ 2 $thOS, a framework to calculate the availability of partially outsourced IT services in the presence of SLAs and to achieve a cost-optimal choice of availability levels for outsourced IT components while guaranteeing a target availability level for the service...|$|R
