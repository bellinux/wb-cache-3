11|10000|Public
50|$|In {{multiple}} regression/correlation analysis, ordinal {{data can}} be accommodated using power polynomials and through <b>normalization</b> <b>of</b> <b>scores</b> and ranks.|$|E
50|$|In {{statistics}} and applications of statistics, normalization {{can have a}} range of meanings. In the simplest cases, normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted values into alignment. In the case of <b>normalization</b> <b>of</b> <b>scores</b> in educational assessment, there may be an intention to align distributions to a normal distribution. A different approach to normalization of probability distributions is quantile normalization, where the quantiles of the different measures are brought into alignment.|$|E
40|$|This paper {{describes}} the derivation of probability of correctness from scores assigned by most recognizers. Derivation of probability values puts {{the output of}} different recognizers on the same scale; this makes comparison across recognizers trivial. Keywords Recognizer, re-estimation methods, feature space, classifier, probability I. Introduction We present in this paper {{the groundwork for the}} use of Bayesian methodology in integration of recognizers with any subsequent processing by deriving meaningful probabilistic measures from recognizers. We also address the important notion of scalability of scores [7] and show how scores from different recognizers can be compared. Such <b>normalization</b> <b>of</b> <b>scores</b> under a common scale promotes effective combination of recognizers. Finally, it is our conjecture that the probability values themselves are more precise in what they convey than the typically output scores of recognizers. Previously, researchers have assumed in majority of the work [...] ...|$|E
3000|$|... [...]) {{was already}} done in [21]. We show the proof again because <b>of</b> our {{modified}} <b>normalization</b> <b>of</b> the <b>score</b> function.|$|R
30|$|In this section, the {{procedure}} developed {{to detect and}} remove the staff lines is presented. The whole procedure includes the detection of the staff lines and their removal using a line tracking algorithm following the characterization in [19]. However, specific processes are included in our implementation, like the <b>normalization</b> <b>of</b> the <b>score</b> size and the local correction of rotation. In the next subsections, the stages of the staff processing procedure are described.|$|R
40|$|We analyze {{students}} data {{carried out}} by INVALSI and propose a Multi Step Approach (MSA) for cheating detection and correction. This method integrates the "mechanistic" logic of fuzzy clustering with a statistical model based approach. The procedure aims to minimize the detection of false positives and to correct test scores at both class and student level. The results show a <b>normalization</b> <b>of</b> the <b>scores</b> and a stronger correction on Southern regions data where the propensity to cheating appears to be highest...|$|R
40|$|Motivation: Structural {{templates}} {{consisting of}} a few atoms in a specific geometric conformation provide {{a powerful tool for}} studying the relationship between protein structure and function. Current methods for template searching constrain template syntax and semantics by their design. Hence {{there is a need for}} a more flexible core algorithm upon which to build more sophisticated tools. Statistical analysis of structural similarity is still in its infancy when compared with its analogue in sequence alignment. In the context of template matching, there is an urgent need for <b>normalization</b> <b>of</b> <b>scores</b> so that results from templates with differing sensitivity may be compared directly. Results: We introduce Jess, a fast and flexible algorithm for searching protein structures for small groups of atoms under arbitrary constraints on geometry and chemistry. We apply the algorithm to a set of manually derived enzyme active site templates, and derive an empirical measure for estimating the relative significance of hits encountered using differing templates. Availability: Jess will be available in the near future under a restricted open source licence. Contact...|$|E
30|$|This paper {{presents}} {{an overview of}} a state-of-the-art text-independent speaker verification system. First, an introduction proposes a modular scheme of the training and test phases of a speaker verification system. Then, the most commonly speech parameterization used in speaker verification, namely, cepstral analysis, is detailed. Gaussian mixture modeling, which is the speaker modeling technique used in most systems, is then explained. A few speaker modeling alternatives, namely, neural networks and support vector machines, are mentioned. <b>Normalization</b> <b>of</b> <b>scores</b> is then explained, as {{this is a very}} important step to deal with real-world data. The evaluation of a speaker verification system is then detailed, and the detection error trade-off (DET) curve is explained. Several extensions of speaker verification are then enumerated, including speaker tracking and segmentation by speakers. Then, some applications of speaker verification are proposed, including on-site applications, remote applications, applications relative to structuring audio information, and games. Issues concerning the forensic area are then recalled, as we believe it is very important to inform people about the actual performance and limitations of speaker verification systems. This paper concludes by giving a few research trends in speaker verification for the next couple of years.|$|E
40|$|Ensuring the {{security}} of medical records is becoming an increasingly important problem as modern technology is integrated into existing medical services. As {{a consequence of the}} adoption of electronic medical records in the health care sector, it {{is becoming more and more}} common for a health professional to edit and view a patient’s record using a tablet PC. In order to protect the patient’s privacy, as required by governmental regulations in the United States, a secure authentication system to access patient records must be used. Biometric-based access is capable of providing the necessary security. On-line signature and voice modalities seem to be the most convenient for the users in such authentication systems because a tablet PC comes equipped with the associated sensors/hardware. This thesis analyzes the performance of combining the use of on-line signature and voice biometrics in order to perform robust user authentication. Signatures are verified using the dynamic programming technique of string matching. Voice is verified using a commercial, off the shelf, software development kit. In order to improve the authentication performance, we combine information from both on-line signature and voice biometrics. After suitable <b>normalization</b> <b>of</b> <b>scores,</b> fusion is performed at the matching score level. A prototyp...|$|E
30|$|We {{developed}} a PF tracker adapted to our NIR facial points tracking problem based on Harris corner detection. Some normalization functions are applied on the samples scoring and on geometric and appearance features. They {{are used to}} combine different measure values to normalize their magnitudes. <b>Normalization</b> <b>of</b> the sample <b>scores</b> might be done by any function, but analytical normalization functions allow tuning <b>of</b> the <b>score</b> values.|$|R
40|$|The plasma of {{patients}} with hepatitis C contains chromosome-damaging substances, the so-called "clastogenic factors" (CFs), as {{this is the case}} for other chronic inflammatory diseases and after radiation exposure. These endogenous clastogens, formed as a consequence of increased superoxide production by inflammatory cells, can be detected with cytogenetic methods, as they are used for exogenous clastogens. The long-lived, autosustained DNA-damaging effects of CFs are risk factors for the development of cancer and leukemia. In hepatitis C, the highest clastogenic scores has been observed in patients with hepatocellular carcinoma. In agreement with the link to inflammation, clastogenic score are correlated with necro-inflammatory scores in liver biopsies. Antioxidant therapy with a powerful superoxide scavenger resulted in <b>normalization</b> <b>of</b> clastogenic <b>scores</b> and significant decreases in aminotransferase levels, but did not influence the virus load. Preliminary results of our study on a limited number {{of patients}} suggest that pre-treatment with antioxidants may improve the outcome of interferon/ribavirin treatment. A comparison of a three-month treatment with either interferon alone or the antioxidant alone, yielded similar results for reduction of ALT levels, but only complete <b>normalization</b> <b>of</b> clastogenic <b>scores</b> for the antioxidant. Further studies have to be conducted to see whether a combination of an antiviral agent with an appropriate antioxidant would allow to reduce interferon and its side effects. Combination of antioxidants with IFN/RIBA was also reported by other authors with discordant results. The CF-test can be useful in clinical trials for the choice of the appropriate antioxidant...|$|R
40|$|This paper {{addresses}} {{the problem of}} detecting keywords in unconstrained speech. The proposed algorithms search for the speech segment maximizing the average observation probability 1 along the most likely path in the hypothesized keyword model. As known, this approach (sometimes referred to as sliding model method) requires a relaxation of the begin/endpoints of the Viterbi matching, {{as well as a}} time <b>normalization</b> <b>of</b> the resulting <b>score.</b> This makes solutions 2 L...|$|R
40|$|This paper {{presents}} {{an overview of}} a state-of-the-art text-independent speaker verification system using score normalization. First, an introduction proposes a modular scheme of the training and test phases of a speaker verification system. Then, the most commonly speech parameterization used in speaker verification, namely, cepstral analysis, is detailed. <b>Normalization</b> <b>of</b> <b>scores</b> is then explained, as {{this is a very}} important step to deal with real-world data. When acoustic and prosodic based systems are established, it is advantageous to normalize the dynamic ranges of the score dimensions, that is, likelihood scores from different quality of acoustic- and prosodic based models. Score normalization methods, linear scaling to unit range and linear scaling to unit variance, are applied to transform the output scores using the background instances so as to obtain meaningful comparison between speaker models. In this fusion system based on linear score weighting approach, the performance of speaker identification is further improved when incorporating prosodic level of information. The evaluation of a speaker verification system is then detailed, and the detection error trade-off (DET) curve is explained [...] Then, some applications of speaker verification are proposed, including won-site applications, remote applications, applications relative to structuring audio information, and games...|$|E
40|$|This paper {{presents}} {{an overview of}} a state-of-the-art text-independent speaker verification system. First, an introduction proposes a modular scheme of the training and test phases of a speaker verification system. Then, the most commonly speech parameterization used in speaker verification, namely, cepstral analysis, is detailed. Gaussian mixture modeling, which is the speaker modeling technique used in most systems, is then explained. A few speaker modeling alternatives, namely, neural networks and support vector machines, are mentioned. <b>Normalization</b> <b>of</b> <b>scores</b> is then explained, as {{this is a very}} important step to deal with real-world data. The evaluation of a speaker verification system is then detailed, and the detection error trade-off (DET) curve is explained. Several extensions of speaker verification are then enumerated, including speaker tracking and segmentation by speakers. Then, some applications of speaker verification are proposed, including on-site applications, remote applications, applications relative to structuring audio information, and games. Issues concerning the forensic area are then recalled, as we believe it is very important to inform people about the actual performance and limitations of speaker verification systems. This paper concludes by giving a few research trends in speaker verification for the next couple of years. </p...|$|E
40|$|This paper {{deals with}} a {{procedure}} to compensate for mismatched recording conditions in forensic speaker recognition, using a statistical score normalization. Bayesian interpretation of the evidence in forensic automatic speaker recognition depends on three sets of recordings in order to perform forensic casework: reference (R) and control (C) recordings of the suspect, and a potential population database (P), {{as well as a}} questioned recording (QR) [1]. The requirement of similar recording conditions between suspect control database (C) and the questioned recording (QR) is often not satisfied in real forensic cases. The aim {{of this paper is to}} investigate a procedure of <b>normalization</b> <b>of</b> <b>scores,</b> which is based on an adaptation of the Test-normalization (T-norm) [2] technique used in the speaker verification domain, to compensate for the mismatch. Polyphone IPSC- 02 database and ASPIC (an automatic speaker recognition system developed by EPFL and IPS-UNIL in Lausanne, Switzerland) were used in order to test the normalization procedure. Experimental results for three different recording condition scenarios are presented using Tippett plots and the effect of the compensation on the evaluation of the strength of the evidence is discussed...|$|E
40|$|Building on Item Response Theory we {{introduce}} students’ optimal {{behavior in}} multiple-choice tests. Our simulations {{indicate that the}} optimal penalty is relatively high, because although correction for guessing discriminates against risk-averse subjects, this effect is small compared with the measurement error that the penalty prevents. This result obtains when knowledge is binary or partial, under different <b>normalizations</b> <b>of</b> the <b>score,</b> when risk aversion is related to knowledge and {{when there is a}} pass-fail break point. We also find that the mean degree of difficulty should be close to the mean level of knowledge and that the variance of difficulty should be high. Item Response Theory, formula scoring, multiple choice tests...|$|R
40|$|Summary: We {{present a}} {{large-scale}} {{implementation of the}} Rankprop protein homology ranking algorithm {{in the form of}} an openly accessible web server. We use the NRDB 40 PSI-BLAST all-versus-all protein similarity network of 1. 1 million proteins to construct the graph for the Rankprop algorithm, whereas previously, results were only reported for a database of 108 000 proteins. We also describe two algorithmic improvements to the original algorithm, including propagation from multiple homologs of the query and better <b>normalization</b> <b>of</b> ranking <b>scores,</b> that lead to higher accuracy and to scores with a probabilistic interpretation. Availability: The Rankprop web server and source code are availabl...|$|R
40|$|This article {{presents}} a semisupervised learning {{solution to the}} result merging problem. The key contribution is the observation that information used to create resource descriptions for resource selection {{can also be used}} to create a centralized sample database to guide the <b>normalization</b> <b>of</b> document <b>scores</b> returned by different databases. At retrieval time, the query is sent to the selected databases, which return database-specific document scores, and to a centralized sample database, which returns database-independent document scores. Documents that have both a database-specific score and a database-independent score serve as training data for learning to normalize the <b>scores</b> <b>of</b> other documents. An extensive set of experiments demonstrates that this method is more effective than the well-known CORI result-merging algorithm under a variety of condition...|$|R
40|$|Abstract. Ensuring the {{security}} of medical records is becoming an increasingly important problem as modern technology is integrated into existing medical services. As {{a consequence of the}} adoption of electronic medical records in the health care sector, it {{is becoming more and more}} common for a health professional to edit and view a patient’s record using a tablet PC. In order to protect the patient’s privacy, as required by governmental regulations in the United States, a secure authentication system to access patient records must be used. Biometric-based access is capable of providing the necessary security. On-line signature and voice modalities seem to be the most convenient for the users in such authentication systems because a tablet PC comes equipped with the associated sensors/hardware. This paper analyzes the performance of combining the use of on-line signature and voice biometrics in order to perform robust user authentication. Signatures are verified using the dynamic programming technique of string matching. Voice is verified using a commercial, off the shelf, software development kit. In order to improve the authentication performance, we combine information from both on-line signature and voice biometrics. After suitable <b>normalization</b> <b>of</b> <b>scores,</b> fusion is performed at the matching score level. A prototype bimodal authentication system for accessing medical records has been designed and evaluated on a small truly multimodal database of 50 users, resulting in an average equal error rate (EER) of 0. 86 %. ...|$|E
40|$|When {{performing}} blind speaker segmentation one of {{the main}} problems is not knowing how many speakers appear in a conversation and wether they appear once or more than once. In this paper, an iterative method, which is based on the Evolutive-HMM is presented. Two main improvements to this system are introduced. On one hand, a repository generic speaker is used to model all utterances and all speaker models are derived from this iteratively. Different <b>normalization</b> <b>of</b> the <b>scores</b> are applied to the repository and the speakers to emphasize speaker changes. On the other hand, in all cases we use Gaussian Mixture Models (GMM) for their flexibility compared to an HMM structure. This method has been successfully tested using multi-speaker speech sequences generated by concatenation of speech segments from Speecon...|$|R
40|$|<b>Normalization</b> <b>of</b> {{citation}} <b>scores</b> using reference sets {{based on}} Web of Science subject categories (WCs) {{has become an}} established (“best”) practice in evaluative bibliometrics. For example, the Times Higher Education World University Rankings are, among other things, based on this operationalization. However, WCs were developed decades ago {{for the purpose of}} information retrieval and evolved incrementally with the database; the classification is machine-based and partially manually corrected. Using the WC “information science & library science” and the WCs attributed to journals in the field of “science and technology studies,” we show that WCs do not provide sufficient analytical clarity to carry bibliometric normalization in evaluation practices because of “indexer effects. ” Can the compliance with “best practices” be replaced with an ambition to develop “best possible practices”? New research questions can then be envisaged...|$|R
40|$|We {{propose a}} model to analyze {{citation}} growth and influences of fitness (competitiveness) factors in an evolving citation network. Applying the proposed method to modeling citations to papers and scholars in the InfoVis 2004 data, a benchmark collection about a 31 -year history of information visualization, leads to findings consistent with citation distributions in general and observations of the domain in particular. Fitness variables based on prior impacts and the time factor have significant influences on citation outcomes. We find considerably large effect sizes from the fitness modeling, which suggest inevitable bias in citation analysis due to these factors. While raw citation scores offer little insight into the growth <b>of</b> InfoVis, <b>normalization</b> <b>of</b> the <b>scores</b> by influences <b>of</b> time and prior fitness offers a reasonable depiction of the field's development. The analysis demonstrates the proposed model's ability to produce results consistent with observed data and to support meaningful comparison <b>of</b> citation <b>scores</b> over time. Comment: 19 pages, 8 figures, 4 table...|$|R
40|$|Abstract We {{propose a}} model to analyze {{citation}} growth and influences of fitness (competitiveness) factors in an evolving citation network. Applying the proposed method to modeling citations to papers and scholars in the InfoVis 2004 data, a benchmark collection about a 31 -year history of information visualization, leads to findings consistent with citation distributions in general and observations of the domain in particular. Fitness variables based on prior impacts and the time factor have significant influences on citation outcomes. We find considerably large effect sizes from the fitness modeling, which suggest inevitable bias in citation analysis due to these factors. While raw citation scores offer little insight into the growth <b>of</b> InfoVis, <b>normalization</b> <b>of</b> the <b>scores</b> by influences <b>of</b> time and prior fitness offers a reasonable depiction of the field’s development. The analysis demonstrates the proposed model’s ability to produce results consistent with observed data and to support meaningful comparison <b>of</b> citation <b>scores</b> over time...|$|R
40|$|This paper {{addresses}} {{the problem of}} detecting keywords in unconstrained speech without explicit modeling of non-keyword segments. The proposed algorithms are based on recent developments in confidence measures using local posterior probabilities, and searches for the segment maximizing the average observation posterior along the most likely path in the hypothesized keyword model. We can also use more complex matching scores. As known, this approach (sometimes referred to as sliding model method) requires a relaxation of the begin/endpoints of the Viterbi matching, {{as well as a}} time <b>normalization</b> <b>of</b> the resulting <b>score,</b> making dynamic programming sub-optimal or more complex (more computation and/or more memory...|$|R
40|$|This {{research}} {{investigates the}} comparative performance {{from three different}} approaches for multimodal recognition of combined iris and fingerprints: classical sum rule, weighted sum rule, and fuzzy logic method. The scores from the different biometric traits of iris and fingerprint are fused at the matching score and the decision levels. The scores combination approach is used after <b>normalization</b> <b>of</b> both <b>scores</b> using the min-max rule. Our experimental {{results suggest that the}} fuzzy logic method for the matching scores combinations at the decision level is the best followed by the classical weighted sum rule and the classical sum rule in order. The performance evaluation of each method is reported in terms of matching time, error rates, and accuracy after doing exhaustive tests on the public CASIA-Iris databases V 1 and V 2 and the FVC 2004 fingerprint database. Experimental results prior to fusion and after fusion are presented followed by their comparison with related works in the current literature. The fusion by fuzzy logic decision mimics the human reasoning in a soft and simple way and gives enhanced results...|$|R
40|$|We {{address the}} problem of {{attribute-based}} people search in real surveillance environments. The system we developed is capable of answering user queries such as“show me all people with a beard and sunglasses, wearing a white hat and a pat-terned blue shirt, from all metro cameras in the downtown area, from 2 pm to 4 pm last Saturday”. In this paper, we describe the lessons we learned from practical deployments of our system, and how we made our algorithms achieve the accuracy and efficiency required by many police depart-ments around the world. In particular, we show that a novel set of multimodal integral filters and proper <b>normalization</b> <b>of</b> attribute <b>scores</b> are critical to obtain good performance. We conduct a comprehensive experimental analysis on video footage captured from a large set of surveillance cameras monitoring metro chokepoints, in both crowded and normal activity periods. Moreover, we show impressive results us-ing images from the recent Boston marathon bombing event, where our system can rapidly retrieve the two suspects based on their attributes from a database containing more than one thousand people present at the event...|$|R
40|$|This paper {{presents}} {{fusion of}} two biometric traits, i. e., palmprint and speech signal, at matching score level architecture uses weighted sum <b>of</b> <b>score</b> technique. The features are {{extracted from the}} pre-processed palm image and pre-processed speech signal. The features of a query image and speech signal are {{compared with those of}} a database images and speech signal to obtain matching scores. The individual scores generated after matching are passed to the fusion module. This module consists of three major steps i. e., <b>normalization,</b> generation <b>of</b> similarity <b>score</b> and fusion <b>of</b> weighted <b>scores.</b> The final score is then used to declare the person as genuine or an impostor. The system is tested on database collected by the authors for 120 subjects and gives an overall accuracy of 98. 47 % with FAR of 1. 36 % and FRR o...|$|R
40|$|Multibiometric {{recognition}} systems, which aggregate {{information from}} multiple biometric sources, are gaining popularity {{because they are}} able to overcome limitations such as non-universality, noisy sensor data and susceptibility. Multibiometric systems promise significant improvements as higher accuracy and increased resistance to spoofing over the single biometric systems. This paper proposes a method which integrates fingerprint, palmprint and face and performs the fusion at score level. Three biometric traits are collected and stored into database at the time of Enrollment. In the Authentication stage query images will be compared against the stored templates and match score is generated. AOV based minutiae algorithm is proposed for fingerprint matching. To compare Face images PCA analysis is used. Palmprint matching score can be generated using PCA analysis. This matching score will be passed to the fusion stage. Fusion stage includes <b>normalization</b> <b>of</b> the <b>scores.</b> Weights can be assigned according to the importance of the biometric traits. These weighted and normalized score will be combined to generate a total score. This total score will be passed to the decision stage. In the decision stage total score will be compared with certain threshold value. That will realize person’s authenticity whether a person is genuine or imposter...|$|R
40|$|Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. This research investigates the comparative performance {{from three different}} approaches for multimodal recognition of combined iris and fingerprints: classical sum rule, weighted sum rule, and fuzzy logic method. The scores from the different biometric traits of iris and fingerprint are fused at the matching score and the decision levels. The scores combination approach is used after <b>normalization</b> <b>of</b> both <b>scores</b> using themin-max rule. Our experimental {{results suggest that the}} fuzzy logicmethod for thematching scores combinations at the decision level is the best followed by the classical weighted sum rule and the classical sum rule in order. The performance evaluation of each method is reported in terms of matching time, error rates, and accuracy after doing exhaustive tests on the public CASIA-Iris databases V 1 and V 2 and the FVC 2004 fingerprint database. Experimental results prior to fusion and after fusion are presented followed by their comparison with related works in the current literature. The fusion by fuzzy logic decision mimics the human reasoning in a soft and simple way and gives enhanced results. 1...|$|R
40|$|Abstract. Searching {{information}} through the Internet often requires users to separately contact several digital libraries, use each library interface to author the query, analyze retrieval results and merge them with results returned by other libraries. Such a solution could be simplified {{by using a}} centralized server {{that acts as a}} gateway between the user and several distributed repositories: The centralized server receives the user query, forwards the user query to federated repositories—possibly translating the query in the specific format required by each repository—and fuses retrieved documents for presentation to the user. To accomplish these tasks efficiently, the centralized server should perform some major operations such as: resource selection, query transformation and data fusion. In this paper we report on some aspects of MIND, a system for managing distributed, heterogeneous multimedia libraries (MIND, 2001, [URL] In particular, this paper focusses on the issue of fusing results returned by different image repositories. The proposed approach is based on <b>normalization</b> <b>of</b> matching <b>scores</b> assigned to retrieved images by individual libraries. Experimental results on a prototype system show the potential of the proposed approach with respect to traditional solutions...|$|R
40|$|BACKGROUND: We {{conducted}} a post-hoc {{analysis of the}} Long-Acting MethylpheniDate in Adult attention-deficit hyperactivity disorder (LAMDA) study to investigate predictors of response in adults with ADHD randomly assigned to Osmotic Release Oral System (OROS) ((R)) -methylphenidate hydrochloride (MPH) 18, 36 or 72 mg or placebo. METHODS: LAMDA comprised a 5 -week, double-blind (DB) period, followed by a 7 -week, open-label (OL) period. A post-hoc analysis of covariance and a logistic regression analysis were undertaken to detect whether specific baseline parameters or overall treatment compliance during the double-blind phase contributed to response. The initial model included all covariates as independent variables; a backward stepwise selection method was used, with stay criteria of p/= 30 % decrease in CAARS:O-SV score from baseline) and <b>normalization</b> <b>of</b> CAARS:O-SV <b>score</b> at DB end point. RESULTS: Taking into account a significant effect of OROS((R)) -MPH treatment versus placebo in the original analysis (p</= 0. 015), across the outcomes considered in this post-hoc analysis, higher baseline CAARS scores were most strongly predictive of superior outcomes. Male gender and lower academic achievement were also predictive for improved results with certain outcomes. CONCLUSIONS: Several baseline factors may help to predict better treatment outcomes in adults receiving OROS((R)) -MPH; however, further research is required to confirm these findings and examine their neurobiological underpinnings...|$|R
40|$|Abstract- Multibiometric {{recognition}} systems, which aggregate {{information from}} multiple biometric sources, are gaining popularity {{because they are}} able to overcome limitations such as non-universality, noisy sensor data and susceptibility. Multibiometric systems promise significant improvements as higher accuracy and increased resistance to spoofing over the single biometric systems. This paper proposes a method which integrates fingerprint, palmprint and face and performs the fusion at score level. Three biometric traits are collected and stored into database at the time of Enrollment. In the Authentication stage query images will be compared against the stored templates and match score is generated. AOV based minutiae algorithm is proposed for fingerprint matching. To compare Face images PCA analysis is used. Palmprint matching score can be generated using PCA analysis. This matching score will be passed to the fusion stage. Fusion stage includes <b>normalization</b> <b>of</b> the <b>scores.</b> Weights can be assigned according to the importance of the biometric traits. These weighted and normalized score will be combined to generate a total score. This total score will be passed to the decision stage. In the decision stage total score will be compared with certain threshold value. That will realize person’s authenticity whether a person is genuine or imposter. Keywords – Multibiometric, fingerprint, palmprint, face, AOV,PCA I...|$|R
40|$|This paper {{addresses}} {{the problem of}} detecting keywords in unconstrained speech without explicit modeling of non-keyword segments. The proposed algorithm is based on recent developments in confidence measures based on local a posterior probabilities, together with the (known) approach of relaxing the begin /endpoints in a Dynamic Programming (DP) matching of partial hypothesized sub-sequences on a specific (keyword) hidden Markov model (HMM), followed by a time <b>normalization</b> <b>of</b> the resulting <b>scores</b> 1. In this case, straightforward DP is no longer optimal and various algorithms were devised, usually requiring more computation and more memory. In this paper, we present an alternative, quite simple and efficient, approach to this problem, which can also be easily generalized to more complex matching scores. The proposed algorithm is based on (1) the average observation posterior along the most likely state sequence, and (2) an iterative Viterbi decoding algorithm, which does not requi [...] ...|$|R
40|$|Abstract Background Protein antigens {{and their}} {{specific}} epitopes are formulation targets for epitope-based vaccines. A number of prediction servers {{are available for}} identification of peptides that bind major histocompatibility complex class I (MHC-I) molecules. The lack of standardized methodology and large number of human MHC-I molecules make the selection of appropriate prediction servers difficult. This study reports a comparative evaluation of thirty prediction servers for seven human MHC-I molecules. Results Of 147 individual predictors 39 have shown excellent, 47 good, 33 marginal, and 28 poor ability to classify binders from non-binders. The classifiers for HLA-A* 0201, A* 0301, A* 1101, B* 0702, B* 0801, and B* 1501 have excellent, and for A* 2402 moderate classification accuracy. Sixteen prediction servers predict peptide binding affinity to MHC-I molecules with high accuracy; correlation coefficients ranging from r = 0. 55 (B* 0801) to r = 0. 87 (A* 0201). Conclusion Non-linear predictors outperform matrix-based predictors. Most predictors can be improved by non-linear transformations of their raw prediction scores. The best predictors of peptide binding are also best in prediction of T-cell epitopes. We propose a new standard for MHC-I binding prediction – a common scale for <b>normalization</b> <b>of</b> prediction <b>scores,</b> applicable to both experimental and predicted data. The {{results of this study}} provide assistance to researchers in selection of most adequate prediction tools and selection criteria that suit the needs of their projects. </p...|$|R
40|$|Abstract—Biometrics has {{dominated}} the areas of security {{for its ability to}} provide uniqueness, higher accuracy and minimum invasion instances. Using iris and its texture as a means of verification and validation evolved three decades ago and since then it {{has been one of the}} most reliable security methods with work being done to increase efficiency along with minimizing cost. The method proposed and implemented considers the two above stated requirements as its essence. Two algorithms have been proposed i. e. Euler number and Cumulative Sum for feature extraction. Both have simple mathematical computations which require less processor time thus reducing cost. Fusion of the two algorithms gives higher accuracy since cases of contradicting results can be cross verified with the individual algorithm score. Euler Number works on topographical features and gives scope for rescaling and resizing. Eight bit planes are obtained of which the four least significant bits are discarded since they give brightness and are random. Using the four most significant bits templates are formed. Cumulative Sum on the other hand explores the uniqueness of every iris template, calculating the cumulative sum where deviation from the mean is calculated and added to the previous value in the template. Each algorithm individually computes its own score which is then fused to obtain a final result. The fusion is carried out after template matching and <b>normalization</b> <b>of</b> the <b>scores,</b> ensuring a more accurate result. Different algorithms for template matching i. e. Vector Difference matchin...|$|R
40|$|BACKGROUND: Protein antigens {{and their}} {{specific}} epitopes are formulation targets for epitope-based vaccines. A number of prediction servers {{are available for}} identification of peptides that bind major histocompatibility complex class I (MHC-I) molecules. The lack of standardized methodology and large number of human MHC-I molecules make the selection of appropriate prediction servers difficult. This study reports a comparative evaluation of thirty prediction servers for seven human MHC-I molecules. RESULTS: Of 147 individual predictors 39 have shown excellent, 47 good, 33 marginal, and 28 poor ability to classify binders from non-binders. The classifiers for HLA-A* 0201, A* 0301, A* 1101, B* 0702, B* 0801, and B* 1501 have excellent, and for A* 2402 moderate classification accuracy. Sixteen prediction servers predict peptide binding affinity to MHC-I molecules with high accuracy; correlation coefficients ranging from r = 0. 55 (B* 0801) to r = 0. 87 (A* 0201). CONCLUSION: Non-linear predictors outperform matrix-based predictors. Most predictors can be improved by non-linear transformations of their raw prediction scores. The best predictors of peptide binding are also best in prediction of T-cell epitopes. We propose a new standard for MHC-I binding prediction – a common scale for <b>normalization</b> <b>of</b> prediction <b>scores,</b> applicable to both experimental and predicted data. The {{results of this study}} provide assistance to researchers in selection of most adequate prediction tools and selection criteria that suit the needs of their projects. ImmunoGrid project (EC contract FP 6 - 2004 -IST- 4 No. 028069; National Institutes of Health (U 19 A 157330...|$|R
