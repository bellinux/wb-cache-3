40|10000|Public
5000|$|Homogeneity {{tests for}} CRMs follow planned {{experimental}} designs. Because the experiment {{is intended to}} test for (or estimate the size of) variation in value between different CRM units, the designs are chosen to allow separation of variation in results due to random measurement error and variation due to differences between units of the CRM. Among the simplest designs recommended for this purpose is a simple balanced nested design (see schematic). [...] Typically 10-30 CRM units are taken from the batch at random; stratified random sampling is recommended so that the selected units are spread across the batch. An equal <b>number</b> <b>of</b> <b>subsamples</b> (usually two or three) is then taken from each CRM unit and measured. Subsamples are measured in random order. Other designs, such as randomized block designs, have also been used for CRM certification.|$|E
40|$|A new {{jackknife}} {{method is}} introduced {{to remove the}} first order bias in unit root models. It is optimal {{in the sense that}} it minimizes the variance among all the jackknife estimators of the form considered in Phillips and Yu (2005) and Chambers and Kyriacou (2013) after the <b>number</b> <b>of</b> <b>subsamples</b> is selected. Simulations show that the new jackknife reduces the variance of that of Chambers and Kyriacou by about 10 % for any selected <b>number</b> <b>of</b> <b>subsamples</b> without compromising bias reduction. The results continue to hold true in near unit root models. (C) 2014 Elsevier B. V. All rights reserved...|$|E
40|$|Bear food {{habits are}} often {{quantified}} using scat analysis, mainly {{due to its}} non-invasiveness and because samples are relatively easy to collect. However, lab processing time can be daunting and may end up competing with other field activities. Sub-sampling a bear scat to analyze its contents may reduce the lab processing time, but the <b>number</b> <b>of</b> <b>subsamples</b> per scat is usually chosen arbitrarily. We investigated {{the effect of the}} <b>number</b> <b>of</b> <b>subsamples</b> per bear scat on the estimatation of the diet composition of the Apennine brown bear in the Abruzzo Lazio and Molise National Park. Based on a sample of 328 bear scats collected in 2006, and from 5 to 1 subsamples (10 ml) per scat, dietary analysis showed qualitative and quantitative stability at a decreasing <b>number</b> <b>of</b> <b>subsamples,</b> and only food items of negligible importance were occasionally missed using 1 - 2 subsamples per scat. We concluded that 2 subsamples can be used without significant loss in accuracy, corresponding to a 60 % reduction in lab time, and to more than 50 days of lab work for one operator to process our entire bear scat sample. By assessing the effect of sub-sampling a bear scat for dietary analysis, we also present preliminary data on the seasonal food habits of the Apennine brown bear population...|$|E
3000|$|... he <b>number</b> <b>of</b> <b>subsample</b> trees (Päivinen 1987). A {{tree was}} defined as {{borderline}} tree if its distance from the plot center differed less than 0.5  m from the radius for a tree with a given size.|$|R
30|$|Measurement {{and model}} errors for the {{variables}} {{used to calculate}} the characteristics of interest have also an effect on precision (e.g. Päivinen 1987, Ståhl et al. 2014). Their combined effect again depends on the <b>number</b> <b>of</b> <b>subsample</b> tree measurements and the models/methods available to generalize the subsample tree measurements to the tally trees. It may be assumed that the errors in volume / biomass for subsample trees are negligible, but not for the tally trees. It is quite possible that the model which is most efficient when all measurements are assumed error-free is not the most efficient when these errors are included (Eid 2003). Therefore, {{it would be best to}} select the models used for generalizing the subsample tree characteristics to tally trees simultaneously with deciding the <b>number</b> <b>of</b> <b>subsample</b> trees and the variables measured for each of them.|$|R
30|$|It {{is used in}} {{the case}} of non-normal data, {{bootstrapping}} technique can be summarized as a resampling method, by which <b>numbers</b> <b>of</b> <b>subsamples</b> are generated through a random dropping and replacing sets of observations from the original data in order to derive the entire distributions and enable the significance tests (Ringle, C. M. et al., 2015).|$|R
40|$|The vast {{majority}} of research on capability indices has assumed that the data consists of one large, representative sample. In practice, and {{in much of the}} quality control literature, process data are collected over time in subsamples representing rational subgroups. In this paper we examine the statistical behavior of two Cpm estimators based on this more realistic data structure. The estimators correspond to pooled and un-pooled variance estimators. The theoretical findings are applied to hypothesis testing and power calculations. The power functions of the tests based on the two estimators are used to determine the minimum <b>number</b> <b>of</b> <b>subsamples</b> needed to meet a threshold requirement that power exceeds 0. 80. Extensive tables of the recommended <b>number</b> <b>of</b> <b>subsamples</b> are provided with comments on their usageValiderad; 2004; 20061013 (evan...|$|E
30|$|We find a {{positive}} association between BMI {{and hours of}} work for a <b>number</b> <b>of</b> <b>subsamples,</b> including single and married White women and single Black men and women. However, the evidence of a causal relationship is only consistent in White women. Our IV estimates suggest that a one-unit increase in BMI leads to a 1.4 % (2.0 %) increase in hours worked among White single (married) women.|$|E
40|$|An {{expression}} for predicting {{the number of}} species in a given area is described. Derivatives of this expression, to increase sampling efficiency in a vegetation stand, include a minimum of four separate subsamples; a maximum <b>number</b> <b>of</b> <b>subsamples</b> when less than 10 % increment in new species is achieved; and subsample size. It is also suggested that species diversity in terms of species per unit area can be more consistent when derived from this expression...|$|E
30|$|The {{measurements}} {{needed for}} each tree {{depend on the}} characteristics of interest (e.g. volume, biomass, stems per ha). Typically not all characteristics needed are measured on all trees within a plot. The diameter at breast height (d 1.3) is measured for all tally trees, but height, upper diameters, age, and growth are measured only for subsample trees. Thus, the measurement time also depends on the <b>number</b> <b>of</b> <b>subsample</b> trees within each plot, and the <b>number</b> <b>of</b> measurements carried out on each tree. As biomass and volume require additional subsample tree measurements compared to stems per ha, also the time consumption needs to be defined separately for each of the variables.|$|R
40|$|A Monte Carlo {{investigation}} {{shows that}} the rejection probability of the structural stability test of Andrews (2003) depends on several characteristics of the DGP, {{one of which is}} the length of the hypothesized break period. This is analyzed and found to be caused, at least in part, {{by the fact that the}} <b>number</b> <b>of</b> <b>subsampling</b> statistics used to compute the P value depends on the sample size and the length of the break period. Simulations show that kernel smoothed P values provide more accurate tests in small samples. Kernel smoothing; Simulation-based test; P value; Stability test...|$|R
40|$|Equiprobable samples with {{replacements}} from Їnite Abelian {{groups are}} con- sidered. Limit theorems are proved describing convergence {{of the distribution}} <b>of</b> the <b>number</b> <b>of</b> ordered <b>subsamples</b> meeting speciЇed linear relations to Poisson distributions. Basing on these theorems we construct a goodness-of-Їt test which checks the hypothesis on the uniform distribution of sample elements...|$|R
40|$|Abstract ? The {{objective}} of this work was to evaluate an inventory method efficiency for ants. We used subsamples collected in 24 transects of 100 m, distributed in 6 plots of 600 ha each in primary forest, {{as part of a}} long-term project. Ten litter subsamples were extracted per transect using Winkler extractors. Ants were identified to genus level, and Crematogaster, Gnamptogenys and Pachycondyla genera to species/morphospecies level. To evaluate the consequences of reduced sampling on the retention of ecological information, we estimated the lowest <b>number</b> <b>of</b> <b>subsamples</b> needed to detect the effects of environmental variables. Multidimensional scaling (MDS) was used to generate dissimilarity matrices, and Mantel correlations between each reduced-sampling effort and maximum effort were used as an index of how much information was maintained and could still be used in multivariate analyses. Lower p-values was observed on the effect of soil pH in the community of genera, and on the effect of the litter volume for the community of Crematogaster. The trend was still detectable in the analysis based on reduced-sampling. The <b>number</b> <b>of</b> <b>subsamples</b> can be reduced, and the cost-efficiency of the protocol can be improved with little loss of information. 200...|$|E
40|$|There is no {{consensus}} in the literature regarding how many subsamples are needed to perform accurate on-farm soil penetration resistance (SPR) mapping. Therefore, {{the objective of this}} study was to define the <b>number</b> <b>of</b> <b>subsamples</b> per sampling point needed to quantify the SPR. The experiment was performed in a 4. 7 ha area and employed a 50 × 50 m grid system (18 sampling points). The SPR was evaluated using a digital penetrometer in two different years with 1, 2, 3, 4, 5, 6, 9, 12, and 15 subsamples per sampling point. The SPR maps produced with increasing numbers of subsamples were compared to the reference maps (15 subsamples) using the relative deviation coefficient and Pearson´s linear correlation. A reduction in the <b>number</b> <b>of</b> <b>subsamples</b> promoted an increase in the variability of the SPR data. Generally, the results from this study suggest the use of at least four subsamples per sampling point to achieve SPR maps with a coefficient of relative deviation less than 10 % (30 % maximum error per point around the mean) and significant correlation with the reference maps (15 subsamples) ...|$|E
40|$|The {{objective}} of this work was to evaluate an inventory method efficiency for ants. We used subsamples collected in 24 transects of 100 m, distributed in 6 plots of 600 ha each in primary forest, {{as part of a}} long-term project. Ten litter subsamples were extracted per transect using Winkler extractors. Ants were identified to genus level, and Crematogaster, Gnamptogenys and Pachycondyla genera to species/morphospecies level. To evaluate the consequences of reduced sampling on the retention of ecological information, we estimated the lowest <b>number</b> <b>of</b> <b>subsamples</b> needed to detect the effects of environmental variables. Multidimensional scaling (MDS) was used to generate dissimilarity matrices, and Mantel correlations between each reduced-sampling effort and maximum effort were used as an index of how much information was maintained and could still be used in multivariate analyses. Lower p-values was observed on the effect of soil pH in the community of genera, and on the effect of the litter volume for the community of Crematogaster. The trend was still detectable in the analysis based on reduced-sampling. The <b>number</b> <b>of</b> <b>subsamples</b> can be reduced, and the cost-efficiency of the protocol can be improved with little loss of information...|$|E
40|$|The {{bootstrap}} {{provides a}} simple and powerful means of assessing the quality of esti-mators. However, in settings involving large datasets—which are increasingly prevalent— the computation of bootstrap-based quantities can be prohibitively demanding com-putationally. While variants such as subsampling and the m out of n bootstrap {{can be used in}} principle to reduce the cost of bootstrap computations, we find that these meth-ods are generally not robust to specification of hyperparameters (such as the <b>number</b> <b>of</b> <b>subsampled</b> data points), and they often require use of more prior information (such as rates of convergence of estimators) than the bootstrap. As an alternative, we intro-duce the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient 1 ar X i...|$|R
30|$|After each {{sampling}} interval, {{the collected}} samples {{were removed from}} the auto-samplers. Three 600 -ml subsamples were taken from each sample and stored in 1 -litre glass bottles. The remaining volume was analysed for chemical parameters. The bottles were labelled with “S” (Shaft) or “B” (Outlet), the <b>number</b> <b>of</b> the interval, the <b>number</b> <b>of</b> the <b>subsample</b> and the date. All samples were deep-frozen at − 18  °C within 2  h after collection.|$|R
30|$|While {{many factors}} {{affecting}} the accuracy can {{be accounted for}} analytically, some aspects like the spatial pattern, are more difficult. The analytical calculations usually assume a random pattern (Mandallaz 2007). Likewise, the <b>number</b> <b>of</b> <b>subsample</b> trees and the selection of measurements taken from each tree (e.g. height and/or upper diameter) {{can be difficult to}} account for in detail in an analytical setting. Therefore, the optimal plot size and type has most often been defined by simulating sampling in an accurately measured and mapped forest area. In the earliest studies, simulation was carried out by measuring a grid of small cells and building larger sample plots as their combination (Johnson & Hixon 1952, Mesavage & Grosenbaugh 1956). In later studies, computer simulation based on mapped data has been utilized (e.g. Kulow 1966). In a simulation based on real data, the optimal plot size is heavily dependent on the forest conditions on the area, which makes definite conclusions difficult (Mesavage & Grosenbaugh 1956).|$|R
40|$|We {{study how}} the {{divide and conquer}} {{principle}} [...] - partition the available data into subsamples, compute an estimate from each subsample and combine these appropriately to form the final estimator [...] - works in non-standard problems where rates of convergence are typically slower than √(n) and limit distributions are non-Gaussian, with a special emphasis on the least squares estimator (and its inverse) of a monotone regression function. We find that the pooled estimator, obtained by averaging non-standard estimates across the mutually exclusive subsamples, outperforms the non-standard estimator based on the entire sample {{in the sense of}} pointwise inference. We also show that, under appropriate conditions, if the <b>number</b> <b>of</b> <b>subsamples</b> is allowed to increase at appropriate rates, the pooled estimator is asymptotically normally distributed with a variance that is empirically estimable from the subsample-level estimates. Further, in the context of monotone function estimation we show that this gain in pointwise efficiency comes at a price [...] - the pooled estimator's performance, in a uniform sense (maximal risk) over a class of models worsens as the <b>number</b> <b>of</b> <b>subsamples</b> increases, leading to a version of the super-efficiency phenomenon. In the process, we develop analytical results for the order of the bias in isotonic regression, which are of independent interest...|$|E
40|$|This paper {{tests for}} the {{existence}} of short-run equilibrium in the urban housing market in Metropolitan Toronto. The alternative hypothesis is the housing market segmented with respect to locational and structural attributes. We found insignificant differences in attribute prices across hypothesized submarkets. This implies that an unstratified hedonic price regressions model, based on the assumption of short-run equilibrium, is equally efficient in the analysis of housing prices as a model based on a <b>number</b> <b>of</b> <b>subsamples</b> stratified along lines of segmentation. Copyright American Real Estate and Urban Economics Association. ...|$|E
40|$|Abstract – The {{objective}} of this work was to evaluate an inventory method effi ciency for ants. We used subsamples collected in 24 transects of 100 m, distributed in 6 plots of 600 ha each in primary forest, {{as part of a}} long-term project. Ten litter subsamples were extracted per transect using Winkler extractors. Ants were identifi ed to genus level, and Crematogaster, Gnamptogenys and Pachycondyla genera to species/morphospecies level. To evaluate the consequences of reduced sampling on the retention of ecological information, we estimated the lowest <b>number</b> <b>of</b> <b>subsamples</b> needed to detect the effects of environmental variables. Multidimensional scaling (MDS) was used to generate dissimilarity matrices, and Mantel correlations between each reduced-sampling effort and maximum effort were used as an index of how much information was maintained and could still be used in multivariate analyses. Lower p-values was observed on the effect of soil pH in the community of genera, and on the effect of the litter volume for the community of Crematogaster. The trend was still detectable in the analysis based on reduced-sampling. The <b>number</b> <b>of</b> <b>subsamples</b> can be reduced, and the cost-effi ciency of the protocol can be improved with little loss of information. Index terms: Formicidae, environmental impact assessment, soil biodiversity, Winkler extractors, tropical forest, sampling protocols. Efi ciência em inventários de formigas em uma reserva fl oresta...|$|E
40|$|We {{propose a}} simple {{approach}} which, given distributed computing resources, can nearly achieve {{the accuracy of}} $k$-NN prediction, while matching (or improving) the faster prediction time of $ 1 $-NN. The approach consists of aggregating denoised $ 1 $-NN predictors over a small <b>number</b> <b>of</b> distributed <b>subsamples.</b> We show, both theoretically and experimentally, that small subsample sizes suffice to attain similar performance as $k$-NN, without sacrificing the computational efficiency of $ 1 $-NN...|$|R
40|$|The {{bootstrap}} {{provides a}} simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets [...] -which are increasingly prevalent [...] -the computation of bootstrap-based quantities can be prohibitively demanding computationally. While variants such as subsampling and the $m$ out of $n$ bootstrap {{can be used in}} principle to reduce the cost of bootstrap computations, we find that these methods are generally not robust to specification of hyperparameters (such as the <b>number</b> <b>of</b> <b>subsampled</b> data points), and they often require use of more prior information (such as rates of convergence of estimators) than the bootstrap. As an alternative, we introduce the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient means of assessing the quality of estimators. BLB is well suited to modern parallel and distributed computing architectures and furthermore retains the generic applicability and statistical efficiency of the bootstrap. We demonstrate BLB's favorable statistical performance via a theoretical analysis elucidating the procedure's properties, as well as a simulation study comparing BLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In addition, we present results from a large-scale distributed implementation of BLB demonstrating its computational superiority on massive data, a method for adaptively selecting BLB's hyperparameters, an empirical study applying BLB to several real datasets, and an extension of BLB to time series data...|$|R
40|$|Phylo-zonations (or lineage-zonations) {{are based}} upon {{morphological}} changes within individual evolutionary lineages. These zonations, although potentially of use for stratigraphic subdivision and correlation, often suffer {{from a lack of}} quantitative exactness in the definitions of chronospecies. Thus exact reproducibility is hindered for stratigraphic determinations. The potential of morphometrically defined phylo-zonations is demonstrated on a temperate South Pacific Late Cenozoic lineage of planktonic foraminifera (Globorotalia conoidea through intermediate forms to Globorotalia inflata in DSDP Site 284) exhibiting phyletic gradualism. Our sampling interval is about 0. 1 m. y. during the last 8 m. y. Changes in the <b>number</b> <b>of</b> chambers in the final whorl, test conicalness, percentage of keeled forms, and test roundness or inflatedness, are used to quantitatively define the following five chronospecies: G. conoidea (Late Miocene; 6. 1 -> 8. 3 m. y.), G. conomiozea (latest Miocene; 5. 3 - 6. 1 m. y.), G. puncticulata sphericomiozea (earliest Pliocene; 4. 5 - 5. 3 m. y.), G. puncticulata puncticulata (Early-Middle Pliocene; 2. 9 - 4. 5 m. y.), and G. inflata (Late Pliocene-Quaternary; 0 - 2. 9 m. y.). This phylo-zonation is directly applicable to temperate cool subtropical Southern Hemisphere areas where the evolution took place (Kennett, 1967, 1973; Scott, 1979). It is still not known if the lineage occurs elsewhere; thus the applicability of the phylo-zonation over broader areas is still uncertain. Trends in general size and aperture shape seem to be climatically controlled, and thus may be only of local stratigraphic utility. The practical applications of morphometric phylo-zonation for stratigraphy is to a large extent dependent upon the amount of time and effort required to statistically define the trends. Experiments with large <b>numbers</b> <b>of</b> <b>subsamples</b> from this lineage demonstrate that accurate stratigraphic determinations are possible from measurements on only 15 specimens per sample, except for those very close to chronospecies boundaries...|$|R
40|$|All known robust {{location}} and scale estimators with high breakdown point for multivariate sample's are very expensive to compute. In practice, this computation {{has to be}} carried out using an approximate subsampling procedure. In this work we describe an alternative subsampling scheme, applicable to both the Stahel-Donoho estimator and the estimator based on the Minimum Volume Ellipsoid, with the property that the <b>number</b> <b>of</b> <b>subsamples</b> required is substantially reduced with respect to the standard subsampling procedures used in both cases. We also discuss some bias and variability properties of the estimator obtained from the proposed subsampling process...|$|E
40|$|This paper {{examines}} {{differences that}} develop {{over time as}} a result of laboratory and field curing conditions in the physical properties of monolithic waste samples. During field placement of waste treated using stabilization/solidification, a <b>number</b> <b>of</b> <b>subsamples</b> were taken and cured under laboratory conditions. These samples were evaluated periodically using a battery of laboratory test methods in parallel with samples removed from the landfill by diamond core. The results indicate that, although the field-cured sample was cured under very different conditions, the physical nature of the matrix is similar to samples cured under laboratory conditions, after 2 years...|$|E
40|$|REDI computes {{a robust}} linear {{regression}} based on Rousseeuw's REsistant DIagnostic described in P. J. Rousseeuw and A. M. Leroy, Robust Regression and Outlier Detection, Wiley, 1987, pp. 238 - 245. Like other "high-breakdown" estimators (including least median of squares and least trimmed squares), REDI detects and downweights outliers and leverage points which may distort the regression estimate. REDI {{is computed by}} a resampling scheme. The user chooses the <b>number</b> <b>of</b> <b>subsamples</b> by setting the option ITERATIONS. Its default value, 3000 subsamples, should be adequate for most applications. The user should consult the Rousseeuw-Leroy book for additional details. regression, robust...|$|E
40|$|The {{objectives}} {{of this work}} was to estimate the <b>number</b> <b>of</b> soil <b>subsamples</b> considering the classical statistics and geostatistics and determine the spatial variability of soil fertility attributes of an Ultisol, with clay texture, {{in an area of}} regenerating natural vegetation in Alegre - ES. Soil samples were collected in a depth of 0. 0 - 0. 2 m, at the crossing points of a regular grid, comprising a total of 64 points located at 10 m-intervals. The area presented low fertility soil. Considering a variation of 5 % around the mean in the classic statistics, it is necessary a larger <b>number</b> <b>of</b> samples in relation to geostatistics. All the chemical attributes showed moderate to high spatial dependence, except for the effective cation exchange capacity (CECe), which showed pure nugget effect. The spherical semivariogram model gave the best fit to the data. Isoline maps allowed visualizing the differentiated spatial distribution of the contents of soil chemical attributes...|$|R
40|$|We {{consider}} a localized {{approach in the}} well-established setting of reproducing kernel learning under random design. The input space X is partitioned into local disjoint subsets X_j (j= 1, [...] .,m) equipped with a local reproducing kernel K_j. It is then straightforward to define local KRR estimates. Our first main contribution is in showing that minimax optimal rates of convergence are preserved if the <b>number</b> m <b>of</b> partitions grows sufficiently slowly with the sample size, under locally different degrees on smoothness assumptions on the regression function. As a byproduct, we show that low smoothness on exceptional sets of small probability does not contribute, leading to a faster rate of convergence. Our second contribution lies in showing that the partitioning approach for KRR can be efficiently combined with local Nyström subsampling, improving computational cost twofold. If the <b>number</b> <b>of</b> locally <b>subsampled</b> inputs grows sufficiently fast with the sample size, minimax optimal rates of convergence are maintained...|$|R
40|$|This paper {{proposes a}} new fBm (fractional Brownian motion) interpolation/reconstruction method from {{partially}} known samples based on CS (Compressive Sampling). Since 1 /f property implies power law decay of the fBm spectrum, the fBm signals should be sparse in frequency domain. This property motivates {{the adoption of}} CS {{in the development of}} the reconstruction method. Hurst parameter H that occurs in the power law determines the sparsity level, therefore the CS reconstruction quality of an fBm signal for a given <b>number</b> <b>of</b> known <b>subsamples</b> will depend on H. However, the proposed method does not require the information of H to reconstruct the fBm signal from its partial samples. The method employs DFT (Discrete Fourier Transform) as the sparsity basis and a random matrix derived from known samples positions as the projection basis. Simulated fBm signals with various values of H are used to show the relationship between the Hurst parameter and the reconstruction quality. Additionally, US-DJIA (Dow Jones Industrial Average) stock index monthly values time-series are also used to show the applicability of the proposed method to reconstruct a real-world data. Comment: 6 double-column pages, 5 figures, submitted to ICEEI- 201...|$|R
40|$|Herein, we {{find that}} the market price of closed-end fund shares tends to {{increase}} (decrease) in anticipation of a rise (fall) in the net asset value (NAV). Similarly, an increase (decrease) in the reported NAV tends to be followed by a rise (fall) in the price of the fund's shares. Interestingly, we also find a powerful negative autocorrelation between closed-end fund shares' overnight and intraday returns in both univariate and multivariate tests for both the overall sample and a <b>number</b> <b>of</b> <b>subsamples.</b> We believe that this tendency results from the strategies that many specialists employ when they open their assigned shares. " Copyright (c) 2010 Financial Management Association International. ...|$|E
30|$|Advance {{microscope}} Nikon 80 i {{was used}} for biofloc microscopic identification and for plankton length and size measurements. Qualitative and quantitative analyses of phytoplankton and zooplankton were done by Lackey’s method. Compound microscope was used for phytoplankton counting. The cover slip was placed over {{a drop of water}} in the slide and whole of cover slip was examined by parallel overlapping strips to count all the organisms in the drop. About 22 strips were examined in each drop. <b>Number</b> <b>of</b> <b>subsamples</b> to be taken depended on examining 2 – 3 successive subsamples without addition of an encounter species when compared to the examined subsamples in the same sample (American Public Health Association APHA 1989).|$|E
40|$|Given n joint {{observations}} on k continuous variables, mcd. src computes a robust mean vector and a robust covariance matrix using the minimum covariance determinant algorithm [P. J. Rousseeuw and A. M. Leroy (1987), Robust Regression and Outlier Detection, New York: Wiley]. Observations whose robust Mahalanobis distances exceed the 97. 5 % chi-square value with k {{degrees of freedom}} are flagged as potential outliers. mcd. src uses a resampling method, and the <b>number</b> <b>of</b> <b>subsamples</b> (each having k+ 1 data) is the procedure's only option (default = 3000 subsamples). mcd. src reads the data as series whose first observation is start and whose last observation is end (i. e., n = end - start + 1). robust estimation...|$|E
40|$|To inform {{planning}} for long-term ecological monitoring, we sampled vegetation and soil-surface attributes across {{a range of}} terrestrial ecosystems (physiognomic types) in seven National Park Service units on the Colorado Plateau. Primary objectives were (1) to evaluate a suite of sampling methods according to measures of repeatability, efficiency, and impacts on plot conditions; and (2) to characterize within- and among-plot variability in monitoring measures. This work was designed to support NPS staff in selecting the combination of methods that best meets their monitoring objectives and resource constraints. We found no differences among cover-estimation techniques in terms of repeatability between observers (measurement precision). Estimates for total live understory canopy cover, cover of individual species, and cover of soil-surface features were highly repeatable between observers for 10 -m 2 quadrats, 1 -m 2 quadrats, and line-point intercept sampling methods. Estimates of shrub and tree density in 10 -m 2 quadrats also were repeatable between observers, although sample sizes for were small for many species. At 10 of 11 ecological sites, we found that sampling with 10 -m 2 quadrats was the most efficient cover-estimation technique with respect to within-plot variability in cover estimates and <b>numbers</b> <b>of</b> <b>subsamples</b> required to estimate plot-level cover with 20 percent precision. According to these same measures, sampling with 1 -m 2 quadrats was the least efficient cover-estimation technique at eight of 11 ecological sites. The line-point technique was most efficient at eight of 11 ecological sites in terms of the amount of time required to estimate total plot-level cover with 20 percent precision – largely because 10 -m 2 quadrats were more time consuming and 1 -m 2 quadrats had greater within-plot variability relative to line-point sampling. However, there was no statistical difference among methods with respect to median subsampling times for 20 percent precision. There also were no differences among methods with respect to mean and median measures of among-plot variability in total live understory canopy cover. But among-plot variability was least for the line-point technique at seven of 11 ecological sites. Sampling activities had greatest impacts on plot conditions at macroplots where there was a high degree of cover by biological and physical soil crusts. Of all sampling procedures, 10 -m 2 quadrat sampling, line-point sampling, and gap-intercept sampling had the most impacts on soil conditions due to trampling of soil crusts by the field team...|$|R
40|$|The {{aim of the}} {{research}} is to determine the frequency, sizestructure, and possible differences of the spinal disorder in the sagital plane in boys from different socioeconomic backgrounds in Montrenegro. Research program included 160 male responedents, of which 80 were from urban of 80 from rural enviroment with average age of 13. 6 years. Assessment of postural status of the spine was performed by using several combined techniques and methods of measurement: somatoscopy method, Adams test, Mattiass test, test of 'voluntary' muscle contraction and higher position test (Koturovic & Jeričevic, 1998; Radisavljevic, 2001; Jovovic, 2008). For all postural variables and their variations have been calculated frequency distribution of interrupted statistical series and the differences between the <b>subsamples</b> <b>of</b> respondents were tested using the chi-square test. The obtained results of {{the research}} indicate that the status of spinal disordersin frontal plane is significantly affected at the large <b>number</b> <b>of</b> responendents. Between <b>subsamples</b> <b>of</b> the respondents have not been confirmed statistically significant differences. It turned out that the largest percentage of deviations form the functional disorders, which can be successfully corrected with adequate application of physical treatments...|$|R
40|$|A {{method for}} the {{identification}} of toxic compounds in industrial wastewater is presented, consisting of sequential solid phase extraction (SPE), fractionation by HPLC and GC-MS for compound identification. All analytical steps are accompanied by an automated detection of the aquatic toxicity by luminescence inhibition of Vibrio fischeri, which helps to reduce the large <b>number</b> <b>of</b> samples and <b>subsamples</b> {{that have to be}} processed by excluding those without toxic effects. The advantages of this procedure in comparison to previous methods of toxicity directed water analysis are discussed. The procedure was successfully applied to various samples of tannery wastewater, showing that benzothiazoles account for the major toxicity of tanyard wastewater. For very polar wastewater con stituents, such as in beamhouse wastewaters, the use of LC-MS/MS for the final compound identification is suggested...|$|R
