0|10000|Public
40|$|In {{this paper}} {{we use a}} {{modified}} LBG algorithm for vector quantization. An evolutionary algorithm was developed to find a quantization that has both not too many reference points and small error. Due {{to the fact that}} computations of LBG are independent from one another the algorithm could be accelerated by making it parallel and distributed. Experiments show that the algorithm usually finds global minimum. I. INTRODUCTION This paper deals with the problem of Vector Quantisation (VQ) : given a large set X={x 1, [...] .,x m } of points from R <b>n</b> (a <b>measurement</b> <b>data)</b> we find only a few representatives W={w 1, [...] .,w k }, called reference points, which form a good approximation of the data set. By "good" we mean that X is in the "vicinity" of the representatives i. e. the distance from each point from X to the closest representative is as small as possible. As a matter of fact this problem is known in statistics as cluster analysis and has numerous statistical applications. Vector quantiza [...] ...|$|R
40|$|Theroblem of {{estimating}} aarameter of {{a quantum}} system {{through a series}} of measurementserformed sequentially on a quantumrobe is analyzed in the general setting where the underlying statistics is explicitly non-i. i. d. Weresent a generalization of the central limit theorem in theresent context, which under fairly general assumptions shows that as the number <b>N</b> of <b>measurement</b> <b>data</b> increases therobability distribution of functionals of the data (e. g., the average of the data) through which the targetarameter is estimated becomes asymptotically normal and independent of the initial state of therobe. At variance with therevious studies (Gu?? M 2011 Phys. Rev. A 83 062324; van Horssen M and Gu?? M 2015 J. Math. Phys. 56 022109) we take a diagrammatic approach, which allows one to compute not only the leading orders in N of the moments of the average of the data but also those of the correlations among subsequent measurement outcomes. Inarticular our analysisoints out that the latter, which are not available in usual i. i. d. data, can be exploited in order to improve the accuracy of thearameter estimation. An explicit application of our scheme is discussed by studying how the temperature of a thermal reservoir can be estimated via sequential measurements on a quantumrobe in contact with the reservoir. publishersversionPeer reviewe...|$|R
40|$|The {{problem of}} {{estimating}} a parameter of a quantum system {{through a series}} of measurements performed sequentially on a quantum probe is analyzed in the general setting where the underlying statistics is explicitly non-i. i. d. We present a generalization of the central limit theorem in the present context, which under fairly general assumptions shows that as the number <b>N</b> of <b>measurement</b> <b>data</b> increases the probability distribution of functionals of the data (e. g., the average of the data) through which the target parameter is estimated becomes asymptotically normal and independent of the initial state of the probe. At variance with the previous studies [M. Guţă, Phys. Rev. A 83, 062324 (2011); M. van Horssen and M. Guţă, J. Math. Phys. 56, 022109 (2015) ] we take a diagrammatic approach, which allows one to compute not only the leading orders in N of the moments of the average of the data but also those of the correlations among subsequent measurement outcomes. In particular our analysis points out that the latter, which are not available in usual i. i. d. data, can be exploited in order to improve the accuracy of the parameter estimation. An explicit application of our scheme is discussed by studying how the temperature of a thermal reservoir can be estimated via sequential measurements on a quantum probe in contact with the reservoir. Comment: 20 pages, 10 figure...|$|R
40|$|A new array type, the γ 11 n arrays are {{introduced}} in this paper, {{in which the}} sequence of the current(C) and potential (P) electrodes is CPCP and {{the distance between the}} last two electrodes is n times the distance between the first two ones and that of the second and third one. These arrays are called quasi null arrays because they are – according to their array and behaviour – between the traditional and null arrays. It is shown by numerical modelling that in detection of small-effect inhomogeneities these configurations may be more effective than the traditional ones including the optimised Stummer configuration. Certain γ 11 n configurations – especially the γ 112, γ 113 and γ 114 – produced better results both in horizontal and vertical resolution investigations. On the basis of the numerical studies, the γ 11 n configurations seem to be very promising in problems where the anomalies are similar to the numerically investigated ones, that is they can detect and characterise, for example,tunnels, caves, cables, tubes, abandoned riverbeds or discontinuity in a clay layer with greater efficacy than those of the traditional configurations. γ 11 <b>n</b> <b>measurements</b> need less <b>data</b> than traditional configurations therefore also the time demand of electrical resistivity tomography (ERT) measurements can be shortened by their use...|$|R
40|$|NA 62 is a {{fixed target}} {{experiment}} at the CERN SPS aiming at {{the search for}} new phenomena beyond the Standard Model (SM) by measuring two of the theoretically most clean processes in kaon decays. Two phases can be distinguished in the NA 62 physics programme. During the current short term phase (data taking completed in years 2007 - 8) we are studying the ratio RK = G(K!ene(g)) G(K!mnm (g)) of leptonic decay rates, which provides a golden probe for testing {{the structure of the}} weak interactions. In the second phase (long term) we will then focus on the measurement of the branching ratio of the very rare kaon decay K+ ! p+n ¯ n aiming at collecting O(100) K+ !p+n ¯ n events with 10 % background in two years of data taking. In this paper we first summarize the status of the RK analysis, based on 40 % of the 2007 NA 62 data sample, and discuss the preliminary result RK = (2 : 500 0 : 016) 105, which breaks {{for the first time the}} 1 % error level and is consistent with the Standard Model. A status report of the longer term project, recently approved at CERN, will then follow, including the current R&D programme and future perspectives for the K+ !p+n ¯ <b>n</b> <b>measurement,</b> for which <b>data</b> taking will start in year 2012...|$|R
5000|$|Given {{a set of}} <b>N</b> <b>measurements</b> [...] {{the mean}} value of z is defined as: ...|$|R
30|$|In the following, we {{calibrate}} the MR-FDPF {{model in}} two cases. The first case is to calibrate the MR-FDPF model {{with all the}} available <b>measurement</b> <b>data,</b> while the second case is to calibrate it with {{only a part of}} the available <b>measurement</b> <b>data.</b> Intuitively, the prediction performance of the MR-FDPF model calibrated with all the <b>measurement</b> <b>data</b> should be better than that calibrated with only a part of the <b>measurement</b> <b>data</b> since more <b>measurement</b> <b>data</b> are used.|$|R
40|$|For {{biological}} arrays, performing multiple experimental repeats is {{a common}} way to improve certainty of gene identification. But this is offset by {{the high cost of}} each experiment. In a typical experiment, <b>n</b> independent <b>measurements</b> (e. g., <b>data</b> from n separate gene expression arrays (Affymetrix, 2002; Lipshutz et al., 1999) are combined to yield a single estimation per assay (e. g., per gene). Thus, 2 <b>n</b> independent <b>measurements,</b> yields only <b>n</b> comparative <b>measurements.</b> We show {{that it is possible to}} compute abstractly the probabilities appropriate for using other subsets of the potential n 2 comparisons derivable from such data. We show the 2 n − 1 independent comparisons available from among the n 2 possibilities perform the best. ...|$|R
5000|$|The {{sample mean}} {{of a set}} of <b>N</b> <b>measurements</b> [...] drawn from a {{circular}} uniform distribution is defined as: ...|$|R
5000|$|Method 1: average the <b>n</b> <b>measurements</b> of T, {{use that}} mean in Eq(2) {{to obtain the}} final g estimate; ...|$|R
40|$|The {{invention}} discloses {{a control}} and diagnostic method for a water chilling unit. The fusion method comprises {{the steps of}} collecting <b>measurement</b> <b>data</b> after direct and indirect measurement of air conditioning load; when direct <b>measurement</b> <b>data</b> are singular values, removing the direct <b>measurement</b> <b>data,</b> and using indirect <b>measurement</b> <b>data</b> to carry out data fusion; or else, after removing measurement noise in collected data through a moving average method, performing data fusion to the <b>data</b> which the <b>measurement</b> noise is removed from, and correcting the indirect <b>measurement</b> <b>data</b> and performing data fusion when system errors occur in the direct measurement data; and finally calculating credibility of fusion values. According to the technical scheme, the direct <b>measurement</b> <b>data</b> and the indirect <b>measurement</b> <b>data</b> are fused, the singular values, the measurement noise {{and the influence of}} the system errors are removed, the accuracy of the <b>measurement</b> <b>data</b> of the air conditioning load is effectively improved, the credibility of the fusion values is improved, and the reliability of controlling a water chilling unit is effectively improved. 本发明公开了一种建筑的空调负荷测量数据的融合方法，包括步骤：直接和间接测量空调负荷之后采集测量数据；当直接测量数据为奇异值，移除所述直接测量数据，使用间接测量数据进行数据融合；否则通过移动平均的方法移除所述采集的数据中的测量噪音后，将所述移除测量噪音后的数据进行数据融合，当直接测量数据存在系统误差，校正所述间接测量数据，进行数据融合；最后计算融合值的可信度。本发明的方案中，将直接测量数据与间接测量数据进行融合，移除奇异值、测量噪音及系统误差的影响，有效地提高空调负荷的测量数据的准确度，提高融合值的可信度，进而有效地提高冷水机组控制的可靠性。Department of Building Services EngineeringInventor name used in this publication: 王盛卫Inventor name used in this publication: 黄公胜Inventor name used in this publication: 孙勇军Title in Traditional Chinese: 一種建築的空調負荷測量數據的融合方法Chin...|$|R
25|$|The main {{advantage}} of the information filter is that <b>N</b> <b>measurements</b> can be filtered at each timestep simply by summing their information matrices and vectors.|$|R
50|$|<b>Measurement</b> <b>data</b> {{from other}} sources like TargetLink and Simulink signal logging or MCD-3 <b>measurement</b> <b>data</b> can be {{assessed}} automatically. This data can be independent from the test execution.|$|R
40|$|Methanogenesis can {{indicate}} the fermentation {{activity of the}} gastrointestinal anaerobic flora. Methane also has a demonstrated anti-inflammatory potential. We hypothesized that enriched methane inhalation can influence the respiratory activity of the liver mitochondria after an ischemia-reperfusion (IR) challenge. The activity of oxidative phosphorylation system complexes was determined after in vitro methane treatment of intact liver mitochondria. Anesthetized Sprague-Dawley rats subjected to standardized 60 -min warm hepatic ischemia inhaled normoxic air (n = 6) or normoxic air containing 2. 2 % methane, from 50 min of ischemia and throughout the 60 -min reperfusion period (<b>n</b> = 6). <b>Measurement</b> <b>data</b> were compared with those on sham-operated animals (n = 6 each). Liver biopsy samples were subjected to high-resolution respirometry; whole-blood superoxide and hydrogen peroxide production was measured; hepatocyte apoptosis was detected with TUNEL staining and in vivo fluorescence laser scanning microscopy. Significantly decreased complex II-linked basal respiration {{was found in the}} normoxic IR group at 55 min of ischemia and a lower respiratory capacity (~ 60 %) and after 5 min of reperfusion. Methane inhalation preserved the maximal respiratory capacity at 55 min of ischemia and significantly improved the basal respiration during the first 30 min of reperfusion. The IR-induced cytochrome c activity, reactive oxygen species (ROS) production and hepatocyte apoptosis were also significantly reduced. The normoxic IR injury was accompanied by significant functional damage of the inner mitochondrial membrane, increased cytochrome c activity, enhanced ROS production and apoptosis. An elevated methane intake confers significant protection against mitochondrial dysfunction and reduces the oxidative damage of the hepatocytes...|$|R
40|$|The paper {{presents}} {{results of}} the simulation research aiming at comparison {{of the quality of}} reconstruction of particle size distribution of dispersed phase in particulate systems by solving the inverse problem for nephelometric <b>measurement</b> <b>data</b> and for turbidimetric <b>measurement</b> <b>data</b> corrupted to a varying extent by random errors. In the case of both measurement techniques mathematical models based on Mie light scattering theory were applied. The results obtained demonstrated that the reconstruction on the basis of turbidimetric measurements is characterized by generally bigger accuracy compared to the reconstruction on the basis of nephelometric measurements. The advanatage of the reconstruction based on turbidimetric <b>measurement</b> <b>data</b> over the reconstruction based on nephelometric <b>measurement</b> <b>data</b> increases significantly in the case of <b>measurement</b> <b>data</b> of both kinds affected by random noise. </p...|$|R
3000|$|Since [...] rank(H)< m, {{at least}} <b>n</b> <b>measurements</b> are {{required}} to extract the optimal state estimation and [...] n-m [...] measurement units {{should be used to}} enhance resilience against errors.|$|R
40|$|In {{a method}} and a mobile {{communications}} receiver for performing signal <b>measurements,</b> signal <b>measurement</b> <b>data</b> is received {{from at least}} one base station during {{at least part of}} a measurement period. Each measurement period comprises a number of non-equidistant snapshot measurement windows during which the receiver receives signal <b>measurement</b> <b>data.</b> At least one average signal strength value of the signal <b>measurement</b> <b>data</b> received during the measurement period is compute...|$|R
40|$|Abstract. Reverse {{engineering}} {{systems are}} used to construct mathematical models of physical models such as clay model based on <b>measurement</b> <b>data.</b> In this study, we proposed a reverse engineering method which can construct high quality surface data automatically. This method consists of the following steps; The first globally and regionally smooths measured data based on the target shape by fitting quadric surface to <b>measurement</b> <b>data.</b> The second defines quadric surfaces and converts measurement points into 3 D lattice points to obtain uniform <b>measurement</b> <b>data</b> density. As the positions of <b>measurement</b> <b>data</b> are converted from coordinate values into 3 D lattice points, {{it is easier to}} find neighboring points and clarify neighboring relations between surfaces. The third acquires segment <b>measurement</b> <b>data</b> based on maximum curvatures and normals at each point. The last defines NURBS surfaces for each segment using the least square method to average positional errors. In order to validate the effectiveness of the proposed method, we developed a reverse engineering system and constructed mathematical models through basic experiments using clay car model <b>measurement</b> <b>data...</b>|$|R
40|$|In {{environmental}} policies and assessments of environment, <b>measurement</b> <b>data</b> {{such as the}} environmental pollutants are used. In these, however, not only <b>measurement</b> <b>data</b> but also evaluations and image by human Kansei are important. The {{purpose of this study}} is to analyze the numerical data measured in the environment and human image data in the waterside space overall in reservoirs along the Kuzuryu River. In this paper, the <b>measurement</b> <b>data</b> and the human image data are analyzed by the self-organizing map, and the measurement point is classified. Also, it is tried to interpret the classified measurement point from the <b>measurement</b> <b>data</b> and the human image data overall...|$|R
3000|$|N) [7]. If {{there are}} K out of <b>N</b> <b>measurements</b> within the local {{neighbourhood}} window for the ordinary Kriging interpolation, the computational time {{complexity of the}} Kriging method is O(M [...]...|$|R
5000|$|A {{series of}} <b>N</b> <b>measurements</b> [...] {{drawn from a}} wrapped Cauchy {{distribution}} {{may be used to}} estimate certain parameters of the distribution. The average of the series [...] is defined as ...|$|R
3000|$|<b>N</b> <b>measurements</b> {{associated}} with it (i.e. it is not consistent enough with the data). This value is set by noting that {{the average number of}} measurements generated by each target is P [...]...|$|R
5000|$|A {{series of}} <b>N</b> <b>measurements</b> [...] {{drawn from a}} von Mises {{distribution}} {{may be used to}} estimate certain parameters of the distribution. (Borradaile, 2003) The average of the series [...] is defined as ...|$|R
40|$|Abstract. Precise {{matching}} of <b>measurement</b> <b>data</b> with CAD {{model for}} hull block {{is the key}} to accurately analysis manufacturing errors and rationally evaluate manufacturing precision. A algorithm to automatically match <b>measurement</b> <b>data</b> with CAD model is proposed. The <b>measurement</b> <b>data</b> is measured by total station, which is currently used in the shipyard. The algorithm is divided into two steps. One is rough matching and another is refined matching. Rough matching that adopts PCA algorithm can narrow the dislocation between <b>measurement</b> <b>data</b> and CAD model. The pair-wise points are matched by searching minimum distance. Initialized by the former result based on rough matching, refined matching that adopts Euler rotation theory leads to perfect matching by translating and rotating the measured points. Experimental results show that the proposed algorithm can automatically match <b>measurement</b> <b>data</b> with CAD model without prior information on transformation, and evaluates manufacturing precision of hull blocks accurately, and provides an instructive basis for subsequent assembly...|$|R
30|$|Gather the <b>measurement</b> <b>data</b> from PMUs.|$|R
40|$|Abstract—Power system {{dynamics}} prediction {{is important for}} online situation awareness and adaptive control. Model-based simulation is not applicable for dynamics prediction due to constant change of network configuration. With more and more Phasor Measurement Units (PMU), power {{system dynamics}} can be studied with pure <b>measurement</b> <b>data.</b> This paper proposes a dynamics prediction method based on <b>measurement</b> <b>data.</b> A mulit-input multi-output Multivariate AutoRegressive model (MAR) is developed and dynamics prediction procedure is proposed. Both simulation <b>data</b> and field <b>measurement</b> <b>data</b> are tested. Examples show that the proposed method can predict power system dynamics with high accuracy for simulation data and give reasonable result for <b>measurement</b> <b>data.</b> It provides an alternative approach to study power system dynamics. Index Terms—Power systems, dynamics prediction, phasor measurement, multivariate autoregressive I...|$|R
40|$|Segmentation is {{a crucial}} step towards the {{interpretation}} of discrete three-dimensional <b>measurement</b> <b>data.</b> This paper presents a robust method for compound free-form surface segmentation and reconstruction. In the proposed method, a cloud of <b>measurement</b> <b>data</b> are collected through a coordinate measuring machine (CMM). The set of <b>measurement</b> <b>data</b> is then sliced along, at most, three orthogonal directions. On each slicing plane, <b>measurement</b> <b>data</b> is fitted by a 2 D NURBS spline. Now, maximum curvature points on each NURBS spline can be calculated. These points represent the boundary of the digitised object. With the proposed method, three-dimensional segmentation is simplified to a two-dimensional problem. The effectiveness of the proposed segmentation method will be illustrated with examples of a hair drier and a telephone receiver. link_to_subscribed_fulltex...|$|R
40|$|The design, construction, {{and testing}} of a {{complete}} system to produce karyotypes and chromosome <b>measurement</b> <b>data</b> from human blood samples, and a basis for statistical analysis of quantitative chromosome <b>measurement</b> <b>data</b> is described. The prototype was assembled, tested, and evaluated on clinical material and thoroughly documented...|$|R
30|$|Hence, the {{procedure}} of power system mode estimation through CWT {{can be summarized}} as follows: For a <b>measurement</b> <b>data</b> x(t), CWT {{is applied to the}} <b>measurement</b> <b>data</b> to obtain the wavelet coefficients of dominant modes. Then, the modes based on the obtained wavelet coefficients can be detected through (7)-(13).|$|R
30|$|Generally, data {{accuracy}} demands the synchrophasor measurements, such as phasor measurements, frequency and ROCOF estimates, and time synchronization, within acceptable errors; data availability requires the <b>measurement</b> <b>data</b> to be complete, consistent, and without loss; and data timeless {{refers to the}} <b>measurement</b> <b>data</b> delivered to their destinations within acceptable latencies.|$|R
40|$|Even nowadays, a {{great deal}} of <b>measurement</b> <b>data</b> is {{collected}} and also saved manually. In this kind of situation, there are phases when human error can easily occur and also when interpreting the typed collected <b>measurement</b> <b>data</b> could be difficult. This research aimed to discover resources for improving the quality of <b>measurement</b> <b>data</b> as well as better and more illustrative tracking of usage information in real time. The objective was both quality improvement of a specific <b>measurement</b> <b>data</b> collection process as well as the elimination of human error. This paper describes one reliable solution for this purpose, which improves the quality and also the visual presentation of manually collected data. The paper presents elements of the system developed for this aim and also the technology deployed along with its operational principles...|$|R
40|$|We have {{measured}} {{neutron flux}} distribution around the core tank of STACY heterogeneous core by position sensitive proportional counter (PSPC) {{to develop the}} method to measure reactivity for subcritical systems. The neutron flux distribution data in the position accuracy of +- 13 mm have been obtained {{in the range of}} uranium concentration of 50 g/L to 210 g/L both in critical and in subcritical state. The prompt neutron decay constant, alpha, was evaluated from the <b>measurement</b> <b>data</b> of pulsed neutron source experiments. We also calculated distribution of neutron flux and sup 3 He reaction rates at the location of PSPC by using continuous energy Monte Carlo code MCNP. The <b>measurement</b> <b>data</b> was compared with the calculation results. As results of comparison, calculated values agreed generally with <b>measurement</b> <b>data</b> of PSPC with Cd cover in the region above half of solution height, but the difference between calculated value and <b>measurement</b> <b>data</b> was large in the region below half of solution height. On the other hand, calculated value agreed well with <b>measurement</b> <b>data</b> of PSPC without Cd cover...|$|R
40|$|In this article, {{utilizing}} Delphi 7. 0 as {{the development}} platform, we design {{the system to}} collect or record <b>measurement</b> <b>data</b> through electronic digital caliper and digital table, compute control parameters of quality control chart, draw the process control chart of controlled objectives and read, analyze and manage the <b>measurement</b> <b>data...</b>|$|R
40|$|Abstract — It {{has become}} a common {{practice}} for Internet Service Providers (ISPs) to instrument their networks with Network Measurement Infrastructures (NMIs). These NMIs support network-wide ”active ” and ”passive ” <b>measurement</b> <b>data</b> collection and analysis to: 1) identify end-to-end performance bottlenecks in network paths and 2) broadly understand Internet traffic characteristics, on an ongoing basis. In this paper, we present our analysis of the active and passive <b>measurement</b> <b>data</b> collected along network backbone paths within typical campus, regional and national networks which carry traffic of cuttingedge Internet applications such as high-quality voice and video conferencing, multimedia streaming and distributed file sharing. The active <b>measurement</b> <b>data</b> has been obtained by using ”ActiveMon” software, which we have developed and deployed along the above network backbone paths. The passive <b>measurement</b> <b>data</b> has been obtained using SNMP, Syslog and NetFlow data available at the intermediate routers located at strategic points along the same network backbone paths. Our analysis of the <b>measurement</b> <b>data</b> includes studying notable trends, network events and relative performance issues of the network backbone paths which {{are reflected in the}} active and passive <b>measurement</b> <b>data</b> collected regularly over several months. Our results thus provide valuable insights regarding traffic dynamics in the different academic network backbones and can be used for better design and control of networks and also to develop traffic source models based on empirical data from real-networks...|$|R
3000|$|... where y is the {{observed}} vector, x is an s-sparse vector (contains at most s non-zero elements) and Φ is an m×n (m ≪ <b>n)</b> <b>measurement</b> matrix which {{in our case}} is the partial DFT matrix formed by selecting N [...]...|$|R
40|$|AbstractSuppose that n {{points are}} located at n {{mutually}} distinct but unknown {{positions on the}} line, and we can measure their pairwise distances. How many measurements are needed to determine their relative positions uniquely? The problem is motivated by DNA mapping techniques based on pairwise distance measures. It is also interesting by itself for its own and surprisingly deep. Continuing our earlier work on this problem, we give a simple randomized two-round strategy that needs, with high probability, only (1 +o(1)) <b>n</b> <b>measurements.</b> We show that deterministic strategies cannot manage the task in two rounds with (1 +o(1)) <b>n</b> <b>measurements</b> in the worst case. We improve an earlier deterministic bound to roughly 4 n/ 3 measurements...|$|R
