10|11|Public
40|$|This R (R Development Core Team, 2011) package {{contains}} software {{designed to}} recover latent dimensions (i. e. a basic space) from a <b>numeric</b> <b>matrix</b> of data. The initial development {{was conducted by}} Poole (1998), who wrote the Fortran executable that was previously used by social scientists to analyze survey data in publications such as Saiegh (2009) and Abrajan...|$|E
40|$|Matrix {{formalism}} is a map {{integration method}} for ODE solving. It allows to present {{solution of the}} system as sums and multiplications of 2 -indexes <b>numeric</b> <b>matrix.</b> This ap-proach can be easy implement in parallel codes. As the most natural for matrix operation GPU architecture has been chosen. The set of the methods for beam dynamics has been implemented. Particles and envelope dynamics are supported. The computing facilities are located in St. Petersburg State University and presented by the NVIDIA Tesla-based clusters...|$|E
40|$|Based on the {{computation}} of a superset of {{the implicit}} support, implicitization of a parametrically given (hyper) surface {{is reduced to}} computing the nullspace of a <b>numeric</b> <b>matrix.</b> Our approach exploits the sparseness of the given parametric equations and of the implicit polynomial. In this work, we study how this interpolation matrix {{can be used to}} reduce some key geometric predicates on the (hyper) surface to simple numerical operations on the matrix, namely membership and sidedness for given query points. We illustrate our results with examples based on our Maple implementation. Key words: geometric representation, implicitization, linear algebra, sparse polynomial, membership, sidedness operation...|$|E
40|$|Abstract. Large <b>numeric</b> <b>matrices</b> and multidimensional data arrays {{appear in}} many science domains, {{as well as}} in {{applications}} of financial and business warehousing. Common applications include eigenvalue deter-mination of large matrices, which decompose into a set of linear algebra operations. With the rise of in-memory databases it is now feasible to execute these complex analytical queries directly in the database without being restricted by hard disc latencies for random accesses. In this paper, we present a way to integrate linear algebra operations and large matrices as first class citizens into an in-memory database following a two-layered architectural model. The architecture consists of a logical component receiving manipulation statements and linear algebra expressions, and of a physical layer, which autonomously administrates multiple matrix storage representations. A cost-based hybrid storage representation is pre-sented and an experimental implementation is evaluated for matrix-vector multiplications. ...|$|R
40|$|Let A be {{a square}} matrix over an {{arbitrary}} field. The permanent {{of the matrix}} A {{is defined as the}} algebraic sum of the products of any N elements of the matrix, one in each row and column. Symbol: perm A – the permanent of the matrix A. Obviosly, the permanent is similar to the determinant, but without the sign change. From now on only <b>numeric</b> <b>matrices</b> will be considered. The evaluation of the permanent is a difficult mathematical problem. It has been put forward by Binet and Cauchy almost two centuries ago (cf. [1]). The fastest exact algorithm for the general case is Ryser’s one, but it runs in O (N. 2 N) time. It has been proved that this problem is #P-hard, i. e. probably unsolvable in polynomial time. Then researchers have concentrated their efforts on discovering polynomial randomized algorithms. The latest results in this field can be found in [2], [3] and [4]. Many problems of practical importance can be reduced to the evaluation o...|$|R
40|$|Background. Interpretation of {{microarray}} data remains challenging because biological meaning {{should be}} extracted from enormous <b>numeric</b> <b>matrices</b> and be presented explicitly. Moreover, huge public repositories of microarray dataset {{are ready to}} be exploited for comparative analysis. This study aimed to provide a platform where essential implication of a microarray experiment could be visually expressed and various microarray datasets could be intuitively compared. Results. On the semantic space, gene sets from Molecular Signature Database (MSigDB) were plotted as landmarks and their relative distances were calculated by Lin’s semantic similarity measure. By formal concept analysis, a microarray dataset {{was transformed into a}} concept lattice with gene clusters as objects and Gene Ontology terms as attributes. Concepts of a lattice were located on the semantic space reflecting semantic distance from landmarks and edges between concepts were drawn; consequently, a specific geographic pattern could be observed from a microarray dataset. We termed a distinctive geography shared by microarray datasets of the same category as “semantic signature. ” Conclusions. “Semantic space,” a map of biological entities, could serve as a universal platform for comparative microarray analysis. When microarray data were displayed on the semantic space as concept lattices, “semantic signature,” characteristic geography for a microarray experiment, could be discovered...|$|R
40|$|Decomposition of matrix is a {{vital part}} of many {{scientific}} and engineering applications. It is a technique that breaks down a square <b>numeric</b> <b>matrix</b> into two different square matrices and is a basis for efficiently solving a system of equations, which in turn is the basis for inverting a matrix. An inverting matrix is a part of many important algorithms. Matrix factorizations have wide applications in numerical linear algebra, in solving linear systems, computing inertia, and rank estimation is an important consideration. This paper presents review of all the matrix decomposition techniques used in signal processing applications {{on the basis of their}} computational complexity, advantages and disadvantages. Various Decomposition techniques such as LU Decomposition, QR decomposition, Cholesky decomposition are discussed here. Keywords...|$|E
40|$|The {{problem of}} {{computing}} {{the intersection of}} parametric and algebraic curves arises in many applications of computer graphics and geometric and solid modeling. Previous algorithms are based on techniques from elimination theory or subdivision and iteration. The former is however, restricted to low degree curves. This is mainly due to issues of efficiency and numerical stability. In this paper we use elimination theory and express the resultant of the equations of intersection as a matrix determinant. The matrix itself rather than its symbolic determinant, a polynomial, is used as the representation. The problem of intersection is reduced to computing the eigenvalues and eigenvectors of a <b>numeric</b> <b>matrix.</b> The main advantage of this approach lies in its efficiency and robustness. Moreover, the numerical accuracy of these operations is well understood. For almost all cases {{we are able to}} compute accurate answers in 64 bit IEEE floating point arithmetic. Keywords: Intersection, curves, a [...] ...|$|E
40|$|Abstract. In {{this thesis}} we explore recent methods for {{computing}} the Newton polytope of the implicit equation and study their applicability to the representation {{change from the}} parametric form to implicit. Computing a (super) set of the monomials appearing in the implicit equation allows us to determine the interpolation space. Following this phase we implement interpolation by exact or numeric linear algebra (applying singular value decomposition). We evaluate the monomials at the points, most suitable for the task, thus building a <b>numeric</b> <b>matrix,</b> ideally of corank 1, whose kernel vector contains the coefficients of the implicit equation. We propose techniques for handling the case of higher corank. This yields an efficient, output-sensitive algorithm for computing the implicit equation. The method {{can be applied to}} polynomial or rational parameterizations of planar curves or (hyper) surfaces of any dimension including parameterizations with base points. Moreover, this technique can be used for problems such as the computation of the discriminant of a multivariate polynomial or the resultant of a system of multivariate polynomials...|$|E
40|$|Part I - Graphics Fundamentals PC GRAPHICS OVERVIEW History and Evolution Short History of PC Video PS/ 2 Video Systems SuperVGA Graphics Coprocessors and Accelerators Graphics Applications State-of-the-Art in PC Graphics 3 D Application Programming Interfaces POLYGONAL MODELING Vector and Raster Data Coordinate Systems Modeling with Polygons IMAGE TRANSFORMATIONS Matrix-based Representations Matrix Arithmetic 3 D Transformations PROGRAMMING <b>MATRIX</b> TRANSFORMATIONS <b>Numeric</b> Data in <b>Matrix</b> Form Array Processing PROJECTIONS AND RENDERING Perspective The Rendering Pipeline LIGHTING AND SHADING Lighti...|$|R
40|$|Cold fermionic atoms {{with three}} {{different}} hyperfine states confined in optical lattices show pronounced atomic density waves (ADWs). These ADWs are pinned due to the confining potential that traps the atoms in the optical lattice and {{can be considered a}} crystal phase of strongly bound trions (CPT). We show that the crystalline phase is incompressible and robust against anisotropic interactions. We also show that it is generic {{in the presence of the}} trap due to its incompressibility. <b>Numeric</b> density <b>matrix</b> renormalization group calculations show evidence of a crossover between a trionic superfluid and the CPT state as a function of the strength of the interaction and as a function of the anisotropy of the interaction. © 2009 The American Physical Society. Supported in part by Spanish Government Grant No. FIS 2006 - 12783 -C 03 - 01 and by CAM-CSIC Grant No. CCG 07 -CSIC/ESP- 1962. Financed by CSIC and the European Commission through the I 3 p program and by a José Castillejo Grant of the Spanish Ministry of Research. Peer Reviewe...|$|R
40|$|The matrix {{form of the}} {{presentation}} of the genetic code is described as the cognitive form to analyze structures of the genetic code. A similar matrix form is utilized in the theory of signal processing. The Kronecker family of the genetic matrices is investigated, which is based on the genetic matrix [C A; U G], where C, A, U, G are the letters of the genetic alphabet. This matrix in the third Kronecker power is the (8 * 8) -matrix, which contains 64 triplets. Peculiarities of the degeneracy of the vertebrate mitochondria genetic code are reflected in the symmetrical black-and-white mosaic of this genetic (8 * 8) -matrix. This mosaic matrix is connected algorithmically with Hadamard matrices unexpectedly, which are famous in the theory of signal processing, spectral analysis, quantum mechanics and quantum computers. A special decomposition of <b>numeric</b> genetic <b>matrices</b> reveals their close relations with a family of 8 -dimensional hypercomplex numbers (not Cayley's octonions). Some hypothesis and thoughts are formulated {{on the basis of these}} phenomenological facts. Comment: 26 pages; 21 figures; added materials and reference...|$|R
40|$|Cataloged from PDF {{version of}} article. In this study, we develop and {{demonstrate}} an efficient self-consistent calculation schema that computes the electronic structure and optical properties {{of a single}} exciton in a spherical quantum dot (QD) with an interacting pair of electron and hole wave functions. To observe modifications on bands, wave functions, and energies due to the attractive Coulomb potential, the full <b>numeric</b> <b>matrix</b> diagonalization technique is employed to determine sublevel energy eigenvalues and their wave functions in effective mass approximation. This treatment allows to observe that the conduction and valance band edges bend, that the electron and hole wave functions strongly localize in the QD, and that the excitonic energy level exhibits redshift. In our approach for the Coulomb term between electron and hole, the Poisson-Schrodinger equations are solved self-consistently in the Hartree approximation. Subsequently, exciton binding energies and associated optical properties are computed. The results are presented {{as a function of}} QD radii and photon energies. We conclude that all of these numerical results are in agreement with the experimental studies. (C) 2009 American Institute of Physics...|$|E
40|$|In this study, {{we develop}} and {{demonstrate}} an efficient self-consistent calculation schema that computes the electronic structure and optical properties {{of a single}} exciton in a spherical quantum dot (QD) with an interacting pair of electron and hole wave functions. To observe modifications on bands, wave functions, and energies due to the attractive Coulomb potential, the full <b>numeric</b> <b>matrix</b> diagonalization technique is employed to determine sublevel energy eigenvalues and their wave functions in effective mass approximation. This treatment allows to observe that the conduction and valance band edges bend, that the electron and hole wave functions strongly localize in the QD, and that the excitonic energy level exhibits redshift. In our approach for the Coulomb term between electron and hole, the Poisson-Schrödinger equations are solved self-consistently in the Hartree approximation. Subsequently, exciton binding energies and associated optical properties are computed. The results are presented {{as a function of}} QD radii and photon energies. We conclude that all of these numerical results are in agreement with the experimental studies. © 2009 American Institute of Physics...|$|E
40|$|We revisit implicitization by {{interpolation}} {{in order}} to examine its properties in the context of sparse elimination theory. Based on the computation of a superset of the implicit support, implicitization is reduced to computing the nullspace of a <b>numeric</b> <b>matrix.</b> The approach is applicable to polynomial and rational parameterizations of curves and (hyper) surfaces of any dimension, including the case of parameterizations with base points. Our support prediction is based on sparse (or toric) resultant theory, {{in order to}} exploit the sparsity of the input and the output. Our method may yield a multiple of the implicit equation: we characterize and quantify this situation by relating the nullspace dimension to the predicted support and its geometry. In this case, we obtain more than one multiples of the implicit equation; the latter can be obtained via multivariate polynomial gcd (or factoring). All of the above techniques extend to the case of approximate computation, thus yielding a method of sparse approximate implicitization, which is important in tackling larger problems. We discuss our publicly available Maple implementation through several examples, including the benchmark of bicubic surface. For a novel application, we focus on computing the discriminant of a multivariate polynomial, which characterizes the existence of multiple roots and generalizes the resultant of a polynomial system. This yields an efficient, output-sensitive algorithm for computing the discriminant polynomial...|$|E
40|$|This {{thesis is}} focused on extracting {{meaningful}} statistics for the characterization and classification of biological, medical, and financial data and contains four chapters. The first chapter contains theoretical background on scaling and wavelets, which supports the work in chapters two and three. In the second chapter, we outline a methodology for representing sequences of DNA nucleotides as <b>numeric</b> <b>matrices</b> in order to analytically investigate important structural characteristics of DNA. This methodology involves assigning unit vectors to nucleotides, placing the vectors into columns of a matrix, and accumulating across the rows of this matrix. Transcribing the DNA in this way allows us to compute the 2 -D wavelet transformation and assess regularity characteristics of the sequence via {{the slope of the}} wavelet spectra. In addition to computing a global slope measure for a sequence, we can apply our methodology for overlapping sections of nucleotides to obtain an evolutionary slope. In the third chapter, we describe various ways wavelet-based scaling may be used for cancer diagnostics. There were nearly half of a million new cases of ovarian, breast, and lung cancer in the United States last year. Breast and lung cancer have highest prevalence, while ovarian cancer has the lowest survival rate of the three. Early detection is critical for all of these diseases, but substantial obstacles to early detection exist in each case. In this work, we use wavelet-based scaling on metabolic data and radiography images in order to produce meaningful features to be used in classifying cases and controls. Computer-aided detection (CAD) algorithms for detecting lung and breast cancer often focus on select features in an image and make a priori assumptions about the nature of a nodule or a mass. In contrast, our approach to analyzing breast and lung images captures information contained in the background tissue of images as well as information about specific features and makes no such a priori assumptions. In the fourth chapter, we investigate the value of social media data in building commercial default and activity credit models. We use random forest modeling, which has been shown in many instances to achieve better predictive accuracy than logistic regression in modeling credit data. This result is of interest, as some entities are beginning to build credit scores based on this type of publicly available online data alone. Our work has shown that the addition of social media data does not provide any improvement in model accuracy over the bureau only models. However, the social media data on its own does have some limited predictive power. Ph. D...|$|R
40|$|Matrix {{computing}} {{has played}} {{an important part in}} <b>numeric</b> computing. Sparse <b>matrix</b> is a type of matrix that used in all kinds of computing, and sparse matrix computing has significant meaning in the different fields. Sparse Matrix computing includes so many different operations, for example, addition, Scalar multiplication, Transpose. This report will discuss the Matrix-vector multiplication {{that is one of the}} most important computations in the science computation, and it always involves enormous computation, therefore there is a need to using parallel technology to implement them. This report focuses on how to implement the sparse matrix vector multiplication using parallel in detail; it will base on sparse matrix’s low computation and higher communication characteristics under parallel computation and will introduce a simple algorithm with parallel computing, and display the results when using different number of processors...|$|R
40|$|Before we can mine data, it is {{important}} to first find a suitable data representation that facilitates computational analysis. For example, for complex data like text, sequences, images, and so on, we must typically extract or construct a suitable set of attributes or features, so that we may represent the data instances as vectors in a suitable d-dimensional real space Rd. That is, given a data instance x (e. g., a sequence), we need to find a mapping φ, sothatφ(x) ∈ Rd is the vector representation of x. Weuse the term feature space to refer to the d-dimensional space of mapped vectors φ(x). This mapping facilitates the probabilistic and geometric view-points of data, and allow us to analyze the complex data instances via numeric analysis methods. Eveninthe case when the input data is already {{in the form of a}} data matrix, ifwewishtodiscover non-linear relationships, then we have to find a non-linear mapping φ, sothatφ(x) represents a vector in the new non-linear feature space. Thus givenasetofdata instances xi, andgiventhemappingfunctionφ, wecantransformtheinputdatainto a n × d <b>numeric</b> data <b>matrix</b> {φ(xi) } n i= 1,whereφ(xi) is a vector in the d-dimensional feature space. Example 5. 1 (Sequence Features) : Consider a dataset of DNA sequences over the alphabet Σ={A, C, G, T}. Onesimplefeaturespaceistorepresenteachsequence in terms of the probability distribution over symbols in Σ. For example, if x = ACAGCAGT A, sincethesymbolA occurs 4 times, C and G occur twice, and T occurs once, we hav...|$|R
40|$|Given {{contingency}} {{tables with}} observed marginals, ecological inference (ei) models estimate each internal cell value for each table. Quinn’s dynamic ei model estimates a dynamic Bayesian model for 2 × 2 tables with temporal dependence across tables (units). The model is implemented using a Markov Chain Monte Carlo algorithm (via {{a combination of}} slice and Gibbs sampling). For a hierarchical Bayesian implementation of ei see Quinn’s dynamic ei model (Section??). For contingency tables larger than 2 rows by 2 columns, see R×C ei (Section??). Syntax> z. out x. out s. out <- sim(z. out, x = x. out) Inputs ˆ t 0, t 1 : numeric vectors (either counts or proportions) containing the column margins of the units to be analyzed. ˆ x 0, x 1 : numeric vectors (either counts or proportions) containing the row margins of the units to be analyzed. ˆ N: total counts in each contingency table (unit). If t 0,t 1, x 0 and x 1 are proportions, you must specify N. Additional Inputs In addition, zelig() accepts the following additional inputs for ei. dynamic to monitor the convergence of the Markov chain: ˆ burnin: number of the initial MCMC iterations to be discarded (defaults to 5, 000). ˆ mcmc: number of the MCMC iterations after burnin (defaults to 50, 000). ˆ thin: thinning interval for the Markov chain. Only every thin-th draw from the Markov chain is kept. The value of mcmc must be divisible by this value. The default value is 1. ˆ verbose: defaults to FALSE. If TRUE, {{the progress of the}} sampler (every 10 %) is printed to the screen. 1 ˆ seed: seed for the random number generator. The default is NA which corresponds to a random seed of 12345. The model also accepts the following additional arguments to specify priors and other parameters: ˆ W: a p × p <b>numeric</b> <b>matrix</b> describing the structure of the temporal dependence among elements of θ 0 and θ 1. The default value is 0, which constructs a weight matrix corresponding to random walk priors for θ 0 and θ 1 (assuming that the tables are equally spaced throughout time, and that the elements of t 0, t 1,x 0,x 1 are temporally ordered). ˆ a 0 : a 0 / 2 is the shape parameter for the Inverse Gamma prior on σ 2 0. The default i...|$|E
40|$|Abstract Background The genomic {{estimated}} breeding values (GEBV) of {{the young}} individuals in the XIV QTL-MAS workshop dataset were predicted by three methods: best linear unbiased prediction with a trait-specific marker-derived relationship matrix (TABLUP), ridge regression best linear unbiased prediction (RRBLUP), and BayesB. Methods The TABLUP method {{is identical to the}} conventional BLUP except that the <b>numeric</b> relationship <b>matrix</b> is replaced with a trait-specific marker-derived relationship matrix (TA). The TA matrix was constructed based on both marker genotypes and their estimated effects on the trait of interest. The marker effects were estimated in a reference population consisting of 2 326 individuals using RRBLUP and BayesB. The GEBV of individuals in the reference population as well as 900 young individuals were estimated using the three methods. Subsets of markers were selected to perform low-density marker genomic selection for TABLUP method. Results The correlations between GEBVs from different methods are over 0. 95 in most scenarios. The correlations between BayesB using all markers and TABLUP using 200 or more selected markers to construct the TA matrix are higher than 0. 98 in the candidate population. The accuracy of TABLUP is higher than 0. 67 with 100 or more selected markers, which is nearly equal to the accuracy of BayesB with all markers. Conclusions TABLUP method performed nearly equally to BayesB method with the common dataset. It also provides an alternative method to predict GEBV with low-density markers. TABLUP is therefore a promising method for genomic selection deserving further exploration. </p...|$|R
40|$|An unmistakable {{trend in}} {{embedded}} systems is {{the growth of}} soft real-time computing. A soft real-time application is one for which deadlines can occasionally be missed, but the probability of this event has to be controllable and predictable. This work is aimed {{to close the gap}} in the research of stochastic real-time analysis related to resource reservation scheduling algorithms. This dissertation attempts to: 1. give a quick overview of classic real-time analysis 2. analyze the problems related to use the well-known techniques in the context of soft real-time applications: • overvalue the assignation of parameters as in hard real- time systems based on worst case execution times • time and memory complexity using the known theoretical stochastic analysis 3. propose solutions able to overcome the limitation showed in point 2 4. show some specific examples (theoretical and practical) in which resource reservation lead to advantages. The novel contributions of this thesis are: • a new bound to predict the probability of a deadline misses in a resource reservation systems • a very efficient <b>numeric</b> solution for <b>matrix</b> generated with well-know abstraction models of reservation based on Quasi Birth Death Markov Process • an analytical solution, with some conservative approximations, for the same models. • a new model for specific applications, like interrupts. • experiments using resource reservation in different contexts The thesis is evolved following two different approaches: 1. the first based on the exact model of reservation, and the contributions is: • define a new pessimistic bound, efficient in term of computation, able to overcome the problem of complete knowledge of the computation time. The solution is an approximation of the real solution of the model. 2. the second based on an approximation model in which the novel contributions are: • presents an exact and numeric efficient solution for the model based on Quasi Birth and Death Markov Process • introduces an approximate analytical solution which can be computed with no complexity and which is reversible These techniques are applicable since the minimum interarrival of a request is greater than a server period. Unfortunately exists situations in which this assumption is not feasible. An important example is using resource reservation to scheduling interrupts. In order to consider also this situation, another important novel result of this thesis is: • to introduce a new model for scheduling interrupts In addition, some practical examples of using resource reservation are presented. ...|$|R

