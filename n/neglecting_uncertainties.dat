5|129|Public
40|$|The {{distribution}} of cadmium {{in the modern}} ocean gives important information about ocean circulation and nutrient distributions. As the Cd/Ca-ratio of foraminiferal shells is proportional to that of seawater, Cd/Ca-records of benthic foraminifera {{have been used to}} reconstruct the Glacial oceanic distributions of dissolved Cd and PO 4 quantitatively. The results, in turn, have served as boundary conditions for ocean carbon cycle models. This quantitative reconstruction, however, requires that the present oceanic Cd-PO 4 relationship and the distribution coefficient converting Cd/Ca-ratios into dissolved Cd and PO 4 concentrations, remain constant through geological time. We have constructed a multi-box model (MENU: metal-nutrients) to test various hypotheses about the biogeochemical mechanisms determining the modern oceanic Cd-PO 4 relationship and its behaviour in time. The results indicate that the oceanic Cd-PO 4 relatiohship may change in response to changing oceanic conditions, such as circulation, productivity changes and upwelling intensity, for example during a glacial period. In addition, we discuss the frequent omission in the literature of statistical considerations pertaining to regression analysis, resulting in an unduly optimistic estimate of the distribution coefficient, thereby <b>neglecting</b> <b>uncertainties</b> on the order of 30 % or more. In combination with spatial variability of the distribution coefficient, this conceivably poses intrinsic restrictions to its use as a conversion factor. We conclude that, awaiting a significant reduction of the aforementioned uncertainties, interpretation of the sedimentary Cd/Ca record should be performed only qualitatively. ...|$|E
40|$|Partial or total {{progressive}} collapse under abnormal loading conditions (e. g. deliberate terrorist attacks, uncontrolled gas releases, and vehicle or aircraft impacts) {{is one of}} the most vivid examples of low probability-high consequence (LPHC) event that may occur in the lifetime of a structure. Despite this, structural safety for extreme loads that may lead to disproportionate (or progressive) collapse has been probabilistically assessed and controlled in a few cases, thus <b>neglecting</b> <b>uncertainties</b> in loads and system capacity. As such, this paper moves from a deterministic to a probabilistic framework, proposing fragility models at multiple damage states for low-rise reinforced concrete (RC) framed bare structures which may be applied for {{progressive collapse}} risk assessment and management. Two building classes representative of structures designed for either gravity loads or earthquake resistance in ac-cordance with current European rules were investigated. Monte Carlo (MC) simulation was used to generate random realizations of two-dimensional (2 D) and three-dimensional (3 D) structural models. Their fiber-based finite element (FE) representations were developed within an open source platform for nonlinear static pushdown analysis. The output consisted of fragility functions for each damage state of interest. Such fragility models were then compared to those derived through incremental dynamic analysis (IDA) in a previous study. IDA-based and pushdown-based capacities were additionally used to propose regression models for quick estimation of dynamic amplification factor (DAF) at a given displacement/drift target. The analysis results show a significant influence of both seismic design/detailing and secondary beams on robustness of the case-study building classe...|$|E
40|$|We {{estimated}} and modelled how uncertainties in stochastic {{population dynamics}} and biases in parameter estimates affect {{the accuracy of}} the projections of a small island population of song sparrows which was enumerated every spring for 24 years. The estimate of the density regulation in a theta-logistic model (theta = 1. 09 suggests that the dynamics are nearly logistic, with specific growth rate r 1 = 0. 99 and carrying capacity K = 41. 54. The song sparrow population was strongly influenced by demographic (ŝigma 2 (d) = 0. 66) and environmental (ŝigma 2 (d) = 0. 41) stochasticity. Bootstrap replicates of the different parameters revealed that the uncertainties in the estimates of the specific growth rate r 1 and the density regulation theta were larger than the uncertainties in the environmental variance sigma 2 (e) and the carrying capacity K. We introduce the concept of the population prediction interval (PPI), which is a stochastic interval which includes the unknown population size with probability (1 - alpha). The width of the PPI increased rapidly with time because of uncertainties in the estimates of density regulation as well as demographic and environmental variance in the stochastic population dynamics. Accepting a 10 % probability of extinction within 100 years, <b>neglecting</b> <b>uncertainties</b> in the parameters will lead to a 33 % overestimation of the time it takes for the extinction barrier (population size X = 1) to be included into the PPI. This study shows that ignoring uncertainties in population dynamics produces a substantial underestimation of the extinction risk...|$|E
60|$|A further {{attempt was}} made to roughly {{appraise}} the <b>neglected</b> <b>uncertainties</b> relating to the outside conditions, but large as they are, they seem much inferior in their joint effect to the magnitude of that just discussed.|$|R
5000|$|Your {{model is}} but one of the {{plausible}} models - you <b>neglected</b> model <b>uncertainty</b> ...|$|R
40|$|Based on reconstructions of past {{temperatures}} from proxy data, Hegerl et al. estimate {{a confidence}} interval for climate sensitivity that suggests a substantially reduced probability {{of very high}} climate sensitivity compared with previous empirical estimates. Here I show that the inference procedure used by Hegerl et al. <b>neglects</b> <b>uncertainties</b> in temperature reconstructions and in the estimated climate sensitivity and can even be used to infer that the climate sensitivity is zero with vanishing uncertainty. Similar procedures based on temperature reconstructions from proxy data generally underestimate uncertainties in climate sensitivity...|$|R
40|$|In {{conventional}} design approaches, structural safety is assessed without considering extreme load conditions that may cause global system collapse {{as a consequence}} of local failures in a component or localized portion of the structure. Even when extreme conditions are taken into account, structural safety is not probabilistically assessed and controlled, thus <b>neglecting</b> <b>uncertainties</b> in loads and system capacity. A few studies have been carried out so far, emphasizing the need for probabilistic risk assessment and management of structures to disproportionate (or progressive) collapse. To this aim, fragility analysis may be used to predict the probability of progressive collapse given that local damage has occurred. This approach is a well-established tool in earthquake engineering and explicitly accounts for uncertainties in both demand and capacity, providing the probability of failure as a function of a given intensity measure. In this paper, fragility functions for low-rise reinforced concrete (RC) framed building structures are presented to be implemented in progressive collapse risk assessment. Two building classes representative of European buildings designed for gravity loads and earthquake resistance in accordance with Eurocodes 2 and 8, respectively, were investigated. Fiber-based finite element (FE) models were developed and integrated with numerical techniques able to simulate the removal of first-story columns within an open source platform. Nonlinear response, resisting mechanisms and damage patterns under sudden column loss scenarios were reproduced at both local and global structural levels. Based on statistics and probability distribution functions for geometry, material properties and loads of the case-study building classes, Monte Carlo simulation was performed to generate random realizations of structural models. Fragility functions at multiple damage states show a significant influence of both seismic design/detailing and secondary beams on robustness of the case-study RC building classes...|$|E
40|$|Uncertainty is an {{inherent}} characteristic of construction projects. <b>Neglecting</b> <b>uncertainties</b> associated with different input parameters {{in the planning}} stage could well lead to misleading and/or unachievable project schedules. Many {{attempts have been made}} in the past to account for uncertainty during planning for construction projects and many tools and techniques were presented to facilitate modelling of such uncertainty. Some of the presented techniques are widely accepted and used frequently like Project Evaluation and Review Technique (PERT) and Monte Carlo Simulation, while others are more complicated and less popular, such as fuzzy set-based scheduling. Although accounting for uncertainty has been a topic of interest for more than four decades, it was rarely attempted to account for uncertainty when scheduling repetitive construction projects. Repetitive projects impose an additional challenge to the already complicated construction scheduling process that accounts for the need to maintain crew work continuity throughout project execution. This special characteristic necessitates producing scheduling techniques specifically suited to resource driven scheduling. Therefore, the main objective of this research is to produce a comprehensive scheduling, monitoring and control methodology for repetitive construction projects that is capable of accounting for uncertainties in various input parameters, while allowing for optimized acceleration and time-cost trade-off analysis. The proposed methodology encompasses three integrated models; Optimized Scheduling and Buffering Model, Monitoring and Dynamic Rescheduling Model and Acceleration Model. The first model presents an optimization technique that accounts for uncertainty in input parameters. It employs a modified dynamic programming technique that utilizes fuzzy set theory to model uncertainties. This model includes a schedule defuzzification tool and a buffering tool. The defuzzification tool converts the optimized fuzzy schedule into a deterministic one, and the buffering tool utilizes user’s required level of confidence in the produced schedule to build and insert time buffers, thus providing protection against anticipated delays affecting the project. The Monitoring and Dynamic Rescheduling Model capitalizes on the repetitive nature of these projects, by using actual progress on site to reduce uncertainty in the remaining part of the schedule. This model also tracks project progress through comparing the actual buffer consumption to the planned buffer consumption. The Acceleration Model presents an iterative unit based optimized acceleration procedure. It comprises a modified algorithm for identifying critical units of the project to accelerate. This model presents queuing criteria that accounts for uncertainty in additional cost of acceleration and for contractor’s judgment in relation to prioritizing critical units for acceleration. Moreover, this model offers six strategies for schedule acceleration and maintains crew work continuity. Together, the three developed models offer an integrated system that is capable of accounting for uncertainty in different variables through different project stages, aiming at helping managers keep repetitive construction projects on track. The presented optimization technique is automated in an Object Oriented program; coded in C# programming language. A number of case studies are analyzed and presented to demonstrate and validate the capabilities and features of the presented methodology. ...|$|E
30|$|Most of {{the real}} options {{literature}} has concentrated on market environments without strategic interactions. On the other hand, the industrial organization literature endogenizes market structure; so far it usually <b>neglects</b> <b>uncertainty</b> and so the option value of flexibility. Considering the linkage effects of both uncertainty and competition, this paper extends a method for assessing technology investment decisions in an oligopolistic market structure. It combines the game-theoretic models of strategic market interactions with a real options approach to investment under uncertainty, and gives an improved comprehension {{of the results of}} uncertainty and competition on the strategic exercise of real options inserted in technology investments.|$|R
30|$|In this problem, {{the vector}} a_t(Ψ_i,j^k) denotes the {{transmit}} antenna array response evaluated at angle Ψ_i,j^k∈ T_j^k; here, the same antenna array response {{as in the}} CSI feedback dictionary (10) is used. Notice, we account for the angular uncertainty only in the interference terms, {{but not in the}} signal term. This is because the system reacts much more sensitive w.r.t uncertainty in the interference directions as compared to the signal direction, since nulls in the beamforming pattern are commonly very narrow whereas peaks are comparatively broad in the angular domain (see also the example in Fig.  2). Another reason for <b>neglecting</b> <b>uncertainty</b> in the angular direction of the intended signal is to keep the problem convex. 3 Problem (P 4) can be brought into the form of a second-order cone program, similar to Problem (P 3).|$|R
40|$|Abstract − Self-heat of {{platinum}} resistance thermometers (PRTs) is {{a well-known}} phenomenon that occurs when measurement current additionally heats up a PRT sensor. This temperature increase depends on measurement current, PRT design, operating temperature and surrounding medium. Self-heat temperature increase can be corrected with some residual uncertainty, but this applies mainly to calibration of PRTs, while in practical temperature measurements self-heat measurement or estimation is difficult due to poor temperature stability and/or short measurement time that does not allow temperature transient effects to fade away. If not handled properly, self-heat uncertainty {{can be one of}} the largest, but often <b>neglected</b> <b>uncertainty</b> contributions in temperature measurement. A study of uncertainty optimization is presented for a measurement system composed of up to twenty PRTs that are connected to the ohmmeter via a scanner and sequentially measured. The optimal measurement procedure is discussed and the uncertainty analysis is given...|$|R
40|$|URL] audienceLittle {{is known}} about the {{distribution}} of the 'value-at-risk' (VaR) estimate and the associated estimation risk. In the case of the normal VaR, the key problem {{comes from the fact that}} it is estimated using a couple of parameters whose estimates are distributed differently. Previous research has either <b>neglected</b> <b>uncertainty</b> around the mean parameter, or resorted to simulations. By contrast, this paper derives analytical results for the normal VaR with the help of asymptotic theory and the so-called 'delta method'. Properties of the estimation errors are then explored in detail and the VaR estimation risk is broken down into its various components. It is then shown, among other things, that the fraction of error owing to mean uncertainty is limited in a prudential context. In other words, the approximate approach defended by Jorion and Chappell and Dowd is shown to still be relevant...|$|R
3000|$|In this subsection, we {{investigate}} how accurately the AEGP metric (17) approximates the EGP from (13). As a reference, we compare the accuracy with the predicted GP (PGP) introduced in [20] and the IC- κESM introduced in [19]. The PGP is obtained by <b>neglecting</b> the <b>uncertainty</b> on H given the actual CSI, and is calculated by substituting H by μ [...]...|$|R
40|$|Face {{detection}} using components {{has been}} proved to produce superior results due to its robustness to occlusions and pose and illumination changes. A first level of processing {{is devoted to the}} detection of individual components, while a second level deals with the fusion of the component detectors. However, the fusion methods investigated up to now <b>neglect</b> the <b>uncertainties</b> that characterize the component locations...|$|R
40|$|Ontologies {{have become}} {{ubiquitous}} in currentgeneration information systems. An ontology is an explicit, formal {{representation of the}} entities and relationships that can exist in a domain of application. Following a well-trodden path, initial research in computational ontology has <b>neglected</b> <b>uncertainty,</b> developing almost exclusively {{within the framework of}} classical logic. As appreciation grows of the limitations of ontology formalisms that cannot represent uncertainty, the demand from user communities increases for ontology formalisms with the power to express uncertainty. Support for uncertainty is essential for interoperability, knowledge sharing, and knowledge reuse. Bayesian ontologies are used to describe knowledge about a domain with its associated uncertainty in a principled, structured, sharable, and machine-understandable way. This paper considers Multi-Entity Bayesian Networks (MEBN) as a logical basis for Bayesian ontologies, and describes PR-OWL, a MEBN-based probabilistic extension to the ontology language OWL. To illustrate the potentialities of Bayesian probabilistic ontologies in the development of AI systems, we present a case study in information security, in which ontology development played a key role. ...|$|R
40|$|Traditional systems {{engineering}} methods {{for the performance}} evaluation of manufacturing systems assume that machine reliability parameters (Mean Time to Failure and Mean Time to Repair) are precisely known. However, in practical situations, these parameters are either estimated from real life data or based on experts’ knowledge. In both cases, {{they are subject to}} uncertainty. This paper proposes for the first time an approach for the performance evaluation of unreliable manufacturing systems that considers uncertain machine parameter estimates. The proposed method is based on the combined use of Bayesian estimation, probability density function discretization and existing decomposition-based techniques for analyzing manufacturing lines composed of unreliable machines and capacitated buffers. Numerical results show that <b>neglecting</b> <b>uncertainty</b> in the input parameter estimates generates consistent errors in the output performance measure estimates, thus making the consequent system design and operation decisions sub-performing. An industrial case is proposed to show the benefits of this method in real production settings...|$|R
40|$|The {{literature}} on taxation of rents from nonrenewable resources uses different theoretical assumptions and methods {{and a variety}} of empirical observations to arrive at widely diverging conclusions. Many studies use models and methods which disregard uncertainty, investigating distortionary effects of different taxes on whether, when, and how to explore for, develop and operate resource deposits. Introducing uncertainty into the analysis opens a range of challenges, and leads to results which cast doubt upon the relevance of studies which <b>neglect</b> <b>uncertainty.</b> There are, however, several ways to analyze uncertainty, regarding companies' behavior, resource price processes, and diversification opportunities, all with different implications for taxation. Methods developed in financial economics since the 1980 's are promising, but still not in widespread use. Some more specific topics covered in this review are optimal risk sharing between companies and gov- ernments, time consistency and scal stability, the relationship between taxes and discount rates, and transfer pricing. Natural resources; rent tax; royalty; oil; minerals; energy...|$|R
40|$|The {{principal}} {{difficulty in}} using {{nuclear magnetic resonance}} (NMR) data for biomolecular structure determination {{is not so much}} experimental imperfections but approximate theories relating structure to measurands. Furthermore, these theories are incomplete as they involve auxiliary parameters which are not measurable. In order to give a reliable picture of a biomolecule, structure determination methods need to determine unknown parameters from definite rules and ought to provide the uncertainty of the derived coordinates. Conventional approaches <b>neglect</b> <b>uncertainties</b> of any kind and therefore by definition fail to give an estimate of structural reliability. In order to deal with auxiliary parameters, they resort to heuristics which renders an objective interpretation of the generated atom positions impossible. Recently, we have introduced a fully probabilistic approach to structure determination from NMR data. We describe here an extension of this approach which incorporates inconsistent nuclear Overhauser effect and J-coupling measurements. Auxiliary parameters are estimated along with the atomic coordinates using Markov Chain Monte Carlo. We apply the method to data sets for two small proteins...|$|R
60|$|The Prince {{at first}} took {{the answer for}} a compliment; but Mr. Beckendorff not returning, he began to have a faint {{idea that he was}} <b>neglected.</b> In this <b>uncertainty</b> he rang the bell for his friend Clara.|$|R
40|$|It {{has been}} {{suggested}} that parton densities global analysis is insensitive to the in-clusion of correlated systematics. In this work we discuss this statistical issue by comparing two parton sets: the first is obtained including correlations; the sec-ond is produced <b>neglecting</b> correlated <b>uncertainties</b> and simply adding in quadra-ture all systematics. Both parton sets have been determined, from a set of purely deep-inelastic scattering data, using the NNPDF method, based on a Monte Carl...|$|R
40|$|We {{live in an}} {{uncertain}} world, yet {{a lot of research}} into the sustainability of welfare states is done in the context of certainty. There are good reasons why the analysis is mostly confined to a model of a certain world. A full analysis of the sustainability of welfare states which includes all relevant economic interactions is already intricate in a certain world because it requires the use of complex dynamic general equilibrium models. Even without stochastics, understanding all the mechanisms and its results is sometimes difficult. In addition, when building stochastics into these type of models one may run into the limitations of computer capacity. In this paper we investigate whether uncertainty on the real rate of return on capital and productivity growth (labelled as economic uncertainty) is more or less important than mortality and fertility uncertainty (labelled as demographic uncertainty) for a consumer facing a decision how much to save. Furthermore we look at the errors that are made when <b>uncertainty</b> is <b>neglected</b> in consumer behaviour. The results indicate that economic uncertainty is far more important than demographic uncertainty. The welfare costs of <b>neglecting</b> <b>uncertainty</b> in consumer behaviour seem to be small. ...|$|R
40|$|Precautionary {{management}} for fish stocks {{in need of}} recovery requires that likely stock increases can be distinguished from model artefacts and that the uncertainty of stock status can be handled. Yet, ICES stock assessments are predominantly deterministic and many EC management plans are designed for deterministic advice. Using the eastern Baltic cod (Gadus morhua) stock as an example, we show how deterministic scientific advice can lead to illusive certainty of a rapid stock recovery and management decisions taken in unawareness of large uncertainties in stock status. By (i) performing sensitivity analyses of key assessment model assumptions, (ii) quantifying {{the uncertainty of the}} estimates due to data uncertainty, and (iii) developing alternative stock and ecosystem indicators, we demonstrate that estimates of recent fishing mortality and recruitment of this stock were highly uncertain and show that these uncertainties are crucial when combined with management plans based on fixed reference points of fishing mortality. We therefore call for fisheries management that does not <b>neglect</b> <b>uncertainty.</b> To this end, we outline a four-step approach to handle uncertainty of stock status in advice and management. We argue {{that it is time to}} use these four steps towards an ecosystem-based approach to fisheries management...|$|R
40|$|The {{measured}} B̅→ D^(*) l ν̅ decay {{rates for}} light leptons (l=e,μ) constrain all B̅→ D^(*) semileptonic form factors, by including both the leading and O(Λ_QCD/m_c,b) subleading Isgur-Wise {{functions in the}} heavy quark effective theory. We perform a novel combined fit to the B̅→ D^(*) l ν̅ decay distributions to predict the B̅→ D^(*) τν̅ rates and determine the CKM matrix element |V_cb|. Most theoretical and experimental papers have <b>neglected</b> <b>uncertainties</b> in the predictions for form factor ratios at order Λ_QCD/m_c,b, which we include. We also calculate O(Λ_QCD/m_c,b) and O(α_s) contributions to semileptonic B̅→ D^(*) ℓν̅ decays for all possible b → c currents. This result has not been available for the tensor current form factors, and for two of those, which are O(Λ_QCD/m_c,b), the corrections are of the same order as approximations used in the literature. These results allow us to determine with improved precision how new physics may affect the B̅→ D^(*) τν̅ rates. Our predictions can be systematically improved with more data; they need not rely on lattice QCD results, although these can be incorporated. Comment: 32 pages, 5 figures, 11 tables; v 2 : additional fit scenario added; plots 3 & 4 corrected; numerical results unchanged; v 3 : transcription of Eqs (A 1 a,b) corrected; all results unchange...|$|R
40|$|Since {{transportation}} infrastructure projects have {{a lifetime of}} many decades, project developers must consider not only the current demand for the project but also the future demand. Future demand is of course uncertain and {{should be treated as}} such during project design. Previous research for Southwest Region University Transportation Center (Report 167556) explored the impact of uncertainty on roadway improvements and found <b>neglecting</b> <b>uncertainty</b> to lead to suboptimal network design decisions. This research is extended in the current work by considering not only motor vehicle traffic, but other modes as well. The first half of this report examines the problem of network flexibility in the face of uncertainty when constructing a potentially revenue-generating toll road project. Demand uncertainty and network design are considered by way of a bilevel stochastic recourse model. The results from a test network, for which a closed form solution is possible, indicate that the value of network flexibility directly depends on initial network conditions, variance in future travel demand, and toll pricing decisions. The second half of this report integrates Environmental Justice into the transit frequency-setting problem while considering uncertainty in travel demand from protected populations. The overarching purpose is to improve access via transit to basic amenities to: 1) reduce the disproportionate burden transit dependent populations’ experience; and 2...|$|R
40|$|Co-registration {{of point}} clouds of {{partially}} scanned objects {{is the first}} step of the 3 D modeling workflow. The aim of coregistration is to merge the overlapping point clouds by estimating the spatial transformation parameters. In computer vision and photogrammetry domain {{one of the most popular}} methods is the ICP (Iterative Closest Point) algorithm and its variants. There exist the 3 D Least Squares (LS) matching methods as well (Gruen and Akca, 2005). The co-registration methods commonly use the least squares (LS) estimation method in which the unknown transformation parameters of the (floating) search surface is functionally related to the observation of the (fixed) template surface. Here, the stochastic properties of the search surfaces are usually omitted. This omission is expected to be minor and does not disturb the solution vector significantly. However, the a posteriori covariance matrix will be affected by the <b>neglected</b> <b>uncertainty</b> of the function values of the search surface.. This causes deterioration in the realistic precision estimates. In order to overcome this limitation, we propose a method where the stochastic properties of both the observations and the parameters are considered under an errors-in-variables (EIV) model. The experiments have been carried out using diverse laser scanning data sets and the results of EIV with the ICP and the conventional LS matching methods have been compared...|$|R
40|$|Three sources – {{research}} on monetary policy under uncertainty, the managerial literature, and the real-life strategies of five inflation targeters – {{have been used}} to survey methods that are available to monetary policy makers to deal with uncertainty. The methods have been compared within a framework that is based on a decision matrix. The comparative framework has been designed in order to encompass different representations of uncertainty employed by various central banks. The results of comparative analysis suggest that central banks use models, intuition, judgement as well as traditional managerial methods to deal with uncertainty. This finding helps understanding why economic research cannot fully explain differences between monetary policy actions and outcomes of model simulations. The results of the comparative analysis also suggest that central banks have not so far fully utilised the whole spectrum of methods available to them. Economic research, other banks’ strategies as well as decision analysis may be interesting sources of inspiration when designing the decision-making process. It is emphasised that central banks introducing inflation targeting should pay equal attention to both building their forecasting models as well as selecting methods to deal with uncertainty. In the case of emerging economies where uncertainty can be much higher than in advanced economies, <b>neglecting</b> <b>uncertainty</b> may increase probability of policy errors significantly. Inflation targeting Uncertainty Decision matrix Survey of methods...|$|R
40|$|Abstract—The {{predictable}} {{trajectory of}} underwater gliders {{can be used}} in geographic routing protocols. Factors such as drifting and localization errors cause uncertainty when estimating a glider’s trajectory. Existing geographic routing protocols in underwater networks generally assume the positions of the nodes are accurately determined by <b>neglecting</b> position <b>uncertainty.</b> In this paper, a paradigm-changing geographic routing protocol that relies on a statistical approach to model position uncertainty is proposed. Our routing protocol is combined with practical cross-layer optimization to minimize energy consumption. Our solution’s performance is tested and compared with existing solutions using a real-time testbed emulation that uses underwater acoustic modems. I...|$|R
40|$|We {{construct}} uncertainty intervals for weak Poisson {{signals in}} the presence of background. We consider the case where a primary experiment yields a realization of the signal plus background, and a second experiment yields a realization of the background. The data acquisitions times for the background-only experiment,T_bg, and the primary experiment,T, are selected so that their ratio varies from 1 to 25. The expected number of background counts in the primary experiment varies from 0. 2 to 2. We construct 90 and 95 percent confidence intervals based on a propagation-of-errors method as well as two implementations of a Neyman procedure where acceptance regions are constructed based on a likelihood-ratio criterion that automatically determines whether the resulting confidence interval is one-sided or two-sided. The first Neyman procedure (due to Feldman and Cousins) <b>neglects</b> <b>uncertainty</b> in the background. In the other Neyman procedure, we account for uncertainty in the background with a parametric bootstrap method. We also construct minimum length Bayesian credibility intervals. For each method, we test for the presence of a signal based on the value of the lower endpoint of the uncertainty interval. When T_bg/T is 5 or more and the expected background is 2 or less, the Feldman Cousins method outperforms the other methods considered. Comment: 12 pages, 12 tables, 10 figures. This is the final version of a manuscript that has been accepted for publication by Measurement Science and Technolog...|$|R
30|$|The present {{literature}} {{has focused on}} identifying the most important criteria and indicators, {{as well as on}} explaining the main steps of various MCDM methods applied for planning and design of linear transportation infrastructure. The majority of the literature utilizes MCDM methods taking deterministic values on preferences of route alternatives conducted among mostly quantitative criteria. Only a few papers (e.g. Ballestero et al. [12]) considers the project evaluation under uncertainty. In fact, the problem is simplified as it <b>neglects</b> the <b>uncertainty</b> issues that in reality affect infrastructure projects. These uncertainty issues commonly refer to different limited resources, lack of information or uncertain project outputs.|$|R
40|$|Fisheries {{scientists}} habitually consider {{uncertainty in}} parameter values, but often <b>neglect</b> <b>uncertainty</b> about model structure, {{an issue of}} increasing importance as ecosystem models are devised to support the move to an ecosystem approach to fisheries (EAF). This paper sets out pragmatic approaches with which to account for uncertainties in model structure and we review current {{ways of dealing with}} this issue in fisheries and other disciplines. All involve considering a set of alternative models representing different structural assumptions, but differ in how those models are used. The models can be asked to identify bounds on possible outcomes, find management actions that will perform adequately irrespective of the true model, find management actions that best achieve one or more objectives given weights assigned to each model, or formalize hypotheses for evaluation through experimentation. Data availability is likely to limit the use of approaches that involve weighting alternative models in an ecosystem setting, and the cost of experimentation is likely to limit its use. Practical implementation of an EAF should therefore be based on management approaches that acknowledge the uncertainty inherent in model predictions and are robust to it. Model results must be presented in ways that represent the risks and trade-offs associated with alternative actions and the degree of uncertainty in predictions. This presentation should not disguise the fact that, in many cases, estimates of model uncertainty may be based on subjective criteria. The problem of model uncertainty is far from unique to fisheries, and a dialogue among fisheries modellers and modellers from other scientific communities will therefore be helpful...|$|R
40|$|Development {{programs}} have typically <b>neglected</b> <b>uncertainty</b> and variability {{in terms of}} outcomes and socio-ecological context when promoting conservation agriculture (CA) throughout sub-Saharan Africa. We developed a simple Monte Carlo-based decision model, calibrated to global data-sets and parameterized to local conditions, to predict the range of yield benefits farmers may obtain when adopting CA in two ongoing agricultural development projects in East Africa. Our general model predicts the yield effects of adopting CA-related practices average − 0. 60 ± 2. 05 (sd) Mg maize ha− 1 year− 1, indicating a near equal chance {{of positive and negative}} impacts on yield. When using site-specific, socio-economic, and biophysical data, mean changes in yield were more negative (− 1. 29 and − 1. 34 Mg ha− 1 year− 1). Moreover, practically the entire distributions of potential yield impacts were negative suggesting CA is highly unlikely to generate yield benefits for farmers in the two locations. Despite comparable aggregate effects at both sites, factors such as land tenure, access to information, and livestock pressure contrast sharply highlighting the need to quantify the range of livelihood and landscape effects when evaluating the suitability of the technology. This analysis illustrates the potential of incorporating uncertainty in rapid assessments of agricultural development interventions. Whereas this study examines project-level decisions on one specific intervention, the approach is equally relevant to address decision-making for multiple interventions, at multiple scales, and for multiple criteria (e. g., across ecosystem services), and thus is an important tool that can support linking knowledge with actio...|$|R
40|$|Road safety {{has become}} the {{critical}} issue in transportation around the world. Safety specialists are continuously striving for improvement in this area. The most difficult part; however, is determining the road features, driver behaviors, and vehicular failures that unduly {{increase the risk of}} crash. One of the frequently used methods of safety improvement is investigating roads with an excessive number of crashes. However, the lack of standard techniques causes investigative teams to rely on their own experience and judgment. Moreover, the road safety investigation process includes various assumptions that are needed to deal with the lack of data and the inconsistency among expert opinions. The simple approach is to <b>neglect</b> <b>uncertainty,</b> which, unfortunately, can lead to more issues and to a distortion of the findings. This dissertation establishes and evaluates a technique to incorporate uncertainty considerations into the road safety investigation process. First, a knowledge base is developed and organized into a Bayesian Belief Network structure. The knowledge is acquired from multiple sources including the National Cooperative Highway Research Program (NCHRP) reports, safety guidelines, observation of experts during work, and road safety investigation reports. Next, the uncertainty is incorporated into the developed network, and Bayesian inference is used to update the network probabilities. ^ The method is evaluated by comparing the results obtained by the participating experts and non-experts using two methods; namely, with and without uncertainty consideration. The evaluation results show that taking into account uncertainty can significantly improve the findings and prevent overlooking important factors. Finally, a discussion of the research contribution and possible future improvements are presented. ...|$|R
40|$|A {{solution}} for compliance proving and emission trading {{in case of}} big uncer-tainties in emission observations is proposed. It {{is based on the}} undershooting concept, from which both mathematical conditions for proving the compliance with a risk, and for calculation of eective emissions for trading is obtained. This notions are used for dening eective permits, which can be traded on a normal basis, <b>neglecting</b> the underlying <b>uncertainty.</b> ...|$|R
40|$|Non-thermal {{pressure}} from turbulence and bulk flows {{is a fundamental}} ingredient in hot gaseous halos, and in the intracluster medium it will be measured through emission line kinematics with calorimeters on future X-ray spacecraft. In this paper we present a complementary method for measuring these effects, using forbidden FUV emission lines of highly ionized Iron which trace 10 ^ 7 K gas. The brightest of these is [Fe XXI] λ 1354. 1. We search for these lines in archival HST-COS spectra from the well-known elliptical galaxies M 87 and NGC 4696, which harbor large reservoirs of 10 ^ 7 K gas. We report a 2. 2 σ feature which we attribute to [Fe XXI] from a filament in M 87, and positive residuals in the nuclei of M 87 and NGC 4696, for which the 90 % upper limits on the line flux {{are close to the}} predicted fluxes based on X-ray observations. In a newer reduction of the data from the Hubble Spectroscopic Legacy Archive, these limits become tighter and the [Fe XXI] feature reaches a formal significance of 5. 3 σ, <b>neglecting</b> <b>uncertainty</b> in fitting the continuum. Using our constraints, we perform emission measure analysis, constraining the characteristic path length and column density of the ∼ 10 ^ 7 K gas. We also examine several sightlines towards filaments or cooling flows in other galaxy clusters, for which the fraction of gas at 10 ^ 7 K is unknown, and place upper limits on its emission measure in each case. A medium-resolution HST-COS observation of the M 87 filament for ∼ 10 orbits would confirm our detection of [Fe XXI] and measure its width. Comment: accepted to MNRA...|$|R
40|$|AbstractInformation Retrieval (IR) aims to {{discover}} relevant information {{according to a}} user's information need. The classic Probability Ranking Principle (PRP) forms the theoretical basis for probabilistic IR models. This ranking principle, however, <b>neglects</b> the <b>uncertainty</b> introduced through the estimations from retrieval models. Inspired by the Post-Modern Portfolio Theory (PMPT), this paper proposes a mean-semivariance framework to handle the uncertainty. The proposed framework not only deals with the uncertainty but {{has the ability to}} distinguish bad surprises (downside uncertainty) and good surprises (upside uncertainty) when optimizing a ranking list. The experimental results shows that the proposed method improves the IR performance over the PRP baseline in terms of most of IR evaluation metrics; moreover, the results suggest that the mean-semivariance framework can further boost the top-position ranking quality...|$|R
40|$|The {{practical}} {{realization of}} the differential drag technique for orbital relative maneuvers must cope with the several and severe uncertainty sources affecting drag modeling. <b>Neglecting</b> these <b>uncertainties</b> might yield to oversimplified solutions whose representation of a real-life scenario is questionable. The first outcome of this study consists in the synthesis of a robust optimal controller which combines differential flatness theory and the scenario approach to generate a reference path which can be easily followed. The second outcome is the characterization of the relevant uncertainties of the differential drag problem, with a special focus on the aerodynamic force. The developments are validated in a highly detailed simulation environment including, among the perturbations, advanced drag modeling and coupled attitude and orbital dynamics...|$|R
