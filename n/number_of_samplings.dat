32|10000|Public
30|$|Obtain Sg, {{which is}} the subset of the {{training}} data set, by random sampling without replacement from L so that the same <b>number</b> <b>of</b> <b>samplings</b> is taken from the cases as from the controls.|$|E
40|$|Monte carlo {{simulation}} method of evaluating composite power system reliability uses {{the concept of}} sampling random numbers, which simulate the random failure of components of the power system. It requires more <b>number</b> <b>of</b> <b>samplings.</b> to get required accuracy which in turn needs more computational efforts. Variance reduction techniques such as control variates, Importance sampling and antithetic variates will reduce the <b>number</b> <b>of</b> <b>samplings</b> and time of computation for the same accuracy as obtained by the traditional Monte carlo simulation. In this paper the IEEE RBTS-Test system is considered and the Monte carlo method and Antithetic variates technique are applied for composite system reliability evaluation and results are analysed...|$|E
30|$|Despite the {{tremendous}} {{reduction in the}} number of observations required for provable success of CS methods, the traditional CS methods do suffer several drawbacks [3]. Typically, existing CS architectures are not adaptive and the <b>number</b> <b>of</b> <b>samplings</b> is determined before the acquisition process begins with no feedback during the acquisition process on the improved quality [4]. Even more, in measuring signal, such as radar signal, where its interest is time-varying, there may not be sufficient prior knowledge to decide appropriate CS observations.|$|E
40|$|BACKGROUND: transabdominal {{chorionic}} villous sampling {{is generally}} preferred to the transvaginal approach. The procedure may, however, {{be associated with}} complcations due to a <b>number</b> <b>of</b> factors. OBJECTIVES: to review {{the relationship between the}} <b>number</b> <b>of</b> cases and other variables in transabdominal chorionic villous sampling and also to identify the complications associated with the procedure. METHODS: two hundred and twenty-six cases of trans-abdominal chorionic sampling performed by a single operator were reviewed. Pearson’s correlation coefficient was used to individually analyze the relationship between <b>numbers</b> <b>of</b> <b>samplings</b> performed and other procedure-related variables such as the <b>number</b> <b>of</b> needle aspirations, gestational age...|$|R
40|$|Minimum {{variance}} projection {{is widely}} used in geophysical and space plasma measurements to identify the wave propagation direction and the wavenumber of the wave fields. The advantage of the minimum variance projection {{is its ability to}} estimate the energy spectra directly in the wavenumber domain using only a limited <b>number</b> <b>of</b> spatial <b>samplings.</b> While the minimum variance projection is constructed for discrete signals in the data, we find that the minimum variance projection can reasonably reproduce the spectral slope of the power-law spectrum if the data represent continuous power-law signals. The spectral slope study using the minimum variance projection is tested against synthetic random data with a power-law spectrum. The method is applicable even for a small <b>number</b> <b>of</b> spatial <b>samplings.</b> Conversely, the spatial aliasing causes a flattening of the spectrum...|$|R
5000|$|Another {{relationship}} {{involves the}} Pascal Triangle: Whereas the classical Pascal Triangle with sides [...] has diagonals {{with the natural}} numbers, triangular numbers, and tetrahedral numbers, generating the Fibonacci <b>numbers</b> as sums <b>of</b> <b>samplings</b> across diagonals, the sister Pascal with sides [...] has equivalent diagonals with odd numbers, square numbers, and square pyramidal numbers, respectively, and generates (by the same procedure) the Lucas numbers rather than Fibonacci.|$|R
40|$|Abstract — In the paper, {{the authors}} have {{extended}} the classical coupon collector problem {{to the case of}} group drawings with indistinguishable items. The results are applied to a statistical quality control problem that arises in a dairy's bottle filling process with s nozzles. A sequential m sized samplings is made in order to detect the nozzles. The present research concerns the <b>number</b> <b>of</b> <b>samplings,</b> called the waiting time, required until each nozzle is detected at least once. Bose Einstein statistics is used to analyze the waiting time distribution and a numerical example is given. Index Terms—Bose Einstein statistics, indistinguishable items, waiting tim...|$|E
40|$|Abstract. The {{traditional}} way of representing motion in 3 D space-time uses a trajectory, i. e. a sequence of (x,y,t) points. Such a trajectory may be produced by periodic sampling of a Global Positioning System (GPS) receiver. The are two problems with this representation of motion. First, imprecision due to errors (e. g. GPS receivers often produce off-the-road locations), and second, space complexity due to a large <b>number</b> <b>of</b> <b>samplings.</b> We examine an alternative representation, called a nonmaterialized trajectory, which addresses both problems {{by taking advantage of}} the a priori knowledge that the motion occurs on a transport network. ...|$|E
3000|$|This {{subsection}} explains RC {{with the}} traditional criteria, such as Mean Absolute Difference (MAD) and Mean Square Difference (MSD) in (8). It is theoretically proper for MMFs. The idea of RC is to subtract the corresponding mean values from all for-matching sequences and then calculate the correlation degree {{between them and the}} measurement sequence also with its average removed. With this preprocess, the constant-like deviation of real-time samplings caused by daily variation or other variations can be omitted during the matching. As matching for each time takes just a certain <b>number</b> <b>of</b> <b>samplings,</b> both averages removal can overcome the influences of daily variation and so on [...]...|$|E
40|$|AbstractAn optimal {{algorithm}} for approximating bandlimited functions from localized sampling is established. Several equivalent formulations for the {{approximation error}} of the optimal algorithm are presented and its {{upper and lower}} bound estimates for the univariate case are provided. The estimates show that the approximation error decays exponentially (but not faster) as the <b>number</b> <b>of</b> localized <b>samplings</b> increases. As a consequence of these results, we obtain an upper bound estimate for the eigenvalues of an integral operator that arises in the bandwidth problem...|$|R
40|$|Objectives To {{evaluate}} {{the impact of}} a screening strategy in the first trimester, introduced in Denmark during 2004 - 6, on the <b>number</b> <b>of</b> infants born with Down’s syndrome and the <b>number</b> <b>of</b> chorionic villus <b>samplings</b> and amniocenteses, and to determine detection and false positive rates in the screened population in 2005 and 2006...|$|R
40|$|BACKGROUND: In {{clinical}} practise {{the high}} dose ACTH stimulation test (HDT) is frequently {{used in the}} assessment of adrenal insufficiency (AI). However, there is uncertainty regarding optimal time-points and <b>number</b> <b>of</b> blood <b>samplings.</b> The present study compared the utility of a single cortisol value taken either 30 or 60 minutes after ACTH stimulation with the traditional interpretation of the HDT. METHODS: Retrospective analysis of 73 HDT performed at a single tertiary endocrine centre. Serum cortisol was measured at baseline, 30 and 60 minutes after intravenous administration of 250 µg synthetic ACTH 1 - 24. Adrenal insufficiency (AI) was defined as a stimulated cortisol level < 550 nmol/l. RESULTS: There were twenty patients (27. 4...|$|R
40|$|Abstract—We {{consider}} epidemic-style {{information dissemination}} strategies that leverage the nonuniformity of host distribution over subnets (e. g., IP subnets) {{to optimize the}} information spread. Such epidemic-style strategies are based on random sampling of target hosts according to a sampling rule. In this paper, {{the objective is to}} optimize the information spread with respect to minimizing the total <b>number</b> <b>of</b> <b>samplings</b> to reach a target fraction of the host population. This is of general interest for the design of efficient information dissemination systems and more specifically, to identify requirements for the containment of worms that use subnet preference scanning strategies. We first identify the optimum <b>number</b> <b>of</b> <b>samplings</b> to reach a target fraction of hosts, given global information about the host distribution over subnets. We show that the optimum can be achieved by either a dynamic strategy for which the per host sampling rate over subnets is allowed to vary over time or by a static strategy for which the sampling over subnets is fixed. These results appear to be novel and are informative about (a) what best possible performance is achievable and (b) what factors determine the performance gain over oblivious strategies such as uniform random scanning. We then consider several simple, online sampling strategies that require only local knowledge, where each host biases sampling based on its observed sampling outcomes and keeps only O(1) state at any point in time. Using real datasets from several large-scale Internet measurements, we evaluate the significance of the factors revealed by our analytical results on the sampling efficiency. I...|$|E
40|$|Rice {{production}} {{is a major}} source of global methane emissions and many studies have been conducted to quantify flux rates of methane alongside N 2 O emissions from rice fields. The closed chamber method with manual sampling is the conventional approach for determining greenhouse gas (GHG) emission rates from rice fields. However, as of now, there is no commonly accepted standard for the measurement protocols, so that published studies encompass a variety of different modalities depending on the specific settings and labour limitations of a given study. This literature study comprises 155 peer-reviewed articles on manual GHG sampling with a static closed chamber as a basis to determine the most common practices of this method regarding the following features: (1)  procedures (duration of chamber closure, number of gas samples per chamber closure and number of replicate chambers), (2)  timing (<b>number</b> <b>of</b> <b>samplings</b> per day and time of day of sampling) and (3)  intervals (between two consecutive sampling days). It has been found that some features show a high degree of uniformity among these studies, namely, the procedures. On the other hand, other features of the measuring protocol diverge widely (timing and intervals). Derived from these results, the following common practices (compiling features being applied in more than 66 % of the studies that give information on the particular feature) have been identified: duration of chamber closure (30 minutes or less), number of gas samples per chamber closure (3 or 4), number of replicates (3), <b>number</b> <b>of</b> <b>samplings</b> per day (1 or 2), time of day of sampling (late morning) and interval (7 days or less) ...|$|E
40|$|Abstract—The maximum clique {{of a given}} graph G is the {{subgraph}} C of G {{such that}} two vertices in C are adjacent in G with maximum cardinality. Finding the maximum clique in an arbitrary graph is an NP-Hard problem, motivated by the social networks analysis. In the real world applications, the nature of interaction between nodes is stochastic and the probability distribution function of the vertex weight is unknown. In this paper a learning automata-based algorithm is proposed for solving maximum clique problem in the stochastic graph. The simulation results on stochastic graph demonstrate that the proposed algorithm outperforms standard sampling method {{in terms of the}} <b>number</b> <b>of</b> <b>samplings</b> taken by algorithm. Keywords- maximum clique problem; NP-Hard; stochastic graph; learning automata; social networks. I...|$|E
30|$|Methods: The {{study was}} {{approved}} by the Institutional Review Board, and written, informed consent was obtained from all participants. This study included 28 patients underwent elective cardiac surgery with preoperative HbA 1 c[*]>[*] 6.0  %. The management of blood glucose was performed from immediately after admission to the ICU for two research days. The patients were randomly assigned to control blood glucose levels with the STG- 55 group (n[*]=[*] 16) or sliding scale method, SS group (n[*]=[*] 12). The usefulness of blood glucose management defined the incidence of hyperglycemia (blood glucose[*]>[*] 180  mg/dL), hypoglycemia (< 80  mg/dL), and the maximum glycemic variability (maximum blood glucose level minus minimum blood glucose level). The workload of ICU nurses defined the <b>number</b> <b>of</b> blood <b>samplings</b> for the management of blood glucose and the <b>number</b> <b>of</b> calls made to the physician.|$|R
40|$|Abstract. We give {{a simple}} and natural (probabilistic) {{construction}} of hypergraph regularization. It is done just by taking a constant-bounded <b>number</b> <b>of</b> random vertex <b>samplings</b> only one time (thus, iteration-free). It is independent from the definition of quasi-randomness and yields a new elementary proof of a strong hypergraph regularity lemma. Consequently, {{as an example of}} its applications, we have a new self-contained proof of Szemerédi’s classic theorem on arithmetic progressions (1975) as well as its multidimensional extension by Furstenberg-Katznelson (1978). 1...|$|R
40|$|We analyse {{the problem}} of {{controllability}} for parameter-dependent linear finite-dimensional systems. The goal is to identify the most distinguished realisations of those parameters so to better describe or approximate {{the whole range of}} controls. We adapt recent results on greedy and weak greedy algorithms for parameter depending PDEs or, more generally, abstract equations in Banach spaces. Our results lead to optimal approximation procedures that, in particular, perform better than simply sampling the parameter-space to compute the controls for each of the parameter values. We apply these results for the approximate control of finite-difference approximations of the heat and the wave equation. The numerical experiments confirm the efficiency of the methods and show that the <b>number</b> <b>of</b> weak-greedy <b>samplings</b> that are required is particularly low when dealing with heat-like equations, because of the intrinsic dissipativity that the model introduces for high frequencies...|$|R
40|$|The {{two-dimensional}} (2 D) {{data structure}} generated under a high resolution GC×GC {{system with a}} small <b>number</b> <b>of</b> <b>samplings</b> taken across the first dimension is evaluated {{for the purpose of}} the application of chemometric deconvolution methods. Chemometric techniques such as generalized rank annihilation method (GRAM) place high demands on the reproducibility of chromatographic experiments. For GRAM to be employed for GC×GC data interpretation, it is critical that the separation method provides data with a bilinear structure; the peak-shape and retention times on both columns must be reproducible. With a limited <b>number</b> <b>of</b> <b>samplings</b> across a 1 D (first dimension) peak (e. g. four to six samplings) repeatability of the pattern of the modulated peaks (controlled by the modulation phase) becomes important in producing a bilinear data structure. Reproducibility of modulation phase can be affected by both reliability of the modulation period and reproducibility of the retention time of the peak on the first column (which arises from oven temperature and carrier flow rate stability). Evaluation of within-run and run-to-run retention time reproducibility (retention time uncertainty) on both columns, and modulation phase reproducibility using a modulated cryogenic system for a pair of overlapping components (fatty acid methyl esters) was undertaken. An investigation of the quality of data to permit quantification of each component by using GRAM deconvolution, was also conducted. Less than 4 % run-to-run retention time uncertainty was obtained on column 1 and less than 9 % run-to-run and within-run retention time uncertainty was obtained on column 2, where these R. S. D. measures are reported normalised to peak widths on each respective dimension. The R. S. D. of duplicate quantification results by GRAM ranged from 2 to 26 % although the average quantification error using GRAM was less than 5 %...|$|E
40|$|Effective {{verification}} methods {{are necessary for}} finding bugs in complex system design. Given domain specific knowledge, design verification engineers typically specify cross coverage for certain risky areas where bugs tend to appear. This paper proposes a pragmatic coverage model based on cross coverage. We address the verification on user specified cross coverage regions. The proposed analysis models the probability of exposing the bug within a given <b>number</b> <b>of</b> <b>samplings,</b> and derives the expected number of samples until bug detection. The approach is applicable to random, round-robin, and hybrid sampling strategies in recurring and nonrecurring cases based on our cross coverage model. We have written Matlab and C programs that use our formulas to calculate the probabilities. Experimental results show that our analysis is consistent with Monte Carlo simulation...|$|E
40|$|Fuzzy {{control is}} well known as a {{powerful}} technique for designing and realizing controllers. However, statistical evidence for their correct behavior may be not enough, {{even when it is}} based on a large <b>number</b> <b>of</b> <b>samplings.</b> Therefore, much work is being done to provide a systematic verification of fuzzy controllers and to asses their robustness, that is the ability of a controller to maintain good performance even in the presence of significant disturbances or parameter variations. In the present paper, we introduce a model checking based methodology for the fuzzy controller robustness analysis, that can be applied on plant-controller pairs in a nearly automatic way, giving higher precision results than other approaches, such as cell mapping. We support our conclusions with a case study that compares two different fuzzy controllers for the inverted pendulum on a cart problem. © 2009 Springer Berlin Heidelberg...|$|E
40|$|The top visible dewlap leaf {{potassium}} (K) of sugarcane ratoons {{varies in}} an irregular manner with age ofcane. However, {{by increasing the}} <b>number</b> <b>of</b> leaf <b>samplings,</b> the fluctuation ofthe mean K values with age becomes less and less significant. As a result in Mauritius, where the running average leaf K concentration of 3 consecutive years of double leafsampling is interpreted, age corrections ofthe analytical leaf K value {{will not have a}} noteworthy impact on the accuracy of foliar diagnosis for K status. Examination of daily rainfall which fell 30 days prior to leafsampling failed to reveal the exact significance ofmoisture regime on the K nutritional status ofsugarcane. In some instances a high rainfall prior to leaf sampling gave rise to a high leafK value, while in other instances at the same site the opposite was true...|$|R
40|$|Abstract. The {{well-known}} regularity lemma of E. Szemerédi for graphs (i. e. 2 -uniform hypergraphs) {{claims that}} for any graph {{there exists a}} vertex partition with the property of quasi-randomness. We give a simple construction of such a partition. It is done just by taking a constant-bounded <b>number</b> <b>of</b> random vertex <b>samplings</b> only one time (thus, iteration-free). Since it is independent from the definition of quasi-randomness, it can be generalized very naturally to hypergraph regularization. In this expository note, we show only a graph case of the paper [5] on hypergraphs, but may help the reader to access [5]. 1...|$|R
40|$|Index Terms—Linear {{functional}} observer, state feedback, un- certain systems, multirate output sampling. Abstract—In {{this paper}} a design of controller {{has been proposed}} for linear uncertain systems based on multirate output sampling {{for the purpose of}} state feedback and nullification of disturbance effect on system response. If certain conditions are satisfied, construction of proposed functional type controller require less <b>number</b> <b>of</b> output samples than any other method based on fast output sampling. The advantage of proposed method is that it can be extended to unobservable systems and avoids the boundedness constraint on disturbance. A numerical exam- ple demonstrates the procedure, superiority in terms <b>number</b> <b>of</b> required output <b>samplings</b> and application to unobservable systems...|$|R
30|$|This {{procedure}} {{adopted the}} same <b>number</b> <b>of</b> <b>samplings,</b> for example, 20 POAG and 20 healthy controls were sampled from 42 POAG and 42 healthy controls {{in the training}} data set, respectively. This {{reason is that the}} contribution of the characteristics of POAG and control should be as close to equal possible. Besides, it is preferable for the genotype and cytokine data to be evaluated as equally as possible (e.g., K = N.) However, it may be impossible to predict one group by dividing it in half if the total number of sampling repeats is an even number. In this study, since the size of the genotype data set was greater than that of the cytokines, K is taken as N + 1 to avoid the situation of a tie vote. In addition, note that use of the base classifier should be limited to one kind of classifier from the beginning of this procedure to the end.|$|E
40|$|Abstract — The minimum {{connected}} dominating set (MCDS) {{of a given}} graph G is {{the smallest}} sub-graph of G such that every vertex in G belongs either to the sub-graph or is adjacent to a vertex of the sub-graph. Finding the MCDS in an arbitrary graph is a NP-Hard problem, and several approximation algorithms have been proposed for solving this problem in deterministic graphs, but {{to the best of}} our knowledge no work has been done on finding the MCDS in stochastic graphs. In this paper, the MCDS problem in the stochastic graphs is first introduced, and then a learning automata-based approximation algorithm called SCDS is proposed for solving this problem when the probability distribution function of the vertex weight is unknown. It is shown that by a proper choice of the parameters of the proposed algorithm, the probability with which the proposed algorithm find the MCDS is close enough to unity. The simulation results show the efficiency of the proposed algorithm in terms of the <b>number</b> <b>of</b> <b>samplings.</b> Keywords-component; Dominating set, minimum connected dominating set, stochastic graph, learning automata I...|$|E
40|$|We {{propose a}} {{modification}} of the Hybrid Monte-Carlo method to sample equilibrium distributions of continuous field models. The method allows an efficient implementation of Fourier acceleration and is shown to reduce completely critical slowing down for the Gaussian model, i. e., z = 0. 1 The development of efficient numerical algorithms to study equilibrium properties of field-theoretical models near second order phase transitions {{is a very important}} subject in particle and statistical physics[1, 2, 3, 4, 5]. For that purpose, several methods such as Molecular Dynamics, Langevin and Monte–Carlo (MC) have been used. While the first two methods suffer from systematic step–size time discretization errors which may affect the computed mean values of observables, the only errors present in MC methods are of statistical origin and can be easily controlled, in principle, by varying the <b>number</b> <b>of</b> <b>samplings.</b> In practice, however, it turns out that for many problems of interest the number of configurations necessary to achieve a given small error is very large and grows as some power of the system size, thus requirin...|$|E
40|$|Life cycle {{assessment}} (LCA) {{is increasingly}} becoming a common technique {{to assess the}} embodied energy and carbon of buildings and their components over their life cycle. However, {{the vast majority of}} existing LCAs result in very definite, deterministic values which carry a false sense of certainty and can mislead decisions and judgments. This article tackles the lack of uncertainty analysis in LCAs of buildings by addressing the main causes for not undertaking this important activity. The research uses primary data for embodied energy collected from European manufacturers as a starting point. Such robust datasets are used as inputs for the stochastic modelling of uncertainty through Monte Carlo algorithms. Several groups <b>of</b> random <b>samplings</b> between 10 1 and 10 7 are tested under two scenarios: data are normally distributed (empirically verified) and data are uniformly distributed. Results show that the hypothesis on the data no longer influences the results after a high enough <b>number</b> <b>of</b> random <b>samplings</b> (10 4). This finding holds true both in terms of mean values and standard deviations and is also independent {{of the size of the}} life cycle inventory (LCI) : it occurs in both large and small datasets. Findings from this research facilitate uncertainty analysis in LCA. By reducing significantly the amount of data necessary to infer information about uncertainty, a more widespread inclusion of uncertainty analysis in LCA can be encouraged in assessments from practitioners and academics alike...|$|R
40|$|Over {{the last}} 50 years the general average <b>number</b> <b>of</b> vertebras of anchovy kilka has {{decreased}} that {{is connected with}} the change of thermal mode of the Caspian Sea. As {{the result of the}} cluster analysis <b>of</b> 16 <b>samplings</b> <b>of</b> anchovy kilka by measurements of different sections of spine accurate distribution <b>of</b> <b>samplings</b> confined to {{the eastern part of the}} Middle Caspian, the western part of the South Caspian, the easteern part of the South Caspian is revealed...|$|R
40|$|Identifying {{potential}} drug targets is {{a crucial}} task for drug discovery. Traditional in silico approaches utilize only pro-tein sequence or structural information to predict whether a protein can be a drug target, and achieve limited suc-cess. Since proteins function {{in the context of}} interaction networks by interacting with other cellular macromolecules, analysis of topological features of proteins in such networks can reveal important insights on whether a protein can be a potential drug target. In this paper, we introduced ten new topological features extracted from human protein in-teraction networks. When designing these new features, we specially emphasized the roles of three disease-related groups of proteins: known drug targets, disease genes, and essen-tial genes. Based on the topological feature set, we built supervised learning models using support vector machines, L 1 -regularized logistic regression, and k-nearest neighbors to predict whether testing proteins can be drug targets or not. We also analyzed the relevance of each feature to the probability of proteins being drug targets. We achieved up to 80 % classification accuracy using tenfold cross validation, and yielded very stable results with a large <b>number</b> <b>of</b> ran-dom <b>samplings.</b> Our method {{can also be used to}} prioritize multiple candidate proteins according to their probability of being drug targets...|$|R
40|$|The aim of {{the thesis}} was to {{evaluate}} and to compare the rate of corn borer and the production capability of 4 hybrids of corn (Zeamays L.) of different earliness. For the comparison a variety test was layed out {{in the land of}} my father in Klimětice (Central Bohemia). Before the harvest a sampling for detection of dry matter content in the biomass has been made to determinate the date of harvest. The <b>number</b> <b>of</b> <b>samplings</b> depended on the attainment of required dry matter content in the biomass. By the harvest the yield and dry matter content of the biomass, yield and percentage of corn ears, dry matter yield and the starch yield were determined. In the experiments the differences in infestation of corn borer were determined. Further differences in observed parameters were found out, which were dependent on the different utility trends and earliness of the given hybrids. The result was the appreciation of the given hybrids and determining of their suitability for their growing in conditions of my fathers lands...|$|E
40|$|Fuzzy {{control is}} well known as a {{powerful}} technique for designing and realizing control systems. However, statisti-cal evidence for their correct behavior may be not enough, {{even when it is}} based on a large <b>number</b> <b>of</b> <b>samplings.</b> In order to provide a more systematic verification pro-cess, the cell-to-cell mapping technology has been used in a number of cases as a verification tool for fuzzy control systems and, more recently, to assess their optimality and robustness. However, cell-to-cell mapping is typically limited in the number of cells it can explore. To overcome this limitation, in this paper we show how model checking techniques may be instead used to verify the correct behavior of a fuzzy con-trol system. To this end, we use {{a modified version of the}} Murphi veri-fier, which ease the modeling phase by allowing to use finite precision real numbers and external C functions. In this way, also already designed simulators may be used for the verification phase. With respect to the cell mapping technique, our approach appears to be complementary; indeed, it explores a much larger number of states, at the cost of being less informative on the global dynamic of the system. ...|$|E
40|$|This paper {{presents}} a fast multiple sampling method for low-noise CMOS image sensor (CIS) applications with column-parallel {{successive approximation register}} analog-to-digital converters (SAR ADCs). The 12 -bit SAR ADC using the proposed multiple sampling method decreases the A/D conversion time by repeatedly converting a pixel output to 4 -bit after the first 12 -bit A/D conversion, reducing noise of the CIS by one over the square root of the <b>number</b> <b>of</b> <b>samplings.</b> The area of the 12 -bit SAR ADC is reduced by using a 10 -bit capacitor digital-to-analog converter (DAC) with four scaled reference voltages. In addition, a simple up/down counter-based digital processing logic is proposed to perform complex calculations for multiple sampling and digital correlated double sampling. To verify the proposed multiple sampling method, a 256 × 128 pixel array CIS with 12 -bit SAR ADCs was fabricated using 0. 18 μm CMOS process. The measurement results shows that the proposed multiple sampling method reduces each A/D conversion time from 1. 2 μs to 0. 45 μs and random noise from 848. 3 μV to 270. 4 μV, achieving a dynamic range of 68. 1 dB and an SNR of 39. 2 dB...|$|E
40|$|Aims: The {{aim of this}} {{prospective}} study was to evaluate the accuracy of transcutaneous bilirubinometry using the Minolta Air-Shields JM- 103 device in preterm newborns of gestational age 32 – 34 weeks, and to identify the most appropriate measurement site. Methods: Transcutaneous bilirubin (TcB) measurements were performed over forehead, sternum and abdomen, if total serum bilirubin (TSB) had to be determined on clinical indication in neonates of selected gestational age. TSB levels were measured in a clinical laboratory using direct spectrophotometry. In order to assess transcutaneous bilirubinometry accuracy, differences between TSB and TcB, their CI 95 %, and correlation coefficients (r) between TcB and TSB were evaluated. Results: The study group consisted of 44 infants, including 6 very low birth weight (VLBW) neonates. The correlations between transcutaneous and laboratory values {{were found to be}} significant and close. Minimal differences were observed when measured over sternum. The measurements over forehead had a tendency to underestimate TSB levels. Conclusions: Noninvasive measurement by Minolta JM- 103 demonstrated significant accuracy. The authors recommend measurements over sternum or abdomen in premature infants born within 32 – 34 gestational weeks as a reliable and accurate neonatal hyperbilirubinemia screening test. Transcutaneous bilirubinometry has the potential to reduce the <b>number</b> <b>of</b> blood <b>samplings,</b> thus reducing neonatal pain and discomfort, parental distress and medical care cost...|$|R
40|$|AbstractSuppose Γ′ to be a {{subgraph}} of a graph Γ. We {{define a}} sampling of a Γ-design B=(V,B) into a Γ′-design B′=(V,B′) as a surjective map ξ:B→B′ mapping each block of B into one of its subgraphs. A sampling will be called regular when the <b>number</b> <b>of</b> preimages of each block of B′ under ξ is a constant. This new concept is closely related with the classical notion of embedding, which has been extensively studied, for many classes of graphs, by several authors; see, for example, the survey by Quattrocchi (2001) [29]. Actually, a sampling ξ might induce several embeddings of the design B′ into B, although the converse is not true in general. In the present paper, we study in more detail the behaviour <b>of</b> <b>samplings</b> <b>of</b> Γ-complete designs of order n into Γ′-complete designs of the same order and show how the natural necessary condition {{for the existence of}} a regular sampling is actually sufficient. We also provide some explicit constructions <b>of</b> <b>samplings,</b> as well as propose further generalisations...|$|R
30|$|ER 3. All source <b>of</b> <b>sampling’s</b> search {{units and}} their units of {{observation}} must {{be identified by}} a logical or numerical id.|$|R
