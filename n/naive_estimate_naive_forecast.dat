0|410|Public
40|$|We {{consider}} large-scale {{studies in}} which it is of interest to test {{a very large number of}} hypotheses, and then to estimate the effect sizes corresponding to the rejected hypotheses. For instance, this setting arises in the analysis of gene expression or DNA sequencing data. However, <b>naive</b> <b>estimates</b> of the effect sizes suffer from selection bias, i. e., some of the largest <b>naive</b> <b>estimates</b> are large due to chance alone. Many authors have proposed methods to reduce the effects of selection bias under the assumption that the <b>naive</b> <b>estimates</b> of the effect sizes are independent. Unfortunately, when the effect size estimates are dependent, these existing techniques can have very poor performance, and in practice there will often be dependence. We propose an estimator that adjusts for selection bias under a recently-proposed frequentist framework, without the independence assumption. We study some properties of the proposed estimator, and illustrate that it outperforms past proposals in a simulation study and on two gene expression data sets. Comment: 21 pages, 2 figure...|$|R
40|$|This study {{proposes that}} a no-change <b>naïve</b> <b>forecast</b> of (operating) cash flow is as {{accurate}} as time-series and cross-sectional regression forecasts of cash flow. The study first demonstrates that cross-sectional regression forecasts of cash flow with firm-size controls are as accurate time-series (firm-specific) regression forecasts. Next the study confirms the expectation that a <b>naïve</b> <b>forecast</b> is {{as accurate as}} the regression model forecasts. Finally, the study confirms that some studies in the literature misapply Theil’s U-statistic, and overstate the ability of regression forecast models to outperform a <b>naïve</b> <b>forecast</b> model. (RQFA review process requires 3 - 6 mos per Cabells) ...|$|R
40|$|The paper {{deals with}} the {{approximation}} of the best deterministic choice (minimizing mean integrated squared error) of the parameter in naive kernel and cubic partitioning regression estimation by cross-validation. The result is essentially distribution-free. Regression <b>estimate</b> <b>Naive</b> kernel Cubic partition Mean integrated squared error (MISE) Best deterministic parameter choice Cross-validation...|$|R
40|$|When {{covariates}} in Longitudinal {{data are}} subject to errors, the <b>naive</b> <b>estimates</b> of the model parameters are often biased. In this research, we exploit a Dynamic Binary Mixed-effects Model using a Generalized Quasi-likelihood approach. Through simulations, we shall study the patterns in the bias of the naive estimator of the parameters that ignores the errors in the covariates...|$|R
40|$|The Kaplan-Manohar {{ambiguity}} in light quark masses {{allows for a}} larger uncertainty in the ratio of up to down quark masses than <b>naive</b> <b>estimates</b> from the chiral Lagrangian would indicate. We show that it allows for a relaxation of experimental bounds on the QCD axion, specifically KSVZ axions in the 2 - 3 μeV mass range composing 100...|$|R
40|$|This paper {{compares the}} {{accuracy}} of the Consensus forecasts for euro-area GDP growth, consumer and producer price inflation, and the USD/EUR exchange rate to those of the European Commission, International Monetary Fund, and Organization for Economic Co-operation and Development, and also to the <b>naive</b> <b>forecast</b> and the forecast implied by the forward exchange rate. In the period from 1994 to 2009 the Consensus forecasts for effective euro-area consumer price inflation and GDP growth beat the alternatives by a difference which is typically statistically significant. The results are more diverse for the pre-crisis sample (1994 – 2007). The Consensus forecast for euro-area producer price inflation significantly outperforms the <b>naive</b> <b>forecast</b> in the short term. Finally, the Consensus forecast for the USD/EUR exchange rate during the period from 2002 to 2009 is more precise than the <b>naive</b> <b>forecast</b> and the forecast implied by the forward rate. forecasting accuracy; prediction process; survey forecast...|$|R
40|$|We propose an easy direct {{regression}} test to evaluate forecasts {{with respect to}} the naïve most recent observation alternative. The test is applied to thirteen macroeconomic forecasts published by the OECD. Four time horizons are considered: starting with eighteen months and declining to zero for the December current year forecast. Our results show that the longest horizon forecasts are essentially worthless. Unfortunately, the <b>naïve</b> <b>forecasts</b> do not have any value either. In general, <b>naïve</b> <b>forecasts</b> are inferior to OECD forecasts, but frequently the current year forecasts compare in quality to the one-year-ahead forecast. info:eu-repo/semantics/publishe...|$|R
40|$|Abstract: A {{parameter}} called overlap rate {{is proposed}} {{to control the}} number of valid detectors generated for a T-detector Maturation Algorithm. The achieved algorithm TMA-OR can {{reduce the number of}} detectors for abnormal detection. Experiment results show that TMA-OR is more effective than V-detector algorithms such as <b>naive</b> <b>estimate</b> and hypothesis testing method and can be applied on different data sets. Key-Words: Artificial immune system, overlap rate, match range...|$|R
40|$|Electricite de France, the French {{monopoly}} {{provider of}} electricity, has been testing {{a number of}} new policy instruments to avoid disconnection among its low income customers. This paper uses decentralization, a common feature of social programmes, to identify the impact of Electricite de France's energy assistance programmes on the probability of disconnection for households with difficulties of payment. <b>Naive</b> <b>estimates</b> of programme impact are shown to be seriously biased. ...|$|R
40|$|The strong {{longitudinal}} {{expansion of the}} reaction zone formed in relativistic heavy-ion collisions is found to significantly reduce the spatially averaged pion phase-space density, compared to <b>naive</b> <b>estimates</b> based on thermal distributions. This {{has important implications for}} data interpretation and leads to larger values for the extracted pion chemical potential at kinetic freeze-out. Comment: 5 pages, 3 figures included via epsfig, added discussion of different transverse density profiles, 1 new figur...|$|R
40|$|Consensus Economics {{forecasts}} for euro-area GDP growth, {{consumer and}} producer price inflation and the USD/EUR exchange rate {{are used by}} the Czech National Bank to make assumptions about future external economic developments. This paper compares {{the accuracy of the}} aforementioned Consensus forecasts to those of the European Commission, International Monetary Fund and Organization for Economic Co-operation and Development, and also to the <b>naive</b> <b>forecast</b> and the forecast implied by the forward exchange rate. In the period from 1994 to 2009 Consensus forecasts for effective euro-area consumer price inflation and GDP growth beat the alternatives by a difference which is typically statistically significant. The results are more diverse for the pre-crisis sample (1994 – 2007). The Consensus forecast for euro-area producer price inflation significantly outperforms the <b>naive</b> <b>forecast</b> in the short-term. Finally, the Consensus forecast for the USD/EUR exchange rate during the period from 2002 to 2009 is more precise than the <b>naive</b> <b>forecast</b> and the forecast implied by the forward rate. Forecasting accuracy, prediction process, survey forecasts. ...|$|R
40|$|Abstract. Sometimes {{forecasts}} {{of the original}} variable are of interest al-though a variable appears in logarithms (logs) {{in a system of}} time series. In that case converting the forecast for the log of the variable to a <b>naive</b> <b>forecast</b> of the original variable by simply applying the exponential transformation is not optimal theoretically. A simple expression for the optimal forecast un-der normality assumptions is derived. Despite its theoretical advantages the optimal forecast is shown to be inferior to the <b>naive</b> <b>forecast</b> if specification and estimation uncertainty are taken into account. Hence, in practice using the exponential of the log forecast is preferable to using the optimal forecast...|$|R
40|$|This {{research}} compares practical {{methods of}} forecasting basis, using current market information for wheat, soybeans, corn, and milo (grain sorghum) in Kansas. Though generally not statistically superior, an historical one-year average was optimal for corn, milo, and soybean harvest and post-harvest basis forecasts. A one-year average was also best for wheat post-harvest basis forecasts, whereas a five-year average {{was the best}} method for forecasting wheat harvest basis. Incorporating current market information, defined as basis deviation from historical average, improved the accuracy of post-harvest basis <b>forecasts.</b> A <b>naive</b> <b>forecast</b> incorporating current information was often the most accurate for post-harvest basis forecasts. basis forecast, crop basis, current information, <b>naive</b> <b>forecast,</b> Marketing,...|$|R
40|$|Sometimes {{forecasts}} {{of the original}} variable are of interest al- though a variable appears in logarithms (logs) {{in a system of}} time series. In that case converting the forecast for the log of the variable to a <b>naive</b> <b>forecast</b> of the original variable by simply applying the exponential transformation is not optimal theoretically. A simple expression for the optimal forecast un- der normality assumptions is derived. Despite its theoretical advantages the optimal forecast is shown to be inferior to the <b>naive</b> <b>forecast</b> if speci¯cation and estimation uncertainty are taken into account. Hence, in practice using the exponential of the log forecast is preferable to using the optimal forecast. ...|$|R
40|$|Purpose – This paper aims {{to extend}} the {{research}} into company financial <b>forecasts</b> by modelling <b>naïve</b> earnings <b>forecasts</b> derived from normalised historic accounting data disclosed during Australian initial public offerings (IPOs). It seeks to investigate <b>naïve</b> <b>forecast</b> errors and compare them against their management forecast counterparts. It also seeks to investigate determinants of differential error behaviour. Design/methodology/approach – IPOs were sampled and their prospectus forecasts, historic financial data and subsequent actual financial performance were analysed. Directional and absolute forecast error behaviour was analysed using univariate and multivariate techniques. Findings – Systematic factors associated with error behaviour were observed across the management <b>forecasts</b> and the <b>naïve</b> <b>forecasts,</b> the most notable being audit quality. In certain circumstances, the <b>naïve</b> <b>forecasts</b> performed at least as well as management forecasts. In particular, forecast interval was an important discriminator for accuracy, with the superiority of management forecasts only observed for shorter forecast intervals. Originality/value – The results imply a level of "disclosure management" regarding company IPO forecasts and normalised historic accounting data, with forecast overestimation and error size more extreme {{in the absence of}} higher quality third-party monitoring services via the audit process. The results also raise questions regarding the serviceability of normalised historic financial information disclosed in prospectuses, in that many of those data do not appear to enhance the forecasting process, particularly when accompanied by published management forecasts and shorter forecast intervals...|$|R
40|$|We have by Statistics Sweden (SCB) {{been given}} the task of using {{different}} dynamic regression models in order to forecast service price index for sea transport. The aim is to see whether these models provide better forecasts than those previously used. This essay aim to identify, estimate and evaluate the selected prediction models.   Through our data material we were given access to 28 sightings of sea transport index {{during the period of}} 2004 q 1 to 2010 q 4. We have chosen to evaluate three different transfer function models, one ARIMA model and one <b>naive</b> <b>forecasting</b> model. The input variables we decided to test in our transfer function models were the price of petroleum products, the port activity in Swedish ports and the lending rate of Swedish Central bank.   The results of our study suggest that transfer function models generally provide better models than the ARIMA model and the <b>naive</b> <b>forecast</b> model. Results also show that both the transfer function models and ARIMA model seem to provide better models than the <b>naïve</b> <b>forecasting</b> model.   The transfer function model that gave the lowest forecasting errors had interest rate as an input variable...|$|R
40|$|Theory {{predicts that}} life cycle saving and {{consumption}} behaviour could cause real exchange rate variations as the age structure varies. Time series regressions {{show that the}} Swedish demographic structure has significant explanatory power on the real exchange rate during 1960 to 2002. A model using age shares as regressors is used for medium-term out-of-sample forecasts, which perform well both compared to <b>naive</b> <b>forecasts</b> and forecasts based on an autoregressive model. ...|$|R
40|$|It {{is shown}} that {{intrinsic}} neutrino flavor violation invariably occurs when neutrinos are created within the SM augmented by the known massive neutrinos, with mixing and nondegenerate masses. The effects {{are very small}} but {{much greater than the}} <b>naive</b> <b>estimate</b> Δ m^ 2 /E_ν^ 2 or the branching ratio of indirect flavor violating processes such as μ→ eγ within the SM. We specifically calculate the probability (branching ratio) of pion decay processes with flavor violation, such as π→μν̅_e, showing nonzero results. Comment: v 3 : published versio...|$|R
40|$|An approach, {{based on}} a global {{averaging}} procedure, is presented for estimating the power spectrum of a second order stationary zero-mean ergodic stochastic process from a finite length record. This estimate is derived by smoothing, with a cubic smoothing spline, the <b>naive</b> <b>estimate</b> of the spectrum obtained by applying Fast Fourier Transform techniques to the raw data. By means of digital computer simulated results, a comparison is made between {{the features of the}} present approach and those of more classical techniques of spectral estimation. ...|$|R
40|$|If I is non-coercive, a similar, but {{slightly}} more involved, result holds. These results prove, in broad generality, that Monte Carlo {{estimates of the}} steady-state mean position of a RRW have a high likelihood of over-estimation. This has serious implications for the performance evaluation of queueing systems by simulation techniques where steady state expected queue-length and waiting time are key performance metrics. The results show that <b>naïve</b> <b>estimates</b> of these quantities from simulation are highly likely to be conservative. Comment: 23 pages, 8 figure...|$|R
40|$|We {{consider}} {{the implications for}} laser interferometry of the quantum-gravity-motivated modifications in the laws of particle propagation, which are presently being considered in attempts to explain puzzling observations of ultra-high-energy cosmic rays. We show that there are interferometric setups in which the Planck-scale effect on propagation leads to a characteristic signature. A <b>naive</b> <b>estimate</b> is encouraging {{with respect to the}} possibility of achieving Planck-scale sensitivity, but we also point out some severe technological challenges which would have to be overcome in order to achieve this sensitivity. ...|$|R
40|$|The {{results of}} a {{national}} fear of crime survey are compared with results following the use of different nonresponse correction procedures. We compared <b>naive</b> <b>estimates,</b> weighted estimates, estimates after a thorough nonresponse follow-up and estimates after multiple imputation. A strong similarity between the MI and the follow-up-estimates was found. This suggests, that if the assumptions of MAR hold, carefully selected and collected additional data applied in a MI could yield similar estimates to a nonresponse follow-up {{at a much lower}} price and respondent burden. " (author's abstract...|$|R
40|$|The {{generation}} of the plasma current resulting from Bremsstrahlung absorption is considered. It is shown that the electric current {{is higher than the}} <b>naive</b> <b>estimates</b> assuming that electrons absorb only the photon momentum and using the Spitzer conductivity would suggest. The current enhancement is in part because electrons get the recoil momentum from the Coulomb field of ions during the absorption and in part because the electromagnetic power is absorbed asymmetrically within the electron velocity distribution space. Comment: 8 pages, 4 figures, submitted to PR...|$|R
40|$|We {{discuss the}} effects of [eta], [eta]' mixing and of the gluon anomaly in the decays [eta] [...] > 3 [pi] 0, [eta]' [...] > 3 [pi] 0, [psi] [...] >[eta][gamma] and [psi] [...] >[eta]' [gamma]. These effects are {{particularly}} important for [eta]' [...] > 3 [pi] 0 in reducing the total rate by about two orders of magnitude from the <b>naive</b> <b>estimate.</b> The decay rates are calculated for various values of the [eta], [eta]' mixing angle ([theta]) and a reasonable fit with experiment is obtained for - 20 [deg][les][theta][les]- 16 [deg]...|$|R
40|$|We {{argue that}} calculating vacuum energy {{requires}} {{quantum field theory}} whose axioms are adapted to curved spacetime. In this context, we suggest that non-zero vacuum energy is connected to dynamical breaking of electroweak symmetry. The observed meV scale can {{be understood in terms}} of electroweak physics via a <b>naive</b> <b>estimate.</b> The scenario requires all particle masses to have a dynamical origin. Any Higgs particle has to be a composite, and the origin of vacuum energy might be probed at the LHC. Comment: 6 pages, written for the Gravity Research Foundation essay competitio...|$|R
40|$|Strictly {{adhering}} to the standard supersymmetric seesaw mechanism, we present a neutrino mass model which allows successful standard thermal leptogenesis compatible with gravitino cosmology. At {{least some of the}} neutrino Yukawa couplings must be much larger than the <b>naïve</b> <b>estimates</b> following from the seesaw formula. This leads to large BR(µ → eγ), detectable in the next round of experiments. Ratios of µ → eγ, τ → eγ and τ → µγ branching ratios are predicted in terms of the measurable neutrino The observed neutrino masses can be explained by adding to the Standard Model (SM...|$|R
40|$|Strictly {{adhering}} to the standard supersymmetric seesaw mechanism, we present a neutrino mass model which allows successful standard thermal leptogenesis compatible with gravitino cosmology. Some neutrino Yukawa couplings are naturally {{much larger than the}} <b>naive</b> <b>estimates</b> following from the seesaw formula. This leads to large BR(mu -> e gamma), detectable in the next round of experiments. Ratios of mu -> e gamma, tau -> ey and tau -> mu gamma branching ratios are predicted in terms of the measurable neutrino mass matrix. (c) 2005 Elsevier B. V. All rights reserved...|$|R
40|$|A {{simulation}} {{study was}} conducted to compare <b>estimates</b> from a <b>naïve</b> estimator, using standard conditional regression, and an IPTW (Inverse Probability of Treatment Weighted) estimator, to true causal parameters for a given MSM (Marginal Structural Model). The study was extracted from a larger epidemiological study (Longitudinal Study of Effects of Physical Activity and Body Composition on Functional Limitation in the Elderly, by Tager et. al [accepted, Epidemiology, September 2003]), which examined the causal effects of physical activity and body composition on functional limitation. The simulation emulated the larger study in terms of the exposure and outcome variables of interest [...] physical activity (LTPA), body composition (LNFAT), and physical limitation (PF), but used one time-dependent confounder (HEALTH) to illustrate the effects of estimating causal effects in the presence of time-dependent confounding. In addition to being a time-dependent confounder (i. e. predictor of exposure and outcome over time), HEALTH was also affected by past treatment. Under these conditions, <b>naïve</b> <b>estimates</b> are known to give biased estimates of the causal effects of interest (Robins, 2000). The true causal parameters for LNFAT (- 0. 61) and LTPA (- 0. 70) were obtained by assessing the log-odds of functional limitation for a 1 -unit increase in LNFAT and participation in vigorous exercise in an ideal experiment in which the counterfactual outcomes were known for every possible combination of LNFAT and LTPA for each subject. Under conditions of moderate confounding, the IPTW estimates for LNFAT and LTPA were - 0. 62 and - 0. 94, respectively, versus the <b>naïve</b> <b>estimates</b> of - 0. 78 and - 0. 80. For increased levels of confounding of the LNFAT and LTPA variables, the IPTW estimates were - 0. 60 and - 1. 28, respectively, and the <b>naïve</b> <b>estimates</b> were - 0. 85 and - 0. 87. The bias of the IPTW estimates, particularly under increased levels of confounding, was explored and linked to violation of particular assumptions regarding the IPTW estimation of causal parameters for the MSM. Causal inference, Marginal Structural Models (MSMs), Inverse Probability of Treatment causal inference, Marginal Structural Models (MSMs), Inverse Probability of Treatment causal inference, Marginal Structural Models (MSMs), Inverse Probability of Treatment causal inference, Marginal Structural Models (MSMs), Inverse Probability of Treatment Weighted Estimator (IPTW), longitudinal study, functional limitation, body composition, physical activity,...|$|R
5000|$|A 2011 {{paper by}} Filip Novotny and Marie Rakova for the Czech National Bank {{compared}} {{the accuracy of}} the Consensus Forecasts estimates with those of the International Monetary Fund and the Organisation for Economic Co-operation and Development. Echoing the result of Ager, Kappler, and Osterloh, it found: [...] "In the period from 1994 to 2009 Consensus forecasts for effective euro-area consumer price inflation and GDP growth beat the alternatives by a difference which is typically statistically significant. The results are more diverse for the pre-crisis sample (1994-2007). The Consensus forecast for euro-area producer price inflation significantly outperforms the <b>naïve</b> <b>forecast</b> in the short-term. Finally, the Consensus forecast for the USD/EUR exchange rate during the period from 2002 to 2009 is more precise than the <b>naïve</b> <b>forecast</b> and the forecast implied by the forward rate." ...|$|R
40|$|With recent {{advances}} in high throughput technology, researchers often find themselves running {{a large number of}} hypothesis tests (thousands+) and esti- mating a large number of effect-sizes. Generally there is particular interest in those effects estimated to be most extreme. Unfortunately <b>naive</b> <b>estimates</b> of these effect-sizes (even after potentially accounting for multiplicity in a testing procedure) can be severely biased. In this manuscript we explore this bias from a frequentist perspective: we give a formal definition, and show that an oracle estimator using this bias dominates the <b>naive</b> maximum likelihood <b>estimate.</b> We give a resampling estimator to approximate this oracle, and show that it works well on simulated data. We also connect this to ideas in empirical Bayes...|$|R
40|$|Capital market {{forecasts}} sometimes {{suffer from}} a too intensive orientation towards the current market situation and lose their future-orientated character. This phenomenon {{can be referred to}} as Topical Orientated Trend Adjustment (GOVA). For an individual statistical proof we develop a new instrument called GOVA coefficient. In the second step we combine the GOVA coefficient with the conventional forecast quality measure of Theil's inequality coefficient. The result is the Forecast Quality Matrix. With this instrument, it is possible to verify whether the examined forecast is better or worse than the <b>naive</b> <b>forecast</b> and to exclude Topical Orientated Trend Adjustment. This results in four different possibilities: 1. The forecast considering the future is an ideal case. This forecast is significantly better than a <b>naive</b> <b>forecast</b> and is not influenced by the market situation during the making of the forecast. 2. The quasi-naive forecast is characterized ba a Topical Orientated Trend Adjustment, but is not better than the <b>naive</b> <b>forecast.</b> 3. The wrong forecast does not reflect the past, but unfortunately not even the future. 4. The direction-orientated forecast follows the past, but mostly indicates the correct market trend. This classification facilitates the evaluation of the practicability of forecasts and points out the reasons for a possibly lacking forecast accuracy. Forecast quality measures, topical orientated trend adjustment, forecast quality matrix, GOVA-coefficient, quasi-naive forecast...|$|R
40|$|A recent {{experimental}} {{claim of}} {{the detection of}} analogue Hawking radiation in an optical system [PRL 105 (2010) 203901] has led to some controversy [PRL 107 (2011) 149401, 149402]. While this experiment strongly suggests some form of particle creation from the quantum vacuum (and hence it is per se very interesting), {{it is also true}} that it seems difficult to completely explain all features of the observations by adopting the perspective of a Hawking-like mechanism for the radiation. For instance, the observed photons are emitted parallel to the optical horizon, and the relevant optical horizon is itself defined in an unusual manner by combining group and phase velocities. This raises the question: Is this really Hawking radiation, or some other form of quantum vacuum radiation? <b>Naive</b> <b>estimates</b> of the amount of quantum vacuum radiation generated due to the rapidly changing refractive index [...] - sometimes called the dynamical Casimir effect [...] - are not encouraging. However we feel that <b>naive</b> <b>estimates</b> could be misleading depending on the quantitative magnitude of two specific physical effects: "pulse steepening" and "pulse cresting". Plausible bounds on the maximum size of these two effects results in estimates much closer to the experimental observations, and we argue that the dynamical Casimir effect is now worth additional investigation. Comment: 9 pages, 3 figure...|$|R
5000|$|The main {{difference}} with the method for non-seasonal time series, {{is that the}} denominator is the mean absolute error of the one-step [...] "seasonal <b>naive</b> <b>forecast</b> method" [...] on the training set, which uses the actual value from the prior season as the forecast: Ft = Yt−m, where m is the seasonal period.|$|R
40|$|This paper evaluates {{inflation}} forecasts made by Norges Bank {{which is}} a successful forecast targeting central bank. It is expected that Norges Bank produces inflation forecasts that are on average better than other <b>forecasts,</b> both <b>naive</b> <b>forecasts,</b> and forecasts from econometric models outside the central bank. We find that {{the superiority of the}} bank's forecast cannot be asserted, when compared with genuine ex-ante real time forecasts from an independent econometric model. The 1 -step Monetary Policy Report forecasts are preferable to the 1 -step forecasts from the outside model, but for the policy relevant horizons (e. g., 4 quarters ahead), the forecasts from the outsider model are preferred with a wider margin. An explanation in terms of too high speed of adjustment to the inflation target is supported by the evidence. Norges Bank's forecasts are better than the <b>naive</b> <b>forecasts.</b> Norwegian inflation appears to be predictable, despite the reduction in inflation persistence that has taken place over the last two decades. [...] inflation forecasts,monetary policy,forecast comparison,forecast targeting central bank,econometric models...|$|R
40|$|This paper {{analyzes}} {{the impact of}} job insecurity perceptions on individual well-being. In contrast to previous studies, we explicitly take into account perceptions about both the likelihood and the potential costs of job loss and demonstrate that most contributions to the literature suffer from simultaneity bias. When accounting for simultaneity, we find the true unbiased effect of perceived job insecurity {{to be more than}} twice the size of <b>naive</b> <b>estimates.</b> Accordingly, perceived job insecurity ranks {{as one of the most}} important factors in employees' well-being and can be even more harmful than actual job loss with subsequent unemployment. job security, life satisfaction, unemployment...|$|R
40|$|Tiebout {{choice among}} districts {{is the most}} {{powerful}} market force in American public education. <b>Naive</b> <b>estimates</b> of its effects are biased by endogenous district formation. I derive instruments from the natural boundaries in a metropolitan area. My results suggest that metropolitan areas with greater Tiebout choice have more productive public schools and less private schooling. Little of the effect of Tiebout choice works through its effect on household sorting. This finding may be explained by another finding: students are equally segregated by school in metropolitan areas with greater and lesser degrees of Tiebout choice among districts. ...|$|R
