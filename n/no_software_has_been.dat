4|10000|Public
50|$|If {{a system}} is so new that <b>no</b> <b>{{software}}</b> <b>has</b> <b>been</b> written for it, then software is developed on another self-hosting system and placed on a storage device that the new system can read. Development continues this way until the new system can reliably host its own development. Writing new software development tools without using another host system is rare.|$|E
40|$|DNA {{hybridization}} {{methods have}} become {{the most widely used}} tools in molecular biology to identify organisms and evaluate gene expression levels. PCR (Polymerase Chain Reaction) -based methods, fluorescent in situ hybridization (FISH) and the recent development of DNA microarrays as a high throughput technology need efficient primers or probes design. Evaluation of the metabolic capacities of complex microbial communities found in terrestrial or aquatic environments requires new probe design algorithms that reflect the genetic diversity. As {{only a small part of}} the microbial diversity is known, gene sequences deposited in international databases do not reflect the entire diversity. In this context we propose to use oligopeptide sequences for the design of complete set of DNA probes that are able to target the entire genetic diversity of genes encoding enzymes. Due to the degenerated genetic code backtranslation must be managed efficiently. To our knowledge <b>no</b> <b>software</b> <b>has</b> <b>been</b> developed to propose a full backtranslation. This complexity is tractable since we only need to focus on short oligopeptides for DNA probe design. We propose new algorithms that perform a high performance oligopeptide backtranslation into all potential nucleic sequences. We use different efficient techniques such as memory mapping to perform such a computing. We also propose a MPI parallel computing that reduces the whole execution time using data load balancing and network file stream distribution on a cluster architecture...|$|E
40|$|Abstract Background Association mapping using {{abundant}} {{single nucleotide}} polymorphisms is {{a powerful tool for}} identifying disease susceptibility genes for complex traits and exploring possible genetic diversity. Genotyping large numbers of SNPs individually is performed routinely but is cost prohibitive for large-scale genetic studies. DNA pooling is a reliable and cost-saving alternative genotyping method. However, <b>no</b> <b>software</b> <b>has</b> <b>been</b> developed for complete pooled-DNA analyses, including data standardization, allele frequency estimation, and single/multipoint DNA pooling association tests. This motivated the development of the software, 'PDA' (Pooled DNA Analyzer), to analyze pooled DNA data. Results We develop the software, PDA, for the analysis of pooled-DNA data. PDA is originally implemented with the MATLAB ® language, {{but it can also be}} executed on a Windows system without installing the MATLAB ®. PDA provides estimates of the coefficient of preferential amplification and allele frequency. PDA considers an extended single-point association test, which can compare allele frequencies between two DNA pools constructed under different experimental conditions. Moreover, PDA also provides novel chromosome-wide multipoint association tests based on p-value combinations and a sliding-window concept. This new multipoint testing procedure overcomes a computational bottleneck of conventional haplotype-oriented multipoint methods in DNA pooling analyses and can handle data sets having a large pool size and/or large numbers of polymorphic markers. All of the PDA functions are illustrated in the four bona fide examples. Conclusion PDA is simple to operate and does not require that users have a strong statistical background. The software is available at [URL]. </p...|$|E
5000|$|Shortly after {{independent}} researchers {{broke the}} story, security software vendors followed up, releasing {{detailed descriptions of}} the components of XCP - as well as software to remove the [...] cloaking component of it. On the other hand, <b>no</b> <b>software</b> <b>has</b> yet <b>been</b> released to remove the CD-ROM filter driver component. Computer Associates, makers of the PestPatrol anti-spyware software, characterize the XCP software as both a trojan horse and arootkit: ...|$|R
25|$|In 2012, {{according}} to server records, a hacker in Serbia scanned Clinton's Chappaqua server at least twice, in August and in December 2012. It {{was unclear whether}} the hacker knew the server belonged to Clinton, although it did identify itself as providing email services for clintonemail.com. During 2014, Clinton's server {{was the target of}} repeated intrusions originating in Germany, China, and South Korea. Threat monitoring software on the server blocked at least five such attempts. The software was installed in October 2013, and for three months prior to that, <b>no</b> such <b>software</b> <b>had</b> <b>been</b> installed.|$|R
50|$|Web servers are {{not only}} used for serving the World Wide Web. They {{can also be found}} {{embedded}} in devices such as printers, routers, webcams and serving only a local network. The web server may then be used {{as a part of a}} system for monitoring or administering the device in question. This usually means that <b>no</b> additional <b>software</b> <b>has</b> to <b>be</b> installed on the client computer, since only a web browser is required (which now is included with most operating systems).|$|R
40|$|International audienceExcessive daytime {{somnolence}} (EDS) {{is defined}} as the inability to stay awake in daily life activities. Several scales have been used to diagnose excessive daytime sleepiness, the most widely used being the Epworth Sleepiness Scale (ESS). Sleep disorders and EDS are very common in the general population. It is therefore important to be able to screen patients for this symptom in order to obtain an accurate diagnosis of sleep disorders. Embodied Conversational Agents (ECA) have been used in the field of affective computing and human interactions but up to now <b>no</b> <b>software</b> <b>has</b> <b>been</b> specifically designed to investigate sleep disorders. We created an ECA able to conduct an interview based on the ESS and compared it to an interview conducted by a sleep specialist. We recruited 32 consecutive patients and a group of 30 healthy volunteers free of any sleep complaints. The ESS is a self-administered questionnaire that asks the subject to rate (with a pen and paper paradigm) his or her probability of falling asleep. For the purpose of our study, the ECA or real-doctor questionnaire was modified as follows: Instead of the “I” formulate, questions were asked as “Do you. ” Our software is based on a common 3 D game engine and several commercial software libraries. It can run on standard and affordable hardware products. The sensitivity and specificity of the interview conducted by the ECA were measured. The best results (sensibility and specificity > 98 %) were obtained to discriminate the sleepiest patients (ESS ≥ 16) but very good scores (sensibility and specificity > 80 %) were also obtained for alert subjects (ESS< 8). ESS scores obtained in the interview conducted by the physician were significantly correlated with ESS scores obtained in the interview the ECA conducted. Most of the subjects had a positive perception of the virtual physician and considered the interview with the ECA as a good experience. Sixty-five percent of the participants felt that the virtual doctor could significantly help real physicians. Our results show that a virtual physician can conduct a very simple interview to evaluate EDS with very similar results to those obtained by a questionnaire administered by a real physician. The expected massive increase in sleep complaints in the near future likely means that more and more physicians will be looking for computerized systems to help them to diagnose their patients...|$|E
40|$|Measurement <b>has</b> always <b>been</b> {{fundamental}} to the progress to any engineering discipline and <b>software</b> testing <b>is</b> <b>no</b> exception. <b>Software</b> metrics <b>have</b> <b>been</b> used in making quantitative/qualitative decisions {{as well as in}} risk assessment and reduction in software projects. In this paper we discuss software measurement and metrics and their fundamental role in software development life cycle. This paper focusing on software test metrics discusses their key role in software testing process and also classifies and systematically analyzes the various test metrics...|$|R
50|$|As of September 2009, {{ownership}} of the ikonboard name (and possibly the software itself) passed back to Joshua Johnson. In June 2010, Ikonboard 3.1.5A was released, containing bug fixes and minor updates. In May 2012 {{it was reported that}} Ikonboard PHP was making progress, and is now used as the support forum. However, as of January 2017 <b>no</b> new <b>software</b> releases <b>have</b> <b>been</b> made, and their website has ceased to exist. The WHOIS records suggest the domain is no longer owned by either Ikonboard Services Inc or Joshua Johnson.|$|R
40|$|Sweave {{combines}} typesetting with LATEX {{and data}} anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, [...] .) is created {{on the fly}} and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence <b>no</b> new <b>software</b> <b>has</b> to <b>be</b> learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research. (author's abstract) Series: Report Series SFB "Adaptive Information Systems and Modelling in Economics and Management Science...|$|R
40|$|Phishing is a {{semantic}} attack that {{takes advantage of}} the naivety of the human behind electronic systems (e. g. e-banking). Educating end-users can minimize the impact of phishing attacks, however it remains relatively expensive and time consuming. Thus, many software-based solutions, such as classifiers, are being proposed by researchers. However, <b>no</b> <b>software</b> solutions <b>have</b> <b>been</b> proposed to minimize the impact of spear phishing attacks, which are the targeted form of phishing, and have a higher success rate than generic bulk phishing attacks. In this paper, we describe a novel framework to mitigate spear phishing attacks via the use of document authorship techniques — Anti-Spear phishing Content-based Authorship Identification (ASCAI). ASCAI informs the user of possible mismatches between the writing styles of a received email body and of trusted authors by studying the email body itself (i. e. the writeprint), as opposed to traditional user ID-based authentication techniques which can be spoofed or abused. As a proof of concept, we implemented the proposed framework using Source Code Author Profiles (SCAP), and the evaluation results are presented...|$|R
40|$|In {{many areas}} of psychology, one is {{interested}} in disclosing the underlying structural mechanisms that generated an object by variable data set. Often, based on theoretical or empirical arguments, it may be expected that these underlying mechanisms imply that the objects are grouped into clusters that are allowed to overlap (i. e., an object may belong {{to more than one}} cluster). In such cases, analyzing the data with Mirkin’s additive profile clustering model may be appropriate. In this model: (1) each objectmay belong to no, one or several clusters, (2) there is a specific variable profile associated with each cluster, and (3) the scores of the objects on the variables can be reconstructed by adding the cluster-specific variable profiles of the clusters the object in question belongs to. Until now, however, <b>no</b> <b>software</b> program <b>has</b> <b>been</b> publicly available to perform an additive profile clustering analysis. For this purpose, in this article, the ADPROCLUS program, steered by a graphical user interface, is presented. We further illustrate its use by means of the analysis of a patient by symptom data matrix. status: publishe...|$|R
40|$|Motivation: In the bilingual {{world of}} nucleic acids and proteins, usually {{only one of}} the gene’s two DNA strands, namely a sense strand, is protein-coding. The {{preceding}} monolingual RNA world was likely much more strand-symmetric so that both (plus and minus) complementary replicas of ancestral genes could <b>have</b> <b>been</b> used with equal success as first templates (Rodin, Ohno, 1995, 1997; Carter, Duax, 2002). One would expect then for this primordial strand symmetry to be reflected in the genetic code organization. Results: The updated analysis of the current tRNA gene compilation (Sprinzl, Vassilenko, 2005) confirmed and strengthened our previous discovery that in pairs of consensus/ancestral tRNAs with complementary anticodons, their 2 nd bases in the acceptor stem are also complementary (Rodin et al., 1996; Rodin S., Rodin A., 2006). Here we show that, indeed, it might <b>have</b> <b>been</b> an in-frame double-strand coding of RNA genes that has directed the code’s earliest expansion and preserved this fundamental complementary link between the acceptors and the anticodons. Availability: All datasets used in this study and the reconstructed tRNA trees are available from the authors. <b>No</b> proprietary <b>software</b> <b>has</b> <b>been</b> used in this study...|$|R
40|$|In {{behavioral}} research, PARAFAC analysis, a three-mode {{generalization of}} standard {{principal component analysis}} (PCA), {{is often used to}} disclose the structure of three-way three-mode data. To get insight into the underlying mechanisms, one often wants to relate the component matrices resulting from such a PARAFAC analysis to external (two-way two-mode) information, regarding one of the modes of the three-way data. To this end, linked-mode PARAFAC-PCA analysis can be used, in which the three-way and the two-way data set, which have one mode in common, are simultaneously analyzed. More specifically, a PARAFAC and a PCA model are fitted to the three-way and the two-way data, respectively, restricting the component matrix for the common mode to be equal in both models. Until now, however, <b>no</b> <b>software</b> program <b>has</b> <b>been</b> publicly available to perform such an analysis. Therefore, in this article, the LMPCA program, a free and easy-to-use MATLAB graphical user interface, is presented to perform a linked-mode PARAFAC-PCA analysis. The LMPCA software can be obtained from the authors at [URL] For users who {{do not have access to}} MATLAB, a stand-alone version is provided. status: publishe...|$|R
40|$|The Imbrie and Kipp {{transfer}} function method (IKM) {{and the modern}} analog technique (MAT) are accepted tools for quantitative paleoenvironmental reconstructions. However, <b>no</b> uncomplicated, flexible <b>software</b> <b>has</b> <b>been</b> available to apply these methods on modern computer devices. For this reason the software packages PaleoToolBox, MacTransfer, WinTransfer, MacMAT, and PanPlot <b>have</b> <b>been</b> developed. The PaleoToolBox package provides a flexible tool for the preprocessing of microfossil reference and downcore data as well as hydrographic reference parameters. It includes procedures to randomize the raw databases; to switch specific species {{in or out of}} the total species list; to establish individual ranking systems and their application on the reference and downcore databasessemi; and to convert the prepared databases into the file formats of IKM and MAT software for estimation of paleohydrographic parameters...|$|R
40|$|Passive Acoustic Monitoring (PAM) {{recently}} {{extended to}} a very wide range of animals, but <b>no</b> available open <b>software</b> <b>has</b> <b>been</b> sufficiently generic to automatically treat several taxonomic groups. Here we present Tadarida, a software toolbox allowing for the detection and labelling of recorded sound events, and to classify any new acoustic data into known classes. It {{is made up of}} three modules handling Detection, Labelling and Classification and running on either Linux or Windows. This development resulted in the first open software (1) allowing generic sound event detection (multi-taxa), (2) providing graphical sound labelling at a single-instance level and (3) covering the whole process from sound detection to classification. This generic and modular design opens numerous reuse opportunities among (bio) acoustics researchers, especially for those managing and/or developing PAM schemes. The whole toolbox is openly developed in C++ (Detection and Labelling) and R (Classification) and stored at [URL]...|$|R
40|$|The task {{of finding}} optimal {{policies}} in stochastic dynamic systems is challenging. The theory of stochastic dynamic programming (SDP) is quite complex and the available software packages {{are not intended}} for non-specialists. Furthermore, SDP is traditionally limited to quite small and well defined problems. Stochastic optimisation in policy space (SOPS) {{seems to be an}} attractive alternative, particularly for people with a background in simulation of dynamic systems. However, to date <b>no</b> user friendly <b>software</b> <b>has</b> <b>been</b> available for this method. In this paper we present and demonstrate a new program package for this task. The resulting software allows the user to formulate the model in a well-known simulation program, Powersim Studio 2005. The model is automatically transferred to a standalone program. The SOPS program allows the user to reset model parameters, to specify search criteria, and to study the results of repeated searches for optimal policies. Key words: stochastic dynamic optimization, user friendly, Monte Carlo, search, optimization in policy space 1...|$|R
40|$|A {{blind or}} {{visually}} impaired user can get lost in hyperspace on one single W 3 -page. This paper presents Web Access for Blind users (WAB), a working system which makes web-surfing easier for blind persons. Normally, structure and meta information in HTML documents are displayed visually and are difficult for blind users to recognize. WAB transforms structure and meta information into a form that can be more easily read by visually handicapped people. Titles, links and form elements are described textually. Special navigation aids allow the blind user to navigate through the title hierarchy as well as through the list of links. The implemented system works with any World-Wide-Web-browser and any screen reader. <b>No</b> additional <b>software</b> <b>has</b> to <b>be</b> installed on the user's computer. KEYWORDS: Blindness and visual impairment, WWW, W 3, Proxy Servers, User Interfaces, Document Structure and Formats, `Screen Reader'. 1. INTRODUCTION A sighted user can get an overview of the structure of a World- [...] ...|$|R
40|$|Over {{the past}} twenty years, the use of {{computational}} methods {{in the study of}} argumentation <b>has</b> <b>been</b> steadily increasing. Although full automation is not yet possible, smaller software tools that support scholars in their argumentative tasks are becoming widely available. Software is employed, for example, to visualise diagrams of argumentation structures, or to mine argumentative content from large text corpora. Such software implements notions and models specific to an underlying theory. This generally limits the compatibility of the software to that specific theoretical approach. While the pragma-dialectical theory of argumentation {{is one of the leading}} approaches in argumentation studies, <b>no</b> <b>software</b> <b>has</b> yet <b>been</b> developed specifically catered to it. A reason for this may be found in the lack of a formalisation of (parts of) the theory. The current study serves as a foundation for the formalisation and subsequent computational development of pragma-dialectics. A dialogue game is proposed as a formalisation of the ideal model of a critical discussion, which is at the core of the pragma-dialectical methods of analysing and evaluating argumentative discourse. The resulting dialogue game for critical discussion is an interpretation of the ideal model in terms of two interlocutors playing a game to reasonably resolve a difference of opinion. The dialogue game represents essential parts of the model, such as the distinction between different discussion stages, the role of speech acts, and the structure of argumentation. The formalisation as a dialogue game provides a starting point for the development of software based on the pragma-dialectical theory...|$|R
40|$|A priori global {{identifiability}} is {{a structural}} property of biological and physiological models. It {{is considered a}} prerequisite for well-posed estimation, since it concerns the possibility of recovering uniquely the unknown model parameters from measured input-output data, under ideal conditions (noise-free observations and error-free model structure). Of course, determining if the parameters can be uniquely recovered from observed data is essential before investing resources, time and effort in performing actual biomedical experiments. Many interesting biological models are nonlinear but identifiability analysis for nonlinear system {{turns out to be}} a difficult mathematical problem. Different methods <b>have</b> <b>been</b> proposed in the literature to test identifiability of nonlinear models but, to the best of our knowledge, so far <b>no</b> <b>software</b> tools <b>have</b> <b>been</b> proposed for automatically checking identifiability of nonlinear models. In this paper, we describe a software tool implementing a differential algebra algorithm to perform parameter identifiability analysis for (linear and) nonlinear dynamic models described by polynomial or rational equations. Our goal is to provide the biological investigator a completely automatized software, requiring minimum prior knowledge of mathematical modelling and no in-depth understanding of the mathematical tools. The DAISY (Differential Algebra for Identifiability of SYstems) software will potentially be useful in biological modelling studies, especially in physiology and clinical medicine, where research experiments are particularly expensive and/or difficult to perform. Practical examples of use of the software tool DAISY are presented. DAISY is available at the web site [URL]...|$|R
40|$|Many {{commercial}} {{software tools}} exist for {{fault tree analysis}} (FTA), an accepted method for mitigating risk in systems. The method embedded in the tools identifies a root as use in system components, but when software is identified as a root cause, it does not build trees into the <b>software</b> component. <b>No</b> commercial <b>software</b> tools <b>have</b> <b>been</b> built specifically for development and analysis of software fault trees. Research indicates that the methods of FTA {{could be applied to}} software, but the method is not practical without automated tool support. With appropriate automated tool support, software fault tree analysis (SFTA) may be a practical technique for identifying the underlying cause of software faults that may lead to critical system failures. We strive to demonstrate that existing commercial tools for FTA can be adapted for use with SFTA, and that applied to a safety-critical system, SFTA can be used to identify serious potential problems long before integrator and system testing...|$|R
40|$|The {{language}} {{staff at}} the United Nations makes a very selective use of language technologies. So far <b>no</b> computer-assisted translation <b>software</b> <b>has</b> <b>been</b> installed on translators ” workstations even though tests <b>have</b> <b>been</b> conducted for several years on the two major computer-assisted translation (CAT) systems at United Nations Headquarters in New York, for instance. The aim {{of this paper is}} twofold: 1) to show why CAT systems are not considered as potential sources of improvement of quality nor quantity in translation work at the United Nations, and 2) to present the kind of language resources that are considered essential for the adequate rendering of content in any of the six official languages of the United Nations (Arabic, Chinese, English, French, Russian and Spanish). This paper analyzes the particular linguistic and technical constraints specific to an international setting and argues in favour of a selected number of language resources used at the United Nations other than translation tools readily available on the market. Among such language resources, one finds search engines, government and research institutions ” websites, and, in a not too distant future, institutional knowledge bases. 1...|$|R
40|$|Abstract Background Wavelets <b>have</b> {{proven to}} <b>be</b> a {{powerful}} technique {{for the analysis}} of periodic data, such as those that arise in the analysis of circadian oscillators. While many implementations of both continuous and discrete wavelet transforms are available, we are aware of <b>no</b> <b>software</b> that <b>has</b> <b>been</b> designed with the nontechnical end-user in mind. By developing a toolkit that makes these analyses accessible to end users without significant programming experience, we hope to promote the more widespread use of wavelet analysis. Findings We have developed the WAVOS toolkit for wavelet analysis and visualization of oscillatory systems. WAVOS features both the continuous (Morlet) and discrete (Daubechies) wavelet transforms, with a simple, user-friendly graphical user interface within MATLAB. The interface allows for data to be imported from a number of standard file formats, visualized, processed and analyzed, and exported without use of the command line. Our work <b>has</b> <b>been</b> motivated by the challenges of circadian data, thus default settings appropriate to the analysis of such data <b>have</b> <b>been</b> pre-selected in order to minimize the need for fine-tuning. The toolkit is flexible enough to deal {{with a wide range of}} oscillatory signals, however, and may be used in more general contexts. Conclusions We have presented WAVOS: a comprehensive wavelet-based MATLAB toolkit that allows for easy visualization, exploration, and analysis of oscillatory data. WAVOS includes both the Morlet continuous wavelet transform and the Daubechies discrete wavelet transform. We have illustrated the use of WAVOS, and demonstrated its utility {{for the analysis of}} circadian data on both bioluminesence and wheel-running data. WAVOS is freely available at [URL] </p...|$|R
40|$|Abstract. Symphony is {{a virtual}} {{parallel}} network computer. Remote computers participating in the Symphony processing are coupled together simply by setting a local java-enabled web browser, or a local Java Virtual Machine (JVM), to one specific Uniform Reference Locator (URL). The range of computation tasks, which can be processed by Symphony includes Monte-Carlos, image rendering, simulation of stochastic processes and all sort of problem requiring no inter-process communication. A simple job scheduler, an algorithm for the efficient storage of results and online control facilities <b>have</b> <b>been</b> implemented. 1. Purpose of Symphony There is a well-known problem in every site nowadays: the wasted cpu time of idling computers. Many approaches exist {{to solve this problem}} using more or less sophisticated strategies to join together the computing power. Most systems require the installation of complex software, some other even require modifications at operating system level. We follow a straightforward and practical approach 1 to build a massively parallel network com-puter: <b>no</b> further <b>software</b> <b>has</b> to <b>be</b> installed on the remote systems, a Java-enabled browser is sufficient to couple the local computing power to the Symphony system. Due to the platfor...|$|R
40|$|Background : The {{majority}} of amino acid residues are encoded {{by more than}} one codon, and a bias in the usage of such synonymous codons <b>has</b> <b>been</b> repeatedly demonstrated. One assumption is that this phenomenon has evolved to improve the efficiency of translation by reducing the time required for the recruitment of isoacceptors. The most abundant tRNA species are preferred at sites on the protein which are key for its functionality, a behavior which <b>has</b> <b>been</b> termed “translational accuracy”. Although observed in many species, as yet <b>no</b> public domain <b>software</b> <b>has</b> <b>been</b> made available for its quantification. Findings : We present here Seforta (Selection for Translational Accuracy), a program designed to quantify translational accuracy. It searches for synonymous codon usage bias in both conserved and non-conserved regions of coding sequences and computes a cumulative odds ratio and a Z-score. The specification of a set of preferred codons is desirable, but the program can also generate these. Finally, a randomization protocol calculates the probability that preferred codon combinations could have arisen by chance. Conclusions : Seforta is the first public domain program able to quantify translational accuracy. It comes with a simple graphical user interface and can be readily installed and adjusted to the user's requirements...|$|R
40|$|In France, risk {{assessment}} studies <b>have</b> to <b>be</b> conducted before the implementation or the enlargement of some new facilities {{and before the}} reuse of some contaminated sites. These studies <b>have</b> to <b>be</b> performed in accordance to four principles : precautionary, specificity, proportionality, transparence. Until now, <b>no</b> devoted <b>software</b> <b>has</b> <b>been</b> developed in France to assess future exposure and risks for such studies. Because of that, many problems <b>have</b> <b>been</b> reported in exposure assessment studies leading to inconsistency between {{risk assessment}} studies {{and a lack of}} confidence in results provided. To improve the practices and the transparency of the estimates obtained in these studies, INERIS develops and diffuses modeling tools in the framework of one's missions for the Ministry in charge of the Environment. A peer-reviewed handbook, entitled 'Sets of equations for modeling exposure linked to soil contamination or emissions from an industrial facility', <b>has</b> <b>been</b> published and is available on INERIS'website. This document presents the equations used at INERIS for estimating the media concentrations, the exposure and risk levels. It also describes the origin of these equations and underlines the hypotheses on which they are built and their limits. A modeling and simulation platform (MODUL'ERS) based on the equations presented in this handbook <b>has</b> also <b>been</b> developed. Our main objectives during its conception were to provide a tool (1) suited to different site conditions and tier studies, (2) transparent for any stakeholders and helpful to perform uncertainties analysis. MODUL'ERS consists in a library of preset modules enabling the users to build models in accordance with the site conceptual model (that is to say the pathways from the source to the receptor), by downloading modules and connecting them, to create an exposure matrix. Many options are also available to create a customized application. To improve transparency of studies all the equations, parameters can be viewed by the users, as well all the intermediate calculations performed. Especially, hyperlinks enable to browse among variables and their equations. The first version of the tool will be available at soon. Coupling with a GIS is forecast. Some reports will be also published to make more transparent the assignment of values to input parameters, by describing the values collected and the choices made (best-estimate, ranges, probabilistic distributions) ...|$|R
40|$|Refactoring aims at {{improving}} the internal {{structure of a}} software system without changing its external behavior. Previous studies empirically assessed, on the one hand, the benefits of refactoring in terms of code quality and developers' productivity, {{and on the other}} hand, the underlying reasons that push programmers to apply refactoring. Results achieved in the latter investigations indicate that besides personal motivation such as the responsibility concerned with code authorship, refactoring is mainly performed as a consequence of changes in the requirements rather than driven by software quality. However, these findings <b>have</b> <b>been</b> derived by surveying developers, and therefore <b>no</b> <b>software</b> repository study <b>has</b> <b>been</b> carried out to corroborate the achieved findings. To bridge this gap, we provide a quantitative investigation on the relationship between different types of code changes (i. e., Fault Repairing Modification, Feature Introduction Modification, and General Maintenance Modification) and 28 different refactoring types coming from 3 open source projects. Results showed that developers tend to apply a higher number of refactoring operations aimed {{at improving}} maintainability and comprehensibility of the source code when fixing bugs. Instead, when new features are implemented, more complex refactoring operations are performed to improve code cohesion. Most of the times, the underlying reasons behind the application of such refactoring operations are represented by the presence of duplicate code or previously introduced self-admitted technical debts...|$|R
40|$|TET- 1 {{is a small}} {{experimental}} satellite with 11 different space experiments on-board. Built by German space industry it’s a satellite dedicated to verification of newly developed space hardware and software. The German Space Operations Center (GSOC) {{as part of the}} German Aerospace Center (DLR) is responsible for satellite operations. Development of space and ground segment started back in 2006, with a scheduled launch {{in the second half of}} 2011, expected in December. Keeping this in mind we perform a survey of mission preparation activities focusing on first the reuse of existing GSOC mission infrastructure and second the specific adaption’s necessary for TET- 1. Lessons learned are compiled with respect to applicability for other missions, especially with respect to flight procedure development and satellite commanding. Operations team training started in 2010 with different training and simulation sessions. Engineering models and the flight model itself <b>have</b> <b>been</b> used; <b>no</b> dedicated <b>software</b> simulator <b>has</b> <b>been</b> available. Advantages and disadvantages of this approach are discussed. After a concise conclusion of preparation of LEOP and commissioning sequences that are commanded from ground control center a short introduction to the activation sequence of the satellite is given. This sequence is a set of predefined commands executed after spacecraft activation at separation. Discussion is extended to our planned routine operations concept for 11 different payloads. We conclude with a collection of space segment design decisions with the biggest (positive as well as negative) impact on ground segment design and subsequent operations...|$|R
40|$|International audienceThe {{field of}} gamma-ray {{astronomy}} has seen important progress {{during the last}} decade, yet to date <b>no</b> common <b>software</b> framework <b>has</b> <b>been</b> developed for the scientific analysis of gamma-ray telescope data. We propose to fill this gap {{by means of the}} GammaLib software, a generic library that we have developed to support the analysis of gamma-ray event data. GammaLib was written in C++ and all functionality is available in Python through an extension module. Based on this framework we have developed the ctools software package, a suite of software tools that enables flexible workflows to be built for the analysis of Imaging Air Cherenkov Telescope event data. The ctools are inspired by science analysis software available for existing high-energy astronomy instruments, and they follow the modular ftools model developed by the High Energy Astrophysics Science Archive Research Center. The ctools were written in Python and C++, and can be either used from the command line via shell scripts or directly from Python. In this paper we present the GammaLib and ctools software versions 1. 0 that were released at the end of 2015. GammaLib and ctools are ready for the science analysis of Imaging Air Cherenkov Telescope event data, and also support the analysis of Fermi-LAT data and the exploitation of the COMPTEL legacy data archive. We propose using ctools as the science tools software for the Cherenkov Telescope Array Observatory...|$|R
40|$|Uncovering the {{underlying}} clustered structure of Internet {{is essential to}} unveil insights into its functional organization. This thesis is focused on discovering such clustered structure {{in terms of the}} building blocks it is composed of, building blocks ofter referred to as communities. There <b>have</b> <b>been</b> proposed several definitions of community in the literature and in this work we will try to present some of the most studied and widely used, discussing about their characteristics and properties. Among these definitions, the k-clique community has seemed us the most significant in catching the characteristics cohese groups shoud have. The reasons have driven us to focus on such definition of community will be extensively discussed. Extracting k-clique communities requires a substantial amount of computational load at least for real-world datasets. At the best of our knowledge, <b>no</b> existing <b>software</b> tool <b>has</b> <b>been</b> able to extract these communities from the Internet at the Autonomous System level of abstraction and this has encouraged us in developing a new parallel method that could extract communities efficiently by exploiting a parallel shared memory computing architecture togheter with particular data structures known as disjoint-set data structures. In this thesis the new method will be presented and discussed and experimental results showing the efficiency and the effectivity of this method in extracting communities will be exposed. This work will be concluded with the analysis of the structural properties of the Interent in terms of communities and interconnections between them using both sigle- and multi-criterion scores as meters of the quality of such extracted clusters...|$|R
40|$|The Red List Categories and the {{accompanying}} five criteria developed by the International Union for Conservation of Nature (IUCN) provide an authoritative and comprehensive methodology to assess the conservation status of organisms. Red List criterion B, which principally uses distribution data, is {{the most widely used}} to assess conservation status, particularly of plant species. <b>No</b> <b>software</b> package <b>has</b> previously <b>been</b> available to perform large-scale multispecies calculations of the three main criterion B parameters [extent of occurrence (EOO), area of occupancy (AOO) and an estimate of the number of locations] and provide preliminary conservation assessments using an automated batch process. We developed ConR, a dedicated R package, as a rapid and efficient tool to conduct large numbers of preliminary assessments, thereby facilitating complete Red List assessment. ConR (1) calculates key geographic range parameters (AOO and EOO) and estimates the number of locations sensu IUCN needed for an assessment under criterion B; (2) uses this information in a batch process to generate preliminary assessments of multiple species; (3) summarize the parameters and preliminary assessments in a spreadsheet; and (4) provides a visualization of the results by generating maps suitable for the submission of full assessments to the IUCN Red List. ConR can be used for any living organism for which reliable georeferenced distribution data are available. As distributional data for taxa become increasingly available via large open access datasets, ConR provides a novel, timely tool to guide and accelerate the work of the conservation and taxonomic communities by enabling practitioners to conduct preliminary assessments simultaneously for hundreds or even thousands of species in an efficient and time-saving way...|$|R
40|$|Abstra t. Software agents {{represent}} an interesting paradigm to approa h omplex and distributed systems. Their so iality enables to build multiagent systems, where dierent agents intera t {{to pursue their}} goals. Multiagent systems an involve both ooperative and ompetitive agents. In both ases, the omposition of dierent agents {{is an issue that}} must be fa ed by developers. In this paper, we propose to build infrastru tures based on roles, whi h are abstra tions that enable the omposition of dierent agents in an open s enario. Some on rete examples are provided to support our proposal. Key words. agents, roles, multiagent systems, intera tion 1. Introdu tion. With <b>no</b> doubt <b>software</b> agents <b>have</b> <b>been</b> proposing as an innovative paradigm for some years, and we envision a digital world populated by them to support users belonging to the human world. In fa t, they are able to perform tasks on behalf of users, due to their main featuresautonomy, proa tiveness, rea tivity and so iality [19 ℄. In addition, omplex appli ations an be divided into smaller and simpler tasks, ea h one delegated to one agent [18 ℄. This leads to systems omposed of several agents, alled multiagent systems, where agents intera t and oordinate to arry out a ommon goal. Going further, in a wide open s enario the so ial behaviour of the agents implies intera tions not only between agents ooperating in one appli ation, but also between agents of dierent appli ations, whi h may have a ompetitive behaviour, to gain the use of resour es [12 ℄. The feature of mobility [20 ℄, enhan ing the autonomy of agents, implies further advantages...|$|R
40|$|In the inter-subject {{correlation}} (ISC) based {{analysis of}} the {{functional magnetic resonance imaging}} (fMRI) data, the extent of shared processing across subjects during the experiment is determined by calculating correlation coefficients between the fMRI time series of the subjects in the corresponding brain locations. This implies that ISC can be used to analyze fMRI data without explicitly modeling the stimulus and thus ISC is a potential method to analyze fMRI data acquired under complex naturalistic stimuli. Despite of the suitability of ISC based approach to analyze complex fMRI data, <b>no</b> generic <b>software</b> tools <b>have</b> <b>been</b> made available for this purpose, limiting a widespread use of ISC based analysis techniques among neuroimaging community. In this paper, we present a graphical user interface (GUI) based software package, ISC Toolbox, implemented in Matlab for computing various ISC based analyses. Many advanced computations such as comparison of ISCs between different stimuli, time window ISC, and inter-subject phase synchronization are supported by the toolbox. The analyses are coupled with re-sampling based statistical inference. The ISC based analyses are data and computation intensive and the ISC toolbox is equipped with mechanisms to execute the parallel computations in a cluster environment automatically and with an automatic detection of the cluster environment in use. Currently, SGE-based (Oracle Grid Engine, Son of a Grid Engine, or Open Grid Scheduler) and Slurm environments are supported. In this paper, we present a detailed account on the methods behind the ISC Toolbox, the implementation of the toolbox and demonstrate the possible use of the toolbox by summarizing selected example applications. We also report the computation time experiments both using a single desktop computer and two grid environments demonstrating that parallelization effectively reduces the computing time. The ISC Toolbox is available inPeer reviewe...|$|R
40|$|The {{analysis}} of protein glycosylation became of increasing importance in glycobiological research, {{as well as}} for bioprocess monitoring during development and production of biopharmaceuticals, in the recent past. Hence, the systematic study and characterization of the broad variety of glyco-moieties in different cell types and organisms demands for sophisticated analytical tools and dedicated databases in glycobiology, respectively glycobiotechnology. In order to enhance and improve the comparatively small existing glycoanalytical toolbox, automated high-throughput (HT) and high-resolution analysis methods with automated data processing and analysis are required. Besides several mass spectrometry and liquid chromatography based analysis techniques, electrokinetic separation techniques for the {{analysis of}} oligosaccharides became apparent during the recent past. Especially, multiplexed capillary gel electrophoresis with laser induced fluorescence detection (xCGE-LIF), a quite feasible electromigrative separation technique - using standard DNA sequencer equipment - <b>has</b> <b>been</b> developed for HT- glycoprofiling of APTS-labeled glycans [1, 2]. The application of this technique with up to 96 capillaries in parallel, results in massive reduction of the effective separation time per sample combined with an impressive sensitivity achieved due to LIF detection [3]. Due to the speed of the xCGE-LIF based method itself, the analysis of the enormous amount of upcoming data became the HT-bottleneck. Until now, <b>no</b> bioinformatics <b>software</b> tools <b>have</b> <b>been</b> available to automatically process and analyze the xCGE-LIF generated glyco-data. To tackle this issue, the automated analysis software “glyXtool” for xCGE-LIF based data was developed. The software comes as a platform-independent distribution with a user-friendly graphical interface, that supports data-processing, -analysis and -interpretation. The software provides automated raw data smoothing, baseline correction and the normalization to an internal standard, enabling structural elucidation and annotation by referring to a corresponding oligosaccharide-database. Furthermore, a peak comparison function, as well as several implemented statistical tools facilitate HT-screening for biomarkers in large sample sets. The resulting high-performance system (method, software & database) enables fully automated, highly sensitive, instrument-, lab- and operator-independent “real” HT-glycoanalysis in high-resolution, even when operated by non-experts...|$|R
40|$|International audienceDialectology {{addresses}} {{the study of}} the linguistic features of languages having a strong oral tradition like local dialects. These linguistic features can be of very different natures: phonetic, morphosyntactic, lexical, semantic or prosodic. They evolve according to space, time and/or social environment. To study local dialects, corpuses of phonetic data <b>have</b> <b>been</b> transcribed into Linguistic Atlases. These atlases gather a set of maps on which are registered, for a given lexical entry, the phonetic forms collected at several geographical points of inquiries. Between 1902 and 1910, J. Gilliéron and E. Edmont – precursors in the collect, description, the processing and the analysis of dialectal data – have built the French Linguistic Atlas (ALF). Made up of 1700 maps representing 639 points of inquiries, the ALF constitutes a corpus of 1. 086. 300 (1700 x 639) reliable lexical data. Such data were collected from a single questionnaire and noted in a homogeneous way with indication of places, dates and circumstances. This important document provides the base for the analysis of the lexical variations and the elaboration of second-generation atlases, called interpretative atlases, within the framework of international projects. Whereas the theoretical approaches used for the construction of linguistic atlas are structured, reliable and homogeneous, data analysis and the elaboration of interpretative maps is still based on manual and non-standardized approaches. One explanation for such a limitation is the fact that <b>no</b> <b>software</b> solution <b>has</b> <b>been</b> yet designed for the exploitation of geolinguistic data. Most of the work is indeed done manually which on the scale of the ALF is very time-consuming. In particular, the use of Geographical Information Systems (GIS) and spatial analysis methods is poorly developed with lexical maps that remain hand-drawn. Such limitations in the cartographic production capacities do not favour an efficient exploitation of geographical knowledge by researchers in dialectology. It becomes thus urgent to propose software that will facilitate the extraction, the analysis and the sharing of geolinguistic data. This paper introduces GIS for processing dialectology data. Since dialectological data are qualitative, unstructured and heterogeneous data, their integration into a usual GIS software raises various issues for: i) scanning and georeferencing old map; ii) geocoding point of enquiries; iii) structuring and modelling various components of geolinguisitc data; iv) creating geographical information layers adapted to the representation of geolinguistic data. We have defined the specifications of a GeoLinguistic Information System that provides researchers with spatial analysis and geovisualisation functionalities. The way how lexical and dialectological data - coming from heritage cartographical documents - can be integrated into a GIS are studied. We also study what kind of cartographical methodologies may be adapted to the treatment of dialectological data, and we propose various algorithms to automatically create “isoglosses”, namely limits that separate different linguistic areas. By taking into account the heterogeneous spatial distribution of geolinguistic point of enquiry, these algorithms allow to create homogeneous linguistics areas based on various dialectological criteria. The features of the dialectological data coming from Linguistic Atlases of France - edited more than 100 years ago - are presented. The methodology to structure and to model these data is described with their inclusion into a GIS. Our proposal is based on the design of geolinguistic information layers integrating phonetic, lexical, morphosyntactic, motivational and geographical dimensions. Finally, the main functionalities of our Geolinguistic Information System are described. We focus on the geovisualization of dialectological data and more specifically on the various ways to map isoglosses. A case study illustrates ours proposals...|$|R
50|$|Copyrighted works, like software, {{are meant}} to pass into public domain after the {{copyright}} term, losing their copyright privilege. Due to the decades-long copyright protection granted by the Berne Convention, <b>no</b> <b>software</b> <b>has</b> ever passed into public domain by leaving copyright terms. The question how fast works should pass into public domain <b>has</b> <b>been</b> a matter of scientific and public debates, also for software like video games.|$|R
