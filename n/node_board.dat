12|28|Public
50|$|In 2005, Cray {{released}} the X1E upgrade, which uses dual-core processors, allowing two quad-processor nodes {{to fit on}} a <b>node</b> <b>board.</b> The processors are also upgraded to 1150 MHz. This upgrade almost triples the peak performance per board, but reduces the per-processor memory and interconnect bandwidth. X1 and X1E boards can be combined within the same system.|$|E
5000|$|Each Origin 2000 node fit on {{a single}} 16" [...] by 11" [...] printed circuit board that {{contains}} one or two processors, the main memory, the directory memory and the Hub ASIC. The <b>node</b> <b>board</b> plugs into the backplane through a 300-pad CPOP (Compression Pad-on-Pad) connector. The connector actually combines two connections, one to the NUMAlink router network and another to the XIO I/O subsystem.|$|E
50|$|The 2U compute module {{contained}} the processors, memory and four PCI-X slots on two buses. Each compute module features an IP53 <b>node</b> <b>board,</b> which contains two or four MIPS R16000 microprocessors clocked at 600 or 700 MHz with 4 MB of ECC L2 cache, eight DIMM slots for 1 to 8 GB of ECC memory, a Bedrock ASIC {{serving as the}} crossbar for enabling communication between the processors, memory and PCI-X slots.|$|E
50|$|<b>Node</b> <b>boards</b> from Onyx/Origin 350/3900 {{systems are}} {{compatible}} {{and use the}} same RAM. For example, a quad-R16K/700 MHz (8MB L2) board from a CX-Brick will work in a Tezro; likewise, other boards up to the same quad-1 GHz edition.|$|R
50|$|Each Origin 2000 module {{was based}} on nodes that are plugged into a backplane. Each module can contain up to four <b>node</b> <b>boards,</b> two router boards and twelve XIO options. The modules are then mounted inside a deskside {{enclosure}} or a rack. Deskside enclosures can only contain one module, while racks can contain two. In configurations with more than two modules, multiple racks are used.|$|R
50|$|The CX-brick is a 4U-high {{enclosure}} that is {{only used}} in Origin 3900 and Onyx 3900 systems. It differs from the C-brick by containing four <b>node</b> <b>boards</b> and eight-port router ASIC. The CX-brick can support up to 16 processors and 32 GB of memory. The CX-brick initially used the IP53 motherboard that supported 500 MHz R14000 and 600 MHz R14000A processors with 8 MB secondary caches, later upgraded to use the R16000 and R16000A. It connects to the system using NUMAlink 3.|$|R
50|$|The two {{processors}} and their secondary caches is contained on a PIMM (Processor Integrated Memory Module) daughter card that plugs into two 240-pin connectors on the <b>node</b> <b>board.</b> Initially, the Origin 3000 used the 360 MHz R12000 and the 400 MHz R12000A processors with 4 or 8 MB of secondary cache. In May 2001, the 500 MHz R14000 was introduced with 8 MB of secondary cache and in February 2002, the 600 MHz R14000A was made available. Near {{the end of}} its lifetime, the C-brick was updated with 800 MHz MIPS processors.|$|E
50|$|Each <b>node</b> <b>board</b> {{can support}} {{a maximum of}} 4 GB of memory through 16 DIMM slots by using {{proprietary}} ECC SDRAM DIMMs with capacities of 16, 32, 64 and 256 MB. Because the memory bus is 144 bits wide (128 bits for data and 16 bits for ECC), memory modules are inserted in pairs. To support the Origin 2000 distributed shared memory model, the memory modules are proprietary and include directory memory, which contains information on the contents of remote caches for maintaining cache coherency, supporting up to 32 processors. Additional directory memory is required in configurations with more than 32 processors. The additional directory memory is contained on proprietary DIMMs that are inserted into eight DIMM slots set aside for its use.|$|E
50|$|In the KSR design, all of {{the memory}} was treated as cache. The design called for no home location- to reduce storage {{overheads}} and to software transparently, dynamically migrate/replicate memory based on where it was be utilized; A Harvard architecture, separate bus for instructions and memory was used. Each <b>node</b> <b>board</b> contained 256 kB of I-cache and D-cache, essentially primary cache. At each node was 32 MB of memory for main cache. The system level architecture was shared virtual memory, which was physically distributed in the machine. The programmer or application only saw one contiguous address space, which was spanned by a 40-bit address. Traffic between nodes traveled at up to 4 gigabytes per second. The 32 megabytes per node, in aggregate, formed the physical memory of the machine.|$|E
50|$|The I/O {{subsystem}} {{is based}} around the Crossbow (Xbow) ASIC, which shares many similarities with the SPIDER ASIC. Since the Xbow ASIC {{is intended for}} use with the simpler XIO protocol, its hardware is also simpler, allowing the ASIC to feature eight ports, compared with the SPIDER ASIC's six ports. Two of the ports connect to the <b>node</b> <b>boards,</b> and the remaining six to XIO cards. While the I/O subsystem's native bus is XIO, PCI-X and VME64 buses can also be used, provided by XIO bridges.|$|R
5000|$|The XT3 runs an {{operating}} system called UNICOS/lc that partitions the machine into three sections, the largest comprising the Compute nodes, and two smaller sections for Service nodes and IO nodes. In UNICOS/lc 1.x, the Compute PEs run a Sandia developed microkernel called Catamount, which is {{descended from the}} SUNMOS OS of the Intel Paragon; in UNICOS/lc 2.0, Catamount {{was replaced by a}} specially tuned version of Linux called Compute Node Linux (CNL). Service and IO PEs run the full version of SuSE Linux and are used for interactive logins, systems management, application compiling and job launch. I/O PEs use physically distinct hardware, in that the <b>node</b> <b>boards</b> include PCI-X slots for connections to Ethernet and Fibre Channel networks.|$|R
40|$|International audienceA {{method and}} the {{corresponding}} platform devoted to operational SEE-rate prediction are presented and illustrated by experimental results. Predicted error-rates are in well agreement with results issued from the activation of an SRAM platform, in 90 nm technology <b>node,</b> on <b>board</b> stratospheric balloons flights. Direct ionization of protons is investigated for a 65 SRAM memory virtually boarded on the balloon flight...|$|R
50|$|Each {{processor}} and their secondary cache is contained on a HIMM (Horizontal Inline Memory Module) daughter card that plugs into the <b>node</b> <b>board.</b> At {{the time of}} introduction, the Origin 2000 used the IP27 board, featuring one or two R10000 processors clocked at 180 MHz with 1 MB secondary cache(s). A high-end model with two 195 MHz R10000 processors with 4 MB secondary caches was also available. In February 1998, the IP31 board was introduced with two 250 MHz R10000 processors with 4 MB secondary caches. Later, the IP31 board was upgraded to support two 300, 350 or 400 MHz R12000 processors. The 300 and 400 MHz models had 8 MB L2 caches, while the 350 MHz model had 4 MB L2 caches. Near {{the end of its}} life, a variant of the IP31 board that could utilize the 500 MHz R14000 with 8 MB L2 caches was made available.|$|E
50|$|Compute nodes were {{packaged}} two per compute card, with 16 compute cards plus up to 2 I/O nodes per <b>node</b> <b>board.</b> There were 32 node boards per cabinet/rack. By {{the integration}} of all essential sub-systems on a single chip, {{and the use of}} low-power logic, each Compute or I/O node dissipated low power (about 17 watts, including DRAMs). This allowed aggressive packaging of up to 1024 compute nodes, plus additional I/O nodes, in a standard 19-inch rack, within reasonable limits of electrical power supply and air cooling. The performance metrics, in terms of FLOPS per watt, FLOPS per m2 of floorspace and FLOPS per unit cost, allowed scaling up to very high performance. With so many nodes, component failures were inevitable. The system was able to electrically isolate faulty components, down to a granularity of half a rack (512 compute nodes), to allow the machine to continue to run.|$|E
5000|$|The {{design of}} Blue Gene/P is a {{technology}} evolution from Blue Gene/L. Each Blue Gene/P Compute chip contains four PowerPC 450 processor cores, running at 850 MHz. The cores are cache coherent and the chip {{can operate as}} a 4-way symmetric multiprocessor (SMP). The memory subsystem on the chip consists of small private L2 caches, a central shared 8 MB L3 cache, and dual DDR2 memory controllers. The chip also integrates the logic for node-to-node communication, using the same network topologies as Blue Gene/L, but at {{more than twice the}} bandwidth. A compute card contains a Blue Gene/P chip with 2 or 4 GB DRAM, comprising a [...] "compute node". A single compute node has a peak performance of 13.6 GFLOPS. 32 Compute cards are plugged into an air-cooled <b>node</b> <b>board.</b> A rack contains 32 node boards (thus 1024 nodes, 4096 processor cores).By using many small, low-power, densely packaged chips, Blue Gene/P exceeded the power efficiency of other supercomputers of its generation, and at 371 MFLOPS/W Blue Gene/P installations ranked at or {{near the top of the}} Green500 lists in 2007-2008.|$|E
50|$|Similarly, PCBA shorts {{coverage}} in the past {{was defined as the}} number of accessible <b>board</b> <b>nodes</b> divided {{by the total number of}} <b>board</b> <b>nodes.</b> This node-oriented measure did not take into account how nodes may have many or few connection points to device pins, and how some of these could never (in practice) be shorted. Others that could be shorted were also not tested. For example, a small-value inductor, a closed switch or jumper or a small-value resistor could have a short across its terminals, but not be tested due to resolution problems with the impedance measurement.|$|R
40|$|Corporate networks, as {{induced by}} {{interlocking}} directorates between corporations, provide structures of personal communication {{at the level}} of their boards. This paper studies such networks from a perspective of close communication in sub-networks, where each pair of <b>nodes</b> (<b>boards</b> of a corporation) are either neighbours, or have a common neighbour. These correspond to subgraphs of diameter at most 2, designated by us earlier as 2 -clubs, with three types (coteries, social circles and hamlets) as degrees of close communication in social networks, within the concept of boroughs of a network. Boroughs are maximal areas and containers of close communication between nodes of a network. This framework is applied in this paper to an analysis of corporate board interlocks between the top 300 European corporations 2010, as studied by Heemkerk (2013), with data provided by him for that purpose. The paper gives results for several perspectives of close communication in the European corporate network of 2010, a year close to the global crash of 2008, as a further elaboration of those given in Heemskerk (2013). Comment: Keywords: social networks, corporate networks, interlocking directorates, close communication, 2 -clubs, social circle, hamlet, coterie, borough, pivo...|$|R
40|$|Wireless Sensor Networks (WSNs) have {{attracted}} an increasing attention {{in recent years}} {{because of the large}} number of potential applications. They are used for collecting, storing and sharing data, for monitoring application, surveillance purposes and much more. Taking into account such multipurpose applications, a new experimental electronic board has been designed to be used specifically as a multipurpose WSN <b>node.</b> The <b>board</b> has been completely designed as an open system in order to be configured by only varying the firmware on the microcontroller to be connected with different types of sensors, such as, for example, solid state triaxial accelerometer, analog temperature sensors, GNSS receivers, etcâ€¦ The board allow different interfaces and is equipped with a recovery system, guaranteed by a watchdog chip which continuously monitor the onboard microcontroller. A free open source operative system has been ported on the microcontroller in order to give greater flexibility to the node, and to perform multi tasking operations. Low power consumptions together with its compact size, and its multiple functionalities made the board perfectly suited as a multipurpose WSN <b>node.</b> The <b>boards</b> have been already employed in two installed WSN: a GPS monitoring network and an WSN designed as anti-theft alarm system for photovoltaic panels...|$|R
40|$|The 2018 / 2019 upgrade of LHCb Muon System {{foresees}} a 40 MHz readout {{scheme and}} requires {{the development of}} a new Off Detector Electronics (<b>nODE)</b> <b>board</b> that will be based on the nSYNC, a radiation tolerant custom ASIC developed in UMC 130 nm technology. Each <b>nODE</b> <b>board</b> has 192 input channels processed by 4 nSYNCs. The nSYNC is equipped with fully digital TDCs and it implements all the required functionalities for the readout: bunch crossing alignment, data zero suppression, time measurements. Optical interfaces, based on GBT and Versatile link components, are used to communicate with DAQ, TFC and ECS systems...|$|E
40|$|Abstract: Our {{laboratory}} {{has previously}} {{reported on the}} basic design concepts of an updated FireWire based data acquisition system for depth-of-interaction detector systems designed at the University of Washington. The new version of our data acquisition system leverages the capabilities of modern field programmable gate arrays (FPGA) and puts almost all functions into the FPGA, including the FireWire elements, the embedded processor, and pulse timing and integration. The design is centered around an acquisition <b>node</b> <b>board</b> (ANB) that includes 64 serial ADC channels, one high speed parallel ADC, FireWire 1394 b support, the FPGA, a serial command bus and signal lines to support a rough coincidence window implementation to reject singles events from being sent on the FireWire bus. Adapter boards convert detector signals into differential paired signals to connect to the ANB. In this paper we discuss many of the design details, including steps taken to minimize the number of layers in the printed circuit board and to avoid skewing of parallel signals and unwanted bandwidth limitations. I...|$|E
40|$|A two-person {{zero-sum game}} invented by Andrew Gleason {{in the early}} 1950 's has a very simple {{description}} and yet {{turns out to be}} quite di#cult to solve. This game is a stochastic game with an information lag for both players. No strategy with a bounded memory of past moves can be optimal. Yet using the notion of generalized subgames, we show that there exist optimal strategies of a simple nature based on functions easily approximable by standard methods of computation for stochastic games. 1. Description of the game. Two players, Andy and Dave, move a counter around a three <b>node</b> <b>board.</b> The nodes are arranged in a circle and are labeled + 1, + 2, and - 3. Initially the counter rests on node + 1 and Andy starts. + 1 + 2 - 3 Thereafter, the players alternate moves. There is a one move delay in informing the players of the position of the counter, so that, except for the first move, players make their moves only knowing the node from which the opponent has just moved. A move consists o [...] ...|$|E
40|$|Corporate networks, as {{induced by}} {{interlocking}} directorates between corporations, provide structures of personal communication between their boards. This paper studies such networks using {{the framework of}} a previous paper by Laan et al. (Soc Netw Anal Min, 2016. doi: 10. 1007 /s 13278 - 016 - 0326 - 0) where close communication is defined by sub-networks, so that each pair of <b>nodes</b> (<b>boards</b> of a corporation) are either neighbours or have at least one common neighbour. These correspond to sub-graphs of diameter at most 2, designated by us earlier as 2 -clubs of three types (coteries, social circles and hamlets), and conform three levels of close communication in social networks. They are all contained within the disjoint boroughs of a network, supercommunities which envelope all close communication between nodes of a network. This framework is applied here to an analysis of corporate board interlocks between the top 300 European corporations 2010, using the data from an earlier study by one of us (Heemskerk in Econ Soc 42 : 74 - 101, 2013). While the results corroborate the main findings of the earlier studies, our approach also uncovers additional, thus far unrevealed patterns. A single dominant European borough with the Francophone network as its centre and that of Germany only regionally and internally connected. The UK business elite on the other hand is very present and prominent in this European structure of corporate close communication...|$|R
50|$|One of {{the first}} nCUBE {{machines}} to be released was the nCUBE 10 of late 1985. It was originally called NCUBE/ten but the name morphed over time. These {{were based on a}} set of custom chips, where each compute node had a processor chip with 32-bit ALU, a 64-bit IEEE 754 FPU, special communication instructions, and 128 KB of RAM. A node delivered 2 MIPS, 500 kiloFLOPS (32-bit single precision), or 300 kiloFLOPS (64-bit double precision). There were 64 <b>nodes</b> per <b>board.</b> The host board, based on an Intel 80286, ran Axis, a custom Unix-like operating system, and each compute node ran a 4KB kernel, Vertex.|$|R
30|$|In [11], Brust et al. present {{algorithms}} for cluster head candidate selection {{that are}} based on topology (location) of the nodes. The algorithms aim to avoid selecting nodes located close to the network partition border because those nodes are more likely {{to move out of the}} partition, thus, causing a CH re-election. By using the connectivity information, they propose three algorithms to find the strong, weak, bridge, and <b>board</b> <b>nodes</b> in the network. Authors do not provide any information on how to select the CHs among their selection of nodes (strong, weak, bridge, and <b>board</b> <b>nodes).</b> Overall, this classification of nodes for CH selection would be useful for the mobile ad hoc networks (MANETs) where mobility is the prime factor that changes the network topology. However, the network topology in WSNs is quite stable compared to MANETs, and therefore, this kind of node classification is unnecessary and also expensive (power consuming) for CH selection.|$|R
30|$|There {{are many}} ways to support {{mobility}} for the <b>nodes</b> on <b>board</b> of the train. In this article, we assumed that the network mobility is managed solely by the MCE on board of the train and that the onboard devices are unaware {{of the fact that they}} are part of a moving network. Moreover, all third party communicating nodes are also unaware of any mobility of the onboard devices. Therefore, all data traffic is routed to the WCE at the wayside. This way, mobility is transparent in the complete system (which was reflected in the addressing scheme in Figure 9) and it allows to centrally optimize bandwidth capacity, prioritize some data traffic flows etc.|$|R
5000|$|Several art {{critics and}} editors {{have been writing}} about his art: Flati {{represents}} the world in its complexities, grasping the moment in which it organizes itself, shapelessly flowing and defines itself from the first sound of the big bang, bursting and expanding in the euphony of space-time. In the works of Flati there are tangled woods, metallic <b>nodes,</b> electronic <b>boards,</b> broken glass, grains of sand, stones, and nutshell sounds of Triton. Every element has {{the memory of the}} arpeggio of the forest, the lapping of the waves, the sounds of electronic machines, the inebriation of the wind meeting the clouds. A new polyphony of space, a new melody with multiple voices, a sort of ars nova, a music between art and science. Luigina Bortolatto ...|$|R
40|$|The Expected-Outcome Model of Two-Player Games {{deals with}} the expected-outcome model of two-player games, in which the {{relative}} merit of game-tree <b>nodes,</b> rather than <b>board</b> positions, is considered. The ambiguity of static evaluation and the problems it generates in the search system are examined {{and the development of}} a domain-independent static evaluator is described. Comprised of eight chapters, this book begins with an overview of the rationale for the mathematical study of games, followed by a discussion on some previous artificial intelligence (AI) research efforts on game-trees. The ne...|$|R
40|$|The DDMlite is a COMA shared-memory {{multiprocessor}} prototype {{built at}} SICS. Starting in June last year, {{less than two}} man years has been spent on architecture, hardware design, implementation and debugging to a running system. Using experience from previous projects, focusing on the research-relevant architectural issues while sacrificing some performance, and using state-of-the art tools, {{we have been able}} to build a functional multiprocessor prototype with a minimal effort. When fully equipped the DDMlite will have 24 processors and 192 MB COMA attraction memory. The custom <b>node</b> controller <b>board</b> consists of three Xilinx XC 4013 and one AMD Mach 435 FPGAs, SRAM memory and buffer circuits. The DDMlite is an implementation of the BB-COMA architecture previously presented in [1]. We present the detailed architecture of the DDMlite - in particular the node controller implementation, and explain the complexity/performance trade-offs that enabled us to reach our goals on time. Early results from experiments with the prototype will als be presented...|$|R
50|$|SGI {{continued}} to enhance its line of servers (including some supercomputers) {{based on the}} SN architecture. SN, for Scalable Node, is a technology developed by SGI in the mid-1990s that uses cache-coherent non-uniform memory access (cc-NUMA). In an SN system, processors, memory, and a bus- and memory-controller are coupled together into an entity called a node, usually on a single circuit <b>board.</b> <b>Nodes</b> are connected by a high-speed interconnect called NUMAlink (originally marketed as CrayLink). There is no internal bus, and instead access between processors, memory, and I/O devices is done through a switched fabric of links and routers.|$|R
40|$|The Coldfire SDN Diagnostics {{software}} is a flexible means of exercising, testing, and debugging custom computer hardware. The {{software is}} a set of routines that, collectively, serve as a common software interface through which one can gain access to various parts of the hardware under test and/or cause the hardware to perform various functions. The routines can be used to construct tests to exercise, and verify the operation of, various processors and hardware interfaces. More specifically, the software can be used to gain access to memory, to execute timer delays, to configure interrupts, and configure processor cache, floating-point, and direct-memory-access units. The software is designed to be used on diverse NASA projects, and can be customized for use with different processors and interfaces. The routines are supported, regardless of the architecture of a processor that one seeks to diagnose. The present version of the software is configured for Coldfire processors on the Subsystem Data <b>Node</b> processor <b>boards</b> of the Solar Dynamics Observatory. There is also support for the software with respect to Mongoose V, RAD 750, and PPC 405 processors or their equivalents...|$|R
40|$|This paper {{deals with}} design, {{implementation}} {{and testing of}} device driver for a DUART TL 16 C 2550 peripheral on a MDROADM board. The MDROADM board is used in optical networks for switching the optical signals between the <b>nodes.</b> The MDROADM <b>board</b> is designed with a MPC 8308 PowerQUICC II Pro Processor as the main processor and the TL 16 C 2550 peripheral is used for extending the serial interface on the board. The MDROADM board runs an embedded Linux operating system. The device driver for the DUART TL 16 C 2550 is designed based on the customized interfacing with the MPC 8308 PowerQUICC II Pro Processor. The device driver is in compliance with Linux kernel V 2. 6. 29. ...|$|R
40|$|The Stage 3 {{revision}} {{sets up the}} Theory Palette with hues and {{tones of}} green (author, history, and nature), yellow (culture), purple (psyche), grey (text), red (reader), dark blue (literature), light blue (language), rose (embodiment/perception), and hot pink (intersectional feminisms). Airbrush and pencil tools suggest color blending without diminishing the complexity of theory hues and tones, to allow more nuanced explorations of the theory concerns in play, in individual texts and across our readings. Larger paint splotches echo the setup of an artist’s palette; smaller paint splotches indicate secondary and tertiary hues and tones. Pencil lines complement the paint splotches, layering in circulation, gradation, and entanglement. In an curious side-effect, the airbrushed splotches and curling pencil lines also resemble stylized strands of holiday lights or <b>nodes</b> on circuit <b>boards...</b>|$|R
40|$|Abstract—Internet of Things is envisioned to {{drastically}} {{chance the}} way sensor data from physical phenomena can be utilized by users on the Internet. However, one concern in deploying {{and maintaining a}} large number of sensor nodes is that replacing spent batteries will not be feasible. One solution to this issue may involve utilising energy harvesting technologies, e. g. solar, heat, or vibration, with solar being the most promising for general applications. However, using solar panels is currently a relatively ex-pensive approach as they require a time-consuming and therefore costly assembly process. As an alternative, this paper suggests a new approach to powering networked sensors: the direct integration of a solar cell onto a sensor <b>nodes</b> printed circuit <b>board.</b> This approach eliminates the need for manual assembly and the use of expensive connectors. Keywords-Dye sensitised solar cells, energy harvesting, Internet of Things, wireless sensor networks I...|$|R
40|$|Abstract. In {{a transit}} network {{involving}} vehicles with rigid capacities, we advocate {{the use of}} strategies for describing consumer behavior. At each <b>boarding</b> <b>node,</b> a user sorts the transit lines in decreasing order of preference, and boards the first vehicle in this list whose residual capacity is nonzero. Since a user’s position in the queue varies from day to day, the delay experienced is stochastic. This leads to an equilibrium problem where, at a solution, users are assigned to strategies that minimize their expected delay. This situation is formulated as a variational inequality, whose cost mapping is discontinuous and strongly asymmetric, due to the priority of current passengers over incoming users. We prove that the solution set is nonempty and provide numerical results obtained by an efficient solution algorithm. Key words. transit networks – equilibrium assignment – strategy – hyperpath – capacities – priorities – variational inequalities 1...|$|R
40|$|Abstract − For {{monitoring}} natural landslide disaster, wireless sensing node {{network is}} so effective system. The sensing node is constructed by some sensors (acceleration, soil moisture, GPS and so on), data processing unit (micro processor) and wireless communication unit. The network topology we have constructed is mesh type. In the system, at sensing landslide, the node will transport {{the information to}} host system through the node network by using wireless communication. Generally, in natural field, the landslide is sensed as the changing of acceleration. But, {{it is so difficult}} to distinguish the landslide obviously. The acceleration of the sensing node will change by not only the landslide disaster but also collision with several hard blocks like stones. This paper describes the distinction method of landslide disaster by using only the signal of acceleration sensor mounted in the sensing node. The method focuses the time transition of frequency distribution of the acceleration signal. Observing the time transition, the situations of sensing node have been distinguished. 1) the network is constructed autonomously, 2) the network topology uses mesh type comparing to tree type, 3) node path for data transmission switches flexibly to avoid broken sensing nodes, 4) the network takes in new nodes flexibly, 5) the data transmission is controlled automatically. Fig. 3 shows the system construction of sensing node. The sensing node is constructed with four main units; <b>node</b> control <b>board,</b> sensor unit, GPS unit and wireless communication unit...|$|R
40|$|Two {{complementary}} wireless sensor nodes {{for building}} two-tiered heterogeneous networks are presented. A larger node with a 25 mm by 25 mm size {{acts as the}} backbone of the network, and can handle complex data processing. A smaller, cheaper node with a 10 mm by 10 mm size can perform simpler sensor-interfacing tasks. The 25 mm node is based on previous work that has been done in the Tyndall National Institute that created a modular wireless sensor node. In this work, a new 25 mm module is developed operating in the 433 / 868 MHz frequency bands, with a range of 3. 8 km. The 10 mm node is highly miniaturised, while retaining a high level of modularity. It has been designed to support very energy efficient operation for applications with low duty cycles, with a sleep current of 3. 3 μA. Both nodes use commercially available components and have low manufacturing costs to allow the construction of large networks. In addition, interface boards for communicating with nodes have been developed for both the 25 mm and 10 mm <b>nodes.</b> These interface <b>boards</b> provide a USB connection, and support recharging of a Li-ion battery from the USB power supply. This paper discusses the design goals, the design methods, and the resulting implementation...|$|R
