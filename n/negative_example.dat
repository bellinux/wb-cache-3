152|1559|Public
25|$|Sun Quan mourned Chen Wu's {{death and}} {{attended}} the latter's funeral. Sun Quan also had Chen Wu's favourite concubine sacrificed to join Chen in death, and he awarded Chen's family 200 taxable households in their estate. The historian Sun Sheng criticised Sun Quan's act of forcing Chen Wu's concubine to join Chen in death, citing an earlier <b>negative</b> <b>example</b> of Duke Mu of Qin {{and a positive}} example of Wei Ke (魏顆).|$|E
25|$|In Federalist No. 8, Alexander Hamilton {{worried that}} {{maintaining}} a large standing army {{would be a}} dangerous and expensive undertaking. In his principal argument for the ratification of the proposed constitution, he argued that only by maintaining a strong union could the new country avoid such a pitfall. Using the European experience as a <b>negative</b> <b>example</b> and the British experience as a positive one, he presented {{the idea of a}} strong nation protected by a navy with no need of a standing army. The implication was that control of a large military force is, at best, difficult and expensive, and at worst invites war and division. He foresaw the necessity of creating a civilian government that kept the military at a distance.|$|E
25|$|The machine {{learning}} community most often uses the ROC AUC statistic for model comparison. However, this practice {{has recently been}} questioned based upon new {{machine learning}} research that shows that the AUC is quite noisy as a classification measure and has some other significant problems in model comparison. A reliable and valid AUC estimate {{can be interpreted as}} the probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen <b>negative</b> <b>example.</b> However, the critical research suggests frequent failures in obtaining reliable and valid AUC estimates. Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. Nonetheless, the coherence of AUC as a measure of aggregated classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.|$|E
5000|$|... 7 Yaş ve Üzeri & Olumsuz (7+ <b>Negative</b> <b>Examples)</b> is {{used for}} shows {{suitable}} for children aged 7 and up which contain <b>negative</b> <b>examples.</b> Shows in the Total Drama franchise currently use this rating.|$|R
30|$|We {{start from}} the {{original}} set of N examples described by nominal and numerical attributes that may contain unknown values. An artificial classification problem is formulated as follows: the examples from the original set constitute the positive <b>examples,</b> while the <b>negative</b> <b>examples</b> are artificially constructed by shuffling {{the values of the}} original examples. The shuffling is performed at the level of attributes so that we randomly mix values among the examples. The values remain within the same attribute as in the original set of examples. As a result, we have the same values in positive and <b>negative</b> <b>examples,</b> but in <b>negative</b> <b>examples</b> we have randomized connections between the attribute values. For small problems with up to 200 examples, we typically construct four times as many <b>negative</b> <b>examples</b> as in the original (positive) example set, while for larger domains we construct the same number of positive and <b>negative</b> <b>examples.</b>|$|R
5000|$|Operationalize(Literal, Positive <b>examples,</b> <b>Negative</b> <b>examples)</b> ...|$|R
2500|$|A bounded poset admits a grading if {{and only}} if all maximal chains in P have the same length: and [...] both being maximal chains. setting the rank of the least element to 0 then determines the rank {{function}} completely. This covers many finite cases of interest; see picture for a <b>negative</b> <b>example.</b> However, unbounded posets can be more complicated.|$|E
2500|$|Smith {{had been}} contemplating writing a [...] "space-police novel" [...] since early 1927; {{once he had}} [...] "the Lensmen's {{universe}} fairly well set up", he reviewed his science-fiction collection for [...] "cops-and-robbers" [...] stories. He cites Clinton Constantinescue's [...] "War of the Universe" [...] as a <b>negative</b> <b>example,</b> and Starzl and Williamson as positive ones. Tremaine responded extremely positively to {{a brief description of}} the idea.|$|E
2500|$|Color of title, {{claim of}} title, or claim of right. Color of title and claim of title involve a legal {{document}} that appears (incorrectly) {{to give the}} disseisor title. In some jurisdictions the mere intent to take the land as one's own may constitute [...] "claim of right", with no documentation required. [...] Other cases have determined that a claim of right exists if the person believes he has rightful claim to the property, even if that belief is mistaken. A <b>negative</b> <b>example</b> would be a timber thief who sneaks onto a property, cuts timber not visible from the road, and hauls the logs away at night. His actions, though they demonstrate actual possession, also demonstrate knowledge of guilt, as opposed to claim of right.|$|E
5000|$|... 7 Yaş ve Üzeri & Şiddet & Olumsuz (7+ with Violence and <b>Negative</b> <b>Examples)</b> is {{used for}} shows {{suitable}} for children aged 7 and up which contain both fantasy violence and <b>negative</b> <b>examples.</b> Adventure Time and Regular Show currently use this rating.|$|R
40|$|PU {{learning}} occurs {{frequently in}} Web pages classification and text retrieval applications because users {{may be interested}} in information on the same topic. Collecting reliable <b>negative</b> <b>examples</b> is a key step in PU (Positive and Unlabeled) text classification, which solves a key problem in machine learning when no labeled <b>negative</b> <b>examples</b> are available in the training set or <b>negative</b> <b>examples</b> are difficult to collect. Thus, this paper presents a novel clustering-based method for collecting reliable <b>negative</b> <b>examples</b> (C-CRNE). Different from traditional methods, we remove as many probable positive examples from unlabeled set as possible, which results that more reliable <b>negative</b> <b>examples</b> are found out. During the process of building classifier, a novel TFIDF-improved feature weighting approach, which reflects the importance of the term in the positive and <b>negative</b> training <b>examples</b> respectively, is presented to describe documents in the Vector Space Model. We also build a weighted voting classifier by iteratively applying the SVM algorithm and implement OCS (One-class SVM), PEBL (Positive Example Based Learning) and 1 -DNFII (Constrained 1 -DNF) methods used for comparison. Experimental results on three real-world datasets (Reuters Corpus Volume 1 (RCV 1), Reuters- 21578 and 20 Newsgroups) show that our proposed C-CRNE extracts more reliable <b>negative</b> <b>examples</b> than the baseline algorithms with very low error rates. And our classifier outperforms other state-of-art classification methods from the perspective of traditional performance metrics...|$|R
40|$|Abstract. This paper {{describes}} use of <b>negative</b> <b>examples</b> {{in training}} the HVS semantic model. We present a novel initialization of the lexical model using <b>negative</b> <b>examples</b> extracted automatically from a semantic corpus {{as well as}} description of an algorithm for extraction these examples. We evaluated the use of <b>negative</b> <b>examples</b> on a closed domain human-human train timetable dialogue corpus. We significantly improved the standard PARSEVAL scores of the baseline system. The labeled F-measure (LF) was increased from 45. 4 % to 49. 1 %. ...|$|R
2500|$|The Bowdlers {{were not}} the first to {{undertake}} such a project, but, despite being considered a <b>negative</b> <b>example</b> by some, their editions made it more acceptable to teach Shakespeare to wider and younger audiences. The poet Algernon Charles Swinburne said, [...] "More nauseous and more foolish cant was never chattered than that which would deride the memory or depreciate the merits of Bowdler. No man ever did better service to Shakespeare than the man who made it possible to put him into the hands of intelligent and imaginative children". Bowdler's commitment not to augment Shakespeare's text was in contrast with the practice of some earlier editors and performers. [...] Nahum Tate as Poet Laureate had rewritten the tragedy of King Lear with a happy ending. [...] In 1807 Charles Lamb and Mary Lamb published Tales from Shakespeare for children with synopses of 20 of the plays, seldom quoting the original text.|$|E
2500|$|The Nazis on the {{one hand}} would jam {{transmissions}} from the Allies' stations, {{but on the other hand}} would also copy them. The band Charlie and His Orchestra is considered as a <b>negative</b> <b>example,</b> also called Mr. Goebbels Jazz Band. Several of Germany’s most talented swing musicians, such as saxophonist Lutz Templin and vocalist Karl “Charlie” Schwedler, were active in a Jazz band. Here the Nazis replaced the original texts with their own provocative propaganda texts that were pro-Nazi and anti-American/British. For example, the lyrics for “Little Sir Echo” has anti-American/British appeal with lyrics such as “German U-boats are making you sore, You’re always licked, not a victory came through…You’re nice, little fellow, but by now you should know that you can never win this war!” [...] Goebbels’ propaganda was broadcast over pirated short-wave frequencies into America, Britain, and Canada in order to spread fear and weaken the morale of Germany’s enemies (WFMU Staff).|$|E
2500|$|Once Dawn Doughnuts became {{profitable}} in late 1936, Smith {{wrote an}} 85-page outline for {{what became the}} four core Lensman novels; in early 1937, Tremaine committed to buying them. Segmenting the story into four novels required considerable effort to avoid dangling loose ends; Smith cites Edgar Rice Burroughs as a <b>negative</b> <b>example.</b> After the outline was complete, he wrote a more detailed outline of Galactic Patrol, plus a detailed graph of its structure, with [...] "peaks of emotional intensity and the valleys of characterization and background material." [...] He notes, however, that he {{was never able to}} follow any of his outlines at all closely, as the [...] "characters get away from me and do exactly as they damn please." [...] After completing the rough draft of Galactic Patrol, he wrote the concluding chapter of the last book in the series, Children of the Lens. Galactic Patrol was published in the September 1937 through February 1938 issues of Astounding; unlike the revised book edition, it was not set in the same universe as Triplanetary.|$|E
5000|$|Add Operationalize(L, Positive <b>examples,</b> <b>Negative</b> <b>examples)</b> to OperationalLiterals ...|$|R
40|$|Concept Learning is a Machine Learning {{technique}} {{in which the}} learning process is driven by providing positive and <b>negative</b> <b>examples</b> to the learner. From those examples, the learner builds a hypothesis (concept) that describes the positive examples and excludes the <b>negative</b> <b>examples.</b> Inductive Logic Programming (ILP) systems hav...|$|R
40|$|Identifying {{transcription}} factor binding sites computationally {{is a hard}} problem as it produces many false predictions. Combining the predictions from existing predictors can improve the overall predictions by using classification methods like Support Vector Machines (SVM). But conventional <b>negative</b> <b>examples</b> (that is, example of non-binding sites) {{in this type of}} problem are highly unreliable. In this study, we have used different types of <b>negative</b> <b>examples.</b> One class of the <b>negative</b> <b>examples</b> has been taken from far away from the promoter regions, where the occurrence of binding sites is very low, and another one has been produced by randomization. Thus we observed the effect of using different <b>negative</b> <b>examples</b> in predicting {{transcription factor}} binding sites in mouse. We have also devised a novel cross-validation technique for this type of biological problem. © 2011 Springer-Verlag. </p...|$|R
2500|$|Lichtman compares and {{contrasts}} Trump's actions {{with the}} Impeachment of Richard Nixon {{stemming from the}} Watergate scandal. Discussion of prior impeachment proceedings for presidents Andrew Johnson and Bill Clinton are placed within historical context, and the book makes comparisons between Trump and Richard Nixon. The author writes that Trump's decisions threaten American values: [...] "Even early in his presidency, Donald Trump exhibits the same tendencies that led Nixon to violate the most basic standards of morality and threaten the foundations of our democracy." [...] Lichtman argues that Trump's failure to learn from Nixon's <b>negative</b> <b>example</b> will harm his presidency, noting: [...] "They also shared a compulsion to deflect blame, and they were riddled with insecurities." [...] He criticizes what he documents as Trump's disregard for veracity: [...] "Neither man allowed the law, the truth, the free press, or the potential for collateral damage to others to impede their personal agendas." [...] Lichtman feels Trump and Nixon's potential political success {{can be viewed as}} stifled by a perceived need for covert decision making devoid of critical viewpoints: [...] "They obsessed over secrecy and thirsted for control without dissent." ...|$|E
50|$|If both {{positive}} and <b>negative</b> <b>example</b> strings are given, Dupont et al. build the lattice from the positive examples, and then investigate the separation border between automata that generate some <b>negative</b> <b>example</b> and such that do not.Most interesting are those automata immediately below the border.In the image, separation borders are shown for the <b>negative</b> <b>example</b> strings 11, 1001, 101, 0.|$|E
5000|$|Also {{appeared}} on That '70s Show as Glen, a man stuck in an awful marriage with his high-school sweetheart whose <b>negative</b> <b>example</b> gives Eric {{second thoughts about}} marrying Donna ...|$|E
40|$|The {{original}} publication {{is available}} at www. springerlink. com Copyright SpringerIdentifying transcription factor binding sites computationally is a hard problem as it produces many false predictions. Combining the predictions from existing predictors can improve the overall predictions by using classification methods like Support Vector Machines (SVM). But conventional <b>negative</b> <b>examples</b> (that is, example of non-binding sites) {{in this type of}} problem are highly unreliable. In this study, we have used different types of <b>negative</b> <b>examples.</b> One class of the <b>negative</b> <b>examples</b> has been taken from far away from the promoter regions, where the occurrence of binding sites is very low, and another one has been produced by randomization. Thus we observed the effect of using different <b>negative</b> <b>examples</b> in predicting transcription factor binding sites in mouse. We have also devised a novel cross-validation technique for this type of biological problem...|$|R
5000|$|Schema: {{positive}} <b>examples</b> + <b>negative</b> <b>examples</b> + {{background knowledge}} &rArr; hypothesis.|$|R
40|$|Abstract. Restarting {{automaton}} is {{a special}} type of a linear bounded automaton designed for modelling the so-called analysis by reduction. We use genetic algorithms to learn restarting automata to recognize languages according to input consisting of sets of positive and <b>negative</b> <b>examples</b> of words from the language together with positive and <b>negative</b> <b>examples</b> of simplifications. ...|$|R
5000|$|For a <b>negative</b> <b>example,</b> {{consider}} the real {{numbers in the}} unit interval 0,1, ordered by their natural order. This bounded-complete cpo is not algebraic. In fact its only compact element is 0.|$|E
5000|$|Update weight {{according}} to the result of evaluation. There are two cases: predicted positive on <b>negative</b> <b>example</b> ( [...] and targets {{are not in the}} list of active features) and predicted negative on positive example( [...] and targets are in the list of active features).|$|E
50|$|The {{goal was}} to help enlisted men with weak {{literacy}} skills learn through animated cartoons (and also supplementary comic books). They featured simple language, racy illustrations, mild profanity, and subtle moralizing. Private Snafu did (almost) everything wrong, so that his <b>negative</b> <b>example</b> taught basic lessons about secrecy, disease prevention, and proper military protocols.|$|E
50|$|There {{are also}} content informations which {{indicate}} violence/horror, sexuality and <b>negative</b> <b>examples.</b>|$|R
5000|$|Compute {{information}} gain of the clause over Positive <b>examples</b> and <b>Negative</b> <b>examples</b> ...|$|R
3000|$|To {{speed up}} classification, Viola and Jones [9] {{proposed}} a cascade structure where several strong classifiers are associated into successive levels. The {{idea is that}} the first strong classifiers reject most of the <b>negative</b> <b>examples,</b> while the last strong classifiers try to discriminate positive <b>examples</b> from hard <b>negative</b> <b>examples.</b> In such cascades, strong classifiers are slightly changed into [...]...|$|R
50|$|A bounded poset admits a grading if {{and only}} if all maximal chains in P have the same length: setting the rank of the least element to 0 then determines the rank {{function}} completely. This covers many finite cases of interest; see picture for a <b>negative</b> <b>example.</b> However, unbounded posets can be more complicated.|$|E
50|$|In 1970 {{the first}} American-style {{shopping}} mall in Switzerland opened. In 1974, a second, larger one followed. Spreitenbach {{served as a}} <b>negative</b> <b>example</b> for urban sprawl on the Swiss Plateau. Between 1960 and today, the population has more than quintupled. No other municipality in the canton has registered such a large growth in recent times.|$|E
50|$|Biochemical cascades {{exist in}} biology, {{where a small}} {{reaction}} can have system-wide implications. One <b>negative</b> <b>example</b> is ischemic cascade, in which a small ischemic attack releases toxins which kill off far more cells than the initial damage, resulting in more toxins being released. Current research {{is to find a}} way to block this cascade in stroke patients to minimize the damage.|$|E
40|$|Many {{state-of-the-art}} ILP systems require {{large numbers}} of <b>negative</b> <b>examples</b> to avoid overgeneralization. This is a considerable disadvantage for many ILP applications, namely inductive program synthesis where relativelly small and sparse example sets are a more realistic scenario. Integrity constraints are first order clauses that can {{play the role of}} <b>negative</b> <b>examples</b> in an inductive process. One integrity constraint can replace a long list of ground <b>negative</b> <b>examples.</b> However, checking the consistency of a program with a set of integrity constraints usually involves heavy theorem-proving. We propose an efficient constraint satisfaction algorithm that applies {{to a wide variety of}} useful integrity constraints and uses a Monte Carlo strategy. It looks for inconsistencies by random generation of queries to the program. This method allows the use of integrity constraints instead of (or together with) <b>negative</b> <b>examples.</b> As a consequence programs to induce can be specified more rapidl [...] ...|$|R
30|$|The [13] study {{suggested}} that generating randomly the <b>negative</b> <b>examples</b> produces better results.|$|R
5000|$|Inputs Literal to be operationalized, List of {{positive}} <b>examples,</b> List of <b>negative</b> <b>examples</b> ...|$|R
