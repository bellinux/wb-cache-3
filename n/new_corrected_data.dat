1|4713|Public
40|$|This {{dissertation}} {{has involved}} {{the exploration of}} a new effect in photoelectron emission, multi-atom resonant photoemission (MARPE), {{as well as the}} development of new software, data analysis techniques, and detectors of general use in such research. We present experimental and theoretical results related to MARPE, in which the photoelectron intensity from a core level on one atom is influenced by a core-level absorption resonance on another. We point out that some of our and others prior experimental data has been strongly influenced by detector non-linearity and that the effects seen in <b>new</b> <b>corrected</b> <b>data</b> are smaller and of different form. Corrected data for the MnO(001) system with resonance between the O 1 s and Mn 2 p energy levels are found to be well described by an extension of well-known intraatomic resonant photoemission theory to the interatomic case, provided that interactions beyond the usual second-order Kramers-Heisenberg treatment are included. This theory is also found to simplify under certain conditions so as to yield results equivalent to a classical x-ray optical approach, with the latter providing an accurate and alternative, although less detailed and general, physical picture of these effects. Possible future applications of MARPE as a new probe of near-neighbor identities and bonding and its relationship to other known effects are also discussed. We also consider in detail specially written data acquisition software that has been used for most of the measurements reported here. This software has been used with an existing experimental system to develop the method of detector characterization and then data correction required for the work described above. The development of a next generation one-dimensional, high-speed, electron detector is also discussed. Our goal has been to design, build and test a prototype high-performance, one-dimensional pulse-counting detector that represents a significant advancement in detector technology and is well matched to modern high-brightness synchrotrons radiation sources and high-transmission electron-energy analyzers as typically used in photoelectron spectroscopy experiments. The general design of the detector and the results of initial tests are discussed and the acquisition of photoelectron spectra with the first test detector is described...|$|E
40|$|International audienceCharletonia elbasani sp. nov. (Acari: Erythraeidae) is {{described}} and illustrated from larvae collected from herbaceous plants on the Elbasan (Albania). It is the eighteenth report of larval {{species of the}} genus Charletonia with two setae between coxae II and III. <b>New</b> and <b>corrected</b> <b>data</b> for C. kalithensis are given...|$|R
40|$|An {{improved}} {{method for}} {{the preparation of}} 4, 6 -O-benzylidene-beta-D-glucopyranose (BG), and <b>new</b> or <b>corrected</b> <b>data</b> on its 1 H and 13 C NMR spectra, specific rotations, and tautomeric equilibria, and on those of its anomeric sodium salt (BGNa), are reported. Evidence is presented in favour of the hypothesis that crystalline BGNa exists entirely in its beta-anomeric form and {{that it can be}} useful in the access to beta-glucosides in reactions with strong electrophiles under strictly heterogeneous conditions...|$|R
40|$|Abstract. Whenever an ASR company {{promises}} to deliver error-proof tran-scripts {{to the end}} user, manual verification and correction of the raw ASR transcripts cannot be avoided. This manual post-editing process systematically generates <b>new</b> and <b>correct</b> domain-specific <b>data</b> {{which can be used}} to incre-mentally improve the original ASR system. This paper proposes a statistic, SMT-based ASR error correction method, which takes advantage of the past corrected ASR errors to automatically post-process its future transcripts. We show that the proposed method can bring more than 10 % WER improvements using only 2000 user-corrected sentences...|$|R
5000|$|... "Previously {{reported}} {{discrepancies between}} {{the amount of}} warming near the surface and higher in the atmosphere have been used to challenge the reliability of climate models and the reality of human induced global warming. Specifically, surface data showed substantial global-average warming, while early versions of satellite and radiosonde data showed little or no warming above the surface. This significant discrepancy no longer exists because errors in the satellite and radiosonde data have been identified and <b>corrected.</b> <b>New</b> <b>data</b> sets have also been developed that do not show such discrepancies." ...|$|R
40|$|Abstract We have {{performed}} a full cross-validation of this clinical Femina data collection against the routinely collected {{data of the}} Medical Birth Registry of Norway to validate the estimates of reduced mortality in the total population. The original estimate of fewer deaths during the intervention with OR 0. 7 remains virtually unchanged for the original data collection. The validation procedures revealed inaccuracies in data from the Medical Birth Registry of Norway for a partial comparison with mortality outside the study area, and we here correct this comparison. We present <b>new,</b> <b>corrected</b> and cross-validated <b>data.</b> Despite comparability issues, the most robust and cross-validated estimates confirm similar estimates of reduced mortality during the quality improvement intervention. </p...|$|R
40|$|The {{method of}} Taylor series {{expansion}} {{is used to}} develop a numerical solution to the reactor point kinetics equations. It is shown that taking a first order expansion of the neutron density and precursor concentrations at each time step gives results that are comparable to those obtained using other popular and more complicated methods. The algorithm developed using a Taylor series expansion is simple, completely transparent, and highly accurate. The procedure is tested {{using a variety of}} initial conditions and input data, including step reactivity, ramp reactivity, sinusoidal, and zigzag reactivity. These results are compared to those obtained using other methods. Comment: 13 pages, added 3 new figures, and 3 <b>new</b> reactivity conditions. <b>Corrected</b> <b>data</b> in table for sin reactivity cas...|$|R
40|$|We have {{performed}} a full cross-validation of this clinical Femina data collection against the routinely collected {{data of the}} Medical Birth Registry of Norway to validate the estimates of reduced mortality in the total population. The original estimate of fewer deaths during the intervention with OR 0. 7 remains virtually unchanged for the original data collection. The validation procedures revealed inaccuracies in data from the Medical Birth Registry of Norway for a partial comparison with mortality outside the study area, and we here correct this comparison. We present <b>new,</b> <b>corrected</b> and cross-validated <b>data.</b> Despite comparability issues, the most robust and cross-validated estimates confirm similar estimates of reduced mortality during the quality improvement intervention. The online version of the original article {{can be found at}} 10. 1186 / 1471 - 2393 - 9 - 32 </p...|$|R
5000|$|In 2007, McIntyre started {{auditing}} {{the various}} corrections made to temperature records, in particular those {{relating to the}} urban heat island effect. He discovered a discontinuity in some U.S. records in the Goddard Institute for Space Studies (GISS) dataset starting in January 2000. He emailed GISS advising them {{of the problem and}} within a couple of days GISS issued a <b>new,</b> <b>corrected</b> set of <b>data</b> and thanked McIntyre for [...] "bringing to our attention that such an adjustment is necessary to prevent creating an artificial jump in year 2000". The adjustment reduced the average temperatures for the continental United States by about 0.15 °C during the years 2000-2006. Changes in other portions of the record did not exceed 0.03 °C; it made no discernible difference to the global mean anomalies.|$|R
40|$|In {{the present}} work the {{characteristics}} of backward (90 sup d eg<theta sub s < 180 sup d eg) and forward (theta sub s < 90 sup d eg) scattered gamma radiation originating at interaction of a beam of a bremsstrahlung radiation of electrons with energy 13 and 22 MeV with plane targets of different thickness from glass textolite, aluminium, iron, lead and their combination are investigated. The dependence of thickness of saturation of 'forward' scattered gamma radiation, a on angles of detection (theta sub s) and orientation (phi) of plane targets depending on a direction of probing beam was observed for the first time. For the first time, the numerical performances of beams of forward scattered gamma radiation from different targets were investigated and determined depending on their orientation and thickness. The <b>new</b> and <b>corrected</b> <b>data</b> on numerical performances of beams of the inverse scattered gamma radiation is obtained. The distinction in characteristics of beams of the scattered gamma radiation is shown depending on energy of a bremsstrahlung radiation of electrons with E sub e = 22 and 13 MeV. The numerical characteristics of annihilation component in beams of the scattered gamma radiation were described by calculated data {{within the framework of}} the designed simplified model...|$|R
40|$|A new, fast, non-iterative {{version of}} the "Wall Pressure Signature Method" is {{described}} and used to determine blockage and angle-of-attack wind tunnel corrections for highly-powered jet-flap models. The correction method is complemented by the application of tangential blowing at the tunnel floor to suppress flow breakdown there, using feedback from measured floor pressures. This tangential blowing technique was substantiated by subsequent flow investigations using an LV. The basic tests on an unswept, knee-blown, jet flapped wing were supplemented to include the effects of slat-removal, sweep {{and the addition of}} unflapped tips. C sub mu values were varied from 0 to 10 free-air C sub l's in excess of 18 were measured in some cases. Application of the <b>new</b> methods yielded <b>corrected</b> <b>data</b> which agreed with corresponding large tunnel "free air" resuls to within the limits of experimental accuracy in almost all cases. A program listing is provided, with sample cases...|$|R
40|$|In {{reviewing}} the progress made in acoustic measurements in wind tunnels over the 5 -yr {{span of the}} workshops, {{it is evident that}} a great deal of progress has occurred. Specialized facilities are now on line, special measurement techniques were developed, and corrections were devised and proven. This capability {{is in the process of}} creating a <b>new</b> and more <b>correct</b> <b>data</b> bank on acoustic phenomena, and represents a major step forward in acoustics technology. Additional work is still required, but now, rather than concentrating on facilities and techniques, researchers may more profitably concentrate on noise-source modeling, with the simulation of propulsor noise source (in flight) and of propulsor/airframe airflow characteristics. Promising developments in directional acoustic receivers and other discrimination/correlation techniques should now be regularly exploited, in part for model noise-source diagnosis, but also to expedite extraction of the lone source signal from any residual background noise and reverberation in the working chamber and from parasitic noise due to essential rigs or instrumentation inside the airstream...|$|R
50|$|Uncertain, {{please help}} the wiki by finding <b>correct</b> <b>data.</b>|$|R
5000|$|Increase {{data quality}} by {{providing}} <b>correct</b> <b>data</b> to public {{from the source}} ...|$|R
5000|$|... {{the keys}} or mouse clicks {{to get to}} the <b>correct</b> <b>data</b> entry window ...|$|R
5000|$|Geocoding - {{for name}} and address <b>data.</b> <b>Corrects</b> <b>data</b> to U.S. and Worldwide postal {{standards}} ...|$|R
5000|$|Hence [...] is only {{related the}} <b>correct</b> <b>data</b> [...] This kind error can be {{calculated}} as ...|$|R
5000|$|Normal Operation (NO) - Indicates {{the data}} in this word is {{considered}} to be <b>correct</b> <b>data.</b>|$|R
3000|$|... {{the main}} {{difference}} consists in the decrease in the energy associated with some modes contributing to <b>corrected</b> <b>data.</b>|$|R
50|$|<b>Correct</b> <b>data</b> {{about the}} {{frequency}} spectrum {{is provided with}} the help of the technique of the neutron scattering by solids.|$|R
50|$|The <b>corrected</b> <b>data</b> {{is used to}} {{generate}} reports on topics ranging from web traffic to video streaming activity and consumer buying power.|$|R
30|$|We {{experimentally}} {{applied a}} typical rain attenuation correction of 0.0018 R 1.05 (dB km−[*] 1) for C-band (wavelength =  5  cm) radars (Doviak and Zrnić 1993) to the obtained data {{and found that}} there is no serious difference between the original and <b>corrected</b> <b>data.</b> For example in Fig.  3 e, total echo coverage (convective rain area fraction) of the original and <b>corrected</b> <b>data</b> during the campaign period were 13.0 % (7.7 %) and 13.2 % (7.5 %), respectively.|$|R
40|$|There is a {{mathematical}} {{error in the}} first version of this paper. A <b>new</b> <b>corrected</b> version will be posted when the error is fixed, possibly with a modified title. Comment: There is {{a mathematical}} error in {{the first version of}} this paper. A <b>new</b> <b>corrected</b> version will be posted when the error is fixed, possibly with a modified titl...|$|R
5000|$|This kind error {{are called}} bowl effect. Bowl effect does not related the unknown object [...] , {{it is only}} related the <b>correct</b> <b>data</b> ...|$|R
40|$|This Synthesis and Assessment Product is an {{important}} revision to the conclusions of earlier reports from the U. S. National Research Council and the Intergovernmental Panel on Climate Change. Previously reported discrepancies {{between the amount of}} warming near the surface and higher in the atmosphere have been used to challenge the reliability of climate models and the reality of human-induced global warming. Specifically, surface data showed substantial global-average warming, while early versions of satellite and radiosonde data showed little or no warming above the surface. This significant discrepancy no longer exists because errors in the satellite and radiosonde data have been identified and <b>corrected.</b> <b>New</b> <b>data</b> sets have also been developed that do not show such discrepancies. This Synthesis and Assessment Product is {{an important}} revision to the conclusions of earlier reports from the U. S. National Research Council and the Intergovernmental Panel on Climate Change. For recent decades, all current atmospheric data sets now show global-average warming that is similar to the surface warming. While these data are consistent with the results from climate models at the global scale, discrepancies in the tropics remain to be resolved. Nevertheless, the most recent observational and model evidence has increased confidence in our understanding of observed climatic changes and their causes...|$|R
40|$|Constraints {{are used}} in {{traditional}} database systems to define consistent database states. For multimedia data {{it is also important}} to define constraints for a <b>correct</b> <b>data</b> output. The producer of multimedia data should specify constraints for a <b>correct</b> <b>data</b> output. We show the modeling of output constraints. The multimedia database system must guarantee that the stored multimedia data and the data output of multimedia data are according to the defined output constraints. For that an efficient check of output constraints must be possible. 1...|$|R
50|$|Barriers {{are simple}} to {{implement}} and provide good responsiveness. They {{are based on}} the concept of implementing wait cycles to provide synchronization.Consider three threads running simultaneously, starting from barrier 1. After time t, thread1 reaches barrier 2 but it still has to wait for thread 2 and 3 to reach barrier2 as it does not have the <b>correct</b> <b>data.</b> Once, all the threads reach barrier 2 they all start again. After time t, thread 1 reaches barrier3 but it will have to wait for threads 2 and 3 and the <b>correct</b> <b>data</b> again.|$|R
5000|$|The {{following}} series representation, {{found by}} Amdeberhan in 1996, gives (asymptotically) 1.43 <b>new</b> <b>correct</b> decimal places per term: ...|$|R
5000|$|Here [...] is the <b>correct</b> <b>data</b> {{in which}} there is no the {{influence}} of the object function in outside. From this data it is easy to get correct solution, ...|$|R
40|$|In-network {{storage of}} data in {{wireless}} sensor networks (such as DCS-GHT, for instance) is considered {{a viable alternative to}} external storage since it contributes to reduce the communications inside the network and to favor data aggregation. In current approaches it exploits pure data replication to assure data availability. In this paper, we consider the use of codes and data dispersal in combination to in-network storage. In particular, given an abstract model of in-network storage we show how codes can be employed, and we discuss how this can be achieved in three cases of study. Since the configuration of the network is particularly critical with respect to <b>correct</b> <b>data</b> encoding, we define framework aimed at evaluating the probability of <b>correct</b> <b>data</b> encoding and decoding. Then we exploit this result and simulations to show how, in the cases of study, the parameters of the codes and the network should be configured in order to achieve <b>correct</b> <b>data</b> coding and decoding with high probability. ...|$|R
5000|$|The {{following}} series representation, {{found by}} Mohamud Mohammed in 2005, gives (asymptotically) 3.92 <b>new</b> <b>correct</b> decimal places per term:where ...|$|R
50|$|Memory {{scrubbing}} {{consists of}} reading from each computer memory location, correcting bit errors (if any) with an error-correcting code (ECC), and writing the <b>corrected</b> <b>data</b> {{back to the}} same location.|$|R
5000|$|The {{following}} series representation, {{found by}} Amdeberhan and Zeilberger in 1997, gives (asymptotically) 3.01 <b>new</b> <b>correct</b> decimal places per term: ...|$|R
50|$|Peano first {{published}} the theorem in 1886 with an incorrect proof. In 1890 he published a <b>new</b> <b>correct</b> proof using successive approximations.|$|R
30|$|Before using {{recorded}} contexts {{as learning}} data for application volume estimation, {{they need to}} be preprocessed. One of the preprocessing steps involves GPS data, whereas another involves <b>correct</b> <b>data</b> generation for machine learning.|$|R
5000|$|Data Validation {{tests the}} {{validity}} of input into fields by comparing the input to patterns, checking for the <b>correct</b> <b>data</b> type (such as a string or an integer), and in other customizable ways.|$|R
