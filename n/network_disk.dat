23|335|Public
2500|$|On January 2, 2009, InsanelyMac's Live DVD team {{published}} a new {{method by which}} a Mac OS X v10.5.x Live DVD could be produced, allowing users to boot a fully working [...] desktop from a DVD or USB flash drive. [...] The method was more reliable than previous methods because it manipulated Apple's existing Netboot and Imageboot functionalities and behaved as if the system were running off a <b>network</b> <b>disk.</b> It was easier to produce; requiring only a single script {{to be added to}} an existing installation. Distributions of the live DVD have been made since its inception. Since then, it is notable that this method has been shown to work on normal Apple Mac hardware.|$|E
5000|$|... geom_gate (creates {{a virtual}} disk using <b>network</b> <b>disk</b> back-end) ...|$|E
5000|$|Monitor devices {{supporting}} SNMP versions 1 and 2c for <b>network,</b> <b>disk,</b> {{memory and}} cpu usage or create custom SNMP checks ...|$|E
3000|$|... iCanCloud {{provides}} {{a set of}} components which emulate the behavior of real components in the real architecture, for instance, <b>networks,</b> <b>disks,</b> file systems, and memory. These components constitute the core simulation engine, while additional components {{can be added to}} its repository, and the substitution of substitution of components.|$|R
50|$|The company sold {{hardware}} and software products such as local area <b>networks,</b> <b>disk</b> drives, printers, personal computers, random access memory chips, central processing units and integrated circuit boards. The principal vendors of the company included Seagate Technology, Hewlett-Packard, Microsoft, IBM, Sun and Creative Labs, 3Com, Microsoft, Epson, and Intel.|$|R
40|$|Parallel disk systems, {{which have}} been widely used in {{building}} networked and data intensive applications, are highly scalable and can alleviate the problem of disk I/O bottleneck. Although a number of parallel disk systems have been developed, the systems lack a means to optimize quality of security for dynamically changing networked environments. We remedy this situation by proposing an adaptive quality of security control scheme for <b>networked</b> parallel <b>disk</b> systems (or ASPAD for short) that makes it possible for <b>networked</b> <b>disk</b> systems to adapt to changing security requirements and workload conditions. ASPAD is carried out in three phases: dynamic data partitioning, response time estimation, and adaptive security quality control. Hence, ASPAD is conducive to adaptively and expeditiously determining security schemes for disk requests in a way to improve security of <b>networked</b> parallel <b>disk</b> systems while making an effort to guarantee desired response times of the requests To prove the efficiency of the proposed approach, we simulate a <b>networked</b> parallel <b>disk</b> system into which nine cryptographic schemes are integrated. Empirical results show that ASPAD significantly improves overall performance over an existing strategy with an average of 65 %. ...|$|R
5000|$|... 2 bytes volume type = short integer mac os value (types are Fixed HD = 0; <b>Network</b> <b>Disk</b> = 1; 400kB FD = 2;800kB FD = 3; 1.4MB FD = 4; Other Ejectable Media = 5 [...] ) ...|$|E
50|$|Multiple SV1 {{cabinets}} {{could be}} clustered together using the GigaRing I/O channel, which also provided connection to HIPPI, FDDI, ATM, Ethernet and SCSI devices for <b>network,</b> <b>disk,</b> and tape services. In theory, up to 32 nodes could be clustered together, offering {{up to one}} teraflop in theoretical peak performance.|$|E
50|$|Amanda, {{previously}} {{known as}} Advanced Maryland Automatic <b>Network</b> <b>Disk</b> Archiver {{is an open}} source computer archiving tool that is able to back up data residing on multiple computers on a network. It uses a client-server model, where the server contacts each client to perform a backup at a scheduled time.|$|E
5000|$|A Zabbix agent {{can also}} be {{installed}} on UNIX and Windows hosts to monitor statistics such as CPU load, <b>network</b> utilization, <b>disk</b> space, etc.|$|R
50|$|The {{suits are}} <b>Networks,</b> Cubicles, <b>Disks</b> and Hosts, {{and the court}} cards—instead of being King, Queen, Knight and Page—are CIO, Marketeer, Salesman and New Hire.|$|R
5000|$|DataDirect <b>Network's</b> (DDN) <b>disk</b> {{subsystems}} {{such as the}} S2A9900 and SFA10000, {{which use}} the SRP target implementation in the disk subsystem's controllers to present LUNs to servers (the servers act as SRP initiators).|$|R
5000|$|Contemporary desktop {{personal}} computers generally provide {{an option to}} boot from the network in their BIOS via the Preboot Execution Environment (PXE). Post-1998 PowerPC (G3 - G5) Mac systems can also boot from their firmware to a <b>network</b> <b>disk</b> via NetBoot. [...] Old {{personal computers}} without network boot firmware support can utilize a floppy disk or flash drive containing software to boot from the network.|$|E
50|$|Zmanda Cloud Backup (ZCB) is {{an online}} backup {{software}} by open source backup company Zmanda which enables users {{to back up}} their data to cloud storage. The software uses the Amazon S3 service from Amazon Web Services or Google Cloud Storage as the cloud storage service but also supports private cloud solutions such as OpenStack Swift or Walrus by Eucalyptus via the open source ZCloud API.As of March 2011, the software runs only on Windows platforms. For non-Windows platforms the company offers the open source Advanced Maryland Automatic <b>Network</b> <b>Disk</b> Archiver with Amazon S3 as the storage location.|$|E
5000|$|The basic {{scheduling}} unit in Kubernetes {{is called}} a [...] "pod". It adds {{a higher level of}} abstraction to containerized components. A pod consists of one or more containers that are guaranteed to be co-located on the host machine and can share resources. Each pod in Kubernetes is assigned a unique (within the cluster) IP address, which allows applications to use ports without the risk of conflict. A pod can define a volume, such as a local disk directory or a <b>network</b> <b>disk,</b> and expose it to the containers in the pod. Pods can be manually managed through the Kubernetes API, or their management can be delegated to a controller.|$|E
50|$|Around 1980, {{approximately}} 10,000 {{people were}} working at CERN with different hardware, software and individual requirements. Much {{work was done}} by email and file exchange. The scientists needed {{to keep track of}} different things and different projects became involved with each other. Berners-Lee started to work for 6 months on 23 June 1980 at CERN while he developed ENQUIRE. The requirements for setting up a new system were compatibility with different <b>networks,</b> <b>disk</b> formats, data formats, and character encoding schemes, which made any attempt to transfer information between dissimilar systems a daunting and generally impractical task. The different hypertext-systems before ENQUIRE were not passing these requirements i.e. Memex and NLS.|$|R
50|$|The Digital {{video is}} stored on a storage area <b>network</b> (SAN) hard <b>disk</b> array.|$|R
5000|$|Monitoring of server resources: system load, <b>network</b> load, and <b>disk</b> usage, {{using an}} agent.|$|R
5000|$|Altos 586 (despite {{what its}} name might suggest today) used a 10 MHz 8086 {{processor}}, among the fastest for a 1983 microcomputer. An 8089 chip {{aided by a}} Z80 queuing processor supported up to eight terminals. Ran Xenix or MP/M-86. The 586 had 512 KB standard memory and came with six RS-232C serial port and one RS-422, which was intended for networking rather than terminal attachment. The Altos 986 was a variant with 1 MB RAM and four extra serial ports. 3Com developed their new Ethernet card for the 986 model, running Xenix 3.0 and sold as a <b>network</b> <b>disk</b> server for IBM PC, XT computers installed with 3Com Ethernet expansion cards.|$|E
5000|$|On January 2, 2009, InsanelyMac's Live DVD team {{published}} a new {{method by which}} a Mac OS X v10.5.x Live DVD could be produced, allowing users to boot a fully working Mac OS X desktop from a DVD or USB flash drive. [...] The method was more reliable than previous methods because it manipulated Apple's existing Netboot and Imageboot functionalities and behaved as if the system were running off a <b>network</b> <b>disk.</b> It was easier to produce; requiring only a single script {{to be added to}} an existing installation. Distributions of the live DVD have been made since its inception. Since then, it is notable that this method has been shown to work on normal Apple Mac hardware.|$|E
50|$|The Cray T3E was Cray Research's second-generation massively {{parallel}} supercomputer architecture, launched in late November 1995. The first T3E was {{installed at the}} Pittsburgh Supercomputing Center in 1996. Like the previous Cray T3D, it was a fully distributed memory machine using a 3D torus topology interconnection network. The T3E initially used the DEC Alpha 21164 (EV5) microprocessor and was designed to scale from 8 to 2,176 Processing Elements (PEs). Each PE had between 64 MB and 2 GB of DRAM and a 6-way interconnect router with a payload bandwidth of 480 MB/s in each direction. Unlike many other MPP systems, including the T3D, the T3E was fully self-hosted and ran the UNICOS/mk distributed operating system with a GigaRing I/O subsystem integrated into the torus for <b>network,</b> <b>disk</b> and tape I/O.|$|E
5000|$|A bespoke network-hub {{that allowed}} up to 16 {{computers}} to connect in a <b>network,</b> sharing <b>disks</b> and printers. The server was a Tiki-100 with harddisk, running the MP/M operating system, serving up to 3 different printers simultaneously.|$|R
5000|$|... run {{processes}} that consume resources (CPU, memory, <b>disk,</b> <b>network)</b> on the Web and database servers ...|$|R
50|$|Additional APIs exist, beyond simple {{transaction}} demarcation, {{providing for}} the more advanced capabilities necessary to address practical issues found when dealing with performance optimization and scalability for systems with large amounts of data, many concurrent users, <b>network</b> latency, <b>disk</b> bottlenecks, etc.|$|R
5000|$|The June 2013 {{release of}} the 5th {{generation}} models features a name change to AirPort Time Capsule, and a redesign with measurements 3.85 in square, and 6.6 in high. The square dimensions echo the size of both the latest AirPort Express and Apple TVs (2nd generation onwards), just with the height being significantly higher. The 2013 models feature the same [...] ports on the back as previous generations, and come in the same capacities as the 4th generation of 2 TB & 3 TB, but have introduced the newest Wi-Fi standard 802.11ac. The AirPort Extreme released {{at the same time}} is exactly the same in dimensions and I/O ports, just without the internal harddrive of the AirPort Time Capsule. 2013 models feature faster download speed, beam-forming improvements and wireless or desktop network control with iCloud integration. Airport is compatible with devices using the 802.11a, 802.11b, 802.11g, 802.11n and 802.11ac specifications. Also improved, Airport Utility has added one click Time Capsule format from the utility's Airport Time Capsule, Edit, Disks menu, allowing easy and rapid Erase Disk and Archive Disk to start over or configure <b>Network.</b> <b>Disk</b> Erase includes up to 35 passes and device includes encrypted storage plus optional WAN sharing, making Airport extremely secure and flexible for home, class and office environments. Airport Utility is a free download.|$|E
40|$|Data {{aggregation}} plays a basal role in {{the design}} of scalable distributed applications, allowing the determination of meaningful system-wide properties to direct the execution of the system. For instance, aggregation can be used to estimate: the size of the network to dimension of Distributed Hash Table (DHT) structures, or to set a quorum in dynamic settings; the average system load to guide local loadbalancing decisions; the total <b>network</b> <b>disk</b> space in a P 2 P sharing system. In the particular case of Wireless Sensor Networks (WSN), due to energy constraints, data collection is often only practicable if aggregation is performed...|$|E
40|$|This paper {{studies the}} {{performance}} implications of using cryptographic controls in performance-critical systems. Full cryptographic controls beyond basic authentication are considered and experimentally validated {{in the concept}} of network file systems. This paper demonstrates that processor speeds have recently become fast enough to support cryptographic controls in many performance-critical systems. Integrity and authentication using keyed-hash and RSA as well as confidentiality using RC 5 are tested. This analysis demonstrates that full cryptographic controls are feasible in a distributed network file system, by showing the performance overhead for including signature, hash and encryption algorithms on various embedded and workstation computers. The results from these experiments are used to predict the performance impact using three proposed <b>network</b> <b>disk</b> security schemes...|$|E
40|$|Abstract Our {{work was}} {{motivated}} by the problem of managing data on storage devices, typically a set of parallel disks. Due to dynamic changes in load, one needs to develop algorithms for quickly changing one storage configuration into another. We refer to this problem as the data migration problem. 1 Introduction To handle high demand, especially for multimedia data, a common approach is to replicate data objects within the storage system. Typically, a large storage server consists of several disks connected using a dedicated network, called a Storage Area <b>Network.</b> <b>Disks</b> typically have constraints on storage {{as well as the}} number of clients that can access data from a single disk simultaneously...|$|R
5000|$|... {{overall size}} of each boot image can be {{controlled}} to fit within <b>network</b> or removable <b>disk</b> limits ...|$|R
5000|$|Applying a {{captured}} image involves {{running a}} second Windows PE [...] "Apply" [...] boot {{image on the}} target system to receive the image. This boot image also needs the appropriate <b>network</b> and <b>disk</b> controller drivers as with the Windows PE Capture boot image.|$|R
40|$|PERG is an FPGA {{application}} that accelerates {{the process of}} searching a stream of bytes against a large, fixed database of string patterns. The stream could be <b>network,</b> <b>disk,</b> or file traffic, while the pattern database may represent computer viruses, spam, keyword sequences, or watermarks. A full pattern, or rule, consists of a sequence {{of one or more}} segments separated by gaps. Each segment is an exact sequence of bytes, possibly 100 s of bytes long. Each gap contains arbitrary bytes, but is a known length. PERG uses a pattern compiler to transform a database of these rules into a hardware implementation. To the authors ' knowledge, this is the first pattern match engine hardware designed for large virus databases. It is also first among network intrusion detection system...|$|E
40|$|Data {{aggregation}} 1 plays a basal role in {{the design}} of scalable distributed applications [1], allowing the determination of meaningful system-wide properties to direct the execution of the system. For instance, aggregation can be used to estimate: the size of the network to dimension of Distributed Hash Table (DHT) structures [2], or to set a quorum in dynamic settings [3]; the average system load to guide local loadbalancing decisions; the total <b>network</b> <b>disk</b> space in a P 2 P sharing system. In the particular case of Wireless Sensor Networks (WSN), due to energy constraints, data collection is often only practicable if aggregation is performed. Several aggregation algorithms have been proposed in the recent years, tackling the problem for different settings, and yielding different characteristics in terms of accuracy, time and communication tradeoffs. Traditional tree-based approache...|$|E
40|$|We {{propose a}} method to reuse unmodified device drivers and to improve system {{dependability}} using virtual machines. We run the unmodified device driver, with its original operating system, in a virtual machine. This approach enables extensive reuse of existing and unmodified drivers, independent of the OS or device vendor, significantly reducing the barrier to building new OS endeavors. By allowing distinct device drivers to reside in separate virtual machines, this technique isolates faults caused by defective or malicious drivers, thus improving a system’s dependability. We show that our technique requires minimal support infrastructure and provides strong fault isolation. Our prototype’s network performance is within 3 – 8 % of a native Linux system. Each additional virtual machine increases the CPU utilization by about 0. 12 %. We have successfully reused {{a wide variety of}} unmodified Linux <b>network,</b> <b>disk,</b> and PCI device drivers. ...|$|E
50|$|Playing online games {{requires}} that users {{set up the}} system's network connection configuration, which is saved to a memory card. This {{can be done with}} the <b>network</b> Startup <b>Disk</b> that came with the network adapter or using one of the many games that had the utility built into them, such as Resident Evil Outbreak, to set up the network settings. The new slimline PlayStation 2 came with a disk in the box by default. The last version of the <b>disk</b> was <b>network</b> startup <b>disk</b> 5.0, which was included with the newer SCPH 90004 model released in 2009. However, as of December 31, 2012, the PlayStation 2 has been discontinued, and the servers for games have all since been shut down.|$|R
40|$|Since 1993, NASA Langley Research Center {{has been}} {{developing}} and implementing a low-cost Internet connection model, including system architecture, training, and support, to provide Internet access for an entire network of computers. This infrastructure allows local area networks which exceed 50 machines per school to independently access the complete functionality of the Internet by connecting to a central site, using state-of-the-art commercial modem technology, through a single standard telephone line. By locating high-cost resources at this central site and sharing these resources and their costs among the school districts throughout a region, a practical, efficient, and affordable infrastructure for providing scale-able Internet connectivity has been developed. As the demand for faster Internet access grows, the model has a simple expansion path that eliminates the need to replace major system components and re-train personnel. Observations of optical Internet usage within an environment, particularly school classrooms, have shown that after an initial period of 'surfing,' the Internet traffic becomes repetitive. By automatically storing requested Internet information on a high-capacity <b>networked</b> <b>disk</b> drive at the local site (<b>network</b> based <b>disk</b> caching), then updating this information only when it changes, well over 80 percent of the Internet traffic that leaves a location can be eliminated by retrieving {{the information from the}} local disk cache...|$|R
40|$|The {{realization}} of end-to-end {{quality of service}} (QoS) guarantees in emerging networkbased applications requires mechanisms that support first dynamic discovery and then advance or immediate reservation of resources that will often be heterogeneous in type and implementation and independently controlled and administered. We propose the Globus Architecture for Reservation and Allocation (GARA) to address the four highlighted issues. GARA treats both reservations and computational elements such as processes, network flows, and memory blocks as first class entities, allowing them to be created, monitored, and managed independently and uniformly. It simplifies management of heterogeneous resource types by defining uniform mechanisms for computers, <b>networks,</b> <b>disk,</b> memory, and other resources. Layering on these standard mechanisms, GARA enables the construction of application-level co-reservation and co-allocation libraries that applications can use to dynamically assemble collection [...] ...|$|R
