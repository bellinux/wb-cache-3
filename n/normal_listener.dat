8|202|Public
50|$|People {{with mild}} and {{moderate}} hearing loss were tested for the effectiveness of phonemic restoration. Those with mild hearing loss performed {{at the same level}} of a <b>normal</b> <b>listener.</b> Those with {{moderate hearing loss}} had almost no perception and failed to identify the missing phonemes. This research is also dependent on the amount of words the observer is comfortable understanding {{because of the nature of}} top-down processing.|$|E
50|$|Softness Imperception (SI) {{is a term}} {{coined by}} Florentine and colleagues to {{describe}} the inability to hear quiet sounds that are audible to normal listeners. This phenomenon is particularly common among people with cochlear hearing loss. When a person with SI hears a sound at threshold, it sounds louder than a sound at threshold would do for a <b>normal</b> <b>listener.</b> Therefore, people with hearing loss may find softer sounds more intrusive when fitted with hearing aids that simply amplify all soft sounds to threshold.|$|E
50|$|Whilst {{working as}} a salesman, Tao was offered a job by Taiwanese {{producer}} Wang Chih-ping {{and went back to}} Taiwan, initially writing, and later producing songs for many singers before releasing his self-titled album David Tao in 1997. He has since released four more albums, a live concert recording and a compilation of his best songs. His works have influenced many singers to acknowledge his works more than the <b>normal</b> <b>listener.</b> Tao is a prolific composer and songwriter and has written songs for fellow artists such as A-Mei and S.H.E..|$|E
40|$|The roles which word class (open/closed) and sentential stress {{play in the}} {{sentence}} comprehension processes of both agrammatic (Broca’s) aphasics and <b>normal</b> <b>listeners</b> were examined with a word monitoring task. Overall, <b>normal</b> <b>listeners</b> responded more quickly to stressed than to unstressed items, but showed no effect of word class. Aphasics also responded more quickly to stressed than to unstressed materials, but, unlike the normals, responded faster to open than to closed class words regardless of their stress. The results are interpreted as support for the theory that Broca’s aphasics lack the functional underlying open/closed class word distinction used in word recognition by <b>normal</b> <b>listeners...</b>|$|R
50|$|In a {{clinical}} study, patients with unilateral neglect {{were shown to}} experience the scale illusion. Further, in an MEG study on <b>normal</b> <b>listeners</b> the scale illusion {{was found to be}} neurally represented at or near the auditory cortex.|$|R
40|$|AbstractIntroductionDichotic {{listening}} tests {{should be}} used in local languages and adapted for the population. ObjectiveStandardize the Brazilian Portuguese version of the Dichotic Sentence Identification test in <b>normal</b> <b>listeners,</b> comparing the performance for age and ear. MethodsThis prospective study included 200 <b>normal</b> <b>listeners</b> divided into four groups according to age: 13 – 19 years (GI), 20 – 29 years (GII), 30 – 39 years (GIII), and 40 – 49 years (GIV). The Dichotic Sentence Identification was applied in four stages: training, binaural integration and directed sound from right and left. ResultsBetter results for the right ear were observed in the stages of binaural integration in all assessed groups. There was a negative correlation between age and percentage of correct responses in both ears for free report and training. The worst performance in all stages of the test was observed for the age group 40 – 49 years old. ConclusionsReference values for the Brazilian Portuguese version of the Dichotic Sentence Identification test in <b>normal</b> <b>listeners</b> aged 13 – 49 years were established according to age, ear, and test stage; they {{should be used}} as benchmarks when evaluating individuals with these characteristics...|$|R
5000|$|Easily the two tracks {{making the}} album sparkle are Limehouse Blues and Malagueña {{arranged}} by Bill Holman. Though Holman {{is well known}} for Kenton charts on the New Concepts of Artistry in Rhythm, Contemporary Concepts, and Stan Kenton Presents Bill Holman LP's, these two instrumental charts penned for Kenton are now hallmarks of the modern jazz orchestra repertiore. Kenton would use Malagueña with the band all the way to his death in 1978. [...] The two charts have also become staples of both drums corps and marching bands because of the inventiveness Bill Holman has with musical material.Great credit should be given to the band itself, these are very difficult works Holman created. These are flagwaiver pieces that catch the ears of the <b>normal</b> <b>listener</b> and especially a NARAS/Grammy panel that might not know the jazz orchestra genre but certainly knows exciting and well played music.|$|E
40|$|This study {{examined}} {{three levels of}} self-concept in adults with voice disorders. The three levels are their self-perceived severity of voice problems, their meta-perception and their attitudes towards others with voice disorders. Eleven pairs of vocally healthy (vocally healthy listener group) and voice-disordered listeners (dysphonic listener group) were asked to give ratings on their self-perceived severity of their voice conditions, meta-perception and their attitudes towards individuals with different severities of voice disorders using semantic differential scale. The result showed that the dysphonic listener group gave significantly more negative ratings in their self-perceived severity and meta-perception than <b>normal</b> <b>listener</b> group. Similar attitude scores towards individuals with voice disorders were obtained from both listeners groups. The results suggested that voice-disordered individuals with voice disorders and speech therapists should not under-estimate the consequence of voice disorders. The study concluded that voice-disordered people individuals hold a generally negative attitude towards themselves. Limitations {{of the study and}} recommendations for future researches were discussed. published_or_final_versionSpeech and Hearing SciencesBachelorBachelor of Science in Speech and Hearing Science...|$|E
40|$|Our project aim is {{to build}} a {{handheld}} device that would help deaf and dumb people to communicate with others in every day spoken language such as English. Deaf and dumb often communicate via sign language, a kind of representation ntation of words through hand and finger positions. But it has got serious limitations because {{it is not easy to}} understand by a <b>normal</b> <b>listener</b> on the opposite and to make things worse, not many in the world know sign language at all. Also, it is difficult to represent all the words of a plain language like English into a sign language symbol. Even if there is one, then learning and using them would be tough and cumbersome. In this paper, we will focus on the history of communication technologies that have given better access to the world for those with sensory disabilities. The areas that will be covered are communication technologies that improve or augment hearing and vision, and technologies that support alternatives strategies to communication without hearing and/or vision...|$|E
40|$|ABSTRACT INTRODUCTION: Dichotic {{listening}} tests {{should be}} used in local languages and adapted for the population. OBJECTIVE: Standardize the Brazilian Portuguese version of the Dichotic Sentence Identification test in <b>normal</b> <b>listeners,</b> comparing the performance for age and ear. METHODS: This prospective study included 200 <b>normal</b> <b>listeners</b> divided into four groups according to age: 13 - 19 years (GI), 20 - 29 years (GII), 30 - 39 years (GIII), and 40 - 49 years (GIV). The Dichotic Sentence Identification was applied in four stages: training, binaural integration and directed sound from right and left. RESULTS: Better results for the right ear were observed in the stages of binaural integration in all assessed groups. There was a negative correlation between age and percentage of correct responses in both ears for free report and training. The worst performance in all stages of the test was observed for the age group 40 - 49 years old. CONCLUSIONS: Reference values for the Brazilian Portuguese version of the Dichotic Sentence Identification test in <b>normal</b> <b>listeners</b> aged 13 - 49 years were established according to age, ear, and test stage; they {{should be used}} as benchmarks when evaluating individuals with these characteristics...|$|R
5000|$|When {{sensorineural hearing}} loss (damage to the cochlea or in the brain) is present, the {{perception}} of loudness is altered. Sounds at low levels (often perceived by those without hearing loss as relatively quiet) are no longer audible to the hearing impaired, but sounds at high levels often are perceived as having the same loudness as they would for an unimpaired listener. This phenomenon {{can be explained by}} two theories: loudness grows more rapidly for these <b>listeners</b> than <b>normal</b> <b>listeners</b> with changes in level. This theory is called [...] "loudness recruitment" [...] and has been accepted as the classical explanation. More recently, it has been proposed that some listeners with {{sensorineural hearing loss}} may in fact exhibit a normal rate of loudness growth, but instead have an elevated loudness at their threshold. That is, the softest sound that is audible to these listeners is louder than the softest sound audible to <b>normal</b> <b>listeners.</b> This theory is called [...] "softness imperception", a term coined by Mary Florentine.|$|R
40|$|We used {{statistical}} modeling {{to investigate}} {{variability in the}} cortical auditory representations of 24 normal-hearing epilepsy patients undergoing electrocortical stimulation mapping (ESM). Patients were identified as <b>normal</b> or impaired <b>listeners</b> based on recognition accuracy for acoustically filtered words used to simulate everyday listening conditions. The experimental ESM task was a binary (same– different) auditory syllable discrimination paradigm that both listener groups performed accurately at baseline. Template mixture modeling of speech discrimination deficits during ESM showed larger and more variable cortical distributions for impaired <b>listeners</b> than <b>normal</b> <b>listeners,</b> despite comparable behavioral performances. These results demonstrate that individual differences in speech recognition {{abilities are reflected in}} the underlying cortical representations. Key words: statistical modeling; auditory cortex; speech recognition; epilepsy; brain mapping; superior temporal gyru...|$|R
40|$|Objective: One {{type of test}} {{commonly}} used to examine auditory processing disorders (APD) is the low-pass filtered speech test (LPFST), {{of which there are}} various versions. In LPFSTs, a monaural, low-redundancy speech sample is distorted by using filtering to modify its frequency content. Due to the richness of the neural pathways in the auditory system and the redundancy of acoustic information in spoken language, a <b>normal</b> <b>listener</b> is able to recognize speech even when parts of the signal are missing, whereas this ability is often impaired in listeners with APD. One limitation of the various versions of the LPFST is that they are carried out using a constant level of low-pass filtering (e. g. a fixed 1 kHz corner frequency) which makes them prone to ceiling and floor effects. The {{purpose of this study was}} to counter these effects by modifying the LPFST using a computer-based adaptive procedure, and to evaluate the performance of normal-hearing participants of varying ages on the test. Methods: In this preliminary study, 33 adults and 30 children (aged 8 to 11 years) with no known history of listening difficulties were tested. The University of Canterbury Adaptive Speech Test (UCAST) platform was used to administer a four-alternative forced-choice adaptive test that altered a low-pass filter (LPF) to track the corner frequency at which participants correctly identified a certain percentage of the word stimuli. Results: Findings on the University of Canterbury Adaptive Speech Test – Filtered Words (UCAST-FW) indicated a significant maturational effect. Adult participants performed significantly better on the UCAST-FW in comparison to the child participants. The UCAST-FW test was reliable over repeated administrations. Conclusions: An adaptive low-pass filtered speech test such as the UCAST-FW is sensitive to maturational changes in auditory processing ability...|$|E
40|$|Auditory {{processing}} is {{the ability}} of the brain to manipulate and utilise the neural output of the ear based on the frequency, intensity, and temporal features of the incoming acoustic signal. An auditory processing disorder (APD) is a deficiency in this ability. One category of tests that examine auditory processing ability are the various versions of the "filtered words test" (FWT), whereby a monaural, low-redundancy speech sample is distorted by using filtering to modify its frequency content. Due to the richness of the neural pathways in the auditory system and the redundancy of acoustic information in spoken language, a <b>normal</b> <b>listener</b> is able to recognize speech even when parts of the signal are missing, whereas this ability is often impaired in listeners with APD. One limitation of the various versions of the FWT is that they are carried out using a constant level of low-pass filtering (e. g. a corner frequency of 1000 Hz), which is prone to ceiling and floor effects. The {{purpose of this study was}} to counter these effects by modifying the FWT to use a computer-based adaptive procedure, to improve the sensitivity of the test over its constant-level counterparts. The University of Canterbury Monosyllabic Adaptive Speech Test (UC MAST) was performed on 23 normal adults, and 32 normal children (7 to 11 years of age). The child participants also underwent the SCAN-C test for APD in Children (Revised). Findings indicated a significant maturational effect on the UC MAST. Adult participants performed significantly better on the UC MAST in comparison to the child participants. In addition, adult participants performed the UC MAST more reliably than their younger counterparts. No correlation was found between performance on the UC MAST and SCAN-C test. The development of the UC MAST is discussed and the clinical implications of the findings are explored...|$|E
40|$|Two {{experiments}} investigated pitch perception for stimuli {{where the}} place of excitation was held constant. Experiment 1 used pulse trains in which the interpulse interval alternated between 4 and 6 ms. In experiment 1 a these " 4 - 6 " pulse trains were bandpass filtered between 3900 and 5300 Hz and presented acoustically against a noise background to <b>normal</b> <b>listeners.</b> The rate of an isochronous pulse train (in which all the interpulse intervals were equal) was adjusted so that its pitch matched that of the " 4 - 6 " stimulus. The pitch matches were distributed unimodally, had a mean of 5. 7 ms, and never corresponded to either 4 or to 10 ms (the period of the stimulus). In experiment 1 b the pulse trains were presented both acoustically to <b>normal</b> <b>listeners</b> and electrically to users of the LAURA cochlear implant, via a single channel of their device. A forced-choice procedure {{was used to measure}} psychometric functions, in which subjects judged whether the 4 - 6 stimulus was higher or lower in pitch than isochronous pulse trains having periods of 3, 4, 5, 6, or 7 ms. For both groups of listeners, the point of subjective equality corresponded to a period of 5. 6 to 5. 7 ms. Experiment 1 c confirmed that these psychometric functions were monotonic over the range 4 - 12 ms. In experiment 2, <b>normal</b> <b>listeners</b> adjusted the rate of an isochronous filtered pulse train to match the pitch of mixtures of pulse trains having rates of F 1 and F 2 Hz, passed through the same bandpass filter (3900 - 5400 Hz). The ratio F 2 /F 1 was 1. 29 and F 1 was either 70, 92, 109, or 124 Hz. Matches were always close to F 2 Hz. It is concluded that the results of both experiments are inconsistent with models of pitch perception which rely on higher-order intervals. Together with those of other published data on purely temporal pitch perception, the data are consistent with a model in which only first-order interpulse intervals contribute to pitch, and in which, over the range 0 - 12 ms, longer intervals receive higher weights than short intervals. status: publishe...|$|R
40|$|Production variability, {{traditionally}} {{associated with}} AOS, constitutes {{a potential threat}} to test reliability for single word intelligibility. In this investigation, we examined test-retest reliability for 13 apraxic speakers. Using a speech sample consisting of 50 monosyllabic words and 11 <b>normal</b> <b>listeners</b> responding through orthographic transcription, overall intelligibility was highly consistent for identical target samples recorded on the same day. Item-by-item analysis of listener responses indicated that this stability was not due to consistency in either articulation changes or how listeners perceived the errors. Therefore, clinical applications should consider both speech sample size and listening group size carefully...|$|R
40|$|The {{purpose of}} this study was to {{investigate}} the effects of increased intensity on the bone conducted speech discrimination ability of <b>normal</b> <b>listeners</b> utilizing standard audiological equipment. The NU- 6 word lists were utilized to test the bone conducted speech discrimination skills of ten normal hearing subjects, 21 to 30 years of age, on standard clinical equipment. Both the hearing levels (dB HL) and the sensation levels (dB SL) of the test administration were considered. In general, it was recommended that 100 dB Hl is the most appropriate dial setting for the administration of bone conducted speech discrimination tests even though comparable speech discrimination scores may be obtained with a 95 dB HL dial setting. This study indicates that the most appropriate sensation levels for the administration of bone conducted speech discrimination tests are 55 and 60 dB SL. Most <b>normal</b> <b>listeners</b> can be expected to achieve a 55 dB sensation level at the limits of the speech audiometer (100 dB HL). Additionally, it was found that when bone conducted speech discrimination tests are administered at levels of less than 55 dB SL, the results may be compromised by variances that occurred in this normal hearing sample. Therefore, the clinical audiologist should accept bone conducted speech discrimination results as valid only when the scores obtained at 40, 45 and 50 dB sensation levels are within the limits of clinical normality (90 % or better). Recommendations for further research are discussed...|$|R
40|$|Fundamental {{frequency}} (F 0) {{variation is}} {{one of a number of}} acoustic cues <b>normal</b> hearing <b>listeners</b> use for guiding lexical segmentation of degraded speech. This study examined whether F 0 contour facilitates lexical segmentation by listeners fitted with cochlear implants (CIs). Lexical boundary error patterns elicited under unaltered and flattened F 0 conditions were compared across three groups: listeners with conventional CI, listeners with CI and preserved low-frequency acoustic hearing, and <b>normal</b> hearing <b>listeners</b> subjected to CI simulations. Results indicate that all groups attended to syllabic stress cues to guide lexical segmentation, and that F 0 contours facilitated performance for listeners with low-frequency hearing...|$|R
40|$|Are <b>normal</b> <b>listeners</b> able to {{identify}} any significant differences between multichannel audio codecs when listening to commercial music releases on good quality, consumer audio equipment? Audio professionals have often questioned whether consumers are able to hear the difference between high density, uncompressed multichannel formats, and lower data-rate delivery formats. In this study, formal subjective listening tests were conducted according to the ITU-R BS. 1534 (MUSHRA) recommendation to evaluate consumer perception of popular 5. 1 surround sound formats, namely Dolby AC- 3, DTS, WMA Pro and mp 3 surround. Results suggest there is a threshold data-rate below which consumers are able to hear audible differences. Experimental design, methodology, and results will be presented and discussed...|$|R
40|$|Abstract- A {{major part}} of the {{interaction}} between humans takes place via speech communication. It is very difficult to understand speech signals in presence of background noise for the <b>normal</b> <b>listeners</b> and hearing impaired persons. Speech enhancement algorithms reduce the background noise, and thereby improvethe signal quality and intelligibility. Estimation of noise level in an audio speech signal is a very important parameter to improve the efficiency of denoising. This article presents different approaches used so far by the researchers to estimate the noise level (additive background noise) in a speech signal using various statistical and analytical techniques. Advantages and limitations of various algorithms are also discussed. The study can be further extended to other signal processing applications...|$|R
40|$|Contralateral masking is the {{phenomenon}} where a masker presented to one ear affects {{the ability to}} detect a signal in the opposite ear. For <b>normal</b> hearing <b>listeners,</b> contralateral masking results in masking patterns that are both sharper and dramatically smaller in magnitude than ipsilateral masking. The goal {{of this study was}} to investigate whether medial olivocochlear (MOC) efferents are needed for the sharpness and relatively small magnitude of the contralateral masking function. To do this, bilateral cochlear implant patients were tested because, by directly stimulating the auditory nerve, cochlear implants circumvent the effects of the MOC efferents. The results indicated that, as with <b>normal</b> hearing <b>listeners,</b> the contralateral masking function was sharper than the ipsilateral masking function. However, although there was a reduction in the magnitude of the contralateral masking function compared to the ipsilateral masking function, it was relatively modest. This is in sharp contrast to the results of <b>normal</b> hearing <b>listeners</b> where the magnitude of the contralateral masking function is greatly reduced. These results suggest that MOC function may not play a large role in the sharpness of the contralateral masking function but may play a considerable role in the magnitude of the contralateral masking function...|$|R
40|$|Speech {{recognition}} {{becomes more}} difficult and effortful with age, even for <b>normal</b> hearing <b>listeners</b> (van Rooij and Plomp, 1990; Dubno et al., 1997). • Speech recognition is particularly affected in older adults when listening conditions are demanding (Sommers and Danielson, 1999; Gordon-Salant and Fitzgibbons, 2001...|$|R
40|$|<b>Normal</b> <b>listeners</b> {{effortlessly}} {{determine a}} person’s gender by voice, but the cerebral mechanisms underlying this ability remain unclear. Here, we demonstrate 2 stages of cerebral processing during voice gender categorization. Using voice morphing {{along with an}} adaptation-optimized {{functional magnetic resonance imaging}} design, we found that secondary auditory cortex including the anterior part of the temporal voice areas in the right hemisphere responded primarily to acoustical distance with the previously heard stimulus. In contrast, a network of bilateral regions involving inferior prefrontal and anterior and posterior cingulate cortex reflected perceived stimulus ambiguity. These findings suggest that voice gender recognition involves neuronal populations along the auditory ventral stream responsible for auditory feature extraction, functioning in pair with the prefrontal cortex in voice gender perception...|$|R
40|$|In {{this issue}} of Neuron, Grossmann et al. provide the first {{evidence}} of voice-sensitive regions in the brain of 7 -month-old, but not 4 -month-old, infants. We discuss {{the implications of these}} findings for our understanding of cerebral voice processing in the first months of life. Vocal communication {{is at the heart of}} human life. It relies on speech perception but also on a rich set of abilities to extract, evaluate, and categorize the vast amounts of nonlinguistic information available in voices. These voice cognition abilities allow <b>normal</b> <b>listeners</b> to effortlessly identify a speaker’s gender and approximate age, to recognize his/her affective state and whether or not it matches the verbal information, and include more subtle percepts such as attractiveness or trustworthines...|$|R
40|$|In {{everyday}} settings, {{the ability}} to selectively attend is critical for communication. Most normalhearing listeners are able to selectively attend to a talker of interest {{in a sea of}} competing sources, and to rapidly shift attention as the need arises. However, hearing-impaired (HI) listeners and cochlear implant users have difficulty communicating when there are multiple sources. This paper reviews some of the processes governing selective attention in <b>normal</b> <b>listeners.</b> Results suggest that selective attention operates to select out perceptual &quot;objects, &quot; and thus depends directly on {{the ability to}} separate a source of interest from a mixture of competing sources. In turn, this view suggests that one factor affecting how well HI listeners can communicate in everyday settings is their ability to perceptually organize the auditory scene...|$|R
40|$|The {{perception}} {{and production of}} voice onset time (VOT) was investigated in a Thai patient with an adventitious, profound sensorineural hearing loss. Thai exhibits a three-category voicing distinction for bilabial (/b, p, ph/) and alveolar (Id, t, th/) stops, and a two-category distinction for velar (/k, k h n stops. VOT perception was measured in labeling reiponses to synthetic speech continua differing in VOT; VOT production was measured in word-initial stops of words produced in isolation. These measurements were compared with previously published VOT data for normal-hearing Thai speakers. The results of acoustic analyses of this subject’s productions suggested only minor articulatory pertur-bations, and the target phonemes were generally identified accurately by <b>normal</b> <b>listeners.</b> Key words: deafness, voice onset time, Tha...|$|R
40|$|SummaryA {{larger number}} of {{research}} studies has been performed with different people and objectives and {{have shown that the}} sentence recognition test in noise is the best instrument to evaluate individuals’ daily communication. However, we believe these tests are not applied so frequently because they require a lot of research to establish the parameters and variables related to their application and interpretation of the results. Aim: To check the reliability of the recognition threshold of the sentences in quiet and in noise for a group of young <b>normal</b> <b>listeners.</b> Study design: transversal cohort. Material and Method: The group comprised 40 subjects, 20 males and 20 females, with ages between 18 and 28 and all of them with normal hearing threshold. First, we applied the Basic Audiological Evaluation and after this, the Sentence Recognition Threshold test in quiet (LRSS) and in noise (LRSR). The sentences and the noise (fixed in 65 dB HL) were presented monoaurally, by earphones through “ascending-descending” strategy. The test and retest were done in different evaluation sessions, with an interval of seven days between them, respecting the same hour of evaluation. Results: The results showed strong positive statistically significant correlation between the test and retest of LRSS, both for right ear (r = 0. 6107) and left ear (r = 0. 5853), as S/N ratio, for right ear (r = 0. 5711) and for left ear (r = 0. 5867) for the assessed individuals. Conclusion: In the end of this study, we concluded that LRSS and S/N ratio obtained from the Portuguese Sentence List Test showed to be highly reliable, with strong positive correlation when compared to the results obtained in different sessions of evaluation in a group of young <b>normal</b> <b>listeners...</b>|$|R
40|$|The {{binaural}} hearing of four hearing impaired listeners was tested in interaural time discrimination, interaural intensity discrimination, interaural correlation discrimination and binaural detection experiments. All {{tests were conducted}} on each subject using third-octave bands of noise centered at 250, 500, 1000, 2000 and 4000 Hz. In general, the pattern of loss in {{binaural hearing}} was independent of the pattern of loss as measured by current audiometric techniques. By augmenting a simple, representative, narrowband model of binaural interaction with an interaural-differences averager, {{the results from the}} interaural time and intensity discrimination tests were used to predict and relate the correlation discrimination and binaural detection results. For both hearing impaired and <b>normal</b> <b>listeners,</b> this study suggests that narrowband correlation discrimination and NOS'rf detection can be characterized by a listener's sensitivity to interaural time and intensity differences...|$|R
40|$|The {{purpose of}} this {{research}} was two-fold. Firstly to develop a music perception test for hearing aid users and secondly to evaluate the influence of non-linear frequency compression (NFC) on music perception {{with the use of the}} self-compiled test. This article focuses on the description of the development and validation of a music perception test. To date, the main direction in frequency lowering hearing aid studies has been in relation to speech perception abilities. With improvements in hearing aid technology, interest in musical perception as a dimension that could improve hearing aid users’ quality of life grew. The Music Perception Test (MPT) was designed to evaluate different aspects of rhythm, timbre, pitch and melody. The development of the MPT could be described as design based. Phase 1 of the study included test development and recording while Phase 2 entailed presentation of stimuli to <b>normal</b> hearing <b>listeners</b> (n= 15) and hearing aid users (n= 4). Based on the findings of Phase 2, item analysis was performed to eliminate or change stimuli that resulted in high error rates. During Phase 3 the adapted version of the test was performed on a smaller group of <b>normal</b> hearing <b>listeners</b> (n= 4) and twenty hearing aid users. Results proved that normal hearing adults as well as adults using hearing aids were able to complete all the sub-tests of the MPT although hearing aid users scored less on the various sub-tests than <b>normal</b> hearing <b>listeners.</b> For the rhythm section of the MPT <b>normal</b> hearing <b>listeners</b> scored on average 93. 8 % versus 75. 5 % of hearing aid users and 83 % for the timbre section compared to 62. 3 % by hearing aid users. <b>Normal</b> hearing <b>listeners</b> obtained an average score of 86. 3 % for the pitch section and 88. 2 % for the melody section compared to the 70. 8 % and 61. 9 % respectively obtained by hearing aid users. This implicates that the MPT can be used successfully for assessment of music perception in hearing aid users within the South African context and can therefore result in more accountable hearing aid fittings taking place. The test can further be used as a counseling tool to assist audiologists and patients in understanding the problems they experience regarding music perception and might be used for future musical training in areas where participants experience problems to customize individual fitting...|$|R
40|$|The {{purpose of}} this {{research}} was twofold: firstly, to develop a music perception test (MPT) for hearing-aid users, and secondly, to evaluate the influence of non-linear frequency compression (NFC) on music perception {{with the use of the}} self-compiled test. This article focuses on the description of the development and validation of the MPT. To date, the main direction in frequency-lowering hearing-aid studies has been in relation to speech perception abilities. As hearing-aid technology has improved, interest has grown in musical perception as a dimension that could improve hearing-aid users’ quality of life. The MPT was designed to evaluate different aspects of rhythm, timbre, pitch and melody. The development of the MPT could be described as design-based. Phase 1 of the study included test development and recording, while phase 2 entailed presentation of stimuli to <b>normal</b> hearing <b>listeners</b> (n= 15) and hearing-aid users (n= 4). Based on the findings of phase 2, item analysis was performed to eliminate or change stimuli that resulted in high error rates. During phase 3 the adapted version of the test was performed on a smaller group of <b>normal</b> hearing <b>listeners</b> (n= 4) and 20 hearing-aid users. Results proved that adults with normal hearing as well as adults using hearing aids were able to complete all the sub-tests of the MPT, although hearing-aid users scored lower on the various sub-tests than <b>normal</b> hearing <b>listeners.</b> For the rhythm section of the MPT <b>normal</b> hearing <b>listeners</b> scored on average 93. 8 % versus 75. 5 % of hearing-aid users; for the timbre section the scores were 83 % versus 62. 3 % respectively. <b>Normal</b> hearing <b>listeners</b> obtained an average score of 86. 3 % for the pitch section and 88. 2 % for the melody section, compared with the 70. 8 % and 61. 9 % respectively obtained by hearing-aid users. This implies that the MPT can be used successfully for assessment of music perception in hearing-aid users within the South African context and may therefore result in more effective hearing-aid fittings taking place. The test can be used as a counselling tool to assist audiologists and patients in understanding the problems they experience regarding music perception, and might be used for future musical training in areas where participants experience problems in customising individual fittings. [URL]...|$|R
40|$|Objectives: The {{purpose of}} this study was to {{investigate}} the potential of measures of auditory short-term memory (ASTM) to provide a clinical measure of intrusion in tinnitus. Design: Response functions for six <b>normal</b> <b>listeners</b> on a delayed pitch discrimination task were contrasted in three conditions designed to manipulate attention in the presence and absence of simulated tinnitus: (1) no-tinnitus, (2) ignore-tinnitus, and (3) attend-tinnitus. Results: Delayed pitch discrimination functions were more variable in the presence of simulated tinnitus when listeners were asked to divide attention between the primary task and the amplitude of the tinnitus tone. Conclusions: Changes in the variability of auditory short-term memory may provide a novel means of quantifying the level of intrusion associated with the tinnitus percept during listening. This	research	was	part-funded	by	an	Action	on	Hearing	Loss	summer	bursary	 awarded	to	the	first	author. Peer-reviewedPost-prin...|$|R
40|$|This thesis {{includes}} four studies investigating neural correlates underlying pitch perception, {{and effects of}} tonal context on this percept. Each study addressed the issue from a unique methodological perspective. The first study confirmed that tonal context can affect the way a tone's pitch is perceived. In this study, <b>normal</b> <b>listeners</b> made pitch discriminations between tones varying in pitch and/or timbre, a difficult task when presented in isolation. Increasing tonal context increased performance, with melodic context providing the most facilitation. A similar task was presented to patients with unilateral focal excisions in the temporal lobe. Patients with right but not left temporal lobe lesions were impaired at using melodic cues to facilitate performance. Posterior extent of the lesions did not affect results, implying that right anterior temporal regions can process pitch information relative to tones heard previously. A {{functional magnetic resonance imaging}} (fMRI) study using a similar task with <b>normal</b> <b>listeners</b> found converging evidence. Melodic context produced the most activity in right anterior superior temporal gyrus (STG), as well as the most facilitation behaviorally. A positron emission tomography study investigating neural processing of song stimuli broadened the investigation to include a comparison between musical and linguistic processing. Left frontal and temporal structures known to be involved in language processing were active when subjects attended to song lyrics, and right temporal-lobe structures were again implicated in melodic processing, suggesting that a song's lyrics and melodies are processed separately. These studies find pitch processing in tonal contexts to involve right temporal-lobe structures. The right anterior STG in particular appears to be involved in processing pitch relative to previously heard tones. This suggests that the right anterior STG processes tones with respect to their tonal context, which entails holding contextual tones in memory while processing subsequent tones. This region has connections to right dorsolateral frontal areas previously implicated in tonal working memory, possibly providing a mechanism for holding contextual tones in memory. Supporting this theory, all contextual conditions in the fMRI study produced activity in right dorsolateral frontal cortex...|$|R
40|$|A Greek Emotional Speech Database {{has been}} {{recorded}} and analyzed. The scope of this work is to study through this database the prosodical phenomena {{as a function of}} each emotional state and the application to a text to speech synthesis system. Our database contains recordings of a female professional actress. We used an actress for this task because in order to faithfully simulate a number of emotions. The simulated emotions for our database were sadness, anger, fear, joy and a neutral session. The recordings consisted of ten single words, twenty short sentences, twenty five long sentences and twelve passages of fluent speech (ranging from three to five sentences each). Following the recordings, a listening test was performed to test whether <b>normal</b> <b>listeners</b> could identify the type of emotion that characterized the recorded utterances. Six qualified listeners were used, both men and women, of different ages, from several social environments. 1...|$|R
40|$|<b>Normal</b> hearing <b>listeners</b> {{exploit the}} formant {{transition}} (FT) detection to identify place of articulation for stop consonants. Neuro-imaging studies revealed that short FT induced less cortical activation than long FT. To determine {{the ability of}} hearing impaired listeners to distinguish short and long formant transitions (FT) from vowels of the same duration, 84 mild to severe hearing impaired <b>listeners</b> and 5 <b>normal</b> hearing <b>listeners</b> were asked to detect 10 synthesized stimuli with long (200 ms) or short (40 ms) FT among 30 stimuli of the same duration without FT. Hearing impaired listeners were tested with and without hearing aids. The effect {{of the difficulty of}} the task (short/long FT) was analysed {{as a function of the}} hearing loss with and without hearing aids. <b>Normal</b> hearing <b>listeners</b> were able to detect every FT (short and long). For hearing impaired listeners, the detection of long FT was better than that of short ones irrespective of their degree of hearing loss. The use of hearing aids improved detection of both kinds of FT; however, the detection of long FT remained much better than the detection of the short ones. The length of FT modified the ability of hearing impaired patients to detect FT. Short FT had access to less cortical processing than long FT and cochlea damages enhanced this specific deficit in short FT brain processing. These findings help to understand the limit of deafness rehabilitation in the time domain and should be taken into account in future devices development...|$|R
40|$|Two sets of {{synthetic}} sentences were tape recorded through two identical hearing aids at different distances from the talker in {{a background of}} noise. The recordings were made during a regular session of a hearing-impaired nursery program. One hearing aid was approximately 2. 5 meters from the talker and constituted the "Far " condition, while a second aid was ap-proximately 25 centimeters from the talker and constituted the "Near " condition. The two aids led to separate channels of a tape recorder. Sentences under the two conditions were recorded simultaneously on the two channels of the recorder. The tape was played back to eleven <b>normal</b> <b>listeners.</b> Their sentence identification scores were 10 percent for the "Far" and 98. 5 percent for the "Near " condition. These results sup-port recommendations that personnel working with hearing-impaired children should talk as near {{as possible to the}} microphone of the aided child. Talking near the microphone improves speech reception through hearing aids, auditory trainers or any other electroacoustic system...|$|R
40|$|This {{is a first}} {{installment}} in newly initiated studies of the speech perception of the hearing impaired. The {{aim of this study}} was to assess the ability of hard of hearing children and adolescents to interpret, through spoken, language, the emotional state of the speaker. Hard of hearing subjects were compared to <b>normal</b> <b>listeners</b> on a test of identifying emotions in speech. It was found, as expected from earlier research, that the hearing impaired subjects were considerably less sure of how to interpret the speech signal in terms of the emotions of the speaker than young adult <b>listeners</b> with <b>normal</b> hearing. It was also found that the loud speech often used in every day communication with the hearing impaired is associated with the emotion "angry" by both hard of hearing and normal subjects. A moderate correlation was found between degree of hearing loss and the ability to identify the emotional content of speech...|$|R
