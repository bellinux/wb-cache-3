10000|4093|Public
5|$|Random {{matrices}} are matrices whose {{entries are}} random numbers, subject to suitable probability distributions, such as matrix <b>normal</b> <b>distribution.</b> Beyond probability theory, they are applied in domains ranging from number theory to physics.|$|E
5|$|Logarithms {{also occur}} in {{log-normal}} distributions. When the logarithm of a random variable has a <b>normal</b> <b>distribution,</b> the variable {{is said to}} have a log-normal distribution. Log-normal distributions are encountered in many fields, wherever a variable is formed as the product of many independent positive random variables, for example in the study of turbulence.|$|E
5|$|The MSE {{method is}} also {{sensitive}} to secondary clustering. One {{example of this}} phenomenon is when a set of observations is thought {{to come from a}} single <b>normal</b> <b>distribution,</b> but in fact comes from a mixture normals with different means. A second example is when the data is thought to come from an exponential distribution, but actually comes from a gamma distribution. In the latter case, smaller spacings may occur in the lower tail. A high value of M(θ) would indicate this secondary clustering effect, and suggesting {{a closer look at the}} data is required.|$|E
5000|$|Simple {{examples}} can {{be given}} by a mixture of two <b>normal</b> <b>distributions.</b> (See Multimodal distribution#Mixture of two <b>normal</b> <b>distributions</b> for more details.) ...|$|R
50|$|Unlike the t-test it {{does not}} require the {{assumption}} of <b>normal</b> <b>distributions.</b> It is nearly as efficient as the t-test on <b>normal</b> <b>distributions.</b>|$|R
40|$|Inference for the {{coefficient}} of variation in <b>normal</b> <b>distributions</b> is considered. An explicit estimator of a coefficient of variation that is shared by several populations with <b>normal</b> <b>distributions</b> is proposed. Methods for making confidence intervals and statistical tests, based on McKay's approximation for {{the coefficient}} of variation, are provided. Exact expressions {{for the first two}} moments of McKay's approximation are given. An approximate F-test for equality of a coefficient of variation that is shared by several <b>normal</b> <b>distributions</b> and a coefficient of variation that is shared by several other <b>normal</b> <b>distributions</b> is introduced...|$|R
5|$|Radiocarbon {{dates are}} {{generally}} {{presented with a}} range of one standard deviation (usually represented by the Greek letter sigma as 1σ) {{on either side of the}} mean. However, a date range of 1σ represents only 68% confidence level, so the true age of the object being measured may lie outside the range of dates quoted. This was demonstrated in 1970 by an experiment run by the British Museum radiocarbon laboratory, in which weekly measurements were taken on the same sample for six months. The results varied widely (though consistently with a <b>normal</b> <b>distribution</b> of errors in the measurements), and included multiple date ranges (of 1σ confidence) that did not overlap with each other. The measurements included one with a range from about 4250 to about 4390 years ago, and another with a range from about 4520 to about 4690.|$|E
25|$|This simple {{combination}} is possible because the sample mean and sample variance of the <b>normal</b> <b>distribution</b> are independent statistics; {{this is only}} true for the <b>normal</b> <b>distribution,</b> and in fact characterizes the <b>normal</b> <b>distribution.</b>|$|E
25|$|The {{median of}} a <b>normal</b> <b>distribution</b> with mean μ and {{variance}} σ2 isnbsp&μ. In fact, for a <b>normal</b> <b>distribution,</b> mean = median = mode.|$|E
50|$|A {{study of}} a mixture density of two <b>normal</b> <b>distributions</b> data found that {{separation}} into the two <b>normal</b> <b>distributions</b> was difficult unless the means were separated by 4-6 standard deviations.|$|R
30|$|The second {{multimodal}} dataset {{consists of}} a mixture of four <b>normal</b> <b>distributions,</b> each with the same covariance matrix as the unimodal dataset, but different mean values. From {{each of the four}} <b>normal</b> <b>distributions,</b> 100 points are drawn.|$|R
40|$|Two {{conditions}} are shown under which elliptical distributions are scale mixtures of <b>normal</b> <b>distributions</b> {{with respect to}} probability distributions. The issue of finding the mixing distribution function is also considered. As a unified theoretical framework, it is also shown that any scale mixture of <b>normal</b> <b>distributions</b> is always a term of a sequence of elliptical distributions, increasing in dimension, and that all the terms of this sequence are also scale mixtures of <b>normal</b> <b>distributions</b> sharing the same mixing distribution function. Some examples are shown as applications of these concepts, showing the way of finding the mixing distribution function. Bayesian methods Elliptically contoured distribution Elliptical distribution Laplace transform Scale mixtures of <b>normal</b> <b>distributions...</b>|$|R
25|$|Measurement {{errors in}} {{physical}} experiments are often modeled by a <b>normal</b> <b>distribution.</b> This {{use of a}} <b>normal</b> <b>distribution</b> {{does not imply that}} one is assuming the measurement errors are normally distributed, rather using the <b>normal</b> <b>distribution</b> produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.|$|E
25|$|In reality, {{biological}} parameters {{tend to have}} a log-normal distribution, {{rather than}} the arithmetical <b>normal</b> <b>distribution</b> (which is generally referred to as <b>normal</b> <b>distribution</b> without any further specification).|$|E
25|$|For the {{important}} {{case in which}} the data are hypothesized to follow the <b>normal</b> <b>distribution,</b> depending {{on the nature of the}} test statistic and thus the underlying hypothesis of the test statistic, different null hypothesis tests have been developed. Some such tests are z-test for <b>normal</b> <b>distribution,</b> t-test for Student's t-distribution, f-test for f-distribution. When the data do not follow a <b>normal</b> <b>distribution,</b> it can still be possible to approximate the distribution of these test statistics by a <b>normal</b> <b>distribution</b> by invoking the central limit theorem for large samples, as in the case of Pearson's chi-squared test.|$|E
30|$|All random {{variables}} follow <b>normal</b> <b>distributions.</b>|$|R
30|$|KL {{divergence}} between two <b>normal</b> <b>distributions</b> (KL-Gaussian).|$|R
5000|$|... #Subtitle level 3: Nonlinearly {{correlated}} <b>normal</b> <b>distributions</b> ...|$|R
25|$|Benford's law was empirically tested {{against the}} numbers (up to the 10th digit) {{generated}} {{by a number of}} important distributions, including the uniform distribution, the exponential distribution, the half-normal distribution, the right-truncated normal, the <b>normal</b> <b>distribution,</b> the chi square distribution and the log <b>normal</b> <b>distribution.</b> In addition to these the ratio distribution of two uniform distributions, the ratio distribution of two exponential distributions, the ratio distribution of two half-normal distributions, the ratio distribution of two right-truncated normal distributions, the ratio distribution of two chi-square distributions (the F distribution) and the log <b>normal</b> <b>distribution</b> were tested.|$|E
25|$|Matrix <b>normal</b> <b>distribution</b> {{describes}} {{the case of}} normally distributed matrices.|$|E
25|$|The {{cumulative}} distribution function of the <b>normal</b> <b>distribution</b> in mathematics and statistics.|$|E
40|$|Statistical {{approaches}} play {{an important}} role in computer vision, <b>normal</b> <b>distributions</b> especially are widely used. In this paper we present a new approach for a continuous parametrization of <b>normal</b> <b>distributions.</b> Our method is based on arbitrary interpolation techniques. This approach is used to improve the discrete statistical eigenspace approach for object recognition. The continuous parametrization of <b>normal</b> <b>distributions</b> allows an estimation of object poses where no training images were available. In an experiment with real objects we will show that our continuous approach leads to better localization and classification results than the discrete approach...|$|R
5000|$|... #Subtitle level 3: Kullback-Leibler {{divergence}} for multivariate <b>normal</b> <b>distributions</b> ...|$|R
40|$|The {{problem of}} {{estimating}} a mean vector of scale mixtures of multivariate <b>normal</b> <b>distributions</b> with the quadratic loss function is considered. For a certain class of these distributions, which includes at least multivariate-t distributions, admissible minimax estimators are given. Admissible Minimax Scale mixtures of multivariate <b>normal</b> <b>distributions</b> The FKG inequality...|$|R
25|$|The {{length of}} chess games tends {{to follow a}} log <b>normal</b> <b>distribution.</b>|$|E
25|$|It {{explains}} the ubiquitous {{occurrence of the}} <b>normal</b> <b>distribution</b> in nature.|$|E
25|$|Conditional probit - Allows full {{covariance}} among alternatives using a joint <b>normal</b> <b>distribution.</b>|$|E
50|$|It {{is obvious}} that if {{the means of the}} two <b>normal</b> <b>distributions</b> are equal then the {{combined}} distribution is unimodal. Conditions for unimodality of the combined distribution were derived by Eisenberger. Necessary and sufficient conditions for a mixture of <b>normal</b> <b>distributions</b> to be bimodal have been identified by Ray and Lindsay.|$|R
5000|$|Cochran's theorem, on decomposing sum of {{squares of}} <b>normal</b> <b>distributions</b> ...|$|R
5000|$|Kalman filter, a {{recursive}} Bayesian filter for multivariate <b>normal</b> <b>distributions</b> ...|$|R
25|$|Then Z0 and Z1 are {{independent}} random variables with a standard <b>normal</b> <b>distribution.</b>|$|E
25|$|As {{shown in}} diagram at right, this {{phenomenon}} has relatively small effect if {{the standard deviation}} (as compared to the mean) is relatively small, as it makes the log-normal distribution appear similar to an arithmetical <b>normal</b> <b>distribution.</b> Thus, the arithmetical <b>normal</b> <b>distribution</b> may be more appropriate to use with small standard deviations for convenience, and the log-normal distribution with large standard deviations.|$|E
25|$|Note {{that some}} of these (such as median, or mid-range) are {{measures}} of central tendency, and are used as estimators for a location parameter, such as the mean of a <b>normal</b> <b>distribution,</b> while others (such as range or trimmed range) are measures of statistical dispersion, and are used as estimators of a scale parameter, such as the standard deviation of a <b>normal</b> <b>distribution.</b>|$|E
50|$|A {{package for}} R is {{available}} for testing for bimodality. This package assumes that the data are distributed as a sum of two <b>normal</b> <b>distributions.</b> If this assumption is not correct the results may not be reliable. It also includes functions for fitting a sum of two <b>normal</b> <b>distributions</b> to the data.|$|R
40|$|AbstractTwo {{conditions}} are shown under which elliptical distributions are scale mixtures of <b>normal</b> <b>distributions</b> {{with respect to}} probability distributions. The issue of finding the mixing distribution function is also considered. As a unified theoretical framework, it is also shown that any scale mixture of <b>normal</b> <b>distributions</b> is always a term of a sequence of elliptical distributions, increasing in dimension, and that all the terms of this sequence are also scale mixtures of <b>normal</b> <b>distributions</b> sharing the same mixing distribution function. Some examples are shown as applications of these concepts, showing the way of finding the mixing distribution function...|$|R
2500|$|Bhattacharyya {{distance}} – {{method used}} to separate mixtures of <b>normal</b> <b>distributions</b> ...|$|R
