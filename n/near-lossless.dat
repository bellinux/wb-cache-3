130|0|Public
50|$|The basic spec was updated in 2012, {{adding a}} new profile for {{lossless}} and <b>near-lossless</b> archiving.|$|E
50|$|On February 7, 2017, Logitech {{announced}} {{to launch the}} Logitech Brio, the first webcam with Ultra HD support with HDR and a 5x <b>near-lossless</b> zoom. The Brio supports Windows Hello, Microsoft’s face-recognition security system.|$|E
50|$|JPEG-LS is a lossless/near-lossless {{compression}} {{standard for}} continuous-tone images. Its official designation is ISO-14495-1/ITU-T.87. It {{is a simple}} and efficient baseline algorithm which consists of two independent and distinct stages called modeling and encoding. JPEG-LS was developed {{with the aim of}} providing a low-complexity lossless and <b>near-lossless</b> image compression standard that could offer better compression efficiency than lossless JPEG. It was developed because at the time, the Huffman coding-based JPEG lossless standard and other standards were limited in their compression performance. Total decorrelation cannot be achieved by first order entropy of the prediction residuals employed by these inferior standards. JPEG-LS, on the other hand, can obtain good decorrelation. Part 1 of this standard was finalized in 1999. Part 2, released in 2003, introduced extensions such as arithmetic coding. The core of JPEG-LS is based on the LOCO-I algorithm, that relies on prediction, residual modeling and context-based coding of the residuals. Most of the low complexity of this technique comes from the assumption that prediction residuals follow a two-sided geometric distribution (also called a discrete Laplace distribution) and from the use of Golomb-like codes, which are known to be approximately optimal for geometric distributions. Besides lossless compression, JPEG-LS also provides a lossy mode ("near-lossless") where the maximum absolute error can be controlled by the encoder. Compression for JPEG-LS is generally much faster than JPEG 2000 and much better than the original lossless JPEG standard.|$|E
40|$|Includes bibliographical {{references}} (pages [125]- 127) <b>Near-lossless</b> {{data compression}} algorithm and its applications are investigated in this thesis. <b>Near-lossless</b> data compression {{is an alternative}} to the lossless compression scheme. The <b>near-lossless</b> compression techniques studied give quantitative measure about the type and the amount of distortion introduced. The <b>near-lossless</b> compression techniques can yield a much higher data compression ratios than the lossless compression techniques. The algorithm proposed in this thesis is a predictive, context-base, graph searched, entropy-coded DPCM (Differential Pulse Code Modulation) technique with a window-based error criterion. The <b>near-lossless</b> data compression algorithm is developed in the context of image compression. An image is compressed in such a way th a t the total error between the original and the coded images over a W x W window around each pixel does not exceed e > 0 in magnitude. This <b>near-lossless</b> error criterion is developed to preserve the image brightness and color. Issues related to practical implementations are discussed in the thesis to solve the zero-state problem. Methods implemented to solve the zero-state problem are: violate the per-pixel error criterion, use lossless coding when zero-state occurs, set the maximum allowed per-pixel error dynamically, limit the number of states at each stage and the ML-algorithm. The <b>near-lossless</b> data compression are applied in two areas: the edge preserving image compression and <b>near-lossless</b> EEG compression. Images are compressed using the <b>near-lossless</b> data compression scheme to preserve the edge information as defined by a Laplacian operator. The EEG signals are compressed based on the predictive models and the sample-to-sample reconstruction errors are limited to a specified range. Compression results obtained for both applications suggest th a t the proposed <b>near-lossless</b> data compression methods are useful in achieving a high compression ratio while preserving the specific information in the original data. M. S. (Master of Science...|$|E
40|$|We propose <b>near-lossless</b> digital {{watermarking}} for copyright protection of remote sensing images. In particular, we show that, by forcing a maximum absolute {{difference between the}} original and watermarked scene, the <b>near-lossless</b> paradigm {{makes it possible to}} decrease the effect of watermarking on remote sensing applications to be carried out on the images. As an example, the effect of <b>near-lossless</b> watermarking on image classification is analyzed...|$|E
40|$|Abstract—We {{present a}} {{compression}} technique that provides progressive transmission {{as well as}} lossless and <b>near-lossless</b> compression in a single framework. The proposed technique produces a bit stream that results in a progressive, and ultimately lossless, reconstruction of an image similar to what one can obtain with a reversible wavelet codec. In addition, the proposed scheme provides <b>near-lossless</b> reconstruction {{with respect to a}} given bound, after decoding of each layer of the successively refinable bit stream. We formulate the image data-compression problem as one of successively refining the probability density function (pdf) estimate of each pixel. Within this framework, restricting the region of support of the estimated pdf to a fixed size interval then results in <b>near-lossless</b> reconstruction. We address the contextselection problem, as well as pdf-estimation methods based on context data at any pass. Experimental results for both lossless and <b>near-lossless</b> cases indicate that the proposed compression scheme, that innovatively combines lossless, <b>near-lossless,</b> and progressive coding attributes, gives competitive performance in comparison with state-of-the-art compression schemes. Index Terms—Embedded bit stream, image compression, lossless compression, <b>near-lossless</b> compression, probability mass estimation, successive refinement. I...|$|E
40|$|This letter {{proposes a}} <b>near-lossless</b> coder for hyperspectral images. The coding {{technique}} is fully embedded and minimizes the distortion in the l 2 norm initially {{and in the}} l∞ norm subsequently. Based on a two-stage <b>near-lossless</b> compression scheme, it includes a lossy and a <b>near-lossless</b> layer. The novelties are: the observation of the convergence of the entropy of the residuals in the original domain and in the spectral-spatial transformed domain; and an embedded <b>near-lossless</b> layer. These contributions enable a progressive transmission while optimising both SNR and PAE performance. The embeddedness is accomplished by bitplane encoding plus arithmetic encoding. Experimental {{results suggest that the}} proposed method yields a highly competitive coding performance for hyperspectral images, outperforming multi-component JPEG 2000 for l∞ norm and pairing its performance for l 2 norm, and also outperforming M-CALIC in the <b>near-lossless</b> case –for PAE ≥ 5 –...|$|E
40|$|In <b>near-lossless</b> image coding, each reconstructed pixel of the decoded image {{differs from}} the {{corresponding}} one in the original image by not more than a pre-specified value δ. Such schemes are mainly based on predictive coding techniques, which are not capable of scalable decoding. Lossless image coding with scalable decoding is mainly based on integer wavelet transforms. In this paper, methods to modify integer wavelet transforms for <b>near-lossless</b> image coding with scalable decoding features are presented. This is achieved by incorporating the <b>near-lossless</b> quantisation process, driven by δ, into lifting steps (online quantisation). Two online quantisation techniques based on 1 -D and 2 -D transforms are presented. They outperform the pre-quantisation based <b>near-lossless</b> image coding method in both bit rate and rms error performances. Further, they result in both subjectively and objectively superior performance in spatial and bit rate scalable decoding. The 2 -D online scheme results in comparable performance with JPEG-LS, which is not capable of scalable decoding. It is evident from this research that with these novel schemes, scalable decoding features can be integrated into <b>near-lossless</b> coding with only a small increase in bit rate compared to those achieved in JPEG-LS. Keywords: <b>Near-lossless</b> image coding, online quantisation, scalable image coding, pre-quantisation, lifting, integer wavelet transforms, JPEG-LS 1...|$|E
40|$|This paper {{presents}} {{a comparison of}} the performances of neural network and linear predictors for <b>near-lossless</b> compression of EEG signals. Three neural network predictors, namely, single-layer perceptron (SLP), multilayer perceptron (MLP), and Elman network (EN), and two linear predictors, namely, autoregressive model (AR) and finite-impulse response filter (FIR) are used. For all the predictors, uniform quantization is applied on the residue signals obtained as the difference between the original and the predicted values. The maximum allowable reconstruction error delta is varied to determine the theoretical bound delta(0) for <b>near-lossless</b> compression and the corresponding bit rate r(p). It is shown that among all the predictors, the SLP yields the best results in achieving the lowest values for delta(0) and r(p). The corresponding values of the fidelity parameters, namely, percent of root-mean-square difference, peak SNR and cross correlation are also determined. A compression efficiency of 82. 8 % is achieved using the SLP with a <b>near-lossless</b> bound delta(0) = 3, with the diagnostic quality of the reconstructed EEG signal preserved. Thus, the proposed <b>near-lossless</b> scheme facilitates transmission of real time as well as offline EEG signals over network to remote interpretation center economically with less bandwidth utilization compared to other known lossless and <b>near-lossless</b> schemes...|$|E
40|$|The aim of {{this paper}} is to develop an {{effective}} loss less algorithm technique to convert original image into a compressed one. Here we are using a lossless algorithm technique in order to convert original image into compressed one. Without changing the clarity of the original image. Lossless image compression is a class of image compression algorithms that allows the exact original image to be reconstructed from the compressed data. We present a compression technique that provides progressive transmission as well as lossless and <b>near-lossless</b> compression in a single framework. The proposed technique produces a bit stream that results in a progressive and ultimately lossless reconstruction of an image similar to what one can obtain with a reversible wavelet codec. In addition, the proposed scheme provides <b>near-lossless</b> reconstruction with respect to a given bound after decoding of each layer of the successively refineable bit stream. We formulate the image data compression problem as one of successively refining the probability density function (pdf) estimate of each pixel. Experimental results for both lossless and <b>near-lossless</b> cases indicate that the proposed compression scheme, that innovatively combines lossless, <b>near-lossless</b> and progressive coding attributes, gives competitive performance in comparison to state-of-the-art compression schemes...|$|E
40|$|This work {{extends the}} {{lossless}} data compression technique described in Fast Lossless Compression of Multispectral- Image Data, (NPO- 42517) NASA Tech Briefs, Vol. 30, No. 8 (August 2006), page 26. The original technique {{was extended to}} include a <b>near-lossless</b> compression option, allowing substantially smaller compressed file sizes when {{a small amount of}} distortion can be tolerated. <b>Near-lossless</b> compression is obtained by including a quantization step prior to encoding of prediction residuals. The original technique uses lossless predictive compression and is designed for use on multispectral imagery. A lossless predictive data compression algorithm compresses a digitized signal one sample at a time as follows: First, a sample value is predicted from previously encoded samples. The difference between the actual sample value and the prediction is called the prediction residual. The prediction residual is encoded into the compressed file. The decompressor can form the same predicted sample and can decode the prediction residual from the compressed file, and so can reconstruct the original sample. A lossless predictive compression algorithm can generally be converted to a <b>near-lossless</b> compression algorithm by quantizing the prediction residuals prior to encoding them. In this case, since the reconstructed sample values will not be identical to the original sample values, the encoder must determine the values that will be reconstructed and use these values for predicting later sample values. The technique described here uses this method, starting with the original technique, to allow <b>near-lossless</b> compression. The extension to allow <b>near-lossless</b> compression adds the ability to achieve much more compression when small amounts of distortion are tolerable, while retaining the low complexity and good overall compression effectiveness of the original algorithm...|$|E
40|$|ABSTRACT⎯This letter {{proposes a}} new <b>near-lossless</b> image {{compression}} method {{with only one}} line buffer cost for a digital camera with Bayer format image. For such format data, it can provide a low average compression rate (4. 24 bits/pixel) with high-image quality (larger than 46. 37 dB where the error of every pixel is less than two). The experimental {{results show that the}} <b>near-lossless</b> compression method has better performance than JPEG-LS (lossless) with δ = 2 for a Bayer format image. Keywords⎯Near-lossless image compression, Bayer format image, JPEG-LS. I...|$|E
40|$|In {{the field}} of remote sensing image {{compression}} it is often argued that traditional MSE-based fidelity metrics might not effectively describe the quality of remote sensing lossy or <b>near-lossless</b> compressed images. In this paper we introduce a performance evaluation framework based on both reconstruction fidelity and impact on image exploitation. Besides MSE, the framework also considers hard classification and mixed pixel classification, as well as anomaly detection. We apply this framework to evaluate and compare the quality of state-of-the-art lossy and <b>near-lossless</b> compression techniques applied to hyperspectral AVIRIS scene...|$|E
40|$|We propose an integrated, wavelet based, {{two-stage}} {{coding scheme}} for lossy, <b>near-lossless</b> and lossless compression of medical volumetric data. The method presented determines the bit-rate while encoding for the lossy layer {{and without any}} iteration. It is {{in the spirit of}} “lossy plus residual coding ” and consists of a wavelet-based lossy layer followed by an arithmetic coding of the quantized residual to guarantee a given pixel-wise maximum error bound. We focus on the selection of the optimum bit rate for the lossy coder to achieve the minimum total (lossy plus residual) bit rate in the <b>near-lossless</b> and the lossless cases. We propose a simple and practical method to estimate online the optimal bit rate and provide a theoretical justification for it. Experimental results show that the proposed scheme provides improved, embedded lossy, and lossless performance competitive with the best results published so far in the literature, with an added feature of <b>near-lossless</b> coding...|$|E
40|$|Predictive and multiresolution {{techniques}} for near- lossless image compression {{based on the}} criterion of maximum allowable deviation of pixel values are investigated. A procedure for <b>near-lossless</b> compression using a modification of lossless predictive coding techniques to satisfy the specified tolerance is described. Simulation results with modified versions {{of two of the}} best lossless predictive coding techniques known, CALIC and JPEG-LS, are provided. Application of lossless coding based on reversible transforms in conjunction with prequantization is shown to be inferior to predictive {{techniques for}} <b>near-lossless</b> compression. A partial embedding two-layer scheme is proposed in which an embedded multiresolution coder generates a lossy base layer, and a simple but effective context-based lossless coder codes the difference between the original image and the lossy reconstruction. Results show that this lossy plus <b>near-lossless</b> technique yields compression ratios close to those obtained with predictive techniques, while providing the feature of a partially embedded bit-stream. © 1998 SPIE and IS&T...|$|E
40|$|International audienceA new {{adaptive}} {{approach for}} lossless and <b>near-lossless</b> scalable compression of medical images is presented. It combines the adaptivity of DPCM schemes with hierarchical oriented prediction (HOP) {{in order to}} provide resolution scalability with better compression performances. We obtain lossless results which are about 4 % better than resolution scalable JPEG 2000 and close to non scalable CALIC on a large scale database. The HOP algorithm is also well suited for <b>near-lossless</b> compression, providing interesting rate-distortion trade-off compared to JPEG-LS and equivalent or better PSNR than JPEG 2000 for high bit-rate on noisy (native) medical images...|$|E
40|$|Abstract: Lossy image {{compression}} methods {{are based on}} error measuring at entire image level only. In some areas there is an obvious need for getting an upper bound for the error at pixel level. In the {{paper we propose a}} <b>near-lossless</b> compression algorithm based on DPCM simple scheme, followed by a vector quantization. The exit will have a non uniform distribution, so it will be further compressed using an entropy based method. Experimental results obtained and presented in the paper prove that the vector quantization method gives us better results than the scalar quantization and the classic LBG algorithm, in the <b>near-lossless</b> context...|$|E
40|$|This work proposes {{lossless}} and <b>near-lossless</b> compression algorithms for multi-channel biomedical signals. The algorithms are sequential and efficient, {{which makes}} them suitable for low-latency and low-power signal transmission applications. We make use of information theory and signal processing tools (such as universal coding, universal prediction, and fast online implementations of multivariate recursive least squares), combined with simple methods to exploit spatial as well as temporal redundancies typically present in biomedical signals. The algorithms are tested with publicly available electroencephalogram and electrocardiogram databases, surpassing in all cases {{the current state of}} the art in <b>near-lossless</b> and lossless compression ratios. Comment: 13 page...|$|E
40|$|Abstract: Classical image {{compression}} methods {{are based on}} measuring the error only at entire image level. In some areas there is an obvious need for getting an upper bound for the error at the pixel level. In the paper we propose such a near-lossess method based on LZW dictionary algorithm. The modifications needed to adapt LZW to become a <b>near-lossless</b> method are presented. As {{far as we know}} this is the first attempt to use LZW as a <b>near-lossless</b> method. Experimental results done and presented in the paper prove that the method gives better that the one based on quadtree partitioning so the proposed method is promising...|$|E
40|$|In {{this paper}} {{we present a}} new {{watermarking}} algorithm for joint <b>near-lossless</b> compression and authentication of remote sensing images. The adopted compression algorithm is the standard JPEG-LS algorithm. Our methodology has been designed by integrating into the standard JPEG-LS compression algorithm, {{by means of a}} stripe approach, a known authentication technique derived from Fridrich. This procedure points out two advantages: firstly, the produced bit-stream is perfectly compliant with the JPEG-LS standard, secondly, when the image has been decoded, it is always authenticated because information has been embedded in the reconstructed values. <b>Near-lossless</b> coding does not harm authentication procedure and robustness against different attacks is preserved...|$|E
40|$|The {{instantaneous}} side {{of information}} source code (SISC) design is considered. In the SISC configuration, the encoder describes source X to the decoder; the decoder uses this description and side information Y, to reconstruct X. Prior work on lossless and <b>near-lossless</b> SISC design demonstrates that globally optimal design is NP-hard. A family of polynomial complexity code design algorithms is introduced that approximates the optimal solution for lossless and <b>near-lossless</b> SISCs. The algorithm {{may be used}} to design both Huffman and arithmetic SISCs for an arbitrary probability mass function p(x,y). Experimental results comparing the resulting performances {{to each other and to}} the theoretical limit are included...|$|E
40|$|<b>Near-lossless</b> image compression-decompression {{scheme is}} {{proposed}} {{in this paper}} using Zipper Transformation (ZT) and inverse zipper transformation (iZT). The proposed ZT exploits the conjugate symmetry property of Discrete Fourier Transformation (DFT). The proposed transformation is implemented using two different configurations: the interlacing and concatenating ZT. In order to quantify {{the efficacy of the}} proposed transformation, we benchmark with Discrete Cosine Transformation (DCT) and Fast Walsh Hadamard Transformation (FWHT) in terms of lossless compression capability and computational cost. Numerical simulations show that ZT-based compression algorithm is <b>near-lossless,</b> compresses better, and offers faster implementation than both DCT and FWHT. Also, interlacing and concatenating ZT are shown to yield similar results in most of the test cases considered...|$|E
3000|$|In {{order to}} {{decrease}} the communication bandwidth and save the transmitting power in the wireless endoscopy capsule, this paper presents a new <b>near-lossless</b> image compression algorithm based on the Bayer format image suitable for hardware design. This algorithm can provide low average compression rate ([...] [...]...|$|E
40|$|Abstract—A <b>near-lossless,</b> {{adaptive}} watermarking algorithm {{based on}} DCT {{is presented to}} protect DEM from theft and illegal reproduction. Due to the high accuracy of the DEM, {{the focus of this}} paper is put on ensuring that the watermarked DEM should meet the precision requirement which also means the watermarked DEM should be <b>near-lossless.</b> The contribution of this work is that not only the DEM precision, but also the precision of slope and aspect are considered. In order to improve the robustness, the watermark should be embedded in the terrain lines. The preliminary results show that the error both of the aspect and the slope are very small and the watermarked DEM meets the medium error and maximum error proposed in the national DEM precision criterion. The watermarking can resist the compression and cropping attack...|$|E
40|$|Image Compression reduces {{redundancy}} in {{data representation}} {{in order to}} achieve saving in the cost of storage and transmission. Image compression compensates for the limited on-board resources, in terms of mass memory and downlink bandwidth and thus it provides a solution to the “bandwidth vs. data volume ” dilemma of modern spacecraft. Thus compression is very important feature in payload image processing units of many satellites. A low complexity and high efficiency <b>near-lossless</b> image compression algorithm is suggested in this paper. The algorithm provides the average compression ratio of 1. 403 with high image quality for lossless compression. Compression ratio increases as ∆ parameter increases. Using proposed algorithm compression ratio of 4. 208 is achieved for <b>near-lossless</b> compression. The proposed algorithm has low memory cost suitable for hardware implementation...|$|E
40|$|Systems using low bit-rate coding of {{high quality}} digital audio signals will be in {{widespread}} use for consumer applications in the near future. Similar systems are needed for professional applications. They need to exhibit a much larger ratio of the audibility threshold of artifacts to the actual noise level. The term <b>near-lossless</b> coding is introduced to describe such coding systems. Standard low bitrate audio coding systems {{can be used for}} these applications, but are much less efficient compared to specially optimized coding schemes. First results on the feasibility of <b>near-lossless</b> high quality audio coding systems using transform coding techniques are presented. The results are based on simulation runs at bitrates between 3 and 8 bits/sample. They are tested using both a 'maximum residual noise amplitude' criterion and a perceptual measurement technique (NMR) ...|$|E
40|$|Depth coding in 3 D-HEVC for the {{multiview}} video plus depth (MVD) architecture (i) deforms object shapes due to block-level edge-approximation; (ii) misses {{an opportunity}} for high compressibility at <b>near-lossless</b> quality by failing to exploit strong homogeneity (clustering tendency) in depth syntax, motion vector components, and residuals at frame-level; and (iii) restricts interactivity and limits responsiveness of independent use of depth information for "non-viewing" applications due to texture-depth coding dependency. This paper presents a standalone depth sequence coder, which operates in the lossless to <b>near-lossless</b> quality range while compressing depth data superior to lossy 3 D-HEVC. It preserves edges implicitly by limiting quantisation to the spatial-domain and exploits clustering tendency efficiently at frame-level with a novel binary tree based decomposition (BTBD) technique. For mono-view coding of standard MVD test sequences, on average, (i) lossless BTBD achieved × 42. 2 compression-ratio and - 60. 0 % coding gain against the pseudo-lossless 3 D-HEVC, using the lowest quantisation parameter QP = 1, and (ii) <b>near-lossless</b> BTBD achieved - 79. 4 % and 6. 98 dB Bjøntegaard delta bitrate (BD-BR) and distortion (BD-PSNR), respectively, against 3 D-HEVC. In view-synthesis applications, decoded depth maps from BTBD rendered superior quality synthetic-views, compared to 3 D-HEVC, with - 18. 9 % depth BD-BR and 0. 43 dB synthetic-texture BD-PSNR on average. Comment: Submitted to IEEE Transactions on Image Processing. 13 pages, 5 figures, and 5 table...|$|E
40|$|A new {{cost-effective}} and 2 ̆ 2 <b>near-lossless</b> 2 ̆ 2 quality {{compression system}} for images containing both video and graphics is presented. The scheme {{can be used}} successfully in two distinct architectures: for professional video distribution from content provider to broadcaster, and for local memory reduction in a consumer DTV receiver...|$|E
40|$|We {{propose a}} {{compression}} algorithm for hyperspectral images featuring both lossy and nearlossless compression. The algorithm {{is based on}} JPEG 2000, and provides better <b>near-lossless</b> compression performance than 3 D-CALIC. We also show that {{its effect on the}} results of selected applications is negligible, and in some cased better than JPEG 2000...|$|E
40|$|International audienceWe {{propose a}} new {{hierarchical}} approach for lossless and <b>near-lossless</b> resolution scalable compression. It combines the adaptability of DPCM schemes with new hierarchical oriented predictors {{in order to}} provide resolution scalability with better compression performances than usual hierarchical interpolation predictor or wavelet transform. Because the proposed hierarchical oriented prediction (HOP) is not really efficient on smooth images, we also introduce new predictors, dynamically optimized using a least-square criterion. Lossless compression results, obtained on a large-scale medical images database, are more than 4 % better on CTs and 9 % better on MRIs than resolution scalable JPEG- 2000 and close to non-scalable CALIC. The HOP algorithm is also well suited for <b>near-lossless</b> compression, providing interesting rate-distortion trade-off compared to JPEG-LS and equivalent or better PSNR than JPEG- 2000 for high bit-rate on noisy (native) medical images...|$|E
40|$|The {{primary goal}} of this thesis is to {{implement}} a hardware version of the JPEG-LS, or JPEGLossless, image compression algorithm in VHDL. The JPEG-LS algorithm is currently the designated standard for lossless compression of grayscale and color images by the JPEG committee. Although lossy image compression is widely used when dealing with grayscale images, there are some applications that require lossless image compression so that the original image may be recovered. This {{is often the case}} for historical and legal document image archives, medical and satellite imagery, and biometric images. The JPEG-LS algorithm is much less complex than other current lossless image compression algorithms and offers similar or better compression gains. <b>Near-lossless</b> compression offers higher compression gains by using a pixel tolerance specified by the user. The algorithm uses a predictive technique for compression, and the resulting prediction error is encoded, not the pixel value itself. This prediction error is encoded with Golomb-Rice coding, which is optimal for a geometric distribution such as prediction error. The predictor enters a special run-length mode to encode pixels with identical values in lossless mode (or nearly identical values within a known value in <b>near-lossless</b> mode), which maximizes compression further. In this thesis, the JPEG-LS algorithm is implemented in C, VHDL, and further synthesized using the Synopsys synthesis tool suite. Pictorial, document, medical, remote sensing, and biometric images are used for testing the project against another standard-compliant software implementation. The compression ratio for lossless compression is approximately 2 and is greater for <b>near-lossless</b> compression. The end result is a Synopsys schematic that represents a JPEG-LS codec, which is capable of lossless and <b>near-lossless</b> encoding and decoding. Performance characteristics such as chip area, speed, and power consumption are extracted from the synthesis tool. These are approximately 375, 000 gates, a 15 ns clock cycle, and 59 mW respectively. A hardware implementation of this algorithm on an FPGA or ASIC would give a digital camera or scanner an edge in the marketplace...|$|E
40|$|Predictive coding is {{attractive}} for compression onboard of spacecrafts {{thanks to its}} low computational complexity, modest memory requirements {{and the ability to}} accurately control quality on a pixel-by-pixel basis. Traditionally, predictive compression focused on the lossless and <b>near-lossless</b> modes of operation where the maximum error can be bounded but the rate of the compressed image is variable. Rate control is considered a challenging problem for predictive encoders due to the dependencies between quantization and prediction in the feedback loop, and the lack of a signal representation that packs the signal's energy into few coefficients. In this paper, we show {{that it is possible to}} design a rate control scheme intended for onboard implementation. In particular, we propose a general framework to select quantizers in each spatial and spectral region of an image so as to achieve the desired target rate while minimizing distortion. The rate control algorithm allows to achieve lossy, <b>near-lossless</b> compression, and any inbetween type of compression, e. g., lossy compression with a nearlossless constraint. While this framework is independent of the specific predictor used, in order to show its performance, in this paper we tailor it to the predictor adopted by the CCSDS- 123 lossless compression standard, obtaining an extension that allows to perform lossless, <b>near-lossless</b> and lossy compression in a single package. We show that the rate controller has excellent performance in terms of accuracy in the output rate, rate-distortion characteristics and is extremely competitive with respect to state-of-the-art transform codin...|$|E
40|$|Image Experts Group (JBIG), is a {{representative}} of a bilevel image compression algorithm. It compresses bilevel images with high per-formance, but it shows relatively low performance in compressing error-diffused halftone images. This paper proposes a new bilevel image compression for error-diffused images, {{which is based on}} Bayes ’ theorem. The proposed coding procedure consists of two passes. It groups 2 2 dots into a cell, where each cell is repre-sented by the number of black dots and the locations of the black dots in the cell. The number of black dots in the cell is encoded in the first pass, and their locations are encoded in the second pass. The first pass performs a <b>near-lossless</b> compression, which can be refined to be lossless by the second pass. Experimental results show a high compression performance for the proposed method when it is applied to error-diffused images. Index Terms—Bilevel image compression, error-diffusion, loss-less compression, <b>near-lossless</b> compression, progressive coding. I...|$|E
40|$|We {{categorize}} comprehensively {{image quality}} measures, extend measures defined for gray scale images to their multispectral case, and propose novel image quality measures. The statistical {{behavior of the}} measures and their sensitivity to various kinds of distortions, data hiding and coding artifacts are investigated via Analysis of Variance techniques. Their similarities or differences have been illustrated by plotting their Kohonen maps. Measures that give consistent scores across an image class and that are sensitive to distortions and coding artifacts are pointed out. We present techniques for steganalysis of images that have been potentially subjected to watermarking or steganographic algorithms. Our hypothesis is that watermarking and steganographic schemes leave statistical evidence that can be exploited for detection {{with the aid of}} image quality features and multivariate regression analysis. The steganalyzer is built using multivariate regression on the selected quality metrics. In the absence of the ground-truth, a common reference image is obtained based on blurring. Simulation results with the chosen feature set and well-known watermarking and steganographic techniques indicate that our approach is able to reasonably accurately distinguish between marked and unmarked images. We also present a technique that provides progressive transmission and <b>near-lossless</b> compression in one single framework. The proposed technique produces a bitstream that results in progressive reconstruction of the image just like what one can obtain with a reversible wavelet codec. In addition, the proposed scheme provides <b>near-lossless</b> reconstruction with respect to a given bound after each layer of the successively refinable bitstream is decoded. Experimental results for both lossless and <b>near-lossless</b> cases are presented, which are competitive with the state-of-the-art compression schemes...|$|E
40|$|In this work, we {{investigate}} the lossless and <b>near-lossless</b> compression of electrocardiogram (ECG) signals with different block-sorting transformations. We show that transformations with smaller context depths are {{a better choice}} for ECG signal compression when speed and memory utilization are considered. Further, we show that compression results of our proposed technique is better than other well known techniques, such as bzip 2, gzip, and the shorten waveform coder...|$|E
40|$|We {{present a}} {{technique}} that provides progressive transmission and <b>near-lossless</b> compression in one single framework. The proposed technique produces a bitstream that results in progressive reconstruction of the image just like what one can obtain with a reversible wavelet codec. In addition, the proposed scheme provides nearlossless reconstruction {{with respect to a}} given bound after each layer of the successively refinable bitstream is decoded. We formulate the image data compression problem as one of asking the optimal questions to determine, respectively, the value or the interval of the pixel, depending on whether one is interested in lossless or nearlossless compression. New prediction methods based {{on the nature of the}} data at a given pass are presented and links to the existing methods are explored. The trade-off between non-causal prediction and data precision is discussed within the context of successive refinement. Context selection for prediction in different passes is addressed. Finally, experimental results for both lossless and <b>near-lossless</b> cases are presented, which are competitive with the state-of-the-art compression schemes...|$|E
