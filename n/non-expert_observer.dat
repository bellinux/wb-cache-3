2|25|Public
5000|$|Independent {{observers}} {{noted that}} the intrinsic-extrinsic test used by the Ninth Circuit almost always led to {{a rejection of the}} copyright infringement accusation. The extrinsic test is performed by a <b>non-expert</b> <b>observer,</b> who will eventually decide on the presence of a substantial similarity. Indeed, [...] "a lay observer may mistakenly determine that two user interfaces are similar because they appear similar, even though the details of their implementation may be very different". Another criticism of the extrinsic-intrinsic test is the [...] "failure to consider medium and market needs." [...] For instance, the printing and the information industry do not share some of the same characteristics. Namely, the marginal costs of copying data in the information industry are typically insignificant compared to the printing industry. These characteristics could be kept in mind when deciding on a substantial similarity, since there are no clear guidelines for making this decision, but [...] "the preservation of the balance between competition and protection reflected in the patent and copyright laws".|$|E
50|$|This type of retinoschisis is {{very common}} with a {{prevalence}} of up to 7 percent in normal persons. Its cause is unknown. It can easily be confused with retinal detachment by the <b>non-expert</b> <b>observer</b> and in difficult cases even the expert may have difficulty differentiating the two. Such differentiation is important since retinal detachment almost always requires treatment while retinoschisis never itself requires treatment and leads to retinal detachment (and hence to visual loss) only occasionally. Unfortunately one still sees cases of uncomplicated retinoschisis treated by laser retinopexy or cryopexy {{in an attempt to}} stop its progression towards the macula. Such treatments are not only ineffective but unnecessarily risk complications. There is no documented case in the literature of degenerative retinoschisis itself (as opposed to the occasional situation of retinal detachment complicating retinoschisis) in which the splitting of the retina has progressed through the fovea. There is no clinical utility in differentiating between typical and reticular retinoschisis. Degenerative retinoschisis is not known to be a genetically inherited condition.There is always vision loss in the region of the schisis as the sensory retina is separated from the ganglion layer. But like the loss is in the periphery, it goes unnoticed. It is the very rare schisis that encroaches on the macula where retinopexy is then properly used.|$|E
40|$|The study aim was {{to compare}} expert with {{non-expert}} swimmers' rating of the aesthetic and technical qualities of front crawl in video-taped recordings of swimmers with low, middle, and high level proficiency. The results suggest that: i) observers' experience affects their judgment: only the expert observers correctly rated the swimmers' proficiency level; ii) evaluation of movement (technical and aesthetic scores) is correlated {{with the level of}} skill as expressed in the kinematics of the observed action (swimming speed, stroke frequency, and stroke length); iii) expert and <b>non-expert</b> <b>observers</b> use different strategies to rate the aesthetic and technical qualities of movement: equating the technical skill with the aesthetic quality is a general rule <b>non-expert</b> <b>observers</b> follow in the evaluation of human movement...|$|R
40|$|No-take marine {{reserves}} (NTMRs) {{are being}} implemented for various conservation and fishery management objectives. Well-designed ecological monitoring programs should accompany {{the implementation of}} NTMRs to assess their performance against stated objectives. However, monitoring programs often have limited funding and limited access to experienced ("expert") underwater visual census (UVC) observers. Consequently, use of <b>non-expert,</b> volunteer <b>observers</b> to collect marine ecological data has increased. The need to assess the quality and utility of non-expert data has been acknowledged. Few studies have directly compared expert and non-expert assessments, and these are generally one-point-in-time comparisons. Here we assess the magnitude of NTMR:Non-reserve density ratios for target and non-target reef fish collected in five separate years over a decade (2000 – 2009) by expert and <b>non-expert</b> <b>observers</b> in the Palm Islands, Great Barrier Reef, Australia. Despite variations in methods and resultant differences in absolute fish density estimates, NTMR:Non-reserve density ratio estimates made by experts and non-experts usually differed by 10 % or less. Furthermore, estimates of the statistical significance of reserve effects made by experts and non-experts had at least 80 % agreement over the decade. The findings {{support the notion that}} UVC data collected by trained <b>non-expert</b> <b>observers</b> can provide reliable information on the magnitude of NTMR effects...|$|R
30|$|The testing {{protocol}} {{is described}} as follows. We asked for 15 non-expert volunteers {{to participate in the}} experiments. All of these subjects neither were part of our team nor are related to texture synthesis work. This is important to mention since <b>non-expert</b> <b>observers</b> yield more critical evaluations about the synthesis quality. We placed the input texture video and the corresponding output on a screen side by side and asked our subjects to rate the quality of the video.|$|R
40|$|Introduction: Thrombus density {{may be a}} {{predictor}} for acute ischemic stroke treatment success. However, only limited data on observer variability for thrombus density measurements exist. This study assesses the variability and bias of four common thrombus density measurement methods by expert and <b>non-expert</b> <b>observers.</b> Methods: For 132 consecutive patients with acute ischemic stroke, three experts and two trained observers determined thrombus density by placing three standardized regions of interest (ROIs) in the thrombus and corresponding contralateral arterial segment. Subsequently, absolute and relative thrombus densities were determined using either one or three ROIs. Intraclass correlation coefficient (ICC) was determined, and Bland–Altman analysis was performed to evaluate interobserver and intermethod agreement. Accuracy of the trained observer was evaluated with a reference expert observer using the same statistical analysis. Results: The highest interobserver agreement was obtained for absolute thrombus measurements using three ROIs (ICCs ranging from 0. 54 to 0. 91). In general, interobserver agreement was lower for relative measurements, and for using one instead of three ROIs. Interobserver agreement of trained non-experts and experts was similar. Accuracy of the trained observer measurements was comparable to the expert interobserver agreement and was better for absolute measurements and with three ROIs. The agreement between the one ROI and three ROI methods was good. Conclusion: Absolute thrombus density measurement has superior interobserver agreement compared to relative density measurement. Interobserver variation is smaller when multiple ROIs are used. Trained <b>non-expert</b> <b>observers</b> can accurately and reproducibly assess absolute thrombus densities using three ROIs...|$|R
30|$|The first {{scenario}} {{illustrates a}} crowd sourced approach to forming shared situation awareness in which groups {{of individuals with}} increasing expertise take on the Joint Director of Labs (JDL) fusion levels[29]. Imagine {{a situation in which}} American coalition forces cannot embed intelligence agents into a village. However, a number of locals have cooperated with the American forces and agree to provide their observations. Each individual “observer” may present an incomplete view of the event based on their limited perspective. Many observers do not have the depth of knowledge to add meaning to the observations. However MCPS tools can integrate the <b>non-expert</b> <b>observers</b> with experts possessing sufficient knowledge to make relevant inferences.|$|R
40|$|International audienceThe vergence-accommodation conflict, {{excessive}} screen disparity, binocular distortions and {{the motion}} component in stereoscopic videos are considered as main {{factors that may}} induce visual discomfort. In our previous study {{which was based on}} the experts-only experiment, we also found that the large relative disparity between the foreground and background and the fast planar motion were more likely to induce visual discomfort. In this study, we conducted the same subjective experiment but on <b>non-expert</b> <b>observers.</b> The subjective experiment results coincided with our previous findings. The two objective visual discomfort models developed in our previous study have been evaluated and showed high correlation with subjective data. Finally, we found that the observers could be classified into different clusters according to their visual discomfort sensitivity to the velocity or the relative disparity. For some observers, the velocity is the predominant factor that may induce visual discomfort; some consider that the relative disparity is the key factor, and some are sensitive to both the velocity and relative disparity...|$|R
40|$|Replacement of {{traditional}} analog TV broadcasting services with various video services based on new advanced digital technologies not always provide {{high quality of}} video image if evaluated by viewer. This paper presents an analysis of impact of the video content and technical specifications on the subjective assessment of video picture quality made by <b>non-expert</b> <b>observers.</b> The measurements are performed in experimental setup with 25 observers, using 11 video scenes with wide range of technical specification, various analog and digital interfaces. The results show that high definition analog and digital video interfaces are evaluated by observers as providing nearly the same quality, but temporal activity in a video scene {{had a significant effect}} on the subjective quality assessment score. For a very vibrant video scenes with a high level values of temporal and spatial activity there is no significant difference in subjective quality assessment score either using standard definition or high definition, analog or digital interface. Ill. 3, bibl. 20, tabl. ...|$|R
40|$|Nine {{algorithms}} {{were implemented}} {{to overcome the}} problem associated with rendering high-dynamic-range scientific imagery to low-dynamic-range display devices. The algorithms were evaluated using two paired-comparison psychophysical experiments judging preference and ”sci-entific usefulness”. The results showed that, on average, the Zone System algorithm performed best and the Local Color Correction method performed the worst. However, {{the performance of the}} algorithms depended on the type of data being visualized. The low correlation between the preference and scientific usefulness judgments (R 2 = 0. 31) indicated that observers used different criteria when judging the image preference versus scientific usefulness. The experiment was re-peated using expert observers (radiologists) for an MR scan (Medical image). The results showed that the radiologists used similar criteria as the <b>non-expert</b> <b>observers</b> when judging the usefulness of the rendered images. A target detection experiment was conducted to measure the detectabil-ity of an embedded target in the Medical image. The result of the target detection experiment illustrated that the detectability of targets in the image is greatly influenced by the rendering algorithms due to the inherent difference in tone mapping among the algorithms...|$|R
40|$|For future 3 D TV {{systems with}} multi-viewpoint (look around) capability, {{it is not}} known how many {{spatially}} adjacent images of the same scene ought to be reproduced. For an empirical test, an experimental system was developed which permitted a variation of the number of images covering a fixed range of viewing positions. Within the viewing zones of discrete stereopairs, there was constant luminance and no crosstalk between adjacent images. Twelve <b>non-expert</b> <b>observers</b> assessed the quality of the 3 D reproduction as well as typical interferences with a standardized rating procedure. The essential performance feature {{turned out to be the}} maximum magnitude of the parallax shifts, which occur at the transition points between adjacent image zones (so-called flipping). It is concluded that flipping must not exceed 1. 15 min of arc (0. 34 mrad) for a system to be rated good on the CCIR quality scale. Hence, the requirements for a 3 D reproduction covering the full depth range utilized in 3 D cinematography amount to about 60 views per interocular distance...|$|R
40|$|The vergence-accommodation conflict, {{excessive}} screen disparity, binocular distortions and {{the motion}} component in stereoscopic videos are considered as main {{factors that may}} induce visual discomfort. In our previous study {{which was based on}} the experts-only experiment, we also found that the large relative disparity between the foreground and background and the fast planar motion were more likely to induce visual discomfort. In this study, we conducted the same subjective experiment but on <b>non-expert</b> <b>observers.</b> The subjective experiment results coincided with our previous findings. The two objective visual discomfort models developed in our previous study have been evaluated and showed high correlation with subjective data. Finally, we found that the observers could be classified into different clusters according to their visual discomfort sensitivity to the velocity or the relative disparity. For some observers, the velocity is the predominant factor that may induce visual discomfort; some consider that the relative disparity is the key factor, and some are sensitive to both the velocity and relative disparity. Index Terms — Stereoscopic videos, velocity, relative disparity, visual discomfort, observer classificatio...|$|R
40|$|International audienceThis paper {{introduces}} a homogeneity assessment method for the printed versions of uniform color images. This parameter has been specifically selected {{as one of}} the relevant attributes of printing quality. The method relies on image processing algorithms from a scanned image of the printed surface, especially the computation of gray level cooccurrence matrices and of objective homogeneity attribute inspired of Haralick's parameters. The viewing distance is also taken into account when computing the homogeneity index. Resizing and filtering of the scanned image are performed {{in order to keep the}} level of details visible by a standard human observer at short and long distances. The combination of the obtained homogeneity scores on both high and low resolution images provides a homogeneity index, which can be computed for any printed version of a uniform digital image. We tested the method on several hardcopies of a same image, and compared the scores to the empirical evaluations carried out by <b>non-expert</b> <b>observers</b> who were asked to sort the samples and to place them on a metric scale. Our experiments show a good matching between the sorting by the observers and the score computed by our algorith...|$|R
40|$|The aim of {{this study}} was to compare experts to naïve {{practitioners}} in rating the beauty and the technical quality of a Tai Chi sequence observed in video-clips (of high and middle level performances). Our hypothesis are: i) movement evaluation will correlate with the level of skill expressed in the kinematics of the observed action but ii) only experts will be able to unravel the technical component from the aesthetic component of the observed action. The judgments delivered indicate that both expert and <b>non-expert</b> <b>observers</b> are able to discern a good from a mediocre performance; however, as expected, only experts discriminate the technical from the aesthetic component of the action evaluated and do this independently of the level of skill shown by the model (high or middle level performances). Furthermore, the judgments delivered were strongly related to the kinematic variables measured in the observed model, indicating that observers rely on specific movement kinematics (e. g. movement amplitude, jerk and duration) for action evaluation. These results provide evidence of the complementary functional role of visual and motor action representation in movement evaluation and underline the role of expertise in judging the aesthetic quality of movements...|$|R
40|$|Compression {{schemes for}} 3 D images and -video gain {{much of their}} {{efficiency}} from transformations that convert the signal into forms suitable for quantization. The achieved compression efficiency is principally determined by rate-distortion analysis using objective quality evaluation metrics. In 2 D, quality evaluation metrics operating in the pixel domain implicitly assumes an ideal display modelled as a unity transformation. Similar simplifications are not feasible in 3 D analysis and different coding schemes introduce significantly different compression artefacts even though operating at the same rate-distortion ratio.   In this paper we have performed a subjective assessment {{of the quality of}} compressed 3 D images presented on an autostereoscopic display. In the qualitative part of the assessment different properties of the induced coding artefacts was identified with respect to image depth, pixelation, and zero-parallax distortion. The quantitative part was conducted using a group of <b>non-expert</b> <b>observers</b> that assessed the 3 D quality. In the results we show how the compression schemes introduce specific groups of artefacts manifesting with significantly different characteristics. In addition, each characteristic is derived from the transformation domains and the relationships between coding scheme and distortion property are presented. Moreover, the characteristics are related to the image quality assessment produced by the observation group...|$|R
40|$|Existing {{objective}} video {{quality metrics}} such as VQM from NTIA [1] and MOVIE [2] {{are known to}} perform well for assessing compression degradation in natural scene and broadcast television sequences but their suitability for the quality evaluation of compressed medical video has not been studied extensively. In this work we assess the quality of compressed medical video sequences using objective metrics and a subjective evaluation study conducted with non-expert subjects. Test sequences consist of High Definition medical video of laparascopic surgery. Four compression types (Motion JPG and three variants of H. 264) at four bit-rates (5, 12, 20, and 45 Mbps) are studied and compared to original uncompressed sequences. One reduced reference metric (VQM) and one full-reference metric (MOVIE) are studied. Subjective video evaluation consists of overall quality scores as well as difference scores between compressed and uncompressed sequences for similarity and five types of artifacts or attributes: blurring, blocking, noise, color fidelity, and motion artifacts. The results of the subjective and objective evaluations exhibit similar trends across the compression types and bit-rates, and may indicate that these objective quality metrics may be valid reflections of subjective quality judgments made by <b>non-expert</b> <b>observers</b> on compressed medical video sequences. In future work we will expand the subjective quality evaluation to include expert laparoscopic surgeons as subjects...|$|R
40|$|Research {{examining}} {{detection of}} verbal deception reveals that lay observers generally perform at chance. Yet, {{in the criminal}} justice system, laypersons that have not undergone specialist investigative training are frequently called upon to make veracity judgements (e. g., solicitors; magistrates; juries). We sought to improve performance by manipulating the timing of information revelation during investigative interviews. A total of 151 participants played an interactive computer game as either a truth-teller or a deceiver, and were interviewed afterwards. Game information known to the interviewer was revealed either early, {{at the end of the}} interview, or gradually throughout. Subsequently, 30 laypersons individually viewed a random selection of interviews (five deceivers and five truth-tellers from each condition), and made veracity and confidence judgements. Veracity judgements were most accurate in the gradual condition, p <. 001, η 2 =. 97 (above chance), and observers were more confident in those judgements, p <. 001, η 2 =. 99. Deceptive interviewees reported the gradual interviews to be the most cognitively demanding, p <. 001; η 2 =. 24. Our findings suggest that the detection of verbal deception by <b>non-expert</b> <b>observers</b> can be enhanced by employing interview techniques that maximize deceivers' cognitive load, while allowing truth-tellers the opportunity to respond to evidence incrementally...|$|R
40|$|What {{makes one}} artistrsquo;s style so {{different}} from anotherrsquo;s? How do we perceive these differences? Studying the perception of artistic style has proven difficult. Observers typically view several artworks and must group them or rate similarities between pairs. Responses are often driven by semantic variables, such as scene type or the presence/absence of particular subject matter, which leaves little room for studying how viewers distinguish a Degas ballerina from a Toulouse-Lautrec ballerina, for example. In the current paper, we introduce a new psychophysical paradigm for studying artistic style that focuses on visual qualities and avoids semantic categorization issues by presenting only very local views of a piece, thereby precluding object recognition. The task recasts stylistic judgment in a psychophysical texture discrimination framework, where visual judgments can be rigorously measured for trained and untrained observers alike. Stimuli were a dataset of drawings by Pieter Bruegel the Elder and his imitators studied by the computer science community, which showed that statistical analyses of the drawingsrsquo; local content can distinguish an authentic Bruegel from an imitation. Our <b>non-expert</b> <b>observers</b> also successfully discriminated the authentic and inauthentic drawings and furthermore discriminated stylistic variations within the categories, demonstrating the new paradigmrsquo;s feasibility for studying artistic style perception. At the same time, however, we discovered several issues in the Bruegel dataset that bear on conclusions drawn by the computer vision studies of artistic style...|$|R
40|$|There is {{overwhelming}} evidence that wind turbines cause {{serious health problems}} in nearby residents, usually stress-disorder type diseases, at a nontrivial rate. The bulk of the evidence {{takes the form of}} thousands of adverse event reports. There is also a small amount of systematically-gathered data. The adverse event reports provide compelling evidence of the seriousness of the problems and of causation in this case because of their volume, the ease of observing exposure and outcome incidence, and case-crossover data. Proponents of turbines have sought to deny these problems by making a collection of contradictory claims including that the evidence does not "count", the outcomes are not "real " diseases, the outcomes are the victims' own fault, and that acoustical models cannot explain why there are health problems so the problems must not exist. These claims appeared to have swayed many <b>non-expert</b> <b>observers,</b> though they are easily debunked. Moreover, though the failure of models to explain the observed problems does not deny the problems, it does mean that we do not know what, other than kilometers of distance, could sufficiently mitigate the effects. There has been no policy analysis that justifies imposing these effects on local residents. The attempts to deny the evidence cannot be seen as honest scientific disagreement, and represent either gross incompetence or intentional bias...|$|R
40|$|We {{investigated}} {{the changes to}} calcareous grassland plots within protected sites, and whether Tephroseris integrifolia subsp. integrifolia {{can act as a}} useful indicator species for re-visitation studies within vegetation predicted to remain relatively stable. Twenty-two plots located across lowland England and all formerly containing T. integrifolia were re-surveyed following the methodology used in the original survey undertaken in the 1960 s. Pseudo-turnover and between-observer bias were minimised by sampling replicate quadrats at each fixed plot using a single surveyor and at a similar time of year as the original survey. Qualitative details concerning grazing management were obtained for all sites. In contrast to other long-term re-visitation studies, all our study plots were intact and retained diverse, herb-rich vegetation, demonstrating the value of site protection. However, there were clear shifts in vegetation composition, most notably where T. integrifolia was absent, as shown by an increase in Ellenberg fertility and moisture signifying nutrient enrichment, and a decrease in the cover of low-growing, light-demanding specialists, with a change likely to be associated predominantly with grazing management. Whereas in the mid- 20 th century the greatest threat to calcareous grassland was habitat loss, undergrazing or temporary neglect now appears to pose the principal threat. Distinctive species such as T. integrifolia with marked sensitivity to habitat change provide a potentially useful tool for rapid assessment and monitoring of site quality. Focusing monitoring on such species allows <b>non-expert</b> <b>observers</b> to recognise the early stages of habitat degradation, providing, in effect, a “health check” on individual sites and groups of sites...|$|R
40|$|This {{study is}} a {{preliminary}} investigation towards the design of effective colour spaces and colour tools to allow users to quickly and accurately select a given colour in a digital-display environment. It {{has been shown that}} many non-expert users find the RGB colour space to be non-intuitive. The choice of colour space on various visual tasks has also been shown to be an important factor and that experts show greater precision in colour-matching experiments than <b>non-expert</b> <b>observers.</b> We propose that non-experts find manipulation and selection in an RGB colour space to be difficult because they do not possess an appropriate internal model for additive colour mixing. On the other hand, observers from a young age may develop a useful internal model of subtractive colour mixing processes as they experiment with inks and paints. The purpose of this work is to determine whether it is indeed the case that observers possess more useful internal models for subtractive colour mixing than for additive colour mixing. The work reported in this study describes only an assessment of subtractive colour mixing. Three experiments are described whereby expert and naïve observers select matches from a library of colours for individual samples or for imagined subtractive mixtures of paint samples. Qualitative and quantitative analyses are presented to measure the ability of observers to make predictions of subtractive mixing processes. When mixing the subtractive primaries (e. g. cyan with magenta) the performance of expert and naïve observers are the same but in general the expert performance far exceeds that of the naïve observer for other colours...|$|R
40|$|International audienceEvolution and {{geometry}} generate {{complexity in}} similar ways. Evolution drives natural 6 selection while geometry may capture {{the logic of}} this selection and express it visually, in terms of 7 specific generic properties representing some kind of advantage. Geometry is ideally suited for 8 expressing the logic of evolutionary selection for symmetry, which {{is found in the}} shape curves of 9 vein systems and other natural objects such as leaves, cell membranes, or tunnel systems built by 10 ants. The topology and geometry of symmetry is controlled by numerical parameters, which act in 11 analogy with a biological organism's DNA. The introductory part of this paper reviews findings 12 from experiments illustrating the critical role of two-dimensional design parameters and shape 13 symmetry for visual or tactile shape sensation, and for perception-based decision making in 14 populations of experts and non-experts. Thereafter, results from a pilot study on the effects of 15 fractal symmetry, referred to herein as the symmetry of things in a thing, on aesthetic judgments and 16 visual preference are presented. In a first experiment (psychophysical scaling procedure), 17 <b>non-expert</b> <b>observers</b> had to rate (scale from 0 to 10) the perceived beauty of a random series of 2 D 18 fractal trees with varying degrees of fractal symmetry. In a second experiment (two-alternative 19 forced choice procedure), they had to express their preference for one of two shapes from the series. 20 The shape pairs were presented successively in random order. Results show that the smallest 21 possible fractal deviation from "symmetry of things in a thing" significantly reduces the perceived 22 attractiveness of such shapes. The potential of future studies where different levels of complexity of 23 fractal patterns are weighed against different degrees of symmetry is pointed out in the conclusion. 2...|$|R
40|$|Evolution and {{geometry}} generate {{complexity in}} similar ways. Evolution drives natural selection while geometry may capture {{the logic of}} this selection and express it visually, in terms of specific generic properties representing some kind of advantage. Geometry is ideally suited for expressing the logic of evolutionary selection for symmetry, which {{is found in the}} shape curves of vein systems and other natural objects such as leaves, cell membranes, or tunnel systems built by ants. The topology and geometry of symmetry is controlled by numerical parameters, which act in analogy with a biological organism’s DNA. The introductory part of this paper reviews findings from experiments illustrating the critical role of two-dimensional (2 D) design parameters, affine geometry and shape symmetry for visual or tactile shape sensation and perception-based decision making in populations of experts and non-experts. It will be shown that 2 D fractal symmetry, referred to herein as the “symmetry of things in a thing”, results from principles very similar to those of affine projection. Results from experiments on aesthetic and visual preference judgments in response to 2 D fractal trees with varying degrees of asymmetry are presented. In a first experiment (psychophysical scaling procedure), <b>non-expert</b> <b>observers</b> had to rate (on a scale from 0 to 10) the perceived beauty of a random series of 2 D fractal trees with varying degrees of fractal symmetry. In a second experiment (two-alternative forced choice procedure), they had to express their preference for one of two shapes from the series. The shape pairs were presented successively in random order. Results show that the smallest possible fractal deviation from “symmetry of things in a thing” significantly reduces the perceived attractiveness of such shapes. The potential of future studies where different levels of complexity of fractal patterns are weighed against different degrees of symmetry is pointed out in the conclusion...|$|R
40|$|Even <b>non-expert</b> human <b>observers</b> {{sometimes}} still outperform automatic {{extraction of}} man-made objects from remotely sensed data. We conjecture {{that some of}} this remarkable capability {{can be explained by}} Gestalt mechanisms. Gestalt algebra gives a mathematical structure capturing such part-aggregate relations and the laws to form an aggregate called Gestalt. Primitive Gestalten are obtained from an input image and the space of all possible Gestalt algebra terms is searched for well-assessed instances. This can be a very challenging combinatorial effort. The contribution at hand gives some tools and structures unfolding a finite and comparably small subset of the possible combinations. Yet, the intended Gestalten still are contained and found with high probability and moderate efforts. Experiments are made with images obtained from a virtual globe system, and use the SIFT method for extraction of the primitive Gestalten. Comparison is made with manually extracted ground-truth Gestalten salient to human observers...|$|R
40|$|No study to-date {{explored}} {{the relationship between}} perceived image quality (IQ) and perceived depth (DP) in stereoscopic medical images. However, this is crucial to design objective quality metrics suitable for stereoscopic medical images. This study examined this relationship using volume-rendered stereoscopic medical images for both dual-and single-view distortions. The reference image was modified to simulate common alterations occurring during the image acquisition stage or at the display side: added white Gaussian noise, Gaussian filtering, changes in luminance, brightness and contrast. We followed a double stimulus five-point quality scale methodology to conduct subjective tests with eight <b>non-expert</b> human <b>observers.</b> The results suggested that DP was very robust to luminance, contrast and brightness alterations and insensitive to noise distortions until standard deviation sigma= 20 and crosstalk rates of 7 %. In contrast, IQ seemed sensitive to all distortions. Finally, for both DP and IQ, the Friedman test indicated that the quality scores for dual-view distortions were significantly worse than scores for single-view distortions for multiple blur levels and crosstalk impairments. No differences were found for most levels of brightness, contrast and noise distortions. So, DP and IQ didn't react equivalently to identical impairments, and both depended whether dual-or single-view distortions were applied...|$|R
40|$|Digital breast tomosynthesis is {{a recent}} three {{dimensional}} imaging modality that allows visualization of the breast as a stack of parallel slices. When compared to projection mammography, tomosynthesis is preferred for visualizing mass lesions while mammography is preferred for microcalcifications. In clinical evaluations, the diagnostic accuracy of tomosynthesis {{is at least as}} good as that of mammography, and both modalities combined outperform mammography used alone. Technical evaluations show that iterative reconstruction methods perform better than filtered backprojection reconstruction, which was used in most of the clinical evaluations. Therefore, the goal of this work was to design and evaluate a maximum a posteriori reconstruction algorithm for digital breast tomosynthesis with a focus on the visualization of microcalcifications. The first step was to implement a sequence of preprocessing steps to account for the typical assumptions of mono-energetic and scatter-free data acquisition used in iterative reconstruction. With this precorrection, reconstructed attenuation values of adipose breast tissue was found to be close to the expected theoretical value. A further examination of the difference between scatter precorrection and model based scatter correction during reconstruction was performed by evaluating the contrast to noise ratio of simulated masses in patient data. Results showed that the application of either correction method resulted in a similar contrast to noise ratio, which meant precorrection was preferred due to the lower computational cost. The second part of the work concentrates on developing a maximum a posteriori reconstruction algorithm for breast tomosynthesis. To improve visualization of microcalcifications, a resolution model based on the motion of the x-ray source during image acquisition was combined with a grouped coordinate ascent algorithm that sequentially updated planes parallel to the detector, each with their own position dependent parameters for the resolution model. This new method was evaluated in reconstructions of a simulated power law background containing microcalcifications and resulted in higher contrast to noise ratio when compared to iterative reconstruction without resolution model and improved detectability in a free search observer experiment when compared to filtered backprojection. One drawback of the plane-by-plane updates in this method was the need for careful initialization of the reconstruction volume in order to avoid severe limited angle artifacts. To remedy this problem and to further accelerate convergence, multigrid updates were implemented, and an update scheme was selected that combined the least amount of artifacts and the best convergence after a fixed computational cost. A further comparison was made with a popular alternative update method using ordered subsets rather than plane-by-plane updates, and found that when using an optimal multigrid sequence, both update methods resulted in similar performance. The final parts of this work focused on the evaluation of reconstruction methods. A channelized Hotelling observer was designed to detect groups of five microcalcifications in a background of acrylic spheres, and was applied to optimize the detectability of these microcalcifications {{as a function of the}} smoothing prior. The model observer correlated well with human observer evaluations of the same data, and found that detectability only varied slightly over a large range of strengths for the quadratic and combined quadratic and total variation priors. Therefore, it was not possible to pick an optimal smoothness based only on this criterion. On the other hand, with this information, the smoothing in the reconstruction could be set according to radiologist preference without worrying about calcification detectability. This model observer was then applied together with evaluations by a group of expert and <b>non-expert</b> human <b>observers</b> to compare three reconstruction algorithms for breast tomosynthesis. These were the iterative reconstruction developed in this work, the existing filtered backprojection of the Siemens Mammomat Inspiration system, and a new super-resolution filtered backprojection with post-reconstruction denoising. The evaluation consisted of a four-alternative forced-choice experiment to determine microcalcification and mass detectability in phantom data, and a visual grading study on patient data. Both new reconstruction methods showed improved performance on the lesion detection task compared to the system filtered backprojection, but resulted in significantly different overall appreciation of image quality in the visual grading study. From these results and the feedback from the radiologists that participated in the study, we can conclude that the new super-resolution filtered backprojection can replace the original system reconstruction in the clinic. Although the new iterative reconstruction improved the detectability of microcalcifications significantly, the unfamiliar properties of the images were not received as positive by the radiologists. Therefore further development of the iterative reconstruction should focus on artifact reduction and improving image contrast, and should use frequent clinical input in order to obtain a more familiar look and feel (noise pattern, contrast, …) for the radiologists. status: publishe...|$|R

