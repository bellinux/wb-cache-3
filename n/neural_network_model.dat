4251|10000|Public
25|$|A {{sufficiently}} {{complex and}} accurate {{model of the}} neurons is required. A traditional artificial <b>neural</b> <b>network</b> <b>model,</b> for example multi-layer perceptron network model, is not considered as sufficient. A dynamic spiking <b>neural</b> <b>network</b> <b>model</b> is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It {{is likely that the}} model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.|$|E
25|$|Training a <b>neural</b> <b>network</b> <b>model</b> {{essentially}} means selecting {{one model}} {{from the set}} of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models. Most {{of them can be}} viewed as a straightforward application of optimization theory and statistical estimation.|$|E
25|$|Newer {{functional}} neuroimaging techniques include functional {{magnetic resonance}} imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic <b>neural</b> <b>network</b> <b>model</b> rather than a localized function model.|$|E
40|$|In {{this paper}} the {{exchange}} rate forecasting performance of <b>neural</b> <b>network</b> <b>models</b> are evaluated against random walk {{and a range of}} time series models. There are no guidelines available {{that can be used to}} choose the parameters of <b>neural</b> <b>network</b> <b>models</b> and therefore the parameters are chosen according to what the researcher considers to be the best. Such an approach, however, implies that the risk of making bad decisions is extremely high which could explain why in many studies <b>neural</b> <b>network</b> <b>models</b> do not consistently perform better than their time series counterparts. In this paper through extensive experimentation the level of subjectivity in building <b>neural</b> <b>network</b> <b>models</b> is considerably reduced and therefore giving them a better chance of performing well. Our results show that in general <b>neural</b> <b>network</b> <b>models</b> perform better than traditionally used time series models in forecasting exchange rates...|$|R
40|$|Forecast {{performance}} of artificial <b>neural</b> <b>network</b> <b>models</b> are investigated using Ashley et al. (1980) and the <b>neural</b> <b>network</b> nonlinearity test proposed by Lee et al. (1993) is employed to find possible existence of business cycle asymmetries in Canada, France, Japan, UK and USA real GDP growth rates. The {{results show that}} <b>neural</b> <b>network</b> <b>models</b> are more accurate than linear models for in-sample forecasts. However, when comparing the out-of-sample, linear models performed better than <b>neural</b> <b>network</b> <b>models</b> in all series. Results from <b>neural</b> <b>network</b> tests show that business cycle asymmetries do prevail in all the series. ...|$|R
40|$|An out-of-sample {{prediction}} of Kansas farmers' responses to five surveyed questions involving risk {{is used to}} compare ordered multinomial logistic regression models with feedforward backpropagation <b>neural</b> <b>network</b> <b>models.</b> Although the logistic models often predict more accurately than the <b>neural</b> <b>network</b> <b>models</b> in a mean-squared error sense, the <b>neural</b> <b>network</b> <b>models</b> are shown to be more accommodating of loss functions associated {{with a desire to}} predict certain combinations of categorical responses more accurately than others. Copyright 1996, Oxford University Press. ...|$|R
25|$|As {{an attempt}} to correct some of the error due to a non-zero , the usage of local linear {{weighted}} regression with ABC to reduce the variance of the posterior estimates has been suggested. The method assigns weights to the parameters according to how well simulated summaries adhere to the observed ones and performs linear regression between the summaries and the weighted parameters {{in the vicinity of}} observed summaries. The obtained regression coefficients are used to correct sampled parameters in the direction of observed summaries. An improvement was suggested in the form of nonlinear regression using a feed-forward <b>neural</b> <b>network</b> <b>model.</b> However, {{it has been shown that}} the posterior distributions obtained with these approaches are not always consistent with the prior distribution, which did lead to a reformulation of the regression adjustment that respects the prior distribution.|$|E
25|$|Gelenbe {{has contributed}} {{pioneering}} research concerning {{the performance of}} multiprogramming computer systems, virtual memory management, data base reliability optimisation, distributed systems and network protocols. He formed, led, and trained the team that designed the commercial QNAP Computer and Network Performance Modeling Tool. He introduced the Flexsim Object Oriented approach for the simulation in manufacturing systems. He carried {{out some of the}} first work on adaptive control of computer systems, and published seminal papers on the performance optimisation of computer network protocols and on the use of diffusion approximations for network performance. He developed new product form queueing networks with negative customers and triggers known as G-networks. He also introduced a new spiked stochastic <b>neural</b> <b>network</b> <b>model</b> known as the random neural network, developed its mathematical solution and learning algorithms, and applied it to both engineering and biological problems. His inventions include the design of the first random access fibre-optics local area network, a patented admission control technique for ATM networks, a neural network based anomaly detector for brain magnetic resonance scans, and the cognitive packet network routing protocol to offer quality of service to users.|$|E
25|$|In 2009, {{a simple}} {{electronic}} circuit consisting of an LC network and a memristor {{was used to}} model experiments on adaptive behavior of unicellular organisms. It was shown that subjected to a train of periodic pulses, the circuit learns and anticipates the next pulse similar to the behavior of slime molds Physarum polycephalum where the viscosity of channels in the cytoplasm responds to periodic environment changes. Applications of such circuits may include, e.g., pattern recognition. The DARPA SyNAPSE project funded HP Labs, {{in collaboration with the}} Boston University Neuromorphics Lab, has been developing neuromorphic architectures which may be based on memristive systems. In 2010, Versace and Chandler described the MoNETA (Modular Neural Exploring Traveling Agent) model. MoNETA is the first large-scale <b>neural</b> <b>network</b> <b>model</b> to implement whole-brain circuits to power a virtual and robotic agent using memristive hardware. Application of the memristor crossbar structure in the construction of an analog soft computing system was demonstrated by Merrikh-Bayat and Shouraki. In 2011 they showed how memristor crossbars can be combined with fuzzy logic to create an analog memristive neuro-fuzzy computing system with fuzzy input and output terminals. Learning is based on the creation of fuzzy relations inspired from Hebbian learning rule.|$|E
40|$|This paper {{examines}} the forecasting performance of ARIMA and artificial <b>neural</b> <b>networks</b> <b>model</b> with published stock {{data obtained from}} New York Stock Exchange. The empirical results obtained reveal the superiority of <b>neural</b> <b>networks</b> <b>model</b> over ARIMA model. The findings further resolve and clarify contradictory opinions reported in literature over the superiority of <b>neural</b> <b>networks</b> and ARIMA <b>model</b> and vice versa...|$|R
50|$|Quantum <b>neural</b> <b>networks</b> (QNNs) are <b>neural</b> <b>network</b> <b>models</b> {{which are}} based on the {{principles}} of quantum mechanics. There are two different approaches to QNN research, one exploiting quantum information processing to improve existing <b>neural</b> <b>network</b> <b>models</b> (sometimes also vice versa), and the other one searching for potential quantum effects in the brain.|$|R
40|$|In this paper, the {{exchange}} rate forecasting performance of <b>neural</b> <b>network</b> <b>models</b> are evaluated against the random walk, autoregressive moving average and generalised autoregressive conditional heteroskedasticity models. There are no guidelines available {{that can be used}} to choose the parameters of <b>neural</b> <b>network</b> <b>models</b> and therefore, the parameters are chosen according to what the researcher considers to be the best. Such an approach, however, implies that the risk of making bad decisions is extremely high, which could explain why in many studies, <b>neural</b> <b>network</b> <b>models</b> do not consistently perform better than their time series counterparts. In this paper, through extensive experimentation, the level of subjectivity in building <b>neural</b> <b>network</b> <b>models</b> is considerably reduced and therefore giving them a better chance of performing well. The results show that in general, <b>neural</b> <b>network</b> <b>models</b> perform better than the traditionally used time series models in forecasting exchange rates. exchange rates; forecasting; linear models; nonlinear models; autoregressive integrated moving average; ARIMA models; neural networks; ANNs; generalised autoregressive conditional heteroskedasticity; GARCH models; random walk models. ...|$|R
2500|$|A <b>neural</b> <b>network</b> <b>model</b> {{describes}} {{a population of}} physically interconnected neurons {{or a group of}} disparate neurons whose inputs or signalling targets define a recognizable circuit. [...] These models aim to describe how the dynamics of neural circuitry arise from interactions between individual neurons. Local interactions between neurons can result in the synchronization of spiking activity and form the basis of oscillatory activity. In particular, models of interacting pyramidal cells and inhibitory interneurons have been shown to generate brain rhythms such as gamma activity.|$|E
5000|$|Anderson [...] {{shows that}} {{combination}} of Hebbian learning rule and McCullough-Pitts dynamical rule allow network {{to generate a}} weight matrix that can store associations between different memory patterns - such matrix is the form of memory storage for the <b>neural</b> <b>network</b> <b>model.</b> Major differences between the matrix of multiple traces hypothesis and the <b>neural</b> <b>network</b> <b>model</b> is that while new memory indicates extension of the existing matrix for the multiple traces hypothesis, weight matrix of the <b>neural</b> <b>network</b> <b>model</b> does not extend; rather, the weight {{is said to be}} updated with introduction of new association between neurons.|$|E
5000|$|Helmholtz machine, a <b>neural</b> <b>network</b> <b>model</b> {{trained by}} the wake-sleep algorithm.|$|E
40|$|AbstractIP 2 Cs (Ionic Polymer-Polymer Composites) are {{electro-active}} polymers {{which can}} be used both as sensors and as actuators. In this work <b>Neural</b> <b>Network</b> <b>models</b> of IP 2 Cs working as actuators and with water as the solvent are developed using experimental data. Moreover, models proposed here take into account the humidity dependence of the device as modifying input. Three different <b>Neural</b> <b>Network</b> <b>models,</b> i. e. Feed-forward <b>neural</b> <b>network,</b> Radial-basis <b>neural</b> <b>network</b> and recurrent <b>neural</b> <b>network</b> based <b>models</b> are developed and a comparison of proposed model is reported...|$|R
5000|$|Artificial <b>neural</b> <b>network</b> <b>modelling</b> for {{describing}} in situ MEOR processes.|$|R
5000|$|... <b>neural</b> <b>network</b> <b>models</b> of {{information}} processing {{in the brain}} structures; ...|$|R
5000|$|E. Gelenbe, Stability of {{the random}} <b>neural</b> <b>network</b> <b>model,</b> Neural Computation, vol. 2, no. 2, pp. 239-247, 1990.|$|E
5000|$|Shunting {{model is}} one of the Grossberg's <b>neural</b> <b>network</b> <b>model</b> based on Leaky integrator, given by the expression: ...|$|E
50|$|A {{sufficiently}} {{complex and}} accurate {{model of the}} neurons is required. A traditional artificial <b>neural</b> <b>network</b> <b>model,</b> for example multi-layer perceptron network model, is not considered as sufficient. A dynamic spiking <b>neural</b> <b>network</b> <b>model</b> is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It {{is likely that the}} model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.|$|E
5000|$|... #Subtitle level 3: Analyzing chicken {{performance}} data by <b>neural</b> <b>network</b> <b>models</b> ...|$|R
40|$|The {{objectives}} {{in this research}} is to investigate the interaction between air surface temperature with others climatologically parameters such as humidity, sunshine, rain and others in Cameron Highlands and to construct an appropriate <b>neural</b> <b>networks</b> <b>model</b> for air surface temperature. The other purpose is to provide operational prediction outlook of short and medium term using <b>neural</b> <b>networks</b> <b>model...</b>|$|R
50|$|In {{order for}} <b>neural</b> <b>network</b> <b>models</b> {{to be shared}} by {{different}} applications, a common language is necessary. The Predictive Model Markup Language (PMML) has been proposed to address this need. PMML is an XML-based language which provides a way for applications to define and share <b>neural</b> <b>network</b> <b>models</b> (and other data mining models) between PMML compliant applications.|$|R
50|$|The first {{ideas on}} neural {{computation}} {{have been published}} by Subhash Kak, who discusses the similarity of the neural activation function with the quantum mechanical Eigenvalue equation. Kak also discussed the application of these ideas {{to the study of}} brain function and the limitations of this approach. Ajit Narayanan and Tammy Menneer proposed a photonic implementation of a quantum <b>neural</b> <b>network</b> <b>model</b> that is based on the many-universe theory and “collapses” into the desired model upon measurement. Since then, more and more articles have been published in journals of computer science as well as quantum physics in order to find a superior quantum <b>neural</b> <b>network</b> <b>model.</b>|$|E
50|$|Verdigris {{forecasting}} for {{demand management}} uses a deep learning recurrent <b>neural</b> <b>network</b> <b>model.</b> It reads a building's energy usage in real time, and, combined with weather or building occupancy, produces a probability distribution for estimated power consumption (kW).|$|E
50|$|The novel <b>neural</b> <b>network</b> <b>model</b> {{has been}} {{presented}} {{the first time in}} 2015, by Luca Marchese, at WIRN - 25th Italian Workshop on Neural Networks - Societa Italiana Reti Neuroniche (SIREN) - International Institute for Advanced Scientific Studies (IIASS).|$|E
40|$|This study {{refers to}} the {{prediction}} of liquefaction potential of alluvial soil by artificial <b>neural</b> <b>network</b> <b>models.</b> To meet the objective 160 data sets from field and laboratory tests were collected for the development of ANN models. Initially these data sets were used to determine liquefaction parameters like cyclic resistance ratio and cyclic stress ratio by Idriss and Boulanger method to identify the liquefaction prone areas. Artificial <b>neural</b> <b>network</b> <b>models</b> were trained with six input vectors by optimum numbers of hidden layers, epoch and suitable transfer functions. Out of 160 data sets, 133 data sets were used for development of models and 27 datasets were used for validating the models. The predicted values of liquefaction potential by artificial <b>neural</b> <b>networks</b> <b>models</b> have been compared with Idriss and Boulanger method, which exhibits that trained artificial <b>neural</b> <b>networks</b> <b>models</b> are capable of predicting soils liquefaction potential adequately...|$|R
50|$|<b>Neural</b> <b>network</b> <b>models</b> {{and other}} {{sophisticated}} {{models have been}} tested on bankruptcy prediction.|$|R
5000|$|Rethinking Innateness applied {{insights}} from neurobiology and <b>neural</b> <b>network</b> <b>modelling</b> to brain development.|$|R
5000|$|A {{multilayer}} <b>neural</b> <b>network</b> <b>model</b> by Linsker, having local connections {{from each}} cell layer to the next, whose connection strengths develop {{according to a}} Hebbian rule, generates orientation-selective cells and orientation columns. The resulting columnar arrangement contains fractures and [...] "pinwheel" [...] singularities of the same types as those found experimentally.|$|E
5000|$|Cowell, R. A. and French, R. M. (2011). Noise and the Emergence of Rules in Category Learning: A Connectionist Model. IEEE Transactions on Autonomous Mental Development, 3(3), 194-206. This paper {{presents}} “a <b>neural</b> <b>network</b> <b>model</b> of category {{learning that}} addresses {{the question of how}} rules for category membership are acquired.” ...|$|E
5000|$|Neuromemristive {{systems are}} a {{subclass}} of neuromorphic computing systems {{that focus on}} the use of memristors to implement neuroplasticity. While neuromorphic engineering focuses on mimicking biological behavior, neuromemristive systems focus on abstraction. [...] For example, a neuromemristive system may replace the details of a cortical microcircuit's behavior with an abstract <b>neural</b> <b>network</b> <b>model.</b>|$|E
40|$|Artificial <b>neural</b> <b>networks</b> achieve fast {{parallel}} processing via massively parallel non-linear computational elements. Most <b>neural</b> <b>network</b> <b>models</b> base {{their ability to}} adapt to problems on changing {{the strength of the}} interconnections between computational elements according to a given learning algorithm. However, constrained interconnection structures may limit such ability. Field programmable hardware devices allow the implementation of <b>neural</b> <b>networks</b> with in-circuit structure adaptation. This paper describes an FPGA implementation of the FAST (Flexible Adaptable-Size Topology) architecture, a <b>neural</b> <b>network</b> that dynamically changes its size. Since initial experiments indicated a good performance on pattern clustering tasks, we have applied our dynamicstructure FAST <b>neural</b> <b>network</b> to an image segmentation and recognition problem. 1 Introduction Artificial <b>neural</b> <b>network</b> <b>models</b> offer an attractive paradigm: learning to solve problems from examples. Most <b>neural</b> <b>network</b> <b>models</b> base [...] ...|$|R
40|$|Abstract: This paper {{describes}} {{the development of}} <b>neural</b> <b>network</b> <b>models</b> for noise reduction. The networks used to enhance the performance of modeling captured signals by reducing the effect of noise. Both recurrent and multi-layer Backpropagation <b>neural</b> <b>networks</b> <b>models</b> are examined and compared with different training algorithms. The paper presented is to illustrate the effect of training algorithms and <b>network</b> architecture on <b>neural</b> <b>network</b> performance for a given application...|$|R
40|$|Although {{much has}} been written on this subject, there still seems to be {{considerable}} confusion in the literature concerning dissociations, double dissociations and what they really mean, especially when connectionist or <b>neural</b> <b>network</b> <b>models</b> are involved. In this paper I attempt to clarify matters by looking at the subject {{from the point of view}} of patterns of learning rates in <b>neural</b> <b>network</b> <b>models.</b> ...|$|R
