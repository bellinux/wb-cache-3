27|14|Public
500|$|On {{the release}} {{date of the}} Potato Sack bundle, players found the games within it had {{recently}} received updates. Most provided an immediate cosmetic change by replacing or adding assets that referred to potatoes. When players started looking deeper into these new assets, they discovered a series of glyphs that referred to other games associated with specific letters, as well as <b>nonsense</b> <b>sentences</b> that lead to specific cyphers. Other hints were less direct, using online services such as Twitter and YouTube to embed clues. [...] In the case of Toki Tori, sections of new levels included braille code that referred to latitude and longitude coordinates of Two Tribes' headquarters. One player, [...] "Jake_R", traveled to Two Tribes, where he discovered the glyphs and cyphers posted outside their headquarters. Several of Two Tribes' developers, upon learning of his presence, began filming him from a barbershop across the street. They would later use this footage of him climbing a pole to find these clues as part of another clue during the second phase.|$|E
50|$|The game of {{exquisite}} corpse is {{a method}} for generating <b>nonsense</b> <b>sentences.</b> It was named after the first sentence generated in the game in 1925 Le cadavre exquis boira le vin nouveau (the exquisite corpse will drink the new wine).|$|E
5000|$|These <b>nonsense</b> <b>sentences</b> {{were created}} in order to {{eliminate}} any non-prosodic interference (e.g phonological differences, different number of syllables, etc.) thus babies would {{only be able to}} differentiate between the two languages based on the prominence of prosodic cues in the sentences. The table above depicts the sentences heard by the French babies (translated as [...] "The large orangoutang was nervous"), where the bolded and enlarged letter indicates word stress and prominence (Christophe et al. 2003). As predicted, French babies tended to prefer the modified nonsense French phrases, based solely on prosodic prominence, given by the location of the head direction parameter.|$|E
5000|$|The German {{sentence}} in the figures reads: [...] " [...] of Old Schwabacher: [...] ". This is a <b>nonsense</b> <b>sentence</b> meaning [...] "Victor chases twelve boxers across the great dam of Sylt", but contains all 26 {{letters of the alphabet}} plus the German umlauts and is thus an example of a pangram.|$|R
5000|$|In {{the opening}} theme, one sound clip is changed {{in each episode}} which, when {{following}} the previous clip, makes up a <b>nonsense</b> <b>sentence.</b> Depending on the season, the preceding clip is either Franklin D. Roosevelt's [...] "The only {{thing we have to}} fear is..." [...] (from his famous inauguration speech)(seasons 1, 3 & 4), a man saying, [...] "I can't believe I ate that whole..." [...] (from a commercial for Alka-Seltzer)(season 2), or an announcer saying [...] "Tonight's episode brought to you by..." [...] (season 5).|$|R
40|$|We {{prove that}} an {{integral}} Cauchy-Riemann inequality holds for any pair of smooth functions (f,h) on the 2 -sphere S^ 2, and equality holds iff f and h are related λ_ 1 -eigenfunctions. We extend such inequality to 4 -tuples of functions, only valid on the L^ 2 -orthogonal complement of a suitable nonzero finite dimensional space of functions. As a consequence we prove that 2 -spheres are not Ω-stable surfaces with parallel mean curvature in R^ 7 for the associative calibration Ω. Comment: 24 pages. LaTex 2 e V 2 : we correct some minor misprints. Remove a <b>nonsense</b> <b>sentence</b> in corollary 1. 1, and correct a referenc...|$|R
50|$|The {{stump speech}} was a comic {{monologue}} from blackface minstrelsy (which is an American entertainment consisting of comic skits, variety acts, dancing, and music, performed by {{white people in}} blackface). A typical stump speech consisted of malapropisms (the substitution of a word for a word with a similar sound), <b>nonsense</b> <b>sentences,</b> and puns delivered in a parodied version of Black Vernacular English. The stump speaker wore blackface makeup and moved about like a clown. Topics varied from pure nonsense to parodies of politics, science, and social issues. Although both the topic itself and the black character's inability to comprehend it served as sources of comedy, minstrels used such speeches to deliver social commentary that might be considered taboo in another setting. The stump speech was an important precursor to modern stand-up comedy.|$|E
5000|$|As Bayesian {{filtering}} {{has become}} popular as a spam-filtering technique, spammers have started using methods to weaken it. To a rough approximation, Bayesian filters rely on word probabilities. If a message contains many {{words that are}} used only in spam, and few that are never used in spam, {{it is likely to}} be spam. To weaken Bayesian filters, some spammers, alongside the sales pitch, now include lines of irrelevant, random words, in a technique known as Bayesian poisoning. A variant on this tactic may be borrowed from the Usenet abuser known as [...] "Hipcrime"—to include passages from books taken from Project Gutenberg, or <b>nonsense</b> <b>sentences</b> generated with [...] "dissociated press" [...] algorithms. Randomly generated phrases can create spoetry (spam poetry) or spam art. The perceived credibility of spam messages by users differs across cultures; for example, Korean unsolicited email frequently uses apologies, likely to be based on Koreans’ modeling behavior and a greater tendency to follow social norms.|$|E
5000|$|On {{the release}} {{date of the}} Potato Sack bundle, players found the games within it had {{recently}} received updates. Most provided an immediate cosmetic change by replacing or adding assets that referred to potatoes. When players started looking deeper into these new assets, they discovered a series of glyphs that referred to other games associated with specific letters, as well as <b>nonsense</b> <b>sentences</b> that lead to specific cyphers. Other hints were less direct, using online services such as Twitter and YouTube to embed clues. In the case of Toki Tori, sections of new levels included braille code that referred to latitude and longitude coordinates of Two Tribes' headquarters. One player, [...] "Jake_R", traveled to Two Tribes, where he discovered the glyphs and cyphers posted outside their headquarters. Several of Two Tribes' developers, upon learning of his presence, began filming him from a barbershop across the street. They would later use this footage of him climbing a pole to find these clues as part of another clue during the second phase.|$|E
40|$|This paper {{presents}} the Demo / Kemo corpus of Dutch and Korean emotional speech. The corpus has been specifically {{developed for the}} purpose of cross-linguistic comparison, and is more balanced than any similar corpus available so far: a) it contains expressions by both Dutch and Korean actors as well as judgments by both Dutch and Korean listeners; b) the same elicitation technique and recording procedure was used for recordings of both languages; c) the same <b>nonsense</b> <b>sentence,</b> which was constructed to be permissible in both languages, was used for recordings of both languages; and d) the emotions present in the corpus are balanced in terms of valence, arousal, and dominance. The corpus contains a comparatively large number of emotions (eight) uttered by a large number of speakers (eight Dutch and eight Korean). The counterbalanced nature of the corpus will enable a stricter investigation of language-specific versus universal aspects of emotional expression than was possible so far. Furthermore, given the carefully controlled phonetic content of the expressions, it allows for analysis of the role of specific phonetic features in emotional expression in Dutch and Korean. 1...|$|R
5000|$|Other phrases {{typically}} {{used in the}} comment content can be stolen comments from other websites, [...] "nice article", something about their imaginary friends, plagiarised parts from books, unfinished <b>sentences,</b> <b>nonsense</b> words (usually to defeat a minimum comment length restriction) or the same link repeated.|$|R
40|$|International audienceThe {{purpose of}} this study was to assess several {{acoustic}} features of the voices of 26 parkinsonian patients under two conditions, with and without bilateral chronic stimulation of the subthalamic nucleus (STN) to estimate the effectiveness of this procedure on parkinsonian speech. When compared to unstimulated patients, stimulated patients showed longer duration of sustained vowels, shorter duration of <b>sentences,</b> <b>nonsense</b> words, and pauses, more variable fundamental frequency (f 0) in sentences, and more stable f 0 during sustained vowels. Relative intensity was unchanged in both conditions. Further acoustic analyses are warranted to clarify the role of STN stimulation on parkinsonian speech...|$|R
40|$|This study {{investigated}} how confusability between target and masking utterances affects the masking release achieved through spatial separation. Important distinguishing characteristics between competing voices were removed by processing speech with six-channel envelope vocoding, which simulates {{some aspects of}} listening with a cochlear implant. In the first experiment, vocoded target <b>nonsense</b> <b>sentences</b> were presented against two-talker vocoded maskers in conditions that provide different spatial impressions but not reliable cues that lead to traditional release from masking. Surprisingly, no benefit of spatial separation was found. The absence of spatial release was hypothesized {{to be the result}} of the highly positive target-to-masker ratios necessary to understand vocoded speech, which may have been sufficient to reduce confusability. In experiment 2, words excised from the vocoded <b>nonsense</b> <b>sentences</b> were presented against the same vocoded two-talker masker in a four-alternative forced-choice detection paradigm where threshold performance was achieved at negative target-to-masker ratios. Here, the spatial release from masking was more than 20 dB. The results suggest the importance of signal-to-noise ratio in the observation of “informational” masking and indicate that careful attention should be paid to this type of masking as implant processing improves and listeners begin to achieve success in poorer listening environments...|$|E
40|$|A {{sentence}} verification {{task was}} developed to investigate semantic memory in schizophrenia. The test consisted of three types of sentence (true, unlikely and nonsense) and seven different types of content (neutral, persecutory, grandiose, political, religious, relationships and somatic) representing common delusional themes present in schizophrenic patients. Sixty-three schizophrenic patients and 66 matched control {{subjects were asked to}} make true/false judgements to 143 sentences. Overall accuracy was similar across the two groups; sentences with some emotional themes and sentences of the unlikely type produced the most violations. Significant differences between the two subject groups were found specifically on <b>nonsense</b> <b>sentences</b> with persecutory and religious themes. Patients made significantly more incorrect responses (acceptance) to <b>nonsense</b> <b>sentences</b> that had an emotional content congruent with their delusional beliefs, past or present, and also on unlikely sentences (incorrect rejections) whose content was not congruent with their delusions. Further analysis of response bias in the patients showed, overall, that there were more incorrect rejections (a reflection of the large number of unlikely sentence errors) and more incorrect responses to sentences congruent with patients delusions. Furthermore, analysis of those patients currently experiencing delusions revealed more incorrect responses to sentences congruent with their delusional ideas compared with patients not currently deluded. These findings are indicative of cognitive bias in schizophrenia towards certain emotional themes that may underlie illogical semantic connections and delusions...|$|E
40|$|We {{evaluated}} {{the effects of}} different electrical parameter settings on the intelligibility of speech in patients with Parkinson's disease (PD) bilaterally treated with deep brain stimulation (DBS) in the subthalamic nucleus (STN). Ten patients treated with DBS for 15 +/- 5 months (mean, SD) with significant (P < 0. 01) symptom reduction (Unified Parkinson's Disease Rating Scale III) were included. In the medication off condition, video laryngostroboscopy was performed and then, in random order, 1 1 DBS parameter settings were tested. Amplitude was increased and decreased by 25 %, frequency was varied in the range 70 to 185 pps, {{and each of the}} contacts was tested separately as a cathode. The patients read a standard running text and five <b>nonsense</b> <b>sentences</b> per setting. A listener panel transcribed the <b>nonsense</b> <b>sentences</b> as perceived and valued the quality of speech on a visual analogue scale. With the patients' normally used settings, there was no significant (P = 0. 058) group difference between DBS OFF and ON, but in four patients the intelligibility deteriorated with DBS ON. The higher frequencies or increased amplitude caused significant (P < 0. 02) impairments of intelligibility, whereas changing the polarity between the separate contacts did not. The settings of amplitude and frequency have a major influence on the intelligibility of speech, emphasizing the importance of meticulous parameter adjustments when programming DBS to minimize side effects related to speech...|$|E
40|$|International audienceThis paper {{presents}} preliminary {{analysis and}} modelling of facial motion capture data recorded on a speaker uttering <b>nonsense</b> syllables and <b>sentences</b> with various acted facial expressions. We analyze here {{the impact of}} facial expressions on articulation and determine prediction errors of simple models trained to map neutral articulation to the various facial expressions targeted. We show that movement of some speech organs such as the jaw and lower lip are relatively unaffected by the facial expressions considered here (smile, disgust) while others such as {{the movement of the}} upper lip or the jaw translation are quite perturbed. We also show that these perturbations are not simply additive, and that they depend on articulation...|$|R
40|$|The {{purpose of}} this study was to assess several {{acoustic}} features of the voices of 26 parkinso-nian patients under two conditions, with and without bilateral chronic stimulation of the subtha-lamic nucleus (STN) to estimate the effectiveness of this procedure on parkinsonian speech. When compared to unstimulated patients, stimulated patients showed longer duration of sus-tained vowels, shorter duration of <b>sentences,</b> <b>nonsense</b> words, and pauses, more variable funda-mental frequency (f 0) in sentences, and more stable f 0 during sustained vowels. Relative intensity was unchanged in both conditions. Further acoustic analyses are warranted to clarify the role of STN stimulation on parkinsonian speech. ª 2001 Academic Press Key Words: Parkinson’s disease; surgical treatment; subthalamic nucleus stimulation; dysar-thria; voice; acoustic analysis. Voice abnormalities of patients with Parkinson’s disease (PD) include breathy o...|$|R
40|$|In his Paradoxes (1995 : Cambridge University Press: 149) Mark Sainsbury {{presents}} the following pair of sentences: Line 1 : The sentence written on Line 1 is nonsense. Line 2 : The sentence written on Line 1 is nonsense. Sainsbury (1995 : 149, 154) here makes three assertions: (1) The sentence in Line 1 is so viciously self-referential that it {{falls into the}} truth-value gap. The <b>sentence</b> is really <b>nonsense.</b> (2) The <b>sentence</b> in Line 2 is by contrast true. For it states precisely that the sentence in Line 1 is nonsense. (3) The two sentences in Lines 1 and 2 are {{an example of the}} principle that two sentence tokens of the same sentence-type can have different truth-values, although they have the same reference and state the same property of the object of reference. In this paper, I argue that Sainsbury’s assumptions are false in all three case...|$|R
40|$|Three {{experiments}} investigated {{factors that}} influence the creation of and release from informational masking in speech recognition. The target stimuli were <b>nonsense</b> <b>sentences</b> spoken by a female talker. In experiment 1 the masker {{was a mixture of}} three, four, six, or ten female talkers, all reciting similar <b>nonsense</b> <b>sentences.</b> Listeners’ recognition performance was measured with both target and masker presented from a front loudspeaker ~F–F! or with a masker presented from two loudspeakers, with the right leading the front by 4 ms ~F–RF!. In the latter condition the target and masker appear to be from different locations. This aids recognition performance for one- and two-talker maskers, but not for noise. As the number of masking talkers increased to ten, the improvement in the F–RF condition diminished, but did not disappear. The second experiment investigated whether hearing a preview ~prime! of the target sentence before it was presented in masking improved recognition for the last key word, which {{was not included in the}} prime. Marked improvements occurred only for the F–F condition with two-talker masking, not for continuous noise or F–RF two-talker masking. The third experiment found that the benefit of priming in the F–F condition was maintained if the prime sentence was spoken by a different talker or even if it was printed and read silently. These results suggest that informational masking can be overcome by factors that improve listeners’ auditory attention toward the target...|$|E
30|$|Interestingly, {{semantic}} {{processing of}} words presented between the hands {{appears to be}} impaired, consistent with a decrease in holistic analysis and a greater focus on visual detail. Davoli, Du, Montana, Garverick, and Abrams (2010) presented sentences between participants’ hands {{and asked them to}} judge whether or not they made sense. Detection of <b>nonsense</b> <b>sentences</b> between the hands was impaired. In a follow-up experiment, participants completed a Stroop task (naming the font color of a word while ignoring the word’s meaning). Stroop interference was decreased when the word was presented between participants’ hands. The authors interpreted this as a decrease in semantic processing and an increase is spatial processing.|$|E
40|$|The {{present study}} {{examined}} information processing differences {{between good and}} poor and fast and slow college-age readers. Four groups (high comprehension-high speed, high comprehension-low speed, low comprehension-high speed, and low comprehension-low speed) were {{selected on the basis}} of performance on the Nelson-Denny reading comprehension subtest and a timed reading sample. Performance among groups was compared on five information processing tasks, including word verification, sentence verification, letter reordering, word reordering, and reading span tasks. ^ No significant differences were found among groups for raw scores in the verification tasks. Groups did differ significantly on time scores for the verification tasks and on performance on letter reordering, word reordering, and reading span tasks. Follow-up procedures revealed that high comprehenders with either fast or slow reading speed outperformed low comprehenders on verification of words, nonwords, and <b>nonsense</b> <b>sentences,</b> and on letter reordering, word reordering, and reading span tasks. High comprehenders outperformed low comprehenders in verifying real sentences only if they were low speed readers, while high speed readers outperformed low speed readers in verification of nonwords and <b>nonsense</b> <b>sentences</b> only when they were low comprehenders. High speed readers in both low and high comprehension categories outperformed low speed readers in letter reordering. ^ Performance on tasks was discussed in light of speed and comprehension variables and type of information processing task (Palmer, MacLeod, Hunt, 2 ̆ 6 Davidson, 1985), while the Just and Carpenter (1980) model of reading provided the framework for understanding results in relationship to the reading process. Differences in working memory were proposed as a major source of individual differences in reading performance. ...|$|E
40|$|The Nemours {{database}} is {{a collection}} of 814 short <b>nonsense</b> sentences; 74 <b>sentences</b> spoken by each of 11 male speakers with varying degrees of dysarthria. Additionally, the database contains two connected-speech paragraphs produced by each of the 11 speakers. The database was designed to test the intelligibility of dysarthric speech before and after enhancement by various signal processing methods, and is available on CD-ROM. It {{can also be used to}} investigate general characteristics of dysarthric speech such as production error patterns. The entire database has been marked at the word level and sentences for 10 of the 11 talkers have been marked at the phoneme level as well. This paper describes the database structure and techniques adopted to improve the performance of a Discrete Hidden Markov Model (DHMM) labeler used to assign initial phoneme labels to the elements of the database. These techniques may be useful in the design of automatic recognition systems for persons with speech [...] ...|$|R
50|$|Bouwsma taught {{philosophy}} at the University of Nebraska from 1928 until 1965 and the University of Texas from 1965 until 1977. His greatest influence came, {{not so much}} through his humorously and finely written essays, but through the many graduate students he trained in his unique style of exploring the borderlands of sense and <b>nonsense</b> in philosophical <b>sentences.</b> Although he wrote incessantly and presented numerous papers, he published only one book {{toward the end of}} his career - a collection of essays titled Philosophical Essays. He died in 1978. His papers and daily notebooks, the latter filling hundreds of legal pads, are housed in the Harry Ransom Humanities Research Center at The University of Texas, Austin. J.L. Craft and Ronald E. Hustwit Sr. co-edited and published two additional volumes of his papers and selections of his commonplace book. His notebooks recording his discussions with Wittgenstein, published with the title, Wittgenstein Conversations, 1949-51, have become a primary source for Wittgenstein studies.|$|R
40|$|Speech {{perception}} is a bimodal process that involves both auditory and visual inputs. The auditory signal typically provides enough information for speech perception; however, when the auditory signal is compromised, {{such as when}} listening in a noisy environment or due to a hearing loss, people rely on visual cues to aid in understanding speech. Visual cues {{have been shown to}} significantly improve speech perception when the auditory signal is degraded in some way. The McGurk and MacDonald study (1976) strongly supported the fact that speech is not a purely auditory process {{and that there is a}} visual influence even with perfect auditory input. There is a growing interest in the benefit that listeners receive from audio-visual integration when the auditory signal is compromised. Remez et al, (1981) studied intelligibility when the speech waveform is reduced to three sine waves that represent the first three formants of the original signal. Remez discovered that sine wave speech is still highly intelligible even though a considerable amount of information was removed from the speech signal. Grant and Seitz (1998) looked at audio-visual integration performance of hearing impaired listeners by comparing a variety of audio-visual integration tasks using <b>nonsense</b> syllables and <b>sentences...</b>|$|R
40|$|We {{tested the}} {{hypothesis}} that, given {{the consistency of}} tone-syllable alignment found in recent research, accuracy of tone perception {{is dependent on the}} accuracy of syllable perception. In two experiments, subjects either judged the number of sylla-bles or identified the tones in <b>nonsense</b> <b>sentences</b> that were spectrally intact, low-pass filtered at 300 Hz or converted to sustained schwa carrying the original F 0. It was found that removing spectral information affected not only subjects’ ability to judge the number of syllables in a sentence, but also their ability to identify the tones. The results thus confirm the dependence of tone perception on syllable perception. Index Terms: speech perception, tone perception, syllable perception, tone-syllable alignmen...|$|E
40|$|Many {{languages}} have {{restrictions on}} word-final segments, {{such as a}} requirement that any word- final obstruent be voiceless. There is a phonetic basis for such restrictions {{at the ends of}} utterances, but not the ends of words. Historical linguists have long noted this mismatch, and have attributed it to an analogical generalization of such restrictions from utterance-final to word-final position. To test whether language learners actually generalize in this way, two artificial language learning experiments were conducted. Participants heard <b>nonsense</b> <b>sentences</b> in which there was a restriction on utterance-final obstruents, but in which no information was available about word-final, utterance-medial obstruents. They were then tested on utterances that included obstruents in both positions. They learned the pattern and generalized it to word-final utterance-medial position, confirming that learners are biased toward word-based distributional patterns...|$|E
40|$|It has {{consistently}} {{been shown that}} among the three mainland Scandinavian languages, Danish is most difficult to understand for fellow Scandinavians. Recent research suggests that Danish is spoken significantly faster than Norwegian and Swedish. This finding might partly explain the asymmetric intelligibility among Scandinavian languages. However, since fast speech {{goes hand in hand}} with a high amount of speech reduction, the question arises whether the high speech rate as such impairs intelligibility, or the high amount of reduction. In this paper we tear apart these two factors by auditorily presenting 168 Norwegian- and Swedish-speaking participants with 50 monotonised <b>nonsense</b> <b>sentences</b> in four conditions (quick and unclear, slow and clear, quick and clear, slow and unclear) in a translation task. Our results suggest that speech rate has a larger impact on the intelligibility of monotonised speech than naturally occurring reduction...|$|E
40|$|This study {{investigated}} the effects of stimulus type on the measurement s of short-term auditory memory span and short-term auditory memory for sequence to determine if span and sequence measures were the same within each of five subtests and if span and/or sequence measures varied across all five subtests. A total of forty-five normal second, third, and fourth grade subjects were individually administered the Auditory Memory Test Battery (AMTB) which consisted of five tape-recorded tests of recall for digit sequences, unrelated word sequences, related word sequences, nonsense word sequences, and sentences. The subjects responded verbally to the randomly presented subtests. Each subject obtained ten scores: a span score and a sequence score {{for each of the}} five subtests. The results of the study showed the span and sequence scores for the digit task differed significantly, with the span task being easier; however, the scores did not differ significantly for unrelated words, related words, <b>nonsense</b> words, or <b>sentences.</b> Both span and sequence performances were found to vary significantly with the type of stimulus; however, no difference was found in sequence performance between related and unrelated words. Generally, sentence recall was easier than recall of individual words, and recall of nonsense words was most difficult...|$|R
40|$|This paper reviews past work {{comparing}} modern {{speech recognition}} systems and humans {{to determine how}} far recent dramatic advances in technology have progressed towards the goal of human-like performance. Comparisons use six modern speech corpora with vocabularies ranging from 10 to more than 65, 000 words and content ranging from read isolated words to spontaneous conversations. Error rates of machines are often more than {{an order of magnitude}} greater than those of humans for quiet, wideband, read speech. Machine performance degrades further below that of humans in noise, with channel variability, and for spontaneous speech. Humans can also recognize quiet, clearly spoken nonsense syllables and <b>nonsense</b> <b>sentences</b> with little high-level grammatical information. These comparisons suggest that the human–machine performance gap can be reduced by basic research on improving low-level acoustic-phonetic modeling, on improving robustness with noise and channel variability, and on more accurately modeling spontaneous speech...|$|E
30|$|Participants were {{required}} to read sentences while trying to remember {{the same set of}} unrelated letters as Ospan. For this task, participants read a sentence and determined whether the sentence made sense or not (e.g., “The prosecutor’s dish was lost because it was not based on fact. ?”). Half of the sentences made sense while the other half did not. <b>Nonsense</b> <b>sentences</b> were made by simply changing one word (e.g., “dish” from “case”) from an otherwise normal sentence. Participants {{were required}} to read the sentence and to indicate whether it made sense or not. After participants gave their response they were presented with a letter for 1 s. At recall, letters from the current set were recalled in the correct order by clicking on the appropriate letters (see Unsworth et al., 2009 for more details). There were three trials of each list-length with list-length ranging from 3 – 7 for a total possible of 75. The same scoring procedure as Ospan was used.|$|E
40|$|In adverse {{listening}} conditions, {{large and}} robust increases in intelligibility {{can be achieved}} by speaking clearly. The most striking differences between clear and conver-sational speech are associated with differences in speaking rate. To understand these differences, the intelligibility of speech in a variety of speaking modes was investigated at three different speaking rates. Talkers with significant speaking experience were asked to produce clear and conversational speech at slow, normal, and quick rates. Previous studies show that the speaking rate for clear speech (100 words-per-minute) is roughly one-half that of normal rates for conversational speech. Therefore, during training, the talkers were given feedback on their intelligibility in order to elicit the clearest possible speech at each speaking rate. Talkers also recorded sentences in sev-eral other speaking modes such as soft, loud, and conversational with pauses inserted, as required for input to some automatic speech recognition systems. All speech materials used for intelligibility tests were <b>nonsense</b> <b>sentences</b> whic...|$|E
40|$|There {{are reasons}} to believe that infant-directed (ID) speech may make {{language}} acquisition easier for infants. However, the effects of ID speech on infants 2 ̆ 7 learning remain poorly understood. The experiments reported here assess whether ID speech facilitates word segmentation from fluent speech. One group of infants heard a set of <b>nonsense</b> <b>sentences</b> spoken with intonation contours characteristic of adult-directed (AD) speech, and the other group heard the same sentences spoken with intonation contours characteristic of ID speech. In both cases, the only cue to word boundaries was the statistical structure of the speech. Infants were able to distinguish words from syllable sequences spanning word boundaries after exposure to ID speech but not after hearing AD speech. These results suggest that ID speech facilitates word segmentation and may be useful for other aspects of language acquisition as well. Issues of direction of preference in preferential listening paradigms are also considered...|$|E
40|$|This study {{tested the}} {{hypothesis}} that the previously reported advantage of musicians over non-musicians in understanding speech in noise arises from more efficient or robust coding of periodic voiced speech, particularly in fluctuating backgrounds. Speech intelligibility was measured in listeners with extensive musical training, and in those with very little musical training or experience, using normal (voiced) or whispered (unvoiced) grammatically correct <b>nonsense</b> <b>sentences</b> in noise that was spectrally shaped to match the long-term spectrum of the speech, and was either continuous or gated with a 16 -Hz square wave. Performance was also measured in clinical speech-in-noise tests and in pitch discrimination. Musicians exhibited enhanced pitch discrimination, as expected. However, no systematic or statistically significant advantage for musicians over non-musicians was found in understanding either voiced or whispered sentences in either continuous or gated noise. Musicians also showed no statistically significant advantage in the clinical speech-in-noise tests. Overall, the results provide no evidence for a significant difference between young adult musicians and non-musicians in their ability t...|$|E
40|$|Clear {{speech is}} a form of {{communication}} that talkers naturally use when speaking in difficult listening conditions or with a person who has a hearing loss. Clear speech, on average, provides listeners with hearing impairments an intelligibility benefit of 17 percentage points (Picheny, Durlach, 2 ̆ 6 Braida, 1985) over conversational speech. In addition, it provides increased intelligibility in various listening conditions (Krause 2 ̆ 6 Braida, 2003, among others), with different stimuli (Bradlow 2 ̆ 6 Bent, 2002; Gagne, Rochette, 2 ̆ 6 Charest, 2002; Helfer, 1997, among others) and across listener populations (Bradlow, Kraus, 2 ̆ 6 Hayes, 2003, among others). Recently, researchers have attempted to compare their findings with clear and conversational speech, at slow and normal rates, with results from other investigators 2 ̆ 7 studies in an effort to determine the relative benefits of clear speech across populations and environments. However, relative intelligibility benefits are difficult to determine unless baseline performance levels can be equated, suggesting that listener psychometric functions with clear speech are needed. The {{purpose of this study was}} to determine how speech intelligibility, as measured by percentage key words correct in <b>nonsense</b> <b>sentences</b> by young adults, varies with changes in speaking condition, talker and signal-to-noise ratio (SNR). Forty young, normal hearing adults were presented with grammatically correct <b>nonsense</b> <b>sentences</b> at five SNRs. Each listener heard a total of 800 sentences in four speaking conditions: clear and conversational styles, at slow and normal rates (i. e., clear/slow, clear/normal, conversational/slow, and conversational/normal). Overall results indicate clear/slow and conversational/slow were the most intelligible conditions, followed by clear/normal and then conversational/normal conditions. Moreover, the average intelligibility benefit for clear/slow, clear/normal and conversational/slow conditions (relative to conversational/normal) was maintained across an SNR range of - 4 to 0 dB in the middle, or linear, portion of the psychometric function. However, when results are examined by talker, differences are observed in the benefit provided by each condition and in how the benefit varies across noise levels. In order to counteract talker variability, research with a larger number of talkers is recommended for future studies...|$|E
40|$|The {{effect of}} {{perceived}} spatial differences on masking release was examined using a 4 AFC speech detection paradigm. Targets were 20 words {{produced by a}} female talker. Maskers were recordings of continuous streams of <b>nonsense</b> <b>sentences</b> spoken by two female talkers and mixed into each of two channels (two talker, and the same masker time reversed). Two masker spatial conditions were employed: “RF” with a 4 ms time lead to the loudspeaker 60 ° horizontally to the right, and “FR” with the time lead to the front (0 °) loudspeaker. The reference nonspatial “F” masker was presented from the front loudspeaker only. Target presentation was always from the front loudspeaker. In Experiment 1, target detection threshold for both natural and time-reversed spatial maskers was 17 – 20 dB lower than that for the nonspatial masker, suggesting that significant release from informational masking occurs with spatial speech maskers regardless of masker understandability. In Experiment 2, {{the effectiveness of the}} FR and RF maskers was evaluated as the right loudspeaker output was attenuated until the two-source maskers were indistinguishable from the F masker, as measured independently in a discrimination task. Results indicated that spatial release from masking can be observed with barely noticeable target-masker spatial differences...|$|E
40|$|Clear speech {{refers to}} a {{speaking}} style that is more intelligible than typical, conversational speaking styles. It is usually produced at a slower rate compared to conversational speech. Clear speech {{has been shown to}} be more intelligible than conversational speech for a large variety of populations, including both hearing impaired (Schum, 1996; Picheny, Durlach, 2 ̆ 6 Braida, 1985; and Payton, Uchanski, 2 ̆ 6 Braida, 1994) and normal hearing individuals (e. g. Uchanski, Choi, Braida, Reed, 2 ̆ 6 Durlach, 1996) under a variety of conditions, including those in which presentation level, speaker, and environment are varied. Although clear speech is typically slower than normally produced conversational speech, recent studies have shown that it can be produced at normal rates with training (Krause 2 ̆ 6 Braida, 2002). If clear speech at normal rates is shown to be as effective for individuals with hearing loss as clear speech at slow rates, it would have both clinical and research implications. The purpose of this study was to determine the effectiveness of clear speech at normal rates for older individuals with hearing loss. It examined the way in which intelligibility, measured as percent correct keyword scores on <b>nonsense</b> <b>sentences,</b> varied as a result of speaking mode (clear versus conversational speech) and speaking rate (slow versus normal) in six adults aged 55 - 75 years old with moderate, sloping, hearing loss. Each listener was presented with <b>nonsense</b> <b>sentences</b> in four speech conditions: clear speech at slow rates (clear/slow), clear speech at normal rates (clear/normal), conversational speech at slow rates (conv/slow), and conversational speech at normal rates (conv/normal) read by four different talkers. Sentences were presented monaurally in quiet to the listeners via headphones. Results indicated that clear/slow speech was the most intelligible condition overall. Neither conv/slow nor clear/normal provided an intelligibility benefit relative to conv/normal speech on average, suggesting that for older adults with moderate, sloping hearing loss, the combination of using clear speech and a slower speaking rate is more beneficial to intelligibility than the additive effects of altering either speaking rate or speaking mode alone. It has been suggested previously (Krause, 2001) that audiological characteristics may contribute to the lack of clear/normal benefit for certain listeners with hearing loss. Although clear/normal speech was not beneficial on average to listeners in this study, there were cases in which the clear/normal speech of a particular talker provided a benefit to a particular listener. Thus, severity and configuration of hearing loss alone cannot fully explain the degree to which listeners from hearing loss do (or do not) benefit from clear/normal speech. More studies are needed to investigate the benefits of clear/normal speech for different audiological configurations, including individuals with flat losses. In addition, the listening tasks should include more difficult conditions in order to compensate for potential ceiling effects...|$|E
