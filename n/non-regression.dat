55|0|Public
5000|$|<b>Non-regression</b> testing (NRT), {{or simply}} {{validation}} {{of a new}} issue, is an approach to software testing. The purpose of <b>non-regression</b> testing is to verify whether, after introducing or updating a given software application, the change has had the intended effect.|$|E
50|$|In {{this case}} <b>non-regression</b> testing is appropriate.|$|E
5000|$|In {{automotive}} applications, <b>non-regression</b> {{testing is}} performed as follows: ...|$|E
5000|$|A <b>non-regression</b> {{test can}} be {{performed}} according the following steps: ...|$|E
5000|$|... #Subtitle level 2: How {{to define}} a good <b>non-regression</b> testing {{strategy}} ...|$|E
50|$|In contrast, <b>non-regression</b> testing aims {{to verify}} whether, after {{introducing}} or updating a given software application, the change {{has had the}} intended effect.|$|E
50|$|The {{intent of}} {{regression}} testing is {{to assure that}} {{in the process of}} fixing a defect no existing functionality has been broken. <b>Non-regression</b> testing is performed to test that an intentional change has had the desired effect.|$|E
50|$|Usually, the {{occurrence}} of software bugs can result in unexpected delays to the project. Due to time-to-market restrictions, the validation phase of software functionalities must be well organized and efficient. In this context, <b>non-regression</b> testing provides a systematic procedure for fast and efficient validation and discovery of bugs within the software architecture.|$|E
50|$|Throughout {{the years}} {{engine control unit}} (ECU) {{software}} requirements have become more complex and difficult to reach. This {{is due to the}} increaslingly stringent emission norms and the ambitious performance in terms of fuel consumption and power request. This, in turn, increases the demand and complexity of in-vehicle driving tests and diagnostic functionalities. As a consequence, along engine control systems development, each new software release results from a sequence of many others, each one introducing new functions seeking to satisfy, time after time, the demands. In this context, <b>non-Regression</b> testing is useful to verify that the performance and robustness of each software release does not decrease in relation with the previous one, or, in other terms, does not introduce regression.|$|E
40|$|Tumor {{draining}} sentinel {{lymph nodes}} (SLNs) are {{the sites of}} selective changes as compared to non-SLNs. They show features of tumor-reactive lymphadenopathy, including increased total number of functional blood vessels, but a relative immunosuppressed status has also been described in them. We explored the hypothesis of a selective regression or <b>non-regression</b> in SLNs versus non-SLNs in 142 patients with 110 estrogen receptor-positive and 32 estrogen receptor-negative tumors undergoing both SLN biopsy and axillary lymph node dissection after neoadjuvant therapy by assessing the tumoral (metastatic) and regression statuses of SLNs and non-SLNs separately. Of the 89 cases with signs of nodal regression, 22 cases (25 %) {{were in favor of}} a selective <b>non-regression</b> in SLNs, 18 cases (20 %) were supportive of a selective and more pronounced regression in the SLNs and the remaining showed equal degrees of regression or <b>non-regression</b> in SLNs and non-SLNs. The results indicate that there is no obvious difference in the degree of regressive histological changes shown by SLNs and NSLNs. Therefore, this phenomenon may not be a major contributor to the higher false negative rate of SLN biopsy after neoadjuvant treatment...|$|E
40|$|International audienceRecently, it {{has been}} {{proposed}} to use Bernstein races for implementing <b>non-regression</b> testing in noisy genetic programming. We study the population size of such a (1 +λ) evolutionary algorithm applied to a noisy fitness function optimization by a progress rate analysis and experiment it on a policy search application...|$|E
40|$|Incremental {{development}} is now {{state of the}} practice. Indeed, it is promoted from the rational unified process to agile development methods. Few methods however guide software developers and architects in doing so. For instance, no tool is proposed to verify the <b>non-regression</b> of functionalities, modeled as behavior specifications, between increments. This work helps to incrementally specify software functionalities using UML state machines. It provides an on-the-fly evaluation of a specified behavior as {{compared to that of}} previous increments. The proposed contribution is based on two formally specified relations that are proved to preserve refinement when composed. Architects and developers are free to choose their preferred behavior specification strategy by iteratively applying them, so as to develop the required functionalities, having at each step the benefit of a formal <b>non-regression</b> checking to guide the global specification process. Our proposal is implemented in a proof-of-concept tool and illustrated by a didactic casestudy...|$|E
40|$|The {{present study}} focuses on single-case data {{analysis}} and specifically on two procedures for quantifying differences between baseline and treatment measurements The first technique tested {{is based on}} generalized least squares regression analysis and is compared to a proposed <b>non-regression</b> technique, which allows obtaining similar information. The comparison is {{carried out in the}} context of generated data representing a variety of patterns (i. e., independent measurements, different serial dependence underlying processes, constant or phase-specific autocorrelation and data variability, different types of trend, and slope and level change). The results suggest that the two techniques perform adequately {{for a wide range of}} conditions and researchers can use both of them with certain guarantees. The regression-based procedure offers more efficient estimates, whereas the proposed <b>non-regression</b> procedure is more sensitive to intervention effects. Considering current and previous findings, some tentative recommendations are offered to applied researchers in order to help choosing among the plurality of single-case data analysis techniques...|$|E
40|$|The paper {{analyzes}} the ECJ case law on fixed-term work, with specific regard to <b>non-regression</b> clause, measures to prevent abuses and {{the principle of}} non-discrimination. In particular, the Author {{points out that the}} principle of non-discrimination is to be regarded as being the core of the fixed-term work regulation; in this respect, especially in more recent judgments, the Court seems to maximize the scope of such principle...|$|E
40|$|Test case {{prioritization}} {{techniques have}} been empirically {{proved to be}} effective in improving the rate of fault detec-tion in regression testing. However, most of previous tech-niques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from pre-vious execution of test cases or source code changes, few of them can be directly applied to <b>non-regression</b> testing. In this paper, aiming to improve the rate of severe faults de-tection for both regression testing and <b>non-regression</b> test-ing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e. g., method) covered by test cases. As a proof of concept, we implement Apros, a test case prior-itization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result repre-sents that our approach could be a promising solution to improve the rate of severe faults detection. 1...|$|E
40|$|Resilient {{self-help}} {{is essential}} in coping with life’s upsets. This essay explores the prospect of recognizing Resilience as a Principle of Law. The propositions set forth here were debated at two conferences held in Brasilia, in December of 2013. The first, for legislators, was convened in the Senate of Brazil by the National Congress’ Joint Permanent Committee on Climate Change, and the second, for judges, was convened by the Federal Judicial Council’s Judicial Studies Center (Conselho da Justiça Federal Centro de Estudos Judiciários) and the High Court of Brazil (Superior Tribunal de Justiça). This eJournal of the IUCN Academy of Environmental Law {{has emerged as a}} leader in exploring new principles of environmental law, such as the Principle of <b>Non-Regression,</b> framed by Prof. Michel Prieur. Just as courts have begun to recognize the Principle of <b>Non-Regression,</b> the welfare of both humans and nature requires recognition of Resilience. Many of the jurisprudential foundations for the legal Principle of Resilience are set forth by Lia Helena Monteiro de Lima Demange in her insightful recent article about the Principle...|$|E
40|$|Motu Economic and Public Policy Research for {{comments}} on the paper. We also thank James Newell for providing us with data and assistance in creating local labour market boundaries. Access to the data {{used in this study}} was provided by Statistics New Zealand under conditions designed to give effect to the security and confidentiality provisions of the Statistics Act 1975. All <b>non-regression</b> results are subject to base three rounding in accordance with Statistics Ne...|$|E
40|$|In {{this paper}} we analyze {{some aspects of}} the current European {{environmental}} policy from two perspectives: firstly, in its origin, which requires taking into account the recent amendments to the primary law (Lisbon Treaty) and secondary legislation in the light of action programs and on the other hand, application state, which examines the interpretation and application of European environmental principles and their corollaries (in particular the controversial principles of cost recovery and <b>non-regression)</b> ...|$|E
40|$|Meta-analysis {{has been}} used to {{synthesize}} research findings and {{to evaluate the effectiveness of}} treatments or the accuracy of diagnostic tools. Although meta-analytic techniques were developed to synthesize the results of several studies, controversy exists as to how to quantify the results from single-subject experimental designs (SSEDs). The most commonly used metrics are reviewed, including <b>non-regression</b> and regression based methods. The application of the SAS template is demonstrated through simulated data sets. The SAS templates can be modified to accommodate a more complex data structure. Key words: Single-subject experimental designs (SSED), SAS template...|$|E
40|$|Natural landscapes often reveal {{extremely}} complex {{patterns that}} can only be very rougly characterized by methods of Euclidean geometry. In contrast, fractals can be applied to a variety of landscape ecology problems because they conveniently describe many of the irregular, fragmented patterns found in nature. This paper focuses on a fractal-based measure of landscape complexity for grid-based GIS layers. A <b>non-regression</b> technique for measuring the distribution of diversity within a raster database consisting of square cells is generalized to incorporate any regular shaped grid cell (eg regular polygon, rectangle) that forms a continuous, fully tessellated grid...|$|E
40|$|Abstract. This paper {{introduces}} BeGoood, {{a generic}} system for man-aging <b>non-regression</b> tests on knowledge-bases. BeGoood {{is a system}} al-lowing to define test plans in order to monitor the evolution of knowledge-bases. Any system answering queries by providing results {{in the form of}} set of strings can be tested with BeGoood. BeGoood has been devel-oped following a REST architecture and is independent of any applica-tion domain. This paper describes the architecture of the system and gives a use case to illustrate how BeGoood is able to manage a collab-orative knowledge evolution in the framework of a case-based reasoning system. ...|$|E
40|$|During the {{development}} process of a thermal-hydraulic system code, a <b>non-regression</b> test (NRT) must be performed repeatedly {{in order to prevent}} software regression. The NRT process, however, is time-consuming and labor-intensive. Thus, automation of this process is an ideal solution. In this study, we have developed a program to support an efficient NRT for the SPACE code and demonstrated its usability. This results in a high degree of efficiency for code development. The program was developed using the Visual Basic for Applications and designed {{so that it can be}} easily customized for the NRT of other computer codes...|$|E
30|$|Access to {{the data}} {{used in this study}} was {{provided}} by Statistics New Zealand under conditions designed to give effect to the security and confidentiality provisions of the Statistics Act 1975. All <b>non-regression</b> results are subject to base three rounding in accordance with Statistics New Zealand’s release policy for census data. The project was originally funded as part of Motu’s “Understanding Adjustment and Inequality” research programme, which had core funding from the Foundation for Research, Science and Technology. Any views expressed are the sole responsibility of the authors and do not purport to represent those of Motu, the Free University of Bozen-Bolzano, or Statistics New Zealand.|$|E
40|$|Abstract. This paper formalizes an {{incremental}} approach to design VCI to PI protocol converters (VCI-PI wrappers) and presents {{a hierarchy of}} wrappers ranking from the simplest one up to the most complex one. In order to formally verify the correctness of a wrapper, a set of CTL properties is assigned to it. The purpose of the paper is to explore how, given a property that is true in a simple model, a new property, satisfied in a more complex model, {{can be derived from}} the first one, and reciprocaly. We propose some transformation rules to build new properties satisfied on more complex models. The properties transformation have been automated and applied in the context of <b>non-regression</b> analysis of VCI-PI wrappers...|$|E
40|$|Natural landscapes often reveal {{extremely}} complex {{patterns that}} can be only very roughly characterized by methods of Euclidean geometry. Conversely, fractals {{can be applied to}} a variety of landscape ecology problems because they conveniently describe many of the irregular, fragmented patterns found in nature. A strategy for computing local landscape complexity on classified raster GIS layers with a fractal <b>non-regression</b> technique is presented. This strategy is based on the use of Merchant's adaptive geographic window which is designed to operate on a neighborhood of patches instead of a fixed rectangular neighborhood of pixels (the conventional approach in image analysis). Preliminary results show that, owing to its enhanced flexibility, the geographic window seems to be a more appropriate tool for calculating local landscape complexity of classified raster GIS layers than the traditional rectangular geometric window...|$|E
40|$|International audienceWriting {{relevant}} properties for a realistic component’s specification is not easy. In [1], we formalized an incremental design process. A component is {{obtained from a}} simpler component and some new behaviors modeled by an increment. From a component at a step i of the design process, its specification can be derived into {{a part of the}} specification of the same component at a step i + 1. The obtained specification is exactly the set of rules necessary to check the <b>non-regression</b> between two components. In the present paper, we extend the transformation rules, previously stated, in considering that the increment verifies a set of CTL formulas. This allows to build automatically a larger set of formulas that is the entire specification of the component at step i + 1...|$|E
40|$|Quantile {{regression}} is {{an increasingly}} important empirical tool in economics and other sciences for analyzing {{the impact of a}} set of regressors on the conditional distribution of an outcome. Extremal quantile regression, or quantile regression applied to the tails, is of interest in many economic and financial applications, such as conditional value-at-risk, production efficiency, and adjustment bands in (S,s) models. In this paper we provide feasible inference tools for extremal conditional quantile models that rely upon extreme value approximations to the distribution of self-normalized quantile regression statistics. The methods are simple to implement and can be of independent interest even in the <b>non-regression</b> case. We illustrate the results with two empirical examples analyzing extreme fluctuations of a stock return and extremely low percentiles of live infants' birthweights in the range between 250 and 1500 grams. ...|$|E
40|$|Abstract. Writing {{relevant}} properties for a realistic component's specication is not easy. In [1], we formalized an incremental design process. A component is {{obtained from a}} simpler component and some new behaviors modeled by an increment. From a component at a step i of the design process, its specication can be derived into {{a part of the}} specication of the same component at a step i + 1. The obtained specication is exactly the set of rules necessary to check the <b>non-regression</b> between two components. In the present paper, we extend the transformation rules, previously stated, in considering that the increment veries a set of CTL formulas. This allows to build automatically a larger set of formulas that is the entire specication of the component at step i+ 1. ...|$|E
40|$|An {{executable}} {{formal specification}} of the Tobias combinatorial test generator Tobias is a combinatorial testing tool {{that was used}} succesfully on several case studies. Currently, {{the evolution of the}} tool goes through a significant redevelopment effort. This paper presents an executable specification of the Tobias Test Generator. This prototype is part of the development of the new version of the tool. The goal of this specification effort is to provide a synthetic and precise description of Tobias to the developers of the new tool. The specification is expressed in the Z language, supported by the Jaza animator. This paper shows how the executable character of the specification is exploited (1) to assess <b>non-regression</b> of the specification with respect to the existing tool, and (2) to explore new functionalities for the tool. 1...|$|E
40|$|International audienceThis paper formalizes an {{incremental}} approach to design flow-control oriented hardware devices described by Moore machines. The method {{is based on}} successive additions of new behaviors to a simple device {{in order to build}} a more complex one. The new behaviors added must not override the previous ones. A set of CTL formulae is assigned to each step of the design. The links between the formulae of two consecutive design steps are formalized as a set of formula-transformations F, stating that : a CTL formula f is satisfied on a design at step i, iff F (f) is satisfied on the design extended at step i+ 1. This result has been applied during the design of bus protocol converters in the context on <b>non-regression</b> analysis. It could also be applied in order to simplify both system and formulae in particular cases...|$|E
40|$|Quantile {{regression}} (QR) is {{an increasingly}} important empirical tool in economics and other sciences for analysing the impact a set of regressors has on the conditional distribution of an outcome. Extremal QR, or QR applied to the tails, is of interest in many economic and financial applications, such as conditional value at risk, production efficiency, and adjustment bands in (S,s) models. This paper provides feasible inference tools for extremal conditional quantile models that rely on extreme value approximations to the distribution of self-normalized QR statistics. The methods are simple to implement and can be of independent interest even in the univariate (<b>non-regression)</b> case. We illustrate the results with two empirical examples analysing extreme fluctuations of a stock return and extremely low percentiles of live infant birthweight in the range between 250 and 1500 g. Copyright 2011, Oxford University Press. ...|$|E
40|$|AbstractThis paper formalizes an {{incremental}} approach to design flow-control oriented hardware devices described by Moore machines. The method {{is based on}} successive additions of new behaviors to a simple device {{in order to build}} a more complex one. The new behaviors added must not override the previous ones. A set of CTL formulae is assigned to each step of the design. The links between the formulae of two consecutive design steps are formalized as a set of formula-transformations F, stating that : a CTL formula f is satisfied on a design at step i, iff F(f) is satisfied on the design extended at step i+ 1. This result has been applied during the design of bus protocol converters in the context on <b>non-regression</b> analysis. It could also be applied in order to simplify both system and formulae in particular cases...|$|E
40|$|International audienceHigh-Level Synthesis (HLS) {{has opened}} an {{opportunity}} for soft- ware programmers to target FPGA more rapidly. When developing HLS tools, tests are desirable to ensure their function, reliability and performance. When modifications are applied to a tool, Non- Regression Test (NRT) asserts that the changes have intended effect while Regression Test (RT) verifies that the tool still performs cor- rectly without unwanted behaviour. The work {{presented in this paper}} is focused on a method to auto- matically perform <b>Non-Regression</b> Test in HLS tool developments, although it can also be used as a Regression Testing technique. This method relies on a framework which allows HLS tool developers to verify the circuits generated from the tool directly on FPGA, in- stead of using simulations. The verification flow is automatic, so that knowing the details of the system is unnecessary for developers. The framework has been tested successfully over several applications from HLS benchmark and it gives more promising results than its simulation counterpart...|$|E
40|$|International audienceIn {{this paper}} we {{describe}} how an executable model of interactive software can be exploited to allow programmers or spec-ifiers to express properties {{that will be}} automatically checked on the components they create or reuse. The djnn framework relies on a theoretical model of interactive software in which applications are described in their totality as hierarchies of interactive components, with no additional code. This includes high level components, but also the graphics, behaviors, computations and data manipulations that constitute them. Because of this, {{the structure of the}} application tree provides significant insights in the nature and behavior of components. Pattern recognition systems can then be used to express and check simple properties, such as the external signature of a component, its internal flows of control, or even the continued visibility of a component on a display. This provides programmers with solutions for checking their components, ensuring <b>non-regression,</b> or working in a contract-oriented fashion with other UI development stakeholders...|$|E
40|$|One of {{the most}} {{important}} and recurrent concept in international macroeconomics is Purchasing Power Parity (PPP) hypothesis. PPP {{has been used as a}} theory of domestic price determination under fixed exchange rate regime and a theory of exchange rate determination under flexible exchange rate regime. The main purpose of this study is to examine how well the PPP theory fit to the developing countries. The purpose is accomplished through conducting a battery of tests – <b>non-regression</b> based, regression based and co-integration based. An important feature of the study is that test of PPP which relies on capital account is also carried out. In general our findings do not support the PPP theory. PPP is not supported even if we rely on capital account in derivation of PPP. Only the relative version of PPP as a theory of price determination in Pakistan does have some empirical support. The paper also discusses potential reasons for empirical failure of PPP in developing countries. Purchasing Power Parity; Uncovered Interest Rate Parity; Real Exchange Rate; Random Walk Process; Unit Root Test...|$|E
40|$|Abstract. Quantile {{regression}} {{is a basic}} {{tool for}} estimation of conditional quantiles of a response variable given a vector of regressors. It {{can be used to}} measure the effect of covariates not only {{in the center of a}} distribution, but also in the upper and lower tails. Quantile regression applied to the tails, or simply extremal quantile regression is of interest in numerous economic and financial applications. For example, it can be employed to measure conditional value-at-risk, production-efficiency, adjustment bands in the (S,s) models, and cost functions of most efficient bidders in auctions. In order to facilitate the applications, this paper provides feasible inference tools that rely upon extreme value approximations of the distribution of self-normalized extremal quantile regression statistics. The methods are simple to implement in practice and are of independent interest even in the <b>non-regression</b> case. The value of the methods is explored in the analysis of extremely low percentiles of live infant birthweights (in the ranges between 250 and 1500 grams) and in the study of factors of extreme fluctuations of a stock return...|$|E
