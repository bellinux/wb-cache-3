3|41|Public
40|$|AbstractMany cells use {{diffusion}} in calcium concentration transmits {{messages in}} the form of chemical signaling. The diffusion of calcium provided numerous results influx of calcium in the cytosol and bind with buffer by pump. In this paper, the model incorporates the physiological parameters like diffusion coefficient, excess buffer etc. Appropriate boundary conditions have been framed. Finite element model using two-dimensional triangular ring elements has been developed to study radial and angular calcium diffusion problem in <b>neuron.</b> <b>Computer</b> simulation in MATLAB 7. 11 are employed to investigate mathematical models of reaction diffusion equation, the details of the implementation can heavily affect the numerical solutions and, thus, the outcome simulated on Core(TM) i 3 CPU M 330 2. 13 GHz processing speed and 3 GB memory...|$|E
40|$|The axonal arborization of {{chandelier}} {{cells is}} characterized by its conspicuous, vertically oriented, bouton aggregates. The efferent synaptic relationships established by these terminal formations were investigated by electron microscopy of Golgi preparations after gold toning and deimpregnation. In all cases examined from layers II and III of cat areas 17 and 18, the terminal formations, here denominated specific terminal portions (stp), make symmetric synapses upon axon initial segments of pyramidal neurons. Some identified stp's were reconstructed from ultrathin serial sections {{with the aid of}} a microcomputer-based system, and the number of synaptic contacts established on axon initial segments was evaluated. No evidence was found that parts of the axonal tree other than stp's also engage in synaptic contacts. Specific terminal portions are rather variable in complexity. However, the synaptic contacts they engage in are constant and the complexity of stp's from the same axonal arborization varies. It is, therefore, clear that all stp's are terminal axonal formations of a unique, specialized type of <b>neuron.</b> <b>Computer</b> techniques and conventional Golgi observations were used to study further details of chandelier cell morphology. Axonal plexuses are preferentially, although not exclusively, local and distribute within spheric, ovoid, or disk-shaped spaces. In most chandelier cells, the main axonal trunk descends to the white matter, where we have been unable to follow it further. Peer Reviewe...|$|E
40|$|The Aplysia {{sensorimotor}} synapse {{is a key}} site of plasticity {{for several}} simple forms of learning. Plasticity of this synapse has been extensively studied, albeit primarily with individual action potentials elicited at low frequencies. Yet, the mechanosensory neurons fire high-frequency bursts in response to even moderate tactile stimuli delivered to the skin. In the present study, we extend this analysis to show that sensory neurons also fire bursts {{in the range of}} 1 – 60 Hz in response to electrical stimuli similar to those used in behavioral studies of sensitization. Intracellular stimulation of sensory neurons to fire a burst of action potentials at 10 Hz for 1 sec led to significant homosynaptic depression of postsynaptic responses. The depression was transient and fully recovered within 10 min. During the burst, the steady-state depressed phase of the postsynaptic response, which was only 20 % of the initial EPSP of the burst, still contributed to firing the motor neuron. To explore the functional contribution of transient homosynaptic depression to the response of the motor <b>neuron,</b> <b>computer</b> simulations of the sensorimotor synapse with and without depression were compared. Depression allowed the motor neuron to produce graded responses over a wide range of presynaptic input strength. In addition, enhancement of synaptic transmission throughout a burst increased motor neuron output substantially more than did preferential enhancement of the initial phase of a burst. Thus, synaptic depression increased the dynamic range of the sensorimotor synapse and can, in principle, {{have a profound effect on}} information processing. Key words: burst; depression; modulation; facilitation; synaptic filtering; synaptic plasticit...|$|E
40|$|AbstractA {{model for}} end-stopping that {{requires}} only excitatory inputs is presented. This model {{is based on}} multiplication of the outputs from two orientation tuned and spatial-frequency selective <b>neurons.</b> <b>Computer</b> simulations show that, provided the optimal orientations of the two neurons are sufficiently different, the resulting product will display orientation-independent end-stopping. Neurons simulated in this manner display the main characteristics of actual hypercomplex cells...|$|R
40|$|In this study, {{in order}} to {{investigate}} the effect of chaotic os-cillations of real biological signals on information processing ability, we investigate {{the performance of the}} Hopfield neu-ral networks solving traveling salesman problems when the sampled data of chaotic pulse waves obtained from real bio-logical experiments are poured into the <b>neurons.</b> <b>Computer</b> simulated results show that the performance of the chaotic pulse wave is better than the case of random noise. 1...|$|R
5000|$|The <b>Neurons</b> are <b>computer</b> {{animated}} characters (stylised {{with human}} facial features and body, but no legs) and are named {{to reflect the}} five senses which they represent: ...|$|R
40|$|Motivated by V. B. Mountcastle's {{organizational}} {{principle for}} neocortical function, and by M. E. Fisher's model of physical spin systems, we introduce a cooperative {{model of the}} cortical column incorporating an idealized substructure, the trion, which represents a localized group of <b>neurons.</b> <b>Computer</b> studies reveal that typical networks composed of {{a small number of}} trions (with symmetric interactions) exhibit striking behavior [...] e. g., hundreds to thousands of quasistable, periodic firing patterns, any of which can be selected out and enhanced with only small changes in interaction strengths by using a Hebb-type algorithm...|$|R
40|$|Abstract: In this paper, an {{improved}} modeling based on Elman neural network was proposed {{to analyze the}} nonlinear features of nonlinear circuits with the memory effect. The input vector of the hidden layer in neural network is normalized to enhance the neural network convergence precision. A group of Chebyshev orthogonal basis functions was employed to activate hidden layer <b>neurons.</b> <b>Computer</b> simulation results of the nonlinear power amplifier (PA) {{have shown that the}} proposed behavioral modeling not only accurately describes the nonlinear distortions of PAs, but also well depicts memory effect of PAs. And, the proposed approach could be also applied to analyze linear circuits and RF power amplifiers...|$|R
40|$|In this study, {{we propose}} a network model with input of {{bio-metric}} {{data to the}} hidden layer neurons of the Multi-Layer Perceptron (MLP). The injecting of the noise {{is known to be}} effective to the improvement of the performance of the MLP. We inject the biological pulse wave as a source of this noise. We investigate the performance of the MLP when the sam-pled data of biological pulse waves obtained from real bio-logical experiments are injected into the <b>neurons.</b> <b>Computer</b> simulated results show that the performance of the biological pulse wave is better than the case of random noise. Further, we investigated the characteristics of the proposed method, 1...|$|R
40|$|It {{has been}} shown that a nonmonotone neural network model can {{recognize}} spatiotemporal patterns without expanding them into spatial patterns. We improve the recognition ability of this model by introducing hidden neurons. We also show a simple method of training the hidden <b>neurons.</b> <b>Computer</b> simulation shows that this model can recognize complicated spatiotemporal patterns. KEYWORDS: recognition, spatiotemporal pattern, nonmonotone neural network, hidden neuron. 1. INTRODUCTION We previously proposed a nonmonotone neural network model which recognizes spatiotemporal patterns without expanding them into spatial patterns [1]. This model has simple structure and learning algorithm and can recognize learned patterns even if they are temporally extended or contracted. However, this model works inadequately unless the input patterns have simple structures, that is, their trajectories in the pattern space are rather short and not so much intertwined. This is because the input and output pat [...] ...|$|R
40|$|The {{present study}} {{describes}} significant, selective age-related {{changes in the}} rat inferior colliculus (IC) using two measures of GABA neurotransmission. An area incorporating the central nucleus of the IC of the Fisher 344 rat was studied using an antibody against a GABA conjugate to immuno-label neurons and terminals in the IC. Using micropunches of the CIC the spontaneous and K+ evoked release of assorted of NTS was measured. Paired young adult (2 - 8 mo.) and aged (18 - 29 mo.) Fisher 344 rats were studied in order to compare numbers of GABA positive <b>neurons.</b> <b>computer</b> morphometry was used to generate maps of the ventral lateral portion of the IC. Both labeled puncta and neurons were observed throughout the central nucleus of the IC. Immunostained terminals were observed on {{both large and small}} positive neurons as well as on negative neurons...|$|R
40|$|Our {{knowledge}} of the cerebral cortex is increasing at an accelerating pace as new anatomical and physiological techniques become available. It is now possible, with whole-cell patch recording (Hamill et al., 1991), to obtain intimate {{knowledge of}} single cortical neurons in vivo. Yet, cortical neurons are embedded within circuits that entwine the intrinsic properties of single neurons as they interact through synaptic actions. New properties emerge from these network interactions {{that are difficult to}} predict from the properties of the individual <b>neurons.</b> <b>Computer</b> models of <b>neurons</b> and their synaptic interactions may help us dissect these emergent properties in conjunction with experimental studies. Our models of cortical neurons are based on the pioneering model of the action potential in the squid giant axon by Hodgkin and Huxley (1952) and the dendritic models of Rall (1967). In these models, each element of the neuron is treated as an electrical element having resistive and capacitative properties, and membrane conductances are modeled b...|$|R
40|$|Monolithic GaAs {{optoelectronic}} {{integrated circuits}} developed {{for use as}} artificial <b>neurons.</b> Neural-network <b>computer</b> contains planar arrays of optoelectronic neurons, and variable synaptic connections between neurons effected by diffraction of light from volume hologram in photorefractive material. Basic principles of neural-network computers explained more fully in "Optoelectronic Integrated Circuits For Neural Networks" (NPO- 17652). In present circuits, devices replaced by metal/semiconductor field effect transistors (MESFET's), which consume less power...|$|R
40|$|Daily {{patterns}} of behavior and physiology in animals in temperate zones often differ substantially between summer and winter. In mammals, {{this may be a}} direct consequence of seasonal changes of activity of the suprachiasmatic nucleus (SCN). The {{purpose of this study was}} to understand such variation on the basis of the interaction between pacemaker <b>neurons.</b> <b>Computer</b> simulation demonstrates that mutual electrical activation between pacemaker cells in the SCN, in combination with cellular electrical activation by light, is sufficient to explain a variety of circadian phenomena including seasonal changes. These phenomena are: self-excitation, that is, spontaneous development of circadian rhythmicity in the absence of a light-dark cycle; persistent rhythmicity in constant darkness, and loss of circadian rhythmicity in pacemaker output in constant light; entrainment to light-dark cycles; aftereffects of zeitgeber cycles with different periods; adjustment of the circadian patterns to day length; generation of realistic phase response curves to light pulses; and relative independence from day-to-day variation in light intensity. In the model, subsets of cells turn out to be active at specific times of day. This is of functional importance for the exploitation of the SCN to tune specific behavior to specific times of day. Thus, a network of on-off oscillators provides a simple and plausible construct that behaves as a clock with readout for time of day and simultaneously as a clock for all seasons. ...|$|R
40|$|How the axonal {{distribution}} of Na+ channels affects {{the precision of}} spike timing is not well understood. We addressed this question in auditory relay neurons of the avian nucleus magnocellularis. These neurons encode and convey information about the fine structure of sounds {{to which they are}} tuned by generating precisely timed action potentials in response to synaptic inputs. Patterns of synaptic inputs differ as a function of tuning. A small number of large inputs innervate high- and middle-frequency neurons, while a large number of small inputs innervate low-frequency neurons. We found that the distribution and density of Na+ channels in the axon initial segments varied with the synaptic inputs, and were distinct in the low-frequency neurons. Low-frequency neurons had a higher density of Na+ channels within a longer axonal stretch, and showed a larger spike amplitude and whole-cell Na+ current than high/middle-frequency <b>neurons.</b> <b>Computer</b> simulations revealed that for low-frequency neurons, a large number of Na+ channels were crucial for preserving spike timing because it overcame Na+ current inactivation and K+ current activation during compound EPSPs evoked by converging small inputs. In contrast, fewer channels were sufficient to generate a spike with high precision in response to an EPSP induced by a single massive input in the high/middle-frequency neurons. Thus the axonal Na+ channel distribution is effectively coupled with synaptic inputs, allowing these neurons to convey auditory information in the timing of firing...|$|R
40|$|To {{investigate}} the morphological and electrical development of <b>neurons</b> using <b>computer</b> models {{it is useful}} to extend the compartmental modelling framework to include the situation where neurites (dendrites and axons) may change their length, diameter and membrane characteristics over time. This requires changes the size and other properties of existing compartments and adding new compartments during a particular computer simulation. We have been concerned with modelling the outgrowth of neurites when elongation and branching are controlled by the concentration of a substance in the growth cones. The substance is produced in the soma and diffuses or is actively transported along the growing neurites. We describe the technical issues involved in the ompartmental modelling of this situation...|$|R
40|$|High-density spike-based {{computing}} {{systems will}} enable memristivebased analog synapse arrays. A synapse {{is essentially a}} programmable wire used to connect groups of neurons together. The human brain possesses approximately 10 billion neurons, {{each of which has}} direct synaptic connections to approximately 10, 000 <b>neurons.</b> Neuromorphic <b>computers</b> aimed at mimicking biological computation, and which have numbers of neurons and synapses approaching biological scale, can be modeled with supercomputers or neural hardware accelerators. However, in order for such neural computing devices to achieve a biologically plausible synaptic density, it is imperative to minimize synaptic size. This feat is challenging because the synaptic weight of each synapse must be stored. Since digital synapse implementations require that several bits of data per synapse be memorized...|$|R
40|$|The I. E. Block Community Lecture {{has become}} a {{highlight}} of the SIAM Annual Meeting; this year’s lecture, given in San Diego on July 11 by Steve Strogatz of Cornell University, was no exception. Titled “Collective Dynamics of Small-world Networks, ” the lecture concerned a loosely defined class of networks that seems to include many if not most of the ones science and industry would like to learn more about. Many of the results presented were first obtained by Strogatz in collaboration with Duncan Watts, then {{a graduate student at}} Cornell and currently at Columbia University (Department of Sociology) and the Santa Fe Institute. Large sparse networks are all around us. Of the six billion people in the world, few are acquainted with more than a thousand others. The “global acquaintance graph ” thus contains fewer than 3 × 1012 edges and is sparse in comparison with the complete graph on six billion nodes, which has about six million times as many edges. A brain, with about ten billion neurons, each connected to perhaps ten thousand others, has a comparably sparse wiring diagram. The Internet links millions of computers, relatively few of which ever communicate directly with one another, meaning that the “Internet connection graph ” is also large and sparse. The foregoing networks are partially random, in that people make unlikely acquaintances, and <b>neurons</b> and <b>computers</b> make chance long-range connections. Yet people know their neighbors, neurons connect to nearby <b>neurons,</b> and <b>computers</b> are linked in local networks far more often than if such connections were completely random. Practically important networks, then, seldom resemble either totally random graphs or the completely regular graphs (such as chains, rings, grids, lattices, trivalent maps, and complete graphs) of traditional graph theory...|$|R
40|$|The somatic {{voltage clamp}} {{technique}} has revolutionized understanding of synaptic physiology and the excitability of <b>neurons.</b> Although <b>computer</b> simulations {{have indicated that}} the somatic voltage clamp poorly controls voltage in the dendritic tree of neurons, {{where the majority of}} synaptic contacts are made, there has not been an experimental description of the performance of the somatic voltage clamp. Here, we directly quantify errors in the measurement of dendritic synaptic input by the somatic voltage clamp using simultaneous whole-cell recordings from the soma and apical dendrite of rat neocortical pyramidal neurons. The somatic voltage clamp did not control voltage at sites other than the soma and distorted measurement of the amplitude, kinetics, slope conductance and reversal potential of synaptic inputs in a dendritic distance–dependent manner. These errors question the use of the somatic voltage clamp as a quantitative tool in dendritic neurons...|$|R
40|$|Abstract—In this study, {{we propose}} Multi-Layer Perceptron (MLP) with pulse glial chain {{including}} neurogenesis. In this network, we one-by-one connect glia unit with neurons in a hidden-layer. The glia generates a pulse {{according to the}} connecting neuron output. This pulse increases {{the threshold of the}} neuron and excites the neighboring glias. The pulse generation frequency is also changed according to the connecting neuron output. Moreover, we introduce a neurogenesis to the network. By the neurogenesis, any neurons are removed and newborn neurons are set to same position. The removed neurons are chosen by the number of pulse generation of the glia, and the newborn neuron has random value in the connection with other neurons. We consider that the unimportant neurons for the learning are removed and the newborn neurons become the important <b>neurons.</b> By <b>computer</b> simulations, we confirm that the proposed MLP obtains a better solving ability than the previous MLP. I...|$|R
40|$|We have {{investigated}} the role that different connectivity regimes play in the dynamics {{of a network of}} Hodgkin-Huxley <b>neurons</b> by <b>computer</b> simulations. The different connectivity topologies exhibit the following features: random topologies give rise to fast system response yet are unable to produce coherent oscillations in the average activity of the network; on the other hand, regular topologies give rise to coherent oscillations, but in a temporal scale that is not in accordance with fast signal processing. Finally, small-world topologies, which fall between random and regular ones, take advantage of the best features of both, giving rise to fast system response with coherent oscillations. We acknowledge G. Laurent, A. Bäcker, M. Bazhenov, M. Rabinovich, and H. Abarbanel for insightful discussions. We thank the Dirección General de Enseñanza Superior e Investigación Científica for financial support (PB 97 - 1448), the CAM for financial support to L. F. L., and the CCCFC (UAM) for the use of computation resources...|$|R
40|$|We have {{investigated}} the role that different connectivity regimes play on the dynamics {{of a network of}} Hodgkin-Huxley <b>neurons</b> by <b>computer</b> simulations. The different connectivity topologies exhibit the following features: random connectivity topologies give rise to fast system response yet are unable to produce coherent oscillations in the average activity of the network; on the other hand, regular connectivity topologies give rise to coherent oscillations and temporal coding, but in a temporal scale that is not in accordance with fast signal processing. Finally, small-world (SW) connectivity topologies, which fall between random and regular ones, take advantage of the best features of both, giving rise to fast system response with coherent oscillations along with reproducible temporal coding on clusters of neurons. Our work is the first, {{to the best of our}} knowledge, to show the need for a small-world topology in order to obtain all these features in synergy within a biologically plausible time scale. Comment: 4 pages, 5 figure...|$|R
40|$|We apply genetic {{algorithms}} to the Hopfield model of associative memory. Previously, we {{reported that a}} genetic algorithm evolves a network with random synaptic weights to store eventually a set of random patterns. In this paper, we present the Baldwin effect on the evolution enhances the storage capacity. 1 Introduction Associative memory is {{a system in which}} an incomplete or a noisy input of stored patterns should result in the retrieval in its complete form. The errorcorrecting capability is due to its distributed storage of the patterns. We are exploring basic behaviors of the associative memory under simple evolutionary processes. In 1982, Hopfield [1] proposed a neural network model of the associative memory. He used the Hebbian rule [2] to make connection matrix store a set of patterns. He estimated the storage capacity to be at most 15 % of the number of <b>neurons</b> with <b>computer</b> simulations. Later the capacity was verified analytically by Amit et al. [3] with spin-glass theory. Si [...] ...|$|R
40|$|There {{have been}} a lot of {{researches}} which apply evolutionary techniques to layered neural networks. However, their applications to Hopfield neural networks remain few so far. We {{have been a}}pplying Genetic Algorithms to fully connected associative memory model of Hopfield, and reported elsewhere that the network can store some number of patterns only by evolving weight matrices with a simple Genetic Algorithm. In this paper we present that the memory capacity can be enhanced by incorporating Lamarckian inheritance to the Genetic Algorithm. I. Introduction In 1982 Hopfield [1] proposed a neural network model as an associative memory system. He used Hebbian rule [2] to make connection matrix store a set of patterns. He estimated the memory capacity to be at most 15 % of the number of <b>neurons</b> with <b>computer</b> simulations. Since then many researchers have been trying to enlarge the memory capacity in various ways (see e. g., [3]). Here we are trying that by means of a Genetic Algorithm. Genetic [...] ...|$|R
40|$|Single spikes {{and their}} timing matter in {{changing}} synaptic efficacy, {{which is known}} as spike-timing-dependent plasticity (STDP). Most previous studies treated spikes as all-or-none events, and considered their duration and magnitude as negligible. Here we explore the effects of action potential (AP) duration on synaptic plasticity in a simplified model <b>neuron</b> using <b>computer</b> simulations. We propose a novel STDP model that depresses synapses using an AP duration dependent LTD window and induces potentiation of synaptic strength when presynaptic spikes arrive before and during a postsynaptic AP (dSTDP). We demonstrate that AP duration is another key factor for insensitizing the postsynaptic neural firing and for controlling the shape of synaptic weight distribution. Extended AP durations produce a wide unimodal weight distribution that resembles the ones reported experimentally and make the postsynaptic neuron tranquil when disturbed by poisson noise spike trains, while equivalently sensitive to the synchronized. Our {{results suggest that the}} impact of AP duration, modeled here as an AP-dependent STDP window, on synaptic plasticity can be dramatic and should motivate future STDP studies...|$|R
40|$|This study aims at proposing an {{implementation}} of regularization mechanisms compatible with biological operators. More precisely, cortical maps code vectorial parametric quantities, computed by network of <b>neurons.</b> In <b>computer</b> vision, similar quantities are efficiently computed using implementations of partial differential equations which define regularization processes allowing to obtain well-defined estimations of these quantities. One of these methods, introduced by Raviat and developed by Degond and Mas-Gallic, {{is based on}} an integral approximation of the diffusion operator used in regularization mechanisms. Following this formulation, the present development defines a somehow optimal {{implementation of}} such an integral operator with two interesting properties: (i) when used on sampled data such as image pixels or 3 D data voxels, it provides an unbiased discrete implementation of such an operator; when used as a model of biological plausible mechanisms, it corresponds to a simple local feedback defined over a small bounded region of any shape inside the parametric space. As such it may be linked to what is processed in a cortical column of the brain and provides an interesting model of general operators corresponding to such a neuronal structure. The present development is illustrated by some experiments of visual motion estimation...|$|R
40|$|Neurons make {{synaptic}} connections at {{locations where}} axons and dendrites are sufficient close in space. Typically the required proximity {{is based on}} the dimensions of dendritic spines and axonal boutons. Based on this principle one can search those locations in networks formed by reconstructed <b>neurons</b> or <b>computer</b> generated <b>neurons.</b> Candidate synapses are then located where axons and dendrites are within a given criterion distance from each other. Both experimentally reconstructed and model generated neurons are usually represented morphologically by piece wise linear structures (line pieces or cylinders). Proximity tests are then to be performed on all pairs of line pieces from both axonal and dendritic branches. Applying just a test on the distance between line pieces may result in local clusters of synaptic sites when more than one pair of nearby line pieces from axonal and dendritic branches is sufficient close, and may introduce a dependency on the length scale of the individual line pieces. The present paper describes a new algorithm for defining locations of candidate synapses which {{is based on the}} crossing requirement of a line piece pair, while the length of the orthogonal distance between the line pieces is subjected to the distance criterion for testing 3 D proximit...|$|R
30|$|For many years, {{groups have}} been trying to find ways of {{accurately}} recreating the highly ramified shapes of <b>neurons</b> on a <b>computer.</b> For example, the Trees toolbox [10] creates dendrites by first distributing points in 3 D space according to the density obtained by overlapping many examples of a given cell type, and then uses a minimal spanning tree to connect the points in a stochastic way that yet reproduces the connectivity and appearance of various neuron classes. The NetMorph algorithm [11] grows each neurite as a quasi-random walk in space where the next point is chosen by summing up forces that reflect biophysical properties such as microtubule-based neurite stiffness and biochemical processes that lead to bifurcations. Luczak [12] has used a Diffusion Limited Aggregation scheme with a spatially imhomogeneous distribution of diffusing particles within a prescribed volume to create distinct neuronal dendritic shapes.|$|R
40|$|AbstractAn upper {{ontology}} {{tries to}} express theoretical and common concepts which are same across domains. A fundamental upper ontology defining {{the concept of}} reality is devised here and {{is referred to as}} super ontology. This super ontology is based on the fundamental beliefs and is independent of any specific domain of application and other applications to which it will be put to use in future. We have described the structure of the universe which consists of six substances that are called dravyas. The super ontology will use a unique representation scheme for representing knowledge in its knowledge base having a uniform code of structure of an Extended Hierarchical Censored Production Rule (EHCPR). The basic idea behind the EHCPRs is to simulate densely interconnected <b>neurons</b> inside a <b>computer</b> memory (through a set of interconnected EHCPRs here) so as to make the system learn things, recognize patterns, record changes and make decisions in real time...|$|R
40|$|Stochastic resonance(SR) {{has been}} {{observed}} to improve detection of subthreshold signals with additive noise(or fluctuations) in a single or an array of nonlinear threshold systems like neurons. However, it has been unclear how SR affects information transfer of supra-threshold neural stimuli in the central nervous system. The objective is to observe whether information transmission of supra-threshold stimuli can be improved by uncorrelated noise {{in an array of}} hippocampal CA 1 <b>neuron</b> models through <b>computer</b> simulations. Mutual information was estimated as an index of information transmission from the population of spike trains. Results show that the mutual information was maximized at specific noise amplitude in the array of larger number of neuron models(N> 4) with supra-threshold SS(SSR). Moreover, it is shown that SSR could be observed at lower mean frequency(< 20 Hz) of the input signals. In conclusion, SSR could {{play an important role in}} information processing such as memory formation in hippocampal neurons...|$|R
40|$|Michio Kaku, the New York Times bestselling {{author of}} Physics of the Impossible and Physics of the Future tackles the most {{fascinating}} and complex object in the known universe: the human brain. The Future of the Mind brings a topic that once belonged solely to the province of science fiction into a startling new reality. This scientific tour de force unveils the astonishing research being done in top laboratories around the world—all based on the latest advancements in neuroscience and physics—including recent experiments in telepathy, mind control, avatars, telekinesis, and recording memories and dreams. The Future of the Mind is an extraordinary, mind-boggling exploration of the frontiers of neuroscience. Dr. Kaku looks toward the day when we may achieve the ability to upload the human brain to a <b>computer,</b> <b>neuron</b> for neuron; project thoughts and emotions {{around the world on}} a brain-net; take a “smart pill” to enhance cognition; send our consciousness across the universe; and push the very limits of immortality...|$|R
40|$|The {{abilities}} of neuronal populations to encode rapidly varying stimuli and respond quickly to abrupt input changes are crucial for basic neuronal computations, such as coincidence detection, grouping by synchrony, and spike-timing-dependent plasticity, {{as well as}} for the processing speed of neuronal networks. Theoretical analyses have linked these abilities to the fast-onset dynamics of action potentials (APs). Using a combination of whole-cell recordings from rat neocortical <b>neurons</b> and <b>computer</b> simulations, we provide the first experimental evidence for this conjecture and prove its validity for the case of distal AP initiation in the axon initial segment (AIS), typical for cortical neurons. Neocortical neurons with fast-onset APs in the soma can phase-lock their population firing to signal frequencies up to ∼ 300 – 400 Hz and respond within 1 – 2 ms to subtle changes of input current. The ability to encode high frequencies and response speed were dramatically reduced when AP onset was slowed by experimental manipulations or was intrinsically slow due to immature AP generation mechanisms. Multicompartment conductance-based models reproducing the initiation of spikes in the AIS could encode high frequencies only if AP onset was fast at the initiation site (e. g., attributable to cooperative gating of a fraction of sodium channels) but not when fast onset of somatic AP was produced solely by backpropagation. We conclude that fast-onset dynamics is a genuine property of cortical AP generators. It enables fast computations in cortical circuits that are rich in recurrent connections both within each region and across the hierarchy of areas...|$|R
40|$|Spinal lamina I {{receives}} nociceptive primary afferent {{input to}} project through diverse ascending pathways, including the anterolateral tract (ALT). Large projection neurons (PNs) form {{only a few}} per cent of the cell population in this layer, and {{little is known about}} their local input from other lamina I neurons. We combined single-cell imaging in the isolated spinal cord, paired recordings, 3 -D reconstructions of biocytin-labelled <b>neurons</b> and <b>computer</b> simulations to study the monosynaptic input to large ALT-PNs from neighbouring (somata separated by less than 80 μm) large lamina I neurons. All 11 connections identified were excitatory. We have found that an axon of a presynaptic neuron forms multiple synapses on an ALT-PN, and both Ca 2 +-permeable and Ca 2 +-impermeable AMPA receptors are involved in transmission. The monosynaptic EPSC latencies (1 – 12 ms) are determined by both post- and presynaptic factors. The postsynaptic delay, resulting from the electrotonic EPSC propagation in the dendrites of an ALT-PN, could be 4 ms at most. The presynaptic delay, caused by the spike propagation in a narrow highly branched axon of a local-circuit neuron, can be about 10 ms for neighbouring ALT-PNs and longer for more distant neurons. In many cases, the EPSPs evoked by release from a lamina I neuron were sufficient to elicit a spike in an ALT-PN. Our data show that ALT-PNs can receive input from both lamina I local-circuit neurons and other ALT-PNs. We suggest that lamina I is a functionally interconnected layer. The intralaminar network described here can amplify the overall output from the principal spinal nociceptive projection area...|$|R
40|$|An animal's {{ability to}} {{navigate}} through space rests on {{its ability to}} create a mental map of its environment. The hippocampus is the brain region centrally responsible for such maps, and it has been assumed to encode geometric information (distances, angles). Given, however, that hippocampal output consists of patterns of spiking across many neurons, and downstream regions must be able to translate those patterns into accurate information about an animal's spatial environment, we hypothesized that 1) the temporal pattern of neuronal firing, particularly co-firing, is key to decoding spatial information, and 2) since co-firing implies spatial overlap of place fields, a map encoded by co-firing will be based on connectivity and adjacency, i. e., it will be a topological map. Here we test this topological hypothesis with a simple model of hippocampal activity, varying three parameters (firing rate, place field size, and number of <b>neurons)</b> in <b>computer</b> simulations of rat trajectories in three topologically and geometrically distinct test environments. Using a computational algorithm based on recently developed tools from Persistent Homology theory in the field of algebraic topology, we find that the patterns of neuronal co-firing can, in fact, convey topological information about the environment in a biologically realistic length of time. Furthermore, our simulations reveal a "learning region" that highlights the interplay between the parameters in combining to produce hippocampal states that are more or less adept at map formation. For example, within the learning region a lower number of neurons firing can be compensated by adjustments in firing rate or place field size, but beyond a certain point map formation begins to fail. We propose that this learning region provides a coherent theoretical lens through which to view conditions that impair spatial learning by altering place cell firing rates or spatial specificity...|$|R
40|$|Extent: 14 p. An animal's {{ability to}} {{navigate}} through space rests on {{its ability to}} create a mental map of its environment. The hippocampus is the brain region centrally responsible for such maps, and it has been assumed to encode geometric information (distances, angles). Given, however, that hippocampal output consists of patterns of spiking across many neurons, and downstream regions must be able to translate those patterns into accurate information about an animal's spatial environment, we hypothesized that 1) the temporal pattern of neuronal firing, particularly co-firing, is key to decoding spatial information, and 2) since co-firing implies spatial overlap of place fields, a map encoded by co-firing will be based on connectivity and adjacency, i. e., it will be a topological map. Here we test this topological hypothesis with a simple model of hippocampal activity, varying three parameters (firing rate, place field size, and number of <b>neurons)</b> in <b>computer</b> simulations of rat trajectories in three topologically and geometrically distinct test environments. Using a computational algorithm based on recently developed tools from Persistent Homology theory in the field of algebraic topology, we find that the patterns of neuronal co-firing can, in fact, convey topological information about the environment in a biologically realistic length of time. Furthermore, our simulations reveal a “learning region” that highlights the interplay between the parameters in combining to produce hippocampal states that are more or less adept at map formation. For example, within the learning region a lower number of neurons firing can be compensated by adjustments in firing rate or place field size, but beyond a certain point map formation begins to fail. We propose that this learning region provides a coherent theoretical lens through which to view conditions that impair spatial learning by altering place cell firing rates or spatial specificity. Y. Dabaghian, F. Mémoli, L. Frank, G. Carlsso...|$|R
40|$|This paper {{presents}} {{the effects of}} spontaneous random activity on information transmission in an auditory brain stem <b>neuron</b> model. In <b>computer</b> simulations, the supra-threshold synaptic current stimuli ascending from auditory nerve fibers (ANFs) were modeled by a filtered inhomogeneous Poisson process modulated by sinusoidal functions at a frequency of 220 – 3520 Hz {{with regard to the}} human speech spectrum. The stochastic sodium and stochastic high- and low-threshold potassium channels were incorporated into a single compartment model of the soma in spherical bushy neurons, so as to realize threshold fluctuations or a variation of spike firing times. The results show that the information rates estimated from the entropy of inter-spike intervals of spike trains tend toward a convex function of the spontaneous rates when the intensity of sinusoidal functions decreases. Furthermore, the results show that a convex function of the spontaneous rates tends to disappear as the frequency of the sinusoidal function increases, such that the phase-locked response can be unobserved. It is concluded that this sort of stochastic resonance (SR) phenomenon, which depends on the spontaneous rates with supra-threshold stimuli, can better enhance information transmission in a smaller intensity of sinusoidal functions within the human speech spectrum, like the situation in which the regular SR can enhance weak signals...|$|R
