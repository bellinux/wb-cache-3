3|10000|Public
40|$|Abstract. We {{consider}} {{the problem of}} testing whether a given function f: Fnq → Fq is close to an n-variate degree d polynomial over the finite field Fq of q elements. The natural, low-query test for this property would be to first pick the smallest dimension t = tq,d ≈ d/q such that every function of degree greater than d reveals this aspect on some t-dimensional affine subspace of Fnq. Then, one would test that f when restricted to a random t-dimensional affine subspace is a polynomial of degree at most d on this subspace. Such a test makes only qt queries, independent of <b>n.</b> <b>Previous</b> <b>works,</b> by Alon et al. [IEEE Trans. Inform. Theory, 51 (2005), pp. 4032 – 4039], Kaufman and Ro...|$|E
40|$|Abstract — We {{consider}} {{the problem of}} testing if a given function f: F n q → Fq is close to a n-variate degree d polynomial over the finite field Fq of q elements. The natural, low-query, test for this property would be to pick the smallest dimension t = tq,d ≈ d/q such that every function of degree greater than d reveals this aspect on some t-dimensional affine subspace of F n q and to test that f when restricted to a random t-dimensional affine subspace is a polynomial of degree at most d on this subspace. Such a test makes only q t queries, independent of <b>n.</b> <b>Previous</b> <b>works,</b> by Alon et al. [1], and Kaufman and Ron [6] and Jutla et al. [5], showed that this natural test rejected functions that were Ω(1) -far from degree d-polynomials with probability at least Ω(q −t). (The initial work [1] considered only the case of q = 2, while the work [5] only considered the case of prime q. The results in [6] hold for all fields.) Thus to get a constant probability o...|$|E
40|$|We {{consider}} {{the problem of}} testing if a given function f: Fn q → Fq is close to a n-variate degree d polynomial over the finite field Fq of q elements. The natural, low-query, test for this property would be to pick the smallest dimension t = tq,d ≈ d/q such that every function of degree greater than d reveals this feature on some t-dimensional affine subspace of Fn q and to test that f when restricted to a random t-dimensional affine subspace is a polynomial of degree at most d on this subspace. Such a test makes only q t queries, independent of <b>n.</b> <b>Previous</b> <b>works,</b> by Alon et al. [AKK + 05], and Kaufman and Ron [KR 06] and Jutla et al. [JPRZ 04], showed that this natural test rejected functions that were Ω(1) -far from degree d-polynomials with probability at least Ω(q −t) (the results of [KR 06] hold for all fields Fq, while the results of [JPRZ 04] hold only for fields of prime order). Thus to get a constant probability of detecting functions that were at constant distance from the space of degree d polynomials, the tests made q 2 t queries. Kaufman and Ron also noted that when q is prime, then q t queries are necessary. Thus these tests were off by at least a quadratic factor from known lower bounds. It was unclear if the soundness analysis of these tests were tight and this question relates closely to the tas...|$|E
40|$|The {{ability to}} perform {{reliable}} early estimation is essential to support early design space ex- ploration. In this paper we present a tight upper bound for the area of slicing floorplans. The proposed formulation outperforms the best known bounds {{on a set of}} practical/realistic floorplan- <b>ning</b> scenarios. <b>Previous</b> <b>work</b> has shown that slicing floorplans can be quite efficient in packing modules tightly, and thus the relevance of our proposed area estimate...|$|R
40|$|The {{problem of}} network-constrained {{averaging}} is {{to compute the}} average {{of a set of}} values distributed throughout a graph G using an algorithm that can pass messages only along graph edges. We study this problem in the noisy setting, in which the communication along each link is modeled by an additive white Gaussian noise channel. We propose a two-phase decentralized algorithm, and we use stochastic approximation methods in conjunction with the spectral graph theory to provide concrete (non-asymptotic) bounds on the mean-squared error. Having found such bounds, we analyze how the number of iterations T_G(n; δ) required to achieve mean-squared error δ scales {{as a function of the}} graph topology and the number of nodes <b>n.</b> <b>Previous</b> <b>work</b> provided guarantees with the number of iterations scaling inversely with the second smallest eigenvalue of the Laplacian. This paper gives an algorithm that reduces this graph dependence to the graph diameter, which is the best scaling possible...|$|R
40|$|We {{introduce}} {{and give}} a complete description of a new graph {{to be used for}} DNA sequencing questions. This graph has the advantage over the classical de Bruijn graph that it fully accounts for the double stranded nature of DNA, rather than dealing with single strands. Technically, our graph may be thought of as the quotient of the de Bruijn graph under the natural involution of sending a DNA strand to its complementary strand. However, this involution has fixed points, and this complicates the structure of the quotient graph which we have therefore modified herein. As an application and motivating example, we give an efficient algorithm for constructing universal footprinting templates for n-mers. This problem may be formulated as the task of finding a shortest possible segment of DNA which contains every possible sequence of base pairs of some fixed length <b>n.</b> <b>Previous</b> <b>work</b> by Kwan et al has attacked this problem from a numerical point of view and generated minimal length universal footprinting templates for n= 2, 3, 5, 7, together with unsubstantiated candidates for the case n= 4. We show that their candidates for n= 4 are indeed minimal length universal footprinting templates...|$|R
40|$|We {{construct}} automata over {{a binary}} alphabet with 2 n states, n≥ 2, whose states freely generate a free group of rank 2 <b>n.</b> Combined with <b>previous</b> <b>work,</b> {{this shows that}} a free group of every finite rank can be generated by finite automata over a binary alphabet. We also construct free products of cyclic groups of order two via such automata...|$|R
40|$|Simple group-theoretical {{arguments}} {{are used to}} demonstrated that in the high temperature (chirally restored) phase of QCD with N massless flavours, all n-point correlation functions of quark bilinears are invariant under U(1) axial transformations provided <b>n</b> 2. Unlike <b>previous</b> <b>work,</b> this result {{does not depend on}} the topological properties of QCD and can be formulated without explicit reference to functional integrals. Comment: 3 pages, RevTe...|$|R
40|$|International audienceThe {{design of}} a knee joint is a key issue in {{robotics}} and biomechanics to improve the compatibility between prosthesis and human movements and to improve the bipedal robot performances. We propose a novel design for the knee joint of a planar bipedal robot, based on a four-bar linkage. <b>n</b> <b>previous</b> a <b>work,</b> we have proved a bipedal robot with four-bar knees has a less energy consumption than a bipedal robot equipped of revolute knee joints for walking gates composed of single support phases separated by impulsive impact. Our objective in this work is to extend this result for walking motion which included double support phase with a feet rotation...|$|R
40|$|We {{analyze the}} {{dynamics}} of a class of Z_ 2 n-equivariant differential equations of the form ż=pz^n- 1 z̅^n- 2 +sz^nz̅^n- 1 -z̅^ 2 n- 1, where z is complex, the time t is real, while p and s are complex parameters. This study is the generalisation to Z_ 2 <b>n</b> of <b>previous</b> <b>works</b> with Z_ 4 and Z_ 6 symmetry. We reduce the problem of finding limit cycles to an Abel equation, and provide criteria for proving in some cases uniqueness and hyperbolicity of the limit cycle that surrounds either 1, 2 n+ 1 or 4 n+ 1 equilibria, the origin being always one of these points...|$|R
40|$|Let (ℍn,g) be the {{hyperbolic}} {{space of}} dimension <b>n.</b> By our <b>previous</b> <b>work</b> (Theorem 2. 3 of (Yang (2012))), for any 0 0 depending only on n and α such that supu∈W 1,n(ℍn),∥u∥ 1,τ≤ 1 ∫ℍn(eαun/(n- 1) -∑k= 0 n- 2 αk|u|nk/(n- 1) /k!) dvg 0, the above mentioned inequality holds with {{the definition of}} u 1,τ replaced by (∫ℍn‍(|∇gu|n+τ|u|n) dvg) 1 /n. We solve this problem by gluing local uniform estimates...|$|R
40|$|Given any linear {{threshold}} function f on n Boolean variables, we construct a linear {{threshold function}} g which disagrees with f on at most an ɛ fraction of inputs and has integer weights each of magnitude at most √ n · 2 Õ(1 /ɛ 2). We {{show that the}} construction is optimal {{in terms of its}} dependence on n by proving a lower bound of Ω (√ n) on the weights required to approximate a particular linear threshold function. We give two applications. The first is a deterministic algorithm for approximately counting the fraction of satisfying assignments to an instance of the zero-one knapsack problem to within an additive ±ɛ. The algorithm runs in time polynomial in n (but exponential in 1 /ɛ 2). In our second application, we show that any linear threshold function f is specified to within error ɛ by estimates of its Chow parameters (degree 0 and 1 Fourier coefficients) which are accurate to within an additive ± 1 /(n · 2 Õ(1 /ɛ 2)). This is the first such accuracy bound which is inverse polynomial in <b>n</b> (<b>previous</b> <b>work</b> of Goldberg [12] gave a 1 /quasipoly(n) bound), and gives the first polynomial bound (in terms of n) on the number of examples required for learning linear threshold functions in the “restricted focus of attention ” framework...|$|R
40|$|Building upon work of Clozel, Harris, Shepherd-Barron, and Taylor, {{this paper}} shows that certain Galois {{representations}} become automorphic after one makes a suitably large totally-real extension {{to the base}} field. The main innovation {{here is that the}} result applies to Galois representations to GL_ 2 <b>n,</b> where <b>previous</b> <b>work</b> dealt with representations to GSp_n. The main technique is the consideration of the cohomology the Dwork hypersurface, and in particular, of pieces of this cohomology other than the invariants under the natural group action. Comment: 37 pages, 1 figure; essentially final version, to appear in Journal für die reine und angewandte Mathematik (Crelle's Journal). This version does not incorporate any minor changes (e. g. typographical changes) made in proo...|$|R
40|$|Abstract. The max k-armed bandit {{problem is}} a recently-introduced online {{optimization}} problem with practical applications to heuristic search. Given a set of k slot machines, each yielding payoff from a fixed (but unknown) distribution, we wish to allocate trials to the machines so as to maximize the maximum payoff received over a series of <b>n</b> trials. <b>Previous</b> <b>work</b> on the max k-armed bandit problem has assumed that payoffs are drawn from generalized extreme value (GEV) distributions. In this paper we present a simple algorithm, based on an algorithm for the classical k-armed bandit problem, that solves the max k-armed bandit problem effectively without making strong distributional assumptions. We demonstrate the effectiveness of our approach by applying it {{to the task of}} selecting among priority dispatching rules for the resource-constrained project scheduling problem with maximal time lags (RCPSP/max). ...|$|R
40|$|HOSVD) is a {{possible}} generalization of the Singular Value Decomposition (SVD) to tensors, which have been successfully applied in various domains. Unfortunately, this decomposition is computationally demanding. Indeed, the HOSVD of a N th-order tensor involves the computation of the SVD of <b>N</b> matrices. <b>Previous</b> <b>works</b> have shown {{that it is possible}} to reduce the complexity of HOSVD for third-order structured tensors. These methods exploit the columns redundancy, which is present in the mode of structured tensors, especially in Hankel tensors. In this paper, we propose to extend these results to fourth order Hankel tensor. We propose two ways to extend Hankel structure to fourth order tensors. For these two types of tensors, a method to build a reordered mode is proposed, which highlights the column redundancy and we derive a fast algorithm to compute their HOSVD. Finally we show the benefit of our algorithms in terms of complexity. I...|$|R
40|$|Systematic {{inaccuracy}} {{is inherent}} in any computational estimate of a non-linear average, such as the free energy difference (Delta-F) between two states or systems, because {{of the availability of}} only a finite number of data values, <b>N.</b> In <b>previous</b> <b>work,</b> we outlined the fundamental statistical description of this ``finite-sampling error. '' We now give a more complete presentation of (i) rigorous general bounds on the free energy and other nonlinear averages, which underscore the universality of the phenomenon; (ii) asymptotic N->infinity expansions of the average behavior of the finite-sampling error in Delta-F estimates; (iii) illustrative examples of large-N behavior, both in free-energy and other calculations; and (iv) the universal, large-N relation between the average finite-sampling error and the fluctuation in the error. An explicit role is played by Levy and Gaussian limiting distributions. Comment: Version to appear in J. Stat. Phys. [...] only minor correction...|$|R
40|$|International audienceThe Higher-Order Singular Value Decomposition (HOSVD) is a {{possible}} generalization of the Singular Value Decomposition (SVD) to tensors, which have been successfully applied in various domains. Unfortunately, this decomposition is computationally demanding. Indeed, the HOSVD of a Nth- order tensor involves the computation of the SVD of <b>N</b> matrices. <b>Previous</b> <b>works</b> have shown {{that it is possible}} to reduce the complexity of HOSVD for third-order structured tensors. These methods exploit the columns redundancy, which is present in the mode of structured tensors, especially in Hankel tensors. In this paper, we propose to extend these results to fourth order Hankel tensor. We propose two ways to extend Hankel structure to fourth order tensors. For these two types of tensors, a method to build a reordered mode is proposed, which highlights the column redundancy and we derive a fast algorithm to compute their HOSVD. Finally we show the benefit of our algorithms in terms of complexity...|$|R
40|$|The {{disparity}} in {{the prices of}} slaves in the antebellum south {{could have come from}} two sources, {{disparity in}} the observed characteristics of the slaves, or disparity in the unobserved valuations of the slave buyers. I use standard hedonic regression techniques to rid the Fogel and Engerman (1974) dataset of the effects of slave heterogeneity. Then, using structural-econometric techniques, I estimate that the most likely number of bidders that could have induced the remaining price dispersion is between six and thirteen. Although far from conclusive, this supports the argument that slave auctions were efficient. JEL Classifications: N 31, N 81, <b>N</b> 91 <b>Previous</b> <b>work</b> on the efficiency of slavery has tended to focus on issues of productive efficiency, comparing the output of agricultural slave labor to that of free labor (for example. Fogel and Engerman 1974). They have attempted to paint slave owners as rationally maximizing profit, so that a slave’s price is equal to his margina...|$|R
40|$|A {{new method}} for {{constructing}} minimum-redundancy binary prefix codes is described. Our method does not explicitly build a Huffman tree; instead {{it uses a}} property of optimal prefix codes to compute the codeword lengths corresponding to the input weights. Let n be the number of weights and k be the number of distinct codeword lengths as produced by the algorithm for the optimum codes. The running time of our algorithm is O(k · <b>n).</b> Following our <b>previous</b> <b>work</b> in be, no algorithm can possibly construct optimal prefix codes in o(k · n) time. When the given weights are presorted our algorithm performs O(9 ^k ·^ 2 kn) comparisons. Comment: 23 pages, a preliminary version appeared in STACS 200...|$|R
40|$|AbstractWe {{present an}} {{algorithm}} that finds the edge connectivity λ of a graph having n vectices and m edges. The running time is O(λ m log(n 2 /m)) for directed graphs and slightly less for undirected graphs, O(m+λ 2 n log(n/λ)). This improves the previous best time bounds, O(min{mn, λ 2 n 2 }) for directed graphs and O(λn 2) for undirected graphs. We present an algorithm that finds k edge-disjoint arborescences on a directed graph in time O((kn) 2). This improves the previous best time bound, O(kmn + k 3 <b>n</b> 2). Unlike <b>previous</b> <b>work,</b> our approach {{is based on}} two theorems of Edmonds that link these two problems and show {{how they can be}} solved...|$|R
40|$|This paper {{describes}} the discrimination of- conformational pattern classes for protein amino acid residues which were de <b>ned</b> in our <b>previous</b> <b>works.</b> Statistical discriminant analysis technique has been employed {{for the present}} analysis. Each residue was characterized by its peripheral physicochemical environment. The environment was described in a vector representation of which components involve Van der Waals volume, hydrophobic parameter, and partial charges of a carbon atom, hydrogen atom of NH and oxygen atom of C'O of ten neighbor residues (ve neighbors in each terminal side of the target residue). The discriminant functions obtained with 67 proteins taken from the PDB le correctly discriminated 58. 3 % of the residues for their conformational pattern classes. ...|$|R
40|$|Galactose oxidase (EC 1. 1. 3. 9) is a monomeric {{enzyme that}} {{contains}} a single copper ion and catalyses the stereospecific oxidation of primary alcohols to their corresponding aldehydes. The protein contains an unusual covalent thioether bond between a tyrosine, which acts as a radical center during the two-electron reaction, and a cysteine. The enzyme is produced in a precursor form lacking the thioether bond and also possessing an additional 17 -aa pro-sequence at the <b>N</b> terminus. <b>Previous</b> <b>work</b> {{has shown that the}} aerobic addition of Cu 2 + to the precursor is sufficient to generate fully processed mature enzyme. The structure of the precursor protein has been determined to 1. 4 Å, revealing the location of the pro-sequence and identifying structural differences between the precursor and the mature protein. Structural alignment of the precursor and mature forms of galactose oxidase shows that five regions of main chain and some key residues of the active site differ significantly between the two forms. The precursor structure provides a starting point for modeling the chemistry of thioether bond formation and pro-sequence cleavage...|$|R
30|$|Brown coal, {{also known}} as lignite, is an {{alternative}} material with properties that make it appealing {{for use as a}} <b>N</b> fertiliser carrier. <b>Previous</b> <b>work</b> found that brown coal incorporation into soil slightly reduced ammonium availability in one soil, but the study did not look at mineral N leaching or nitrous oxide emissions [13]. One reason that brown coals may differ in their N retention behaviour compared with biochars that unlike most biochars, brown coals are generally acidic rather than alkaline. Furthermore, whereas most biochars are of limited availability and relatively high cost of $ 500 – 3000 per tonne [14, 15], brown coal is readily available in many countries at lower cost, for example, approximately $ 50 per tonne in Australia.|$|R
40|$|This paper {{considers}} {{the problem of}} multi-channel blind image restoration and blur identification. By constructing the blind identification problem into an optimization problem, we propose a subspace decomposition based algorithm to blindly identify the blur functions. The proposed algorithm is inherently the same {{as many of the}} others in the literature, but at significantly reduced computation complexity. Let M be the number of blurred images available, N 1 ×N 2 be the size of the images and L 1 ×L 2 be the size of blur functions, our algorithm has a computation complexity of O(M 2 L 1 2 L 2 2 N 1 N 2), as compared to O(M 4 L 1 2 L 2 2 N 1 <b>N</b> 2) for <b>previous</b> <b>works.</b> The proposed algorithm is therefore more suitable for practical applications. link_to_subscribed_fulltex...|$|R
40|$|The {{superior}} {{properties of}} ZnO {{such as a}} high exciton binding energy combined with a low lasing threshold den-sity [1] and a good resistance to electron [2] and proton [3] irradiation makes it a potential competitor for GaN-based light-emitting devices in the ultraviolet and blue spectral range. To avoid problems like lattice and thermal mis-match homojunctions of n- and p-type ZnO are required. Despite efforts of several groups {{it is still a}} problem to ob-tain p-type ZnO with pronounced high conductivity. Poten-tial candidates for acceptors and thus p-type doping are As [4], P [5], as well as a codoping with Ga and N [6], and primarily <b>N</b> [7]. In <b>previous</b> <b>work</b> we found five additional lines in the low-energy region of the vibrational spectrum of ZnO:N [8], which we assigned to be nitrogen-related local vibra...|$|R
30|$|Here {{we discuss}} {{how to choose}} {{parameters}} (n,q,t,σ) of the SHE scheme suitable (maybe not optimal) for the secure inner product (6) over packed ciphertexts, and we give several parameters of more than 80 -bit security level. For simplicity, we only consider secure inner product between two binary vectors A⃗, B⃗ of length n. In this case, we can pack each of A⃗ and B⃗ into a single ciphertext with our packing method (see the below diagram). In the application scenario of Section 1.1, {{we assume that the}} number m of customer’s ID is smaller than the lattice dimension <b>n</b> (see our <b>previous</b> <b>work</b> [22] for the case m>n), and two binary vectors A⃗ and B⃗ represent purchase history data of items X and Y, respectively (note that it is different from the representation of purchase history data in Figure 1).|$|R
40|$|We {{consider}} N-fold 4 -block decomposable integer programs, which simultaneously generalize N-fold integer {{programs and}} two-stage stochastic integer programs with <b>N</b> scenarios. In <b>previous</b> <b>work</b> [R. Hemmecke, M. Koeppe, R. Weismantel, A polynomial-time algorithm for optimizing over N-fold 4 -block decomposable integer programs, Proc. IPCO 2010, Lecture Notes in Computer Science, vol. 6080, Springer, 2010, pp. 219 [...] 229], it was proved that for fixed blocks but variable N, these integer programs are polynomial-time solvable for any linear objective. We extend this result to the minimization of separable convex objective functions. Our algorithm combines Graver basis techniques with a proximity result [D. S. Hochbaum and J. G. Shanthikumar, Convex separable optimization {{is not much}} harder than linear optimization, J. ACM 37 (1990), 843 [...] 862], which allows us to use convex continuous optimization as a subroutine. Comment: 16 pages. arXiv admin note: substantial text overlap with arXiv: 0911. 405...|$|R
40|$|Let Γ_n be an n× n Haar-invariant {{orthogonal}} matrix. Let Z_n be the p× q upper-left submatrix of Γ_n, where p=p_n and q=q_n are two positive integers. Let G_n be a p× q matrix whose pq {{entries are}} independent standard normals. In this paper {{we consider the}} distance between √(n) Z_n and G_n {{in terms of the}} total variation distance, the Kullback-Leibler distance, the Hellinger distance and the Euclidean distance. We prove that each of the first three distances goes to zero as long as pq/n goes to zero, and not so this rate is sharp in the sense that each distance does not go to zero if (p, q) sits on the curve pq=σ n, where σ is a constant. However, it is different for the Euclidean distance, which goes to zero provided pq^ 2 /n goes to zero, and not so if (p,q) sits on the curve pq^ 2 =σ <b>n.</b> A <b>previous</b> <b>work</b> by Jiang Jiang 06 shows that the total variation distance goes to zero if both p/√(n) and q/√(n) go to zero, and it is not true provided p=c√(n) and q=d√(n) with c and d being constants. One of the above results confirms a conjecture that the total variation distance goes to zero as long as pq/n→ 0 and the distance does not go to zero if pq=σ n for some constant σ. Comment: 42 page...|$|R
40|$|In {{the area}} of {{evolutionary}} theory, a key question is which portions of the genome of a species are targets of natural selection. Genetic hitchhiking is a theoretical concept that has helped to identify various such targets in natural populations. In the presence of recombination, a severe reduction in sequence diversity is expected around a strongly beneficial allele. The site frequency spectrum is an important tool in genome scans for selection and is composed of the numbers S 1,:::,Sn{ 1, where Sk {{is the number of}} single nucleotide polymorphisms (SNPs) present in k from <b>n</b> individuals. <b>Previous</b> <b>work</b> has shown that both the number of low- and high-frequency variants are elevated relative to neutral evolution when a strongly beneficial allele fixes. Here, we follow a recent investigation of genetic hitchhiking using a marked Yule process to obtain an analytical prediction of the site frequency spectrum in a panmictic population at the time of fixation of a highly beneficial mutation. We combine standard results from the neutral case with the effects of a selective sweep. As simulations show, the resulting formula produces predictions that are more accurate than previous approaches for the whole frequency spectrum. In particular, the formula correctly predicts the elevation of low- and high-frequency variants and is significantly more accurate than previously derived formulas for intermediate frequency variants...|$|R
40|$|We {{present a}} precise {{computation}} of the topological susceptibility χ__YM of SU(N) Yang-Mills {{theory in the}} large N limit. The computation is done on the lattice, using high-statistics Monte Carlo simulations with N= 3, 4, 5, 6 and three different lattice spacings. Two major improvements {{make it possible to}} go to finer lattice spacing and larger <b>N</b> compared to <b>previous</b> <b>works.</b> First, the topological charge is implemented through the gradient flow definition; and second, open boundary conditions in the time direction are employed {{in order to avoid the}} freezing of the topological charge. The results allow us to extrapolate the dimensionless quantity t_ 0 ^ 2 χ__YM to the continuum and large N limits with confidence. The accuracy of the final result represents a new quality in the verification of large N scaling. Comment: 7 pages, 3 figures. Presented at the 34 th International Symposium on Lattice Field Theory, 24 - 30 July 2016, University of Southampton, U...|$|R
40|$|International audienceWe {{propose a}} general kinetic and {{hydrodynamic}} description of self-gravitating Brownian particles in d dimensions. We go beyond usual approximations by considering inertial effects and finite <b>N</b> effects while <b>previous</b> <b>works</b> use a mean-field approximation valid {{in a proper}} thermodynamic limit (N→ +∞) and consider an overdamped regime (ξ→ +∞). We recover known models in some particular cases of our general description. We derive {{the expression of the}} Virial theorem for self-gravitating Brownian particles and study the linear dynamical stability of isolated clusters of particles and uniform systems by using technics introduced in astrophysics. We investigate the influence of the equation of state, of the dimension of space and of the friction coefficient on the dynamical stability of the system. We obtain the exact expression of the critical temperature T_c for a multi-components self-gravitating Brownian gas in d= 2. We also consider the limit of weak frictions ξ→ 0 and derive the orbit-averaged-Kramers equation...|$|R
40|$|Finite automata on {{infinite}} words (omega-automata) {{proved to}} be a powerful weapon for modeling and reasoning infinite behaviors of reactive systems. Complementation of omega-automata is crucial in many of these applications. But the problem is non-trivial; even after extensive study during the past two decades, we still have an important type of omega-automata, namely Streett automata, for which the gap between the current best lower bound 2 ^(Omega(n lg nk)) and upper bound 2 ^(O (nk lg nk)) is substantial, for the Streett index size k can be exponential in the number of states <b>n.</b> In a <b>previous</b> <b>work</b> we showed a construction for complementing Streett automata with the upper bound 2 ^(O(n lg n+nk lg k)) for k = O(n) and 2 ^(O(n^ 2 lg n)) for k = omega(n). In this paper we establish a matching lower bound 2 ^(Omega (n lg n+nk lg k)) for k = O(n) and 2 ^(Omega (n^ 2 lg n)) for k = omega(n), and therefore showing that the construction is asymptotically optimal with respect to the ^(Theta(.)) notation...|$|R
40|$|A {{statistical}} analysis of texture on the COBE-DMR first year sky maps based on the genus and spot number is presented. A generalized χ^ 2 statistic is {{defined in terms of}} "observable" quantities: the genus and spot density that would be measured by different cosmic observers. This strategy together with the use of Monte Carlo simulations of the temperature fluctuations, including all the relevant experimental parameters, represent the main difference with previous analyses. Based on the genus analysis we find a strong anticorrelation between the quadrupole amplitude Q_rms-PS and the spectral index n of the density fluctuation power spectrum at recombination of the form Q_rms-PS= 22. 2 ± 1. 7 - (4. 7 ± 1. 3) × n for fixed <b>n,</b> consistent with <b>previous</b> <b>works.</b> The result obtained based on the spot density is consistent with this Q_rms-PS (n) relation. In addition to the previous results we have determined, using Monte Carlo simulations, the minimum uncertainty due to cosmic variance for the determination of the spectral index with the genus analysis. This uncertainty is δ n≈ 0. 2...|$|R
40|$|Burst-mode {{clock and}} data {{recovery}} circuits (BMCDR) {{are widely used}} in passive optical networks (PON) [1] and {{as a replacement for}} conventional CDRs in clock-forwarding links to reduce power [2]. In PON, a single CDR performs the task of clock and data recovery for several burst sequences, each originating from a different source. As a result, the BMCDR is required to lock to an incoming data stream within tens of UIs (for example 40 <b>ns</b> in GPON). <b>Previous</b> <b>works</b> use either injection locking [3, 4] or gated VCO [5, 6] to achieve this fast locking. In both cases, the control voltage of the CDR’s VCO is generated by a reference PLL with a matching VCO to guarantee accurate frequency locking. However, any component mismatch between the two VCO’s results in a frequency offset between the reference PLL frequency and the CDR’s VCO frequency, and hence in a reduction of the CDR’s tolerance for consecutive identical digits (CID). For example, [7] reports a frequency offset of over 20 MHz (2000 ppm) for 10 Gb/s operation. We present a BMCDR that is based on phase interpolation (PI), eliminating the possibility of local frequency offset between the referenc...|$|R
40|$|Cayley hash {{functions}} are {{a particular kind}} of cryptographic hash functions with very appealing properties. Unfortunately, their security is related to a mathematical problem whose hardness is not very well understood, the factorization problem in finite groups. Given a group G, a set of generators S for this group and an element g ∈ G, the factorization problem asks for a “short” representation of g as a product of the generators. In this paper, we provide a new algorithm for solving this problem for the group G: = SL(2,F 2 n). We first reduce the problem to the resolution of {{a particular kind of}} multivariate equation over F 2 n. Then, we introduce a dedicated approach to solve this equation with Gröbner bases. We provide a complexity analysis of our approach that is of independent interest {{from the point of view}} of Gröbner basis algorithms. Finally, we give the first subexponential time algorithm computing polynomiallength factorizations of any element g with respect to any generator set S of SL(2,F 2 <b>n).</b> <b>Previous</b> algorithms only <b>worked</b> for specific generator sets, ran in exponential time or produced factorizations that had at least a subexponential length. In practice, our algorithm beats the birthday-bound complexity of previous attacks for medium and large values of n...|$|R
40|$|PACS. 87. 10. +e – General {{theory and}} {{mathematical}} aspects. PACS. 02. 50. -r – Probability theory, stochastic processes, and statistics. Abstract. – We study {{the performance of}} principal component analysis (PCA). In particular, we consider {{the problem of how}} many trainingpattern vectors are required to accurately represent the low-dimensional structure of the data. This problem is of particular relevance now that PCA is commonly applied to extremely high-dimensional (N � 5000 – 30000) real data sets produced from molecular-biology research projects. In these applications the number of patterns p is often orders of magnitude less than the data dimension (p ≪ <b>N).</b> We follow <b>previous</b> <b>work</b> and perform the analysis in the context of p random patterns which are isotropically distributed {{with the exception of a}} single symmetry-breaking direction. The standard mean-field theory for the performance of PCA is constructed by consideringthe thermodynamic limit N →∞, with α = p/N fixed. For real data sets the strength of the symmetry breaking may increase with N, and therefore one must reconsider the accuracy of the mean-field theory. We show, usingsimulation results, that the mean-field theory is still accurate even when the strength of the symmetry breakingscales with N, and even for small values of α that are more appropriat...|$|R
