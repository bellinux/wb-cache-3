18|1721|Public
40|$|This report {{compares the}} {{performance}} of different computer systems message passing. Latency and bandwidth are measured on Convex, Cray, IBM, Intel, KSR, Meiko, nCUBE, NEC, SGI, and TMC multiprocessors. Communication performance is contrasted with the computational power of each system. The comparison includes both shared a memory computers as well as <b>networked</b> <b>workstation</b> cluster...|$|E
40|$|Biometric {{authentication}} systems {{represent a}} valid {{alternative to the}} conventional username-password based approach for user authentication. However, authentication systems composed of a biometric reader, a smartcard reader, and a <b>networked</b> <b>workstation</b> which perform user authentication via software algorithms {{have been found to}} be vulnerable in two areas: firstly in their communication channels between readers and workstation (communication attacks) and secondly through their processing algorithms and/or matching results overriding (replay attacks, confidentiality and integrity threats related to the stored information of the <b>networked</b> <b>workstation).</b> In this paper, a full hardware access point for HPC environments is proposed. The access point is composed of a fingerprint scanner, a smartcard reader, and a hardware core for fingerprint processing and matching. The hardware processing core can be described as a Handel-C algorithmic-like hardware programming language and prototyped via a Field Programmable Gate Array (FPGA) based board. The known indexes False Acceptance Rate (FAR) and False Rejection Rate (FRR) have been used to test the prototype authentication accuracy. Experimental trials conducted on several fingerprint DBs show that the hardware prototype achieves a working point with FAR= 1. 07 % and FRR= 8. 33 % on a proprietary DB which was acquired via a capacitive scanner, a working point with FAR= 0. 66 % and FRR= 6. 13 % on a proprietary DB which was acquired via an optical scanner, and a working point with FAR= 1. 52 % and FRR= 9. 64 % on the official FVC 2002 -DB 2 B database. In the best case scenario (depending on fingerprint image size), the execution time of the proposed recognizer is 183. 32 ms. © 2010 Springer Science+Business Media, LLC...|$|E
40|$|The {{design and}} {{implementation}} of an interface between the C Language Integrated Production System (CLIPS) expert system development environment and the graphic user interface development tools of the X-Window system are described. The underlying basis of the CLIPS/X-Window is a client-server model in which multiple clients can attach to a single server that interprets, executes, and returns operation results, in response to client action requests. Implemented in an AIX (UNIX) operating system environment, the interface has been successfully applied {{in the development of}} graphics interfaces for production rule cooperating agents in a knowledge-based computer aided design (CAD) system. Initial findings suggest that the client-server model is particularly well suited to a distributed parallel processing operational mode in a <b>networked</b> <b>workstation</b> environment...|$|E
50|$|The College's four {{computer}} laboratories contain 70 fully <b>networked</b> <b>workstations.</b> Printing and photocopying {{facilities are}} available to students in the Openlab.|$|R
50|$|Tower models stand {{upright on}} the ground or on a desk/shelf, and are {{typically}} used in <b>network</b> <b>workstations</b> or desktop computer applications.|$|R
40|$|The {{exploitation}} of <b>networked</b> <b>workstations</b> {{as a set}} of under-utilized computational resources is an attrac-tive idea for cost effective parallel computing. The use of these resources, however, raises a number of in-teresting policy and performance questions (e. g, what impact will a parallel application have on the normal workstation user, what performance levels can be ex-pected when many users are logged on?). We present performance models for parallel computations execut-ing on shared, <b>networked</b> <b>workstations</b> that can help to address these questions. ...|$|R
40|$|The {{evolution}} of spatial {{data processing systems}} {{to a large extent}} been has driven by technology. The technological progress from a mainframe computer environment to a <b>networked</b> <b>workstation</b> environment has raised a new challenge to the strategy design of large spatial data base systems. To take advantage of new developments in network technology, the conventional structure of spatial information systems, which was initially developed based on single computer systems, needs to be re-designed. This paper reports a proposed structure for a modular Integrated Geographic Information System that operates over a distributed network. The implementation of such a system is now feasible, constrained only by the lack of communication and data standards. The discussion on the concepts of the new networked spatial data base, therefore, becomes necessary to ensure that critical issues are well addressed before a workable system can be developed...|$|E
40|$|It {{is often}} {{necessary}} to measure time in distributed computer systems. For example, timestamps {{are used for}} measuring the intervals between events (like banking transactions). They are also used for ordering events, as in the generation of transaction logs and debug traces. Perhaps their most common use is in performance evaluation, capacity planning, and the detection of unpredicted delays or bottlenecks. Measuring and using time in a distributed environment presents some unique challenges. This thesis illuminates these problems and presents techniques for the proper measurement and use of time. The first challenge is the clocks themselves. Chapter 3 describes the tradeoff between keeping clocks synchronized and keeping them accurate. The inaccuracies of <b>networked</b> <b>workstation</b> clocks are examined, and two statistical techniques are described for collecting time measurements {{in spite of these}} insufficiencies. The second challenge is synchronization. There is often a need to order eve [...] ...|$|E
40|$|This report {{compares the}} {{performance}} of different computer systems for basic message passing. Latency and bandwidth are measured on Convex, Cray, Fujitsu, IBM, Intel, KSR, Meiko, nCUBE, NEC, SGI, and TMC multiprocessors. Communication performance is contrasted with the computational power of each system. The comparison includes both shared and distributed memory computers as well as <b>networked</b> <b>workstation</b> clusters. 1 Introduction and Motivation 1. 1 The Rise of the Microprocessor The past decade {{has been one of}} the most exciting periods in computer development that the world has ever experienced. Performance improvements, in particular, have been dramatic; and that trend promises to continue for the next several years. In particular, microprocessor technology has changed rapidly. Microprocessors have become smaller, denser, and more powerful. Indeed, microprocessors have made such progress that, if cars had made equal progress since the day they were invented, we would now be able to b [...] ...|$|E
50|$|PIKT {{has been}} ported {{to run on}} Linux, Solaris, FreeBSD, Mac OS X, and several other UNIX variants. By design, PIKT is {{intended}} for managing a complex of many heterogeneous <b>networked</b> <b>workstations.</b>|$|R
5000|$|BCWipe - Enterprise Edition allows IT Admins to {{remotely}} install, configure {{and manage}} BCWipe across all <b>network</b> <b>workstations.</b> BCWipe Enterprise features Enforcer for central administration and policy management with no end-user intervention.|$|R
40|$|Abstract. A quick matrix {{multiplication}} algorithm is presented and evaluated on {{a cluster of}} <b>networked</b> <b>workstations</b> consisting of Pentium hosts connected together by Ethernet segments. The obtained results confirm the feasibility of using <b>networked</b> <b>workstations</b> to provide fast and low cost solutions to many computationally intensive applications such as large linear algebraic systems. The paper also presents and verifies an accurate timing model to predict {{the performance of the}} proposed algorithm on arbitrary clusters of workstations. Through this model the viability of the proposed algorithm can be revealed without the extra effort that would be needed to carry out real testing...|$|R
40|$|We {{describe}} our {{development of}} a "real world" electromagnetic application on distributed computing systems. A computational electromagnetics (CEM) simulation for radar crosssection (RCS) modeling of full scale airborne systems has been ported to three <b>networked</b> <b>workstation</b> cluster systems: an IBM RS/ 6000 cluster with Ethernet connection; a DEC Alpha farm connected by a FDDI-based Gigaswitch; and an ATM-connected SUN IPXs testbed. We used the ScaLAPACK LU solver from Oak Ridge National Laboratory/University of Tennessee in our parallel implementation for solving the dense matrix which forms the computationally intensive kernel of this application, and we have adopted BLACS as the message passing interface {{in all of our}} code development to achieve high portability across the three configurations. The performance data from this work is reported, together with timing data from other MPP systems on which we have implemented this application including an Intel iPSC/ 860 and a CM- 5, and which [...] ...|$|E
40|$|Abstract — Sharing data among {{collaborators}} in {{widely distributed}} systems remains a challenge due to limitations with existing methods for defining groups across administrative domain boundaries with various file systems. Groups in traditional systems {{are bound to}} particular domains or file systems using centralized storage locations either beyond ordinary users ’ ability to manage, inaccessible outside a closed system, or both. We present a method for users to independently create and manage groups on any <b>networked</b> <b>workstation</b> using global user identities and to control access to shared data and storage resources based on group membership, regardless of domain boundaries or underlying file systems. Decentralized groups are decoupled from shared user databases and centralized authentication servers {{through the use of}} a virtual user namespace. We describe how owners of shared resources can define security policies through the use of caching, and demonstrate how each caching policy represents tradeoffs between performance, scalability, and consistency. I...|$|E
40|$|One of {{the primary}} mass storage systems in use at the Los Alamos National Laboratory (LANL) is the Common File System, or CFS. CFS went into {{production}} in 1979, servicing supercomputer environments, and later was expanded for use with a broader <b>networked</b> <b>workstation</b> environment. It is now used by a very large user population at LANL. It {{can be used by}} any employee for storage purposes, and is used by all of the large supercomputers at LANL. CFS is being phased out for the supercomputing environment due to the need for a more scalable mass storage system design. To our benefit, records have been kept {{for the last seven years}} of all activity on CFS. A statistical analysis of these records has been performed, to understand how the mass storage system was used over a long period of time. Example usage statistics include maximum and average file sizes, data rates, and bytes moved for each month...|$|E
50|$|In addition, {{with the}} advent of local area {{networks}} (LAN), several EIS products for <b>networked</b> <b>workstations</b> became available. These systems require less support and less expensive computer hardware. They also increase EIS information access to more company users.|$|R
50|$|The University's Computing Center (German Rechenzentrum) {{operates}} {{a series of}} central servers {{and a large number}} of computer labs via a high-speed gigabit network. Altogether, approximately 1600 <b>networked</b> <b>workstations</b> are available to students on campus, as well as campus-wide wireless access.|$|R
40|$|This thesis {{describes}} {{the development of}} a portion of a distributed linear algebra library for use on <b>networks</b> of <b>workstations.</b> The library was designed with special consideration towards three characteristics of networks of workstations: small numbers of processes, availability of multithreading, and high communication latency. Two aspects of the library are highlighted. First, modifications to message passing primitives to permit their use in a multithreaded environment. Second, modifications to basic linear algebra algorithms to improve their performance on <b>networks</b> of <b>workstations.</b> A model of distributed linear algebra on <b>networks</b> of <b>workstations</b> is developed, and used to predict the performance of the modified algorithms. These predictions are compared to experimental results on several <b>networks</b> of <b>workstations...</b>|$|R
40|$|Sharing data among {{collaborators}} in {{widely distributed}} systems remains a challenge due to limitations with existing methods for defining groups across administrative domain boundaries with various file systems. Groups in traditional systems {{are bound to}} particular domains or file systems using centralized storage locations either beyond ordinary users ’ ability to manage, inaccessible outside a closed system, or both. We present a method for users to independently create and manage groups on any <b>networked</b> <b>workstation</b> using global user identities and to control access to shared data and storage resources based on group membership, regardless of domain boundaries or underlying file systems. Decentralized groups are decoupled from shared user databases and centralized authentication servers {{through the use of}} a virtual user name space. We describe how owners of shared resources can define security policies through the use of caching, and demonstrate how each caching policy represents tradeoffs between performance, scalability, and consistency. 1...|$|E
40|$|In {{this paper}} we {{investigate}} {{the network and}} storage requirements of an v irtual classroom. The v irtual classroom replaces traditional class methodologies by using the computer as the sole instrument for all class activity. The instructor and the students each have a <b>networked</b> <b>workstation</b> in an X cluster that provides for the creation, modification, and distribution of presentations, note taking, capturing of presentation material, out-of-class reviewing of presentation material, and viewing of supplemental materials provided by the instructor (including selected readings, exams, and assignments). We have designed and implemented a virtual classroom {{as a means of}} enhancing the teaching/learning process. The creation of this virtual classroom was accomplished by using only existing computing resources: SUN workstations, X tools, an Ethernet network, and UNIX operating system support. Network statistics were collected to determine how well existing networks can be utilized within this environment. We also observed the performance of the system in a realistic setting by using it to teach an Office Information Systems class at North Dakota State University...|$|E
40|$|AbstractComputer tools usage has {{became one}} of the major trends in {{humanity}} and product design processes are not stranger to this situation. That is the reason why different computer tools (hardware and software) have been developed in order to support design tasks, enabling design processes to achieve more innovative solutions by offering more time to creativity. Nevertheless, the quantity and variety of available tools bring a high level of complexity and their articulation becomes an important need to be tackled, aiming to integrate hardware and software applications onto a single platform. This article describes the development of a collaborative design environment supported by the usage of computer tools for product design processes. The proposed model is based on the integration of different commercial hardware into a single platform. It is meant to support product design meetings of collocated design teams in a work station where all team members can interact with design concepts in a synchronous way. After the construction of a first functional solution, a redesign process was conducted to develop a platform where key aspects such as functionality, aesthetics and ergonomics were considered. Finally, the proposed solution is not only intended to perform as a stand-alone platform, but also as a <b>networked</b> <b>workstation</b> that will allow distributed design processes...|$|E
40|$|Performance of {{database}} {{systems can}} be improved by applying parallel processing techniques. Several commercial parallel database systems are available but these are expensive. In the parallel processing area, there is a trend to use <b>networks</b> of <b>workstations</b> as a virtual parallel machine. The obvious advantage of such systems is their low cost. The availability of public domain software (such as PVM and MPI) to configure a parallel virtual machine over a <b>network</b> of <b>workstations</b> has led several researchers to study performance issues in this area. In this paper, we propose the use of <b>network</b> of <b>workstations</b> to improve the performance of database queries. We present analysis of a four-way join operation to demonstrate the feasibility of our proposal. However, using <b>networks</b> of <b>workstations</b> for database processing poses its own problems. These problems have to be solved before we can profitably employ <b>networks</b> of <b>workstations</b> for database applications. We provide {{a discussion of the}} issues [...] ...|$|R
50|$|On 2 December 2009, Microsoft Sells Part Of Fast Search To Rocket Software. Microsoft {{is selling}} Fast’s Folio and NXT {{businesses}} to Rocket Software; the complementary products are application suites used by businesses to publish and index reference material onto discs, <b>network</b> <b>workstations,</b> and online.|$|R
50|$|In 1970, Kay joined Xerox Corporation's Palo Alto Research Center, PARC. In the 1970s {{he was one}} of the {{key members}} there to develop prototypes of <b>networked</b> <b>workstations</b> using the {{programming}} language Smalltalk. These inventions were later commercialized by Apple Computer in their Lisa and Macintosh computers.|$|R
40|$|Human Systems Integration ReportIt is {{important}} to understand the impact that the proliferation of information displays has on the warfighters ability to reason about, or make sense of, battlefield information. This research investigates how information sources at a tactical operations center (TOC) workstation affected a battle captains ability to understand and portray ground truth in a simulated battlefield scenario. Twelve active-duty officers with previous battle-captain experience were {{randomly assigned to one of}} four groups. Each group was exposed once to each source condition (two or six sources) and tactical scenario. A replicated prenetwork centric warfare (NCW) TOC workstation and modern digitally <b>networked</b> <b>workstation</b> were used for comparison. During each 40 - minute battlefield scenario, participants provided situational reports (SITREPs), placed friendly and enemy unit symbols on the battlefield map, and provided perceived mental workload. The results of this research indicate that there is no difference for situational understanding between the modern battle captain workstation (six sources) and the legacy workstation (two sources), when the amount of information from the sources remains the same. Contrary to expectations, perceived mental workload using the two-source workstation is significantly higher than the six-source workstation. Results of this research could have implications for the design of future information system and networked workstations in TOCs. Major, United States Arm...|$|E
40|$|This paper {{describes}} some of {{the design}} criteria for a facility to support problem-based tutorials, known as the Computer-Mediated Tutorial Laboratory (CMTL). In the CMTL, a <b>networked</b> <b>workstation</b> will be provided for the tutor {{and each of the}} students. The tutor's workstation will be connected to a projection system, permitting the entire group to view the tutor's screen. The software used in the CMTL will have three components: a Patient Simulation Stack (PSS), the group/student Tutorial Stacks (TSs) and the network communication interface. The PSS represents a clinical problem; it is designed to realistically simulate an encounter with an actual patient. The TSs serve as a personalized record of what transpired in the tutorial session. Each student will maintain a private TS and the tutor will maintain a shared TS, viewable by all members of the group. The network communication interface will permit the participants in the tutorial to direct electronic messages to each other. The communication interface has two components: the client software available on the student's and tutor's workstation and the server software. The CMTL client software consists of two applications-one for sending messages and one for viewing the stream of incoming messages. Research is planned to investigate the effects of computer-mediation on the tutorial process...|$|E
40|$|The overall {{scope of}} this {{endeavor}} was to develop an integrated computer system, running on a network of heterogeneous computers, {{that would allow the}} rapid development of tool designs, and then use process models to determine whether the initial tooling would have characteristics which produce the prototype parts. The major thrust of this program for ORNL was the definition of the requirements {{for the development of the}} integrated die design system with the functional purpose to link part design, tool design, and component fabrication through a seamless software environment. The principal product would be a system control program that would coordinate the various application programs and implement the data transfer so that any <b>networked</b> <b>workstation</b> would be useable. The overall system control architecture was to be required to easily facilitate any changes, upgrades, or replacements of the model from either the manufacturing end or the design criteria standpoint. The initial design of such a program is described in the section labeled ``Control Program Design``. A critical aspect of this research was the design of the system flow chart showing the exact system components and the data to be transferred. All of the major system components would have been configured to ensure data file compatibility and transferability across the Internet. The intent was to use commercially available packages to model the various manufacturing processes for creating the die and die inserts in addition to modeling the processes for which these parts were to be used. In order to meet all of these requirements, investigative research was conducted to determine the system flow features and software components within the various organizations contributing to this project. This research is summarized...|$|E
40|$|Traditional {{computerized}} writing tools {{designed for}} single user access often create barriers to group collaboration. Usually, participants {{have to work}} in an interleaved fashion to prevent inconsistency. Oral communication is normally used for coordinating group activities. With the recent development of computer networks and widespread deployment of <b>networked</b> <b>workstations,</b> automating the group writing process for geographically distributed users has become feasible. This paper introduces a Distributed Collaborative Writing Aid (DCWA) developed for <b>networked</b> <b>workstations.</b> The DCWA can help users cooperate on a writing task (such as programming, report writing, note taking, etc.) logically, conveniently, and efficiently. Among many important open problems, the paper addresses issues related to the four major components in the DCWA. These are group organization, multicasting within groups, distributed database, and user interface. Key Words: Computer Supported Cooperative Wor [...] ...|$|R
40|$|Most {{applications}} {{share the}} resources of <b>networked</b> <b>workstations</b> with other applications. Since system load can vary dramatically, allocation strategies that assume that resources have a constant availability and/or capability are unlikely to promote performance-efficient allocations in practice. In fact, {{it is critical to}} provide a realistic model of the effects of contention on application performance in order to best allocate application tasks to machines. In this paper, we present a model that provides an estimate of the slowdown imposed by competing load on applications targeted to highperformance clusters and <b>networks</b> of <b>workstations.</b> The model provides a basis for predicting realistic communication and computation costs and is shown to achieve good accuracy for a set of scientific benchmarks commonly found in high-performance applications. 1 Introduction In the last decade, <b>networks</b> of <b>workstations</b> have emerged as powerful platforms for executing high-performance parallel applic [...] ...|$|R
40|$|<b>Networks</b> of <b>workstations</b> are a {{dominant}} {{force in the}} distributed computing arena, due primarily to the excellent price/performance ratio of such systems when compared to traditionally massively parallel architectures. It is therefore critical to develop programming languages and environments that can potentially harness the raw computational power availab le on these systems. In this article, we present JavaNOW (Java on <b>Networks</b> of <b>Workstations),</b> a Java based framework for parallel programming on <b>networks</b> of <b>workstations.</b> It creates a virtual parallel machine similar to the MPI (Message Passing Interface) model, and provides distributed associative shared memory similar to Linda memory model but with a flexible set of primitive operations. JavaNOW provides a simple yet powerful framework for performing computation on <b>networks</b> of <b>workstations.</b> In addition to the Linda memory model, it provides for shared objects, implicit multithreading, implicit synchronization, object data [...] ...|$|R
40|$|The Teacher Enhancement Institute (TEI), {{under the}} {{direction}} of the Center Education Programs Officer offered three two-week workshops to 58 elementary and middle school teachers in science, math, and technology using the Problem Based Learning Model. The 1995 program was designed with input from evaluations and recommendations from previous TEI participants and faculty. The TEI focused on Aviation and Aeronautics as the unifying theme. Four specific objectives were developed. After completing the requirements for the TEI, the participants should be able to: (1) Increase their content knowledge, particularly in aeronautics, science, math, and technology; (2) Design and implement lessons that use scientific inquiry through Problem Based Learning; (3) Demonstrate knowledge of instructional technologies, their uses, and applications to curricula; and (4) Disseminate to their school communities the information acquired through the TEI. Thirty percent of the program was devoted to the effective use of computer technology. SpaceLink, the NASA telecomputing service for educators, was the primary tool used in the technology component of the institute. The training focused on the use of SpaceLink and its many educational services, and Internet tools because of its universal, nongraphical link to any computer plafform the participant may use at his or her school or home. All participants were given Educator Accounts to facilitate the use of E-mail, and access to the Internet and the World Wide Web using their SpaceLink accounts. Classroom demonstrations used videotaped guides and handouts to support concepts presented followed by intensive hands-on activities. Each participant was assigned to an individual Power Mac <b>networked</b> <b>workstation</b> and introduced to the state of the art, graphical, Word Wide Web with the Netscape browser. The methodology proved very effective in reaching the program's goals for technology integration by having the participants learn to use the computer as a tool for communication and research rather than teaching the use of any particular software application alone. However, because of the skill level of the majority of the participants, more hands-on computer time is recommended for future Teacher Enhancement Institutes...|$|E
40|$|Many {{universities}} and research laboratories have developed low cost clusters, built from Commodity-Off-The-Shelf (COTS) components and running mostly free software. Research {{has shown that}} these types of systems are well-equipped to handle many problems requiring parallel processing. The primary components of clusters are hardware, networking, and system software. An important system software consideration for clusters is {{the choice of the}} message passing library. MPI (Message Passing Interface) has arguably become the most widely used message passing library on clusters and other parallel architectures, due in part to its existence as a standard. As a standard, MPI is open for anyone to implement, as long as the rules of the standard are followed. For this reason, a number of proprietary and freely available implementations have been developed. Of the freely available implementations, two have become increasingly popular: LAM (Local Area Multicomputer) and MPICH (MPI Chameleon). This thesis compares the performance of LAM and MPICH in an effort to provide performance data and analysis of the current releases of each to the cluster computing community. Specifically, the accomplishments of this thesis are: comparative testing of the High Performance Linpack benchmark (HPL); comparative testing of su 3 _rmd, an MPI application used in physics research; and a series of bandwidth comparisons involving eight MPI point-to-point communication constructs. All research was performed on a partition of the Wyeast SMP Cluster in the High Performance Computing Laboratory at Portland State University. We generate a vast amount of data, and show that LAM and MPICH perform similarly on many experiments, with LAM outperforming MPICH in the bandwidth tests and on a large problem size for su 3 _rmd. These findings, along with the findings of other research comparing the two libraries, suggest that LAM performs better than MPICH in the cluster environment. This conclusion may seem surprising, as MPICH has received more attention than LAM from MPI researchers. However, the two architectures are very different. LAM was originally designed for the cluster and <b>networked</b> <b>workstation</b> environments, while MPICH was designed to be portable across many different types of parallel architectures...|$|E
40|$|In {{the early}} days of JPL's solar system exploration, each {{spacecraft}} mission required its own dedicated data system with all software applications written in the mainframe's native assembly language. Although these early telemetry processing systems were a triumph of engineering in their day, since that time the computer industry has advanced {{to the point where it}} is now advantageous to replace these systems with more modern technology. The Space Flight Operations Center (SFOC) Prototype group was established in 1985 as a workstation and software laboratory. The charter of the lab was to determine if it was possible to construct a multimission telemetry processing system using commercial, off-the-shelf computers that communicated via networks. The staff of the lab mirrored that of a typical skunk works operation [...] a small, multi-disciplinary team with a great deal of autonomy that could get complex tasks done quickly. In an effort to determine which approaches would be useful, the prototype group experimented with all types of operating systems, inter-process communication mechanisms, network protocols, packet size parameters. Out of that pioneering work came the confidence that a multi-mission telemetry processing system could be built using high-level languages running in a heterogeneous, <b>networked</b> <b>workstation</b> environment. Experience revealed that the operating systems on all nodes should be similar (i. e., all VMS or all PC-DOS or all UNIX), and that a unique Data Transport Subsystem tool needed to be built to address the incompatibilities of network standards, byte ordering, and socket buffering. The advantages of building a telemetry processing system based on emerging industry standards were numerous: by employing these standards, we would no longer be locked into a single vendor. When new technology came to market which offered ten times the performance at one eighth the cost, it would be possible to attach the new machine to the network, re-compile the application code, and run. In addition, we would no longer be plagued with lack of manufacturer support when we encountered obscure bugs. And maybe, hopefully, the eternal elusive goal of software portability across different vendors' platforms would finally be available. Some highlights of our prototyping efforts are described...|$|E
40|$|In this paper, {{we examine}} and {{characterize}} effects of communication interactions of parallel and sequential jobs on a nondedicated ATM <b>network</b> of <b>workstations.</b> We quantitatively model the interaction process and measure communication delays of parallel jobs {{caused by the}} interaction. In addition, a scheme is proposed to distinguish and prioritize {{the two types of}} communications messages to reduce the disturbance of sequential jobs' communications on the communication performance of parallel jobs. Measurement and simulation results on an ATM <b>network</b> of <b>workstations</b> support our analytical models and the scheme. Keywords: Communication, <b>networks</b> of <b>workstations</b> (NOW), parallel processing, TCP/IP. 1 Introduction The wide availability of workstations and improving speed of networks have made <b>networks</b> of <b>workstations</b> (NOWs) become important platforms for parallel computation. The network used by NOW is usually a general purpose network. To address the communication issue on NOW, cur [...] ...|$|R
40|$|Large BDD {{applications}} push {{computing resources}} to their limits. One solution to overcoming resource limitations is {{to distribute the}} BDD data structure across multiple <b>networked</b> <b>workstations.</b> This paper presents an efficient parallel BDD package for a distributed environment such as a <b>network</b> of <b>workstations</b> (NOW) or a distributed memory parallel computer. The implementation exploits {{a number of different}} forms of parallelism that can be found in depth-first algorithms. Significant effort is made to limit the communication overhead, including a two-level distributed hash table and an uncomputed cache. The package simultaneously executes multiple threads of computation on a distributed BDD. 1...|$|R
5000|$|Sony NEWS-OS, a BSD-based {{operating}} system for their <b>network</b> engineering <b>workstations</b> ...|$|R
