17|94|Public
50|$|There {{was some}} dispute about the {{legality}} of the hobbit as a <b>non-standard</b> <b>measure</b> in commerce after Parliament established the Winchester measure. The hobbit's mixed use as a unit of both volume and of weight complicated the issue, as laws applied to the two cases differently. Courts sometimes nullified grain contracts denominated in hobbits, and sometimes upheld them, according to varying customs in precisely how hobbits were defined and measured.|$|E
50|$|This line of thinking, in turn, {{may reflect}} a <b>non-standard</b> <b>measure</b> of utility, which is {{ultimately}} subjective and unique to the consumer. A ticket-buyer who purchases a ticket to an event they won't enjoy in advance makes a semi-public commitment to watching it. To leave early is to make this lapse of judgment manifest to strangers, an appearance they might otherwise choose to avoid. Alternatively, they may take pride in having recognized the opportunity cost of the alternative use of time.|$|E
40|$|Abstract. Finding {{a maximum}} acyclic {{subgraph}} {{is on the}} list of problems that seem to be hard to tackle from a parameterized perspective. We develop two quite efficient algorithms (one is exact, the other parameterized) for (1, n) -graphs, a class containing cubic graphs. The running times are O ∗ (1. 1871 m) and O ∗ (1. 212 k), respectively, determined by an amortized analysis via a <b>non-standard</b> <b>measure.</b> ...|$|E
40|$|The thesis {{analyses}} {{the change}} of monetary policy of the European central bank during financial and sovereign debt crisis. Main focus of the thesis is given on the analysis of <b>non-standard</b> <b>measures</b> of monetary policy of the ECB. This thesis explains in detail how the ECB has responded to the various phases of the financial and debt crisis, starting with the period of financial turmoil, continuing with the intensification {{of the financial crisis}} and the sovereign debt crisis. Finally, the thesis compares these phases and assesses the impact of these <b>non-standard</b> <b>measures...</b>|$|R
40|$|The thesis {{deals with}} {{non-standard}} monetary policies of three central banks throughout {{the global financial}} crisis. The reason for using <b>non-standard</b> <b>measures</b> was also liquidity trap when monetary policy becomes ineffective. An important milestone was collapse of investment bank Lehman Brothers in September 2008. The central banks carried out some <b>non-standard</b> <b>measures</b> before the collapse such as {{the emergence of new}} or expanding existing facilities. However, after the collapse there was panic at the financial and capital markets and market interest rate spreads rose. Central banks were forced to respond to expanding its balance sheet and reducing the monetary policy rate to zero. The main reason for increasing total assets was securities purchases by central banks. The measure, which is expanding the balance sheet is called quantitative easing. In the thesis I try to describe and compare the <b>non-standard</b> <b>measures</b> (with a focus on quantitative easing) taken by the three central banks (Bank of England, the Fed and the ECB) and answer the question whether the measures are effective and whether they can replace the standard monetary policy...|$|R
40|$|The {{following}} paper {{examines how}} the European Central Bank (ECB) {{has responded to}} the challenges of the financial and sovereign debt crisis within the EMU. First theo-ry is introduced to get an understanding of the problems that the ECB faces. The theo-ry regards central banking, optimum currency areas, and the monetary transmission mechanism. The theory is followed by an explanation of the crisis that struck the Eu-rozone in 2008, which revealed macroeconomic imbalances in the Eurozone. This leads to the analysis, which mainly focuses on analysing how the ECB has attempted to re-establish the singles of its monetary policy transmission through <b>non-standard</b> <b>measures.</b> In the light of the macroeconomic imbalances and the <b>non-standard</b> <b>measures</b> introduced by the ECB, two alternative solutions beyond the potential of the ECB are suggested. ...|$|R
40|$|The {{problem of}} finding a {{spanning}} tree in an undirected graph with a maximum number of leaves {{is known to be}} NP-hard. We present an algorithm which finds a spanning tree with at least k leaves in time O∗(3. 4575 k) which improves the currently best algorithm. The estimation of the running time is done by using a <b>non-standard</b> <b>measure.</b> The present paper is one of the still few examples that employ the Measure & Conquer paradigm of algorithm analysis in the area of Parameterized Algorithmics...|$|E
40|$|AbstractIn MaxSat, {{we ask for}} an {{assignment}} to the variables which satisfies {{the maximum number of}} clauses for a boolean formula in CNF. We present an algorithm yielding a run time upper bound of O⁎(2 K 6. 265) for Max- 2 -Sat (each clause contains at most 2 literals), where K is the number of clauses. The run time has been achieved by using heuristic priorities on the choice of the variable on which we branch. The implementation of these heuristic priorities is rather simple, though they {{have a significant effect on}} the run time. The analysis uses a <b>non-standard</b> <b>measure.</b> ...|$|E
40|$|Within {{the task}} of {{collaborative}} filtering two challenges for computing conditional probabilities exist. First, the amount of training data available is typically sparse {{with respect to the}} size of the domain. Thus, support for higher-order interactions is generally not present. Second, the variables that we are conditioning upon vary for each query. That is, users label different variables during each query. For this reason, there is no consistent input to output mapping. To address these problems we purpose a maximum entropy approach using a <b>non-standard</b> <b>measure</b> of entropy. This approach can be simplified to solving a set of linear equations that can be efficiently solved. ...|$|E
40|$|Abstract. The European Central Bank {{was forced}} to start using <b>non-standard</b> <b>measures</b> in order to manage the {{situation}} determined by the global financial and sovereign debt crisis, namely to sort out liquidity problems and expand credit supply. The European Central Bank is criticized for applying non-standard tools because of increase in inflation risk. However, the analysis shows that the inflation could be managed by the absorption of liquidity surplus. However, there is a negative side of using <b>non-standard</b> <b>measures,</b> such as a significant increa-se in the credit risk, which arises due to having government bonds in the balance sheet of the European Central Bank. In addition, this indicates that the European Central Bank indirectly finances governments. Key words: monetary policy, inflation, sovereign debt crisis, credit risk, quantitative easin...|$|R
40|$|Economic and Monetary Unión {{is a big}} {{endeavor}} {{leading towards}} the integration of European Unión economies. It involves the coordination of economic and fiscal policies, a common monetay policy and a common currency, the Euro. From the 28 member states of the European Unión (EU hereinafter), 19 of them have reached {{a higher degree of}} integration and have adopted the euro. The current económic crisis that started in 2007 has led the European Central BAnk (ECB) to apply <b>non-standard</b> <b>measures</b> like quantitative easing (QE) to avoid deflation and increase output in the euro área. This work explains the <b>non-standard</b> <b>measures</b> tha have been applied in the euro área until now. It also estimates an autoregressive vector (VAR) to quantify the expected effects of QE on production, prices, interest rates and Exchange rates in the euro area. Facultad de Ciencias de la EmpresaUniversidad Politécnica de Cartagen...|$|R
40|$|This paper aims to {{make two}} contributions: to review the ECB 2 ̆ 019 s <b>non-standard</b> {{monetary}} policy <b>measures</b> {{in response to the}} financial and sovereign debt crisis against the background of the institutional framework and financial structure of the euro area; and to interpret this response from a flow-of-funds perspective. The paper highlights how the rationale behind the ECB 2 ̆ 019 s nonstandard measures differs from that underlying quantitative easing policies. As a complement to rather than a substitute for standard interest rate decisions, the <b>non-standard</b> <b>measures</b> are aimed at supporting the effective transmission of monetary policy to the economy rather than at delivering additional direct monetary stimulus. The flow-of-funds analysis proposes an interpretation of central banks 2 ̆ 019 crisis responses as fulfilling their traditional role as lender of last resort to the banking system and, more broadly, reflecting their capacity to act as the 2 ̆ 01 cultimate sector 2 ̆ 01 d that can take on leverage when other sectors are under pressure to deleverage. It also provides examples that trace the impact of <b>non-standard</b> <b>measures</b> across different sectors and markets...|$|R
40|$|Abstract. In MaxSat, {{we ask for}} an {{assignment}} which satisfies {{the maximum number of}} clauses for a boolean formula in CNF. We present an algorithm yielding a run time upper bound of O ∗ (2 1 6. 2158 K) for Max- 2 -Sat (each clause contains at most 2 literals), where K is the number of clauses. The run time has been achieved by using heuristic priorities on the choice of the variable on which we branch. The implementation of these heuristic priorities is rather simple, though they {{have a significant effect on}} the run time. The analysis is done using a tailored <b>non-standard</b> <b>measure.</b> ...|$|E
40|$|In {{this paper}} {{we present a}} new insertion-based {{construction}} heuristic to solve the multivehicle pickup and delivery problem with time windows. The new heuristic does not only consider the classical incremental distance measure in the insertion evaluation criteria but also the cost of reducing the time window slack due to the insertion. We also present a <b>non-standard</b> <b>measure,</b> Crossing Length Percentage, in the insertion evaluation criteria to quantify the visual attractiveness of the solution. We compared our heuristic with a sequential and a parallel insertion heuristic on different benchmarking problems, and the computational {{results show that the}} proposed heuristic performs better with respect to both the standard and non-standard measures...|$|E
40|$|One of the {{standard}} basic steps in drawing hierarchical graphs is to invert some arcs of the given graph to make the graph acyclic. We discuss exact and parameterized algorithms for this problem. In particular we examine a graph class called (1,n) -graphs, which contains cubic graphs. For both exact and parameterized algorithms we use a <b>non-standard</b> <b>measure</b> approach for the analysis. The analysis of the parameterized algorithm is of special interest, as {{it is not an}} amortized analysis modelled by 2 ̆ 7 finite states 2 ̆ 7 but rather a 2 ̆ 7 top-down 2 ̆ 7 amortized analysis. For (1,n) -graphs we achieve a running time of Oh^*(1. 1871 ^m) and Oh^*(1. 212 ^k), for cubic graphs Oh^*(1. 1798 ^m) and Oh^*(1. 201 ^k), respectively. As a by-product the trivial bound of 2 ^n for sc Feedback Vertex Set on planar directed graphs is broken...|$|E
40|$|This paper {{examines}} {{the degree of}} fragmentation in the Euro overnight unsecured money market during the period June 2008 2 ̆ 013 August 2013 using interbank loans constructed from payments data. After controlling for cross-country differences in bank risk, we document several episodes of significant market fragmentation. While <b>non-standard</b> <b>measures</b> such as the provision of long-term liquidity were successful in reducing tensions, considerable signs of market fragmentation remained {{at the end of}} the sample period...|$|R
40|$|We assess {{professional}} forecasters 2 ̆ 019 {{perceptions of}} the effects of the unconventional monetary policy measures announced by the US Federal Reserve after the collapse of Lehman Brothers. Using survey data, collected at individual level, we analyze the change in the forecasts for Treasury and corporate bond yields around the announcement dates of the <b>non-standard</b> <b>measures.</b> We find that forecasters expected bond yields to drop significantly for at least one year after the announcement of accommodative policies...|$|R
40|$|The {{conventional}} {{test methods}} combined with <b>non-standard</b> <b>measuring</b> of fracture toughness Kic for {{the determination of}} low- temperature limit of exploitation was applied on nine different structural steels either fine-grained micro- alloyed steels or low-alloyed structural steels. These experiments confirmed {{that the results of}} low-temperature tests could be succesfully used for the calculation of some fracture mechanics characteristics of the investigated steels as for instance the dynamics yield strenght [MATH], the dynamics fracture toughness Kid and, of course, the static fracture toughness Kic...|$|R
40|$|Abstract. One of the {{standard}} basic steps in drawing hierarchical graphs is to invert some arcs of the given graph to make the graph acyclic. We discuss exact and parameterized algorithms for this problem. In particular we examine a graph class called (1, n) -graphs, which contains cubic graphs. We discuss exact and parameterized algorithms, where we use a <b>non-standard</b> <b>measure</b> approach for the analysis. Especially {{the analysis of the}} parameterized algorithm is of special interest, as it is not an amortized analysis modelled by ’finite states ’ but is rather a ’top-down ’ amortized analysis. For (1, n) -graphs we achieve a running time of O ∗ (1. 1871 m) and O ∗ (1. 212 k), for cubic graphs O ∗ (1. 1798 m) and O ∗ (1. 201 k), respectively. As a by-product the the trivial bound of 2 n for Feedback Vertex Set on planar directed graphs is broken. ...|$|E
40|$|Abstract. Given an n-node graph and {{a subset}} of k {{terminal}} nodes, the NP-hard Steiner tree problem is to compute a minimum-size tree which spans the terminals. All the known algorithms for this problem which improve on trivial O(1 : 62 n) -time enumeration are based on dy-namic programming, and require exponential space. Motivated {{by the fact that}} exponential-space algorithms are typically impractical, in this paper we address the problem of designing faster polynomial-space algorithms. Our rst contribution is a simple polynomial-space O(6 knO(log k)) -time algorithm, based on a variant of the classical tree-separator theorem. This improves on trivial O(nk+O(1)) enumeration for, roughly, k n= 4. Combining the algorithm above (for small k), with an improved branch-ing strategy (for large k), we obtain an O(1 : 60 n) -time polynomial-space algorithm. The rened branching is based on a charging mechanism which shows that, for large values of k, convenient local congurations of terminals and non-terminals must exist. The analysis of the algorithm relies on the Measure & Conquer approach: the <b>non-standard</b> <b>measure</b> used here is a linear combination of the number of nodes and number of non-terminals. As a byproduct of our work, we also improve the (exponential-space) time complexity of the problem from O(1 : 42 n) to O(1 : 36 n). ...|$|E
40|$|Given an n-node graph and {{a subset}} of k {{terminal}} nodes, the NP-hard Steiner tree problem is to compute a minimum-size tree which spans the terminals. All the known algorithms for this problem which improve on trivial O(1. 62 n) -timeenumerationarebasedondynamic programming, and require exponential space. Motivated {{by the fact that}} exponential-space algorithms are typically impractical, in this paper we address the problem of designing faster polynomial-space algorithms. Our first contribution is a simple polynomialspace O(6 k n O(log k)) -time algorithm, based on a variant of the classical tree-separator theorem. This improves on trivial O(n k+O(1)) enumeration for, roughly, k ≤ n/ 4. Combining the algorithm above (for small k), with an improved branching strategy (for large k), we obtain an O(1. 60 n) -time polynomial-space algorithm. The refined branching is based on a charging mechanism which shows that, for large values of k, convenient local configurations of terminals and non-terminals must exist. The analysis of the algorithm relies on the Measure & Conquer approach: the <b>non-standard</b> <b>measure</b> used here is a linear combination of the number of nodes and number of non-terminals. As a byproduct of our work, we also improve the (exponential-space) time complexity of the problem from O(1. 42 n) to O(1. 36 n) ...|$|E
40|$|This paper {{describes}} a Savings Based algorithm for the Extended Vehicle Routing Problem. This algorithm is {{compared with a}} Sequential Insertion algorithm on real-life data. Besides the traditional quality measures such as total distance traveled and total workload, we compare the routing plans of both algorithms according to <b>non-standard</b> quality <b>measures</b> that help to evaluate the "visual attractiveness" of the plan. Computational results show that, in general, the Savings Based algorithm not only performs better {{with respect to the}}se <b>non-standard</b> quality <b>measures,</b> but also with respect to the traditional measures. distribution;vehicle routing;road transport...|$|R
40|$|We {{analyze the}} {{pass-through}} of monetary policy measures to lending rates to fi rms and {{households in the}} euro area using a novel bank-level dataset. Banks characteristics such as the capital ratio, the exposure to sovereign debt, {{and the percentage of}} non-performing loans are responsible for the heterogeneity in pass-through of conventional monetary policy changes. The location of a bank is irrelevant. <b>Non-standard</b> <b>measures</b> normalized the capacity of banks to grant loans. Banks with high level of non-performing loans and low capital ratio were most affected. Bank's lending margins fell considerably. Macroeconomic implications are discussed...|$|R
40|$|Although {{non-parametric}} {{tests have}} already been proposed for that purpose, statistical significance tests for <b>non-standard</b> <b>measures</b> (different from the classification error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole "population", we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions...|$|R
40|$|For {{more than}} 40 years Branch & Reduce exponential-time {{backtracking}} algorithms {{have been among}} the most common tools used for finding exact solutions of NP-hard problems. Despite that, the way to analyze such recursive algorithms is still far from producing tight worst-case running time bounds. Motivated by this we use an approach, that we call “Measure & Conquer”, as an attempt to step beyond such limitations. The approach is based on the careful design of a <b>non-standard</b> <b>measure</b> of the subproblem size; this measure is then used to lower bound the progress made by the algorithm at each branching step. The idea is that a smarter measure may capture behaviors of the algorithm that a standard measure {{might not be able to}} exploit, and hence lead to a significantly better worst-case time analysis. In order to show the potentialities of Measure & Conquer, we consider two well-studied NPhard problems: minimum dominating set and maximum independent set. For the first problem, we consider the current best algorithm, and prove (thanks to a better measure) a much tighter running time bound for it. For the second problem, we describe a new, simple algorithm, and show that its running time is competitive with the current best time bounds, achieved with far more complicated algorithms (and standard analysis). Ou...|$|E
40|$|Given an n-node edge-weighted graph and {{a subset}} of k {{terminal}} nodes, the NP-hard (weighted) Steiner tree problem is to compute a minimum-weight tree which spans the terminals. All the known algorithms for this problem which improve on trivial O(1. 62 n) -time enumeration are based on dynamic programming, and require exponential space. Motivated {{by the fact that}} exponential-space algorithms are typically impractical, in this paper we address the problem of designing faster polynomial-space algorithms. Our first contribution is a simple O((27 / 4) k n O(log k)) -time polynomial-space algorithm for the problem. This algorithm is based on a variant of the classical tree-separator theorem: every Steiner tree has a node whose removal partitions the tree in two forests, containing at most 2 k/ 3 terminals each. Exploiting separators of logarithmic size which evenly partition the terminals, we are able to reduce the running time to O(4 k n O(log 2 k)). This improves on trivial enumeration for roughly k < n/ 3, which covers most of the cases of practical interest. Combining the latter algorithm (for small k) with trivial enumeration (for large k) we obtain a O(1. 59 n) -time polynomial-space algorithm for the weighted Steiner tree problem. As a second contribution of this paper, we present a O(1. 55 n) -time polynomial-space algorithm for the cardinality version of the problem, where all edge weights are one. This result is based on a improved branching strategy. The refined branching is based on a charging mechanism which shows that, for large values of k, convenient local configurations of terminals and non-terminals exist. The analysis of the algorithm relies on the Measure & Conquer approach: the <b>non-standard</b> <b>measure</b> used here is a linear combination of the number of nodes and number of non-terminals. Using a recent result in [Nederlof’ 09], the running time can be reduced to O(1. 36 n). The previous best algorithm for the cardinality case runs in O(1. 42 n) time and exponential space...|$|E
40|$|This thesis {{contributes}} to {{research on the}} effects of cognitive task complexity on the written complexity of second language learners, an area currently underrepresented in research. In addition, the variables pre-task planning time and post-task editing time, which bookend the writing process, are investigated, as are the potential effects of task motivation on syntactic output. This research was conducted on 94 intermediate English learners from three Auckland language schools. Two variations of the positivist/normative approach were used: an experimental model investigating cause and effect between the main variables, and an associative model to explore the strength of association between task motivation and syntactic output. Participants were placed in three groups in which three letter-writing tasks of varying cognitive complexity and Likert-scale questionnaires were performed over two sittings. Each group had different conditions; these were no planning time, 10 minutes pre-task planning time, and 10 minutes post-task editing time. Additional factors investigated were the potential benefits of using a patently low complexity task, the effects of modality, the use of a <b>non-standard</b> <b>measure</b> of syntactic complexity, and the efficacy of Robinson’s predictions for resource-directing and resource-dispersing variables. The results suggested that increases in cognitive task complexity may adversely effect dependant clause production, but benefit lexical production; however, the inclusion of pre-task planning time appeared to aid dependant clause production while slightly decreasing lexical complexity. Proficiency issues rendered the post-task data unusable. Potential support for Robinson prediction was posited for lexical complexity in the non-planning group, and both syntactic and lexical complexity in the planning time group; however, this support came with the caveat that the results were framed within a limited attentional framework. Overall, the patently low complexity task appeared useful, though the potential for the low complexity task to be a different task type remains a potential issue. Modality only appeared to benefit task complexity development when it was compounded with pre-task planning time. The new measure of syntactic complexity revealed significant results not noticeable using the older measure. Regarding task motivation and subordination, negative perceptions of task relevance and expectancy may have contributed to decreases in motivation and thus attention demanding syntax. However, a further negative construal was posited to have elicited either positive or neutral results. Negative task construal, and its theorised reduction of attention maintenance across Dörnyei’s (2002, 2003, 2009, 2010) motivational task processing system, may be especially effective during the attention demanding formulation of dependent clauses under increasing cognitive task complexity...|$|E
40|$|This thesis {{deals with}} the {{monetary}} policy of the European Central Bank. At {{the beginning of the}} work is described historical development. The next section focuses on the implementation of monetary policy, including the performance goals in the first twelve years of operation, with emphasis on the consequences of the global financial crisis. The last section summarizes the problems of the euro area financial and economic crisis of the years 2007 / 2008. In addition to the ECB's response to the crisis in the form of <b>non-standard</b> <b>measures</b> is decommissioned range of issues relating to fiscal policy and debt crisis euro area...|$|R
40|$|The thesis {{deals with}} the European Central Bank which is a central {{institution}} of the European Monetary Union; its establishment, structure, organs {{and its relationship to}} other European Union institutions. Core principles of monetary poli cy and instruments are described. The second part is dedicated to the analysis of monetary policy with focus on changes in the rate of main refinancing operations with respect to the economic development (inflation rate, GDP growth etc.). The time from 1999 is divided into several periods and particular attention is paid to the contemporary economic crisis and ECB's <b>non-standard</b> <b>measures...</b>|$|R
40|$|This paper {{describes}} {{the response of}} three central banks to the 2007 - 09 financial crisis: the European Central Bank, the Federal Reserve and the Bank of England. In particular, the paper discusses the design, implementation and impact of so-called "non-standard" monetary policy measures focusing on those introduced in the euro area {{in the aftermath of}} the failure of Lehman Brothers in September 2008. Having established the impact of these measures on various observable money market spreads, we propose an empirical exercise intended to quantify the macroeconomic impact of <b>non-standard</b> monetary policy <b>measures</b> insofar as it has been transmitted via these spreads. The results suggest that <b>non-standard</b> <b>measures</b> have played a quantitatively significant role in stabilising the financial sector and economy after the collapse of Lehman Bros., even if insufficient to avoid a significant fall in economic and financial activity. JEL Classification: E 52, E 58 financial crisis, Non-standard monetary policy...|$|R
40|$|Abstract. Although {{non-parametric}} {{tests have}} already been proposed for that purpose, sta-tistical signicance tests for <b>non-standard</b> <b>measures</b> (dierent from the classication error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole ", we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. A surprising conclusion is that paired tests such as McNemar badly fail when comparing models which are similar (such as SVMs with dierent kernels). On the other hand, non-parametric test...|$|R
40|$|Quantifying {{the loss}} in value of maize {{associated}} with insect damage {{can be difficult}} where commodities are sold using <b>non-standard</b> <b>measures</b> and where price is determined through bargaining, as in most African markets. This paper describes a methodology developed in Ghana in which panels of experienced maize traders priced prepared maize samples showing different levels of insect damage. The relative price of damaged maize was quite consistent across the markets studied. Of the grain characteristics evaluated, percent damaged and mould-discoloured grains was the most practical and reliable predictor of price. At harvest a 1 % increase in damaged grains decreased price on average by 1 %, but later more damage was tolerated as maize became more scarce...|$|R
40|$|International audienceWe {{introduce}} {{a notion of}} separativity for positively ordered monoids (POMs), similar in definition {{to the notion of}} separativity for commutative semigroups but which has a simple categorical equivalent, weaker that injectivity, the transfer property. We show that existence in a separative extension of the ground POM of a solution of a given linear system is equivalent to the satisfaction by the ground POM of a certain set of equations and inequalities, the resolvent. We deduce in particular a characterization of the POMs that are injective relatively to the class of embeddings of countable POMs; those include in particular divisible weak cardinal algebras. We also deduce that finitely additive positive <b>non-standard</b> <b>measures</b> invariant relatively to a given exponentially bounded group separate equidecomposability types modulo this group...|$|R
40|$|Abstract. In {{the last}} few years several methods to obtain the moments around the origin of the density of zeros of {{orthogonal}} polynomials have been developed. One of them generates these moments starting from the explicit expression of the monic orthogonal polynomial. In this paper the corresponding algorithm is constructed in the “Mathematica ” symbolic package context. A discussion of its goodness and applications to some interesting cases: <b>non-standard</b> <b>measures,</b> modifications of the recurrence relations, [...] ., are given. 1. Introduction. Let {Pn(x) }n∈N be a monic orthogonal polynomial sequence with respect to a positive measure σ(x) on some interval I ⊂ R. This sequence satisfies [1] a three-term recurrence relation of the form: Pn+ 1 (x) = (x − βn) Pn(x) − γnPn− 1 (x); n ≥ 1 (1...|$|R
40|$|We analyse the {{pass-through}} {{of monetary}} policy measures to lending rates to firms and {{households in the}} euro area using a unique bank-level dataset. Bank balance sheet characteristics such as the capital ratio and the exposure to sovereign debt {{are responsible for the}} heterogeneity of pass-through of conventional monetary policy changes. The location of a bank is instead irrelevant. <b>Non-standard</b> <b>measures</b> normalized the capacity of banks to grant loans resulting in a significant compression in lending rates. Banks {{with a high level of}} non-performing loans and a low capital ratio were the most responsive to the measures. Finally, we quantify the effects of non-standard policies on the real economic activity using a standard macroeconomic model and find that in absence of these measures both inflation and output would have been significantly lower...|$|R
40|$|This Letter initiates {{the study}} {{of what we call}} non-chiral {{staggered}} Virasoro modules, indecomposable modules on which two copies of the Virasoro algebra act with the two zero-modes acting non-semisimply. This is motivated by the "puzzle" recently reported in arXiv: 1110. 1327 [math-ph] involving a <b>non-standard</b> <b>measured</b> value, meaning that the value is not familiar from chiral studies, for the "b-parameter" (logarithmic coupling) of a c= 0 bulk conformal field theory. Here, an explanation is proposed by introducing a natural family of bulk modules and showing that the only consistent, non-standard logarithmic coupling that is distinguished through structure is that which was measured. This observation is shown to persist for general central charges and a conjecture is made for the values of certain non-chiral logarithmic couplings. Comment: 10 pages; v 2 : 11 pages, some modifications to introduction, added conclusions and reference...|$|R
