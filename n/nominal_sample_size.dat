3|10000|Public
40|$|Incomplete data {{presents}} {{a problem in}} both inferential and predictive modeling applications. Traditional methods such as complete case analysis and simple imputation tend to produce results that inadequately estimate standard errors and/or parameter estimates, particularly where missingness is not completely at random (MCAR). Modern methods for handling incomplete data, including maximum likelihood parameter estimation and multiple imputation methods, enable researchers to derive appropriate parameter estimates and inference from incomplete data when data are missing at random (MAR). However, {{for a variety of}} reasons, multiple imputation is not always practical, particularly with very large data sets or in applications where {{it is important to have}} only one working data set (such as public sector data). This paper illustrates the use of PROC MI, which provides maximum likelihood (ML) estimates of the covariance matrix and mean vector using the expectation-maximization (EM) algorithm. Using simulations, a number of incomplete data handling methods and resulting parameter estimates, standard errors, and error rates in classification of a validation data set are compared and evaluated in the context of discriminant analysis. Additionally, the proper specification of an appropriate <b>nominal</b> <b>sample</b> <b>size</b> for the analysis when the ML covariance matrix is used as input to the analysis is discussed. Results suggest that the covariance estimates from the EM algorithm produce results superior to other methods evaluated, particularly with an appropriately specified <b>nominal</b> <b>sample</b> <b>size.</b> Suggestions for handling missingness in the context of predictive and inferential modeling applications are presented...|$|E
40|$|Most {{national}} health surveys {{do not permit}} precise measurement of the health of racial/ethnic subgroups that comprise < 1 per cent of the U. S. population. We identify three potentially promising sample design strategies for increasing the accuracy of {{national health}} estimates for a small target subgroup when used to supplement a small probability sample of that group and apply these strategies to American Indians/Alaska Natives (AI/AN) and Chinese using National Health Interview Survey data. These sample design strategies include (1) complete sampling of targets within households, (2) oversampling selected macrogeographic units, and (3) oversampling from an incomplete list frame. Stage (1) is promising for Chinese and AI/AN; (2) works for both groups, {{but it would be}} more cost-effective for AI/AN because of their greater residential concentration; (3) is somewhat effective for groups like Chinese with viable surname lists, but not for AI/AN. Both (2) and (3) efficiently improve measurement precision when the supplement is {{the same size as the}} existing core sample, with diminishing additional returns as the supplement grows relative to the core sample, especially for (3). To avoid large design effects, the oversampled geographic areas or lists must have good coverage of the target population. To reduce costs, oversampled geographic tracts and lists must consist primarily of targets. These techniques can be used simultaneously to substantially increase effective sample sizes (ESSs). For example, (1) and (2) in combination can be used to multiply the <b>nominal</b> <b>sample</b> <b>size</b> of AI/AN or Chinese by 8 and the ESS by 4. Copyright © 2008 John Wiley & Sons, Ltd...|$|E
40|$|This {{dissertation}} studies statistical {{properties and}} {{applications of the}} Markov switching models for economic time series in five separate papers. The two main statistical themes are (i) the task of choosing the number of states {{to use in the}} model, and (ii) inference on time-varying transition probabilities. Our empirical applications span a wide array of topics in international finance and macroeconomics. Specifically, we study the dynamics of nominal exchange rates, interest rates, the business cycle and currency crises. In the first paper, ?Exchange Rates and Markov Switching Dynamics (joint work with Yin-Wong Cheung), we develop a simulation procedure to construct a two-sided test for testing hypotheses regarding the number of states in an empirical data-set. We use the test to resolve conflicting results regarding the existence of Markov switching dynamics in three dollar-denominated exchange rates. Our test shows that some of the earlier data-sets were not informative enough to draw a conclusion upon, with the result of previous tests sometimes contradicting each other. In data spanning 1973 - 1998 and on the monthly and higher frequency, we do, however, find conclusive evidence of Markov switching dynamics. The second paper, ?Regime Switches in Swedish Interest Rates?, uses a similar simulation approach to arrive at the conclusion of three states in a set of weekly data on Swedish interbank offered interest rates. A short-lived and highly volatile states pin-point the repeated speculative attacks on the Swedish krona in the early 90 s. With the states in place, we are able to obtain conditional normality even in this extremely leptokurtic time-series. Furthermore, we show that the specification is quite apt at forecasting interest rate volatility, and the evidence in favor of the model versus a number of competing alternatives is strong for all the forecast horizons we have applied. Turning to the dynamics of Markov models with time-varying transitions probabilities, the third paper, ?Constructing Early-Warning Systems: A Modified Markov Switching Approach? sets out to investigate the relatively infrequent use of this intuitive extension to the original model. It is shown in a simple theoretical setting, and in several simulation exercises, that the maximum likelihood estimator overemphasizes the short-run effects of predictors of transition probabilities in modestly sized samples. This leads to extreme parameter estimates and projected transition probabilities with too abrupt behavior. We introduce a penalized estimator, which is able to reduce parameter estimates toward their true magnitudes, and also increases correlation between the projected and true transition probabilities, and offer some suggestions on how to choose the magnitude of the penalty. In an application to the U. S. business cycle, we show that the penalized estimator yields a more parsimonious model with better forecasting properties in the medium to long term compared to its non-penalized counterpart. In the fourth paper, ?Transition Variables in the Markov Switching Model: Some Small Sample Properties?, we offer a note of caution in terms of using time-varying transition probabilities. We argue that rather than the <b>nominal</b> <b>sample</b> <b>size,</b> as measured {{by the total number of}} observations in the time-series, the number of regime switches should be considered instead when making inference on variable significance in the transition equations. By simulation, it is shown that for many cases the likelihood ratio statistic is over-sized, leading to too many transition variables being deemed significant. The size-distortion is not only dependent upon the number of switches in the data, but also the degree of uncertainty inherent in it and the persistence in the regressed variable. We suggest a straightforward, but computationally intensive approach to obtaining statistics with proper size. Looking at a number of business cycle predictors, we show that their statistics must be adjusted by a considerable magnitude to reflect true confidence levels. For model specification, this consequently has marked effect. The last paper (which is joint work with Guillaume Arias), ?Regime Switching as an Alternative Early-Warning System of Currency Crises: An Application to South-East Asia? is devoted to inferring indicators predicting the on-set of the South-East Asian currency crises in 1997. We survey the current literature in the area, and based on this, derive a number of possible determinants of currency crises. In a framework based on the Markov switching model with time-varying transition probabilities, we evaluate these determinants and specify models for forecasting of oncoming crises in the medium to long term. We show that the model is quite efficient in describing the underlying data and that it possesses relatively good forecasting properties...|$|E
40|$|This thesis {{performs}} a quantitative study, {{derived from the}} Neyman-Pearson framework, on the robustness of the matched filter detector corrupted by zero mean, independent and identically distributed white Gaussian noise. The variance of the noise {{is assumed to be}} imperfectly known, but some knowledge about a nominal value is presumed. We utilized slope as a unit to quantify the robustness for different signal strengths, <b>nominals,</b> and <b>sample</b> <b>sizes.</b> Following to this, a weighting method is applied to the slope range of interest, the so called tolerable range, as to analyze the likelihood of these extreme slopes to occur. A ratio of the first and last quarter section of the tolerable range have been taken in order to obtain the likelihood ratio for the low slopes to occur. We finalized our analysis by developing a method that quantifies confidence as a measure of robustness. Both weighted and non-weighted procedures were applied over the tolerable range, where the weighted procedure puts greater emphasis on values near the nominal. The quantitative analysis results show the detector to be non-robust and deliver poor performance for low signal-to-noise ratios. For moderate signal strengths, the detector performs rather well if the <b>nominal</b> and <b>sample</b> <b>size</b> are chosen wisely. The detector has great performance and robustness for high signal-to-noise ratios. This even remains true when only a few samples are taken or when the practitioner is uncertain about the nominal chosen...|$|R
40|$|Population subgroups {{defined by}} {{demographic}} and other characteristics are often an important {{focal point of}} samples in telephone surveys. We consider a class of two-stratum telephone sample designs where part of the frame with higher subgroup concentration (the first stratum) is disproportionately sampled compared {{to the rest of}} the frame (the second stratum). The relative intensity of sampling in the first versus the second stratum (r) thereby determines the gain in <b>nominal</b> subgroup <b>sample</b> <b>size.</b> Using proportionate <b>sampling</b> as the referent standard, we first compare the effect of r on the nominal and effective change in <b>sample</b> <b>sizes.</b> We then develop the optimum solution for r considering the dampening effect of variable sample weights on effective <b>sample</b> <b>size</b> (due to the varying sampling intensity between strata). Finally, as sample attrition and thus unit costs vary between strata, we also develop a solution for optimum r considering both variable weights and cost. In all findings, we take into account the impact of the correlation between the sample weights and key survey measurements and apply our results to two recent telephone surveys...|$|R
40|$|We {{investigate}} {{the properties of}} Johansen''s (1988, 1991) maximum eigenvalue and trace tests for cointegration under the empirically relevant situation of near-integrated variables. Using Monte Carlo techniques, we show that in a system with near-integrated variables, the probability of reaching an erroneous conclusion regarding the cointegrating rank {{of the system is}} generally substantially higher than the nominal size. The risk of concluding that completely unrelated series are cointegrated is therefore non-negligible. The spurious rejection rate can be reduced by performing additional tests of restrictions on the cointegrating vector(s), although it is still substantially larger than the nominal size. Economic models;cointegration, inflation, nominal interest rate, equation, <b>nominal</b> interest rates, <b>sample</b> <b>size,</b> statistics, real exchange rates, probability, time series, monte carlo simulations, real interest rate, confidence intervals, confidence interval, correlations, statistic, inflation rate, maximum likelihood estimation, predictability, vector autoregression, statistical model, equations, econometrics, constant term, maximum likelihood estimator, samples, number of variables, inflation rates, monte carlo simulation, hypothesis testing, correlation, monetary economics...|$|R
40|$|Background: In the {{industrial}} setting, injuries {{to the low}} back account for {{a large percentage of}} Workers 2 ̆ 7 Compensation claims. The physicians of McKenzie-Willamette Occupational Health Department, Paul Panum, :MD, Robert Davis, :MD, and Phillip Dean, MD, each treat patients independently. The {{purpose of this study was}} to gather general information on the low back pain patients seen while discovering any significant differences in the treating practices of the physicians. Methods: All medical records for Panum, Davis and Dean patients seen for low back pain between January 1, 2000 and May 1, 2001 were identified and reviewed. Data was collected by one analyst on a flow· sheet and entered into a computer spreadsheet for analysis. Data included patient demographics, prescribing patterns, use of diagnostic studies, frequency of specialist referrals and duration of claim information. ANOVA testing was done for comparison of means. Chi square testing was done to analyze <b>nominal</b> data. Results: <b>Sample</b> <b>sizes</b> were 126, 93 and 36 for Panum, Davis and Dean respectively. Significant results included a difference in mean days from initial evaluation until final evaluation (...|$|R
40|$|Epidemiologists {{sometimes}} {{study the}} association between two measures of exposure on the same subjects by grouping the data into categories that are defined by sample quantiles of the two marginal distributions. Although such grouped data are presented in a twoway contingency table, the cell counts in this table {{do not have a}} multinomial distribution. We use the term “bivariate quantile distribution” (BQD) to describe the joint distribution of counts in such a table. Blomqvist (1950) gave an exact BQD theory for the case of only 4 categories based on division at the sample medians. The asymptotic theory he presented was not valid, however, except in special cases. We present a valid asymptotic theory for arbitrary numbers of categories and apply this theory to construct confidence intervals for the kappa statistic. We show by simulations that the confidence interval procedures we propose have near <b>nominal</b> coverage for <b>sample</b> <b>sizes</b> exceeding 90, both for 2 x 2 and 3 x 3 tables. These simulations also illustrate that the asymptotic theory of Blomqvist (1950) and the methods given by Fleiss, Cohen and Everitt (1969) for multinomial sampling can yield subnominal coverage for BQD data, although in some cases the coverage for these procedures is near nominal levels...|$|R
30|$|The <b>sample</b> <b>size</b> was {{selected}} based on Comrey and Lee (1992) inferential statistics. According to this statistic, a <b>sample</b> <b>size</b> of below 50 respondents is a weaker <b>sample,</b> a <b>sample</b> <b>size</b> of 100 respondents is weak, 200 respondents <b>sample</b> <b>size</b> is adequate, 300 is good, 500 is very good, and 1000 is excellent. Therefore, a <b>sample</b> <b>size</b> of two hundred (200) respondents {{was selected}}.|$|R
40|$|In {{this paper}} we discuss the <b>sample</b> <b>size</b> problem for {{balanced}} one way ANOVA under a posterior Bayesian formulation of the problem. Using the distribution theory of appropriate quadratic forms we derive explicit <b>sample</b> <b>sizes</b> for prespeci ed posterior precisions. Comparisons with classical <b>sample</b> <b>sizes</b> are made. Instead of extensive tables, a mathematica program for <b>sample</b> <b>size</b> calculation is given. The formulations given in this article form a foundational step towards Bayesian calculation of <b>sample</b> <b>size,</b> in general. Key words and phrases: <b>Sample</b> <b>size</b> problem, ANOVA, Bayesian point of view...|$|R
30|$|The <b>sample</b> <b>size</b> {{within the}} {{statistical}} analysis must be defined to ensure reliability and range. In conventional statistical analyses, the <b>sample</b> <b>size</b> is defined to guarantee target reliability levels and range. However, <b>sampling</b> <b>size</b> was fixed in this study; thus, reliability level and range were calculated to validate <b>sampling</b> <b>size.</b>|$|R
30|$|The {{bootstrap}} method {{may not be}} reliable for very small <b>sample</b> <b>sizes,</b> regardless of how many bootstrap samples are generated. Thus, a certain <b>sample</b> <b>size</b> should be acquired. In this study, the <b>sample</b> <b>size</b> of each drying test was more than 100, so a sufficient <b>sample</b> <b>size</b> was prepared for the bootstrap analysis.|$|R
40|$|Public {{transport}} {{planners are}} required to make decisions on transport infrastructure and services worth billions of dollars. The decision-making process for transport planning needs to be informed, accountable, and founded on comprehensive, current, and reliable data. One of the major issues affecting {{the accuracy of the}} estimated origin-destination (O-D) matrices is <b>sample</b> <b>size.</b> Cost, time, precision, and biases are some issues associated with <b>sample</b> <b>size.</b> Smart card data can potentially provide much information based on better understanding and assessment of the <b>sample</b> <b>size</b> impact on the estimated O-D matrices. This paper uses South East Queensland (SEQ) data to study the effect of different data <b>sample</b> <b>sizes</b> on the accuracy level of the generated public transport O-D matrices and to quantify the <b>sample</b> <b>size</b> required for a certain level of accuracy. As a result, the total number of O-D trips for the whole network can be accurately estimated at all levels of <b>sample</b> <b>sizes.</b> However, a wide distribution of O-D trips appeared at different <b>sample</b> <b>sizes.</b> The large difference from the actual distribution at 100 % <b>sample</b> <b>size</b> was readily captured at small <b>sample</b> <b>sizes</b> where more O-D pairs were not representative. The wide distribution of O-D trips at different levels of <b>sample</b> <b>sizes</b> caused significant errors even at large <b>sample</b> <b>sizes.</b> The variation of the errors within the same sample was also captured {{as a result of the}} 80 iterations for each <b>sample</b> <b>size.</b> It is concluded that three major parameters (distribution, number, and <b>sample</b> <b>size</b> of selected stations) have a significant impact on the estimated O-D matrices. These results can be also reflected on the <b>sample</b> <b>size</b> of the traditional O-D estimation methods, such household travel surveys...|$|R
40|$|This paper {{provides}} {{closed form}} expressions for the <b>sample</b> <b>size</b> for two-level factorial experiments when {{the response is}} the number of defectives. The <b>sample</b> <b>sizes</b> are obtained by approximating the two-sided test for no effect through tests for the mean of a normal distribution, and borrowing the classical <b>sample</b> <b>size</b> solution for that problem. The proposals are appraised relative to the exact <b>sample</b> <b>sizes</b> computed numerically, without appealing to any approximation to the binomial distribution, {{and the use of the}} <b>sample</b> <b>size</b> tables provided is illustrated through an example. factorial experiments, binary data, <b>sample</b> <b>size,</b> deviance,...|$|R
40|$|DThe paper reviews {{both the}} {{influencing}} factors and calculation strategies of <b>sample</b> <b>size</b> determination for survey research. It indicate {{the factors that}} affect the <b>sample</b> <b>size</b> determination procedure and explains how. It also provides calculation methods (including formulas) {{that can be applied}} directly and easily to estimate the <b>sample</b> <b>size</b> needed in most popular situations. Saudi Med J 2003; Vol. 24 (4) : 323 - 330 selected? Unfortunately, the answer to these questions are not as easy as the researcher desires. There are factors which influence determining <b>sample</b> <b>size</b> and others influence determining sampling design. The researcher needs to know these factors and their effect beforehand to succeed in determining the adequate <b>sample</b> <b>size.</b> This paper attempts to highlight the factors relevant to determining the minimum <b>sample</b> <b>size</b> needed for descriptive studies and introduce some useful strategies that can be employed for the purpose of <b>sample</b> <b>size</b> determination. Factors influencing determining <b>sample</b> <b>size.</b> Determining the adequate <b>sample</b> <b>size</b> is the most important design decision that faces the researcher. 2 Th {{reason for this is that}} using too low <b>sample</b> <b>size,</b> the research will lack the precision to provide reliable answers to the questions that are under investigation. Moreover, using too large <b>sample</b> <b>size,</b> time, and resources will be wasted often for minimal gain. As stated previously, there are factors playing a vital role in determining the <b>sample</b> <b>size.</b> Knowing these factors and their effect helps the researcher to determine the <b>sample</b> <b>Sample</b> <b>size</b> determination Influencing factors and calculation strategies for survey researc...|$|R
30|$|Table  3 {{also shows}} that the <b>sample</b> <b>size</b> varied across the groups. These <b>sample</b> <b>sizes</b> were chosen to {{resemble}} current large-scale assessments, where group <b>sample</b> <b>sizes</b> are typically large and vary per participating country. Values for <b>sample</b> <b>sizes</b> were randomly drawn from[*]~[*]N(5000, 1000) for 20 groups; every other <b>sample</b> <b>size</b> from the 20 -group pool {{was assigned to the}} 10 -group. Within each group, approximately the same number of simulees received each of the booklets, also assigned at random.|$|R
30|$|The overall <b>sample</b> <b>size</b> was {{determined}} by using the <b>sample</b> <b>size</b> determination equation {{that takes into account}} the desired confidence level (95 %), the error margin (5 %), and the prevalence of the issue under investigation (p[*]=[*] 0.5). The required <b>sample</b> <b>size</b> {{was determined}} using Kothari (2004) <b>sample</b> <b>size</b> determination formula. 28 households did not respond the major modules of the structured household survey and were considered as non-response cases (9.8 % of the total <b>sample</b> <b>size).</b>|$|R
40|$|This article {{introduces}} the general concepts {{and methods of}} <b>sample</b> <b>size</b> estimation and testing power analysis. It focuses on parametric methods of <b>sample</b> <b>size</b> estimation, including <b>sample</b> <b>size</b> estimation of estimating the population mean and the population probability. It also provides estimation formulas and introduces how to realize <b>sample</b> <b>size</b> estimation manually and by SAS software...|$|R
40|$|<b>Sample</b> <b>size</b> {{determination}} (SSD) is {{an important}} aspect of experimental design. In most comparative experiments, a decision about <b>sample</b> <b>size</b> must be made prior to data acquisition. This involves power analysis within a classical statistical framework. We are going to formulate required <b>sample</b> <b>size</b> determination within a Bayesian framework. Required <b>sample</b> <b>size</b> is chosen to achieve a pre-specified model performance criterion. We also take <b>sample</b> <b>size</b> determination into a model selection environment. Here a <b>sample</b> <b>size</b> is calculated to separate two different models. We also provide analytical results on the behavior of our <b>sample</b> <b>size</b> determination criteria when possible. ^ Determination of <b>sample</b> <b>size</b> is an issue frequently faced by practitioners. Bayesian methods are ideally suited to this design aspect. First, information is usually available prior to experiment. It is better to incorporate this prior information into the study at design stage. In fact, the prior can play the role of a “what if” specification, allowing the designer to assess Bayesian learning as a function of <b>sample</b> <b>size</b> over a range of specifications. Second, it is sensible to average over the sample space since the sample has not yet been observed and the general principle of averaging over what is unknown applies. ^ Historically, <b>sample</b> <b>size</b> determination has been confined primarily to one and two sample problems. We are aiming to address the <b>sample</b> <b>size</b> problem for more complicated modeling frameworks, e. g., the attractive hierarchical models which are the standard Bayesian environment. Simulation-based model fitting is customarily employed for inference under hierarchical models. For <b>sample</b> <b>size</b> determination we require replications of such simulation adequate to assess performance averaged over the sample space. As a result, our approach is computationally intensive and does not provide explicit formulas for required <b>sample</b> <b>size.</b> We present illustrative examples to show our <b>sample</b> <b>size</b> determination approaches. In summary, this dissertation involves addressing the <b>sample</b> <b>size</b> problem under Bayesian modeling for rather general classes of models. ...|$|R
40|$|The {{determination}} of <b>sample</b> <b>size</b> {{is a common}} task for many organizational researchers. Inappropriate, inadequate, or excessive <b>sample</b> <b>sizes</b> continue to influence the quality and accuracy of research. This manuscript describes the procedures for determining <b>sample</b> <b>size</b> for continuous and categorical variables using Cochran’s (1977) formulas. A discussion and illustration of <b>sample</b> <b>size</b> formulas, including the formula for adjusting the <b>sample</b> <b>size</b> for smaller populations, is included. A table is provided {{that can be used}} to select the <b>sample</b> <b>size</b> for a research problem based on three alpha levels and a set error rate. Procedures for determining the appropriate <b>sample</b> <b>size</b> for multiple regression and factor analysis, and common issues in <b>sample</b> <b>size</b> determination are examined. Non-respondent sampling issues are addressed. I n troduct ion A common goal of survey research is to collect data representative of a population. The researcher uses information gathered from the survey to generalize findings from a drawn sample back to a population...|$|R
40|$|This paper {{deals with}} the basic {{principles}} involved in <b>sample</b> <b>size</b> calculation of phase III cancer clinical trials. It illustrates the concepts and factors determining the <b>sample</b> <b>size.</b> Various examples of phase III cancer clinical trials are provided and the <b>sample</b> <b>size</b> is calculated {{taking into account the}} assumptions made. The examples provided include <b>sample</b> <b>sizes</b> for comparing proportions and <b>sample</b> <b>sizes</b> for comparing survival times, Several special topics are also discussed including choice of endpoint, number of treatment groups, factorial designs and equivalence trials...|$|R
40|$|To design {{clinical}} trials, efficiency, ethics, cost effectively, research {{duration and}} <b>sample</b> <b>size</b> calculations {{are the key}} things to remember. This review highlights the statistical issues to estimate the <b>sample</b> <b>size</b> requirement. It elaborates the theory, methods and steps for the <b>sample</b> <b>size</b> calculation in randomized controlled trials. It also emphasizes that researchers should consider the study design first and then choose appropriate <b>sample</b> <b>size</b> calculation method...|$|R
40|$|We {{prove the}} {{following}} conjecture of Narayana: {{there are no}} nontrivial dominance refinements of the Smirnov two-sample test {{if and only if}} the two <b>sample</b> <b>sizes</b> are relatively prime. We also count the number of natural significance levels of the Smirnov two-sample test in terms of the <b>sample</b> <b>sizes</b> and relate this to the Narayana conjecture. In particular, Smirnov tests with relatively prime <b>sample</b> <b>sizes</b> turn out to have many more natural significance levels than do Smirnov tests whose <b>sample</b> <b>sizes</b> are not relatively prime (for example, equal <b>sample</b> <b>sizes)</b> ...|$|R
40|$|Quality of {{clinical}} trials has improved steadily over last two decades, but certain areas in trial methodology still require special attention like in <b>sample</b> <b>size</b> calculation. The <b>sample</b> <b>size</b> {{is one of}} the basic steps in planning any clinical trial and any negligence in its calculation may lead to rejection of true findings and false results may get approval. Although statisticians {{play a major role in}} <b>sample</b> <b>size</b> estimation basic knowledge regarding <b>sample</b> <b>size</b> calculation is very sparse among most of the anesthesiologists related to research including under trainee doctors. In this review, we will discuss how important <b>sample</b> <b>size</b> calculation is for research studies and the effects of underestimation or overestimation of <b>sample</b> <b>size</b> on project′s results. We have highlighted the basic concepts regarding various parameters needed to calculate the <b>sample</b> <b>size</b> along with examples...|$|R
50|$|<b>Sample</b> <b>size</b> {{determination}} is the act {{of choosing}} the number of observations or replicates to include in a statistical <b>sample.</b> The <b>sample</b> <b>size</b> is an important feature of any empirical study in which {{the goal is to}} make inferences about a population from a sample. In practice, the <b>sample</b> <b>size</b> used in a study is determined based on the expense of data collection, and the need to have sufficient statistical power. In complicated studies there may be several different <b>sample</b> <b>sizes</b> involved in the study: for example, in a stratified survey there would be different <b>sample</b> <b>sizes</b> for each stratum. In a census, data are collected on the entire population, hence the <b>sample</b> <b>size</b> is equal to the population size. In experimental design, where a study may be divided into different treatment groups, this may be different <b>sample</b> <b>sizes</b> for each group.|$|R
40|$|Abstract:Engine {{parameters}} {{vary from}} one cycle {{to the other}} and this makes engine analysis with data from a single working cycle insufficient in capturing or modelling an engine behaviour. The variation observed in engine has necessitated the use <b>sample</b> <b>sizes</b> of data obtained during an engine operation to obtain results that are representative of the engine being investigated. Research has shown that the use of very large data <b>sample</b> <b>size</b> increases the storage needed and processing time and does not necessary give better results over results obtained with lesser <b>sample</b> <b>sizes.</b> The number of <b>sample</b> <b>size</b> to use for analysis remains a subject of debate and investigation with researchers proposing the use of varying <b>sample</b> <b>sizes</b> for combustion analysis in engines. There is a need for the selection of an optimum <b>sample</b> <b>size</b> for engine analysis. Engine data were obtained from a spark ignition engine which operated on gasoline and varying degree of blend of gasoline and biofuel. The effects of the use of <b>sample</b> <b>sizes</b> of 20, 40, 60 and 100 on the result of the analysis were determined. The percentage difference and the mean percentage difference for each of the <b>sample</b> <b>sizes</b> tested relative to the maximum available <b>sample</b> <b>size</b> were determined too. Based on results from the analysis, it was suggested that <b>sample</b> <b>sizes</b> that gave mean percentage difference values within the range ± 1. 5 relative to the maximum available <b>samples</b> <b>size</b> are appropriate for use in combustion analysis in engines...|$|R
3000|$|The <b>sample</b> <b>size</b> was {{statistically}} estimated using <b>sample</b> <b>size</b> formula for descriptive studies as follows: n[*]=[*]Z [...]...|$|R
40|$|Background and purpose: <b>Sample</b> <b>size</b> and its {{determination}} {{is one of}} {{the most}} important problems in health researches. Calculating <b>sample</b> <b>size</b> for prevalence studies {{is one of the}} common questions of <b>sample</b> <b>size</b> topics. Minimum <b>sample</b> <b>size</b> with least complexity is desirable in order to achieve the basic goal of these studies. This study aims to compare two formulas of <b>sample</b> <b>size</b> calculation for prevalence researches and finally, to use the simplest formula to get the most appropriate <b>sample</b> <b>size.</b> Methods <b>Sample</b> <b>size</b> for proportions: 0. 9, 0. 95, 0. 99, 0. 999 candidates of p close to 1 proportions 10 - 5, 10 - 4, 10 - 3, 10 - 2, 0. 05, 0. 1 candidates of p close to 0, and proportions 0. 3, 0. 4, 0. 5, 0. 6, 0. 7 candidates of p close to 0. 5 were calculated. For comparing n 1, n 2 φ =n_ 1 ⁄n_ 2, it was computed by R package (2. 10. 1). Results Computed <b>sample</b> <b>size</b> by (f 2) is lightly greater than <b>sample</b> <b>size</b> computed by (f 1) and maximum value of φ index for comparing the two formulas equals 1. Conclusion Results show that the calculated <b>sample</b> <b>size</b> by (f 1) is similar to what was obtained by (f 2), though, according to its interpretation and easy computation,it is suggested for all values of p...|$|R
40|$|<b>Sample</b> <b>size</b> {{determination}} {{is one of}} {{the central}} tenets of medical research. If the <b>sample</b> <b>size</b> is inadequate, then the study will fail to detect a real difference between the effects of two clinical approaches. On the contrary, if the <b>sample</b> <b>size</b> is larger than what is needed, the study will become cumbersome and ethically prohibitive. Apart from this, the study will become expensive, time consuming and will have no added advantages. A study which needs a large <b>sample</b> <b>size</b> to prove any significant difference in two treatments must ensure the appropriate <b>sample</b> <b>size.</b> It is better to terminate such a study when the required <b>sample</b> <b>size</b> cannot be attained so that the funds and manpower can be conserved. When dealing with multiple sub-groups in a population the <b>sample</b> <b>size</b> should be increased the adequate level for each sub-group. To ensure the reliability of final comparison of the result, the significant level and power must be fixed before the <b>sample</b> <b>size</b> determination. <b>Sample</b> <b>size</b> determination is very important and always a difficult process to handle. It requires the collaboration of a specialist who has good scientific knowledge in the art and practice of medical statistics. A few suggestions are made in this paper regarding the methods to determine an optimum <b>sample</b> <b>size</b> in descriptive and analytical studies...|$|R
50|$|The <b>sample</b> <b>size</b> {{determines the}} amount of {{sampling}} error inherent in a test result. Other things being equal, effects are harder to detect in smaller <b>samples.</b> Increasing <b>sample</b> <b>size</b> is often {{the easiest way to}} boost the statistical power of a test. How increased <b>sample</b> <b>size</b> translates to higher power {{is a measure of the}} efficiency of the test—for example, the <b>sample</b> <b>size</b> required for a given power.|$|R
40|$|Bayesian <b>sample</b> <b>size</b> {{determination}} in {{a randomized}} controlled trial (RCT) with continuous outcome data {{is dependent on}} an initial belief about the common unknown variance {{in the form of}} a prior distribution. A disagreement between this prior and the observed variance can lead to a poor estimate of the required <b>sample</b> <b>size.</b> <b>Sample</b> <b>size</b> re-estimation, thoroughly discussed in the frequentist framework, is an obvious alternative. Unfortunately, it has rarely been suggested when re-estimation is appropriate. In this paper, a <b>sample</b> <b>size</b> determination procedure is extended to allow <b>sample</b> <b>size</b> re-estimation. In addition, a Bayesian predictive approach based on credible intervals is proposed to establish when re-estimation is appropriate in combination with a maximum available <b>sample</b> <b>size</b> as a realistic constraint. This approach is shown to provide stable estimates of the required <b>sample</b> <b>size</b> in the face of limited prior information and a maximum available <b>sample</b> <b>size</b> often encountered in RCTs in small populations. This is further illustrated using data from a realized randomized trial in the field of pediatrics...|$|R
40|$|To {{assess how}} often {{calculation}} of <b>sample</b> <b>sizes</b> {{were reported in}} leading ophthalmology journals. Study Design Retrospective literature survey. <b>Sample</b> <b>size</b> calculations should {{be a part of}} the methods and published report of diagnostic performance studies. Currently, they are not being reported in the ophthalmology literature. Results A total of 1698 articles were identified, of which 40 studies were on diagnostic accuracy. One study reported that <b>sample</b> <b>size</b> was calculated before initiating the study. Another study reported consideration of <b>sample</b> <b>size</b> without calculation. The mean (SD) <b>sample</b> <b>size</b> of all diagnostic studies was 172. 6 (218. 9). The median prevalence of the target condition was 50. 5 %. Conclusions <b>Sample</b> <b>size</b> calculations should {{be a part of the}} methods and published report of diagnostic performance studies. Currently, they are not being reported in the ophthalmology literature. Only a few studies consider <b>sample</b> <b>size</b> in their methods. Inadequate <b>sample</b> <b>sizes</b> in diagnostic accuracy studies may result in misleading estimates of test accuracy. An improvement over the current standards on the design and reporting of diagnostic studies is warranted...|$|R
40|$|One {{concern in}} the early stages of study {{planning}} and design is the minimum <b>sample</b> <b>size</b> needed to provide statistically credible results. This minimum <b>sample</b> <b>size</b> is usually determined via the use of simple formulas, or equivalently, from tables. However, the more popular formulas involve large-sample approximations and hence may be too conservative. This article provides empirical evidence indicating that this conservatism is drastic for certain <b>sample</b> <b>size</b> formulas based on confidence interval width. Common <b>sample</b> <b>size</b> formulas that consider statistical power are also discussed; these are shown to perform quite well, even for small <b>sample</b> <b>size</b> situation...|$|R
40|$|CDATA[Research {{has become}} {{mandatory}} for career advancement of medical graduates. Researchers are often confounded by {{issues related to}} calculation of the required <b>sample</b> <b>size.</b> Various factors like level of significance, power of the study, effect size, precision and variability affect <b>sample</b> <b>size.</b> Also design issues like sampling technique and loss to follow up {{need to be considered}} before calculating <b>sample</b> <b>size.</b> Once these are understood, the researcher can estimate the required <b>sample</b> <b>size</b> using softwares like Open Epi. Correct estimation of <b>sample</b> <b>size</b> is important for the internal validity of the study and also prevents unnecessary wastage of resources. ]]...|$|R
3000|$|... grow rapidly {{when the}} <b>sample</b> <b>sizes</b> increase. Therefore, we can, so far, only target small <b>sample</b> <b>sizes</b> in our work.|$|R
5000|$|The <b>sample</b> <b>size</b> is {{relatively}} large (say, n > 10— and R charts are typically used for smaller <b>sample</b> <b>sizes)</b> ...|$|R
