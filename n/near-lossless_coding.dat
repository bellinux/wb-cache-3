7|6|Public
40|$|In {{this paper}} {{we present a}} new {{watermarking}} algorithm for joint near-lossless compression and authentication of remote sensing images. The adopted compression algorithm is the standard JPEG-LS algorithm. Our methodology has been designed by integrating into the standard JPEG-LS compression algorithm, {{by means of a}} stripe approach, a known authentication technique derived from Fridrich. This procedure points out two advantages: firstly, the produced bit-stream is perfectly compliant with the JPEG-LS standard, secondly, when the image has been decoded, it is always authenticated because information has been embedded in the reconstructed values. <b>Near-lossless</b> <b>coding</b> does not harm authentication procedure and robustness against different attacks is preserved...|$|E
40|$|Systems using low bit-rate coding of {{high quality}} digital audio signals will be in {{widespread}} use for consumer applications in the near future. Similar systems are needed for professional applications. They need to exhibit a much larger ratio of the audibility threshold of artifacts to the actual noise level. The term <b>near-lossless</b> <b>coding</b> is introduced to describe such coding systems. Standard low bitrate audio coding systems {{can be used for}} these applications, but are much less efficient compared to specially optimized coding schemes. First results on the feasibility of near-lossless high quality audio coding systems using transform coding techniques are presented. The results are based on simulation runs at bitrates between 3 and 8 bits/sample. They are tested using both a 'maximum residual noise amplitude' criterion and a perceptual measurement technique (NMR) ...|$|E
40|$|Based on message-passing techniques, a novel {{iterative}} grid algorithm for {{the general}} two-dimensional (2 D) digital least metric (DLM) problem is proposed and applied to image halftoning. The algorithm attempts to achieve a globally optimal solution via a local-metric computation and message passing as opposed to other 2 D iterative globalmetric optimizations such as simulated annealing and toggle /swap scheme. A reduced-complexity version of the proposed digital image halftoning technique is demonstrated. Results show {{that the quality of}} the halftone images is comparable to that of the state-of-the-art toggle/swap algorithm. Since the algorithm is not constrained by the specific metric used in this work, the proposed method is directly applicable to other digital image processing tasks (e. g., optimal <b>near-lossless</b> <b>coding</b> or entropy-constrained halftoning). An expanded set of results from this work can be viewed at [URL] 1...|$|E
40|$|A {{multiple}} access source code (MASC) {{is a source}} code designed for the following network configuration: a pair of correlated information sequences {X-i}(i= 1) (infinity), and {Y-i}(i= 1) (infinity) is drawn independent and identically distributed (i. i. d.) according to joint probability mass function (p. m. f.) p(x, y); the encoder for each source operates without knowledge of the other source; the decoder jointly decodes the encoded bit streams from both sources. The work of Slepian and Wolf describes all rates achievable by MASCs of infinite coding dimension (n [...] > infinity) and asymptotically negligible error probabilities (P-e((n)) [...] > 0). In this paper, we consider the properties of optimal instantaneous MASCs with finite coding dimension (n 0) performance. The interest in <b>near-lossless</b> <b>codes</b> is inspired by the discontinuity in the limiting rate region at P-e((n)) = 0 and the resulting performance benefits achievable by using near-lossless MASCs as entropy codes within lossy MASCs. Our central results include generalizations of Huffman and arithmetic codes to the MASC framework for arbitrary p(x, y), n, and P-e((n)) and polynomial-time design algorithms that approximate these optimal solutions...|$|R
40|$|A Slepian-Wolf {{coding scheme}} for {{compressing}} two uniform memoryless binary sources using a single channel code that can achieve arbitrary rate allocation among encoders was {{outlined in the}} work of Pradhan and Ramchandran. Inspired by this work, we address the problem of practical code design for general multiterminal lossless networks where multiple memoryless correlated binary sources are separately compressed and sent; each decoder receives a set of compressed sources and attempts to jointly reconstruct them. First, we propose a <b>near-lossless</b> practical <b>code</b> design for the Slepian-Wolf system with multiple sources. For two uniform sources, if the code approaches the capacity of the channel that models the correlation between the sources, then the system will approach the theoretical limit. Thus, the great advantage of this design method is its possibility to approach the theoretical limits with a single channel code for any rate allocation among the encoders. Based on Slepian-Wolf code constructions, we continue with providing practical designs for the general lossless multiterminal network which consists of an arbitrary number of encoders and decoders. Using irregular repeat-accumulate and turbo codes in our designs, we obtain the best results reported so far and almost reach the theoretical bounds...|$|R
40|$|Medical imaging {{technology}} and applications are continuously evolving, dealing {{with images of}} increasing spatial and temporal resolutions, which allow easier and more accurate medical diagnosis. However, this increase in resolution demands a growing amount of data to be stored and transmitted. Despite the high coding efficiency achieved by the most recent image and video coding standards in lossy compression, they are not well suited for quality-critical medical image compression where either <b>near-lossless</b> or lossless <b>coding</b> is required. In this dissertation, two different approaches to improve lossless coding of volumetric medical images, such as Magnetic Resonance and Computed Tomography, were studied and implemented using the latest standard High Efficiency Video Encoder (HEVC). In a first approach, the use of geometric transformations to perform inter-slice prediction was investigated. For the second approach, a pixel-wise prediction technique, based on Least-Squares prediction, that exploits inter-slice redundancy was proposed to extend the current HEVC lossless tools. Experimental results show a bitrate reduction between 45 % and 49 %, when compared with DICOM recommended encoders, and 13. 7 % when compared with standard HEVC...|$|R
40|$|We propose an integrated, wavelet based, {{two-stage}} {{coding scheme}} for lossy, near-lossless and lossless compression of medical volumetric data. The method presented determines the bit-rate while encoding for the lossy layer {{and without any}} iteration. It is {{in the spirit of}} “lossy plus residual coding ” and consists of a wavelet-based lossy layer followed by an arithmetic coding of the quantized residual to guarantee a given pixel-wise maximum error bound. We focus on the selection of the optimum bit rate for the lossy coder to achieve the minimum total (lossy plus residual) bit rate in the near-lossless and the lossless cases. We propose a simple and practical method to estimate online the optimal bit rate and provide a theoretical justification for it. Experimental results show that the proposed scheme provides improved, embedded lossy, and lossless performance competitive with the best results published so far in the literature, with an added feature of <b>near-lossless</b> <b>coding...</b>|$|E
40|$|An {{iterative}} grid message-passing algorithm for model-based {{digital image}} halftoning is introduced. Based {{on the standard}} message-passing algorithm on the grid graphical model, the algorithm is designed to suboptimally solve general two-dimensional (2 D) digital least metric (DLM) problems and {{is found to be}} very successful (i. e., nearly optimal) for 2 D data detection in page-oriented optical-memory (POM) systems. In contrast to many 2 D (iterative) optimization techniques, this grid algorithm attempts to achieve a globally optimal solution via a local-metric computation and message-passing scheme. Using a reduced-complexity technique, the simplified grid algorithm is proposed for the halftoning problem and is shown to provide similar image quality as compared to the best halftoning algorithms in the literature. Since the grid algorithm does not exploit the properties of a specific metric, it is directly applicable to other digital image processing tasks (e. g., optimal <b>near-lossless</b> <b>coding,</b> entropy-constrained halftoning, or image/video dependent quantization). Keywords: Model-based digital image halftoning, iterative message-passing algorithms, grid algorithm 1...|$|E
40|$|In {{near-lossless}} image coding, each reconstructed pixel of the decoded image {{differs from}} the corresponding one in the original image by not more than a pre-specified value δ. Such schemes are mainly based on predictive coding techniques, which are not capable of scalable decoding. Lossless image coding with scalable decoding is mainly based on integer wavelet transforms. In this paper, methods to modify integer wavelet transforms for near-lossless image coding with scalable decoding features are presented. This is achieved by incorporating the near-lossless quantisation process, driven by δ, into lifting steps (online quantisation). Two online quantisation techniques based on 1 -D and 2 -D transforms are presented. They outperform the pre-quantisation based near-lossless image coding method in both bit rate and rms error performances. Further, they result in both subjectively and objectively superior performance in spatial and bit rate scalable decoding. The 2 -D online scheme results in comparable performance with JPEG-LS, which is not capable of scalable decoding. It is evident from this research that with these novel schemes, scalable decoding features can be integrated into <b>near-lossless</b> <b>coding</b> with only a small increase in bit rate compared to those achieved in JPEG-LS. Keywords: Near-lossless image coding, online quantisation, scalable image coding, pre-quantisation, lifting, integer wavelet transforms, JPEG-LS 1...|$|E
40|$|Abstract—We {{present a}} {{compression}} technique that provides progressive transmission {{as well as}} lossless and near-lossless compression in a single framework. The proposed technique produces a bit stream that results in a progressive, and ultimately lossless, reconstruction of an image similar to what one can obtain with a reversible wavelet codec. In addition, the proposed scheme provides near-lossless reconstruction {{with respect to a}} given bound, after decoding of each layer of the successively refinable bit stream. We formulate the image data-compression problem as one of successively refining the probability density function (pdf) estimate of each pixel. Within this framework, restricting the region of support of the estimated pdf to a fixed size interval then results in near-lossless reconstruction. We address the contextselection problem, as well as pdf-estimation methods based on context data at any pass. Experimental results for both lossless and near-lossless cases indicate that the proposed compression scheme, that innovatively combines lossless, <b>near-lossless,</b> and progressive <b>coding</b> attributes, gives competitive performance in comparison with state-of-the-art compression schemes. Index Terms—Embedded bit stream, image compression, lossless compression, near-lossless compression, probability mass estimation, successive refinement. I...|$|R
40|$|The aim of {{this paper}} is to develop an {{effective}} loss less algorithm technique to convert original image into a compressed one. Here we are using a lossless algorithm technique in order to convert original image into compressed one. Without changing the clarity of the original image. Lossless image compression is a class of image compression algorithms that allows the exact original image to be reconstructed from the compressed data. We present a compression technique that provides progressive transmission as well as lossless and near-lossless compression in a single framework. The proposed technique produces a bit stream that results in a progressive and ultimately lossless reconstruction of an image similar to what one can obtain with a reversible wavelet codec. In addition, the proposed scheme provides near-lossless reconstruction with respect to a given bound after decoding of each layer of the successively refineable bit stream. We formulate the image data compression problem as one of successively refining the probability density function (pdf) estimate of each pixel. Experimental results for both lossless and near-lossless cases indicate that the proposed compression scheme, that innovatively combines lossless, <b>near-lossless</b> and progressive <b>coding</b> attributes, gives competitive performance in comparison to state-of-the-art compression schemes...|$|R
40|$|Application {{of error}} {{correcting}} codes for data compression is first investigated by Shannon where {{he suggests that}} there is a duality between source coding and channel coding. This duality implies that good channel codes are likely to be good source codes (and vice versa). Recently the problem of source coding using channel codes is receiving increasing attention. The main application of this problem is when data are transmitted over noisy channels. Since standard data compression techniques are not designed for error correction, compressing data and transmitting over noisy channels may cause corruption of the whole compressed sequence. However, instead of employing standard compression techniques, like Huffman coding, one may compress data using error correcting codes that are suitable for both data compression and error correction purposes. Recently, turbo codes, repeat-accumulate codes, low density parity check codes, and fountain codes have been used as lossless source codes and have achieved compression rates very close to the source entropy. When a near-lossless compression is desired, i. e. a small level of distortion is acceptable, the source encoder generates fixed-length codewords and the encoding complexity is low. Theoretically, random <b>codes</b> could achieve <b>near-lossless</b> compression. In literature, this has been proved by presenting a random binning scheme. Practically, all powerful channel codes, e. g. turbo codes, can follow the same procedure as suggested in random binning and achieve compression rates close to the entropy. On the other hand, if a completely lossless compression is required, i. e. if the distortion must be forced to zero, the source encoding is a complicated iterative procedure that generates variable-length codewords to guarantee zero distortion. However, the large complexity of encoding imposes a large delay to the system. The iterative encoding procedure can be regarded as using a nested code where each codeword of a higher-rate code is formed by adding parities to a codeword of some lower-rate code. This iterative encoding is proposed for practical codes, e. g. turbo codes and low density parity check (LDPC) codes, in the literature. In contrast to <b>near-lossless</b> source <b>coding,</b> in the lossless case no random coding theory is available to support achievability of entropy and specify distribution of the compression rate. We have two main contributions in this thesis. Our first contribution is presenting a tree structured random binning scheme to prove that nested random codes asymptotically achieve the entropy. We derive the probability mass function of the compression rate and show how it varies when increasing the block length. We also consider a more practical tree structured random binning scheme, where parities are generated independently and randomly, but they are biased. Our second contribution is to decrease the delay in turbo source coding. We consider turbo codes for data compression and observe that existing schemes achieve low compression rates; but because of large block length and large number of iterations they impose a large delay to the system. To decrease this delay we look at the problem of source coding using short block length turbo codes. We show how to modify different components of the encoder to achieve low compression rates. Specifically we modify the parity interleaver and use rectangular puncturing arrays. We also replace a single turbo code by a library of turbo codes to further decrease the compression rate. Since the scheme is variable-length and also many codes are used, the codeword length along with the code index (index of the turbo code which is used for compression) are transmitted as an overhead. Transmission of this overhead increases the compression rate. We propose a detection method to detect this overhead from the codeword. Therefore, the overhead is no longer transmitted since it is detected from the codeword at the decoder. This detection method will reduce the compression rate for short block length systems but it becomes less attractive for large block length codes...|$|R
40|$|Abstract—Computers are {{developing}} {{along with a}} new trend from the dual-core and quad-core processors to ones with tens or even hundreds of cores. Multimedia, {{as one of the}} most important applications in computers, has an urgent need to design parallel coding algorithms for compression. Taking intraframe/image coding as a start point, this paper proposes a pure line-by-line coding scheme (LBLC) to meet the need. In LBLC, an input image is processed line by line sequentially, and each line is divided into small fixed-length segments. The compression of all segments from prediction to entropy coding is completely independent and concurrent at many cores. Results on a general-purpose computer show that our scheme can get a 13. 9 times speedup with 15 cores at the encoder and a 10. 3 times speedup at the decoder. Ideally, such near-linear speeding relation with the number of cores can be kept for more than 100 cores. In addition to the high parallelism, the proposed scheme can perform comparatively or even better than the H. 264 high profile above middle bit rates. At <b>near-lossless</b> <b>coding,</b> it outperforms H. 264 more than 10 dB. At lossless coding, up to 14 % bit-rate reduction is observed compared with H. 264 lossless coding at the high 4 : 4 : 4 profile. Index Terms—Image coding, lossless coding, many cores. I...|$|E

