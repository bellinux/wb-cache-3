188|18|Public
25|$|A. Ideas {{are always}} imagistic (3rd), {{so we have}} no idea of {{thinking}} substance (<b>non-image</b> idea).|$|E
25|$|After the {{information}} has been prepared for production (the prepress step), each printing process has definitive means of separating the image from the <b>non-image</b> areas.|$|E
25|$|Watermark: Held {{up to the}} light, {{a hidden}} {{portrait}} (of Shin Saimdang), produced by using the variation in thickness of the note paper, appears in the <b>non-image</b> area {{on the left side}} of the obverse.|$|E
40|$|Objective: To {{assess the}} rate of {{detection}} of pulmonary nodules on preoperative computed tomography, based on intraoperative palpation of <b>non-imaged</b> pulmonary nodules, in patients who underwent thoracotomy for metastasectomy with bimanual lung palpation. Methods: A retrospective {{study carried out on}} all cases of pulmonary metastasectomy performed in King Faisal Specialist Hospital and Research Center over a 10 -year period (2001 – 2011). The number of nodules detected on preoperative computed tomography by a radiologist was compared with the number of nodules identified on pathology. Resected pulmonary nodules were classified to benign or malignant. Secondary outcomes were operative approach and primary malignancy. Results: 215 metastasectomies were performed by thoracotomy. The incidence of nodules that were not imaged preoperatively was 36 % (41 % of mesenchymal tumors, 28 % of epithelial tumors). Conclusion: Metastasectomy by an open approach, which affords bimanual palpation of the entire lung, discovered ipsilateral <b>non-imaged</b> malignant pulmonary metastases in 36 % of cases (41 % of mesenchymal tumors) ...|$|R
40|$|Session 2 - Imaging and <b>Non-Imaging</b> Diffraction System Concepts: 7818 09 [7818 A- 08]Static Feature-specific imaging (SFSI) {{employing}} a fixed/static measurement basis {{has been shown}} to achieve superior reconstruction performance to conventional imaging under certain conditions. 1 - 5 In this paper, we describe an adaptive FSI system in which past measurements inform the choice of measurement basis for future measurements so as to maximize the reconstruction fidelity while employing the fewest measurements. An algorithm to implement an adaptive FSI system for principle component (PC) measurement basis is described. The resulting system is referred to as a PC-based adaptive FSI (AFSI) system. A simulation study employing the root mean squared error (RMSE) metric to quantify the reconstruction fidelity is used to analyze the performance of the PC-based AFSI system. We observe that the AFSI system achieves as much as 30 % lower RMSE compared to a SFSI system. © 2010 SPIE. published_or_final_versionAdaptive Coded Aperture Imaging, <b>Non-Imaging,</b> and Unconventional Imaging Sensor Systems II, San Diego, CA., 1 - 2 August 2010. In Proceedings of SPIE - The International Society for Optical Engineering, 2010, v. 7818, p. 781809 - 1 - 781809 -...|$|R
40|$|The <b>non-Imaging</b> Cherenkov {{air shower}} {{measurement}} technique holds great promise in furthering our understanding the Knee-to-Ankle {{region of the}} cosmic ray spectrum. In particular, this technique offers a unique way to determine {{the evolution of the}} cosmic ray nuclear composition, and an example is given by the recent spectrum results of the Tunka Collaboration. With this in mind, we are organizing a workshop, to be held at the University of Utah, to bring together the various practitioners of this cosmic ray measurement technique to share simulations, analyses, detector designs, and past experimental results amongst the community. The workshop will also be in support of our effort, NICHE, to extend the reach of the TA/TALE detector systems down to the Knee. We anticipate that the workshop will result in a white paper on the scientific importance of these high-energy cosmic ray measurements and on using the Cherenkov technique to accomplish them. Our goal is to have contributions from members of the previous generation of experiments such as BLANCA and AIROBICC; from members of KASCADE-GRANDE which recently completed data-taking operations; from members of Tunka, which is current operating; and from NICHE, the planned <b>Non-Imaging</b> CHErenkov array installation as an extension of TA/TALE. We also aim to have presentations on the simulation of events and detector response as well as presentations on the underlying cosmic ray physics of this energy region...|$|R
25|$|In March 2017, Karel Tavernier from Ucamco {{published}} a draft specification to include fab documentation in Gerber for public review. Initially Ucamco had suggested using {{a subset of}} IPC-2581 for this <b>non-image</b> information {{but this was not}} followed by either the users or the IPC-2581 consortium.|$|E
25|$|Rodger Sparks, a {{radiocarbon}} expert from New Zealand, had {{countered that}} an error of thirteen centuries stemming from bacterial contamination in the Middle Ages {{would have required}} a layer approximately doubling the sample weight. Because such material could be easily detected, fibers from the shroud were examined at the National Science Foundation Mass Spectrometry Center of Excellence at the University of Nebraska. Pyrolysis-mass-spectrometry examination failed to detect any form of bioplastic polymer on fibers from either <b>non-image</b> or image areas of the shroud. Additionally, laser-microprobe Raman analysis at Instruments SA, Inc. in Metuchen, New Jersey, also failed to detect any bioplastic polymer on shroud fibers.|$|E
25|$|In offset lithography, which {{depends on}} {{photographic}} processes, flexible aluminum, polyester, mylar or paper printing plates {{are used in}} place of stone tablets. Modern printing plates have a brushed or roughened texture and are covered with a photosensitive emulsion. A photographic negative of the desired image is placed {{in contact with the}} emulsion and the plate is exposed to ultraviolet light. After development, the emulsion shows a reverse of the negative image, which is thus a duplicate of the original (positive) image. The image on the plate emulsion can also be created through direct laser imaging in a CTP (Computer-To-Plate) device called a platesetter. The positive image is the emulsion that remains after imaging. For many years, chemicals have been used to remove the <b>non-image</b> emulsion, but now plates are available that do not require chemical processing.|$|E
40|$|Submillimeter and {{near-infrared}} {{images of}} cool dusty debris disks and rings suggest {{the existence of}} unseen planets. At dusty but <b>non-imaged</b> stars, semi-major axes of associated planets can be estimated from the dust temperature. For some young stars these semi-major axes are greater than an arc second as seen from Earth. Such stars are excellent targets for sensitive near-infrared imaging searches for warm planets. To probe {{the full extent of}} the dust and hence of potential planetary orbits, SIRTF observations should include measurements with the 160 µm filter. Subject headings: (stars:) planetary systems — (stars:) planetary systems: protoplanetary disks — infrared: stars — astrobiology 1...|$|R
5000|$|STSS is {{designed}} to be the low earth orbiter within the layered Ballistic Missile Defense System. It complements the geosynchronous Defense Support Program, the Space-Based Infrared System, and other Overhead <b>Non-Imaging</b> Infrared systems (ONIR) [...] and provides tracking cues to systems on the surface. The STSS program is developed in phases, the first of which is the launch of two demonstrator satellites. The demonstrators will perform experiments and prove out systems and processes to establish a knowledge base for future operational designs. The demonstration satellites, built by Northrop Grumman and Raytheon detected and tracked a two-stage Ground-Based Interceptor (GBI) during a U.S. Missile Defense Agency flight test on June 6, 2010.|$|R
40|$|For {{the past}} few years, great efforts {{have been done in}} {{improving}} the tracking accuracy of a newly proposed rotation-elevation tracking mode heliostat. A special simulation program has been developed to systematically analyze the image movement and to find out the error of the parameters. In the simulation program, ray-tracing method was applied to work out the central point position of the master mirror image on the target plane during the primary tracking. From the experiment, less than 5 cm of tracking error was achieved {{with the help of the}} simulation program. We discussed the error analysis of the two prototypes of so called <b>Non-Imaging</b> Focusing Heliostat (NIFH) in Universiti Teknologi Malaysia (UTM) which has greatly reduced the optical alignment process and resulting more precise result...|$|R
2500|$|For offset lithography, which {{depends on}} {{photographic}} processes, flexible aluminum, polyester, mylar or paper printing plates are {{used instead of}} stone tablets. Modern printing plates have a brushed or roughened texture and are covered with a photosensitive emulsion. [...] A photographic negative of the desired image is placed {{in contact with the}} emulsion and the plate is exposed to ultraviolet light. After development, the emulsion shows a reverse of the negative image, which is thus a duplicate of the original (positive) image. The image on the plate emulsion can also be created by direct laser imaging in a CTP (Computer-To-Plate) device known as a platesetter. The positive image is the emulsion that remains after imaging. <b>Non-image</b> portions of the emulsion have traditionally been removed by a chemical process, though in recent times plates have come available that do not require such processing.|$|E
2500|$|Lithography {{works because}} of the mutual {{repulsion}} of oil and water. The image is drawn {{on the surface of}} the print plate with a fat or oil-based medium (hydrophobic) such as a wax crayon, which may be pigmented to make the drawing visible. A wide range of oil-based media is available, but the durability of the image on the stone depends on the lipid content of the material being used, and its ability to withstand water and acid. After the drawing of the image, an aqueous solution of gum arabic, weakly acidified with nitric acid [...] is applied to the stone. The function of this solution is to create a hydrophilic layer of calcium nitrate salt, , and gum arabic on all <b>non-image</b> surfaces. The gum solution penetrates into the pores of the stone, completely surrounding the original image with a hydrophilic layer that will not accept the printing ink. Using lithographic turpentine, the printer then removes any excess of the greasy drawing material, but a hydrophobic molecular film of it remains tightly bonded to the surface of the stone, rejecting the gum arabic and water, but ready to accept the oily ink.|$|E
50|$|A. Ideas {{are always}} imagistic (3rd), {{so we have}} no idea of {{thinking}} substance (<b>non-image</b> idea).|$|E
40|$|Speckle {{velocimetry}} is investigated as a {{means of}} determining odometry data with potential for application on autonomous robotic vehicles. The technique described here relies on the integration of translation measurements made by normalized cross-correlation of speckle patterns to determine the change in position over time. The use of objective (<b>non-imaged)</b> speckle offers a number of advantages over subjective (imaged) speckle, such as a {{reduction in the number of}} optical components, reduced modulation of speckles at the edges of the image, and improved light efficiency. The influence of the source/detector configuration on the speckle translation to vehicle translation scaling factor for objective speckle is investigated using a computer model and verified experimentally. Experimental measurements are presented at velocities up to 80 mms= 1 which show accuracy better than 0. 4 %...|$|R
40|$|Co-sited with TA/TALE, the <b>Non-Imaging</b> CHErenkov Array (NICHE) will {{measure the}} flux and nuclear {{composition}} of cosmic rays from below 10 ^ 16 eV to 10 ^ 18 eV in its initial deployment. Furthermore, the low-energy threshold can be significantly decreased below the cosmic ray knee via counter redeployment or by including additional counters. NICHE uses easily deployable detectors {{to measure the}} amplitude and time-spread of the air-shower Cherenkov signal to achieve an event-by-event measurement of Xmax and energy, each with excellent resolution. NICHE will have sufficient area and angular acceptance to have significant overlap with the TA/TALE detectors to allow for energy cross-calibration. Simulated NICHE performance {{has shown that the}} array has the ability to distinguish between several different composition models as well as measure the end of Galactic cosmic ray spectrum. Comment: Contribution to the 33 rd ICRC, Rio de Janeiro, Brazil, July 2013; Paper# 0365, 4 pages, 9 figure...|$|R
40|$|We {{show how}} to {{estimate}} the size of spatially unresolved solar and stellar coronal flaring regions from the X-ray light curve and time-resolved temperature and emission measure values during the flare decay. By means of extensive hydrodynamic modeling of decaying flaring loops, we propose and test {{a relationship between the}} decay time of the light curve in the band of a specific instrument and the slope of the trajectory in the densitytemperature diagram. From this relationship, we obtain an expression of the loop length {{as a function of the}} decay time, the slope and the flare maximum temperature. The novelty of this approach is that it takes into proper quantitative account, and allows us to estimate, the effect of a prolonged heating during the decay. In view of its application to <b>non-imaged</b> solar flares and to stellar flares, we have tested our relationship on resolved solar flares observed with Yohkoh SXT. The comparison of the predictions to the morphology of the structures in the SXT images proves the reliability of our approach under a wide range of conditions...|$|R
5000|$|ABCD0005.WAV (a DCF object {{formed by}} naming <b>non-image</b> {{file with the}} same file number as an image file) ...|$|E
5000|$|... "For innovative, rigorous, and {{fundamental}} {{contributions to the}} biophysics of sensory transduction in rod, cone, and <b>non-image</b> visual systems and in olfaction." ...|$|E
50|$|After the {{information}} has been prepared for production (the prepress step), each printing process has definitive means of separating the image from the <b>non-image</b> areas.|$|E
40|$|In {{its initial}} deployment, the <b>Non-Imaging</b> CHErenkov Array (NICHE) will measure the flux and nuclear {{composition}} of cosmic rays from below 10 ^ 16 eV to 10 ^ 18 eV {{by using measurements}} of the amplitude and time-spread of the air-shower Cherenkov signal to achieve a robust event-by-event measurement of Xmax and energy. NICHE will have sufficient area and angular acceptance to have significant overlap with TA/TALE, within which NICHE is located, to allow for energy cross-calibration. In order to quantify NICHE's ability to measure the cosmic ray nuclear composition, 4 -component composition models were constructed based upon a poly-gonato model of J. Hoerandel using simulated Xmax distributions of the composite composition {{as a function of}} energy. These composition distributions were then unfolded into individual components via an analysis technique that included NICHE's simulated Xmax and energy resolution performance as a function of energy as well as the effects of finite event statistics. Details of the construction of the 4 -component composition models and NICHE's ability to determine the individual components as a function of energy are presented. Comment: Contribution to the 33 rd ICRC, Rio de Janeiro, Brazil, July 2013; Paper# 0366, 4 pages, 8 figure...|$|R
40|$|The {{identification}} of deceased individuals {{is important in}} society as it not only facilitates the progression of criminal investigations into suspicious deaths, but also enables the resolution of legal matters and brings closure to the families affected by the death. When a corpse is skeletonized, heavily burned, or the soft tissue has degraded {{to a point that}} other professionals cannot obtain information about the deceased, a forensic anthropologist or odontologist is often tasked with identification. A variety of methods exist that enable forensic anthropologists to achieve identification. These include: <b>non-imaged</b> records comparisons; craniofacial superimposition and comparative radiography. Facial reconstructions can also be utilized when no ante mortem information about the deceased individual is available or when law enforcement have no suspicions on who the deceased person is. Facial reconstructions are traditionally a manual method however with the recent advancement of photogrammetry and three-dimensional and computer-aided design modeling software, the process can be performed within a virtual space. The purpose of this literature review is to identify an efficient and low-cost method of generating facial reconstructions using photogrammetry and open-source three-dimensional and computer-aided design software...|$|R
40|$|Objective—Severe {{impairment}} of left ventricular (LV) contraction {{is associated with}} an adverse prognosis in patients with ischaemic heart disease. Revascularisation may improve the impaired LV contraction if hibernating myocardium is present. The proportion of patients likely to benefit from this intervention is unknown. Therefore, the prevalence of hibernating myocardium in patients with ischaemic heart disease and severe {{impairment of}} LV contraction was assessed.  Design—From a consecutive series of patients undergoing coronary angiography for the investigation of chest pain or LV impairment, all patients with ischaemic heart disease and an LV ejection fraction (LVEF) ⩽  30 % were identified. These patients underwent positron emission tomography (PET) to detect hibernating myocardium, identified by perfusion metabolism mismatch.  Setting—A teaching hospital directly serving 500   000 people.  Results—Of a total of 301  patients, 36  had ischaemic heart disease and an LVEF ⩽  30 %. Twenty-seven patients had PET images, while nine patients were not imaged because of emergency revascularisation (three), loss to follow up (one), inability to give consent (four), and age <  50 years (one, ethics committee guidelines). Imaged and <b>non-imaged</b> groups were similar in LV impairment, demographic characteristics, and risk factor profile. Fourteen patients (52 % of the imaged or 39 % of all patients with ischaemic heart disease and LVEF ⩽  30 %) had significant areas of hibernating myocardium on PET.  Conclusion—It is possible that up to 50 % of patients with ischaemic heart disease and severely impaired left ventricles have hibernating myocardium.    Keywords: hibernating myocardium;  left ventricular impairment;  positron emission tomograph...|$|R
50|$|Waterless inks are {{heat-resistant}} and {{are used}} to keep silicone-based plates from showing toning in <b>non-image</b> areas. These inks are typically used on waterless Direct Imaging presses.|$|E
50|$|The {{material}} stack up, {{components and}} finishes are typically provided in informal text files or drawings. Ucamco recommends using {{a subset of}} IPC-2581 for this <b>non-image</b> information.|$|E
5000|$|Watermark: Held {{up to the}} light, {{a hidden}} {{portrait}} (of Shin Saimdang), produced by using the variation in thickness of the note paper, appears in the <b>non-image</b> area {{on the left side}} of the obverse.|$|E
40|$|Dementia is {{a global}} {{epidemic}} with Alzheimer’s disease (AD) being the leading cause. Early identification of patients at risk of developing AD is now becoming an international priority. Neocortical Aβ (extracellular β-amyloid) burden (NAB), as assessed by positron emission tomography (PET), represents one such marker for early identification. These scans are expensive and are not widely available, thus, {{there is a need}} for cheaper and more widely accessible alternatives. Addressing this need, a blood biomarker-based signature having efficacy for the prediction of NAB and which can be easily adapted for population screening is described. Blood data (176 analytes measured in plasma) and Pittsburgh Compound B (PiB) -PET measurements from 273 participants from the Australian Imaging, Biomarkers and Lifestyle (AIBL) study were utilised. Univariate analysis was conducted to assess the difference of plasma measures between high and low NAB groups, and cross-validated machine-learning models were generated for predicting NAB. These models were applied to 817 <b>non-imaged</b> AIBL subjects and 82 subjects from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) for validation. Five analytes showed significant difference between subjects with high compared to low NAB. A machine-learning model (based on nine markers) achieved sensitivity and specificity of 80 and 82 %, respectively, for predicting NAB. Validation using the ADNI cohort yielded similar results (sensitivity 79 % and specificity 76 %). These results show that a panel of blood-based biomarkers is able to accurately predict NAB, supporting the hypothesis for a relationship between a blood-based signature and Aβ accumulation, therefore, providing a platform for developing a population-based scree...|$|R
40|$|CSP to SDP NIP Data Rates & Data Models (version 1. 1) this release {{contains}} an ipython notebook and collection of python scripts. These {{make it easier}} to model SKA Science Data Processor (SDP) data rates. Diagrams are included that define the conceptual and logical structure of the <b>Non-Imaging</b> Processing (NIP) data models. Also included are activity diagrams for all NIP pipelines. Finally, formulas are presented that provide accurate estimates of NIP pipeline data rates. Author: Rob Lyon Email : robert. lyon@manchester. ac. uk web : www. scienceguyrob. com Overview This release consists of, 	an ipython notebook that explores and describes SDP data models for NIP pipelines. 	diagrams containing Entity-Relationship Models (ERMs), describing the data models/pipeline activity in NIP. 	formulas useful for deriving NIP pipeline data rates. 	standalone unit tested source code, which validates the correctness of the interactive ipython notebook code. Notebook The notebook describes the conceptual and logical data models for the <b>Non-Imaging</b> Processing (NIP) components of the SKA Science Data Processor (SDP), according to Software Engineering Institute (SEI) standards. The notebook was created for JIRA TSK- 1294, which requested SEI compliant versions of the NIP data models. This document represents the 4 th iteration (at least) of the data models work, however only the last two iterations, TSK- 12 and TSK- 73 are recorded in JIRA. All data models presented in this notebook are described via Entity-Relationship Models (ERMs). These are sometimes referred to as an entity-relationship diagrams (ERDs). There is no standard notation for ERMs, however we have attempted to adhere to defacto standards. Unified Modelling Language (UML) activity diagrams are also used to characterise the processing activity within NIP, which ingests and outputs the data models. This ipython notebook also models the input data rates and data volumes for NIP. It includes models for, 	pulsar/transient search mode. 	pulsar timing mode. 	dynamic spectra mode. The SKA science working group are discussing changing the name of this mode, to better reflect its function. The current name proposed is “Pulsar search filterbank format Data Stream”. Please be aware of this, as this change may come into effect after this notebook is finished. The name may even be changed to something entirely different. Data rate estimates are provided via interactive Python 2. 7 code. This can be run either within the notebook, or externally via the supplied source code. Given the use of ERM & UML, basic familiarity with SEI standards is recommended to fully understand the diagrams and terminology used in this notebook. License The code and the contents of this notebook are released under the GNU GENERAL PUBLIC LICENSE, Version 3, 29 June 2007. We kindly request that if you make use of the notebook, you cite the work appropriately. Acknowledgements The notebook builds upon earlier work by Dr. Lina Levin-Preston & Prof. Ben Stappers. Lina wrote the original data models document, did all the data volume/rate calculations, and provided the original notation! Ben worked on this too. This notebook relies greatly on their earlier work. Thank you Andrea Possenti, for providing valuable feedback which has helped produce the latest version. Change log Version 1. 1 - Incorporates feedback from Andrea Possenti. Altered the description of a program and schedule block in Section 2...|$|R
40|$|CT {{is widely}} used for {{anatomic}} referencing of PET and SPECT images of small animals but requires sufficiently high radiation doses capable of causing significant DNA damage. Therefore, we described the relationship between radiation dose, biologic damage, and image quality to determine whether CT can be used without significantly compromising radiotherapy and tumor development studies. Methods: The CT dose index gen-erated by the nanoSPECT/CT system was compared with measurements using EBT 2 gafchromic film. The effects of micro-CT were evaluated in 2 mouse strains that differ in sen-sitivity to radiation. gH 2 AX foci analysis to determine leukocyte, liver, and jejunum DNA damage and hematoxylin and eosin staining to investigate macroscopic jejunum damage were per-formed. Signal-to-noise ratio, contrast-to-noise ratio, and scan-ner linearity were determined to assess image quality. Results: For the standard settings, that is, as set by the manufacturers, EBT 2 gafchromic film dosimetry showed that the nanoSPECT/ CT system underestimated the absorbed dose. Moreover, sig-nificant doses were obtained, resulting in {{a significant increase in}} gH 2 AX formation in leukocytes, liver, and jejunum 40 min after CT, using preset parameters when compared with <b>non-imaged</b> controls. The jejenum response was more pronounced for the more radiosensitive strain. In contrast to leukocytes, the liver and jejunum still showed evidence of DNA damage 3 d after CT. Contrast-to-noise ratio, signal-to-noise ratio, and scanner linearity were sufficient to allow for anatomic referenc-ing for both imaging protocols tested. Conclusion: Anatomic reference images can be produced with no observable DNA damage or compromising image quality using low radiographic voltage, flux, and duration. Key Words: micro-CT; radiation dose; biologic damag...|$|R
50|$|The photoreceptors were {{identified}} in 2002 by Samer Hattar, David Berson and colleagues, where they were shown to be melanopsin expressing ganglion cells that possessed an intrinsic light response and projected {{to a number of}} brain areas involved in <b>non-image</b> forming vision.|$|E
50|$|The theory behind {{waterless}} printing is {{that the}} silicone material which makes up the <b>non-image</b> area of the plate has a very low surface energy. This material will resist ink provided the ink's viscosity is such {{that it has a}} greater affinity for itself than it does for the silicone.|$|E
50|$|The {{visual system}} {{is the part of}} the central nervous system which gives organisms the ability to process visual detail, as well as {{enabling}} the formation of several <b>non-image</b> photo response functions. It detects and interprets information from visible light to build a representation of the surrounding environment. The visual system carries out a number of complex tasks, including the reception of light and the formation of monocular representations; the buildup of a nuclear binocular perception from a pair of two dimensional projections; the identification and categorization of visual objects; assessing distances to and between objects; and guiding body movements in relation to the objects seen. The psychological process of visual information is known as visual perception, a lack of which is called blindness. <b>Non-image</b> forming visual functions, independent of visual perception, include the pupillary light reflex (PLR) and circadian photoentrainment.|$|E
40|$|The major {{results from}} SMM (Solar Max Mission) are {{presented}} {{as they relate to}} the understanding of the energy release and particle transportation processes that led to the high energy X-ray aspects of solar flares. Evidence is reviewed for a 152 - to 158 -day periodicity in various aspects of solar activity including the rate of occurrence of hard X-ray and gamma-ray flares. The statistical properties of over 7000 hard X-ray flares detected with the Hard X-Ray Burst Spectrometer are presented including the spectrum of peak rates and the distribution of the photo number spectrum. A flare classification scheme is used to divide flares into three different types. Type A flares have purely thermal, compact sources with very steep hard X-ray spectra. Type B flares are impulsive bursts which show double footpoints in hard X-rays, and soft-hard-soft spectral evolution. Type C flares have gradually varying hard X-ray and microwave fluxes from high altitudes and show hardening of the X-ray spectrum through the peak and on the decay. SSM data are presented for examples of Type B and Type C events. New results are presented showing coincident hard X rays, O V, and UV continuum observations in Type B events with a time resolution of 128 ms. The subsecond variations in the hard X-ray flux during 10 % of the stronger events are discussed and the fastest observed variation in a time of 20 ms is presented. The properties of Type C flares {{are presented as}} determined primarily from the <b>non-imaged</b> hard X-ray and microwave spectral data. A model based on the association of Type C flares and coronal mass ejections is presented to explain many of the characteristics of these gradual flares...|$|R
40|$|Seismic waves will detect {{changes in}} density and velocity, and {{potentially}} additional information with {{respond to these}} parameters will be useful for the seismic imaging of the Earth. Knowledge of faults and fault systems is of great importance in reservoir characterization. This requires sub-surface mapping of faults, {{which can only be}} done with seismic data, except in limited well locations. The seismic resolution will then be the ultimate limitation for how small faults can be imaged, but due to noise and other effects, the small faults, from a seismic point of view, are hard to image. The main emphasis of the work has been on building realistic faulted earth models, simulate seismic data collection and analyse the resulting synthetic seismic: Fieldwork was carried out on several extensional faults, and hand specimens weresystematically collected and analysed in the laboratory to obtain P- and S-wave velocities, as well as density and mineral content. Analyses of these velocity results concluded that three different scenarios can be observed in close vicinity to faults: • Low-Velocity-Zone : Increasing velocities as a function of distance from the fault plane. • High-Velocity-Zone : Decreasing velocities as a function of distance from the fault plane. • Constant velocit y: Velocities appear to be independent of the distance from the fault plane. Based on the above scenarios, three groups of earth models were designed: one group of models having displacements, one group having velocity changes and an additional group of models combining both the displacement and velocity changes as characteristics of a fault. Synthetic seismic was simulated by using a Finite Difference scheme on the different earth models, and different migration algorithms have been tested. The resulting images were then systematized to achieve some general results: • Both the displacement and the lateral velocity changes are shown to impact the seismic image. • Discontinuity of reflectors in the footwall of a fault should be interpreted with care since they could be caused by lateral velocity changes. • The spatial position of a <b>non-imaged</b> fault plane should be based on terminations of reflectors in the hangingwall and not in the footwall. • Bending close to a fault plane should be interpreted with care, since such phenomena could be artificially caused by the migration algorithm. The general observed effects of a fault having an area of lateral velocity changes are useful for anyone interpreting faults. More specific effects in the seismic image can be utilized if a direct comparison is made between migrated images which used the same algorithm. Finally, a seismic data set from the Gullfaks Field in the North Sea was chosen in order to test if the results obtained from the synthetic data could be applied on real seismic data. More specifically, one fault at the Gullfaks Field was chosen to test if lateral velocity variations in the nearby area of a fault could be used to improve the seismic imaging of the fault. The result shows that a weak improvement of the imaging of the fault plane can be observed in the middle area of the fault. </p...|$|R
50|$|FITS is {{also often}} used to store <b>non-image</b> data, such as spectra, photon lists, data cubes, or even {{structured}} data such as multi-table databases. A FITS file may contain several extensions, and each of these may contain a data object. For example, {{it is possible to}} store x-ray and infrared exposures in the same file.|$|E
