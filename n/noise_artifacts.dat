170|648|Public
2500|$|In many {{scientific}} fields, {{results are}} {{often difficult to}} reproduce accurately, being obscured by <b>noise,</b> <b>artifacts,</b> and other extraneous data. That means that even if a scientist does falsify data, they can expect {{to get away with}} it – or at least claim innocence if their results conflict with others in the same field. There are no [...] "scientific police" [...] who are trained to fight scientific crimes; all investigations are made by experts in science but amateurs in dealing with criminals. It is relatively easy to cheat although difficult to know exactly how many scientists fabricate data.|$|E
2500|$|Karlheinz Brandenburg used a CD {{recording}} of Suzanne Vega's song [...] "Tom's Diner" [...] {{to assess and}} refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, {{making it easier to}} hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as [...] "The mother of MP3". This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of <b>noise</b> <b>artifacts</b> unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model. Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats. LAME is the most advanced MP3 encoder. LAME includes a VBR variable bit rate encoding which uses a quality parameter rather than a bit rate goal. Later versions 2008+) support an n.nnn quality goal which automatically selects MPEG-2 or MPEG-2.5 sampling rates as appropriate for human speech recordings which need only 5512Hz bandwidth resolution.|$|E
5000|$|Charge to be (re-)distributed between {{components}} in the sensor during use are channeled via the LBCAST lower layers, where CMOS channels this charge over the surface layer. This difference is also claimed to reduce the presence of <b>noise</b> <b>artifacts.</b>|$|E
40|$|ECG {{signals are}} {{corrupted}} by {{various kinds of}} <b>noise</b> and <b>artifacts</b> that may negatively affect any subsequent analysis. In particular, narrowband artifacts include power-line interference and harmonic <b>artifacts.</b> Customarily, <b>noise</b> reduction and <b>artifact</b> rejection are tackled as two distinct problems. In this paper, we propose a joint approach to de- <b>noising</b> and narrowband <b>artifact</b> rejection that exploits the local structure of a noisy ECG. Simulation results confirm {{the effectiveness of the}} approach and highlight a notable ability to remove both <b>noise</b> and narrowband <b>artifacts</b> in ECG signals...|$|R
40|$|International audience–Low-dose CT (LDCT) {{images are}} often {{severely}} degraded by amplified mottle <b>noise</b> and streak <b>artifacts.</b> These artifacts are often hard to suppress without introducing tissue blurring effects. In this paper, we propose to process LDCT images using a novel image-domain algorithm called "artifact suppressed dictionary learning (ASDL) ". In this ASDL method, orientation and scale information on artifacts is exploited to train artifact atoms, {{which are then}} combined with tissue feature atoms to build three discriminative dictionaries. The streak artifacts are cancelled via a discriminative sparse representation (DSR) operation based on these dictionaries. Then, a general dictionary learning (DL) processing is applied to further reduce the <b>noise</b> and residual <b>artifacts.</b> Qualitative and quantitative evaluations on a large set of abdominal and mediastinum CT images are carried out and {{the results show that}} the proposed method can be efficiently applied in most current CT systems. Index Terms—Low-dose CT (LDCT), dictionary learning, <b>noise,</b> <b>artifact</b> suppression, artifact suppressed dictionary learning algorithm (ASDL...|$|R
30|$|The above {{indicates}} {{that there is a}} choice of several algorithms for denoising methods which reduce <b>noise</b> and <b>artifacts</b> in the EEG signals from their TFDs considered as images. The power of <b>noise</b> and <b>artifact</b> in the signal resulting from reconstructing the enhanced TFDs are expected to be much less than those in the original signals. This in turn can significantly improve the performance of any newborn EEG classification/source localization method.|$|R
5000|$|This {{division}} is to speed reading, and {{the separation of}} green pixels, to which the human eye is most sensitive, reduces <b>noise</b> <b>artifacts</b> which might otherwise be introduced by residual electrical charge in the accumulation circuitry, acquired previously from reading a pixel of a different colour.|$|E
5000|$|In many {{scientific}} fields, {{results are}} {{often difficult to}} reproduce accurately, being obscured by <b>noise,</b> <b>artifacts,</b> and other extraneous data. That means that even if a scientist does falsify data, they can expect {{to get away with}} it - or at least claim innocence if their results conflict with others in the same field. There are no [...] "scientific police" [...] who are trained to fight scientific crimes; all investigations are made by experts in science but amateurs in dealing with criminals. It is relatively easy to cheat although difficult to know exactly how many scientists fabricate data.|$|E
50|$|Miller's {{impact in}} the field of brain mapping in Medical imaging, {{specifically}} statistical methods for iterative image reconstruction began in the mid 80's when he joined Donald L. Snyder at Washington University to work on time-of-flight positron emission tomography (PET) systems being instrumented inMichel Ter-Pogossian's group. Working with Snyder, Miller's notable contribution was to stabilize likelihood-estimators of radioactive tracer intensities via the method-of-sieves.This became one of the main approachesfor controlling <b>noise</b> <b>artifacts</b> in the Shepp-Vardi algorithm in the context of low count, time-of-flight emission tomography. It was during this period that Miller met Lawrence (Larry) Shepp, and subsequently visited Shepp several times at Bell Labsto speak as part of the Henry Landau seminar series.Shepp remained a mentor and friend throughout Miller's career.|$|E
40|$|Automated or semiautomated pattern {{recognition}} in multidimensional NMR spectroscopy is strongly {{hampered by the}} large number of <b>noise</b> and <b>artifact</b> peaks occurring under practical conditions. A general Bayesian method which is able to assign probabilities that observed peaks are members of given signal classes (e. g., the class of true resonance peaks or the class of <b>noise</b> and <b>artifact</b> peaks) was proposed previously. The discriminative power of this approach is dependent on the choice of the properties characterizing the peaks. The automated class recognition is improved by the addition of a nonlocal feature, the similarities of peak shapes in symmetry-related positions. It turns out that this additional property strongly decreases the overlap of the multivariate probability distributions for true signals and noise and hence largely increases the discrimination of true resonance peaks from <b>noise</b> and <b>artifact...</b>|$|R
30|$|The {{presence}} of <b>noise</b> and/or <b>artifact</b> (e.g. muscle activity and eye blinks) in the recorded EEG signals degrades {{the performance of}} any EEG based signal processing method. Image processing techniques {{can be used to}} reduce <b>noise</b> and <b>artifact</b> from the TFD of EEG signals while retaining key details such as edges and texture of the TFD. T-F image techniques can then be adapted and used to enhance the quality of T-F representations in order to better represent the T-F features which characterize different abnormalities.|$|R
40|$|ECG {{signals are}} {{corrupted}} by {{several kinds of}} <b>noise</b> and <b>artifacts,</b> which negatively affect any subsequent analysis. In the literature, the only approach that can handle any <b>noise</b> and <b>artifacts</b> corrupting the ECG is linear time-invariant filtering. However, it suffers from some important limitations regarding effectiveness and computational complexity. In this {{paper we propose a}} novel frame- work for ECG signal preprocessing based on the notion of quadratic variation reduction. The framework is very general, since it can cope with all the different kinds of <b>noise</b> and <b>artifacts</b> that corrupt ECG records. It relies on a single algorithmic structure, thus enjoying an easy and robust implementation. Results show that the framework is effective in improving the quality of ECG, while preserving signal morphology. Moreover, it is very fast, even on long recordings, thus being perfectly suited for real-time applications and implementation on devices with reduced computational power, such as handheld devices...|$|R
5000|$|Karlheinz Brandenburg used a CD {{recording}} of Suzanne Vega's song [...] "Tom's Diner" [...] {{to assess and}} refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, {{making it easier to}} hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as [...] "The mother of MP3". This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of <b>noise</b> <b>artifacts</b> unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model. Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats. LAME is the most advanced MP3 encoder. LAME includes a VBR variable bit rate encoding which uses a quality parameter rather than a bit rate goal. Later versions 2008+) support an n.nnn quality goal which automatically selects MPEG-2 or MPEG-2.5 sampling rates as appropriate for human speech recordings which need only 5512 Hz bandwidth resolution.|$|E
40|$|Elastography or {{elasticity}} {{imaging techniques}} typically image local strains or Young’s modulus variations along the insonification direction. Recently, techniques that utilize angular displacement estimates obtained from multiple angular insonification of tissue have been reported. Angular displacement estimates obtained along different angular insonification directions have been utilized for spatial-angular compounding to reduce <b>noise</b> <b>artifacts</b> in axial-strain elastograms, and for estimating the axial and lateral {{components of the}} displacement vector and the corresponding strain tensors. However, these angular strain estimation techniques {{were based on the}} assumption that <b>noise</b> <b>artifacts</b> in the displacement estimates were independent and identically distributed and that the displacement estimates could be modeled using a zero-mean normal probability density function. Independent and identically distributed random variables refer to a collection of variables that have the same probability distribution and are mutually independent. In this article, a modified least-squares approach is presented that does not make any assumption regarding the noise in the angular displacement estimates and incorporates displacement <b>noise</b> <b>artifacts</b> into the strain estimation process using a cross-correlation matrix of the displacement <b>noise</b> <b>artifacts.</b> Two methods for estimating <b>noise</b> <b>artifacts</b> from the displacement images are described. Improvements in the strain tensor (axial and lateral) estimation performance are illustrated utilizing both simulation data obtained using finite-element analysis and experimental data obtained from a tissue-mimicking phantom. Improvements in the strain estimation performance are quantified in terms of the elastographic signal-to-noise and contrast-to-noise ratios obtained with and without the incorporation of the displacement <b>noise</b> <b>artifacts</b> into the least-squares strain estimator...|$|E
40|$|We {{investigate}} a new audio denoising algorithm. Complex wavelets protect phase of signals {{and are thus}} preferred in audio signal processing to real wavelets. The block attenuation eliminates the residual <b>noise</b> <b>artifacts</b> in reconstructed signals and provides a good approximation of the attenuation with oracle. A connection between the block attenuation and the decision-directed a priori SNR estimator of Ephraim and Malah is studied. Finally we introduce an adaptive block technique based on the dyadic CART algorithm. The experiments show that not only the proposed method does eliminate the residual <b>noise</b> <b>artifacts,</b> but it also preserves transients of signals better than short-time Fourier based methods do...|$|E
40|$|International audienceThe aim of {{the work}} is to {{integrate}} the information modulation of the inter-relations between EEG scalp measurements of two brain states in a connectivity graph. We present a sparse differential connectivity graph (SDCG) to distinguish the effectively modulated connections between epileptiform and non-epileptiform states of the brain from all the common connections created by <b>noise,</b> <b>artifact,</b> unwanted background activities and their related volume conduction effect. The proposed method is applied on real epileptic EEG data. Clustering the extracted features from SDCG may present valuable information about the epileptiform focus and their relations...|$|R
40|$|Abstract. The goal of {{this paper}} is to develop region based image {{segmentation}} algorithms. Two new variational PDE image segmentation models are proposed. The first model is obtained by minimizing an energy function which depends on a modified Mumford-Shah algorithm. The second model is acquired by utilizing prior shape information and region intensity values. The numerical experiments of the proposed models are tested against synthetic data and simulated normal human-brain MR images. The preliminary experimental results show the effectiveness and robustness of presented models against to <b>noise,</b> <b>artifact,</b> and loss of information. ...|$|R
50|$|Unwanted {{electrical}} and electromagnetic signal <b>noise,</b> typically switching <b>artifacts.</b>|$|R
3000|$|..., the {{intensity}} gradient is strongly {{constrained by the}} LR frame. Since the LR frame {{does not have any}} ringing and <b>noise</b> <b>artifacts,</b> increasing of {{the intensity}} gradient is prevented in the estimated HR frame, and those artifacts can be suppressed.|$|E
30|$|The second scanned object {{consists}} of large bundle of myelinated axons in the cortex of a mouse. Samples {{were prepared for}} routine electron microscopy (i.e., fixed with aldehydes, stained with osmium, dehydrated, and embedded in plastic). Data were acquired for 1501 projections over 180 ^∘ using a detector with 2048 × 2448 pixels, binned to 1124 detector pixels. In Fig.  10, reconstructions of a single slice are shown for the SIRT-FBP method and gridrec using the Shepp–Logan filter and the Parzen filter. Note that the gridrec reconstruction using the Shepp–Logan filter contains significantly more <b>noise</b> <b>artifacts</b> compared {{with the other two}} reconstructions. Specifically, the myelin rings of the axons are clearly visible in the reconstructions using SIRT-FBP and the Parzen filter, but not in the reconstruction using the Shepp–Logan filter, due to severe <b>noise</b> <b>artifacts.</b> The reconstructions of the SIRT-FBP method and gridrec using the Parzen filter are visually similar, with the SIRT-FBP reconstruction containing slightly less <b>noise</b> <b>artifacts,</b> but having a slightly lower resolution as well. If required for the analysis, however, it is possible {{to increase the number of}} approximated iterations in the SIRT-FBP method to increase the resolution of the reconstruction.|$|E
40|$|Ultrasound elastography {{has been}} well applied in early tumor {{diagnosis}} for obtaining tissue stiffness information. Elastograpyy may provide useful clinical information for the tissue characterization. But ultrasonic wave interference will produce speckle in both phase and envelope. So in conventional ultrasound elastography, there are <b>noise</b> <b>artifacts</b> which produce some misdiagnosis. In this paper, we investigate bilateral filter de-noising method to reduce the speckle. Because the bilateral filter de-noising method can greatly smooth the speckle {{at the same time}} protect the lesion edge well, it {{has been well}} proved good impact in B-mode. But in ultrasound elastography, the bilateral filter hasn’t been used. So we use the bilateral filter to reduce artifacts to prove the performance of this method. In the experiment, because of the bilateral filter de-noising method, the <b>noise</b> <b>artifacts</b> will be reduced largely. We use SNRe and CNRe to verify the performance of the bilateral filter and finally this method proved a significant improvement to SNRe and CNRe. </p...|$|E
30|$|<b>Noise</b> is an <b>artifact</b> {{found in}} {{images in the}} form of a random {{variation}} of brightness and color information (see Section 1.3. 4 for more details on noise). An empirical formulation of the objective measure of image quality based on blur and noise has been proposed in[83]. The method is based on the level of image intensity variations around its edges. The authors argue that in modern digital cameras, the image signal processor (ISP) enhances the image by removing noise but in doing so, it may deteriorate the image texture. Hence, there is a need of finding a trade-off between noise and blur and it provides a rationale for combining the estimation of noise and blur in the same method. Specifically, this method considers simulated conditions of white noise as source of the <b>noise</b> <b>artifact</b> in the test stimuli.|$|R
30|$|We {{performed}} evaluations {{in a set}} of 150 different images: {{from the}} Berkeley database [21], video sequences frames, and images commonly evaluated on denoising; all of them present natural <b>noise</b> and <b>artifacts.</b>|$|R
40|$|Motion <b>artifact</b> <b>noise</b> in ECG {{processing}} {{is difficult}} to remove since its spectrum is known to overlap the ECG signal spectrum. The combination of wavelet based denoising and high-pass/low-pass filtering is presented and shown to provide good motion <b>artifact</b> <b>noise</b> removal capabilities. The relative performance of the new technique is demonstrated using ECGs from the MIT-BIH ECG database...|$|R
30|$|Areas of high {{curvature}} will diffuse {{faster than}} areas of low curvature. Hence, small jagged <b>noise</b> <b>artifacts</b> will disappear quickly, while large-scale interfaces will be slow to evolve. Moreover, {{the front of}} level set equals zero. Thus, the attractive quality {{of this approach is}} that sharp boundaries are preserved; smoothing takes place inside a region, but not across region boundaries, thereby preserving sharp boundaries between objects.|$|E
40|$|In this thesis, {{analysis}} and classification of alcoholics is conducted using single trials of Visual Evoked Potential (VEP) signals {{extracted from the}} scalp during the perception of visual stimulus. A problem with the analysis of VEP signals is the corruption from ongoing electroencephalogram (EEG) and <b>noise</b> <b>artifacts.</b> VEP signals are relatively lower in signal strength as compared to EEG and therefore, the EEG distorts the VEP signal...|$|E
30|$|Despite its {{superiority over}} TD-OCT, FD-OCT {{implementation}} exhibit drawbacks {{in terms of}} autocorrelation <b>noise</b> <b>artifacts,</b> which obscure details of the image and degrade the system sensitivity. The autocorrelation terms arise from the interference occurring between different sample reflectors within the target. Jun Ai, et.al proposed the elimination of autocorrelation noise through asynchronous acquisition of two interferograms using an optical switch and attaining an axial resolution of 15  μm in air [13].|$|E
30|$|For ECG {{with high}} {{frequency}} <b>noise</b> and <b>artifact,</b> most existing methods produced more missing and false detections. However, the presented method illustrates an excellent detection performance {{as shown in}} Fig.  4 for ECG recording 200.|$|R
30|$|Recent {{attempts}} {{to handle the}} corruptions by <b>noise,</b> <b>artifact,</b> and missing data in physiological signals focus on using redundant measurements, and fusing data from multiple sensors. Researchers have developed methods to robustly estimate heart rate (HR) by fusing information from multiple signals[8]. In addition to various ECG channels, researchers also make use of ABP and PPG signals for HR estimation[9]. In this context, researchers have proposed segmentation methods for ABP and PPG that involve detecting {{the parts of the}} waveforms corresponding to the onset of a pulse[10, 11]. These methods independently segment the physiological signals. Hence, they don’t preserve the alignment between the segments across different signals.|$|R
40|$|Extensions to a {{previously}} published nonlinear model for generating realistic artificial electrocardiograms to include {{blood pressure and}} respiratory signals are presented. The model accurately reproduces many of the important clinical qualities of these signals such as QT dispersion, realistic beat to beat variability in timing and morphology and pulse transit time. The advantage of this artificial model is that the signal is completely known (and therefore its clinical descriptors can be specified exactly) and contains no <b>noise.</b> <b>Artifact</b> and <b>noise</b> can therefore be added in a quantifiable and controlled manner {{in order to test}} relevant biomedical signal processing algorithms. Application examples using Independent Component Analysis to remove artifacts are presented...|$|R
30|$|A common {{approach}} for reducing <b>noise</b> <b>artifacts</b> in direct reconstructions is to either change the convolution filter, or to apply additional filtering {{to the image}} after reconstruction. Note that additional filtering usually requires choosing parameters that have a large influence on the final image quality, and can increase the required processing time for large-scale data significantly. In Fig.  9, the SIRT-FBP reconstruction is compared with reconstructions computed using gridrec with the Shepp–Logan filter and additional Gaussian filtering, and using gridrec with the Parzen filter, which is often used in problems with low signal-to-noise ratios. The results show that although both approaches are able to reduce <b>noise</b> <b>artifacts</b> compared with standard gridrec using the Shepp–Logan filter (Fig.  8 a), the SIRT-FBP reconstruction contains less artifacts. Furthermore, the reconstructions using additional filtering and the Parzen filter contain significantly more limited-angle artifacts compared with the SIRT-FBP reconstruction. A possible {{reason for this is}} the fact that the SIRT-FBP filter is computed specifically for the limited-angle geometry of the actual experiment, while the standard filters and additional filtering steps do not take the limited-angle geometry into account.|$|E
40|$|Bibliography: pages 106 - 109. This thesis {{describes}} {{techniques for}} the correction of spatial <b>noise</b> <b>artifacts</b> in a {{mercury cadmium telluride}} infrared camera system. The spatial <b>noise</b> <b>artifacts</b> {{are a result of}} nonuniformities within the infrared focal plane detector array. The techniques presented dispense with the need for traditional temperature references, and provide nonuniformity compensation by using only the statistics of the moving infrared scene and motion of the camera assembly for calibration. Frame averaging is employed, assuming that all of the detector pixels will eventually be irradiated with the same levels of incident flux after some extended period of time. Using a statistical analysis of the camera image data, the correction coefficients are re-calculated and updated. These techniques also ensure that the calculated coefficients continually track the variations in the dark currents as well as temperature changes within the dewar sensor cooling vessel. These scene-based reference free approaches to the calculation of compensation coefficients in the infrared camera are shown to be successful in compensating for the effects of fixed pattern spatial noise...|$|E
30|$|If {{we had a}} {{model of}} the source, the signal quality {{assessment}} would be straightforward, the signal to noise ratio (SNR) gives an estimate of the signal quality. However, for physiological signals, the source characteristics vary significantly over the time. Therefore, researchers use indirect measures to estimate the signal quality of the physiological signals. These methods assume that some characteristics of the signals are known a priori, and the <b>noise</b> <b>artifacts</b> cause any deviation present in the signal.|$|E
40|$|Existing sinogram {{restoration}} methods cannot handle <b>noises</b> and nonstationary <b>artifacts</b> simultaneously. Although bilateral filter {{provides an}} {{efficient way to}} preserve image details while denoising, its performance in sinogram restoration for low-dosed X-ray computed tomography (LDCT) is unsatisfied. The main reason for this situation is that the range filter of the bilateral filter measures similarity by sinogram values, which are polluted seriously by <b>noises</b> and nonstationary <b>artifacts</b> of LDCT. In this paper, we propose a simple method to obtain satisfied restoration results for sinogram of LDCT. That is, the range filter weighs the similarity by Gaussian smoothed sinogram. Since smoothed sinogram can reduce the influence of both <b>noises</b> and nonstationary <b>artifacts</b> for similarity measurement greatly, our new method can provide more satisfied denoising results for sinogram restoration of LDCT. Experimental results show that our method has good visual quality and can preserve anatomy details in sinogram restoration even in both <b>noises</b> and non-stationary <b>artifacts...</b>|$|R
30|$|For {{the present}} discussion, we may note that while all {{observations}} (electromagnetic or optical) may suffer <b>noise</b> or <b>artifacts,</b> there are {{features of the}} signals that are claimed {{to be consistent with}} a lightning origin, notably their altitude dependence.|$|R
40|$|Clinical {{applications}} that require extraction {{and interpretation of}} physiological signals or waveforms are susceptible to corruption by <b>noise</b> or <b>artifacts.</b> Real-time hemodynamic monitoring systems are important for clinicians to assess the hemodynamic stability of surgical or intensive care patients by interpreting hemodynamic parameters generated by an analysis of aortic blood pressure (ABP) waveform measurements. Since hemodynamic parameter estimation algorithms often detect events and features from measured ABP waveforms to generate hemodynamic parameters, <b>noise</b> and <b>artifacts</b> integrated into ABP waveforms can severely distort the interpretation of hemodynamic parameters by hemodynamic algorithms. In this article, we propose {{the use of the}} Kalman filter and the 4 -element Windkessel model with static parameters, arterial compliance C, peripheral resistance R, aortic impedance r, and the inertia of blood L, to represent aortic circulation for generating accurate estimations of ABP waveforms through <b>noise</b> and <b>artifact</b> reduction. Results show the Kalman filter could very effectively eliminate noise and generate a good estimation from the noisy ABP waveform based on the past state history. The power spectrum of the measured ABP waveform and the synthesized ABP waveform shows two similar harmonic frequencies...|$|R
