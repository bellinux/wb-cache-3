11|52|Public
40|$|This paper {{examines}} efficient predictive broadcoverage parsing without dynamic programming. In {{contrast to}} bottom-up methods, depth-first top-down parsing produces partial parses that are fully connected trees spanning the entire left context, from which {{any kind of}} <b>non-local</b> <b>dependency</b> or partial semantic interpretation can in principle be read. We contrast two predictive parsing approaches, topdown and left-corner parsing, and find both to be viable. In addition, we find that enhancement with non-local information not only improves parser accuracy, but also substantially improves the search efficiency. ...|$|E
40|$|We {{present an}} online discriminative {{training}} approach to grapheme-to-phoneme (g 2 p) conversion. We employ a manyto-many alignment between graphemes and phonemes, which overcomes {{the limitations of}} widely used one-to-one alignments. The discriminative structure-prediction model incorporates input segmentation, phoneme prediction, and sequence modeling in a unified dynamic programming framework. The learning model is able to capture both local context features in inputs, as well as <b>non-local</b> <b>dependency</b> features in sequence outputs. Experimental results show that our system surpasses the state-of-the-art on several data sets. Index Terms: grapheme-to-phoneme conversion, speech synthesis, discriminative trainin...|$|E
40|$|To date, work on Non-Local Dependencies (NLDs) {{has focused}} almost {{exclusively}} on English {{and it is an}} open research question how well these approaches migrate to other languages. This paper surveys <b>non-local</b> <b>dependency</b> constructions in Chinese as represented in the Penn Chinese Treebank (CTB) and provides an approach for generating proper predicate-argument-modifier structures including NLDs from surface contextfree phrase structure trees. Our approach recovers non-local dependencies at the level of Lexical-Functional Grammar f-structures, using automatically acquired subcategorisation frames and f-structure paths linking antecedents and traces in NLDs. Currently our algorithm achieves 92. 2 % f-score for trace insertion and 84. 3 % for antecedent recovery evaluating on gold-standard CTB trees, and 64. 7 % and 54. 7 %, respectively, on CTBtrained state-of-the-art parser output trees. ...|$|E
5000|$|Alexiadou, Artemis, Tibor Kiss & Gereon Müller (eds.): Local Modelling of <b>Non-Local</b> <b>Dependencies</b> in Syntax. Berlin, 2012: Mouton de Gruyter.|$|R
40|$|This paper {{proposes a}} direct parsing of <b>non-local</b> <b>dependencies</b> in English. To this end,we useprobabilisticlinearcontext-free {{rewriting}} systems for data-driven parsing, following recent work on parsing German. Inordertodoso,wefirstperformatransformation of the Penn Treebank annotation of <b>non-local</b> <b>dependencies</b> into an annotation usingcrossingbranches. Theresultingtreebank {{can be used}} for PLCFRS-based parsing. Our evaluation shows that, compared to PCFG parsing with the same techniques, PLCFRS parsing yields slightly better results. Inparticularwhenevaluatingonlythe parsingresultsconcerninglong-distancedependencies,thePLCFRSapproachwithdiscontinuousconstituentsis able to recognize about 88 % of the dependenciesof type *T* and *T*-PRN encoded in the Penn Treebank. Even the evaluation results concerning local dependencies, which can in principle be captured by a PCFG-based model, are better with our PLCFRS model. This demonstratesthatbydiscardinginformation onnon-localdependenciesthePCFGmodel losesimportantinformationonsyntacticdependenciesin general. ...|$|R
40|$|This paper {{shows that}} a simple {{two-stage}} approach to handle <b>non-local</b> <b>dependencies</b> in Named Entity Recognition (NER) can outperform existing approaches that handle <b>non-local</b> <b>dependencies,</b> while being much more computationally efficient. NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text. We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from {{the output of the}} first CRF. Using features capturing <b>non-local</b> <b>dependencies</b> from the same document, our approach yields a 12. 6 % relative error reduction on the F 1 score, over state-of-theart NER systems using local-information alone, when compared to the 9. 3 % relative error reduction offered by the best systems that exploit non-local information. Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13. 3 % error reduction over NER systems using local-information alone. Additionally, our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference. ...|$|R
40|$|As lifelong {{statistical}} learners, {{humans are}} remarkably {{sensitive to the}} unfolding of elements and events in their surroundings. In the present work, we examined the time-course of <b>non-local</b> <b>dependency</b> learning using a self-paced moving window display. We exposed participants to an artificial grammar of shape sequences and extracted processing times, or how long they viewed each shape, {{over the course of}} the experiment. On-line learning was quantified as the growing difference in viewing duration between predictable and predictive items. In other words, as participants learned, they processed predictable items increasingly faster. Our results indicate that participants who make implicit predictions as they learn, and have their expectations met, achieve higher learning outcomes on an off-line post-test. Potential links between these findings, obtained with novel stimuli in an experimental context, and the role of prediction in natural language comprehension are considered...|$|E
40|$|This paper {{examines}} efficient predictive broad-coverage parsing without dynamic programming. In {{contrast to}} bottom-up methods, top-down parsing produces partial parses that are fully connected trees spanning the entire left context, from which {{any kind of}} <b>non-local</b> <b>dependency</b> or partial semantic interpretation can in principle be read. We contrast top-down and left-corner parsing, and find both to be viable approaches. In addition, we find that enhancement with non-local information not only improves parser accuracy, but also substantially improves the search efficiency. 1 Introduction Strong empirical evidence has been presented over the past 15 years indicating that the human sentence processing mechanism makes on line use of contextual information in the preceding discourse (Crain and Steedman, 1985; Altmann and Steedman, 1988; Britt, 1994) and in the visual environment (Tanenhaus et al., 1995). These results lend support to Mark Steedman's (1989) "intuition" that sentence interpr [...] ...|$|E
40|$|Linguistic {{dependencies}} between non-adjacent {{words have}} been shown to cause comprehension difficulty, compared to local dependencies. According to one class of sentence comprehension accounts, non-local dependencies are difficult because they require the retrieval of the first dependent from memory when the second dependent is encountered. According to these memory-based accounts, making the first dependent accessible at the time when the second dependent is encountered should help alleviate the difficulty associated with the processing of non-local dependencies. In a dual-task paradigm, participants read sentences that did vs. didn’t contain a <b>non-local</b> <b>dependency</b> (i. e., object- and subject-extracted cleft constructions) while simultaneously remembering a word. The memory task was aimed at making the word held in memory accessible throughout the sentence. In an object-extracted cleft (e. g., It was Ellen who John consulted…), the object (Ellen) must be retrieved from memory when consulted is encountered. In the critical manipulation, the memory word was identical to the verb’s object (ELLEN). In these conditions, the extraction effect was reduced in the comprehension accuracy data and eliminated in the reading time data. These results add to the body of evidence supporting memory-based accounts of syntactic complexity...|$|E
40|$|We {{present a}} novel, two-step {{approach}} for detecting <b>non-local</b> <b>dependencies.</b> First a shallow trace tagger finds extraction sites and second, {{working on the}} output of the tagger, a PCFG-based parser recovers the extracted constituents. This method achieves state-of-the-art performance on antecedent recovery, despite being conceptually simpler than competing models...|$|R
40|$|Abstract. The Composite {{design pattern}} poses a {{challenge}} for reasoning about invariants with <b>non-local</b> <b>dependencies.</b> Region logic is a Hoare logic augmented with simple notations for object sets {{that can be used}} as dynamic frames in modifies clauses. Using region logic, this paper provides an elementary specification pattern for the Composite design pattern and evaluates the specification by using it in verifications of sample clients. ...|$|R
40|$|This paper {{explores the}} problem of finding <b>non-local</b> <b>dependencies.</b> First, we isolate a set of {{features}} useful for this task. Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds nonlocal dependencies while parsing. We find that the former outperforms the latter because it makes better use of the features we isolate. ...|$|R
40|$|The {{understanding}} of sentences involves {{not only the}} retrieval {{of the meaning of}} single words, but the identification of the relation between a verb and its arguments. The way the brain manages to process word meaning and syntactic relations during language comprehension on-line still is a matter of debate. Here we review the different views discussed in the literature and report data from crucial experiments investigating the temporal and neurotopological parameters of different information types encoded in verbs, i. e. word category information, the verb’s argument structure information, the verb’s selectional restriction and the morphosyntactic information encoded in the verb’s inflection. The neurophysiological indices of the processes dealing with these different information types suggest an initial independence of the processing of word category information from other information types as the basis of local phrase structure building, and a later processing stage during which different information types interact. The relative ordering of the subprocesses appears to be universal, whereas the absolute timing of when during later phrases interaction takes places varies as a function of when the relevant information becomes available. Moreover, the neurophysiological indices for <b>non-local</b> <b>dependency</b> relations vary {{as a function of the}} morphological richness of the language...|$|E
40|$|We study a superconducting {{transmission}} line (TL) formed by distributed LC oscillators and excited by external magnetic fluxes which are aroused from random magnetization (A) placed in substrate or (B) distributed at interfaces of a two-wire TL. Low-frequency dynamics of a random magnetic field is described {{based on the}} diffusion Langevin equation with a short-range source caused by (a) random amplitude or (b) gradient of magnetization. For a TL modeled as a two-port network with open and shorted ends, the effective magnetic flux at the open end has <b>non-local</b> <b>dependency</b> on noise distribution along the TL. The flux-flux correlation function is evaluated and analyzed for the regimes (Aa), (Ab). (Ba), and (Bb). Essential frequency dispersion takes place around the inverse diffusion time of random flux along the TL. Typically, noise effect increases with size faster than the area of TL. The flux-flux correlator can be verified both via the population relaxation rate of the qubit, which is formed by the Josephson junction shunted by the TL with flux noises, and via random voltage at the open end of the TL. Comment: 11 pages, 5 figure...|$|E
40|$|Music and {{language}} are complex {{means of communication}} that embody hierarchically-organised sequences. Processing hierarchical sequences require the resolution of both local and non-local dependencies. That is, one {{must be able to}} relate elements beyond its immediate temporal order. A type of <b>non-local</b> <b>dependency</b> that is frequently encountered arises from nested or centre-embedded structures. It was recently shown that both musicians and non-musicians perceive nested non-local dependencies in tonal harmony. However, its neuroanatomical basis is unknown, and it is unclear if the perception of nested non-local dependencies is specific to harmony. Original musical sequences composed by concatenating three-note motifs in a nested matter were presented alongside modified counterparts to musicians in a two-alternative forced choice task. Using functional MRI (fMRI), the present experiment demonstrates that processing nested motific structures in music recruits the right homologue of Broca’s area, and for the first time, that musical structures with at least two levels of embedding can be perceived. This finding establishes the functional neuroanatomy of processing nested non-local dependencies in music and provides striking evidence that the functional neuroanatomical network of musical syntax is a mirror image of that in human language...|$|E
40|$|Abstract. We {{consider}} {{the problem of}} sequence labeling and propose a two steps method which combines the scores of local classifiers with a relaxation labeling technique. This framework can account for sparse dynamically changing dependencies, which allows us to efficiently discover relevant <b>non-local</b> <b>dependencies</b> and exploit them. This {{is in contrast to}} existing models which incorporate only local relationships between neighboring nodes. Experimental results show that the proposed method gives promising results. ...|$|R
40|$|We {{describe}} {{a method for}} enriching the output of a parser with information available in a corpus. The method is based on graph rewriting using memorybased learning, applied to dependency structures. This general framework allows us to accurately recover both grammatical and semantic information as well as <b>non-local</b> <b>dependencies.</b> It also facilitates dependency-based evaluation of phrase structure parsers. Our method is largely independent of the choice of parser and corpus, and shows {{state of the art}} performance...|$|R
40|$|We {{describe}} an algorithm for recovering <b>non-local</b> <b>dependencies</b> in syntactic dependency structures. The patternmatching approach proposed by Johnson (2002) {{for a similar}} task for phrase structure trees is extended with machine learning techniques. The algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure {{and a set of}} structural features for this fragment. Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented in (Johnson, 2002). ...|$|R
40|$|This thesis {{describes}} a treebank-based approach to automatically acquire robust,wide-coverage Lexical-Functional Grammar (LFG) resources for Chinese parsing and generation, {{which is part}} of a larger project on the rapid construction of deep, large-scale, constraint-based, multilingual grammatical resources. I present an application-oriented LFG analysis for Chinese core linguistic phenomena and (in cooperation with PARC) develop a gold-standard dependency-bank of Chinese f-structures for evaluation. Based on the Penn Chinese Treebank, I design and implement two architectures for inducing Chinese LFG resources, one annotation-based and the other dependency conversion-based. I then apply the f-structure acquisition algorithm together with external, state-of-the-art parsers to parsing new text into "proto" f-structures. In order to convert "proto" f-structures into "proper" f-structures or deep dependencies, I present a novel <b>Non-Local</b> <b>Dependency</b> (NLD) recovery algorithm using subcategorisation frames and f-structure paths linking antecedents and traces in NLDs extracted from the automatically-built LFG f-structure treebank. Based on the grammars extracted from the f-structure annotated treebank, I develop a PCFG-based chart generator and a new n-gram based pure dependency generator to realise Chinese sentences from LFG f-structures. The work reported in this thesis is the first effort to scale treebank-based, probabilistic Chinese LFG resources from proof-of-concept research to unrestricted, real text. Although this thesis concentrates on Chinese and LFG, many of the methodologies, e. g. the acquisition of predicate-argument structures, NLD resolution and the PCFG- and dependency n-gram-based generation models, are largely language and formalism independent and should generalise to diverse languages as well as to labelled bilexical dependency representations other than LFG...|$|E
40|$|The term ’switch-reference marking ’ was {{introduced}} by Jacobsen (1967) to describe a system of referential tracking. According to his original definition, switchreference “consists {{in the fact that}} a switch in subject or agent [ [...] . ] is obligatorily indicated in certain situations by a morpheme, usually suffixed, {{which may or may not}} carry other meanings in addition. ” (Jacobsen 1967 : 268) A typical example is given in (1) from the Papuan language Kâte. As shown by the gloss (SS indicating a same-subject relation, DS a different-subject relation), markers on every verb indicate whether its suject is identical to the subject of the immediately following verb. (1) Kâte (Trans-New-Guinea); Pilhofer (1933) (as cited in Bickel (2011)) ra fisi-pie fahare-râ yâpeP-yopa-pie go arrive-SEQ. 3 pDS rise-SEQ. SS chase. away- 3 pDO-SEQ. 3 pDS mafa-yeNiP behe-râ wise-pie fiuP stuff- 3 pPOSS throw. away-SEQ. SS flee-SEQ. 3 pDS illicitly ro=fâre-mbiN. take=all- 3 pREMOTE. PAST ’When theyi (the foreigners) arrived, they j (the villagers) got up and chased them away. Theyi threw away their stuff and fleed. Then, they j stole their stuff. ’ The phenomenon of switch-reference is interesting from an empirical as well as a theoretical perspective. Empirically, it is still an understudied topic as the definitions and generalizations are often blurred and not fully understood. Theoretically, the fact that the reference of both subjects has to be taken into account to determine the form of the switch-reference morpheme constitutes a <b>non-local</b> <b>dependency.</b> Since such non-local dependencies are often considered to be undesirable in syntax in general (cf. discussion in Alexiadou et al. (2012)) quite a lot of theoretica...|$|E
40|$|This {{dissertation}} {{is about}} the syntax and semantics of <b>non-local</b> <b>dependencies.</b> It focuses on wh-displacement and operator scope and addresses the challenge they pose to theoretical linguistics: How are form and meaning related and to which extent do syntax and semantics operate in parallel? A common approach of formal grammar theories is to assume that displacement and scope construal go hand in hand, and consequently to impose a strict correspondence between syntax and semantics. This approach is challenged, however, by {{a considerable amount of}} cases where the syntactic position of an operator expression does not coincide with its semantic scope position. This dissertation therefore pursues the opposite approach. It argues that syntactic displacement and semantic scope-taking do not interact at all. The overall picture that is developed is that grammar consists of two parts: a core system for establishing local dependencies, with syntax and semantics operating in parallel, and extensions to this core system for establishing <b>non-local</b> <b>dependencies,</b> with syntax and semantics operating independently. In developing this picture, it is shown that a substantial amount of data (such as pronunciation in wh-clusters, order preservation with multiple displacement, remnant movement and freezing, as well as different scopal behavior) can indeed be accounted for with a syntax and semantics that are largely independent from one anothe...|$|R
40|$|This paper {{addresses}} {{the nature of}} the temporary storage buffer used in implicit or statistical learning. Kuhn and Dienes [Kuhn, G., & Dienes, Z. (2005). Implicit learning of nonlocal musical rules: implicitly learning more than chunks. Journal of Experimental Psychology-Learning Memory and Cognition, 31 (6) 1417 – 1432] showed that people could implicitly learn a musical rule that was solely based on <b>non-local</b> <b>dependencies.</b> These results seriously challenge models of implicit learning that assume knowledge merely takes the form of linking adjacent elements (chunking). We compare two models that use a buffer to allow learning of long distance dependencies, the Simple Recurrent Network (SRN) and the memory buffer model. We argue that these models – as models of the mind – should not be evaluated simply by fitting them to human data but by determining the characteristic behaviour of each model. Simulations showed {{for the first time that}} the SRN could rapidly learn <b>non-local</b> <b>dependencies.</b> However, the characteristic performance of the memory buffer model rather than SRN more closely matched how people came to like different musical structures. We conclude that the SRN is more powerful than previous demonstrations have shown, but it’s flexible learned buffer does not explain people’s implicit learning (at least, the affective learning of musical structures) as well as fixed memory buffer models do...|$|R
40|$|We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese {{with the}} new Chinese CCGbank, {{bringing}} a formalism capable of transparently recovering <b>non-local</b> <b>dependencies</b> to a language {{in which they are}} particularly frequent. We train two state-of-the-art English parsers: the parser of Petrov and Klein (P&K), and the Clark and Curran (C&C) parser, uncov-ering a surprising performance gap between them not observed in English — 72. 73 (P&K) and 67. 09 (C&C) F-score on 6. We explore the challenges of Chinese parsing through three novel ideas: develop-ing corpus variants rather than treating the cor-pus as fixed; controlling noun/verb and other ambiguities; and quantifying the impact of constructions like pro-drop. ...|$|R
40|$|A central {{problem for}} {{real-time}} scheduling is to acquire tight but conservative bounds on task execution times. We present a prototype for {{an environment where}} such bounds are interactively presented, in terms of source code constructs, to the programmer during development. The prototype {{is based on the}} language development tool APPLAB and uses an extended attribute grammar formalism, reference attributed grammars (RAGs), which overcomes some drawbacks of conventional attribute grammars in this context (e. g. description of <b>non-local</b> <b>dependencies).</b> In this paper we show how timing schemata can be implemented as RAGs. Our experience is that the RAG approach allows timing schemata to be implemented in a clear, concise, and modular manner...|$|R
40|$|Attribute grammars are a {{powerful}} specification formalism for tree-based computation, particularly for software language processing. Various extensions {{have been proposed}} to abstract over common patterns in attribute grammar specifications. These include various forms of copy rules to support <b>non-local</b> <b>dependencies,</b> collection attributes, and expressing dependencies that are evaluated to a fixed point. Rather than implementing extensions natively in an attribute evaluator, we propose attribute decorators that describe an abstract evaluation mechanism for attributes, {{making it possible to}} provide such extensions as part of a library of decorators. Inspired by strategic programming, decorators are specified using generic traversal operators. To demonstrate their effectiveness, we describe how to employ decorators in name, type, and flow analysis. 16 page(s...|$|R
40|$|AbstractRemote {{attribute}} grammars use {{objects with}} separately defined fields to induce direct <b>non-local</b> <b>dependencies</b> in attribute grammars. Fields {{of an object}} may be read remotely from where it is created, and special “collection” fields may be written remotely as well. Building on earlier work which shows that remote attribute grammars can be scheduled statically, this paper shows how they may be implemented incrementally. The static schedule is used to ensure an object's fields are defined before they are read and that we never re-evaluate an attribute multiple times per edit-cycle. Dynamic dependencies are used to mark remote use sites as affected when a field is changed. The result is an efficient and practical incremental evaluation...|$|R
40|$|Text {{from social}} media {{provides}} {{a set of}} challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet 2 vec, which finds vector-space representations of whole tweets by learning complex, <b>non-local</b> <b>dependencies</b> in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences. Our tweet 2 vec encoder is publicly available. Comment: 6 pages, 2 figures, 4 tables, accepted as conference paper at ACL 201...|$|R
40|$|Making complex {{decisions}} {{in real world}} problems often involves assigning values to sets of interdependent variables where an expressive dependency structure among these can influence, or even dictate, what assignments are possible. Commonly used models typically ignore expressive dependencies since the traditional way of incorporating <b>non-local</b> <b>dependencies</b> is inefficient and hence lead to expensive training and inference. This paper presents Constrained Conditional Models (CCMs), a framework that augments probabilistic models with declarative constraints {{as a way to}} support {{decisions in}} an expressive output space while maintaining modularity and tractability of training. We develop, analyze and compare novel algorithms for training and inference with CCMs. Our main experimental study exhibits the advantage our framework provides when declarative constraints are used in the context of supervised and semi-supervised training of a probabilistic model. 1...|$|R
40|$|Maximum entropy (ME) {{techniques}} {{have been successfully}} used to combine different sources of linguistically meaningful constraints in language models. However, most of the current ME models can only be used for small corpora, since the computational load in training ME models for large corpora is unbearable. This problem is especially severe when <b>non-local</b> <b>dependencies</b> are considered. In this paper, we show how to train and use topic-dependent ME models efficiently for a very large corpus, Broadcast News (BN). The training time is greatly reduced by hierarchical training and divide-and-conquer approaches. The computation in using the model is also simplified by pre-normalizing the denominators of the ME model. We report new speech recognition results showing improvement with the topic model relative to the standard N-gram model for the Broadcast News task...|$|R
40|$|Abstract — For {{an aerial}} robot, perceiving and {{avoiding}} obstacles are necessary skills to function autonomously in a cluttered unknown environment. In this work, {{we use a}} single image captured from the onboard camera as input, produce obstacle classifications, {{and use them to}} select an evasive maneuver. We present a Markov Random Field based approach that models the obstacles as a function of visual features and <b>non-local</b> <b>dependencies</b> in neighboring regions of the image. We perform efficient inference using new low-power parallel neuromorphic hardware, where belief propagation updates are done using leaky integrate and fire neurons in parallel, while consuming less than 1 W of power. In outdoor robotic experiments, our algorithm was able to consistently produce clean, accurate obstacle maps which allowed our robot to avoid a wide variety of obstacles, including trees, poles and fences. I...|$|R
40|$|Describing {{the static}} {{semantics}} of programming languages with attribute grammars is eased when the formalism allows direct dependencies to be induced between rules for nodes arbitrarily {{far away in}} the tree. Such direct <b>non-local</b> <b>dependencies</b> cannot be analyzed using classical methods, which enable efficient evaluation. This article defines an attribute grammar extension (“remote attribute grammars”) to permit references to objects with fields to be passed through the attribute system. Fields may be read and written through these references. The extension has a declarative semantics {{in the spirit of}} classical attribute grammars. It is shown that determining circularity of remote attribute grammars is undecidable. The article then describes a family of conservative tests of noncircularity and shows how they can be used to “schedule ” a remote attribute grammar using standard techniques. The article discusses practical batch and incremental evaluation of remote attribute grammars...|$|R
40|$|Discontinuities come in {{different}} flavours. In minimalistic and previous {{editions of the}} generative enterprise, long distance dependencies and discontinutities resulting from verb clustering are analyzed in quite different terms. This paper analyzes wh-structures and crossing dependencies as emanations of a single modalized regime of composition in a combinatory categorial setting. 1 Introduction Mankind owes to Generative Grammar the discovery and subsequent theoretical exploitation {{of at least two}} phenomena in natural language: leftward dislocation or fronting (1), in particular whmovement, and Dutch verb clustering (2), in particular cross-serial dependencies. Leftward dislocation typically creates <b>non-local</b> <b>dependencies</b> between a structural position at the left edge of a sentence and a structural position somewhere in the sentence's kernel. Verb clustering groups together a bunch of verbal heads, leaving a sequence of motherless arguments to these verbs. (1) Hier heb ik op gewac [...] ...|$|R
40|$|Graphical {{techniques}} for modeling the of random variables have been explored {{in a variety}} of different areas including statistics, statistical physics, artificial intelligence (AI), speech recognition, processing, genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we e hidden Markov models and related graphical structures within the general framework of directed acyclic graphs In particular we show that there exists a general methodology for deriving an exact inference method for any that this algorithm produces the well-known forward-backward and Viterbi algorithms as special cases, and that the complexity of inference can be characterized for general families of DAGs including HMMs which are augmented with <b>non-local</b> <b>dependencies.</b> Applications to problems such as speech recognition and biological sequence modeling. are discussed. 1 Introduction A directed acyclic graph is a graphical specificat [...] ...|$|R
40|$|Preprint {{of paper}} {{published}} in: Compiler Construction, Lecture Notes in Computer Science 5501, 2009; doi: 10. 1007 / 978 - 3 - 642 - 00722 - 4 _ 11 Attribute grammars are a powerful specification formalism for tree-based computation, particularly for software language processing. Various extensions {{have been proposed}} to abstract over common patterns in attribute grammar specifications. These include various forms of copy rules to support <b>non-local</b> <b>dependencies,</b> collection attributes, and expressing dependencies that are evaluated to a fixed point. Rather than implementing extensions natively in an attribute evaluator, we propose attribute decorators that describe an abstract evaluation mechanism for attributes, {{making it possible to}} provide such extensions as part of a library of decorators. Inspired by strategic programming, decorators are specified using generic traversal operators. To demonstrate their effectiveness, we describe how to employ decorators in name, type, and flow analysis. Software Computer TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Design {{patterns}} are abstract descriptions of solutions to often recurring problems. They are {{a means to}} communicate experience in design. Over the past years, along {{with the increase in}} popularity of object-oriented design patterns, some problems with the use of them have been identified. One of these lies in documenting software systems using design patterns. Experience has shown that both in the initial design, and especially in later code revisions, it is all too easy for code and documentation to diverge, rendering the documentation misleading and the code inconsistent. In this paper we present a flexible and extensible tool which enables designers to use design patterns in a safe and easy way and which semi-automatically documents and maintains the documentation of a software system. The system is implemented using reference attributed grammars (RAGs) which are capable of describing <b>non-local</b> <b>dependencies.</b> Both the programming language and the design {{patterns are}} specifi [...] ...|$|R
40|$|We {{present a}} {{computational}} method for interactive 3 D design and rationalization of surfaces via auxetic materials, i. e., flat flexible material that can stretch uniformly up {{to a certain}} extent. A key motivation for studying such material is that one can approximate doubly-curved surfaces (such as the sphere) using only flat pieces, making it attractive for fabrication. We physically realize surfaces by introducing cuts into approximately inextensible material such as sheet metal, plastic, or leather. The cutting pattern is modeled as a regular triangular linkage that yields hexagonal openings of spatially-varying radius when stretched. In the same way that isometry is fundamental to modeling developable surfaces, we leverage conformal geometry to understand auxetic design. In particular, we compute a global conformal map with bounded scale factor to initialize an otherwise intractable non-linear optimization. We demonstrate that this global approach can handle non-trivial topology and <b>non-local</b> <b>dependencies</b> inherent in auxetic material. Design studies and physical prototypes are used to illustrate a wide range of possible applications...|$|R
