19|14|Public
40|$|Over {{years the}} {{searches}} made over web still remains <b>non</b> <b>semantic.</b> The internet media {{is filled with}} lot of unstructured data that results this state of <b>non</b> <b>semantic</b> search over web. To make the search more specific over the query given, its attributes are to be defined while the search along with its object name. By defining so the normal web search appears to be semantic. The attributes relevant to the context is made easy over text, audio and video files. Whereas Image files still suffers with its undefined attributes along with the relevant image being uploaded. Moreover the images over web aren‟t tagged yet with relevant attributes in them. JPEG, Bitmap and all other commonly used image supporters yet remain without tags. Only this leads a <b>non</b> <b>semantic</b> search over images on web...|$|E
40|$|In this paper, we {{show that}} Higher-Order Coloured Unification - a form of {{unification}} developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, <b>non</b> <b>semantic</b> information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple et al., 1991) 's analysis called for...|$|E
40|$|This paper {{shows that}} {{previously}} reported generation algorithms run into problems {{in dealing with}} f-structure representations. A generation algorithm that is suitable {{for this type of}} representations, the Semantic Kernel Generation (SKG) algorithm is presented. The SKG method has the same processing strategy as the Semantic Head Driven generation (SHDG) algorithm and relies on the assumption {{that it is possible to}} compute the Semantic Kernel (SK) and <b>non</b> <b>Semantic</b> Kernel (Non-SK) information for each input structure...|$|E
30|$|To {{obtain the}} embeddings {{to be used}} as input for the neural network we use the Doc 2 vec {{algorithm}} over unlabeled examples of our entire corpus in the conceptual representation. From the Doc 2 Vec model we can extract word 2 vec embeddings which are able to preserve <b>non</b> trivial <b>semantic</b> relationships between words. For instance where v(King) is the vector for king it has been found that the expression v(King)[*]−[*]v(Man)[*]+[*]v(Woman) returns a vector that is roughly equivalent to v(Queen). We do not train the word 2 vec algorithm directly but extract out word 2 vec vectors from the best performing Doc 2 Vec model in Magumba and Nabende [22]. We do this to demonstrate that a single model may be used for multiple purposes.|$|R
40|$|In a {{compositional}} semantic theory {{it should}} be possible to determine the interpretation of each expression irrespective of the contexts in which it possibly occurs. However, in order to account for the inter-sentential anaphoric links a coherent text usually contains, it seems that each sentence in the text should be interpreted with respect to the context set up by the interpretation of the sentences which precede it. A problem thus arises concerning whether a compositional theory of text interpretation is possible at all and, in case, what the distinctive features of such a theory should be. According to [JvEB 81] and [Jan 97], a <b>non</b> compositionl <b>semantic</b> theory can be made compositional by refining the notion of meaning it is based on. By assuming the formal framework of Discourse Representation Theory (DRT), in the paper it will be considered how the notion of (sentence) meaning DRT is based on should be refined in order to support a compositional theory of text interpretation. By [...] ...|$|R
40|$|Background and Objectives: Semantic {{satiation}} {{is defined}} as the subjective experience of the loss of access to the meanings of words or images caused by prolonged and quick repetitions of the material. Previous researches indicated that the semantic satiation of words and images occurs faster in schizophrenics than in healthy subjects. Individuals suffering from schizophrenia reveal the tendency to lose of access to the meaning of words after fewer words repetition than healthy controls. The aim of the research was to establish whether the semantic satiation of images in schizophrenia is the effect of the loss of meanings of images or is caused by <b>non</b> - <b>semantic</b> factors i. e. fatiguing experimental procedure. Methods: It was assumed that in conditions where the participant’s level of fatigue was parallel to the fatigue observed in the research on semantic satiation and the meaning of satiated images was not required for semantic decisions, schizophrenic patients and healthy controls would not reveal the semantic satiation effect defined as an increase in reaction time. Two groups of participants: patients suffering from schizophrenia (10 women and 10 men, average age 30) and healthy controls (9 female and 9 male, average age 30. 7) were shown 80 trails. Each one of them consisted of a satiated image which appeared repeatedly on the computer screen, and a non – satiated image accompanied by a written word, which were shown simultaneously after the final presentation of the satiated image. The participants’ task was to decide whether the written word named the object presented on the non – satiated picture correctly. The participants did not make any decisions on the basis of satiated images. Results: The results obtained confirmed the hypothesis. In conditions where participants were shown the images flashing on the computer screen but were not required to make a semantic decision related to those images, their reaction time to subsequently presented stimuli did not lengthen. Conclusions: The results confirmed the hypothesis that the semantic satiation effect in schizophrenia is a semantic phenomenon and is not related to <b>non</b> – <b>semantic</b> factors such as the subjects’ fatig...|$|R
40|$|International audienceThe DNS {{structure}} discloses {{useful information}} about the organization and the operation of an enterprise network, {{which can be used}} for designing attacks as well as monitoring domains supporting malicious activities. Thus, this paper introduces a new method for exploring the DNS domains. Although our previous work described a tool to generate existing DNS names accurately in order to probe a domain automatically, the approach is extended by leveraging semantic analysis of domain names. In particular, the semantic distributional similarity and relatedness of sub-domains are considered as well as sequential patterns. The evaluation shows that the discovery is highly improved while the overhead remains low, comparing with <b>non</b> <b>semantic</b> DNS probing tools including ours and others...|$|E
40|$|In {{this work}} we have {{developed}} an information measure called maxcorr suitable for closed loop controllers that makes use of temporal unsupervised learning. It is novel because is computed at the input side of the controller and consider the semantic value of signals, rather then being based on the <b>non</b> <b>semantic</b> approach of Shannon's entropy. The maxcorr {{can be applied to}} individual agents to estimate their learning ability, but most importantly to social swarms where agents are learning all the time to achieve a common goal. Indeed in a social system all agents learn at the same time thus being unpredictable. However maxcorr quantitatively explains how agents of a social system select information to make the closed loop model more predictable. Results are compatible with the Luhmann's theory of social differentiation...|$|E
40|$|Information {{retrieval}} from web {{has been}} a tough task due to an increased amount of contents dynamically day by day. In that current information retrieval results are only based on syntactic description for web service which may not provide an accurate solution for all service requests. On the other hand semantic based information retrieval has been the better option for precise and accurate results for queries. This paper proposes a semantic based hybrid matching approach that comprises different kinds of matching strategies combined together with an aim of flexible and efficient service retrieval in terms of accuracy. Using semantic based web ontology language service (OWL-S) conversion of <b>non</b> <b>semantic</b> description into corresponding semantic description is proposed. We also extend prototype to support save-off preprocessed data. The proposed work showed an attractive result after experiments by considering the functions namely recall and precision...|$|E
40|$|In {{this paper}} {{the notion of}} action {{atomicity}} is relaxed by permitting actions to be observed {{in the middle of}} their evolution. <b>Non</b> atomic <b>semantic</b> equivalences, based on the notion of bisimulation, are studied over stable event structures. Splitn bisimulation equivalence (denoted n ¸) considers each event as composed of n phases. ST bisimulation equivalence (denoted ST ¸) is a slight refinement of 2 ¸ where each ending phase is unambiguously associated to a beginning phase. We prove that, by increasing n, we get finer and finer equivalences (i. e. n+ 1 ¸ ` n ¸) and, moreover, that n+ 1 ¸ coincides with ST ¸ over those event structures whose autoconcurrency is at most n. The main consequence of these results is that, for image finite event structures, ST ¸ is the intersection of all the n ¸. 1 Introduction Most of the behavioural equivalences for concurrent systems are usually based on the assumption that the execution of an action is an atomic activity which cannot b [...] ...|$|R
40|$|The {{huge amounts}} of data {{produced}} in high-throughput techniques in the life sciences {{and the need for}} integration of heterogeneous data from disparate sources in new fields such as Systems Biology and translational drug development require better approaches to data integration. The semantic web is anticipated to provide solutions through new formats for knowledge representation and management. Software libraries for semantic web formats are becoming mature, but there exist multiple tools based on foundationally different technologies. SWI-Prolog, a tool with semantic web support, was integrated into the Bioclipse bio- and cheminformatics workbench software and evaluated in terms of performance against <b>non</b> Prolog-based <b>semantic</b> web tools in Bioclipse, Jena and Pellet, for querying a data set consisting of mostly numerical, NMR shift values, in the semantic web format RDF. The integration has given access to the convenience of the Prolog language for working with semantic data and defining data management workflows in Bioclipse. The performance comparison shows that SWI-Prolog is superior in terms of performance over Jena and Pellet for this specific dataset and suggests Prolog-based tools as interesting for further evaluations...|$|R
40|$|The Unified Modeling Language (UML) {{is one of}} the few {{modeling}} languages that {{is widely}} used in industry. While UML is mostly known as diagrammatic modeling lan-guage (e. g., visualizing class models), it is complemented by a textual language, called Object Constraint Language (OCL). OCL is a textual annotation language, based on a three-valued logic, that turns UML into a formal language. Unfortunately the semantics of this specification language, captured in the “Annex A ” of the OCL standard, leads to different interpretations of corner cases. Many of these corner cases had been subject to formal analysis since more than ten years. The situation complicated when with version 2. 3 the OCL was aligned with the latest version of UML: this led to the extension of the three-valued logic by a second exception element, called null. While the first exception element invalid has a strict seman-tics, null has a <b>non</b> strict <b>semantic</b> interpretation. These semantic difficulties lead to remarkable confusion for implementors of OCL compilers and interpreters. In this paper, we provide a formalization of the core of OCL in HOL. It provides deno...|$|R
40|$|In this paper, we {{show that}} Higher [...] Order Coloured Unification [...] a form of {{unification}} developed for automated theorem proving [...] provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, <b>non</b> <b>semantic</b> information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple et al., 1991) 's analysis called for. 1 Introduction It {{is well known that}} Higher [...] Order Unification (HOU) can be used to construct the semantics of Natural Language: (Dalrymple et al., 1991) [...] henceforth, DSP [...] show that it allows a treatment of VP [...] Ellipsis which successfully captures the interaction of VPE with quantification and nominal anaphora; (Pulman, 1995; Gardent and Kohlhase, 1996) use HOU to model the interpretation of focus and its interaction with focus sensitive operators, adverbial quantifiers and second occurrence expressions; (Gardent et al., 1996) shows that HOU yields a simple but precise [...] ...|$|E
40|$|The {{thesis is}} that a branch of {{discrete}} mathematics, Formal Concept Analysis, when applied to Semantic File Systems can lead to an improved personal information space. Semantic File Systems share many properties with their <b>non</b> <b>semantic</b> brethren, bringing more rich metadata {{and the ability to}} directly resolve user queries within the filesystem interface itself. A filesystem might offer upwards of a million files each of which having in the order of hundreds of discerning attributes. Formal Concept Analysis has typically been applied to a much smaller input data set and there are issues with scalability both in the initial finding of the set of Formal Concepts and also ongoing issues such as finding the list of files which are currently applicable (the extent) for a Formal Concept. The thesis is largely dependent on improving the scalability of Formal Concept Analysis in order for it to be applied to such a large dynamic data store...|$|E
40|$|URL] audienceOntology {{matching}} is {{a crucial}} issue {{in the domain of}} semantic web and data interoperability. In this paper, a core word based method for measuring similarity from the semantic level of ontology entities is described. In ontology, most labels of entities are compound words rather than single meaningful words. However, the main meaning is represented usually by one word of them, which is called core word. The core word is learned by investigating certain patterns, which are defined based on part of speech (POS) and linguistics knowledge. The other information is noted as complementary information. An algorithm is given to measure the similarity between a pair of compound words and short texts. In order to support diverse situation, especially when core words cannot be recognized, <b>non</b> <b>semantic</b> based ontology matching techniques are applied from lexical and structural level of ontology. The described method is tested on real ontology and benchmarking data sets. It showed good matching ability and obtained promising results...|$|E
50|$|No two apperceptive agnosic {{patients}} are the same, but case {{studies have been}} used to make theories on what causes the object recognition deficits. While it is established that semantics plays a large role in apperceptive agnosia deficits, it is not agreed upon how semantics alter recognition processes. One theory proposes that semantic memories are divided into differential semantic categories. Brain damage leads to apperceptive agnosia because there is damage to a particular semantic category. Another theory proposes that there are no different categories but there are different areas of the brain that stores different types of features. Living things are thought to be identified by their perceptual features and man-made objects are identified by their functional features. In this theory, damage to one particular area might cause a loss of ability to differentiate living things or non-living things depending on the area of damage. Yet another theory suggests that the pattern of deficit arise from independent impairments to a particular input modality and a single <b>non</b> perceptual <b>semantic</b> system that is organized by category. Deficits are largely due to semantics, however many categories are related perceptually as well. Objects that are biologically similar are likely to have physical resemblance to each other as well. Perceptual confusion arises because of structural similarity contributes or accounts for some modality specific deficit.|$|R
40|$|DCTG-GP is {{a genetic}} {{programming}} system that uses definite clause translation grammars. A DCTG {{is a logical}} version of an attribute grammar that supports the definition of context [...] free languages, and it allows semantic information associated with a language to be easily accomodated by the grammar. This is useful in genetic programming for defining the interpreter of a target language, or incorporating both syntactic and semantic problem [...] specific contraints into the evolutionary search. The DCTG-GP system improves on other grammar [...] based GP systems by permitting <b>non</b> [...] trivial <b>semantic</b> aspects of the language to be defined with the grammar. It also automatically analyzes grammar rules {{in order to determine}} their minimal depth and termination characteristics, which are required when generating random program trees of varied shapes and sizes. An application using DCTG-GP is described. 1 INTRODUCTION Genetic programming (GP) implementations have benefitted from simple program denotations [...] . ...|$|R
40|$|This paper {{presents}} a semantic parsing approach for <b>non</b> domain-specific texts. <b>Semantic</b> parsing {{is one of}} the major bottlenecks of Natural Language Understanding (NLU) systems and usually requires the building of expensive resources not easily portable to a different domain. Our approach obtains a case-role analysis, in which the semantic roles of the verb are identified. In order to cover all the possible syntactic realisations of a verb, our system combines their argument structure with a set of general semantic labelled diatheses models. Combining them, the system builds a set of syntactic-semantic patterns with their own role-case representation. Once the patterns are build, we use an approximate tree pattern-matching algorithm to identify the most reliable pattern for a sentence. The pattern matching is performed between the syntacticsemantic patterns and the feature-structure tree representing the morphological, syntactical and semantic information of the analysed sentence. For s [...] ...|$|R
40|$|The {{present study}} {{deals with the}} {{linguistic}} production of four children with Down Syndrome (aged 6 - 10 years). Each subject was audiotaped during verbal elicitation tasks. Both phonological and morphological abilities of the children are described paying particular attention to consonantal system and to free-standing morphemes. Our study differs from previous Italian research on Down Syndrome: {{for the first time}} the phonological description is based on a spectrographic analysis. The acoustic results reveal high percentage of substitution, assimilation processes and cluster reduction. A relevant delay in phonological development, with more errors in the production of later-developing sounds, was observed. Articulatory imprecision can be directly attributed to the oral and facial anomalies; in addition, general hypotonia and specific oral motor problems affect the coordination and the timing of articulatory gestures. The findings of the morphological analysis confirm that children with Down Syndrome omit many grammatical morphemes demonstrating particular difficulty with the production of the functional categories such as: articles, clitics, prepositions and auxiliaries. They show a consistent speech disorder that causes a simplification of many linguistic structures. The omission is probably a sort of economy strategy: functional words are <b>non</b> <b>semantic</b> elements with scarce phonetic and prosodic weight...|$|E
40|$|This {{research}} {{sets out}} to help computational modellers, to select the most cost effective Cloud service provider. This is when they opt to use Cloud computing in preference to using the in-house High Performance Computing (HPC) facilities. A novel Quality-aware computational Cloud Selection (QAComPS) service is proposed and evaluated. This selects the best (cheapest) Cloud provider‟s service. After selection it automatically sets-up and runs the selected service. QaComPS includes an integrated ontology that makes use of OWL 2 features. The ontology provides a standard specification and a common vocabulary for describing different Cloud provider‟s services. The semantic descriptions are processed by the QaComPS Information Management service. These provider descriptions are then used by a filter and the MatchMaker to automatically select the highest ranked service that meets the user‟s requirements. A SAWSDL interface is used to transfer semantic information to/from the QAComPS Information Management service and the <b>non</b> <b>semantic</b> selection and run services. QAComPS selection service has been quantitatively evaluated for accuracy and efficiency against Quality Matchmaking Process (QMP) and Analytical Hierarchy Process (AHP). The service was also evaluated qualitatively {{by a group of}} computational modellers. The results for the evaluation were very promising and demonstrated QaComPS‟s potential to make Cloud computing more accessible and cost effective for computational modellers...|$|E
40|$|The main {{objective}} of this internship is to design and implement a software model permitting to do analysis on stream of events which describe daily activities of a user. The interest of a complete interaction history between a user and its data is to permit a user to use his own episodic memory (and <b>non</b> <b>semantic)</b> {{and use it as}} a search criteria for elements of contextual nature. Therefore, the {{objective of this}} internship consists of extending the work on multimedia streams initiated by M. Beaudouin-Lafon and W. Mackay to adapt to the system proposed, and to analyse the data collected by the observers of Micromegas. Including other classic operations on streams (for example temporal filtering, insertion and deletion of events), along with the operations permitting aggregation of events with an objective of being able to work on different scales are also included in this internship. We present an activity algebra model which is based on the hierarchical relationships between activities and events. The model permits to resolve different activity overlapping issues and provides event-based and time-based filtering methods for data aggregation and multiscale viewing. We also provide details of its implementation and we believe that this model should prove useful for a wide variety of applications...|$|E
40|$|The role {{of context}} in {{construction}} grammar {{has been up}} to now viewed from a restricted point of view. That is, context is used to help in analysing the meaning and the pragmatic aspects of constructions, {{in keeping with the}} following assumption: 1. Context (common ground) forms part of the construction’s semantics Namely, it has been observed that, in many cases, constructions have a noncompositional meaning. A sub class of <b>semantic</b> <b>non</b> compositional construction is characterized by the fact that its interpretation involves the incorporation of pieces of information derived by taking into account the context, broadly speaking, of the utterance effected through the construction. The introduction of these contextual pieces of information is achieved in most cases by means of lexemes carrying an instructional meaning: “look at this aspect of context to process the meaning of the construction”. This situation is best described by this excerpt from Paul Kay...|$|R
40|$|The {{writing is}} the {{description}} about the scientific {{truth of the}} Islamic law, or in the Islamic science treasury is called Fiqh. Through the scientific process, {{we can find out}} lits theories of truth, for instant the correspondent, coherent, pragmatic, syntatic, <b>semantic,</b> <b>non</b> descriptive and over logic trutht. Toward those theories of truth, there are three approach methodologies that we can apply to get them. The first methodology is the religious approach, the second one is the scientific approach, and the last one is the philosophic approach. These three approach metodologies are closely relates with the development of the Islamic law, since they are laso the scientific disciplines. In the Islamic law, there is also mention about the correspondence and coherence truth. The methodology that is used to result the laws related to the general scientific methodology as well. For that reason, the truth which is resulted from that scientific approach is mostly positive...|$|R
40|$|This study {{explores the}} {{hypothesis}} that there are particular difficulties for secondary school students with specific developmental language disorder (SDLD) in understanding contextual, pragmatic meaning in relation to <b>non</b> pragmatic (<b>semantic)</b> meaning. It compares sixty-four SDLD students, aged between twelve and fourteen years, with chronolgical-age-matched and language-age-matched non-language impaired students. Language age is measured by a test of non-pragmatic meaning comprehension. Incorporating {{the development of new}} procedures, the study examines the students' comprehension of two types of ambiguity where the context determines the speaker's intention: inconsistent messages of emotion and multiple meanings in context. These types of ambiguity are evident in a range of communicative intent, for example, to express sarcasm, idiomatic expression, deceipt and humour. Preliminary study into adolescent language suggests that, at this age, there is a particular expectation for students to be able to understand these kinds of communication, both in the classroom and socially. The study provides much evidence to support its central hypothesis: SDLD students made significantly fewer pragmatic responses than both comparison groups. The way students responded suggested two types of pragmatic analysis, one concerning plausibility judgment and a second concerning awareness of multiple reference and detection of miscomprehension. Nonlanguage- impaired children were significantly more able to use these types of analysis, for example, to rule out literal interpretations when they did not know the contextually implied meaning. Some evidence is provided to suggest that these analyses are underpinned by skills in both the metacommunicative and linguistic domains. The study's findings have several implcations for research and practice. The are serious implications, for example, for diagnostic assessment, {{in the light of the}} literature survey revealing that those currently available do not assess pragmatic meaning comprehension. The findings further provide a basis to challenge a view that disorders in the semantic and pragmatic domains necessarily co-occur, as reflected in the diagnostic category semanti-pragmatic disorder...|$|R
40|$|The World Wide Web (WWW) now {{is widely}} used as a {{universal}} medium for information exchange. Semantic interoperability among different information systems in the WWW is limited due to information heterogeneity, and the <b>non</b> <b>semantic</b> nature of HTML and URLs. Ontologies have been suggested {{as a way to}} solve the problem of information heterogeneity by providing formal, explicit definitions of data and reasoning ability over related concepts. Given that no universal ontology exists for the WWW, work has focused on finding semantic correspondences between similar elements of different ontologies, i. e., ontology mapping. Ontology mapping can be done either by hand or using automated tools. Manual mapping becomes impractical as the size and complexity of ontologies increases. Full or semi-automated mapping approaches have been examined by several research studies. Previous full or semi-automated mapping approaches include analyzing linguistic information of elements in ontologies, treating ontologies as structural graphs, applying heuristic rules and machine learning techniques, and using probabilistic and reasoning methods etc. In this paper, two generic ontology mapping approaches are proposed. One is the PRIOR+ approach, which utilizes both information retrieval and artificial intelligence techniques in the context of ontology mapping. The other is the non-instance learning based approach, which experimentally explores machine learning algorithms to solve ontology mapping problem without requesting any instance. The results of the PRIOR+ on different tests at OAEI ontology matching campaign 2007 are encouraging. The non-instance learning based approach has shown potential for solving ontology mapping problem on OAEI benchmark tests...|$|E
40|$|In this review, {{we propose}} that the neural basis for the spontaneous, diversified human tool use is an area devoted to the {{execution}} and observation of tool actions, located in the left anterior supramarginal gyrus (aSMG). The aSMG activation elicited by observing tool use is typical of human subjects, as macaques show no similar activation, even after an extensive training to use tools. The execution of tool actions, {{as well as their}} observation, requires the convergence upon aSMG of inputs from different parts of the dorsal and ventral visual streams. <b>Non</b> <b>semantic</b> features of the target object may be provided by the posterior parietal cortex (PPC) for tool-object interaction, paralleling the well-known PPC input to AIP for hand-object interaction. Semantic information regarding tool identity, and knowledge of the typical manner of handling the tool, could be provided by inferior and middle regions of the temporal lobe. Somatosensory feedback and technical reasoning, as well as motor and intentional constraints also play roles during the planning of tool actions and consequently their signals likewise converge upon aSMG. We further propose that aSMG may have arisen though duplication of monkey AIP and invasion of the duplicate area by afferents from PPC providing distinct signals depending on the kinematics of the manipulative action. This duplication may have occurred when Homo Habilis or Homo Erectus emerged, generating the Oldowan or Acheulean Industrial complexes respectively. Hence tool use may have emerged during hominid evolution between bipedalism and language. We conclude that humans have two parietal systems involved in tool behavior: a biological circuit for grasping objects, including tools, and an artifactual system devoted specifically to tool use. Only the latter allows humans to understand the causal relationship between tool use and obtaining the goal, and {{is likely to be the}} basis of all technological developments. <br/...|$|E
40|$|Introduction. The {{semantic}} {{variant of}} primary progressive aphasia (svPPA) {{is characterized by}} a progressive deterioration of semantic memory, associated with word comprehension and naming deficits (Gorno-Tempini et al., 2011). Patients with svPPA also show acquired surface dyslexia and dysgraphia. According to the 'dual-route' model, reading aloud (Coltheart, 1978) and spelling (Ellis, 1982) of nonwords are achieved by resorting to a nonlexical and <b>non</b> <b>semantic</b> route, through the application of grapheme-phoneme conversion (GPC) rules. The aim {{of the present study was}} to examine the performance of two individuals with svPPA in tasks tapping consistent GPC rules of two types: a) non-contextual GPC rules (n-cGPC) in which each grapheme has only one unequivocal phonemic correspondence; 2) context-sensitive GPC rules (c-sGPC) in which the pronunciation or spelling is determined by the preceding or following letters (e. g. C and G, which are consistently pronounced /k/ and /g/, change to /s/ and /ʒ/ before the letters E and I). Method. Participants. We report the performance of two French-speaking patients (NG and ND) that fulfilled clinical and imaging-supported criteria for svPPA. Both presented with surface dyslexia and dysgraphia. Each of them was matched by age, gender and education to three normal controls. Tasks and procedure. NG and ND were administered the following experimental tasks comprising bisyllabic and trisyllabic words and nonwords with n-cGPC and c-sGPC rules, placed at the beginning, the middle or the end of the stimulus: 1) regular word and nonword reading; 2) adding a missing letter in words and nonwords: the patients were asked to complete stimuli by a letter to make them correspond to the word or nonword pronounced by the experimenter (for n-cGPC rules, the missing letter was randomly chosen; for c-sGPC rules, one of the letters of the context was missing); 3) rhyme judgement of nonwords: stimuli were auditorily presented and the patients had to indicate if they rhymed or not (for c-sGPC stimuli, in non-rhyming pairs, the difference between nonwords implied one of the context letters). Results. As shown in Table 1, NG and ND's performance was well preserved in tasks with words and nonwords with n-cGPC rules. However, ND was impaired in all tasks with c-sGPC rules, whilst NG's performance was affected for c-sGPC stimuli only in the rhyming task. Discussion. The processing of regular words and nonwords is assumed to be unimpaired in svPPA since their production do not necessary rely on the lexical-semantic route of reading and spelling. In the present study, this hypothesis was confirmed for n-cGPC rules only. However, both patients presented with difficulty in c-sGPC stimuli. This pattern of performance, never reported before in svPPA, questions the possible role of semantic memory in the reading and spelling of stimuli comprising consistent but contextual GPC rules...|$|E
40|$|Self-referential {{processing}} relies {{mainly on}} the medial prefrontal cortex (MPFC) and enhances memory encoding (i. e., Self-Reference Effect, SRE) as it improves the accuracy and richness of remembering in both young and older adults. However, studies on age-related changes in the neural correlates of the SRE on the subjective (i. e., autonoetic consciousness) and the objective (i. e., source memory) qualitative features of episodic memory are lacking. In the present fMRI study, we compared {{the effects of a}} self-related (semantic autobiographical memory task) and a <b>non</b> self-related (general <b>semantic</b> memory task) encoding condition on subsequent episodic memory retrieval. We investigated encoding-related activity during each condition in two groups of 19 younger and 16 older adults. Behaviorally, the SRE improved subjective memory performance in both groups but objective memory only in young adults. At the neural level, a direct comparison between self-related and non self-related conditions revealed that SRE mainly activated the cortical midline system, especially the MPFC, in both groups. Additionally, in older adults and regardless of the condition, greater activity was found in a fronto-parietal network. Overall, correlations were noted between source memory performance and activity in the MPFC (irrespective of age) and visual areas (mediated by age). Thus, the present findings expand evidence {{of the role of the}} MPFC in self-referential processing in the context of source memory benefit in both young and older adults using incidental encoding via semantic autobiographical memory. However, our finding suggests that its role is less effective in aging...|$|R
40|$|This paper {{presents}} a sizable grammar for English {{written in the}} Tree Adjoining grammar (TAG) formalism. The grammar uses a TAG that is both lexicalized (Schabes, Abeillé, Joshi 1988) and feature-based (Vijay-Shankar, Joshi 1988). In this paper, we describe {{a wide range of}} phenomena that it covers. A Lexicalized TAG (LTAG) is organized around a lexicon, which associates sets of elementary trees (instead of just simple categories) with the lexical items. A Lexicalized TAG consists of a finite set of trees associated with lexical items, and operations (adjunction and substitution) for composing the trees. A lexical item is called the anchor of its corresponding tree and directly determines both the tree 2 ̆ 7 s structure and its syntactic features. In particular, the trees define the domain of locality over which constraints are specified and these constraints are local with respect to their anchor. In this paper, the basic tree structures of the English LTAG are described, along with some relevant features. The interaction between the morphological and the syntactic components of the lexicon is also explained. Next, the properties of the different tree structures are discussed. The use of S complements exclusively allows us {{to take full advantage of}} the treatment of unbounded dependencies originally presented in Joshi (1985) and Kroch and Joshi (1985). Structures for auxiliaries and raising-verbs which use adjunction trees are also discussed. We present a representation of prepositional complements that is based on extended elementary trees. This representation avoids the need for preposition incorporation in order to account for double wh-questions (preposition stranding and pied-piping) and the pseudo-passive. A treatment of light verb constructions is also given, similar to what Abeillé (1988 c) has presented. Again, neither noun nor adjective incorporation is needed to handle double passives and to account for CNPC violations in these constructions. TAG 2 ̆ 7 S extended domain of locality allows us to handle, within a single level of syntactic description, phenomena that in other frameworks require either dual analyses or reanalysis. In addition, following Abeillé and Schabes (1989), we describe how to deal with <b>semantic</b> <b>non</b> compositionality in verb-particle combinations, light verb constructions and idioms, without losing the internal syntactic composition of these structures. The last sections discuss current work on PRO, case, anaphora and negation, and outline future work on copula constructions and small clauses, optional arguments, adverb movement and the nature of syntactic rules in a lexicalized framework...|$|R
40|$|A {{person with}} aphasia, DRG, showed a severe phonological/neologistic jargon when reading aloud {{but in a}} lesser degree in naming and {{spontaneous}} speech. Reading digits was free of phonological errors. 	Case report: DRG, 58 y. o.; education = 8 years. Vascular lesion: left occipito-temporal and right temporo-parietal (CT). No auditory cortical ERP were found. 	Neuropsychological examination: DRG showed the typical pattern of cortical deafness. He could not perceive any verbal and non-verbal sound. He did not blink to very loud sounds. His spontaneous speech was relatively understandable, fluent and loud, with phonemic paraphasias and a few neologisms. He had no problems in visual recognition and written calculation showed only occasional errors in complex operations. His reading comprehension was preserved while reading aloud was severely neologistic (Table 1). 	Experimental investigation: DRG's naming and reading aloud of words were compared at an interval of nine months. The amount of correct items and of non target-recognizable neologisms is reported in Table 1. A worse performance in reading aloud was consistent over time and tasks. 	Reading aloud of numbers, up to six digits long, presented in the Arabic code (e. g., " 9 ", " 267 ") and in the alphabetic code (e. g., "nine", "two hundred sixty seven"), resulted in a different performance. Reading aloud of alphabetically presented words was successful in only 28 / 110 items (25 %), with neologisms being the most frequent error type. Reading Arabic numbers was successful in 25 / 55 items (45 %); no phonological/neologistic error was produced, errors being of the "lexical" type, thus numbers were substituted by other phonologically correct numbers. 	Discussion: DRG showed a previously unreported dissociation in speech output: naming and connected spontaneous speech were less affected by phonological disturbances than reading aloud. The opposite dissociation, reading aloud superior to naming, was described by Semenza et al. (1992), also {{in a case of}} cortical deafness. These authors explained their case suggesting a difficulty in activating the speech output lexicon from the semantic system and by a disturbed print to sound conversion (sublexical route). Reading aloud could be performed either via the direct, lexical, <b>non</b> <b>semantic,</b> route or by summation of residual capacities of disturbed access to speech output lexicon from the semantic system and disturbed print to sound conversion. In contrast, DRG would be disturbed in the direct lexical route and in print to sound conversion: lexical activation of the speech output lexicon from the semantic system would not be sufficient to read correctly aloud without the help of the other two routes. Lack of phonological errors in reading Arabic numbers would be {{explained by the fact that}} Arabic numbers, consistently with recent literature (Bencini et al., 2011; Semenza et al., 2014; Dotan and Friedmann, 2015), would directly activate the whole phonological form rather than needing grapheme to phoneme conversion. This case provides insights about the processing dynamics among the three reading routes...|$|E
40|$|AbstractAutism Spectrum Disorders (ASD) {{has been}} {{associated}} with impaired multisensory processing, however research on the topic has been inconclusive. For instance, research on the synchrony perception of complex stimuli has shown that children with ASD have impaired integration of audiovisual speech (e. g., a woman counting or telling a story) but normal non speech integration (e. g., a ball moving through a series of plastic ramps and cliffs; Bebko et al., 2006), while research on adult ASD patients has shown impaired integration for both speech (e. g., syllables) and non speech events (e. g., flash-beeps, handclap; de Boer- Schellekens et al., 2013). Studies utilizing simple stimuli and illusory paradigms such as the double-flash illusion have suggested that individuals with ASD exhibit an extended temporal integration window as compared to healthy participants (Foss-Feig et al., 2010; Kwakye et al., 2011), while others have shown no such effect (e. g., Van der Smagt et al., 2007). It is as yet unclear, therefore, whether or not individuals with ASD have impaired integration mechanisms and whether this impairment is due to the stimuli presented (simple vs. complex; social vs. non social), the population used (adult vs. children, severity of symptoms), and/or the tasks utilized (e. g., preferential looking paradigm vs. temporal order judgments). Additionally, it is as yet unclear whether individuals with ASD are impaired in terms of timing or binding per se (e. g., Freeman et al., 2013). In order to elucidate this issue, we aim to further examine the nature of these deficits through a well-formed group of children with similar symptom severity and two types of tasks. Initially, using a reaction time (RT) task, we will assess the audiovisual integration capabilities of children with ASD as compared to typically developing (TD) children without the involvement of timing (i. e., no timing differences will be introduced). According to the ‘unity effect’, a multisensory event is perceived as an integrated multisensory event (rather than multiple unimodal events) when signals are present close in time and space and due to other factors (e. g., informational relatedness; e. g., Vatakis & Spence, 2007). In the RT experiment, therefore, we will not modulate space and time, but informational relatedness. Specifically, participants will be asked to complete speeded detection of two targets. The targets will be audiovisual, visual, and auditory and for the audiovisual cases the steams will be presented in congruent and incongruent format. Subsequently, the same group of individuals will be tested in a simultaneity judgment (SJ) task where the temporal window of integration will be assessed. The SJ task will target the evaluation of temporal processing. For both tasks, three types of stimuli will be used: a) simple stimuli in order to minimize the meaningful context (Bien et al., 2013), b) stimuli with emotional context depicted through human faces in order to assess ASD processing of facial social stimuli, and c) stimuli with emotional context using body expressions (instead of faces). The use of the RT and SJ tasks we allow the evaluation of the interaction of multisensory integration and synchrony perception in ASD as well as the role of stimulus type (semantic vs. <b>non</b> <b>semantic,</b> social vs. non social) in multisensory processing...|$|E

