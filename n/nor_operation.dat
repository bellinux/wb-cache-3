15|211|Public
25|$|The NAND (dually <b>NOR)</b> <b>operation</b> lacks all these, thus {{forming a}} basis by itself.|$|E
6000|$|And {{again in}} color. I {{imagine that the}} quality of it which we term purity is {{dependent}} on the full energizing of the rays that compose it, whereof if in compound hues any are overpowered and killed by the rest, so as to be of no value <b>nor</b> <b>operation,</b> foulness is the consequence; while so long as all act together, whether side by side, or from pigments seen one through the other, so that all the coloring matter employed comes into play in the harmony desired, and none be quenched nor killed, purity results. And so in all cases I suppose that pureness is made to us desirable, because expressive of the constant presence and energizing of the Deity in matter, through which all things live and move, and have their being, and that foulness is painful as the accompaniment of disorder and decay, and always indicative of the withdrawal of Divine support. And the practical analogies of life, the invariable connection of outward foulness with mental sloth and degradation, as well as with bodily lethargy and disease, together with the contrary indications of freshness and purity belonging to every healthy and active organic frame, (singularly seen in the effort of the young leaves when first their inward energy prevails over the earth, pierces its corruption, and shakes its dust away from their own white purity of life,) all these circumstances strengthen the instinct by associations countless and irresistible. And then, finally, with the idea of purity comes that of spirituality, for the essential characteristic of matter is its inertia, whence, by adding to it purity or energy, we may in some measure spiritualize even matter itself. Thus in the descriptions of the Apocalypse it is its purity that fits it for its place in heaven; the river of the water of life, that proceeds out of the throne of the Lamb, is clear as crystal, and the pavement of the city is pure gold, like unto clear glass.[28] ...|$|E
50|$|The NAND (dually <b>NOR)</b> <b>operation</b> lacks all these, thus {{forming a}} basis by itself.|$|E
5000|$|The {{matrices}} S and P {{correspond to}} the Sheffer (NAND) and the Peirce (<b>NOR)</b> <b>operations,</b> respectively: ...|$|R
5|$|DragonFly {{switched}} to multiprocessor safe slab allocator, which requires neither mutexes <b>nor</b> blocking <b>operations</b> for memory assignment tasks. It was eventually ported into standard C library in the userland, where it replaced FreeBSD's malloc implementation.|$|R
5000|$|Do {{not permit}} {{tampering}} with the voting systems <b>operations,</b> <b>nor</b> allow voters {{to sell their}} votes.|$|R
50|$|The <b>NOR</b> <b>{{operation}}</b> is {{a logical}} operation on two logical values, typically the values of two propositions, that produces a value of true {{if and only if}} both operands are false. In other words, it produces a value of false if and only if at least one operand is true.|$|E
50|$|In {{order to}} form and test {{fundamental}} community ecology hypotheses or address applications such as impact assessment, conservation, and exploitation of the marine environment, one needs to investigate the complex interactions between sediments, organisms, and water. A host of burgeoning technologies are slowly gaining acceptance to measure and explore this dynamic interface through biological, chemical, and physical approaches. Viollier et al. (2003) and Rhoads et al. (2001) provide overviews of this topic though the technologies involved and the standards used are changing rapidly. Several techniques have allowed benthologists to address ‘big-picture’ questions of geochemical-biological interactions and ecosystem functioning. Betteridge et al. (2003) used acoustic technology to measure sedimentary dynamics in situ at a scale relevant to macrofauna. Their benthic landers recorded water velocities near the seabed while simultaneously quantifying sediment disturbance patterns in high resolution. Benthic chambers {{have been used to}} examine the productivity of realistic macrofaunal assemblages under different flow regimes (Biles et al. 2003). Isotopic analysis methods permit food-web and environmental impact investigations (e.g. Rogers 2003; Schleyer et al. 2006) impossible to conduct outside of a laboratory only a few years ago. Short-sequence DNA methods (e.g. Biodiversity Institute of Ontario 2006) are rapidly moving toward automated identification and diversity assessment techniques that hold the promise of revolutionising benthic ecology. Keegan et al. (2001) described the relationships among workers and authorities evaluating long-established, though often expensive and slow, methodologies with more recent technological developments as sometimes discordant. Gray et al. (1999b) lamented that there is a strong institutional tendency for sediment ecologists to rely on sampling methods developed in the early 1900s! A fine balance needs to be struck. Some degree of paradigm inertia is necessary to maintain intellectual continuity, but it can be taken too far. Physics, as a science, confronted this issue long ago and has widely embraced new technologies after establishing a scientific culture of always linking new techniques to established findings in a period of calibration and evaluation. The pace of this process in biology, as a whole, has quickened {{over the past few decades}} and ecology has only recently come to this horizon. This article introduces one such technology, sediment profile imagery (SPI) that is slowly gaining acceptance and currently undergoing its evaluation and calibration period even though it has existed since the 1970s. Like many of the technologies mentioned above, each new capability requires a careful consideration of its appropriateness in any particular application. This is especially true when they cross important, though often subtle, boundaries of data collection limitations. For example, much of our benthic knowledge has been developed from point-sample methods like cores or grabs, whereas continuous data collection, like some video transect analysis methods (e.g. Tkachenko 2005), may require different spatial interpretations that more explicitly integrate patchiness. While remote sampling techniques often improve our point-sampling resolution, benthologists need to consider the real-world heterogeneity at small spatial scales and compare them to the noise inherent to most high-volume data collection methods (e.g. Rabouille et al. 2003 for microelectrode investigations of pore water). New developments in the field of SPI will provide tools for investigating dynamic sediment processes, but also challenge our ability to accurately interpolate point-data collected at spatial densities approaching continuous data sets.SP imagery as embodied in the commercial REMOTS system (Rhoads et al. 1997) is expensive (>NZ$60,000 at time of writing), requires heavy lifting gear (ca. 66-400 kg with a full complement of weights to effectively penetrate sediments), and is limited to muddy sediments. REMOTS is not well suited to small research programmes, <b>nor</b> <b>operation</b> in shallow water from small vessels, which is, quite possibly, an area where it could be most useful. Studying shallow sub-tidal environments can be a challenging exercise, especially among shifting sands. Macrofaunal sampling usually occurs at the sub-metre scale, whilst the dominant physical factors such as wave exposure and sediment texture can change at a scale of only metres, even though they are often only resolved to a scale of hundreds of metres. In such a dynamic environment, monitoring potentially transient disturbances like a spoil mound requires benthic mapping at fine spatial and temporal scales, an application ideally suited to SPI.|$|E
40|$|Utilization of {{a quantum}} system whose time-development is {{described}} by the non-linear Schrodinger equation in the transformation of qubits would {{make it possible to}} construct quantum algorithms which would be useful in a large class of problems. An example of such a system for implementing the logical <b>NOR</b> <b>operation</b> is demonstrated...|$|E
40|$|A {{scheme for}} logical {{computation}} using non-linear dynamical systems is presented. Examples of discrete-time maps configured as AND, OR, NAND and NOR gates are given. It is {{seen that the}} logical operations are flexible {{in the sense that}} an AND gate can be transformed into an OR gate with a simple change of a single parameter and vice-versa. Also, a NAND gate can be transformed into a NOR gate and vice-versa. It is shown, by example, that the scheme can even be extended to continuous-time flows. Since the NAND and <b>NOR</b> <b>operations</b> are universal, it is possible to implement any switching function by interconnecting blocks that realize these operations. Comment: 4 pages, 6 figures, 3 tables. Title-case corrected. Incorrect references corrected. Submitted to Physical Review...|$|R
50|$|What {{proposition}} 6. really says is {{that any}} logical sentence {{can be derived from}} a series of <b>NOR</b> <b>operations</b> on the totality of atomic propositions. This is in fact a well-known logical theorem produced by Henry M. Sheffer, of which Wittgenstein makes use. Sheffer's result was, however, restricted to the propositional calculus, and so, of limited significance. Wittgenstein's N-operator is however an infinitary analogue of the Sheffer stroke, which applied to a set of propositions produces a proposition that is equivalent to the denial of every member of that set. Wittgenstein shows that this operator can cope with the whole of predicate logic with identity, defining the quantifiers at 5.52, and showing how identity would then be handled at 5.53-5.532.|$|R
50|$|A {{limited partner}} may not {{conclude}} partnership <b>operations</b> <b>nor</b> may such a partner represent the partnership to 3rd parties.|$|R
40|$|Utilization of {{a quantum}} system whose time-development is {{described}} by the nonlinear Schrödinger equation in the transformation of qubits would {{make it possible to}} construct quantum algorithms which would be useful in a large class of problems. An example of such a system for implementing the logical <b>NOR</b> <b>operation</b> is demonstrated. © World Scientific Publishing Company...|$|E
40|$|Abstract — Binary Decision Diagrams (BDDs) {{are useful}} data {{structures}} for symbolic Boolean manipulations. BDDs {{are used in}} many tasks in VLSI/CAD, such as equivalence checking, property checking, logic synthesis, and false paths. In this paper we describe a new approach for the realization of a BDD package. To perform manipulations of Boolean functions, the proposed approach {{does not depend on}} the recursive synthesis operation of the IF-Then-Else (ITE). Instead of using the ITE operation, the basic synthesis algorithm is done using Boolean <b>NOR</b> <b>operation...</b>|$|E
40|$|PFSF) License Application (LA). Major {{changes of}} this {{amendment}} include: 9 The License Application has been revised {{to add an}} Appendix that includes PFS commitment letters submitted to the NRC subsequent to the final {{request for additional information}} (RAI) responses associated with the PFSF Safety Analysis Report (SAR) and Environmental Report (ER). In addition, the License Application has been revised to emphasize that neither construction <b>nor</b> <b>operation</b> of the PFSF shall commence until adequate funding for each of these phases has been committed. UJM$$OI PP 4 hU. S. NRC Document Control Desk Page...|$|E
50|$|In {{the same}} way, {{doctrine}} is neither <b>operations</b> <b>nor</b> tactics. It {{serves as a}} conceptual framework uniting all three levels of warfare.|$|R
3000|$|... +), three constant-coefficient-{{multiplication}}s (with 3 and 2), four additions, and one division. A multiplication {{with numbers}} such as 2 and 3 is usually substituted by one bit-shift and addition. Thus, in practice, three multiplications, five additions, one division, and three bit-shifts are used. Neither additional information <b>nor</b> other <b>operation</b> is required.|$|R
40|$|In {{this paper}} we discuss the {{functional}} requirements for building an optical cellular automaton. We show that three key functions suffice to design the system: namely routing and shifting of information contained in two-dimensional (2 -D) data planes, and logic <b>NOR</b> <b>operations</b> applied on these data. Then we introduce a basic instruction set - a kind of low level language - which finds {{its roots in the}} above-mentioned basic functions. We show how two well-known cellular algorithms, cellular logic image processing (CLIP) and symbolic substitution logic (SSL), can be reduced to this code. Furthermore, we discuss aspects of the practical hardware implementation of the latter functions and we illustrate their confluence and hardware compatibility by suggesting a possible processor layout. We demonstrate by means of computer simulations how CLIP and SSL can be implemented with this processor. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Utilization of {{a quantum}} system whose time-development is {{described}} by the nonlinear Schrodinger equation in the transformation of qubits would {{make it possible to}} construct quantum algorithms which would be useful in a large class of problems. An example of such a system for implementing the logical <b>NOR</b> <b>operation</b> is demonstrated. Comment: 4 pages 1 figure, Latex. Changes in second version: Two important references (numbered 2 and 10) have been added, and associated discussion has been modified. Explanatory notes intended for a more general audience are available at [URL]...|$|E
40|$|Conference Name:International Conference on Intelligent Computation Technology and Automation. Conference Address: Changsha, PEOPLES R CHINA. Time:OCT 20 - 22, 2008. This paper put the Rough set {{methodology}} {{based on}} the information table disposal to expand the dual (binary) relations described by the neighborhood system. To make full use of logic operations including AND operation by-bit, XOR operation by-bit and <b>NOR</b> <b>operation</b> by-bit, gains the certain and uncertain information among the various decision-making factors. The use of knowledge related with Grey system theory, establishes their mathematical model for decision making and data mining, according to the principle of the priority of certain information in decision-making and the ambiguity degree from, small to big. At last, a new method or data mining is proposed by the use of combining grey system theory and Rough set...|$|E
40|$|Behaviorally {{induced stress}} is {{associated}} with increased arginine vasopressin (AVP) secretion. In this report we describe a phasic conditioned response of AVP secretion yielding 2. 6 - 7. 1 times normal plasma concentration of this hormone in association with a physiological state of decreased activation, that associated with the mental technique of "transcendental meditation" (TM) in long-term practitioners (6 - 8 years of regular elicitation). Such a very large phasic response of AVP was previously unknown in the normal physiology of AVP. This elevation was not accompanied by elevation of plasma osmolality. Unstylized ordinary eyes closed rest in a separate group of subjects studied {{in the same manner}} was associated with normal plasma AVP concentration. Galvanic skin resistance (GSR) increased during both TM and rest with significantly larger increase associated with TM. Other measures of activation, including muscle metabolism, and the Spielberger Anxiety Inventory indicated marked relaxation in association with TM. In previous research {{it has been shown that}} blood pressure does not change acutely during this behavior. These observations indicate that neither stress <b>nor</b> <b>operation</b> of other usual homeostatic control mechanisms are responsible for elevated for AVP in the meditators. It is speculated that the apparently unique mechanism of TM-induced AVP secretion may be more specifically related to the behavioral effects of meditation. © 1985...|$|E
40|$|AbstractDNA {{self-assembly}} is {{the most}} advanced and versatile system that has been experimentally demonstrated for programmable construction of patterned systems on the molecular scale. It has been demonstrated that the simple binary arithmetic and logical operations can be computed {{by the process of}} self assembly of DNA tiles. Here we report a one-dimensional algorithmic self-assembly of DNA triple-crossover molecules {{that can be used to}} execute five steps of a logical NAND and <b>NOR</b> <b>operations</b> on a string of binary bits. To achieve this, abstract tiles were translated into DNA tiles based on triple-crossover motifs. Serving as input for the computation, long single stranded DNA molecules were used to nucleate growth of tiles into algorithmic crystals. Our method shows that engineered DNA self-assembly can be treated as a bottom-up design techniques, and can be capable of designing DNA computer organization and architecture...|$|R
50|$|In the {{intermediate}} case (neither cw, <b>nor</b> short pulse <b>operation),</b> the rate equations for excitation and relaxation in the optical {{medium must be}} considered together.|$|R
40|$|A {{family of}} {{multiuser}} detectors is analyzed which require neither matrix inversions <b>nor</b> other <b>operations</b> with significant complexity. The time complexity per bit {{of most of}} them is independent {{of the number of}} users. Nevertheless, their spectral efficiency for random spreading sequences is shown to be not far behind that of linear MMSE detection...|$|R
40|$|The mission {{critical}} operation archiving {{system has been}} designed and built using the Oracle database for the twenty-two synchrotron radiation beam lines at the 2. 5 GeV positron storage ring at the Photon Factory, where X-ray/VUV synchrotron radiation experiments are simultaneously carried out. When any one of beam lines is malfunctioning, neither injecting the 2. 5 GeV beam into the storage ring <b>nor</b> <b>operation</b> of the ring is allowed due to the radiation safety reason. The system is designed for critical operation of the synchrotron radiation beam lines to provide a quick recovery from a failure, allowing a long term operation. The system has real-time capability to automatically store the database with all possible operational events of all vacuum valves/shutters and safety interlock signals, and all static operational data, including {{the pressures of the}} beam lines and the storage ring, and related operational data which represent the physical behaviors of the beam lines. By retrieving any combination of operational data, the system allows to reproduce the physical behaviors that have occurred in the beam lines. The total number of items to be inspected by the system is over 40 million in order to obtain a correlation between the faulty component and other physical components that suggests the cause of the failure. With the aid of the system, the operator at the control room can easily determine the faulty component, and recover the accelerator component. 1...|$|E
40|$|This paper {{addresses}} Alexander’s {{theory of}} the sensible world, of both its essential structure and {{of the most important}} sorts of change within in, such as alteration and blending. First (i), I overview the minimal requirements for an item to be considered a "body" according to Alexander; then, {{the main part of the}} article(ii-iv) determines which factors explain the physical characteristics of the bodies and their mutual interactions (that is, their relations of activity and passivity with one another). In examining these questions, I ask why Alexander does not find the Stoic account of material bodies to be satisfactory and, simultaneously, I consider Alexander’s strange construal of the (pseudo-) Stoic definition of body as a "stuff" composed either of matter, or of matter and qualities (e. g. in the "De anima") : why does Alexander "need" this fictive theory? The following part of this article (v) shows, that the answer to this question lies in the "anti-materialist" and "anti-reductionist" orientation of Alexander’s physics: according to this option, neither the constitution <b>nor</b> <b>operation</b> of bodies can be reduced to their minimal and necessary conditions (that are, respectively, matter and contact). Thus, Alexander's physical ontology is the exact opposite of the Stoic view, and it has been developped in "reaction" to it: according to the Exegete, the essence of bodies and their changes can only be understood by reference to the incorporeal and intelligible principles that ultimately form the very factors responsible for what I call the specific "coherence" of the sensible world (that is, of both its polarity and selectivity). Finally (vi), I examine a specific problem Alexander solves by emphasizing the non-corporeal causes of the natural world; in this context, I suggest that his position leads him to a very special type of "naturalism", which was particularly exciting for the Neoplatonists and, later, highly consonant with the medieval approaches to accounting for the causes of sensible things. Because bodies are, for Alexander, more than mere bodies, his physics becomes not only the common point of reference for Plotinus and his followers in Late Antiquity and Middle Ages, but also an important point of departure for what we might call a metaphysical approach to the sensible world and phenomena. status: publishe...|$|E
40|$|This {{doctoral}} dissertation centers on robust adaptive networks. Robust adaptation strategies are devised to solve typical network inference {{tasks such as}} estimation and detection in a decentralized manner {{in the presence of}} impulsive contamination. Typical in wireless communication environments, an impulsive noise process can be described as one whose realizations contain sparse, random samples of amplitude much higher than nominally accounted for. An attractive feature that these robust adaptive strategies enjoy is that neither their development <b>nor</b> <b>operation</b> hinges on the availability of exact knowledge of the noise distribution: The robust adaptive strategies are capable of learning it on-the-fly and adapting their parameters accordingly. Forgoing data fusion centers, the network agents employing these strategies rely solely on local interactions and in-network processing to perform inference tasks, which renders networks more reliable, resilient to node and link failure, scalable, and resource efficient. Distributed cooperative processing finds applications in many areas including wireless sensor networks in smart-home, environmental, and industrial monitoring; healthcare; and military surveillance. Since adaptive systems based on the mean-square-error criterion see their performance degrade in the presence of non-Gaussian noise, the robust adaptive strategies developed in this dissertation harness nonlinear data processing and robust statistics instead to mitigate the detrimental effects of impulsive noise. To this end, a robust adaptive filtering algorithm is developed that employs an adaptive error nonlinearity. The error nonlinearity is chosen to be a convex combination of preselected basis functions where the combination coefficients are adapted jointly with the estimate of the parameter of interest such that the mean-square-error relative to the optimal error nonlinearity is minimized in each iteration. Then, a robust diffusion adaptation algorithm of the adapt-then-combine variety is developed as an extension of its stand-alone counterpart for distributed estimation over networks where the measurements may be corrupted by impulsive noise. Each node in the network runs a combination of its neighbors’ estimates through one iteration of a local robust adaptive filter update to ameliorate the effects of contamination, leading to better overall network performance matching that of a centralized strategy at steady-state. Finally, the robust diffusion adaptation algorithm is extended further {{to solve the problem of}} distributed detection over adaptive networks where the measurements may be corrupted by impulsive noise. The estimates generated by the robust algorithm are used as basis for the design of robust local detectors, where the form of the test- statistics and the rule for the computation of the detection thresholds are motivated by the analysis of the algorithm dynamics. Each node in the network cooperates with its neighbors, utilizing their estimates, to update its local detector. Effectively, information pertaining to the event of interest percolates across the network, leading to enhanced detection performance. The transient and steady-state behavior of the developed algorithms are analyzed in the mean and mean-square sense using the energy conservation framework. The performance of the algorithm is also examined in the context of distributed detection. Performance is validated extensively through numerical simulations in an impulsive noise scenario, revealing the robustness of the proposed strategies in comparison with state-of-the-art algorithms as well as good agreement between theory and practice...|$|E
40|$|The {{well-known}} Routh Array settles {{the problem}} of stability of systems of differential equations with rral coefficients. The Extended Routh Array (ERA) is the complex counterpart of the Routh Array and it settles the stability of these systems when the coefficients are complex. Since its construction, the ERA remained more of a theoretical achievement, than a practical tool to test the stability of linear systems. Some {{attempts were made to}} overcome the complexity of the ERA. The Modified Extended Routh Array (MERA) was then introduced and it reduced the burden of computations, but still it involved lots of divisions and many operations with complex numbers. In the present work, we use the interlacing property to propose an equivalent criterion to both ERA and MERA, we call the Generalized Routh Array (GRA). The new array has advantages over both ERA and MERA in the sense that neither division algorithm, <b>nor</b> <b>operations</b> of complex numbers are involved. An example is given to illustrate the feasibility of the new test...|$|R
5000|$|The stroke {{is named}} after Henry M. Sheffer, who in 1913 {{published}} a paper in the Transactions of the American Mathematical Society (Sheffer 1913) providing an axiomatization of Boolean algebras using the stroke, and proved its equivalence to a standard formulation thereof by Huntington employing the familiar operators of propositional logic (and, or, not). Because of self-duality of Boolean algebras, Sheffer's axioms are equally valid for either of the NAND or <b>NOR</b> <b>operations</b> {{in place of the}} stroke. Sheffer interpreted the stroke as a sign for nondisjunction (NOR) in his paper, mentioning non-conjunction only in a footnote and without a special sign for it. It was Jean Nicod who first used the stroke as a sign for non-conjunction (NAND) in a paper of 1917 and which has since become current practice. Russell and Whitehead used the Sheffer stroke in the 1927 second edition of Principia Mathematica and suggested it as a replacement for the [...] "or" [...] and [...] "not" [...] operations of the first edition.|$|R
5000|$|... {{several of}} the more {{technical}} drawing modes are eliminated, including frontbuffer and accumulation buffer. Bitmap operations, specifically copying pixels (individually) is not allowed, nor are evaluators, <b>nor</b> (user) selection <b>operations,</b> ...|$|R
50|$|It {{may not be}} {{possible}} to remove all lesions, <b>nor</b> will the <b>operation</b> prevent new lesions from growing. Development of new fibroids will be seen in 42-55% of patients undergoing a myomectomy.|$|R
40|$|The text {{describes}} a technique used in Salento (Puglia), {{for the creation}} of openings in existing load-bearing walls, which does not require installation of either steel section <b>nor</b> shoring <b>operations,</b> reducing processing time, costs and guaranteeing observance of the principles of preservation. A procedure for finite element analysis was developed. The calculation model has confirmed the exceptional earthquake resistance of this type of intervention and also its limits of application...|$|R
5000|$|Chris Logan, {{author of}} a {{biography}} of Barnard, wrote that Naki [...] "did not at any stage assist in the first or subsequent human heart transplant <b>operations,</b> <b>nor</b> could he have done under the apartheid laws at the time".|$|R
2500|$|On 24 September 1998, as a {{precondition}} to {{the restoration of}} diplomatic relations with Britain, the Iranian government, then headed by moderate Muhammad Khatami, gave a public commitment that it would [...] "neither support <b>nor</b> hinder assassination <b>operations</b> on Rushdie".|$|R
40|$|International audienceCommutativity has {{the same}} {{inherent}} limitations as compatibility. Then, it is worth conceiving simple concurrency control techniques. We propose a restricted form of commutativity which increases parallelism without incurring a higher overhead than compatibility. Advantages of our proposition are: (1) commutativity of operations is determined at compile-time, (2) run-time checking is as efficient as for compatibility, (3) neither commutativity relations, (4) <b>nor</b> inverse <b>operations,</b> need to be specified, and (5) log space utilization is reduced...|$|R
40|$|A 39 -year-old {{man with}} aortic {{stenosis}} and regurgitation developed Q fever endocarditis. After 15 weeks of chemotherapy with tetracycline the damaged aortic valve {{was replaced with}} a homograft. Organisms {{were present in the}} excised valve. Some months later the valve began to leak and the endocarditis recurred fatally. Because of the nature of rickettsial infection neither a course of chemotherapy <b>nor</b> an <b>operation</b> can guarantee a cure of Q fever endocarditis. Chemotherapy should be continued indefinitely even after operation...|$|R
