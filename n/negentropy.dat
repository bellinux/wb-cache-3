216|10|Public
2500|$|Synergetics {{refers to}} synergy: either {{the concept of}} the output of a system not {{foreseen}} by the simple sum of the output of each system part, or simply [...] less used [...] another term for negative entropy [...] <b>negentropy.</b>|$|E
5000|$|The <b>negentropy</b> has {{different}} meanings in information theory and theoretical biology. In a biological context, the <b>negentropy</b> (also negative entropy, syntropy, extropy, ectropy or entaxy) {{of a living}} system is the entropy that it exports to keep its own entropy low; it lies {{at the intersection of}} entropy and life. In other words, <b>negentropy</b> is reverse entropy. It means things becoming more in order. By 'order' is meant organisation, structure and function: the opposite of randomness or chaos. The concept and phrase [...] "negative entropy" [...] was introduced by Erwin Schrödinger in his 1944 popular-science book What is Life? Later, Léon Brillouin shortened the phrase to <b>negentropy,</b> to express it in a more [...] "positive" [...] way: a living system imports <b>negentropy</b> and stores it. In 1974, Albert Szent-Györgyi proposed replacing the term <b>negentropy</b> with syntropy. That term may have originated in the 1940s with the Italian mathematician Luigi Fantappiè, who tried to construct a unified theory of biology and physics. Buckminster Fuller tried to popularize this usage, but <b>negentropy</b> remains common.|$|E
50|$|Another {{approach}} is using <b>negentropy</b> instead of kurtosis. <b>Negentropy</b> is a robust method for kurtosis, as kurtosis is {{very sensitive to}} outliers.The <b>negentropy</b> method are based on an important property of Gaussian distribution: a Gaussian variable has the largest entropy among all random variables of equal variance. This is also {{the reason why we}} want to find the most nongaussian variables. A simple proof can be found in Differential entropy.|$|E
30|$|The HOS-based ICA {{algorithm}} {{is actually a}} neural network learning rule transformed into an efficient fixed-point iteration. It {{is based on an}} assumption of the independence and non-Gaussianity of the sources. Due to using the CDMA technique, it is rationally presumed that each user and path is approximately independent of each other, so each received data sequence can be regarded as an independent source sequence. Afterward, specific features of the received mixed-up signals will be extracted by maximizing their approximate <b>negentropies.</b>|$|R
40|$|The {{performance}} of six neuromorphic adaptive structurally different algorithms was analyzed in blind separation of independent artificially generated signals using the stationary linear independent component analysis (ICA) model. The estimated independent components were assessed and compared aiming to rank the neural ICA implementations. All algorithms were run with different contrast functions, which were optimally selected {{on the basis}} of maximizing the sum of individual <b>negentropies</b> of the network outputs. Both subGaussian and superGaussian one-dimensional time series were employed throughout the numerical simulations. status: publishe...|$|R
40|$|Non-Gaussian multivariate {{probability}} distributions, {{derived from}} climate and geofluid statistics, allow for nonlinear correlations between linearly uncorrelated components, due to joint Shannon <b>negentropies.</b> Triadic statistical dependence under pair-wise (total or partial) independence is thus possible. Synergy or interaction information among triads is estimated. We formulate an optimization method of triads {{in the space}} of orthogonal rotations of normalized principal components, relying on the maximization of third-order cross-cumulants. Its application to a minimal one-dimensional, periodic, advective model leads to enhanced triads that occur between oscillating components of circular or locally confined wave trains satisfying the triadic wave resonance condition...|$|R
50|$|In {{information}} theory and statistics, <b>negentropy</b> {{is used as}} a measure of distance to normality. Out of all distributions with a given mean and variance, the normal or Gaussian distribution is the one with the highest entropy. <b>Negentropy</b> measures the difference in entropy between a given distribution and the Gaussian distribution with the same mean and variance. Thus, <b>negentropy</b> is always nonnegative, is invariant by any linear invertible change of coordinates, and vanishes if and only if the signal is Gaussian.|$|E
50|$|In 2009, Mahulikar & Herwig redefined <b>negentropy</b> of a {{dynamically}} ordered sub-system as {{the specific}} entropy deficit of the ordered sub-system relative to its surrounding chaos. Thus, <b>negentropy</b> has SI units of (J kg−1 K−1) when defined based on specific entropy per unit mass, and (K−1) when defined based on specific entropy per unit energy. This definition enabled: i) scale-invariant thermodynamic representation of dynamic order existence, ii) formulation of physical principles exclusively for dynamic order existence and evolution, and iii) mathematical interpretation of Schrödinger's <b>negentropy</b> debt.|$|E
5000|$|In 2009, Mahulikar & Herwig redefined {{thermodynamic}} <b>negentropy</b> as {{the specific}} entropy deficit of the dynamically ordered sub-system relative to its surroundings. [...] This definition enabled {{the formulation of}} the <b>Negentropy</b> Principle, which is mathematically shown to follow from the 2nd Law of Thermodynamics, during order existence.|$|E
40|$|In {{this paper}} the authors present the {{considerations}} about the shape as a concept. The notion of “shape” {{is treated as}} a shape, order, internal order and “real fact”, beginning from the Greek philosophical ideas until the nowadays interpretations. There are presented some-aspects of the shape as negative entropies (<b>negentropies)</b> and “the pieces of information” and some considerations {{in connection with the}} correlation SHAPE – INFORMATION, in her passive interpretation. Thus, while design uses relevant aspects of other disciplines, it also has certain identifiable characteristics, such as the process and functions of design, that are its own...|$|R
40|$|Abstract. The {{paper is}} an {{overview}} of the most frequently used neural network algorithms for implementing Independent Component Analysis (ICA). The performance of six structurally different algorithms was ranked in blind separation of independent artificially generated signals using the stationary linear ICA model. Ranking of the estimated components was also carried out and compared among different ICA approaches. All algorithms were run with different contrast functions, which were optimally {{selected on the basis of}} maximizing the sum of individual <b>negentropies</b> of the network outputs or minimizing their mutual information. Both subgaussian and supergaussian onedimensional time series were employed throughout the numerical simulations. status: publishe...|$|R
40|$|This article {{displays}} {{an application}} of the statistical method motivated by Bruno de Finetti's operational subjective theory of probability. We use exchangeable forecasting distributions based on mixtures of linear combinations of exponential power (EP) distributions to forecast the sequence of daily rates of return from the Dow-Jones index of stock prices over a 20 year period. The operational subjective statistical method for comparing distributions {{is quite different from}} that commonly used in data analysis, because it rejects the basic tenets underlying the practice of hypothesis testing. In its place, proper scoring rules for forecast distributions are used to assess the values of various forecasting strategies. Using a logarithmic scoring rule, we find that a mixture linear combination of EP distributions scores markedly better than does a simple mixture over the EP family, which scores much better than does a simple Normal mixture. Surprisingly, a mixture over a linear combination of three Normal distributions also makes a substantial improvement over a simple Normal mixture, although it does not quite match the performance of even the simple EP mixture. All substantive forecasting improvements become most marked after extreme tail phenomena were actually observed in the sequence, in particular after the abrupt drop in market prices in October, 1987. However, the improvements continue to be apparent over the long haul of 1985 - 2006 which has seen a number of extreme price changes. This result is supported by an analysis of the <b>Negentropies</b> embedded in the forecasting distributions, and a proper scoring analysis of these <b>Negentropies</b> as well...|$|R
5000|$|... #Subtitle level 2: Brillouin's <b>negentropy</b> {{principle}} of information ...|$|E
50|$|<b>Negentropy</b> - a {{shorthand}} colloquial {{phrase for}} negative entropy.|$|E
5000|$|... #Subtitle level 2: Correlation between {{statistical}} <b>negentropy</b> and Gibbs' {{free energy}} ...|$|E
40|$|We {{present an}} {{extensive}} numerical {{study of the}} relation between the cosmic peculiar velocity field and the gravitational acceleration field. We show that on mildly non-linear scales (3 - 10 Mpc Gaussian smoothing), the distribution of the Cartesian coordinates of each of these fields is very well approximated by a Gaussian. In particular, their <b>negentropies</b> and kurtoses are small compared to those of the velocity divergence and density fields. We find that at these scales the relation between the velocity and acceleration fields follows linear theory to high accuracy. The non-linear correction of Kudlicki et al (2000 a,b) works still better: its reconstruction errors are several times smaller than those of the linear theory approximation...|$|R
40|$|Abstract: This article {{displays}} {{an application}} of the statistical method moti-vated by Bruno de Finetti’s operational subjective theory of probability. We use exchangeable forecasting distributions based on mixtures of linear com-binations of exponential power (EP) distributions to forecast the sequence of daily rates of return from the Dow-Jones index of stock prices over a 20 year period. The operational subjective statistical method for comparing distributions {{is quite different from}} that commonly used in data analysis, because it rejects the basic tenets underlying the practice of hypothesis test-ing. In its place, proper scoring rules for forecast distributions are used to assess the values of various forecasting strategies. Using a logarithmic scoring rule, we find that a mixture linear combination of EP distributions scores markedly better than does a simple mixture over the EP family, which scores much better than does a simple Normal mixture. Surprisingly, a mix-ture over a linear combination of three Normal distributions also makes a substantial improvement over a simple Normal mixture, although it does not quite match the performance of even the simple EP mixture. All sub-stantive forecasting improvements become most marked after extreme tail phenomena were actually observed in the sequence, in particular after the abrupt drop in market prices in October, 1987. However, the improvements continue to be apparent over the long haul of 1985 - 2006 which has seen a number of extreme price changes. This result is supported by an analysis of the <b>Negentropies</b> embedded in the forecasting distributions, and a proper scoring analysis of these <b>Negentropies</b> as well. Key words: Dow-Jones index, exponential power distributions, fat tails, log-arithmic scoring rule, mixture distributions, partial exchangeability, proper scoring rules, subjective probability, subjectivist statistical methods. 1...|$|R
40|$|We {{present a}} {{numerical}} {{study of the}} relation between the cosmic peculiar velocity field and the gravitational acceleration field. We show that on mildly non-linear scales (4 - 10 Mpc Gaussian smoothing), the distribution of the Cartesian coordinates of each of these fields is well approximated by a Gaussian. In particular, their kurtoses and <b>negentropies</b> are small compared to those of the velocity divergence and density fields. We find that at these scales the relation between the velocity and gravity field follows linear theory to good accuracy. Specifically, the systematic errors in velocity-velocity comparisons due to assuming the linear model do not exceed 6 % in beta. To correct for them, we test various nonlinear estimators of velocity from density. We show that a slight modification of the alpha-formula proposed by Kudlicki et al. yields an estimator which is essentially unbiased and has a small variance. Comment: 11 pages, 15 figures; matches the version accepted for publication in MNRA...|$|R
50|$|Physics - <b>negentropy,</b> {{stochastic}} processes, and {{the development}} of new physical techniques and instrumentation as well as their application.|$|E
50|$|In risk management, <b>negentropy</b> is {{the force}} that seeks to achieve {{effective}} organizational behavior and lead to a steady predictable state.|$|E
50|$|Indeed, <b>negentropy</b> {{has been}} used by {{biologists}} as the basis for purpose or direction in life, namely cooperative or moral instincts.|$|E
40|$|Abstract. We {{present a}} robust {{algorithm}} for independent component analysis {{that uses the}} sum of marginal quadratic <b>negentropies</b> as a depen-dence measure. It can handle arbitrary source density functions by using kernel density estimation, but is robust for {{a small number of}} samples by avoiding empirical expectation and directly calculating the integration of quadratic densities. In addition, our algorithm is scalable because the gradient of our contrast function can be calculated in O(LN) using the fast Gauss transform, where L is the number of sources and N is the number of samples. In our experiments, we evaluated the performance of our algorithm for various source distributions and compared it with other, well-known algorithms. The results show that the proposed al-gorithm consistently outperforms the others. Moreover, it is extremely robust to outliers and is particularly more effective when the number of observed samples is small and the number of mixed sources is large. ...|$|R
50|$|<b>Negentropy</b> {{is used in}} {{statistics}} and signal processing. It is related to network entropy, which is used in Independent Component Analysis.|$|E
50|$|The Minimization-of-Mutual {{information}} (MMI) {{family of}} ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy. The non-Gaussianity family of ICA algorithms, {{motivated by the}} central limit theorem, uses kurtosis and <b>negentropy.</b>|$|E
50|$|Brillouin was {{a founder}} of modern solid state physics for which he discovered, among other things, Brillouin zones. He applied {{information}} theory to physics and the design of computers and coined the concept of <b>negentropy</b> to demonstrate the similarity between entropy and information.|$|E
5000|$|Synergetics {{refers to}} synergy: either {{the concept of}} the output of a system not {{foreseen}} by the simple sum of the output of each system part, or simply [...] - [...] less used [...] - [...] another term for negative entropy [...] - [...] <b>negentropy.</b>|$|E
50|$|The same {{description}} of CTC physics was derived independently in 2001 by Michael Devin, {{and applied to}} thermodynamics. The same model {{with the introduction of}} a noise term allowing for inexact periodicity, allows the grandfather paradox to be resolved, and clarifies the computational power of a time machine assisted computer. Each time traveling qubit has an associated <b>negentropy,</b> given approximately by the logarithm of the noise of the communication channel. Each use of the time machine can be used to extract as much work from a thermal bath. In a brute force search for a randomly generated password, the entropy of the unknown string can be effectively reduced by a similar amount. Because the <b>negentropy</b> and computational power diverge as the noise term goes to zero, complexity class {{may not be the best}} way to describe the capabilities of time machines.|$|E
5000|$|Harold Blum's 1951 book Time's Arrow and Evolution [...] "explored the {{relationship}} between time's arrow (the second law of thermodynamics) and organic evolution." [...] This influential text explores [...] "irreversibility and direction in evolution and order, <b>negentropy,</b> and evolution." [...] Blum argues that evolution followed specific patterns predetermined by the inorganic nature {{of the earth and}} its thermodynamic processes.|$|E
5000|$|Out of Control: The New Biology of Machines, Social Systems, and the Economic World (...) is a 1994 book by Kevin Kelly. (The {{book was}} also {{published}} as Out of control: {{the rise of}} neo-biological civilization.) Major themes in Out of Control are cybernetics, emergence, self-organization, complex systems, <b>negentropy</b> and chaos theory {{and it can be}} seen as a work of techno-utopianism.|$|E
50|$|In 1966-67 M.S. Neiman {{generalized}} <b>negentropy</b> {{principle of}} information to digital data processing systems, and defined the relation restrictions between their speed and energy level of the functioning of their elements. He {{made a number of}} fundamental assumptions of the theory of extracting information from the objective processes, identified the cause of her contradictions and paradoxes. Also he worked on the history and prospects of development of electronics.|$|E
50|$|There is a {{physical}} quantity closely linked to free energy (free enthalpy), with a unit of entropy and isomorphic to <b>negentropy</b> known in statistics and information theory. In 1873, Willard Gibbs created a diagram illustrating the concept of free energy corresponding to free enthalpy. On the diagram {{one can see the}} quantity called capacity for entropy. This quantity is the amount of entropy that may be increased without changing an internal energy or increasing its volume. In other words, it is a difference between maximum possible, under assumed conditions, entropy and its actual entropy. It corresponds exactly to the definition of <b>negentropy</b> adopted in statistics and information theory. A similar physical quantity was introduced in 1869 by Massieu for the isothermal process (both quantities differs just with a figure sign) and then Planck for the isothermal-isobaric process. More recently, the Massieu-Planck thermodynamic potential, known also as free entropy, has been shown to play a great role in the so-called entropic formulation of statistical mechanics, applied among the others in molecular biology and thermodynamic non-equilibrium processes.|$|E
5000|$|Finally, the {{application}} of entropy and the Second Law to living organisms is totally unwarranted. The most famous statement aboutentropy and life was made by Erwin Schrödinger, in his book What is Life?. In this book, Schrödinger not only discusses entropy and life and associates entropy with disorder, he also [...] "invents" [...] the concept of [...] "negative entropy," [...] which was later renamed <b>negentropy</b> by Léon Brillouin. This erroneous application is further discussed in Ben-Naim's books.|$|E
50|$|In 1944, {{he wrote}} What Is Life?, which {{contains}} {{a discussion of}} <b>negentropy</b> {{and the concept of}} a complex molecule with the genetic code for living organisms. According to James D. Watson's memoir, DNA, the Secret of Life, Schrödinger's book gave Watson the inspiration to research the gene, which led to the discovery of the DNA double helix structure in 1953. Similarly, Francis Crick, in his autobiographical book What Mad Pursuit, described how he was influenced by Schrödinger's speculations about how genetic information might be stored in molecules.|$|E
5000|$|In 1951 Blum {{published}} Time's Arrow and Evolution, which [...] "explores {{the relationship}} between time's arrow (the second law of thermodynamics) and organic evolution." [...] This influential text studies [...] "irreversibility and direction in evolution and order, <b>negentropy,</b> and evolution." [...] Blum argues that evolution followed specific patterns predetermined by the inorganic nature {{of the earth and}} its thermodynamic processes. Scholar Robert Scholes, writing about influences on science fiction, calls the work a [...] "milestone in science writing" [...] that is [...] "one of the finest pieces of science writing ever done." ...|$|E
5000|$|LST's {{analysis}} of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter-energy-processing and information-processing, as well as LST's {{analysis of}} the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or [...] "organizational pathologies" [...] of systems functioning (e.g., system stress and strain, feedback irregularities, information-input overload). It explicates the role of entropy in social research while it equates <b>negentropy</b> with information and order. It emphasizes both structure and process, {{as well as their}} interrelations.|$|E
50|$|Shannon entropy {{has been}} related by {{physicist}} Léon Brillouin to a concept sometimes called <b>negentropy.</b> In 1953, Brillouin derived a general equation {{stating that the}} changing of an information bit value requires at least kT ln(2) energy. This is the same energy as the work Leo Szilard's engine produces in the idealistic case, which in turn equals to the same quantity found by Landauer. In his book, he further explored this problem concluding that any cause of a bit value change (measurement, decision about a yes/no question, erasure, display, etc.) will require the same amount, kT ln(2), of energy. Consequently, acquiring information about a system’s microstates is associated with an entropy production, while erasure yields entropy production only when the bit value is changing. Setting up a bit of information in a sub-system originally in thermal equilibrium results in a local entropy reduction. However, there is no violation of the second law of thermodynamics, according to Brillouin, since a reduction in any local system’s thermodynamic entropy results {{in an increase in}} thermodynamic entropy elsewhere. In this way, Brillouin clarified the meaning of <b>negentropy</b> which was considered as controversial because its earlier understanding can yield Carnot efficiency higher than one. Additionally, the relationship between energy and information formulated by Brillouin has been proposed as a connection between the amount of bits that the brain processes and the energy it consumes.|$|E
