537|10000|Public
2500|$|Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems 7: Proceedings of the 1994 Conference (editor) ...|$|E
5000|$|... #Article: Conference on <b>Neural</b> <b>Information</b> <b>Processing</b> Systems ...|$|E
5000|$|<b>Neural</b> <b>Information</b> <b>Processing</b> System (NIPS) {{outstanding}} paper award 2008 (supervised) ...|$|E
40|$|The {{purpose of}} this chapter is two-fold: (1) {{to make the case}} that a {{standard}} backward propagation artificial neural network (ANN) can be used as a general model of the <b>information</b> <b>processing</b> activities of the firm, and (2) to present a synthesis of Barr and Saraceno (BS) (2002, 2004, 2005), who offer various models of the firm as an artificial neural network. <b>neural</b> networks, <b>information</b> <b>processing,</b> firm learning, agent-based...|$|R
40|$|Some of {{the main}} results in the {{mathematical}} evaluation of <b>neural</b> networks as <b>information</b> <b>processing</b> systems are discussed. The basic operation of feedback and feed-forward neural networks is described. Their memory capacity and computing power are considered. The concept of learning by example {{as it applies to}} neural networks is examined...|$|R
50|$|Allison Jane Doupe (1954-24 October 2014) was an {{influential}} Canadian psychiatrist, biologist, and neuroscientist. She {{is best known}} for her pioneering work in avian neurobiology that linked birdsong to human language, showing that birds and humans learn to communicate in similar ways. In 2014, Doupe was awarded the Pradel Research Award by the National Academy of Sciences for her work on <b>neural</b> circuits and <b>information</b> <b>processing</b> in song birds.|$|R
5000|$|Posner keynote {{lecturer}} at the <b>Neural</b> <b>Information</b> <b>Processing</b> Systems conference ...|$|E
5000|$|Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems 7: Proceedings of the 1994 Conference (editor) ...|$|E
5000|$|P. Häfliger, M. Mahowald and L. Watts: [...] "A spike based {{learning}} neuron in analog VLSI", Advances in <b>neural</b> <b>information</b> <b>processing</b> systems, 9: 692-698, 1996 ...|$|E
40|$|System {{and method}} for {{performing}} {{one or more}} relevant measurements at a target site in an animal body, using a probe. One or more {{of a group of}} selected internal measurements is performed at the target site, is optionally combined with one or more selected external measurements, and is optionally combined with one or more selected heuristic information items, in order to reduce to a relatively small number the probable medical conditions associated with the target site. One or more of the internal measurements is optionally used to navigate the probe to the target site. <b>Neural</b> net <b>information</b> <b>processing</b> is performed to provide a reduced set of probable medical conditions associated with the target site...|$|R
40|$|Whether hESC-derived neurons {{can fully}} {{integrate}} with and functionally regulate an existing neural network remains unknown. Here, we demonstrate that hESC-derived neurons receive unitary postsynaptic currents both in vitro and in vivo and adopt the rhythmic firing behavior of mouse cortical networks via synaptic integration. Optical stimulation of hESC-derived neurons expressing Channelrhodopsin- 2 elicited both inhibitory and excitatory postsynaptic currents and triggered network bursting in mouse neurons. Furthermore, light stimulation of hESC-derived neurons transplanted to the hippocampus of adult mice triggered postsynaptic currents in host pyramidal neurons in acute slice preparations. Thus, hESC-derived neurons {{can participate in}} and modulate neural network activity through functional synaptic integration, suggesting {{they are capable of}} contributing to <b>neural</b> network <b>information</b> <b>processing</b> both in vitro and in vivo...|$|R
40|$|Although certain neurophysiological {{functions}} of the amygdala complex in learning seem well established, {{the purpose of this}} review is to propose that an additional conceptualization of amygdala function is now needed. The research we review provides evidence that a subsystem within the amygdala provides a coordinated regulation of attentional processes. An important aspect of this additional neuropsychology of the amygdala is that it may aid in understanding the importance of connections between the amygdala and other <b>neural</b> systems in <b>information</b> <b>processing...</b>|$|R
5000|$|... 2007 Map-reduce for machine {{learning}} on multicore, with Cheng Chu, Sang Kyun Kim, Yi-An Lin, YuanYuan Yu, Andrew Y Ng, Kunle Olukotun. Advances in <b>neural</b> <b>information</b> <b>processing</b> systems ...|$|E
50|$|On December 8, 2009, at the <b>Neural</b> <b>Information</b> <b>Processing</b> Systems (NIPS) conference, a Google {{research}} {{team led by}} Hartmut Neven used D-Wave's processor to train a binary image classifier.|$|E
5000|$|Stephen M. Omohundro, [...] "Family Discovery", in Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems 8, eds. D. S. Touretzky, M. C. Mozer and M. E. Hasselmo, MIT Press, Cambridge, MA, 1996.|$|E
40|$|Abstract:- In {{terms of}} <b>information</b> <b>processing</b> model, {{learning}} represents {{the process of}} gathering information, and organizing it into mental schemata. Information-processing theory has definite educational implications {{for students with learning}} and behavior problems. Teachers with a greater understanding of the theory and how it is formed to, select learning strategies in order to improve the retention and retrieval of learning. But it must also be taken into consideration that the learning environment has specific effects on academic achievement. Socialization alters the levels of stress, confidence, and even the content knowledge. Social support provides encouragement, stress reduction, feedback, and communication factors which enable learning. Furthermore, an evaluation of 4 learning strategies attempted via a well-formed LVQ <b>Neural</b> Network. Key-Words:- <b>information</b> <b>processing,</b> memory, matacognition, learning strategies, general education classroom, learning, behavioral problems and neural networks...|$|R
5000|$|Evangelia Micheli-Tzanakou (March 22, 1942 - September 24, 2012) was a {{professor}} of biomedical engineering and the Director of Computational Intelligence Laboratories at Rutgers University. Dr. Micheli-Tzanakou was also a Founding Fellow of the American Institute for Medical and Biological Engineering (AIMBE), a Fellow of the Institute of Electrical and Electronics Engineers (IEEE), and a Fellow of the New Jersey Academy of Medicine. Dr. Micheli-Tzanakou's areas of interest included <b>neural</b> networks, <b>information</b> <b>processing</b> in the brain, image and signal processing applied to biomedicine, telemedicine, mammography, hearing aids and electronic equivalents of neurons. [...] Dr. Micheli-Tzanakou received international attention in 1974 when she established the first Brain to Computer Interface (BCI) using her algorithm ALOPEX. This method {{was used in the}} study of Parkinson's disease. The ALOPEX algorithm has also been applied toward signal processing, image processing, and pattern recognition. Dr. Micheli-Tzanakou died on September 24, 2012, after a long fight with cancer.|$|R
40|$|Attention endows animals {{an ability}} to {{concentrate}} on the most relevant information among a deluge of distractors at any given time, either through volitionally 'top-down' biasing, or driven by automatically 'bottom-up' saliency of stimuli, in favour of advantageous competition in <b>neural</b> modulations for <b>information</b> <b>processing.</b> Nevertheless, instead of being limited to perceive simple features, human and other advanced animals adaptively learn the world into categories and abstract concepts from experiences, imparting the world meanings. This thesis suggests that the high-level cognitive ability of human is more likely driven by attention basing on abstract perceptions, which is defined as concept based attention (CbA). Comment: 7 pages, 2 figure...|$|R
50|$|The {{proceedings}} {{from the}} conferences {{have been published}} in book form by Morgan Kaufmann (1987-1993), MIT Press (1994-2004) and Curran Associates (2005-2013) under the name Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems.|$|E
5000|$|S Makeig, Bell AJ, Jung T-P, Sejnowski TJ. Independent {{component}} analysis of electroencephalographic data In: D. Touretzky, M. Mozer and M. Hasselmo (Eds). Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems 8:145-151 (1996) ...|$|E
50|$|In 1987, Abu-Mostafa cofounded the Conference on <b>Neural</b> <b>Information</b> <b>Processing</b> Systems (NIPS), a major machine {{learning}} meeting.He {{is known for}} his recent textbook on {{machine learning}}.He has also developed an online course about machine learning.|$|E
40|$|Computational {{neuroscience}} {{has contributed}} significantly {{to our understanding of}} higher brain function by combining experimental neurobiology, psychophysics, modeling, and mathematical analysis. This article reviews recent advances in a key area: <b>neural</b> coding and <b>information</b> <b>processing.</b> It is shown that synapses are capable of supporting computations based on highly structured temporal codes. Such codes could provide a substrate for unambiguous representations of complex stimuli and be used to solve difficult cognitive tasks, such as the binding problem. Unsupervised learning rules could generate the circuitry required for precise temporal codes. Together, these results indicate that neural systems perform a rich repertoire of computations based on action potential timing...|$|R
30|$|The basic <b>information</b> <b>processing</b> {{units of}} the neuro-symbolic network are {{so-called}} neuro-symbols. To use neuro-symbols as elementary <b>information</b> <b>processing</b> units came from the following observation: in the brain, information is processed by neurons. However, humans do not {{think in terms of}} firing nerve cells but in terms of symbols. In perception, these symbols are perceptual images like a face, a person, a melody, a voice, and so forth. <b>Neural</b> and symbolic <b>information</b> <b>processing</b> can be seen as <b>information</b> <b>processing</b> in the brain on two different levels of abstraction. Nevertheless, there seems to exist a correlation between these two levels. Actually, there have been found neurons in the brain which react for instance exclusively if a face is perceived in the environment [30 – 32]. This fact can be seen as evidence for such a correlation and was the motivation for using neuro-symbols as basic <b>information</b> <b>processing</b> units. Neuro-symbols show certain characteristics of neurons and others of symbols. Analyses of structures in the human mind have shown that certain characteristics and mechanisms are repeated on different levels, for example, afference and efference. This repetition of characteristics is a key element to the concept of neuro-symbolic processing.|$|R
40|$|International audienceArtificial Neural Networks {{are very}} {{efficient}} adaptive models {{but one of}} their recognized weaknesses is about information representation, often carried out in an input vector without a structure. Beyond the classical elaboration of a hierarchical representation {{in a series of}} layers, we report here inspiration from neuroscience and argue for the design of heterogenous <b>neural</b> networks, <b>processing</b> <b>information</b> at feature, configuration and history levels of granularity, and interacting very efficiently for high-level and complex decision making. This framework is built from known characteristics of the sensory cortex, the hippocampus and the prefrontal cortex and is exemplified here in the case of pavlovian conditioning, but we propose that it can be advantageously applied in a wider extent, to design flexible and versatile <b>information</b> <b>processing</b> with neuronal computation...|$|R
50|$|Journal of Neuroscience,Conference on Computer Vision and Pattern Recognition,International Conference on Computer Vision,Conference on <b>Neural</b> <b>Information</b> <b>Processing</b> Systems,European Conference on Computer Vision,International Journal of Computer Vision, and IEEE Transactions on Pattern Analysis and Machine Intelligence.|$|E
5000|$|Moody, J.E. (1992), [...] "The Effective Number of Parameters: An Analysis of Generalization and Regularization in Nonlinear Learning Systems", in Moody, J.E., Hanson, S.J., and Lippmann, R.P., Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems 4, 847-854.|$|E
50|$|It is {{the current}} {{hypothesis}} that EEG Microstates represent the basic steps of cognition and <b>neural</b> <b>information</b> <b>processing</b> in the brain, {{but there is still}} much research {{that needs to be done}} to cement this theory.|$|E
40|$|In this {{dissertation}} I {{will provide}} evidence for a novel principle of neural computation. This principle is widely known {{in the field of}} electrical engineering as maximal-ratio combining. Given two noisy copies of a signal, maximal-ratio combining constructs a single signal with the optimally achievable signal-to-noise ratio (SNR). I will show that this principle holds for a neural circuit in the mammalian retina. Furthermore, I will provide evidence, based on data from other investigators, that this principle holds for other neural circuits, including circuits in the mammalian brain. Maximal-ratio combining improves the reliability of <b>neural</b> signals. Optimal <b>information</b> <b>processing</b> in <b>neural</b> circuits has been postulated as a guiding principle for neural processing, but direct experimental evidence is extremely rare. ...|$|R
30|$|BP <b>neural</b> network’s <b>information</b> <b>processing</b> {{capacity}} {{is determined by}} the input and output neurons’ characteristics, the network structure, and the connection weights. The complexity of the neural network structure affects the performance of the neural network. In [6], a hybrid STLF method is proposed based on the improved ensemble empirical mode decomposition (IEEMD) and BP neural network. The remaining IMFs and residual are forecast by BPNN, and then, the forecasting results of each component are combined with BPNN to obtain the final predicted load series. In order to devise bus lines and make daily scheduling more precisely [7], back propagation (BP) neural network to predict bus traffic is used. As the time factor and meteorological factor are the two important factors which affect traffic, they did the data preprocessing for bus data in Guangzhou from August to December in 2014 and conducted several experiments to determine the network structure and parameters which are suitable for prediction of bus traffic. Through tenfold cross-validation experiments, the model is demonstrated being able to apply to bus traffic forecast.|$|R
40|$|The {{human brain}} has {{accumulated}} many useful building blocks over its evolutionary history, {{and the best}} knowledge of these has often derived from experiments performed in animal species that display fi nely honed abilities. In this article we review a model system {{at the forefront of}} investi-gation into the <b>neural</b> bases of <b>information</b> <b>processing,</b> plas-ticity, and learning: the barn owl auditory localization pathway. In addition to the broadly applicable principles gleaned from three decades of work in this system, there are good reasons to believe that continued exploration of the owl brain will be invaluable for further advances in understanding of how neu-ronal networks give rise to behavior. Key Words: adaptive plasticity; auditory space map; barn owl (Tyto alba); orienting behavior; inferior colliculus; neu-roethology; sound localizatio...|$|R
5000|$|Christoph Bregler and Stephen M. Omohundro, [...] "Surface Learning with Applications to Lipreading", in Cowan, J. D., Tesauro, G., and Alspector, J., (eds.) Advances in <b>Neural</b> <b>Information</b> <b>Processing</b> Systems 6, Morgan Kaufmann Publishers, San Francisco, CA, 1994 ...|$|E
5000|$|Edward Charles [...] "Ed" [...] Posner (August 10, 1933 - June 15, 1993) was an American {{information}} theorist and {{neural network}} researcher who became chief technologist at the Jet Propulsion Laboratory and founded the Conference on <b>Neural</b> <b>Information</b> <b>Processing</b> Systems.|$|E
50|$|In {{response}} to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007 and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on <b>Neural</b> <b>Information</b> <b>Processing</b> Systems.|$|E
40|$|Forecasting {{has long}} been the domain of {{traditional}} statistical models. Recent research has shown that novel and complex forecasting models do not necessarily outperform simpler models. These include in particular Artificial Neural Networks (ANNs). Even though claims of superior forecasting performance were made by Neural Network researchers, these claims were often unsubstantiated. Artificial <b>neural</b> networks are <b>information</b> <b>processing</b> paradigms motivated by the <b>information</b> <b>processing</b> functions of the human brain. ANNs are widely recognized as universal function approximators and are capable of exploiting nonlinear relationships between variables. Given these strengths, we believed it was possible to design a neural network that would provide excellent forecasting ability over a wide variety of data. Inspired by recent research into deep learning nets, we were able to model a new Hybrid ANN model and compared its performance to other forecasting models used in the M 3 Time Series Competition. The results show that on average the Hybrid model outperforms the other methods investigated an...|$|R
40|$|There is {{a growing}} {{interest}} in using dynamic neural fields for modeling biological and technical systems, but constructive ways to set up such models are still missing. We discuss gradient-based, evolutionary and hybrid algorithms for data-driven adaptation of neural field parameters. The proposed methods are evaluated using artificial and neurophysiological data. Key words: dynamic neural fields; gradient-based optimization; evolutionary optimization; population representation; primary visual cortex of cat To be published in: Neurocomputing, 2000 1 Introduction Neural fields (NFs) have been introduced as mathematical descriptions of cortical <b>neural</b> tissues, where <b>information</b> <b>processing</b> {{takes place in the}} form of excitation patterns [22, 1, 12, 7]. They permit a systematic treatment of dynamical processes not only in distributed neural representations, but also {{in the context of the}} dynamical system approach to perception and behavior, behavior based robotics, computer vision and [...] ...|$|R
40|$|The {{field of}} Artificial Neural Networks has {{received}} {{in the last years}} {{a great deal of attention}} from the academic community and the industry. Neural Network applications are now found in a wide range of areas, including Control Systems, Medicine, Communication, Cognitive Sciences, Linguistic, Robotics, Physics and Economics. This report focuses on one of the key challenges of the field of Artificial Neural Networks, the creation of good general-purpose learning algorithms. As opposed to conventional programmed computing, where a series of explicit commands (a program) processes information on an associated database, Artificial <b>Neural</b> Networks develop <b>information</b> <b>processing</b> capabilities by learning from examples. Learning techniques can be roughly divided into two categories: Supervised Learning, and SelfOrganization or Unsupervised Learning. Supervised learning requires a set of examples for which the desired network response is known. The learning process consists then of adapting th [...] ...|$|R
