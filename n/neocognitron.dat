89|4|Public
25|$|<b>Neocognitron,</b> a {{hierarchical}} multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, {{is one of}} the first Deep Learning Neural Networks models.|$|E
25|$|Deep {{learning}} often uses convolutional {{neural networks}} (CNNs), whose origins {{can be traced}} back to the <b>Neocognitron</b> introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.|$|E
2500|$|Deep {{and highly}} {{nonlinear}} neural architectures {{similar to the}} <b>neocognitron</b> and the [...] "standard architecture of vision", inspired by simple and complex cells, were pre-trained using unsupervised methods, by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.|$|E
25|$|Invariant {{representations}} {{has been}} incorporated into several learning architectures, such as <b>neocognitrons.</b> Most of these architectures, however, provided invariance through custom-designed features or properties of architecture itself. While it helps {{to take into account}} some sorts of transformations, such as translations, it is very nontrivial to accommodate for other sorts of transformations, such as 3D rotations and changing facial expressions. M-Theory provides a framework of how such transformations can be learned. In addition to higher flexibility, this theory also suggests how human brain may have similar capabilities.|$|R
40|$|Abstract. In {{this paper}} we propose and {{investigate}} a novel nonlinear unit, called Lp unit, for deep neural networks. The proposed Lp unit receives signals from several projections of a subset of units in the layer below and computes a normalized Lp norm. We notice two interesting interpretations of the Lp unit. First, the proposed unit {{can be understood as}} a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and <b>neocognitrons.</b> Furthermore, the Lp unit is, to a certain degree, similar to the recently proposed maxout unit [13] which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the Lp unit is more efficient at representing complex, nonlinear separating boundaries. Each Lp unit defines a superelliptic boundary, with its exact shape defined by the order p. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few Lp units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed Lp units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the Lp units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed Lp unit on the recently proposed deep recurrent neural networks (RNN) ...|$|R
2500|$|Gary Marcus, a {{research}} psychologist {{and professor at}} New York University, [...] says only the name PRTM is new. He says the basic theory behind PRTM is [...] "in the spirit of" [...] a model of vision known as the <b>neocognitron,</b> introduced in 1980. He also says PRTM even more strongly resembles Hierarchical Temporal Memory promoted by Jeff Hawkins in recent years. [...] Marcus feels any theory like {{this needs to be}} proven with an actual working computer model. And to that end he says that [...] "a whole slew" [...] of machines have been programmed with an approach similar to PRTM, and they have often performed poorly.|$|E
50|$|There {{are various}} kinds of <b>neocognitron.</b> For example, some types of <b>neocognitron</b> can detect {{multiple}} patterns in the same input by using backward signals to achieve selective attention.|$|E
5000|$|The <b>neocognitron</b> [...] was {{introduced}} in 1980. The <b>neocognitron</b> does not require units located at multiple network positions {{to have the same}} trainable weights. This idea appears in 1986 in the book version of the original backpropagation paper (Figure 14). Neocognitrons were developed in 1988 for temporal signals. Their design was improved in 1998, generalized in 2003 and simplified in the same year.|$|E
50|$|<b>Neocognitron,</b> a {{hierarchical}} multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, {{is one of}} the first Deep Learning Neural Networks models.|$|E
50|$|The <b>neocognitron</b> is {{a natural}} {{extension}} of these cascading models. The <b>neocognitron</b> consists of multiple types of cells, the most important of which are called S-cells and C-cells. The local features are extracted by S-cells, and these features' deformation, such as local shifts, are tolerated by C-cells. Local features in the input are integrated gradually and classified in the higher layers. The idea of local feature integration is found in several other models, such as the LeNet model and the SIFT model.|$|E
50|$|The <b>neocognitron</b> is a hierarchical, {{multilayered}} {{network that}} was {{modeled after the}} visual cortex. It uses multiple types of units, (originally two, called simple and complex cells), as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of <b>neocognitron</b> are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It {{has been used for}} pattern recognition tasks and inspired convolutional neural networks.|$|E
50|$|The <b>neocognitron</b> is a hierarchical, {{multilayered}} artificial {{neural network}} proposed by Kunihiko Fukushima in the 1980s. It {{has been used for}} handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.|$|E
50|$|The <b>neocognitron</b> was {{inspired}} by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called simple cell and complex cell, and also proposed a cascading model of these two types of cells for use in pattern recognition tasks.|$|E
5000|$|Deep, highly {{nonlinear}} neural architectures {{similar to}} the <b>neocognitron</b> and the [...] "standard architecture of vision", inspired by simple and complex cells were pre-trained by unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.|$|E
50|$|By 1991 {{such systems}} {{were used for}} {{recognizing}} isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition directly from cluttered scenes. Cresceptron is a cascade of layers similar to <b>Neocognitron.</b> But while <b>Neocognitron</b> required a human programmer to hand-merge features, Cresceptron automatically learned an open number of unsupervised features in each layer, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution {{by a factor of}} (2x2) to 1 through the cascade for better generalization.|$|E
50|$|Other deep {{learning}} working architectures, specifically those built for computer vision, {{began with the}} <b>Neocognitron</b> introduced by Fukushima in 1980. In 1989, LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network {{with the purpose of}} recognizing handwritten ZIP codes on mail. While the algorithm worked the training time was an impractical 3 days.|$|E
50|$|Deep {{learning}} often uses convolutional {{neural networks}} (CNNs), whose origins {{can be traced}} back to the <b>Neocognitron</b> introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.Since 2011, fast implementations of CNNs on GPUs havewon many visual pattern recognition competitions.|$|E
5000|$|Gary Marcus, a {{research}} psychologist {{and professor at}} New York University, says only the name PRTM is new. He says the basic theory behind PRTM is [...] "in the spirit of" [...] a model of vision known as the <b>neocognitron,</b> introduced in 1980. He also says PRTM even more strongly resembles Hierarchical Temporal Memory promoted by Jeff Hawkins in recent years. Marcus feels any theory like {{this needs to be}} proven with an actual working computer model. And to that end he says that [...] "a whole slew" [...] of machines have been programmed with an approach similar to PRTM, and they have often performed poorly.|$|E
40|$|The <b>neocognitron</b> is a {{neural network}} for pattern {{recognition}} and feature extraction. An analog CCD parallel processing architecture developed at Lincoln Laboratory is particularly {{well suited to}} the computational re-quirements of shared-weight networks such as the <b>neocognitron,</b> and imple-mentation of the <b>neocognitron</b> using the CCD architecture was simulated. A modification to the <b>neocognitron</b> training procedure, which improves network performance under the limited arithmetic precision that would be imposed by the CCD architecture, is presented. ...|$|E
40|$|A rotation-invariant <b>neocognitron,</b> {{which has}} been {{recently}} proposed by authors, is trained by using hand-written numerical patterns provided by ETL- 1 database. A new learning algorithm, "auto-generating algorithm", is proposed and the learning time is reduced. The model can recognize realistic hand-written patterns. It is also shown that the model is completely robust for rotations of hand-written patterns. KEYWORDS: <b>neocognitron,</b> ETL- 1, rotated pattern 1. INTRODUCTION A <b>neocognitron</b> [1], which is a hierarchical neural networks for pattern or signal recognition, is considerably robust for distortion, scaling, and/or translation of patterns {{but it can not}} recognize patterns which are rotated in large angles. A rotation-invariant <b>neocognitron</b> was proposed by authors in order to make up the weak point of the original <b>neocognitron</b> [2],[3]. In the new model rotational information of inputs are added besides positional information in the original <b>neocognitron.</b> The rotation-invariant neoc [...] ...|$|E
40|$|When a {{neural network}} {{solution}} to a problem (e. g. handwritten character recognition) is proposed, {{it is important to know}} if the structure of the network and the function of the component neurons are well suited to the task. Recent research has examined the structure of Fukushima's <b>neocognitron</b> and the effect that it has on the classification of distorted input patterns. We present results which assess the classification performance of the <b>neocognitron</b> when the function of the component neurons is altered. The tests we describe demonstrate that using S-cells with a sigmoidal transfer function and modified activation function significantly enhances the classification performance of the <b>neocognitron.</b> 1 Introduction Fukushima's <b>neocognitron</b> [1, 2, 3, 4] has received attention over the past decade as a partially shift invariant [5, 6], distortion tolerant handwritten character classifier. The performance of novel structures of the <b>neocognitron</b> has been investigated to a certain extent [7] [...] ...|$|E
40|$|We {{describe}} {{a series of}} experiments that evaluate the performance of Fukushima's <b>neocognitron</b> using a database of handwritten ZIP code digits. A number of improvements to the original <b>neocognitron</b> were proposed and implemented, resulting in a peak performance of 85. 54 % correct classification, with 95. 69 % reliability. This result suggests that, with appropriate modifications, the <b>neocognitron</b> is almost capable of state-of-the-art recognition performance. The experimental evidence {{calls into question the}} effectiveness of Fukushima's supervised learning scheme and shows that the classification of input data can be reliably performed by the addition of a multilayer perceptron to the final layer of the <b>neocognitron.</b> 1 Introduction The <b>neocognitron</b> is a massively parallel, hierarchical neural network, designed, primarily, for 2 -D pattern recognition. Proposed by Fukushima in 1979 [1], it was inspired by Hubel and Wiesel's serial model of biological vision [2] and, for the last decade, it has [...] ...|$|E
40|$|The <b>neocognitron</b> is an {{artificial}} neural network which applies {{certain aspects of the}} mammalian visual process to the task of 2 -D pattern recognition. The resulting network model is complex in both structure and parameterization. We describe experiments which show that the performance of the <b>neocognitron</b> is sensitive to certain parameters whose values are seldom detailed in the relevant literature. We also present results which suggest that the selectivity parameters in the <b>neocognitron</b> can be adjusted in a straightforward manner so as to improve the classification performance of the <b>neocognitron.</b> 1 Introduction Fukushima's <b>neocognitron</b> [1, 2, 3] has received attention over the past decade as a partially shift invariant [4, 5], distortion tolerant classifier. It {{is one of the most}} complex {{artificial neural network}} structures to simulate, and this is perhaps the main reason that its performance has not been scrutinized to the extent of other, comparatively simpler networks. There are [...] ...|$|E
40|$|This paper {{describes}} a novel vehicle plate recognition algorithm based on text detection and improved <b>neocognitron</b> neural network, similar to [1] {{and based on}} Fukushima’s <b>neocognitron.</b> The proposed recognition algorithm allows us to improve the recognition speed and accuracy comparing to both traditional <b>neocognitron</b> and some state-of-art algorithms (multilayer perceptron, topological methods). It {{can be used as}} a solution for image classification and analysis tasks. As an example, the <b>neocognitron</b> can be utilized for symbols recognition [2]. We propose several modifications comparing to the Fukushima’s modification of the neocognitron: namely, layer dimensions adjustment, threshold function and connection Gaussian kernel parameters estimation. The patterns’ width and height are taken into account independently in order to improve the recognition of patterns of slightly different dimensions. The learning and recognition calculations are performed as FFT convolutions in order to overcome the complexity of the <b>neocognitron</b> output calculations. The algorithm was tested on low-resolution (360 × 288) video sequences and gave more accurate results comparing to the state-of-the-art methods for low-resolution test set...|$|E
40|$|This paper {{presents}} the parallel {{implementation of the}} <b>neocognitron</b> neural network paradigm on star topology, A detailed theoretical timing analysis has been presented along with a comparison between the experimental and theoretical results. These studies demonstrate that a linear speedup {{can be achieved by}} mapping the <b>neocognitron</b> onto a star topology...|$|E
40|$|Fuzzy {{neural network}} model based on <b>neocognitron</b> is {{proposed}} to identify layout objects on images of topological layers of integrated circuits. Testing of the model on images of real chip layouts was showed a high е r degree of identification of the proposed neural network in comparison to base <b>neocognitron.</b> </p...|$|E
40|$|A {{supervised}} learning feedforward neural net, which combines {{the advantages of}} <b>Neocognitron</b> and Perceptron, is introduced. The net topology is constrained to local connections between layers. A weight-sharing technique is used similiar to Fukushima's <b>Neocognitron</b> network model. Thus objects deformed in shape, shifted in position or variing in size can be recognized without any prepocessing or normalisation. In contrast ot the <b>Neocognitron</b> layers containing simple cells (S-cells), complex cells (C-cells) and inhibitory cells (V-cells) are subsituted by layers built up from McCulloch-Pitts neurons with sigmoidal nonlinearity. Additionally, {{in order to train}} each layer independently the {{supervised learning}} algorithm of the <b>Neocognitron</b> is replaced by the Least-Mean-Square learning rule commonly used for Perceptrons. In a first application the network has been successfully trained to recognize handwritten numerals of different size and position within a 24 by 24 pixel image...|$|E
40|$|Many {{kinds of}} {{artificial}} neural networks {{have been applied}} to the recognition of handwritten characters. Fukushima's <b>neocognitron</b> is one of few networks that demonstrates invariance to input translation and tolerance of a significant degree of input distortion and deformation. This paper shows that the behaviour of the <b>neocognitron</b> is dependent upon the form of non-linearity used by the feature extracting components of the network. Preliminary experimental {{results show that the}} threshold transfer function performs poorly in comparison to the threshold-linear function used in the original <b>neocognitron.</b> The threshold-linear function is marginally outperformed by the biologically inspired sigmoid function. 1 Introduction The <b>neocognitron</b> [1] is a massively parallel hierarchical neural network, designed primarily for character recognition and inspired by the research of Hubel and Wiesel into the mammalian visual process [2]. There have been many investigations into various characteristics [...] ...|$|E
40|$|Optical {{character}} recognition {{is useful in}} many aspects of business. However, the use of conventional computers to provide {{a solution to this}} problem has not been very effective. Over the past two decades, researchers have utilized artificial neural networks for optical {{character recognition}} with considerable success. One such neural network is the <b>neocognitron,</b> a real-valued, multi-layered hierarchical network that simulates the human visual system. The <b>neocognitron</b> was shown to have the capability for pattern recognition despite variations in size, shape or the presence of deformations from the trained patterns. Unfortunately, the <b>neocognitron</b> is an analog network which prevents it from taking full advantage of the many advances in 2 ̆ 2 VLSI technology. Major advances in VLSI technology have been in the digital medium. Therefore, it appears necessary to adapt the <b>neocognitron</b> to an efficient digital neural network {{if it is to be}} implemented in VLSI. Recent research has shown that through preprocessing approximations and definition of new model functions, the <b>neocognitron</b> is well suited for implementation in digital VLSI. This thesis uses this methodology to implement a large scale digital <b>neocognitron</b> model. The new model, the digi-neocognitron, uses supervised learning and is trained to recognize ten handwritten numerals with widths of one pixel. The development of the <b>neocognitron</b> and the digi-neocognitron software models, and a comparison of their performance will be discussed. This is followed by the development and simulation of the digital model using the VHSIC Hardware Description Language (VHDL). The VHDL model is used to demonstrate the functionality of the hardware model and to aid in its design. The model functions of the digi-neocognitron are then implemented and simulated for a 1. 2 micrometers CMOS process...|$|E
40|$|A feature-extraction-based {{optoelectronic}} {{neural network}} is introduced. The system implementation approach applies {{the principle of}} the <b>neocognitron</b> paradigm first introduced by Fukushima et al. (1983). A multichannel correlator is used as a building block of a generic single layer of the <b>neocognitron</b> for shift-invariant feature correlation. Multilayer processing is achieved by iteratively feeding back the output of the feature correlator to the input spatial light modulator. Successful pattern recognition with intraclass fault tolerance and interclass discrimination is achieved using this optoelectronic <b>neocognitron.</b> Detailed system analysis is described. Experimental demonstration of radar signature processing is also provided...|$|E
40|$|Some {{modifications}} to an existing neural network, the <b>neocognitron,</b> are proposed {{in order to}} overcome some of its limitations and to achieve an improved recognition of patterns (for instance, characters). Motivation for the present work arose {{from the results of}} extensive simulation experiments on the <b>neocognitron.</b> Inhibition during training is dispensed with, including it only during the testing phase of the <b>neocognitron.</b> Even during testing, inhibition is totally discarded in the initial layer because it leads, otherwise, to some undesirable results. However, inhibition, which is feature-based, is incorporated in the later stages. The number of network parameters which are to be set manually during training is reduced. The training is made simple without involving multiple training patterns of the same nature. A new layer has been introduced after the C-layer (of the <b>neocognitron)</b> to scale down the network size. Finally, the response of the S-cell has been simplified, and the blurring operation between the S- and the C-layers has been changed. The new architecture, which is robust with respect to small variations {{in the value of the}} network parameters, and the associated training are believed to be simpler and more efficient than those of the <b>neocognitron...</b>|$|E
40|$|This work {{presents}} {{the application of}} <b>Neocognitron</b> to the human face recognition. Using a large-scale human face database (CMU PIE), the optimal thresholds of the <b>Neocognitron</b> to human face recognition are verified. During the first experiment, increasing the activation thresholds of the <b>Neocognitron,</b> their best values {{to be used in}} the second experiment, increasing the number of training images per subjects, are obtained. As a result it is verified that a number of 25 training images per subjects is enough to obtain very high recognition rate (98 %) to the frontal pose images from the database. 350 validation images, non-overlapping with the training images, were used. 1...|$|E
40|$|Abstract: This paper {{presents}} {{the development of}} Assyrian machine printed character recognition system. Well-known <b>neocognitron</b> artificial neural network is chosen for its fast processing time and its good performance for pattern recognition problems. The average recognition rate of 95 % has been achieved this confirms that the proposed <b>neocognitron</b> artificial neural network approach is suitable {{for the development of}} Assyrian machine printed character recognition system...|$|E
40|$|This paper {{presents}} {{the development of}} Gurumukhi character recognition system of isolated handwritten characters by using <b>Neocognitron</b> at the first time. Well- known <b>neocognitron</b> artificial neural network is chosen for its fast processing time and its good performance for pattern recognition problems. Here we have found the recognition accuracy of both learned and unlearned images of characters. Learned images have recognition accuracy as 91. 77 % and unlearned images have recognition accuracy as 93. 79 %. The overall recognition accuracy for both learned and unlearned Gurmukhi characters are 92. 78 %. This confirms that the proposed <b>neocognitron</b> artificial neural network approach is suitable {{for the development of}} isolated handwritten characters of Gurumukhi script...|$|E
40|$|Convolutional neural {{networks}} provide an efficient method to constrain {{the complexity of}} feedforward {{neural networks}} by weightsharing. This network topology has been applied in particular to image classification when raw images are to be classified without preprocessing. In this paper two variations of convolutional networks - <b>Neocognitron</b> and Neoperceptron - are compared with classifiers based on fully connected feedforward layers (i. e. Multilayerperceptron, Nearest Neighbor Classifier, Autoencoding network) {{with respect to their}} visual recognition performance. Beside the original <b>Neocognitron</b> a modification called Neoperceptron is proposed which combines neurons from Perceptron with the localized network structure of <b>Neocognitron.</b> Instead of training convolutional networks by time-consuming error backpropagation in this work a modular procedure is applied, whereby layers are trained sequentially from the input to the output layer in order to recognize features of increasing complexit [...] ...|$|E
40|$|We {{examine the}} number of cells and {{execution}} time taken to correctly recognize rotated patterns in two models: a rotation-invariant <b>neocognitron</b> (RNeocognitron) and a neocognitron-type model (TDR -Neocognitron) which recognizes rotated patterns by use of an associative recalled pattern. In numerical simulations handwritten patterns in CEDER database are used for training and evaluation of recognition rate. We show that TD-R-Neocognitron needs less cells than R-Neocognitron if {{the number of}} pattern classes is large. Execution time by TDR -Neocognitron is about three times as much as that of R-Neocognitron, but TD-R-Neocognitron is more e#ective and e#cient for patterns including many classes like Japanese characters. 1 Introduction Various visual models for pattern recognition based on the Neocognitron[1] have been proposed. We have proposed a rotation-invariant <b>Neocognitron</b> (referred to R-Neocognitron, see [2]), which is based on the <b>Neocognitron.</b> It can recognize also rotated patter [...] ...|$|E
