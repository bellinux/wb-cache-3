3756|10000|Public
5|$|Integer sorting {{algorithms}} including pigeonhole sort, counting sort, and radix sort {{are widely}} used and practical. Other integer sorting algorithms with smaller worst-case time bounds are not believed to be practical for computer architectures with 64 or fewer bits per word. Many such algorithms are known, with performance depending {{on a combination of}} the number of items to be sorted, <b>number</b> <b>of</b> <b>bits</b> per key, and <b>number</b> <b>of</b> <b>bits</b> per word of the computer performing the sorting algorithm.|$|E
5|$|When clocked, {{the phase}} {{accumulator}} (PA) creates a modulo-2N sawtooth waveform {{which is then}} converted by the phase-to-amplitude converter (PAC) to a sampled sinusoid, where N is the <b>number</b> <b>of</b> <b>bits</b> carried in the phase accumulator. N sets the NCO frequency resolution and is normally {{much larger than the}} <b>number</b> <b>of</b> <b>bits</b> defining the memory space of the PAC look-up table. If the PAC capacity is 2M, the PA output word must be truncated to M bits as shown in Figure 1. However, the truncated bits can be used for interpolation. The truncation of the phase output word does not affect the frequency accuracy but produces a time-varying periodic phase error which is a primary source of spurious products. Another spurious product generation mechanism is finite word length effects of the PAC output (amplitude) word.|$|E
5|$|Another {{method of}} {{constructing}} hash functions with both high quality and practical speed is tabulation hashing. In this method, the hash value for a key {{is computed by}} using each byte of the key as an index into a table of random numbers (with a different table for each byte position). The numbers from those table cells are then combined by a bitwise exclusive or operation. Hash functions constructed this way are only 3-independent. Nevertheless, linear probing using these hash functions takes constant expected time per operation. Both tabulation hashing and standard methods for generating 5-independent hash functions are limited to keys that have a fixed <b>number</b> <b>of</b> <b>bits.</b> To handle strings or other types of variable-length keys, {{it is possible to}} compose a simpler universal hashing technique that maps the keys to intermediate values and a higher quality (5-independent or tabulation) hash function that maps the intermediate values to hash table indices.|$|E
50|$|The {{bit error}} rate (BER) is the <b>number</b> <b>of</b> <b>bit</b> errors per unit time. The bit error ratio (also BER) is the <b>number</b> <b>of</b> <b>bit</b> errors divided by the total <b>number</b> <b>of</b> {{transferred}} <b>bits</b> during a studied time interval. Bit error ratio is a unitless performance measure, often expressed as a percentage.|$|R
25|$|The samples may use {{different}} <b>numbers</b> <b>of</b> <b>bits.</b>|$|R
50|$|In digital transmission, the <b>number</b> <b>of</b> <b>bit</b> errors is the <b>number</b> <b>of</b> {{received}} <b>bits</b> <b>of</b> a {{data stream}} over a communication channel {{that have been}} altered due to noise, interference, distortion or bit synchronization errors.|$|R
25|$|The {{amount of}} entropy {{is not always}} an integer <b>number</b> <b>of</b> <b>bits.</b>|$|E
25|$|In {{general the}} number of {{available}} hosts on a subnet is 2h−2, where h is the <b>number</b> <b>of</b> <b>bits</b> used for the host portion of the address. The number of available subnets is 2n, where n is the <b>number</b> <b>of</b> <b>bits</b> used for the network portion of the address. This is the RFC 1878 standard used by the IETF, the IEEE and COMPTIA.|$|E
25|$|Other {{commonly}} used measures are the total <b>number</b> <b>of</b> <b>bits</b> transmitted {{in the network}} (cf. communication complexity).|$|E
40|$|We {{consider}} the asymmetric distributed source coding problem, where the recipient interactively communicates with N correlated informants to gather their data. We are mainly interested in minimizing the worst-case <b>number</b> <b>of</b> informant <b>bits</b> required for successful data-gathering at recipient, {{but we are}} also concerned with minimizing the <b>number</b> <b>of</b> rounds {{as well as the}} <b>number</b> <b>of</b> recipient <b>bits.</b> We provide two algorithms, one that optimally minimizes the <b>number</b> <b>of</b> informant <b>bits</b> and other that trades-off the <b>number</b> <b>of</b> informant <b>bits</b> to efficiently reduce the <b>number</b> <b>of</b> rounds and <b>number</b> <b>of</b> recipient <b>bits...</b>|$|R
30|$|In {{addition}} to the success rate, the BER performance is {{used to assess the}} accuracy of the whole system. It expresses the <b>number</b> <b>of</b> <b>bit</b> errors per second divided by the total <b>number</b> <b>of</b> transferred <b>bits.</b>|$|R
50|$|Each RxQual value {{corresponds}} {{to an estimated}} <b>number</b> <b>of</b> <b>bit</b> errors in a <b>number</b> <b>of</b> bursts.|$|R
25|$|Both of {{the above}} {{algorithms}} are scalable, as each node processes and sends only small (polylogarithmic in n, the network size) <b>number</b> <b>of</b> <b>bits</b> per round.|$|E
25|$|In {{telecommunications}} and computing, bit rate (sometimes written bitrate or as a variable R) is the <b>number</b> <b>of</b> <b>bits</b> that are conveyed or processed {{per unit of}} time.|$|E
25|$|Fiber-optic {{capacity}} – The <b>number</b> <b>of</b> <b>bits</b> {{per second}} {{that can be}} sent down an optical fiber increases exponentially, faster than Moore's law. Keck's law, in honor of Donald Keck.|$|E
50|$|The {{bit error}} rate or bit error ratio (BER) is the <b>number</b> <b>of</b> <b>bit</b> errors divided by the total <b>number</b> <b>of</b> {{transferred}} <b>bits</b> during a studied time interval. BER is a unitless performance measure, often expressed as a percentage.|$|R
40|$|When {{algorithms}} for sorting {{and searching}} {{are applied to}} keys that are represented as bit strings, we can quantify {{the performance of the}} algorithms not only in terms <b>of</b> the <b>number</b> <b>of</b> key comparisons required by the algorithms but also in terms <b>of</b> the <b>number</b> <b>of</b> <b>bit</b> comparisons. Some <b>of</b> the standard sorting and searching algorithms have been analyzed with respect to key comparisons but not with respect to bit comparisons. In this extended abstract, we investigate the expected <b>number</b> <b>of</b> <b>bit</b> comparisons required by Quickselect (also known as Find). We develop exact and asymptotic formulae for the expected <b>number</b> <b>of</b> <b>bit</b> comparisons required to find the smallest or largest key by Quickselect and show that the expectation is asymptotically linear with respect to the <b>number</b> <b>of</b> keys. Similar results are obtained for the average case. For finding keys of arbitrary rank, we derive an exact formula for the expected <b>number</b> <b>of</b> <b>bit</b> comparisons that (using rational arithmetic) requires only finite summation (rather than such operations as numerical integration) and use it to compute the expectation for each target rank. ...|$|R
5000|$|Yuen {{played a}} <b>number</b> <b>of</b> <b>bit</b> parts in Hollywood action and martial arts films {{beginning}} in the 1990s ...|$|R
25|$|Performing the mod M at each {{iteration}} {{ensures that}} all intermediate results are at most p bits (otherwise the <b>number</b> <b>of</b> <b>bits</b> would double each iteration). The same strategy {{is used in}} modular exponentiation.|$|E
25|$|The term average bitrate {{is used in}} case of {{variable}} bitrate multimedia source coding schemes. In this context, the peak bit rate is the maximum <b>number</b> <b>of</b> <b>bits</b> required for any short-term block of compressed data.|$|E
25|$|An {{algorithm}} {{is said to}} take superpolynomial time if T(n) is not bounded above by any polynomial. It is ω(n'c) time for all constants c, where n is the input parameter, typically the <b>number</b> <b>of</b> <b>bits</b> in the input.|$|E
3000|$|... (t) {{denotes the}} <b>number</b> <b>of</b> <b>bit</b> loss during the slot t for the kth user. Obviously, smaller C̅ is preferred.|$|R
5000|$|... to {{represent}} the <b>number</b> <b>of</b> <b>bit</b> errors on path [...] (the Hamming distance between the branch labels and the received sequence) ...|$|R
50|$|The {{transmission}} BER is the <b>number</b> <b>of</b> detected <b>bits</b> {{that are}} incorrect before error correction, {{divided by the}} total <b>number</b> <b>of</b> transferred <b>bits</b> (including redundant error codes). The information BER, approximately equal to the decoding error probability, is the <b>number</b> <b>of</b> decoded <b>bits</b> that remain incorrect after the error correction, divided by the total <b>number</b> <b>of</b> decoded <b>bits</b> (the useful information). Normally the transmission BER {{is larger than the}} information BER. The information BER is affected by the strength of the forward error correction code.|$|R
25|$|In conclusion, {{the exact}} <b>number</b> <b>of</b> <b>bits</b> of {{precision}} {{needed in the}} significand of the intermediate result is somewhat data dependent but 64 bits is sufficient to avoid precision loss {{in the vast majority}} of exponentiation computations involving double precision numbers.|$|E
25|$|Steinitz's {{original}} induction-based proof can {{be strengthened}} in this way. However, the integers {{that would result}} from this construction are doubly exponential {{in the number of}} vertices of the given polyhedral graph. Writing down numbers of this magnitude in binary notation would require an exponential <b>number</b> <b>of</b> <b>bits.</b>|$|E
25|$|Variable-length {{representations}} of integers, such as bignums, can store any integer that fits in the computer's memory. Other integer data types are implemented with a fixed size, usually a <b>number</b> <b>of</b> <b>bits</b> {{which is a}} power of2 (4, 8, 16, etc.) or a memorable number of decimal digits (e.g., 9 or10).|$|E
3000|$|... is the {{execution}} {{time of the}} GA-based optimization procedure expressed in <b>number</b> <b>of</b> <b>bit</b> periods). Now, the GA switches to the decision-directed adaptive modality.|$|R
5000|$|The Wald-Wolfowitz {{runs test}} tests for the <b>number</b> <b>of</b> <b>bit</b> {{transitions}} between 0 bits, and 1 bits, comparing the observed frequencies with expected frequency <b>of</b> a random <b>bit</b> sequence.|$|R
50|$|The {{disparity}} <b>of</b> a <b>bit</b> {{pattern is}} {{the difference in the}} <b>number</b> <b>of</b> one <b>bits</b> vs the <b>number</b> <b>of</b> zero <b>bits.</b> The running disparity is the running total of the disparity of all previously transmitted words.|$|R
25|$|Coding {{theory is}} one of the most {{important}} and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the <b>number</b> <b>of</b> <b>bits</b> needed to describe the data, which is the information entropy of the source.|$|E
25|$|With data being {{transferred}} 64 bits at a time, DDR SDRAM gives a transfer rate (in bytes/s) of (memory bus clock rate) × 2 (for dual rate) × 64 (<b>number</b> <b>of</b> <b>bits</b> transferred) / 8 (number of bits/byte). Thus, with a bus frequency of 100MHz, DDR SDRAM gives a maximum transfer rate of 1600MB/s.|$|E
25|$|The entropy rate of a {{data source}} means the average <b>number</b> <b>of</b> <b>bits</b> per symbol needed to encode it. Shannon's {{experiments}} with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text.|$|E
5000|$|After {{making her}} film debut in Pinky (1949), Moore had a <b>number</b> <b>of</b> <b>bit</b> parts and {{supporting}} roles in motion pictures through the 1950s and 1960s.|$|R
5000|$|... 2b. Count (to the left) the <b>number</b> <b>of</b> <b>bit</b> {{positions}} {{to the next}} most significant non-zero bit. If there are no more-significant bits, then take the value <b>of</b> the current <b>bit</b> position.|$|R
40|$|Graduation date: 1989 A {{study of}} the running time of several known {{algorithms}} and several new algorithms to compute the n[superscript th] element of the Fibonacci sequence is presented. Since {{the size of the}} n[superscript th] Fibonacci number grows exponentially with n, the <b>number</b> <b>of</b> <b>bit</b> operations, instead <b>of</b> the <b>number</b> <b>of</b> integer operations, was used as the unit <b>of</b> time. The <b>number</b> <b>of</b> <b>bit</b> operations used to compute f[subscript n] is reduced to less than 1 / 2 <b>of</b> the <b>number</b> <b>of</b> <b>bit</b> operations used to multiply two n bit numbers. The algorithms were programmed in Ibuki Common Lisp and timing runs were made on a Sequent Balance 21000. Multiplication was implemented using the standard n² algorithm. Times for the various algorithms are reported as various constants times n². An algorithm based on generating factors <b>of</b> Fibonacci <b>numbers</b> had the smallest constant. The Fibonacci sequence, arranged in various ways, is searched for redundant information that could be eliminated to reduce the <b>number</b> <b>of</b> operations. Cycles in the b[superscript th] <b>bit</b> <b>of</b> f[subscript n] were discovered but are not yet completely understood...|$|R
