10000|10000|Public
5|$|Common {{locations}} {{for the start}} of seizures and <b>neural</b> <b>networks</b> {{have been found to be}} affected in the majority of epilepsy. Efforts to figure out how epilepsy occurs is working to take into account the different regions of the brain and the timing of their activity.|$|E
5|$|Pre-conference {{tutorials}} {{have played}} an important role in ISMB since the first conference. Tutorials at ISMB 1994 included introductions to genetic algorithms, <b>neural</b> <b>networks,</b> AI for molecular biologists and molecular biology for computer scientists. Tutorials on computational mass spectrometry-based proteomics and ENCODE data access were presented at ISMB/ECCB 2013.|$|E
5|$|The {{new field}} was unified and {{inspired}} {{by the appearance of}} Parallel Distributed Processing in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. <b>Neural</b> <b>networks</b> would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and speech recognition.|$|E
30|$|The {{top layer}} is {{designed}} to analyze the facial muscle activity collected in the AU activity map, in a pattern recognition task, and output a score from 1 to 10 {{for each of the}} 16 PF traits, in accordance with the 16 PF framework. To accomplish this, we have defined 16 FFNNs denoted as follows: warmth (A) - <b>neural</b> <b>network</b> (A-NN), reasoning (B) - <b>neural</b> <b>network</b> (B-NN), emotional stability (C) - <b>neural</b> <b>network</b> (C-NN), dominance (E) - <b>neural</b> <b>network</b> (E-NN), liveliness (F) - <b>neural</b> <b>network</b> (F-NN), rule-consciousness (G) - <b>neural</b> <b>network</b> (G-NN), social boldness (H) - <b>neural</b> <b>network</b> (H-NN), sensitivity (I) - <b>neural</b> <b>network</b> (I-NN), vigilance (L) - <b>neural</b> <b>network</b> (L-NN), abstractedness (M) - <b>neural</b> <b>network</b> (M-NN), privateness (N) - <b>neural</b> <b>network</b> (P-NN), apprehension (O) - <b>neural</b> <b>network</b> (O-NN), openness to change (Q 1) - <b>neural</b> <b>network</b> (Q 1 -NN), self-reliance (Q 2) - <b>neural</b> <b>network</b> (Q 2 -NN), perfectionism (Q 3) - <b>neural</b> <b>network</b> (Q 3 -NN), and tension (Q 4) - <b>neural</b> <b>network</b> (Q 4 -NN).|$|R
40|$|Abstract. In this study, a new multi-layered Group Method of Data Handling (GMDH) -type <b>neural</b> <b>network</b> self-selecting optimum <b>neural</b> <b>network</b> {{architecture}} is proposed. We call this algorithm as revised GMDH-type <b>neural</b> <b>network</b> algorithm self-selecting optimum <b>neural</b> <b>network</b> architecture. Revised GMDH-type <b>neural</b> <b>network</b> algorithm has an ability of self-selecting optimum <b>neural</b> <b>network</b> architecture from three <b>neural</b> <b>network</b> architectures such as sigmoid function <b>neural</b> <b>network,</b> radial basis function (RBF) <b>neural</b> <b>network</b> and polynomial <b>neural</b> <b>network.</b> Revised GMDH-type <b>neural</b> <b>network</b> also has abilities of self-selecting {{the number of}} layers, the number of neurons in hidden layers and useful input variables. This algorithm {{is applied to the}} nonlinear system identification problem and it is shown that this algorithm is useful for the nonlinear system identification because optimum <b>neural</b> <b>network</b> {{architecture is}} automatically organized...|$|R
40|$|AbstractCharacteristics and {{precision}} of BP <b>neural</b> <b>network,</b> genetic <b>neural</b> <b>network</b> and annealing <b>neural</b> <b>network</b> applied in GPS height fitting were hereby compared and analyzed {{to improve the}} precision of transforming GPS geodetic height into normal height. The results indicated that Genetic <b>Neural</b> <b>Network</b> method is superior to BP <b>neural</b> <b>network</b> method as well as Annealing <b>Neural</b> <b>Network</b> 1 method with good precision through several instances...|$|R
5|$|Theorists {{have worked}} to {{understand}} these response patterns by constructing mathematical models of neurons and <b>neural</b> <b>networks,</b> which can be simulated using computers. Some useful models are abstract, focusing on the conceptual structure of neural algorithms rather than {{the details of how}} they are implemented in the brain; other models attempt to incorporate data about the biophysical properties of real neurons. No model on any level is yet considered to be a fully valid description of brain function, though. The essential difficulty is that sophisticated computation by <b>neural</b> <b>networks</b> requires distributed processing in which hundreds or thousands of neurons work cooperatively—current methods of brain activity recording are only capable of isolating action potentials from a few dozen neurons at a time.|$|E
25|$|Neurocomputational speech {{processing}} is {{speech processing}} by artificial <b>neural</b> <b>networks.</b> Neural maps, mappings and pathways as described below, are model structures, i.e. important structures within artificial <b>neural</b> <b>networks.</b>|$|E
25|$|Today, <b>neural</b> <b>networks</b> {{are often}} {{trained by the}} {{backpropagation}} algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to <b>neural</b> <b>networks</b> by Paul Werbos.|$|E
50|$|A {{physical}} <b>neural</b> <b>network</b> includes electrically {{adjustable resistance}} material to simulate artificial synapses. Examples include the ADALINE memristor-based <b>neural</b> <b>network.</b> An optical <b>neural</b> <b>network</b> {{is a physical}} implementation of an artificial <b>neural</b> <b>network</b> with optical components.|$|R
5000|$|One of {{the major}} {{benefits}} of a modular <b>neural</b> <b>network</b> {{is the ability to}} reduce a large, unwieldy <b>neural</b> <b>network</b> to smaller, more manageable components. [...] There are some tasks it appears are for practical purposes intractable for a single <b>neural</b> <b>network</b> as its size increases. The following are benefits of using a modular <b>neural</b> <b>network</b> over a single all-encompassing <b>neural</b> <b>network.</b>|$|R
40|$|The current fault {{diagnosis}} methods based on conventional BP <b>neural</b> <b>network</b> and RBF <b>neural</b> <b>network</b> exist long training time, slow convergence speed and low judgment accuracy rate and so on. In {{order to improve}} the ability of {{fault diagnosis}}, this paper puts forward a kind of fault diagnosis method based on RBF <b>Neural</b> <b>Network</b> improved by PSO algorithm. By using particle swarm algorithm’s heuristic global optimization ability, the connection weight values of RBF <b>neural</b> <b>network</b> are optimized. And then combined with RBF <b>neural</b> <b>network’s</b> nonlinear processing ability, transformer fault samples are trained and tested. The experimental results show that, compared with conventional fault diagnosis methods based on BP <b>neural</b> <b>network</b> and RBF <b>neural</b> <b>network,</b> the method based on RBF <b>Neural</b> <b>Network</b> improved by PSO algorithm can effectively avoid the problems of RBF <b>neural</b> <b>network’s</b> instability, RBF <b>neural</b> <b>network</b> easily falling into local minima and low correct diagnosis rate, which can effectively improve the convergence speed and the efficiency of fault diagnosis...|$|R
25|$|The main {{categories}} of networks are acyclic or feedforward <b>neural</b> <b>networks</b> (where the signal passes {{in only one}} direction) and recurrent <b>neural</b> <b>networks</b> (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. <b>Neural</b> <b>networks</b> {{can be applied to}} the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning, GMDH or competitive learning.|$|E
25|$|Large {{memory storage}} and {{retrieval}} <b>neural</b> <b>networks</b> (LAMSTAR) are fast deep learning <b>neural</b> <b>networks</b> of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.|$|E
25|$|Electronic <b>neural</b> <b>networks</b> {{have been}} {{deployed}} which shift the learning phase from the user to the computer. Experiments by scientists at the Fraunhofer Society in 2004 using <b>neural</b> <b>networks</b> led to noticeable improvements within 30 minutes of training.|$|E
40|$|The {{impact of}} {{artificial}} <b>neural</b> <b>network</b> model output precision technology widespread attention. Quality sample study of <b>neural</b> <b>network</b> output accuracy {{is not much}} affected, {{most of the research}} is the structure (number of layers and the number of nodes), the impact of this paper to analyze samples of artificial <b>neural</b> <b>network</b> output for the <b>neural</b> <b>network,</b> to improve the output of <b>neural</b> <b>network</b> accuracy is important...|$|R
40|$|Abstract: Wavelet <b>Neural</b> <b>Network</b> (WNN) is a {{new form}} of <b>neural</b> <b>network</b> {{combined}} with the wavelet theory and artificial <b>neural</b> <b>network.</b> The wavelet <b>neural</b> <b>network</b> model based on Morlet wavelet and the corresponding learning algorithm were studied in this paper. And through learning the wavelet <b>neural</b> <b>network</b> model is applied to all kinds of engineering examples, it proved that the wavelet <b>neural</b> <b>network</b> prediction model which has a more flexible and efficient function approximation ability and strong fault tolerance, and with high predicting precision...|$|R
40|$|AbstractIn this study, {{a hybrid}} multi-layered Group Method of Data Handling (GMDH) -type <b>neural</b> <b>network</b> {{algorithm}} using principal component-regression analysis is proposed {{and applied to}} the computer aided image diagnosis (CAD) of liver cancer. In the GMDH-type <b>neural</b> <b>network,</b> a heuristic self-organization method that {{is a type of}} evolutionary computation, is used to organize the <b>neural</b> <b>network</b> architecture. In this revised GMDH-type <b>neural</b> <b>network,</b> the optimum <b>neural</b> <b>network</b> architecture is automatically organized from three types of <b>neural</b> <b>network</b> architectures, such as the sigmoid function <b>neural</b> <b>network,</b> the radial basis function (RBF) network and the polynomial <b>neural</b> <b>network</b> architecture, by the heuristic self-organization method. Furthermore, the structural parameters such as the number of layers, the number of neurons in hidden layers and useful input variables, are automatically determined using the heuristic self-organization method. In the revised GMDH-type <b>neural</b> <b>network</b> proposed in this paper, the principal component-regression analysis is used to protect multi-colinearity which has occurred in the learning calculations of neurons, and accurate and stable prediction values are obtained. This new algorithm is applied to the medical image diagnosis of liver cancer. In this application, two types of <b>neural</b> <b>network</b> architectures fitting the complexity of the multi-detector row CT (MDCT) medical images, are automatically organized using the revised GMDH-type <b>neural</b> <b>network</b> algorithm The first <b>neural</b> <b>network</b> recognizes and extracts the liver regions from the MDCT images of the liver, and the second <b>neural</b> <b>network</b> recognizes and extracts the liver cancer regions. These results are compared with the conventional sigmoid function <b>neural</b> <b>network</b> trained using the back propagation method, and this GMDH-type <b>neural</b> <b>network</b> algorithm is shown to be useful for CAD of liver cancer...|$|R
25|$|Igor Aizenberg {{and colleagues}} {{introduced}} it to Artificial <b>Neural</b> <b>Networks</b> in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes {{the learning of}} a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward <b>neural</b> <b>networks</b> (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial <b>neural</b> <b>networks,</b> deep <b>neural</b> <b>networks</b> can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep <b>neural</b> <b>networks</b> that contain many layers of non-linear hidden units and a very large output layer.|$|E
25|$|OpenNN: Open <b>neural</b> <b>networks</b> library.|$|E
25|$|KCC2b, on {{the other}} hand, is {{scarcely}} present during prenatal development and is strongly upregulated during postnatal development. The upregulation of KCC2b expression {{is thought to be}} responsible for the “developmental shift” observed in mammals from depolarizing postsynaptic effects of inhibitory synapses in early <b>neural</b> <b>networks</b> to hyperpolarizing effects in mature <b>neural</b> <b>networks.</b>|$|E
40|$|<b>Neural</b> <b>network</b> is a {{model of}} brains’s {{cognitive}} process. <b>Neural</b> <b>network</b> originated as a model of how the brain works. <b>Neural</b> <b>network</b> research has its beginnings in psychology. Today <b>neural</b> <b>network</b> methods are being used to solve numerous problems associated with manufacturing operations. A review of <b>neural</b> <b>network</b> applications to problems in production and operations management is presented. Applications reviewed in this paper include character, image andpattern recognition, managerial decision making, manufacturing cell design, tool condition monitoring, real-time robot scheduling and statistical process control. Methods and structures of <b>neural</b> <b>network</b> are explained...|$|R
40|$|Key words: {{generalized}} congruence neural network;BP neural network; rotary machine; {{fault diagnosis}} Abstract. Replace power function with generalized congruence <b>neural</b> <b>network</b> and {{put forward a}} modified model and algorithm of generalized congruence <b>neural</b> <b>network.</b> Analyse structure, power function, algorithm of adjusting power and compare it with BP <b>neural</b> <b>network.</b> Compareing it with BP <b>neural</b> <b>network</b> in approaching sine function, it indicate the modified generalized congruence <b>neural</b> <b>network</b> has fast constringency speed and good stability which BP <b>neural</b> <b>network</b> has. The model and algorithm advance constringency speed and diagnosis precision...|$|R
40|$|Abstract—This paper {{presents}} a <b>neural</b> <b>network</b> {{with a novel}} neuron model. In this model, the neuron has two activation func-tions and exhibits a node-to-node relationship in the hidden layer. This <b>neural</b> <b>network</b> provides better performance than a tradi-tional feedforward <b>neural</b> <b>network,</b> and fewer hidden nodes are needed. The parameters of the proposed <b>neural</b> <b>network</b> are tuned by a genetic algorithm with arithmetic crossover and nonuniform mutation. Some applications are given to show {{the merits of the}} proposed <b>neural</b> <b>network.</b> Index Terms—Genetic algorithm (GA), <b>neural</b> <b>network,</b> short-term load forecasting. I...|$|R
25|$|In {{the early}} 1930s, Rashevsky {{developed}} {{the first model}} of <b>neural</b> <b>networks.</b> This was paraphrased in a Boolean context by his student Walter Pitts together with Warren McCulloch, {{in an article published}} in Rashevsky's Bulletin of Mathematical Biophysics in 1943. The Pitts-McCulloch article subsequently became extremely influential for research on artificial intelligence and artificial <b>neural</b> <b>networks.</b>|$|E
25|$|A {{fundamental}} objection is {{that they}} do not reflect how real neurons function. Back propagation is a critical part of most artificial <b>neural</b> <b>networks,</b> although no such mechanism exists in biological <b>neural</b> <b>networks.</b> How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological <b>neural</b> <b>networks</b> is known.|$|E
25|$|There are quantum analogues or generalisations of {{classical}} neural nets which {{are known as}} quantum <b>neural</b> <b>networks.</b>|$|E
40|$|Abstract- This paper {{presents}} a modified <b>neural</b> <b>network</b> for solving strictly quadratic programming problems with general linear constraints. It is {{shown that the}} proposed <b>neural</b> <b>network</b> is globally convergent to a unique optimal solution within a finite time. Compared with the existing primal-dual <b>neural</b> <b>network</b> and the dual <b>neural</b> <b>network</b> for solving such problems, the proposed <b>neural</b> <b>network</b> has a low complexity for implementation and can be guaranteed to have an exponential convergence rate. The good performance of the proposed <b>neural</b> <b>network</b> is illustrated by a real-time application to Kinematic Control...|$|R
40|$|This paper {{introduces}} {{a kind of}} diagnosis principle and learning algorithm of steam turbine fault diagnosis which based on Elman <b>neural</b> <b>network.</b> Comparing {{the results of the}} Elman <b>neural</b> <b>network</b> and the traditional BP <b>neural</b> <b>network</b> diagnosis, the results shows that Elman <b>neural</b> <b>network</b> is an effective way to improve the learning speed, effectively suppress the minimum defects that the traditional <b>neural</b> <b>network</b> easily trapped in, and shorten the autonomous learning time. All these proves that the Elman <b>neural</b> <b>network</b> is an effective way to diagnose the steam turbine...|$|R
5000|$|<b>Neural</b> <b>Network</b> Leadership Award (International <b>Neural</b> <b>Network</b> Society), 1994 ...|$|R
25|$|Eyeriss, {{a design}} aimed {{explicitly}} at convolutional <b>neural</b> <b>networks,</b> using a scratchpad and on chip network architecture.|$|E
25|$|Warren McCulloch and Walter Pitts (1943) {{created a}} {{computational}} model for <b>neural</b> <b>networks</b> based on mathematics and algorithms called threshold logic. This model {{paved the way}} for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of <b>neural</b> <b>networks</b> to artificial intelligence. This work led to work on nerve networks and their link to finite automata.|$|E
25|$|Alternatives to {{backpropagation}} include extreme learning machines, no-prop networks, training without backtracking, weightless networks. and non-connectionist <b>neural</b> <b>networks.</b>|$|E
40|$|Abstract. The {{feedback}} Group Method of Data Handling (GMDH) -type <b>neural</b> <b>network</b> {{algorithm is}} proposed and is applied to 3 -dimensional medical image recognition of the lungs, the pulmonary vessels and the bronchial trees. In this feedback GMDH-type <b>neural</b> <b>network</b> algorithm, the optimum <b>neural</b> <b>network</b> architecture is automatically selected from three types of <b>neural</b> <b>network</b> architectures such as the sigmoid function type <b>neural</b> <b>network,</b> the radial basis function (RBF) type <b>neural</b> <b>network</b> and the polynomial type <b>neural</b> <b>network.</b> Furthermore, the structural parameters such {{as the number of}} layers, the number of neurons in the hidden layers and the relevant input variables are automatically selected so as to minimize the prediction error criterion defined as Prediction Sum of Squares (PSS). The recognition results show that the feedback GMDH-type <b>neural</b> <b>network</b> algorithm is useful for the 3 -dimensional medical image recognition of the lungs, the pulmonary vessels and the bronchial trees and is ideal for such practical complex problems since the optimum <b>neural</b> <b>network</b> architecture is automatically organized...|$|R
50|$|In 1994, André C. P. L. F. de Carvalho, {{together}} with Fairhurst and Bisset, published experimental {{results of a}} multi-layer boolean <b>neural</b> <b>network,</b> {{also known as a}} weightless <b>neural</b> <b>network,</b> composed of a self-organising feature extraction <b>neural</b> <b>network</b> module followed by a classification <b>neural</b> <b>network</b> module, which were independently trained.|$|R
40|$|Abstract: Problem statement: Accurate weather {{forecasting}} plays {{a vital role}} for planning day to day activities. <b>Neural</b> <b>network</b> has been use in numerous meteorological applications including {{weather forecasting}}. Approach: A <b>neural</b> <b>network</b> model has been developed for weather forecasting, based on various factors obtained from meteorological experts. This study evaluates the performance of Radial Basis Function (RBF) with Back Propagation (BPN) <b>neural</b> <b>network.</b> The back propagation <b>neural</b> <b>network</b> and radial basis function <b>neural</b> <b>network</b> were {{used to test the}} performance in order to investigate effective forecasting technique. Results: The prediction accuracy of RBF was 88. 49 %. Conclusion: The results indicate that proposed radial basis function <b>neural</b> <b>network</b> is better than back propagation <b>neural</b> <b>network...</b>|$|R
