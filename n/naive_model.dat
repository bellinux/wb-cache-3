228|945|Public
500|$|In September, Monroe {{made her}} {{television}} {{debut in the}} Jack Benny Show, playing Jack's fantasy woman in the episode [...] "Honolulu Trip". She co-starred with Betty Grable and Lauren Bacall in her third movie of the year, How to Marry a Millionaire, which was released in November. It featured Monroe {{in the role of}} a <b>naïve</b> <b>model</b> who teams up with her friends to find rich husbands, repeating the successful formula of Gentlemen Prefer Blondes. It was the second film ever released in CinemaScope, a widescreen format that Fox hoped would draw audiences back to theaters as television was beginning to cause losses to film studios. Despite mixed reviews, the film was Monroe's biggest box office success at that point in her career, earning $8 million in world rentals.|$|E
5000|$|Bridgette Wilson as Sahara, a <b>naive</b> <b>model</b> with dyed {{blonde hair}} ...|$|E
5000|$|We {{consider}} this example in more detail. A first <b>naive</b> <b>model</b> is to presuppose {{that there are}} [...] clusters of normally distributed velocities with common known fixed variance [...] Denoting {{the event that the}} th observation is in the th cluster as [...] we can write this model as: ...|$|E
40|$|Many {{researchers}} {{appear to}} {{operate under the}} impression that causal models lead to more accurate forecasts than those provided by <b>naive</b> <b>models</b> (or “projections”). This study was {{based on the premise that}} causal models lead to better forecasts than do <b>naive</b> <b>models</b> in certain situations. The key element of these situations is that there are “large changes. ” One situation where large changes might be expected is that of long-range forecasting—and, in particular, long-range forecasting for international markets. Recent improvements in the quality and availability of international data have substantially reduced the cost of developing causal models in this situation. A study of camera markets in seventeen countries indicated that the margin of superiority of causal <b>models</b> over <b>naive</b> <b>models</b> is of great practical importance...|$|R
40|$|How {{do people}} learn {{intuitive}} {{models of the}} world from experience? We describe a simulation that uses analogical generalization to learn <b>naïve</b> <b>models</b> of pushing and blocking from experience. Experiences are represented by a type of comic strip, consisting of sequences of sketches and simplified English that are automatically encoded by the simulation. We show that the models it learns are compatible with <b>naïve</b> <b>models</b> found in the literature, and analyze the effects of presentation order...|$|R
50|$|For {{some types}} of {{probability}} <b>models,</b> <b>naive</b> Bayes classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for <b>naive</b> Bayes <b>models</b> uses the method of maximum likelihood; in other words, one can work with the <b>naive</b> Bayes <b>model</b> without accepting Bayesian probability or using any Bayesian methods.|$|R
5000|$|Gruen's {{achievements}} {{in nine years}} at Monash include leading a major long range forecasting study on Australian agriculture funded by the US Department of Agriculture. Though John Freebairn subsequently tested its price projections for 1970 and found them [...] "neither significantly more or less accurate than the <b>naïve</b> <b>model</b> price forecasts" [...] the study achieved worthwhile technical advances which later contributed to the building of Australia's ORANI model mostly at Monash University.|$|E
5000|$|The {{paradox is}} that rent-seekers wanting {{political}} favors can bribe politicians {{at a cost}} {{much lower than the}} value of the favor to the rent-seeker. For instance, a rent seeker who hopes to gain a billion dollars from a particular political policy may need to bribe politicians only to the tune of ten million dollars, which is about 1% of the gain to the rent-seeker. Luigi Zingales frames it by asking, [...] "Why is there so little money in politics?" [...] because a <b>naive</b> <b>model</b> of political bribery and/or campaign spending should result in beneficiaries of government subsidies being willing to spend an amount up to the value of the subsidies themselves, when in fact {{only a small fraction of}} that is spent.|$|E
50|$|Sarah Owens (Cassidy Rae)Sarah Owens came to L.A. {{from the}} Midwest to fulfill {{her dream of}} being a model. When she arrived at Models Inc., Hillary quickly took her under her wing and let Sarah move into the beach house where the top models of the agency lived. Sarah quickly became enemies with housemate Julie Dante, who felt {{threatened}} by her youth and fresh image. Sarah was an innocent, <b>naive</b> <b>model</b> who {{ended up on the}} wrong side of the tracks with alcohol and drugs, but by the end of the series, she seemed to be on the way to recovery when she checked herself into a drug rehab clinic after an intervention from her visiting father. The character of Sarah Owens was first introduced on Melrose Place with Hillary Michaels.|$|E
40|$|Abstract. Classification {{problems}} {{have a long}} history in the machine learning literature. One of the simplest, and yet most consistently well performing set of classifiers is the <b>Naïve</b> Bayes <b>models.</b> However, an inherent problem with these classifiers is the assumption that all attributes used to describe an instance are conditionally independent given the class of that instance. When this assumption is violated (which is often the case in practice) it can reduce classification accuracy due to “information double-counting ” and interaction omission. In this paper we focus on a relatively new set of <b>models,</b> termed Hierarchical <b>Naïve</b> Bayes <b>models.</b> Hierarchical <b>Naïve</b> Bayes <b>models</b> extend the <b>modeling</b> flexibility of <b>Naïve</b> Bayes <b>models</b> by introducing latent variables to relax some of the independence statements in these models. We propose a simple algorithm for learning Hierarchical <b>Naïve</b> Bayes <b>models</b> in the context of classification. Experimental results show that the learned models can significantly improve classification accuracy as compared to other frameworks...|$|R
40|$|The aim of {{this study}} is to {{estimate}} the effect of education on the probability of married Malawian women using modern contraceptive methods by accounting for both observed and unobserved confounders. We conduct a sensitivity analysis and compare the results of <b>naive</b> <b>models</b> with instrumental variable models to account for the potential endogeneity of education. Our findings demonstrate conflicting results between the two <b>modelling</b> approaches. The <b>naive</b> <b>models</b> report smaller education effects on the probability of using modern contraceptive methods compared to instrumental variable models. We also find that by relaxing the functional form assumption on the effect of continuous covariates, using a flexible instrumental variable model, the education's effect follows a positive, nonlinear pattern. This finding is not observed with a classic instrumental variable model...|$|R
40|$|The <b>Naïve</b> Bayes <b>Model</b> is {{a special}} case of Bayesian {{networks}} with strong independence assumptions. It is typically used for classification problems. The <b>Naïve</b> Bayes <b>model</b> is trained using the given data to estimate the parameters necessary for classification. This model of classification is very popular since it is simple yet efficient and accurate. While the <b>Naïve</b> Bayes <b>model</b> is considered accurate {{on most of the}} problem instances, there is a set of problems for which the Naïve Bayes does not give accurate results when compared to other classifiers such as the decision tree algorithms. One reason for it could be the strong independence assumption of the <b>Naïve</b> Bayes <b>model.</b> This project aims at searching for dependencies between the features and studying the consequences of applying these dependencies in classifying instances. We propose two different algorithms, the Backward Sequential Joining and the Backward Sequential Elimination that can be applied in order to improve the accuracy of the <b>Naïve</b> Bayes <b>model.</b> We then compare the accuracies of the different algorithms and derive conclusion based on the results...|$|R
5000|$|In September, Monroe {{made her}} {{television}} {{debut in the}} Jack Benny Show, playing Jack's fantasy woman in the episode [...] "Honolulu Trip". She co-starred with Betty Grable and Lauren Bacall in her third movie of the year, How to Marry a Millionaire, which was released in November. It featured Monroe {{in the role of}} a <b>naïve</b> <b>model</b> who teams up with her friends to find rich husbands, repeating the successful formula of Gentlemen Prefer Blondes. It was the second film ever released in CinemaScope, a widescreen format that Fox hoped would draw audiences back to theaters as television was beginning to cause losses to film studios. Despite mixed reviews, the film was Monroe's biggest box office success at that point in her career, earning $8 million in world rentals.|$|E
50|$|Joe is an {{aspiring}} actor {{working as a}} bus boy in a high-class restaurant. His longtime girlfriend Mary works as a cosmetician for the fashion industry and largely supports him with her steady income. Joe {{is more concerned with}} expressing himself than getting a paying job, and has been unwilling to accept roles that do not live up to his artistic standard. Mary supports Joe, but urges him to accept any role to get his foot in the door. Meanwhile, his co-worker Bob lands a lucrative role on a soap opera. Bob is a classically trained actor, but is willing to overlook the quality of the material for the money. He also has a fetish for natural blonde women, leading him to date Sahara, a <b>naive</b> <b>model,</b> and then dump her after discovering that her hair is dyed.|$|E
5000|$|With this in hand we {{can better}} {{understand}} the computational merits of the Dirichlet process. Suppose {{that we wanted to}} draw [...] observations from the <b>naive</b> <b>model</b> with exactly [...] clusters. A simple algorithm for doing this would be to draw [...] values of [...] from , a distribution [...] from [...] and then for each observation independently sample the cluster [...] with probability [...] and the value of the observation according to [...] It is easy to see that this algorithm does not work in case where we allow infinite clusters because this would require sampling an infinite dimensional parameter [...] However, it is still possible to sample observations [...] One can e.g. use the Chinese restaurant representation described below and calculate the probability for used clusters and a new cluster to be created. This avoids having to explicitly specify [...] Other solutions are based on a truncation of clusters: A (high) upper bound to the true number of clusters is introduced and cluster numbers higher than the lower bound are treated as one cluster.|$|E
40|$|International audienceInspired by genetic {{programming}} (GP), we study iterative algorithms for non-computable tasks and {{compare them to}} <b>naive</b> <b>models.</b> This framework justifies many practical standard tricks from GP and also provides complexity lower-bounds which justify the computational cost of GP thanks {{to the use of}} Kolmogorov's complexity in bounded time...|$|R
40|$|Classification {{problems}} {{have a long}} history in the machine learning literature. One of the simplest, and yet most consistently well performing set of classifiers is the <b>Naive</b> Bayes <b>models.</b> However, an inherent problem with these classifiers is the assumption that all attributes used to describe an instance are conditionally independent given the class of that instance. When this assumption is violated (which is often the case in practice) it can reduce classification accuracy due to ``information double-counting'' and interaction omission. In this paper we focus on a relatively new set of <b>models,</b> termed Hierarchical <b>Naive</b> Bayes <b>models.</b> Hierarchical <b>Naive</b> Bayes <b>models</b> extend the <b>modelling</b> flexibility of <b>Naive</b> Bayes <b>models</b> by introducing latent variables to relax some of the independence statements in these models. We propose a simple algorithm for learning Hierarchical <b>Naive</b> Bayes <b>models</b> in the context of classification. Experimental results show that the learned models can significantly improve classification accuracy as compared to other frameworks. Furthermore, the algorithm gives an explicit semantics for the latent structures (both variables and states), which enables the user to reason about the classification of future instances and thereby boost the user's confidence in the model used...|$|R
30|$|As {{shown in}} Table  5, the best {{accuracy}} and Kappa index of Naïve base and <b>Naïve</b> base kernel <b>models</b> gained when they ran on Maximum Relevance dataset (77 %), {{and again the}} indices were lower. The results confirmed that <b>Naïve</b> base <b>model</b> was is better than Naïve base kernel.|$|R
30|$|In track {{precipitation}} forecasting, the <b>naïve</b> <b>model</b> (simple linear regression) {{is introduced}} and {{a comparison of}} the forecast performances of the seasonal and global models and the <b>naïve</b> <b>model</b> (Mason and Baddour 2008; Gabriele and Francesco 2012) is done. In cross-validation precipitation forecasting, time, t, is taken as the covariate variable for seasonal precipitation forecasting, called the time model. Comparison is done for evaluating the forecasting performances of seasonal and global models as well as the time model.|$|E
40|$|The out-of-sample {{forecasting}} {{performance of}} error-correction models of exchange rates is tested on recent monthly data and on annual data from 1900 to 1995. The {{results for the}} monthly data set strongly favor the <b>naive</b> <b>model,</b> even when the series are pooled. In the annual data, the best model is usually a regression model of some form, {{but there is no}} evidence that a researcher can pick a regression model that outpredicts a <b>naive</b> <b>model</b> more often than not, either by choosing at random or by selecting the model that best fits past data. ...|$|E
40|$|Contingent {{processing}} {{occurs when}} agents select computational procedures for making decisions contingent on specific {{features of the}} decision environment they face. We illustrate contingent processing in a simple strategic situation—the well-known takeover game where the <b>naive</b> <b>model</b> of bidding provides {{an explanation of the}} winner’s curse. We interpret the <b>naïve</b> <b>model</b> as a simplification procedure used when the complexity of the takeover game is sufficiently great in a well-defined way we call type complexity. Experimental variation of type complexity across takeover games changes the frequency of naïve simplification, consistent with contingent processing theories of decision cognition...|$|E
40|$|Combining <b>naive</b> {{forecasting}} <b>models</b> as {{an alternative}} to using any one model has shown promise for improving forecasting accuracy at reasonable additional cost. This study extends previous combination modeling by selectively including models in combination rather than using all individual models in a set, evaluating six new model combinations and comparing them to three previously investigated model combinations, investigating the impact of forecasting time horizon, and investigating alternative error measures to mean square error (MSE). Results support using a model-combination that (1) selects the best 3 to 5 models from the 10 models studied and (2) weights the selected models based upon the inverse proportion of their individual accuracy as measured by MSE. forecasting: <b>naive</b> <b>models,</b> forecasting: combination modeling...|$|R
40|$|Agricultural outlook {{baseline}} projections {{published by}} FAPRI, USDA and the OECD are intended as policy analysis tools. It is shown that projections {{have been used}} as forecasts. Therefore, users of baseline projections need information about the past performance of baseline projections. This thesis measures the projection errors of baseline projections using RMSE and MAPE and assesses the ability of models to predict directional movements. The baseline model results are compared to corresponding results of <b>naive</b> <b>models.</b> For projection errors, a mixed model is used to estimate the mean bias and confidence intervals about the mean. Projections of corn, soybean, wheat, beef and pork exports, corn, soybean and wheat farm prices and direct government farm payments are analyzed. Results from the baseline models are compared to the corresponding results from <b>naive</b> <b>models.</b> This analysis shows that baseline projections are not consistently superior to <b>naive</b> <b>models</b> in projecting the future export and price levels. For soybean, beef and pork exports and for corn, soybean and wheat prices, baseline projections have been shown to perform relatively well. However, baseline projections for corn and wheat exports and government payments models have relatively large RMSE and bias. It is important for the users of baseline projections to understand the assumptions and structure of the models. Projection users and funding agencies {{need to be aware of}} the magnitude and direction of projection errors and bias. Modelers need to update and refine models based on the results of past projection analyses to produce better projections given the baseline assumptions. Additionally, organizations should more strongly explain the proper use and limitations of baseline projections...|$|R
40|$|The {{effect of}} adding noise to both an {{equilibrium}} <b>model</b> and a <b>naive</b> Bayesian <b>model</b> of behavior in step-level public good games is studied. Quantal response equilibria are derived for these games and a naive Bayesian quantal response function is presented. The models are fit for experimental data {{from such a}} game and compared. The results seem more promising for the <b>naive</b> Bayesian <b>model</b> than for the equilibrium model...|$|R
40|$|This paper {{compares the}} results from data envelopment {{analysis}} (DEA) to a naïve efficiency measurement model, which generates a scalar efficiency score by averaging all output-input ratios. Random data and real-life data are {{used to test the}} relative performance of the <b>naïve</b> <b>model</b> against various DEA models. The results suggest that the proposed the <b>naïve</b> <b>model</b> replicates DEA efficiency scores almost perfectly for constant return-to-scales and low heterogeneity in output-input data. It is therefore concluded that heterogeneity in output-input data is important {{to take advantage of the}} capability of DEA. It is also shown that heterogeneity is more relevant to efficiency measurement than the number of dimensions. Data envelopment analysis Efficiency analysis Robustness and sensitivity analysis...|$|E
40|$|In {{this report}} we examine a data set from TrønderTaxi, which {{contains}} information about all phone calls their call center received from 1 March 2014 to 31 January 2016. We model {{the number of}} phone calls received per 30 minute time interval using two different models. The first model is a seasonal autoregressive integrated moving average model, or SARIMA model. The second model is a <b>naive</b> <b>model,</b> which treats all of the weeks of the data set as independent and identically distributed. We use both the SARIMA and naive models to make predictions of both one and two weeks into the future. The {{results show that the}} SARIMA model appears to make slightly better predictions for one week into the future, while the <b>naive</b> <b>model</b> is better for more distant predictions...|$|E
40|$|An {{important}} {{prerequisite for}} traffic management {{is to find}} efficient ways to model and predict traffic flow. Here we are presenting a <b>naïve</b> <b>model</b> for the route choice adaptation of learning commuters with heuristics based behaviour. Our simulation {{results show that the}} heuristics learnt lead to a situation similar to that obtained in real experiments...|$|E
40|$|At present, {{most of the}} precipitation’s level {{predictions}} use {{the laws}} of nature to build the mathematical model which contains one or more series level to carry out the numerical simulation, as thus to analyze the causes and consequences of the evolution. Bayesian model is one kind of the foregoing said. In the Bayesian classification <b>model,</b> <b>Naive</b> Bayes <b>model</b> is known for its stability and easy to operate, but the established precedent assumption tends to be inadmissible. So here the article proposed a new precipitation’s level prediction model based on the tree Augmented Naïve Bayes(we called TAN model for short hereafter), which improve the original <b>Naïve</b> Bayes <b>model</b> defects and increase the association between the leaf nodes {{on the basis of the}} original model. And we use the Dongtai station, Jiangsu province meteorology data to test the new precipitation model. The results show that the new precipitation prediction model’s performance is superior to the traditional <b>Naive</b> Bayes <b>model...</b>|$|R
5000|$|Convergence is {{determined}} based on improvement {{to the model}} likelihood , where [...] denotes {{the parameters of the}} <b>naive</b> Bayes <b>model.</b>|$|R
40|$|The {{centering}} framework explains local coherence by relating {{local focus}} and {{the form of}} referring expressions. It has proven useful in monolog, but its utility for multiparty discourse has not been shown, {{and a variety of}} issues must be tackled to adapt the model for dialog. This paper reports our application of three <b>naive</b> <b>models</b> of centering theory for dialog. These results will be used as baselines for evaluating future models. 1...|$|R
40|$|We use {{biannual}} microdata (1952 - 89) {{provided by}} Joseph Livingston's surveys on stock price expectations at the New York Stock Exchange (Standard and Poor's Industrial Index). We show that experts generate their forecasts neither rationally nor {{according to a}} <b>naive</b> <b>model,</b> but using a weighted average of the three traditional extrapolative, regressive and adaptive expectation processes...|$|E
40|$|This paper {{evaluates the}} {{predictability}} of monthly stock return using out-of-sample (multi-step ahead and dynamic) prediction intervals. Past studies have exclusively used point forecasts, which are of limited value since they carry {{no information about}} the intrinsic predictive uncertainty associated. We compare empirical performances of alternative prediction intervals for stock return generated from a <b>naive</b> <b>model,</b> univariate autoregressive model, and multivariate model (predictive regression and VAR), using the U. S. data from 1926. For evaluation free from data snooping bias, we adopt moving sub-sample windows of different lengths. It is found that the <b>naive</b> <b>model</b> often provides the most informative prediction intervals, outperforming those generated from the univariate model and multivariate models incorporating a range of economic and financial predictors. This strongly suggests that the U. S. stock market has been informationally efficient in the weak-form {{as well as in}} the semi-strong form, subject to the information set consideredin this study...|$|E
40|$|Indian {{convertible}} bonds have two peculiar features {{that make them}} possibly unique in the world: a) the bonds are compulsorily converted into equity without any option, and b) the conversion terms are not specified {{at the time of}} issue but are left to be determined subsequently by the Controller of Capital Issues (CCI) who is the government functionary regulating capital issues in India. A <b>naive</b> <b>model</b> would say that the market simply forms an estimate of the likely conversion terms and then values the bond as if these terms were prespecified. However, the empirical investigation reported in a companion paper, Barua, Madhavan and Varma (1991) convincingly rejects the <b>naive</b> <b>model</b> and makes a more sophisticated model necessary. In this paper, we use the general theory of derivative securities (Cox, Ingersoll and Ross, 1985) to obtain a closed form expression for the value of the Indian convertible bond. The testable implication...|$|E
40|$|In recent years, mixture {{models have}} found {{widespread}} usage in discovering latent cluster structure from data. A popular special case of finite mixture <b>models</b> are <b>naive</b> Bayes <b>models,</b> where {{the probability of}} a feature vector factorizes over the features for any given component of the mixture. Despite their popularity, <b>naive</b> Bayes <b>models</b> suffer from two important restrictions: first, {{they do not have a}} natural mechanism for handling sparsity, where each data point may have only a few observed features; and second, they do not allow objects to be generated from different latent clusters with varying degrees (i. e., mixed-memberships) in the generative process. In this paper, we first introduce marginal <b>naive</b> Bayes (MNB) <b>models,</b> which generalize <b>naive</b> Bayes <b>models</b> to handle sparsity by marginalizing over all missing features. More importantly, we propose mixed-membership <b>naive</b> Bayes (MMNB) <b>models,</b> which generalizes (marginal) <b>naive</b> Bayes <b>models</b> to allow for mixed memberships in the generative process. MMNB models can be viewed as a natural generalization of latent Dirichlet allocation (LDA) with the ability to handle heterogenous and possibly sparse feature vectors. We propose two variational inference algorithms to learn MMNB models from data. While the first exactly follows the corresponding ideas for LDA, the second uses much fewer variational parameters leading to a much faster algorithm with smaller time and space requirements. An application of the same idea in the context of topic modeling leads to a new Fast LDA algorithm. The efficacy of the proposed mixed-membership models and the fast variational inference algorithms are demonstrated by extensive experiments on a wide variety of different datasets. ...|$|R
40|$|In {{this paper}} we {{introduce}} the Wastewater Treatment Plant Problem, a real-world scheduling problem, {{and compare the}} performance of several tools on it. We show that, for a <b>naive</b> <b>modeling,</b> state-of-the-art SMT solvers outperform other tools ranging from mathematical programming to constraint programming. We use both real and randomly generated benchmarks. From this and similar results, we claim {{for the convenience of}} developing compiler front-ends being able to translate from constraint programming languages to the SMT-LIB standard language...|$|R
40|$|The naïve Bayes {{approach}} is a simple but often satisfactory method for supervised classification. In this paper, {{we focus on the}} <b>naïve</b> Bayes <b>model</b> and propose the application of regularization techniques to learn a naïve Bayes classifier. The main contribution of the paper is a stagewise version of the selective naïve Bayes, which can be considered a regularized version of the <b>naïve</b> Bayes <b>model.</b> We call it forward stagewise naïve Bayes. For comparison’s sake, we also introduce an explicitly regularized formulation of the <b>naïve</b> Bayes <b>model,</b> where conditional independence (absence of arcs) is promoted via an L 1 /L 2 -group penalty on the parameters that define the conditional probability distributions. Although already published in the literature, this idea has only been applied for continuous predictors. We extend this formulation to discrete predictors and propose a modification that yields an adaptive penalization. We show that, whereas the L 1 /L 2 group penalty formulation only discards irrelevant predictors, the forward stagewise naïve Bayes can discard both irrelevant and redundant predictors, which are known to be harmful for the naïve Bayes classifier. Both approaches, however, usually improve the classical <b>naïve</b> Bayes <b>model’s</b> accuracy...|$|R
