7|17|Public
40|$|SUMMARY This paper {{analytically}} formulates {{both the}} optimal quantization <b>noise</b> <b>allocation</b> ratio and the coding gain of the two-dimensional morphological Haar wavelet transform. The two-dimensional morphological Haar wavelet transform {{has been proposed}} as a nonlinear wavelet transform. It has been anticipated for application to nonlinear transform coding. To utilize a transformation to transform coding, both the optimal quantization <b>noise</b> <b>allocation</b> ratio and the coding gain of the transformation should be derived beforehand {{regardless of whether the}} transformation is linear or nonlinear. The derivation is crucial for progress of nonlinear transform image coding with nonlinear wavelet because the two-dimensional morphological Haar wavelet is the most basic nonlinear wavelet. We derive both the optimal quantization <b>noise</b> <b>allocation</b> ratio and the coding gain of the two-dimensional morphological Haar wavelet transform by introducing appropriate approximations to handle the cumbersome nonlinear operator included in the transformation. Numerical experiments confirmed the validity of formulations. key words: coding gain, morphological Haar wavelet, nonlinear transform coding, nonlinear wavelet, optimal quantization <b>noise</b> <b>allocation</b> 1...|$|E
40|$|Using {{local or}} non-local {{features}} {{has proven to}} be a competent approach for denoising images. As noise and edges have similar effect of changes in gradient in many cases, <b>noise</b> <b>allocation</b> for denoising is still significant challenge. This work addresses the classic problem but introducing the combination concept of local and non-local factors with deviation refinement procedure. A new algorithm of the concept is proposed to ameliorate noise reduction. Sensitivity of noise detection is examined by iterative non-local mean and bilateral filter with refinement of range deviation. The final methodology is tested with Gaussian noise and compared with both non-local mean, bilateral filter. Experiment demonstrates improvement of denoising level in the new algorithm...|$|E
40|$|To all my teachers, and {{especially}} to my parents, {{the best of them}} all. iii With the advent of applications such as mobile systems and sensor networks, low power wireless receivers have become the need of the hour. In this thesis two low power design techniques for the front end (LNA-Mixer) of a Radio Frequency(RF) communication receiver are explored. First we address the problem of optimal power and <b>noise</b> <b>allocation</b> among the analog and digital sections of the radio receiver, which minimizes the power consump-tion of the overall RF receiver while mainitaining the Bit Error Rate(BER) constraint. We propose a methodology to arrive at the power-optimal noise and linearity specifi-cations of all the components of the radio receiver, including that of the digital base band. Next we discuss the concept of adaptive receivers, which adapt their performanc...|$|E
40|$|This paper investigates secure multi-antenna {{transmission}} in {{slow fading}} channels without the eavesdropper's channel state information. The {{use of multiple}} transmit antennas enables the transmitter to strengthen the signal reception at the intended receiver while simultaneously confusing the eavesdropper by delivering artificial noise. A recently developed secrecy outage formulation, which can separately measure the quality of service {{and the level of}} security, is used to characterize the security performance. We show that an arbitrarily low secrecy outage probability cannot be achieved by adding more transmit antennas alone without optimizing other system parameters. To facilitate the practical system design, we present an on-off transmission scheme with optimal artificial <b>noise</b> power <b>allocation,</b> which minimizes the secrecy outage probability whilst guaranteeing a minimum required quality of service. © 2011 IEEE...|$|R
40|$|In this work, we take an {{information}} theoretic approach {{to analyze the}} watermark communication problem {{in the presence of}} perceptual coding. Our effective watermark channel is modeled as a set of parallel independent zero-mean uniformly distributed additive <b>noise</b> channels. Energy <b>allocation</b> principles are identified to maximize the capacity results. Our findings are compared to the traditional water-filling solution and shed light on strategies to maximize the data hiding rate in the presence of compression. 1...|$|R
5000|$|In public economics, he {{developed}} the demand revealing mechanism for public project selection which was noted in the Nobel Committee's award of the 1996 Nobel Prize in Economics to William Vickrey. Clarke's 1994 TRB paper (with Wayne Brough and Nicolaus Tideman), entitled [...] "Airport Congestion and <b>Noise,</b> Interplay of <b>Allocation</b> and Distribution" [...] illustrates the potential applicability of the Vickrey and Clarke/Groves demand revealing mechanisms to problems of transportation congestion. This method {{was the subject of}} Nobel Prizes awarded in 2007.|$|R
40|$|A central {{problem in}} {{releasing}} aggregate information about sensitive data {{is to do}} so accurately while providing a privacy guarantee on the output. Recent work focuses on the class of linear queries, which include basic counting queries, data cubes, and contingency tables. The goal is to maximize the utility of their output, while giving a rigorous privacy guarantee. Most results follow a common template: pick a "strategy" set of linear queries {{to apply to the}} data, then use the noisy answers to these queries to reconstruct the queries of interest. This entails either picking a strategy set that is hoped to be good for the queries, or performing a costly search over the space of all possible strategies. In this paper, we propose a new approach that balances accuracy and efficiency: we show how to improve the accuracy of a given query set by answering some strategy queries more accurately than others. This leads to an efficient optimal <b>noise</b> <b>allocation</b> for many popular strategies, including wavelets, hierarchies, Fourier coefficients and more. For the important case of marginal queries we show that this strictly improves on previous methods, both analytically and empirically. Our results also extend to ensuring that the returned query answers are consistent with an (unknown) data set at minimal extra cost in terms of time and noise...|$|E
40|$|Abstract — A central {{problem in}} {{releasing}} aggregate information about sensitive data {{is to do}} so accurately while providing a privacy guarantee on the output. Recent work focuses on the class of linear queries, which include basic counting queries, data cubes, and contingency tables. The goal is to maximize the utility of their output, while giving a rigorous privacy guarantee. Most results follow a common template: pick a “strategy ” set of linear queries {{to apply to the}} data, then use the noisy answers to these queries to reconstruct the queries of interest. This entails either picking a strategy set that is hoped to be good for the queries, or performing a costly search over the space of all possible strategies. However, once the strategy is fixed, its evaluation can be done efficiently, using standard linear algebraic methods. In this paper, we propose a new approach that balances accuracy and efficiency: we show how to optimize the accuracy of a given strategy by answering some strategy queries more accurately than others, based on the target queries. This leads to an efficient optimal <b>noise</b> <b>allocation</b> for many popular strategies, including wavelets, hierarchies, Fourier coefficients and more. For the important case of marginal queries (equivalently, subsets of the data cube), we show that this strictly improves on previous methods, both analytically and empirically. Our results also extend to ensuring that the returned query answers are consistent with an (unknown) data set at minimal extra cost in terms of time and noise. I...|$|E
40|$|Digital audio {{compression}} allows the efficient storage and transmission of audio data. The various {{audio compression}} techniques offer {{different levels of}} complexity, audio quality, and amount of data compression. The MPEG audio algorithm {{is a kind of}} lose compression. It is a psycho-acoustic algorithm that provides signal-to-mask ratio for bits or <b>noise</b> <b>allocation</b> and bit stream formatting. MPEG- 1 standard enables coding of a single- or two-channel sound at the sampling frequencies of 32, 44. 1 and 48 kHz. The principles and features of MPEG algorithm are described and a Matlab implementation is presented. Then some special features of this compression algorithm are dealt with. We describe a variable bit rate (VBR) coding algorithm proposal with the possibility of implementation in MPEG- 1 Audio standard, especially in layer 1. VBR coding means that the bit rate is not constant at the output of a coder like in MPEG- 1 Audio standard bit stream but it alters according to the input information content of the compressed signal and the maximum distortion allowed. We did listening tests for the proposed algorithm that validate its accuracy. Low-bit rate coding, particularly MPEG- 1, is being used {{in a wide range of}} state-of-the-art audio applications such as digital audio broadcasting or network conferencing, too. It has been found that audio quality can degrade markedly if multiple stages of encoding and decoding are used. The usual approach to designing audio codes that preserve the quality in cascaded arrangement consists in transferring some additional information in the decoded audio signal that enables to correct the growing distortion. The experiments indicate that the quality degradation problem can be caused by improper quantization characteristic in case of quantization with 2 bits. We altered standard quantization in the encoder by changing allocated scale factor at the nearest higher. Thereby the distortion was considerably decreased. The condition of this improvement is the synchronization of codes. Available from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|E
40|$|Abstract—In this paper, we {{investigate}} {{the design of}} artificial-noise-aided secure multi-antenna transmission in slow fading channels. The primary design concerns include the transmit power allocation and the rate parameters of the wiretap code. We consider two scenarios with different complexity levels: i) the design parameters are chosen to be fixed for all transmissions, ii) they are adaptively adjusted based on the instantaneous channel feedback from the intended receiver. In both scenarios, we provide explicit design solutions for achieving the maximal throughput subject to a secrecy constraint, given by a maximum allowable secrecy outage probability. We then derive accurate approximations for the maximal throughput in both scenarios in the high signal-to-noise ratio region, and give {{new insights into the}} additional power cost for achieving a higher security level, whilst maintaining a specified target throughput. In the end, the throughput gain of adaptive transmission over non-adaptive transmission is also quantified and analyzed. Index Terms—Physical-layer security, multi-antenna transmis-sion, artificial <b>noise,</b> power <b>allocation,</b> secrecy outage probability, throughput optimization. I...|$|R
40|$|Abstract—We study physical-layer {{security}} in wireless ad hoc networks and investigate {{two types of}} multi-antenna transmission schemes for providing secrecy enhancements. To establish secure transmission against malicious eavesdroppers, we consider the generation of artificial noise with either sectoring or beamform-ing. For both approaches, we provide a statistical characterization and tradeoff analysis of the outage performance of the legitimate communication and the eavesdropping links. We then investi-gate the networkwide secrecy throughput performance of both schemes {{in terms of the}} secrecy transmission capacity, and study the optimal power allocation between the information signal and the artificial noise. Our analysis indicates that, under transmit power optimization, the beamforming scheme outperforms the sectoring scheme, except for the case where the number of transmit antennas are sufficiently large. Our study also reveals some interesting differences between the optimal power allocation for the sectoring and beamforming schemes. Index Terms—Physical-layer security, ad hoc networks, multi-antenna transmission, artificial <b>noise,</b> power <b>allocation,</b> outage probability, throughput optimization. I...|$|R
40|$|National audienceOptical Network-on-Chip (ONoC) employs Wavelength Division Multiplexing (WDM) {{supporting}} multiple transactions at same time. In this context, many senders and receivers {{can share}} the same waveguide to increase the total bandwidth utilization. However, simultaneous transmissions on closed adjacent wavelengths may introduce crosstalk noise through different optical switching elements within the network due to the low wavelength channel spacing and the large Full Width at Half Maximum (FWHM) of the resonance pic of the Microring Res-onator (MR). To address this problem, this paper proposes crosstalk <b>noise</b> aware wavelength <b>allocation</b> in WDM 3 D ONoC to improve Signal to Noise Ratio (SNR) performance...|$|R
40|$|International audienceThe {{problem of}} bit rate {{maximization}} of a linear precoded orthogonal frequency division multiplexing ({LP-OFDM}) system is considered for imperfect channel state information. The discrete bit loading algorithms are proposed, which sustain the target {{bit error rate}} even under high mean square error (MSE) of estimation. The proposed scheme enhances the robustness of the system against noisy channel estimation without significantly compromising on the system throughput. The results are shown for a power line communication system using a well-known multipath channel model. It is shown that the proposed {LP-OFDM} allocation is more robust to estimation <b>noise</b> than {OFDM} <b>allocations</b> and provides higher throughputs even at high MSEs...|$|R
40|$|ISBN: 978 - 1 - 4244 - 4473 - 1 International audienceThe {{resource}} allocation problem {{to maximize the}} bit rate of a linear precoded orthogonal frequency division multiplexing ({LP-OFDM}) system is considered {{taking into account the}} effects of imperfect channel state information. A discrete bit loading algorithm is proposed, which sustains the target bit error rate even under high mean square error (MSE) of estimation. The proposed scheme enhances the robustness of the system against noisy channel estimation without significantly compromising on the system throughput. It is shown that the proposed {LP-OFDM} allocation is more robust to estimation <b>noise</b> than {OFDM} <b>allocations</b> and provides sustainable mean bit error rate performance even at high MSEs. The results are shown for a power line communication system using a well-known multipath channel model...|$|R
40|$|In {{this paper}} we address the question: Is water-filling {{appropriate}} for watermarking? The water-filling paradigm is a traditional solution to the capacity maximization of parallel zero-mean additive white Gaussian channels subject to a signal energy constraint. In this work, we take an information theoretic approach to analyze the watermark communication problem {{in the presence of}} perceptual coding. Our effective watermark channel is modeled as a set of parallel independent zero-mean uniformly distributed additive <b>noise</b> channels. Energy <b>allocation</b> principles are identified to maximize the capacity results. Our findings are compared to the traditional water-filling solution and shed light on strategies to maximize the data hiding rate in the presence of compression. 1. INTRODUCTION The research area of watermarking has resulted in an incredible number of algorithmic proposals suited to the diverse needs of multimedia industries. One can easily be overwhelmed by the sheer number of mod [...] ...|$|R
40|$|This paper {{studies the}} problem of power {{allocation}} in compressed sensing when different components in the unknown sparse signal have different probability to be non-zero. Given the prior information of the non-uniform sparsity and the total power budget, {{we are interested in}} how to optimally allocate the power across the columns of a Gaussian random measurement matrix so that the mean squared reconstruction error is minimized. Based on the state evolution technique originated from the work by Donoho, Maleki, and Montanari, we revise the so called approximate message passing (AMP) algorithm for the reconstruction and quantify the MSE performance in the asymptotic regime. Then the closed form of the optimal power allocation is obtained. The results show that in the presence of measurement <b>noise,</b> uniform power <b>allocation,</b> which results in the commonly used Gaussian random matrix with i. i. d. entries, is not optimal for non-uniformly sparse signals. Empirical results are presented to demonstrate the performance gain. Comment: 5 pages, 3 figure...|$|R
40|$|Sophisticated audio coding {{paradigms}} incorporate human perceptual {{effects in}} order to reduce data rates, while maintaining high fidelity of the reconstructed signal. Auditory masking is the phenomenon that is the key to exploiting perceptual redundancy in audio signals. Most auditory models conservatively estimate masking, as they were developed for medium to high rate coders where distortion can be made inaudible. At very low coding rates, more accurate auditory models will be beneficial since some audible distortion is inevitable. This thesis focuses on the application of human perception to low-rate audio coding. A novel auditory model that estimates masking levels is proposed. The new model is based on a study of existing perceptual literature. Among other features, it represents transient masking effects by tracking the temporal evolution of masking components. Moreover, an innovative bit allocation algorithm is developed that considers the excitation of quantization <b>noise</b> in the <b>allocation</b> process. The new adaptive allocation scheme is applicable with any auditory model that is based on the excitation pattern model of masking...|$|R
40|$|A new {{compression}} algorithm for fingerprint images is introduced. A modified wavelet packet scheme {{which uses a}} fixed decomposition structure. matched to the statistics of fingerprint images, is used. Based on statistical studies of the subbands, different compression techniques are chosen for different subbands. The decision {{is based on the}} effect of each subband on reconstructed image, taking into account the characteristics of the Human Visual System (HVS). A <b>noise</b> shaping bit <b>allocation</b> procedure which considers the HVS, is then used to assign the bit rate among subbands. Using Lattice Vector Quantization (LVQ), a new technique for determining the largest radius of the Lattice and its scaling factor is presented. The design is based on obtaining the smallest possible Expected Total Distortion (ETD) measure, using the given bit budget. At low bit rates, for the coefficients with high-frequency content, we propose the Positive-Negative Mean (PNM) algorithm to improve the resolution of the reconstructed image. Furthermore, for the coefficients with low-frequency content, a lossless predictive compression scheme is developed. The proposed algorithm results in a high compression ratio and a high reconstructed image quality with a low computational load compared to other available algorithms...|$|R
30|$|The {{problem of}} {{minimizing}} the DL transmit power {{required to meet}} users' SINR constraints by joint optimization of transmit beamforming (BF) vectors and power allocation scalars was first solved in [13] and was later treated in [14, 15] with feasibility issues. These solutions are based upon the duality of uplink (UL) and DL channels. Exploiting this UL-DL duality, iterative algorithms were proposed to find the optimal BF vectors and the optimal power assignments to the users, and the convergence of these algorithms was shown to the optimal solution. For MU channels (either UL or DL) with Gaussian signalling, [15] showed {{that the problem of}} minimizing the transmit power to achieve specific SINR targets bears a relatively simple solution due to the added structure that may be exploited by successive interference cancellation (SIC) in the UL and by DPC-based encoding for known interference in the DL channels, and the results were presented in [15 – 17]. The optimal BF strategy {{turns out to be the}} minimum-mean-square-error (MMSE) solution, where each user will see no interference from the already encoded users, due to DPC-based encoding and each BF treats the interference of unencoded users as extra <b>noise,</b> and power <b>allocation</b> for each user is done to raise its SINR level to the target SINR. Actually, the DL problem is solved by first solving the dual UL problem, due to its relatively simple structure.|$|R
40|$|In this paper, {{we study}} the {{resource}} allocation algorithm design for multiple-input single-output (MISO) multicarrier non-orthogonal multiple access (MC-NOMA) systems, {{in which a}} full-duplex base station serves multiple half-duplex uplink and downlink users on the same subcarrier simultaneously. The resource allocation is optimized for maximization of the weighted system throughput while the information leakage is constrained and artificial noise is injected to guarantee secure communication {{in the presence of}} multiple potential eavesdroppers. To this end, we formulate a robust non-convex optimization problem taking into account the imperfect channel state information (CSI) of the eavesdropping channels and the quality-of-service (QoS) requirements of the legitimate users. Despite the non-convexity of the optimization problem, we solve it optimally by applying monotonic optimization which yields the optimal beamforming, artificial <b>noise</b> design, subcarrier <b>allocation,</b> and power allocation policy. The optimal resource allocation policy serves as a performance benchmark since the corresponding monotonic optimization based algorithm entails a high computational complexity. Hence, we also develop a low-complexity suboptimal resource allocation algorithm which converges to a locally optimal solution. Our simulation results reveal that the performance of the suboptimal algorithm closely approaches that of the optimal algorithm. Besides, the proposed optimal MISO NOMA system can not only ensure downlink and uplink communication security simultaneously but also provides a significant system secrecy rate improvement compared to traditional MISO orthogonal multiple access (OMA) systems and two other baseline schemes. Comment: Submitted for possible publicatio...|$|R
40|$|The overall aim of {{this study}} was to examine {{experimentally}} the effects of noise upon short-term memory tasks in the hope of shedding further light upon the apparently inconsistent results of previous research in the area. Seven experiments are presented. The first chapter of the thesis comprised a comprehensive review of the literature on noise and human performance while in the second chapter some theoretical questions concerning the effects of noise were considered in more detail follovred by a more detailed examination of the effects of noise upon memory. Chapter 3 described an experiment which examined the effects of <b>noise</b> on attention <b>allocation</b> in short-term memory as a function of list length. The results provided only weak evidence of increased selectivity in noise. In further chapters no~effects Here investigated in conjunction vrith various parameters of short-term memory tasks e. g. the retention interval, presentation rate. The results suggested that noise effects were significantly affected by the length of the retention interval but not by the rate of presentation. Later chapters examined the possibility of differential noise effects on the mode of recall (recall v. recognition) and the type of presentation (sequential v. simultaneous) as well as an investigation of the effect of varying the point of introduction of the noise and the importance of individual differences in noise research. The results of this study were consistent with the hypothesis that noise at presentation facilitates phonemic coding. However, noise during recall appeared to affect the retrieval strategy adopted by the subject...|$|R
40|$|Compressed sensing takes {{advantage}} {{that most of}} the natural signals can be sparsely represented via linear transformations, therefore it is possible to have accurate reconstruction from sub-Nyquist samplings. The properties of the measurement matrix directly affect the relation between the sampling rate and the distortion of the reconstructions. People have been trying to either design measurement matrices from the signal statistics, or train the matrices from large amount of similar signals. Hence the relevant techniques they keep developing become very hot research topics. This thesis focuses on discussing the impact of the measurement matrices on representing and sensing sparse signals. The full text is divided into four parts (presented in Chapter 2 to 5, respectively). In Chapter 2 we focus on the dictionary update stage in dictionary learning. Given observations of the sparse signals via an over-complete measurement matrix,, dictionary learning is to find this measurement matrix, i. e., dictionary, to accurately reconstruct the sparse signals. Usually a dictionary learning problem includes two stages that are implemented iteratively, sparse coding and dictionary update. Sparse coding is to fix the dictionary and update the sparse pattern of the estimated sparse signals. Dictionary update is to fix the sparse pattern and update the dictionary. We show that the failure of the update procedure to find a global optimum is not because of their converging to local minima or saddle points but to singular points. Afterwards, against this singularity issue, we revise the original objective function and propose a continuous counterpart. This modification is applied in the SimCO dictionary update framework and can be proved that in the limit case, the new objective function is the best possible lower semi-continuous approximation of the original. In Chapter 3 we present a joint source separation and dictionary learning algorithm to separate the noise corrupted mixed sources. The idea behind is that for our different targeted sources, such as images and audios, have different sparse representations. We choose the deterministic scenarios, where the number of mixtures is not less than that of sources. The technique presented in Chapter 2 to alleviate singularity is used in the algorithm and we use examples to show its benefit. In Chapter 4, we notice that rely on the prior known statistics of the sparse signals, it is possible to allocate the sensing power accordingly to achieve the best possible performance. Given the non-uniform signal sparsity and the total power budget, we study how to optimally allocate the power across the columns of a Gaussian random measurement matrix so as to meet the reconstruction requirements. We revise the so called approximate message passing algorithm and quantify the MSE performance in the asymptotic regime. The obtained closed form of the optimal power allocation shows that in the presence of measurement <b>noise,</b> uniform power <b>allocation</b> is not optimal for non-uniformly sparse signals. In Chapter 5 we study distributed compressed sensing problem. We consider the scenarios where unequal number of measurements can be assigned for each signal block, and look for the optimal measuring rate allocation for recovering the sparse signals with common support. For simplification we assume the signals have Bernoulli-Gaussian distribution and again use AMP for analysis and obtain the exact phase transition curve in an asymptotic region. Interestingly, via the state evolution technique it can be shown that the rate region is concave, suggesting the corner points at the curve are optimal operating points and equal rate allocations is strictly sub-optimal. Besides the rate allocation, we also numerically quantify how the expected reconstruction error is affected by lack of enough measurements, the presence of Gaussian noise and the inter correlation across the signal blocks. Open Acces...|$|R
40|$|The {{main goal}} of this {{research}} is to design an efficient compression al gorithm for fingerprint images. The wavelet transform technique is the principal tool used to reduce interpixel redundancies and to obtain a parsimonious representation for these images. A specific fixed decomposition structure is designed to be used by the wavelet packet in order to save on the computation, transmission, and storage costs. This decomposition structure is based on analysis of information packing performance of several decompositions, two-dimensional power spectral density, effect of each frequency band on the reconstructed image, and the human visual sensitivities. This fixed structure is found to provide the "most" suitable representation for fingerprints, according to the chosen criteria. Different compression techniques are used for different subbands, based on their observed statistics. The decision is based on the effect of each subband on the reconstructed image according to the mean square criteria as well as the sensitivities in human vision. To design an efficient quantization algorithm, a precise model for distribution of the wavelet coefficients is developed. The model is based on the generalized Gaussian distribution. A least squares algorithm on a nonlinear function of the distribution model shape parameter is formulated to estimate the model parameters. A <b>noise</b> shaping bit <b>allocation</b> procedure is then used to assign the bit rate among subbands. To obtain high compression ratios, vector quantization is used. In this work, the lattice vector quantization (LVQ) is chosen because of its superior performance over other types of vector quantizers. The structure of a lattice quantizer is determined by its parameters known as truncation level and scaling factor. In lattice-based compression algorithms reported in the literature the lattice structure is commonly predetermined leading to a nonoptimized quantization approach. In this research, a new technique for determining the lattice parameters is proposed. In the lattice structure design, no assumption about the lattice parameters is made and no training and multi-quantizing is required. The design is based on minimizing the quantization distortion by adapting to the statistical characteristics of the source in each subimage. 11 Abstract Abstract Since LVQ is a multidimensional generalization of uniform quantizers, it produces minimum distortion for inputs with uniform distributions. In order {{to take advantage of the}} properties of LVQ and its fast implementation, while considering the i. i. d. nonuniform distribution of wavelet coefficients, the piecewise-uniform pyramid LVQ algorithm is proposed. The proposed algorithm quantizes almost all of source vectors without the need to project these on the lattice outermost shell, while it properly maintains a small codebook size. It also resolves the wedge region problem commonly encountered with sharply distributed random sources. These represent some of the drawbacks of the algorithm proposed by Barlaud [26). The proposed algorithm handles all types of lattices, not only the cubic lattices, as opposed to the algorithms developed by Fischer [29) and Jeong [42). Furthermore, no training and multiquantizing (to determine lattice parameters) is required, as opposed to Powell's algorithm [78). For coefficients with high-frequency content, the positive-negative mean algorithm is proposed to improve the resolution of reconstructed images. For coefficients with low-frequency content, a lossless predictive compression scheme is used to preserve the quality of reconstructed images. A method to reduce bit requirements of necessary side information is also introduced. Lossless entropy coding techniques are subsequently used to remove coding redundancy. The algorithms result in high quality reconstructed images with better compression ratios than other available algorithms. To evaluate the proposed algorithms their objective and subjective performance comparisons with other available techniques are presented. The quality of the reconstructed images is important for a reliable identification. Enhancement and feature extraction on the reconstructed images are also investigated in this research. A structural-based feature extraction algorithm is proposed in which the unique properties of fingerprint textures are used to enhance the images and improve the fidelity of their characteristic features. The ridges are extracted from enhanced grey-level foreground areas based on the local ridge dominant directions. The proposed ridge extraction algorithm, properly preserves the natural shape of grey-level ridges as well as precise locations of the features, as opposed to the ridge extraction algorithm in [81). Furthermore, it is fast and operates only on foreground regions, as opposed to the adaptive floating average thresholding process in [68). Spurious features are subsequently eliminated using the proposed post-processing scheme...|$|R

