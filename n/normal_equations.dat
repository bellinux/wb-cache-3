542|452|Public
25|$|For {{non-linear}} {{least squares}} systems a similar argument {{shows that the}} <b>normal</b> <b>equations</b> should be modified as follows.|$|E
25|$|In {{some cases}} the (weighted) <b>normal</b> <b>equations</b> matrix XTX is ill-conditioned. When fitting polynomials the <b>normal</b> <b>equations</b> matrix is a Vandermonde matrix. Vandermonde {{matrices}} become increasingly ill-conditioned as {{the order of the}} matrix increases. In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate. Various regularization techniques can be applied in such cases, the most common of which is called ridge regression. If further information about the parameters is known, for example, a range of possible values of , then various techniques can be used to increase the stability of the solution. For example, see constrained least squares.|$|E
2500|$|... and {{the best}} fit can be found by solving the <b>normal</b> <b>equations.</b>|$|E
5000|$|Therefore, the {{minimizing}} vector [...] is {{a solution}} of the <b>normal</b> <b>equation</b> ...|$|R
40|$|AbstractSelecting {{the optimal}} {{reference}} satellite {{is an important}} component of high-precision relative positioning because the reference satellite directly influences the strength of the <b>normal</b> <b>equation.</b> The reference satellite selection methods based on elevation and positional dilution of precision (PDOP) value were compared. Results show that all the above methods cannot select the optimal reference satellite. We introduce condition number of the design matrix in the reference satellite selection method to improve structure of the <b>normal</b> <b>equation,</b> because condition number can indicate the ill condition of the <b>normal</b> <b>equation.</b> The experimental results show that the new method can improve positioning accuracy and reliability in precise relative positioning...|$|R
40|$|Practical {{measurement}} schemes require {{redundant observations}} for quality control and errors checking. This led to inconsistent solution where every subset (minimum required data) gives different results. Least Square Estimation (LSE) {{is a method}} to provide a unique solution (of the <b>normal</b> <b>equation)</b> from redundant observations by minimizing the sum of squares of the residuals. Analysis of LSE also provide estimate quality of parameters, observations and residuals, assessment of network’s reliability and precision, detection of gross errors etc. Many methods {{can be applied to}} solve <b>normal</b> <b>equation,</b> e. g. Gauss-Doolittle, Gauss-Jordan Elimination, Singular Value Decomposition, Iterative Jacoby etc. Cholesky Decomposition is an efficient method to solve <b>normal</b> <b>equation</b> with positive definite and symmetric coefficient matrix. It is also capable of detecting weak condition 1 of the system. Solving large <b>normal</b> <b>equation</b> will require a lot of times and computer memory. Implementation of sparse matrix in Cholesky Decomposition will speed up the execution times and minimize the memory usage by exploiting the zeros and symmetrical of coefficient matrix. This paper discusses the procedures and benefits of implementing sparse matrix in Cholesky Decomposition. Some preliminary results are also included...|$|R
2500|$|... which, in {{a linear}} least squares system give the {{modified}} <b>normal</b> <b>equations,</b> ...|$|E
2500|$|For {{non-linear}} systems similar reasoning {{shows that}} the <b>normal</b> <b>equations</b> for an iteration cycle can be written as ...|$|E
2500|$|This {{results in}} a system of two {{equations}} in two unknowns, called the <b>normal</b> <b>equations,</b> which when solved give ...|$|E
50|$|The Unified WGS Solution, {{as stated}} above, was a {{solution}} for geodetic positions and associated parameters of the gravitational field based on an optimum combination of available data. The WGS 72 ellipsoid parameters, datum shifts and other associated constants were derived separately. For the unified solution, a <b>normal</b> <b>equation</b> matrix was formed based {{on each of the}} mentioned data sets. Then, the individual <b>normal</b> <b>equation</b> matrices were combined and the resultant matrix solved to obtain the positions and the parameters.|$|R
5000|$|This {{expression}} {{is known as}} the <b>Normal</b> <b>Equation</b> and gives us a possible solution to the inverse problem. It is equivalent to Ordinary Least Squares ...|$|R
40|$|Abstract. Solving {{a system}} of linear <b>equations</b> by its <b>normal</b> <b>equation</b> usually is highly unrecommended because this {{approach}} worsens the condition number and inflates the computational cost. For linear systems whose unknowns are matrices, such as the Sylvester equation, Lyapunov equation, Stein equation, and a variate of their generalizations, the formulation of the corresponding <b>normal</b> <b>equation</b> {{in the sense of}} tensor operators offers a common structure via gradient dynamics. This paper explains the setting of this framework and demonstrates its versatility by one simple ODE integrator that can handle almost all these types of problems...|$|R
2500|$|A {{justification}} for choosing this criterion {{is given in}} properties below. This minimization problem has a unique solution, provided that the n columns of the matrix [...] are linearly independent, given by solving the <b>normal</b> <b>equations</b> ...|$|E
2500|$|There are m {{observations}} in y and n parameters in β with m>n. X is a m×n matrix whose elements are either constants or {{functions of the}} independent variables, x. The weight matrix W is, ideally, the inverse of the variance-covariance matrix [...] of the observations y. The independent variables {{are assumed to be}} error-free. The parameter estimates are found by setting the gradient equations to zero, which results in the <b>normal</b> <b>equations</b> ...|$|E
2500|$|The {{method in}} Europe {{stems from the}} notes of Isaac Newton. In 1670, he wrote that all the algebra books known to him lacked a lesson for solving {{simultaneous}} equations, which Newton then supplied. [...] Cambridge University eventually published the notes as Arithmetica Universalis in 1707 long after Newton left academic life. [...] The notes were widely imitated, which made (what is now called) Gaussian elimination a standard lesson in algebra textbooks {{by the end of}} the 18th century. [...] Carl Friedrich Gauss in 1810 devised a notation for symmetric elimination that was adopted in the 19th century by professional hand computers to solve the <b>normal</b> <b>equations</b> of least-squares problems. [...] The algorithm that is taught in high school was named for Gauss only in the 1950s as a result of confusion over the history of the subject.|$|E
5000|$|In Germany, the {{notation}} of a <b>normal</b> <b>equation</b> {{is used for}} dividend, divisor and quotient (cf. first section of Latin American countries above, where it's done virtually the same way): ...|$|R
40|$|We study {{numerical}} stability for interior-point methods {{applied to}} Linear Programming, LP, and Semidefinite Programming, SDP. We analyze the difficulties inherent in current methods and present robust algorithms. We {{start with the}} error bound analysis of the search directions for the <b>normal</b> <b>equation</b> approach for LP. Our error analysis explains the surprising fact that the ill-conditioning is not a significant problem for the <b>normal</b> <b>equation</b> system. We also explain why most of the popular LP solvers have a default stop tolerance of only 10 - 8 when the machine precision on a 32 -bit computer is approximately 10 - 16. We then propose a simple alternative approach for the <b>normal</b> <b>equation</b> based interior-point method. This approach has better numerical stability than the <b>normal</b> <b>equation</b> based method. Although, our approach is not competitive in terms of CPU time for the NETLIB problem set, we do obtain higher accuracy. In addition, we obtain significantly smaller CPU times compared to the <b>normal</b> <b>equation</b> based direct solver, when we solve well-conditioned, huge, and sparse problems by using our iterative based linear solver. Additional techniques discussed are: crossover; purification step; and no backtracking. Finally, we present an algorithm to construct SDP problem instances with prescribed strict complementarity gaps. We then introduce two measures of strict complementarity gaps. We empirically show that: (i) these measures can be evaluated accurately; (ii) {{the size of the}} strict complementarity gaps correlate well with the number of iteration for the SDPT 3 solver, {{as well as with the}} local asymptotic convergence rate; and (iii) large strict complementarity gaps, coupled with the failure of Slater's condition, correlate well with loss of accuracy in the solutions. In addition, the numerical tests show that there is no correlation between the strict complementarity gaps and the geometrical measure used in [31], or with Renegar's condition number...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with the solution of systems of linear equations TNXN = bN, where ∗TN∗NϵN denotes a sequence of nonsingular nonsymmetric Toeplitz matrices arising from a generating function of the Wiener class. We present a technique for the fast construction of optimal trigonometric preconditioners MN = MN(T′NTN) of the corresponding <b>normal</b> <b>equation</b> which can be extended to Toeplitz least squares problems in a straightforward way. Moreover, we prove that the spectrum of the preconditioned matrix MN 1 T′NTN is clustered at 1 such that the PCG-method applied to the <b>normal</b> <b>equation</b> converges superlinearly. Numerical tests confirm the theoretical expectations...|$|R
50|$|The {{conjugate}} gradient method {{can be applied}} to an arbitrary n-by-m matrix by applying it to <b>normal</b> <b>equations</b> ATA and right-hand side vector ATb, since ATA is a symmetric positive-semidefinite matrix for any A. The result is {{conjugate gradient}} on the <b>normal</b> <b>equations</b> (CGNR).|$|E
5000|$|... #Subtitle level 2: Conjugate {{gradient}} on the <b>normal</b> <b>equations</b> ...|$|E
5000|$|... #Subtitle level 3: Inverting {{the matrix}} of the <b>normal</b> <b>equations</b> ...|$|E
40|$|This {{paper is}} {{concerned}} with the solution of systems of linear equations T N xN = bN, where fT N gN 2 IN denotes a sequence of nonsingular nonsymmetricToeplitz matrices arising from a generating function of the Wiener class. We present a technique for the fast construction of optimal trigonometric preconditioners M N = M N (T 0 N T N) of the corresponding <b>normal</b> <b>equation</b> which can be extended to Toeplitz least squares problems in a straightforward way. Moreover, we prove that the spectrum of the preconditioned matrix M Γ 1 N T 0 N T N is clustered at 1 such that the PCG-method applied to the <b>normal</b> <b>equation</b> converges superlinearly. Numerical tests confirm the theoretical expectations. 1991 Mathematics Subject Classification. 65 F 10, 65 F 15, 65 T 10. Key words and phrases. Toeplitz matrix, Krylov space methods, CG-method, preconditioners, <b>normal</b> <b>equation,</b> clusters of eigenvalues. 1 Introduction Consider the system of linear equations T N xN = bN; (1. 1) where T N 2 IR N;N [...] ...|$|R
40|$|A {{generalized}} numerical wave-front reconstruction {{method is}} proposed that {{is suitable for}} diversified irregular pupil shapes of optical systems to be measured. That is, to make a generalized and regular <b>normal</b> <b>equation</b> set, the test domain is extended to a regular square shape. The compatibility of this method is discussed in detail, and efficient algorithms (such as the Cholesky method) for solving this <b>normal</b> <b>equation</b> set are given. In addition, the authors give strict analyses of not only the error propagation in the wave-front estimate {{but also of the}} discretization errors of this domain extension algorithm. Finally, some application examples are given to demonstrate this algorithm...|$|R
40|$|AbstractWe {{study the}} integrability of Hamiltonian systems with two degrees of freedom. We {{investigate}} the <b>normal</b> variational <b>equations</b> and obtain {{a necessary condition}} for integrability of these systems. As an application we study the integrability of the Hénon–Heiles system, whose <b>normal</b> variational <b>equation</b> is of Lamé type...|$|R
5000|$|The {{algebraic}} {{solution of}} the <b>normal</b> <b>equations</b> can be written as ...|$|E
5000|$|... the {{solution}} {{of which can be}} written with the <b>normal</b> <b>equations,</b> ...|$|E
5000|$|... which, on rearrangement, become n {{simultaneous}} linear equations, the <b>normal</b> <b>equations</b> ...|$|E
40|$|The {{purpose of}} this paper is to develop a new method of {{estimating}} productive inefficiency of frontier production functions. A composed additive disturbance term, as the sum of symmetric and non-positive random variables, is specified. <b>Normal</b> <b>equation</b> is derived to estimate the coefficient and inefficiency measures...|$|R
30|$|It {{has been}} shown that a very similar network {{configuration}} can make a huge difference in the estimated parameters. For many regional networks, either the latitude or the longitude of the Euler pole is nearly collinear with the magnitude of the rotation rate, such that an iterative solution does not converge in the direct estimation of the Euler pole parameters. In the given example, the latitude of the Euler pole is highly correlated with the rotation rate since the Euler pole is almost in the south of the Anatolian plate. The <b>normal</b> <b>equation</b> matrix {{is a function of the}} geometry of the distribution of the network, and the condition number of the <b>normal</b> <b>equation</b> matrix is an indication of how well-posed the problem is. Ill-posed problems are generally identified by the high condition numbers of the <b>normal</b> <b>equation</b> matrix (Hansen, 2010). For instance in the given examples, while the condition number of the <b>normal</b> <b>equation</b> matrix in a direct estimation of the Euler pole parameters is about ~ 1014, it decreases to ~ 103 in estimating the Cartesian Euler vector, which is still very large and poses a weakly multicollinear problem. The contribution of the proposed method depends on the quality of the network distribution and the a priori values. For a nearly ideal network, the contribution could be negligible. The distribution of errors for the sites, and the geometry of the sites, determine the performance of the proposed method. The performance of the proposed method, when using different a priori values, is also dependent on the network geometry. The same a priori values used in three networks affected the performance differently due to the different network configurations and error distribution. However, using the regularization constant, the proposed method can always be tuned to give better MSE, or at least, the same MSE with the standard least-squares.|$|R
40|$|In this paper, the {{conjugate}} gradient (CG) algorithm is modified using the RLS <b>normal</b> <b>equation</b> and new data windowing scheme. It {{is known that}} CG algorithm has fast convergence rate and numerical stability. However, the existing CG algorithms still suffer from either slow convergence or high misadjustment compared with the RLS algorithm. In this paper, the parameter beta for CG algorithm is redesigned from the RLS <b>normal</b> <b>equation</b> and a general data windowing scheme reusing the data inputs is presented to solve these problems. The optimal property of parameter alpha is also analyzed using the control Lyapunov function (CLF) of the square deviation of weight error vector. The superior performance of the proposed algorithms over the RLS algorithm and the other existing CG algorithms is demonstrated by computer simulations...|$|R
5000|$|... which, in {{a linear}} least squares system give the {{modified}} <b>normal</b> <b>equations,</b> ...|$|E
5000|$|... and {{the best}} fit can be found by solving the <b>normal</b> <b>equations.</b>|$|E
5000|$|The {{solution}} of the <b>normal</b> <b>equations</b> yields the vector [...] of the optimal parameter values.|$|E
40|$|The least-squares {{transformation}} of a discrete-time multivariable linear system into a desired one by convolving the {{first with a}} polynomial system yields optimal polynomial {{solutions to the problems}} of system compensation, inversion, and approximation. The polynomial coefficients are obtained from the solution to a so-called <b>normal</b> linear matrix <b>equation,</b> whose coefficients are shown to be the weighting patterns of certain linear systems. These, in turn, can be used in the recursive solution of the <b>normal</b> <b>equation...</b>|$|R
40|$|In {{this paper}} we propose a fast {{algorithm}} for the least squares solution of overdetermined Toeplitz linear systems. The resolvent operator of <b>normal</b> <b>equation</b> is approximated as the Schur complement of a suitable augmented matrix. Numerical results are reported to highlight the performance of our algorithm with respect to other direct methods...|$|R
50|$|If {{the problem}} is linear we can use some type of matrix inverse method—often {{the problem is}} ill-posed or {{unstable}} so {{we will need to}} regularize it: good, simple methods include the <b>normal</b> <b>equation</b> or singular value decomposition. If the problem is weakly nonlinear, an iterative method such Newton-Raphson may be appropriate.|$|R
