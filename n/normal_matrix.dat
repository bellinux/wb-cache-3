294|637|Public
25|$|But, in {{the matrix}} case, (M* M)½ is a <b>normal</b> <b>matrix,</b> so ||M* M||½ {{is the largest}} {{eigenvalue}} of (M* M)½, i.e. the largest singular value of M.|$|E
2500|$|If [...] is normal, {{one sees}} that [...] Therefore, [...] must be {{diagonal}} since a normal upper triangular matrix is diagonal (see <b>normal</b> <b>matrix).</b> The converse is obvious.|$|E
2500|$|In an -dimensional Hilbert space, [...] can {{be written}} as an [...] column vector, and then [...] is an [...] matrix with complex entries. The ket [...] can be {{computed}} by <b>normal</b> <b>matrix</b> multiplication.|$|E
40|$|Abstract: Ways {{to combine}} {{normality}} with the fast Fourier transformation ideas are studied by employing various matrix structures. The Toeplitz decomposition is natural for polynomially generating <b>normal</b> <b>matrices</b> while the so-called persymmetric splitting {{provides a framework}} for polynomially extending the Toeplitz matrix structure. In this context fast matrix-vector multiplications with the FFT techniques {{can be applied to}} different Toeplitz related matrices. Two sparse matrix methods for generating <b>normal</b> <b>matrices</b> are introduced to have more alternatives with normality. The method based on embedding <b>matrices</b> in <b>normal</b> <b>matrices</b> allows us to invert nonnormal <b>matrices</b> through inverting <b>normal</b> <b>matrices.</b> This is a potential approach for combining the FFT ideas with preconditioning nonnormal problems. To end with, we introduce a new iterative method...|$|R
40|$|AbstractRecently new optimal Krylov {{subspace}} {{methods have}} been discovered for <b>normal</b> <b>matrices.</b> In light of this, novel ways to quantify nonnormality are considered in connection with various families of matrices. We use as a criterion how, for a given matrix, these iterative methods introduced can be employed via, e. g., inexpensive matrix factorizations. The unitary orbit of the set of binormal matrices provides {{a natural extension of}} <b>normal</b> <b>matrices.</b> Its elements yield polynomially <b>normal</b> <b>matrices</b> of moderate degree. In this context several matrix nearness problems arise...|$|R
40|$|AbstractWe {{consider}} {{the class of}} <b>normal</b> complex <b>matrices</b> that commute with their complex conjugate. We show that such matrices are real orthogonally similar to a canonical direct sum of 1 -by- 1 and certain 2 -by- 2 matrices. A canonical form for quasi-real <b>normal</b> <b>matrices</b> is obtained as a special case. We also exhibit a special form of the spectral theorem for <b>normal</b> <b>matrices</b> that commute with their conjugate...|$|R
2500|$|In an -dimensional Hilbert space, [...] can {{be written}} as a [...] row vector, and [...] (as in the {{previous}} section) is an [...] matrix. Then the bra [...] can be computed by <b>normal</b> <b>matrix</b> multiplication.|$|E
2500|$|In linear algebra, the singular-value {{decomposition}} (SVD) is a factorization {{of a real}} or complex matrix. [...] It is {{the generalization}} of the eigendecomposition of a positive semidefinite <b>normal</b> <b>matrix</b> (for example, a symmetric matrix with positive eigenvalues) to any [...] matrix via {{an extension of the}} polar decomposition. [...] It has many useful applications in signal processing and statistics.|$|E
2500|$|An {{orthogonal}} matrix is {{the real}} specialization of a unitary matrix, and thus always a <b>normal</b> <b>matrix.</b> Although we consider only real matrices here, the definition {{can be used for}} matrices with entries from any field. However, orthogonal matrices arise naturally from dot products, and for matrices of complex numbers that leads instead to the unitary requirement. Orthogonal matrices preserve the dot product, so, for vectors [...] and [...] in an -dimensional real Euclidean space ...|$|E
40|$|We {{revisit the}} {{normality}} preserving augmentation of <b>normal</b> <b>matrices</b> studied by Ikramov and Elsner in 1998 and complement their results by showing how the eigenvalues {{of the original}} matrix are perturbed by the augmentation. Moreover, we construct all augmentations that result in <b>normal</b> <b>matrices</b> with eigenvalues on a quadratic curve in the complex plane, using the stratification of <b>normal</b> <b>matrices</b> presented by Huhtanen in 2001. To make this construction feasible, but also for its own sake, we study normality preserving normal perturbations of <b>normal</b> <b>matrices.</b> For 2 × 2 and for rank- 1 matrices, the analysis is complete. For higher rank, all essentially Hermitian normality perturbations are described. In all cases, {{the effect of the}} perturbation on the eigenvalues of the original matrix is given. The paper is concluded with a number of explicit examples that illustrate the results and constructions...|$|R
40|$|We {{consider}} Schur function expansion for {{the partition}} {{function of the}} model of <b>normal</b> <b>matrices.</b> We show that this expansion coincides with Takasaki expansion Tinit for tau functions of Toda lattice hierarchy. We show that the partition function of the model of <b>normal</b> <b>matrices</b> is, at the same time, a partition function of certain discrete models, which can be solved by the method of orthogonal polynomials. We obtain discrete versions of various known matrix models: models of non-negative <b>matrices,</b> unitary <b>matrices,</b> <b>normal</b> <b>matrices.</b> Comment: 21 pages, no figures. Some parts of this paper were presented on ISLAND II conference, Arran 200...|$|R
40|$|Abstract. The {{relation}} between random <b>normal</b> <b>matrices</b> and conformal mappings discovered by Wiegmann and Zabrodin is made rigorous by restricting <b>normal</b> <b>matrices</b> to have spectrum in a bounded set. It is shown {{that for a}} suitable class of potentials the asymptotic density of eigenvalues is uniform with support in the interior domain of a simple smooth curve. 1...|$|R
2500|$|In {{the special}} case that [...] is a <b>normal</b> <b>matrix,</b> which by {{definition}} must be square, the spectral theorem {{says that it}} can be unitarily diagonalized using a basis of eigenvectors, so {{that it can be}} written [...] for a unitary matrix [...] and a diagonal matrix [...] [...] When [...] is also positive semi-definite, the decomposition [...] is also a singular-value decomposition. [...] Otherwise, it can be recast as an SVD by moving the phase of each [...] to either its corresponding [...] or [...] [...] The natural connection of the SVD to non-normal matrices is through the polar decomposition theorem: [...] , where [...] is positive semidefinite and normal, and [...] is unitary.|$|E
2500|$|Two {{features}} are noteworthy. First, {{one of the}} roots (or eigenvalues) is 1, which tells us that some direction is unaffected by the matrix. For rotations in three dimensions, this is the axis of the rotation (a concept that has no meaning in any other dimension). Second, the other two roots are a pair of complex conjugates, whose product is 1 (the constant term of the quadratic), and whose sum is [...] (the negated linear term). This factorization is of interest for [...] rotation matrices because the same thing occurs for all of them. (As special cases, for a null rotation the [...] "complex conjugates" [...] are both 1, and for a 180° rotation they are both −1.) Furthermore, a similar factorization holds for any [...] rotation matrix. If the dimension, , is odd, {{there will be a}} [...] "dangling" [...] eigenvalue of 1; and for any dimension the rest of the polynomial factors into quadratic terms like the one here (with the two special cases noted). We are guaranteed that the characteristic polynomial will have degree [...] and thus [...] eigenvalues. And since a rotation matrix commutes with its transpose, it is a <b>normal</b> <b>matrix,</b> so can be diagonalized. We conclude that every rotation matrix, when expressed in a suitable coordinate system, partitions into independent rotations of two-dimensional subspaces, at most [...] of them.|$|E
5000|$|Comment: Every <b>normal</b> <b>matrix</b> A (i.e., matrix {{for which}} , where [...] is a {{conjugate}} transpose) can be eigendecomposed. For <b>normal</b> <b>matrix</b> A (and only for <b>normal</b> <b>matrix),</b> the eigenvectors {{can also be}} made orthonormal (...) and eigendecomposition reads as [...] In particular all unitary, Hermitian, and skew-Hermitian (in real-valued case, all orthogonal, symmetric, and skew-symmetric, respectively) matrices are normal and therefore possess this property.|$|E
5000|$|The {{spectral}} theorem {{permits the}} classification of <b>normal</b> <b>matrices</b> {{in terms of their}} spectra, for example: ...|$|R
5000|$|... <b>normal</b> <b>matrices</b> {{can be seen}} {{as normal}} {{operators}} if one takes the Hilbert space to be Cn.|$|R
5000|$|In general, the sum or {{product of}} two <b>normal</b> <b>matrices</b> {{need not be}} normal. However, the {{following}} holds: ...|$|R
5000|$|Proposition. A <b>normal</b> <b>matrix</b> is self-adjoint if {{and only}} if its {{spectrum}} is contained in [...] In other words: A <b>normal</b> <b>matrix</b> is Hermitian if {{and only if}} all its eigenvalues are real.|$|E
5000|$|Eigenvectors of {{distinct}} eigenvalues of a <b>normal</b> <b>matrix</b> are orthogonal.|$|E
5000|$|Every {{generalized}} eigenvector of a <b>normal</b> <b>matrix</b> is {{an ordinary}} eigenvector.|$|E
5000|$|The {{concept of}} {{normality}} {{is important because}} <b>normal</b> <b>matrices</b> are precisely those to which the spectral theorem applies: ...|$|R
40|$|AbstractComplex {{matrices}} {{that are}} structured {{with respect to}} a possibly degenerate indefinite inner product are studied. Based on earlier works on <b>normal</b> <b>matrices,</b> the notions of hyponormal and strongly hyponormal matrices are introduced. A full characterization of such matrices is given and it is shown how those matrices are related to different concepts of <b>normal</b> <b>matrices</b> in degenerate inner product spaces. Finally, the existence of invariant semidefinite subspaces for strongly hyponormal matrices is discussed...|$|R
40|$|AbstractIn {{a recent}} work by Sidi and Bridger some old {{and some new}} {{extensions}} of the power method have been considered, {{and some of these}} extensions have been shown to produce estimates of several dominant eigenvalues of an arbitrary square matrix. In the present work we continue the analysis of two versions of one of these extensions, called the MPE extensions, as they are applied to <b>normal</b> <b>matrices.</b> We show that the convergence rate of these methods for <b>normal</b> <b>matrices</b> is twice that for nonnormal matrices. We also give precise asymptotic bounds on the errors of the estimates obtained for the eigenvalues. Further deflation-type extensions of the power method for <b>normal</b> <b>matrices</b> are suggested and analyzed for their convergence. All the results are stated and proved in the general setting of inner-product spaces...|$|R
5000|$|U is a <b>normal</b> <b>matrix</b> with {{eigenvalues}} {{lying on}} the unit circle.|$|E
5000|$|A complex <b>normal</b> <b>matrix</b> (...) has an {{orthogonal}} eigenvector basis, so a <b>normal</b> <b>matrix</b> can be decomposed aswhere U is {{a unitary}} matrix. Further, if A is Hermitian (...) , which implies {{that it is}} also complex normal, the diagonal matrix Λ has only real values, and if A is unitary, Λ takes all its values on the complex unit circle.|$|E
5000|$|If A is a <b>normal</b> <b>matrix</b> then [...] is the {{convex hull}} of its eigenvalues.|$|E
30|$|For <b>normal</b> <b>matrices</b> the {{statement}} of Theorem  3 in [7] (inequality (1.5)) {{can no longer be}} valid. However, we have the following theorem.|$|R
40|$|In this {{research}} project computational and theoretical issues linked with spectral {{properties of the}} classes of normal and perturbed <b>normal</b> <b>matrices</b> will be studied. The overall research goal {{is the development of}} new, theoretically well-supported, fast, accurate and reliable algorithms for computing (approximate) spectral decompositions of (perturbed) <b>normal</b> <b>matrices.</b> The embracing goal encloses three weighty subgoals. We aim at notable progression in, and aspire towards significant contributions in theory, algorithms design and the development of competitive software. nrpages: 166 status: publishe...|$|R
40|$|In this article, we {{consider}} a fairly general {{potential in the}} plane and the corresponding Boltzmann-Gibbs distribution of eigenvalues of random <b>normal</b> <b>matrices.</b> As {{the order of the}} matrices tends to infinity, the eigenvalues condensate on a certain compact subset of the plane—the “droplet. ” We prove that fluctuations of linear statistics of eigenvalues of random <b>normal</b> <b>matrices</b> converge on compact subsets of the interior of the droplet to a Gaussian field, and we discuss various ramifications of this result...|$|R
50|$|A <b>normal</b> <b>matrix</b> is {{the matrix}} {{expression}} of a normal operator on the Hilbert space Cn.|$|E
5000|$|Any <b>normal</b> <b>matrix</b> {{is similar}} to a {{diagonal}} matrix, since its Jordan normal form is diagonal.|$|E
50|$|If {{the entries}} are real numbers or complex numbers, {{then it is}} a <b>normal</b> <b>matrix</b> as well.|$|E
40|$|In this paper, {{we present}} methods for image {{compression}} {{on the basis}} of eigenvalue decomposition of <b>normal</b> <b>matrices.</b> The proposed methods are convenient and self-explanatory, requiring fewer and easier computations as compared to some existing methods. Through the proposed techniques, the image is transformed to the space of <b>normal</b> <b>matrices.</b> Then, the properties of spectral decomposition are dealt with to obtain compressed images. Experimental results are provided to illustrate the validity of the methods. Comment: arXiv admin note: text overlap with arXiv: 1506. 0195...|$|R
40|$|We give a new proof for an {{equality}} of certain max-min and min-max approximation problems involving <b>normal</b> <b>matrices.</b> The previously published proofs of this equality apply tools from matrix theory, (analytic) optimization theory and constrained convex optimization. Our proof uses a classical characterization theorem from approximation theory and thus exploits {{the link between}} the two approximation problems with <b>normal</b> <b>matrices</b> {{on the one hand and}} approximation problems on compact sets in the complex plane on the other. Comment: Written in memory of Bernd Fische...|$|R
40|$|Abstract. An easy {{proof to}} show that every complex <b>normal</b> Toeplitz <b>matrix</b> is {{classified}} as either of type I or of type II is given. Instead of difference equations on elements in the matrix used in past studies, polynomial equations with coefficients of elements are used. In a similar fashion, it is shown that a real <b>normal</b> Toeplitz <b>matrix</b> {{must be one of}} four types: symmetric, skew-symmetric, circulant, or skew-circulant. Here trigonometric polynomials in the complex case and algebraic polynomials in the real case are used. Key words. <b>Normal</b> <b>matrices,</b> Toeplitz matrices. AMS subject classifications. 15 A 57, 47 B 15, 47 B 3...|$|R
