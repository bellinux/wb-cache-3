10|30|Public
5000|$|Homodyne {{detection}} is {{a method}} of extracting information encoded as modulation of the phase and/or frequency of an oscillating signal, by comparing that signal with a standard oscillation that would be identical to the signal if it carried <b>null</b> <b>information.</b> [...] "Homodyne" [...] signifies a single frequency, {{in contrast to the}} dual frequencies employed in heterodyne detection.|$|E
5000|$|Optical {{heterodyne}} detection {{is a method}} of extracting information encoded as modulation of the phase and/or frequency (wavelength) of electromagnetic radiation in the wavelength band of visible or infrared light. The light signal is compared with standard or reference light from a [...] "local oscillator" [...] (LO) {{that would have a}} fixed offset in frequency and phase from the signal if the latter carried <b>null</b> <b>information.</b> [...] "Heterodyne" [...] signifies more than one frequency, in contrast to the single frequency employed in homodyne detection. The comparison of the two light signals is typically accomplished by combining them in a photodiode detector, which has a response that is linear in energy, and hence quadratic in amplitude of electromagnetic field. Typically, the two light frequencies are similar enough that their difference or beat frequency produced by the detector is in the radio or microwave band that can be conveniently processed by electronic means.|$|E
30|$|Dommenget (2007) {{suggested}} evaluating data-derived EOF modes against modes synthesised from a stochastic null hypothesis. He {{argued that}} a spatial first-order autoregressive process (AR 1) {{can be used as}} the <b>null</b> <b>information</b> for the spatial structure of the climate variability. Similarly, we may use a temporal second-order autoregressive process (AR 2) as the null hypothesis for the magnetic field variability (based on the f^- 4 magnetic energy spectrum). Dommenget (2007) showed EOF modes for realisations of the two-dimensional AR 1 stochastic process and introduced the idea of distinct EOFs, i.e. the modes that are the most distinguished from the modes of the null hypothesis. We may use the same approach, now in the time domain, to search for principal components associated with waves in the core, especially the torsional oscillations with about 6  years period (Gillet et al. 2010).|$|E
40|$|The {{proposed}} {{face recognition}} algorithm regularises the within-class scatter matrix by extrapolating its eigenvalues. This way both the range {{space and the}} <b>null</b> space <b>information</b> of the within-class scatter matrix is utilised. The results are promising when the algorithm is experimented on face recognition datasets and compared with several other methods. Griffith Sciences, Griffith School of EngineeringNo Full Tex...|$|R
40|$|Information theoretic {{tests for}} serial {{independence}} and linearity in time series are proposed. The conditional mutual information {{is used as}} a test statistic and estimated nonparametrically using the correlation integral from chaos theory. An advantage of this approach is, that in case of rejection of the <b>null</b> hypothesis, <b>information</b> about the order in which the dependence is present is readily available. The significance of the test statistic is determined by means of bootstrap methods. Size and power of the test are studied using simulated time series using both linear and nonlinear models, with an emphasis on econometrically relevant models. Finally, application to macroeconomic data show evidence of nonlinear dependence. ...|$|R
40|$|Regularisation of eigenfeatures by {{extrapolation}} of scatter-matrix in face-recognition problem A. Sharma and K. K. Paliwal The proposed face recognition algorithm regularises the within-class scatter matrix by extrapolating its eigenvalues. This way both the range {{space and the}} <b>null</b> space <b>information</b> of the within-class scatter matrix is utilised. The results are promising when the algorithm is experimented on face recognition datasets and compared with several other methods. Introduction: In face recognition problems, the dimensionality of the feature space is very large compared {{to the number of}} training data samples available. It is therefore necessary to reduce the dimensionality of the space for improving the robustness (or generalisation capability) and computational complexity of the face recognition classifier. Man...|$|R
40|$|AbstractThe article substantiates {{the need}} to intensify practice-oriented {{teaching}} of information modelling. Commonality of information modelling tasks and creative tasks is disclosed; the role of such tasks in developing creative abilities of senior pupils in teaching informatics is described. The specificity of the task statement allows to classify information modelling tasks into the tasks with insufficient, redundant, contradictory or <b>null</b> <b>information</b> and depending {{on the character of}} the purpose they may be of prognostication, optimization, research, reviewing and other types. Experimental work in teaching pupils to solve information modelling tasks within the school course of informatics is described. 148 schoolchildren were involved in the forming stage of the experiment. It has been proved that teaching pupils to solve information modelling problems leads to positive changes in the development level of flexibility, fluency, and originality of thinking as components of creative abilities...|$|E
40|$|We {{examine a}} principal-agent model with moral hazard {{in which the}} {{technology}} – the vector of probability distributions from the agent’s actions to the possible outcomes – is initially unknown. A signal correlated with the technology is observed after the principal and agent agree to the contract (ex ante contracting). The signal may be uninformative (<b>null</b> <b>information)</b> or informative and observed only by the principal (private information) or observed by both the principal and agent (public information). We show that: (i) if the principal implements different actions for each signal with private information, then the principal strictly prefers both public to private information and private to null information; (ii) if the principal implements the same action for either signal with public information, then the principal is indifferent between null and private information, which she prefers to public information; (iii) the value of information can be non-monotonic both with private and with public information; and (iv) the value of information may be greater either with private or with public information. Moral Hazard, Ex Ante Contracting, Informed Principal, Technology, Value of Information...|$|E
40|$|Is {{there an}} {{intrinsic}} nonconcavity {{to the value}} of information? In an influential paper, Radner and Stiglitz (1984, henceforth RS) suggests that there is. They demonstrated, in a seemingly general model, that the marginal value of a small amount of information is zero. Since costless information is always (weakly) valuable, this finding implies that, unless the information is useless, it must exhibit increasing marginal returns over some range. RS do present a few examples that violate their assumptions for which information exhibits decreasing marginal returns. Yet, the conditions under which they obtain the nonconcavity do not seem initially to be overly strong. They index the information structure, represented by a Markov matrix of state-conditional signal distributions, by a parameter representing the `amount' of information, with a zero level of the parameter representing <b>null</b> <b>information.</b> The main assumption is that this Markov matrix be a differentiable in the index parameter at <b>null</b> <b>information,</b> which seems to be a standard smoothness assumption. As noted by RS, this nonconcavity has several implications: the demand for information will be a discontinuous function of its price; agents will not buy `small' quantities of information; and agents will tend to specialize in information production. The nonconcavity has been especially vexing to the literature on experimentation. If the value of information is not concave in the present action, then the analysis of optimal experimentation is much more complex. Moreover, some recent papers have considered experimentation in strategic settings (Harrington (JET 1995); Mirman, Samuelson and Schlee (JET 1994)). In these models, the nonconcavity means that the best reply mappings may not be convex-valued, so that pure strategy equilibria may not exist. The {{purpose of this paper is}} to re-examine the conditions under which a small amount of information has zero marginal value. Much of the experimentation and information demand literature has assumed either an infinite number of signal realizations or an infinite number of states, unlike the finite RS framework. Our objective is to clarify the conditions under which the nonconcavity holds in this more common framework. This general setting will help us to evaluate the robustness of the nonconcavity. We find that the assumptions required to obtain the nonconcavityare fairly strong; although some of the assumptions are purely technical, most are substantive: we present examples showing that their failure leads to a failure of nonconcavity. ...|$|E
40|$|Abstract — Upper {{and lower}} bounds are derived on the {{capacity}} and fading number of multi-antenna systems operating over flat fading channels. The fading process may exhibit memory, and the receiver {{is assumed to}} have access to some incomplete (possibly <b>null)</b> side <b>information</b> related to the fading realization. For single-input single-output (SISO) systems these bounds typically yield an exact expression for the system’s fading number. This expression is evaluated explicitly in the SISO Gaussian case {{as a function of the}} fading mean, its power spectral density, and the mutual information rate between the fading process and side information. In the absence of side information, the fading number of such systems depends only on the mean and variance of the fading process an...|$|R
40|$|AbstractA nonapplicable null {{appears in}} a {{relation}} whenever {{the value of the}} corresponding attribute does not exist. In order to evaluate a query on a relation r involving such <b>null</b> values, the <b>information</b> contained in r is represented by a set of null-free instances, then the query on r—expressed in a user-friendly query language (Generalized Relational Calculus) —is translated into a set of queries on the null-free instances. Conversely, we define the operations on relations with nulls (Generalized Relational Algebra) and we proved an extension of Codd's completeness theorem...|$|R
40|$|To {{increase}} {{capacity and}} offload traffic {{from the current}} macro cell cellular system, small cells have been deployed extensively. In the current deployments small cells coexist with their respective macro cells in the same spectrum, due to the difficulty and costs involved in acquiring new spectrum licenses. This leads to considerable interference between the two systems. In this context, we propose an underlay cognitive interference alignment technique to remove the interference from the small cell to the macro cell system. In the proposed scheme the small cells sense the unused resources in the space dimension of the macro cell, i. e. its null space, and precode their signals such that they lie in the sensed null space. We demonstrate that only {{the sign of the}} null space components is necessary to obtain close to full <b>null</b> space <b>information</b> performance. This results in less overhead in the sensing process making the system more efficient and practically feasible...|$|R
40|$|Cost-benefit {{analysis}} has a considerable literature in which information {{systems have been}} patently ignored. This reflects the considerable difficulties of applying the theory to information systems, and the state-of-the art remains relatively as Koopmans described it some 19 years ago (1957). A bar to further development {{would appear to be}} the lack of an applicable value-of-information concept. This paper seeks to clarify the issues and provide a robust theoretical and data analysis framework that will cover most situations. The approach here is to separate explicitly the dimensions of cost from those of information benefit, and examine the implications. The <b>Null</b> <b>Information</b> Benefit condition emerges as a special theoretical case, but potentially a most important one in applications. This case together with the Pareto optimum defines a large class of such problems that can be handled by the decision criteria and data analysis techniques tabulated and discussed here. The selection of input data techniques defines the limits of later project justification and may be crucial to the political viability of the projects throughout its life. Finally, the general management vs information systems management relationships are discussed in terms of this situation. ...|$|E
40|$|Since a query {{language}} {{is used as}} a handy tool to obtain information from a database, users want more user-friendly and fault-tolerant query interfaces. When a query search condition does not match with the underlying database, users would rather receive approximate answers than <b>null</b> <b>information</b> by relaxing the condition. They also prefer a less rigid querying structure, one which allows for vagueness in composing queries, and want the system to understand the intent behind a query. This paper presents a data abstraction approach to facilitate the development of such a fault-tolerant and intelligent query processing system. It specifically proposes a knowledge abstraction database that adopts a multilevel knowledge representation scheme called the knowledge abstraction hierarchy. Furthermore, the knowledge abstraction database extracts semantic data relationships from the underlying database and supports query relaxation using query generalization and specialization steps. Thus, it can broaden the search scope of original queries to retrieve neighborhood information and help users to pose conceptually abstract queries. Specifically, four types of vague queries are discussed, including approximate selection, approximate join, conceptual selection and conceptual join. � 2000 Elsevier Science B. V. All rights reserved...|$|E
40|$|In genomic {{selection}} (GS) programmes, direct genomic values (DGV) {{are evaluated}} using {{information provided by}} high-density SNP chip. Being DGV accuracy strictly dependent on SNP density, {{it is likely that}} {{an increase in the number}} of markers per chip will result in severe computational consequences. Aim of present work was to test the effectiveness of principal component analysis (PCA) carried out by chromosome in reducing the marker dimensionality for GS purposes. A simulated data set of 5700 individuals with an equal number of SNP distributed over six chromosomes was used. PCs were extracted both genome-wide (ALL) and separately by chromosome (CHR) and used to predict DGVs. In the ALL scenario, the SNP variance–covariance matrix (S) was singular, positive semi-definite and contained <b>null</b> <b>information</b> which introduces ‘spuriousness’ in the derived results. On the contrary, the S matrix for each chromosome (CHR scenario) had a full rank. Obtained DGV accuracies were always better for CHR than ALL. Moreover, in the latter scenario, DGV accuracies became soon unsettled as the number of animals decreases, whereas in CHR, they remain stable till 900 – 1000 individuals. In real applications where a 54 k SNP chip is used, the largest number of markers per chromosome is approximately 2500. Thus, a number of around 3000 genotyped animals could lead to reliable results when the original SNP variables are replaced by a reduced number of PCs...|$|E
40|$|Abstract: We {{propose a}} new {{approach}} to test the full-information rational expectations hypothesis which can identify whether rejections of the <b>null</b> arise from <b>information</b> rigidities. This approach quantifies the economic significance of departures from the null and the underlying degree of information rigidity. Applying this approach to U. S. and international data of professional forecasters and other agents yields pervasive evidence consistent with the presence of information rigidities. These results therefore provide a set of stylized facts which can be used to calibrate imperfect information models. Finally, we document evidence of state-dependence in the expectations formation process...|$|R
40|$|This paper proposes and applies a {{practical}} method for including a stochastic {{component in the}} traditional nonparametric tests of economic hypotheses developed by Afriat and Varian. The method proposed here requires no a priori knowledge of the variance covariance structure of the stochastic components within each observation as in the test proposed by Epstein and Yarchew. In this work nonparametric regression is used to get estimates of the price response functions under the <b>null</b> hypothesis. The <b>information</b> obtained from these regressions is then used in combination with a minimum distance statistic to test the null hypothesis...|$|R
40|$|Genetic {{algorithm}} full waveform inversion (GA-FWI) is able {{to predict}} complex shear-wave velocity (Vs) models fairly from surface waves, even in the case when very limited or <b>null</b> a-priori <b>information</b> is available (Xing and Mazzotti, 2017 a). Out of consideration for computing time reduction, a two-grid approach (Sajeva et al., 2016; Aleardi and Mazzotti, 2017; Mazzotti et al., 2017), one coarse grid for the inversion and one small grid for the modeling, is recommended for the method. Thus, we generally obtain smooth velocity models whose wavelengths are dependent on the coarse grid spacing. In this paper, we show that these models are suitable starting models for FWI approaches with local optimization methods and that, in general, significant details of the depth model can be retrieved. Instead, we do not discuss the influences caused by different surface wave modeling strategies (Thorbecke and Draganov, 2011; Groos, 2013; Xing and Mazzotti, 2016) and by assumptions in wave modeling (Xing and Mazzotti, 2017 b), thus we focus on model prediction and refinement...|$|R
40|$|AbstractThe {{sustainability}} in {{the building}} industry is currently a trend of the governmental policies in the EU and the USA. In the EU, the target for 2020 is to achieve net-zero energy buildings (NZEB) for the new constructions, as enforced by the 2010 recast Energy Performance Building Directive. On the other hand, {{it is necessary to}} make decisions about the technologies to include in buildings when undergoing a major renovation. The aim {{of this paper is to}} help to define guidelines about the feasibility of achieving a near-zero energy condition on the existing building stock using the Axiomatic Design theory. Applying this design process to a set of existing office buildings, it is possible to evaluate the energy use of renovated buildings. In the context of this study, the renovation focuses on the reduction of internal loads, isolating the facade and applying new efficient systems in each building. Following the decoupled design matrix of such renovation, it is possible to obtain the fuzzy functions of the energy use. In reality, these functions are the fuzzy sum of contributions to the energy use of the design parameters applied according to the design matrix. The membership function of the energy use allows in turn determining the information content of each type of building, for a design limit of 100 kW·h/(m 2 · a) of primary energy use. It is found that a renovated building using a photovoltaic system area of about 20 % of the floor area of the building allows achieving <b>null</b> <b>information</b> content, or in other words a 100 % probability of success...|$|E
40|$|We {{propose a}} new {{approach}} to test the full-information rational expectations hypothesis which can identify whether rejections of the <b>null</b> arise from <b>information</b> rigidities. This approach quantifies the economic significance of departures from the null and the underlying degree of information rigidity. Applying this approach to U. S. and international data of professional forecasters and other agents yields pervasive evidence consistent with the presence of information rigidities. These results therefore provide a set of stylized facts which can be used to calibrate imperfect information models. Finally, we document evidence of state-dependence in the expectations formation process. (JEL codes: E 3, E 4, E 5...|$|R
30|$|In practice, {{not all the}} subcarriers are {{available}} for transmission. It {{is often the case}} that null subcarriers are set on both the edges of the allocated bandwidth to mitigate interferences from/to adjacent bands [10, 11]. For example, IEEE 802.11 a has 64 subcarriers among which 12 subcarriers, one {{at the center of the}} band (DC component) and at the edges of the band are set to be <b>null,</b> i.e., no <b>information</b> is sent [12]. The presence of null subcarriers complicates the design of both the training preamble for channel estimation and pilot symbols for pilot-aided channel estimation over the frequency-selective channels. Null subcarriers may render equi-distant and equi-powered pilot symbols impossible to use in practice.|$|R
40|$|Galaxies {{acting as}} {{gravitational}} lenses are surrounded by, at most, {{a handful of}} images. This apparent paucity of information forces one {{to make the best}} possible use of what information is available to invert the lens system. In this paper, we explore the use of a genetic algorithm to invert in a non-parametric way strong lensing systems containing {{only a small number of}} images. Perhaps the most important conclusion of this paper is that it is possible to infer the mass distribution of such gravitational lens systems using a non-parametric technique. We show that including <b>information</b> about the <b>null</b> space (i. e. the region where no images are found) is prerequisite to avoid the prediction of a large number of spurious images, and to reliably reconstruct the lens mass density. While the total mass of the lens is usually constrained within a few percent, the fidelity of the reconstruction of the lens mass distribution depends on the number and position of the images. The technique employed to include <b>null</b> space <b>information</b> can be extended in a straightforward way to add additional constraints, such as weak lensing data or time delay information. Comment: 9 pages, accepted for publication by MNRA...|$|R
40|$|The sparse hypermatrix storage scheme {{produces}} a recursive 2 D partitioning of a sparse matrix. Data subblocks are stored as dense matrices. Since {{we are dealing}} with sparse matrices some zeros can be stored in those dense blocks. The overhead introduced by the operations on zeros can become really large and considerably degrade performance. In this paper, we present several tech-niques for reducing the operations on zeros in a sparse hypermatrix Cholesky factorization. By associating a bit to each column within a data submatrix we create a bit vector. We can avoid computations when the bitwise AND of their bit vectors is <b>null.</b> By keeping <b>information</b> about the actual space within a data submatrix which stores non-zeros (dense window) we can reduce both storage and computation...|$|R
30|$|It is {{desirable}} to apply multi-antenna techniques in the CR system, since the interference to the PR can be suppressed greatly {{by using the}} beamforming at the ST [11]. However, a proper beamforming requires the channel direction information from the ST to the PR (CDIsp) to be available at the ST. Similar to the CGIsp, the CDIsp has to be probed as well. However, the problem of CDIsp probing is more challenging than that of CGIsp probing. This is because the CDIsp is not so explicit in the hidden feedback information as the CGIsp. For example, the channel gain associated with the CGIsp can be measured directly with the strength of link adaptation in the hidden feedback information [7], while the channel direction associated with the CDIsp can not. The work in [9, 12] has proposed the algorithms to learn the <b>null</b> space <b>information</b> of the CDIsp from the hidden feedback information in primary system, which however, rely on the binary-search method. The binary-search method requires to adapt the probing signals with the received hidden feedback information until the acquired information is converged, which may cause too much overhead because of the iteration between the probing and acquisition. Therefore, it is more desirable to find a CDIsp probing scheme free of iteration for the multi-antenna CR system.|$|R
40|$|Computing a {{basis for}} the null space of a large and sparse matrix is often a key step of some {{algorithms}} in structural analysis, uid mechanics, constrained optimization, image reconstruction or electrical engineering. Various algorithms based on rank revealing LU or rank revealing orthogonal decompositions have been proposed in the literature; see e. g. [1, 2, 3, 4, 5]. Nevertheless {{to the best of our}} knowledge the case of large sparse matrices in the framework of multifrontal methods has been rarely addressed. In this talk we propose to use a sparse Gaussian Elimination of A ∈ Cn×n to derive <b>null</b> space <b>information</b> by inspecting only U ∈ Cn×n, the singular upper triangular factor obtained after numerical factorization of the preprocessed matrix Ã = PsAPcP T s, where Ps corresponds to a permutation that aims at minimizing the ll-in during factorization and Pc is a column permutation to obtain a zero-free diagonal. Thus we would like to determine accurately the de ciency of U and a basis of its null space. We will detail the two main modi cations included in a multifrontal method to derive a (hopefully) rank-revealing sparse Gaussian Elimination algorithm. Finally we will analyse the behaviour of the proposed algorithm- implemented in MUMPS- on some singular matrices related to both academic problems and real-life applications...|$|R
40|$|The cluster lens Cl 0024 + 1654 is {{undoubtedly}} {{one of the}} most beautiful examples of strong gravitational lensing, providing five large images of a single source with well-resolved substructure. Using the information contained in the positions and the shapes of the images, combined with the <b>null</b> space <b>information,</b> a non-parametric technique is used to infer the strong lensing mass map of the central region of this cluster. This yields a strong lensing mass of 1. 60 x 10 (14) M(circle dot) within a 0. 5 arcmin radius around the cluster centre. This mass distribution is then used as a case study of the monopole degeneracy, which may be {{one of the most}} important degeneracies in gravitational lensing Studies and which is extremely hard to break. We illustrate the monopole degeneracy by adding circularly symmetric density distributions with zero total mass to the original mass map of Cl 0024 + 1654. These redistribute mass in certain areas of the mass map without affecting the observed images in any way. We show that the monopole degeneracy and the mass-sheet degeneracy together lie at the heart of the discrepancies between different gravitational lens reconstructions that can be found in the literature for a given object, and that many images/sources, with an overall high image density in the lens plane, are required to construct an accurate, high-resolution mass map based on strong lensing data...|$|R
40|$|Abstract – In the Web {{environment}} web {{log file}} capture operational data generated through internet for analysing user’s browsing behaviour {{and many other}} security issues. The captured operational data is useful for build use profile, web designing and acts as evidence in web forensic and many other security issues. In real world there are lots systems that participate in web environment having incomplete information because of that web log file affected through noise which lead many of inconvenience. Estimation and handling of these noises in web log is major issue in web forensic and other web related security issue. For evaluating that incomplete <b>information</b> <b>null</b> value estimation is very precious technique. This paper proposed a null value estimation technique based on fuzzy rule based k-means algorithm {{to deal with that}} noise. Proposed technique enhances the performance of k-means clustering algorithm by encapsulating advantage of fuzzy rule over that...|$|R
40|$|Abstract—Discovery of Full {{functional}} dependencies from relations {{has been}} identified as an important database analysis technique. In order to deal with information inexactness, fuzzy techniques have extensively been integrated with different database models and theories. However, the information is often vague or ambiguous and very difficult to represent in implementing the application software. This problem can be handled by getting secured information to reduce imprecise <b>information.</b> <b>Null</b> queries are queries that elicit a null answer from the database. In fuzzy object oriented database model, a theoretical framework utilizes analogical reasoning and fuzzy functional dependency or full functional dependency to answer null queries. In this paper, the concept of fuzzy functional dependency is extended to full functional dependency on similarity based fuzzy object oriented data model. In addition, a data mining algorithm is proposed to discover all functional dependencies among attributes. From this functional dependency, we shall be able to reach full functional dependency. Keywords- database; fuzzy; attributes; tuples; null query; functional dependency; partition; algorithms. I...|$|R
40|$|In two {{important}} recent papers, Finkelstein and McGarry [25] and Finkelstein and Poterba [28] propose a new test for asymmetric information in insurance markets that considers explicitly unobserved heterogeneity in insurance demand. In this {{paper we propose}} an alternative implementation of the Finkelstein-McGarry-Poterba test based on the identification of unobservable types by use of finite mixture models. The actual implementation of our test follows some recent advances on marginal modelling as applied to latent class analysis; formal testing procedures for the <b>null</b> of asymmetric <b>information</b> and for the hypothesis that private information is indeed multidimensional can be performed by imposing restrictions {{on the behavior of}} these unobservable types. To show the potential applicability of our approach, we look at the long term insurance market as analyzed in Finkelstein and McGarry [25], where we also find strong evidence for both asymmetric information and multidimensional unobserved heterogeneity. Asymmetric Information, Unobservable Types, Latent Class Analysis, Long Term Insurance Market. ...|$|R
40|$|There {{are some}} classes of SD 4 ft-{{patterns}} defined in this work. SD 4 ft-patterns are the patterns which the GUHA procedure SD 4 ft-Miner deals with. The {{aim of this}} work was to define the classes of SD 4 ft-patterns, which have similar properties like the classes of association rules; in particular to find the criteria of correctness of deduction rules and data mining in databases, which contain an uncomplete <b>information</b> (<b>NULL</b> values). The logical calculi such that its formulae correspond to SD 4 ft-patterns and principles of work with uncomplete information are defined in this paper. Our effort was to utilize the existing knowledge of association rules (also called 4 ft-patterns) to study SD 4 ft-patterns. Some potentially useful SD 4 ft-quantifiers based on useful 4 ft-quantifiers are defined. The investigated classes of SD 4 ft-patterns were designed, so that they contain SD 4 ft-patterns based on these useful SD 4 ft-quantifiers...|$|R
40|$|Background: Previous {{studies have}} {{suggested}} that use of aspirin or other nonsteroidal anti-inflammatory drugs (NSAIDs) may be associatedwith reduced risk of lung cancer, but the data are inconsistent and are limited particularly with respect to the effects of aspirin, separate from other NSAIDs. Methods: The Iowa Women’s Health Study is a prospective cohort of 41, 836 Iowawomen ages 55 to 69 years old at baseline in 1986. NSAID use was assessed in 1992. Over 10 years of follow-up, 403 incident cases of lung cancer were identified. The association of incident lung cancer with current use of aspirin or non-aspirin NSAIDs was analyzed after adjustment for lung cancer risk factors. Hazard ratios (HR) were estimated using multivariate COX proportional hazards regression. Results: There were 27, 162 women in the analytic cohort. After controlling for age, education, alcohol intake, pack-years, smoking status, body mass index, and total fruit intake, the RR of women taking six or more aspirin weekly was 1. 21 (95 % confidence interval, 0. 92 - 1. 59). The HR was 1. 23 for women taking six or more non-aspirin NSAIDs weekly (95 % confidence interval, 0. 92 - 1. 65). There was no statistically significant trend by frequency of use for either aspirin (P trend = 0. 22) or non-aspirin NSAIDs (P trend = 0. 53). Analyses by histologic type and smoking status yielded similar <b>null</b> results. <b>Information</b> on dosage and duration of use were not available for this analysis. Conclusion: These findings do not suggest that aspirin or other NSAIDs reduce risk of lung cancer in this cohort of postmenopausal women. (Cancer Epidemiol Biomarkers Prev 2006; 15 (11) : 2226 – 31...|$|R
40|$|Abstract. Measures of {{quantity}} of information {{have been studied}} extensively for more than fifty years. The seminal work on information theory is by Shannon [67]. This work, based on probability theory, {{can be used in}} a logical setting when the worlds are the possible events. This work is also the basis of Lozinskii’s work [48] for defining the {{quantity of}} information of a formula (or knowledgebase) in propositional logic. But this definition is not suitable when the knowledgebase is inconsistent. In this case, it has no classical model, so we have no “event ” to count. This is a shortcoming since in practical applications (e. g. databases) it often happens that the knowledgebase is not consistent. And it is definitely not true that all inconsistent knowledgebases contain the same (<b>null)</b> amount of <b>information,</b> as given by the “classical information theory”. As explored for several years in the paraconsistent logic community, two inconsistent knowledgebases can lead to very different conclusions, showing that they do not convey the same information. There has been som...|$|R
40|$|A {{stationary}} surface slanted {{around the}} vertical axis and observed during a head translation {{appears to be}} stationary during monocular, but not binocular viewing when the same image is projected to the two eyes (null disparity). This effect is likely caused by the perceptual interpretation of the optic flow affected by the <b>null</b> disparity <b>information,</b> rather than by extra-retinal signals coming from vergence and accommodation, which instead should lead to an unbiased perception (Fantoni, Domini & Caudek 2010). Here, we investigated the neural basis of this phenomenon with a rotation-detection task during active binocular and monocular viewing before and after offline inhibitory rTMS over early visual areas V 2 /V 3 and the Middle Temporal area (hMT). At baseline subjects reported a rotating object significantly more often in the binocular relative to the monocular viewing condition. Stimulation overV 2 /V 3 caused a reduction of the response bias and a general improvement of the sensitivity in the binocular viewing condition only. These findings were consistent with: I) a disruption of disparity information conflicting with motion and II) a residual effect of binocular summation occurring earlier in the visual hierarchy. Interestingly, stimulation of hMT led to a selective impairment of performance in the monocular viewing condition. Contrary to V 2 /V 3 stimulation, binocular viewing condition was not affected by hMT stimulation. This might indicate that any potential performance gain due to the inhibition of hMT disparity neurons was constrained by the performance impairment due to the inhibition of hMT motion-sensitive neurons. Overall, the results suggest that: (1) the perception of 3 D surfaces during active vision is likely mediated by the activity of cortical areas involved in the processing of retinal but not extra-retinal signals; (2) both hMT and V 2 /V 3 are critically involved in the encoding and integration of motion and disparity signals generated during active vision...|$|R
40|$|Grouping and {{clustering}} alerts for {{intrusion detection}} {{based on the}} similarity of features {{is referred to as}} structurally base alert correlation and can discover a list of attack steps. Previous researchers selected different features and data sources manually based on their knowledge and experience, which lead to the less accurate identification of attack steps and inconsistent performance of clustering accuracy. Furthermore, the existing alert correlation systems deal with a huge amount of data that contains <b>null</b> values, incomplete <b>information,</b> and irrelevant features causing the analysis of the alerts to be tedious, time-consuming and error-prone. Therefore, this paper focuses on selecting accurate and significant features of alerts that are appropriate to represent the attack steps, thus, enhancing the structural-based alert correlation model. A two-tier feature selection method is proposed to obtain the significant features. The first tier aims at ranking the subset of features based on high information gain entropy in decreasing order. The‏ second tier extends additional features with a better discriminative ability than the initially ranked features. Performance analysis results show the significance of the selected features in terms of the clustering accuracy using 2000 DARPA intrusion detection scenario-specific dataset...|$|R
40|$|Bosonic {{string theory}} with the {{possibility}} for an arbitrary number of strings - i. e. a string field theory - is formulated by a Hilbert space (a Fock space), which is just that for massless noninteracting scalars. We earlier presented this novel type of string field theory, but now we show that it leads to scattering just given by the Veneziano model amplitude. Generalization to strings with fermion modes would presumably be rather easy. It is characteristic for our formulation /model that: 1) We have thrown away some <b>null</b> set of <b>information</b> compared to usual string field theory, 2) Formulated {{in terms of our}} " (= the non-interacting scalars) there is no interaction and essentially no time development(Heisenberg picture), 3) so that the S-matrix is in our Hilbert space given as the unit matrix, S= 1, and 4) the Veneziano scattering amplitude appear as the overlap between the initial and the final state described in terms of the ". 5) The integration in the Euler beta function making up the Veneziano model appear from the summation over the number of " from one of the incoming strings which goes into a certain one of the two outgoing strings. A correction from Weyl anomaly is needed to get the correct form of the Veneziano amplitude and it only fits for 26 dimensions. Comment: 43 pages, 7 figures, To be published in Proc. of 17 th Bled Workshop 201...|$|R
40|$|Extraction of filled-in {{information}} from document {{images in the}} presence of template poses challenges due to geometrical distortion. Filled-in document image consists of <b>null</b> background, general <b>information</b> foreground and vital information imposed layer. Template document image consists of null background and general information foreground layer. In this paper a novel document image registration technique has been proposed to extract imposed layer from input document image. A convex polygon is constructed around the content of the input and the template image using convex hull. The vertices of the convex polygons of input and template are paired based on minimum Euclidean distance. Each vertex of the input convex polygon is subjected to transformation for the permutable combinations of rotation and scaling. Translation is handled by tight crop. For every transformation of the input vertices, Minimum Hausdorff distance (MHD) is computed. Minimum Hausdorff distance identifies the rotation and scaling values by which the input image should be transformed to align it to the template. Since transformation is an estimation process, the components in the input image do not overlay exactly on the components in the template, therefore connected component technique is applied to extract contour boxes at word level to identify partially overlapping components. Geometrical features such as density, area and degree of overlapping are extracted and compared between partially overlapping components to identify and eliminate components common to input image and template image. The residue constitutes imposed layer. Experimental results indicate the efficacy of the proposed model with computational complexity. Experiment has been conducted on variety of filled-in forms, applications and bank cheques. Data sets have been generated as test sets for comparative analysis. ...|$|R
