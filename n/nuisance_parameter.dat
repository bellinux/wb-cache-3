542|1145|Public
2500|$|... where In is the {{identity}} matrix in dimension n, and σ2 is a parameter which determines {{the variance of}} each observation. This σ2 is considered a <b>nuisance</b> <b>parameter</b> in the model, although usually it is also estimated. If this assumption is violated then the OLS estimates are still valid, but no longer efficient.|$|E
50|$|In statistics, a <b>nuisance</b> <b>{{parameter}}</b> is any parameter {{which is}} not of immediate interest but which must be accounted for {{in the analysis of}} those parameters which are of interest. The classic example of a <b>nuisance</b> <b>parameter</b> is the variance, σ2, of a normal distribution, when the mean, μ, is of primary interest.|$|E
5000|$|Nuisance {{parameters}} are often variances, but not always; {{for example in}} an errors-in-variables model, the unknown true location of each observation is a <b>nuisance</b> <b>parameter.</b> In general, any parameter which intrudes on the analysis of another may be considered a <b>nuisance</b> <b>parameter.</b> A parameter may also cease to be a [...] "nuisance" [...] if it becomes the object of study, as the variance of a distribution may be.|$|E
40|$|In {{most of the}} cases, only a subvector of the {{parameters}} is tested in a model. The remaining parameters arise in the tests as <b>nuisance</b> <b>parameters.</b> The presence of <b>nuisance</b> <b>parameters</b> causes biases in key estimates used in the tests. So inferences made {{on the presence of}} <b>nuisance</b> <b>parameters</b> may lead to less accurate conclusions. Even the presence of <b>nuisance</b> <b>parameters</b> can destroy the test. Thus in eliminating the influence of <b>nuisance</b> <b>parameters</b> from the test can improve the tests' performance. The effect of the <b>nuisance</b> <b>parameters</b> can be eliminated by the marginal likelihood, conditional likelihood, canonical likelihood, profile likelihood and Bayesian tests. This paper is concerned with marginal likelihood-based test for eliminating the influence of <b>nuisance</b> <b>parameters.</b> In general, existing one-sided and two-sided tests for autocorrelation are tested only autocorrelation coefficients but not the regression coefficients in the model. So we proposed a distance-based marginal likelihood one-sided Likelihood Ratio (DMLR) test in eliminating the influence of <b>nuisance</b> <b>parameters</b> for testing higher order autocorrelation with one-sided alternatives in linear regression model using marginal likelihood and distance-based approach. Monte Carlo simulations are conducted to compare power properties of the proposed DMLR test with their respective conventional counterparts. It is found that the DMLR test shows substantially improved power for most of cases considered. <br /...|$|R
50|$|The {{difference}} between Barnard's exact test and Fisher's exact test {{is how they}} handle the <b>nuisance</b> <b>parameter(s)</b> of the common success probability when calculating the p-value. Fisher's test avoids estimating the <b>nuisance</b> <b>parameter(s)</b> by conditioning on the margins, an approximately ancillary statistic. Barnard's test considers all possible values of the <b>nuisance</b> <b>parameter(s)</b> and chooses the value(s) that maximizes the p-value.|$|R
50|$|Sometimes it is {{possible}} to find a sufficient statistic for the <b>nuisance</b> <b>parameters,</b> and conditioning on this statistic results in a likelihood which does not depend on the <b>nuisance</b> <b>parameters.</b>|$|R
50|$|Adaptive {{estimator}} {{estimates the}} parameter of interest equally well regardless whether {{the value of}} the <b>nuisance</b> <b>parameter</b> is known or not.|$|E
50|$|The term {{nuisance}} {{variable is}} sometimes {{also used in}} more general contexts, simply to designate those variables that are marginalised over when finding a marginal distribution. In particular, the term may sometimes {{be used in the}} context of Bayesian analysis as an alternative to <b>nuisance</b> <b>parameter,</b> given that Bayesian statistics allows parameters to be treated as having probability distributions. However this is usually avoided as the term <b>nuisance</b> <b>parameter</b> has a specific meaning in statistical theory.|$|E
5000|$|Andrews, Donald W. K. & Ploberger, Werner, [...] "Optimal Tests When a <b>Nuisance</b> <b>Parameter</b> Is Present Only Under the Alternative". Econometrica, 62(6), 1994.|$|E
50|$|In statistics, an {{adaptive}} estimator is an estimator in a parametric or semiparametric model with <b>nuisance</b> <b>parameters</b> {{such that the}} presence of these <b>nuisance</b> <b>parameters</b> does not affect efficiency of estimation.|$|R
40|$|In many {{practical}} {{signal detection}} problems, the detectors have to designed from training data. Due to limited training data, {{which is usually}} the case, {{it is imperative to}} exploit some inherent signal structure for reliable detector design. The signals of interest in a variety of applications manifest such structure in the form of <b>nuisance</b> <b>parameters.</b> However, data-driven design of detectors by exploiting <b>nuisance</b> <b>parameters</b> is virtually impossible in general due to two major difficulties: identifying the appropriate <b>nuisance</b> <b>parameters,</b> and estimating the corresponding detector statistics. We address this problem by using recent results that relate joint signal representations (JSRs), such as time-frequency and time-scale representations, to quadratic detectors {{for a wide variety of}} <b>nuisance</b> <b>parameters.</b> We propose a general data-driven framework that: 1) identifies the appropriate <b>nuisance</b> <b>parameters</b> from an arbitrarily chosen finite set, and 2) estimates the second-order statistics [...] ...|$|R
30|$|In the {{following}} section, we detail the statistical test {{that takes into}} account both the expectation and the variance as <b>nuisance</b> <b>parameters,</b> and we study the optimal detection when those parameters are known. A discussion on <b>nuisance</b> <b>parameters</b> is also provided in Section 4.|$|R
5000|$|... where In is the {{identity}} matrix in dimension n, and σ2 is a parameter which determines {{the variance of}} each observation. This σ2 is considered a <b>nuisance</b> <b>parameter</b> in the model, although usually it is also estimated. If this assumption is violated then the OLS estimates are still valid, but no longer efficient.|$|E
5000|$|... with loglikelihood,Here, [...] is the {{canonical}} parameter and the parameter of interest, and [...] is a <b>nuisance</b> <b>parameter</b> which {{plays a role}} in the variance.We use the Bartletts Identities to derive a general expression for the variance function.The first and second Bartlett results ensures that under suitable conditions ((see Leibniz integral rule)), for a density function dependent on , ...|$|E
5000|$|Formally, let {{parameter}} θ in a parametric model {{consists of}} two parts: the parameter of interest ν ∈ N ⊆ Rk, and the <b>nuisance</b> <b>parameter</b> η ∈ H ⊆ Rm. Thus θ = (ν,η) ∈ N×H ⊆ Rk+m. Then we will say that [...] is an adaptive estimator of ν {{in the presence of}} η if this estimator is regular, and efficient for each of the submodels ...|$|E
5000|$|... #Subtitle level 2: Likelihoods that {{eliminate}} <b>nuisance</b> <b>parameters</b> ...|$|R
40|$|This {{manuscript}} considers inference on {{a single}} parameter in a multivariate canonical exponential family, where the effect of <b>nuisance</b> <b>parameters</b> on the p-value is mitigated by conditioning on {{the event that the}} sufficient statistics associated with the <b>nuisance</b> <b>parameters</b> lie in a neighborhood about the observed value. This manuscript has three aims. First, we provide a method for approximating p-values using approximate conditioning that is more accurate than that presented by Pierce and Peters (Biometrika 86 (1999) 265 - 277), at the price of greater computational difficulty. Second, we examine the sensitivity of approximate conditioning methods to the values of the <b>nuisance</b> <b>parameters.</b> Third, we describe a method for presenting a valid approximate-conditioning observed significance level accounting for this dependence on the <b>nuisance</b> <b>parameters.</b> Approximate conditional inference Saddlepoint approximation...|$|R
40|$|Many {{kinds of}} {{computer}} vision {{problems can be}} formalized as statistical estimation problems with <b>nuisance</b> <b>parameters.</b> In the past, such problems have been solved without making any distinction between the <b>nuisance</b> <b>parameters</b> and structural ones. However, a theory of statistics suggests that eliminating the <b>nuisance</b> <b>parameters</b> by assuming a probability distribution on them improves estimation accuracy of the structural parameters. In this paper, we apply this strategy to problem of estimating motion parameters from optical flow, which is a typical computer vision problem, and compare the estimation accuracy with that obtained by the conventional estimation method...|$|R
50|$|When the {{likelihood}} function depends on many parameters, {{depending on the}} application, we {{might be interested in}} only a subset of these parameters. It is often possible {{to reduce the number of}} the uninteresting (nuisance) parameters by writing them as functions of the parameters of interest. For example, the functions might be the value of the <b>nuisance</b> <b>parameter</b> which maximizes {{the likelihood}} given the value of the other (interesting) parameters.|$|E
5000|$|The above {{definition}} is phrased {{in the context}} of Bayesian statistics. In classical (frequentist) statistics, the concept of marginal likelihood occurs instead {{in the context of}} a joint parameter θ=(ψ,λ), where ψ is the actual parameter of interest, and λ is a non-interesting <b>nuisance</b> <b>parameter.</b> If there exists a probability distribution for λ, it is often desirable to consider the likelihood function only in terms of ψ, by marginalizing out λ: ...|$|E
5000|$|Nuisance {{parameters}} {{should be}} known, or estimated with high accuracy (an {{example of a}} <b>nuisance</b> <b>parameter</b> would be the standard deviation in a one-sample location test). Z-tests focus on a single parameter, and treat all other unknown parameters as being fixed at their true values. In practice, due to Slutsky's theorem, [...] "plugging in" [...] consistent estimates of nuisance parameters can be justified. However if the sample size is not large enough for these estimates to be reasonably accurate, the Z-test may not perform well.|$|E
40|$|We {{consider}} two-sample bootstrap {{tests for}} testing equality {{of a given}} functional in two populations. The usual bootstrap analogue of the permutation test "mixes " the two samples prior to resampling. It is shown that {{in the presence of}} <b>nuisance</b> <b>parameters</b> this test can be invalid or. even if valid. can have bad power characteristics. An alternative bootstrap test procedure is suggested which "mixes " only after resampling from the two separate samples. The new procedure is valid in the presence of <b>nuisance</b> <b>parameters</b> and has better power than the usual procedure under fairly general conditions. Key words: Bootstrap; Penmutation tests; <b>Nuisance</b> <b>parameters...</b>|$|R
5000|$|These {{approaches}} are useful because standard likelihood methods can become unreliable or fail entirely {{when there are}} many <b>nuisance</b> <b>parameters</b> or when the <b>nuisance</b> <b>parameters</b> are high-dimensional. This is particularly true when the <b>nuisance</b> <b>parameters</b> can {{be considered to be}} [...] "missing data"; they represent a non-negligible fraction of the number of observations and this fraction does not decrease when the sample size increases. Often these approaches can be used to derive closed-form formulae for statistical tests when direct use of maximum likelihood requires iterative numerical methods. These approaches find application in some specialized topics such as sequential analysis.|$|R
5000|$|... #Subtitle level 3: The {{case of a}} {{likelihood}} with <b>nuisance</b> <b>parameters</b> ...|$|R
5000|$|A {{well-known}} {{example of}} a semiparametric model is the Cox proportional hazards model. If {{we are interested in}} studying the time [...] to an event such as death due to cancer or failure of a light bulb, the Cox model specifies the following distribution function for :where [...] is the covariate vector, and [...] and [...] are unknown parameters[...] Here [...] is finite-dimensional and is of interest; [...] is an unknown non-negative function of time (known as the baseline hazard function) and is often a <b>nuisance</b> <b>parameter.</b> The collection of possible candidates for [...] is infinite-dimensional.|$|E
50|$|GLMs {{essentially}} cover one-parameter {{models from}} the classical exponential family,and include 3 {{of the most}} important statistical regression models:the linear model, Poisson regression for counts, and logistic regressionfor binary responses.However, the exponential family is far too limiting for regular data analysis.For example, for counts, zero-inflation, zero-truncation and overdispersion are regularlyencountered, and the makeshift adaptations made to the binomial andPoisson models in the form of quasi-binomial andquasi-Poisson can be argued as being ad hoc and unsatisfactory.But the VGLM framework readily handles models such aszero-inflated Poisson regression,zero-altered Poisson (hurdle) regression,positive-Poisson regression, andnegative binomial regression.As another example, for the linear model,the variance of a normal distribution is relegated as a scale parameter and it is treatedoften as a <b>nuisance</b> <b>parameter</b> (if it is considered as a parameter at all).But the VGLM framework allows the variance to be modelled using covariates.|$|E
5000|$|Many {{statistical}} analyses involve {{the estimation of}} several unknown quantities. In simple cases, {{all but one of}} these quantities is a <b>nuisance</b> <b>parameter.</b> In this setting, the only relevant power pertains to the single quantity that will undergo formal statistical inference. In some settings, particularly if the goals are more [...] "exploratory", there may be a number of quantities of interest in the analysis. For example, in a multiple regression analysis we may include several covariates of potential interest. In situations such as this where several hypotheses are under consideration, it is common that the powers associated with the different hypotheses differ. For instance, in multiple regression analysis, the power for detecting an effect of a given size is related to the variance of the covariate. Since different covariates will have different variances, their powers will differ as well.|$|E
40|$|In this paper, we {{investigate}} whether similar improvements are observed {{when we have}} a non-sample information regarding the <b>nuisance</b> <b>parameters</b> in the testing problem. It is shown under several realistic regularity conditions, that tests based on inequality constrained estimates of <b>nuisance</b> <b>parameters</b> and tests based on unconstrained estimates have identical asymptotic properties. testing, simulation...|$|R
5000|$|... {{credible}} intervals {{and confidence}} intervals treat <b>nuisance</b> <b>parameters</b> in radically different ways.|$|R
40|$|AbstractWe {{consider}} the incidental parameters {{problem in this}} paper, i. e. the estimation for {{a small number of}} parameters of interest {{in the presence of a}} large number of <b>nuisance</b> <b>parameters.</b> By assuming that the observations are taken from a multiple strictly stationary process, the two estimation methods, namely the maximum composite quasi-likelihood estimation (MCQLE) and the maximum plug-in quasi-likelihood estimation (MPQLE) are considered. For the MCQLE, we profile out <b>nuisance</b> <b>parameters</b> based on lower-dimensional marginal likelihoods, while the MPQLE is based on some initial estimators for <b>nuisance</b> <b>parameters.</b> The asymptotic normality for both the MCQLE and the MPQLE is established under the assumption that the number of <b>nuisance</b> <b>parameters</b> and the number of observations go to infinity together, and both the estimators for the parameters of interest enjoy the standard root-n convergence rate. Simulation with a spatial–temporal model illustrates the finite sample properties of the two estimation methods...|$|R
50|$|A quasi-maximum {{likelihood}} estimate (QMLE, {{also known}} as a pseudo-likelihood estimate or a composite likelihood estimate) is an estimate of a parameter θ in a statistical model that is formed by maximizing a function that is related to the logarithm of the likelihood function, but is not equal to it. In contrast, the maximum likelihood estimate maximizes the actual log likelihood function for the data and model. The function that is maximized to form a QMLE is often a simplified form of the actual log likelihood function. A common way to form such a simplified function is to use the log-likelihood function of a misspecified model that treats certain data values as being independent, even when in actuality they may not be. This removes any parameters from the model that are used to characterize these dependencies. Doing this only makes sense if the dependency structure is a <b>nuisance</b> <b>parameter</b> with respect to the goals of the analysis.|$|E
40|$|Lazar & Mykland (1999) {{showed that}} an {{empirical}} likelihood defined by two estimating equations with a <b>nuisance</b> <b>parameter</b> {{need not be}} Bartlett-correctable. This paper shows that Bartlett correction of empirical likelihood {{in the presence of}} a <b>nuisance</b> <b>parameter</b> depends critically on the way the <b>nuisance</b> <b>parameter</b> is removed when formulating the likelihood for the parameter of interest. We establish in the broad framework of estimating functions that the empirical likelihood is still Bartlett-correctable if the <b>nuisance</b> <b>parameter</b> is profiled out given the value of the parameter of interest. Copyright 2006, Oxford University Press. ...|$|E
40|$|In this article, {{a simple}} {{algorithm}} {{is used to}} maximize a family of optimal statistics for hypothesis testing with a <b>nuisance</b> <b>parameter</b> not defined under the null hypothesis. This arises from genetic linkage and association studies and other hypothesis testing problems. The maximum of optimal statistics over the <b>nuisance</b> <b>parameter</b> space {{can be used as}} a robust test in this situation. Here, we use the maximum and minimum statistics to examine the sensitivity of testing results with respect to the unknown <b>nuisance</b> <b>parameter.</b> Examples from genetic linkage analysis using affected sub pairs and a candidate-gene association study in case-parents trio design are studied. Genetic Analysis, Maximal Statistics, <b>Nuisance</b> <b>Parameter,</b> Robust Test,...|$|E
40|$|In {{this paper}} {{we show that}} the well­known {{asymptotic}} efficiency bounds for full mixture models remain valid if individual sequences of <b>nuisance</b> <b>parameters</b> are considered. This is made precise both for some classes of random (i. i. d.) and non­random <b>nuisance</b> <b>parameters.</b> For the random case it is shown that superefficiency of the kind given by an example of Pfanzagl (1993) can happen only with low probability. The non-random case deals with permutation invariant estimators under one­dimensional <b>nuisance</b> <b>parameters.</b> It is shown that the efficiency bounds remain valid for individual non­random arrays of <b>nuisance</b> <b>parameters</b> whose empirical process, if it is centered around its limit and standardized, satisfies a compactness condition. The compactness condition is satisfied in the random case with high probability. The results make use of basic LAN-theory. Regularity conditions are {{stated in terms of}} L^ 2 ­differentiability. (authors' abstract) Series: Forschungsberichte / Institut für Statisti...|$|R
40|$|This {{paper is}} {{concerned}} with a paradox associated with parameter estimation {{in the presence of}} <b>nuisance</b> <b>parameters.</b> In a statistical model with unknown <b>nuisance</b> <b>parameters,</b> the efficiency of an estimator of a parameter usually increases when the <b>nuisance</b> <b>parameters</b> are known. However the opposite phenomenon can sometimes occur. In this paper, we elucidate the occurrence of this paradox by examining estimating functions. In particular, we focus on the projected estimating function, which is defined by the projection of the score function on to a given estimating function. A sufficient condition for the paradox to occur is the orthogonality of the two components of the projected estimating functions corresponding to parameters of interest and <b>nuisance</b> <b>parameters.</b> In addition, a numerical assessment is conducted {{in the context of a}} simple model to investigate the improvement of the asymptotic efficiency of estimators. Copyright 2004, Oxford University Press. ...|$|R
40|$|We {{consider}} the incidental parameters {{problem in this}} paper, i. e. the estimation for {{a small number of}} parameters of interest {{in the presence of a}} large number of <b>nuisance</b> <b>parameters.</b> By assuming that the observations are taken from a multiple strictly stationary process, the two estimation methods, namely the maximum composite quasi-likelihood estimation (MCQLE) and the maximum plug-in quasi-likelihood estimation (MPQLE) are considered. For the MCQLE, we profile out <b>nuisance</b> <b>parameters</b> based on lower-dimensional marginal likelihoods, while the MPQLE is based on some initial estimators for <b>nuisance</b> <b>parameters.</b> The asymptotic normality for both the MCQLE and the MPQLE is established under the assumption that the number of <b>nuisance</b> <b>parameters</b> and the number of observations go to infinity together, and both the estimators for the parameters of interest enjoy the standard root-nn convergence rate. Simulation with a spatial–temporal model illustrates the finite sample properties of the two estimation methods...|$|R
