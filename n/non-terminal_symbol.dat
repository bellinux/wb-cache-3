27|59|Public
2500|$|Terminals in a grammar {{are words}} {{and through the}} grammar rules a <b>non-terminal</b> <b>symbol</b> is {{transformed}} into a string of either terminals and/or non-terminals. The above grammar is read as [...] "beginning from a non-terminal [...] the emission can generate either [...] or [...] or [...] ".|$|E
5000|$|... where , , ... {{are each}} either a {{terminal}} string or a <b>non-terminal</b> <b>symbol.</b>|$|E
50|$|Predictive parsers can be {{depicted}} using transition diagrams {{for each}} <b>non-terminal</b> <b>symbol</b> where the edges between the initial {{and the final}} states are labelled by the symbols (terminals and non-terminals) of {{the right side of}} the production rule.|$|E
5000|$|Rewrite {{rules of}} a DCPSG are {{identical}} to those of a CFG, {{with the addition of}} a meta-symbol, denoted here as an underscore. DCPSG rules therefore have the general form [...] where [...] is a string of terminal <b>symbols</b> and/or <b>non-terminal</b> <b>symbols</b> and at most one underscore.|$|R
50|$|For readability, the CYK {{table for}} P is {{represented}} {{here as a}} 2-dimensional matrix M containing a set of <b>non-terminal</b> <b>symbols,</b> such that Rk is in Mi,j if, and only if, Pi,j,k.In the above example, since a start symbol S is in M7,1, the sentence can be generated by the grammar.|$|R
5000|$|... {{an element}} content, {{which means that}} there must be no text {{elements}} in the children elements of the content (all whitespaces encoded between child elements are then ignored, just like comments). Such element content is specified as content particle in a variant of Backus-Naur form without terminal symbols and element names as <b>non-terminal</b> <b>symbols.</b> Element content consists of: ...|$|R
5000|$|Terminals in a grammar {{are words}} {{and through the}} grammar rules a <b>non-terminal</b> <b>symbol</b> is {{transformed}} into a string of either terminals and/or non-terminals. The above grammar is read as [...] "beginning from a non-terminal [...] the emission can generate either [...] or [...] or [...] ".Its derivation is: ...|$|E
50|$|The rewrite {{semantics}} of GCFGs {{is fairly}} straightforward. An occurrence of a <b>non-terminal</b> <b>symbol</b> is rewritten using rewrite rules as in a context-free grammar, eventually yielding just compositions (composition functions applied to string tuples or other compositions). The composition functions are then applied, successively reducing the tuples {{to a single}} tuple.|$|E
50|$|A {{recursive}} grammar is a grammar {{that contains}} production rules that are recursive. For example, a grammar for a context-free language is left-recursive if {{there exists a}} <b>non-terminal</b> <b>symbol</b> A that can be put through the production rules to produce a string with A as the leftmost symbol.All types of grammars in the Chomsky hierarchy can be recursive.|$|E
5000|$|A GCFG {{consists}} of two components: a set of composition functions that combine string tuples, {{and a set of}} rewrite rules. The composition functions all have the form , where [...] is either a single string tuple, or some use of a (potentially different) composition function which reduces to a string tuple. Rewrite rules look like , where , , ... are string tuples or <b>non-terminal</b> <b>symbols.</b>|$|R
5000|$|... where f is any index symbol, [...] is any {{string of}} {{terminals}} and/or <b>non-terminal</b> <b>symbols,</b> and x is a terminal is a terminal symbol. Because occasionally a rewrite rule {{might need to}} be conditioned on the stack being in some sense empty, the symbol # is used as the bottom-most stack symbol, meaning an [...] "empty" [...] stack contains exactly one symbol, #.|$|R
3000|$|From the {{recommendation}} system’s perspective, we interpret a service composition step as {{an application of}} a composition rule that compactly describes a formally correct modification during the composition process. The syntax of composition rules {{is identical to the}} syntax of production rules for specifying a formal grammar. A grammar G is defined by the tuple (N,Σ,P,S), where N denotes a finite set of <b>non-terminal</b> <b>symbols,</b> Σ denotes a finite set of terminal symbols, P denotes a finite set of production rules, and S∈N denotes a distinguished start symbol. In our context, <b>non-terminal</b> <b>symbols</b> correspond to functionality that still has to be realized, i.e., the remaining path in the search tree from the current node to the goal node. Terminal symbols correspond to concrete services, which cannot be replaced anymore. The start symbol corresponds to the formally specified request. In case of our example, it corresponds to the path from initial state q [...]...|$|R
50|$|For example, a grammar for a {{context-free}} {{language is}} (left-)recursive if {{there exists a}} <b>non-terminal</b> <b>symbol</b> A that can be put through the production rules to produce a string with A (as the leftmost symbol).All types of grammars in the Chomsky hierarchy can be recursive and it is recursion that allows the production of infinite sets of words.|$|E
50|$|In logic, arrows usually point left to right. In {{this article}} this {{convention}} isreversed for consistency with the notation of context-free grammars, where thesingle <b>non-terminal</b> <b>symbol</b> {{is always on}} the left. We use the symbol in a production rule as in Backus-Naur form. Some authors use an arrow, whichunfortunately may point in either direction, depending on whether the grammar isthought of as generating or recognizing the language.|$|E
50|$|An Indian {{parallel}} grammar {{is simply}} a CFG in which to use a rewrite rule, all instances of the rules <b>non-terminal</b> <b>symbol</b> must be rewritten simultaneously. Thus, for example, given the string aXbYcXd, with two instances of X, and some rule , {{the only way to}} rewrite this string with this rule is to rewrite it as awbYcwd; neither awbYcXd nor aXbYcwd are valid rewrites in an Indian parallel grammar, because they did not rewrite all instances of X.|$|E
50|$|A k-grammar is {{yet another}} kind of {{parallel}} grammar, very different from an Indian parallel grammar, but still {{with a level of}} parallelism. In a k-grammar, for some number k, exactly k <b>non-terminal</b> <b>symbols</b> must be rewritten at every step (except the first step, where the only symbol in the string is the start symbol). If the string has less than k non-terminals, the derivation fails.|$|R
50|$|The reverse {{construction}} is also possible. Given some Turing machine, {{it is possible}} to create an equivalent unrestricted grammar which even uses only productions with one or more <b>non-terminal</b> <b>symbols</b> on their left-hand sides. Therefore, an arbitrary unrestricted grammar can always be equivalently converted to obey the latter form, by converting it to a Turing machine and back again. Some authors use the latter form as definition of unrestricted grammar.|$|R
40|$|An {{important}} {{problem when}} using Stochastic Inversion Transduction Grammars is their computational cost. More specifically, {{when dealing with}} corpora such as Europarl only one iteration of the estimation algorithm becomes prohibitive. In this work, we apply a reduction of the cost by taking profit of the bracketing information in parsed corpora and show machine translation results obtained with a bracketed Europarl corpus, yielding interresting improvements when {{increasing the number of}} <b>non-terminal</b> <b>symbols.</b> 1...|$|R
50|$|In {{the case}} of grammar induction, the {{transplantation}} of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a terminal symbol of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a <b>non-terminal</b> <b>symbol</b> (e.g. a noun phrase or a verb phrase) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.|$|E
50|$|A {{compiler}} parses {{input from}} {{a programming language}} to an internal representation by matching the incoming symbols to production rules. Production rules are commonly defined using Backus-Naur form. An LL parser {{is a type of}} parser that does top-down parsing by applying each production rule to the incoming symbols, working from the left-most symbol yielded on a production rule and then proceeding to the next production rule for each <b>non-terminal</b> <b>symbol</b> encountered. In this way the parsing starts on the Left of the result side (right side) of the production rule and evaluates non-terminals from the Left first and, thus, proceeds down the parse tree for each new non-terminal before continuing to the next symbol for a production rule.|$|E
5000|$|The linearly indexed {{languages}} {{are a subset}} of the indexed languages, and thus all LIGs can be recoded as IGs, making the LIGs strictly less powerful than the IGs. A conversion from a LIG to an IG is relatively simple. LIG rules in general look approximately like , modulo the push/pop part of a rewrite rule. The symbols [...] and [...] represent strings of terminal and/or non-terminal symbols, and any <b>non-terminal</b> <b>symbol</b> in either must have an empty stack, by the definition of a LIG. This is, of course, counter to how IGs are defined: in an IG, the non-terminals whose stacks are not being pushed to or popped from must have exactly the same stack as the rewritten non-terminal. Thus, somehow, we need to have non-terminals in [...] and [...] which, despite having non-empty stacks, behave as if they had empty stacks.|$|E
40|$|In {{this paper}} the tool TELIOS is presented, for the {{automatic}} generation of a hardware machine, corresponding {{to a given}} logic program. The machine is implemented using an FPGA, where a corresponding inference machine, in application specific hardware, is created on the FPGA, based on a BNF parser, {{to carry out the}} inference mechanism. The unification mechanism is based on actions embedded between the <b>non-terminal</b> <b>symbols</b> and implemented using special modules on the FPGA. © 2009 International Federation for Information Processing...|$|R
40|$|This paper {{presents}} an approach for inducing transformation rules that map natural-language sentences into a formal semantic representation language. The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the <b>non-terminal</b> <b>symbols</b> in this grammar. Patterns for the transformation rules are learned using an induction algorithm based on longestcommon-subsequences previously developed for an information extraction system. Experimental {{results are presented}} on learning to map English coaching instructions for Robocup soccer into an existing formal language for coaching simulated robotic agents. ...|$|R
30|$|In this grammar, <b>non-terminal</b> <b>symbols</b> {{are written}} between and (e.g. domain). Brackets denote {{optional}} symbols (e.g. [!], indicating that ! is optional). Braces {{indicate that the}} delimited element may have zero or more repetitions (e.g. domain). Terminal symbols are written without special delimiters (e.g. alert, access, etc.). The non-terminal string denotes a sequence of characters. In the specification of domains, the operator ! means complement. For example, !A denotes a domain containing any object that {{is not included in}} A. Finally, the symbol * matches any object, regardless of its domain.|$|R
30|$|Left-hand side of {{rewriting}} rule contains node <b>non-terminal</b> <b>symbol</b> with the context on a left defined by traversing parent nodes {{up to the}} root inclusively and concatenating their labels.|$|E
30|$|Whenever a new {{composition}} process starts, {{the composition}} module notifies the recommendation module {{by means of}} an initialization message (Interaction 1 in Figure 6) containing the initial state and the goal state of the composition task. The CRM identifies (or generates) the <b>non-terminal</b> <b>symbol</b> that corresponds to the desired functionality. Subsequently, the TDL marks the respective non-terminal as initial state for the upcoming search process.|$|E
40|$|We {{consider}} grammar-based text compression with longest first substitution (LFS), where non-overlapping {{occurrences of}} a longest repeating factor of the input text {{are replaced by}} a new <b>non-terminal</b> <b>symbol.</b> We present the first linear-time algorithm for LFS. Our algorithm employs a new data structure called sparse lazy suffix trees. We also deal with a more sophisticated version of LFS, called LFS 2, that allows better compression. The first linear-time algorithm for LFS 2 is also presented...|$|E
40|$|Pattern-based machine {{translation}} {{systems can be}} easily customized by adding new patterns. To gain full profits from this character, input of patterns should be both expressive and simple to understand. The pattern-based {{machine translation}} system we have developed simplifies the handling of features in patterns by allowing sharing constraints between <b>non-terminal</b> <b>symbols,</b> and implementing an automated scheme of feature inheritance between syntactic classes. To avoid conflicts inherent to the pattern-based approach the system has priority control between patterns and between dictionaries. This approach proved its scalability in the web-based collaborative translation environment ‘Yakushite Net. ...|$|R
40|$|Abstract. Grammatical Evolution is an {{automatic}} programming system, where {{a population of}} binary strings is evolved, from which phenotype strings are generated through a mapping process, that employs a grammar to define the syntax of such output strings. This paper presents {{a study of the}} effect of grammar size and complexity on the performance of the system. A simple method {{to reduce the number of}} <b>non-terminal</b> <b>symbols</b> in a grammar is presented, along with the reasoning behind it. Results obtained on a series of problems suggest that performance can be increased with the approach presented. ...|$|R
40|$|AbstractIn {{addressing}} certain problems about membrane computing, {{a recent}} and active branch of natural computing, it first {{was necessary to}} address certain problems from the area of regulated rewriting. Thus, the present paper is a contribution to both these domains. A central problem in membrane computing {{is that of the}} hierarchy with respect to the number of membranes: Are systems with n+ 1 membranes more powerful than systems with n membranes? Does the number of membranes induce an infinite hierarchy of the computed functions? Usually, when proving the universality of membrane systems (also called P systems), one starts from a matrix grammar and the number of membranes depends on the number of <b>non-terminal</b> <b>symbols</b> used by this grammar in the so-called appearance checking mode. We first prove that recursively enumerable languages can be generated by matrix grammars with only two <b>non-terminal</b> <b>symbols</b> being used in the appearance checking mode. The proofs of this fact and of several related results are based on a simulation of register machines by means of graph-controlled grammars. Then, we consider three classes of membrane systems, and in all the three cases the hierarchies with respect to the number of membranes are shown to collapse at level four: systems with four membranes are computationally universal (but we do not know whether or not this result is optimal) ...|$|R
40|$|We {{consider}} grammar based text compression with longest first substitution, where non-overlapping {{occurrences of}} a longest repeating substring of the input text {{are replaced by}} a new <b>non-terminal</b> <b>symbol.</b> We present a new text compression algorithm by simplifying the algorithm presented in [4]. We give a new formulation of the correctness proof introducing the sparse lazy suffix tree data structure. We also present another type of longest first substitution strategy that allows better compression. We show results of preliminary experiments comparing grammar sizes of the {{two versions of the}} longest first strategy and the most frequent strategy. ...|$|E
40|$|We {{study the}} {{computational}} complexity of reachability, coverability and inclusion for extensions of context-free commutative grammars with integer counters and reset operations on them. Those grammars can alternatively {{be viewed as}} an extension of communication-free Petri nets. Our main results are that reachability and coverability are inter-reducible and both NP-complete. In particular, this class of commutative grammars enjoys semi-linear reachability sets. We also show that the inclusion problem is, in general, coNEXP-complete and already Π_ 2 ^P-complete for grammars with only one <b>non-terminal</b> <b>symbol.</b> Showing the lower bound for the latter result requires us to develop a novel Π_ 2 ^P-complete variant of the classic subset sum problem. Comment: 33 page...|$|E
40|$|Abstract. This {{paper is}} focused on the process of {{computing}} First Sets. The First Sets are used to build structures which control a syntax analyser (also known as parser). Three methods of creating First Sets were compared in terms of execution time. The first method is known sequential algorithm and the author’s own methods are concurrent computing sets for each <b>non-terminal</b> <b>symbol</b> (called the CEN method) and concurrent computing sets for each production (called the CEP method). These methods have been tested on personal computer. Three programming languages (including the C language) were used in the research. The results and the analysis of calculations allow the author to hypothesise that the problem of computing First Sets is hard to concurrence...|$|E
40|$|This paper {{defines a}} {{generative}} probabilistic model of parse trees, {{which we call}} PCFG-LA. This model {{is an extension of}} PCFG in which <b>non-terminal</b> <b>symbols</b> are augmented with latent variables. Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86. 6 % (F ¥, sentences ¦ 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. ...|$|R
40|$|The {{most popular}} {{algorithms}} for {{the estimation of}} the probabilities of a context-free grammar are the Inside-Outside algorithm and the Viterbi algorithm, which are Maximum Likelihood approaches. The difference between the logarithm of {{the likelihood of a}} string and the logarithm of the likelihood of the most probable parse of a string is upper bounded linearly by the length of the string and the logarithm of the number of <b>non-terminal</b> <b>symbols.</b> However, this theoretical bound is too pessimistic. For this reason, an experimental work to show the behaviour of the two functions in practical cases is necessary...|$|R
3000|$|Experts can {{comment on}} {{real-world}} problems and events using comparative linguistic expressions. These type hesitant linguistic expressions and linguistic terms {{in a natural}} language can be generated using context-free grammar that is shown as G_H [...]. A context-free grammar G_H [...] is defined based on the 4 -tuple (V_N, V_T, I, P), where V_N [...] is the set of <b>non-terminal</b> <b>symbols</b> such as primary term, composite term; V_T [...] is the set of terminal symbols such greater than, lower than; I is the starting symbol and an element of V_N and the production rules, P are defined based on {{the extension of the}} Backus–Naur form [17].|$|R
