15|37|Public
40|$|In {{this paper}} we show the {{possibility}} to formalize the design process by means of one type of <b>non-standard</b> <b>logic</b> - modal logic [1]. The type chosen for this study is modal logic S 4. The reason for this choice {{is the ability of}} this formalism to describe modeling of the individual discrete steps of design, respecting necessity or possibility types of design knowledge...|$|E
40|$|In an {{adaptive}} logic APL, {{based on a}} (monotonic) <b>non-standard</b> <b>logic</b> PL the consequences of Gamma can be {{defined in terms of}} a selection of the PL-models of Gamma. An important property of the adaptive logics ACLuN 1, ACLuN 2, ACLuNs 1, and ACLuNs 2 logics is proved: whenever a model is not selected, this is justified in terms of a selected model (Strong Reassurance). The property fails for Priest's LPm because its way of measuring the degree of abnormality of a model is incoherent - correcting this delivers the property...|$|E
40|$|The goal of {{this paper}} is to explain the usage and {{semantics}} of hierarchical defaults in logical specifications. We discuss the usefulness of defaults for different specification scenarios like specialization, aggregation, explanation, revision, etc. To understand defaults formally, we introduce a general framework parameterized on the underlying logical institution extended by an instantiation mechanism for formulae. It is shown that hierarchical defaults have intended models if the extended institution is compact. As an example for a <b>non-standard</b> <b>logic,</b> we give the semantics of defaults in the multi-modal object calculus of the is-core project. To structure and compose specifications with defaults, default-preserving specification morphisms are defined and corresponding colimit constructions are sketched. ...|$|E
5000|$|... "Contributions à Philippe Smets et al.(eds.)", <b>Non-Standard</b> <b>Logics</b> for Automated Reasoning, London, Academic Press, 1988 ...|$|R
40|$|We {{present a}} {{mechanism}} for recovering consistent data from inconsistent set of assertions. For a common family of knowledge-bases we also provide an efficient algorithm for doing so automaticly. This method is nonmonotonic and paraconsistent. It is particularly useful for making diag-noses on faulty devices. Subject areas: Knowledge-based systems, <b>Non-standard</b> <b>logics</b> for AI, Reasoning under uncertainty, Model-based diagnosis. 1...|$|R
40|$|One of the {{obstacles}} {{against the use of}} tableau-based theorem provers for <b>non-standard</b> <b>logics</b> is the inefficiency of tableau systems in practical applications, though they are highly intuitive and extremely flexible from a proof theoretical point of view. We present a method for increasing the efficiency of tableau systems in the case of multiple-valued logics by introducing a generalized notion of signed formulas and give sound and complete tableau systems for arbitrary propositional finite-valued logics. Introduction One of the main advantages of the method of semantic tableaux [Smullyan, 1968, Beth, 1986] is that it yields analytic proof theories {{for a wide variety of}} standard and <b>non-standard</b> <b>logics</b> within a single framework. With relatively minor modifications tableau proof systems can be designed for such different logics as temporal, intuitionistic and multiple-valued logics [Wolper, 1981, Fitting, 1983, Schmitt, 1989]. In addition, one could easily obtain tableau proof system [...] ...|$|R
40|$|The {{difficulty}} in representing disposable resources as for-mulas of classical logic motivates {{the study of}} a new logic modeling system. There are {{similarities and differences between}} resource consumption/production and theorem proving in logic. Both are about deducibility (or pro-ducibility) of a formula (or a thing). The major differ-ence is that once a disposable resource is used to produce something, it is not available any more; but a formula can be repeatedly used in deduction. We adopt a form of <b>non-standard</b> <b>logic</b> called linear logic to develop a logic model-ing system of resources and implement it in a variant of the logic programming paradigm. The resource logic mod-eling system can process state space models represented by Petri nets or their subclasses such as marked graphs and state machines. It can serve as a modeling tool for business procedures and production scheduling...|$|E
40|$|Building {{a science}} on the {{foundations}} of practitioners’ experimentation was one of Donald Schön’s abiding interests. This paper outlines a method of reflective practice research on which this kind of science can be built. Its basic elements are: (1) A focus on the “sense” practitioners make of their situations. This allows for a much richer explication of practitioners’ understanding. (2) Formally differentiating between kinds of reflective thinking to demonstrate rigorous reliance on our practice, as experienced, as we reflect. (3) Use of a <b>non-standard</b> <b>logic</b> and a distinctive grammar in describing the “sense” we make of our practice situations, as this enables a much more faithful rendering in models and theories of our sense of orientation. (4) A method of combining these in an experimental practice that enables models of practice traditions to evolve, taking in insights from innovative practice, and experiences from diverse cases. 15 page(s...|$|E
40|$|A {{program is}} first-order {{reducible}} (FO-reducible) {{with respect to}} (wrt) a set of integrity constraints if there exists a first-order theory T such that the set of models for T is exactly the set of intended models for the program wrt all possible EDB's. In this case, we say that P is FO-reducible to T wrt IC. For FO-reducible programs, {{it is possible to}} characterize, using first-order logic implications, properties of programs that are related to all possible EDB's as in the database context. These properties include, among others, containment of programs, independence of updates wrt queries and integrity constraints, and characterization and implication of integrity constraints in programs, all of which have no known proof procedures. Therefore, many important problems formalized in a <b>non-standard</b> <b>logic</b> can be dealt with using the rich reservoir of first-order theorem-proving tools, provided that the program is FO-reducible. The following classes of programs are shown to be FO-reduc [...] ...|$|E
40|$|In this paper, {{we present}} the {{interval}} neutrosophic logics which generalizes the fuzzy logic, paraconsistent logic, intuitionistic fuzzy logic {{and many other}} non-classical and <b>non-standard</b> <b>logics.</b> We will give the formal definition of interval neutrosophic propositional calculus and interval neutrosophic predicate calculus. Then we give one application of interval neutrosophic logics to do approximate reasoning. Comment: 18 pages, 4 figure...|$|R
40|$|Logical frameworks, formal {{systems for}} {{programming}} consequence based proof systems, are well known. The notations {{that have been}} proposed are suited best to natural deduction style presentations based on truth consequence. We develop a conservative extension of a typical logical framework providing a modal connective which we can use to formalise validity. We argue that this extension is sensible, and provide example encodings of <b>non-standard</b> <b>logics</b> in its terms...|$|R
40|$|In 1961 Maehara {{introduced}} a proof-theoretical method {{to prove the}} interpolation theorem for standard logics. By developing Maehara 2 ̆ 7 s method, we can prove interpolation theorem for some <b>non-standard</b> <b>logics,</b> including the commutative predicate logics Fle dan Fle,w. In the present paper we show that by modifying Maehara 2 ̆ 7 s method we can also prove the interpolation theorem for non-commutative predicate logics FL and FLw...|$|R
40|$|The {{optical bench}} {{realization}} of {{a feed forward}} optical neural network, developed by the authors, is presented. The network uses a thermal nonlinear material that modulates the phase front of a forward propagating HeNe beam by dynamically altering the index of refraction profile of the material. The index of refraction cross-section of the nonlinear material was modified by applying a separate argon laser, which was modulated by a liquid crystal display used as a spatial light modulator. On-line training of the network was accomplished by using a reinforcement learning paradigm to achieve several standard and <b>non-standard</b> <b>logic</b> gates. 1. Introduction Artificial neural networks are becoming more common because they can do tasks that are difficult, or even impossible, for a common numerical processor, such as pattern recognition, nonlinear control, speech recognition, and so on. Their strength, compared with normal numerical processors, resides {{in the use of}} {{a very large number of}} simp [...] ...|$|E
40|$|To {{help resolve}} {{stubborn}} aporias in quantum mechanics, Kastner, Kauffman and Epperson have recently proposed a new interpretation of Heisenbergs ontological duality of res extensa and res potentia. In related work, Khrennikov and Aerts {{and their respective}} associates have explored the intermediate domain of mesoscopic phenomena in which quantum-like behavior can be observed and formalized. We generalize this work to define a third domain, one of macroscopic, interactive processes to which quantum concepts, but not quantum formalism apply. We show that they follow the <b>non-standard</b> <b>logic</b> proposed {{in the last century}} by the FrancoRomanian thinker Stephane Lupasco. This logical system, grounded in the ontological dualities in nature, describes the evolution of complex processes and emergence at macroscopic levels of reality as chains of interacting actualities and potentialities. Without going outside the laws of physics, some new approaches to the problems of life, cognition and information can be made. The most appropriate formalism for describing this logic is under current scrutiny. Comment: Res Potentiae should appear in italics as a non-English language term (Latin...|$|E
40|$|Logical relativism is {{the claim}} that {{different}} cultures may think according to different logical laws. For example, it is often argued that whereas ‘we’ (Westerners) operate {{according to the law}} of the excluded middle, ‘they’ (e. g., the Chinese or the Azande) may not. In this article, we question whether logical relativism is an empirical thesis, i. e., a thesis that is substantiated through anthropological examples. We distinguish two forms of logical relativism, both of which try to account for alleged contradictions in the beliefs of other cultures. The ‘alternative logic’ approach suggests that contradictions only appear if we judge beliefs according to classical logic, but do not exist if we judge them according to an alternative (<b>non-standard)</b> <b>logic.</b> The ‘symmetric treatment’ suggests that whether there is a contradiction or not is itself a culture-specific matter, such that what may be a contradiction ‘for them’ may not be a contradiction ‘for us’ and vice versa. We question whether either of these arguments really involves relativism and show that problems arise in the treatment of the examples, firstly, in terms of questionable preconceptions made about the status of logic as a standard of comparison and, secondly, in the ways in which relevant beliefs are formulated...|$|E
40|$|In {{the first}} part of this article, there are {{described}} two ways out of the design process theory: metamodel as a basic principle of a design process description and <b>non-standard</b> <b>logics</b> as a feasible formal background of the design theory. In the second part, one possibility of the design process description is discussed by means of one type of nonstandard logic – modal logic formalism. A simple example shows some of the properties of our approach...|$|R
40|$|The aim of {{this book}} is to present the {{fundamental}} theoretical results concerning inference rules in deductive formal systems. Primary attention is focused on: admissible or permissible inference rules the derivability of the admissible inference rules the structural completeness of logics the bases for admissible and valid inference rules. There is particular emphasis on propositional <b>non-standard</b> <b>logics</b> (primary, superintuitionistic and modal logics) but general logical consequence relations and classical first-order theories are also considered. The book is basically self-contained an...|$|R
40|$|This {{paper is}} {{concerned}} with certain ontological issues in the foundations of geographic representation. It sets out what these basic issues are, describes the tools needed to deal with them, and draws some implications for a general theory of spatial representation. Our approach has ramifications in the domains of mereology, topology, and the theory of location, {{and the question of}} the interaction of these three domains within a unified spatial representation theory is addressed. In the final part we also consider the idea of non-standard geographies, which may be associated with geography under a classical conception in the same sense in which <b>non-standard</b> <b>logics</b> are associated with classical logic...|$|R
40|$|The present paper {{presents}} an anaphoric account of presupposition. It {{is argued that}} presuppositional expressions should {{not be seen as}} referring expressions, nor is presupposition to be explicated in terms of some <b>non-standard</b> <b>logic.</b> The notion of presupposition should not be relegated to a pragmatic theory either. Instead presuppositional expressions are claimed to be anaphoric expressions which have internal structure and semantic content. In fact they only differ from pronouns and other semantically less loaded anaphors in that they have more descriptive content. It is this fact which enables them to create an antecedent in case discourse does not provide one. If their capacity to accommodate is taken into account they can be treated by basically the same mechanism which handles the resolution of pronouns. The theory is elaborated in the framework of discourse representation theory. It is shown that pragmatic factors interfere in the resolution of presuppositional anaphors. The resulting account can neither be classified as wholly semantic nor wholly pragmatic. Section 1 presents a survey of standing problems in the theory of presupposition projection and discusses the major competing approaches. An argumentation for a purely anaphoric account of presupposition is given in section 2. Section 3 presents a coding of presuppositional expressions in an extension of discourse representation theory. The final section is devoted to a discussion of the constraints which govern the resolution of presuppositional anaphors...|$|E
40|$|In this paper, we analyse {{the design}} of a support plate for the {{accumulator}} of an electric motor vehicle. The support {{is an integral part of}} motor vehicle chassis. Therefore, geometrical configuration and boundary conditions require careful optimisation research of both function and structural behaviour, since lightness and dimension problems in the presence of dynamic stresses due to external factors have to be considered. Since these factors are complex and not homogeneous, the problem requires multi-criteria analysis. The presence, of factors that are not precisely computable calls for fuzzy-logic application to optimisation problems, because fuzzy-logic is <b>non-standard</b> <b>logic,</b> particularly suitable for making choices in structural design. In plate optimisation, in fact, not numerically quantifiable characteristics such as a part's workability, numerically determinable structural values such as stresses and strains, and analytically calculable properties such as weight come into play. These four parameters become the domain of fuzzy membership functions, by which we will extract membership grade values (codomain). Design variables (domain) are plate thickness, ashlar's number on the plate and stiffening ashlar's depth. In our research, we characterise fuzzy correlation between parameters and required characteristics in order to determine, according to non-standard logics, the best topological configuration which corresponds to the optimisation of individualised characteristics in conformity with design constraints. Results show value improvement in stress and strain in comparison with the not yet optimsed plate and small reduction in workability, whereas the mass is almost the sam...|$|E
40|$|Formal {{models of}} argumentation have been {{investigated}} in several areas, from multi-agent systems and artificial intelligence (AI) to decision making, philosophy and law. In artificial intelligence, logic-based models have been the standard for the representation of argumentative reasoning. More recently, the standard logic-based models have been shown equivalent to standard connectionist models. This {{has created a new}} line of research where (i) neural networks {{can be used as a}} parallel computational model for argumentation and (ii) neural networks can be used to combine argumentation, quantitative reasoning and statistical learning. At the same time, <b>non-standard</b> <b>logic</b> models of argumentation started to emerge. In this paper, we propose a connectionist cognitive model of argumentation that accounts for both standard and non-standard forms of argumentation. The model is shown to be an adequate framework for dealing with standard and non-standard argumentation, including joint-attacks, argument support, ordered attacks, disjunctive attacks, meta-level attacks, self-defeating attacks, argument accrual and uncertainty. We show that the neural cognitive approach offers an adequate way of modelling all of these different aspects of argumentation. We have applied the framework to the modelling of a public prosecution charging decision as part of a real legal decision making case study containing many of the above aspects of argumentation. The results show that the model can be a useful tool in the analysis of legal decision making, including the analysis of what-if questions and the analysis of alternative conclusions. The approach opens up two new perspectives in the short-term: the use of neural networks for computing prevailing arguments efficiently through the propagation in parallel of neuronal activations, and the use of the same networks to evolve the structure of the argumentation network through learning (e. g. to learn the strength of arguments from data) ...|$|E
2500|$|Philosophical logic {{deals with}} formal {{descriptions}} of ordinary, non-specialist ("natural") language, that is strictly {{only about the}} arguments within philosophy's other branches. Most philosophers assume {{that the bulk of}} everyday reasoning can be captured in logic if a method or methods to translate ordinary language into that logic can be found. Philosophical logic is essentially a continuation of the traditional discipline called [...] "logic" [...] before the invention of mathematical logic. Philosophical logic has a much greater concern with the connection between natural language and logic. As a result, philosophical logicians have contributed a great deal to the development of <b>non-standard</b> <b>logics</b> (e.g. free logics, tense logics) as well as various extensions of classical logic (e.g. modal <b>logics)</b> and <b>non-standard</b> semantics for such logics (e.g. Kripke's supervaluationism in the semantics of logic).|$|R
40|$|We {{present a}} {{mechanism}} for recovering consistent data from inconsistent set of assertions. For a common family of knowledge-bases we also provide an efficient algorithm for doing so automaticly. This method is nonmonotonic and paraconsistent. It is particularly useful for making diagnoses on faulty devices. Subject areas: Knowledge-based systems, <b>Non-standard</b> <b>logics</b> for AI, Reasoning under uncertainty, Model-based diagnosis. 1. Introduction It is well-known that the classical calculus allows only a trivial reasoning {{in the presence of}} inconsistency. This property is particularly problematic when the system under consideration is aimed to deal with conflicts. This is the case, for instance, with diagnostic systems that are supposed to analize the discrepancy between the actual behavior of some device and the way it is meant to behave. A common approach of handling inconsistent information is to consider some consistent subsets that still contain meaningful data. The usual method of do [...] ...|$|R
40|$|Abstract. Second-order {{uncertainty}}, {{also known}} as model uncertainty and Knightian uncertainty, arises when decision-makers can (partly) model the parameters of their decision problems. It is widely believed that subjective probability, and more generally Bayesian theory, are illsuited to represent a number of interesting second-order uncertainty features, especially “ignorance ” and “ambiguity”. This failure is sometimes taken as an argument for {{the rejection of the}} whole Bayesian approach, triggering a Bayes vs anti-Bayes debate which is in many ways analogous to what the classical vs non-classical debate used to be in logic. This paper attempts to unfold this analogy and suggests that the development of <b>non-standard</b> <b>logics</b> offers very useful lessons on the contextualisation of justified norms of rationality. By putting those lessons to work I will flesh out an epistemological framework suitable for extending the expressive power of standard Bayesian norms of rationality to secondorder uncertainty in a way which is both formally and foundationally conservative...|$|R
40|$|In a {{previous}} paper an intensional theory of relations was formulated [Plo 90]. It {{was intended as}} a formalisation of some of the ideas of Situation Theory concerning relations, assignments, states-of-a airs and facts; it was hoped it could serve as a springboard for formalising other notions especially those concerning situations and propositions. The method chosen was to present a formal theory in a variation of classical first-order logic allowing terms with bound variables (and also quantification over function variables, but no axioms of choice). One infelicity of this work was that not every formula corresponded to a state-of-a airs according to a certain notion of internal definability; indeed one could show such correspondences inconsistent with the theory. Jon Barwise suggested changing the logic to allow partial predicates and partial functions. The idea of using a 3 -valued approach is an old one: see [Fef 84] for general information about results closely related to those given below. Another infelicity, pointed out by Peter Aczel, was that the logic formalised part of the metalanguage of the structures concerned, and these structures already had their own notion of proposition or, better, state-of-a airs. This meant that there was a repetition of logical apparatus; for example the logical conjunction was replicated by a conjunction for soas. In this paper we present a <b>non-standard</b> <b>logic</b> for our structures. It is a type-free intensional logic, and is also in the tradition of Curry’s illative logic [HS 86]; see also [AczN, FM 87, Smi 84, MA 88]. The logic has two judg- ments: that an object is a fact and that an object is a state-of-a airs (cf. truth and proposition). Objects are given using a variant of the traditional situation theory notation which is more standard, logically speaking, with explicit negation and quantification (see also [Bar 87]). No metalinguistic apparatus is employed...|$|E
40|$|AbstractRecently, one of {{the authors}} {{introduced}} a simple and yet powerful non-monotonic knowledge representation framework, called the Autoepistemic Logic of Beliefs, AEB. Theories in AEB are called autoepistemic belief theories. Every belief theory T has been shown to have the least static expansionT which is computed by iterating a natural monotonic belief closure operatorΨT starting from T. This way, the least static expansion T of any belieftheory provides its natural non-monotonic semantics which is called the static semantics. It is easy to see that if a belief theory T is finite then the construction of its least static expansion T stops after countably many iterations. However, a somewhat surprising result obtained in this paper shows that the least static expansion of any finite belief theory T is in fact obtained by means of a single iteration of the belief closure operator ΨT (although this requires T to be of a special form, we also show that T can be always put in this form). This result eliminates the need for multiple iterations in the computation of static semantics and allows us to replace the fixed-point definition of static semantics by the equivalent explicit and straightforward definition given by T=ΨT(T). The second, closely related result establishes an intriguing relationship between static semantics T and Clark's completions comp(T) of finite belief theories. Here we use a slightly generalized version of comp(T) (see Definition 3. 2). It shows that the static semantics T of T is obtained by augmenting T with the set Bcomp(T) ={BF:F∈comp(T) } thus ensuring that all formulae that belong to Clark's completion comp(T) of T are believed to be true. Both results open the way for a more efficient implementation of static semantics: the first, because only one iteration is needed, and the second because reasoning in a <b>non-standard</b> <b>logic</b> (belief theories under static semantics) can be reduced to classical theorem proving involving Clark's completion...|$|E
40|$|In {{the most}} general if {{unconventional}} terms, science {{is the study}} of how man is part of the universe. Philosophy {{is the study of}} man’s ideas of the universe and how man differs {{from the rest of the}} universe. It has of course been recognized that philosophy and science are not totally disjointed. Science is in any case not a monolithic entity but refers to knowledge as the results of reasoning and both invasive and non-invasive experiment. We argue that the philosophy of science, in studying the foundations, methods and implications of science and the link between philosophy and science, must now take into account the impact of the rapidly developing science and philosophy of information. We suggest that the philosophy of information is in fact a metaphilosophy, since informational processes operate in all the sciences and their philosophies. The simplest definition of (a) metaphilosophy is that of a set of statements about (a) philosophy, and any definition of a metaphilosophy thus requires one of philosophy and of the task of philosophy as well. According to Sellars, “the aim of philosophy is to understand how things in the broadest possible sense of the term hang together in the broadest possible sense of the term”. In this paper, we focus on the recursive thought underlying those statements as real processes, occurring both in and between the fundamental and the meta-level. We propose a <b>non-standard</b> <b>logic,</b> Logic in Reality, as the logic of those processes. The metaphilosophy of information is thus a framework for talking about the scientific aspects of philosophy and the philosophical aspects of science. Both Logic in Reality and the metaphilosophy of information provide a basis for understanding the physical and epistemological dynamics of existence, that is, from where the properties of things come that enable both them and the concepts of them to contrast, conflict and ultimately “hang together”. We conclude that the current convergence of science and philosophy under the influence of information science constitutes a revolution in philosophy, that is, in how science and philosophy are done. Many of the issues discussed in the metaphilosophy of information may thus be viewed as part of an emerging informational metaphilosophy of science...|$|E
40|$|Among the {{key issues}} in {{computational}} semantics, the notion that meanings {{should be seen as}} context changing operations and the need to deal with utterances with multiple readings have received a great deal of attention. Approaches to both topics tend to involve inventing new logics. I believe that the move to <b>non-standard</b> <b>logics</b> should be taken only as a very last resort [...] - changing the nature of the representation is a much bigger step than adding new content within an existing representational framework. In the current paper I will show how to use a simple doxastic logic to deal with both these phenomena. Keywords: dynamic semantics, underspecification, knowledge & belief 1 Why Stick to Doxastic Logic? Most people these days accept the idea that meanings of natural language utterances have a dynamic, context-changing character. You produce an utterance in a context. Your hearer processes this utterance with respect to the context, doing things like anchoring referring exp [...] ...|$|R
40|$|Second-order {{uncertainty}}, {{also known}} as model uncertainty and Knightian uncertainty, arises when decision-makers can (partly) model the parameters of their decision problems. It is widely believed that subjective probability, and more generally Bayesian theory, are ill-suited to represent a number of interesting second-order uncertainty features, especially “ignorance” and “ambiguity”. This failure is sometimes taken as an argument for {{the rejection of the}} whole Bayesian approach, triggering a Bayes vs anti-Bayes debate which is in many ways analogous to what the classical vs non-classical debate used to be in logic. This paper attempts to unfold this analogy and suggests that the development of <b>non-standard</b> <b>logics</b> offers very useful lessons on the contextualisation of justified norms of rationality. By putting those lessons to work I will flesh out an epistemological framework suitable for extending the expressive power of standard Bayesian norms of rationality to second- order uncertainty in a way which is both formally and foundationally conservative. Content...|$|R
40|$|AbstractWe {{introduce}} {{and motivate}} a <b>non-standard</b> multi-modal <b>logic</b> to represent and reason about ignorance in Multi-Agent Systems. We {{argue that in}} Multi-agent systems being able to reason about what agents ignore {{is just as important}} as being able to reason about what agents know. We show a sound and complete axiomatisation for the logic. We investigate its applicability by restating the feasibility condition for the FIPA communication primitive of inform...|$|R
40|$|Abstract A modal {{logical schema}} is {{introduced}} for {{the exploration of}} a multi-player generalization of the Prisoner’s Dilemma in which (a) each participant has at least k available moves, and (b) participants can be members {{of more than one}} coalition of successful participants. The methodology employed illustrates how the principle governing the aggregative behaviour of formulae within the scope of the ✷ operator in the models for a class of <b>non-standard</b> modal <b>logics</b> can be manipulated to represent listcolouring properties for hypergraphs...|$|R
2500|$|Formalists are {{relatively}} tolerant and inviting to {{new approaches to}} <b>logic,</b> <b>non-standard</b> number systems, new set theories etc. The more games we study, the better. However, in {{all three of these}} examples, motivation is drawn from existing mathematical or philosophical concerns. The [...] "games" [...] are usually not arbitrary.|$|R
40|$|In {{this paper}} I am {{concerned}} with {{an analysis of}} negative existential sentences that contain proper names only by using negative or neutral free logic. I will compare different versions of neutral free logic with the standard system of negative free logic (Burge, Sainsbury) and aim to defend my version of neutral free logic that I have labeled <b>non-standard</b> neutral free <b>logic...</b>|$|R
40|$|Our {{field is}} called {{automated}} theorem proving because traditionally {{it has been}} concerned with the art of finding proofs automatically. In the beginning, researchers were motivated by the wish to build computer systems that can automatically solve hard, mathematical problems. When searching for a hard proof, it is acceptable for a system to eat up all resources and not to recognise false theorems. However in the last years, one has {{become aware of the}} fact that for applications, one also needs to be able to efficiently identify non-theorems. For example, automated theorem proving systems are now being used as assistants which must automatically solve easy subtasks in large, interactive projects. For such problems, the expectations to the automated theorem prover are different: The input problems are not terribly hard, usually contain additional irrelevant information, and often they are not provable. In case the subgoal is incorrect, it is not acceptable to simply remain silent and consume all resources in an interactive system. Apart from the applications, the field of disproving has triggered many interesting theoretical research questions, which are interesting on their own. For example, one of the contributed papers addresses the problem of how to repair (modify) a non-theorem in such a way that it becomes a theorem. We also have two contributions about finding counter-models in <b>non-standard</b> <b>logics.</b> One of the contributions addresses this problem for resource logics, the other for Gödel-Dummett logic. The workshop consists of seven contributed talks and one invited talk by Alan Bundy with title Finding and Using Counter Examples. In addition, we share an invite...|$|R
40|$|Techniques for the {{verification}} of elementary properties of pointer programs are highly desirable. Programming with pointers is error–prone with potential pitfalls such as dereferencing null pointers {{and the creation}} of memory leaks. So far, the field of pointer analysis has focused primarily on sequential programs. But pointer programming becomes even more vulnerable in a concurrent setting where threads can be dynamically created, and where data structures such as linked lists are manipulated and inspected by several threads. We present an approach to model checking of concurrent programs that operate on singly–linked lists. To the best of our knowledge, it is the first work which combines unbounded creation of pointer–manipulating threads with an expressive logic. Moreover existing approaches that support reasoning about dynamic data structures make use of <b>non–standard</b> <b>logics,</b> advanced model–checking procedures or extended versions of Hoare logics with accompanying deduction techniques. In contrast, the approach advocated in this paper stays within the realm of traditional (linear–time) model checking. This facilitates the usage of standard model checkers for validating temporal properties addressing absence of memory leaks, dereferencing of null pointers, dynamic creation of cells, and simple and position– dependent aliasing. Our approach is illustrated by considering a simple concurrent programming language that besides the usual control structures offers primitives for thread creation, pointer manipulation, cell creation and destruction, and (guarded) atomic regions that allow concurrency control constructs such as test–and–set primitives and monitors. It is worth mentioning that the restriction to singly–linked pointer structures, which we share with many publications in the field var x, y; proc main(01 new(x); 02 spawn(server) ...|$|R
40|$|Abstract. This paper {{reports on}} {{extensions}} of the Description <b>Logics</b> <b>non-standard</b> inference system Sonic. The recent contributions to the system are two-fold. Firstly, Sonic is extended by two new of non-standard inferences, namely, implementations of the good common subsumer w. r. t. a background terminology and a heuristics for computing a minimal rewriting. Secondly, Sonic is available as a plugin for the wellknown ontology editor Protégé [11]. ...|$|R
