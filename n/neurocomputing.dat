204|1|Public
2500|$|Sfakiotakisa M and Tsakiris DP (2007) [...] <b>Neurocomputing,</b> 70(10-12): 1907-1913.|$|E
2500|$|Sudha Murty Distinguished (Visiting) Chair of <b>Neurocomputing</b> and Data Science, Indian Institute of Science, 2016 ...|$|E
2500|$|In 2013, Honavar {{joined the}} faculty of Penn State College of Information Sciences and Technology at Pennsylvania State University where he holds the Edward Frymoyer endowed {{professorship}} and serves on {{the faculty of}} graduate programs in Computer Science, Information Sciences and Technology, [...] Bioinformatics and Genomics, [...] Neuroscience, and of Operations Research. Honavar also serves as the Director of the Artificial Intelligence Research Laboratory, Associate Director of the Institute for Cyberscience and the Director of the Center for Big Data Analytics and Discovery Informatics at Pennsylvania State University. Honavar [...] serves on the Executive Board of the Northeast Big Data Innovation Hub. Honavar served on the [...] Computing Research Association's Computing Community Consortium Council during 2014-2017, where he chaired the task force on Convergence of Data and Computing, and {{was a member of the}} task force on Artificial Intelligence. In 2015, Honavar was elected to the Electorate Nominating Committee of the [...] Information, Computing, and Communication Section of the American Association for the Advancement of Science. In 2016, Honavar was selected as the first Sudha Murty Distinguished Visiting Chair of <b>Neurocomputing</b> and Data Science by the Indian Institute of Science, Bangalore, India.|$|E
5000|$|... <b>Neurocomputing,</b> (Elsevier, indexed by ISI, SCIE, from 2007) ...|$|E
5000|$|Sudha Murty Distinguished (Visiting) Chair of <b>Neurocomputing</b> and Data Science, Indian Institute of Science, 2016 ...|$|E
5000|$|H.kazemian and K.Ouazzane (2012) ‘Neuro-fuzzy {{approach}} to video transmission over Zigbee; <b>Neurocomputing</b> journal, DOI: 10.1016/j.neucom.2012.10.006 ...|$|E
5000|$|Anderson, J.A., Pellionisz, A. and Rosenfeld, E. (Eds.)(1990), <b>Neurocomputing</b> 2: Directions for Research. Cambridge, MA: MIT Press, 1990.|$|E
50|$|Stepless PWM {{speed control}} of AC motors: A neural network approach, <b>Neurocomputing,</b> Volume 6, Issues 5-6, October 1994, Pages 523-539.|$|E
5000|$|... "Collective {{plasticity}} {{and individual}} stability in cultured neural networks". Raichman, N., Volman, V., Ben-Jacob, E., <b>Neurocomputing</b> Vol. 69, pp 1150-1154 (2006).|$|E
50|$|He {{has engaged}} in {{mathematical}} fundamental research and its application concerning pattern recognition, image processing, multivariate analysis, artificial intelligence, and <b>neurocomputing.</b> Otsu's method, an image binarization technique, is still a standard technique widely used both in Japan and abroad.|$|E
50|$|<b>Neurocomputing</b> publishes {{articles}} {{in the field of}} neural networks and machine learning. The journal is published by Elsevier. It is abstracted and indexed in Scopus and Science Citation Index. According to the Journal Citation Reports, its 2015 impact factor is 2.083 and its 5-year impact factor is 2.292.|$|E
50|$|Cios {{has been}} the {{recipient}} of the Norbert Wiener Outstanding Paper Award (Kybernetes), the University of Toledo Outstanding Faculty Research Award, the <b>Neurocomputing</b> Best Paper Award and the Fulbright Senior Scholar Award. Cios is listed by the Kosciuszko Foundation as one of the eminent scientists of Polish origin and ancestry, is a Foreign Member of the Polish Academy of Arts and Sciences, and a Fellow of the American Institute for Medical and Biological Engineering.|$|E
50|$|He has {{authored}} {{more than}} 300 research papers including around 100 international journal papers, 16 co-authored/edited Books and over 60 Book chapters. He has published in leading high impact Journals including, amongst others: IEEE Transactions on Cybernetics, IEEE Intelligent Systems, IEEE Computational Intelligence, IEEE Transactions on Communications, IEEE Communications Magazine, IEEE Sensors Journal, Neural Networks, Knowledge Based Systems (KBS), IET Proceedings on Vision, Image & Signal Processing, <b>Neurocomputing,</b> Speech Communication, (IET) Electronics Letters, Journal of Theoretical Biology, and others.|$|E
50|$|He was the Editor-in-Chief of IEEE Transactions on Neural Networks (1998-2003). Dr. Zurada was an Associate Editor of IEEE Transactions on Circuits and Systems, Pt. I and Pt. II, {{and served}} on the Editorial Board of the Proceedings of IEEE. He is an Associate Editor of <b>Neurocomputing,</b> Schedae Informaticae, the International Journal of Applied Mathematics and Computer Science, Advisory Editor of the International Journal of Information Technology and Intelligent Computing, and Editor of the Springer Natural Computing Book series.|$|E
50|$|He {{is in the}} Editorial Board of IEE Proceedings Systems Biology, SIAM Review Synthetic and Systems Biology International Journal of Biological Sciences, Journal of Computer and Systems Sciences, and Neural Computing Surveys (Board of Advisors), and {{a former}} Board member of IEEE Transactions in Automatic Control, Systems and Control Letters, Dynamics and Control, <b>Neurocomputing,</b> Neural Networks, Control-Theory and Advanced Technology, and Control, Optimization and the Calculus of Variations. In addition, he is a co-founder and co-managing editor of Mathematics of Control, Signals, and Systems.|$|E
50|$|In 2013, Honavar {{joined the}} faculty of Penn State College of Information Sciences and Technology at Pennsylvania State University where he holds the Edward Frymoyer endowed {{professorship}} and serves on {{the faculty of}} graduate programs in Computer Science, Information Sciences and Technology, Bioinformatics and Genomics, Neuroscience, and of Operations Research. Honavar also serves as the Director of the Artificial Intelligence Research Laboratory, Associate Director of the Institute for Cyberscience and the Director of the Center for Big Data Analytics and Discovery Informatics at Pennsylvania State University. Honavar serves on the Executive Board of the Northeast Big Data Innovation Hub. Honavar served on the Computing Research Association's Computing Community Consortium Council during 2014-2017, where he chaired the task force on Convergence of Data and Computing, and {{was a member of the}} task force on Artificial Intelligence. In 2015, Honavar was elected to the Electorate Nominating Committee of the Information, Computing, and Communication Section of the American Association for the Advancement of Science. In 2016, Honavar was selected as the first Sudha Murty Distinguished Visiting Chair of <b>Neurocomputing</b> and Data Science by the Indian Institute of Science, Bangalore, India.|$|E
40|$|Abstract. This article {{presents}} {{a survey of}} models of rough neu-rocomputing that have their roots in rough set theory. Historically, rough <b>neurocomputing</b> has three main threads: training set production, calculus of granules, and interval analysis. This form of <b>neurocomputing</b> gains its inspiration {{from the work of}} Pawlak on rough set philosophy as a basis for machine learning and from work on data mining and pattern recognition by Swiniarski and others in the early 1990 s. This work has led to a variety of new rough <b>neurocomputing</b> computational models that are briefly presented in this article. The contribution of this article is a survey of representative approaches to rough <b>neurocomputing...</b>|$|E
40|$|A continuous-time shift-invariant {{cellular}} {{neural network}} (CNN) with hardware annealing capability, digitally programmable synaptic weights, and optical inputs/outputs has been developed. This electro-optical <b>neurocomputing</b> processor has great potential in solving many important scientific problems in signal processing and optimization. Advanced packaging for the proposed optoelectronic <b>neurocomputing</b> system is a thin-film silicon-substrate multichip module with flip-chip connection technologies. This paper presents two functional chips designed for the proposed electro-optical <b>neurocomputing</b> processor: a monolithic GaAs 2 -D array of optical receivers, and a VLSI CMOS 2 -D array of smart pixels based on the annealed CNN. Due to the multichip module integration of these chips in the same silicon substrate, a complete optoelectronics <b>neurocomputing</b> system can be realized in a very compact hardware...|$|E
40|$|The present work is {{intended}} to give technologists, research scientists, and mathematicians a graduate-level overview {{of the field of}} <b>neurocomputing.</b> After exploring the relationship of this field to general neuroscience, attention is given to neural network building blocks, the self-adaptation equations of learning laws, the data-transformation structures of associative networks, and the multilayer data-transformation structures of mapping networks. Also treated are the <b>neurocomputing</b> frontiers of spatiotemporal, stochastic, and hierarchical networks, 'neurosoftware', the creation of neural network-based computers, and <b>neurocomputing</b> applications in sensor processing, control, and data analysis...|$|E
40|$|We model here a {{distributed}} {{implementation of}} cross-stopping, {{a combination of}} cross-validation and early-stopping techniques, for {{the selection of the}} optimal architecture of feed-forward networks. Due to the very large computational demand of the method, we use the RAIN system (Redundant Array of Inexpensive workstations for <b>Neurocomputing)</b> as a target platform for the experiments and show that this kind of system can be effectively used for computational intensive <b>neurocomputing</b> tasks...|$|E
40|$|In {{the past}} decade, {{research}} in <b>neurocomputing</b> has been {{divided into two}} relatively wellde ned tracks: one track dealing with cognition {{and the other with}} behavior. Cognition deals with organizing, classifying and recognizing sensory stimuli. Behavior is more dynamic, involving sequences of actions and changing interactions with an external environment. The mathematical techniques that apply to these areas, at least from the point of <b>neurocomputing,</b> appear to have been quite separate as well. The {{purpose of this paper is}} to give anoverview of some recent powerful mathematical results in behavioral <b>neurocomputing,</b> speci cally the concept of Q-learning due to C. Watkins, and some new extensions. Finally,we propose ways in which the mathematics of cognition and the mathematics of behavior can move closer to build more unified systems of information processing and action...|$|E
40|$|In {{this paper}} we present KERNEL, a neuro-fuzzy {{system for the}} {{extraction}} of knowledge directly from data, and a toolbox developed in the Matlab environment for its implementation. The KERNEL system belongs to the novel approach which concerns the use and representation of explicit knowledge within the <b>neurocomputing</b> paradigm: the Knowledge Based <b>Neurocomputing.</b> A specific neural network is designed, that reflects in its topology {{the structure of the}} fuzzy inference model on which is based the KERNEL system. A well-known system identification benchmark is used as illustrative example...|$|E
40|$|For centuries, coffee {{has been}} brewed and {{consumed}} in households, hot shops and restaurants. Today flavoured milks {{have become very}} popular and they contain nutrients as compared with soft drinks. Sterilized milk is the product made by heating milk to high temperature (121 o C) with 15 m holding time so that it remains fit for human consumption for longer time at room temperature. Efficiency of single and double hidden layers of Cascade <b>neurocomputing</b> models for prediction of sensory quality of roasted coffee flavoured sterilized drink were studied. Colour and appearance, viscosity, flavour and sediment were taken as input parameters, while overall acceptability was used as output parameter. The results of cascade <b>neurocomputing</b> models were calculated with two types of prediction performance measures, viz., {{root mean square error}} and coefficient of determination R 2. The study revealed that more the number of neurons in single hidden layer, less the error for cascade <b>neurocomputing</b> models (RMSE: 0. 00011; R 2 : 0. 999999; neurons: 50) ...|$|E
40|$|Neurocomputers offer a massively {{parallel}} computing paradigm by mimicking the human brain. Their efficient use in statistical information processing {{has been proposed}} to overcome critical bottlenecks with traditional computing schemes for applications such as image and speech processing, and associative memory. In neural networks information is generally represented by phase (e. g., oscillatory neural networks) or amplitude (e. g., cellular neural networks). Phase-based <b>neurocomputing</b> is constructed as a network of coupled oscillatory neurons that are connected via programmable phase elements. Representing each neuron circuit with one oscillatory device and implementing programmable phases among neighboring neurons, however, are not clearly feasible from circuits perspective if not impossible. In contrast to nascent oscillatory <b>neurocomputing</b> circuits, mature amplitude-based neural networks offer more efficient circuit solutions using simpler resistive networks where information is carried via voltage- and current-mode signals. Yet, such circuits have not been efficiently realized by CMOS alone due to the needs for an efficient summing mechanism for weighted neural signals and a digitally-controlled weighting element for representing couplings among artificial neurons. Large power consumption and high circuit complexity of such CMOS-based implementations have precluded adoption of amplitude-based <b>neurocomputing</b> circuits as well, and have led researchers to explore the use of emerging technologies for such circuits. Although they provide intriguing properties, previously proposed <b>neurocomputing</b> components based on emerging technologies have not offered a complete and practical solution to efficiently construct an entire system. In this thesis we explore the generalized problem of co-optimization of technology and architecture for such systems, and develop a recipe for device requirements and target capabilities. We describe four plausible technologies, each of which could potentially enable the implementation of an efficient and fully-functional <b>neurocomputing</b> system. We first investigate fully-digital neural network architectures that have been tried before using CMOS technology in which many large-size logic gates such as D flip-flops and look-up tables are required. Using a newly-proposed all-magnetic non-volatile logic family, mLogic, we demonstrate the efficacy of digitizing the oscillators and phase relationships for an oscillatory neural network by exploiting the inherent storage as well as enabling an all-digital cellular neural network hardware with simplified programmability. We perform system-level comparisons of mLogic and 32 nm CMOS for both networks consisting of 60 neurons. Although digital implementations based on mLogic offer improvements over CMOS in terms of power and area, analog <b>neurocomputing</b> architectures {{seem to be more}} compatible with the greatest portion of emerging technologies and devices. For this purpose in this dissertation we explore several emerging technologies with unique device configurations and features such as mCell devices, ovenized aluminum nitride resonators, and tunable multi-gate graphene devices to efficiently enable two key components required for such analog networks – that is, summing function and weighting with compact D/A (digital-to-analog) conversion capability. We demonstrate novel ways to implement these functions and elaborate on our building blocks for artificial neurons and synapses using each technology. We verify the functionality of each proposed implementation using various image processing applications based on compact circuit simulation models for such post-CMOS devices. Finally, we design a proof-of-concept <b>neurocomputing</b> circuitry containing 20 neurons using 65 nm CMOS technology that is based on the primitives that we define for our analog <b>neurocomputing</b> scheme. This allows us to fully recognize the inefficiencies of an all-CMOS implementation for such specific applications. We share our experimental results that are in agreement with circuit simulations for the same image processing applications based on proposed architectures using emerging technologies. Power and area comparisons demonstrate significant improvements for analog <b>neurocomputing</b> circuits when implemented using beyond- CMOS technologies, thereby promising huge opportunities for future energy-efficient computing...|$|E
40|$|We {{consider}} two {{machine learning}} related problems, optimal control and reinforcement learning. We show that, {{even when their}} state space is very large (possibly infinite), natural algorithmic solutions can be implemented in an asynchronous <b>neurocomputing</b> way, that is by an assembly of interconnected simple neuron-like units which does not require any synchronization. From a neuroscience perspective, this work might help understanding how an asynchronous assembly of simple units can give rise to efficient control. From a computational point of view, such <b>neurocomputing</b> architectures can exploit their massively parallel structure and be significantly faster than standard sequential approaches. Th...|$|E
40|$|<b>Neurocomputing,</b> an {{evolving}} discipline {{of science and}} technology which involves the study and use of nonlinear networks of parallel processing elements for certain classes of information processing (computing) is reviewed in its current stage, including theoretical aspects, practical environments, applications, and the status quo. These networks possess a "learning from examples"-capability via training on self-organization, and their analysis was originally inspired by the study of the amazing information processing capability via training or self-organization, and their analysis was originally inspired by the study of the amazing information processing capabilities of biological organisms. As theoretical aspects of <b>neurocomputing</b> network architectures, learning rules, and its relation to conventional methods are presented. Practical environments are then discussed including software and hardware implementations...|$|E
40|$|Fuzzy neural {{networks}} are a connecting link between fuzzy logic and <b>neurocomputing.</b> The {{goal of this}} connection is to combine the advantages of both disciplines in order to process uncertain or vague information. Fuzzy {{neural networks}} are (layer by layer) fully interconnected feedforward networks whose processing elements - the formal neurons - operate on fuzzy numbers instead of real numbers. Our aim is to achieve supervised learning {{in this kind of}} neural networks. Our work is based on the traditional backpropagation algorithm. Therefore we will give a brief survey of the basics of fuzzy logic and <b>neurocomputing</b> first. (orig.) Available from TIB Hannover: RO 7057 (1995, 12) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|Knowledge-based <b>neurocomputing</b> addresses, {{among other}} things, the {{encoding}} and refinement of symbolic knowledge in a <b>neurocomputing</b> paradigm. Prior symbolic knowledge derived outside of neural networks can be encoded in neural network form, and then further trained. Previous research suggested certain {{values for the}} weights that represent prior knowledge, based on {{an analysis of the}} derivative of the error function. This inductive bias is investigated empirically, and furthermore, we show how to use sensitivity analysis methods to investigate this bias. This work shows that the bias of the encoding method for the prior knowledge corresponds well with a range of good parameter values that retain the encoded knowledge and allows refinement by further training. Department of ComputingRefereed conference pape...|$|E
40|$|Viewgraphs on a <b>neurocomputing</b> {{approach}} to model determination of large space structures are presented. Topics covered include: background on neural networks; math modeling with neural networks; computer architectures for real-time modeling; and {{hardware and software}} implementation considerations, OPTIMA/ 1 data acquisition, modeling, and control workstation...|$|E
40|$|Traditional {{methods of}} control {{allocation}} optimization have shown difficulties in exploiting the full potential of controlling large arrays of control devices on innovative air vehicles. Artificial neutral networks {{are inspired by}} biological nervous systems and <b>neurocomputing</b> has successfully been applied {{to a variety of}} complex optimization problems. This project investigates the potential of applying <b>neurocomputing</b> to the control allocation optimization problem of Hybrid Wing Body (HWB) aircraft concepts to minimize control power, hinge moments, and actuator forces, while keeping system weights within acceptable limits. The main objective of this project is to develop a proof-of-concept process suitable to demonstrate the potential of using <b>neurocomputing</b> for optimizing actuation power for aircraft featuring multiple independently actuated control surfaces. A Nastran aeroservoelastic finite element model is used to generate a learning database of hinge moment and actuation power characteristics for an array of flight conditions and control surface deflections. An artificial neural network incorporating a genetic algorithm then uses this training data to perform control allocation optimization for the investigated aircraft configuration. The phase I project showed that optimization results for the sum of required hinge moments are improved by more than 12 % over the best Nastran solution by using the neural network optimization process...|$|E
40|$|An {{original}} circuit-level {{model of}} two-terminal vanadium dioxide electron devices exhibiting electronic hysteresis is presented. Such devices allow realisation of very compact relaxation nano-oscillators that potentially {{can be used}} in bio-inspired <b>neurocomputing.</b> The proposed model is exploited to determine the parameters, values that ensure stable periodic oscillations...|$|E
40|$|The paper Ran and Hu (2014, <b>Neurocomputing)</b> {{examines}} identifiability and parameter redundancy {{in classes}} of models used in machine learning. This note discusses {{the results on}} global identifiability and also clarifies that the paper's results on parameter redundancy already exist in the paper Cole et al. (2010, Mathematical Biosciences) ...|$|E
40|$|Direct {{communication}} between two disconnected nerve cells via a semiconductor chip with integrated circuitry {{has been achieved}} (see Figure), enabling reliable feeding of single action potentials from nerve cells into a digital electronic processor. The intrinsic problem of stimulation/recording crosstalk is also addressed. This system represents a key step towards <b>neurocomputing...</b>|$|E
40|$|In this paper, {{we present}} the {{architecture}} {{and describe the}} functionality of a Web-based Intelligent Tutoring System (ITS), which uses neurules for knowledge representation. Neurules are a type of hybrid rules integrating symbolic rules with <b>neurocomputing.</b> The use of neurules as the knowledge representation basis of the ITS results {{in a number of}} advantages...|$|E
40|$|In {{this chapter}} the {{knowledge-based}} <b>neurocomputing</b> {{will be applied}} to expert systems. Two main approaches to represent the knowledge base, namely the explicit and implicit representations will first be introduced and compared in rule-based and neural expert systems, respectively. Then, several possible integration strategies that {{make an effort to}} eliminate the drawbacks of both approaches in hybrid systems, will be surveyed. To illustrate the full power of knowledge-based <b>neurocomputing,</b> the main ideas of the prototypical, strictly neural expert system MACIE will be sketched. Here, a neural network is enriched by other functionalities to achieve all required features of expert systems. The neural knowledge processing will further be demonstrated on the system EXPSYS which exploits the powerful back-propagation learning to automatically create the knowledge base. In addition, EXPSYS introduces the interval neuron states to cope with incomplete information and it provides a simple expla [...] ...|$|E
40|$|We {{discuss a}} <b>neurocomputing</b> system for {{operational}} decision support in water distribution networks. An analog neural network {{is used to}} calculate 'loop-equations based' state estimates. This is followed by the confidence limit analysis (CLA) of the calculated estimates and the general fuzzy min-max neural pattern classification/clustering (PC). The latter emulates the process of experience-building by human operators of water systems. We refer to the resulting <b>neurocomputing</b> system as CLA/PC. Water distribution systems are representative of a large and important class of systems that are primarily driven by external, incompletely defined stimuli yet, the operation of which needs to be optimised according to some well defined criteria. The operational control of such systems presents considerable challenge to operators who need to develop the ability to 'distil' the overall system state from a large number of fuzzy system snapshots (measurements and estimates) in order to decide on the appropriate control act...|$|E
