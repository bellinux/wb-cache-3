416|1660|Public
2500|$|To provide {{resilience}} in {{the event}} of computer or <b>network</b> <b>failure,</b> multiple DNS servers are usually provided for coverage of each domain. At the top level of global DNS, thirteen groups of root name servers exist, with additional [...] "copies" [...] of them distributed worldwide via anycast addressing.|$|E
5000|$|... 2010 « <b>Network</b> <b>Failure</b> »Galerie Fotoloft, Centre d’art contemporain Winzavod, Moscow, Russie ...|$|E
50|$|In {{the absence}} of <b>network</b> <b>failure</b> - that is, when the {{distributed}} system is running normally - both availability and consistency can be satisfied.|$|E
30|$|Propose defense {{strategies}} for <b>network</b> <b>failures.</b>|$|R
5000|$|Continued {{operation}} during partial <b>network</b> <b>failures</b> in server <b>network</b> ...|$|R
50|$|Better cluster support, {{reducing}} the recovery times from host and <b>network</b> <b>failures.</b>|$|R
50|$|In 2007, Red Condor {{introduced}} Vx Technology, which integrates a local appliance with a Hosted Service. The technology redirects {{email from}} the appliance to the Hosted Service {{in the event}} of <b>network</b> <b>failure.</b>|$|E
5000|$|Star or mesh networks, {{which can}} {{continue}} to operate when a node or connection has failed (though for a star <b>network,</b> <b>failure</b> of the central hub will still cause the network to fail) ...|$|E
5000|$|Paging procedure: Initiated by {{the network}} and used to request the {{establishment}} of a NAS signalling connection or to prompt the UE to re-attach if necessary {{as a result of a}} <b>network</b> <b>failure.</b>|$|E
5000|$|Well defined {{semantics}} of sharing, even in {{the presence}} of <b>network</b> <b>failures</b> ...|$|R
30|$|Evaluate {{the impacts}} of <b>network</b> <b>failures</b> on power system {{depending}} on evaluation indicators.|$|R
50|$|RemoteSHADOW {{provides}} users to mirror their data {{to a remote}} site, to protect against <b>network</b> <b>failures.</b>|$|R
50|$|The primary form of {{communicating}} node status is via a network device (commonly Ethernet), {{although in the}} case of possible <b>network</b> <b>failure,</b> quorum can be decided through secondary methods such as shared storage or multicast.|$|E
5000|$|Mocking {{dependent}} {{systems is}} an effective mechanism to isolate the system under test to ensure tests run reliably and only fail {{when there is a}} genuine error, this avoids tests failing due to irrelevant external changes such as <b>network</b> <b>failure</b> or a server being rebooted / redeployed.|$|E
5000|$|To provide {{resilience}} in {{the event}} of computer or <b>network</b> <b>failure,</b> multiple DNS servers are usually provided for coverage of each domain. At the top level of global DNS, thirteen groups of root name servers exist, with additional [...] "copies" [...] of them distributed worldwide via anycast addressing.|$|E
25|$|Session connectivity: Sessions can be {{disconnected}} and reconnected. Remote sessions {{have become}} more tolerant of temporary <b>network</b> <b>failures.</b>|$|R
30|$|Instead of {{focusing}} on internal hardware failures, Gill et al. focus on <b>network</b> <b>failures</b> in data centers [40, 41]. These studies conclude that 1) Data center networks are highly reliable, 2) Switches are highly reliable, 3) Load balancers most often experience faults due to software <b>failures,</b> 4) <b>Network</b> <b>failures</b> typically cause small failures that lose {{a large number of}} smaller packets, and 5) Redundancy is useful, but not a perfect solution.|$|R
5000|$|<b>Network</b> <b>failures</b> are {{currently}} handled through manually coded time-out and exception routines. This approach is labor-intensive and relatively unreliable.|$|R
5000|$|DCS devices can be {{used for}} [...] "grooming" [...] {{telecommunications}} traffic, switching traffic from one circuit to another {{in the event of a}} <b>network</b> <b>failure,</b> supporting automated provisioning, and other applications. Having a DCS in a circuit-switched network provides important flexibility that can otherwise only be obtained at higher cost using manual [...] "DSX" [...] cross-connect patch panels.|$|E
50|$|Wireless data {{communications}} {{are used to}} span a distance beyond the capabilities of typical cabling in point-to-point communication or point-to-multipoint communication, to provide a backup communications link in case of normal <b>network</b> <b>failure,</b> to link portable or temporary workstations, to overcome situations where normal cabling is difficult or financially impractical, or to remotely connect mobile users or networks.|$|E
50|$|Transport networks, the {{underlying}} optical fiber-based layer of telecommunications networks, have evolved from Digital cross connect system (DCS)-based mesh architectures in the 1980s, to SONET/SDH (Synchronous Optical Networking/Synchronous Digital Hierarchy) ring architectures in the 1990s. In DCS-based mesh architectures, telecommunications carriers deployed restoration systems for DS3 circuits such as at&t FASTAR (FAST Automatic Restoration) and MCI Real Time Restoration (RTR), restoring circuits in minutes after a <b>network</b> <b>failure.</b> In SONET/SDH rings, carriers implemented ring protection such as SONET Unidirectional Path Switched Ring (UPSR) (also called Sub-Network Connection Protection (SCNP) in SDH networks) or SONET Bidirectional Line Switched Ring (BLSR) (also called Multiplex Section - Shared Protection Ring (MS-SPRing) in SDH networks), protecting against and {{recovering from a}} <b>network</b> <b>failure</b> in 50 ms or less, a significant improvement over the recovery time supported in DCS-based mesh restoration, and a key driver for the deployment of SONET/SDH ring-based protection.|$|E
5000|$|End-to-end principle: Enables {{development}} of robust {{applications in the}} face of <b>network</b> <b>failures.</b> NDN retains and expands this design principle.|$|R
5000|$|Many {{aspects of}} network {{quality of service}} depend on coping with traffic peaks that might cause <b>network</b> <b>failures,</b> such as ...|$|R
30|$|Physical problems, as {{hardware}} <b>failures</b> (disk, memory, <b>network</b> <b>failures,</b> etc.), {{depend on}} human intervention for correction and {{are harder to}} address.|$|R
50|$|In {{the case}} of a link or <b>network</b> <b>failure,</b> the {{simplest}} mechanism for network survivability is automatic protection switching (APS). APS techniques involve reserving a protection channel (dedicated or shared) with the same capacity of the channel or element being protected. When a shared protection technique is used, an APS protocol is needed to coordinate access to the shared protection bandwidth.|$|E
50|$|Wireless {{failover}} is {{a business}} continuity function. That is, it allows businesses to continue operations even {{in the event of}} a <b>network</b> <b>failure.</b> In retail, wireless failover is typically used when a standard connection for a point of sale credit card machine fails. In this instance, the wireless failover allows business transactions to continue to be processed, ensuring business continuity.|$|E
5000|$|The Kiisa Power Plant is an {{emergency}} reserve power plant in Kiisa, Estonia, about 25 km from Tallinn. As {{an emergency}} plant, it operates only {{in the case of}} a <b>network</b> <b>failure</b> or capacity shortfall, and it does not participate in the everyday electricity market. [...] The power plant is owned and operated by the Estonian transmission system operator Elering.|$|E
40|$|Abstract — BGP is {{traditionally}} configured to implement traffic engineering objectives without considering potential network dynamics. This {{might result in}} undesirable traffic distribution when <b>network</b> <b>failures</b> occur. In this paper, we present algorithms for interdomain traffic engineering that achieve the interdomain traffic engineering objectives under <b>network</b> <b>failures.</b> That is, we aim to configure routing policies so that traffic is distributed evenly. More importantly, the configuration is robust {{in the sense that}} it is able to achieve the specified traffic engineering goals despite <b>network</b> <b>failures.</b> We first investigate the coarsegrained robust configurations. The derived configuration can achieve optimal robust traffic engineering objectives for most <b>network</b> <b>failures.</b> Further, we develop a greedy algorithm to derive robust BGP configuration for any traffic distribution and link capacities. We use simulations to evaluate the robustness of the derived BGP configurations by applying the algorithm to both transit and stub ASs under realistic traffic demands. Our results show that the derived BGP configurations can improve the default configuration significantly in terms of achieving the robust traffic engineering objectives. Furthermore, our algorithm achieves robust traffic engineering goals without diminishing other routing objectives...|$|R
40|$|This paper {{seeks to}} {{understand}} how <b>network</b> <b>failures</b> affect the availability of service delivery across wide area networks and to evaluate classes of techniques for improving end-to-end service availability. Using several large-scale connectivity traces, we develop a model of failures that includes key parameters such as failure location and failure duration. We then evaluate several classes of techniques for coping with <b>network</b> <b>failures</b> using trace-based simulation. We find that caching alone is seldom effective at insulating services from failures but {{that the combination of}} mobile extension code and prefetching can improve failure rates by as much as an order of magnitude for classes of service whose semantics support disconnected operation. We find that routing-based techniques may provide significant improvements, but that the improvements of many individual techniques is limited because they do not address all significant classes of <b>network</b> <b>failures...</b>|$|R
50|$|WS-ReliableMessaging {{describes}} a protocol that allows SOAP messages to be reliably delivered between distributed {{applications in the}} presence of software component, system, or <b>network</b> <b>failures.</b>|$|R
50|$|A {{while after}} she leaves, Chakkara too leaves the {{restaurant}} while a fearful Siddharth tries to contact Anjali. Anjali too {{is unable to}} contact Siddharth due to <b>network</b> <b>failure</b> and she fails to find any ATM. Behind her, she notices Chakkara's truck following her and honking relentlessly in order to scare her. She continues driving but is unable to escape {{as it is a}} one-way road.|$|E
50|$|PURLs {{have been}} criticized for their need to resolve a URL, thus tying a PURL to a network location. Network {{locations}} have several vulnerabilities, such as Domain Name System registrations and host dependencies. A failure to resolve a PURL could lead to an ambiguous state: It would not be clear whether the PURL failed to resolve because a <b>network</b> <b>failure</b> prevented it or because it did not exist.|$|E
5000|$|However, real {{computers}} are faulty; they fail and recover from failure at unpredictable, possibly inopportune, times. For example, in the follow-the-leader algorithm, {{what if the}} leader fails at the wrong time? In such an environment achieving atomic broadcasts is difficult. [...] A number of protocols have been proposed for performing atomic broadcast, under various assumptions about the <b>network,</b> <b>failure</b> models, availability of hardware support for multicast, and so forth.|$|E
40|$|Intense {{competition}} between transport network service providers and the widespread deployment of vulnerable high-capacity fiber optic transport facilities {{has created the}} need for networks with short restoration times. This paper presents an optimized distributed real time path restoration mechanism, named OPRA, capable of restoring <b>network</b> <b>failures</b> quickly with performance close to centralized multi-commodity max-flow. OPRA synthesizes restoration pathsets by autonomous, database-free, self-organizing interaction between nodes. Simulation results predict that OPRA will restore <b>network</b> <b>failures</b> in very minimally redundant networks {{in less than two}} seconds...|$|R
40|$|The {{impact of}} <b>network</b> <b>failures</b> can be {{minimized}} if users are promptly notified by appropriately designed applications. Specifically, for Voice-over-IP (VoIP) networks, an RTP/RTCP-based detection method {{can be used}} to rapidly distinguish between network congestion and <b>network</b> <b>failures.</b> Users and <b>network</b> managers can exploit this information in various ways, such as rapid network recovery or seeking application usage alternatives. In this paper, we present the main ideas behind these proposals, along with some analytical/simulation results, plus insights from a Linux-based implementation with its experimental results...|$|R
40|$|Link {{capacity}} dimensioning is {{the periodic}} task where ISPs {{have to make}} provisions for sudden traffic bursts and <b>network</b> <b>failures</b> to assure uninterrupted operations. This provision {{comes in the form}} of link working capacities with noticeable amounts of headroom, i. e., spare capacities that are used in case of congestions or <b>network</b> <b>failures.</b> Distributed routing protocols like OSPF provide convergence after <b>network</b> <b>failures</b> and have proven their reliable operation over decades, but require overprovisioning and headroom of over 50 %. However, SDN has recently been proposed to either replace or work together with OSPF in routing Internet traffic. This paper addresses the question of how to robustly dimension the link capacities in emerging hybrid SDN/OSPF networks. We analyze the networks with various implementations of hybrid SDN/OSPF control planes, and show that our idea of SDN Partitioning requires less amounts of spare capacity compared to legacy or other hybrid SDN/OSPF schemes, outperformed only by a full SDN deployment. Comment: 6 pages, 6 figure...|$|R
