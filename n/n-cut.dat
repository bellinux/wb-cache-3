9|6|Public
40|$|This paper {{presents}} an approach for extending the normalizedcut (<b>n-cut)</b> segmentation algorithm to find symmetric regions present in natural images. We use an existing algorithm to quickly detect possible symmetries present in an image. The detected symmetries are then individually verified using the modified <b>n-cut</b> algorithm to eliminate spurious detections. The weights of the <b>n-cut</b> algorithm are modified {{so as to}} include both symmetric and spatial affinities. A global parameter is defined to model the tradeoff between spatial coherence and symmetry. Experimental results indicate that symmetric quality measure for a region segmented by our algorithm is a good indicator for {{the significance of the}} principal axis of symmetry. 1...|$|E
40|$|We {{describe}} an annealing procedure that computes the normalized <b>N-cut</b> of a weighted graph G. The first phase transition computes {{the solution of}} the approximate normalized 2 -cut problem, while the low temperature solution computes the normalized <b>N-cut.</b> The intermediate solutions provide a sequence of refinements of the 2 -cut {{that can be used to}} split the data to K clusters with 2 ≤ K ≤ N. This approach only requires specification of the upper limit on the number of expected clusters N, since by controlling the annealing parameter we can obtain any number of clusters K with 2 ≤ K ≤ N. We test the algorithm on an image segmentation problem and apply it to a problem of clustering high dimensional data from the sensory system of a cricket. Key words: Clustering, annealing, normalized <b>N-cut.</b> ...|$|E
40|$|This paper {{proposes a}} novel {{approach}} for vehicle orientation detection using “vehicle color ” and edge information based on clustering framework. To extract the “vehicle color”, this paper proposes a novel color transform model which is global and {{does not need to}} be re-estimated for any new vehicles or new images. This model is invariant to various situations like contrast changes, background and lighting. Compared with traditional methods which use motion feature to determine vehicle orientations, this paper uses only one still image to finish this task. After feature extraction, the normalized cut spectral clustering (<b>N-cut)</b> is used for vehicle orientation clustering. The <b>N-cut</b> criterion tries to minimize the ratio of the total dissimilarity between groups to the total similarity within the groups. Then, the vehicle orientation can be detected using the eigenvector derived from the <b>N-cut</b> result. Experimental results reveal the superior performances in vehicle orientation estimation. 1...|$|E
40|$|In {{this paper}} {{we focus on}} the issue of {{normalization}} of the affinity matrix in spectral clustering. We show that the difference between <b>N-cuts</b> and Ratio-cuts is in the error measure being used (relative-entropy versus L 1 norm) in finding the closest doubly-stochastic matrix to the input affinity matrix. We then develop a scheme for finding the optimal, under Frobenius norm, doubly-stochastic approximation using Von-Neumann’s successive projections lemma. The new normalization scheme is simple and efficient and provides superior clustering performance over many of the standardized tests. ...|$|R
40|$|White matter fiber {{clustering}} aims to get insight about anatomical {{structures in}} order to generate atlases, perform clear visualizations, and compute statistics across subjects, all important and current neuroimaging problems. In this work, we present a diffusion maps clustering method applied to diffusion MRI {{in order to}} segment complex white matter fiber bundles. It {{is well known that}} diffusion tensor imaging (DTI) is restricted in complex fiber regions with crossings and this is why recent high-angular resolution diffusion imaging (HARDI) such as Q-Ball imaging (QBI) has been introduced to overcome these limitations. QBI reconstructs the diffusion orientation distribution function (ODF), a spherical function that has its maxima agreeing with the underlying fiber populations. In this paper, we use a spherical harmonic ODF representation as input to the diffusion maps clustering method. We first show the advantage of using diffusion maps clustering over classical methods such as <b>N-Cuts</b> and Laplacian eigenmaps. In particular, our ODF diffusion maps requires a smaller number of hypothesis from the input data, reduces the number of artifacts in the segmentation, and automatically exhibits the number of clusters segmenting the Q-Ball image by using an adaptive scalespace parameter. We also show that our ODF diffusion maps clustering can reproduce published results using the diffusion tensor (DT) clustering with <b>N-Cuts</b> on simple synthetic images without crossings. On more complex data with crossings, we show that our ODF-based method succeeds to separate fiber bundles and crossing regions whereas the DT-based methods generate artifacts and exhibit wrong number of clusters. Finally, we show results on a real-brain dataset where we segment well-known fiber bundles...|$|R
40|$|We {{describe}} a geometric-flow based algorithm for computing a dense over-segmentation of an image, {{often referred to}} as superpixels. It produces segments that on one hand respect local image boundaries, while on the other hand limit under-segmentation through a compactness constraint. It is very fast, with complexity that is approximately linear in image size, and can be applied to megapixel sized images with high superpixel densities in a matter of minutes. We show qualitative demonstrations of high quality results on several complex images. The Berkeley database is used to quantitatively compare its performance to a number of over-segmentation algorithms, showing that it yields less under-segmentation than algorithms that lack a compactness constraint, while offering a significant speed-up over <b>N-cuts,</b> which does enforce compactness...|$|R
40|$|Changes in this version: For users: 	New {{gradient}} magnitude operators added (Scharr, Sobel, Prewitt). 	The default gradient magnitude calculation is {{switched to}} 3 D Scharr. 	Switched to Matplotlib 2. 0 (Matplotlib 1. 5. 3 is not compatible anymore). 	Rotation button added. 	New {{options in the}} command line interface. 	Small improvements in <b>N-cut</b> pipeline. For developers: 	Travis and Appveyor continuous integration added. 	Started writing tests :) ...|$|E
40|$|Abstract. According to {{the problem}} that {{classical}} graph-based image segmentation algorithms are not robust to segmentation of texture image. We propose a novel segmentation algorithm that GBCTRS, which overcame the shortcoming of existed graph-based segmentation algorithms <b>N-cut</b> and EGBIS. It extract feature vector of blocks using color-texture feature, calculate weight between each block using the neighborhood relationship, use minimum spanning tree method to clustering segmentation. The experimental show that the new algorithm is more efficient and robust to segment texture image and strong edges image...|$|E
40|$|We {{construct}} an N= 1 theory with gauge group U(nN) and degree n+ 1 tree level superpotential whose matrix model spectral curve develops an A_{n+ 1 } Argyres-Douglas singularity. We evaluate the coupling constants of the low-energy U(1) ^n theory {{and show that}} the large N expansion is singular at the Argyres-Douglas points. Nevertheless, {{it is possible to}} define appropriate double scaling limits which are conjectured to yield four dimensional non-critical string theories as proposed by Ferrari. In the Argyres-Douglas limit the <b>n-cut</b> spectral curve degenerates into a solution with n/ 2 cuts for even n and (n+ 1) / 2 cuts for odd n. Comment: 31 pages, 1 figure; the expression of the superpotential has been corrected and the calculation of the coupling constants of the low-energy theory has been adde...|$|E
40|$|Using {{the example}} of the elastic π N-amplitude, we discuss the low energy {{expansion}} of QCD amplitudes in the sector with baryon number one. We show that the chiral expansion of these amplitudes breaks down in certain regions of phase space and present a framework which leads to a coherent description throughout the low energy region, while keeping Lorentz and chiral invariance manifest at every stage of the calculation. We explain how to construct a representation of the pion nucleon scattering amplitude in terms of functions of a single variable, which is valid to O(q^ 4) and properly accounts for the ππ- and π <b>N-cuts</b> required by unitarity. Comment: Latex, 12 pages. Plenary talk given at "Chiral Dynamics 2000 : Theory and Experiment", Newport News, USA, 17 - 22 July 200...|$|R
40|$|The {{theoretical}} {{basis for this}} thesis {{can be found in}} the subject of differential geometry where both line and surface curvature is a core feature. We begin with a review of curvature basics, establish notational conventions, and contribute new results (on <b>n-cuts)</b> which are of importance for this thesis. A new scale invariant curvature measure is presented. Even though curvature of continuous smooth lines and surfaces is a well-defined property, when working with digital surfaces, curvature can only be estimated. We review the nature of digitized surfaces and present a number of curvature estimators, one of which (the 3 -cut mean estimator) is new. We also develop an estimator for our new scale invariant curvature measure, and apply it to digital surfaces. Surface curvature maps are defined and examples are presented. A number of curvature visualization examples are provided. In practical applications, the noise present in digital surfaces usually precludes the possibility of direct curvature calculation. We address this noise problem with solutions including a new 2. 5 D filter. Combining techniques, we introduce a data processing pipeline designed to generate surface registration markers which can be used to identify correspondences between multiple surfaces. We present a method (projecting curvature maps) in which high resolution detail is merged with a simplified mesh model for visualization purposes. Finally, we present the results of experiments (using texture projection merging and image processing assisted physical measurement) in which we have identified, characterized, and produced visualizations of selected fine surface detail from a digitization of Michelangelo’s David statue. iii Acknowledgement...|$|R
40|$|New {{generation}} sequencing technologies offer unique {{opportunities and}} challenges for re-sequencing studies. In this article, we focus on re-sequencing experiments using the Solexa technology, based on bacterial artificial chromosome (BAC) clones, and address an experimental design problem. In these specific experiments, approximate coordinates of the BACs on a reference genome are known, and fine-scale differences between the BAC sequences and the reference are of interest. The high-throughput characteristics of the sequencing technology {{makes it possible to}} multiplex BAC sequencing experiments by pooling BACs for a cost-effective operation. However, the way BACs are pooled in such re-sequencing experiments has an effect on the downstream analysis of the generated data, mostly due to subsequences common to multiple BACs. The experimental design strategy we develop in this article offers combinatorial solutions based on approximation algorithms for the well-known max <b>n-cut</b> problem and the related max n-section problem on hypergraphs. Our algorithms, when applied to a number of sample cases give more than a 2 -fold performance improvement over random partitioning...|$|E
40|$|Image {{segmentation}} is {{an important}} step in the image analysis process. Current image segmentation techniques, however, require that the user tune several parameters in order to obtain maximum segmentation accuracy, a computationally inefficient approach, especially when a large number of images must be processed sequentially in real time. Another major challenge, particularly with medical image analysis, is the discrepancy between objective measures for assessing and guiding the segmentation process, on the one hand, and the subjective perception of the end users (e. g., clinicians), on the other. Hence, the setting and adjustment of parameters for medical image segmentation should be performed in a manner that incorporates user feedback. Despite the substantial number of techniques proposed in recent years, accurate segmentation of digital images remains a challenging task for automated computer algorithms. Approaches based on machine learning hold particular promise in this regard because, in many applications, including medical image analysis, frequent user intervention can be assumed as a means of correcting the results, thereby generating valuable feedback for algorithmic learning. This thesis presents an investigation of the use of evolving fuzzy systems for designing a method that overcomes the problems associated with medical image segmentation. An evolving fuzzy system can be trained using a set of invariant features, along with their optimum parameters, which act as a target for the system. Evolving fuzzy systems are also capable of adjusting parameters based on online updates of their rule base. This thesis proposes three different approaches that employ an evolving fuzzy system for the continual adjustment of the parameters of any medical image segmentation technique. The first proposed approach is based on evolving fuzzy image segmentation (EFIS). EFIS can adjust the parameters of existing segmentation methods and switch between them or fuse their results. The evolving rules have been applied for breast ultrasound images, with EFIS being used to adjust the parameters of three segmentation methods: global thresholding, region growing, and statistical region merging. The results for ten independent experiments {{for each of the three}} methods show average increases in accuracy of 5 %, 12 % and 9 % respectively. A comparison of the EFIS results with those obtained using five other thresholding methods revealed improvements. On the other hand, EFIS has some weak points, such as some fixed parameters and an inefficient feature calculation process. The second approach proposed as a means of overcoming the problems with EFIS is a new version of EFIS, called self-configuring EFIS (SC-EFIS). SC-EFIS uses the available data to estimate all of the parameters that are fixed in EFIS and has a feature selection process that selects suitable features based on current data. SC-EFIS was evaluated using the same three methods as for EFIS. The results show that SC-EFIS is competitive with EFIS but provides a higher level of automation. In the third approach, SC-EFIS is used to dynamically adjust more than one parameter, for example, three parameters of the normalized cut (<b>N-cut)</b> segmentation technique. This method, called multi-parametric SC-EFIS (MSC-EFIS), was applied to magnetic resonance images (MRIs) of the bladder and to breast ultrasound images. The results show the ability of MSC-EFIS to adjust multiple parameters. For ten independent experiments for each of the bladder and the breast images, this approach produced average accuracies that are 8 % and 16 % higher respectively, compared with their default values. The experimental results indicate that the proposed algorithms show significant promise in enhancing image segmentation, especially for medical applications...|$|E
40|$|One of {{the results}} of modern era is a massive {{production}} and usage of manifold electronic resources. Number of digital collections, digital libraries and repositories who offer these resources to users, usually by search mechanisms, are increasing. This is especially evident in scientific research and education area. Above mentioned services for managing electronic resources use metadata and metadata records, respectively. Many authors present metadata as data about data or information about information, although better definition exists for some time that metadata represent structured information that describe, explain, locate or on any other way provide easier retrieval, usage and managing of information sources. Despite the large awareness of metadata importance their usage doesn’t achieve the potential they offer. Many resources have metadata with bad or low quality or even don’t have them. Namely, the exceptional growth of numbers of electronic resources, changes in hardware and software and services their management and creation become complicated. Creation of metadata or their generation respectively, can be in general tackled by one of the following approaches: handmade generation, automated generation, combination of handmade and automated, and conversion from existing metadata. Automated generation can be further divided into two fields: metadata extraction and metadata harvesting. Accordingly, several tools for that exist. Especially interesting are tools for keywords extraction, as a subset of metadata elements that represent comprehensive description of electronic resource contents. In literature, the efficiency of these tools is measured with metrics from information retrieval: precision, recall and f-measure. In Master’s Thesis metadata as a whole are considered, most attention is devoted to tools for their automated generation. Latter tools for keywords extraction usually use combinations of the following approaches and techniques: stemmation, using phrase boundaries, stop words and stop phrases, evolution algorithms, machine learning, and natural language processing. Common section of stemmation and evaluation of extraction efficiency are matching criteria. In the first experiment of Thesis the efficiency of different keywords extraction tools (Kea, Yahoo! Term Extractor, SAmgI, and TextRank) is considered by using two real sets of resources (educational resources and conference contributions) in Slovene language, different matching criteria (exact matching, <b>n-cut,</b> soundex, metaphone, and similar text). Different conversions (Apache Tika, pdftotext, copy & paste, and manual conversion) that prepare original file to form that is acceptable for these tools were used. We have shown that conversions influence on extraction, but not always to improve results. Matching criteria were comparable, and significant better was tool Kea. We also observed that extraction from educational resources was worse than extraction from conference contributions, and that extraction from Slovene texts is worse than extraction from English texts. In the second experiment multi-language searching of educational resources is treated. Three machine natural language translators (Google translate, Microsoft Bing, and Amebis Presis) were used on existing authors' keywords and resource contents that were then used as input for above mentioned keywords extraction tools. Additionally the opposite approach where machine translators were used after keywords extraction was introduced. Significantly best results were obtained by translating given keywords or on average best with combination of two approaches when authors' keywords did not exist. The best machine translator was Google translate and the best keywords extraction tool was Kea. The most important conclusions from experiments are: the most efficient tool for keywords extraction is Kea; a development of such tool for Slovene language is needed to achieve comparable efficiency on English texts; despite the keywords extraction tools that are specialized for English texts the most efficient searching for resources in foreign languages where keywords exist is obtained by translating keywords in the search language and by using combination of two approaches if keywords do not exist. ...|$|E

