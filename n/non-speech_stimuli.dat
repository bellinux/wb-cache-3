50|14|Public
40|$|It {{has long}} been known that the {{identification}} of aural stimuli as speech is context-dependent (Remez et al., 1981). Here, we demonstrate that the discrimination of speech stimuli from their non-speech transforms is further modulated by their linguistic structure. We gauge the effect of phonological structure on discrimination across different manifestations of well-formedness in two distinct languages. One case examines the restrictions on English syllables (e. g., the well-formed melif vs. ill-formed mlif); another investigates the constraints on Hebrew stems by comparing ill-formed AAB stems (e. g., TiTuG) with well-formed ABB and ABC controls (e. g., GiTuT, MiGuS). In both cases, <b>non-speech</b> <b>stimuli</b> that conform to well-formed structures are harder to discriminate from speech than stimuli that conform to ill-formed structures. Auxiliary experiments rule out alternative acoustic explanations for this phenomenon. In English, we show that acoustic manipulations that mimic the mlif–melif contrast do not impair the classification of <b>non-speech</b> <b>stimuli</b> whose structure is well-formed (i. e., disyllables with phonetically short vs. long tonic vowels). Similarly, <b>non-speech</b> <b>stimuli</b> that are ill-formed in Hebrew present no difficulties to English speakers. Thus, <b>non-speech</b> <b>stimuli</b> are harder to classify only when they are well-formed in the participants’ native language. We conclude that the classification of <b>non-speech</b> <b>stimuli</b> is modulated by their linguistic structure: inputs that support well-formed outputs are more readily classified as speech...|$|E
40|$|ERPs of 27 scalp {{electrodes}} {{were recorded}} from fourteen volunteers during {{the presentation of}} binaurally presented speech and <b>non-speech</b> <b>stimuli.</b> Speech stimuli consisted of nine different consonant-vowel syllables. <b>Non-speech</b> <b>stimuli</b> were generated using special envelope-modulated low pass noise. With this method, kinematic signals are time and amplitude normalized, allowing us to observe the processing of the underlying spectral cues. We presented the speech and <b>non-speech</b> <b>stimuli</b> in three different conditions to examine {{the influence of the}} presentation order (alternating vs. randomized) in two passive conditions and the influence of attention (active vs. passive) in one active condition. The ERPs to speech-stimuli were enhanced, in contrast to the ERPs to noise stimuli, independently of the type of stimulus presentation. The results give evidence for an early electrophysiological correlate of distinct neuronal processing of speech and noise stimuli. The present study shows that this type of noise-stimuli seems to be a useful tool for the comparison of the processing of speech...|$|E
40|$|We {{investigate}} the discrimination of phrase final pitch contours within a continuum from statement to question in English. Pre-vious work in German [14] and Dutch [13] has raised ques-tions {{about the relationship}} between discrimination sensitivity and category structure within this continuum. To clarify the re-lationship between linguistic category and simple auditory dis-crimination, we employ both speech and <b>non-speech</b> <b>stimuli.</b> For all stimuli, we find a discrimination peak at the point in the continuum where a pitch fall changes to a pitch rise. This peak {{does not appear to be}} related to the category boundary for speech stimuli, as revealed in a labeling task. Discrimination was somewhat better for <b>non-speech</b> <b>stimuli</b> than speech. 1...|$|E
40|$|On {{the basis}} of {{evidence}} from six areas of speech research, {{it is argued that}} {{there is no reason to}} assume that speech stimuli are processed by structures that are inherently different from structures used for other auditory stimuli. It is concluded that speech and <b>non-speech</b> auditory <b>stimuli</b> are probably perceived in the same way...|$|R
40|$|Studies {{showed that}} {{individuals}} with Autism Spectrum Disorders (ASD) demonstrated enhanced pitch perception ability when compared to typical individuals. This study compared pitch perception of 20 adults with ASD and 20 matched neurotypical (NT) controls who spoke Cantonese as their native language. The matching parameters included gender, age, education background, and experience of formal musical training. Real word, nonsense word, and <b>non-speech</b> <b>stimulus</b> pairs with different levels of pitch differences were synthesized. In an auditory discrimination task, participants had {{to determine whether the}} stimuli in a pair were the same or different. Results revealed {{no significant difference between the}} ASD and the control groups in the three stimulus types implying {{that individuals with}} ASD did not have superior pitch perception ability when compared to NT controls. Instead, people with musical training, regardless of group membership, showed better performance in detecting small differences in pairs in all three stimulus types. published_or_final_versionSpeech and Hearing SciencesBachelorBachelor of Science in Speech and Hearing Science...|$|R
40|$|University of Minnesota M. A. thesis. May 2014. Major: Speech-Language Pathology. Advisor: Dr. Yang Zhang. 1 {{computer}} file (PDF); viii, 96 pages, appendices A-E. The current study employed behavioral and event-related potential (ERP) measures to investigate brain plasticity associated with second-language (L 2) phonetic learning {{based on an}} adaptive computer training program. The program utilized the acoustic characteristics of Infant-Directed Speech (IDS) to train monolingual American English-speaking listeners to perceive Mandarin lexical tones. Behavioral identification and discrimination tasks were conducted using naturally recorded speech, carefully controlled synthetic speech, and <b>non-speech</b> control <b>stimuli.</b> The ERP experiments were conducted with selected synthetic speech stimuli in a passive listening oddball paradigm. Identical pre- and post- tests were administered on nine adult listeners, who completed two-to-three hours of perceptual training. The perceptual training sessions used pair-wise lexical tone identification, and progressed through seven levels of difficulty for each tone pair. The levels of difficulty included progression in speaker variability from one to four speakers and progression through four levels of acoustic exaggeration of duration, pitch range, and pitch contour. Behavioral results for the natural speech stimuli revealed significant training-induced improvement in identification of Tones 1, 3, and 4. Improvements in identification of Tone 4 generalized to novel stimuli as well. Additionally, comparison between discrimination of across-category and within-category stimulus pairs taken from a synthetic continuum revealed a training-induced shift toward more native-like categorical perception of the Mandarin lexical tones. Analysis of the Mismatch Negativity (MMN) responses in the ERP data revealed increased amplitude and decreased latency for pre-attentive processing of across-category discrimination {{as a result of}} training. There were also laterality changes in the MMN responses to the <b>non-speech</b> control <b>stimuli,</b> which could reflect reallocation of brain resources in processing pitch patterns for the across-category lexical tone contrast. Overall, the results support the use of IDS characteristics in training non-native speech contrasts and provide impetus for further research...|$|R
40|$|Starting from {{a series}} of speech stimuli {{representing}} an F 0 peak shift continuum from German early to medial peak, a series of <b>non-speech</b> <b>stimuli</b> is created. These non-speech sti-muli show the F 0 and intensity courses of the original speech stimuli, but with a constant formant structure. The results of a perception experiment reveal that the organisation of the peak shift continuum found for the identification of early and me-dial peaks in the speech stimuli can be replicated by the <b>non-speech</b> <b>stimuli,</b> indicating that early and medial peaks are sig-nalled by an interplay of the F 0 and intensity courses without reference to the spectral change at the accented-vowel onset. 1...|$|E
40|$|Background: Temporal lobe {{epilepsy}} (TLE) is {{a neurological}} disorder that directly affects cortical areas responsible for auditory processing. The resulting abnormalities {{can be assessed}} using event-related potentials (ERP), which have high temporal resolution. However, {{little is known about}} TLE in terms of dysfunction of early sensory memory encoding or possible correlations between EEGs, linguistic deficits, and seizures. Mismatch negativity (MMN) is an ERP component – elicited by introducing a deviant stimulus while the subject is attending to a repetitive behavioural task – which reflects pre-attentive sensory memory function and reflects neuronal auditory discrimination and perceptional accuracy. Hypothesis: We propose an MMN protocol for future clinical application and research based on the hypothesis that children with TLE may have abnormal MMN for speech and <b>non-speech</b> <b>stimuli.</b> The MMN can be elicited with a passive auditory oddball paradigm, and the abnormalities might be associated with the location and frequency of epileptic seizures. Significance: The suggested protocol might contribute {{to a better understanding of}} the neuropsychophysiological basis of MMN. We suggest that in TLE central sound representation may be decreased for speech and <b>non-speech</b> <b>stimuli.</b> Discussion: MMN arises from a difference to speech and <b>non-speech</b> <b>stimuli</b> across electrode sites. TLE in childhood might be a good model for studying topographic and functional auditory processing and its neurodevelopment, pointing to MMN as a possible clinical tool for prognosis, evaluation, follow-up, and rehabilitation for TLE...|$|E
40|$|Psychophysical {{phenomena}} such as categorical {{perception and}} the perceptual magnet effect indicate that our auditory perceptual spaces are warped for some stimuli. This paper investigates {{the effects of}} two different kinds of training on auditory perceptual space. It is first shown that categorization training using <b>non-speech</b> <b>stimuli,</b> in which subjects learn to identify stimuli within a particular frequency range as member...|$|E
40|$|This {{dissertation}} addressed {{important questions}} regarding audiovisual (AV) perception. Study 1 revealed that AV speech perception modulated auditory processes, whereas AV non-speech perception affected visual processes. Interestingly, stimulus identification improved, yet fewer neural resources, {{as reflected in}} smaller event-related potentials, were recruited, indicating that AV perception led to multisensory efficiency. Also, AV interaction effects were observed of early and late stages, demonstrating that multisensory integration involved a neural network. Study 1 showed that multisensory efficiency is a common principle in AV speech and <b>non-speech</b> <b>stimulus</b> recognition, yet it is reflected in different modalities, possibly due to sensory dominance of a given task. Study 2 extended our understanding of multisensory interaction by investigating electrophysiological processes of AV speech perception in noise and whether those differ between younger and older adults. Both groups revealed multisensory efficiency. Behavioural performance improved while the auditory N 1 amplitude was reduced during AV relative to unisensory speech perception. This amplitude reduction {{could be due to}} visual speech cues providing complementary information, therefore reducing processing demands for the auditory system. AV speech stimuli also led to an N 1 latency shift, suggesting that auditory processing was faster during AV than during unisensory trials. This shift was more pronounced in older than in younger adults, indicating that older adults made more effective use of visual speech. Finally, auditory functioning predicted the degree of the N 1 latency shift, which is consistent with the inverse effectiveness hypothesis which argues that the less effective the unisensory perception was, the larger was the benefit derived from AV speech cues. These results suggest that older adults were better "lip/speech" integrators than younger adults, possibly to compensate for age-related sensory deficiencies. Multisensory efficiency was evident in younger and older adults but it might be particularly relevant for older adults. If visual speech cues could alleviate sensory perceptual loads, the remaining neural resources could be allocated to higher level cognitive functions. This dissertation adds further support to the notion of multisensory interaction modulating sensory-specific processes and it introduces the concept of multisensory efficiency as potential principle underlying AV speech and non-speech perceptio...|$|R
40|$|The {{role of the}} two hemispheres in the neurorehabilitation of {{language}} is still under dispute. This study explored the changes in language-evoked brain activation over a two-week treatment interval with intensive constraint induced aphasia therapy (CIAT), which is also called intensive language action therapy (ILAT). Functional magnetic resonance imaging (fMRI) {{was used to assess}} brain activation in perilesional left hemispheric and in homotopic right hemispheric areas during passive listening to high and low-ambiguity sentences and <b>non-speech</b> control <b>stimuli</b> in chronic non-fluent aphasia patients. All patients demonstrated significant clinical improvements {{of language}} functions after therapy. In an event-related fMRI experiment, a significant increase of BOLD signals was manifest in right inferior frontal and temporal areas. This activation increase was stronger for highly ambiguous sentences than for unambiguous ones. These results suggest that the known language improvements brought about by intensive constraint-induced language action therapy at least in part relies on circuits within the right-hemispheric homologues of left-perisylvian language areas, which are most strongly activated in the processing of semantically complex language...|$|R
40|$|The {{mismatch}} negativity (MMN) {{component of}} auditory event-related brain potentials {{can be used}} as a probe to study the representation of sounds in auditory sensory memory (ASM). Yet it has been shown that an auditory MMN can also be elicited by an illusory auditory deviance induced by visual changes. This suggests that some visual information may be encoded in ASM and is accessible to the auditory MMN process. It is not known, however, whether visual information affects ASM representation for any audiovisual event or whether this phenomenon is limited to specific domains in which strong audiovisual illusions occur. To highlight this issue, we have compared the topographies of MMNs elicited by <b>non-speech</b> audiovisual <b>stimuli</b> deviating from audiovisual standards on the visual, the auditory, or both dimensions. Contrary to what occurs with audiovisual illusions, each unimodal deviant elicited sensory-specific MMNs, and the MMN to audiovisual deviants included both sensory components. The visual MMN was, however, different from a genuine visual MMN obtained in a visual-only control oddball paradigm, suggesting that auditory and visual information interacts before the MMN process occurs. Furthermore, the MMN to audiovisual deviants was significantly different from the sum of the two sensory-specific MMNs, showing that the processes of visual and auditory change detection are not completely independent...|$|R
40|$|Previous {{studies suggest}} {{fundamental}} {{differences between the}} perceptual learning of speech and <b>non-speech</b> <b>stimuli.</b> One major difference is in the way variability in the training set affects learning and its generalization to untrained stimuli: training-set variability appears to facilitate speech learning, while slowing or altogether extinguishing non-speech auditory learning. We asked whether {{the reason for this}} apparent difference is a consequence of the very different methodologies used in speech and non-speech studies. We hypothesized that speech and non-speech training would result in a similar pattern of learning if they were trained using the same training regimen. We used a 2 (random vs. blocked pre- and post-testing) × 2 (random vs. blocked training) × 2 (speech vs. non-speech discrimination task) study design, yielding 8 training groups. A further 2 groups acted as untrained controls, test-ed with either random or blocked stimuli. The speech task required syllable discrimination along 4 minimal-pair continua (e. g., bee-dee), and the <b>non-speech</b> <b>stimuli</b> required duration discrimination around 4 base durations (e. g., 50 ms). Training and testing required listeners to pick the odd-one-out of three stimuli, two of which were the base duration or phonem...|$|E
40|$|Integration of {{simultaneous}} auditory {{and visual}} information about an event can enhance {{our ability to}} detect that event. This is particularly evident {{in the perception of}} speech, where the articulatory gestures of the speaker 2 ̆ 7 s lips and face can significantly improve the listener 2 ̆ 7 s detection and identification of the message, especially when that message is presented in a noisy background. Speech is a particularly important example of multisensory integration because of its behavioural relevance to humans and also because brain regions have been identified that appear to be specifically tuned for auditory speech and lip gestures. Previous research has suggested that speech stimuli may have an advantage over other types of auditory stimuli in terms of audio-visual integration. Here, we used a modified adaptive psychophysical staircase approach to compare the influence of congruent visual stimuli (brief movie clips) on the detection of noise-masked auditory speech and <b>non-speech</b> <b>stimuli.</b> We found that congruent visual stimuli significantly improved detection of an auditory stimulus relative to incongruent visual stimuli. This effect, however, was equally apparent for speech and <b>non-speech</b> <b>stimuli.</b> The findings suggest that speech stimuli are not specifically advantaged by audio-visual integration for detection at threshold when compared with other naturalistic sounds. © 2010 Elsevier Inc...|$|E
40|$|In {{the present}} study we {{investigated}} the functional organization of sublexical auditory perception with specific respect to auditory spectro-temporal processing in speech and non-speech sounds. Participants discriminated verbal and nonverbal auditory stimuli according to either spectral or temporal acoustic features {{in the context of}} a sparse event-related functional magnetic resonance imaging (fMRI) study. Based on recent models of speech processing, we hypothesized that auditory segmental processing, as is required in the discrimination of speech and non-speech sound according to its temporal features, will lead to a specific involvement of a left-hemispheric dorsal processing network comprising the posterior portion of the inferior frontal cortex and the inferior parietal lobe. In agreement with our hypothesis results revealed significant responses in the posterior part of the inferior frontal gyrus and the parietal operculum of the left hemisphere when participants had to discriminate speech and <b>non-speech</b> <b>stimuli</b> based on subtle temporal acoustic features. In contrast, when participants had to discriminate speech and <b>non-speech</b> <b>stimuli</b> on the basis of changes in the frequency content, we observed bilateral activations along the middle temporal gyrus and superior temporal sulcus. The results of {{the present study}} demonstrate an involvement of the dorsal pathway in the segmental sublexical analysis of speech sounds as well as in the segmental acoustic analysis of non-speech sounds with analogous spectro-temporal characteristics...|$|E
40|$|An {{intrinsic}} {{aspect of}} human processing of multi modal streams {{of information is}} the grouping of stimuli across modalities. Without much effort {{we are able to}} link acoustic speech to the visible movements {{in the face of a}} human speaker, for example, even if we are not familiar with the speaker's voice at all. Similarly, in daily life we easily link <b>non-speech</b> auditory <b>stimuli</b> to the objects that generate them. In applications such as graphical user interfaces with interactive audio or in virtual reality, it is essential to understand how interactive audio can be designed in such a way that users are able to link it to objects within the visual display. Without this link the audio is bound to stand apart from the visual information, which can greatly reduce the added value of audio in the context of interfaces and virtual reality applications. In this study experiments were done to investigate the users' ability to link or group synthetic auditory and visual streams. For this purpose, ways were explored to achieve auditory-visual grouping. Furthermore, sensitivity to auditory-visual synchrony of these stimuli was investigated as well as the ability of users to select objects on the basis of auditory-visual grouping. Results indicate that human information processing incorporates robust mechanisms for grouping auditory-visual stimuli. These mechanisms, however, seem to be rather vulnerable to auditory-visual delays...|$|R
40|$|Auditory {{categorization}} {{is a vital}} {{skill for}} perceiving the acoustic environment. Categorization depends on the discriminability of the sensory input {{as well as on}} the ability of the listener to adaptively make use of the relevant features of the sound. Previous studies on categorization have focused either on speech sounds when studying discriminability or on visual stimuli when assessing optimal cue utilization. Here, by contrast, we examined neural sensitivity to stimulus discriminability and optimal cue utilization when categorizing novel, <b>non-speech</b> auditory <b>stimuli</b> not affected by long-term familiarity. In a functional magnetic resonance imaging (fMRI) experiment, listeners categorized sounds from two category distributions, differing along two acoustic dimensions: spectral shape and duration. By introducing spectral degradation after the first half of the experiment, we manipulated both stimulus discriminability and the relative informativeness of acoustic cues. Degradation caused an overall decrease in discriminability based on spectral shape, and therefore enhanced the informativeness of duration. A relative increase in duration-cue utilization was accompanied by increased activity in left parietal cortex. Further, discriminability modulated right planum temporale activity to a higher degree when stimuli were spectrally degraded than when they were not. These findings provide support for separable contributions of parietal and posterior temporal areas to perceptual categorization. The parietal cortex seems to support the selective utilization of informative stimulus cues, while the posterior superior temporal cortex as a primarily auditory brain area supports discriminability particularly under acoustic degradation...|$|R
40|$|Children with {{specific}} language impairments (SLIs) show impaired perception {{and production of}} spoken language, and can also present with motor, auditory and phonological difficulties. Recent auditory studies have shown impaired sensitivity to amplitude rise time (ART) in children with SLIs, along with non-speech rhythmic timing difficulties. Linguistically, these perceptual impairments should affect sensitivity to speech prosody and syllable stress. Here we used two tasks requiring sensitivity to prosodic structure, the DeeDee task and a stress misperception task, to investigate this hypothesis. We also measured auditory processing of ART, rising pitch and sound duration, in both speech (ba) and <b>non-speech</b> (tone) <b>stimuli.</b> Participants were 45 children with SLI aged on average 9 years and 50 age-matched controls. We report data for all the SLI children (N = 45, IQ varying), {{as well as for}} two independent SLI subgroupings with intact IQ. One subgroup, Pure SLI, had intact phonology and reading (N= 16), the other, SLI PPR (N= 15), had impaired phonology and reading. Problems with syllable stress and prosodic structure were found for all the group comparisons. Both sub-groups with intact IQ showed reduced sensitivity to ART in speech stimuli, but the PPR subgroup also showed reduced sensitivity to sound duration in speech stimuli. Individual differences in processing syllable stress were associated with auditory processing. These data support a new hypothesis, the ‘prosodic phrasing’ hypothesis, which proposes that grammatical difficulties in SLI may reflect perceptual difficulties with global prosodic structure related to auditory impairments in processing amplitude rise time and duration...|$|R
40|$|Research {{has shown}} that {{inversion}} is more detrimental to the perception of faces than to the perception {{of other types of}} visual stimuli. Inverting a face results in an impairment of configural information processing that leads to slowed early face processing and reduced accuracy when performance is tested in face recognition tasks. We investigated the effects of inverting speech and <b>non-speech</b> <b>stimuli</b> on audiovisual temporal perception. Upright and inverted audiovisual video clips of a person uttering syllables (experiments 1 and 2), playing musical notes on a piano (experiment 3), or a rhesus monkey producing vocalisations (experiment 4) were presented. Participants made unspeeded temporal-order judgments regarding which modality stream (auditory or visual) appeared to have been presented first. Inverting the visual stream did not have any effect on the sensitivity of temporal discrimination responses in any of the four experiments, thus implying that audiovisual temporal integration is resilient to the effects of orientation in the picture plane. By contrast, the point of subjective simultaneity differed significantly as a function of orientation only for the audiovisual speech stimuli but not for the <b>non-speech</b> <b>stimuli</b> or monkey calls. That is, smaller auditory leads were required for the inverted than for the upright-visual speech stimuli. These results are consistent with the longer processing latencies reported previously when human faces are inverted and demonstrates that the temporal perception of dynamic audiovisual speech can be modulated by changes in the physical properties of the visual speech (ie by changes in orientation) ...|$|E
40|$|Purpose: Phonological {{accounts}} of reading implicate three aspects of phonological awareness tasks {{that underlie the}} relationship with reading; a) the language-based nature of the stimuli (words or nonwords), b) the verbal nature of the response, and c) {{the complexity of the}} stimuli (words can be segmented into units of speech). Yet, it is uncertain which task characteristics are most important as they are typically confounded. By systematically varying response-type and stimulus complexity across speech and <b>non-speech</b> <b>stimuli,</b> the current study seeks to isolate the characteristics of phonological awareness tasks that drive the prediction of early reading. Method: Four sets of tasks were created; tone stimuli (simple non-speech) requiring a non-verbal response, phonemes (simple speech) requiring a non-verbal response, phonemes requiring a verbal response, and nonwords (complex speech) requiring a verbal response. Tasks were administered to 570 2 nd grade children along with standardized tests of reading and non-verbal IQ. Results: Three structural equation models comparing matched sets of tasks were built. Each model consisted of two 'task' factors with a direct link to a reading factor. The following factors predicted unique variance in reading: a) simple speech and <b>non-speech</b> <b>stimuli,</b> b) simple speech requiring a verbal response but not simple speech requiring a non-verbal-response, and c) complex and simple speech stimuli. Conclusions: Results suggest that the prediction of reading by phonological tasks is driven by the verbal nature of the response and not the complexity or 'speechness' of the stimuli. Findings highlight the importance of phonological output processes to early reading...|$|E
40|$|In {{the present}} study, we {{investigated}} {{the processing of}} word stress related acoustic features in a word context. In a passive oddball multi-feature MMN experiment, we presented a disyllabic pseudo-word with two acoustically similar syllables as standard stimulus, and five contrasting deviants that differed from the standard in that they were either stressed on the first syllable or contained a vowel change. Stress was realized by an increase of f 0, intensity, vowel duration or consonant duration. The vowel change was used to investigate if phonemic and prosodic changes elicit different MMN components. As a control condition, we presented non-speech counterparts of the speech stimuli. Results showed all but one feature (non-speech intensity deviant) eliciting the MMN component, which was larger for speech compared to <b>non-speech</b> <b>stimuli.</b> Two other components showed stimulus related effects: the N 350 and the LDN (Late Discriminative Negativity). The N 350 appeared to the vowel duration and consonant duration deviants, specifically to features related to the temporal characteristics of stimuli, while the LDN was present for all features, and it was larger for speech than for <b>non-speech</b> <b>stimuli.</b> We {{also found that the}} f 0 and consonant duration features elicited a larger MMN than other features. These results suggest that stress as a phonological feature is processed based on long-term representations, and listeners show a specific sensitivity to segmental and suprasegmental cues signaling the prosodic boundaries of words. These findings support a two-stage model in the perception of stress and phoneme related acoustical information...|$|E
40|$|The {{possibility}} that developmental dyslexia results from low-level sensory processing deficits has received {{renewed interest in}} recent years. Opponents of such sensory-based explanations argue that dyslexia arises primarily from phonological impairments. However, many behavioural correlates of dyslexia cannot be explained sufficiently by cognitive-level accounts and there is anatomical, psychometric and physiological evidence of sensory deficits in the dyslexic population. This thesis aims {{to determine whether the}} low-level (pre-attentive) processing of simple auditory stimuli is disrupted in compensated adult dyslexics. Using psychometric and neurophysiological measures, the nature of auditory processing abnormalities is investigated. Group comparisons are supported by analysis of individual data in order {{to address the issue of}} heterogeneity in dyslexia. The participant pool consisted of seven compensated dyslexic adults and seven age and IQ matched controls. The dyslexic group were impaired, relative to the control group, on measures of literacy, phonological awareness, working memory and processing speed. Magnetoencephalographic recordings were conducted during processing of simple, <b>non-speech,</b> auditory <b>stimuli.</b> Results confirm that low-level auditory processing deficits are present in compensated dyslexic adults. The amplitude of N 1 m responses to tone pair stimuli were reduced in the dyslexic group. However, there was no evidence that manipulating either the silent interval or the frequency separation between the tones had a greater detrimental effect on dyslexic participants specifically. Abnormal MMNm responses were recorded in response to frequency deviant stimuli in the dyslexic group. In addition, complete stimulus omissions, which evoked MMNm responses in all control participants, failed to elicit significant MMNm responses in all but one of the dyslexic individuals. The data indicate both a deficit of frequency resolution at a local level of auditory processing and a higher-level deficit relating to the grouping of auditory stimuli, relevant for auditory scene analysis. Implications and directions for future research are outlined. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Children with {{listening}} difficulties, but normal audiometry, may {{be diagnosed}} with APD. The diagnosis is typically based on poor performance on tests of perception of both <b>non-speech</b> and speech <b>stimuli.</b> However, <b>non-speech</b> test results correlate only weakly with evaluations of speech-in-noise processing, cognitive skills, and caregiver evaluations of listening ability. The interpretation of speech test results is confounded by the involvement of language processing mechanisms. Overall, listening ability is associated more with higher-level, cognitive and analytic processing than with lower-level sensory processing. Current diagnosis {{of a child with}} APD, rather than another problem (e. g. language impairment, LI), is determined more by the referral route than by the symptoms. Co-occurrence with other learning problems suggests that APD may be a symptom of a more varied neurodevelopmental disorder. Alternately, APD has been proposed as a cause of language-based disorders, but there is no one-to-one mapping between listening and language among individuals. Screening for APD may be most appropriately based on a well-validated, caregiver questionnaire that captures the fundamental problem of listening difficulties and identifies areas for further assessment and management. This approach has proved successful for LI, and may in future serve as a metric to help assess other, objective testing methods<br/...|$|R
40|$|Abnormalities in the {{integration}} of auditory and visual language inputs could underlie many core psychotic features. Perceptual confusion may arise because of the normal propensity of visual speech perception to evoke auditory percepts. Recent functional neuroimaging studies of normal subjects have demonstrated activation in auditory-linguistic brain areas in response to silent lip-reading. Three {{functional magnetic resonance imaging}} experiments were carried out on seven normal volunteers, and 14 schizophrenia patients, half of whom were actively psychotic. The tasks involved listening to auditory speech, silent Lip-reading (visual speech), and perception of meaningless lip movements (visual non-speech). Subjects also undertook a behavioural study of audio-visual word identification designed to evoke perceptual fusions. Patients and controls both showed susceptibility to audio-visual fusions on the behavioural task. The patient group as a whole showed less activation relative to controls in superior and inferior posterior temporal areas while performing the silent lip-reading task. Attending to visual non-speech, the patients activated less posterior (occipito-temporal) and more anterior (frontal, insular and striatal) brain areas than controls. This difference was accounted for Largely by the psychotic subgroup. Insular and striatal areas were also activated in both subject groups in the auditory speech perception condition, thus demonstrating the bimodal sensitivity of these regions. The results suggest that schizophrenia patients with psychotic symptoms respond to visually ambiguous <b>stimuli</b> (<b>non-speech)</b> by activation of polysensory structures. This could reflect particular processing strategies and may increase susceptibility to certain paranoid and hallucinatory symptoms...|$|R
40|$|Abstract —This study {{examined}} differential performance of normally hearing subjects using a tactile device on the dominant versus non-dominant hand. The study evalu-ated whether tactual sensitivity for <b>non-speech</b> <b>stimuli</b> was greater for the dominant hand {{as compared with}} the non-dominant hand, and secondly, whether there was an advantage for speech presented tactually to the dominant hand, resulting from a preferential pathway to the language processing area in the left cerebral hemisphere. Evaluations of threshold pulse width, dynamic ranges, paired electrode identification, and a closed-set tactual pattern discrimination test battery showed no difference in tactual sensitivity measures between the two hands. Speech perception was assessed with closed sets of vowels and consonants and with open-set Harvey Gardner (HG...|$|E
40|$|Abstract Background Recent {{studies have}} shown that the human right-hemispheric {{auditory}} cortex is particularly sensitive to reduction in sound quality, with an increase in distortion resulting in an amplification of the auditory N 1 m response measured in the magnetoencephalography (MEG). Here, we examined whether this sensitivity is specific to the processing of acoustic properties of speech or whether it can be observed also in the processing of sounds with a simple spectral structure. We degraded speech stimuli (vowel /a/), complex <b>non-speech</b> <b>stimuli</b> (a composite of five sinusoidals), and sinusoidal tones by decreasing the amplitude resolution of the signal waveform. The amplitude resolution was impoverished by reducing the number of bits to represent the signal samples. Auditory evoked magnetic fields (AEFs) were measured in the left and right hemisphere of sixteen healthy subjects. Results We found that the AEF amplitudes increased significantly with stimulus distortion for all stimulus types, which indicates that the right-hemispheric N 1 m sensitivity is not related exclusively to degradation of acoustic properties of speech. In addition, the P 1 m and P 2 m responses were amplified with increasing distortion similarly in both hemispheres. The AEF latencies were not systematically affected by the distortion. Conclusions We propose that the increased activity of AEFs reflects cortical processing of acoustic properties common to both speech and <b>non-speech</b> <b>stimuli.</b> More specifically, the enhancement is most likely caused by spectral changes brought about by the decrease of amplitude resolution, in particular the introduction of periodic, signal-dependent distortion to the original sound. Converging evidence suggests that the observed AEF amplification could reflect cortical sensitivity to periodic sounds. </p...|$|E
40|$|Past {{research}} has shown that musical training induces changes in the processing of supra-segmental aspects of speech, such as pitch and prosody. The aim {{of the present study was}} to determine whether musical expertise also leads to an altered neurophysiological processing of sub-segmental information available in the speech signal, in particular the voice onset time (VOT). Using high-density EEG recordings we analysed the neurophysiological responses to voiced and unvoiced CV syllables and noise analogues in 26 German speaking adult musicians and non-musicians. From the EEG the N 1 amplitude of the event-related potential (ERP) and two microstates from the topographical EEG analysis (one around the N 1 amplitude and one immediately preceding the N 1 microstate) were calculated to the different stimuli. Similar to earlier studies the N 1 amplitude was different to voiced and unvoiced stimuli in non-musicians with larger amplitudes to voiced stimuli. The more refined microstate analysis revealed that the microstate within the N 1 time window was shorter to unvoiced stimuli in non-musicians. For musicians there was no difference for the N 1 amplitudes and the corresponding microstates between voiced and unvoiced stimuli. In addition, there was a longer very early microstate preceding the microstate at the N 1 time window to <b>non-speech</b> <b>stimuli</b> only in musicians. Taken together, our findings suggest that musicians process unvoiced stimuli (irrespective whether these stimuli are speech or <b>non-speech</b> <b>stimuli)</b> differently. We propose that musicians utilise the same network to analyse unvoiced stimuli as for the analysis of voiced stimuli. As a further explanation it is also possible that musicians devote more neurophysiological resources into the analysis of unvoiced segments...|$|E
40|$|Previous event-related {{potential}} (ERP) research utilizing oddball stimulus paradigms suggests diminished {{processing of}} speech versus non-speech sounds {{in children with}} an Autism Spectrum Disorder (ASD). However, brain mechanisms underlying these speech processing abnormalities, {{and to what extent}} they are related to poor language abilities in this population remain unknown. In the current study, we utilized a novel paired repetition paradigm in order to investigate ERP responses associated with the detection and discrimination of speech and non-speech sounds in 4 - to 6 -year old children with ASD, compared with gender and verbal age matched controls. ERPs were recorded while children passively listened to pairs of stimuli that were either both speech sounds, both non-speech sounds, speech followed by non-speech, or non-speech followed by speech. Control participants exhibited N 330 match/mismatch responses measured from temporal electrodes, reflecting speech versus non-speech detection, bilaterally, whereas children with ASD exhibited this effect only over temporal electrodes in the left hemisphere. Furthermore, while the control groups exhibited match/mismatch effects at approximately 600 ms (central N 600, temporal P 600) when a non-speech sound was followed by a speech sound, these effects were absent in the ASD group. These findings suggest that children with ASD fail to activate right hemisphere mechanisms, likely associated with social or emotional aspects of speech detection, when distinguishing <b>non-speech</b> from speech <b>stimuli.</b> Together, these results demonstrate the presence of atypical speech versus non-speech processing in children with ASD when compared with typically developing children matched on verbal age...|$|R
40|$|Abstract Background In normal-hearing subjects, {{monaural}} stimulation {{produces a}} normal pattern of asynchrony and asymmetry over the auditory cortices {{in favour of}} the contralateral temporal lobe. While late onset unilateral deafness {{has been reported to}} change this pattern, the exact influence of the side of deafness on central auditory plasticity still remains unclear. The present study aimed at assessing whether left-sided and right-sided deafness had differential effects on the characteristics of neurophysiological responses over auditory areas. Eighteen unilaterally deaf and 16 normal hearing right-handed subjects participated. All unilaterally deaf subjects had post-lingual deafness. Long latency auditory evoked potentials (late-AEPs) were elicited by two types of <b>stimuli,</b> <b>non-speech</b> (1 kHz tone-burst) and speech-sounds (voiceless syllable/pa/) delivered to the intact ear at 50 dB SL. The latencies and amplitudes of the early exogenous components (N 100 and P 150) were measured using temporal scalp electrodes. Results Subjects with left-sided deafness showed major neurophysiological changes, {{in the form of a}} more symmetrical activation pattern over auditory areas in response to non-speech sound and even a significant reversal of the activation pattern in favour of the cortex ipsilateral to the stimulation in response to speech sound. This was observed not only for AEP amplitudes but also for AEP time course. In contrast, no significant changes were reported for late-AEP responses in subjects with right-sided deafness. Conclusion The results show that cortical reorganization induced by unilateral deafness mainly occurs in subjects with left-sided deafness. This suggests that anatomical and functional plastic changes are more likely to occur in the right than in the left auditory cortex. The possible perceptual correlates of such neurophysiological changes are discussed. </p...|$|R
40|$|We {{report the}} case of a neonate tested three weeks after a {{neonatal}} left sylvian infarct. We studied her perception of speech and <b>non-speech</b> <b>stimuli</b> with high-density event-related potentials. The results show {{that she was able to}} discriminate not only a change of timbre in tones but also a vowel change, and even a place of articulation contrast in stop consonants. Moreover, a discrimination response to stop consonants was observed even when syllables were produced by di#erent speakers. Her intact right hemisphere was thus able to extract relevant phonetic information in spite of irrelevant acoustic variation. These results suggest that both hemispheres contribute to phoneme perception during the first months of life and confirm our previous findings concerning bilateral responses in normal infants...|$|E
40|$|Developments in {{the field}} of {{cochlear}} implants (CIs) have expanded at an expeditious rate, particularly in the last two decades. As many CI recipients can now achieve highly satis-factory speech recognition performance, they are looking toward improved perception of other aural stimuli, hence the gxpansion of research interests into <b>non-speech</b> <b>stimuli.</b> This article reviews soilie of the more-recent research related to music perception wlth CIs, and how cochlear implantation may impact on music perception in adults. This issue is not only of interest to potential and current implant recipients and their families, but would also be important for clinicians to address in counselling. Overall, the current state of findings suggests that adult CI userS are significantly poorer than nor-mally hearing listeners at frequency-based music tests, such as tasks involving pitch perception, instrument identification...|$|E
40|$|Infants {{appear to}} learn {{abstract}} rule-like regularities (e. g., la la da follows an AAB pattern) more easily from speech {{than from a}} variety of other auditory and visual stimuli (Marcus et al., 2007). We test if that facilitation reflects a specialization to learn from speech alone, or from modality-independent communicative stimuli more generally, by measuring 7. 5 -month-old infants ’ ability to learn abstract rules from sign language-like gestures. Whereas infants appear to easily learn many different rules from speech, we found that with sign-like stimuli, and under circumstances comparable to those of Marcus et al. (1999), hearing infants were able to learn an ABB rule, but not an AAB rule. This is consistent with results of studies that demonstrate lower levels of infant rule learning {{from a variety of}} other <b>non-speech</b> <b>stimuli,</b> and we discuss implications for accounts of speech-facilitation...|$|E
40|$|AbstractA voice {{emanates from}} a human face and an impact sound from a hammer. These are {{examples}} of everyday multisensory experiences, but how exactly do we perceive unified multisensory events? Close temporal and spatial proximity of sensory inputs enhance the probability of those inputs belonging to a single event. Given, however, the multiple inputs that belong to different events may be presented in close spatio-temporal coherence raises {{the issue of what}} leads to the correct binding of those inputs. The “unity effect” supports that events that “go together” are the ones that eventually get integrated. In these cases, our perceptual system is more likely to treat the “related” sensory inputs as referring to the same multisensory event rather than separate unimodal events (Vatakis & Spence, 2007, 2008). A number of studies have investigated this effect or their findings can be interpreted according to the unity assumption. For example, Laurienti et al. (2004) presented a series of congruent (visual: red or blue, auditory: “red” or “blue”, respectively) and incongruent conditions where an irrelevant stimulus was presented (e. g., visual: red, auditory: “yellow”). In the former case, redundant audiovisual information were presented, whereas, in the latter case conflicting information. Speeded detection of targets (red or blue) was required. The results showed better and faster target detection in the congruent cases as compared to the incongruent. Furthermore, Vatakis and Spence (2007) evaluated the influence of the “unity effect” on the multisensory integration of audiovisual speech stimuli using an orthogonal task (no response was required regarding the matching/mismatching of the stimuli). The speech stimuli (auditory and visual) were either gender matched/mismatched or utterance matched/mismatched. They found participant performance in a temporal order judgment (TOJ) task to be better for mismatched as compared to matched cases. The poor performance in the case of matched pairs {{lies in the fact that}} integration is taking place, thus it is harder for the participants to judge the order of presentation. Vatakis and Spence (2008) demonstrated the unity effect for speech stimuli but no such effect was found for non-speech dynamic stimuli (e. g., smashing ice with a hammer) or animal calls (including humans imitating animal calls). Parise and Spence (2009), however, did demonstrate the effect utilizing simple stimuli related to crossmodal correspondences. It is as yet unclear why no unity was obtained for <b>non-speech</b> <b>stimuli.</b> One could argue that this might be due to the “special” nature of speech in terms of its temporal coherence. Thus, the missing temporal coherence in the <b>non-speech</b> <b>stimuli</b> presented in the previous studies could lead to failure of showing unity for other stimuli other than speech. The purpose of this study, therefore, is to investigate whether unity can be obtained for <b>non-speech</b> <b>stimuli</b> that are ecologically-valid but not dynamic and whether the unity effect is driven by bottom-up or top-down processes. The visual stimuli were composed of static images of a cell phone, a flashlight, and a lighter in their proper form and scrambled. The auditory stimuli were composed of a cell phone ringing, a flashlight button, and a lighter. Scrambled images were used in order to examine whether familiarity or low-level factors lead to the unity effect. The visual stimuli were presented in the off state and subsequently their own state was presented along with the matching or mismatching sound. White noise was presented throughout the experiment. Two tasks were completed, an implicit TOJ, as in the Vatakis and Spence studies, and an explicit reaction time (RT) task, as in the Laurienti et al. studies. During the TOJ task, the stimuli were presented in a matched or mismatched format in 8 different stimulus onset asynchronies (± 250, ± 130, ± 95, ± 75, 0 msec). During the RT task, the participants had to detect whether they heard, saw, or heard and saw a cell phone or a flashlight with the lighter being the irrelevant stimulus (all other combinations will also be tested). We expect, through the TOJ and RT task, to demonstrate the unity effect for <b>non-speech</b> <b>stimuli</b> given the control of the temporal coherence of the stimuli. Additionally, through the TOJ task and the use of scrambled images, we aim to investigate, for the first time, whether the unity effect is driven by top-down or bottom-up processes...|$|E
40|$|This study {{examined}} whether "melodic contour deafness" (insensitivity to {{the direction of}} pitch movement) in congenital amusia is associated with specific types of pitch patterns (discrete versus gliding pitches) or stimulus types (speech syllables versus complex tones). Thresholds for identification of pitch direction were obtained using discrete or gliding pitches in the syllable /ma/ or its complex tone analog, from nineteen amusics and nineteen controls, all healthy university students with Mandarin Chinese as their native language. Amusics, unlike controls, had more difficulty recognizing pitch direction in discrete than in gliding pitches, for both speech and <b>non-speech</b> <b>stimuli.</b> Also, amusic thresholds were not significantly affected by stimulus types (speech versus non-speech), whereas controls showed lower thresholds for tones than for speech. These findings help explain why amusics have greater difficulty with discrete musical pitch perception than with speech perception, in which continuously changing pitch movements are prevalent...|$|E
40|$|Many {{attempts}} have been made to teach native Japanese listeners to perceptually differentiate English/r-l/(e. g. rock-lock). Though improvement is evident, in no case is final performance native English-like. We focused our training on the third formant onset frequency, shown to be the most reliable indicator of/r-l/category membership. We first presented listeners with instances of synthetic/r-l/stimuli varying only in F 3 onset frequency, in a forced-choice identification training task with feedback. Evidence of learning was limited. The second experiment utilized an adaptive paradigm beginning with <b>non-speech</b> <b>stimuli</b> consisting only of/r/and/l/F 3 frequency trajectories progressing to synthetic speech instances of/ra-la/; half of the trainees received feedback. Improvement was shown by some listeners, suggesting some enhancement of/r-l/identification is possible following training with only F 3 onset frequency. However, only a subset of these listeners showed signs of generalization of the training effect beyond the trained synthetic context...|$|E
40|$|Psychophysical {{phenomena}} such as categorical {{perception and}} the perceptual magnet effect indicate that our auditory perceptual spaces are warped for some stimuli. This paper investigates {{the effects of}} two different kinds of training on auditory perceptual space. It is first shown that categorization training using <b>non-speech</b> <b>stimuli,</b> in which subjects learn to identify stimuli within a particular frequency range {{as members of the}} same category, can lead to a decrease in sensitivity to stimuli in that category. This phenomenon is an example of acquired similarity and apparently has not been previously demonstrated for a category -relevant dimension. Discrimination training with the same set of stimuli was shown to have the opposite effect: subjects became more sensitive to differences in the stimuli presented during training. Further experiments investigated some of the conditions that are necessary to generate the acquired similarity found in the first experiment. The results of these [...] ...|$|E
