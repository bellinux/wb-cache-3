3|3284|Public
40|$|A <b>neural</b> <b>net</b> <b>program</b> for pattern {{classification}} is presented, which includes: i) {{an improved}} version of Kohonen's Learning Vector Quantization (LVQ with Training Count); ii) Feed-Forward Neural Networks with Back-Propagation training; iii) Gaussian (or Mahalanobis distance) classification; iv) Fisher linear discrimination. Back-Prop trainings with emulations of Intel's ETANN and Siemens' MA 16 neural chips are available as options. The {{program has been}} developed for High Energy Physics applications. * Program file available from WWW URL: [URL] + E-mail: odorico@bo. infn. it 2 PROGRAM SUMMARY Title of program: NEURAL Catalogue number: Program obtainable from: CPC Program Library, Queen's University of Belfast, N. Ireland. Computer for which the program is designed and others on which it is operable: Any computer running a FORTRAN 77 compiler. The program has been tested on a Alpha VAX workstation running under VMS. Programming language used: FORTRAN [...] ...|$|E
40|$|The use of {{artificial}} neural systems (ANS) for various recognition tasks is well {{founded in the}} literature [Aarts, 1989; Ansari, 1993; Carpenter, 1988; Fukushima, 1983; Mui, 1994; Shang, 1994; Touretzky, 1989; Yong, 1988]. The advantage of using neural nets, as they are commonly called, {{is not that the}} solution they provide is especially elegant or even fast; it is that the system ‘learns ’ its own algorithm for the classification task, and does so on actual samples of data. Indeed, the same basic <b>neural</b> <b>net</b> <b>program</b> that recognizes digits {{can also be used to}} recognize squares, circles, and triangles. To thoroughly explore the use of neural nets is not the goal of this section; instead, the goal is to provide summary of neural nets for the uninitiated, to show the utility of the method, and to provide pointers to more detailed information...|$|E
40|$|In {{a recent}} {{series of three}} papers, Belokurov, Evans, & Le Du (2002, 2004 a, 2004 b) {{attempted}} to re-analyse the MACHO collaboration data and give alternative sets of microlensing events and alternative optical depths to microlensing toward the LMC. Even though they examined less than 0. 2 % of the data they claimed that by using a <b>neural</b> <b>net</b> <b>program</b> they could select a better (and smaller) set of microlensing candidates. Estimating the optical depth from this smaller set they claim the MACHO collaboration overestimated the optical depth by a significant factor and that the MACHO microlensing experiment is consistent with lensing by known stars in the Milky Way and LMC. Unfortunately, the analysis by these authors contains several errors which render their conclusions meaningless. Their efficiency analysis is clearly in error, and since they did not search through the entire MACHO dataset, they don’t know how many microlensing events their neural net would find in the data or what optical depth their method would give. Examination of their selected events suggests that their method misses low signal/noise events and thus would thus have lower efficiency than the MACHO selection criteria. In addition, their method is likely to give many more false positives (non-lensing events identified as lensing). Both effects would increase their estimated optical depth, and perhaps even render their current selection method useless. Finally, we note that the EROS discovery that LMC event- 23 is a variable star reduces the MACHO collaboration estimates of optical depth and Macho halo fraction by around 8 %, and does open the question of additional contamination. Key words: gravitational lensing – Galaxy: halo – dark matter – Magellanic Clouds...|$|E
50|$|The concept may be {{compared}} to ideas about intuition and <b>neural</b> <b>net</b> <b>programming.</b> The same phenomenon, but conceptualized in a radically different way, seems to be described by D.T. Suzuki in swordsmanship teaching stories recounted in his Zen and Japanese Culture, and given in analytical detail in Zen Buddhism and Psychoanalysis.|$|R
50|$|Bots {{have been}} developed, some based on <b>neural</b> <b>net</b> <b>programs</b> like gnubg, JellyFish, TD-Gammon, and Snowie, to allow human players {{to compete with}} these {{computer}} programs on FIBS and to analyze these programs' performance in real-world play. This is one way FIBS has served as an experimental platform {{for the advancement of}} computer science and continues to do so.|$|R
40|$|A {{neural network}} is {{presented}} to learn errors generated by a numerical algorithm for solving coupled nonlinear differential equations. The method is based on using a neural network to correctly learn the error generated by, for example, Runge-Kutta on a model molecular dynamics (MD) problem. The neural network programs {{used in this study}} were developed by NASA. Comparisons are made for training the neural network using backpropagation and a new method which was found to converge with fewer iterations. The <b>neural</b> <b>net</b> <b>programs,</b> the MD model and the calculations are discussed...|$|R
40|$|AbstractTD-Gammon is a {{neural network}} that {{is able to}} teach itself to play backgammon solely by playing against itself and {{learning}} from the results. Starting from random initial play, TD-Gammon's self-teaching methodology results in a surprisingly strong program: without lookahead, its positional judgement rivals that of human experts, and when combined with shallow lookahead, it reaches a level of play that surpasses even the best human players. The success of TD-Gammon has also been replicated by several other programmers; {{at least two other}} <b>neural</b> <b>net</b> <b>programs</b> also appear to be capable of superhuman play. Previous papers on TD-Gammon have focused on developing a scientific understanding of its reinforcement learning methodology. This paper views machine learning as a tool in a programmer's toolkit, and considers how it can be combined with other programming techniques to achieve and surpass world-class backgammon play. Particular emphasis is placed on programming shallow-depth search algorithms, and on TD-Gammon's doubling algorithm, which is described in print here for the first time...|$|R
40|$|Abstract. In [2, 5] it is {{showed that}} {{programming}} languages {{can be translated}} into recurrent <b>neural</b> <b>nets.</b> Implementation of <b>programming</b> languages in <b>neural</b> <b>nets</b> turns {{to be not only}} theoretical exciting but has also some practical implications in the recent efforts to merge symbolic and subsymbolic computation. To be of some use it should be carried in the context of bounded resources. With the guidelines provided in [4, 5], we introduce data types and show how to encode and keep them inside the information flow of <b>neural</b> <b>nets.</b> ...|$|R
40|$|In {{this paper}} {{we show that}} {{programming}} languages can be translated into recurrent (analog, rational weighted) <b>neural</b> <b>nets.</b> Implementation of <b>programming</b> languages in <b>neural</b> <b>nets</b> turns {{to be not only}} theoretical exciting, but has also some practical implications in the recent efforts to merge symbolic and subsymbolic computation. To be of some use, it should be carried in a context of bounded resources. Herein, we show how to use resource bounds to speed up computations over <b>neural</b> <b>nets,</b> through suitable data type coding like in the usual programming languages. We introduce data types and show how to code and keep them inside the information flow of <b>neural</b> <b>nets.</b> Data types and control structures are part of a suitable programming language called NETDEF. Each NETDEF program has a specific <b>neural</b> <b>net</b> that computes it. These nets have a strong modular structure and a synchronization mechanism allowing sequential or parallel execution of subnets, despite the massive parallel f [...] ...|$|R
40|$|Abstract. In {{this paper}} {{we show that}} {{programming}} languages can be translated on recurrent (analog, rational weighted) <b>neural</b> <b>nets.</b> Implementation of <b>programming</b> languages in <b>neural</b> <b>nets</b> turns {{to be not only}} theoretical exciting, but has also some practical implications in the recent efforts to merge symbolic and subsymbolic computation. To be of some use, it should be carried in a context of bounded resources. Herein, we show how to use resource bounds to speed up computations over <b>neural</b> <b>nets,</b> through suitable data type coding like in the usual programming languages. We introduce data types and show how to code and keep them inside the information flow of <b>neural</b> <b>nets.</b> Data types and control structures are part of a suitable programming language called NETDEF. Each NETDEF program has a specific <b>neural</b> <b>net</b> that computes it. These nets have a strong modular structure and a synchronization mechanism allowing sequential or parallel execution of subnets, despite the massive parallel feature of <b>neural</b> <b>nets.</b> Each instruction denotes an independent <b>neural</b> <b>net.</b> There are constructors for assignment, conditional and loop instructions. Besides the language core, many other features are possible using the same method. There is als...|$|R
40|$|Real-time {{identification}} of tissue would improve procedures such as stereotactic brain biopsy (SBX), functional and implantation neurosurgery, and brain tumor excision. To standard SBX equipment has been added: (1) computer-controlled stepper motors {{to drive the}} biopsy needle/probe precisely; (2) multiple microprobes to track tissue density, detect blood vessels and changes in blood flow, and distinguish the various tissues being penetrated; (3) <b>neural</b> <b>net</b> learning <b>programs</b> to allow real-time comparisons of current data with a normative data bank; (4) three-dimensional graphic displays to follow the probe as it traverses brain tissue. The probe can differentiate substances such as pig brain, differing consistencies of the 'brain-like' foodstuff tofu, and gels made to simulate brain, as well as detect blood vessels imbedded in these substances. Multimodality probes should improve the safety, efficacy, and diagnostic accuracy of SBX and other neurosurgical procedures...|$|R
40|$|Land use {{change in}} {{developing}} countries is {{of great interest to}} policymakers and researchers from many backgrounds. Concerns about consequences of deforestation for global climate change and biodiversity have received the most publicity, but loss of wetlands, declining land productivity, and watershed management are also problems facing developing countries. In developing countries, analysis is especially constrained by lack of data. This paper reviews modeling approaches for data-constrained environments that involve methods such as <b>neural</b> <b>nets</b> and dynamic <b>programming</b> and research results that link individual household survey data with satellite images using geographic positioning systems. Land Economics/Use, Q 15, Q 23, R 14,...|$|R
40|$|Land-use {{change in}} {{developing}} countries is {{of great interest to}} policy-makers and researchers with diverse interests. Concerns about consequences of deforestation for global climate change and biodiversity have received the most publicity, but loss of wetlands, declining land productivity and watershed management are also problems facing developing countries. Analyses of these problems are especially constrained by lack of data. This article reviews modelling approaches for data-constrained environments that involve discrete choice methods including <b>neural</b> <b>nets</b> and dynamic <b>programming,</b> and research results that link individual household survey data with satellite images using geographic positioning systems. Copyright 2007 The Agricultural Economics Society. ...|$|R
40|$|We {{consider}} the special {{case of the}} traveling salesman problem (TSP) in which the distance metric is the shortest-path metric of a planar unweighted graph. We present a polynomial-time approximation scheme (PTAS) for this problem. Initial work performed at UCSD, supported by NSF grant No. DMS- 9206251. y Work supported by NSF grant No. CCR- 9521606. z Work supported by NSF grant No. CCR- 9301031. 1 Introduction The traveling salesman problem [5] (TSP) has been the testbed of every new algorithmic idea during the past half-century: linear programming, cutting planes and polyhedral combinatorics, probabilistic algorithms, local search (and more recently Boltzmann machines, genetic algorithms, simulated annealing, and <b>neural</b> <b>nets),</b> dynamic <b>programming,</b> Lagrangean relaxation, even NP-completeness. Approximability was no exception: The TSP was shown early to be non-approximable in its general case, and the 3 = 2 approximation algorithm for the triangle inequality case due to Christofide [...] ...|$|R
40|$|Fifty {{years after}} the {{pioneering}} work of McCulloch and Pitts, the study of <b>neural</b> <b>nets</b> is alive and active. In this paper, I have discussed {{some of the work}} that is of current interest to me and my co-workers. I would, perhaps, be remiss if I failed to mention some of the current hype about <b>neural</b> <b>nets.</b> Can <b>neural</b> <b>nets</b> quickly solve NP-complete problems? No. A look at the proposed nets will show that {{the question of whether the}} net will converge, or where the net will converge to, are as difficult as the original NP-complete problem. This does not prevent the <b>neural</b> <b>net</b> from giving an approximate solution to a hard optimization problem, but no one has yet proven any approximation bounds. Hard problems are only hard in the worst case, so there may be many easy instances of a hard problem. Nothing prevents a <b>neural</b> <b>net</b> from solving these easy instances quickly. Can analog <b>neural</b> <b>nets</b> compute things not computable a Turing machine? Yes. But any analog device with infinite precision has more computational power than a Turing machine, so a <b>neural</b> <b>net</b> with unlimited precision should be a very powerful device. But practically all devices are constructed with limited precision, and these limited precision devices have no more power than a Turing machine. Can <b>neural</b> <b>nets</b> compute faster than other parallel models? No. <b>Neural</b> <b>nets</b> are in fact equivalent to the usual parallel models. The only difference that can occur is if the <b>neural</b> <b>net</b> has infinite precision which as mentioned above is highly unlikely. Does learning in <b>neural</b> <b>nets</b> make <b>programming</b> unnecessary? No. As we saw in the discussion of learning, learning rules must be devised, and it seems that different learning tasks will require different learning rules. Further, the kind of net to use for a particular task will be an important decision. In our decoding example, some network topologies did not lead to good decoders, while other topologies did. <b>Neural</b> <b>nets</b> will not replace programmers, but give programmers another paradigm in which to program. In spite of the hype, I believe that <b>neural</b> <b>nets</b> will be useful both as biological models and as programming paradigms. Finally, according to an often-told tale, there was a golden age of <b>neural</b> <b>nets</b> which suddenly ended in 1970. Depending on the version of the tale, the golden age ended because of the Vietnam war, or Minsky and Papert's book on perceptions, or cuts in funding, or the rise of artificial intelligence. But I hope that the reader of this paper and the rest of this volume will see that the death of Warren McCulloch had a most profound effect on the field. We miss him as a brilliant scientist, as a warm human being, and as the greatest story-teller of our age. " [...] Conclusion...|$|R
40|$|Abstract—The {{comprehensibility}} {{aspect of}} rule discovery is of emerging {{interest in the}} realm of knowledge discovery in databases. Of the many cognitive and psychological factors relating the comprehensibility of knowledge, we focus on the use of human amenable concepts as a representation language in expressing classification rules. Existing work in neural logic networks (or neulonets) provides impetus for our research; its strength lies in its ability to learn and represent complex human logic in decisionmaking using symbolic-interpretable net rules. A novel technique is developed for neulonet learning by composing net rules using genetic programming. Coupled with a sequential covering approach for generating a list of neulonets, the straightforward extraction of human-like logic rules from each neulonet provides an alternate perspective to the greater extent of knowledge that can potentially be expressed and discovered, while the entire list of neulonets together constitute an effective classifier. We show how the sequential covering approach is analogous to association-based classification, leading to the development of an associationbased neulonet classifier. Empirical study shows that associative classification integrated with the genetic construction of neulonets performs better than general association-based classifiers in terms of higher accuracies and smaller rule sets. This is due to the richness in logic expression inherent in the neulonet learning paradigm. Index Terms — Data mining, knowledge acquisition, connectionism and <b>neural</b> <b>nets,</b> genetic <b>programming,</b> rule-based knowledge representation. ...|$|R
40|$|This paper {{discusses}} {{within the}} framework of computational learning theory the current state of knowledge and some open problems in three areas of research about learning on feedforward neural nets: [...] <b>Neural</b> <b>nets</b> that learn from mistakes [...] Bounds for the Vapnik-Chervonenkis dimension of <b>neural</b> <b>nets</b> [...] Agnostic PAC-learning of functions on <b>neural</b> <b>nets.</b> All relevant definitions are given in this paper, and no previous knowledge about computational learning theory or <b>neural</b> <b>nets</b> is required. We refer to [RSO] for further introductory material and survey papers about the complexity of learning on <b>neural</b> <b>nets.</b> Throughout this paper we consider the following rather general notion of a (feedforward) <b>neural</b> <b>net...</b>|$|R
40|$|Credit scoring {{has been}} {{regarded}} as a core appraisal tool of banks {{during the last few}} decades, and has been widely investigated in the area of finance, in general, and banking sectors, in particular. In this thesis, the main aims and objectives are: to identify the currently used techniques in the Egyptian banking credit evaluation process; and to build credit scoring models to evaluate personal bank loans. In addition, the subsidiary aims are to evaluate the impact of sample proportion selection on the Predictive capability of both advanced scoring techniques and conventional scoring techniques, for both public banks and a private banking case-study; and to determine the key characteristics that affect the personal loans' quality (default risk). The stages of the research comprised: firstly, an investigative phase, including an early pilot study, structured interviews and a questionnaire; and secondly, an evaluative phase, including an analysis of two different data-sets from the Egyptian private and public banks applying average correct classification rates and estimated misclassification costs as criteria. Both advanced scoring techniques, namely, <b>neural</b> <b>nets</b> (probabilistic <b>neural</b> <b>nets</b> and multi-layer feed-forward <b>nets)</b> and genetic <b>programming,</b> and conventional techniques, namely, a weight of evidence measure, multiple discriminant analysis, probit analysis and logistic regression were used to evaluate credit default risk in Egyptian banks. In addition, an analysis of the data-sets using Kohonen maps was undertaken to provide additional visual insights into cluster groupings. From the investigative stage, it was found that all public {{and the vast majority of}} private banks in Egypt are using judgemental approaches in their credit evaluation. From the evaluative stage, clear distinctions between the conventional techniques and the advanced techniques were found for the private banking case-study; and the advanced scoring techniques (such as powerful <b>neural</b> <b>nets</b> and genetic <b>programming)</b> were superior to the conventional techniques for the public sector banks. Concurrent loans from other banks and guarantees by the corporate employer of the loan applicant, which have not been used in other reported studies, are identified as key variables and recommended in the specific environment chosen, namely Egypt. Other variables, such as a feasibility study and the Central Bank of Egypt report also play a contributory role in affecting the loan quality. EThOS - Electronic Theses Online ServiceEgyptian GovernmentGBUnited Kingdo...|$|R
40|$|Each spatial region {{viewed by}} an imaging {{spectrometer}} contains various {{elements in a}} mixture. The elements present {{and the amount of}} each are to be determined. A <b>neural</b> <b>net</b> solution is considered. Initial optical <b>neural</b> <b>net</b> hardware is described. The first simulations on the component requirements of a <b>neural</b> <b>net</b> are considered. The pseudoinverse solution is shown to not suffice, i. e. a <b>neural</b> <b>net</b> solution is required...|$|R
40|$|Information theory, {{especially}} Shannon capacity theorem, is used {{to relate}} the <b>neural</b> <b>net</b> capacity to channel capacity. Such an idea benefits the area of <b>neural</b> <b>nets</b> from a well-established area, namely information theory. As a practical example we demonstrate the effectiveness of error correcting codes to mitigate the imperfections of <b>neural</b> <b>nets.</b> IEE...|$|R
40|$|A brief {{introduction}} to the fundamental of <b>Neural</b> <b>Nets</b> is given, followed by two applications in structural optimization. In the first case, the feasibility of simulating with <b>neural</b> <b>nets</b> the many structural analyses performed during optimization iterations was studied. In the second case, the concept of using <b>neural</b> <b>nets</b> to capture design expertise was studied...|$|R
40|$|We give {{a simple}} neural network {{consisting}} of excitatory and inhibitory neural elements and study some fundamental properties with the <b>neural</b> <b>net.</b> We {{clean up the}} competitive process in the <b>neural</b> <b>net</b> {{and the effect of}} the hysterisis of both elements on it. We also give and explain some interesting behavior of responses which the <b>neural</b> <b>net</b> shows under static input stimuli...|$|R
40|$|It {{is shown}} that high order {{feedforward}} <b>neural</b> <b>nets</b> of constant depth with piecewise polynomial activation functions and arbitrary real weights can be simulated for boolean {{inputs and outputs}} by <b>neural</b> <b>nets</b> of a somewhat larger size and depth with heaviside gates and weights from fΓ 1; 0; 1 g. This provides the first known upper bound for the computational power of the former type of <b>neural</b> <b>nets.</b> It is also shown {{that in the case}} of first order nets with piecewise linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits, without changing the boolean function that is computed by the <b>neural</b> <b>net.</b> In order to prove these results we introduce two new methods for reducing nonlinear problems about weights in multi-layer <b>neural</b> <b>nets</b> to linear problems for a transformed set of parameters. These transformed parameters can be interpreted as weights in a somewhat larger <b>neural</b> <b>net.</b> As another application of our new proof technique we s [...] ...|$|R
50|$|Supervised neural networks, fuzzy <b>neural</b> <b>nets,</b> and {{combinations}} of <b>neural</b> <b>nets</b> and rules, have been extensively explored {{and used for}} detecting fraud in mobile phone networks and financial statement fraud.|$|R
50|$|Each {{individual}} makes {{decisions based}} on a <b>neural</b> <b>net</b> using Hebbian learning; the <b>neural</b> <b>net</b> is derived from each individual's genome. The genome does not merely specify the wiring of the <b>neural</b> <b>nets,</b> but also determines their size, speed, color, mutation rate {{and a number of}} other factors. The genome is randomly mutated at a set probability, which are also changed in descendant organisms.|$|R
40|$|Abstract—Traditionally, VLSI {{implementations}} of spiking <b>neural</b> <b>nets</b> have featured large neuron {{counts for}} fixed computations or small exploratory, configurable nets. This paper presents the system architecture {{of a large}} configurable <b>neural</b> <b>net</b> system employing a dedicated mapping algorithm for projecting the targeted biology-analog nets and dynamics onto the hardware with its attendant constraints. Keywords—large scale VLSI <b>neural</b> <b>net,</b> topology mapping, complex pulse communication. I...|$|R
40|$|Abstract: This study {{examines}} {{the value of}} utilizing <b>neural</b> <b>net</b> modeling for issues relating to optimization across a network of cities in space. <b>Neural</b> <b>nets</b> {{are made up of}} many nonlinear computational elements that operate in paral-lel and are arranged {{in a manner similar to}} biological <b>neural</b> <b>nets.</b> Defining a <b>neural</b> <b>net</b> model involves specifying a net topology, arrangement of nodes, training or learning rules, adjustment of weights associated with connections, node characteristics, and rules of transformation from input to output. All of these are the major issues in such regional problems as labor force migration and firm location. I...|$|R
40|$|A major {{challenge}} in performing pattern recognition with neural networks is large input data sets; for example, high-resolution static images. There {{is a direct}} relationship {{between the number of}} inputs and the number of neurons and links required to precess those inputs. Specifically, as the number of inputs increases linearly, the complexity of the <b>neural</b> <b>net</b> increases exponentially. We present a new approach to pattern recognition, where input data is "streamed" into a feedback <b>neural</b> <b>net.</b> This is done by distributing the input temporally, such that a portion of the inputs is used for each iteration of the <b>neural</b> <b>net.</b> Therefore, pattern recognition is automatically performed in conveniently sized segments with a single <b>neural</b> <b>net.</b> This reduces the amount of evolution that mus be performed to train the <b>neural</b> <b>net...</b>|$|R
50|$|Backpropagation Through Structure (BPTS) is a {{gradient-based}} {{technique for}} training recursive <b>neural</b> <b>nets</b> (a superset of recurrent <b>neural</b> <b>nets)</b> and is extensively {{described in a}} 1996 paper written by Christoph Goller and Andreas Küchler.|$|R
40|$|Principles of {{intelligent}} information technologies application, particularly artificial <b>neural</b> <b>nets,</b> {{for improvement of}} NPP diagnostics systems are discussed. Study results of using <b>neural</b> <b>net</b> technologies for leak diagnostics and gamma-spectra analysis are presented. ??????????? ???????? ?????????? ???????????????? ?????????????? ??????????, ? ????????? ????????????? ????????? ????? ??? ????????? ????????????? ??????????????? ?????? ???. ????????? ?????????? ???????????? ?????????? ???????????? ?????????? ??? ??????????? ????? ? ??????? ?????-????????...|$|R
40|$|Abstract. It {{is shown}} that {{high-order}} feedforward <b>neural</b> <b>nets</b> of constant depth with piecewise-polynomial activation functions and arbitrary real weights can be simulated for Boolean {{inputs and outputs}} by <b>neural</b> <b>nets</b> of a somewhat larger size and depth with Heaviside gates and weights from f− 1; 0; 1 g. This provides the rst known upper bound for the computational power of the former type of <b>neural</b> <b>nets.</b> It is also shown {{that in the case}} of rst-order nets with piecewise-linear activation functions one can replace arbitrary real weights by rational numbers with polynomially many bits without changing the Boolean function that is computed by the <b>neural</b> <b>net.</b> In order to prove these results, we introduce two new methods for reducing nonlinear problems about weights in multilayer <b>neural</b> <b>nets</b> to linear problems for a transformed set of parameters. These transformed parameters can be interpreted as weights in a somewhat larger <b>neural</b> <b>net.</b> As another application of our new proof technique we show that <b>neural</b> <b>nets</b> with piecewise-polynomial activation functions and a constant number of analog inputs are probably approximately correct (PAC) learnable (in Valiant’s model for PAC learning [Comm. Assoc. Comput. Mach., 27 (1984), pp. 1134 { 1142]). Key words. neural networks, analog computing, threshold circuits, circuit complexity, learning complexit...|$|R
40|$|Possibilities of {{incorporating}} <b>neural</b> <b>nets</b> in different tasks {{of a gas}} turbine performance diagnostic procedure are investigated. The purpose is to examine how <b>neural</b> <b>nets</b> can be implemented and what advantages they may offer. First, the possibility to constitute a performance model by using <b>neural</b> <b>nets</b> is considered. Different modes of operation are examined and the <b>neural</b> <b>net</b> architectures for achieving better accuracy are discussed. Subsequently, different problems of fault detection and identification are considered. Classification of faults is performed {{on the basis of}} diagnostic parameters produced by adaptive modelling. Both sensor faults and actual engine component faults are examined. A decision logic based on several <b>neural</b> <b>nets</b> is proposed. At a first level it is decided whether a fault exists, and if yes, checks are performed in order to identify the fault in as much detail as possible. Summarizing, the paper discusses different aspects of <b>neural</b> <b>net</b> implementation, in an effort to provide guidelines for application of this type of technique in the field of gas turbine diagnostics...|$|R
40|$|We {{consider}} the computational complexity of learning by <b>neural</b> <b>nets.</b> We {{are interested in}} {{how hard it is}} to design appropriate <b>neural</b> <b>net</b> architectures and to train <b>neural</b> <b>nets</b> for general and specialized learning tasks. Our main result shows that the training problem for 2 -cascade <b>neural</b> <b>nets</b> (which have only two non-input nodes, one of which is hidden) is NP-complete, which implies that finding an optimal net (in terms of the number of non-input units) that is consistent with a set of examples is also NP-complete. This result also demonstrates a surprising gap between the computational complexities of one-node (perceptron) and two-node <b>neural</b> <b>net</b> training problems, since the perceptron training problem can be solved in polynomial time by linear programming techniques. We conjecture that training a k-cascade <b>neural</b> <b>net,</b> which is a classical threshold network training problem, is also NP-complete, for each fixed k 2. We also show that the problem of finding an optimal perceptron (in terms of the number of non-zero weights) consistent with a set of training examples is NP-hard...|$|R
40|$|This paper {{discusses}} {{within the}} framework of computational learning theory the current state of knowledge and some open problems in three areas of research about learning on feedforward <b>neural</b> nets: <b>Neural</b> <b>nets</b> that learn from mistakes Bounds for the Vapnik-Chervonenkis dimension of <b>neural</b> <b>nets</b> Agnostic PAC-learning of functions on <b>neural</b> <b>nets.</b> All relevant denitions are given in this paper, and no previous knowledge about computational learning theory or <b>neural</b> <b>nets</b> is required. We refer to [RSO] for further introductory material and survey papers about the complexity of learning on <b>neural</b> <b>nets.</b> Throughout this paper we consider the following rather general notion of a (feed-forward) <b>neural</b> <b>net.</b> De nition 1. 1 A network architecture (or net") N is a labeled acyclic directed graph. Its nodes of fan-in 0 (input nodes"), as well as its nodes of fan-out 0 (output nodes") are labeled by natural numbers. A node g in N with fan-in r> 0 is called a computation node (or gate), and it is labeled by some activation function g: R! R, some polynomial...|$|R
40|$|ABSTRACT- Though the {{time-delay}} <b>neural</b> <b>net</b> architecture {{has been}} recently {{used in a}} number of speech recognition applications, it has the problem that it can not use longer temporal contexts because this increases the number of connection weights in the network. This is a serious bottleneck because the use of larger temporal contexts can improve the recognition performance. In this paper, a time-derivarive <b>neural</b> <b>net</b> architecture is proposed. This architecture has the advantage that it can utilize information about longer temporal contexts without increasing the number of connection weights in the network. This architecture is studied here for speaker-independent isolated-word recognition and its performance is compared with that of the time-delay <b>neural</b> <b>net</b> architecture. It is shown that the time-derivative <b>neural</b> <b>net</b> architecture, in spite of using less number of connection weights, outperforms the time-delay <b>neural</b> <b>net</b> architecture for speech recognition. 1...|$|R
30|$|The {{results of}} three ANN {{application}} (Auto MLp, <b>Neural</b> <b>Net</b> and Perceptron) showed <b>Neural</b> <b>Net</b> {{was the best}} and the most accuracte model when it agained applied on SVM dataset, while the worst performance belonged to Perceptron model; the accuracies of Auto MLp and Perceptron models were high and nearly at the same levels (86 % and 58 %) when they applied on Information Gain and SVM datasets. Generally the Kappa indexes were less accurate, the best index obtained from three models Auto MLp, <b>Neural</b> <b>Net</b> and Perceptron were respectively 77 %, 80 % and 26 %; therefore the best index gained from <b>Neural</b> <b>Net</b> model, too.|$|R
