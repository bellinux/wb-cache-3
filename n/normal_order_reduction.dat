12|4427|Public
25|$|Most purely {{functional}} programming languages (notably Miranda and its descendents, including Haskell), and the proof languages of theorem provers, use lazy evaluation, {{which is essentially}} the same as call by need. This is like <b>normal</b> <b>order</b> <b>reduction,</b> but call by need manages to avoid the duplication of work inherent in <b>normal</b> <b>order</b> <b>reduction</b> using sharing. In the example given above, (λx.xx) ((λx.x)y) reduces to ((λx.x)y) ((λx.x)y), which has two redexes, but in call by need they are represented using the same object rather than copied, so when one is reduced the other is too.|$|E
40|$|We {{present a}} higher-order call-by-need lambda {{calculus}} enriched with constructors, case-expressions, recursive letrec-expressions, a seq-operator for sequential evaluation and a non-deterministic operator amb that is locally bottom-avoiding. We use a small-step operational semantics {{in form of}} a single-step rewriting system that defines a (nondeterministic) <b>normal</b> <b>order</b> <b>reduction.</b> This strategy can be made fair by adding resources for bookkeeping. As equational theory we use contextual equivalence, i. e. terms are equal if plugged into any program context their termination behaviour is the same, where we {{use a combination of}} may- as well as must-convergence, which is appropriate for non-deterministic computations. We show that we can drop the fairness condition for equational reasoning, since the valid equations w. r. t. <b>normal</b> <b>order</b> <b>reduction</b> are the same as for fair <b>normal</b> <b>order</b> <b>reduction.</b> We evolve different proof tools for proving correctness of program transformations, in particular, a context lemma for may- as well as mustconvergenc...|$|E
40|$|We derive by program {{transformation}} Pierre Crégut s full-reducing Krivine machine KN {{from the}} structural operational semantics of the <b>normal</b> <b>order</b> <b>reduction</b> strategy in a closure-converted pure lambda calculus. We thus establish the {{correspondence between the}} strategy and the machine, and showcase our technique for deriving full-reducing abstract machines. Actually, the machine we obtain is a slightly optimised version that can work with open terms and {{may be used in}} implementations of proof assistants...|$|E
25|$|Call by name: As <b>normal</b> <b>order,</b> but no <b>reductions</b> are {{performed}} inside abstractions. For example, λx.(λx.x)x is in normal form {{according to this}} strategy, although it contains the redex (λx.x)x.|$|R
40|$|The {{concepts}} of admissible orderings and normal form algorithm are basic in Buchberger's Gröbner basis algorithm. We present a constructive and elementary proof of Robbiano's characterization theorem for admissible orderings. Using this characterization, we give a bound on {{the complexity of}} the normal form algorithm for arbitrary admissible orderings. Using a simple refinement of the <b>normal</b> form algorithm (<b>ordered</b> <b>reductions),</b> we obtain significantly improved bounds...|$|R
40|$|This paper proves {{correctness}} of Nocker s {{method of}} strictness analysis, implemented for Clean, {{which is an}} e ective way for strictness analysis in lazy functional languages based on their operational semantics. We improve upon the work of Clark, Hankin and Hunt, which addresses correctness of the abstract reduction rules. Our method also addresses the cycle detection rules, which are the main strength of Nocker s strictness analysis. We reformulate Nocker s strictness analysis algorithm in a higherorder lambda-calculus with case, constructors, letrec, and a nondeterministic choice operator used as a union operator. Furthermore, the calculus is expressive enough to represent abstract constants like Top or Inf. The operational semantics is a small-step semantics and equality of expressions is defined by a contextual semantics that observes termination of expressions. The correctness of several reductions is proved using a context lemma and complete sets of forking and commuting diagrams. The proof is based mainly on an exact analysis of the lengths of <b>normal</b> <b>order</b> <b>reductions.</b> However, there remains a small gap: Currently, the proof for correctness of strictness analysis requires the conjecture that our behavioral preorder is contained in the contextual preorder. The proof is valid without referring to the conjecture, if no abstract constants {{are used in the}} analysis...|$|R
40|$|In {{this thesis}} we {{investigate}} the semantics of concurrent programming languages {{with respect to}} the correctness of program transformations which occur during compilation of programs. We investigate the correctness of transformations on the same language, as well as the correctness of transformations from a higher-level language into a lower-level language. As a semantic model of a concurrent program language we introduce the calculus ¤let amb which is an untyped higher-order lambda calculus extended by data constructors and case-expressions, explicit sharing and recursion via letrec-expressions, seq-expressions for modeling sequential computations, and the nondeterministic operator amb. The binary operator amb is locally bottom-avoiding, that is if applied to two arguments s and t, the operator chooses one of both arguments, but must not choose a divergent argument unless both arguments s and t are divergent. An analysis of such a bottom-avoiding operator is interesting for several reasons. Its implementation requires concurrent (that is interleaved) as well as fair evaluation of both arguments, and hence a calculus with an amb-operator models essential properties of modern concurrent functional programming languages. Moreover, the amb operator is theoretically challenging, since usual properties do not hold if this operator is included. For example a divergent expression is no longer a least element when using a contextual preorder as semantics. From a practical point of view the combination of an amb-operator with an enriched lambda-calculus enables us to implement a lot of other nondeterministic operators, for example erratic choice, demonic choice, parallel-or, a parallel convergence tester, and most importantly a bottom-avoiding nondeterministic merge-operator, which has several applications for the implementation of event driven systems, like graphical user interfaces and operating systems. A further motivation for adding an amb-operator to a programming language is that it allows us to express some algorithms in a more declarative way, for example parallel tree search algorithms. To provide a formal semantics of the calculus ¤let amb as a first step the operational semantics of the calculus ¤let amb is defined by a call-by-need small-step reduction semantics, called <b>normal</b> <b>order</b> <b>reduction,</b> which implements a lazy reduction strategy for concurrent computations: it evaluates the arguments of amb-expressions in an interleaved manner and respects sharing such that multiple evaluations of the same expression are avoided. An amb-expression is finally evaluated by using the (amb) -reduction, which is essentially defined by two rewriting rules: amb v s ! v amb s v ! v where v is a value, that is an abstraction or a (fully saturated) data constructor. The rules show that if one argument of the amb-expression is a value, then it may be chosen {{as the result of the}} whole amb-expression. The (amb) -reduction is nondeterministic, since <b>normal</b> <b>order</b> <b>reduction</b> may choose the first or the second argument if both arguments are values. If only one argument is a value, then <b>normal</b> <b>order</b> <b>reduction</b> does not need to perform the corresponding (amb) -reduction. It may also reduce the other argument of the amb-expression. Based on the operational semantics we define may- and mustconvergence for expressions, which are predicates that classify expressions by observing their termination and nontermination behavior. An expression s is may-convergent (denoted with s#) if s is reducible using <b>normal</b> <b>order</b> <b>reduction</b> steps to a weak head normal form. The expression s is must-convergent (denoted with s+) if may-convergence holds for every of its successors with respect to normal order evaluation. The converse of may-convergence is must-divergence and the converse of must-convergence is may-divergence. For implementing the desired locally bottom-avoiding behavior of the amb-operator fairness of the evaluator is required, since otherwise a successful computation may be ignored and instead a diverging computation may be performed. Since <b>normal</b> <b>order</b> <b>reduction</b> itself does not guarantee fairness, a fair <b>normal</b> <b>order</b> <b>reduction</b> is provided, which uses a scheduling mechanism with resources for book-keeping. This reduction strategy ensures fairness. A main result is that the convergence predicates taking fairness into account are equivalent to the predicates defined on <b>normal</b> <b>order</b> <b>reduction.</b> This implies that the semantics based on fairness is indistinguishable from the semantics based on <b>normal</b> <b>order</b> <b>reduction.</b> As a consequence reasoning involving reduction can be based on <b>normal</b> <b>order</b> <b>reduction</b> and holds for fair <b>normal</b> <b>order</b> <b>reduction,</b> too. This property considerably simplifies proofs. As equational theory we use contextual equivalence, i. e. expressions are considered as equivalent if, and only if the same may-convergence as well as the same must-convergence behavior can be observed when the expressions are plugged into an arbitrary program context. This approach is widely accepted for sequential programming languages. It is also useful for process calculi and nondeterministic lambda calculi, since it provides a coarse program equivalence which leads to a maximal set of equations, but also distinguishes observably different expressions. The use of a combination of a may-convergence and a must-convergence is well known for nondeterministic calculi, but it is also known for the construction of power-domains as domain-theoretic models in denotational semantics. Especially, for an amb-operator observing may-convergence only is too weak, since in this case the operator amb cannot be distinguished from erratic choice, hence bottom-avoidance would be meaningless. A novelty of our approach is our definition of must-convergence which differs from former definitions, since the existence of an infinite evaluation is not a sufficient criterion to disprove must-convergence. This means, that for an expression s disproving must-convergence (and thus showing may-divergence) requires to show that s reduces using normal order reductions to an expression which cannot converge (i. e., is must-divergent). Technically, contextual equivalence »c is the symmetrization of the contextual preorder ·c which itself is the intersection of the contextual may-preorder ·# c and the contextual must-preorder ·+ c, that is if C ranges over contexts and s, t are expressions, then s ·# c t if, and only if 8 C : C[s]# =) C[t]# s ·+ c t if, and only if 8 C : C[s]+ =) C[t]+ s ·c t if, and only if s ·# c t and s ·+ c t s »c t if, and only if s ·c t and t ·c s. Using the equational theory we are interested in proving correctness of program transformations, that is proving that binary relations on expressions preserve contextual equivalence. The application area of correct program transformations has a wide range, for example those transformations can be used as optimizations during the compilation of programs, they can be used in automated theorem proving during verification of programs, and they can be used in program improvement like code refactoring. Proving contextual equivalence is rather difficult, since all program contexts need to be taken into account. Hence a main part of this thesis is to develop different proof techniques and proof tools for showing correctness of program transformations. We prove a context lemma for may- and must convergence, which shows that contextual equivalence of expressions can be derived by taking into account a smaller set of contexts – the reduction contexts – instead of all contexts. Thus, the context lemma obviously simplifies correctness proofs. We introduce and use the formalism of complete sets of commuting diagrams as well as complete sets of forking diagrams, which allow us to analyze and represent the overlappings between normal order reductions and transformation steps. We present a framework of generic properties of a program transformation and show that the validity of these properties ensures correctness of the transformation. Using these tools we prove that the deterministic reduction rules of the calculus remain correct program transformations even if their applicability is extended to arbitrary contexts. This result implies that partial evaluation is correct, and thus can be used for the compilation process of programs. We also prove correctness of some other commonly used optimizations including garbage collection which removes unused letrec bindings (which is also known as dead code removal), copying of variables which is also known as removing indirections, and copying of constructor applications and copying of letrec-bindings that are used only once, which are special forms of inlining. For proving further program equivalences and also for analyzing the contextual ordering more proof tools are developed. An important result is the Standardization Theorem for <b>normal</b> <b>order</b> <b>reduction.</b> As a first part this theorem shows that for every sequence of correct program transformations and (amb) -reductions ending in a weak head normal form there exists a <b>normal</b> <b>order</b> <b>reduction</b> to a weak head normal form. The second part of the theorem states that for every sequence of those correct program transformations and restricted (amb) -reductions ending in a must-divergent expression, there exists a <b>normal</b> <b>order</b> <b>reduction</b> ending in a must-divergent expression, too. A consequence of the Standardization Theorem is that both mustconvergence and must-divergence are preserved by correct program transformations as well as by restricted (amb) -reductions. Since fair <b>normal</b> <b>order</b> <b>reduction</b> introduces equivalent convergence predicates as <b>normal</b> <b>order</b> <b>reduction,</b> we transfer the claims of the Standardization Theorem to fair reductions, too. The Standardization Theorem eases proofs of contextual equivalence, and moreover it is indispensable for the proof of further properties of the calculus ¤let amb and allows us to provide further proof techniques. An ­-term is a must-divergent expression which remains mustdivergent when plugged into an arbitrary letrec-environment. We show that all ­-terms are in a single equivalence class with respect to contextual equivalence, that ­-terms can be inlined without changing the contextual equality, and we show that ­-terms are least {{with respect to the}} may-preorder ·# c. We provide another proof technique which is a weak form of finite simulation and we show correctness of this method. The technique is based on complete successor sets with respect to <b>normal</b> <b>order</b> <b>reduction,</b> that are sets of outcomes of <b>normal</b> <b>order</b> <b>reduction,</b> which are cut at a finite number of reductions. We define a notion of contextual preorder and contextual equivalence for finite sets of expressions based on the notions of contextual equivalence for expressions. Using the combination of complete successor sets and the contextual set equivalence we show that contextual set equivalence of two complete successor sets of two closed expressions implies contextual equivalence of the original expressions. This fact allows easy proofs of some nontrivial equivalences, for example that associativity of the erratic choice encoding holds, that is choice r (choice s t) »c choice (choice r s) t for all closed expressions s, t and r. We analyze open expressions and show that contextual equivalence holds if, and only if the same may- and must-convergence behavior is observed in all closing contexts only. We investigate the contextual preorder and the contextual equivalence and show as main result that contextual equivalence holds if must-convergence in closing contexts is observed only, that is s »c t if, and only if 8 C such that C[s] and C[t] are closed expressions: C[s]+ () C[t]+. Equipped with the Standardization Theorem and the set based proof method we show correctness of some important algebraic laws of the amb-operator and of some of the encodings of the other nondeterministic operators like erratic choice and parallel-or. Besides associativity and commutativity of these operators we show the bottom-avoidance law of amb, that is amb ­ s »c s »c amb s ­ for all expressions s, and an ­-term ­. As a final step we provide a further operational semantics by introducing a concurrent abstract machine semantics for the evaluation of expressions of ¤let amb. This machine is based on a push/enter model, uses multiple threads, and a global heap for sharing expressions between threads. A single thread performs deterministic evaluation of an expression using a stack to store intermediate results. The threads are ordered hierarchically using process trees. We show correctness of this machine, i. e. the convergence predicates defined for the machine and the convergence predicates defined by <b>normal</b> <b>order</b> <b>reduction</b> coincide. For the correctness proofs of the abstract machine an intermediate program calculus ¤let,vao amb is introduced. This calculus has a slightly restricted syntax with respect to ¤let amb, such that only variables are allowed as arguments of applications and constructor applications. The operational semantics of the calculus ¤let,vao amb is defined by a small-step reduction relation, which is called machine order reduction. This reduction strategy is closer to machine transitions than <b>normal</b> <b>order</b> <b>reduction</b> for ¤let amb. Nevertheless, we show that the equational theories for both calculi ¤let amb and ¤let,vao amb are equivalent with respect to a translation from ¤let amb into ¤let,vao amb. This result allows us to transfer all proved equivalences from ¤let amb into ¤let,vao amb. Finally we specify a fair concurrent abstract machine using annotated process trees, such that resources as annotations ensure fairness. Again we show correctness of this machine and we prove that the machine indeed ensures fairness, that is the machine does not ignore an applicable machine transition infinitely often, and thus every transition is performed (or discarded) after a finite sequence of machine transitions. The fair abstract machine semantics allows us to implement an evaluator for ¤let amb easily...|$|E
40|$|Abstract. We {{present a}} higher-order call-by-need lambda {{calculus}} enriched with constructors, case-expressions, recursive letrec-expressions, a seq-operator for sequential evaluation and a non-deterministic operator amb, which is locally bottom-avoiding. We use a small-step operational semantics {{in form of}} a <b>normal</b> <b>order</b> <b>reduction.</b> As equational theory we use contextual equivalence, i. e. terms are equal if plugged into an arbitrary program context their termination behaviour is the same. We {{use a combination of}} may- as well as must-convergence, which is appropriate for non-deterministic computations. We evolve different proof tools for proving correctness of program transformations. We provide a context lemma for may- as well as must- convergence which restricts the number of contexts that need to be examined for proving contextual equivalence. In combination with so-called complete sets of commuting and forking diagrams we show that all the deterministic reduction rules and also some additional transformations keep contextual equivalence. In contrast t...|$|E
40|$|The {{study of}} {{self-replicating}} structures in Computer Science {{has been taking}} place {{for more than half}} a century, motivated by the desire to understand the fundamental principles and algorithms involved in self-replication. The bulk of the literature explores self-replicating forms in Cellular Automata. Though trivially self-replicating programs have been written for dozens of languages, very little work exists that explores self-replicating forms in programming languages. This paper reports initial investigations into selfreplicating expressions in the Lambda Calculus, the basis for functional programming languages. Mimicking results from the work on Cellular Automata, selfreplicating Lambda Calculus expressions that also allow the application of an arbitrary program to arbitrary data are presented. Standard <b>normal</b> <b>order</b> <b>reduction,</b> however, will not reduce the sub-expression representing the program application. Two approaches of dealing with this, hybrid reduction and parallel reduction, are discussed, and have been implemented in an interpreter. Keywords: Self-replicating programs, Lambda Calculus, Functional Programming, Cellular Automat...|$|E
40|$|AbstractWe {{present a}} {{systematic}} construction of environment-based abstract machines from context-sensitive calculi of explicit substitutions, and we illustrate it with ten calculi and machines for applicative order with an abort operation, <b>normal</b> <b>order</b> with generalized <b>reduction</b> and call/cc, the lambda-mu-calculus, delimited continuations, stack inspection, proper tail-recursion, and lazy evaluation. Most {{of the machines}} already exist {{but they have been}} obtained independently and are only indirectly related to the corresponding calculi. All of the calculi are new and they make it possible directly to reason about the execution of the corresponding machines...|$|R
50|$|Note {{that the}} <b>normal</b> <b>ordering</b> {{is a concept}} that only makes sense for {{products}} of operators. Attempting to apply <b>normal</b> <b>ordering</b> to a sum of operators is not useful as <b>normal</b> <b>ordering</b> is not a linear operation.|$|R
50|$|In {{symbolic}} model checking, partial <b>order</b> <b>reduction</b> can {{be achieved}} by adding more constraints (guard strengthening).Further applications of partial <b>order</b> <b>reduction</b> involve automated planning.|$|R
40|$|We {{present a}} higher-order call-by-need lambda {{calculus}} enriched with constructors, case-expressions, recursive letrec-expressions, a seq-operator for sequential evaluation and a non-deterministic operator amb, which is locally bottom-avoiding. We use a small-step operational semantics {{in form of}} a <b>normal</b> <b>order</b> <b>reduction.</b> As equational theory we use contextual equivalence, i. e. terms are equal if plugged into an arbitrary program context their termination behaviour is the same. We {{use a combination of}} may- as well as must-convergence, which is appropriate for non-deterministic computations. We evolve different proof tools for proving correctness of program transformations. We provide a context lemma for may- as well as must- convergence which restricts the number of contexts that need to be examined for proving contextual equivalence. In combination with so-called complete sets of commuting and forking diagrams we show that all the deterministic reduction rules and also some additional transformations keep contextual equivalence. In contrast to other approaches our syntax as well as semantics does not make use of a heap for sharing expressions. Instead we represent these expressions explicitely via letrec-bindings...|$|E
40|$|In {{this paper}} {{we present a}} {{non-deterministic}} call-by-need (untyped) lambda calculus lambda nd with a constant choice and a let-syntax that models sharing. Our main result is that lambda nd has the nice operational properties of the standard lambda calculus: confluence on sets of expressions, and <b>normal</b> <b>order</b> <b>reduction</b> is sufficient to reach head normal form. Using a strong contextual equivalence we show correctness of several program transformations. In particular of lambdalifting using deterministic maximal free expressions. These results show that lambda nd is a new and also natural combination of non-determinism and lambda-calculus, which {{has a lot of}} opportunities for parallel evaluation. An intended application of lambda nd is as a foundation for compiling lazy functional programming languages with I/O based on direct calls. The set of correct program transformations can be rigorously distinguished from non-correct ones. All program transformations are permitted with the slight exception that for transformations like common subexpression elimination and lambda-lifting with maximal free expressions the involved subexpressions have to be deterministic ones...|$|E
40|$|In {{the first}} part of this talk, I will review the thought that led to the G-machine. In the second part, I will {{describe}} some recent work on formalising the `avoiding graph reduction' bit, by doing fold/unfold transformation on a basic inefficient graph constructing interpreter. 1 Introduction In this part of the talk, we review the thoughts that led to our work on compiled graph reduction, and the development of the G-machine. The basic implementation problem, for any language, is how to implement efficiently the computation rules implied by the operational semantics of the language. With functional languages, being based on the -calculus, the focus is on the fi-reduction rule which performs substitution. Expressed in an operational semantics of <b>normal</b> <b>order</b> <b>reduction,</b> the evaluation of an application can be stated as follows: E 1 ! v:e E 1 E 2 ! e[E 2 =v] That is, if E 1 reduces to :e, in zero or more reduction steps, the application E 1 E 2 reduces to the expression e with E [...] ...|$|E
40|$|If {{synchronizing}} (rendez-vous) communications {{are used}} in the Promela models, the unless construct and the weak fairness algorithm are not compatible with the partial <b>order</b> <b>reduction</b> algorithm used in Spin’s verifier. After identifying the wrong partial <b>order</b> <b>reduction</b> pattern that causes the incompatibility, we give solutions for these two problems. To this end we propose corrections in the identification of the safe statements for partial <b>order</b> <b>reduction</b> and as an alternative, we discuss corrections of the partial <b>order</b> <b>reduction</b> algorithm...|$|R
40|$|Partial <b>order</b> <b>reduction</b> is a {{state space}} pruning {{approach}} that has been originally introduced in computer aided verification. Recently, various partial <b>order</b> <b>reduction</b> techniques have also been proposed for planning. Despite very similar underlying ideas, the relevant literature from computer aided verification has hardly been analyzed in the planning area so far, and {{it is unclear how}} these techniques are formally related. We provide an analysis of existing partial <b>order</b> <b>reduction</b> techniques and their relationships. We show that recently proposed approaches in planning are instances of general partial <b>order</b> <b>reduction</b> approaches from computer aided verification. Our analysis reveals a hierarchy of dominance relationships and shows that there is still room for improvement for partial <b>order</b> <b>reduction</b> techniques in planning. Overall, we provide a first step towards a better understanding and a unifying theory of partial <b>order</b> <b>reduction</b> techniques from different areas...|$|R
40|$|Abstract—We {{present a}} new model <b>order</b> <b>reduction</b> {{technique}} for electrically large systems with delay elements, which can be modeled by means of neutral delayed differential equations. An adaptive multipoint expansion and model <b>order</b> <b>reduction</b> of equivalent first order systems are combined in the new proposed method that preserves the neutral delayed differential formulation. An adaptive algorithm to select the expansion points is presented. The proposed model <b>order</b> <b>reduction</b> technique is validated by pertinent numerical results. A comparison with a previous model <b>order</b> <b>reduction</b> algorithm based on a single point expansion is performed to show the considerably improved modeling capability of the new proposed technique. Index Terms—Delayed Partial Element Equivalent Circuit method, model <b>order</b> <b>reduction,</b> neutral delayed differential equations. I...|$|R
40|$|With sound unification, Definite Clause Grammars {{and compact}} {{expression}} of combinatorial generation algorithms, logic programming {{is shown to}} conveniently host a declarative playground where interesting properties and behaviors emerge from the interaction of heterogenous but deeply connected computational objects. Compact combinatorial generation algorithms are given for several families of lambda terms, including open, closed, simply typed and linear terms as well as type inference and <b>normal</b> <b>order</b> <b>reduction</b> algorithms. We describe a Prolog-based combined lambda term generator and type-inferrer for closed well-typed terms of a given size, in de Bruijn notation. We introduce a compressed de Bruijn representation of lambda terms and define its bijections to standard representations. Our compressed terms facilitate derivation of size-proportionate ranking and unranking algorithms of lambda terms and their inferred simple types. The S and K combinator expressions form a well-known Turing-complete subset of the lambda calculus. We specify evaluation, type inference and combinatorial generation algorithms for SK-combinator trees. In the process, we unravel properties shedding new light on interesting aspects of their structure and distribution. A uniform representation, as binary trees with empty leaves, is given to expressions built with Rosser's X-combinator, natural numbers, lambda terms and simple types. Using this shared representation, ranking/unranking algorithm of lambda terms to tree-based natural numbers are described. Our algorithms, expressed as an incrementally developed literate Prolog program, implement a declarative playground for exploration of representations, encodings and computations with uniformly represented lambda terms, types, combinators and tree-based arithmetic. Comment: 70 page...|$|E
40|$|This paper {{proposes a}} {{non-standard}} way to combine lazy functional languages with I/O. In order {{to demonstrate the}} usefulness of the approach, a tiny lazy functional core language “FUNDIO”, which is also a call-by-need lambda calculus, is investigated. The syntax of “FUNDIO ” has case, letrec, constructors and an IO-interface: its operational semantics is described by small-step reductions. A contextual approximation and equivalence depending on the input-output behavior of <b>normal</b> <b>order</b> <b>reduction</b> sequences is defined and a context lemma is proved. This enables to study a semantics of “FUNDIO ” and its semantic properties. The paper demonstrates that the technique of complete reduction diagrams enables to show a considerable set of program transformations to be correct. Several optimizations of evaluation are given, including strictness optimizations and an abstract machine, and shown to be correct w. r. t. contextual equivalence. Correctness of strictness optimizations also justifies correctness of parallel evaluation. Thus this calculus has a potential to integrate non-strict functional programming with a non-deterministic approach to input-output and also to provide a useful semantics for this combination. It is argued that monadic IO and unsafePerformIO can be combined in Haskell, and that the result is reliable, if all reductions and transformations are correct w. r. t. to the FUNDIO-semantics. Of course, we do not address the typing problems the are involved in the usage of Haskell’s unsafePerformIO. The semantics can also be used as a novel semantics for strict functional languages with IO, where the sequence of IOs is not fixed...|$|E
5000|$|... 2. A more {{interesting}} {{example is the}} <b>normal</b> <b>ordering</b> of : Here the <b>normal</b> <b>ordering</b> operation has reordered the terms by placing [...] {{to the left of}} [...]|$|R
50|$|Note {{that this}} {{discussion}} {{is in terms of}} the usual definition of <b>normal</b> <b>ordering</b> which is appropriate for the vacuum expectation values (VEV's) of fields. (Wick's theorem provides as a way of expressing VEV's of n fields in terms of VEV's of two fields.) There are any other possible definitions of <b>normal</b> <b>ordering,</b> and Wick's theorem is valid irrespective. However Wick's theorem only simplifies computations if the definition of <b>normal</b> <b>ordering</b> used is changed to match the type of expectation value wanted. That is we always want the expectation value of the <b>normal</b> <b>ordered</b> product to be zero. For instance inthermal field theory a different type of expectation value, a thermal trace over the density matrix, requires a different definition of <b>normal</b> <b>ordering.</b>|$|R
40|$|This paper {{addresses}} the <b>order</b> <b>reduction</b> of IC conducted emission models. These models are mathematically described by singular matrices of high order. Usually {{the order of}} the matrices can be reduced by the multipoint Krylov based <b>order</b> <b>reduction.</b> To achieve an even lower order we introduce a two step <b>order</b> <b>reduction.</b> The first step is the <b>order</b> <b>reduction</b> method based on the multipoint approximation of system matrices by utilizing the Krylov subspace. The second step is based on the rejection of the dispensable part of a system. To recognize the remaining dispensable system part Lyapunov equations are used. Thus this paper introduces the efficient solutions of the Lyapunov equations. With two-step <b>order</b> <b>reduction</b> a high speed-up in frequency analysis can be achieved...|$|R
40|$|A {{message passing}} {{multiprocessor}} model for computation based on functional languages has been suggested. The model follows the applicative (eager) <b>order</b> of <b>reduction,</b> giving it {{an edge in}} exploiting parallelism over those following <b>normal</b> <b>order.</b> However, the applicative strategy is prone to being unsafe and is incapable of handling recursion. A concept of Partial task has been introduced whose sluggishness in reduction is utilised to achieve a controlled version of recursion. The model has certain other features which make it selectively lazy so that the reduction {{tends to be more}} need-based and hence more safe...|$|R
40|$|We {{study the}} {{features}} of the vacuum of the harmonic oscillator in the Moyal quantization. The vacuums with and without using the <b>normal</b> <b>ordering</b> look different. The vacuum without the <b>normal</b> <b>ordering</b> is shown to be expressed using the Weyl ordering. The Weyl ordered vacuum is then compared with the <b>normal</b> <b>ordered</b> vacuum, and the implication of the difference between them is discussed. Comment: 13 page...|$|R
40|$|This work reports new {{approaches}} for <b>order</b> <b>reduction</b> of nonlinear systems with time periodic coefficients. First, the {{equations of motion}} are transformed using the Lyapunov-Floquet (LF) transformation, which makes the linear part of new set of equations time invariant. At this point, either linear or nonlinear <b>order</b> <b>reduction</b> methodologies can be applied. The linear <b>order</b> <b>reduction</b> technique is based on classical technique of aggregation and nonlinear technique is based on ‘Time periodic invariant manifold theory’. These methods do not assume the parametric excitation term to be small. The nonlinear <b>order</b> <b>reduction</b> technique yields superior results. An example of {{two degrees of freedom}} system representing a magnetic bearing is included to show the practical implementation of these methods. The conditions when <b>order</b> <b>reduction</b> is not possible are also discussed...|$|R
40|$|We {{propose a}} novel model <b>order</b> <b>reduction</b> {{technique}} that {{is able to}} accurately reduce electrically large systems with delay elements, which can be described by means of neutral delayed differential equations. It {{is based on an}} adaptive multipoint expansion and model <b>order</b> <b>reduction</b> of equivalent first order systems. The neutral delayed differential formulation is preserved in the reduced model. Pertinent numerical results validate the proposed model <b>order</b> <b>reduction</b> approach...|$|R
5000|$|The second {{notational}} system labels sets {{in terms}} of their normal form, which depends on the concept of <b>normal</b> <b>order.</b> To put a set in <b>normal</b> <b>order,</b> order it as an ascending scale in pitch-class space that spans less than an octave. Then permute it cyclically until its first and last notes are as close together as possible. In the case of ties, minimize the distance between the first and next-to-last note. (In case of ties here, minimize the distance between the first and next-to-next-to-last note, and so on.) Thus {0, 7, 4} in <b>normal</b> <b>order</b> is {0, 4, 7}, while {0, 2, 10} in <b>normal</b> <b>order</b> is {10, 0, 2}. To put a set in normal form, begin by putting it in <b>normal</b> <b>order,</b> and then transpose it so that its first pitch class is 0 [...] Mathematicians and computer scientists most often order combinations using either alphabetical ordering, binary (base two) ordering, or Gray coding, each of which lead to differing but logical normal forms.|$|R
40|$|Model <b>order</b> <b>reduction</b> {{appears to}} be {{beneficial}} for the synthesis and simulation of compliant mechanisms due to computational costs. Model <b>order</b> <b>reduction</b> is an established method in many technical fields for the approximation of large-scale linear time-invariant dynamical systems described by ordinary differential equations. Based on system theory, underlying representations of the dynamical system are introduced from which the general reduced order model is derived by projection. During the last years, numerous new procedures were published and investigated appropriate to simulation, optimization and control. Singular value decomposition, condensation-based and Krylov subspace methods representing three <b>order</b> <b>reduction</b> methods are reviewed and their advantages and disadvantages are outlined in this paper. The convenience of applying model <b>order</b> <b>reduction</b> in compliant mechanisms is quoted. Moreover, the requested attributes for <b>order</b> <b>reduction</b> as a future research direction meeting the characteristics of compliant mechanisms are commented...|$|R
40|$|Abstract—In this paper, a {{new model}} <b>order</b> <b>reduction</b> {{phenomenon}} is introduced {{at the design stage}} of linear phase digital IIR filter. The complexity of a system can be reduced by adopting the model <b>order</b> <b>reduction</b> method in their design. In this paper a mixed method of model <b>order</b> <b>reduction</b> is proposed for linear IIR filter. The proposed method employs the advantages of factor division technique to derive the reduced order denominator polynomial and the reduced order numerator is obtained based on the resultant denominator polynomial. The <b>order</b> <b>reduction</b> technique is used to reduce the delay units at the design stage of IIR filter. The validity of the proposed method is illustrated with design example in frequency domain and stability is also examined with help of nyquist plot. Keywords—Error index (J), Factor division method, IIR filter, Nyquist plot, <b>Order</b> <b>reduction.</b> ...|$|R
40|$|We {{present a}} new model <b>order</b> <b>reduction</b> {{technique}} for electrically large systems with delay elements, which can be modeled by means of neutral delayed differential equations. An adaptive multipoint expansion and model <b>order</b> <b>reduction</b> of equivalent first order systems are combined in the new proposed method that preserves the neutral delayed differential formulation. An adaptive algorithm to select the expansion points is presented. The proposed model <b>order</b> <b>reduction</b> technique is validated by pertinent numerical results. A comparison with a previous model <b>order</b> <b>reduction</b> algorithm based on a single point expansion is performed to show the considerably improved modeling capability of the new proposed technique...|$|R
40|$|AbstractIn the past, partial <b>order</b> <b>reduction</b> {{has been}} used {{successfully}} to combat the state explosion problem {{in the context of}} model checking for non-probabilistic systems. For both linear time and branching time specifications, methods have been developed to apply partial <b>order</b> <b>reduction</b> in the context of model checking. Only recently, results were published that give criteria on applying partial <b>order</b> <b>reduction</b> for verifying quantitative linear time properties for probabilistic systems. This paper presents partial <b>order</b> <b>reduction</b> criteria for Markov decision processes and branching time properties, such as formulas of probabilistic computation tree logic. Moreover, we provide a comparison of the results established so far about reduction conditions for Markov decision processes...|$|R
40|$|In {{this paper}} we explore partial <b>order</b> <b>reduction</b> that make the task of verifying {{cryptographic}} protocols more efficient. These reduction techniques have been implemented in our tool BRUTUS. Although we have implemented several reduction techniques in our tool BRUTUS, due to space restrictions {{in this paper we}} only focus on partial <b>order</b> <b>reductions.</b> Partial <b>order</b> <b>reductions</b> have proved very useful in the domain of model checking reactive systems. These reductions are not directly applicable in our context because of additional complications caused by tracking knowledge of various agents. We present partial <b>order</b> <b>reductions</b> in the context of verifying security protocols and prove their correctness. Experimental results showing the benefits of this reduction technique are also presented. Keywords: Model checking, partial <b>order</b> <b>reductions,</b> and security. 1 Introduction Due to the rapid growth of such entities as "the Internet" and "the World Wide Web", computer security has r [...] ...|$|R
50|$|We can use contractions and <b>normal</b> <b>ordering</b> {{to express}} any product of {{creation}} and annihilation operators as a sum of <b>normal</b> <b>ordered</b> terms. This {{is the basis of}} Wick's theorem. Before stating the theorem fully we shall look at some examples.|$|R
40|$|Abstract. In {{this article}} {{combinatorial}} aspects of <b>normal</b> <b>ordering</b> annihilation and creation operators of a multi-mode boson system are discussed. The modes {{are assumed to}} be coupled since otherwise the problem of <b>normal</b> <b>ordering</b> is reduced to the corresponding problem of the single-mode case. To describe the <b>normal</b> <b>ordering</b> in the multi-mode case for each mode a colour is introduced and coloured contractions are considered. A depiction for coloured contractions via coloured linear representations is given. In analogy to the single-mode case associated coloured Stirling numbers are defined as coefficients appearing in the process of <b>normal</b> <b>ordering</b> powers of the number operators. Several properties of these coloured Stirling numbers are discussed. PACS numbers: 02. 10. Ox 1...|$|R
40|$|Abstract. In {{this paper}} we explore partial <b>order</b> <b>reduction</b> that make the task of verifying {{cryptographic}} protocols more efficient. These reduction techniques have been implemented in our tool BRUTUS. Although we have implemented several reduction techniques in our tool BRUTUS, due to space restrictions {{in this paper we}} only focus on partial <b>order</b> <b>reductions.</b> Partial <b>order</b> <b>reductions</b> have proved very useful in the domain of model checking reactive systems. These re-ductions are not directly applicable in our context because of additional compli-cations caused by tracking knowledge of various agents. We present partial <b>order</b> <b>reductions</b> in the context of verifying security protocols and prove their correct-ness. Experimental results showing the benefits of this reduction technique are also presented...|$|R
