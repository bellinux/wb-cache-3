8|21|Public
25|$|David Wheeler, {{who earned}} the world's first Computer Science PhD {{working on the}} project, is {{credited}} with inventing {{the concept of a}} subroutine. Users wrote programs that called a routine by jumping {{to the start of the}} subroutine with the return address (i.e. the location-plus-one of the jump itself) in the accumulator (a Wheeler jump). By convention the subroutine expected this and the first thing it did was to modify its concluding jump instruction to that return address. Multiple and <b>nested</b> <b>subroutines</b> could be called so long as the user knew the length of each one in order to calculate the location to jump to; recursive calls were forbidden. The user then copied the code for the subroutine from a master tape onto their own tape following the end of their own program.|$|E
50|$|The B6500 had Display Registers, D1 thru D32 {{to allow}} <b>nested</b> <b>subroutines</b> to access {{variables}} in outer blocks.|$|E
50|$|Some {{languages}} such as Pascal and Ada {{also support}} <b>nested</b> <b>subroutines,</b> which are subroutines callable {{only within the}} scope of an outer (parent) subroutine. Inner subroutines have access to the local variables of the outer subroutine that called them. This is accomplished by storing extra context information within the activation record, also termed a display.|$|E
50|$|Fortran, {{starting}} with Fortran-90, supports a single level of <b>nested</b> (CONTAINed) <b>subroutines</b> and functions.|$|R
50|$|For {{subroutine}} calls, the Branch and Load Workspace Pointer (BLWP) instruction loads new WP and PC values, then {{saves the}} values of WP, PC and ST to the (new) registers 13, 14 and 15 respectively. At {{the end of the}} subroutine, the Return Workspace Pointer (RTWP) restores these in reverse order. Using BLWP/RTWP, it is possible to <b>nest</b> <b>subroutine</b> calls despite the absence of a stack, however, the programmer needs to assign the appropriate register workspace explicitly.|$|R
50|$|The request {{processor}} {{consists of}} the server process' main loop {{and a number of}} state machines. State machines are based on a simple language developed for PVFS that manage concurrency within the server and client. A state machine consists of a number of states, each of which either runs a C state action function or calls a <b>nested</b> (<b>subroutine)</b> state machine. In either case return codes select which state to go to next. State action functions typically submit a job via the job layer which performs some kind of I/O via Trove or BMI. Jobs are non-blocking, so that once a job is issued the state machine's execution is deferred so that another state machine can run servicing another request. When Jobs are completed the main loop restarts the associated state machine. The request processor has state machines for each of the various request types defined in the PVFS request protocol plus a number of nested state machines used internally. The state machine architecture makes it relatively easy to add new requests to the server in order to add features or optimize for specific situations.|$|R
50|$|David Wheeler, {{who earned}} the world's first Computer Science PhD {{working on the}} project, is {{credited}} with inventing {{the concept of a}} subroutine. Users wrote programs that called a routine by jumping {{to the start of the}} subroutine with the return address (i.e. the location-plus-one of the jump itself) in the accumulator (a Wheeler jump). By convention the subroutine expected this and the first thing it did was to modify its concluding jump instruction to that return address. Multiple and <b>nested</b> <b>subroutines</b> could be called so long as the user knew the length of each one in order to calculate the location to jump to; recursive calls were forbidden. The user then copied the code for the subroutine from a master tape onto their own tape following the end of their own program.|$|E
5000|$|Some {{programming}} languages (e.g., Pascal and Ada) support {{declaration of}} <b>nested</b> <b>subroutines,</b> which {{are allowed to}} access {{the context of their}} enclosing routines, i.e., the parameters and local variables within the scope of the outer routines. Such static nesting can repeat - a function declared within a function declared within a function... The implementation must provide a means by which a called function at any given static nesting level can reference the enclosing frame at each enclosing nesting level. Commonly this reference is implemented by a pointer to the frame of the most recently activated instance of the enclosing function, called a [...] "downstack link" [...] or [...] "static link", to distinguish it from the [...] "dynamic link" [...] that refers to the immediate caller (which need not be the static parent function).|$|E
5000|$|Programming {{languages}} {{that support}} <b>nested</b> <b>subroutines</b> {{also have a}} field in the call frame that points to the stack frame of the latest activation of the procedure that most closely encapsulates the callee, i.e. the immediate scope of the callee. This is called an access link or static link (as it keeps track of static nesting during dynamic and recursive calls) and provides the routine (as well as any other routines it may invoke) access to the local data of its encapsulating routines at every nesting level. Some architectures, compilers, or optimization cases store one link for each enclosing level (not just the immediately enclosing), so that deeply nested routines that access shallow data {{do not have to}} traverse several links; this strategy is often called a [...] "display".|$|E
50|$|The {{idea of a}} {{subroutine}} was {{worked out}} after computing machines had already existed for some time.The arithmetic and conditional jump instructions were planned {{ahead of time and}} have changed relatively little; but the special instructions used for procedure calls have changed greatly over the years.The earliest computers and microprocessors, such as the Small-Scale Experimental Machine and the RCA 1802, did not have a single subroutine call instruction.Subroutines could be implemented, but they required programmers to use the call sequence—a series of instructions—at each call site.Some very early computers and microprocessors, such as the IBM 1620, the Intel 8008, and the PIC microcontrollers, have a single-instruction subroutine call that uses dedicated hardware stack to store return addresses—such hardware supports only a few levels of <b>subroutine</b> <b>nesting,</b> but can support recursive subroutines.Machines before the mid 1960s—such as the UNIVAC I, the PDP-1, and the IBM 1130—typically use a calling convention which saved the instruction counter in the first memory location of the called subroutine. This allows arbitrarily deep levels of <b>subroutine</b> <b>nesting,</b> but does not support recursive subroutines.The PDP-11 (1970) {{is one of the first}} computers with a stack-pushing subroutine call instruction; this feature supports both arbitrarily deep <b>subroutine</b> <b>nesting</b> and also supports recursive subroutines.|$|R
40|$|Abstract In Java bytecode, intra-method subroutines are {{employed}} to represent code in &quot;finally&quot;blocks. The {{use of such}} polymorphic subroutines within a method makes bytecode analysis very difficult. Fortunately, such subroutines can be eliminated through recompilation orinlining. Inlining is the obvious choice since {{it does not require}} changing compilers or access to the source code. It also allows transformation of legacy bytecode. However, thecombination of <b>nested,</b> non-contiguous <b>subroutines</b> with overlapping exception handlers poses a difficult challenge. This paper presents an algorithm that successfully solves allthese problems without producing superfluous instructions...|$|R
2500|$|The ENIAC was {{programmed}} using <b>subroutines,</b> <b>nested</b> loops, {{and indirect}} addressing for both data locations and jump destinations. [...] During her work programming the ENIAC, Kay McNulty {{is credited with}} {{the invention of the}} subroutine. [...] Her colleague, Jean Jennings, recalled when McNulty proposed the idea to solve the problem where the logical circuits did not have enough capacity to compute some trajectories. [...] The team collaborated on the implementation.|$|R
5000|$|One {{well-known}} and often-used routine {{is known as}} SCRT (Standard CALL and RETURN Technique), which allows general purpose subroutine Call and Return, including passing of parameters [...] "in line", and <b>nested</b> <b>subroutines</b> using a stack. Although any of the available registers {{can be used for}} this technique, per programmer's preference, many use the routine supplied by RCA in the CDP1802 User Manual, where the suggested register usage is R2 = Stack Pointer, R3 = General Program Counter (PC), R4 = Call, R5 = Return, R6 = Passed Arguments Pointer (non-destructive). Even though these supportive routines are small, there is an execution speed overhead using them. (as opposed to what would be incurred if actual CALL and RET instructions were part of the microprocessor's design) This setup allows R0 to be used for DMA and R1 to be used for Interrupts, if desired, allowing R7 through RF (hex) for general program usage.|$|E
40|$|A nonsmooth equation-oriented multistream heat {{exchanger}} (MHEX) {{model has been}} developed by the Process Systems Engineering Laboratory at Massachusetts Institute of Technology that {{is intended to be}} a part of a rigorous optimization and simulation tool for liquefied natural gas (LNG) processes. The model was successfully used to simulate the poly refrigerant integrated cycle operations (PRICO) process for LNG production, though it suffered from convergence difficulties in more complex single mixed refrigerant processes. The primary challenge was flash calculations, which frequently failed to converge with a Newton solver even for initial guesses close to the solution. Equation-oriented simulation models have the advantage of high efficiency, but are generally less robust than the sequential-modular approach, such that improved performance may be achieved by using a different simulation framework. This master thesis studies two alternative model structures. First, the equation-oriented framework is replaced with a hybrid solution, in which vapour-liquid equilibrium calculations are included as <b>nested</b> <b>subroutines</b> and solved sequentially. Next, a fully sequential-modular approach is considered. The models are tested for different single mixed refrigerant processes, and are solved with a nonsmooth Newton-type solver using Clarke Jacobian elements as generalized derivatives. The implicit function theorem for lexicographically smooth functions is used for computing analytical derivatives in the subroutines. Results showed that the hybrid models were considerably more robust than the original equation-oriented models. In addition, they required fewer iterations to converge. However, as expected, they suffered a loss in efficiency. About half the computing time in the hybrid PRICO model was spent on the vapour-liquid equilibrium modules, which was primarily due to derivatives calculations. As a consequence the time per iteration was between 4 and 5 times longer for the processes studied, and even with fewer required iterations, the models were normally 1. 5 - 3 times slower. On the other hand, the sequential-modular framework turned out to be unsuitable for simulating the LNG models as it was both significantly less robust and efficient than the other approaches. The observed drop in robustness is against the general theory, however, and it was concluded that the convergence problems were caused by modularizing the MHEX model...|$|E
40|$|AbstractIn Java bytecode, intra-method subroutines are {{employed}} to represent code in “finally” blocks. The {{use of such}} polymorphic subroutines within a method makes bytecode analysis very difficult. Fortunately, such subroutines can be eliminated through recompilation or inlining. Inlining is the obvious choice since {{it does not require}} changing compilers or access to the source code. It also allows transformation of legacy bytecode. However, the combination of <b>nested,</b> non-contiguous <b>subroutines</b> with overlapping exception handlers poses a difficult challenge. This paper presents an algorithm that successfully solves all these problems without producing superfluous instructions. Furthermore, inlining can be combined with bytecode simplification, using abstract bytecode. We show how this abstration is extended to the full set of instructions and how it simplifies static and dynamic analysis...|$|R
50|$|One-pass compilers {{are unable}} to {{generate}} as efficient programs as multi-pass compilers due to the limited scope of available information. Many effective compiler optimizations require multiple passes over a basic block, loop (especially <b>nested</b> loops), <b>subroutine,</b> or entire module. Some require passes over an entire program. Some programming languages simply cannot be compiled in a single pass, {{as a result of}} their design. For example PL/I allows data declarations to be placed anywhere within a program, specifically, after some references to the not-yet-declared items, so no code can be generated until the entire program has been scanned. The language definition also includes pre-processor statements that generate source code to be compiled: multiple passes are certain. In contrast, many programming languages have been designed specifically to be compiled with one-pass compilers, and include special constructs to allow one-pass compilation.|$|R
50|$|In {{computer}} programming, a nested function (or <b>nested</b> procedure or <b>subroutine)</b> is {{a function}} which is defined within another function, the enclosing function. Due to simple recursive scope rules, a nested function is itself invisible outside of its immediately enclosing function, but can see (access) all local objects (data, functions, types, etc.) of its immediately enclosing function {{as well as of}} any function(s) which, in turn, encloses that function. The nesting is theoretically possible to any ideas of depth, although only a few levels are normally used in practical programs.|$|R
40|$|In this paper, {{we discuss}} the {{advantages}} of using array access region summaries to parallelize programs. We reformulate {{the definition of the}} types of dependence in terms of array regions, and present a dependence test called the Region Test to use the definitions. The Region Test is designed to detect dependence between arbitrary regions of a program, including loop <b>nests</b> and whole <b>subroutines.</b> This allows us to exploit task parallelism, as well as loop parallelism. The Region Test also facilitates the generation of run-time dependence tests in situations where insufficient information exists at compile time to carry out the dependence test...|$|R
50|$|Quartz {{programming}} through Quartz Composer {{works by}} implementing and connecting patches. Similar to routines in traditional programming languages, patches are base processing units. They execute {{and produce a}} result. For better performance, patch execution follows a lazy evaluation approach, meaning that patches are only executed when their output is needed. There are three types of patches: Consumers, Processors, and External Input patches that can receive and output mouse clicks, scrolls, and movements; MIDI and audio; keyboard; or other movements. A collection of patches can be melded into one, called a macro. Macros can be <b>nested</b> and their <b>subroutines</b> also edited.|$|R
5|$|While {{the five}} women worked on ENIAC, they {{developed}} <b>subroutines,</b> <b>nesting,</b> and other fundamental programming techniques, and arguably invented {{the discipline of}} programming digital computers. Bartik's programming partner on the ENIAC was Betty Holberton. Together, they created the master program for the ENIAC and led the ballistics programming group. The team also learned to physically modify the machine, moving switches and rerouting cables, in order to program it. In addition to performing the original ballistic trajectories they were hired to compute, they soon became operators on the Los Alamos nuclear calculations, and generally expanded the programming repertoire of the machine. Bartik was chosen along with Betty Holberton to write a program to display trajectory simulations for the first public demonstration of the ENIAC.|$|R
40|$|Energy {{consumption}} {{has become}} a major constraint in providing increased functionality for devices with small form factors. Dynamic voltage and frequency scaling has been identified as an effective approach for reducing the energy consumption of embedded systems. Earlier works on dynamic voltage scaling focused mainly on performing voltage scaling when the CPU is waiting for memory subsystem or concentrated chiefly on loop <b>nests</b> and/or <b>subroutine</b> calls having sufficient number of dynamic instructions. This paper concentrates on coarser program regions {{and for the first time}} uses program phase behavior for performing dynamic voltage scaling. Program phases are annotated at compile time with mode switch instructions. Further, we relate the Dynamic Voltage Scaling Problem to the Multiple Choice Knapsack Problem, and use well known heuristics to solve it efficiently. Also, we develop a simple integer linear program formulation for this problem. Experimental evaluation on a set of media applications reveal that our heuristic method obtains a 38 % reduction in energy consumption on an average, with a performance degradation of 1 % and upto 45 % reduction in energy with a performance degradation of 5 %. Further, the energy consumed by the heuristic solution is within 1 % of the optimal solution obtained from the ILP approach...|$|R
40|$|Development of HPF {{versions}} of NPB and ARC 3 D {{has shown that}} HPF provides an efficient, concise way to express parallelism and to organize data traffic. The use of HPF, {{as noted in the}} papers, requires an intimate knowledge of the applications and a detailed analysis of data affinity, data movement, and data granularity. To simplify and accelerate the task of developing HPF {{versions of}} existing CFD applications we have designed and implemented ADAPT (Automatic Data Alignment and Placement Tool). ADAPT analyzes a CFD application working on a single structured grid and generates HPF TEMPLATE, (RE) DISTRIBUTION, ALIGNMENT, and INDEPENDENT directives. The directives can be generated on the <b>nest</b> level, <b>subroutine</b> level, application level, or on the application interface level. ADAPT annotates an existing CFD FORTRAN application, performing computations on single or multiple grids. On each grid the application is considered as a sequence of operators, each applied to a set of variables defined in a particular grid domain. ADAPT automatically detects implicit operators (i. e., having data dependences) and explicit operators (without data dependences). For parallelization of an explicit operator ADAPT creates a template for the operator domain, aligns arrays used in the operator with the template, distributes the template, and declares the loops over the distributed dimensions as INDEPENDENT. For parallelization of an implicit operator, the distribution of the operator's domain should be consistent with the operator's dependences. Any dependence between sections distributed on different processors would preclude parallelization if the compiler does not have an ability to pipeline computations. If a data distribution is "orthogonal" to the dependences of an implicit operator, then the loop which implements the operator can be declared as INDEPENDENT. ADAPT starts with an analysis of array index expressions of the loop nests. For each pair of arrays referenced in an assignment statement, it generates an arc in the alignment graph and annotates it with an affinity relation. The template, alignment, and distribution directives for a particular loop nest are then derived from a transitive closure of the affinity relation. A compromise of data distributions in different <b>nests</b> and <b>subroutines</b> is achieved by merging annotated alignment graphs for adjacent nests/stibroutine calls in the nest/call graph of the application in the process called distribution lifting. ADAPT has been implemented as a C++ program running in conjunction with a parallelization tool called CAPTools. ADAPT uses the parse tree, interprocedural analysis and application database generated by CAPTools. It also uses the Directed Graph class, initially implemented in p 2 d 2 (parallel debugger oi distributed programs), and some other classes supporting symbolic computations. ADAPT uses data distribution techniques described. ADAPT was tested with ARC 3 D and the FT benchmark and has demonstrated a code performance within a factor of 1. 5 of handwritten versions...|$|R
40|$|Development of HPF {{versions}} of NPB and ARC 3 D showed that HPF has {{potential to be}} a high level language for parallelization of CFD applications. The use of HPF requires an intimate knowledge of the applications and a detailed analysis of data affinity, data movement and data granularity. Since HPF hides data movement from the user even with this knowledge it is easy to overlook pieces of the code causing low performance of the application. In order to simplify and accelerate the task of developing HPF {{versions of}} existing CFD applications we have designed and partially implemented ADAPT (Automatic Data Distribution and Placement Tool). The ADAPT analyzes a CFD application working on a single structured grid and generates HPF TEMPLATE, (RE) DISTRIBUTION, ALIGNMENT and INDEPENDENT directives. The directives can be generated on the <b>nest</b> level, <b>subroutine</b> level, application level or inter application level. ADAPT is designed to annotate existing CFD FORTRAN application performing computations on single or multiple grids. On each grid the application can considered as a sequence of operators each applied to a set of variables defined in a particular grid domain. The operators can be classified as implicit, having data dependences, and explicit, without data dependences. In order to parallelize an explicit operator it is sufficient to create a template for the domain of the operator, align arrays used in the operator with the template, distribute the template, and declare the loops over the distributed dimensions as INDEPENDENT. In order to parallelize an implicit operator, the distribution of the operator's domain should be consistent with the operator's dependences. Any dependence between sections distributed on different processors would preclude parallelization if compiler does not have an ability to pipeline computations. If a data distribution is "orthogonal" to the dependences of an implicit operator then the loop which implements the operator can be declared as INDEPENDENT...|$|R
40|$|Writing {{a program}} {{to solve a problem}} is a process that can be divided into two phases: first, we invent a mental model of the solution; secondly, we map the mental model onto a {{physical}} representation. The mental model is multidimensional and syntax-free; in today’s textual programming languages, the physical representation is singledimensional and syntax-burdened. In fact, it hasn’t changed greatly since Algol 60. Mapping from one representation to the other has remained a painstaking and error-prone task, in spite of the ready availability of immensely faster computers, massive amounts of memory, high-resolution graphics displays, and powerful graphic input mechanisms. The Hyperprogramming paradigm exploits these capabilities. A hyperprogramming language employs different visualisations for different program components- for example one visual syntax is suitable for visualising algorithms and another is suitable for visualising <b>subroutine</b> <b>nesting.</b> Each visualisation is designed for minimal overlap with the others, and where overlap is essential, hyperlinks between the views are automatically provided to allow easy navigation between them, and automatic updating of shared information. HyperPascal was developed as a vehicle for exploring this idea. In creating a program, a HyperPascal programmer edits information in three main visualisations: • the action window visualisation, which represents algorithms using a visual language based on structure diagrams; • the data structure templates visual component, which represents dynamic data structure algorithms using beforeand-after pictures • the scope window visualisation, which represents declarations as a nested visualisation analogous to conventional subroutine nestin...|$|R
40|$|The {{demand for}} devices like Personal Digital Assistants (PDA’s), Laptops, Smart Mobile Phones, {{are at an}} all time high. As the demand for these devices increases, so is the push to provide {{sophisticated}} functionalities in these devices. However energy consumption {{has become a major}} constraint in providing increased functionality for these devices. A majority of the applications meant for these devices are rich with multimedia content. In this thesis, we propose two approaches for compiler directed energy reduction, one targeting the memory subsystem and another the processor. The ﬁrst technique is a compiler directed optimization technique that reduces the energy consumption of the memory subsystem, for an oﬀ-chip partitioned memory archi- tecture, having multiple memory banks, and various low-power operating modes for each of these banks. We propose an eﬃcient layout of the data segment {{to reduce the number of}} simultaneously active memory banks, so that the other memory banks that are inactive can be put to low power modes to reduce the energy. We model this problem as a graph partitioning problem, and use well known heuristics to solve the same. We also propose a simple Integer Linear Programming (ILP) formulation for the above problem. Perfor- mance results indicate that our approach achieves an energy reduction of 20 % compared to the base scheme, and a reduction of 8 %- 10 % over a previously suggested method. Also, our results are well within the optimal results obtained by using ILP method. The second approach proposed in this thesis reduces the dynamic energy consumed by the processor using dynamic voltage and frequency scaling technique. Earlier works on dynamic voltage scaling focused mainly on performing voltage scaling when the CPU is waiting for memory subsystem or concentrated chieﬂy on loop <b>nests</b> and/or <b>subroutine</b> calls having suﬃcient number of dynamic instructions. We concentrate on coarser pro- gram regions and for the ﬁrst time uses program phase behavior for performing dynamic voltage scaling. We relate the Dynamic Voltage Scaling Problem to the Multiple Choice Knapsack Problem, and use well known heuristics to solve it eﬃciently. Also, we develop a simple Integer Linear Programming (ILP) problem formulation for this problem. Experi-mental evaluation on a set of media applications reveal that our heuristic method obtains 35 - 40 % reduction in energy consumption on an average, with a negligible performance degradation. Further the energy consumed by our heuristic solution is within 1 % the optimal solution obtained by the ILP approach...|$|R
40|$|This chapter {{sketches}} in {{very general}} terms the cognitive architecture of both language comprehension and production, {{as well as}} the neurobiological infrastructure that makes the human brain ready for language. Focus is on spoken language, since that compares most directly to processing music. It is worth bearing in mind that humans can also interface with language as a cognitive system using sign and text (visual) as well as Braille (tactile); that is to say, the system can connect with input/output processes in any sensory modality. Language processing consists of a complex and <b>nested</b> set of <b>subroutines</b> to get from sound to meaning (in comprehension) or meaning to sound (in production), with remarkable speed and accuracy. The fi rst section outlines a selection of the major constituent operations, from fractionating the input into manageable units to combining and unifying information in the construction of meaning. The next section addresses the neurobiological infrastructure hypothesized to form the basis for language processing. Principal insights are summarized by building on the notion of “brain networks” for speech–sound processing, syntactic processing, and the construction of meaning, bearing in mind that such a neat three-way subdivision overlooks important overlap and shared mechanisms in the neural architecture subserving language processing. Finally, in keeping with the spirit of the volume, some possible relations are highlighted between language and music that arise from the infrastructure developed here. Our characterization of language and its neurobiological foundations is necessarily selective and brief. Our aim is to identify for the reader critical questions that require an answer to have a plausible cognitive neuroscience of language processing...|$|R

