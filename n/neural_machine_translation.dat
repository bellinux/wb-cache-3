272|8827|Public
50|$|Google <b>Neural</b> <b>Machine</b> <b>Translation</b> (GNMT) is a <b>neural</b> <b>machine</b> <b>translation</b> (NMT) system {{developed}} by Google and introduced in November 2016, that uses an {{artificial neural network}} to increase fluency and accuracy in Google Translate.|$|E
50|$|<b>Neural</b> <b>machine</b> <b>translation</b> (NMT) is an {{approach}} to machine translation that uses a large neural network. It departs from phrase-based statistical translation approaches that use separately engineered subcomponents. Google and Microsoft translation services now use NMT. Google uses Google <b>Neural</b> <b>Machine</b> <b>Translation</b> (GNMT) in preference to its previous statistical methods. Microsoft uses a similar technology for its speech translations (including Microsoft Translator live and Skype Translator). An open source <b>neural</b> <b>machine</b> <b>translation</b> system, OpenNMT, has been released by the Harvard NLP group.|$|E
50|$|In September 2016, a {{research}} team at Google announced {{the development of}} the Google <b>Neural</b> <b>Machine</b> <b>Translation</b> system (GNMT) and by November Google Translate began using <b>neural</b> <b>machine</b> <b>translation</b> (NMT) in preference to its previous statistical methods (SMT) which had been used since October 2007, with its proprietary, in-house SMT technology.|$|E
5000|$|... 2005 pending patent {{application}} for a newly developed hybrid technology that uses the intelligence of <b>neural</b> networks for <b>machine</b> <b>translation.</b>|$|R
40|$|This paper {{describes}} the <b>neural</b> and phrase-based <b>machine</b> <b>translation</b> systems submitted by CUNI to English-Czech News Translation Task of WMT 17. We experiment with synthetic data {{for training and}} try several system combination techniques, both neural and phrase-based. Our primary submission CU-CHIMERA ends up being phrase-based backbone which incorporates neural and deep-syntactic candidate translations...|$|R
40|$|Recent {{studies have}} {{demonstrated}} the power of recurrent <b>neural</b> networks for <b>machine</b> <b>translation,</b> image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy {{to take into account}} the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results...|$|R
50|$|A deep {{learning}} based approach to MT, <b>neural</b> <b>machine</b> <b>translation</b> has made rapid progress in recent years, and Google has announced its translation services {{are now using}} this technology in preference to its previous statistical methods. Other providers including KantanMT, Omniscien Technologies and SDL have announced the deployment of <b>neural</b> <b>machine</b> <b>translation</b> technology in 2017 as well.|$|E
5000|$|This {{is a list}} of {{language}} translation pairs supported by Google Translate's <b>Neural</b> <b>Machine</b> <b>Translation</b> (NMT) model. As of July 2017 all languages currently only support translation to and from English: ...|$|E
5000|$|In September 2016, a {{research}} team at Google announced {{the development of}} the Google <b>Neural</b> <b>Machine</b> <b>Translation</b> system (GNMT) to increase fluency and accuracy in Google Translate and in November announced that Google Translate would switch to GNMT.|$|E
40|$|In this paper, {{we propose}} a novel finetuning {{algorithm}} for the recently introduced multi-way, mulitlingual <b>neural</b> <b>machine</b> translate that enables zero-resource <b>machine</b> <b>translation.</b> When used together with novel many-to-one translation strategies, we empirically {{show that this}} finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) {{as well as a}} single-pair neural translation model trained with up to 1 M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters...|$|R
50|$|Hybrid <b>machine</b> <b>translation</b> is {{a method}} of <b>machine</b> <b>translation</b> that is {{characterized}} by the use of multiple <b>machine</b> <b>translation</b> approaches within a single <b>machine</b> <b>translation</b> system. The motivation for developing hybrid <b>machine</b> <b>translation</b> systems stems from the failure of any single technique to achieve a satisfactory level of accuracy. Many hybrid <b>machine</b> <b>translation</b> systems have been successful in improving the accuracy of the translations, and there are several popular <b>machine</b> <b>translation</b> systems which employ hybrid methods. Among these are PROMT, SYSTRAN and Omniscien Technologies (formerly Asia Online).|$|R
40|$|Corpus-based <b>Machine</b> <b>Translation</b> (MT) {{approaches}} such as Statistical <b>Machine</b> <b>Translation</b> (SMT) and Example-based <b>Machine</b> <b>Translation</b> (EBMT) {{have received}} much attention in recent years, and have significantly improved the state-of-the-art of <b>Machine</b> <b>Translation</b> {{for a number}} of different language pairs. These approache...|$|R
50|$|The company {{provides}} {{a range of}} solutions for the localization industry as well as Government, eCommerce, Online Research and Publishing, Online Travel, Media and large Enterprise customers based on statistical machine translation (SMT) and hybrid <b>neural</b> <b>machine</b> <b>translation</b> (NMT) technology. Omniscien Technologies currently supports in excess of 550 global language pairs in 13 industry domains.|$|E
5000|$|Launched in April 2006 as a {{statistical}} machine translation service, it used United Nations and European Parliament transcripts to gather linguistic data. Rather than translating languages directly, it first translates text to English {{and then to the}} target language. During a translation, it looks for patterns in millions of documents to help decide on the best translation. Its accuracy has been criticized and ridiculed on several occasions. In November 2016, Google announced that Google Translate would switch to a <b>neural</b> <b>machine</b> <b>translation</b> engine - Google <b>Neural</b> <b>Machine</b> <b>Translation</b> (GNMT) - which translates [...] "whole sentences at a time, rather than just piece by piece. It uses this broader context to help it figure out the most relevant translation, which it then rearranges and adjusts to be more like a human speaking with proper grammar". Originally only enabled for a few languages in 2016, GNMT is gradually being used for more languages.|$|E
50|$|More recently, {{with the}} advent of Neural MT, a new version of hybrid machine {{translation}} is emerging that combines the benefits of rules, statistical and <b>neural</b> <b>machine</b> <b>translation.</b> The approach allows benefitting from pre- and post-processing in a rule guided workflow as well as benefitting from NMT and SMT. The downside is the inherent complexity which makes the approach suitable only for specific use cases. One of the proponents of this approach for complex use cases is Omniscien Technologies.|$|E
40|$|The article {{gives an}} {{overview}} of such <b>machine</b> <b>translation</b> technologies as Rule-based <b>Machine</b> <b>Translation,</b> Statistical <b>Machine</b> <b>Translation,</b> Hybrid <b>Machine</b> <b>Translation</b> {{as well as of}} Translation Memory. Peculiarities of their implementation in online translators by different specialized IT companies are also consideredye...|$|R
50|$|Statistical <b>machine</b> <b>translation</b> (SMT) is a <b>machine</b> <b>translation</b> {{paradigm}} where translations {{are generated}} {{on the basis}} of statistical models whose parameters are derived from the analysis of bilingual text corpora. The statistical approach contrasts with the rule-based approaches to <b>machine</b> <b>translation</b> as well as with example-based <b>machine</b> <b>translation.</b>|$|R
40|$|With the {{international}} exchanges become more frequent, human translation can’t meet {{the need of}} society, and with the developing of computer technology, <b>machine</b> <b>translation</b> becomes feasible. Reviews the history of <b>machine</b> <b>translation</b> and analyzes the main methods of <b>machine</b> <b>translation,</b> finally puts forward of the suggestions on <b>machine</b> <b>translation</b> construction. </p...|$|R
5000|$|Google Translate's new <b>neural</b> <b>machine</b> <b>translation</b> {{system uses}} a large {{end-to-end}} {{artificial neural network}} capable of deep learning, in particular, long short-term memory networks.GNMT improves the quality of translation because it uses an example-based machine translation (EBMT) method in which the system [...] "learns from millions of examples." [...] It translates [...] "whole sentences at a time, rather than just piece by piece. It uses this broader context to help it figure out the most relevant translation, which it then rearranges and adjusts {{to be more like}} a human speaking with proper grammar". GNMT's [...] "proposed architecture" [...] of [...] "system learning" [...] was first tested on over a hundred languages supported by Google Translate. With the end-to-end framework, [...] "the system learns over time to create better, more natural translations." [...] The GNMT network is capable of interlingual machine translation, which encodes the [...] "semantics of the sentence rather than simply memorizing phrase-to-phrase translations", and the system did not invent its own universal language, but uses [...] "the commonality found inbetween many languages". GNMT was first enabled for eight languages: to and from English and Chinese, French, German, Japanese, Korean, Portuguese, Spanish and Turkish. In March 2017, it was enabled for Hindi, Russian and Vietnamese languages, followed by Indonesian, Bengali, Gujarati, Kannada, Malayalam, Marathi, Punjabi, Tamil and Telugu languages in April.|$|E
50|$|The Google Brain Team has {{recently}} reached significant breakthroughs for Google Translate, {{which is part}} of the Google Brain Project. In September 2016, the team launched the new system, Google <b>Neural</b> <b>Machine</b> <b>Translation</b> (GNMT), which is an end-to-end learning framework, able to learn from a large amount of examples. While its introduction has greatly increased the quality of Google Translate's translations for the pilot languages, it was very difficult to create such improvements for all of its 103 languages. Addressing this problem, the Google Brain Team was able to develop a Multilingual GNMT system, which extended the previous one by enabling translations between multiple languages. Furthermore, it allows for Zero-Shot Translations, which are translations between two languages that the system has never explicitly seen before. Recently, Google has announced, that Google Translate can now also translate without transcribing, using neural networks. This means, that it is possible to translate speech in one language directly into text in another language, without first transcribing it to text. According to the Researchers at Google Brain, this intermediate step can be avoided using neural networks. In order for the system to learn this, they exposed it to many hours of Spanish audio together with the corresponding English text. The different layers of neural networks, replicating the human brain, were able to link the corresponding parts and subsequently manipulate the audio waveform until it was transformed to English text.|$|E
40|$|We {{investigate}} {{the potential of}} attention-based <b>neural</b> <b>machine</b> <b>translation</b> in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing <b>neural</b> <b>machine</b> <b>translation</b> model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents {{a first step toward}} building a full simultaneous translation system based on <b>neural</b> <b>machine</b> <b>translation...</b>|$|E
40|$|The <b>Machine</b> <b>Translation</b> {{has been}} a branch of Natural Language Processing, which comes under the broad area of Artificial Intelligence. <b>Machine</b> <b>Translation</b> system refers to {{computer}} software that translates text or voice from one natural language into another with or without human assistance. Worldwide, large number of <b>machine</b> <b>translation</b> systems have been developed using several approaches including human-assisted, rule-based, statistical, example-based, hybrid and agent based techniques. Among others, Statistical <b>machine</b> <b>translation</b> approach {{is by far the}} most widely studied <b>machine</b> <b>translation</b> method in the field of <b>machine</b> <b>translation.</b> The multi-agent approach is a modern approach to handle complexity of the systems in past five years. This paper reviews existing <b>machine</b> <b>translation</b> approaches and systems including existing chine translation systems...|$|R
40|$|Abstract. <b>Machine</b> <b>translation</b> (MT) {{is one of}} {{the core}} {{application}} of natural language processing and an important branch of artificial intelligence research; statistical methods have already become the mainstream of <b>machine</b> <b>translation.</b> This paper explores the comparative analysis on the translation model of statistical natural language processing based on the large-scale corpus; discusses word-based, phrase-based and syntax-based <b>machine</b> <b>translation</b> methods respectively, summarizes the evaluation factors of <b>machine</b> <b>translation</b> and analyzes evaluation methods of <b>machine</b> <b>translation...</b>|$|R
40|$|The {{manuscript}} {{presents an}} experiment at {{implementation of a}} <b>Machine</b> <b>Translation</b> system in a MapReduce model. The empirical evaluation was done using fully implemented translation systems embedded into the MapReduce programming model. Two <b>machine</b> <b>translation</b> paradigms were studied: shallow transfer Rule Based <b>Machine</b> <b>Translation</b> and Statistical <b>Machine</b> <b>Translation.</b> The {{results show that the}} MapReduce model can be successfully used to increase the throughput of a <b>machine</b> <b>translation</b> system. Furthermore this method enhances the throughput of a <b>machine</b> <b>translation</b> system without decreasing the quality of the translation output. Thus, the present manuscript also represents a contribution to the seminal work in natural language processing, specifically <b>Machine</b> <b>Translation.</b> It first points toward the importance of the definition of the metric of throughput of translation system and, second, the applicability of the <b>machine</b> <b>translation</b> task to the MapReduce paradigm. Comment: 20 pages, 7 figure...|$|R
40|$|<b>Neural</b> <b>machine</b> <b>translation</b> {{has become}} a major {{alternative}} to widely used phrase-based statistical machine translation. We notice however that much of research on <b>neural</b> <b>machine</b> <b>translation</b> has focused on European languages despite its language agnostic nature. In this paper, we apply <b>neural</b> <b>machine</b> <b>translation</b> to the task of Arabic translation (Ar En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the <b>neural</b> <b>machine</b> <b>translation</b> significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment. Comment: EMNLP submissio...|$|E
40|$|In {{this paper}} we provide the largest {{published}} comparison of translation quality for phrase-based SMT and <b>neural</b> <b>machine</b> <b>translation</b> across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v 1. 0 and its large six-way sentence-aligned subcorpus. In {{the second part}} of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient <b>neural</b> <b>machine</b> <b>translation</b> decoder. We demonstrate that current <b>neural</b> <b>machine</b> <b>translation</b> could already be used for in-production systems when comparing words-per-second ratios. Comment: Accepted for presentation at IWSLT 2016, Seattl...|$|E
40|$|<b>Neural</b> <b>machine</b> <b>translation</b> is a {{relatively}} new approach to statistical machine translation based purely on neural networks. The <b>neural</b> <b>machine</b> <b>translation</b> models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the <b>neural</b> <b>machine</b> <b>translation</b> using two models; RNN Encoder [...] Decoder and a newly proposed gated recursive convolutional neural network. We show that the <b>neural</b> <b>machine</b> <b>translation</b> performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically. Comment: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST- 8...|$|E
40|$|The article dwells on {{the most}} {{important}} land-marks in the history of <b>machine</b> <b>translation</b> (MT). In the middle of the twentieth century, at the time when discussions about <b>machine</b> <b>translation</b> started, it was thought that after several years every person would have a small device allowing people to communicate in any language. Today the view towards <b>machine</b> <b>translation</b> has become more pragmatic. For example, everyone agrees that <b>machine</b> <b>translation</b> of fiction will never be of satisfactory quality. The article discusses three key methods of <b>machine</b> <b>translation,</b> namely, statistical, rule-based, and logical. For practical purposes, of-ten several methods of <b>machine</b> <b>translation</b> are integrated. In rule-based <b>machine</b> <b>translation</b> the most important role is played by dictionaries and sets of rules. The article demonstrates that morphological, syntactic, and semantic ambiguities are the main obstacles for creating a high quality formal <b>machine</b> <b>translation</b> product. Statistical <b>machine</b> <b>translation</b> is based on data from parallel and monolingual corpora. While statistical <b>machine</b> <b>translation</b> is good for capturing the key words of a sentence, the overall quality of a translation remains low. The limited size of parallel corpora is the main obstacle of statistical <b>machine</b> <b>translation.</b> The article also discusses evaluation criteria for assessing MT quality, which is often very problematic...|$|R
50|$|RBMT {{systems can}} also be {{characterized}} as the systems opposite to Example-based Systems of <b>Machine</b> <b>Translation</b> (Example Based <b>Machine</b> <b>Translation),</b> whereas Hybrid <b>Machine</b> <b>Translations</b> Systems make use of many principles derived from RBMT.|$|R
40|$|Statistical <b>machine</b> <b>translation</b> is {{the process}} of {{translating}} lan-guages using machine learning methods. By learning from a large parallel text corpora, Statistical <b>Machine</b> <b>Translation</b> au-tomatically learns how to translate one language into another. This paper will present a method to translate English language to Assamese language using Statistical <b>Machine</b> <b>Translation.</b> General Terms: <b>Machine</b> <b>Translation</b> from one language to othe...|$|R
40|$|We aim to {{shed light}} on the {{strengths}} and weaknesses of the newly introduced <b>neural</b> <b>machine</b> <b>translation</b> paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art <b>neural</b> <b>machine</b> <b>translation</b> and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by <b>neural</b> <b>machine</b> <b>translation</b> systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. <b>Neural</b> <b>machine</b> <b>translation</b> systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences. Comment: Conference of the European Chapter of the Association for Computational Linguistics (EACL). April 2017, València, Spai...|$|E
40|$|Word sense {{disambiguation}} {{is necessary}} in translation because different word senses often have different translations. <b>Neural</b> <b>machine</b> <b>translation</b> models learn different senses of words {{as part of an}} end-to-end translation task, and their capability to perform word sense disambiguation has so far not been quantified. We exploit the fact that neural translation models can score arbitrary translations to design a novel cross-lingual word sense disambiguation task that is tailored towards evaluating <b>neural</b> <b>machine</b> <b>translation</b> models. We present a test set of 7, 200 lexical ambiguities for German → English, and 6, 700 for German → French, and report baseline results. With 70 % of lexical ambiguities correctly disambiguated, we find that word sense disambiguation remains a challenging problem for <b>neural</b> <b>machine</b> <b>translation,</b> especially for rare word senses. To improve word sense disambiguation in <b>neural</b> <b>machine</b> <b>translation,</b> we experiment with two methods to integrate sense embeddings. In a first approach we pass sense embeddings as additional input to the <b>neural</b> <b>machine</b> <b>translation</b> system. For the second experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model. While a baseline NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models’ performance on rare word senses...|$|E
40|$|There {{has been}} {{relatively}} little attention to incorporating linguistic prior to <b>neural</b> <b>machine</b> <b>translation.</b> Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based <b>neural</b> <b>machine</b> <b>translation.</b> Our approach encourages the <b>neural</b> <b>machine</b> <b>translation</b> model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show {{the effectiveness of the}} proposed NMT+RNNG. Comment: Accepted as a short paper at the 55 th Annual Meeting of the Association for Computational Linguistics (ACL 2017...|$|E
40|$|In this paper, {{we propose}} a simple, fast {{decoding}} algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on {{the tasks of}} dialogue response generation, abstractive summarization and <b>machine</b> <b>translation.</b> We find that diverse decoding helps across all tasks, especially those for which reranking is needed. We further propose a variation {{that is capable of}} automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique. This paper includes material from the unpublished script "Mutual Information and Diverse Decoding Improve <b>Neural</b> <b>Machine</b> Translation" (Li and Jurafsky, 2016) ...|$|R
40|$|The {{aim of this}} bachelor´s {{thesis is}} to test and {{evaluate}} the translation quality of selected terminological collocations {{in the area of}} "banking" using freely available translators (Google Translate, Bing Translator). The thesis consists of the necessary theoretical knowledge of <b>machine</b> <b>translation</b> and translation evaluation. The theoretical knowledge includes historical development of <b>machine</b> <b>translation,</b> <b>translation</b> types (rule-based <b>machine</b> <b>translation,</b> statistical <b>machine</b> <b>translation,</b> a hybrid <b>machine</b> <b>translation</b> and computer aided translation) and approaches some basic information about Google Translate and Microsoft Bing Translator. The evaluation of the translations was done by analysing machine-translated collocations and their subsequent comparison with the available terminology databases (esp. IATE) and with specialized dictionaries...|$|R
40|$|<b>Machine</b> <b>translation</b> {{system is}} {{software}} designed that essentially takes a text in one language (called the source language) and translates it into another language (called the target language). This paper presents {{the state of}} the art in the field of <b>machine</b> <b>translation.</b> First part of this paper discusses the <b>machine</b> <b>translation</b> systems for non-Indian languages and second part discusses the <b>machine</b> <b>translation</b> systems for Indian languages...|$|R
