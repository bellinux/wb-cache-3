0|4066|Public
40|$|Bit {{error rate}} is {{measured}} by sending a pseudo-random <b>noise</b> (PRN) <b>code</b> <b>test</b> signal simulating digital data through digital equipment to be tested. An incoming signal representing {{the response of the}} equipment being tested, together with any added noise, is received and tracked by being compared with a locally generated PRN code. Once the locally generated PRN code matches the incoming signal a tracking lock is obtained. The incoming signal is then integrated and compared bit-by-bit against the locally generated PRN code and differences between bits being compared are counted as bit errors...|$|R
40|$|The idea {{of using}} mixing {{enhancement}} to reduce jet noise is not new. Lobed mixers have been around since shortly after jet noise became a problem. However, these designs were often a post-design fix that rarely was worth its weight and thrust loss from a system perspective. Recent advances in CFD and some inspired concepts involving chevrons have shown how mixing enhancement can be successfully employed in noise reduction by subtle manipulation of the nozzle geometry. At NASA Glenn Research Center, this recent success has provided an opportunity to explore our paradigms of jet noise understanding, prediction, and reduction. Recent advances in turbulence measurement technology for hot jets have also greatly aided our ability to explore the cause and effect relationships of nozzle geometry, plume turbulence, and acoustic far field. By studying the flow and sound fields of jets with various degrees of mixing enhancement and subsequent noise manipulation, {{we are able to}} explore our intuition regarding how jets make <b>noise,</b> <b>test</b> our prediction <b>codes,</b> and pursue advanced noise reduction concepts. The paper will cover some of the existing paradigms of jet noise as they relate to mixing enhancement for jet noise reduction, and present experimental and analytical observations that support these paradigms...|$|R
40|$|The ultrasonics {{technique}} {{for assessing the}} structural integrity of the primary surface {{of the space shuttle}} vehicles is discussed and evaluated. Analysis was made of transducers, transducer coupling test structure fabrication, flaws, and ultrasonic testing. Graphs of microphone response curves from the initial <b>noise</b> <b>tests,</b> accelerometer response curves from the final <b>noise</b> <b>tests,</b> and microphone curves from the final <b>noise</b> <b>tests</b> are included along with a glossary, bibliography, and results...|$|R
25|$|In mid-2000, Collins {{was sent}} to Ketchikan, Alaska for <b>noise</b> <b>testing</b> with the United States Navy. Although <b>noise</b> <b>testing</b> in Australia was {{believed}} to have been affected by natural background <b>noise,</b> the Alaskan <b>tests</b> confirmed the Australian results. Low-speed testing showed that the Collins class was almost undetectable at patrol speed.|$|R
5000|$|Mock object (used for verifying [...] "indirect output" [...] of the <b>tested</b> <b>code,</b> {{by first}} {{defining}} the expectations before the <b>tested</b> <b>code</b> is executed) ...|$|R
40|$|Testing is the {{predominant}} way of establishing {{evidence that a}} program meets it requirements. When both <b>test</b> <b>code</b> and the application under test are written in the same programming language, a refactoring tool for this language {{should be able to}} refactor both application <b>code</b> and <b>testing</b> <b>code</b> together. However, <b>testing</b> frameworks normally come with particular programming idioms, such as their use of naming conventions, coding patterns, meta-programming techniques and the like. A refactoring tool needs to be aware of those programming idioms in order to refactor <b>test</b> <b>code</b> properly. Meanwhile the particularities of <b>test</b> <b>code</b> also suggest refactorings that are particularly applicable to <b>test</b> <b>code.</b> In this paper we present our experience of extending Wrangler, a refactoring tool for the Erlang programming language, so as to handle the three common testing frameworks for Erlang, as well as discussing the refactoring of <b>test</b> <b>code</b> in its own right...|$|R
40|$|The <b>test</b> <b>code</b> for steam {{turbines}} {{was one of}} the group of ten codes forming the 1915 edition of the A. S. M. E. power <b>test</b> <b>codes</b> [...] . The present 1941 revision [...] . was approved: and on February 28, 1941, it was [...] . adopted as a standard practice of the society. " [...] Foreword. Cover-title: [...] . Steam turbines, 1941. Bound with: Appendix to <b>test</b> <b>codes</b> for {{steam turbines}}. N. Y., 1943. 85 p. At head of title: PTC 6 - 1941. Power <b>test</b> <b>codes.</b> A. S. M. E. 1941. Mode of access: Internet...|$|R
50|$|British, German, other {{national}} and international <b>test</b> <b>codes</b> are used to standardize the procedures and definitions used to test gas turbines. Selection of the <b>test</b> <b>code</b> to be used is an agreement between the purchaser and the manufacturer, and has some significance {{to the design of}} the turbine and associated systems. In the United States, ASME has produced several performance <b>test</b> <b>codes</b> on gas turbines. This includes ASME PTC 22-2014. These ASME performance <b>test</b> <b>codes</b> have gained international recognition and acceptance for testing gas turbines. The single most important and differentiating characteristic of ASME performance <b>test</b> <b>codes,</b> including PTC 22, is that the test uncertainty of the measurement indicates the quality of the test and is not {{to be used as a}} commercial tolerance.|$|R
30|$|This step {{employs a}} white <b>noise</b> <b>test</b> to check whether the {{residual}} series {{from the model}} contains additional information that might be of use to a more complex model. In this case, the analysis must be continued by repeating Steps 3 and 4 until an appropriate ARIMA model is found which passes the white <b>noise</b> <b>test.</b>|$|R
40|$|Automated {{testing is}} a basic {{principle}} of agile development. Its benefits include early defect detection, defect cause localization and removal of fear to apply changes in the code. Therefore, maintaining high quality <b>test</b> <b>code</b> is essential. This study introduces a model that assesses <b>test</b> <b>code</b> quality by combining source code metrics that reflect three main aspects of <b>test</b> <b>code</b> quality: completeness, effectiveness and maintainability. The model is inspired by the SIG Software Quality model which aggregates source code metrics into quality ratings based on benchmarking. To validate the model we assess the relation between <b>test</b> <b>code</b> quality, {{as measured by the}} model, and issue handling performance. An experiment is conducted in which the <b>test</b> <b>code</b> quality model is applied on 18 open source systems. The correlation is tested between the ratings of <b>test</b> <b>code</b> quality and issue handling indicators, which are obtained by mining issue repositories. The results indicate a significant positive correlation between <b>test</b> <b>code</b> quality and issue handling performance. Furthermore, three case studies are performed on commercial systems and the model's outcome is compared to experts' evaluations. Computer ScienceSoftware TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|Abstractâ€”Automated {{testing is}} a basic {{principle}} of agile development. Its benefits include early defect detection, defect cause localization and removal of fear to apply changes to the code. Therefore, maintaining high quality <b>test</b> <b>code</b> is essential. This study introduces a model that assesses <b>test</b> <b>code</b> quality by combining source code metrics that reflect three main aspects of <b>test</b> <b>code</b> quality: completeness, effectiveness and maintainability. The model is inspired by the Software Quality Model of the Software Improvement Group which aggregates source code metrics into quality ratings based on benchmarking. To validate the model we assess the relation between <b>test</b> <b>code</b> quality, {{as measured by the}} model, and issue handling performance. An experiment is conducted in which the <b>test</b> <b>code</b> quality model is applied to 18 open source systems. The test quality ratings are tested for correlation with issue handling indicators, which are obtained by mining issue repositories. In particular, we study the (1) defect resolution speed, (2) throughput and (3) productivity issue handling metrics. The results reveal a significant positive correlation between <b>test</b> <b>code</b> quality and two out of the three issue handling metrics (throughput and productivity), indicating that good <b>test</b> <b>code</b> quality positively influences issue handling performance...|$|R
40|$|To {{diagnose}} the vulnerabilities {{of target}} system using a remote penetration test approach needs {{to avoid the}} modification of the security configuration of the target network. So the carrier of <b>test</b> <b>codes</b> needs {{to hide from the}} IDS filter out of our penetrating <b>test</b> <b>codes.</b> Hence, we proposed a polymorphic carrier which carries encrypted <b>test</b> <b>codes</b> and diversified decrypters, and, the proposed carrier is also able to adjust the OP code distribution {{to make it look like}} a normal packet. To make sure whether it can hide from the detection of an intrusion detection system and deliver the <b>test</b> <b>codes</b> to target system, we use simulation to analyze whether IDS such as STRIDE and APE can successfully detect the polymorphic <b>test</b> <b>codes</b> that we created, and whether the created carrier has a 90 % or above possibility of successful execution in target system...|$|R
40|$|Two key {{aspects of}} extreme {{programming}} (XP) are unit testing and merciless refactoring. Given {{the fact that the}} ideal <b>test</b> <b>code</b> / production code ratio approaches 1 : 1, {{it is not surprising that}} unit tests are being refactored. We found that refactoring <b>test</b> <b>code</b> is different from refactoring production code in two ways: (1) there is a distinct set of bad smells involved, and (2) improving <b>test</b> <b>code</b> involves additional test-specific refactorings. To share our experiences with other XP practitioners, we describe a set of bad smells that indicate trouble in <b>test</b> <b>code,</b> and a collection of test refactorings to remove these smells. Keywords Refactoring, unit testing, extreme programming. ...|$|R
50|$|British, German, other {{national}} and international <b>test</b> <b>codes</b> are used to standardize the procedures and definitions used to test steam turbines. Selection of the <b>test</b> <b>code</b> to be used is an agreement between the purchaser and the manufacturer, and has some significance {{to the design of}} the turbine and associated systems. In the United States, ASME has produced several performance <b>test</b> <b>codes</b> on steam turbines. These include ASME PTC 6-2004, Steam Turbines, ASME PTC 6.2-2011, Steam Turbines in Combined Cycles, PTC 6S-1988, Procedures for Routine Performance Test of Steam Turbines. These ASME performance <b>test</b> <b>codes</b> have gained international recognition and acceptance for testing steam turbines. The single most important and differentiating characteristic of ASME performance <b>test</b> <b>codes,</b> including PTC 6, is that the test uncertainty of the measurement indicates the quality of the test and is not {{to be used as a}} commercial tolerance.|$|R
40|$|This Refactoring is {{the process}} of {{changing}} a software system aimed at organizing the design of source code, making the system easier to change and less error-prone, while preserving observable behavior. This concept has become popular in Agile software methodologies, such as eXtreme Programming (XP), which maintains source code as the only relevant software artifact. Although refactoring was originally conceived to deal with source code changes. Two key aspects of eXtreme Programming (XP) are unit testing and merciless refactoring. We found that refactoring <b>test</b> <b>code</b> is different from refactoring production code in two ways: (1) there is a distinct set of bad smells involved, and (2) improving <b>test</b> <b>code</b> involves additional <b>test</b> <b>code</b> refactorings. We describe a set of code smells indicating trouble in <b>test</b> <b>code</b> and a collection of <b>test</b> <b>code</b> refactorings explaining how to overcome some of these problems through a simple program modification...|$|R
5000|$|Test spy (used for verifying [...] "indirect output" [...] of the <b>tested</b> <b>code,</b> {{by asserting}} the {{expectations}} afterwards, without having defined the expectations before the <b>tested</b> <b>code</b> is executed ,it help in recording {{information about the}} indirect object created.|$|R
50|$|Test {{methods are}} {{declared}} as such by decorating a unit test method with the TestMethod attribute. The attribute {{is used to}} identify methods that contain unit <b>test</b> <b>code.</b> Best practices state that unit test methods should contain only unit <b>test</b> <b>code.</b>|$|R
40|$|Refactoring is an {{activity}} that improves the internal structure of the code without altering its external behavior. When performed on the production <b>code,</b> the <b>tests</b> {{can be used to}} verify that the external behavior of the production code is preserved. However, when the refactoring is performed on <b>test</b> <b>code,</b> there is no safety net that assures that the external behavior of the <b>test</b> <b>code</b> is preserved. In this paper, we propose to adopt mutation testing as a means to verify if the behavior of the <b>test</b> <b>code</b> is preserved after refactoring. Moreover, we also show how this approach can be used to identify the part of the <b>test</b> <b>code</b> which is improperly refactored...|$|R
40|$|The Lewis Research Center cold-flow model {{externally}} {{blown flap}} (EBF) <b>noise</b> research <b>test</b> program is summarized. Both engine under-the-wing and over-the-wing EBF wing section configurations were studied. Ten large scale and nineteen small scale EBF models were tested. A {{limited number of}} forward airspeed effect and flap <b>noise</b> suppression <b>tests</b> were also run. The key results and conclusions drawn from the flap <b>noise</b> <b>tests</b> are summarized and discussed...|$|R
40|$|Abstractâ€”Lack of {{effective}} usage examples in API documents {{has been proven}} to be a great obstacle to API learning. To deal with this issue, several approaches have been proposed to automatically extract usage examples from client code or related web pages, which are unfortunately not available for newly released API libraries. In this paper, we propose a novel approach to mining API usage examples from <b>test</b> <b>code.</b> Although <b>test</b> <b>code</b> can be a good source of usage examples, the issue of multiple test scenarios might lead to repetitive and interdependent API usages in a test method, which make it complicated and difficult to extract API usage examples. To address this issue, we study the JUnit <b>test</b> <b>code</b> and summarize a set of <b>test</b> <b>code</b> patterns. We employ a code pattern based heuristic slicing approach to separate <b>test</b> scenarios into <b>code</b> examples. Then we cluster the similar usage examples for recommendation. An evaluation on four open source software libraries demonstrates that the accuracy of our approach is much higher than the state-of-art approach eXoaDoc on <b>test</b> <b>code.</b> Furthermore, we have developed an Eclipse plugin tool UsETeC. Keywordsâ€”API; usage example; <b>test</b> code; <b>code</b> patterns; code slicing I...|$|R
5000|$|Offers {{reports on}} {{duplicated}} code, <b>coding</b> standards, unit <b>tests,</b> <b>code</b> coverage, code complexity, comments, bugs, and security vulnerabilities.|$|R
30|$|Software {{projects}} {{often include}} <b>test</b> <b>code.</b> Test code may contain bugs, which, in theory, may be reported just like production code. However, bug localization algorithms should not include <b>test</b> <b>code</b> within their scope. Consider, for instance, three bug reports, whose resolution involved the modification of (i) only production <b>code</b> (no <b>test</b> code); (ii) production and test code; and (iii) only <b>test</b> <b>code.</b> In the first case, {{it is obvious}} that localization does not benefit from considering <b>test</b> <b>code.</b> When the resolution of a bug requires changing production and <b>test</b> <b>code</b> (second case), it is usually because a test was added or modified to catch the referred bug in the future. Test code was not the source of the failure, though. Therefore, modified test files are not what developers expect as an answer from the localization algorithm in this case. Finally, when a bug in the <b>test</b> <b>code</b> itself is caught (third case), developers already have detailed information provided by the test framework, which includes the location of the bug. Thus, even if a developer chooses to report a test bug instead of fixing it immediately, it is likely that this report will include the detailed information already provided by the test framework. Therefore, bug reports on <b>test</b> <b>code</b> are rarer (because the developer may choose to fix the bug instead of reporting it) and likely to be localized (because test frameworks already indicate the buggy files). This rationale led us to restrict the localization to production code.|$|R
5000|$|In ABAP Unit {{tests are}} test methods in {{dedicated}} test classes. A test class may contain several test methods. The optional methods SETUP (...) and TEARDOWN (...) offer {{the possibility to}} manage {{the context of the}} unit tests. Usually test classes are local classes within the program under <b>tests.</b> The domain <b>code</b> and the <b>test</b> <b>code</b> share this way the same life cycle and are always in sync. The <b>test</b> <b>code</b> can exercise the domain code of the program but not vice versa. This restriction is checked by ABAP runtime system and ensures the pattern [...] "no <b>test</b> <b>code</b> in productive code".|$|R
30|$|We have {{selected}} a <b>testing</b> <b>code</b> value.|$|R
40|$|Abstractâ€”Numerous {{software}} development practices suggest updating the <b>test</b> <b>code</b> whenever the production code is changed. However, {{previous studies have}} shown that co-evolving <b>test</b> and production <b>code</b> is generally a difficult task that needs to be thoroughly investigated. In this paper we perform a study that, following a mixed methods approach, investigates fine-grained co-evolution patterns of production and <b>test</b> <b>code.</b> First, we mine fine-grained changes from the evolution of 5 open-source systems. Then, we use an association rule mining algorithm to generate the co-evolution patterns. Finally, we interpret the obtained patterns by perform-ing a qualitative analysis. The results show 6 co-evolution patterns and provide insights into their appearance along the history of the analyzed software systems. Besides providing {{a better understanding of how}} <b>test</b> <b>code</b> evolves, these findings also help identify gaps in the <b>test</b> <b>code</b> thereby assisting both researchers and developers. I...|$|R
50|$|National and {{international}} <b>test</b> <b>codes</b> {{are used to}} standardize the procedures and definitions used in testing large condensors. In the U.S., ASME publishes several performance <b>test</b> <b>codes</b> on condensers and heat exchangers. These include ASME PTC 12.2-2010, Steam Surface Condensers,and PTC 30.1-2007, Air cooled Steam Condensers.|$|R
25|$|PTC 19.3-1974(R2004): Performance <b>test</b> <b>code</b> for {{temperature}} measurement.|$|R
40|$|AbstractTranslations and {{scalings}} defined on the Schwartz {{space of}} tempered distributions induce continuous transformations {{on the space}} of white <b>noise</b> <b>test</b> functionals [25]. Continuity of the induced transformations {{with respect to their}} parameters is proved. As a consequence one obtains a direct simple proof {{of the fact that the}} space of white <b>noise</b> <b>test</b> functionals is infinitely differentiable in FrÃ©chet sense. Moreover, it is shown that the Wiener semigroup acts as a mollifier on the space of test functionals...|$|R
30|$|Results and analysis: Table 8 summarises results {{regarding}} the collected metrics. On average, for group A, adapting OO test sets to AO implementations required additions of 5.70 %, modifications in 4.46 % of <b>test</b> <b>code</b> lines, with no code removal in any application. For group B, {{on the other}} hand, more modifications and removals were needed than for group A. On average, <b>test</b> <b>code</b> was 9.57 % modified and 3.10 % removed to conform with OO implementations, while only 1.93 % lines {{were added to the}} <b>test</b> <b>code.</b>|$|R
5000|$|PTC 19.2-2010 : Performance <b>test</b> <b>code</b> for {{pressure}} measurement.|$|R
40|$|A ring-plate-type cycloid {{speed reducer}} {{is one of}} the most {{important}} reducers owing to its low volume, compactness, smooth and high performance, and high reliability. The vibration and <b>noise</b> <b>tests</b> of the reducer prototype are completed using the HEAD acoustics multichannel <b>noise</b> <b>test</b> and analysis system. The characteristics of the vibration and noise are obtained based on coherence analysis and the noise sources are identified. The conclusions provide the bases for further noise research and control of the ring-plate-type cycloid reducer...|$|R
5000|$|... static {{instrumentation}} {{which is}} done upfront, changing the <b>tested</b> <b>code</b> ...|$|R
40|$|Background: One of {{the most}} common {{complaints}} expressed by individuals with hearing-impairment is the difficulty in speech perception in background <b>noise.</b> Different <b>tests</b> have been developed for the evaluation of reduced ability of speech perception in noise, and the Consonant-Vowel in <b>noise</b> <b>test</b> is one of the simplest one regard to speech materials. The goal {{of the present study was}} development and determined validity and reliability of the Persian version of the Consonant-Vowel in <b>noise</b> <b>test,</b> among 18 to 25 year old Persian speaking because of the lack of a Persian version of this test. Methods: This was a tool-making research that had 3 main stages: development of the Persian version of the Consonant-Vowel in <b>noise</b> <b>test</b> (4 lists and each list in 5 different signal to noise ratio), examination of its content validity, and its administration on a total of 50, 18 to 25 year normal hearing individuals (20 men / 30 women) that selected by random sampling method, in order to examine the reliability of the test from the students of the University of Social Welfare and Rehabilitation Sciences, Tehran, Iran. For descriptive reports, central tendencies and indices of dispersion were used and for statistics relations; Pearson correlation test, intra-class correlation coefficient (ICC), paired t-test and independent t-test were used. Results: The content validity ratio for each item was acceptable (CVR> 0. 62). The lists number 2, 3, and 4; and also the lists number 1 and 4 in the ConsonantVowel in <b>noise</b> <b>test,</b> were highly correlated (P 0. 05) and also menâ€™s and womenâ€™s scores (P> 0. 05). Participantâ€™s performance improved as the SNR increased. Conclusion: According to the study results, it can be concluded that the Persian version of the Consonant-Vowel in <b>noise</b> <b>test</b> has acceptable content validity and reliability, and can be used in clinical and research works...|$|R
5000|$|Additional plant <b>test</b> <b>codes</b> {{may be in}} use locally in some areas: ...|$|R
5000|$|Test stub (used for {{providing}} the <b>tested</b> <b>code</b> with [...] "indirect input") ...|$|R
5000|$|Interactive console to {{directly}} <b>test</b> <b>code</b> snippets with local and remote execution ...|$|R
