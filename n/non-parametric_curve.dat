12|36|Public
40|$|Abstract. In the {{following}} we unite <b>non-parametric</b> <b>curve</b> fitting with dynamic estimation {{as a way}} of empirically generating densities from data with potentially unusual and time-varying structure. We include two examples to illustrate the construction and optimization of the joint density of a scalar predictor and scalar response over different cost functions. ...|$|E
40|$|Abstract: The idea of {{volatility}} {{is fundamental}} to precise definition of risk and, hence, its estimation (or prediction) {{is a very important}} task in finance applications. We present some ideas on non-parametric estimation of volatility function in diffusion models. A nonlinear wavelet estimate of the volatility function is proposed and its performance is compared to three kernel estimators in both simulated and real data. Simulation is developed for eight volatility shapes and some interesting, but not unexpected, results are presented. Some issues such as online estimation and prediction, robustness to oversmoothing and performance under sudden changes in pattern of volatility are also discussed. Key words: High frequency financial series, <b>non-parametric</b> <b>curve</b> esti-mation, volatility estimation, wavelets. ...|$|E
40|$|In {{this note}} we show that, from a {{conventional}} viewpoint, there are particularly close parallels between optimal-kernel-choice problems in non-parametric deconvolution, and their better-understood counterparts in density estimation and regression. However, {{other aspects of}} these problems are distinctly different, and this property leads us to conclude that "optimal" kernels do not give satisfactory performance when applied to deconvolution. This unexpected result {{stems from the fact}} that standard side conditions, which are used to ensure that the familiar kernel-choice problem has a unique solution, do not have statistically beneficial implications for deconvolution estimators. In consequence, certain "sub-optimal" kernels produce estimators that enjoy both greater efficiency and greater visual smoothness. Bandwidth Ill-posed problem Inverse problem Kernel density estimation Mean integrated squared error <b>Non-parametric</b> <b>curve</b> estimation Statistical smoothing...|$|E
40|$|Fitting a curve {{through a}} set of planar data which {{represents}} a positive quantity requires that the curve stays above the horizontal axis, The more general problem of designing parametric and <b>non-parametric</b> <b>curves</b> which do not cross the given constraint boundaries is considered. Several methods will be presented...|$|R
40|$|Abstract. Fit/inK a curve throuKh {{a set of}} planar data which {{represents}} a po. ritive quantiry requires that rhe curve stays above the horizonwi axis, The more general problem of desiKning parametric and <b>non-parametric</b> <b>curves</b> which do nol cross the Kiven constraint boundaries is considered. Several methods will be presented,...|$|R
40|$|The {{estimation}} of the homography between two views is a key step in many applications involving multiple view geometry. The homography exists between two views between projections of points on a 3 D plane. A homography exists also between projections of all points if the cameras have purely rotational motion. A number of algorithms have been proposed for the {{estimation of}} the homography relation between two images of a planar scene. They use features or primitives ranging from simple points to a complex ones like <b>non-parametric</b> <b>curves.</b> Different algorithms make different assumptions on the imaging setup and {{what is known about}} them. This article surveys several homography estimation techniques from the literature. The essential theory behind each method is presented briefly and compared with the others. Experiments aimed at providing a representative analysis and comparison of the methods discussed are also presented in the paper. ...|$|R
40|$|In fitting a curve to {{observations}} {{of an independent}} variable, y, at design points, x, one has two main purposes. Firstly exploring and representing {{the relationship between the}} design and response variable; secondly allowing predictions to be made for future {{observations of}} the response variable. Typically a parametric curve is used to t the relationship but this forces the model for the relationship into a rigidly de ned class of models. Non-parametric models for the curve allow much more exibility in the fitted model. A cubic spline curve (Reinsch, 1967; Rice and Rosenblatt, 1983) is a popular <b>non-parametric</b> <b>curve</b> which is very exible but still has useful smoothing properties. A good review of spline smoothing can be found in Silverman (1985) ...|$|E
40|$|The {{purpose of}} this {{contribution}} is to point out and exploit the kinship between identification of linear dynamic systems and classical curve fitting. For curve fitting we discuss both global and local parametric methods as well as non-parametric ones, such as local polynomial methods. We view system identification as the estimation of the frequency function curve. The empirical transfer function estimate is taken as the "observations" of this curve. In particular we discuss how this could be done for multi-variable systems. Local and <b>non-parametric</b> <b>curve</b> fitting methods lead to variants of classical spectral analysis, while the prediction error/maximum likelihood framework corresponds to global parametric methods. The role of the noise model in dynamic models is also illuminated from this perspective...|$|E
40|$|ABSTRACT. Weconsider {{generalized}} {{linear models}} {{in which the}} linear predictor is of '. 9 A additive semi-parametric form, linear {{in most of the}} explanatory variables but with an arbitrary functional dependence on the remainder. Estimation of the parameters and the <b>non-parametric</b> <b>curve</b> in the model is approached by maximizing a penalized likelihood. Two explicit iterative algorithms are presented. The first, which operates in O(n) time per iteration, applies where there is just one variable entering the model in a non-parametric fashion, and an integrated squared second derivative penalty is used. An example in logistic regression of tumour prevalence is given. The second algorithm is for the much more general case of a regression model specified as an arbitrary composite log-likelihood function, permitting nonlinear dependence and several splined variables.) ...|$|E
40|$|For {{the study}} of the law of displacement, any type of {{function}} can be used. Commonly it has been usedpolynomial, sinusoidal and cyclic functions. The traditional methods utilize algebraic polynomials with canonical base and trigonometric polynomials with Fourier base, being these used in almost all the texts. The importance of the present work is the utilization of the <b>non-parametric</b> Beziers <b>curves</b> defined by algebraic polynomials with Bernstein base for the law of displacement...|$|R
30|$|For the <b>non-parametric</b> or {{observed}} <b>curves</b> we {{used the}} Kaplan-Meier product limit estimator for the survival function, [31], the Nelson-Aalen moments estimator for the accumulated hazard function [32], and the kernel density estimator for the hazard rate function, [33].|$|R
40|$|We thank Igor B. Morozov for his {{interest}} in our article and for his comment (Morozov 2011) regarding the <b>non-parametric</b> attenuation <b>curves</b> for the Irpinia–Basilicata region obtained by generalized spectral inversion (Cantore et al. 2011). Morozov's comment has its root in a new model proposed by Morozov (2008, 2010) for the interpretation of seismic attenuation data, where the author comes {{to the conclusion that}} the typically used geometrical spreading terms are oversimplified and argues in favor of a new geometrical spreading [...] ...|$|R
40|$|Kingman’s coalescent process {{opens the}} door for {{estimation}} of population genetics model parameters from molecular sequences. One paramount parame-ter of interest is the effective population size. Temporal variation of this quan-tity characterizes the demographic history of a population. Since researchers are rarely able to choose a priori a deterministic model describing effective pop-ulation size dynamics for data at hand, <b>non-parametric</b> <b>curve</b> fitting methods based on multiple change-point (MCP) models have been developed. We pro-pose an alternative to change-point modeling that exploits Gaussian Markov random fields to achieve temporal smoothing of the effective population size in a Bayesian framework. The main advantage of our approach is that, in con-trast to MCP models, the explicit temporal smoothing does not require strong prior decisions. To approximate the posterior distribution of the population dynamics, we use efficient, fast mixing MCMC algorithms designed for highly structured Gaussian models. In a simulation study, we demonstrate that th...|$|E
40|$|By {{means of}} a {{detailed}} analysis of the returns of the Standard & Poors 500 (S&P 500) composite stock index over the last fifty years we show how theoretical results and methodological recommendations from the statistical theory of <b>non-parametric</b> <b>curve</b> inference allow one to consistently estimate expected return and volatility. In this approach we do not postulate an apriori relationship risk-return nor do we specify the evolution of the first two moments through covariates. Our analysis gives statistical evidence that the expected return of the S&P 500 index as well as the market price of risk (the ratio expected return minus risk free interest rate over volatility) vary significantly through time both in size and sign. In particular, the periods of negative (positive) estimated expected return and market price of risk coincide with the bear (bull) markets of the index as defined in the literature. A complex relationship between risk and expected return emerges which is far from the common assumption of a positive linear time-invariant relation...|$|E
40|$|Magnetic {{resonance}} imaging (MRI) {{is a key}} tool for non-invasive spinal cord lesion analysis; however, accurate, quantitative methods for this analysis are lacking. A new, multi-step, multidimensional approach, utilizing the Classification Expectation Maximization (CEM) algorithm, is proposed for MRI segmentation of spinal cord tissues. Diffusion tensor imaging is used to generate multiple images of each spinal slice with different diffusion direction weightings. The maximum likelihood tissue classifications are then jointly estimated to produce a binary classification image, corresponding to voxels containing either spinal cord or background. Edge detection is employed to find a <b>non-parametric</b> <b>curve</b> encapsulating the entire spinal cord. The algorithm is evaluated using data from in vivo DTI of control and injured mouse spinal cords. The algorithm is shown to remain accurate for whole spinal cord, white matter, and hemorrhage segmentation {{in the presence of}} significant injury. The results of the method are shown to be at least on par with expert manual segmentation. Keywords spinal cord injury; diffusion tensor imaging (DTI); tissue classification; classification expectatio...|$|E
40|$|We {{present an}} arbitrage-free <b>non-parametric</b> yield <b>curve</b> {{prediction}} model which takes the full (discretized) yield curve as state variable. We believe that absence of arbitrage {{is an important}} model feature in case of highly correlated data, {{as it is the}} case for interest rates. Furthermore, the model structure allows to separate clearly the tasks of estimating the volatility structure and of calibrating market prices of risk. The empirical part includes tests on modeling assumptions, back testing and a comparison with the Vasiček short rate model...|$|R
40|$|This text {{features}} {{a broad array}} of research efforts in computer vision including low level processing, perceptual organization, object recognition and active vision. The volume's nine papers specifically report on topics such as sensor confidence, low level feature extraction schemes, <b>non-parametric</b> multi-scale <b>curve</b> smoothing, integration of geometric and non-geometric attributes for object recognition, design criteria for a four degree-of-freedom robot head, a real-time vision system based on control of visual attention and a behavior-based active eye vision system. The scope of the book p...|$|R
40|$|Identification of ice floes {{and their}} {{outlines}} in satellite images {{is important for}} understanding physical processes in the polar regions, for transportation in ice-covered seas and {{for the design of}} offshore structures intended to survive in the presence of ice. At present this is done manually, a long and tedious process which precludes full use of the great volume of relevant images now available. We describe an automatic and accurate method for identifying ice floes and their outlines. Floe outlines are modeled as closed principal curves (Hastie and Stuetzle, 1989), a flexible class of smooth <b>non-parametric</b> <b>curves.</b> We propose a robust method of estimating closed principal curves which reduces both bias and variance. Initial estimates of floe outlines come from the erosion-propagation (EP) algorithm, which combines erosion from mathematical morphology with local propagation of infonnation about floe edges. The edge pixels from the EP algorithm are grouped into floe outlines using a new clustering algorithm. This extends existing clustering methods by allowing groups to be centered about principal curves rather than points or lines. This may open the way to efficient feature extraction using cluster analysis in images more generally. The method is implemented in an objectoriented programming environment for which it is well suited, and is quite computationally efficient...|$|R
40|$|Imprecise {{theories}} do {{not give}} enough guidelines for empirical analyses. A paradigmatic shift from linear to curvilinear relationships is necessary to advance management theories. Within {{the framework of the}} abductive generation of theories, the authors present a data exploratory technique for the identification of functional relationships between variables. Originating in medical-research, the method uses fractional polynomials to test for alternative curvilinear relationships. It is a compromise between <b>non-parametric</b> <b>curve</b> fitting and conventional polynomials. The multivariable fractional polynomial (MFP) technique is a good tool for exploratory research when theoretical knowledge is non-specific and thus, very useful in phenomena discovery. The authors conduct simulations to demonstrate MFP’s performance in various scenarios. The technique’s major benefit is the uncovering of non-traditional shapes that cannot be modeled by logarithmic or quadratic functions. While MFP is not suitable for small samples, there {{does not seem to be}} a downside of overfitting the data as the fitted curves are very close to the true ones. The authors call for a routine application of the procedure in exploratory studies involving medium and large sample sizes...|$|E
40|$|A brief {{review of}} curve fitting {{terminology}} is presented, and the cubic spline interpolation scheme is outlined. Parametric and <b>non-parametric</b> <b>curve</b> fitting techniques are compared. The technique to fit parametric cubic splines is derived using the Euler- Lagrange formulation. Previous work on splines in tension is identified. Employing the notion of splines in tension, a method is proposed to fit a parametric curve {{to a set of}} (n + 1) points in ^-dimension space satisfying a specified set of boundary conditions. The curve fitted will not have any inflection points within any span and will be invariant with respect to coordinate translation and rotation. Using Euler-Lagrange formulation, a system of linear equations in terms of the unknown second derivatives at knots is developed. Three kinds of boundary conditions are investigated. Software is developed in VAX Fortran to fit both parametric splines in tension and parametric cubic splines. Applications where splines in tension may find use are identified. Some examples of such applications are presented and comparison to cubic spline made. Splines in tension offer a better alternative than Fourier transform in describing boundary of shape in digital image processing application. Possible extensions to the numerical scheme developed and related investigations by other workers in this field are also listed...|$|E
40|$|Multilevel {{growth curve}} models are {{appropriate}} methods of modelling longitudinal data. This method requires {{the estimation of}} individual growth trajectories as advocated by several authors (Rogosa, 1995; Singer & Willett, 2003; Willett, 1997). The three-level quadratic growth curve models {{in the present study}} are multilevel because measurement occasions are nested within students within schools. This paper refers to “student growth” as the growth parameters or slope and “student status” as the intercept in growth curve model. Generally school effects are measured as the percentage of the total variance (status or growth) that is situated at the school level. Though there are several methods to estimate school effects, the intraclass correlation coefficient is used most frequently in educational effectiveness research. The estimates of the variances obtained depend very strongly on the assumptions and specifications made about the different sources of random variation in the model. Since school effects estimates are obtained from the estimates of the variances at the different levels, errors in their estimations are carried over to the school effect estimation. This paper illustrates how these estimates can be corrected by using methods that account for multilevel serial correlation. In order to better digest the main contributions of this paper, a qualitative discussion of some sources of random variation is presented as in Liang and Zeger (1994) and Verbeke and Molenberghs (2000) that can be of great interest in statistical analysis in this longitudinal study. The extension of these sources of variability to three-level growth curves is thoroughly illustrated with emphasis on serial correlation. A proposed multilevel linear mixed model (three-level growth curve model) extended to include two levels of serial correlation is discussed in detail. Some background theory of semi-variogram for random intercepts models, similar methods for random intercepts and slopes for two-level models are explored (Verbeke, Lesaffre & Brant, 1998). A Method for random intercepts and slopes for three-level models is discussed as an extension. Finally an application of this proposed method to a real dataset is illustrated. The semi-variogram method is used to explore the presence of serial correlation. It is especially used to describe the association among repeated values and easily estimated with irregular observation times (Diggle, 1990). The semi-variogram v(u) sometimes also called sample variogram (Diggle, Liang, and Zeger, 1994) is estimated from the data, that is with the plot of half-squared differences between pairs of residuals and the time lags. This is usually achieved by fitting a <b>non-parametric</b> <b>curve.</b> An example of such a <b>non-parametric</b> <b>curve</b> is the loess smoothing which is used often in statistical literature because of it great combination of classical methods like linear and nonlinear regression (Cleveland, Devlin, 1988). A double serial correlation method is proposed for a three-level quadratic growth curve model. This method is used to demonstrate {{for the first time the}} complex structure of serial correlation at student and school level and how this corrects the estimations of school effects. The application of this method to a real dataset indicates an enormous improvement in the school effects estimate. School effects estimates are obtained and compared for three different situations of serial correlation. Serial correlation at student level only, school level only and both. status: publishe...|$|E
40|$|This paper {{examines}} {{changes in}} the distribution of income and wealth in Finland using Wealth surveys from 1987, 1994 and 1998. The inequality of both disposable income and of gross wealth has increased substantially over the time period covered, as has the dependence of income on wealth, as measured by the conditional <b>non-parametric</b> mean <b>curve.</b> Regression models based on the Gamma distribution suggest that the bulk of the increased inequality stems from increases in residual dispersion and bivariate analysis suggests that the residual correlation of income and wealth has also increased. ...|$|R
30|$|Standard {{descriptive}} statistics {{were used to}} examine the distribution of key variables (age, gender, CD 4 count at baseline and follow-up, and failure rate) in the sample. Due to the small sample size in this study (n[*]=[*] 16), the relationship of CD 4 count and failure will be examined graphically and cases which failed are discussed individually in the “Results” section. A bar chart is presented {{to examine the relationship between}} implant survival and CD 4 count at baseline. A <b>non-parametric</b> survival <b>curve</b> (using the Kaplan-Meier method) is estimated for individual implants (n[*]=[*] 33), which depicts the relationship between implant survivorship and time since procedure.|$|R
40|$|We {{present a}} novel {{smoothing}} approach to <b>non-parametric</b> regression <b>curve</b> fitting. This {{is based on}} kernel partial least squares (PLS) regression in reproducing kernel Hilbert space. It is our concern to apply the methodology for smoothing experimental data where some level {{of knowledge about the}} approximate shape, local inhomogeneities or points where the desired function changes its curvature is known a priori or can be derived based on the observed noisy data. We propose locally-based kernel PLS regression that extends the previous kernel PLS methodology by incorporating this knowledge. We compare our approach with existing smoothing splines, hybrid adaptive splines and wavelet shrinkage techniques on two generated data sets...|$|R
40|$|We {{propose a}} new model {{to make use of}} geo-referenced genetic data for {{inferring}} the location and shape of a hybrid zone. The model output includes the posterior distribution of a parameter that quan-tifies the width of the hybrid zone. The model proposed is implemented in the GUI and command-line versions of the Geneland program versions ≥ 3. 3. 0. Information about the program can be found on www 2. imm. dtu. dk/~gigu/Geneland/ Background Hybrid zones have been the object of considerable attention as they are seen as windows on the evolutionary process (Harrison, 1990) and inference about genetic structure in their neighbourhood can provide valuable insights about the intensity of selection. This is made possible through the existence of explicit models of cline shapes as a function of selection (Haldane, 1948; Bazykin, 1969; Kruuk et al, 1999). To analyse hybrid zones, scientists have relied on a variety of approaches. They can use hybrid zones models that predict patterns of allele frequencies and fit corresponding parametric curves (Analyse program, Barton & Baird, 1998) or <b>non-parametric</b> <b>curves</b> (Macholán et al, 2008). They can also use general purpose computer programs such as Structure (Pritchard et al, 2000) that seek patterns in ancestries of individuals without reference to any model of hybrid zones. Here we propose a new spatial model that combines features of both approaches: it explicitly accounts for the presence of a cline without making restrictive assumption about the shape of the cline path and it also retains the flexibility of the admixture model of Structure...|$|R
40|$|This paper {{compares the}} {{distribution}} of money income and full income across households in the United States. The concept of full income was introduced in Becker's household model and provides a framework for estimating the economic value of productive non-market activities and leisure. If the allocation of time is voluntary, full income {{may be a better}} measure of economic welfare than money income. <b>Non-parametric</b> Lorenz <b>curves</b> and Gini coefficients are used to compare the two distributions. The data are from the Census Bureau's Survey of Income and Program Participation for 1984 - 86. Full income is more equally distributed than money income. However, the distribution remains very unequal. The income distributions are also compared for specific types of households...|$|R
40|$|We {{estimate}} a {{food demand}} system for Slovakia using a recent Household Budget Survey {{data collected by}} the Slovak statistical office covering the period 2004 – 2010. The Quadratic Almost Ideal Demand System (QUAIDS) augmented with demographic, regional and expenditure controls is employed based on preliminary <b>non-parametric</b> Engel <b>curve</b> analysis. Results indicate that demand for dairy products and fruits and vegetables is expenditure and own-price elastic indicating that such goods are perceived as luxuries. On the other hand, commodity bundles such as cereals, meat and fish and other food {{are found to be}} normal goods with positive budget elasticity smaller than one and price inelastic demand. Rural and low-income households appear more expenditure and, especially, price sensitive compared with the urban and high-income ones. Overall the food consumption patterns changed and food security situation improved in Slovakia between 2004 and 2010...|$|R
40|$|In this {{contribution}} {{we review}} which species richness estimators {{can be used}} if spiders are sampled with pitfall traps or other relative sampling methods. Due to the inherent bias typical for activity-based traps, only sample-based estimators were useful (Chao 2, ICE and two jackknifing estimates). We estimated species richness for spiders from our four fragmented coastal dune habitats: dense and short grasslands, moss dunes and marram dunes. As extrapolation of the collector curve is only appropiate if the curve reaches a ceiling (asymptotic, such as parabolic and hyperbolic models), total species richness could not be determined by extrapolation in either of the investigated habitats. Because of the log-linear nature of the <b>curves,</b> <b>non-parametric</b> methods were applied. An absolute estimate of total habitat species richness was thus difficult to calculate; differences in total regional species richness could be analysed by comparing the different <b>non-parametric</b> estimator <b>curves.</b> If only habitat specific species were taken into account (we analysed this for grey dune habitats), extrapolation of the collector curve was appropiate (hyperbolic models). A relatively low number of traps were already enough for sampling more than 95 % of the specific species. Estimates of the richness of specific species from grey dune patches (in contrast to species richness of the habitat) could already be derived from five traps. Our data revealed that grassland habitats were characterised by higher spider richness than moss-dominated grey dunes and that the latter were more diverse than Marram dunes. The number of specific species was slightly, although non-significantly, larger in dune grasslands than in moss-dominated dunes...|$|R
40|$|The constant-breadth cam mechanisms, which drive a {{parallel}} flat-faced double follower, are desmodromic, and guarantee global bilaterality. In the said mechanisms, {{the law of}} displacement of the follower can only be freely designed for an interval of the rotation angle of the cam – the designed segment – close or equal to 180 °, and the remaining interval – the calculated segment – is obtained through calculation from the first. Guaranteeing the continuity between segments is not a trivial task. This work shows a design procedure which guarantees automatically the global continuity {{of the law of}} displacement. This procedure provides the expressions of calculation for both parallel flat-faced double translating follower and parallel flat-faced double oscillating follower. <b>Non-parametric</b> Bézier <b>curves</b> are used for the definition of the displacement functions. For both types of followers, two numerical examples of the design of the displacement functions are given. Furthermore, the corresponding cam profiles are provided. Postprint (published version...|$|R
40|$|Sensor {{networks}} {{have emerged as}} a fundamentally new tool for monitoring spatial phenomena. This paper describes a theory and methodology for estimating inhomogeneous, two-dimensional fields using wireless sensor networks. Inhomogeneous fields are composed {{of two or more}} homogeneous (smoothly varying) regions separated by boundaries. The boundaries, which correspond to abrupt spatial changes in the field, are <b>non-parametric</b> one-dimensional <b>curves.</b> The sensors make noisy measurements of the field, and the goal is to obtain an accurate estimate of the field at some desired destination (typically remote from the sensor network). The presence of boundaries makes this problem especially challenging. There are two key questions: 1. Given n sensors, how accurately can the field be estimated? 2. How much energy will be consumed by the communications required to obtain an accurate estimate at the destination? Theoretical upper and lower bounds on the estimation error and energy consumption are given. A practical strategy for estimation and communication is presented. The strategy, based on a hierarchical data-handling and communication architecture, provides a near-optimal balance of accuracy and energy consumption...|$|R
40|$|Abstract — Sensor {{networks}} {{have emerged as}} a fundamentally new tool for monitoring spatial phenomena. This paper describes a theory and methodology for estimating inhomogeneous, two-dimensional fields using wireless sensor networks. Inhomogeneous fields are composed {{of two or more}} homogeneous (smoothly varying) regions separated by boundaries. The boundaries, which correspond to abrupt spatial changes in the field, are <b>non-parametric</b> onedimensional <b>curves.</b> The sensors make noisy measurements of the field, and the goal is to obtain an accurate estimate of the field at some desired destination (typically remote from the sensor network). The presence of boundaries makes this problem especially challenging. There are two key questions: 1. Given n sensors, how accurately can the field be estimated? 2. How much energy will be consumed by the communications required to obtain an accurate estimate at the destination? Theoretical upper and lower bounds on the estimation error and energy consumption are given. A practical strategy for estimation and communication is presented. The strategy, based on a hierarchical datahandling and communication architecture, provides a nearoptimal balance of accuracy and energy consumption. Index Terms — sensor networks, distributed estimation, wavelets, multiresolution analysis I...|$|R
40|$|Call centres are {{becoming}} increasingly important in our modern commerce. We are interested in modelling the time-varying pattern of average customer service times at a bank call centre. Understanding such a pattern is essential for efficient operation of a call centre. The call service times are shown to be lognormally distributed. Motivated by this observation and the important application, we propose a new method for inference about <b>non-parametric</b> regression <b>curves</b> when the errors are lognormally distributed. Estimates and pointwise confidence bands are developed. The method builds upon the special relationship between the lognormal distribution and the normal distribution, and improves upon a naive estimation procedure that ignores this distributional structure. Our approach includes local non-parametric estimation for both the mean function and the heteroscedastic variance function of the logged data, and uses local polynomial regression as a fitting tool. A simulation study is performed to illustrate the method. We then apply the method to model the time-varying patterns of mean service times for different types of customer calls. Several operationally interesting findings are obtained and discussed. Copyright © 2006 John Wiley & Sons, Ltd. Link_to_subscribed_fulltex...|$|R
40|$|This paper {{describes}} an algorithm {{to determine whether}} a point is inside or outside a curvilinear polygon, based on the well known algorithm consisting of counting the intersections of an horizontal ray with the polygon. Curvilinear polygons. as analytic resources. are not common in computer graphics, but they do arise on sorne domains. Normally, parametric curves, such as cubic splines or beziers, are used to model curves in graphic applications. But parametric curves cannot be easily manipulated in analytic computations. For example, obtaining a curve parallel to another is not a simple task, and is not even possible under all circumstances. For this reason, sorne computational geometry applications use <b>non-parametric</b> polyline <b>curves,</b> formed by straight-Iine segments and circular arcs. The straightforward geometry of lines and circumferences makes it easy to develop complex algorithms to work with nonparametric curves. But despite the fact that this curves are easy to treat analytically, they lack the parametric benefits of c 1 assical curves; and for this reason, {{it is very hard to}} find in the literature any work about them. If we are to represent curvilinear polygons using patches of straight-line segments and circular arcs, named polyline curvilinear polygons, one ofthe fundamental óperations that we need to implement is the point-in-polygon test. Haven't been able to find any such algorithm. the author adapted the c 1 assical method used with straight-Iine polygons, and extended it for use with polyline curves. Eje: Ingeniería del software. Computación gráfica y visualizació...|$|R
40|$|Abstract — This paper {{proposes a}} {{deterministic}} observer framework for visual tracking based on <b>non-parametric</b> implicit (level-set) <b>curve</b> descriptions. The observer is continuousdiscrete, with continuous-time system dynamics and discretetime measurements. Its state-space {{consists of an}} estimated curve position augmented by additional states (e. g., velocities) associated with every point on the estimated curve. Multiple simulation models are proposed for state prediction. Measurements are performed through standard static segmentation algorithms and optical-flow computations. Special emphasis {{is given to the}} geometric formulation of the overall dynamical system. The discrete-time measurements lead to the problem of geometric curve interpolation and the discrete-time filtering of quantities propagated along with the estimated curve. Interpolation and filtering are intimately linked to the correspondence problem between curves. Correspondences are established by a Laplace-equation approach. The proposed scheme is implemented completely implicitly (by Eulerian numerical solutions of transport equations) and thus naturally allows for topological changes and subpixel accuracy on the computational grid. I...|$|R
40|$|URL des Documents de travail : [URL] de travail du Centre d'Economie de la Sorbonne 2015. 86 RR - ISSN: 1955 - 611 X - Version originale Septembre 2015, révisée en juillet 2016, révisée en février 2017 Ancien titre : "A fully {{non-parametric}} heteroskedastic model". In this paper, {{we propose}} an innovative methodology for modelling the news impact curve. The news impact curve provides a non-linear relation between past returns and current volatility and thus enables to forecast volatility. Our news impact curve {{is the solution}} of a dynamic optimization problem based on variational calculus. Consequently, it is a <b>non-parametric</b> and smooth <b>curve.</b> To our knowledge, {{this is the first}} time that such a method is used for volatility modelling. Applications on simulated heteroskedastic processes as well as on financial data show a better accuracy in estimation and forecast for this approach than for standard parametric (symmetric or asymmetric ARCH) or non-parametric (Kernel-ARCH) econometric techniques...|$|R
40|$|We {{determine}} 37 differential extinctions in 23 {{gravitational lens}} galaxies {{over the range}} 0 < ∼ zl < ∼ 1. Only 7 of the 23 systems have spectral differences consistent with no differential extinction. The median differential extinction for the optically-selected (radio-selected) subsample is ∆E(B − V) = 0. 04 (0. 06) mag. The extinction is patchy and shows no correlation with impact parameter. The median total extinction of the bluest images is E(B − V) = 0. 08 mag, although the total extinction distribution {{is dominated by the}} uncertainties in the intrinsic colors of quasars. The directly measured extinction distributions are consistent with the mean extinction estimated by comparing the statistics of quasar and radio lens surveys, thereby confirming the need for extinction corrections when using the statistics of lensed quasars to estimate the cosmological model. A disjoint subsample of two face-on, radio-selected spiral lenses shows both high differential and total extinctions, but standard dust-to-gas ratios combined with the observed molecular gas column densities overpredict the amount of extinction by factors of 2 – 5. For several systems we can estimate the extinction law, ranging from RV = 1. 5 ± 0. 2 for a zl = 0. 96 elliptical, to RV = 7. 2 ± 0. 1 for a zl = 0. 68 spiral. For the four radio lenses where we can construct <b>non-parametric</b> extinction <b>curves</b> we find no evidence for gray dust over the IR–UV wavelength range. The dust can be used to estimate lens redshifts with reasonable accuracy, although we sometimes find two degenerate redshift solutions...|$|R
