219|0|Public
25|$|The Theil {{index is}} a {{statistic}} primarily {{used to measure}} economic inequality and other economic phenomena, though {{it has also been}} used to measure racial segregation. The basic Theil index TT is the same as redundancy in information theory which is the maximum possible entropy of the data minus the observed entropy. It is a special case of the generalized entropy index. It {{can be viewed as a}} measure of redundancy, lack of diversity, isolation, segregation, inequality, <b>non-randomness,</b> and compressibility. It was proposed by econometrician Henri Theil at the Erasmus University Rotterdam.|$|E
5000|$|In {{practice}} {{the number of}} shuffles required depends both {{on the quality of}} the shuffle and how significant <b>non-randomness</b> is, particularly how good the people playing are at noticing and using <b>non-randomness.</b> Two to four shuffles is good enough for casual play. But in club play, good bridge players take advantage of <b>non-randomness</b> after four shuffles, and top blackjack players supposedly track aces through the deck; this is known as [...] "ace tracking", or more generally, as [...] "shuffle tracking".|$|E
5000|$|... {{combination}} PRNGs which {{attempt to}} combine several PRNG primitive algorithms {{with the goal}} of removing any <b>non-randomness</b> ...|$|E
50|$|Another {{school of}} thought, {{behavioral}} finance, attributes <b>non-randomness</b> to investors' cognitive and emotional biases. This can be contrasted with fundamental analysis.|$|E
5000|$|In practice, [...] {{is set to}} 0.2, 0.3 or 0.48. The latter {{value is}} {{frequently}} used for aqueous systems. The high value reflects the ordered structure caused by hydrogen bonds. However, {{in the description of}} liquid-liquid equilibria the <b>non-randomness</b> parameter is set to 0.2 to avoid wrong liquid-liquid description. In some cases a better phase equilibria description is obtained by setting [...] However this mathematical solution is impossible from a physical point of view, since no system can be more random than random ( [...] =0). In general NRTL offers more flexibility in the description of phase equilibria than other activity models due to the extra <b>non-randomness</b> parameters. However, in practice this flexibility is reduced in order to avoid wrong equilibrium description outside the range of regressed data.|$|E
5000|$|The {{parameters}} [...] and [...] are {{the so-called}} <b>non-randomness</b> parameter, for which usually [...] is set equal to [...] For a liquid, {{in which the}} local distribution is random around the center molecule, the parameter [...] In that case the equations reduce to the one-parameter Margules activity model: ...|$|E
50|$|Random number {{generation}} {{may also be}} performed by humans, {{in the form of}} collecting various inputs from end users and using them as a randomization source. However, most studies find that human subjects have some degree of <b>non-randomness</b> when attempting to produce a random sequence of e.g. digits or letters. They may alternate too much between choices when compared to a good random generator; thus, this approach is not widely used.|$|E
50|$|The {{disadvantage}} of {{the indirect method}} is {{that in some cases}} it can underestimate the value of LAI in very dense canopies, as it does not account for leaves that lie on each other, and essentially act as one leaf according to the theoretical LAI models. Ignorance of <b>non-randomness</b> within canopies may cause underestimation of LAI up to 25%, introducing path length distribution in the indirect method can improve the measuring accuracy of LAI.|$|E
50|$|The {{generalized}} entropy index {{has been}} proposed {{as a measure of}} income inequality in a population. It is derived from information theory as a measure of redundancy in data. In information theory a measure of redundancy can be interpreted as <b>non-randomness</b> or data compression; thus this interpretation also applies to this index. In additional interpretation of the index is as biodiversity as entropy has also been proposed as a measure of diversity.|$|E
50|$|The Theil {{index is}} a {{statistic}} primarily {{used to measure}} economic inequality and other economic phenomena, though {{it has also been}} used to measure racial segregation. The basic Theil index TT is the same as redundancy in information theory which is the maximum possible entropy of the data minus the observed entropy. It is a special case of the generalized entropy index. It {{can be viewed as a}} measure of redundancy, lack of diversity, isolation, segregation, inequality, <b>non-randomness,</b> and compressibility. It was proposed by econometrician Henri Theil at the Erasmus University Rotterdam.|$|E
5000|$|The Kurtz-Zelen-Abell {{analysis}} {{had split}} the sample primarily {{to examine the}} randomness of the 303 selected champions, the <b>non-randomness</b> of which Rawlins demonstrated in 1975 and 1977. Zelen's 1976 [...] "Challenge to Gauquelin" [...] had stated: [...] "We now have an objective way for unambiguous corroboration or disconfirmation ... to settle this question", whereas this aim was now disputed. Rawlins made procedural objections, stating; [...] "... we find an inverse correlation between size and deviation in the Mars-athletes subsamples (that is, the smaller the subsample, the larger the success) - which is what one would expect if bias had infected the blocking off of the sizes of the subsamples".|$|E
50|$|Epidemic routing is {{particularly}} resource hungrybecause it deliberately makes {{no attempt to}} eliminate replicationsthat {{would be unlikely to}} improve the delivery probability of messages.This strategy is effective if the opportunistic encounters between nodesare purely random, but in realistic situations, encounters are rarely totally random. Data Mules (mostly associated with a human) move in a society andaccordingly tend to have greater probabilities of meeting certain Mules than others. The Probabilistic Routing Protocol using History of Encounters and Transitivity (PRoPHET) protocol uses an algorithm that attempts to exploitthe <b>non-randomness</b> of real-world encounters by maintaining a set of probabilitiesfor successful delivery to known destinations in the DTN (delivery predictabilities) and replicating messages during opportunistic encounters only if the Mule that does not have the messageappears to {{have a better chance of}} delivering it. This strategy was first documented in a paper from 2003.|$|E
5000|$|The non-random two-liquid model (short NRTL equation) is an {{activity}} coefficient model that correlates the activity coefficients [...] of a compound i with its mole fractions [...] in the liquid phase concerned. It is frequently {{applied in the}} field of chemical engineering to calculate phase equilibria. The concept of NRTL is based on the hypothesis of Wilson that the local concentration around a molecule is different from the bulk concentration. This difference is due to a difference between the interaction energy of the central molecule with the molecules of its own kind [...] and that with the molecules of the other kind [...] The energy difference also introduces a <b>non-randomness</b> at the local molecular level. The NRTL model belongs to the so-called local-composition models. Other models of this type are the Wilson model, the UNIQUAC model, and the group contribution model UNIFAC. These local-composition models are not thermodynamically consistent due to the assumption that the local composition around molecule i is independent of the local composition around molecule j. This assumption is not true, as was shown by Flemr in 1976.|$|E
5000|$|Nature {{never fails}} to amaze us. My {{continuous}} interest is to apply mathematics {{in the field of}} ecology for a deeper and more fundamental understanding of emerging ecological patterns. This will not only enhance our understanding in the natural sciences but also challenge the development of mathematics. Scientific research, from my perspective, endeavours to measure natural objects, to quantify patterns and structures from these measurements, and ultimately to identify the mechanisms governing these patterns and structures. This is equal to unveiling (i) what patterns exist in nature, (ii) how such patterns emerge, and (iii) why nature organizes itself in such a way. My research, thus, focuses in three specific areas. First, spatial and dynamic complexity caused by organism-environment feedback and biotic interactions (e.g., estimating rates of spread from different dispersal kernels, the consequence of niche construction, the origin of altruism via assortative interactions, and the adaptive dynamics of a co-evolving system). Second, the scaling patterns of biodiversity, with the emphasis on the profound effect of spatial scales on macroecological and community assemblage patterns (e.g., the occupancy frequency distribution and the <b>non-randomness</b> of species distribution and association). Finally, using biological invasions, as a natural experiment, to study how species sharpen their weaponries (invasiveness), how the native ecosystem responds to the intrusion (invasibility, resilience and stability), and how the novel ecosystem rebuilds its structure (e.g., functioning modules and nested architecture). These three areas of research all serve to clarify the interactions among patterns, scales and dynamics in the ever-evolving ecological system.http://math.sun.ac.za/hui ...|$|E
5000|$|Colossus and {{the reasons}} for its {{construction}} were highly secret, and remained so for 30 years after the War. Consequently, it {{was not included in the}} history of computing hardware for many years, and Flowers and his associates were deprived of the recognition they were due. Colossi 1 to 10 were dismantled after the war and parts returned to the Post Office. Some parts, sanitised as to their original purpose, were taken to Max Newman's Royal Society Computing Machine Laboratory at Manchester University. Tommy Flowers was ordered to destroy all documentation and burnt them in a furnace at Dollis Hill. He later said of that order: That was a terrible mistake. I was instructed to destroy all the records, which I did. I took all the drawings and the plans and all the information about Colossus on paper and put it in the boiler fire. And saw it burn. [...] Colossi 11 and 12, along with two replica Tunny machines, were retained, being moved to GCHQ's new headquarters at Eastcote in April 1946, and again with GCHQ to Cheltenham between 1952 and 1954. One of the Colossi, known as Colossus Blue, was dismantled in 1959; the other in 1960. There had been attempts to adapt them to other purposes, with varying success; in their later years they had been used for training. Jack Good related how he was the first to use Colossus after the war, persuading the US National Security Agency that it could be used to perform a function for which they were planning to build a special-purpose machine. Colossus was also used to perform character counts on one-time pad tape to test for <b>non-randomness.</b>|$|E
40|$|Social {{networks}} tend {{to contain}} some amount of randomness and some amount of <b>non-randomness.</b> The amount of randomness versus <b>non-randomness</b> affects {{the properties of}} a social network. In this paper, we theoretically analyze graph randomness and present a framework which provides a series of <b>non-randomness</b> measures at levels of edge, node, and the overall graph. We show that graph nonrandomness can be obtained mathematically from the spectra of the adjacency matrix of the network. We also derive the upper bound and lower bound of <b>non-randomness</b> value of the overall graph. We conduct both theoretical and empirical studies in spectral geometries of social networks and show our proposed <b>non-randomness</b> measures can better characterize and capture graph randomness than previous measures. ...|$|E
40|$|Abstract. Stream cipher {{initialisation}} should {{ensure that}} the initial state or keystream is not detectably related to the key and initialisation vector. In this paper we analyse the key/IV setup of the eSTREAM Phase 2 candidates Salsa 20 and TSC- 4. In the case of Salsa 20 we demonstrate a key recovery attack on six rounds and observe <b>non-randomness</b> after seven. For TSC- 4, <b>non-randomness</b> over the full eight-round initialisation phase is detected, but would also persist for more rounds...|$|E
40|$|In {{the article}} a test is developed, which allows {{to test the}} null-hypothesis of the {{intrinsic}} randomness in the angular distribution of gamma-ray bursts collected at the Current BATSE Catalog. The method is {{a modified version of}} the well-known counts-in-cells test, and fully eliminates the non-uniform skyexposure function of BATSE instrument. Applying this method to the case of 1 all gamma-ray bursts no intrinsic <b>non-randomness</b> was found. The test also did not find intrinsic non-randomnesses for the short and long gamma-ray bursts, respectively. On the other hand, using the method to the new intermediate subclass of gamma-ray bursts, the null-hypothesis of the intrinsic randomness for 181 intermediate gamma-ray bursts is rejected on the 96. 4 % confidence level. Taking 92 dimmer bursts from this subclass itself, we obtain the surprising result: This ”dim ” subclass of the intermediate subclass has an intrinsic <b>non-randomness</b> on the 99. 3 % confidence level. On the other hand, the 89 ”bright ” GRBs show no intrinsic <b>non-randomness...</b>|$|E
40|$|Recent {{advances}} in coarse-grained lattice and off-lattice protein models are reviewed. The sequence dependence of thermodynamical folding properties are investigated and evidence for <b>non-randomness</b> of the binary sequences of good folders are discussed. Similar patterns for <b>non-randomness</b> are found for real proteins. Dynamical parameter MC methods, {{such as the}} tempering and multisequence algorithms, are essential {{in order to obtain}} these results. Also, a new MC method for design, the inverse of folding, is presented. Here, one maximizes conditional probabilities rather than minimizing energies. By construction, this method ensures that the designed sequences represent good folders thermodynamically. 1...|$|E
40|$|Networks of {{cortical}} neurons {{are essentially}} non-ran-dom [1]. Although {{it is known}} that such networks show interesting structure at multiple temporal and spatial scales [2], almost no experimental work has been done to reveal how structures at these different scales relate to each other. This study aimed to clarify important relations between <b>non-randomness</b> in groups of 3 - 6 neurons (clusters) and <b>non-randomness</b> in groups of 50 - 100 neu-rons (communities) through five steps. First, we recorded spontaneous activity of up to 500 neurons from rodent somatosensory cortex using a 512 ch. multi-electrode system over one hour [3]. Second, we recon-structed effective connectivity using transfer entropy [4]. Third, we compared topologies of effective networks at the 3 - 6 neuron scale (clusters including motifs [Figure 1 -B]) with topologies of synaptic connections measured from 12 neuron simultaneous patch clamp experiments [5, 6]. Fourth, we constructed community or modular structures representing <b>non-randomness</b> from larger groups of neurons. Fifth, we evaluated the extent to which structure at each of these scales was robust. We did this by swapping connections from high degree nodes (hubs) with those from low degree nodes (non-hubs). We found three things. First, the degree-distribution followed a power-law This demonstrated that hubs could not have been the result of random sampling from a Gaussian distribution. Second, effective networks consisting of hundreds of cortical neurons have distinc-tive non-random structures of connectivity at two differ-ent scales. Third, structure at the cluster level was relatively more fragile than structure at the community level. The difference between <b>non-randomness</b> evaluated by cluster and community will become the importan...|$|E
40|$|Abstract. We {{consider}} the quantum site percolation model on graphs with an amenable group action. It {{consists of a}} random family of Hamiltonians. Basic spectral properties of these operators are derived: <b>non-randomness</b> of the spectrum and its components, existence of an self-averaging integrated density of states and an associated trace-formula. 1. Introduction: The Quantu...|$|E
40|$|This paper {{examines}} the socio-economic consequences of teenage moth-erhood for {{a cohort of}} British women born in 1970. We apply a number of di¤erent methodologies on the same dataset, including OLS, a propen-sity score matching estimator, and an instrumental variables estimator, using miscarriages as an instrument. We bound the biases introduced through IV due to <b>non-randomness,</b> and misreporting of the instrument. Our results {{are sensitive to the}} methodologies used. Taking only observed characteristics into account, the e¤ects of teenage motherhood appear large and negative. The pathways are through bigger family size, and negative labour market outcomes for the mother and her partner, and are mitigated by transfers from the state through the British bene 8 ̆ 5 t system. Our IV estimates show that almost all these e¤ects are reduced to zero once unobserved heterogeneity is taken into account. However our IV bounds show that biases introduced by <b>non-randomness</b> and misreporting of our instrument could be responsible for all of this apparent reduction in e¤ects...|$|E
40|$|This article {{investigates the}} Lloyd 2 ̆ 7 s of London case of stolen gold using {{statistical}} support of forensic auditing in Rhode Island, taking {{a sample of}} Sammartino 2 ̆ 7 s House of Diamonds 2 ̆ 7 sales receipts and randomly choosing one of the jewelry items listed on each receipt. The auditors presented the results to James Campise, the attorney representing Lloyd 2 ̆ 7 s in early 1990. Campise requested {{to make a report}} visually compelling by using charts and graphs that jury members could easily understand. The authors constructed two hypotheses to test the results of the forensic audit. The first hypothesis was designed to test for <b>non-randomness</b> in the selected sample observations but {{there was no evidence of}} a pattern of <b>non-randomness.</b> The second hypothesis, the amounts of sales selected in the sample were proportional to the amounts of sales in the population on a monthly basis...|$|E
40|$|A {{formula is}} derived to {{estimate}} the expected frequency of potential secondary structure of a given stem length and loop size in a random sequence of nucleotides of known length and composition. An example is provided using the SV 40 sequence which shows significant <b>non-randomness</b> in stem length but whose significance is reduced by more than six orders of magnitude by taking nearest neighbor frequencies into account...|$|E
40|$|In this paper, we cryptanalyze the {{compression}} functions of MD 4, MD 5 and 4 -, 5 -pass HAVAL in encryption mode. We exploit the recently proposed related-key rectangle and boomerang techniques to show <b>non-randomness</b> of MD 4, MD 5 and 4 -, 5 -pass HAVAL and to distinguish {{them from a}} randomly chosen cipher. The attacks are highly practical and have been confirmed by our experiments. status: publishe...|$|E
40|$|We {{consider}} a random family of Schrodinger operators on a cover X of a compact Riemannian manifold M = X/#. We present several results on their spectral theory, in particular almost sure constancy of the spectral components and existence and <b>non-randomness</b> {{of an integrated}} density of states. We also sketch a groupoid based general framework which allows to treat basic features of random operators in di#erent contexts in a unified way...|$|E
40|$|Abstract. In this paper, {{we propose}} a novel technique, called multi-output {{filtering}} model, {{to study the}} <b>non-randomness</b> property of a cryptographic algorithm such as message authentication codes and block ciphers. A multi-output filtering model consists of a linear feedback shift register (LFSR) and a multi-output filtering function. Our contribution in this paper is twofold. First, we propose an attack technique under IND-CPA using the multi-output filtering model. By introducing a distinguishing function, we theoretically determine the success rate of this attack. In particular, we construct a distinguishing function based {{on the distribution of}} the linear complexity of component sequences, and apply it on studying TUAK’s f 1 algorithm, AES, KASUMI and PRESENT. We demonstrate that the success rate of the attack on KASUMI and PRESENT is non-negligible, but f 1 and AES are resistant to this attack. Second, we study the distribution of the cryptographic properties of component functions of a random primitive in the multi-output filtering model. Our experiments show some <b>non-randomness</b> in the distribution of algebraic degree and nonlinearity for KASUMI. ...|$|E
40|$|We {{studied the}} {{complete}} randomness of the angular distribution of BATSE gamma-ray bursts (GRBs). Based on their durations and peak fluxes, we divided the BATSE sample into 5 subsamples (short 1, short 2, intermediate, long 1, long 2) {{and studied the}} angular distributions separately. We used three methods to search for <b>non-randomness</b> in the subsamples: Voronoi tesselation, minimal spanning tree, and multifractal spectra. To study any <b>non-randomness</b> in the subsamples we defined 13 test-variables (9 from Voronoi tesselation, 3 from the minimal spanning tree and one from the multifractal spectrum). We made Monte Carlo simulations {{taking into account the}} BATSE’s sky-exposure function. We tested therandomness by introducing squared Euclidean distances in the parameter space of the test-variables. We recognized that the short 1, short 2 groups deviate significantly (99. 90 %, 99. 98 %) from the fully random case in the distribution of the squared Euclidean distances but this is not true for the long samples. In the intermediate group, the squared Euclidean distances also give significant deviation (98. 51 %) ...|$|E
40|$|Structural {{effects in}} small {{molecule}} and polymer {{systems have been}} studied through heat capacity measurements (small molecule systems), phase diagrams and heat of mixing measurements (polymer systems). It {{has been found that}} structure due to molecular antipathy i. e. large H$ sp{ rm E}$ and G$ sp{ rm E}$ manifests itself through a W-shape C$ sbsp{ rm p}{ rm E}$ which is found to be a summation of two contributions to the thermodynamics: random and non-random. An excellent correlation is made between C$ sbsp{ rm p}{ rm E}$ and the concentration-concentration correlation function (S$ sb{ rm CC}$). The <b>non-randomness</b> effect increases drastically as T moves towards the UCST. <b>Non-randomness</b> (S$ sb{ rm CC}$) and the effect of <b>non-randomness</b> (C$ sbsp{ rm p}{ rm E}$) are found toward the middle of the concentration range. It has been found that the W-shape is not a wide-spread phenomenon for other second-order quantities such as d V$ sp{ rm E}$/dT and d V$ sp{ rm E}$/dP, in spite of a certain similarity of the critical exponent for C$ sb{ rm p}$, $ alpha sb{ rm p}$ and $ kappa sb{ rm T}$. d V$ sp{ rm E}$/dT results have been discussed in terms of Flory theory. The difference between experimental and theoretical values provides a good estimation of the non-random d V$ sp{ rm E}$/dT. Structure has also been studied through H-bonding interactions (self-association or complex formation) between methanol, butanol, decanol in CCl$ sb{ rm 4 }$, acetonitrile or octanenitrile and dodecanenitrile in n-decane, CCl$ sb{ rm 4 }$ or xylene. C$ sbsp{ rm p}{ rm E}$ are predicted using the Treszczanowicz-Kehiaian theory. Structure in polymer solutions has been associated with specific interactions which lead to a low temperature LCST. An extended Flory-Huggins-Prigogine theory has been used to interpret the phase diagram of specifically interacting polymer systems. Compatibility between polymer pairs is also linked to the presence of these specific interactions which are associated with an exothermic heat of mixing. $ Delta$H$ sb{ rm M}$ has been measured for ternary systems i. e. two polymers and a mutual solvent. Exothermic $ Delta$H$ sb{ rm M}$ is observed if the pair strongly interacts while $ Delta$H$ sb{ rm M}$ is positive if the polymers are incompatible or if they do not interact similarly with the solvent, in which case <b>non-randomness</b> plays a major role...|$|E
40|$|A {{sample of}} 224 {{companies}} {{listed in the}} Kuala Lumpur Stock Exchange was taken for the period 1991 - 96. The serial correlations tests of varying lags and the runs tests were employed to test for the random walk theory. The bulk of the results tilts towards the rejection of <b>non-randomness,</b> lending weight to {{the argument that the}} stock market has no memory, and casting doubt upon the usefulness of technical analysis. ...|$|E
40|$|Psychologists often want {{to detect}} {{category}} structure in subjects ' free recall protocols. While runs tests {{based on the}} binomial distribution are commonly used to detect <b>non-randomness</b> within a sequence, many research situations require tests based on the multinomial distribution. We propose a test of randomness versus clustering {{based on the number}} of runs in multinomial data. We illustrate its use with data from a mass communication experiment using a constrained free recall procedure...|$|E
40|$|Abstract—Autocorrelation is {{a widely}} used {{statistical}} tool for determining periodicity and the fundamental period of a time-series. Calculating the autocorrelation function Rxx(k), we cross-correlate the signal with itself in order to detect <b>non-randomness</b> or to find repeating patterns. A continuous output, autocorrelation algorithm is proposed here which is able to detect periodicity in motor behaviour of free-roaming animals. The data stream is coming from a 3 -axis accelerometer mounted on a collar. I...|$|E
30|$|A {{value of}} the test {{statistic}} of the runs test greater than the critical value at a particular level of significance indicates <b>non-randomness</b> in the daily returns under analysis and suggests that the Indian GETF market is not weak-form efficient. On the other hand, a {{value of the}} test statistic below the critical value at a particular level of significance indicates random daily returns and suggests that the Indian GETF weak-form market is efficient.|$|E
40|$|We {{consider}} the quantum site percolation model on graphs with an amenable group action. It {{consists of a}} random family of Hamiltonians. Basic spectral properties of these operators are derived: <b>non-randomness</b> of the spectrum and its components, existence of an self-averaging integrated density of states and an associated trace-formula. Comment: 10 pages, LaTeX 2 e, to appear in "Applied Mathematics and Scientific Computing", Brijuni, June 23 - 27, 2003. by Kluwer publisher...|$|E
40|$|A {{technique}} for generating random binary data using a genetic optimisation algorithm {{has been developed}} and tested. The random data is for use in estimating the average power consumption in CMOS circuits, based on a Monte Carlo simulation of the extracted SPICE netlist. The proposed technique is tested for local and global <b>non-randomness</b> {{using a variety of}} statistical techniques. Power estimates for typical CMOS circuits are presented and are compared to those obtained using a commercial generator...|$|E
40|$|Abstract. We {{consider}} a random family of Schrödinger operators on a cover X of a compact Riemannian manifold M = X/Γ. We present several results on their spectral theory, in particular almost sure constancy of the spectral components and existence and <b>non-randomness</b> {{of an integrated}} density of states. We also sketch a groupoid based general framework which allows to treat basic features of random operators in different contexts in a unified way. Further topics of research are also discussed. 1...|$|E
