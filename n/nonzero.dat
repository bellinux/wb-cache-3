10000|187|Public
5|$|A <b>nonzero</b> scalar {{multiple}} of an identity matrix {{is called a}} scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of <b>nonzero</b> elements of the field.|$|E
5|$|If {{infinite}} matrices {{are used}} to describe linear maps, then only those matrices can be used all of whose columns have but {{a finite number of}} <b>nonzero</b> entries, for the following reason. For a matrix A to describe a linear map f: V→W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vectorv of coefficients, only finitely many entries v'i are <b>nonzero.</b> Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many <b>nonzero</b> entries. There is no restriction on the rows of A however: in the product A·v there are only finitely many <b>nonzero</b> coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many <b>nonzero</b> terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many <b>nonzero</b> entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.|$|E
5|$|Some proofs that 0.999... = 1 {{rely on the}} Archimedean {{property}} of the real numbers: {{that there are no}} <b>nonzero</b> infinitesimals. Specifically, the difference 1 − 0.999... must be smaller than any positive rational number, so it must be an infinitesimal; but since the reals do not contain <b>nonzero</b> infinitesimals, the difference is therefore zero, and therefore the two values are the same.|$|E
40|$|Because of {{efficient}} encoding and decoding algorithms, cyclic {{codes are}} an important family of linear block codes, and have applications in communica- tion and storage systems. However, their weight distributions are known {{only for a few}} cases mainly on the codes with one or two <b>nonzeros.</b> In this paper, the weight distributions of two classes of cyclic codes with three or four <b>nonzeros</b> are determined. Comment: 22 pages, 3 table...|$|R
3000|$|... α, where α∈R^n is {{the vector}} of {{coefficients}} having at most s <b>nonzeros</b> values, i.e., ∥α∥ 0 ≤s. Recall that the ℓ [...]...|$|R
3000|$|... where ∥·∥ 0 {{denotes the}} l 0 norm {{counting}} {{the number of}} <b>nonzeros</b> in a vector or matrix. θ is necessarily the sparsest solution (min∥θ∥ 0) such that y=D θ.|$|R
5|$|Its {{values are}} powers of two, with exponents {{equal to the}} number of <b>nonzero</b> digits in the binary {{representation}} of the step number. Other applications of Rule 90 have included the design of tapestries.|$|E
5|$|On {{the other}} hand, when x is an integer, the identities are valid for all <b>nonzero</b> complex numbers.|$|E
5|$|Photons inside superconductors do {{develop a}} <b>nonzero</b> {{effective}} rest mass; as a result, electromagnetic forces become short-range inside superconductors.|$|E
25|$|The graphs whose {{adjacency}} matrices can {{be ordered}} such that, in each row and each column, the <b>nonzeros</b> of the matrix form a contiguous interval adjacent to the main diagonal of the matrix.|$|R
40|$|Consider an underdetermined {{system of}} linear {{equations}} y = Ax with known y and d × n matrix A. We seek the nonnegative x with the fewest <b>nonzeros</b> satisfying y = Ax. In general, {{this problem is}} NP-hard. However, for many matrices A there is a threshold phenomenon: if the sparsest solution is sufficiently sparse, {{it can be found}} by linear programming. We explain this by the theory of convex polytopes. Let aj denote the jth column of A, 1 ≤ j ≤ n, let a 0 = 0 and P denote the convex hull of the aj. We say the polytope P is outwardly k-neighborly if every subset of k vertices not including 0 spans a face of P. We show that outward k-neighborliness is equivalent to the statement that, whenever y = Ax has a nonnegative solution with at most k <b>nonzeros,</b> it is the nonnegative solution to y = Ax having minimal sum. We also consider weak neighborliness, where the overwhelming majority of k-sets of ajs not containing 0 span a face of P. This implies that most nonnegative vectors x with k <b>nonzeros</b> are uniquely recoverable from y = Ax by linear programming. Numerous corollaries follow by invoking neighborliness results. For example, for most large n by 2 n underdetermined systems having a solution with fewer <b>nonzeros</b> than roughly half the number of equations, the sparsest solution can be found by linear programming...|$|R
40|$|In the {{factorization}} A = QR of {{a matrix}} A, the orthogonal matrix Q {{can be represented}} either explicitly (as a matrix) or implicitly (as a matrix H of Householder vectors). We derive both {{upper and lower bounds}} on the number of <b>nonzeros</b> in H and the number of <b>nonzeros</b> in Q, in the case where the graph of A T A has "good" separators and A need not be square. We also derive an upper bound on the number of <b>nonzeros</b> in the null-basis part of Q in the case where A is the edge-vertex incidence matrix of a planar graph. The significance of these results is that they both illuminate and amplify a folk theorem of sparse QR factorization, which holds that the matrix H of Householder vectors represents the orthogonal factor of A much more compactly than Q itself. To facilitate discussion of this and related issues, we review several related results which have appeared previously. Keywords: Sparse matrix algorithms, QR factorization, separators, column intersection graph, strong Hall [...] ...|$|R
5|$|The case i = n {{has also}} been noted already, and is an easy {{consequence}} of the Hurewicz theorem: this theorem links homotopy groups with homology groups, which are generally easier to calculate; in particular, it shows that for a simply-connected space X, the first <b>nonzero</b> homotopy group πk(X), with k > 0, is isomorphic to the first <b>nonzero</b> homology group H'k(X). For the n-sphere, this immediately implies that for n ge& 2, πn(S'n) = H'n(S'n) = Z.|$|E
5|$|This rule is also {{connected}} to number theory {{in a different}} way, via Gould's sequence. This sequence counts the number of <b>nonzero</b> cells in each time step after starting Rule 90 with a single live cell.|$|E
5|$|For {{a simple}} closed curve, using an {{extension}} of the flow to non-smooth curves based on the level set method, there are only two possibilities. Curves with zero Lebesgue measure (including all polygons and piecewise-smooth curves) instantly evolve into smooth curves, after which they evolve as any smooth curve would. However, Osgood curves with <b>nonzero</b> measure instead immediately evolve into a topological annulus with <b>nonzero</b> area and smooth boundaries. The topologist's sine curve is an example that instantly becomes smooth, despite not even being locally connected; examples such as this show that the reverse evolution of the curve-shortening flow can take well-behaved curves to complicated singularities in a finite amount of time.|$|E
40|$|This paper {{analyzes}} a novel {{method for}} constructing preconditioners for diagonally-dominant symmetric positive-definite matrices. The method discussed here {{is based on}} a simple idea: we construct M by simply dropping offdiagonal <b>nonzeros</b> from A and modifying the diagonal elements to maintain a certain row-sum property. The preconditioners are extensions of Vaidya's augmented maximum-spanning-tree preconditioners. The preconditioners presented here were als mentioned by Vaidya in an unpublished manuscript, but without a complete analysis. The preconditioners that we present have only O(n+ t&sup 2;) <b>nonzeros,</b> where n is the dimension of the matrix and t is a parameter that ne can choose. Their construction is efficient and guarantees that the condition number of the preconditioned system is O(n&sup 2;/t&sup 2;) if the number of <b>nonzeros</b> per row in the matrix is bounded by a constant. We have developed an efficient algorithm to construct these preconditioners and we have implemented it. We used our implementation to solve a simple model problem; we show the combinatorial structure of the preconditioners and we present encouraging convergence results...|$|R
40|$|An n n ray pattern A {{is said to}} be spectrally {{arbitrary}} if {{for every}} monic n-th degree polynomial f(x) with coe ¢ cients from C, there is a matrix in the pattern class of A such that its characteristic polynomial is f(x). In this article the authors develop a nilpotent-Jacobi method for establishing that an irreducible ray pattern and all its superpatterns are spectrally arbitrary. They use this method to establish that a particular family of n n irreducible ray patterns with exactly 3 n <b>nonzeros</b> is spectrally arbitrary. They then show that every n n irreducible, spectrally arbitrary ray pattern has at least 3 n 1 <b>nonzeros...</b>|$|R
40|$|Several {{models have}} been {{proposed}} for analyzing data characterized by a preponderance of zeros. Substantively, the choice between these models should be {{based solely on the}} data generating process. However, datasets can vary as a function of both the proportions of zeros and the distribution for the <b>nonzeros.</b> A Monte Carlo design was used to sample 1, 000 cases from each of six distributions and five proportions of zeros. Between-model superiority was tested using deviance statistics and AICs over 2, 000 simulations per condition. The results suggest that the best-fitting zero-inflated model sometimes depends on the proportion of zeros and the distribution for the <b>nonzeros.</b> In fact, there are situations where the zero-inflated models are not necessary...|$|R
5|$|In most situations, revenue-maximizing {{prices are}} not profit-maximizing prices. For example, if {{variable}} costs per unit are <b>nonzero</b> (which they almost always are), then {{a more complex}} computation of a similar kind yields prices that generate optimal profits.|$|E
5|$|Integer {{powers of}} <b>nonzero</b> complex numbers {{are defined by}} {{repeated}} multiplication or division as above. If i is the imaginary unit and n is an integer, then i'n equals 1, i, −1, or −i, according to whether the integer n is congruent to 0, 1, 2, or 3 modulo 4. Because of this, the powers of i are useful for expressing sequences of period 4.|$|E
5|$|It is {{possible}} to extend {{the definition of the}} flow to more general inputs than curves, for instance by using rectifiable varifolds or the level set method. However, these extended definitions may allow parts of curves to vanish instantaneously or fatten into sets of <b>nonzero</b> area.|$|E
40|$|AbstractAn n×n ray pattern A {{is said to}} be spectrally {{arbitrary}} if {{for every}} monic nth degree polynomial f(x) with coefficients from C, there is a matrix in the pattern class of A such that its characteristic polynomial is f(x). In this article the authors extend the nilpotent-Jacobi method for sign patterns to ray patterns, establishing a means to show that an irreducible ray pattern and all its superpatterns are spectrally arbitrary. They use this method to establish that a particular family of n×n irreducible ray patterns with exactly 3 n <b>nonzeros</b> is spectrally arbitrary. They then show that every n×n irreducible, spectrally arbitrary ray pattern has at least 3 n- 1 <b>nonzeros...</b>|$|R
40|$|Summary This paper {{describes}} {{and tests}} a parallel message-passing code for constructing sparse approximate inverse preconditioners using Frobenius norm minimization. The sparsity {{patterns of the}} preconditioners are chosen as patterns of powers of sparsified matrices. Sparsification is necessary when powers of a matrix have {{a large number of}} <b>nonzeros,</b> making the approximate inverse computation expensive. For our test problems, the minimum solution time is achieved with approximate inverses with less than twice the number of <b>nonzeros</b> of the original matrix. Additional accuracy is not compensated by the increased cost per iteration. The results lead to further understanding of how to use these methods and how well these methods work in practice. In addition, this paper describes programmin...|$|R
40|$|A {{reduction}} in the computational work is possible {{if we do not}} require that the <b>nonzeros</b> of a Jacobian matrix be determined directly. If a column or row partition is available, the proposed substitution technique can be used {{to reduce the number of}} groups in the partition further. In this chapter, we present a substitution method to determine the structure of sparse Jacobian matrices efficiently using forward, reverse, or a combination of forward and reverse modes of AD. Specifically, if it is true that the difference between the maximum number of <b>nonzeros</b> in a column or row and the number of groups in the corresponding partition is large, then the proposed method can save many AD passes. This assertion is supported by numerical examples...|$|R
5|$|Like {{the other}} noble gases, krypton is highly {{chemically}} unreactive. The rather restricted chemistry of krypton in its only known <b>nonzero</b> oxidation state of +2 parallels {{that of the}} neighboring element bromine in the +1 oxidation state; due to the scandide contraction {{it is difficult to}} oxidize the 4p elements to their group oxidation states. Before the 1960s, no noble gas compounds had been synthesized.|$|E
5|$|Since the {{remainders}} decrease {{with every}} step but can never be negative, a remainder r'N must eventually equal zero, {{at which point the}} algorithm stops. The final <b>nonzero</b> remainder r'N−1 is the greatest common divisor of a and b. The number N cannot be infinite because there are only a finite number of nonnegative integers between the initial remainder r0 and zero.|$|E
5|$|The {{result that}} 0.999... = 1 generalizes readily in two ways. First, every <b>nonzero</b> number with a finite decimal {{notation}} (equivalently, endless trailing 0s) has a counterpart with trailing 9s. For example, 0.24999... equals 0.25, exactly {{as in the}} special case considered. These numbers are exactly the decimal fractions, and they are dense.|$|E
40|$|We {{analyze the}} problem of sparse-matrix dense-vector {{multiplication}} (SpMV) in the I/O-model. The task of SpMV is to compute y: = Ax, where A is a sparse N × N matrix and x and y are vectors. Here, sparsity is expressed by the parameter k that states that A has a total of at most kN <b>nonzeros,</b> i. e., an average number of k <b>nonzeros</b> per column. The extreme choices for parameter k are well studied special cases, namely for k = 1 permuting and for k = N dense matrix-vector multiplication. We study the worst-case complexity of this computational task, i. e., {{what is the best}} possible upper bound on the number of I/Os depending on k and N only. We determine this complexity up to a constant factor for large ranges of the parameters. By our arguments, we find that most matrices with kN <b>nonzeros</b> require this number of I/Os, even if the program may depend on the structure of the matrix. The model of computation for the lower bound is a combination of the I/O-models of Aggarwal and Vitter, and of Hong and Kung. We study two variants of the problem, depending on the memory layout of A. If A is stored in column major layout, SpMV has I/O com...|$|R
3000|$|... {{may lead}} to an {{excessive}} penalization of {{these two types of}} behaviours. Our strategy is to make an initial choice for the maximum number of <b>nonzeros</b> that we will allow in the matrices Q^[k], for k= 0, 1,…,M. Then, as the iteration proceeds, the thresholding value θ [...]...|$|R
50|$|It {{was later}} shown {{how to use}} integer {{arithmetic}} while making the distribution even sparser, having very few <b>nonzeroes</b> per column, in work on the Sparse JL Transform. This is advantageous since a sparse embedding matrix means being able to project the data to lower dimension even faster.|$|R
5|$|Amalthea orbits Jupiter at a {{distance}} of 181000km (2.54Jupiter radii). The orbit of Amalthea has an eccentricity of 0.003 and an inclination of 0.37° relative to the equator of Jupiter. Such appreciably <b>nonzero</b> values of inclination and eccentricity, though still small, are unusual for an inner satellite and {{can be explained by the}} influence of the innermost Galilean satellite, Io: in the past Amalthea has passed through several mean-motion resonances with Io that have excited its inclination and eccentricity (in a mean-motion resonance the ratio of orbital periods of two bodies is a rational number like m:n).|$|E
5|$|An {{equivalent}} way of {{stating the}} same condition is that, if the angles are partitioned into two alternating subsets, then {{the sum of}} the angles in either of the two subsets is exactly 180 degrees. However, this equivalent form applies only to a crease pattern on a flat piece of paper, whereas the alternating sum form of the condition remains valid for crease patterns on conical sheets of paper with <b>nonzero</b> defect at the vertex.|$|E
5|$|Rule 90 and Rule 102 {{are called}} {{additive}} cellular automata. This means that, if two initial states are combined by computing the exclusive or of each their states, then their subsequent configurations will be combined {{in the same}} way. More generally, one can partition any configuration of Rule 90 into two subsets with disjoint <b>nonzero</b> cells, evolve the two subsets separately, and compute each successive configuration of the original automaton as the exclusive or of the configurations on the same time steps of the two subsets.|$|E
40|$|Cyclic codes have {{efficient}} encoding and decoding algorithms. The decoding {{error probability}} and the undetected error probability are usually bounded by or given {{from the weight}} distributions of the codes. Most researches are about {{the determination of the}} weight distributions of cyclic codes with few <b>nonzeros,</b> by using quadratic form and exponential sum but limited to low moments. In this paper, we focus on the application of higher moments of the exponential sum to determine the weight distributions of a class of ternary cyclic codes with three <b>nonzeros,</b> combining with not only quadratic form but also MacWilliams' identities. Another application {{of this paper is to}} emphasize the computer algebra system Magma for the investigation of the higher moments. In the end, the result is verified by one example using Matlab. Comment: 10 pages, 3 table...|$|R
40|$|Monge-Kantorovich optimal {{transport}} {{linear programming}} formulation move mass from source to destination sites number of unknowns quadratic {{in number of}} sites number of <b>nonzeros</b> in the solution is linear {{in the number of}} sites propose an iterative method to exploit this combination of narrow band, feasibility relaxation, nested iteration, matrix preconditionin...|$|R
3000|$|We see in Fig. 6 {{that the}} highly ranked nodes are well approximated. Even though the {{original}} dynamic communicability matrix is full, we {{see from the}} zero rows in Fig. 8 that many nodes have no activity recorded after our approximation method is applied. Overall, Q^[M] has 2583 <b>nonzeros,</b> corresponding to 23 [...]...|$|R
