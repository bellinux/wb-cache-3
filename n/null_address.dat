3|25|Public
50|$|Similar zone {{master files}} may be {{created for the}} reverse {{resolution}} of the broadcast address and the <b>null</b> <b>address.</b> Such zone files prevent a DNS server from referring to other, possibly external DNS servers.|$|E
5000|$|This code will print a <b>NULL</b> <b>address</b> for {{the first}} [...] object and some non-NULL address for the second, showing that the source object lost the {{reference}} during the assignment (...) [...] The raw pointer [...] in the example should not be deleted, as it will be deleted by the [...] that owns the reference. In fact, [...] could be passed directly into , {{eliminating the need for}} [...]|$|E
5000|$|The {{corresponding}} Data Definition Language {{statement is}} as follows. CREATE TABLE Supplier ( [...] SupplierNumber INTEGER NOT NULL, Name VARCHAR(20) NOT <b>NULL,</b> <b>Address</b> VARCHAR(50) NOT NULL, Type VARCHAR(10), CONSTRAINT supplier_pk PRIMARY KEY(SupplierNumber), CONSTRAINT number_value CHECK (SupplierNumber > 0) [...] ) CREATE TABLE Invoices ( [...] InvoiceNumber INTEGER NOT NULL, SupplierNumber INTEGER NOT NULL, Text VARCHAR(4096), CONSTRAINT invoice_pk PRIMARY KEY(InvoiceNumber), CONSTRAINT inumber_value CHECK (InvoiceNumber > 0), CONSTRAINT supplier_fk FOREIGN KEY(SupplierNumber) REFERENCES Supplier(SupplierNumber) ON UPDATE CASCADE ON DELETE RESTRICT [...] ) ...|$|E
5000|$|Callback {{verification}} has {{no effect}} if spammers spoof real email addresses or use the <b>null</b> bounce <b>address.</b>|$|R
5000|$|Some mail {{systems that}} {{implement}} Callback verification use [...] "postmaster" [...] {{instead of the}} <b>null</b> return <b>address.</b>|$|R
30|$|Motif {{significance}} testing, {{applying the}} metrics from the “Motif evaluation metrics” section. For the “ratio” metric, this requires a suitable multiplex <b>null</b> model (<b>addressed</b> in the “Null model” section).|$|R
5000|$|Bounce {{messages}} in SMTP are sent with the envelope sender address , {{known as the}} <b>null</b> sender <b>address.</b> They are frequently sent with a [...] header address of [...] at the recipient site.|$|R
5000|$|Because a {{very common}} program error is a null pointer dereference (a read or write through a null pointer, used in C to mean [...] "pointer to no object" [...] and as an error indicator), most {{operating}} systems map the <b>null</b> pointer's <b>address</b> such that accessing it causes a segmentation fault.|$|R
40|$|Abstract—We {{describe}} {{a method for}} investigating non-linearity in irregular fluctuations of time series, even if the data exhibit long term trends. Such situations are theo-retically incompatible with the assumption of previously proposed methods. The <b>null</b> hypothesis <b>addressed</b> by our algorithm is that irregular fluctuations are generated by a stationary linear system. The method is demonstrated for numerical data generated by known systems and applied to several experimental time series. 1...|$|R
50|$|The {{basic idea}} is to send all e-mail with a return address that {{includes}} a timestamp and a cryptographic token that cannot be forged. Any e-mail that is returned as a bounce without a valid signature can then be rejected. E-mail that is being bounced back should have an empty (<b>null)</b> return <b>address</b> so that bounces are never created for a bounce and therefore you can't get messages bouncing back and forth forever.|$|R
25|$|In {{real-world}} exploits {{there are}} a variety of challenges which need to be overcome for exploits to operate reliably. These factors include <b>null</b> bytes in <b>addresses,</b> variability in the location of shellcode, differences between environments and various counter-measures in operation.|$|R
40|$|We {{describe}} {{a method for}} investigating nonlinearity in irregular fluctuations (short-term variability) of time series even if the data exhibit long-term trends (periodicities). Such situations are theoretically incompatible with the assumption of previously proposed methods. The <b>null</b> hypothesis <b>addressed</b> by our algorithm is that irregular fluctuations are generated by a stationary linear system. The method is demonstrated for numerical data generated by known systems and applied to several actual time series. Department of Electronic and Information Engineerin...|$|R
40|$|We {{describe}} {{a method for}} identifying correlation structures in irregular fluctuations (short-term variabilities) of multivariate time series, even if they exhibit long-term trends. This method {{is based on the}} previously proposed small shuffle surrogate method. The <b>null</b> hypothesis <b>addressed</b> by this method {{is that there is no}} short-term correlation structure among data or that the irregular fluctuations are independent. The method is demonstrated for numerical data generated by known systems and applied to several experimental time series. Department of Electronic and Information Engineerin...|$|R
40|$|We {{describe}} {{a method for}} identifying dynamics in irregular time series (short term variability). The method we propose focuses attention on {{the flow of information}} in the data. We can apply the method even for irregular fluctuations which exhibit long term trends (periodicities) : situations in which previously proposed surrogate methods would give erroneous results. The <b>null</b> hypothesis <b>addressed</b> by our algorithm is that irregular fluctuations are independently distributed random variables (in other words, there is no short term dynamics). The method is demonstrated for numerical data generated by known systems, and applied to several actual time series. Department of Electronic and Information Engineerin...|$|R
40|$|Dynamic {{centrality}} metrics {{provide a}} {{quantitative assessment of}} the strength of communication between nodes in temporal networks, as well as the overall capacity of the network for the efficient transmission of information. In this article, the behaviours of two variants of the ‘communicability’ metric are examined in simple null models of uncorrelated temporal networks. Analysis of the long-time behaviour of the null models reveals a simple trade-off {{in the role of the}} parameters of the metric, suggesting methods to calibrate parameters and to adapt to temporal variations in the network properties. The <b>null</b> models introduced <b>address</b> two main classes of temporal networks (contact sequences and interval graphs), and their predictions are compared and contrasted with results coming from real-world telecommunications data...|$|R
40|$|This {{paper will}} analyze a new tools set {{recently}} devised by the primary {{author of this}} proposal. The goal of this analysis will be twofold. First, to ascertain {{if any of the}} tools pose a major risk if used by hackers compared to existing tools. Second, to determine how these tools might be used proactively by system administration to proactively check their autonomous systems for vulnerabilities. It was found that the tools are somewhat analogous to NMAP, but function primarily on the data link layer (OSI layer 2). The analysis revealed that this tools set does offers functionality beyond already existing tools primarily due to its layer 2 orientation. It was found that the tool set could be used quite effectively to check for vulnerabilities in regard to denial of service attacks (DOS). Further, the test scenarios revealed an interesting vulnerability in regard to virtual zones. Specifically, the interfaces in virtual zones often have <b>null</b> hardware <b>address</b> (all 0 ’s) which it make it very difficult to trace a DOS attack back to a physical host so the problem can be rectified. This paper just scratched the surface in regard to using these tools for proactive testing. The authors believe further analysis is warranted...|$|R
40|$|Hypothesis testing {{based on}} {{surrogate}} data {{has emerged as}} a popular way to test the null hypothesis that a signal is a realization of a linear stochastic process. Typically, this is done by generating surrogates which are made to conform to autocorrelation (power spectra) and amplitude distribution of the data (this is not necessary if data are Gaussian). Recently, a new algorithm was proposed, the <b>null</b> hypothesis <b>addressed</b> by this algorithm is that data are a realization of a non stationary linear stochastic process, surrogates generated by this algorithm preserve the autocorrelation and local mean and variance of data. Unfortunately, the assumption of Gaussian amplitude distribution is not always valid. Here we propose a new algorithm; the hypothesis addressed by our algorithm is that data are a realization of a nonlinear static transformation of a non stationary linear stochastic process. Surrogates generated by our algorithm preserve the autocorrelation, amplitude distribution and local mean and variance of data. We present some numerical examples where the previously proposed surrogate data methods fail, but our algorithm is able to discriminate between linear and nonlinear data, whether they are stationary or not. Using our algorithm we also confirm the presence of nonlinearity in the monthly global average temperature and in a small segment of a signal from a Micro Electrode Recording. Comment: 9 pages, 15 figure...|$|R
40|$|The {{research}} {{topic of}} the impact of the pattern of teaching aids in the collection knowledge to students in the Department of Education in patterning of decorative materialTo achieve the objective researcher (6) <b>null</b> hypotheses <b>address</b> the research variables with limited border on the first –grade student in the D department of Art Education. College of Fine Arts. University of Babylon. for the academic year 2012 – 2013 Second semester in the patterning of plant material. The second chapter. where the first two topics. Teaching aids look philosophical. The second pattern means. Included in the third quarter. The research sample consisted of (32) male and female students. Divided into four groups of three including a pilot and the fourth officer. Each group included (8) students. Where the studied experimental group using the first and planned educational way. Studied in the experimental group using a second monitor. the studied with the experimental group using third photographers. Either the control group were examined according to the usual way And subjected to the test of the four groups posttest. According to the researcher. as grades obtained by students according to Asthmas to especially designed for this purpose. The results of the research outweigh the experimental group first used it means the planned educational methods used on the other. Among the recommendations is the need to recruit the planned educational display educational materials because of its positive features. it is recommendation proposals in other materials and observe its impact in improving the level. ...|$|R
40|$|When {{searching for}} items online {{there are three}} common {{problems}} that e-buyers may encounter; null retrieval, retrieving unmanageable number of items, and retrieving unsatisfactory items. In the past information retrieval systems or recommender systems were used as solutions. With information retrieval systems, too rigorous filtering based on the user query to reduce unmanageable number of items result in either null retrieval or filtering out the items users prefer. Recommender systems {{on the other hand}} do not provide sufficient opportunity for users to communicate their needs. As a solution, this paper introduces a novel method combining a user model with an interactive product retrieval process. The new layered user model has the potential of being applied across multiple product and service domains and is able to adapt to changing user preferences. The new product retrieval algorithm is integrated with the user model and is able to successfully <b>address</b> <b>null</b> retrieval, retrieving unmanageable number of items, and retrieving unsatisfactory items. The process is demonstrated using a bench mark dataset and a case study. Finally the Product retrieval process is evaluated using a set of guidelines to illustrate its suitability to current eBuying environments...|$|R
40|$|The {{primary purpose}} of this study was to {{determine}} if there were any significant differences between lecture and discussion methods with regard to students 2 ̆ 7 learning achievement. There were three <b>null</b> hypotheses <b>addressed</b> by this study. Null hypothesis I was: there was no significant difference in the students 2 ̆ 7 learning achievement with respect to either lecture or discussion methods. Null hypothesis II was: there was no significant difference between the students 2 ̆ 7 pre-test scores and post-test scores. Null hypothesis III was: there were no differences in the students satisfaction with respect to the two teaching methods. Three teachers and 151 students in six groups were selected to participate in this study. Each teacher taught two groups for one month in the Spring semester of 1992. During this month each group was taught by the lecture and discussion methods, each for two weeks. Four tests were given to the students: two pre-tests and two post-tests. One pre-test was given at the beginning of the first two weeks before the first treatment (lecture or discussion) was applied. The other pre-test was given at the beginning of the second two weeks after the second treatment (lecture or discussion) was received. The first and the second post-tests were given after completing the first and second treatments, respectively. The students 2 ̆ 7 test scores were recorded and used as data for measuring students 2 ̆ 7 learning achievement. For measuring students 2 ̆ 7 satisfaction, a questionnaire attached to the second post-test was distributed. The data for measuring both students 2 ̆ 7 learning achievement and satisfaction level were used to calculate the respective means, standard deviation, percentages, and t-test values. All the examined three null hypothesis in this study was rejected. The results showed the following: the students gained more knowledge after applying both the lecture and discussion methods; the students obtained higher scores when taught by the lecture method; and 83...|$|R
40|$|IGF-I plays a {{vital role}} in growth and {{development}} and acts in an endocrine and an autocrine/paracrine fashion. The purpose of the current study was to clarify whether elevated levels of IGF-I in serum can rescue the severe growth retardation and organ development and function of igf-I <b>null</b> mice. To <b>address</b> that, we overexpressed a rat igf-I transgene specifically in the liver of igf-I null mice. We found that in the total absence of tissue IGF-I, elevated levels of IGF-I in serum can support normal body size at puberty and after puberty but are insufficient to fully support the female reproductive system (evident by irregular estrous cycle, impaired development of ovarian corpus luteum, reduced number of uterine glands and endometrial hypoplasia, all leading to decreased number of pregnancies and litter size). We conclude that most autocrine/paracrine actions of IGF-I that determine organ growth and function can be compensated by elevated levels of endocrine IGF-I. However, in mice, full compensatory responses are evident later in development, suggesting that autocrine/paracrine IGF-I is critical for neonatal development. Furthermore, we show that tissue IGF-I is necessary {{for the development of the}} female reproductive system and cannot be compensated by elevated levels of serum IGF-I...|$|R
40|$|Abstract—Preamble {{design for}} LS channel {{estimation}} in OFDM/OQAM cooperative systems is investigated in this paper. A simple but important setup is considered, {{consisting of a}} pair of single-antenna terminals (source and destination) assisted in their communication by an AF relay and following a well-established two-phase transmission protocol. The so-called sparse preamble case (i. e., pilot tones surrounded by <b>nulls)</b> was recently <b>addressed</b> and the optimal design – in the sense of minimum MSE subject to transmit energy constraints – was shown to coincide with the one for CP-OFDM, thus resulting in no performance gains from the adoption of OFDM/OQAM. In order to complete this study and exhibit the possibilities of OFDM/OQAM to outperform CP-OFDM, the so-called full preamble design (i. e., with all tones car-rying pilot symbols) is addressed in this paper. The results are in line with the corresponding design for single-link systems, where the interference among pilots is positively exploited to provide signi¿cant performance gains over CP-OFDM. As a byproduct, the solution for cooperative CP-OFDM is also given, through its connection to OFDM/OQAM. The presented simulation results corroborate the analysis, demonstrating superior performance over CP-OFDM for both mildly and highly frequency selective channels and at practical SNR values. Keywords—Channel estimation, ¿lter bank-based multicarrier (FBMC), least squares (LS), offset quadrature amplitude modula-tion (OQAM), orthogonal frequency division multiplexing (OFDM), relaying networks. I...|$|R
40|$|Objectives: In phase II {{clinical}} trials in oncology, the potential efficacy {{of a new}} treatment regimen is assessed in terms of anticancer activity. The standard approach consists of a single-arm two-stage design where a single binary endpoint is compared to a specified target value. However, a new drug would still be considered promising if it showed a lower tumor response rate than the target level but would lead, for example, to disease stabilization. Methods: We present an analytical solution for the calculation of the type I and type II error rate for a two-stage design where the hypothesis test considers two endpoints and provide optimal and minimax solutions. Furthermore, the problem of inference about the two single endpoints following rejection of the global <b>null</b> hypothesis is <b>addressed</b> by deriving a multiple test procedure that controls the experimentwise type I error rate in the strong sense. Results: The proposed methods are illustrated with a real data example, and the new design is tabulated {{for a wide range}} of parameter values. Similar to two-stage designs with a single endpoint, the characteristics of optimal and minimax designs with two endpoints with respect to expected and maximum sample size can be quite different. Therefore, the choice of an admissible design may be a valuable compromise. Conclusions: The new procedure extends Simon’s two-stage design to two endpoints. This approach allows a more comprehensive assessment of the overall picture of anti-tumor efficacy of a new treatment than restriction to a single outcome...|$|R
40|$|Network null {{models are}} {{important}} to drawing conclusions about individual- and population-(or graph) level metrics. While the null models of binary networks are well studied, recent literature on weighted networks suggests that: (1) many so-called 'weighted metrics' do not actually depend on weights, and (2) many metrics that supposedly measure higher-order social structure actually are highly correlated with individual-level attributes. This is important for behavioural ecology studies where weighted network analyses predominate, {{but there is no}} consensus on how null models should be specified. Using real social networks, we developed three <b>null</b> models that <b>address</b> two technical challenges in the networks of social animals: (1) how to specify null models that are suitable for 'proportion-weighted networks' based on indices such as the half-weight index; and (2) how to condition on the degree- and strength-sequence and both. We compared 11 metrics with each other and against null-model expectations for 10 social networks of bottlenose dolphin, Tursiops aduncus, from Shark Bay, Australia. Observed metric values were similar to null-model expectations for some weighted metrics, such as centrality measures, disparity and connectivity, whereas other metrics such as affinity and clustering were informative about dolphin social structure. Because weighted metrics can differ in their sensitivity to the degree-sequence or strength-sequence, conditioning on both is a more reliable and conservative null model than the more common strength-preserving null-model for weighted networks. Other social structure analyses, such as community partitioning by weighted Modularity optimization, were much less sensitive to the underlying null-model. Lastly, in contrast to results in other scientific disciplines, we found that many weighted metrics do not depend trivially on topology; rather, the weight distribution contains important information about dolphin social structure...|$|R
40|$|The false {{discovery}} procedure introduced by Benjamini and Hochberg in 1995 {{has become a}} mainstream method for large scale simultaneous inference {{in a variety of}} bioinformatics problems. The procedure controls the false discovery rate (FDR) at a specified level α assuming that the distribution function F 0 of null p-values Pi is U(0, 1). In a recent paper, Efron (2004) brought to attention that, often, the empirical null p-values do not conform to the theoretical U(0, 1) and the biased distribution of nulls can affect the FDR. Indeed, linear regression settings aimed for genomewide association study provide good examples of a biased F 0. Under these scenarios, the number of covariates p is much greater than the sample size n, which eliminates the option of fitting the full regression model. Nevertheless, a resolution of fitting an abundant number of partial models permits an empirical estimation of the distribution of <b>null</b> p-values. In <b>addressing</b> the bias in F 0, it is more convenient to study the bias in the distribution function G 0 of z-values: Zi = Φ− 1 (Pi). Estimating the deviation of Zi from the N(0, 1) is tantamount to estimating the departure of Pi from the U(0, 1). Efron (2004) proposed a location-scale correction to the empirical distribution G 0. In this proposal, we show that the bias in G 0 can not be represented by a location-scale alteration alone. We propose a skewness adaptation to G 0. We show that variants of a skewed G 0 can lead to better control of FDR compared with the default N(0, 1). To illustrate the procedure, we examine data which are generated using a stochastic process that creates polymorphisms on chromosomal regions. The data can be analyzed using regression models. ...|$|R
40|$|The {{purpose of}} this study is to {{identify}} determinants of scholarly recognition in the study of higher education. The study compares recognized faculty in the study of higher education, who had received scholarly recognition in the form of being appointed to editorial boards, and unrecognized faculty of higher education. Though 33 faculty members were identified as editorial board members from the 709 faculty listed in the Directory of Higher Education Programs and Faculty (1982), 27 returned usable questionnaires. A control group of 27 faculty was randomly drawn from the 676 faculty remaining in the Directory (1982). The independent variables used to investigate scholarly recognition were divided into three sets: 20 productivity, 12 professional, and 15 background variables. After three factor analyses, the three sets were reduced to 16 productivity variables, 11 professional variables, and 11 background variables. ^ After three discriminant analyses, the <b>null</b> hypotheses were <b>addressed.</b> Because the discriminant function for the background variables was nonsignificant, the null hypothesis concerning background measures was accepted; whereas, because the functions for the productivity and professional variables were significant, the other two null hypotheses were rejected. ^ There were five productivity and three professional discriminating variables. The discriminating productivity variables were: (a) the number of multiple authored journal articles; (b) the number of privately funded research grants; (c) the number of multiple authored technical reports; (d) the number of federally funded teaching grants; and (e) the number of other funded (i. e., other than state, federally, or privately funded) teaching grants. The discriminating professional variables were: (a) the amount of interest in research or teaching; (b) the number of memberships in national associations of education; and (c) the percent of time spent teaching. In essence, the recognized scholar in higher education publishes more multiple authored journal articles, acquires more privately funded research grants, has more interest in research and less in teaching, writes fewer multiple authored reports, and acquires fewer federally and other funded teaching grants than unrecognized scholars. ...|$|R
40|$|The aim of {{the study}} was to {{investigate}} the impact of mathematics software on students learning achievements. To accomplish this purpose four research questions were imposed addressing the effect of different treatments conditions. These questions were further disintegrated into ten <b>null</b> hypotheses, <b>addressing</b> the effectiveness of mathematics software instructional method, teacher facilitated mathematics software instruction and traditional instruction (Independent variable) through achievement test (Dependent variable). The study was completed in two phases. In the first phase mathematics software for imparting instruction to Experimental-I and Experimental-II groups was developed in three content areas (Integers, Algebra and Geometry) for 6 th grade. Initially the researcher divided the contents into teaching units and modifies it to make it compatible with technology. Each teaching unit contained some material presented in the form of text, and then examples were given in order to further elaborate the concept. Afterward some exercises with step by step feedback were provided to the users and finally assessments were given to the students. These developed units were given to a team of software developers; they develop a program as per instructional design provided by the researcher. After completing the coding, the program (software) was piloted on 40 students of 6 th grade. On the other hand to ensure the quality it was given to the software quality supervisor, user interface designer and programmer to report the quality of the program. On the basis of students and experts opinion the software program was revised. The second phase {{of the study was}} experimental. To investigate the impact of developed software on students’ achievements, 312 students in four randomly selected schools of Punjab, one from each stratum were placed under three treatment conditions. The first group served the purpose of control group and was taught by traditional method of instruction. The second group (Experimental-I) was treated by mathematics software instruction, in this group the role of teacher was passive and teacher only monitored the progress and maintained the discipline in the classroom. In third group (Experimental-II) the role of teacher was to facilitate the students when they were working on program. 78 sixth grade students of each selected school were randomly selected, and assigned to three treatment conditions after pre-testing. The intervention lasted for ten weeks, at the end, the students were post tested. An achievement test in the three content areas was developed considering three cognitive levels of revised Bloom’s taxonomy. Test was developed according to the specification and piloted to 500 students for reliability and was validated through experts, an attitude scale to measure the students liking of mathematics accompanied test. ANOVA was run to analyze the data; on pre-test score showing equality of groups. On post-test score ANOVA results indicated significant difference between groups, Scheffe, post hoc test highlighted that Experimental-II group outperformed the remaining two groups. Across gender comparison showed again the enhanced performance of Experimental-II group and across local analysis also revealed the same fact. Therefore, the researcher concluded that teacher facilitated mathematics software instruction may be a better instructional method. The researcher recommended more research in the field with different population and different subject areas...|$|R

