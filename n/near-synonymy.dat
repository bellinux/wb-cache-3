20|0|Public
40|$|Semantic Representations of Near-Synonyms for Automatic Lexical Choice Philip Edmonds Doctor of Philosophy Graduate Department of Computer Science University of Toronto 1999 We {{develop a}} new {{computational}} model for representing the fine-grained meanings of nearsynonyms and the differences between them. We also develop a sophisticated lexical-choice process that can decide which of several near-synonyms is most appropriate in any particular context. This research has direct applications in machine translation and text generation, and also in intelligent electronic dictionaries and automated style-checking and document editing. We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for <b>near-synonymy.</b> We then propose a preliminary theory to account for <b>near-synonymy</b> in which {{the meaning of a}} word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit diff [...] ...|$|E
40|$|In this paper, {{we propose}} a {{frame-based}} approach to polysemy by analyzing three near-synonymous verbs biaoshi (表示), biaoda (表達) and biaolu (表露). Based on Liu and Wu (2004), this paper further discusses the cross-frame phenomena of near-synonyms with a detailed comparison of their syntactic and collocational patterns. It is shown that polysemy among related verbs may be well defined and manifested {{within the framework}} of Frame Semantics. In addition, following Liu and Wu’s finding, we also want to further explain the relationship between frames and the mechanism of forming polysemy. Keyword: Frame Semantics, metaphor, metonymy, <b>near-synonymy,</b> polysemy, verbs of Expression...|$|E
40|$|Many {{words have}} more than one {{translation}} across languages. Such "translation ambiguous" words are generally translated more slowly and less accurately than their unambiguous counterparts. Additionally, there are multiple sources of translation ambiguity including within language semantic ambiguity and <b>near-synonymy.</b> The present study examines the extent to which word context and translation dominance reduce the difficulties associated with translation ambiguity, using a primed translation recognition task. We further examine how dominance and linguistic context influence translation ambiguity stemming from the two sources, specifically translation ambiguity derived from semantic ambiguity ("meaning translation ambiguity") and translation ambiguity derived from <b>near-synonymy</b> ("synonym translation ambiguity"). Participants were presented with English-German word pairs that were preceded by a related or unrelated prime and were asked to decide if the word pairs were translation equivalents or not. The speed and accuracy with which pairs were recognized as correct translations was examined with respect to translation ambiguity, source of ambiguity, prime relatedness, and translation dominance. Translation-unambiguous pairs were recognized more quickly and accurately than translation-ambiguous pairs. Further, target-translation pairs preceded by a related prime were responded to more quickly than pairs preceded by an unrelated prime. Dominant translations were responded to more quickly than subordinate translations, and source of ambiguity and translation dominance marginally interacted, such that meaning translation-ambiguous words were more influenced by dominance than synonym translation-ambiguous words. We discuss the results in reference to models of bilingual memory and propose a new model that makes specific predictions about translation ambiguity, the Revised Hierarchical Model of Translation Ambiguity...|$|E
40|$|We {{propose a}} cluster {{analysis}} approach to quantify <b>near-synonymy</b> relations and compare non-parametric and parametric methods. The first approach is model free since {{it does not}} assume an underlying model of lexical knowledge but it uncovers the group structure in the set of near-synonyms of a target word by comparing the list of synonyms of the given entry with those of its near-synonyms as contained into available thesauri. Then, in order to validate the results provided by the cluster analysis, a statistical model is introduced for analyzing human judgments of perceived degree of synonymy, also by a relationship with subjects’ characteristics. Empirical evidence for a selected word of Italian is presented and discussed...|$|E
40|$|Abstract. This paper {{focuses on}} the {{specific}} method of <b>near-synonymy</b> analysis in English and Serbian. Currently, this issue has been extensively researched in English through lexical choice analysis while using various mathematical equations to distinguish the proximity of meaning among the selected synonymous expressions. 1 In this research paper, near synonyms are analysed through the most frequent collocational framework of the node word and the matching co-occurrences of its suggested near synonyms in Serbian {{as well as in}} English. Furthermore, this method is proposed as a useful means of selecting near synonyms within the forthcoming thesauri of the English and Serbian languages. Key words: specific method, near synonyms, collocational framework, grammatical gender, descriptive adjectives 1...|$|E
40|$|This corpus based {{investigation}} {{deals with}} the present-day usage of and the semantic relation between the two verbs rise and arise. Concordance lines containing various forms of the two verbs in question have been taken from six different (sub) corpora and were examined in view of their collocational and semantic characteristics. The basic aims were to investigate the nowadays status of the verbs rise and arise and whether they (still) {{can be regarded as}} synonyms. The results show that both verbs can sometimes be used synonymously. Their general semantic relation appeared to be <b>near-synonymy.</b> Furthermore, both verbs seem to have developed a semantic specialisation, which is regarded a counterargument for the thesis that the verb arise {{is on the verge of}} dying out...|$|E
40|$|We {{develop a}} new {{computational}} model for representing the fine-grained meanings of nearsynonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation. We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for <b>near-synonymy.</b> We then propose a preliminary theory to account for <b>near-synonymy,</b> relying crucially {{on the notion of}} granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together. We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient compututation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of finegrained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behaviour. An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus. ...|$|E
40|$|In this paper, {{we propose}} a new primary lexical {{semantic}} relation—paranymy, {{to explain a}} relation for concept classification {{that has not yet}} be dealt with in WordNet. We observe the relations among the same set of coordinate terms and find out that the concept of antonymy often appears among those coordinate terms. However, antonymy and other relations, such as <b>near-synonymy,</b> are inadequate to account for their conceptual classification or entailed knowledge. In order to give a more precise and richer representation of lexical conceptual structure and ontology, we proposed a new relation of paranymy. Our proposal is based careful examination of data from Chinese Wordnet and WordNe. Our attempt in a way incorporates semantic fields within a wordnet structure. We further distinguishe three types of paranymy: complementary, contrary and overlapping. This classification is further elaborated further a defining paradigm based on perception or convention. ...|$|E
40|$|Normal {{language}} user’s word-association intuition (e. g. drunken – stagger) {{raises questions}} about the mental lexicon organization and its application for natural language processing tasks. We present an automatic contextually related words (contexonym) organizing model (ACOM) that reflects this intuition, giving one of the possible answers to this question. Trained on large corpora, the model (1) selects contexonyms for a given word and (2) classifies these groups of related words on a geometric representation. Some near-synonyms discussed in <b>Near-Synonymy</b> and Lexical Choice (Edmonds and Hirst, 2002) were chosen to test the model. The results showed that our model provides valuable contexonyms that reflect different usage and nuance of each word. Furthermore, the test on polysemous words showed that the model can classify contexonyms by grouping the different senses of a target word. The model can can be used as both theoretical lexicon-related research and practical natural language processing (NLP) research as well as an interactive reference...|$|E
40|$|Usage-based {{linguistics}} abounds with {{studies that}} use statistical classification models to analyse either textual corpus data or behavioral experimental data. Yet, {{before we can}} draw conclusions from statistical models of empirical data that we can feed back into cognitive linguistic theory, we need to assess whether the text-based models are cognitively plausible and whether the behavior-based models are linguistically accurate. In this paper, we review four case studies that evaluate statistical classification models of richly annotated linguistic data by explicitly comparing {{the performance of a}} corpus-based model to the behavior of native speakers. The data come from four different languages (Arabic, English, Estonian, and Russian) and pertain to both lexical as well as syntactic <b>near-synonymy.</b> We show that behavioral evidence is needed in order to fine-tune and improve statistical models built on data from a corpus. We argue that methodological pluralism and triangulation are the keys for a cognitively realistic linguistic theory...|$|E
40|$|Synonymy is an {{important}} part of all natural language but not all synonyms are created equal. Just because two words are synonymous, it usually doesn’t mean they can always be interchanged. The problem that we attempt to address is that of <b>near-synonymy</b> and choosing the right word based purely on its surrounding words. This new computational method, unlike previous methods used on this problem, is capable of making multiple word suggestions which more accurately models human choice. It contains a large number of words, does not require training, and is able to be run in real-time. On previous testing data, when able to make multiple suggestions, it improved by over 17 percentage points on the previous best method and 4. 5 percentage points on average, with a maximum of 14 percentage points, on the human annotators near-synonym choice. In addition this thesis also presents new synonym sets and human annotated test data that more accurately fits this problem...|$|E
40|$|The {{attitudinal}} lexeme on {{the domain}} of kesenangan in Indonesia language has not shown such clear meaning relationship, for both the common and diagnostic meaning of the lexemes. Those lexemes have such circular definitions, confusing upon their use. This study is conducted using a qualitative research approach employing content analysis technique. The {{aim of this study}} is to find out lexical relation and semantic meaning in attitudinal lexeme in {{the domain of}} kesenangan (joy) in Indonesian language. Data is collected from seven Indonesian dictionaries, two magazines, five newspapers, and six literary works. All data is analyzed using a component analysis in the semantic theory. The research findings show that fourteen (14) lexemes (senang, nikmat, enak, puas, asyik, sukacita, ria, bangga, lega, bahagia, gembira, girang, riang, and ceria) of attitudinal lexemes are related with the domain of kesenangan. The result shows that hyponymy and synonymy lexical relations occur in the domain of kesenangan. Synonymy relation consists of <b>near-synonymy</b> and propositional synonymy. In this case, absolute synonymy is not found...|$|E
40|$|In {{this thesis}} we propose an {{unsupervised}} system for semantic relation extraction from texts. The automatic extraction of semantic relationships is crucial both in ontology learning from text and for semantic annotation and represents {{a solution to}} the "knowledge acquisition bottleneck" {{in the context of the}} Semantic Web. The developed system, assessed on English and Italian language but applicable to any other languages, takes as input pairs of words and determines whether there is a semantic relationship between these words. The initial pairs of terms are extracted from a "Target Corpus" by an unsupervised statistical system in charge of determining if two terms can be considered "distributionally similar", on the assumption of distributional semantics that "the meaning of a word is strongly related to the contexts in which it appears. " To verify that there is actually a semantic relation between two terms and determine its nature, the system searches for words on a "Support Corpus" (the Web) in the context of lexico-syntactic "reliable" (low "recall" but "high precision") patterns, where these words appear in the same sentence (as, for example, the words "steer" and "car" in the phrase "the steer is part of the car"). This thesis describes the overall process that led to the development of the RelEx system, starting from the definition and application of the lexico-syntactic patterns, and including the measures used to assess the reliability of specific semantic relations that the system suggests. The work focuses on the semantic relations of hyponymy ("is_a"), meronymy ("part_of") and co-hyponymy (i. e. two terms are hyponyms of the same term, as "lion" and "tiger" with respect to "feline"). The approach may however be extended to extract other relationships by changing the battery of reliable patterns used. The precision of the system was evaluated as 83. 3 % for hyponymy, 75 % for meronymy and 72. 2 % for co-hyponymy, demonstrating the validity of the proposed approach. In this work, in addition to the novel concepts of "Closed Pattern" and "Open Pattern", two new technologies are described. The first methodology, called "trans-language boosting" is devoted to the application of reliable patterns and pairs of terms expressed in different languages with the aim of increasing the performance of the system. The second technique, defined as "cross-reference <b>near-synonymy</b> extraction", is based on the application of "open" patterns for the recognition of <b>near-synonymy</b> relations...|$|E
40|$|This paper {{analyzes}} {{the concept of}} opposition and describes a fully unsupervised method for its automatic discrimination from <b>near-synonymy</b> in Distributional Semantic Models (DSMs). The discriminating method {{is based on the}} hypothesis that, even though both near-synonyms and opposites are mostly distributionally similar, opposites are different from each other in at least one dimension of meaning, which can be assumed to be salient. Such hypothesis has been implemented in APAnt, a distributional measure that evaluates the extent of the intersection among the most relevant contexts of two words (where relevance is measured as mutual dependency), and its saliency (i. e. their average rank in the mutual dependency sorted list of contexts). The measure – previously introduced in some pilot studies – is presented here with two variants. Evaluation shows that it outperforms three baselines in an antonym retrieval task: the vector cosine, a baseline implementing the co-occurrence hypothesis, and a random rank. This paper describes the algorithm in details and analyzes its current limitations, suggesting that extensions may be developed for discriminating antonyms not only from near-synonyms but also from other semantic relations. During the evaluation, we have noticed that APAnt also has a particular preference for hypernyms...|$|E
40|$|The {{present study}} {{provides}} a preliminary qualitative {{investigation into the}} presence and treatment of <b>near-synonymy</b> in selected articles from the Vocabulaire Juridique (VJ) and the Oxford Dictionary of Law (ODL), two compact paper dictionaries currently available on the market. Both dictionaries target native speakers (French and British users, respectively) with various expertise (mainly non-experts and semi-experts). Lemmas are organized in a semasiological structure, and micro- and medio-structures are used to explicitate the semantic relations between terms (e. g. via integrated and non-integrated cross-references). A comparison of the meaning descriptions and cross-referencing systems in VJ and ODL suggests that most near-synonyms are pragmatically motivated. They comprise stylistic, diachronic and diatopic variants, as well as acronyms (e. g. from the Incoterms), borrowings and legal transplants, and instances of francisation. Reference device and near-synonym may constitute the only segment in the meaning description, be integrated in the meaning description, or account for separate additions. However, the data suggests that the cross-referencing system and the cross-references devices and conventions adopted in VJ and ODL are inconsistent and may thus fail to assist dictionary users retrieve information that serves their needs...|$|E
40|$|Synonymous {{relationships}} among biomedical terms are extensively annotated within specialized terminologies, implying that synonymy {{is important for}} practical computational applications within this field. It remains unclear, however, whether text mining actually benefits from documented synonymy and whether existing biomedical thesauri provide adequate coverage of these linguistic relationships. In this study, we examine the impact and extent of undocumented synonymy within a very large compendium of biomedical thesauri. First, we demonstrate that missing synonymy has a significant negative impact on named entity normalization, an important problem {{within the field of}} biomedical text mining. To estimate the amount synonymy currently missing from thesauri, we develop a probabilistic model for the construction of synonym terminologies that is capable of handling a wide range of potential biases, and we evaluate its performance using the broader domain of <b>near-synonymy</b> among general English words. Our model predicts that over 90 % of these relationships are currently undocumented, a result that we support experimentally through "crowd-sourcing. " Finally, we apply our model to biomedical terminologies and predict that they are missing the vast majority (> 90 %) of the synonymous relationships they intend to document. Overall, our results expose the dramatic incompleteness of current biomedical thesauri and suggest the need for "next-generation," high-coverage lexical terminologies...|$|E
40|$|Prepositions as a word class pose various {{questions}} as to the relation between lexical and functional language units and {{their place in the}} lexicon (Jolly 1991, Šarić and Reindl 2001). Though often referred to as function words, prepositions show a) systematic semantic relations, ie. <b>near–synonymy,</b> polysemy, antonymy and b) a wide variety of lexical and functional (grammatical) uses, indicating a complex interplay of systematic features and contextual modifications which participate in the formation of their meaning. Semantic relations such as antonymy are mostly discussed in terms of adjectives, nouns and verbs, leaving out a detailed description of antonymy effects in other word classes such as prepositions (e. g. Lyons 1977, Cruse 1986, Jones et al. 2012). By adopting the methodology of antonymy research developed for identifying and extracting antonyms from corpora, we examine the co–occurrence of prepositional antonyms in the Croatian National Corpus. We take up the cognitive linguistic position of examining antonymy as a prototype based category based on both conceptual opposition and contextual modifications (Paradis et al. 2009), and we observe its workings on the novel prepositional dataset. Based on the primary domains and conceptual structures that motivate prepositional opposition formation, we divide the antonyms into spatial (directional and locational), temporal and non–dimensional types. For each of the antonym types there are different contextual modifications and conceptual structures that shape these antonymy relations, indicating a complex interplay between language system and language use...|$|E
40|$|The paper {{reports from}} the project «From Parallel Corpus to Wordnet » at the University of Bergen (2001 - 2004), which explores a method for {{deriving}} wordnet relations such as synonymy and hyponymy from data extracted from parallel corpora. Assumptions behind the method are that semantically closely related words ought to have strongly overlapping sets of translations, and words with wide meanings {{ought to have a}} higher number of translations than words with narrow meanings. Furthermore, if a word a is a hyponym of a word b (such as tasty of good, for example), then the possible translations of a ought to be a subset of the possible translations of b. Based on assumptions like these a set of definitions are formulated, defining semantic concepts like, e. g., ‘synonymy’, ‘hyponymy’, ‘ambiguity ’ and ‘semantic field ’ in translational terms. The definitions are implemented in a computer program which takes words with their sets of translations from the corpus as input and performs the following calculations: (1) On the basis of the input different senses of each word are identified. (2) The senses are grouped in semantic fields based on overlapping sets of translations, such overlap being assumed to indicate semantic relatedness. (3) On the basis of the structure of a semantic field a set of features is assigned to each individual sense in it, coding its relations to other senses in the field. (4) Based on intersections and inclusions among these feature sets a semilattice is calculated with the senses as nodes. According to our hypothesis, hyponymy/hyperonymy, <b>near-synonymy</b> and other semantic relations among the senses now appear through dominance and other relations among the nodes in the semilattice. Thus, the semilattice is supposed to contain some of the semantic information we want to represent in wordnets. (5) In accordance with this assumption, thesaurus-like entries for words are generated from the information in the semilattice. In the project these assumptions are tested against data from the English-Norwegian parallel corpus ENPC (Johansson (1997)). 1...|$|E
40|$|M. A. This {{dissertation}} is {{an investigation}} into the lexicographical treatment of synonymy in Afrikaans explanatory dictionaries - especially the Verklarende Handwoordeboek van die Afrikaanse Taal (HAT). It is divided into two chapters of theoretical discussion, a chapter of practical examples from HAT and a final chapter with conclusions and suggestions for improvement. Chapter two discusses the concept of 'meaning'. In order to arrive at an understanding of 'meaning', it is necessary to have a measure knowledge of some aspects of the theory. First of all there will be a brief discussion of the theory of reference and cognitive/ descriptive meaning. A lexicographer should describe the conceptual sense of a word. In order to do this he uses sense relations, namely hyponymy, antonymy and synonymy. These relations are interdependent. Hyponymy and antonymy will be touched on briefly, while synonymy will be discussed thoroughly. As far as synonymy is concerned there will be, among other aspects, a discussion of the concept 'synonymy', the difference between absolute and <b>near-synonymy</b> and polysemy and homonymy. Chapter three lists the lexicographical criteria in terms of which synonymy must be applied in a dictionary. In this chapter there will be a look at aspects such as the guide {{to the use of the}} dictionary (front matter), synonym definitions, the arrangement of synonyms, contextual information and a consistant cross-reference. The system according to which syonymy must be applied in a dictionary must be thoroughly explained in the front matter. In the case of synonym definitions every synonym must be taken up as a lemma and supplied with a lexicographic definition. The member of a synonym paradigm with the highest use frequency must be supplied with a lexicographic definition, while the other members of a synonym paradigm get synonym definitions. With reference to chapter three, chapter four deals with an evaluation of the application of synonymy in HAT. All the criteria in chapter three will be discussed on the basis of examples from HAT. A main conclusion reached, is that the criteria are not always maintained. In many cases the criteria are correctly maintained, but deviations of the criteria are, inter alia: not all the synonyms are taken up as lemmas; cross-references are not always correct and consistant; the lexicographic definition is placed with the wrong lemma; the members of a synonym paradigm are not arranged according to the highest use frequency. Chapter five gives a summary of the conclusions regarding criteria. There are also suggestions for improvement, regarding the application of synonymy in the case of multi-word lexical units and sublexical units. Lexicographers are working on a central data base for Afrikaans, The main problem with dictionaries, namely inconsistant cross-referencing, will be reduced if such a data base functions as the basis for lexicographic activity...|$|E
40|$|National audienceFrequency is a {{versatile}} concept in contemporary linguistic theories. One such theory, Cognitive Linguistics, is a usage-based approach to language {{that makes no}} principled distinction between language use and language structure. In Cognitive Linguistics, the more frequently speakers encounter a linguistic unit, the more that linguistic unit is entrenched, i. e. established as a cognitive routine (Langacker, 1987). First-generation usage-based grammars are theory-driven (Lakoff, 1987; Langacker, 1987, 1991). They make extensive reference {{to the role of}} repetition in the establishment of linguistic conventions (Bybee, 1985, 2006, 2010; Bybee & Hopper, 2001; Langacker, 1999) but do very {{little in the way of}} empirical methods. More specifically, usage-based approaches rely on a definition of frequency that is both intuitive and abstract. According to them, what is decisive to assess entrenchment is not so much the frequency that linguists can measure, but the frequency that speakers perceive in linguistic experience. Recently, a small yet growing community of cognitive linguists have begun to realize that the implications of their own theoretical framework were essentially empirical (Geeraerts, Kristiansen, & Peirsman, 2010; Gibbs, 2007; Glynn, 2010 a; Gries, Hampe, & Schönefeld, 2005). Since corpus-linguistics provides a comprehensive array of methods to capture context and knowledge, it has expectedly become central in the investigation of cognitive patterns of usage (Gries & Stefanowitsch, 2006). Traditionally, corpus linguistics explores frequencies of occurrence, frequencies of co-occurrence, and measures of dispersion. More precisely, it makes a monofactorial use of frequency data: one dependent variable is correlated with the behavior of one dependent variable. This is too simple if we approach language holistically and admit that the structure of meaning is based on human experience, and that meaning "involves both conceptual content and the construal of that content" (Langacker, 2008, p. 44). Given that just about anything in language is influenced by several factors at the same time, one challenge that corpus-based Cognitive Linguistics has to address is whether quantitative analysis is possible for the study of usage-based semantics. In this lecture, I address that challenge. I first review recent works that use new usage-based methods to capture semantic relations between near-synonyms (Divjak, 2006 a, 2006 b, 2010; Divjak & Gries, 2008; Glynn, 2010 b) before presenting my own case study. I investigate the use of two English intensifiers: quite and rather. When quite and rather modify attributive adjectives, they can occur in pre-determiner position, an idiosyncratic behavior that other intensifiers do not show: (1) I know it's a fairly / *fairly a difficult question. (2) That's proved to be a quite / quite a difficult question to answer. (3) That is a rather difficult / rather a difficult question to answer. Allerton (1987) observes that, depending on whether the adjective that quite modifies is scalar or absolutive, some restrictions apply, a sign that pre-determiner position is more than just a matter of style or formality: (4) I mean this is quite a good idea / ??a quite good idea actually. (4 ') This is ??quite an excellent idea / a quite excellent idea. The question that naturally arises is whether there is any difference in meaning between the pre-determiner and pre-adjectival positions of quite and rather. Another question is whether these two intensifiers are synonyms. My working hypothesis is that quite and rather have a semantic component paired with a syntactic component over and above their specification of degree. To test this hypothesis, I propose an original method that combines analytical and multivariate statistics. First, I extract all combinations from the 100 M-word British National Corpus (World Edition). Then, I implement a technique known as multiple distinctive collexeme analysis (Gries & Stefanowitsch, 2004) to determine which adjectives are most distinctively attracted to each intensifier depending on the syntactic construction. Finally, I use the frequencies of distinctive adjectives as input for correspondence analysis (Benzécri, 1984; Greenacre, 2007), a multifactorial approach that provides a low-dimensional map of the data by calculating matrices between the rows and the columns of a contingency table using the χ 2 test. My results show that: i. adjectives cluster differently depending on (a) the intensifier that modifies them, (b) the syntax of the intensifying construction where they occur; ii. quite constructions and rather constructions cluster differently depending on their syntactic profiles (pre-determiner position vs. pre-adjectival position; intensifier + attributive adjective vs. intensifier + predicative adjective); iii. quite and rather attract semantically distinct adjective classes and are not, as expected, exact synonyms. References Allerton, D. J. (1987). English Intensifiers and their Idiosyncrasies. In R. Steele & T. Threadgold (Eds.), Language Topics: Essays in Honour of Michael Halliday (Vol. 2, pp. 15 - 31). Amsterdam: John Benjamins. Benzécri, J. -P. (1984). Analyse des correspondances, exposé élémentaire (Vol. 1). Paris: Dunod. Bybee, J. (1985). Morphology : a study of the relation between meaning and form. Amsterdam; Philadelphia: J. Benjamins. Bybee, J. (2006). From Usage to Grammar: The Mind's Response to Repetition. Language, 82 (4), 711 - 733. Bybee, J. (2010). Language, usage and cognition. Cambridge; New York: Cambridge University Press. Bybee, J., & Hopper, P. (2001). Frequency and the Emergence of Linguistic Structure. Amsterdam: John Benjamins. Divjak, D. (2006 a). Ways of intending: Delineating and structuring near-synonyms. In S. Gries & A. Stefanowitsch (Eds.), Corpora in Cognitive Linguistics (Vol. 172, pp. 19 - 56). Berlin; New York: Mouton de Gruyter. Divjak, D. (2006 b). Ways of trying in Russian: Clustering behavioral profiles. Corpus Linguistics and Linguistic Theory, 2 (1), 23 - 60. Divjak, D. (2010). Structuring the lexicon : a clustered model for <b>near-synonymy.</b> Berlin; New York: De Gruyter Mouton. Divjak, D., & Gries, S. T. (2008). Clusters in the mind?: Converging evidence from near synonymy in Russian. The Mental Lexicon, 3 (2), 188 - 213. Geeraerts, D., Kristiansen, G., & Peirsman, Y. (2010). Advances in cognitive sociolinguistics. New York, N. Y. : Mouton de Gruyter. Gibbs, R. W. (2007). Why cognitive linguists should care more about empirical methods. In M. Gonzalez-Marquez, I. Mittelberg, S. Coulson & M. J. Spivey (Eds.), Methods in Cognitive Linguistics (pp. 2 - 18). Amsterdam: John Benjamins. Glynn, D. (2010 a). Corpus-Driven Cognitive Semantics. An introduction to the field. In D. Glynn & K. Fischer (Eds.), Corpus- Driven Cognitive Semantics. Quantitative approaches. (pp. 1 - 42). Berlin: Mouton de Gruyter. Glynn, D. (2010 b). Synonymy, Lexical Fields, and Grammatical Constructions. A study in usage-based Cognitive Semantics. In H. -J. Schmid & S. Handl (Eds.), Cognitive Foundations of Linguistic Usage-Patterns (pp. 89 - 118). Berlin: Mouton de Gruyter. Greenacre, M. J. (2007). Correspondence analysis in practice (2 nd ed.). Boca Raton: Chapman & Hall/CRC. Gries, S., Hampe, B., & Schönefeld, D. (2005). Converging evidence: Bringing together experimental and corpus data on the association of verbs and constructions. Cognitive Linguistics, 16 (4), 635 - 676. Gries, S., & Stefanowitsch, A. (2004). Extending collostructional analysis: A corpus-based perspective on 'alternations'. International Journal of Corpus Linguistics, 9 (1), 97 - 129. Gries, S. T., & Stefanowitsch, A. (2006). Corpora in cognitive linguistics : corpus-based approaches to syntax and lexis. Berlin: Mouton de Gruyter. Lakoff, G. (1987). Women, Fire, and Dangerous Things. Chicago: University Of Chicago Press. Langacker, R. W. (1987). Foundations of cognitive grammar (Vol. 1). Stanford, CA: Stanford University Press. Langacker, R. W. (1991). Foundations of cognitive grammar (Vol. 2) : Stanford. Langacker, R. W. (1999). Grammar and conceptualization. Berlin: Mouton de Gruyter. Langacker, R. W. (2008). Cognitive grammar : a basic introduction. Oxford: Oxford University Press...|$|E

