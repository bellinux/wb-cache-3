31|9|Public
25|$|<b>Neyman–Pearson</b> <b>theory</b> can {{accommodate}} both prior probabilities {{and the costs}} of actions resulting from decisions. The former allows each test to consider the results of earlier tests (unlike Fisher's significance tests). The latter allows the consideration of economic issues (for example) as well as probabilities. A likelihood ratio remains a good criterion for selecting among hypotheses.|$|E
25|$|The {{dispute over}} {{formulations}} is unresolved. Science primarily uses Fisher's (slightly modified) formulation as taught in introductory statistics. Statisticians study <b>Neyman–Pearson</b> <b>theory</b> in graduate school. Mathematicians {{are proud of}} uniting the formulations. Philosophers consider them separately. Learned opinions deem the formulations variously competitive (Fisher vs Neyman), incompatible or complementary. The dispute has become more complex since Bayesian inference has achieved respectability.|$|E
2500|$|The {{orthodox}} <b>Neyman-Pearson</b> <b>theory</b> of {{hypothesis testing}} considers how {{to decide whether}} to accept or reject a hypothesis, rather than what probability to assign to the hypothesis. From this point of view, the hypothesis that [...] "All ravens are black" [...] is not accepted gradually, as its probability increases towards one when more and more observations are made, but is accepted in a single action as the result of evaluating the data that has already been collected. As Neyman and Pearson put it: ...|$|E
25|$|Hypothesis testing {{provides}} a means of finding test statistics used in significance testing. The concept of power is useful in explaining the consequences of adjusting the significance level and is heavily used in sample size determination. The two methods remain philosophically distinct. They usually (but not always) produce the same mathematical answer. The preferred answer is context dependent. While the existing merger of Fisher and <b>Neyman–Pearson</b> <b>theories</b> has been heavily criticized, modifying the merger to achieve Bayesian goals has been considered.|$|R
50|$|Fisher {{thought that}} {{hypothesis}} testing was a useful strategy for performing industrial quality control, however, he strongly disagreed that hypothesis testing {{could be useful}} for scientists.Hypothesis testing provides a means of finding test statistics used in significance testing. The concept of power is useful in explaining the consequences of adjusting the significance level and is heavily used in sample size determination. The two methods remain philosophically distinct. They usually (but not always) produce the same mathematical answer. The preferred answer is context dependent. While the existing merger of Fisher and <b>Neyman-Pearson</b> <b>theories</b> has been heavily criticized, modifying the merger to achieve Bayesian goals has been considered.|$|R
40|$|In {{fields that}} are mainly nonexperimental, such as {{economics}} and finance, it is inescapable to compute test statistics and confidence regions {{that are not}} probabilistically independent from previously examined data. The Bayesian and <b>Neyman-Pearson</b> inference <b>theories</b> {{are known to be}} inadequate for such a practice. We show that these inadequacies also hold m. a. e. (modulo approximation error). We develop a general econometric theory, called the neoclassical inference theory, that is immune to this inadequacy m. a. e. The neoclassical inference theory appears to nest model calibration, and most econometric practices, whether they are labelled Bayesian or à la Neyman-Pearson. We derive a general, but simple adjustment to make standard errors account for the approximation error...|$|R
2500|$|It {{seems obvious}} that one cannot both accept the {{hypothesis}} that all P's are Q and also reject the contrapositive, i.e. that all non-Q's are non-P. Yet {{it is easy to}} see that on the <b>Neyman-Pearson</b> <b>theory</b> of testing, a test of [...] "All P's are Q" [...] is not necessarily a test of [...] "All non-Q's are non-P" [...] or vice versa. A test of [...] "All P's are Q" [...] requires reference to some alternative statistical hypothesis of the form [...] of all P's are Q, , whereas a test of [...] "All non-Q's are non-P" [...] requires reference to some statistical alternative of the form [...] of all non-Q's are non-P, [...] But these two sets of possible alternatives are different ... Thus one could have a test of [...] without having a test of its contrapositive.|$|E
2500|$|The {{two forms}} of {{hypothesis}} testing are based on different problem formulations. The original test is analogous to a true/false question; the Neyman–Pearson test is more like multiple choice. In the view of Tukey the former produces a conclusion {{on the basis of}} only strong evidence while the latter produces a decision on the basis of available evidence. While the two tests seem quite different both mathematically and philosophically, later developments lead to the opposite claim. Consider many tiny radioactive sources. The hypotheses become 0,1,2,3... grains of radioactive sand. There is little distinction between none or some radiation (Fisher) and 0 grains of radioactive sand versus all of the alternatives (Neyman–Pearson). The major Neyman–Pearson paper of 1933 also considered composite hypotheses (ones whose distribution includes an unknown parameter). An example proved the optimality of the (Student's) t-test, [...] "there can be no better test for the hypothesis under consideration" [...] (p 321). <b>Neyman–Pearson</b> <b>theory</b> was proving the optimality of Fisherian methods from its inception.|$|E
50|$|<b>Neyman-Pearson</b> <b>theory</b> can {{accommodate}} both prior probabilities {{and the costs}} of actions resulting from decisions. The former allows each test to consider the results of earlier tests (unlike Fisher's significance tests). The latter allows the consideration of economic issues (for example) as well as probabilities. A likelihood ratio remains a good criterion for selecting among hypotheses.|$|E
40|$|Híducational {{research}} {{is based on}} a diverse collection of tech-niques. Glass (Note 1), in an ad-dress to professors of educational research, discussed Kaplan's (1964) distinction between scien-tific methods and techniques. Methods are cross-disciplinary principles such as replication, generalization, and causation, whereas techniques are disci-pline-specific procedures for the conduct of research. Glass classi-fied techniques by their genesis among major disciplines— agronomy, cultural anthropology, biology, economics, psychology, and sociology. He discussed the three major techniques in use in educational research at that time: Fisherian experimental de-sign, common factor analysis, and <b>Neyman-Pearson</b> decision <b>theory,</b> and he advocated greater interdisciplinary training in sci-entific methods and techniques. His recommendation was based on a number of premises that bear reiteration, his paper not being published. Most of the work and support occurred at the University of South Dakota, to which thanks are given...|$|R
40|$|K-means is {{definitely}} {{the most frequently used}} partitional clustering algorithm in the remote sensing community. Unfortunately due to its gradient decent nature, this algorithm is highly sensitive to the initial placement of cluster centers. This problem deteriorates for the high-dimensional data such as hyperspectral remotely sensed imagery. To tackle this problem, in this paper, the spectral signatures of the endmembers in the image scene are extracted and used as the initial positions of the cluster centers. For this purpose, in the first step, A <b>Neyman–Pearson</b> detection <b>theory</b> based eigen-thresholding method (i. e., the HFC method) has been employed to estimate the number of endmembers in the image. Afterwards, the spectral signatures of the endmembers are obtained using the Minimum Volume Enclosing Simplex (MVES) algorithm. Eventually, these spectral signatures are used to initialize the k-means clustering algorithm. The proposed method is implemented on a hyperspectral dataset acquired by ROSIS sensor with 103 spectral bands over the Pavia University campus, Italy. For comparative evaluation, two other commonly used initialization methods (i. e., Bradley & Fayyad (BF) and Random methods) are implemented and compared. The confusion matrix, overall accuracy and Kappa coefficient are employed to assess the methods’ performance. The evaluations demonstrate that the proposed solution outperforms the other initialization methods and can be applied for unsupervised classification of hyperspectral imagery for landcover mapping...|$|R
40|$|Akaike's {{criterion}} {{is used to}} test composite hypotheses; {{for example}} to determine the order of AR models. A modification is presented to test composite hypotheses given an upper-bound on the error of the first kind (<b>Neyman-Pearson).</b> The presented <b>theory</b> is applied to AR order estimation and verified by simulations. The experimental results are so good that we consider the AR order estimation problem as solved. 1 Introduction This paper is a shortened version of the full paper submitted for publication [1]. We test composite hypotheses [2, pp. 86 [...] 96], hypotheses specified {{except for a few}} parameters to be estimated, to select the Auto-Regressive (AR) model order. This is a difficult estimation problem because lower order models are contained in higher order models. Especially in the critical test, e. g. if both models are nearly equally likely, a reliable test is needed. The Akaike criterion [3, 4], a heuristic which is not convincingly motivated, integrates the method of Maximum Like [...] ...|$|R
50|$|The {{dispute over}} {{formulations}} is unresolved. Science primarily uses Fisher's (slightly modified) formulation as taught in introductory statistics. Statisticians study <b>Neyman-Pearson</b> <b>theory</b> in graduate school. Mathematicians {{are proud of}} uniting the formulations. Philosophers consider them separately. Learned opinions deem the formulations variously competitive (Fisher vs Neyman), incompatible or complementary. The dispute has become more complex since Bayesian inference has achieved respectability.|$|E
50|$|Such {{definition}} of confidence can naturally {{seem to be}} satisfied by the {{definition of}} CLs. It remains true thatboth this and the more common (as associated with the <b>Neyman-Pearson</b> <b>theory)</b> versions of the confidence principle are incompatible with the likelihood principle, and therefore no frequentist method {{can be regarded as}} a truly complete solution to the problems raised by considering conditional properties of confidence intervals.|$|E
50|$|Kyburg's inferences {{are always}} relativized {{to a level}} of {{acceptance}} that defines a corpus of morally certain statements. This is like a level of confidence, except that <b>Neyman-Pearson</b> <b>theory</b> is prohibited from retrospective calculation and post-observational acceptance, while Kyburg's epistemological interpretation of probability licenses both. At a level of acceptance, any statement that is more probable than the level of acceptance can be adopted {{as if it were a}} certainty. This can create logical inconsistency, which Kyburg illustrated in his famous lottery paradox.|$|E
40|$|This article {{addresses}} the following question: Where {{and how often}} do distinct probability density functions cross? For example, what {{can be said about}} crossing points between normal and Student’s t or chi-square densi-ties, respectively, and what are the asymptotic crossing points if the degrees of freedom tend to infinity? Crossing points are especially of interest if, for instance, the normal is used as an approximation according to the central limit theorem because they determine the total variation distance and the areas of over- and underestimation of actual probabilities. They are strongly related to the behavior of the density ratio, a ubiquitous quantity in many fields of statistics like <b>Neyman-Pearson</b> or decision <b>theory.</b> We discuss sev-eral examples with respect to the standard normal density ϕ and provide elegant and appealing limiting forms for asymptotic crossing points of ϕ with respect to densities of standardized sums appearing in the central limit theorem. Finally, we derive and investigate the crossing points of ϕ with the t-density with ν degrees of freedom...|$|R
40|$|Due to {{significantly}} improved spectral resolution, hyperspectral imagery can now uncover many subtle materials and substances that cannot be identified a priori or by visual inspection. In this case unsupervised target detection becomes increasingly important in data analysis, specifically in surveillance applications where many unknown signal sources are of interest. This dissertation investigates this interesting area in addressing two major issues {{which are very}} challenging and yet to be explored. One is to design criteria for determining the number of targets of interest within the data to be processed. Despite {{the fact that a}} new concept of Virtual Dimensionality (VD) was recently developed for this purpose the VD estimation techniques are generally less adaptive or flexible to meet different applications. In order to expand the utility of the VD in versatile applications, two methods, Signal Subspace Estimate (SSE) and Neyman Pearson detection-based method, which are previously shown to be promising in estimation of the VD, are further extended by the other two methods. One is called Orthogonal Subspace Projection (OSP) -based VD which is derived from the SSE and NPD. The other is called Maximum OSP-based VD, which incorporated the <b>Neyman-Pearson</b> detection <b>theory</b> into the Maximum Orthogonal Complement Algorithm (MOCA). Both methods include free parameters to tune the sensitivity of the VD to different types of targets of interest in data analysis. The other major issue discussed in the dissertation is anomaly detection. Since there is no prior target knowledge available for the unsupervised target detection, the most commonly used approach to unsupervised target detection is anomaly detection. However, since the types of targets of interest are generally unknown, this dissertation develops a new anomaly detection approach, called Multiple Window Anomaly Detection (MWAD) which is particularly designed to detect various targets with different sizes. Within the context of the MWAD, several widely used anomaly detectors, such as RX detector developed by Reed and Yu and Dual Window Eigen Separation Transform (DWEST) developed by Known and Nasabadi, become its special cases. Finally, a brief summary and further discussion are provided to conclude this dissertation...|$|R
40|$|This Dissertation {{addresses}} the signal acquisition problem using antenna arrays {{in the general}} framework of Global Navigation Satellite Systems (GNSS) receivers. The term GNSS classi es those navigation systems based on a constellation of satellites, which emit ranging signals useful for positioning. Although the American GPS is already available, which coexists with the renewed Russian Glonass, the forthcoming European contribution (Galileo) along with the Chinese Compass will be operative soon. Therefore, a variety of satellite constellations and signals {{will be available in}} the next years. GNSSs provide the necessary infrastructures for a myriad of applications and services that demand a robust and accurate positioning service. The positioning availability must be guaranteed all the time, specially in safety-critical and mission-critical services. Examining the threats against the service availability, it is important to take into account that all the present and the forthcoming GNSSs make use of Code Division Multiple Access (CDMA) techniques. The ranging signals are received with very low precorrelation signal-to-noise ratio (in the order of ��� 22 dB for a receiver operating at the Earth surface). Despite that the GNSS CDMA processing gain o ers limited protection against Radio Frequency interferences (RFI), an interference with a interference-to-signal power ratio that exceeds the processing gain can easily degrade receivers' performance or even deny completely the GNSS service, specially conventional receivers equipped with minimal or basic level of protection towards RFIs. As a consequence, RFIs (either intentional or unintentional) remain as the most important cause of performance degradation. A growing concern of this problem has appeared in recent times. Focusing our attention on the GNSS receiver, it is known that signal acquisition has the lowest sensitivity of the whole receiver operation, and, consequently, it becomes the performance bottleneck in the presence of interfering signals. A single-antenna receiver can make use of time and frequency diversity to mitigate interferences, even though the performance of these techniques is compromised in low SNR scenarios or in the presence of wideband interferences. On the other hand, antenna arrays receivers can bene t from spatial-domain processing, and thus mitigate the e ects of interfering signals. Spatial diversity has been traditionally applied to the signal tracking operation of GNSS receivers. However, initial tracking conditions depend on signal acquisition, {{and there are a number}} of scenarios in which the acquisition process can fail as stated before. Surprisingly, to the best of our knowledge, the application of antenna arrays to GNSS signal acquisition has not received much attention. This Thesis pursues a twofold objective: on the one hand, it proposes novel arraybased acquisition algorithms using a well-established statistical detection theory framework, and on the other hand demonstrates both their real-time implementation feasibility and their performance in realistic scenarios. The Dissertation starts with a brief introduction to GNSS receivers fundamentals, providing some details about the navigation signals structure and the receiver's architecture of both GPS and Galileo systems. It follows with an analysis of GNSS signal acquisition as a detection problem, using the <b>Neyman-Pearson</b> (NP) detection <b>theory</b> framework and the single-antenna acquisition signal model. The NP approach is used here to derive both the optimum detector (known as clairvoyant detector) and the sov called Generalized Likelihood Ratio Test (GLRT) detector, which is the basis of almost all of the current state-of-the-art acquisition algorithms. Going further, a novel detector test statistic intended to jointly acquire a set of GNSS satellites is obtained, thus reducing both the acquisition time and the required computational resources. The eff ects of the front-end bandwidth in the acquisition are also taken into account. Then, the GLRT is extended to the array signal model to obtain an original detector which is able to mitigate temporally uncorrelated interferences even if the array is unstructured and moderately uncalibrated, thus becoming one of the main contributions of this Dissertation. The key statistical feature is the assumption of an arbitrary and unknown covariance noise matrix, which attempts to capture the statistical behavior of the interferences and other non-desirable signals, while exploiting the spatial dimension provided by antenna arrays. Closed form expressions for the detection and false alarm probabilities are provided. Performance and interference rejection capability are modeled and compared both to their theoretical bound. The proposed array-based acquisition algorithm is also compared to conventional acquisition techniques performed after blind null-steering beamformer approaches, such as the power minimization algorithm. Furthermore, the detector is analyzed under realistic conditions, accounting for the presence of errors in the covariance matrix estimation, residual Doppler and delay errors, and signal quantization e ects. Theoretical results are supported by Monte Carlo simulations. As another main contribution of this Dissertation, the second part of the work deals with the design and the implementation of a novel Field Programmable Gate Array (FPGA) -based GNSS real-time antenna-array receiver platform. The platform is intended to be used as a research tool tightly coupled with software de ned GNSS receivers. A complete signal reception chain including the antenna array and the multichannel phase-coherent RF front-end for the GPS L 1 / Galileo E 1 was designed, implemented and tested. The details of the digital processing section of the platform, such as the array signal statistics extraction modules, are also provided. The design trade-o s and the implementation complexities were carefully analyzed and taken into account. As a proof-of-concept, the problem of GNSS vulnerability to interferences was addressed using the presented platform. The array-based acquisition algorithms introduced in this Dissertation were implemented and tested under realistic conditions. The performance of the algorithms were compared to single antenna acquisition techniques, measured under strong in-band interference scenarios, including narrow/wide band interferers and communication signals. The platform was designed to demonstrate the implementation feasibility of novel array-based acquisition algorithms, leaving the rest of the receiver operations (mainly, tracking, navigation message decoding, code and phase observables, and basic Position, Velocity and Time (PVT) solution) to a Software De ned Radio (SDR) receiver running in a personal computer, processing in real-time the spatially- ltered signal sample stream coming from the platform using a Gigabit Ethernet bus data link. In the last part of this Dissertation, we close the loop by designing and implementing such software receiver. The proposed software receiver targets multi-constellation/multi-frequency architectures, pursuing the goals of e ciency, modularity, interoperability, and exibility demanded by user domains that require non-standard features, such as intermediate signals or data extraction and algorithms interchangeability. In this context, we introduce an open-source, real-time GNSS software de ned receiver (so-named GNSS-SDR) that contributes with several novel features such as the use of software design patterns and shared memory techniques to manage e ciently the data ow between receiver blocks, the use of hardware-accelerated instructions for time-consuming vector operations like carrier wipe-o and code correlation, and the availability to compile and run on multiple software platforms and hardware architectures. At this time of writing (April 2012), the receiver enjoys of a 2 -dimensional Distance Root Mean Square (DRMS) error lower than 2 meters for a GPS L 1 C/A scenario with 8 satellites in lock and a Horizontal Dilution Of Precision (HDOP) of 1. 2. Esta tesis aborda el problema de la adquisición de la señal usando arrays de antenas en el marco general de los receptores de Sistemas Globales de Navegación por Satélite (GNSS). El término GNSS engloba aquellos sistemas de navegación basados en una constelación de satélites que emiten señales útiles para el posicionamiento. Aunque el GPS americano ya está disponible, coexistiendo con el renovado sistema ruso GLONASS, actualmente se está realizando un gran esfuerzo para que la contribución europea (Galileo), junto con el nuevo sistema chino Compass, estén operativos en breve. Por lo tanto, una gran variedad de constelaciones de satélites y señales estarán disponibles en los próximos años. Estos sistemas proporcionan las infraestructuras necesarias para una multitud de aplicaciones y servicios que demandan un servicio de posicionamiento confiable y preciso. La disponibilidad de posicionamiento se debe garantizar en todo momento, especialmente en los servicios críticos para la seguridad de las personas y los bienes. Cuando examinamos las amenazas de la disponibilidad del servicio que ofrecen los GNSSs, es importante tener en cuenta que todos los sistemas presentes y los sistemas futuros ya planificados hacen uso de técnicas de multiplexación por división de código (CDMA). Las señales transmitidas por los satélites son recibidas con una relación señal-ruido (SNR) muy baja, medida antes de la correlación (del orden de - 22 dB para un receptor ubicado en la superficie de la tierra). A pesar de que la ganancia de procesado CDMA ofrece una protección inherente contra las interferencias de radiofrecuencia (RFI), esta protección es limitada. Una interferencia con una relación de potencia de interferencia a potencia de la señal que excede la ganancia de procesado puede degradar el rendimiento de los receptores o incluso negar por completo|$|R
5000|$|The {{orthodox}} <b>Neyman-Pearson</b> <b>theory</b> of {{hypothesis testing}} considers how {{to decide whether}} to accept or reject a hypothesis, rather than what probability to assign to the hypothesis. From this point of view, the hypothesis that [...] "All ravens are black" [...] is not accepted gradually, as its probability increases towards one when more and more observations are made, but is accepted in a single action as the result of evaluating the data that has already been collected. As Neyman and Pearson put it: ...|$|E
50|$|He {{was elected}} a Fellow of the Royal Society in March 1966. His {{candidacy}} citation read: Known {{throughout the world}} as co-author of the <b>Neyman-Pearson</b> <b>theory</b> of testing statistical hypotheses, and responsible for many important contributions to problems of statistical inference and methodology, especially {{in the development and}} use of the likelihood ratio criterion. Has played a leading role in furthering the applications of statistical methods — for example, in industry, and also during and since the war, in the assessment and testing of weapons.|$|E
5000|$|Deborah Mayo expands on {{this further}} as follows:"It must be stressed, however, that {{having seen the}} value the data, <b>Neyman-Pearson</b> <b>theory</b> never permits one to {{conclude}} that the specific confidence interval formed covers the true value of 0 with either (1 − α)100% probability or (1 − α)100% degree of confidence. Seidenfeld's remark seems rooted in a (not uncommon) desire for Neyman-Pearson confidence intervals to provide something which they cannot legitimately provide; namely, a measure of the degree of probability, belief, or support that an unknown parameter value lies in a specific interval. Following Savage (1962), the probability that a parameter lies in a specific interval may be referred to as a measure of final precision. While a measure of final precision may seem desirable, and while confidence levels are often (wrongly) interpreted as providing such a measure, no such interpretation is warranted. Admittedly, such a misinterpretation is encouraged by the word 'confidence'." ...|$|E
5000|$|The {{two forms}} of {{hypothesis}} testing are based on different problem formulations. The original test is analogous to a true/false question; the Neyman-Pearson test is more like multiple choice. In the view of Tukey the former produces a conclusion {{on the basis of}} only strong evidence while the latter produces a decision on the basis of available evidence. While the two tests seem quite different both mathematically and philosophically, later developments lead to the opposite claim. Consider many tiny radioactive sources. The hypotheses become 0,1,2,3... grains of radioactive sand. There is little distinction between none or some radiation (Fisher) and 0 grains of radioactive sand versus all of the alternatives (Neyman-Pearson). The major Neyman-Pearson paper of 1933 also considered composite hypotheses (ones whose distribution includes an unknown parameter). An example proved the optimality of the (Student's) t-test, [...] "there can be no better test for the hypothesis under consideration" [...] (p 321). <b>Neyman-Pearson</b> <b>theory</b> was proving the optimality of Fisherian methods from its inception.|$|E
40|$|Multivariate Analysis is an {{increasingly}} common tool in experimental high energy physics; however, {{many of the}} common approaches were borrowed from other fields. We clarify what {{the goal of a}} multivariate algorithm should be for the search for a new particle and compare different approaches. We also translate the <b>Neyman-Pearson</b> <b>theory</b> into the language of statistical learning theory. 1...|$|E
40|$|In a {{frequently}} cited paper, Becker (1974) asked whether {{data from a}} “mortal ” (= terminated) smallpox outbreak better fit a subcritical or semicritical Galton-Watson branching process. We revisit this question using standard hypothesis-testing methods and reach a somewhat di↵erent conclusion than he did. For a non-terminated process, the related problem of predicting extinction or explosion also can be formulated in a testing framework to which standard <b>Neyman-Pearson</b> <b>theory</b> applies...|$|E
40|$|We {{extend the}} {{classical}} <b>Neyman-Pearson</b> <b>theory</b> for testing composite hypotheses versus composite alternatives, using a convex duality approach as in Witting (1985). Results of Aubin & Ekeland (1984) from non-smooth convex analysis are employed, {{along with a}} theorem of Koml'os (1967), {{in order to establish}} the existence of a max-min optimal test and to investigate its properties. The theory is illustrated on representative examples involving Gaussian measures on Euclidean and Wiener space...|$|E
40|$|Abstract. Shortfall risk is {{considered}} by taking some exposed risks because the superhedging price is too expensive {{to be used in}} practice. Minimizing shortfall risk can be reduced to the problem of finding a randomized test ψ in the static problem. The optimization problem can be solved via the classical <b>Neyman-Pearson</b> <b>theory,</b> and can be also explained in terms of hypothesis testing. We introduce the classical Neyman-Pearson lemma expressed in terms of mathematics and see how it is applied to shortfall risk in finance. 1...|$|E
40|$|This paper {{provides}} {{a brief overview}} and also critiques {{some of the issues}} related to the concepts of superiorit. v, noninferiority, equivalence, and bioequivalence as they are encountered in clinical drug trials. These concepts and their relationships to each other are discussed here in the context of International Conference on Harmonization (ICH) documents E- 9 and E- 10. Most of the well-known and not so well-known properties of those concepts are discussed under the unified framework of standard <b>Neyman-Pearson</b> <b>theory.</b> Sample size adjustments for dropouts differ for intent-to-treat analysis and per protocol analysis. These differences are discussed in the context of noninferiority and superiority hypotheses...|$|E
40|$|ABSTRACT. I {{document}} some of {{the main}} evidence showing that E. S. Pearson rejected the key features of the behavioral-decision philosophy that became associated with the <b>Neyman-Pearson</b> <b>Theory</b> of statistics (NPT). I argue that NPT principles arose not out of behavioral aims, where the concern is solely with behaving correctly sufficiently often in some long run, but out of the epistemological aim of learning about causes of experimental results (e. g., distinguishing genuine from spurious effects). The view Pearson did hold gives a deeper understanding of NPT tests than their typical formulation as 'acceptreject routines', against which criticisms of NPT are really directed. The 'Pearsonian' view that emerges suggests how NPT tests may avoid these criticisms while still retaining what is central to these methods: the control of error probabilities. 1...|$|E
40|$|This paper {{serves to}} {{demonstrate}} that the practise of using one, two, or three asterisks (according to a type-I-risk α either 0. 05, 0. 01, or 0. 001) in significance testing as given par-ticularly with regard to empirical research in psychology is in no way in accordance with the <b>Neyman-Pearson</b> <b>theory</b> of statistical hypothesis testing. Claiming a-posteriori that even a low type-I-risk α leads to significance merely discloses a researcher’s self-deception. Fur-thermore it will be emphasised that by using sequential sampling procedures instead of fixed sample sizes the „practice of asterisks “ would not arise. Besides this, a simulation study will show that sequential sampling procedures are not only efficient concerning a lower sample size but are also robust and nevertheless powerful in the case of non-normal distributions...|$|E
40|$|A test at a {{significance}} level a yields a decision {{which can be}} quantified by assigning the value 1 if the null hypothesis is rejected and 0 otherwise. The theory of guarded weights of evidence for the alternative (Blyth and Staudte, 1995. Statist. Probab. Lett. 23, 45 – 52) generalizes the usual <b>Neyman–Pearson</b> <b>theory</b> by considering L 2 norm risk. The result is a weight of evidence for the alternative which preserves the bound a on the risk of making a type I error and which takes values between 0 and 1. In this paper we establish, under quite general conditions, the asymptotic convergence of guarded weights of evidence. Both fixed and local alternatives are considered. These results are then applied to yield a consistency theorem, {{a version of the}} central limit theorem for guarded weights of evidence and an asymptotic result for guarded weights of evidence based on the F-statistic which in turn is applied to one-way ANOVA...|$|E
40|$|The <b>Neyman–Pearson</b> <b>theory</b> of {{hypothesis}} testing, {{with the}} Type I error rate, α, as the significance level, {{is widely regarded}} as statistical testing orthodoxy. Fisher’s model of significance testing, where the evidential p value denotes the level of significance, nevertheless dominates statistical testing practice. This paradox has occurred because these two incompatible theories of classical statistical testing have been anonymously mixed together, creating the false impression of a single, coherent model of statistical inference. We show that this hybrid approach to testing, with its misleading p < α statistical significance criterion, is common in marketing research textbooks, as well as in a large random sample of papers from twelve marketing journals. That is, researchers attempt the impossible by simultaneously interpreting the p value as a Type I error rate and as a measure of evidence against the null hypothesis. The upshot is that many investigators do not know what our most cherished, and ubiquitous, researc...|$|E
40|$|Recent {{improvements}} on receiver technologies, {{combined with}} the growth of new applications, increase the expectations for positioning and timing performance. New systems based on Global Navigation Satellite System (GNSS) are emerging, while safety and liability-critical applications relying on Global Positioning System (GPS) are becoming more popular than in the past. In this context, interfering signals are still a concern. While jamming blind the target receiver through the transmission of a high signal power, spoofing refers to a malicious transmission of counterfeit GNSS-like signals, that force the victim receiver to compute erroneous positions. Recently, new spoofing detectors have been proposed for civilian GPS receivers. Starting from signal quality monitoring techniques applied to multipath detection, this paper presents a low complexity strategy, based on the <b>Neyman-Pearson</b> <b>theory,</b> for the detection of intermediate spoofing attacks. In addition to conventional metrics to detect asymmetries on the correlation function, we use some extra-correlators, to reveal the presence of unexpected correlation peaks in the search space. The paper describes the designed algorithm with the appropriated theory, under some simplified hypothesis, addressing the problem associated to the thresholds setting in the decision process. The paper concludes the work presenting some open issues authors have to solve before practical implementation of the new techniqu...|$|E
40|$|ABSTRACT. Theories of {{statistical}} testing,nay {{be seen as}} attempts to provide systematic means for evaluating scientific conjectures {{on the basis of}} incomplete or inaccurate observational data. The <b>Neyman-Pearson</b> <b>Theory</b> of Testing (NPT) has purported to provide an objective means for testing statistical hypotheses corresponding to scientific claims. Despite their widespread use in science, methods of NPT have themselves been accused of failing to be objective; and the purported objectivity of scientific claims based upon NPT has been called into question. The {{purpose of this paper is}} first to clarify this question by examining the conceptions of (I) the function served by NPT in science, and (II) the requirements of an objective theory of statistics upon which attacks on NPT's objectivity are based. Our grounds for rejecting these conceptions suggest altered conceptions of (I) and (II) that might avoid such attacks. Second, we propose a reformulation of NPT, denoted by NPT*, based on these altered conceptions, and argue that it provides an objective theory of statistics. The crux of our argument is that by being able to objectively control error frequencies NPT * is able to objectively evaluate what has or has not been learned from the result of a statistical test...|$|E
40|$|Two topics, {{connected}} with <b>Neyman-Pearson</b> <b>theory</b> of testing hypotheses, are treated in this article. The first topic {{is related to}} the information content of an experiment; after a short outline of ordinal comparability of experiments, the two most popular informa-tion measures – by Fisher and by Kullback-Leibler – are considered. As far as we require a comparison of two experiments at a time, the superiority of the couple (a,b) of the two error probabilities in the Neyman-Pearson approach is easily established, owing to their clear operational meaning. &# 13; The second topic deals with the so called Jeffreys – or Lindley – paradox: it can be shown that, if we attach a positive probability to a point null hypothesis, some «paradoxical» posterior probabilities – in a Bayesian approach – result in sharp contrast with the error probabilities in the Neyman-Pearson approach. It is argued that such results are simply the outcomes of absurd assumptions, and it is shown that sensible assumptions about interval – not point – hypotheses can yield posterior probabilities perfectly compatible with the Neyman-Pearson approach (although one must be very careful in making such comparisons, as the two approaches are radically different both in assumptions and in purposes) ...|$|E
40|$|The Fisherian {{prescription}} {{of reporting}} P-values as {{a summary of}} a result, {{as compared to the}} Neyman-Pearson system of acceptance or rejection of a null hypothesis, is more common in applied science. This popularity is largely {{due to the fact that}} the P-value provides a more complete, meaningful and useful evidence regarding the null hypothesis. Conventionally, P-values are defined in the context of one-sided alternatives, although there exist some ideas in the literature concerning two-sided alternatives; see e. g. [Gibbons, J. D., Pratt, J. W., 1975. P-values: Interpretation and methodology. American Statistician 24, 20 - 25; George, E. O., Mudholkar, G. S., 1990. P-values for two-sided tests. Biometrical Journal 32, 747 - 751]. This note takes an axiomatic approach for defining P-values which involves at most ordering of the alternatives but is not restricted by their nature. It also involves a correspondence between a P-value and the associated level [alpha] test for each [alpha]. A P-value turns out to be valid if and only if the associated level [alpha] test is unbiased in the traditional sense for each [alpha]. Furthermore, it is shown that the resulting optimal tests agree with those given by the Neyman-Person framework when the ordering is stochastic. Thus, a theory based on optimal P-values parallels to the <b>Neyman-Pearson</b> <b>theory</b> and bridges the two approaches to testing of hypotheses. ...|$|E
40|$|This thesis {{provides}} {{methods for}} nonparametric analysis {{and design of}} incoherent adaptive CFAR detectors. Adaptive CFAR detectors, both parametric and nonparametric, require a reference sample to learn the statistical properties of the noise. An important issue but one that is often overlooked {{is the need to}} choose a reference sample that is representative of the noise in the test cell. An approach to dynamically choose these windows is developed and studied. A powerful test for randomness based on the Mann-Kendall rank test is employed in this approach. ^ Next the issue of when to use a nonparametric test versus a parametric one is discussed. Bounds for rank detectors operating on square-law data are provided. <b>Neyman-Pearson</b> <b>theory</b> on optimal rank tests is used. The results allow the designer to decide if a rank test is suitable. In addition, a method is developed for real-time implementation of an important class of rank tests. This method solves the difficult and open problem of determining detection thresholds. ^ In situations for which nonparametric detectors cannot achieve reasonable power, parametric approaches must be considered. The OS-CFAR detector of Rohling is shown to possess unique advantages for design and analysis of robust detectors under which improved Pfa control and acceptable Pd can be both realized and, most importantly, quantified. The ideas of Nonparametric Tolerance Intervals are used to provide an elegant and simple method for analysis and design of this detector in arbitrary noise. ...|$|E
