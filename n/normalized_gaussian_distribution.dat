3|10000|Public
2500|$|... where [...] and [...] are the variances of [...] and [...] respectively, {{with the}} {{equality}} attained {{in the case}} of a suitably <b>normalized</b> <b>Gaussian</b> <b>distribution.</b> Although the variances may be analogously defined for the DFT, an analogous uncertainty principle is not useful, because the uncertainty will not be shift-invariant. Still, a meaningful uncertainty principle has been introduced by Massar and Spindel.|$|E
5000|$|For {{the case}} of {{continuous}} functions [...] and , the Heisenberg uncertainty principle states thatwhere [...] and [...] are the variances of [...] and [...] respectively, with the equality attained in {{the case of}} a suitably <b>normalized</b> <b>Gaussian</b> <b>distribution.</b> Although the variances may be analogously defined for the DFT, an analogous uncertainty principle is not useful, because the uncertainty will not be shift-invariant. Still, a meaningful uncertainty principle has been introduced by Massar and Spindel.|$|E
5000|$|... #Caption: This figure {{demonstrates}} the central limit theorem. The sample means are generated using a random number generator, which draws numbers between 0 and 100 from a uniform probability distribution. It illustrates that increasing sample sizes {{result in the}} 500 measured sample means being more closely distributed about the population mean (50 in this case). It also compares the observed distributions with the distributions that would be expected for a <b>normalized</b> <b>Gaussian</b> <b>distribution,</b> and shows the chi-squared values that quantify the goodness of the fit (the fit is good if the reduced chi-squared value is less than or approximately equal to one). The input into the normalized Gaussian function is the mean of sample means (~50) and the mean sample standard deviation divided by the square root of the sample size (~28.87/), which is called the standard deviation of the mean (since {{it refers to the}} spread of sample means).|$|E
40|$|We {{show that}} the refined Donaldson-Thomas invariants of C 3, {{suitably}} <b>normalized,</b> have a <b>Gaussian</b> <b>distribution</b> as limit law. Combinatorially these numbers are given by weighted counts of 3 D partitions. Our technique {{is to use the}} Hardy-Littlewood circle method to analyze the bivariate asymptotics of a q-deformation of MacMahon's function. The proof is based on that of E. M. Wright who explored the single variable case. Comment: 11 pages and 3 figure...|$|R
40|$|Abstract. We {{show that}} the refined Donaldson–Thomas invariants of C 3, {{suitably}} <b>normalized,</b> have a <b>Gaussian</b> <b>distribution</b> as limit law. Combinatori-ally these numbers are given by weighted counts of 3 D partitions. Our tech-nique {{is to use the}} Hardy–Littlewood circle method to analyze the bivariate asymptotics of a q-deformation of MacMahon’s function. The proof is based on that of E. M. Wright who explored the single variable case. 1. Introduction. In [7] physicists suggested that the (IIA) string theory associated to a Calabi–Ya...|$|R
40|$|A {{study was}} made of the field size {{distributions}} for LACIE test sites 5029, 5033, and 5039, People's Republic of China. Field lengths and widths were measured from LANDSAT imagery, and field area was statistically modeled. Field size parameters have log-normal or Poisson frequency distributions. These were <b>normalized</b> to the <b>Gaussian</b> <b>distribution</b> and theoretical population curves were made. When compared to fields {{in other areas of the}} same country measured in the previous study, field lengths and widths in the three LACIE test sites were 2 to 3 times smaller and areas were smaller by an order of magnitude...|$|R
40|$|Although real, <b>normalized</b> <b>Gaussian</b> wave packets {{minimize}} {{the product of}} position and momentum uncertainties, generic complex <b>normalized</b> <b>Gaussian</b> wave packets do not. We prove they minimize an alternative product of uncertainties that correspond to variables that are phase space rotations of position and momentum...|$|R
40|$|In 1941 Kolmogorov and Obukhov {{proposed}} that {{there exists a}} statistical theory of turbulence that should allow the computation of all the statistical quantities that can be computed and measured in turbulent systems. These are quantities such as the moments, the structure functions and the probability density functions (PDFs) of the turbulent velocity field.   In this paper we will outline how to construct this statistical theory from the stochastic Navier-Stokes equation. The additive noise in the stochastic Navier-Stokes equation is generic noise given by the central limit theorem and the large deviation principle. The multiplicative noise consists of jumps multiplying the velocity, modeling jumps in the velocity gradient. We first estimate the structure functions of turbulence and establish the Kolmogorov-Obukhov {'} 62 scaling hypothesis with the She-Leveque intermittency corrections. Then we compute the invariant measure of turbulence writing the stochastic Navier-Stokes equation as an infinite-dimensional Ito process and solving the linear Kolmogorov-Hopf functional differential equation for the invariant measure. Finally we project the invariant measure onto the PDF. The PDFs {{turn out to be}} the <b>normalized</b> inverse <b>Gaussian</b> (NIG) <b>distributions</b> of Barndorff-Nilsen, and compare well with PDFs from simulations and experiments...|$|R
5000|$|... #Caption: A {{comparison}} of <b>Gaussian</b> <b>distribution,</b> rectified <b>Gaussian</b> <b>distribution,</b> and truncated <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|Compounding a <b>Gaussian</b> <b>distribution</b> with mean {{distributed}} {{according to}} another <b>Gaussian</b> <b>distribution</b> yields (again) a <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|... #Caption: <b>Normalized</b> <b>Gaussian</b> curves with {{expected}} value [...] and variance [...] The corresponding parameters are , [...] and [...]|$|R
40|$|In {{the context}} of {{medically}} relevant artificial intelligence, many real-world problems involve both continuous and categorical feature variables. When the data are mixed mode, the assumption of multivariate <b>Gaussian</b> <b>distributions</b> for the gating network of <b>normalized</b> <b>Gaussian</b> (NG) expert networks, such as NG mixture of experts (NGME), becomes invalid. An independence model has been studied to handle mixed feature data {{within the framework of}} NG expert networks. This method is based on the NAIVE assumption that the categorical variables are independent of each other and of the continuous variables. While this method performs surprisingly well in practice as a way of handling problems with mixed feature variables, the independence assumption is likely to be unrealistic for many practical problems. In this chapter, we investigate a dependence model which allows for some dependence between the categorical and continuous variables by adopting a location modeling approach. We show how the expectation-maximization (EM) algorithm can still be adopted to train the location NG expert networks via the maximum likelihoo...|$|R
5000|$|... is the <b>Gaussian</b> <b>distribution,</b> in {{this case}} {{specifically}} the multivariate <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|As λ {{tends to}} infinity, the inverse <b>Gaussian</b> <b>distribution</b> becomes {{more like a}} normal (<b>Gaussian)</b> <b>distribution.</b> The inverse <b>Gaussian</b> <b>distribution</b> has several {{properties}} analogous to a <b>Gaussian</b> <b>distribution.</b> The name can be misleading: it is an [...] "inverse" [...] only in that, while the Gaussian describes a Brownian Motion's level at a fixed time, the inverse <b>Gaussian</b> describes the <b>distribution</b> of the time a Brownian Motion with positive drift takes to reach a fixed positive level.|$|R
30|$|Among them, α is {{the update}} {{rate of the}} <b>Gaussian</b> <b>distribution</b> parameter, and the {{parameters}} remain unchanged for the <b>Gaussian</b> <b>distribution</b> with no matching success. In {{the establishment of the}} background model, we set the number of <b>Gaussian</b> <b>distributions</b> describing each pixel 3 [*]=[*]K. The background model is initialized first, and the initial weights are w 1, 0 [*]=[*] 1, w 2, 0 [*]=[*] 1, w 3, 0 [*]=[*] 1. The pixels of the first frame are used to initialize the first <b>Gaussian</b> <b>distribution</b> mean, and the mean of the remaining <b>Gaussian</b> <b>distribution</b> is 0. The standard deviation of each model takes a larger value σi, 0 [*]=[*] 30, a weight update rate β[*]=[*] 0.33, a learning rate α[*]=[*] 0.7, and a threshold value of 7.0 [*]=[*]T. If no <b>Gaussian</b> <b>distribution</b> is found to match the xt at the time of detection, then a <b>Gaussian</b> <b>distribution</b> with the lowest priority is removed, and a new <b>Gaussian</b> <b>distribution</b> is introduced according to xt, and a smaller weight and a larger variance are assigned, and then weight normalization is performed.|$|R
3000|$|The {{delay is}} drawn from a <b>Gaussian</b> <b>distribution</b> with its mean and {{variance}} given in the cluster parameters. Similarly, the angular parameters are drawn from a wrapped <b>Gaussian</b> <b>distribution</b> [37] (in the wrapped <b>Gaussian</b> <b>distribution,</b> all realisations are mapped to their principal value in [...]...|$|R
5000|$|Compounding a <b>Gaussian</b> <b>distribution</b> with mean {{distributed}} {{according to}} a shifted exponential distribution yields an exponentially modified <b>Gaussian</b> <b>distribution.</b>|$|R
40|$|In this article, {{we discuss}} the {{reconstruction}} of chaotic dynamics by using a <b>normalized</b> <b>Gaussian</b> network(NGnet). Through using an on-line EM algorithm, the NGnet is trained to learn the vector field of the chaotic dynamics. We also investigate the robustness of our approach to two kinds of noise processes: system noise and observation noise. We have found that a trained NGnet is able to reproduce a chaotic attractor, even under these two kinds of noise. The trained NGnet also exhibits good prediction performance. When part of the dynamical variables is observed, a delay coordinate embedding is used; namely, the NGnet is trained to learn the vector field in the delay coordinate space. It is shown that the chaotic dynamics can be learned with this method even under two kinds of noise. 1. INTRODUCTION A <b>normalized</b> <b>Gaussian</b> network(NGnet) [6] is a network of local linear regression units. This model softly partitions input space by using <b>normalized</b> <b>Gaussian</b> functions, and each local uni [...] ...|$|R
3000|$|..., {{we first}} {{eliminate}} the <b>Gaussian</b> <b>distribution</b> whose existence probability {{is smaller than}} a given threshold τ. After pruning, the remaining <b>Gaussian</b> <b>distributions</b> and their existence probabilities are used as inputs for the next filtering recursion. We then select the <b>Gaussian</b> <b>distributions</b> with existence probabilities p [...]...|$|R
5000|$|In {{probability}} theory, the rectified <b>Gaussian</b> <b>distribution</b> is {{a modification}} of the <b>Gaussian</b> <b>distribution</b> when its negative elements are reset to 0 (analogous to an electronic rectifier). It is essentially a mixture of a discrete distribution (constant 0) and a continuous <b>distribution</b> (a truncated <b>Gaussian</b> <b>distribution</b> with interval [...] ).|$|R
40|$|Barndorff-Nielsen and Jørgensen (1989) have {{introduced}} some parametric models {{on the unit}} simplex. The distributions associated with these models have been obtained by conditioning on the sum of d independent generalized inverse Gaussian random variables. We use a constructive approach to derive some of these models by first mapping the inverse Gaussian law on (0, 1) and formally extending it on the unit simplex. This technique is then applied to a mixture-inverse <b>Gaussian</b> <b>distribution</b> studied recently by Jørgensen, Seshadri and Whitmore (1991). The distributions are then retransformed to yield two versions of a multidimensional inverse <b>Gaussian</b> <b>distribution.</b> Dirichlet distribution general exponential families generalized inverse <b>Gaussian</b> <b>distribution</b> inverse <b>Gaussian</b> <b>distribution</b> mixture-inverse <b>Gaussian</b> <b>distribution...</b>|$|R
5000|$|The inverse <b>Gaussian</b> and gamma <b>distributions</b> {{are special}} {{cases of the}} {{generalized}} inverse <b>Gaussian</b> <b>distribution</b> for p = −1/2 and b = 0, respectively. [...] Specifically, an inverse <b>Gaussian</b> <b>distribution</b> of the form ...|$|R
3000|$|... where g(.) is the {{probability}} density function (pdf) of the clipping noise which follows <b>Gaussian</b> <b>distribution</b> and Q(.) is the tail probability {{of the standard}} <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|The {{transverse}} {{field of}} the incident beam will have the form where f(x) is a <b>normalized</b> <b>Gaussian,</b> or other beam form, and βin is the longitudinal component of the propagation constant of the incident beam.|$|R
30|$|Note that (11) is {{a mixture}} of <b>Gaussian</b> <b>distribution.</b> The IMM {{algorithm}} approximates it by a <b>Gaussian</b> <b>distribution</b> with mean x̅_k- 1 ^j and covariance P̅_k- 1 ^j.|$|R
30|$|Both {{the white}} noise and noise with <b>Gaussian</b> <b>distribution</b> are examined. It is found that {{either in the}} nature {{background}} or system noise simulation, the <b>Gaussian</b> <b>distribution</b> constantly yields the smaller errors of the two velocities than the white noise does. Note that a small value of the half width in <b>Gaussian</b> <b>distribution,</b> the random function issues numbers about a constant, while the white noise function randomly gives the numbers. Therefore, the <b>Gaussian</b> <b>distribution</b> contributes similar amplitude of the noise fluctuations to the wave packet, which results in the smaller errors of the two velocities.|$|R
40|$|This paper {{presents}} parameter and topology optimizations of wideband antennas for {{microwave energy}} harvesters based on FDTD computations. The antenna shapes are optimized to reduce return losses {{in a specific}} frequency band using micro genetic algorithm. The shape parameters of a two-arm planar spiral antenna (PSA) are optimized. Moreover, topology optimization is performed using <b>normalized</b> <b>Gaussian</b> network, where spatial symmetries are assumed. It is shown that the return losses of the optimized antennas are less than- 10 dB from 1. 0 GHz to 2. 0 GHz {{in which they have}} isotropic directivity. Index Terms—Topology optimization, <b>normalized</b> <b>Gaussian</b> network (NGnet), Group Theory, FDTD method, energy harvesting. I...|$|R
2500|$|... where [...] is {{arbitrary}} and [...] so that [...] is -normalized. In other words, where [...] is a (<b>normalized)</b> <b>Gaussian</b> function with variance , centered at zero, and its Fourier transform is a Gaussian function with variance [...]|$|R
40|$|This paper {{analyses}} the kernel density estimation on {{spaces of}} <b>Gaussian</b> <b>distributions</b> endowed with different metrics. Explicit expressions of kernels are {{provided for the}} case of the 2 -Wasserstein metric on multivariate <b>Gaussian</b> <b>distributions</b> and for the Fisher metric on multivariate centred distributions. Under the Fisher metric, the space of multivariate centred <b>Gaussian</b> <b>distributions</b> is isometric to the space of symmetric positive definite matrices under the affine-invariant metric and the space of univariate <b>Gaussian</b> <b>distributions</b> is isometric to the hyperbolic space. Thus kernel are also valid on these spaces. The density estimation is successfully applied to a classification problem of electro-encephalographic signals...|$|R
3000|$|... 0 at t= 0, has {{an inverse}} <b>Gaussian</b> <b>distribution.</b> In {{physical}} terms, if energetic particles are injected impulsively {{at a specific}} position in the lower corona and move outward at a constant drift rate accompanied by diffusion, the particles that escape at a specific distance in the upper corona have an inverse <b>Gaussian</b> <b>distribution</b> in time. Therefore, we adopt the inverse <b>Gaussian</b> <b>distribution</b> as the particle injection time profile.|$|R
40|$|International audienceThis paper {{presents}} {{a method for}} object detection based on a cascade of scale and orientation <b>normalized</b> <b>Gaussian</b> derivative classifiers learnt with Adaboost. <b>Normalized</b> <b>Gaussian</b> derivatives provide a small but powerful feature set for rapid learning using Adaboost. Real time detection is made possible by use of a fast integer coefficient algorithm that computes a half-octave Gaussian pyramid with linear algorithmic complexity using a cascade of binomial kernel filters. The method is demonstrated by training a boosted classifier for frontal face detection using standard data sets. Experiments demonstrate that this approach can provide detection rates that are comparable or superior to those obtained with integral images while dramatically reducing the required training effort...|$|R
40|$|Variance {{estimation}} and ranking {{methods are}} developed for stochastic processes modeled by <b>Gaussian</b> mbcture <b>distributions.</b> It is {{shown that the}} variance estimate from a <b>Gaussian</b> mixture <b>distribution</b> has the same properties as a variance estimate from a single <b>Gaussian</b> <b>distribution</b> based on a reduced number of samples. Hence, well known tools of variance estimation and ranking of single <b>Gaussian</b> <b>distributions</b> {{can be applied to}} <b>Gaussian</b> mixture <b>distributions.</b> As an application example, optimization of sensor processing order in the sequential multi-target multi-sensor joint probabilistic data associ- ation (MSJPDA) algorithm is presented...|$|R
40|$|The paper {{focuses on}} the {{influences}} of some factors significant to pyrolysis of forestry biomass on the asymptotic solution of the non-isothermal nth-order distribution energy model (DAEM) using <b>Gaussian</b> <b>distribution.</b> Investigated parameters are the integral upper limit, the frequency factor, and the heating rate parameters of the <b>Gaussian</b> <b>distribution.</b> The influence of these factors {{has been used for}} evaluating the kinetic parameters of the non-isothermal nth-order <b>Gaussian</b> <b>distribution</b> from thermogravimetric analysis of forest waste...|$|R
40|$|There {{are several}} {{problems}} {{where it is}} very important to know whether the tested data are distributed according to the Gaussian law. At the detection of the hidden information within the digitized pictures (steganography), one of the key factors is the analysis of the noise contained in the picture. The incorporated noise should show the typically <b>Gaussian</b> <b>distribution.</b> The departure from the <b>Gaussian</b> <b>distribution</b> might be the first hint that the picture has been changed – possibly new information has been inserted. In such cases the fast <b>Gaussian</b> <b>distribution</b> test is a very valuable tool. The article describes the phase of the noise (in the picture) extraction and the distribution formation. The second phase of the noise analysis is performed by the neural network. The neural network is trained to recognize the <b>Gaussian</b> <b>distribution</b> of the noise. The trained neural network successfully performs the fast <b>Gaussian</b> <b>distribution</b> test...|$|R
3000|$|This {{stems from}} the fact that the gamma <b>distribution</b> {{approaches}} a <b>Gaussian</b> <b>distribution</b> if the degree of freedom, here 2 NM, is large [18]. Furthermore, the <b>Gaussian</b> <b>distribution</b> tends toward a Dirac delta function as its variance, here 2 NM(E [...]...|$|R
50|$|Hidden Markov {{models can}} model complex Markov {{processes}} where the states emit the observations {{according to some}} probability distribution. One such example is the <b>Gaussian</b> <b>distribution,</b> in such a Hidden Markov Model the states output are represented by a <b>Gaussian</b> <b>distribution.</b>|$|R
