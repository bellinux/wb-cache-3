5154|0|Public
5|$|The {{existence}} of expander graphs with constant page number {{is the key}} step in proving {{that there is no}} subquadratic-time simulation of two-tape <b>non-deterministic</b> Turing machines by one-tape <b>non-deterministic</b> Turing machines.|$|E
25|$|However, some {{computational}} {{problems are}} easier to analyze in terms of more unusual resources. For example, a <b>non-deterministic</b> Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The <b>non-deterministic</b> Turing machine has {{very little to do}} with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that <b>non-deterministic</b> time is a very important resource in analyzing computational problems.|$|E
25|$|Objective {{collapse}} theories, {{which involve}} a dynamic (and <b>non-deterministic)</b> {{collapse of the}} wave function (e.g. Ghirardi–Rimini–Weber theory, Penrose interpretation, or causal fermion systems) avoid these absurdities. The theory of causal fermion systems for example, is able to unify quantum mechanics, general relativity and quantum field theory, via a more fundamental theory which is non-linear, but {{gives rise to the}} linear behaviour of the wave function and also gives rise to the non-linear, <b>non-deterministic,</b> wave-function collapse. These theories suggest that {{a deeper understanding of the}} theory underlying quantum mechanics shows the universe is indeed <b>non-deterministic</b> at a fundamental level.|$|E
25|$|A further {{distinction}} is between deterministic (DFA) and <b>non-deterministic</b> (NFA, GNFA) automata. In a deterministic automaton, every state has exactly one transition for each possible input. In a <b>non-deterministic</b> automaton, an input {{can lead to}} one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.|$|E
25|$|Immaterial souls {{exist and}} exert a <b>non-deterministic</b> causal {{influence}} on bodies. (Traditional free-will, interactionist dualism).|$|E
25|$|A {{decision}} problem is in NP {{if it can}} be solved by a <b>non-deterministic</b> algorithm in polynomial time.|$|E
25|$|<b>Non-deterministic,</b> or {{stochastic}} {{systems can}} be studied using {{a different kind of}} mathematics, such as stochastic calculus.|$|E
25|$|NP: The {{complexity}} {{class of}} decision {{problems that can}} be solved on a <b>non-deterministic</b> Turing machine in polynomial time.|$|E
25|$|Deterministic {{algorithms}} {{solve the}} problem with exact decision at {{every step of the}} algorithm whereas <b>non-deterministic</b> algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.|$|E
25|$|The {{model of}} {{computation}}: The most common model of computation is the deterministic Turing machine, but many complexity classes {{are based on}} <b>non-deterministic</b> Turing machines, Boolean circuits, quantum Turing machines, monotone circuits, etc.|$|E
25|$|Central {{subjects}} in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes, which provide mathematical abstractions of <b>non-deterministic</b> or uncertain processes or measured quantities that may either be single occurrences or evolve {{over time in}} a random fashion.|$|E
25|$|Before many-worlds, reality {{had always}} been viewed as a single {{unfolding}} history. Many-worlds, however, views reality as a many-branched tree, wherein every possible quantum outcome is realised. Many-worlds reconciles the observation of <b>non-deterministic</b> events, such as random radioactive decay, with the fully deterministic equations of quantum physics.|$|E
25|$|Prioritised Petri nets add {{priorities}} to transitions, whereby {{a transition}} cannot fire, if a higher-priority transition is enabled (i.e. can fire). Thus, transitions are in priority groups, and e.g. priority group 3 can only fire if all transitions are disabled in groups 1 and 2. Within a priority group, firing is still <b>non-deterministic.</b>|$|E
25|$|Unlike a DFA, it is <b>non-deterministic,</b> i.e., {{for some}} state and input symbol, the next state may be nothing or {{one or two}} or more {{possible}} states. Thus, in the formal definition, the next state {{is an element of}} the power set of the states, which is a set of states to be considered at once.|$|E
25|$|Many {{types of}} Turing {{machines}} {{are used to}} define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, <b>non-deterministic</b> Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.|$|E
25|$|A {{deterministic}} Turing {{machine is}} the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A <b>non-deterministic</b> Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem {{in any of these}} branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see <b>non-deterministic</b> algorithm.|$|E
25|$|If a C'(1/6) {{presentation}} (∗) is finite (that is both X and R are finite), then Dehn's {{algorithm is}} an actual <b>non-deterministic</b> algorithm {{in the sense}} of recursion theory. However, even if (∗) is an infinite C'(1/6) presentation, Dehn's algorithm, understood as an abstract procedure, still correctly decides whether or not a word in the generators Xplusmn&1 represents the identity element of G.|$|E
25|$|The {{interpreting}} structure I includes states, transitions between states, measurement operations, {{and possibly}} information about spatial extension of these elements. A measurement operation {{refers to an}} operation which returns a value and might result in a system state change. Spatial information would be exhibited by states represented as functions on configuration space. The transitions may be <b>non-deterministic</b> or probabilistic or there may be infinitely many states.|$|E
25|$|There are two {{possible}} acceptance criteria: acceptance by empty stack and acceptance by final state. The two are not equivalent for the deterministic pushdown automaton (although {{they are for}} the <b>non-deterministic</b> pushdown automaton). The languages accepted by empty stack are those languages that are accepted by final state and are prefix-free: no word in the language is the prefix of another word in the language.|$|E
25|$|A {{precisely}} specified {{behavior for}} the arithmetic operations: A result {{is required to}} be produced as if infinitely precise arithmetic were used to yield a value that is then rounded according to specific rules. This means that a compliant computer program would always produce the same result when given a particular input, thus mitigating the almost mystical reputation that floating-point computation had developed for its hitherto seemingly <b>non-deterministic</b> behavior.|$|E
25|$|However, {{there are}} many {{interesting}} special cases that are decidable. In particular, {{it is possible to}} reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and <b>non-deterministic)</b> finite-state machines can reach a deadlock. This problem is PSPACE-complete, i.e., it is decidable, but it is not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.|$|E
25|$|SAT is in NP {{because any}} {{assignment}} of Boolean values to Boolean variables that is claimed {{to satisfy the}} given expression can be verified in polynomial time by a deterministic Turing machine. (The statements verifiable in polynomial time by a deterministic Turing machine and solvable in polynomial time by a <b>non-deterministic</b> Turing machine are totally equivalent, and {{the proof can be}} found in many textbooks, for example Sipser's Introduction to the Theory of Computation, section 7.3.).|$|E
25|$|The {{complexity}} class P {{is often}} seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that {{people would like to}} solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special <b>non-deterministic</b> Turing machines, it is easily observed that each problem in P is also member of the class NP.|$|E
25|$|Quantum {{mechanics}} {{is essential}} to understanding the behavior of systems at atomic length scales and smaller. If the physical nature of an atom were solely described by classical mechanics, electrons would not orbit the nucleus, since orbiting electrons emit radiation (due to circular motion) and would eventually collide with the nucleus due to this loss of energy. This framework was unable to explain the stability of atoms. Instead, electrons remain in an uncertain, <b>non-deterministic,</b> smeared, probabilistic wave–particle orbital about the nucleus, defying the traditional assumptions of classical mechanics and electromagnetism.|$|E
25|$|Knowing {{the area}} Area(w) of a {{relation}} w allows to bound, {{in terms of}} |w|, not only the number of conjugates of the defining relations in (♠) but the lengths of the conjugating elements u'i as well. As a consequence, {{it is known that}} if a finitely presented group G given by a finite presentation (∗) has computable Dehn function Dehn(n), then the word problem for G is solvable with <b>non-deterministic</b> time complexity Dehn(n) and deterministic time complexity Exp(Dehn(n)). However, in general there is no reasonable bound on the Dehn function of a finitely presented group in terms of the deterministic time complexity of the word problem and the gap between the two functions can be quite large.|$|E
25|$|Einstein's {{refusal to}} accept the {{revolution}} as complete reflected his desire to see developed {{a model for the}} underlying causes from which these apparent random statistical methods resulted. He did not reject the idea that positions in space-time could never be completely known but did not want to allow the uncertainty principle to necessitate a seemingly random, <b>non-deterministic</b> mechanism by which the laws of physics operated. Einstein himself was a statistical thinker but disagreed that no more needed to be discovered and clarified. Bohr, meanwhile, was dismayed by none of the elements that troubled Einstein. He made his own peace with the contradictions by proposing a principle of complementarity that emphasized the role of the observer over the observed.|$|E
25|$|In {{optimization}} problems, heuristic algorithms {{can be used}} to find {{a solution}} close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting {{closer and closer to the}} optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are <b>non-deterministic</b> algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.|$|E
25|$|In {{computer}} science, {{an ambiguous}} grammar is a context-free grammar {{for which there}} exists a string that can {{have more than one}} leftmost derivation or parse tree, while an unambiguous grammar is a context-free grammar for which every valid string has a unique leftmost derivation or parse tree. Many languages admit both ambiguous and unambiguous grammars, while some languages admit only ambiguous grammars. Any non-empty language admits an ambiguous grammar by taking an unambiguous grammar and introducing a duplicate rule or synonym (the only language without ambiguous grammars is the empty language). A language that only admits ambiguous grammars is called an inherently ambiguous language, and there are inherently ambiguous context-free languages. Deterministic context-free grammars are always unambiguous, and are an important subclass of unambiguous grammars; there are <b>non-deterministic</b> unambiguous grammars, however.|$|E
25|$|In the future, the Internet {{of things}} {{may be a}} <b>non-deterministic</b> and open network in which auto-organized or {{intelligent}} entities (Web services, SOA components), virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information {{as well as the}} objects ability to detect changes in the environment, faults affecting sensors and introduce suitable mitigation measures constitute a major research trend, clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation but more sophisticated forms of intelligence are requested to permit sensor units to be deployed in real environments.|$|E
25|$|The <b>non-deterministic</b> {{property}} {{has been}} a very valuable one, as it lets the user abstract a large number of properties (depending on what the net is used for). In certain cases, however, the need arises to also model the timing, not only the structure of a model. For these cases, timed Petri nets have evolved, where there are transitions that are timed, and possibly transitions which are not timed (if there are, transitions that are not timed have a higher priority than timed ones). A subsidiary of timed Petri nets are the stochastic Petri nets that add nondeterministic time through adjustable randomness of the transitions. The exponential random distribution is usually used to 'time' these nets. In this case, the nets' reachability graph {{can be used as a}} continuous time Markov chain (CTMC).|$|E
25|$|From {{the late}} 1980s, {{a new school}} of {{linguistic}} relativity scholars has {{examined the effects of}} differences in linguistic categorization on cognition, finding broad support for <b>non-deterministic</b> versions of the hypothesis in experimental contexts. Some effects of linguistic relativity have been shown in several semantic domains, although they are generally weak. Currently, a balanced view of linguistic relativity is espoused by most linguists holding that language influences certain kinds of cognitive processes in non-trivial ways, but that other processes are better seen as arising from connectionist factors. Research is focused on exploring the ways and extent to which language influences thought. The principle of linguistic relativity and the relation between language and thought has also received attention in varying academic fields from philosophy to psychology and anthropology, and it has also inspired and coloured works of fiction and the invention of constructed languages.|$|E
25|$|There {{are several}} {{companion}} notions {{closely related to}} the notion of an isoperimetric function. Thus an isodiametric function bounds the smallest diameter (with respect to the simplicial metric where every edge has length one) of a van Kampen diagram for a particular relation w in terms of the length of w. A filling length function the smallest filling length of a van Kampen diagram for a particular relation w in terms of the length of w. Here the filling length of a diagram is the minimum, over all combinatorial null-homotopies of the diagram, of the maximal length of intermediate loops bounding intermediate diagrams along such null-homotopies. The filling length function is {{closely related to the}} <b>non-deterministic</b> space complexity of the word problem for finitely presented groups. There are several general inequalities connecting the Dehn function, the optimal isodiametric function and the optimal filling length function, but the precise relationship between them is not yet understood.|$|E
25|$|In a {{language}} with lazy evaluation, like Haskell, a list is evaluated {{only to the}} degree that its elements are requested: for example, if one asks for the first element of a list, only the first element will be computed. With respect to usage of the list monad for <b>non-deterministic</b> computation that means that we can non-deterministically generate a lazy list of all results of the computation and ask for the first of them, and only as much work will be performed as is needed to get that first result. The process roughly corresponds to backtracking: a path of computation is chosen, and then if it fails at some point (if it evaluates mzero), then it backtracks to the last branching point, and follows the next path, and so on. If the second element is then requested, it again does just enough work to get the second solution, and so on. So the list monad is a simple way to implement a backtracking algorithm in a lazy language.|$|E
25|$|This {{clash of}} {{perspectives}} led to {{something of a}} stand-off {{and a lack of}} communication between the disciplines, resulting in little cooperation and progress for three decades. The stand-off was resolved by Holland's Social Bonding and Nurture Kinship which re-visited biological inclusive fitness theory to draw a distinction between the statistical evolutionary mechanisms for the emergence of social traits and the <b>non-deterministic</b> proximate mechanisms through which they are expressed. In a strict interpretation of the theory, a statistical association of related genes (as would be present in the interactions of close genetic relatives) is understood as a necessary (though not sufficient) condition for the evolutionary emergence of certain traits relating to social cooperation (see kin selection). However, this does not entail that the proximate mechanisms governing the expression of such social traits in primates and humans necessarily depends on (or are determined by) conditions of genetic relatedness per se. For the vast majority of social mammals—including primates and humans—the formation of social bonds (and the resulting social cooperation) are based on familiarity from an early developmental stage, and the same kinds of mechanisms that attachment theorists (see above) have outlined. In short, in humans and in other primates, genetic relatedness is not necessary for the attachment bonds to develop, and it is the performance of nurture that underlies such bonds and the enduring social cooperation that typically accompanies them (see Social Bonding and Nurture Kinship).|$|E
2500|$|... "A <b>non-deterministic</b> {{approach}} to analogy, involving the Ising model of ferromagnetism", in Eduardo Caianiello (ed.), The Physics of Cognitive Processes. Teaneck, NJ: World Scientific, 1987.|$|E
2500|$|The {{primary reason}} for why {{experience}} and learning curve effects apply, of course, is the complex processes of learning involved. [...] As discussed in the main article, learning generally begins with making successively larger finds and then successively smaller ones. [...] The equations for these effects come from the usefulness of mathematical models for certain somewhat predictable aspects of those generally <b>non-deterministic</b> processes.|$|E
