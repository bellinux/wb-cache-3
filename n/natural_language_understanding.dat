1074|10000|Public
25|$|Research in AI is {{concerned}} with producing machines to automate tasks requiring intelligent behavior. Examples include control, planning and scheduling, the ability to answer diagnostic and consumer questions, handwriting, natural language, speech and facial recognition. As such, the study of AI has also become an engineering discipline, focused on providing solutions to real life problems, knowledge mining, software applications, strategy games like computer chess and other video games. One of the biggest limitations of AI is {{in the domain of}} actual machine comprehension. Consequentially <b>natural</b> <b>language</b> <b>understanding</b> and connectionism (where behavior of neural networks is investigated) are areas of active research and development.|$|E
2500|$|In 2008, Reddy {{received}} the IEEE James L. Flanagan Speech and Audio Processing Award, [...] "for leadership and pioneering contributions to speech recognition, <b>natural</b> <b>language</b> <b>understanding,</b> and machine intelligence".|$|E
2500|$|The {{programming}} language Prolog [...] {{was developed in}} 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on <b>natural</b> <b>language</b> <b>understanding,</b> using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic {{could be used to}} represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.|$|E
50|$|He and {{his wife}} Sue founded the <b>natural</b> <b>language</b> company Brillig <b>Understanding</b> in 2012.|$|R
40|$|This paper {{describes}} an object-oriented, message-passing system for <b>natural</b> <b>language</b> text <b>understanding.</b> The application domain is {{the texts of}} Texas Instruments' patent descriptions. The object-oriented environment permits syntactic analysis modules to communicate with domain knowledge modules to resolve ambiguities as they aris...|$|R
40|$|In this paper, we {{describe}} a tool designed to generate semi-automatically the sortal constraints specific to a domain {{to be used}} in a <b>natural</b> <b>language</b> (NL) <b>understanding</b> system. This tool is evaluated using the SRI Gemini NL understanding system in the ATIS domain. Comment: COLING 94 's paper - Latex document - 6 page...|$|R
5000|$|Linguistics: {{computational}} linguistics, <b>natural</b> <b>language</b> <b>understanding,</b> {{natural language}} processing.|$|E
5000|$|Nuance Communications, voice, <b>natural</b> <b>language</b> <b>understanding,</b> {{reasoning}} and systems integration ...|$|E
5000|$|Implementation of <b>natural</b> <b>language</b> <b>understanding</b> systems using dynamic attractor {{sequences}} ...|$|E
50|$|Controlled <b>natural</b> <b>languages</b> are subsets of <b>natural</b> <b>languages</b> whose grammars and {{dictionaries}} {{have been}} restricted {{in order to}} reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind {{the development and implementation of}} a controlled <b>natural</b> <b>language</b> typically is to aid non-native speakers of a <b>natural</b> <b>language</b> in <b>understanding</b> it, or to ease computer processing of a <b>natural</b> <b>language.</b> An example of a widely used controlled <b>natural</b> <b>language</b> is Simplified English, which was originally developed for aerospace industry maintenance manuals.|$|R
40|$|Our {{goal is to}} {{implement}} a program for the machine verification of textbook proofs. We study the task from both the linguistics and automated reasoning perspective and give an in-depth analysis for a sample textbook proof. We propose a framework for <b>natural</b> <b>language</b> proof <b>understanding</b> that extends and integrates state-of-the-art technologies from <b>Natural</b> <b>Language</b> Processing (Discourse Representation Theory) and Automated Reasoning (Proof Planning) in a novel and promising way, having the potential to initiate progress {{in both of these}} disciplines...|$|R
40|$|Abstract. This paper {{describes}} HERMETO, a computational {{environment for}} fully-automatic, both syntactic and semantic, <b>natural</b> <b>language</b> analysis and <b>understanding.</b> HERMETO converts lists into networks {{and has been}} used to enconvert Brazilian Portuguese and English sentences into Universal Networking Language (UNL) hypergraphs...|$|R
5000|$|James Allen (1995); <b>Natural</b> <b>Language</b> <b>Understanding,</b> 2nd Edition, Pearson Education Publishers.|$|E
5000|$|Modularity in Knowledge Representation and <b>Natural</b> <b>Language</b> <b>Understanding.</b> Bradford Books/MIT Press, 1987.|$|E
50|$|Green, Georgia M. 1996. 2nd ed. Pragmatics and <b>Natural</b> <b>Language</b> <b>Understanding.</b> Lawrence Erlbaum Associates.|$|E
50|$|Within the UNL Program, {{the process}} of {{representing}} <b>natural</b> <b>language</b> sentences in UNL graphs is called UNLization, and {{the process of}} generating <b>natural</b> <b>language</b> sentences out of UNL graphs is called NLization. UNLization, which involves <b>natural</b> <b>language</b> analysis and <b>understanding,</b> is intended to be carried out semi-automatically (i.e., by humans with computer aids); and NLization is intended to be carried out fully automatically.|$|R
50|$|Veveo’s {{developments}} in <b>natural</b> <b>language</b> processing and <b>understanding</b> enabled dialog-based real-time conversational interfaces {{to be introduced}} in late 2012. These interfaces allow connected devices and applications {{to be used with}} natural conversational intelligence applied to voice interfaces. As a result, users can talk to devices with normal language and devices can respond with <b>natural</b> <b>language</b> responses.|$|R
40|$|Abstract. Converting a <b>natural</b> <b>language</b> query {{sentence}} into {{a formal}} database query {{is a major}} challenge. We have constructed NaLIX, a <b>natural</b> <b>language</b> interface for querying XML data. Through our experience with NaLIX, we find that failures in <b>natural</b> <b>language</b> query <b>understanding</b> can often be dealt with as ambiguities in term meanings. These failures are typically the result of either the user’s poor knowledge of the database schema or the system’s lack of linguistic coverage. With automatic term expansion techniques and appropriate interactive feedback, {{we are able to}} resolve these ambiguities. In this paper, we describe our approach and present results demonstrating its effectiveness. ...|$|R
5000|$|Integrating Applications of Text and Speech Processing (<b>natural</b> <b>language</b> <b>understanding,</b> question-answering strategies, assistive technologies) ...|$|E
5000|$|The text is {{analyzed}} by a <b>Natural</b> <b>language</b> <b>understanding</b> unit (NLU), which may include: ...|$|E
50|$|Conceptual {{dependency}} {{theory is}} a model of <b>natural</b> <b>language</b> <b>understanding</b> used in artificial intelligence systems.|$|E
40|$|We {{introduce}} a methodology for automating {{the maintenance of}} domain-specific ontologies based on <b>natural</b> <b>language</b> text <b>understanding.</b> A given taxonomy is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual "quality" of various forms of evidence underlying the generation and refinement of concept hypotheses. On {{the basis of the}} quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base...|$|R
40|$|Since the {{introduction}} of the Fortran programming language some 60 years ago, there has been little progress in making error messages more user-friendly. A first step in this direction is to translate them into the <b>natural</b> <b>language</b> of the students. In this paper we propose a simple script for Linux systems which gives word by word translations of error messages. It works for most programming languages and for all <b>natural</b> <b>languages.</b> <b>Understanding</b> the error messages generated by compilers is a major hurdle for students who are learning programming, particularly for non-native English speakers. Not only may they never become "fluent" in programming but many give up programming altogether. Whereas programming is a tool which can be useful in many human activities, e. g. history, genealogy, astronomy, entomology, in many countries the skill of programming remains confined to a narrow fringe of professional programmers. In all societies, besides professional violinists there are also amateurs. It should be the same for programming. It is our hope that once translated and explained the error messages will be seen by the students as an aid rather than as an obstacle and that in this way more students will enjoy learning and practising programming. They should see it as a funny game. Comment: 14 pages, 1 figur...|$|R
40|$|We {{present a}} decoder for {{dialogue}} systems having both {{automatic speech recognition}} (ASR) and <b>natural</b> <b>language</b> (NL) <b>understanding.</b> Accurate and fast decoding of spoken utterances {{is a key to}} speech understanding. A good set of multiple word hypotheses produced by a decoder is also crucial for flexible integration of knowledge sources for <b>language</b> <b>understanding.</b> We introduce three contributions to improve the search efficiency and the effectiveness of our decoder, namely: (1) handling of long-term language models with crossword triphone models, (2) speed-up of search and likelihood computation; and (3) construction of high quality word graphs for interfaces between the ASR and NL understanding modules. The search algorithm was successfully applied to a <b>natural</b> <b>language</b> call routing task in a banking domain. As a result, we achieve both, word hypothesis decoding and word graph generation in real time with nearly no loss in word error rate in speech recognition. 1...|$|R
50|$|Allen is {{the author}} of the {{textbook}} <b>Natural</b> <b>Language</b> <b>Understanding</b> (Benjamin-Cummings, 1987; 2nd ed., 1995).|$|E
5000|$|<b>Natural</b> <b>language</b> <b>understanding</b> (and subproblems such as text mining, machine translation, {{and word}} sense disambiguation) ...|$|E
5000|$|In 2015, Manulife Bank {{introduced}} both <b>Natural</b> <b>Language</b> <b>Understanding</b> and Voice Biometrics {{to their}} telephone banking system.|$|E
50|$|Boris Katz, born in Kishinev, Moldavian SSR, Soviet Union (now Chișinău, Moldova) is a {{principal}} research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory {{and head of the}} Laboratory's InfoLab Group. His research interests include <b>natural</b> <b>language</b> processing and <b>understanding,</b> machine learning and intelligent information access. His brother Victor Kac is a mathematician at MIT.|$|R
40|$|This paper {{introduces}} {{the problem of}} generating descriptions of n-dimensional spatial data by decomposing it via modelbased clustering. I apply the approach to the error function of supervised classification algorithms, a practical problem that uses <b>Natural</b> <b>Language</b> Generation for <b>understanding</b> the behaviour of a trained classifier. I demonstrate my system on a dataset taken from CoNLL shared tasks. ...|$|R
40|$|The LUNA corpus is a multi-lingual, multidomain spoken {{dialogue}} corpus {{currently under}} development {{that will be}} used to develop a robust <b>natural</b> spoken <b>language</b> <b>understanding</b> toolkit for multilingual dialogue services. The LUNA corpus will be annotated at multiple levels to include annotations of syntactic, semantic, and discourse information; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA-based architecture. 1...|$|R
5000|$|... “Unification in Grammar”, in Dahl, V., and P. Saint-Dizier, <b>Natural</b> <b>Language</b> <b>Understanding</b> and Logic Programming, North Holland, 1985.|$|E
50|$|Stochastic {{semantic}} analysis is an approach used {{in computer science}} as a semantic component of <b>natural</b> <b>language</b> <b>understanding.</b>|$|E
50|$|She is {{also one}} of the {{founding}} partners of Weniwen - a tech start-up specialises in <b>natural</b> <b>language</b> <b>understanding.</b>|$|E
40|$|Handling {{everyday}} {{tasks such}} as search, classication and integration is becoming increasingly dicult and sometimes even impossible due to the increasing streams of data available. To overcome such an information overload we need more accurate information processing tools capable of handling big amounts of data. In particular, handling metadata can give us leverage over the data and enable structured processing of data, however, while some of this metadata is in a computer readable format, {{some of it is}} manually created in ambiguous <b>natural</b> <b>language.</b> Thus, accessing the semantics of <b>natural</b> <b>language</b> can increase the quality of information processing. We propose a <b>natural</b> <b>language</b> metadata <b>understanding</b> architecture that enables applications such as semantic matching, classication and search based on <b>natural</b> <b>language</b> metadata by providing a translation into a formal language which outperforms {{the state of the art}} by 15 %...|$|R
40|$|Although it is {{generally}} accepted that semantic knowledge is important to language comprehension, the question remains as to when this knowledge should be applied in the task of machine <b>language</b> <b>understanding.</b> This paper discusses two related results in the filed of Cognitive Neuroscience which suggests humans are activating deep semantic and episodic information early in <b>language</b> <b>understanding.</b> This discussion {{is followed by a}} discussion of Direct Memory Access Parsing (DMAP), a text reading approach that leverages existing knowledge and performs integration {{in the early stages of}} parsing <b>natural</b> <b>language</b> text. DMAP’s <b>understanding</b> is driven by memory structures, and it maps immediately and directly to existing knowledge. Machine reading is fundamentally about integrating new knowledge gleaned from reading with existing knowledge, and it seems not only efficient, but cognitively plausible for this process to be started early in the parsing of <b>natural</b> <b>language...</b>|$|R
40|$|The {{performance}} of deep learning in <b>natural</b> <b>language</b> processing has been spectacular, but {{the reasons for}} this success remain unclear because of the inherent complexity of deep learning. This paper provides empirical evidence of its effectiveness and of a limitation of neural networks for language engineering. Precisely, we demonstrate that a neural language model based on long short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law, two representative statistical properties underlying <b>natural</b> <b>language.</b> We discuss the quality of reproducibility and the emergence of Zipf's law and Heaps' law as training progresses. We also point out that the neural language model has a limitation in reproducing long-range correlation, another statistical property of <b>natural</b> <b>language.</b> This <b>understanding</b> could provide a direction for improving the architectures of neural networks. Comment: 21 pages, 11 figure...|$|R
