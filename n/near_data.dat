16|1614|Public
50|$|Travel to Data Kakus {{involves}} a five hours 4WD drive from Bintulu or Belaga, {{to get to}} Data Kakus {{you have to take}} the Bintulu-Bakun road and look for the junction to KTS camp from KTS camp to Simpang Jonat it takes about 60 km using the logging road and from simpang Jonat it takes another 18 km to reach Data Kakus. or it is possible to trek Data Kakus from Belaga and it takes five to seven hours trekking from Long Sungai Pelaran (Sg. Sanan baan Kenyah), Sungai Ngajah from Rajang river then conquer Gunung Ngajah about 444m. The Kakus River or Batang Tatau begins as a stream <b>near</b> <b>Data</b> Kakus called Alo Nyabet or e-iut Nyabet or Sungai Burok, and it flows through Data Kakus, the end point where small boats such as 'moto setinding' can navigate. It is also possible to travel downriver from Data Kakus by longboat to Tatau town,it takes almost a day to get to Tatau town by boat as the river is too shallow, winding and small.|$|E
40|$|Security {{is a vital}} {{requirement}} in many high-end systems, es-pecially those that make up modern cloud infrastructures. Cloud systems are vulnerable to many attacks, including those by un-trusted cloud operators that have access to physical hardware. Memory authentication confirms that an attacker is not modifying the values being returned by the memory system. But it imposes a severe memory bandwidth overhead that limits its adoption in highly cost-constrained cloud systems. The ideas proposed in this position paper attempt to lower these overheads and make memory authentication more palatable for cloud systems. While <b>near</b> <b>data</b> processing has been explored to improve ap-plication performance and power efficiency, {{it has not been}} lever-aged to improve auxiliary operations for security. This paper ar-gues that logic for memory authentication should be placed on the memory module itself so it has access to significantly higher bandwidth. To preserve security guarantees, the <b>near</b> <b>data</b> pro-cessor and its link to the main host processor have to be made secure. We describe this design and estimate the first-order po-tential for benefit. 1...|$|E
40|$|Long-lived {{systems that}} can be {{untethered}} and unattended – Low-duty cycle operation with bounded latency – Exploit redundancy and heterogeneous tiered systems • Leverage data processing inside the network – Thousands or millions of operations per second can be done using energy of sending a bit over 10 or 100 meters (Pottie 00) – Exploit computation <b>near</b> <b>data</b> to reduce communication • Self configuring systems {{that can be}} deployed ad hoc – Un-modeled physical world dynamics makes systems appear ad hoc – Measure and adapt to unpredictable environment – Exploit spatial diversity and density of sensor/actuator nodes • Achieve desired global behavior with adaptive localized algorithms – Can’t afford to extract dynamic state information needed for centralized contro...|$|E
3000|$|... ‖ω‖is the bound norm of ω. γi is the {{distance}} between <b>nearest</b> <b>data</b> and the hyperplane, and it is usually supposed that γi[*]=[*] 1 to simplify the calculation.|$|R
40|$|We {{present a}} novel {{approach}} that protects trajectory privacy of users who access location-based services through a moving k nearest neighbor (MkNN) query. An MkNN query continuously returns the k <b>nearest</b> <b>data</b> objects for a moving user (query point). Simply updating a user’s imprecise location such as a region instead of the exact position to a location-based service provider (LSP) cannot ensure privacy of the user for an MkNN query: continuous disclosure of regions enables the LSP to follow a user’s trajectory. We identify the problem of trajectory privacy that arises from the overlap of consecutive regions while requesting an MkNN query and provide the first solution to this problem. Our approach allows a user to specify the confidence level that represents a bound of how much more the user may need to travel than the actual kth <b>nearest</b> <b>data</b> object. By hiding a user’s required confidence level and the required number of <b>nearest</b> <b>data</b> objects from an LSP, we develop a technique to prevent the LSP from tracking the user’s trajectory for MkNN queries. We propose an efficient algorithm for the LSP to find k <b>nearest</b> <b>data</b> objects for a region with a user’s specified confidence level, which is an essential component to evaluate an MkNN query in a privacy preserving manner; this algorithm is at least two {{times faster than the}} state-of-the-art algorithm. Extensive experimental studies validate the effectiveness of our trajectory privacy protection technique and the efficiency of our algorithm. ...|$|R
3000|$|... 5) Fetch {{data from}} the {{matching}} waveform between the previous point of the <b>nearest</b> <b>data</b> point to P [...] i [...] and the previous extreme point to obtain the extension waveform and extend the waveform to the left endpoint.|$|R
40|$|Rapidly growing data volumes push today's {{analytical}} systems {{close to the}} feasible processing limit. Massive parallelism is one possible solution to reduce the computational time of analytical algorithms. However, data transfer becomes a significant bottleneck since it blocks system resources moving data-to-code. Technological advances allow to economically place compute units close to storage and perform data processing operations close to data, minimizing data transfers and increasing scalability. Hence the principle of <b>Near</b> <b>Data</b> Processing (NDP) and the shift towards code-to-data. In the present paper we claim {{that the development of}} NDP-system architectures becomes an inevitable task in the future. Analytical DBMS like HPE Vertica have multiple points of impact with major advantages which are presented within this paper. ...|$|E
40|$|As {{the amount}} of {{scientific}} data grows {{to the point where}} the Internet bandwidth no longer supports its transfer, it becomes necessary to make powerful computational services available <b>near</b> <b>data</b> repositories. Such services allow remote researchers to start long-running parallel computations on the data. Current execution services do not provide remote users with adequate management facilities for this style of computing. This paper describes the PEX system. It has an architecture based on partitionable group communication. We describe how PEX maintains replicated state in the face of processor failures and network partitions, and how it allows remote clients to manipulate this state. We present some performance numbers, and close with discussing related work. ...|$|E
40|$|While cluster {{computing}} frameworks are continuously evolving {{to provide}} real-time data analysis capabilities, Apache Spark {{has managed to}} be at the forefront of big data analytics for being a unified framework for both, batch and stream data processing. There is also a renewed interest in <b>Near</b> <b>Data</b> Processing (NDP) due to technological advancement in the last decade. However, it is not known if NDP architectures can improve the performance of big data processing frameworks such as Apache Spark. In this paper, we build the case of NDP architecture comprising programmable logic based hybrid 2 D integrated processing-in-memory and instorage processing for Apache Spark, by extensive profiling of Apache Spark based workloads on Ivy Bridge Server. QC 20171124 </p...|$|E
3000|$|... {{represents}} the summation on j. The function T(t) is introduced {{to exclude the}} contribution from the i-th measurement itself and its <b>nearest</b> <b>data</b> in time, which are considered to vary {{in a manner similar}} to the measurement (the explicit form of the function T(t) will be described later).|$|R
25|$|The {{simplest}} {{interpolation method}} is to locate the <b>nearest</b> <b>data</b> value, and assign the same value. In simple problems, this method {{is unlikely to be}} used, as linear interpolation (see below) is almost as easy, but in higher-dimensional multivariate interpolation, this could be a favourable choice for its speed and simplicity.|$|R
3000|$|The {{separating}} hyper-plane {{that creates}} the maximum distance between the plane and the <b>nearest</b> <b>data</b> is called the optimal separating hyper-plane as shown in Fig.  1. The maximum-margin classifier is the discriminate function that maximizes the geometric margin 1 /||w||, which is equivalent to minimizing ||w|| 2. This leads to the following constrained optimization problem [28, 29]: [...]...|$|R
40|$|A fault {{tolerant}} system like Hadoop [1] running on a commodity based cluster schedules two types of tasks, Regular Map/Reduce tasks and Speculative Map/Reduce tasks. Regular Map/Reduce tasks are spawned when a MapReduce [3, 2] application is launched, whereas speculative tasks are instigated {{when there is a}} slow or a failed task. Hadoop’s task scheduler exploits data locality, and tries to schedule the regular map tasks <b>near</b> <b>data.</b> By assigning one DFS block per map task, a large number of map tasks are generated that are cumbersome to manage and also impacts I/O performance. The number of map tasks can be reduced by increasing the split size from one DFS block to multiple DFS blocks. For example, random sort with 294 map tasks took 388 sec to finish, while 154 map tasks took 292 sec. A logical contiguou...|$|E
40|$|A large {{fraction}} of MapReduce execution {{time is spent}} processing the Map phase, and a large {{fraction of}} Map phase execution time is spent sorting the intermediate key-value pairs generated by the Map function. Sorting accelerators can achieve high performance and low power because they lack the overheads of sorting implementations on general purpose hardware, such as instruction fetch and decode. We find that sorting accelerators are a good match for 3 D-stacked <b>Near</b> <b>Data</b> Processing (NDP) because their sorting throughput is so high that it saturates the memory bandwidth available in other memory organizations. The increased sorting performance and low power requirement of fixed-function hardware lead to very high Map phase perfor-mance and energy efficiency, reducing Map phase execution time by up to 92 %, and reducing energy consumption by up to 91 %. We further find that sorting accelerators in a less exotic form of NDP outperform more expensive forms of 3 D-stacked NDP without accelerators. We also implement the accelerator on an FPGA to validate our claims. I...|$|E
40|$|Half-space depth (also called Tukey depth or {{location}} depth) {{is one of}} {{the most}} commonly studied data depth measures because it possesses many desirable properties for data depth functions. The data depth contours bound regions of increasing depth. For the sample case, there are two competing definitions of contours: the rank-based contours and the cover-based contours. In this paper, we present three dynamic algorithms for maintaining the half-space depth of points and contours: The first maintains the half-space depth of a single point in a data set in O(n) time per update (insertion/deletion) and overall linear space. By maintaining such a data structure for each data point, we present an algorithm for dynamically maintaining the rank-based contours in O(n· n) time per update and overall quadratic space. The third dynamic algorithm maintains the cover-based contours in O(n·^ 2 n) time per update and overall quadratic space. We also augment our first algorithm to maintain the local cover-based contours at data points while maintaining the same complexities. A corollary of this discussion is a strong structural result of independent interest describing the behavior of dynamic cover-based contours <b>near</b> <b>data</b> points. Comment: 31 page...|$|E
40|$|In this paper, a {{training}} data selection method for multilayer neural networks (MLNNs) in on-line training is proposed. Purpose of {{the reduction in}} training data is reducing the computation complexity of the training and saving the memory to store the data without loosing generalization performance. This method uses a pairing method, which selects the <b>nearest</b> neighbor <b>data</b> by finding the <b>nearest</b> <b>data</b> in the different classes. The network is trained by the selected data. Since the selected data located along data class boundary, the trained network can guarantee generalization performance. Efficiency of this method for the on-line training is evaluated by computer simulation. 1. Int reductio...|$|R
40|$|Location-based service Continuous spatial query View field nearest {{neighbor}} query query {{can be employed}} for applications such as augmented reality systems, tour guide sys-jects outsid lled the vi {{nearest neighbor}} query. Given the location and {{the field of view}} of a user, the view field nearest neighbor query retrie <b>nearest</b> <b>data</b> object (from the user’s location) that exists in the user’s view field...|$|R
25|$|PureData {{is focused}} at three main tasks within {{enterprise}} computing: business intelligence, <b>near</b> real-time <b>data</b> analysis and online transactional processing.|$|R
40|$|Graduation date: 1994 A {{numerical}} method {{based on}} the the method of characteristics for hyperbolic systems of partial differential equations in four independent variables is developed and used for solving time domain Maxwell's equations. The method uses the characteristic hypersurfaces and the characteristic conditions to derive a set of independent equations relating the electric and magnetic field components on these hypersurfaces. A discretization scheme is developed to solve for the unknown field components at each time step. The method retains many of the good features of the original method of characteristics for hyperbolic systems in two independent variables, such as optimal time step, good behavior <b>near</b> <b>data</b> discontinuities {{and the ability to}} treat general boundary conditions. The method is exemplified by calculating the time domain response of a few typical planar interconnect structures to Gaussian and unit step excitations. Although the general emphasis is on interconnect problems, the method is applicable to a number of other transient electromagnetic field problems governed by Maxwell's equations. In addition to the method of characteristics a finite difference scheme, known in mathematic circles as the modified Richtmyer scheme, is applied to the time domain solution of Maxwell's equations. Both methods should be useful for efficient full wave analysis of three dimensional electromagnetic field problems...|$|E
40|$|While Processing-in-Memory {{has been}} {{investigated}} for decades, {{it has not been}} embraced commercially. A number of emerging technologies have renewed interest in this topic. In particular, the emergence of 3 D stacking and the imminent release of Micron’s Hybrid Memory Cube device have made it more practical to move computation near memory. However, the literature is missing a detailed analysis of a killer application that can leverage a <b>Near</b> <b>Data</b> Computing (NDC) architecture. This paper focuses on in-memory MapReduce workloads that are commercially important and are especially suitable for NDC because of their embarrassing parallelism and largely localized memory accesses. The NDC architecture incorporates several simple processing cores on a separate, non-memory die in a 3 D-stacked memory package; these cores can perform Map operations with efficient memory access and without hitting the bandwidth wall. This paper describes and evaluates a number of key elements necessary in realizing efficient NDC operation: (i) low-EPI cores, (ii) long daisy chains of memory devices, (iii) the dynamic activation of cores and SerDes links. Compared to a baseline that is heavily optimized for MapReduce execution, the NDC design yields up to 15 X reduction in execution time and 18 X reduction in system energy. 1...|$|E
40|$|We {{investigate}} qualitatively and quantitatively {{the impact}} of the general relativistic gravito-electromagnetic forces on hyperbolic orbits around a massive spinning body. The gravito-magnetic field, which is the cause of the well known Lense-Thirring precessions of elliptic orbits, is generated by the spin S of the central body. It deflects and displaces the trajectories differently according to the mutual orientation of S and the orbital angular momentum L of the test particle. The gravito-electric force, which induces the Einstein precession of the perihelion of the orbit of Mercury, always deflects the trajectories inward irrespective of the L-S orientation. We numerically compute their effect on the range r, radial and transverse components v_r and v_τ of the velocity and speed v of the NEAR spacecraft at its closest approach with the Earth in January 1998 when it experienced an anomalous increase of its asymptotic outgoing velocity v_∞ o of 13. 46 +/- 0. 01 mm sec^- 1; while the gravito-electric force was modeled in the software used to process the <b>NEAR</b> <b>data,</b> this was not done for the gravito-magnetic one. The range-rate and the speed are affected by general relativistic gravito-electromagnetism at 10 ^- 2 (gravito-electric) - 10 ^- 5 (gravito-magnetic) mm sec^- 1 level. The changes in the range are of the order of 10 ^- 2 (gravito-magnetic) - 10 ^ 1 (gravito-electric) mm. Comment: LaTex 2 e, 23 pages, 15 figures, 1 table, 15 references. To appear in Scholarly Research Exchange (SYREXE). I thank the referees S. Adler and J. William...|$|E
30|$|Perform a {{local and}} fast search within the grid {{to find the}} <b>nearest</b> {{neighboring}} <b>data</b> points for each interpolated point.|$|R
30|$|As one of {{the first}} {{countries}} for smart metering infrastructure development, Italy has deployed smart meter to nearly all the customers with the PLC technology to transfer smart meter <b>data</b> to the <b>nearest</b> <b>data</b> concentrator located in the MV/LV substation. Then these data are sent to the DSO’s data centers for recording and data analysis. There are around 30 million meters and 400, 000 secondary substation concentrators installed (Bahmanyar et al., 2016).|$|R
40|$|A {{training}} data selection method is proposed for multilayer neural networks (MLNNs). This method selects {{a small number}} of the {{training data}}, which guarantee both generalization and fast training of the MLNNs applied to pattern classification. The generalization will be satisfied using the data locate close to the boundary of the pattern classes. However, if these data are only used in the training, convergence is slow. This phenomenon is analyzed in this paper. Therefore, in the proposed method, the MLNN is first trained using some number of the data, which are randomly selected (Step 1). The data, for which the output error is relatively large, are selected. Furthermore, they are paired with the <b>nearest</b> <b>data</b> belong to the different class. The newly selected data are further paired with the <b>nearest</b> <b>data.</b> Finally, pairs of the data, which locate close to the boundary, can be found. Using these pairs of the data, the MLNNs are further trained (Step 2). Since, there are some variations to combine Steps 1 and 2, the proposed method can be applied to both off-line and on-line training. The proposed method can reduce the number of the training data, at the same time, can hasten the training. Usefulness is confirmed through computer simulation...|$|R
40|$|Background: This {{qualitative}} study explores {{the transition to}} school for three children with Down syndrome {{from the perspective of}} the parents and school staff involved. Research has identified the importance of the transition from home or early childhood settings to primary school for the child and their families and has also identified the challenges associated with the transition process for parents of children with intellectual disability. Because of legislative and social changes both nationally and internationally, children with Down syndrome in Ireland, are increasingly attending mainstream primary schools, however little is known of how the transition process unfolds and how it is experienced by parents or school staff in an Irish context. Methods: A qualitative description methodology has been used in this study. Multiple interviews were carried out with all participants over the course of the transition using a semi structured interview format. Thematic analysis was used to identify patterns of meaning across the data. and was used to generate rich, <b>near</b> <b>data,</b> description of the transition process and participant experiences. Results: The three central themes were present in the data. These were; Home School Relationship, Social Engagement and Supporting Participation in School Activities. Both parents and school staff actively supported the child?s transition to school within the context of a fluid and dynamic transition process. Challenges that arose during the transition process related to how communication between home and school was managed, varying expectations for social engagement and the requirement by parents and school staff for support from the service provider multi-disciplinary team. Conclusions: The establishment of a positive collaborative relationship between parents, school and the disability service provider is central to how the transition is experienced. This relationship is underpinned by flexible communication and appropriate transition practices Keywords: Transition Process, Down syndrome, Mainstream Primary School, Irish, Parents, Communication. Peer Social Interaction, Transition Practices...|$|E
40|$|The Internet is a partitionable low {{bandwidth}} network. Two computers connected by the Internet could temporarily become unable to communicate for various reasons, from a failure at either end, to overloading of an intermediate node connecting the two, to an out-of-date routing table. The bandwidth of the Internet is relatively low: its data transfer speed is inadequate for applications {{that need to}} access very large data objects. Since its introduction the Internet kept a much faster rate of increase for the data available than for the data transfer speed. This trend {{is not likely to}} change any time soon. This thesis presents techniques to deal with availability and consistency in partitionable {{low bandwidth}} networks, and it presents a service suitable for use in such networks. The first part of the thesis presents replication techniques suitable in a partitionable network. A partitionable network puts goals of availability and consistency at odds. Making objects available in a partitionable network requires replication; yet modifications of replicated objects during a partition can introduce inconsistencies. We show various replication techniques that provide continuous availability while managing inconsistencies among replicas. The second part of the thesis describes the PEX system, an example of a service that instantiates these abstractions. PEX is an execution service that facilitates processing of remote data. It allows computations to be performed <b>near</b> <b>data,</b> so that data need not be copied over the network. It provides a session interface which facilitates the management of related computations operating on scattered data. Internally, PEX replicates various information, including sessions, using some of the techniques introduced in the first section of the thesis. This permits the system to keep computations running in the face of partitions. The third part of the thesis gives two example applications that use PEX (a distributed shell and a parallel make) and reports the performance of the PEX system...|$|E
40|$|We present Ulysses and <b>NEAR</b> <b>data</b> {{from the}} {{detection}} of the short or intermediate duration (2 s) gamma-ray burst GRB 000301 C (2000 March 1. 41 UT). The gamma-ray burst (GRB) was localised by the Inter Planetary Network (IPN) and RXTE to an area of 50 arcmin^ 2. A fading optical counterpart was subsequently discovered with the Nordic Optical Telescope (NOT) about 42 h after the burst. The GRB lies at {{the border between the}} long-soft and the short-hard classes of GRBs. If GRB 000301 C belongs to the latter class, this would be the first detection of an afterglow to a short-hard burst. We present UBRI and JHK photometry {{from the time of the}} discovery until 11 days after the burst. Finally, we present spectroscopic observations of the optical afterglow obtained with the ESO VLT Antu telescope 4 and 5 days after the burst. The optical light curve is consistent with being achromatic from 2 to 11 days after the burst and exhibits a break. A broken power-law fit yields a shallow pre-break decay power-law slope of a_ 1 =- 0. 72 +- 0. 06, a break time of t_b= 4. 39 +- 0. 26 days after the burst, and a post-break slope of a_ 2 =- 2. 29 +- 0. 17, which is best explained by a sideways expanding jet in an ambient medium of constant mean density. In the optical spectrum we find absorption features that are consistent with FeII, CIV, CII, SiII and Ly-a at a redshift of 2. 0404 +- 0. 0008. We find evidence for a curved shape of the spectral energy distribution of the observed afterglow. It is best fitted with a power-law spectral distribution with index b ~ - 0. 7 reddened by an SMC-like extinction law with A_V~ 0. 1 mag. Based on the Ly-a absorption line we estimate the HI column density to be log(N(HI)) = 21. 2 +- 0. 5. This is the first direct indication of a connection between GRB host galaxies and Damped Ly-a Absorbers. Comment: 13 pages, 11 figures, accepted for publication in A&A. Revised and updated section on afterglow SED (6. 2). Findingchart added. References update...|$|E
30|$|The index a(X;i)∈{ 1,…,n(i)} of the {{measurement}} that is associated {{is the one}} with the largest individual likelihood, i.e. <b>nearest</b> neighbour <b>data</b> association is performed (e.g. see [21]).|$|R
50|$|TagPay uses sound-based <b>near</b> sound <b>data</b> {{transfer}} (NSDT) {{technology to}} secure electronic transactions. TagPay's target markets are in emerging economies where populations are un-banked and secure payment systems are needed.|$|R
40|$|The {{radiation}} characteristics of an antenna are fully determined by its aperture distribution. Measured <b>Near</b> Field <b>data</b> gives an impression {{but this is}} not good enough to detect small anomalies. For good antenna diagnostics, the field at the aperture plane is required. The Near Field Measurement Technique calculates the Far Field of the AUT is by determining the so called Plane Wave Spectrum (PWS) from the measured <b>Near</b> Field <b>data.</b> Once the PWS is known, the electric field at any location can be calculated. In fact, the Far Field is only a limiting case of the PWS. Using the PWS, the field at the aperture of the AUT can be calculated. This aperture image is much "sharper" than the measured Near Field. Even small anomalies, hardly visible in the measured <b>Near</b> Field <b>data,</b> appear quite clearly. To determine the excitation coefficients of array elements, the Far Field data has to be corrected for the element radiation pattern. Unfortunately, some steps in the processing confine the accuracy of these method...|$|R
40|$|Big data {{collections}} in many Scientific domains have inherently rich spatial and geospatial features. Spatial location {{is among the}} core aspects of data in Earth observation sciences,astronomy,and seismology to name a few. The NLeSC project aims at developing software technology and tools for building of generic geo-spatial analysis systems with focus on three dimensional (3 D) city models. The project has three major partners, VU Geographic Information Systems (GIS) group / Geodan represented by Prof. Henk Scholten, CWI Database group / MonetDB solutions represented by Prof. Martin Kersten and TUDelft Geo-information for Crisis Management group represented by Prof. Sisi Zlatanova. Digital 3 D city models {{play a crucial role}} in research of urban phenomena; they form the basis for e. g. flow simulations (e. g. wind streams, water runoff and heat island effects), urban planning, analysis of underground formations, etc. Urban scenes consist of large collections of complex objects which have rich semantic properties, such as materials, colors, etc. Modeling and storing these properties indicating the relationships between them is best handled in a relational database. Database management systems (DBMSs) are a well-established solution when it comes to archiving, filtering, analysis, and correlation of large data collections. Ability to perform analysis <b>near</b> <b>data</b> is one of the key requirements identified by the 4 th Paradigm to handle the data deluge. A single spatial DBMS offers functionality for geo-spatial modeling and management of semantic properties in one place, thus avoiding the need for multiple software tools associated with high volume data transfer and format transformations. The provision of spatial and geo-spatial features in database systems needs to be extended and brought to maturity to fulfill the requirements of real-world scientific applications. A class of DBMSs, called column-stores, have proven efficiency for analytic applications on extremely large data sets. Column stores have become the de-facto standard for managing large data warehouses. Although column stores have a proven track record in business analytics, their pros- and cons- for GIS applications are not yet well understood. Such core technology extensions for geo-spatial analytics are beneficial for public as well as private sector. Municipalities and spatial data providers can both extend and enhance their offerings of data products and services. Several municipalities have expressed an interest in using the technology in their urban projects for management and analysis of geo-spatial data collections, and the port of Rotterdam will be used as a driving application during the design and implementation. A Port of Rotterdam’s 3 D GIS has been built to aid various multi-stakeholder construction projects where new structures are built in, on top of and around the existing port (underground) infrastructure. Extending and modifying the port is challenging as it is home to many different companies that often cover extensive areas and manage vast infrastructures such roads, pipes and cables. The port thus requires a 3 D GIS that is able to store all harbor assets and analyze existing assets with future interventions and detect conflicts. The 3 D GIS currently being built is aimed at collecting data from different sources and formats and converting it to a common format to enable 3 D operations and analyses such as 3 D intersections. The development relies as much as possible on existing open-source tools and libraries, and be distributed through an open-source product to maximize the take-up in education and science. It will be offered as an associated NLeSC technology...|$|E
50|$|PHIN {{attempts}} {{to provide the}} public health sector with continuous access to necessary health care information. Access to <b>near</b> real-time <b>data</b> {{attempts to}} improve community based interventions that are implemented {{as a result of}} terrorism or disease outbreaks.|$|R
40|$|Data-driven {{computing}} {{in applied}} mechanics utilizes the material data set directly, and hence is free from errors and uncertainties {{stemming from the}} conventional material modeling. This paper presents a data-driven approach that is robust against noise and outliers in the data set. For each structural element, we extract the material property from some <b>nearest</b> <b>data</b> points. Using the nearest neighbors reduces the influence of noise, compared with the existing method that uses a single data point. Also, the robust regression is adopted to reduce {{the influence of the}} outliers. Numerical experiments on static equilibrium analysis of trusses are performed to illustrate that the proposed method is robust against the presence of noise and outliers and, hence, is effective for dealing with real-world data...|$|R
50|$|Hierarchical {{memory is}} a {{hardware}} optimization {{that takes the}} benefits of spatial and temporal locality {{and can be used}} on several levels of the memory hierarchy. Paging obviously benefits from temporal and spatial locality. A cache is a simple example of exploiting temporal locality, because it is a specially designed, faster but smaller memory area, generally used to keep recently referenced <b>data</b> and <b>data</b> <b>near</b> recently referenced <b>data,</b> which can lead to potential performance increases.|$|R
40|$|Abstract – Cloud {{computing}} delivers infrastructure, platform, {{and software}} (applications) as services, {{which are made}} available to consumers as subscription-based services under the pay-as-you-go model. CloudAnalyst is a tool that helps developers to simulate large-scale Cloud applications {{with the purpose of}} understanding performance of such applications under various deployment configurations. CloudAnalyst deploys different service brokering policies depending on the requirements of the cloud application. The proximity-based routing policy selects <b>nearest</b> <b>data</b> center without taking into account the response time. At times, the proximity-based routing policy may cause overloading of the closest data center. I propose an enhanced proximity-based routing policy that redirects a part/whole of the traffic to the next nearest datacenter in the same region and thereby, reduces overloading of the closest datacenter and avoids SLA violation...|$|R
30|$|For {{more than}} a decade BGS have {{operated}} and developed a programme in geomagnetic observatory instrumentation, data acquisition and processing to enable production of <b>near</b> absolute <b>data</b> from its observatories in <b>near</b> realtime. These <b>data,</b> although not labeled as such at the time of production, to all intents and purposes can, retrospectively, claim to be QD data. One of the intentions of the work described in this paper is to test this statement. For the purposes of clarity and simplicity we use the label “QD data” when referring to the historical data set of retrospectively labeled QD data or candidate QD data that have been used in the analysis.|$|R
