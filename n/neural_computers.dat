17|218|Public
25|$|Differentiable <b>neural</b> <b>computers</b> (DNC) are an NTM extension. They out-performed neural Turing machines, long {{short-term}} memory systems and memory networks on sequence-processing tasks.|$|E
50|$|Differentiable <b>neural</b> <b>computers</b> are an {{outgrowth}} of neural Turing machines, with attention mechanisms that control where the memory is active, and improved performance.|$|E
5000|$|The DNC {{model is}} similar to the Von Neumann architecture, and because of the resizability of memory, it is turing complete. Differentiable <b>Neural</b> <b>Computers</b> were {{inspired}} by the mammalian hippocampus ...|$|E
40|$|<b>Neural</b> <b>computer</b> {{interface}} is alternative way {{to control}} computer without hands. It {{is defined as a}} communication system which allows user control computer or any other digital device using neural breed physiological signal. The main application of <b>neural</b> <b>computer</b> interface is various devices for people with disabilities. For example, electronic prosthetic limbs, PC‘s with additional hardware and software which allows people with motor disabilities to control PC or intelligent wheelchairs. In this work we are analyzing <b>neural</b> <b>computer</b> interface applied for robot control. The author presents <b>neural</b> <b>computer</b> interface system which allows to read EEG and head surface EMG signals, pre-process the signals, classify the signals and control Arduino 4 WD robot. We also propose approach to control robot with head surface EMG signal amplitude using 3 control commands. Robot control research using proposed approach is presented...|$|R
5000|$|Graves is {{also the}} creator of neural Turing {{machines}} and of the closely related differentiable <b>neural</b> <b>computer.</b>|$|R
40|$|In {{hopes of}} {{improving}} real-time speech recognition, a biologically based phoneme recognition algorithm was implemented on the NP- 4 <b>neural</b> <b>computer.</b> The NP- 4 <b>neural</b> <b>computer,</b> which contains programmable interconnects, neurons, synapses, and synaptic time constants, is extremely useful in computation of real-world dynamic patterns as they occur in speech. Prior to this summer, some implementations were {{done on the}} NP- 4 <b>neural</b> <b>computer,</b> and the goal during this summer was to improve work done in early stages. The newly developed algorithm, implemented in the host computer, allows neurons to be trained {{to respond to a}} particular phoneme. Testing was performed once the network was trained to find the overall responses of the neurons. The algorithm shows much promise for recognition of phonemes, with over 90 % positive responses. 1...|$|R
50|$|There {{is active}} {{research}} to make computers {{out of many}} promising new types of technology, such as optical computers, DNA computers, <b>neural</b> <b>computers,</b> and quantum computers. Most computers are universal, {{and are able to}} calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.|$|E
40|$|This is {{the first}} in a new series of books {{presenting}} research results and developments concerning the theory and applications of parallel computers, including vector, pipeline, array, fifth/future generation computers, and <b>neural</b> <b>computers.</b> All aspects of high-speed computing fall within the scope of the series, e. g. algorithm design, applications, software engineering, networking, taxonomy, models and architectural trends, performance, peripheral devices. Papers in Volume One cover the main streams of parallel linear algebra: systolic array algorithms, message-passing systems, algorithms for...|$|E
40|$|Abstract. Information systems {{anticipate}} the real world. Classical databases store, organise and search collections of data of that real world {{but only as}} weak anticipatory information systems. This {{is because of the}} reductionism and normalisation needed to map the structuralism of natural data on to idealised machines with von Neumann architectures consisting of fixed instructions. Category theory developed as a formalism to explore the theoretical concept of naturality shows that methods like sketches arising from graph theory as only non-natural models of naturality cannot capture real-world structures for strong anticipatory information systems. Databases need a schema of the natural world. Natural computing databases need the schema itself to be also natural. Natural computing methods including <b>neural</b> <b>computers,</b> evolutionary automata, molecular and nanocomputing and quantum computation have the potential to be strong. At present they are mainly at the stage of weak anticipatory systems...|$|E
5000|$|... #Caption: A {{differentiable}} <b>neural</b> <b>computer</b> {{being trained}} to store and recall dense binary numbers. Performance of a reference task during training shown.|$|R
40|$|Two {{dimensional}} image motion detection {{neural networks}} {{have been implemented}} using a general purpose analog <b>neural</b> <b>computer.</b> The <b>neural</b> circuits perform spatiotemporal feature extraction based on the cortical motion detection model of Adelson and Bergen. The <b>neural</b> <b>computer</b> provides the neurons, synapses and synaptic time-constants required to realize the model in VLSI hardware. Results show that visual motion estimation can be implemented with simple sum-and-threshold neural hardware with temporal computational capabilities. The neural circuits compute general 20 visual motion in real-time. ...|$|R
40|$|A compact VLSI <b>neural</b> <b>computer</b> {{integrated}} with an active pixel sensor {{has been under}} development to mimic what is inherent in biological vision systems. This electronic eye-brain computer is targeted for real-time machine vision applications which require both high-bandwidth communication and performance computing for data sensing, synergy of multiple types of sensory information, feature extraction, target detection, target recognition, and control functions, The <b>neural</b> <b>computer</b> {{is based on a}} composite structure which combines Annealing Cellular Neural Network (ACNN) and Hierarchical SelfOrganization Neural Network The ACNN architecture is a programmable and scalable multidimensional array of annealing neurons which are locally connected with their neurons. Meanwhile, the HSONN adopts a hierarchical structure with nonlinear basis functions. The <b>neural</b> <b>computer</b> is effectively designed to perform programmable functions for machine vision processing in all levels with its embedded host [...] ...|$|R
40|$|Humans {{possess an}} ability to abstractly reason about objects and their interactions, an ability not shared with {{state-of-the-art}} deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in {{the complexity of the}} reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96. 6 % of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19 / 20 tasks which is competitive with state-of-the-art sparse differentiable <b>neural</b> <b>computers.</b> The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning. Comment: Submitted to ICLR 201...|$|E
40|$|Kohonen {{maps are}} {{self-organizing}} neural networks that classify and quantify n-dimensional data into a one- or two-dimensional array of neurons [1]-[2]. Most applications of Kohonen maps use simulations on conventional computers, even- tually coupled to hardware accelerators or dedicated <b>neural</b> <b>computers.</b> The {{small number of}} different operations involved in the combined learning and classification process makes however the Kohonen model particularly suited to a dedicated VLSI implementation, taking {{full advantage of the}} parallelism and speed that can be obtained on the chip. We propose here a fully analog implementation of a one-dimensional Kohonen map, with on-chip learning and refreshment of on-chip analog synaptic weights. The small number of transistors in each cell allows a high degree of parallelism in the operations, what greatly improves the computation speed compared to other implementations. This paper will emphasize on the storage of analog synaptic weights, based on the principle of current copiers; it will be shown that this technique can be used successfully for the realization of VLSI Kohonen maps...|$|E
40|$|There {{has been}} {{significant}} research {{over the past}} two decades in developing new platforms for spiking neural computation. Current <b>neural</b> <b>computers</b> are primarily developed to mimick biology. They use neural networks which can be trained to perform specific tasks to mainly solve pattern recognition problems. These machines can do more than simulate biology, they allow us to re-think our current paradigm of computation. The ultimate goal is to develop brain inspired general purpose computation architectures that can breach the current bottleneck introduced by the Von Neumann architecture. This work proposes a new framework for such a machine. We show that the use of neuron like units with precise timing representation, synaptic diversity, and temporal delays allows us to set a complete, scalable compact computation framework. The presented framework provides both linear and non linear operations, allowing us to represent and solve any function. We show usability in solving real use cases from simple differential equations to sets of non-linear differential equations leading to chaotic attractors...|$|E
50|$|Differentiable <b>neural</b> <b>computer</b> (DNCs) are an {{extension}} of Neural Turing machines, allowing for fuzzy usage amounts of each memory address, and a record of chronology.|$|R
40|$|This article {{reviews the}} {{limitations}} of the standard computing paradigm and sketches the concept of quantum neural computing. Implications of this idea for the understanding of biological information processing and design of new kinds of computing machines are described. Arguments are presented in support of the thesis that brains are to be viewed as quantum systems with their neural structures representing the classical measurement hardware. From a performance point of view, a quantum <b>neural</b> <b>computer</b> may be viewed as a collection of many conventional computers that are designed to solve different problems. A quantum <b>neural</b> <b>computer</b> is a single machine that reorganizes itself, in response to a stimulus, to perform a useful computation. Selectivity offered by such a reorganization appears to be at the basis of the gestalt style of biological information processing. Clearly, a quantum <b>neural</b> <b>computer</b> is more versatile than the conventional computing machine...|$|R
5000|$|A {{differentiable}} <b>neural</b> <b>computer</b> (DNC) is {{a recurrent}} {{artificial neural network}} architecture with an autoassociative memory. The model was published in 2016 by Alex Graves et. al of DeepMind.|$|R
40|$|The {{dynamics}} of nonlinear systems qualitatively change {{depending on their}} parameters, which is called bifurcation. A quantum-mechanical nonlinear oscillator can yield a quantum superposition of two oscillation states, known as a Schrödinger cat state, via quantum adiabatic evolution through its bifurcation point. Here we propose a quantum computer comprising such quantum nonlinear oscillators, instead of quantum bits, to solve hard combinatorial optimization problems. The nonlinear oscillator network finds optimal solutions via quantum adiabatic evolution, where nonlinear terms are increased slowly, in contrast to conventional adiabatic quantum computation or quantum annealing, where quantum fluctuation terms are decreased slowly. As a result of numerical simulations, it is concluded that quantum superposition and quantum fluctuation work effectively to find optimal solutions. It is also notable that the present computer is analogous to <b>neural</b> <b>computers,</b> which are also networks of nonlinear components. Thus, the present scheme will open new possibilities for quantum computation, nonlinear science, and artificial intelligence. Comment: 8 pages, 3 figure...|$|E
40|$|International audienceThere {{has been}} {{significant}} research {{over the past}} two decades in developing new platforms for spiking neural computation. Current <b>neural</b> <b>computers</b> are primarily developed to mimic biology. They use neural networks, which can be trained to perform specific tasks to mainly solve pattern recognition problems. These machines can do more than simulate biology; they allow us to rethink our current paradigm of computation. The ultimate goal is to develop brain-inspired general purpose computation architectures that can breach the current bottleneck introduced by the von Neumann architecture. This work proposes a new framework for such a machine. We show that the use of neuron-like units with precise timing representation, synaptic diversity, and temporal delays allows us to set a complete, scalable compact computation framework. The framework provides both linear and nonlinear operations, allowing us to represent and solve any function. We show usability in solving real use cases from simple differential equations to sets of nonlinear differential equations leading to chaotic attractors...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe Statistical Mechanical Neural Computer (SMNC) developed {{in this thesis}} utilizes a Statistical Mechanical Nonlinear Algorithm (SMNA) to determine the long-time probability distribution of highly nonlinear stochastic systems. The use of the SMNA and a novel mesoscopic scaling technique help provide the SMNC with the capabilities of <b>neural</b> <b>computers</b> without the drawbacks of huge connection matrices and their attendant computational requirements In this thesis, the SMNC is initially used to verify {{the ability of the}} SMNA to duplicate relatively simple, single variable path integral solutions to nonlinear Folker-Planck equations. After the fundamental algorithms are validated, the SMNC's ability to simulate a two-variable, multicellular problem by modeling a portion of the neocortex consisting of 10 (5) neural units is discussed. There are many important applicatioins of the SMNC and its unique SMNA to C 3 systems including radar, sonar and electronic signals processing, missile guidance systems and an integrated battle management system. Such C 3 systems will benefit from the SMNC's potential to efficiently filter large amounts of data, recognize patterns and anticipate, with some degree of uncertainty, the future state of highly nonlinear stochastic systems. [URL] United States Nav...|$|E
40|$|Title: Applicability of <b>Neural</b> <b>Computer</b> Control in Sport. Objectives: Find out {{applicability}} of <b>neural</b> <b>computer</b> {{control in the}} sport research using Emotiv Epoc Neuroheadset device and Emotiv Control Panel application. Methods: Case study. Results: New research direction in sport using the Emotiv Epoc Neuroheadset device has been the significant contribution of the Thesis. All the research and results that has been gained {{will be used as}} the base for the future research. Results will be applied in more complex research using Emotiv Epoc Neuroheadset in context of different physical activities. Keywords: Emotiv, Cognitiv Suite, virtual cube, physical activity, stres...|$|R
40|$|In this paper, a {{cost-effective}} {{implementation of a}} programmable filterbank front-end for speech recognition is presented. The objective has been to design a real-time bandpass filtering system with a filterbank of 16 filters, with analog audio input and analog output. The output consists of 16 analog signals, which are the envelopes of the filter outputs of the audio signal. These analog signals are then led to an analog <b>neural</b> <b>computer,</b> which performs the feature-based recognition task. One of the main objectives has been to allow the user to easily change the filter specifications without affecting the remaining system, thus a software implementation of the filterbank was preferred. In addition, the <b>neural</b> <b>computer</b> requires analog input. Therefore, we implemented the filterbank on a PC, with the input A/D and the output D/A performed by the PC stereo soundcard. Since multiple analog outputs are necessary for the <b>neural</b> <b>computer</b> (one for each filter), it then follows that the soundcard output should contain the multiplexed 16 filter outputs, while a hardware module is needed for demultiplexing the soundcard output into the final 16 analog signals...|$|R
40|$|A pixel for {{measuring}} two-dimensional (2 -D) visual motion with two one-dimensional (1 -D) detectors has been implemented in very large scale integration. Based on the spatiotemporal feature extraction model of Adelson and Bergen, the pixel is realized using a general-purpose analog <b>neural</b> <b>computer</b> and a silicon retina. Because the <b>neural</b> <b>computer</b> only offers sum-and-threshold neurons, the Adelson and Bergen 2 ̆ 7 s model is modified. The quadratic nonlinearity is {{replaced with a}} full-wave rectification, while the contrast normalization is replaced with edge detection and thresholding. Motion is extracted in two dimensions by using two 1 -D detectors with spatial smoothing orthogonal to the direction of motion. Analysis shows that our pixel, although it has some limitations, has much lower hardware complexity compared to the full 2 -D model. It also produces more accurate results and has a reduced aperture problem compared to the two 1 -D model with no smoothing. Real-time velocity is represented as a distribution of activity of the 18 X and 18 Y velocity-tuned neural filter...|$|R
40|$|The authors’ {{original}} problem-solution-approach concerning {{aviation security}} management in civil aviation apply- ing parallel calculation processes method and {{the usage of}} <b>neural</b> <b>computers</b> is considered in this work. The statement of secure environment modeling problems for grid models and {{with the use of}} neural networks is presented. The research sub- ject area of this article is airport activity in the field of civil aviation, considered in the context of aviation security, defined as the state of aviation security against unlawful interference with the aviation field. The key issue in this subject area is aviation safety provision at an acceptable level. In this case, airport security level management becomes one of the main objectives of aviation security. Aviation security management is organizational-regulation in modern systems that can no longer correspond to changing requirements, increasingly getting complex and determined by external and internal envi- ronment factors, associated with a set of potential threats to airport activity. Optimal control requires the most accurate identification of management parameters and their quantitative assessment. The authors examine the possibility of applica- tion of mathematical methods for the modeling of security management processes and procedures in their latest works. Par- allel computing methods and network neurocomputing for modeling of airport security control processes are examined in this work. It is shown that the methods’ practical application of the methods is possible along with the decision support system, where the decision maker plays the leading role...|$|E
40|$|Research on {{artificial}} {{neural networks}} (ANNs) {{has been carried}} out for more than five decades. A renewed interest appeared in the 80 's with the finding of powerful models like J. Hopfield's recurrent networks, T. Kohonen's self-organizing feature maps, and the back-propagation rule. At that time, there was no platform that was at the same time versatile enough for any ANN model to be implemented and fast enough to solve large problems. Super-computers were the sole exception to this rule, but were prohibitively expensive for most applications. However, both research scientists and application engineers clearly identified the need for such a computing power. This triggered many projects in the field. In parallel, research on multi-processor systems started during the 60 's. Systolic arrays have been proposed in 1979 as a means to fully exploit the possibilities of VLSI. Two previous theses, by F. Blayo and C. Lehmann, have studied the use of bi-dimensional systolic arrays for neural computation. At first, the presented system, called GENES, has been designed for the Hopfield model. Extensions to other ANNs have also been proposed. The goal of the present thesis is to study, design, build, and analyze an efficient accelerator for neural computation. In a first step, the GENES architecture has been extended towards generality and efficiency. This includes a thorough analysis of ANN models, of other <b>neural</b> <b>computers,</b> and of previous GENES implementations. The result of this work is the GENES IV integrated circuit, whose architecture has been co-designed by P. Ienne and the author. The main part of this thesis discusses the architecture, the design and the analysis of the MANTRA I machine, a neural computer based on a GENES IV array with up to 40 × 40 processing elements (PEs). The delta rule (and hence the Perceptron and Adaline rules), the back-propagation rule, the Hopfield model, and the Kohonen model can be implemented on this system. Although not a generic system, such a machine may be regarded as a multi-model neural computer. A prototype has been running for a year and is used daily by software designers. Several novel features distinguish the MANTRA I machine from other neural systems. First, it belongs to the few existing <b>neural</b> <b>computers,</b> contrary to the majority of implementations, which are specific to an application or to an algorithm. The machine does not hard-wire any algorithm, but provides the necessary primitives to implement the target models. This is a key feature for research, since several algorithms or versions of an algorithm can be tested on a problem. It is an important aspect for applications as well, because different ANN models are often cascaded to solve a problem. The GENES IV array — that is, the computing core of the MANTRA I machine — features synapse-level parallelism (i. e., one real or virtual PE is allocated per synapse or neural connection), while most other systems exploit only neuron-level parallelism (i. e., one PE per neuron). Hence, this system aims at a much finer parallelism grain and is well suited for massively parallel architectures. The problem size that can be computed by a neural accelerator should not be limited by the hardware (except for memory size). Therefore, it is essential to support time-sharing of PEs. On the MANTRA I machine, this is achieved by the concept of virtual arrays. Matrices are divided into sub-matrices that can be mapped onto the physical array, which is then time-shared among them. An efficient mechanism has been implemented to swap sub-matrices in background, while some other computation is performed. Since systolic arrays are pipelined systems, it is important to avoid emptying and re-filling them too often, {{in order to keep the}} hardware utilization rate high. Therefore, a systolic instruction flow has been implemented, so that each instruction follows the data for which it has been issued. Like any SIMD system, the MANTRA I machine is composed of a parallel or SIMD module and a control module. The SIMD module consists of the GENES IV array and a set of dedicated units designed for the computation that scales with the number of neurons and would poorly fit on a bi-dimensional array. A complex system of FIFOs and memories sustains the required input/output streams for the systolic array. The control module is a complete SISD system. Its tasks are (1) to control the SIMD module by dispatching instructions, (2) to manage data input and output, (3) to communicate with the external world, and (4) to perform data pre- and post-processing. The SIMD instructions are of the very long instruction word (VLIW) type. Synchronization between the two modules is achieved by an instruction FIFO. The performance of the MANTRA I machine has been analyzed using the delta rule. Measurements show that the sustained performance is very close to its peak value, as long as the problem fits in the memory banks connected to the GENES IV array. Experiments have also been run to investigate the impact of the constraints imposed by the hardware on the convergence of algorithms. Finally, the use of systolic arrays as neural accelerators is discussed in the light of the experience acquired with the GENES IV array and the MANTRA I machine. The weaknesses of the machine are analyzed, and several solutions are proposed to avoid them in a future design. A general discussion of the future of <b>neural</b> <b>computers</b> concludes this thesis...|$|E
40|$|Optical {{techniques}} for implementing <b>neural</b> <b>computers</b> are presented. In particular, holographic associative memories with feedback are investigated. Characteristics of optical neurons and optical interconnections are discussed. An LCLV {{is used for}} simulating a 2 -D array of approximately 160, 000 optical neurons. Thermoplastic plates are used for providing holographic interconnections among these neurons. The problem of degenerate readout in holographic interconnections and the method of sampling grids {{to solve this problem}} are presented. Two optical neural networks for associative memories are implemented and demonstrated. The first one is an optical implementation of the Hopfield network. It performs the function of auto-association that recognizes 2 -D images from a distorted or partially blocked input. The trade-off between distortion tolerance and discrimination capability against new images is discussed. The second optical loop is a 2 -layer network with feedback. It performs the function of hetero-association, which locks the recognized input and its associated image as a stable state in the loop. In both optical loops, it is shown that the neural gain and the similarity between the input and the stored images are the main factors that determine the dynamics of the network. Neural network models for the optical loops are presented. Equations of motion for describing the dynamical behavior of the systems are derived. The reciprocal vector basis corresponding to stored images is derived. A geometrical method is then introduced which allows us to inspect the convergence property of the system. It is also shown that the main factors that determine the system dynamics are the neural gain and the initial conditions. Photorefractive holography for optical interconnections and sampling grids for volume holographic interconnections are presented. A periodic copying method for refreshening multiply exposured photorefractive holograms is presented, which allows the hologram to maintain the same diffraction efficiency as that when a single exposure scheme is used. This scheme provides us with the possibility of achieving maximum storage and maximum diffraction efficiency in holographic associative memories...|$|E
40|$|An {{outstanding}} {{problem in}} quantum computing is {{the calculation of}} entanglement, for which no closed-form algorithm exists. Here we solve that problem, and demonstrate the utility of a quantum <b>neural</b> <b>computer,</b> by showing, in simulation, that such a device can be trained to calculate the entanglement of an input state, something neither an algorithmic quantum computer nor a classical neural net can do. Comment: 8 pages; 2 figures; 3 tables. Submitted to Phys. Rev. Let...|$|R
5000|$|Gülay Öke and Georgios Loukas. A {{denial of}} service {{detector}} based on maximum likelihood detection and the random <b>neural</b> network. <b>Computer</b> Journal, 50(6):717-727, November 2007.|$|R
5000|$|E. Gelenbe, F. Batty, Minimum cost graph {{covering}} {{with the}} random <b>neural</b> network, <b>Computer</b> Science and Operations Research, O. Balci (ed.), New York, Pergamon, pp. 139-147, 1992.|$|R
40|$|The {{development}} of theoretical models that characterize various physical phenomena is extremely crucial in all engineering disciplines. In nondestructive evaluation (NDE), theoretical models are used extensively {{to understand the}} physics of material/energy interaction, optimize experimental design parameters and solve the inverse problem of defect characterization. This dissertation describes methods for developing computational models for electromagnetic NDE applications. Two broad classes of issues that are addressed in this dissertation are related to (i) problem formulation and (ii) implementation of computers;The two main approaches for solving physical problems in NDE are the differential and integral equations. The relative {{advantages and disadvantages of}} the two approaches are illustrated and models are developed to simulate electromagnetic scattering from objects or inhomogeneities embedded in multilayered media which is applicable in many NDE problems. The low storage advantage of the differential approach and the finite solution domain feature of the integral approach are exploited. Hybrid techniques and other efficient modeling techniques are presented to minimize the storage requirements for both approaches;The second issue of computational models is the computational resources required for implementation. Implementations on conventional sequential computers, parallel architecture machines and more recent <b>neural</b> <b>computers</b> are presented. An example which requires the use of massive parallel computing is given where a probability of detection model is built for eddy current testing of 3 D objects. The POD model based on the finite element formulation is implemented on an NCUBE parallel computer. The linear system of equations is solved using direct and iterative methods. The implementations are designed to minimize the interprocessor communication and optimize the number of simultaneous model runs to obtain a maximum effective speedup;Another form of parallel computing is the more recent neurocomputer which depends on building an artificial neural network composed of numerous simple neurons. Two classes of neural networks have been used to solve electromagnetic NDE inverse problems. The first approach depends on a direct solution of the governing integral equation and is done using a Hopfield type neural network. Design of the network structure and parameters is presented. The second approach depends on developing a mathematical transform between the input and output space of the problem. A multilayered perceptron type neural network is invoked for this implementation. The network is augmented to build an incremental learning network which is motivated by the dynamic and modular features of the human brain...|$|E
40|$|This paper {{presents}} MINDWALKER, {{which is}} an ambitious EC funded research project coordinated by Space Applications Services aiming at the development of novel Brain <b>Neural</b> <b>Computer</b> Interfaces (BNCI) and robotics technologies, {{with the goal of}} obtaining a crutch-less assistive lower limbs exoskeleton, with non-invasive brain control approach as main strategy. Complementary BNCI control approaches such as arms electromyograms (EMG) are also researched. In the last phase of the project, the developed system should undergo a clinical evaluation with Spinal Cord Injured (SCI) subjects at the Fondazione Santa Lucia, Italy. © 2012 IEEE. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|R
50|$|Scott doesn't {{know where}} to turn next, when out of the blue, one of Peter's former {{students}} shows up. She tells Scott about a book that holds the secret to {{what is known as}} Project Halford. Scott finds the book and a videotape that reveals the Army's plan to construct a <b>neural</b> <b>computer</b> network that would communicate directly with the brain. It also reveals that the project spun out of control and that Peter stole vital codes in an attempt to halt the computer's drive to take over the minds of the townspeople.|$|R
50|$|The {{investigation}} of Metalearning as a neuroscientific concept has potential benefits {{to both the}} understanding and treatment of Psychiatric Disease, as well as bridging the gaps between <b>Neural</b> Networks, <b>Computer</b> Science and Machine Learning.|$|R
30|$|The {{theory of}} {{nonlinear}} difference equations {{has been widely}} used to study discrete models in many fields, such as statistics, <b>neural</b> network, <b>computer</b> science, electrical circuit analysis, optimal control, biological models, data classification, and so on.|$|R
50|$|The {{research}} group includes: Artificial Intelligence & <b>Neural</b> Networks, <b>Computer</b> Graphics & Image Processing, Computer Networks, Cryptography & Network Security, Database Systems & Software Engineering, Electronics & Communications, Mathematics & Statistics, Accounting, Finance, Management and Marketing.|$|R
