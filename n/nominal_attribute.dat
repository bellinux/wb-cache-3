18|122|Public
5000|$|... the {{relationship}} of the <b>nominal</b> <b>attribute</b> or predicate to the noun.|$|E
50|$|Milewski's {{typology}} can {{be employed}} to analyze languages with case marking but {{can also be used}} with those that use a fixed word order or a specific form of incorporation. For simplicity, the table below classifies casual languages in which the <b>nominal</b> <b>attribute</b> is marked with the genitive case.|$|E
5000|$|Languages of the 6th class use the genitive {{not only}} for the <b>nominal</b> <b>attribute</b> but also for the {{experiencer}} and the patient (the [...] "a" [...] marker"). The other case, the ergative, is used for the agent (the [...] "b" [...] marker). This group is not too numerous: Coast Tsimshian, Tunica and Guarani belong here.|$|E
50|$|There are {{basically}} {{two different types}} of DT algorithms: one for inducing decision trees with only <b>nominal</b> <b>attributes</b> and another for inducing decision trees with both numeric and <b>nominal</b> <b>attributes.</b> This aspect of decision tree induction also carries to gene expression programming and there are two GEP algorithms for decision tree induction: the evolvable decision trees (EDT) algorithm for dealing exclusively with <b>nominal</b> <b>attributes</b> and the EDT-RNC (EDT with random numerical constants) for handling both <b>nominal</b> and numeric <b>attributes.</b>|$|R
3000|$|The program {{output is}} {{directly}} used as <b>attributes.</b> Six <b>nominal</b> <b>attributes</b> {{with the emotional}} category names as possible values indicate which mood is the most, second [...]...|$|R
40|$|Most {{constructive}} induction researchers {{focus only}} on new boolean attributes This paper reports a new constructive induction algorithm, called XofN, that constructs new <b>nominal</b> <b>attributes</b> {{in the form of}} X-of-N representations An X-of-N is a Bet containing one or more attribute-value pairs For a given instance, its value corresponds to the number of its at tribute-value pairs that are true The promising preliminary experimental results, on both artificial and real-world domains, show that constructing new <b>nominal</b> <b>attributes</b> in the form of X-of-N representations can significantly improve the performance of selective induction in terms of both higher prediction accuracy and lower theory complexity...|$|R
5000|$|Languages of the 5th class use the genitive {{not only}} for the <b>nominal</b> <b>attribute</b> but also for the agent and the {{experiencer}} (the [...] "a" [...] marker). The other case, called the accusative, marks only the patient (the [...] "b" [...] marker). The only language of this class mentioned by Milewski is Nisga'a, a Tsimshianic language.|$|E
5000|$|Languages of the 4th class {{could be}} {{considered}} ergative-absolutive languages insofar as they make no distinction between the experiencer and the patient, marking both with the absolutive (the [...] "a" [...] marker). Yet languages of this class are contrary to typical ergative-absolutive languages insofar as they mark both agent and <b>nominal</b> <b>attribute</b> as genitive (ergative-genitive, the [...] "b" [...] marker). Examples of Class 4 languages are the Inuktitut, Salishan languages, and Mayan languages.|$|E
5000|$|In this system, every node in the head, {{irrespective}} of its type (numeric attribute, <b>nominal</b> <b>attribute,</b> or terminal), has {{associated with it}} a random numerical constant, which for simplicity in the example above is represented by a numeral 0-9. These random numerical constants are encoded in the Dc domain and their expression follows a very simple scheme: {{from top to bottom}} and from left to right, the elements in Dc are assigned one-by-one to the elements in the decision tree. So, for the following array of RNCs: ...|$|E
40|$|Kupper and Hafner (1988) {{recently}} developed methods {{for assessing the}} extent of interrater agreement when each observational unit is characterized {{by one or more}} <b>nominal</b> <b>attributes.</b> They proposed a new two-rater concordance statistic, and provided associated inferencemaking tools. Except for the special case when each rater chooses exactly one attribute to describe each unit, their methodology does not permit a rater to claim that a unit has none of the attributes under consideration. This paper closes that methodologic gap. A numerical example is also provided. Key Words: Chance-corrected agreement statistics; Non-central hypergeometric distribution; Kappa statistic; Conditional distribution theory; <b>Nominal</b> <b>attributes...</b>|$|R
40|$|Web service {{discovery}} aims {{at finding}} available services that match a given service description. This involves mainly the matchmaking of the functional {{parameters of the}} services, whereas non-functional attributes {{can also be considered}} and aggregated in the matching score of a candidate service as additional criteria for ranking the results. In this paper, we address the problem of re-ranking discovered services that include <b>nominal</b> <b>attributes</b> in their descriptions in order to satisfy users with diverse preferences. We present an approach to diversify the search results combining the degree of match on functional parameters with a method to achieve good coverage with respect to the values of <b>nominal</b> <b>attributes.</b> An evaluation on a publicly available dataset of Semantic Web services is also presented. 1...|$|R
30|$|The header {{with the}} {{information}} about the name we give to the relation (@relation) and the definition of the attributes that are used and their types (@attribute). <b>Nominal</b> <b>attributes</b> are followed by the set of values they can take, while numeric values are followed by the keyword numeric.|$|R
40|$|Recent work on {{discretization}} of continuous-valued attributes {{in learning}} decision trees has produced some positive results. This paper adopts {{the idea of}} discretization of continuous-valued attributes and applies it to instance-based learning (Aha, 1990; Aha, Kibler & Albert, 1991). Our experiments have shown that instance-based learning (IBL) usually performs well in continuous-valued attribute domains and poorly in <b>nominal</b> <b>attribute</b> domains. Cost and Salzberg (1993) have devised the modified value-difference metric (MVDM) that raises the performance of IBL in <b>nominal</b> <b>attribute</b> domains. This paper explores {{a way in which}} continuous-valued attributes and nominal attributes can be treated cohesively in IBL. An algorithm which combines the discretization of continuous-valued attributes and IB 1 (Aha, Kibler & Albert, 1991) using the modified value-difference metric is introduced. The empirical results show that the proposed algorithm, IB 1 -MVDM* achieves a substantial improvement over C 4 [...] . ...|$|E
40|$|In {{this paper}} methods for {{performing}} classification using Genetic Programming (GP) on datasets with nominal attributes are developed and evaluated. The two methods developed included the splitting of GP program execution {{based upon the}} value of a <b>nominal</b> <b>attribute</b> (execution branching), and the conversion of a <b>nominal</b> <b>attribute</b> to a continuous or binary attribute (numeric conversion). These two methods of using nominal attributes are tested against six datasets containing either nominal and continuous attributes or nominal only attributes. Results show that the use of the methods developed in this paper allow classifiers trained with GP to perform accurate classification of datasets containing nominal attributes. When compared to other well-known methods of classification the GP method is capable of classifying one of six datasets more accurately than any of the conventional methods tested, and accuracy close to the best achieved method on 3 other datasets. 1...|$|E
40|$|Abstract. Memory-Based Reasoning and K-Nearest Neighbor Searching are {{frequently}} adopted data mining techniques. But, they suffer from scalability. Indexing is a promising solution. However, {{it is difficult}} to index categorical attributes, since there does not exist linear ordering property among categories in a <b>nominal</b> <b>attribute.</b> In this paper, we proposed heuristic algorithms to map categories to numbers. Distance relationships among categories are preserved as many as possible. We empirically studied the performance of the algorithms under different distance situations. ...|$|E
40|$|Abstract—This paper {{presents}} a new clustering technique named as the Olary algorithm, which is suitable to cluster nominal data sets. This algorithm uses a new code {{with the name}} of the Olary code to transform <b>nominal</b> <b>attributes</b> into integer ones through a process named as the Olary transformation. The number of integer attributes we get through the Olary transformation is usually {{different from that of the}} original <b>nominal</b> <b>attributes.</b> Meanwhile, an extension of the Olary algorithm, which we call the ex-Olary algorithm, is introduced. Furthermore, we provide a useful way to estimate the number of underlying clusters by the use of a new kind of diagram, which is called Number of Clusters versus Distance Diagram (NCDD for short). Index Terms-Clustering, nominal, the Olary code, the Olary transformation, the ex-Olary algorithm, NCD...|$|R
40|$|We {{present a}} greedy {{algorithm}} for supervised discretization using a metric defined {{on the space}} of partitions {{of a set of}} objects. This proposed technique is useful for preparing the data for classifiers that require <b>nominal</b> <b>attributes.</b> Experimental work on decision trees and naive Bayes classifiers confirm the efficacy of the proposed algorithm...|$|R
40|$|Abstract Subgroup {{discovery}} is a Knowledge Discovery task that aims at finding subgroups {{of a population}} with high generality and distributional unusualness. While several subgroup discovery algorithms have been presented in the past, they focus on databases with <b>nominal</b> <b>attributes</b> or make use of discretization {{to get rid of}} the numerical attributes. In this paper, we illustrate why the replacement of numerical <b>attributes</b> by <b>nominal</b> <b>attributes</b> can result in suboptimal results. Thereafter, we present a new subgroup discovery algorithm that prunes large parts of the search space by exploiting bounds between related numerical subgroup descriptions. The same algorithm can also be applied to ordinal attributes. In an experimental section, we show that the use of our new pruning scheme results in a huge performance gain when more that just a few split-points are considered for the numerical attributes...|$|R
40|$|Instance-based {{learning}} techniques typically handle {{continuous and}} linear input values well, but {{often do not}} handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between <b>nominal</b> <b>attribute</b> values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes. 1. Introduction Instance-Based Learning (IBL) (Aha, [...] ...|$|E
40|$|We explore in {{this paper}} a novel {{clustering}} algorithm, named CORE (standing for CORrelated-Force Ensemble), for categorical data. In general, {{it is more difficult}} to perform clustering on categorical data than on numerical data due to the absence of the ordered property in the former. Though several clustering algorithms which concentrate on categorical date were proposed, acquiring the desirable quality remains a challenging issue. Note that there is significance hidden in the correlation between attribute values that can be explored to aid clustering, especially extracting clusters in the high dimensional data. Therefore by employing the concept of correlated-force ensemble, clusters which consist of the highly correlated set of <b>nominal</b> <b>attribute</b> values, can be acquired by the proposed algorithm, CORE. As validated by variant real datasets, it is shown in our experimental results that algorithm CORE significantly outperforms the prior works. ...|$|E
40|$|Abstract. In most databases, it is {{possible}} to identify small partitions of the data where the observed distribution is notably {{different from that of the}} database as a whole. In classical subgroup discovery, one considers the distribution of a single <b>nominal</b> <b>attribute,</b> and exceptional subgroups show a surprising increase in the occurrence of one of its values. In this paper, we introduce Exceptional Model Mining (EMM), a framework that allows for more complicated target concepts. Rather than finding subgroups based on the distribution of a single target attribute, EMM finds subgroups where a model fitted to that subgroup is somehow exceptional. We discuss regression as well as classification models, and define quality measures that determine how exceptional a given model on a subgroup is. Our framework is general enough to be applied to many types of models, even from other paradigms such as association analysis and graphical modeling. ...|$|E
40|$|The {{importance}} of skyline analysis {{has been well}} recognized in multicriteria decision-making applications. All of the previous studies assume a fixed order on the attributes in question. However, in some applications, users {{may be interested in}} skylines with respect to various total or partial orders on <b>nominal</b> <b>attributes.</b> In this paper, we identify and tackle the problem of online skyline analysis with dynamic preferences on <b>nominal</b> <b>attributes.</b> We investigate how changes of orders in attributes lead to changes of skylines. We address two novel types of interesting queries: a viewpoint query returns with respect to which orders a point is (or is not) in the skylines, and a refined skyline query retrieves the skyline with respect to a specific order. We develop two methods systematically and report an extensive performance study using both synthetic and real data sets to verify the effectiveness and the efficiency of our methods...|$|R
40|$|Abstract—The {{importance}} of skyline analysis {{has been well}} recognized in multicriteria decision-making applications. All of the previous studies assume a fixed order on the attributes in question. However, in some applications, users {{may be interested in}} skylines with respect to various total or partial orders on <b>nominal</b> <b>attributes.</b> In this paper, we identify and tackle the problem of online skyline analysis with dynamic preferences on <b>nominal</b> <b>attributes.</b> We investigate how changes of orders in attributes lead to changes of skylines. We address two novel types of interesting queries: a viewpoint query returns with respect to which orders a point is (or is not) in the skylines, and a refined skyline query retrieves the skyline with respect to a specific order. We develop two methods systematically and report an extensive performance study using both synthetic and real data sets to verify the effectiveness and the efficiency of our methods. Index Terms—Skyline, materialization, data warehouses, preferences. ...|$|R
40|$|In a {{previous}} work we {{have proposed a}} hybrid Particle Swarm Optimisation/Ant Colony Optimisation (PSO/ACO) algorithm for the discovery of classification rules, {{in the context of}} data mining. Unlike a conventional PSO algorithm, this hybrid algorithm can directly cope with <b>nominal</b> <b>attributes,</b> without converting <b>nominal</b> values into numbers in a pre-processing phase. The design of this hybrid algorithm was motivated by the fact that <b>nominal</b> <b>attributes</b> are common in data mining, but the algorithm can in principle be applied to other kinds of problems involving nominal variables (though this paper focuses only on data mining). In this paper we propose several modifications to the original PSO/ACO algorithm. We evaluate the new version of the PSO/ACO algorithm (PSO/ACO 2) in 16 public-domain real-world datasets often used to benchmark the performance of classification algorithms. PSO/ACO 2 is evaluated with two different rule quality (particle "fitness") functions. We show that the choice of rule quality measure greatly effects the end performance of PSO/ACO 2. In addition, the results show that PSO/ACO 2 is very competitive with respect to two well-known rule induction algorithms...|$|R
40|$|While many {{constructive}} induction algorithms {{focus on}} generating new binary attributes, this paper explores novel methods of constructing nominal and numeric attributes. We propose a new constructive operator, X-of-N. An X-of-N representation {{is a set}} containing one or more attribute-value pairs. For a given instance, {{the value of an}} X-of-N representation corresponds to the number of its attribute-value pairs that are true of the instance. A single X-of-N representation can directly and simply represent any concept that can be represented by a single conjunctive, a single disjunctive, or a single M-of-N representation commonly used for constructive induction, and the reverse is not true. In this paper, we describe a constructive decision tree learning algorithm, called XofN. When building decision trees, this algorithm creates one X-of-N representation, either as a <b>nominal</b> <b>attribute</b> or as a numeric attribute, at each decision node. The construction of X-of-N representations is carrie [...] ...|$|E
40|$|To {{find the}} optimal {{branching}} of a <b>nominal</b> <b>attribute</b> at a node in an L-ary decision tree, one is often forced to search over all possible L-ary partitions {{for the one}} that yields the minimum impurity measure. For binary trees (L = 2) when there are just two classes a short-cut search is possible that is linear in n,the number of distinct values of the attribute. For the general {{case in which the}} number of classes, k,may be greater than two, Burshtein et al. have shown that the optimal partition satisfies a condition that involves the existence of i L 2 j hyperplanes in the class probability space. We derive a property of the optimal partition for concave impurity measures (including in particular the Gini and entropy impurity measures) in terms of the existence of L vectors in the dual of the class probability space, which implies the earlier condition. Unfortunately, these insights still do not offer a practical search method when n and k are large, even for binary trees. We th [...] ...|$|E
40|$|AbstractLots of {{clustering}} algorithms {{have been}} developed, {{while most of}} them cannot process objects in hybrid numerical/nominal attribute space or with missing values. In most of them, the number of clusters should be manually determined and the clustering results {{are sensitive to the}} input order of the objects to be clustered. These limit applicability of the clustering and reduce the quality of clustering. To solve this problem, an improved clustering algorithm based on rough set (RS) and entropy theory was presented. It aims at avoiding the need to prespecify the number of clusters, and clustering in both numerical and <b>nominal</b> <b>attribute</b> space with the similarity introduced to replace the distance index. At the same time, the RS theory endows the algorithm with the function to deal with vagueness and uncertainty in data analysis. Shannon's entropy was used to refine the clustering results by assigning relative weights to the set of attributes according to the mutual entropy values. A novel measure of clustering quality was also presented to evaluate the clusters. This algorithm was analyzed and applied later to cluster the data set of one industrial product. The experimental results confirm that performances of efficiency and clustering quality of this algorithm are improved...|$|E
40|$|Current skyline {{evaluation}} techniques {{assume a}} fixed ordering on the attributes. However, dynamic preferences on <b>nominal</b> <b>attributes</b> are more realistic in known applications. In order to generate online response for any such preference issued by a user, we propose two methods of different characteristics. The {{first one is}} a semi-materialization method {{and the second is}} an adaptive SFS method. Finally, we conduct experiments to show the efficiency of our proposed algorithms. Comment: 10 page...|$|R
40|$|Abstract — Most {{real-world}} classification problems involve continuous (real-valued) attributes, as well as, <b>nominal</b> (discrete) <b>attributes.</b> The {{majority of}} Ant Colony Optimisation (ACO) classification algorithms have {{the limitation of}} only being {{able to cope with}} <b>nominal</b> <b>attributes</b> directly. Extending the approach for coping with continuous attributes presented by cAnt-Miner (Ant-Miner coping with continuous attributes), in this paper we propose two new methods for handling continuous attributes in ACO classification algorithms. The first method allows a more flexible representation of continuous attributes’ intervals. The second method explores the problem of attribute interaction, which originates from the way that continuous attributes are handled in cAnt-Miner, in order to implement an improved pheromone updating method. Empirical evaluation on eight publicly available data sets shows that the proposed methods facilitate the discovery of more accurate classification models...|$|R
40|$|New {{methods are}} {{developed}} {{for assessing the}} extent of interrater agreement when each unit to be rated is characterized {{by one or more}} <b>nominal</b> <b>attributes.</b> For such multiple attribute response data, a two-rater concordance statistic is derived, and associated statistical inferencemaking procedures are provided. This concordance statistic is corrected for chance agreement based on an underlying hypergeometric model. Numerical examples are given to illustrate the proposed methodology, and extensions to situations involving more than two raters are briefly considered. 1...|$|R
40|$|We have {{previously}} proposed a hybrid particle swarm optimisation/ant colony optimisation (PSO/ACO) algorithm for {{the discovery of}} classification rules. Unlike a conventional PSO algorithm, this hybrid algorithm can directly cope with nominal attributes, without converting nominal values into binary numbers in a preprocessing phase. PSO/ACO 2 also directly deals with both continuous and <b>nominal</b> <b>attribute</b> values, a feature that current PSO and ACO rule induction algorithms lack. We evaluate the {{new version of the}} PSO/ACO algorithm (PSO/ACO 2) in 27 public-domain, real-world data sets often used to benchmark the performance of classification algorithms. We compare the PSO/ACO 2 algorithm to an industry standard algorithm PART and compare a reduced version of our PSO/ACO 2 algorithm, coping only with continuous data, to our new classification algorithm for continuous data based on differential evolution. The results show that PSO/ACO 2 is very competitive in terms of accuracy to PART and that PSO/ACO 2 produces significantly simpler (smaller) rule sets, a desirable result in data mining—where the goal is to discover knowledge that is not only accurate but also comprehensible to the user. The results also show that the reduced PSO version for continuous attributes provides a slight increase in accuracy when compared to the differential evolution variant...|$|E
40|$|An {{important}} subproblem in supervised {{tasks such}} as decision tree induction and subgroup discovery is finding an interesting binary feature (such as a node split or a subgroup refinement) based on a numeric or <b>nominal</b> <b>attribute,</b> with respect to some discrete or continuous target variable. Often one is faced with a trade-off between the expressiveness of such features {{on the one hand}} and the ability to efficiently traverse the feature search space on the other hand. In this article, we present efficient algorithms to mine binary features that optimize a given convex quality measure. For numeric attributes, we propose an algorithm that finds an optimal interval, whereas for nominal attributes, we give an algorithm that finds an optimal value set. By restricting the search to features that lie on a convex hull in a coverage space, we can significantly reduce computation time. We present some general theoretical results on the cardinality of convex hulls in coverage spaces of arbitrary dimensions and perform a complexity analysis of our algorithms. In the important case of a binary target, we show that these algorithms have linear runtime in the number of examples. We further provide algorithms for additive quality measures, which have linear runtime regardless of the target type. Additive measures are particularly relevant to feature discovery in subgroup discovery. Our algorithms are shown to perform well through experimentation and furthermore provide additional expressive power leading to higher-quality results. status: publishe...|$|E
40|$|This work {{describes}} a methodology to combine logic-based systems and connectionist systems. Our approach uses finite truth valued Łukasiewicz logic, where we {{take advantage of}} fact what {{in this type of}} logics every connective can be define by a neuron in an artificial network having by activation function the identity truncated to zero and one. This allowed the injection of first-order formulas in a network architecture, and also simplified symbolic rule extraction. Our method trains a neural network using Levenderg-Marquardt algorithm, where we restrict the knowledge dissemination in the network structure. We show how this reduces neural networks plasticity without damage drastically the learning performance. Making the descriptive power of produced neural networks similar to the descriptive power of Łukasiewicz logic language, simplifying the translation between symbolic and connectionist structures. This method is used in the reverse engineering problem of finding the formula used on generation of a truth table for a multi-valued Łukasiewicz logic. For real data sets the method is particularly useful for attribute selection, on binary classification problems defined using <b>nominal</b> <b>attribute.</b> After attribute selection and possible data set completion in the resulting connectionist model: neurons are directly representable using a disjunctive or conjunctive formulas, in the Łukasiewicz logic, or neurons are interpretations which can be approximated by symbolic rules. This fact is exemplified, extracting symbolic knowledge from connectionist models generated for the data set Mushroom from UCI Machine Learning Repository. Comment: 24 page...|$|E
40|$|Five {{classification}} algorithms namely J 48, Naive Bayes, K Nearest Neighbour, IBK and Decision Tree {{are evaluated}} using Mc Nemar’s test over datasets including both <b>nominal</b> <b>attributes</b> and numeric attributes. It {{was found that}} K Nearest Neighbor performed {{better than the other}} classification methods for both nominal datasets and numerical datasets. It was also observed that the results of this evaluation confers with two other evaluation metrics used for evaluating classification algorithms or machine learning algorithms, Root Mean Squared Error and Kappa statistic...|$|R
40|$|The {{importance}} of dominance and skyline analysis {{has been well}} recognized in multi-criteria decision making applications. Most previous studies assume a fixed order on the attributes. In practice, different customers may have different preferences on <b>nominal</b> <b>attributes.</b> In this paper, we identify an interesting data mining problem, finding favorable facets, {{which has not been}} studied before. Given a set of points in a multidimensional space, for a specific target point p we want to discover with respect to which combinations of orders (e. g., customer preferences) on the <b>nominal</b> <b>attributes</b> p is not dominated by any other points. Such combinations are called the favorable facets of p. We consider both the effectiveness and the efficiency of the mining. A given point may have many favorable facets. We propose the notion of minimal disqualifying condition (MDC) which is effective in summarizing favorable facets. We develop efficient algorithms for favorable facet mining for different application scenarios. The first method computes favorable facets on the fly. The second method pre-computes all minimal disqualifying conditions so that the favorable facets can be looked up in constant time. An extensive performance study using both synthetic and real data sets is reported to verify their effectiveness and efficiency...|$|R
40|$|Many distance-related algorithms, such as k-nearest {{neighbor}} learning algorithms, locally weighted learning algorithms etc, {{depend upon}} a good distance metric to be successful. In {{this kind of}} algorithms, a key problem is how to measure the distance between each pair of instances. In this paper, we provide a survey on distance metrics for <b>nominal</b> <b>attributes,</b> including some basic distance metrics and their improvements based on attribute weighting and attribute selection. The experimental results on the whole 36 UCI datasets published on the main web site of Weka platform validate their effectiveness. </span...|$|R
