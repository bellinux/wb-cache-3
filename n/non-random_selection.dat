206|50|Public
5|$|Progressive {{alignments}} are {{not guaranteed}} to be globally optimal. The primary problem is that when errors are made at any stage in growing the MSA, these errors are then propagated through to the final result. Performance is also particularly bad {{when all of the}} sequences in the set are rather distantly related. Most modern progressive methods modify their scoring function with a secondary weighting function that assigns scaling factors to individual members of the query set in a nonlinear fashion based on their phylogenetic distance from their nearest neighbors. This corrects for <b>non-random</b> <b>selection</b> of the sequences given to the alignment program.|$|E
25|$|Randomness {{can be seen}} as {{conflicting}} {{with the}} deterministic ideas of some religions, such as those where the universe is created by an omniscient deity who is aware of all past and future events. If the universe is regarded to have a purpose, then randomness {{can be seen as}} impossible. This is one of the rationales for religious opposition to evolution, which states that <b>non-random</b> <b>selection</b> is applied to the results of random genetic variation.|$|E
500|$|This {{objection}} {{is fundamentally}} an argument {{by lack of}} imagination, or argument from incredulity: a certain explanation is seen as being counterintuitive, and therefore an alternate, more intuitive explanation is appealed to instead. Supporters of evolution generally respond by arguing that evolution {{is not based on}} [...] "chance," [...] but on predictable chemical interactions: natural processes, rather than supernatural beings, are the [...] "designer." [...] Although the process involves some random elements, it is the <b>non-random</b> <b>selection</b> of survival-enhancing genes that drives evolution along an ordered trajectory. The fact that the results are ordered and seem [...] "designed" [...] is no more evidence for a supernatural intelligence than the appearance of complex natural phenomena (e.g. snowflakes). It is also argued that there is insufficient evidence to make statements about the plausibility or implausibility of abiogenesis, that certain structures demonstrate poor design, and that the implausibility of life evolving exactly as it did is no more evidence for an intelligence than the implausibility of a deck of cards being shuffled and dealt in a certain random order.|$|E
40|$|<b>Non-random</b> sample <b>selection</b> is {{a commonplace}} amongst many {{empirical}} studies {{and it appears}} when an output variable of interest is available only for a restricted non-random sub-sample of data. We introduce {{an extension of the}} generalized additive model which accounts for <b>non-random</b> sample <b>selection</b> by using a selection equation. The proposed approach allows for different distributions of the outcome variable, various dependence structures between the (outcome and selection) equations through the use of copulae, and nonparametric effects on the responses. Parameter estimation is carried out within a penalized likelihood and simultaneous equation framework. We establish asymptotic theory for the proposed penalized spline estimators, which extends the recent theoretical results for penalized splines in generalized additive models, such as those by Kauermann et al. (2009) and Yoshida & Naito (2014). The empirical effectiveness of the approach is demonstrated through a simulation study...|$|R
40|$|Sample {{selection}} {{models are}} employed when {{an outcome of}} interest is observed for a restricted non-randomly selected sample of the population. We consider the {{case in which the}} response is binary and continuous covariates have a nonlinear relationship to the outcome. We introduce two statistical methods for the estimation of two binary regression models involving semiparametric predictors in the presence of <b>non-random</b> sample <b>selection.</b> This is achieved using a multiple-stage procedure, and a newly developed simultaneous equation estimation scheme. Both approaches are based on the penalized likelihood estimation framework. The problems of identification and inference are also discussed. The empirical properties of the proposed approaches are studied through a simulation study. The methods are then illustrated using data from the American National Election Study where the aim is to quantify public support for school integration. If <b>non-random</b> sample <b>selection</b> is neglected then the predicted probability of giving, for instance, a supportive response may be biased, an issue that can be tackled using the proposed tools...|$|R
50|$|Evolutionary {{synthesis}} and {{the theory}} of evolution state that random mutation leads to inherited traits that become more or less common due to <b>non-random</b> natural <b>selection</b> and random genetic drift, as well as other mechanisms. Therefore, the PSSI statement is overly vague and worded in a misleading fashion, since few real evolutionary biologists would subscribe to the version of evolution presented by the statement. Evolution does not include the study of the origin of life, as the statement implies.|$|R
2500|$|RNA viruses such as {{the human}} {{immunodeficiency}} virus (HIV) evolve at an extremely rapid rate, orders of magnitude faster than mammals or birds. For these organisms, ancestral reconstruction can be applied on a much shorter time scale; for example, in order to reconstruct the global or regional progenitor of an epidemic that has spanned decades rather than millions of years. [...] A team around Brian Gaschen proposed that such reconstructed strains be used as targets for vaccine design efforts, as opposed to sequences isolated from patients in the present day. Because HIV is extremely diverse, a vaccine designed to work on one patient's viral population might not work for a different patient, because the evolutionary distance between these two viruses may be large. However, their most recent common ancestor is closer {{to each of the}} two viruses than they are to each other. Thus, a vaccine designed for a common ancestor could {{have a better chance of}} being effective for a larger proportion of circulating strains. Another team took this idea further by developing a center-of-tree reconstruction method to produce a sequence whose total evolutionary distance to contemporary strains is as small as possible. Strictly speaking, this method was not ancestral reconstruction, as the center-of-tree (COT) sequence does not necessarily represent a sequence that has ever existed in the evolutionary history of the virus. However, Rolland and colleagues did find that, in the case of HIV, the COT virus was functional when synthesized. Similar experiments with synthetic ancestral sequences obtained by maximum likelihood reconstruction have likewise shown that these ancestors are both functional and immunogenic, lending some credibility to these methods. [...] Furthermore, ancestral reconstruction can potentially be used to infer the genetic sequence of the transmitted HIV variants that have gone on to establish the next infection, with the objective of identifying distinguishing characteristics of these variants (as a <b>non-random</b> <b>selection</b> of the transmitted population of viruses) that may be targeted for vaccine design.|$|E
5000|$|The imagine, perhaps {{and might}} (italicized by Coulter in the book) refer {{to what she}} {{believes}} is the speculative, mythical, 'made-up-story' nature of the modern evolutionary synthesis theory that species evolved through mutation and <b>non-random</b> <b>selection.</b> Chapter 9, entitled Proof for How the Walkman Evolved into the iPOD by Random Mutation, begins ...|$|E
50|$|The {{best defense}} against {{weaknesses}} {{is to begin}} with a set of initial informants that are as diverse as possible.Efforts to improve the main disadvantage of snowball sampling resulted in the Respondent Driven Sampling (RDS) method. RDS augments the referral method by weighting the sample in order to compensate for the initial <b>non-random</b> <b>selection,</b> which may lead to the reduction of errors occurring in sampling by the referral method.|$|E
40|$|We {{study the}} {{relation}} between workers 2 ̆ 019 age and their productivity in work teams, based on a new and unique data set that combines data on errors occurring in the production process of a large car manufacturer with detailed information on the personal characteristics of workers re-lated to the errors. We correct for <b>non-random</b> sample <b>selection</b> and the potential endogeneity of the age-composition in work teams. Our results suggest that productivity does not decline at least up to age 60...|$|R
40|$|We {{study the}} {{relation}} between workers’ age and their productivity in work teams, based on a new and unique data set that combines data on errors occurring in the production process of a large car manufacturer with detailed information on the personal characteristics of workers related to the errors. We correct for <b>non-random</b> sample <b>selection</b> and the potential endogeneity of the age-composition in work teams. Our results suggest that productivity in this plant which is typical for large-scale manufacturing does not decline at least up to age 60. ...|$|R
40|$|The paper {{starts with}} {{discussing}} institutional framework for public sector wage setting in Russia. Given that individual {{choice of the}} sector is endogenous to wages, the authors recommend alternative econometric techniques for the public-private wage gap estimation. Applying switching regression that allows correcting for <b>non-random</b> sector <b>selection,</b> the paper provides wage gap estimates for various demographic, occupational, and territorial population subgroups. As it is shown, there is significant cross-group variation in the wage gap. The paper concludes that to eliminate the negative gap wages {{in the public sector}} should be linked to the private sector wages at the regional level. public sector...|$|R
50|$|According to a 2016 survey {{published}} from The Gild, {{a global}} brand consultancy, British Gen Zers, defined here as those born 2001 and onwards, are {{more conservative than}} Millennials, Gen Xers and Baby Boomers with respect to marijuana legalization, transgender issues and same sex marriage. However, some argue that this study has several methodological problems such as <b>non-random</b> <b>selection</b> and double-barreled questions, rendering the study's findings unreliable in discerning the political ideologies of the generation.|$|E
50|$|Randomness {{can be seen}} as {{conflicting}} {{with the}} deterministic ideas of some religions, such as those where the universe is created by an omniscient deity who is aware of all past and future events. If the universe is regarded to have a purpose, then randomness {{can be seen as}} impossible. This is one of the rationales for religious opposition to evolution, which states that <b>non-random</b> <b>selection</b> is applied to the results of random genetic variation.|$|E
50|$|Progressive {{alignments}} are {{not guaranteed}} to be globally optimal. The primary problem is that when errors are made at any stage in growing the MSA, these errors are then propagated through to the final result. Performance is also particularly bad {{when all of the}} sequences in the set are rather distantly related. Most modern progressive methods modify their scoring function with a secondary weighting function that assigns scaling factors to individual members of the query set in a nonlinear fashion based on their phylogenetic distance from their nearest neighbors. This corrects for <b>non-random</b> <b>selection</b> of the sequences given to the alignment program.|$|E
40|$|<b>Non-random</b> sample <b>selection</b> may render {{estimated}} treatment effects biased even if {{assignment of}} treatment is purely random. Lee (2009) proposes an estimator for treatment effect bounds that limit the possible {{range of the}} treatment effect. In this approach, the lower and upper bound, respectively, correspond to extreme assumptions about the missing information, which {{are consistent with the}} observed data. As opposed to conventional parametric approaches to correcting for sample selection bias, Lee's bounds estimator rests on very few assumptions. We introduce the new Stata command leebounds that implements the estimator in Stata. The command allows for several options, such as tightening bounds by the use of covariates...|$|R
40|$|We extend {{existing}} estimators for duration {{data that}} suffer from <b>non-random</b> sample <b>selection</b> {{to allow for}} time-varying covariates. Rather than a continuous-time duration model, we propose a discrete-time alternative that models the effects of sample selection {{at the time of}} selection across all subsequent years of the resulting spell. Properties of the estimator are compared to those of a naive discrete duration model through Monte Carlo analysis and indicate that our estimator outperforms the naive model when selection is non-trivial. We then apply this estimator {{to the question of the}} duration of monetary regimes and find evidence that ignoring selection into pegs leads to faulty inferences. ...|$|R
40|$|In its essence, the {{explanatory}} {{potential of the}} theory of natural selection is based on the iterative process of random production and variation, and subsequent <b>non-random,</b> directive <b>selection.</b> It is shown that within this explanatory framework, there is no place for the explanation of sexual reproduction. Thus in Darwinistic literature, sexual reproduction - one of nature's most salient characteristics - is often either assumed or ignored, but not explained. This fundamental and challenging gap within a complete naturalistic understanding of living beings calls for the need of a cybernetic account for sexual reproduction, meaning an understanding of the dynamic and creative potential of living beings to continuously and autonomously produce new organisms with unique and specific constellations...|$|R
50|$|CSICOP also contended, after {{reviewing}} the results, that the Gauquelins had not chosen randomly. They had had difficulty finding sufficient same-week and same-village births to compare with champions born in rural areas and so had chosen only champions born in larger cities. The Gauquelins' original total list of about 2,088 champions had included 42 Parisians and their subsample of 303 athletes also included 42 Parisians. Further, Paris is divided into 20 arrondissements, different economic classes and ethnic groups typically inhabiting different arrondissements. The Gauquelins had compared the 42 Parisian champions (who had been born throughout Paris) to non-champions of only one arrondissement. If the 22% correlation was an artifact partly based on factors such as rural recordkeeping, economic, class or ethnic differences in birth patterns, this fact would be blurred by this <b>non-random</b> <b>selection.</b>|$|E
5000|$|The main characters, Amy and Dan Cahill, {{are then}} introduced. They are Grace's grandchildren going to her funeral at her mansion with Grace's sister and their {{guardian}} Aunt Beatrice. Right before the funeral Amy and Dan {{run into the}} Holts. The parents, Eisenhower and Mary-Todd, and their children, Hamilton (fourteen), Madison and Reagan, (eleven), turn Dan upside down. Then a <b>non-random</b> <b>selection</b> of many Cahills, including Amy and Dan, are called away in a private meeting for the will reading. Also called away are the Holts, the Kabras (nicknamed the Cobras); Ian (fourteen) and Natalie (eleven), Alistair Oh (inventor of microwaveable burritos), Irina Spasky (ex-KGB agent), The Starling triplets (Ned, Ted, and Sinead), Jonah Wizard (famous rapper host of the reality TV show [...] "Who Wants to be a Gangsta"), Aunt Ingrid, and Aunt Beatrice. William McIntyre shows them a video of Grace Cahill telling them they're {{on the brink of}} their greatest challenge yet. Mr. McIntyre then says they have a choice, one million dollars, or a chance to be the greatest Cahill in history and gives them five minutes to decide. Dan wants the money for baseball cards, while Amy wants the chance in order to make Grace proud. Then the Kabras try to discourage them from taking the challenge. They are told by Mr. McIntyre that Abraham Lincoln, Harry Houdini and Lewis and Clark, among others, are Cahills. In the end, Amy and Dan chose the chance and receive a sealed envelope which contain a clue, they are the last ones to accept the challenge. The Holts, Alistair Oh, Starlings, Kabras and Spasky all accept the challenge. The envelope says: [...] "Resolution: The fine print to guess. Seek out Richard S_." ...|$|E
50|$|RNA viruses such as {{the human}} {{immunodeficiency}} virus (HIV) evolve at an extremely rapid rate, orders of magnitude faster than mammals or birds. For these organisms, ancestral reconstruction can be applied on a much shorter time scale; for example, in order to reconstruct the global or regional progenitor of an epidemic that has spanned decades rather than millions of years. A team around Brian Gaschen proposed that such reconstructed strains be used as targets for vaccine design efforts, as opposed to sequences isolated from patients in the present day. Because HIV is extremely diverse, a vaccine designed to work on one patient's viral population might not work for a different patient, because the evolutionary distance between these two viruses may be large. However, their most recent common ancestor is closer {{to each of the}} two viruses than they are to each other. Thus, a vaccine designed for a common ancestor could {{have a better chance of}} being effective for a larger proportion of circulating strains. Another team took this idea further by developing a center-of-tree reconstruction method to produce a sequence whose total evolutionary distance to contemporary strains is as small as possible. Strictly speaking, this method was not ancestral reconstruction, as the center-of-tree (COT) sequence does not necessarily represent a sequence that has ever existed in the evolutionary history of the virus. However, Rolland and colleagues did find that, in the case of HIV, the COT virus was functional when synthesized. Similar experiments with synthetic ancestral sequences obtained by maximum likelihood reconstruction have likewise shown that these ancestors are both functional and immunogenic, lending some credibility to these methods. Furthermore, ancestral reconstruction can potentially be used to infer the genetic sequence of the transmitted HIV variants that have gone on to establish the next infection, with the objective of identifying distinguishing characteristics of these variants (as a <b>non-random</b> <b>selection</b> of the transmitted population of viruses) that may be targeted for vaccine design.|$|E
40|$|Over {{the past}} two decades the Canadian Labour market {{experienced}} a rise in the overall wage inequality and de-unionization. This paper provides empirical evidence on effects of unionization on the real hourly wage before and after the 2008 financial crisis in Canada. The Ordinary Least Square (OLS) model is used to estimate the impact of union status and time related (such as time trend, year, and post- 2008) variables on the real hourly wage level. Moreover, Heckman’s two- step approach is performed to correct for <b>non-random</b> sample <b>selection.</b> The findings show unionization affects the real hourly wage positively, while the time variables’ coefficients indicate the 2008 financial crisis did not {{have a significant impact on}} the real hourly wage level in Canada...|$|R
40|$|This study utilises HILDA data to {{evaluate}} the performance of second generation Australians in full-time employment over the period 2001 - 2005. Attention is paid to the effects and key drivers of overeducation and overskilling. Quantile regression is employed to account for <b>non-random</b> sample <b>selection.</b> The evidence provides several new insights: (a) Greek-Australians are overeducated; (b) Asian languages and Chinese or Vietnamese ancestry associate with a wage premium; (c) the returns to tertiary education, overeducation and overskilling very with ability; (d) parental occupational status is a key determinant of overeducation; and (e) lack of employer provision for new skills and personality traits are important drivers of overskilling. * I have benefited from valuable comments provided by the editor, two anonymous referees...|$|R
40|$|The No Child Left Behind Act imposes {{sanctions}} on schools if {{the fraction of}} any of five racial groups of students demonstrating proficiency on a high stakes exam falls below a statewide pass rate. This system places pressure on school administrators to redirect educational resources from groups of students likely to demonstrate proficiency towards those who are marginally below proficient. Using statewide observations of 3 rd and 4 th grade math tests, this paper demonstrates that students of successful racial groups at schools likely to be sanctioned gain less academically over their subsequent test year than comparable peers at passing schools. This effect is stronger at schools {{more likely to suffer}} from NCLB sanctions and is robust to corrections for <b>non-random</b> sample <b>selection.</b> Strategic instruction Education incentives...|$|R
5000|$|Views superficially similar, but {{unrelated}} to Hoyle's, are thus invariably justified with arguments from analogy. The basic {{idea of this}} argument for a designer is the teleological argument, an argument {{for the existence of}} God based on the perceived order or purposefulness of the universe. A common way of using this as an objection to evolution is by appealing to the 18th-century philosopher William Paley's watchmaker analogy, which argues that certain natural phenomena are analogical to a watch (in that they are ordered, or complex, or purposeful), which means that, like a watch, they must have been designed by a [...] "watchmaker"—an intelligent agent. This argument forms the core of intelligent design, a neo-creationist movement seeking to establish certain variants of the design argument as legitimate science, rather than as philosophy or theology, and have them be taught alongside evolution.This objection is fundamentally an argument by lack of imagination, or argument from incredulity: a certain explanation is seen as being counterintuitive, and therefore an alternate, more intuitive explanation is appealed to instead. Supporters of evolution generally respond by arguing that evolution is not based on [...] "chance," [...] but on predictable chemical interactions: natural processes, rather than supernatural beings, are the [...] "designer." [...] Although the process involves some random elements, it is the <b>non-random</b> <b>selection</b> of survival-enhancing genes that drives evolution along an ordered trajectory. The fact that the results are ordered and seem [...] "designed" [...] is no more evidence for a supernatural intelligence than the appearance of complex natural phenomena (e.g. snowflakes). It is also argued that there is insufficient evidence to make statements about the plausibility or implausibility of abiogenesis, that certain structures demonstrate poor design, and that the implausibility of life evolving exactly as it did is no more evidence for an intelligence than the implausibility of a deck of cards being shuffled and dealt in a certain random order.|$|E
40|$|This work investigates whether World Bank loans {{fostering}} {{trade liberalization}} {{are associated with}} less distorted export policies, by employing some gravity model-based measures of anti-export bias, and a Herfindhal index of export revenues concentration. When accounting for <b>non-random</b> <b>selection</b> {{in a sample of}} 88 developing countries over the period 1980 - 2000, the receipt of trade adjustment loans seems to have reduced the policy distortion under scrutiny. Such a beneficial influence, however, vanishes when a longer time horizon is considered, casting doubts on the country ownership of waves of liberalizations supported by the Bank. Trade adjustment loans, special interest groups, anti-export bias, gravity models, <b>non-random</b> <b>selection,...</b>|$|E
40|$|We {{assess the}} causal effect of private {{tutoring}} on {{the probability of}} university placement in Turkey. We find that tutoring increases the probability of being placed in a university when <b>non-random</b> <b>selection</b> is ignored. Moreover, among those utilizing private tutoring, greater expenditure on tutoring is also positively associated with university placement. However, we find evidence of positive selection into tutoring, but negative selection into greater expenditures among those receiving tutoring. Accounting for this pattern of <b>non-random</b> <b>selection,</b> we conclude that private tutoring has a negative causal effect on university placement overall, but conditional on receiving any tutoring, spending more on tutoring has a positive causal effect on university placement. tutoring, Turkey, tertiary education, program evaluation...|$|E
50|$|This {{second step}} makes the {{technique}} non-probability sampling. In quota sampling, there is <b>non-random</b> sample <b>selection</b> {{and this can}} be unreliable. For example, interviewers {{might be tempted to}} interview those people in the street who look most helpful, or may choose to use accidental sampling to question those closest to them, for time-keeping sake. The problem is that these samples may be biased because not everyone gets a chance of selection. Whereas in stratified sampling (its probabilistic version), the chance of any unit of the population is the same as 1/n (n= number of units in the population). This non-random element is a source of uncertainty {{about the nature of the}} actual sample and quota versus probability has been a matter of controversy for many years.|$|R
40|$|Sample {{selection}} models {{deal with}} the situation in which an outcome of interest is observed for a restricted non-randomly selected sample of the population. The estimation of these models is based on a binary equation, which describes the selection process, and an outcome equation, which is used to examine the substantive question of interest. Classic sample selection models assume a priori that continuous covariates have a linear or pre-specified non-linear relationship to the outcome, and that the distribution linking the two equations is bivariate normal. We introduce the R package SemiParSampleSel which implements copula regression spline sample selection models. The proposed implementation can deal with <b>non-random</b> sample <b>selection,</b> non-linear covariate-response relationships, and non-normal bivariate distributions between the model equations. We provide details of the model and algorithm and describe the implementation in SemiParSampleSel. The package is illustrated using simulated and real data examples...|$|R
40|$|We {{consider}} {{the problem of}} estimating an average treatment effect for a target population from a survey sub-sample. Our motivating example is generalizing a treatment effect estimated in a sub-sample of the National Comorbidity Survey Replication Adolescent Supplement to the population of U. S. adolescents. To address this problem, we evaluate easy-to-implement methods that account for both non-random treatment assignment and a <b>non-random</b> two-stage <b>selection</b> mechanism. We compare {{the performance of a}} Horvitz-Thompson estimator using inverse probability weighting (IPW) and two double robust estimators in a variety of scenarios. We demonstrate that the two double robust estimators generally outperform IPW in terms of mean-squared error even under misspecification of one of the treatment, selection, or outcome models. Moreover, the double robust estimators are easy to implement, providing an attractive alternative to IPW for applied epidemiologic researchers. We demonstrate how to apply these estimators to our motivating example...|$|R
3000|$|For women’s wages, {{we first}} use Guvenen (2005)’s {{estimates}} for the variance of the productivity shocks and fix σ^ 2 _ϵ= 0.061. Second, due to <b>non-random</b> <b>selection</b> of married women into the labor market, the wage coefficients of the Mincer equation, (β [...]...|$|E
3000|$|This {{result is}} {{consistent}} with the findings of Eckstein and Wolpin (1989), who show that simple wage regressions on female wages yield biased estimates because of <b>non-random</b> <b>selection</b> in labor markets and experience accumulation. They find that, when using a structural model of women’s employment decisions, the coefficient on experience and experience squared in the Mincer equation, β [...]...|$|E
30|$|We {{restrict}} our overview {{to studies}} that convincingly {{address the problem}} of <b>non-random</b> <b>selection</b> into treatment, e.g., by conducting randomized field experiments, exploiting exogenous variation in program access, controlling for a large set of informative control variables, or using other state-of-the-art econometric evaluation techniques. The bulk of evaluation studies use the conditional independence or “unconfoundedness” assumption for identification, thus assuming that controlling for observed characteristics is sufficient to capture selection into the different treatment options. The ALMP evaluation literature offers some guidelines regarding the most informative characteristics to be included to control for <b>non-random</b> <b>selection</b> (e.g., Caliendo et al. 2014; Lechner and Wunsch 2013), and we preselected studies adhering to these guidelines. However, it is clear that by the short previous labor market history of youth and the stronger variability in terms of initial labor market attachment, there is a risk that unobserved heterogeneity plays an even stronger role for the youth than for the adult working population.|$|E
40|$|The paper {{presents}} empirical {{findings regarding}} {{the economic policy}} consequences of constitutional arrangements, in three different dimensions. First, the data are consistent with several theoretical predictions {{about the consequences of}} electoral rules and forms of government for fiscal policy and rent extraction, even when <b>non-random</b> constitution <b>selection</b> is taken into account. Second, empirical tests of the predictions from a new comprehensive model of parliamentary democracy show that proportional elections raise government spending through their indirect consequences for party structures and types of government, rather than through their direct effects on policymaking incentives. Third, new empirical results suggest that constitutional arrangements may have important consequences for structural polices that promote long-run economic performance, hinting at a missing link in the causal chain from history to current economic performance. All these empirical findings appear statistically robust, and the estimated effects are large enough to be of genuine economic interest. (JEL...|$|R
40|$|Credit scoring {{models are}} {{normally}} built using only applicants {{who have been}} previously accepted for credit. Such <b>non-random</b> sample <b>selection</b> may produce bias in estimated model parameters and accordingly model predictions of repayment performance may not be optimal. Previous empirical research suggests that omission of rejected applicants has a detrimental impact on model estimation and prediction., This paper explores {{the extent to which}} the number of included variables influences the efficacy of a commonly used reject inference technique, reweighting. Analysis benefits from availability of a rare sample where virtually no applicant was denied credit. The general indication is that the efficacy of reject inference is little influenced by either model leanness or interaction between model leanness and the rejection rate that determined the sample. However, there remains some hint that very lean models may benefit from reject inference where modelling is conducted on data characterized by a very high rate of applicant rejection. ...|$|R
30|$|A {{first issue}} {{that one might}} discuss {{regarding}} the data {{is the use of}} Raven’s progressive matrices test as a measure of cognitive ability or even IQ. In the sample, the correlation of the ability measure with the one of the father is 0.363. This value is very close to the 0.347 and 0.38 estimated by Björklund et al. (2010) and Black et al. (2009), respectively, both using more detailed IQ measures. The same correlation with respect to the mother was found to be 0.387, which is slightly higher than the father–son correlation. Considering only the two oldest siblings in a family gives a siblings IQ-correlation of 0.506, which is again relatively close to the values reported by Björklund et al. (2010) who find estimates between 0.473 and 0.510. Interestingly, and giving a first evidence for assortative mating, the spousal IQ-correlation is 0.400. The spousal education-correlation based on the years of schooling is 0.646, which is even higher and supports the idea of an important role of <b>non-random</b> spousal <b>selection.</b>|$|R
