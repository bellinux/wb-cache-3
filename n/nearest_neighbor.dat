9381|3854|Public
5|$|The nearest-neighbor chain {{algorithm}} {{was developed}} and implemented in 1982 by Jean-Paul Benzécri and J. Juan. They based this algorithm on earlier methods that constructed hierarchical clusterings using mutual <b>nearest</b> <b>neighbor</b> pairs without taking advantage of <b>nearest</b> <b>neighbor</b> chains.|$|E
5|$|Predecessor and {{successor}} queries can {{be performed}} with rank queries. Once the rank of the target value is known, its predecessor is the element at the position given by its rank (as {{it is the largest}} element that is smaller than the target value). Its successor is the element after it (if it is present in the array) or at the next position after the predecessor (otherwise). The <b>nearest</b> <b>neighbor</b> of the target value is either its predecessor or successor, whichever is closer.|$|E
5|$|The above {{procedure}} only performs exact matches, {{finding the}} position of a target value. However, due to the ordered nature of sorted arrays, it is trivial to extend binary search to perform approximate matches. For example, binary search can be used to compute, for a given value, its rank (the number of smaller elements), predecessor (next-smallest element), successor (next-largest element), and <b>nearest</b> <b>neighbor.</b> Range queries seeking the number of elements between two values can be performed with two rank queries.|$|E
3000|$|For those {{classes that}} are {{not found in the}} k <b>nearest</b> <b>neighbors,</b> we use the {{distance}} to the k+ 1 <b>nearest</b> <b>neighbors</b> of x [...]...|$|R
5000|$|... nn = <b>nearest</b> <b>neighbors.</b> For a (d+1)-dimensional hypercubic system, the {{hypercube}} is in d {{dimensions and}} the time direction points to the 2D <b>nearest</b> <b>neighbors.</b>|$|R
30|$|The most {{important}} and basic idea behind utilizing a space partitioning is to perform a local search within local regions rather than a global search. When searching <b>nearest</b> <b>neighbors,</b> it is computationally optimal to first search approximate <b>nearest</b> <b>neighbors</b> within several local cells and then to find the exact <b>nearest</b> <b>neighbors</b> by filtering undesired points.|$|R
5|$|The {{main idea}} of the {{algorithm}} is to find pairs of clusters to merge by following paths in the <b>nearest</b> <b>neighbor</b> graph of the clusters. Every such path will eventually terminate at a pair of clusters that are nearest neighbors of each other, and the algorithm chooses that pair of clusters as the pair to merge. In order to save work by re-using {{as much as possible}} of each path, the algorithm uses a stack data structure to keep track of each path that it follows. By following paths in this way, the nearest-neighbor chain algorithm merges its clusters in a different order than methods that always find and merge the closest pair of clusters. However, despite that difference, it always generates the same hierarchy of clusters.|$|E
5|$|Hyperion is Titan's <b>nearest</b> <b>neighbor</b> in the Saturn system. The two moons {{are locked}} in a 4:3 mean-motion {{resonance}} with each other, meaning that while Titan makes four revolutions around Saturn, Hyperion makes exactly three. With an average diameter of about 270km, Hyperion is smaller and lighter than Mimas. It has an extremely irregular shape, and a very odd, tan-colored icy surface resembling a sponge, though its interior may be partially porous as well. The average density of about 0.55g/cm3 indicates that the porosity exceeds 40% even assuming it has a purely icy composition. The surface of Hyperion is covered with numerous impact craters—those with diameters 2–10km are especially abundant. It is the only moon besides the small moons of Pluto known to have a chaotic rotation, which means Hyperion has no well-defined poles or equator. While on short timescales the satellite approximately rotates around its long axis {{at a rate of}} 72–75° per day, on longer timescales its axis of rotation (spin vector) wanders chaotically across the sky. This makes the rotational behavior of Hyperion essentially unpredictable.|$|E
25|$|Burgess {{compared}} the biological forces that produce social groups in shoaling and schooling fish, flocking and territorial birds, monkey colonies, and human social groups; developing techniques of measuring and displaying proxemic distances between <b>nearest</b> <b>neighbor</b> individuals {{in a social}} group.|$|E
5000|$|Each vertex {{also has}} 56 third <b>nearest</b> <b>neighbors,</b> {{which are the}} negatives of its <b>nearest</b> <b>neighbors,</b> and one {{antipodal}} vertex, {{for a total of}} [...] vertices.|$|R
50|$|Vazquez {{proposed}} a growth model {{based on a}} recursive 'copying' mechanism, continuing to 2nd <b>nearest</b> <b>neighbors,</b> 3rd <b>nearest</b> <b>neighbors</b> etc. The authors call it a 'random walk' mechanism.).|$|R
30|$|Due to {{the impact}} of the {{similarity}} of the <b>near</b> <b>neighbor</b> set, UbCF and IbCF vary in the accuracy of predication. For example, {{it is obvious that the}} UbCF-based recommendation results are more accurate when the similarity of the user’s <b>near</b> <b>neighbor</b> set is { 1, 0.8, 0.9 } while the similarity of the item’s <b>near</b> <b>neighbor</b> set is { 0.4, 0.5, 0.5 }. So the confidence weight [20] is introduced to balance the final prediction result. And the larger the similarity of the <b>near</b> <b>neighbors</b> set is, the bigger its confidence weight is.|$|R
25|$|The {{separator}} based {{divide and}} conquer paradigm has also been used to design data structures for dynamic graph algorithms and point location, algorithms for polygon triangulation, shortest paths, {{and the construction of}} <b>nearest</b> <b>neighbor</b> graphs, and approximation algorithms for the maximum independent set of a planar graph.|$|E
25|$|Point pattern {{analysis}} {{deals with the}} distribution of individuals through space, and is used {{to determine whether the}} distribution is random. It also describes the type of pattern and draws conclusions on what kind of process created the observed pattern. Quadrat-density and the <b>nearest</b> <b>neighbor</b> methods are the most commonly used statistical methods.|$|E
25|$|In {{the case}} of <b>nearest</b> <b>neighbor</b> interactions, E. Ising {{provided}} an exact solution of the model. At any positive temperature (i.e. finite β) the free energy is analytic in the thermodynamics parameters and the truncated two-point spin correlation decays exponentially fast. At zero temperature, (i.e. infinite β), {{there is a second}} order phase transition: the free energy is infinite and the truncated two point spin correlation does not decay (remains constant). Therefore, T = 0 is the critical temperature of this case. Scaling formulas are satisfied.|$|E
40|$|This paper {{presents}} Clustering {{based on}} <b>Near</b> <b>Neighbor</b> Influence (CNNI), a new clustering algorithm which {{is inspired by}} the idea of <b>near</b> <b>neighbor</b> and the superposition principle of influence. In order to clearly describe this algorithm, it introduces some important concepts, such as <b>near</b> <b>neighbor</b> point set, <b>near</b> <b>neighbor</b> influence, and similarity measure. By simulated experiments of some artificial data sets and seven real data sets, we observe that this algorithm can often get good clustering quality when making proper value of some parameters. At last, it gives some research expectations to popularize this algorithm. Comment: 21 pages, 9 figures, and 8 table...|$|R
5000|$|... where [...] is {{the set of}} all type 3 objects, [...] is {{the fuzzy}} {{membership}} vector of object , [...] is the set of <b>nearest</b> <b>neighbors</b> of , and [...] with [...] are the coefficients reflecting the relative proximities of the <b>nearest</b> <b>neighbors.</b>|$|R
3000|$|... [...]. The W with {{dimension}} C[*]−[*] 1 {{projects the}} training data onto a new space called fisherfaces. We use W to project all training samples onto the fisherfaces. The resulting vectors {{are used to}} create a KD-tree which is employed in finding the approximate <b>nearest</b> <b>neighbors</b> during the classification of a sample image. We use five <b>nearest</b> <b>neighbors,</b> and the class with the highest number of <b>nearest</b> <b>neighbors</b> is assigned as the class of the vehicle.|$|R
25|$|Human {{catalase}} forms a tetramer {{composed of}} four subunits, {{each of which}} can be conceptually divided into four domains. The extensive core of each subunit is generated by an eight-stranded antiparallel b-barrel (b1-8), with <b>nearest</b> <b>neighbor</b> connectivity capped by b-barrel loops on one side and a9 loops on the other. A helical domain at one face of the b-barrel is composed of four C-terminal helices (a16, a17, a18, and a19) and four helices derived from residues between b4 and b5 (a4, a5, a6, and a7). Alternative splicing may result in different protein variants.|$|E
25|$|Most admirable {{of all her}} qualities, however, {{were her}} taste, elegance and deeply musical use of {{ornamentation}} {{in all its forms}} and complications, the weighting and length of every appoggiatura, the smooth incorporation of the turn in melodic lines, the accuracy and pacing of her trills, the seemingly inevitable timing of her portamentos, varying their curve with enchanting grace and meaning. There were innumerable exquisite felicities—minuscule portamentos from one note to its <b>nearest</b> <b>neighbor,</b> or over widespread intervals—and changes of color that were pure magic. In these aspects of bel canto she was supreme mistress of that art.|$|E
25|$|Baker Island, a {{territory}} of the United States {{and a part of}} the United States Minor Outlying Islands, is an uninhabited atoll located just north of the equator in the central Pacific Ocean about 1,700 miles (3,100 kilometers) southwest of Honolulu, Hawaii. The island lies almost halfway between the U.S. state of Hawaii and Australia. Its <b>nearest</b> <b>neighbor</b> is Howland Island, 37 nautical miles (68 kilometers) to the north. Now known as a National Wildlife Refuge, Baker Island is an insular area administered and managed by the United States Fish and Wildlife Service. According to an article in Pacific Magazine dated in 2000, Baker island was reportedly first sighted in 1825 by U.S. Captain Obed Starbuck in the ship Lopez. The newly discovered island was named New Nantucket (and also as Phoebe). It was in 1832 that U.S. Captain Michael Baker, after whom the island is now named, also came to the island aboard the whaler Gideon Howard.|$|E
40|$|An {{efficient}} {{branch and}} bound search algorithm is proposed for the computation of the K <b>nearest</b> <b>neighbors</b> in a multidimensional vector space. In a preprocessing step, the sample of feature vectors is decomposed hierarchically using hyperplanes determined by principal component analysis (PCA). During the search of the <b>nearest</b> <b>neighbors,</b> the tree that represents this decomposition is traversed in depth first order avoiding nodes that cannot contain <b>nearest</b> <b>neighbors.</b> The behavior of the algorithm is studied on artificial data...|$|R
40|$|In <b>nearest</b> <b>neighbors</b> {{search the}} task is to find points from a data set that lie close in space to a given query point. To improve on brute force search, that computes {{distances}} between the query point and all data points, numerous data structures have been developed. These however perform poorly in high dimensional spaces. To tackle <b>nearest</b> <b>neighbors</b> search in high dimensions it is commonplace to use approximate methods that only return <b>nearest</b> <b>neighbors</b> with high probability. In practice an approximate solution is often as good as an exact one, among other reasons because approximations can be of such a high quality that they are practically indistinguishable from exact solutions. Approximate <b>nearest</b> <b>neighbors</b> search has found applications in many different fields, and can for example {{be used in the}} context of recommendation systems. One class of approximate <b>nearest</b> <b>neighbors</b> algorithms is space partitioning methods. These algorithms recursively partition the data set to smaller subsets in order to construct a search structure. Queries can then be performed very efficiently by using this structure to prune data points without needing to evaluate their distances to the query point. A recent proposal belonging to this class of algorithms is multiple random projections trees (MRPT). MRPT uses random projection trees (RP-trees) to prune the set from which <b>nearest</b> <b>neighbors</b> are searched. This thesis proposes a voting algorithm for using multiple RP-trees in <b>nearest</b> <b>neighbors</b> search. We also discuss a further improvement, called mixture method. The performance of these algorithms was evaluated against the previous MRPT algorithm using two moderately high dimensional data sets. Mixture method was found to improve considerably on MRPT in terms of accuracy attained. The results presented in this thesis suggest that the mixture method may potentially be a strong algorithm for <b>nearest</b> <b>neighbors</b> search, especially in very high dimensional spaces...|$|R
5000|$|... k-nearest {{neighbor}} search {{identifies the}} top k <b>nearest</b> <b>neighbors</b> to the query. This technique {{is commonly used}} in predictive analytics to estimate or classify a point based on the consensus of its neighbors. k-nearest neighbor graphs are graphs in which every point is connected to its k <b>nearest</b> <b>neighbors.</b>|$|R
25|$|Nabarro–Herring creep is {{strongly}} temperature dependent. For lattice diffusion of atoms {{to occur in}} a material, neighboring lattice sites or interstitial sites in the crystal structure must be free. A given atom must also overcome the energy barrier to move from its current site (it lies in an energetically favorable potential well) to the nearby vacant site (another potential well). The general form of the diffusion equation is D = D0exp(E/KT) where D0 has a dependence on both the attempted jump frequency {{and the number of}} <b>nearest</b> <b>neighbor</b> sites and the probability of the sites being vacant. Thus there is a double dependence upon temperature. At higher temperatures the diffusivity increases due to the direct temperature dependence of the equation, the increase in vacancies through Schottky defect formation, and an increase in the average energy of atoms in the material. Nabarro–Herring creep dominates at very high temperatures relative to a material's melting temperature.|$|E
500|$|If a {{distance}} function has the reducibility property, then merging two clusters [...] and [...] can only cause the <b>nearest</b> <b>neighbor</b> of [...] to change if that <b>nearest</b> <b>neighbor</b> {{was one of}} [...] and [...] This has two important consequences for the <b>nearest</b> <b>neighbor</b> chain algorithm. First, it can be shown using this property that, at {{each step of the}} algorithm, the clusters on the stack [...] form a valid chain of nearest neighbors, because whenever a <b>nearest</b> <b>neighbor</b> becomes invalidated it is immediately removed from the stack.|$|E
500|$|Intuitively, the <b>nearest</b> <b>neighbor</b> chain {{algorithm}} repeatedly {{follows a}} chain of clusters [...] where each cluster is the <b>nearest</b> <b>neighbor</b> of the previous one, until reaching a pair of clusters that are mutual nearest neighbors.|$|E
40|$|Numerous {{applications}} in search, databases, machine learning, and computer vision, {{can benefit from}} efficient algorithms for <b>near</b> <b>neighbor</b> search. This paper proposes a simple framework for fast <b>near</b> <b>neighbor</b> search in high-dimensional binary data, which are common in practice (e. g., text). We develop a very simple and effective strategy for sub-linear time <b>near</b> <b>neighbor</b> search, by creating hash tables directly using the bits generated by b-bit minwise hashing. The advantages of our method are demonstrated through thorough comparisons with two strong baselines: spectral hashing and sign (1 -bit) random projections. NSF Grant # 113184...|$|R
40|$|In {{this paper}} we {{describe}} {{a method for}} hybridizing a genetic algorithm and a k <b>nearest</b> <b>neighbors</b> classification algorithm. We use the genetic algorithm and a training data set to learn real-valued weights associated with individual attributes in the data set. We use the k <b>nearest</b> <b>neighbors</b> algorithm to classify new data records based on their weighted distance from {{the members of the}} training set. We applied our hybrid algorithm to three test cases. Classification results obtained with the hybrid algorithm exceed the performance of the k <b>nearest</b> <b>neighbors</b> algorithm in all three cases. ...|$|R
40|$|This paper {{presents}} {{a simple but}} effective density-based outlier detection approach with the local kernel density estimation (KDE). A Relative Density-based Outlier Score (RDOS) is introduced to measure the local outlierness of objects, in which the density distribution {{at the location of}} an object is estimated with a local KDE method based on extended <b>nearest</b> <b>neighbors</b> of the object. Instead of using only $k$ <b>nearest</b> <b>neighbors,</b> we further consider reverse <b>nearest</b> <b>neighbors</b> and shared <b>nearest</b> <b>neighbors</b> of an object for density distribution estimation. Some theoretical properties of the proposed RDOS including its expected value and false alarm probability are derived. A comprehensive experimental study on both synthetic and real-life data sets demonstrates that our approach is more effective than state-of-the-art outlier detection methods. Comment: 22 pages, 14 figures, submitted to Pattern Recognition Letter...|$|R
500|$|Second, {{and even}} more importantly, it follows from this {{property}} that, if two clusters [...] and [...] both belong to the greedy hierarchical clustering, and are mutual nearest neighbors {{at any point in}} time, then they will be merged by the greedy clustering, for they must remain mutual nearest neighbors until they are merged. It follows that each mutual <b>nearest</b> <b>neighbor</b> pair found by the <b>nearest</b> <b>neighbor</b> chain algorithm is also a pair of clusters found by the greedy algorithm, and therefore that the <b>nearest</b> <b>neighbor</b> chain algorithm computes exactly the same clustering (although in a different order) as the greedy algorithm.|$|E
500|$|Each {{of these}} {{iterations}} may spend time scanning {{as many as}} [...] inter-cluster distances to find the <b>nearest</b> <b>neighbor.</b>|$|E
500|$|The {{correctness}} of this algorithm {{relies on}} a property of its distance function called reducibility. This property was identified by [...] in connection with an earlier clustering method that used mutual <b>nearest</b> <b>neighbor</b> pairs but not chains of nearest neighbors. A distance function [...] on clusters is defined to be reducible if, for every three clusters , [...] and [...] in the greedy hierarchical clustering such that [...] and [...] are mutual nearest neighbors, the following inequality holds: ...|$|E
40|$|We report site {{percolation}} thresholds for {{square lattice}} with neighbor interactions at various increasing ranges. Using Monte Carlo techniques {{we found that}} <b>nearest</b> <b>neighbors</b> (N$^ 2 $), next <b>nearest</b> <b>neighbors</b> (N$^ 3 $), next next <b>nearest</b> <b>neighbors</b> (N$^ 4 $) and fifth <b>nearest</b> <b>neighbors</b> (N$^ 6 $) yield the same $p_c= 0. 592 [...] . $. At odds, fourth <b>nearest</b> <b>neighbors</b> (N$^ 5 $) give $p_c= 0. 298 [...] . $. These results are given an explanation in terms of symmetry arguments. We then consider combinations of various ranges of interactions with (N$^ 2 $+N$^ 3 $), (N$^ 2 $+N$^ 4 $), (N$^ 2 $+N$^ 3 $+N$^ 4 $) and (N$^ 2 $+N$^ 5 $). The calculated associated thresholds are respectively $p_c= 0. 407 [...] ., 0. 337 [...] ., 0. 288 [...] ., 0. 234 [...] . $. The existing Galam [...] Mauger universal formula for percolation thresholds does not reproduce the data showing dimension and coordination number are not sufficient to build a universal law which extends to complex lattices. Comment: 4 pages, revtex...|$|R
30|$|The {{finding of}} k nearest {{neighboring}} data points for each interpolated points can be inherently parallelized. Assuming there are n interpolated points, and we allocate n threads {{to search the}} <b>nearest</b> <b>neighbors</b> for all the interpolated points. Each thread is invoked to find the <b>nearest</b> <b>neighbors</b> for only one interpolated point.|$|R
30|$|For {{a pair of}} images, CNN {{descriptors}} are tentatively {{matched by}} searching their <b>nearest</b> <b>neighbors</b> (L 2 distances) and refined by taking mutually <b>nearest</b> <b>neighbors.</b> Note that the standard ratio test [11] removes too many feature matches as neighborhood features on a regularly sampled grid tend {{to be similar to}} each other.|$|R
