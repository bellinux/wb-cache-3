10000|10000|Public
5|$|State-of-the-art deep <b>neural</b> <b>network</b> {{architectures}} {{can sometimes}} even rival human accuracy in fields like computer vision, specifically {{on things like}} the MNIST database, and traffic sign recognition.|$|E
25|$|Since the {{function}} of the human mind, and how it might arise from the working of the brain's <b>neural</b> <b>network,</b> are poorly understood issues, mind uploading relies on the idea of <b>neural</b> <b>network</b> emulation. Rather than having to understand the high-level psychological processes and large-scale structures of the brain, and model them using classical artificial intelligence methods and cognitive psychology models, the low-level structure of the underlying <b>neural</b> <b>network</b> is captured, mapped and emulated with a computer system. In computer science terminology, rather than analyzing and reverse engineering the behavior of the algorithms and data structures that resides in the brain, a blueprint of its source code is translated to another programming language. The human mind and the personal identity then, theoretically, is generated by the emulated <b>neural</b> <b>network</b> in an identical fashion to it being generated by the biological <b>neural</b> <b>network.</b>|$|E
25|$|File:Artificial neural network.svg|An {{artificial}} <b>neural</b> <b>network.</b>|$|E
30|$|Over {{the past}} decades, {{considerable}} {{attention has been}} devoted to the study of artificial <b>neural</b> <b>networks</b> due to their extensive application in signal and image processing, pattern recognition, and combinatorial optimization [1, 2]. Numerous models of <b>neural</b> <b>networks</b> such as cellular <b>neural</b> <b>networks,</b> Hopfield-type <b>neural</b> <b>networks,</b> Cohen-Grossberg <b>neural</b> <b>networks,</b> and bidirectional associative memory <b>neural</b> <b>networks</b> have been extensively investigated in the literature [3 – 7].|$|R
30|$|<b>Neural</b> <b>networks</b> with {{deviating}} argument, {{which are}} {{proposed in the}} model of recurrent <b>neural</b> <b>networks</b> by Akhmet et al. [41], are suitable for modeling situations in physics, economy, and biology. In these situations, not only past but also future events are critical for the current properties. The deviating argument changes its type from advanced to retarded alternately and it can link past and future events [42 – 49]. <b>Neural</b> <b>networks</b> with deviating argument conjugate continuous <b>neural</b> <b>networks</b> and discrete <b>neural</b> <b>networks.</b> Hence, this type of <b>neural</b> <b>networks</b> has the properties of both continuous <b>neural</b> <b>networks</b> and discrete <b>neural</b> <b>networks.</b> From a mathematical perspective, these <b>neural</b> <b>networks</b> are of a mixed type. With {{the evolution of the}} process, the deviating state can be advanced and retarded, commutatively. The dynamic behavior of this type of <b>neural</b> <b>networks</b> is studied extensively [50 – 52] and deserves further investigation.|$|R
30|$|The {{generalized}} <b>neural</b> <b>networks</b> (GNNs) model, {{which is}} a combination of local field <b>neural</b> <b>networks</b> (LFNNs) and static <b>neural</b> <b>networks</b> (SNNs), has received increasing attention in recent years, {{due to the fact that}} it provides an unified frame for stability analysis of both SNNs and LFNNs [1 – 6]. It should be mentioned that back-propagation <b>neural</b> <b>networks</b> and optimization type <b>neural</b> <b>networks</b> can be modeled as SNNs, whereas Hopfield <b>neural</b> <b>networks,</b> bidirectional associative memory <b>neural</b> <b>networks,</b> and cellular <b>neural</b> <b>networks</b> can be modeled as LFNNs [6]. Therefore, it is enough to study the stability of GNNs instead of both LFNNs and SNNs. On the other hand, as a source of instability and poor performance, time-delays [7 – 9] always appear in many <b>neural</b> <b>networks.</b> Thus, stability analysis for delayed <b>neural</b> <b>networks</b> has received considerable attention over the past few decades [2 – 6, 10 – 35].|$|R
25|$|File:Two layer ann.svg|A two-layer {{feedforward}} artificial <b>neural</b> <b>network.</b>|$|E
25|$|Training a <b>neural</b> <b>network</b> model {{essentially}} means selecting {{one model}} {{from the set}} of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training <b>neural</b> <b>network</b> models. Most {{of them can be}} viewed as a straightforward application of optimization theory and statistical estimation.|$|E
25|$|By {{assigning}} a softmax activation function, a {{generalization of}} the logistic function, on the output {{layer of the}} <b>neural</b> <b>network</b> (or a softmax component in a component-based <b>neural</b> <b>network)</b> for categorical target variables, the outputs {{can be interpreted as}} posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.|$|E
30|$|<b>Neural</b> <b>networks,</b> {{including}} Hopfield <b>neural</b> <b>networks</b> {{and cellular}} <b>neural</b> <b>networks,</b> {{have been widely}} investigated in past decades [1 – 23]. Synchronization, as a typical collective dynamical behavior of <b>neural</b> <b>networks,</b> has attracted more and more attention in various fields. For achieving the synchronization of <b>neural</b> <b>networks,</b> especially of chaotic <b>neural</b> <b>networks,</b> many control methods and techniques have been adopted to design proper and effective controllers, such as feedback control, intermittent control, adaptive control, impulsive control, and so on.|$|R
3000|$|Using {{the time}} scales {{calculus}} theory, the coincidence degree theory, and the Lyapunov functional method, we obtain sufficient {{conditions for the}} existence and global exponential stability of anti-periodic solutions for a class of generalized <b>neural</b> <b>networks</b> with impulses and arbitrary delays. This class of generalized <b>neural</b> <b>networks</b> include many continuous or discrete time <b>neural</b> <b>networks</b> such as, Hopfield type <b>neural</b> <b>networks,</b> cellular <b>neural</b> <b>networks,</b> Cohen-Grossberg <b>neural</b> <b>networks,</b> and so on. To {{the best of our}} knowledge, the known results about the existence of anti-periodic solutions for <b>neural</b> <b>networks</b> are all done by a similar analytic method, and only good for <b>neural</b> <b>networks</b> without impulse. Our results obtained in this paper are completely new even if the time scale [...]...|$|R
3000|$|... which {{contains}} many popular models such as discrete-time Hopfield <b>neural</b> <b>networks,</b> discrete-time cellular <b>neural</b> <b>networks,</b> and discrete-time recurrent <b>neural</b> <b>networks,</b> and so on.|$|R
25|$|In 2014, El-Dahshan et al. {{suggested}} to use pulse coupled <b>neural</b> <b>network.</b>|$|E
25|$|Although {{it is true}} that {{analyzing}} {{what has}} been learned by an artificial <b>neural</b> <b>network</b> is difficult, {{it is much easier to}} do so than to analyze what has been learned by a biological <b>neural</b> <b>network.</b> Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs non-local learning and shallow vs deep architecture.|$|E
25|$|<b>Neural</b> <b>network</b> {{research}} stagnated after {{machine learning}} research by Minsky and Papert (1969), who discovered two key {{issues with the}} computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. <b>Neural</b> <b>network</b> research slowed until computers achieved far greater processing power.|$|E
40|$|Abstract—The {{application}} of <b>neural</b> <b>networks</b> {{in the data}} mining has become wider. Although <b>neural</b> <b>networks</b> may have complex structure, long training time, and uneasily understandable representation of results, <b>neural</b> <b>networks</b> have high acceptance ability for noisy data and high accuracy and are preferable in data mining. In this paper the data mining based on <b>neural</b> <b>networks</b> is researched in detail, and the key technology and ways to achieve the data mining based on <b>neural</b> <b>networks</b> are also researched. Keywords—Data mining; <b>neural</b> <b>networks,</b> data mining process, implementation. I...|$|R
40|$|In modern {{software}} implementations {{of artificial}} <b>neural</b> <b>networks</b> the approach inspired by biology has {{more or less}} been abandoned for a more practical approach based on statistics and signal processing. In some of these systems, <b>neural</b> <b>networks,</b> or parts of <b>neural</b> <b>networks</b> (such as artificial neurons), are used as components in larger systems that combine both adaptive and non-adaptive elements. There are many problems which are solved with <b>neural</b> <b>networks,</b> especially in business and economic domains. Key words: neuron, <b>neural</b> <b>networks,</b> artificial intelligence, feed-forward <b>neural</b> <b>networks,</b> classificatio...|$|R
40|$|Recently, a new {{training}} algorithm, multigradient, {{has been}} published for <b>neural</b> <b>networks</b> and it is reported that the multigradient outperforms the backpropagation when <b>neural</b> <b>networks</b> are used as a classifier. When <b>neural</b> <b>networks</b> are used as an equalizer in communications, they {{can be viewed as}} a classifier. In this paper, we apply the multigradient algorithm to train the <b>neural</b> <b>networks</b> that are used as equalizers. Experiments show that the <b>neural</b> <b>networks</b> trained using the multigradient noticeably outperforms the <b>neural</b> <b>networks</b> trained by the backpropagation...|$|R
25|$|In neuroscience, the PageRank of a neuron in a <b>neural</b> <b>network</b> {{has been}} found to {{correlate}} with its relative firing rate.|$|E
25|$|Yang, J. and Honavar, V. (1999). DistAl: An Inter-Pattern Distance Based Constructive <b>Neural</b> <b>Network</b> Learning Algorithm.. Intelligent Data Analysis. Vol. 3. pp.55–73.|$|E
25|$|SpiNNaker is a many-core design {{combining}} traditional ARM architecture cores with {{an enhanced}} network fabric design specialised for simulating a large <b>neural</b> <b>network.</b>|$|E
40|$|Abstract—In recent years, fast <b>neural</b> <b>networks</b> for object/face {{detection}} {{have been}} introduced based on cross correlation in the frequency domain between the input matrix and the hidden weights of <b>neural</b> <b>networks.</b> In our previous papers [3, 4], fast <b>neural</b> <b>networks</b> for certain code detection was introduced. It was proved in [10] that for fast <b>neural</b> <b>networks</b> to give the same correct results as conventional <b>neural</b> <b>networks,</b> both the weights of <b>neural</b> <b>networks</b> and the input matrix must be symmetric. This condition made those fast <b>neural</b> <b>networks</b> slower than conventional <b>neural</b> <b>networks.</b> Another symmetric form for the input matrix was introduced in [1 - 9] {{to speed up the}} operation of these fast <b>neural</b> <b>networks.</b> Here, corrections for the cross correlation equations (given in [13, 15, 16]) to compensate for the symmetry condition are presented. After these corrections, it is proved mathematically that the number of computation steps required for fast <b>neural</b> <b>networks</b> is less than that needed by classical <b>neural</b> <b>networks.</b> Furthermore, {{there is no need for}} converting the input data into symmetric form. Moreover, such new idea is applied to increase the speed of <b>neural</b> <b>networks</b> in case of processing complex values. Simulation results after these corrections using MATLAB confirm the theoretical computations...|$|R
30|$|Recently, <b>neural</b> <b>networks</b> have {{attracted}} much attention due to their great potential prospectives in various areas, such as signal processing, associative memory, pattern recognition, and so on [1 – 18]. As a general class of recurrent <b>neural</b> <b>networks,</b> Cohen-Grossberg <b>neural</b> <b>networks,</b> including Hopfield <b>neural</b> <b>networks</b> and cellular <b>neural</b> <b>networks</b> as two special cases, were proposed by Cohen and Grossberg in 1983. From then on, many researchers have investigated this type of <b>neural</b> <b>networks</b> extensively [19 – 24]. In [19], the robust stability about the integer-order Cohen-Grossberg <b>neural</b> <b>networks</b> is explored based on the comparison principle. Liu et al. [20] investigate the multistability of Cohen-Grossberg <b>neural</b> <b>networks</b> with nonlinear activation functions in any open interval. In addition, the dynamic properties of Cohen-Grossberg <b>neural</b> <b>networks</b> can describe {{the evolution of the}} competition between species in living nature, where the equilibrium points stand for the survival or extinction of the species.|$|R
40|$|We present Forward Bipartite Alignment (FBA), {{a method}} that aligns the topological {{structures}} of two <b>neural</b> <b>networks.</b> <b>Neural</b> <b>networks</b> {{are considered to be}} a black box, because <b>neural</b> <b>networks</b> contain complex model surface determined by their weights that combine attributes non-linearly. Two networks that make similar predictions on training data may still generalize differently. FBA enables a diversity of applications, including visualization and canonicalization of <b>neural</b> <b>networks,</b> ensembles, and cross-over between unrelated <b>neural</b> <b>networks</b> in evolutionary optimization. We describe the FBA algorithm, and describe implementations for three applications: genetic algorithms, visualization, and ensembles. We demonstrate FBA 2 ̆ 7 s usefulness by comparing a bag of <b>neural</b> <b>networks</b> to a bag of FBA-aligned <b>neural</b> <b>networks.</b> We also show that aligning, and then combining two <b>neural</b> <b>networks</b> has no appreciable loss in accuracy which means that Forward Bipartite Alignment aligns <b>neural</b> <b>networks</b> in a meaningful way...|$|R
25|$|Neocognitron, a {{hierarchical}} multilayered <b>neural</b> <b>network</b> proposed by Professor Kunihiko Fukushima in 1987, {{is one of}} the first Deep Learning Neural Networks models.|$|E
25|$|A {{sufficiently}} {{complex and}} accurate {{model of the}} neurons is required. A traditional artificial <b>neural</b> <b>network</b> model, for example multi-layer perceptron network model, is not considered as sufficient. A dynamic spiking <b>neural</b> <b>network</b> model is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It {{is likely that the}} model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.|$|E
25|$|No <b>neural</b> <b>network</b> has solved such {{computationally}} {{difficult problems}} such as the n-Queens problem, the travelling salesman problem, or the problem of factoring large integers.|$|E
30|$|In {{the last}} decade, {{artificial}} <b>neural</b> <b>networks</b> {{have been used}} in various industries considerably [2]. <b>Neural</b> <b>networks</b> are nonlinear calculation algorithms for images, signals, and numerical data processing. Regarding some features of <b>neural</b> <b>networks</b> such as internal dynamic of <b>neural</b> <b>networks</b> in prediction, changing information error, unnecessary information in input data, employing <b>neural</b> <b>networks</b> in the engineering field as a tool to control and observe the process performance has been increased substantially. Multilayer perceptron {{is one of the most}} frequent structures which are utilized in <b>neural</b> <b>networks</b> [3].|$|R
40|$|Recently low {{displacement}} rank (LDR) matrices, or so-called structured matrices, {{have been}} proposed to compress large-scale <b>neural</b> <b>networks.</b> Empirical results have shown that <b>neural</b> <b>networks</b> with weight matrices of LDR matrices, referred as LDR <b>neural</b> <b>networks,</b> can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR <b>neural</b> <b>networks</b> with a mild condition on the displacement operators. We then show that the error bounds of LDR <b>neural</b> <b>networks</b> are as efficient as general <b>neural</b> <b>networks</b> with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR <b>neural</b> <b>networks.</b> Comment: 13 pages, 1 figur...|$|R
40|$|Two {{types of}} new {{recurrent}} RBF <b>neural</b> <b>networks</b> are introduced here and are applied on four test {{problems that are}} used for identification. The proposed recurrent RBF <b>neural</b> <b>networks</b> use both {{the power of the}} recurrent <b>neural</b> <b>networks</b> together with the abilities of the RBF <b>neural</b> <b>networks</b> so it can achieve good results...|$|R
25|$|Parekh, R., Yang, J., and Honavar, V. (2000). Constructive <b>Neural</b> <b>Network</b> Learning Algorithms for Multi-Category Pattern Classification. IEEE Transactions on Neural Networks. Vol. 11. No. 2. pp.436–451.|$|E
25|$|Judith Dayhoff has a Mathematical Biophysics PhD from University of Pennsylvania and is {{the author}} of <b>Neural</b> <b>network</b> architectures: An {{introduction}} and coauthor of Neural Networks and Pattern Recognition.|$|E
25|$|File:Single-layer {{feedforward}} artificial neural network.png|A single-layer feedforward artificial <b>neural</b> <b>network</b> with 4 inputs, 6 {{hidden and}} 2 outputs. Given position state and direction outputs wheel based control values.|$|E
40|$|Abstract. Echo State <b>neural</b> <b>networks</b> (ESN) {{belong to}} a newer {{perspective}} on architecture and training of recurrent <b>neural</b> <b>networks.</b> On the other hand, Finite impulse response (FIR) <b>neural</b> <b>networks</b> belong to mathematically well underlaid and time reliably proved feedforward <b>neural</b> <b>networks.</b> In this paper we compare and evaluate predictive abilities of these two different approaches to architecture and training of <b>neural</b> <b>networks.</b> Each of the mentioned <b>neural</b> <b>networks</b> can perceive the time context, but different means of their architectures are employed. Both approaches were tested in laser fluctuations and Mackey-glass data prediction, whereby we have obtained interesting results and conclusions...|$|R
40|$|Artificial <b>neural</b> <b>networks</b> {{are often}} {{understood}} as {{a good way to}} imitate mind through the web structure of neurons in brain, but the very high complexity of human brain prevents to consider <b>neural</b> <b>networks</b> as good models for human mind;anyway <b>neural</b> <b>networks</b> are good devices for computation in parallel. The difference between feed-forward and feedback <b>neural</b> <b>networks</b> is introduced; the Hopfield network and the multi-layers Perceptron are discussed. In a very weak isomorphism (not similitude) between brain and <b>neural</b> <b>networks,</b> an artificial form of short term memory and of acknowledgement, in Elman <b>neural</b> <b>networks,</b> is proposed...|$|R
50|$|Biological <b>neural</b> <b>networks</b> have {{inspired}} {{the design of}} artificial <b>neural</b> <b>networks.</b>|$|R
