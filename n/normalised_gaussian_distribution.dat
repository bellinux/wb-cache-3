0|10000|Public
5000|$|... #Caption: A {{comparison}} of <b>Gaussian</b> <b>distribution,</b> rectified <b>Gaussian</b> <b>distribution,</b> and truncated <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|Compounding a <b>Gaussian</b> <b>distribution</b> with mean {{distributed}} {{according to}} another <b>Gaussian</b> <b>distribution</b> yields (again) a <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|... is the <b>Gaussian</b> <b>distribution,</b> in {{this case}} {{specifically}} the multivariate <b>Gaussian</b> <b>distribution.</b>|$|R
5000|$|As λ {{tends to}} infinity, the inverse <b>Gaussian</b> <b>distribution</b> becomes {{more like a}} normal (<b>Gaussian)</b> <b>distribution.</b> The inverse <b>Gaussian</b> <b>distribution</b> has several {{properties}} analogous to a <b>Gaussian</b> <b>distribution.</b> The name can be misleading: it is an [...] "inverse" [...] only in that, while the Gaussian describes a Brownian Motion's level at a fixed time, the inverse <b>Gaussian</b> describes the <b>distribution</b> of the time a Brownian Motion with positive drift takes to reach a fixed positive level.|$|R
30|$|Among them, α is {{the update}} {{rate of the}} <b>Gaussian</b> <b>distribution</b> parameter, and the {{parameters}} remain unchanged for the <b>Gaussian</b> <b>distribution</b> with no matching success. In {{the establishment of the}} background model, we set the number of <b>Gaussian</b> <b>distributions</b> describing each pixel 3 [*]=[*]K. The background model is initialized first, and the initial weights are w 1, 0 [*]=[*] 1, w 2, 0 [*]=[*] 1, w 3, 0 [*]=[*] 1. The pixels of the first frame are used to initialize the first <b>Gaussian</b> <b>distribution</b> mean, and the mean of the remaining <b>Gaussian</b> <b>distribution</b> is 0. The standard deviation of each model takes a larger value σi, 0 [*]=[*] 30, a weight update rate β[*]=[*] 0.33, a learning rate α[*]=[*] 0.7, and a threshold value of 7.0 [*]=[*]T. If no <b>Gaussian</b> <b>distribution</b> is found to match the xt at the time of detection, then a <b>Gaussian</b> <b>distribution</b> with the lowest priority is removed, and a new <b>Gaussian</b> <b>distribution</b> is introduced according to xt, and a smaller weight and a larger variance are assigned, and then weight normalization is performed.|$|R
3000|$|The {{delay is}} drawn from a <b>Gaussian</b> <b>distribution</b> with its mean and {{variance}} given in the cluster parameters. Similarly, the angular parameters are drawn from a wrapped <b>Gaussian</b> <b>distribution</b> [37] (in the wrapped <b>Gaussian</b> <b>distribution,</b> all realisations are mapped to their principal value in [...]...|$|R
5000|$|Compounding a <b>Gaussian</b> <b>distribution</b> with mean {{distributed}} {{according to}} a shifted exponential distribution yields an exponentially modified <b>Gaussian</b> <b>distribution.</b>|$|R
3000|$|..., {{we first}} {{eliminate}} the <b>Gaussian</b> <b>distribution</b> whose existence probability {{is smaller than}} a given threshold τ. After pruning, the remaining <b>Gaussian</b> <b>distributions</b> and their existence probabilities are used as inputs for the next filtering recursion. We then select the <b>Gaussian</b> <b>distributions</b> with existence probabilities p [...]...|$|R
5000|$|In {{probability}} theory, the rectified <b>Gaussian</b> <b>distribution</b> is {{a modification}} of the <b>Gaussian</b> <b>distribution</b> when its negative elements are reset to 0 (analogous to an electronic rectifier). It is essentially a mixture of a discrete distribution (constant 0) and a continuous <b>distribution</b> (a truncated <b>Gaussian</b> <b>distribution</b> with interval [...] ).|$|R
40|$|Barndorff-Nielsen and Jørgensen (1989) have {{introduced}} some parametric models {{on the unit}} simplex. The distributions associated with these models have been obtained by conditioning on the sum of d independent generalized inverse Gaussian random variables. We use a constructive approach to derive some of these models by first mapping the inverse Gaussian law on (0, 1) and formally extending it on the unit simplex. This technique is then applied to a mixture-inverse <b>Gaussian</b> <b>distribution</b> studied recently by Jørgensen, Seshadri and Whitmore (1991). The distributions are then retransformed to yield two versions of a multidimensional inverse <b>Gaussian</b> <b>distribution.</b> Dirichlet distribution general exponential families generalized inverse <b>Gaussian</b> <b>distribution</b> inverse <b>Gaussian</b> <b>distribution</b> mixture-inverse <b>Gaussian</b> <b>distribution...</b>|$|R
5000|$|The inverse <b>Gaussian</b> and gamma <b>distributions</b> {{are special}} {{cases of the}} {{generalized}} inverse <b>Gaussian</b> <b>distribution</b> for p = −1/2 and b = 0, respectively. [...] Specifically, an inverse <b>Gaussian</b> <b>distribution</b> of the form ...|$|R
40|$|International audienceWe {{show that}} Wigner semi-circle law holds for Hermitian {{matrices}} with dependent entries, provided the deviation of the cumulants from the <b>normalised</b> <b>Gaussian</b> case obeys a simple power law bound {{in the size}} of the matrix. To establish this result, we use replicas interpreted as a zero-dimensional quantum field theoretical model whose effective potential obey a renormalisation group equation...|$|R
3000|$|... where g(.) is the {{probability}} density function (pdf) of the clipping noise which follows <b>Gaussian</b> <b>distribution</b> and Q(.) is the tail probability {{of the standard}} <b>Gaussian</b> <b>distribution.</b>|$|R
30|$|Note that (11) is {{a mixture}} of <b>Gaussian</b> <b>distribution.</b> The IMM {{algorithm}} approximates it by a <b>Gaussian</b> <b>distribution</b> with mean x̅_k- 1 ^j and covariance P̅_k- 1 ^j.|$|R
30|$|Both {{the white}} noise and noise with <b>Gaussian</b> <b>distribution</b> are examined. It is found that {{either in the}} nature {{background}} or system noise simulation, the <b>Gaussian</b> <b>distribution</b> constantly yields the smaller errors of the two velocities than the white noise does. Note that a small value of the half width in <b>Gaussian</b> <b>distribution,</b> the random function issues numbers about a constant, while the white noise function randomly gives the numbers. Therefore, the <b>Gaussian</b> <b>distribution</b> contributes similar amplitude of the noise fluctuations to the wave packet, which results in the smaller errors of the two velocities.|$|R
40|$|We {{show that}} Wigner semi-circle law holds for Hermitian {{matrices}} with dependent entries, provided the deviation of the cumulants from the <b>normalised</b> <b>Gaussian</b> case obeys a simple power law bound {{in the size}} of the matrix. To establish this result, we use replicas interpreted as a zero-dimensional quantum field theoretical model whose effective potential obey a renormalisation group equation. Comment: 9 pages, 4 figure...|$|R
40|$|This paper {{analyses}} the kernel density estimation on {{spaces of}} <b>Gaussian</b> <b>distributions</b> endowed with different metrics. Explicit expressions of kernels are {{provided for the}} case of the 2 -Wasserstein metric on multivariate <b>Gaussian</b> <b>distributions</b> and for the Fisher metric on multivariate centred distributions. Under the Fisher metric, the space of multivariate centred <b>Gaussian</b> <b>distributions</b> is isometric to the space of symmetric positive definite matrices under the affine-invariant metric and the space of univariate <b>Gaussian</b> <b>distributions</b> is isometric to the hyperbolic space. Thus kernel are also valid on these spaces. The density estimation is successfully applied to a classification problem of electro-encephalographic signals...|$|R
3000|$|... 0 at t= 0, has {{an inverse}} <b>Gaussian</b> <b>distribution.</b> In {{physical}} terms, if energetic particles are injected impulsively {{at a specific}} position in the lower corona and move outward at a constant drift rate accompanied by diffusion, the particles that escape at a specific distance in the upper corona have an inverse <b>Gaussian</b> <b>distribution</b> in time. Therefore, we adopt the inverse <b>Gaussian</b> <b>distribution</b> as the particle injection time profile.|$|R
40|$|Variance {{estimation}} and ranking {{methods are}} developed for stochastic processes modeled by <b>Gaussian</b> mbcture <b>distributions.</b> It is {{shown that the}} variance estimate from a <b>Gaussian</b> mixture <b>distribution</b> has the same properties as a variance estimate from a single <b>Gaussian</b> <b>distribution</b> based on a reduced number of samples. Hence, well known tools of variance estimation and ranking of single <b>Gaussian</b> <b>distributions</b> {{can be applied to}} <b>Gaussian</b> mixture <b>distributions.</b> As an application example, optimization of sensor processing order in the sequential multi-target multi-sensor joint probabilistic data associ- ation (MSJPDA) algorithm is presented...|$|R
40|$|The paper {{focuses on}} the {{influences}} of some factors significant to pyrolysis of forestry biomass on the asymptotic solution of the non-isothermal nth-order distribution energy model (DAEM) using <b>Gaussian</b> <b>distribution.</b> Investigated parameters are the integral upper limit, the frequency factor, and the heating rate parameters of the <b>Gaussian</b> <b>distribution.</b> The influence of these factors {{has been used for}} evaluating the kinetic parameters of the non-isothermal nth-order <b>Gaussian</b> <b>distribution</b> from thermogravimetric analysis of forest waste...|$|R
40|$|There {{are several}} {{problems}} {{where it is}} very important to know whether the tested data are distributed according to the Gaussian law. At the detection of the hidden information within the digitized pictures (steganography), one of the key factors is the analysis of the noise contained in the picture. The incorporated noise should show the typically <b>Gaussian</b> <b>distribution.</b> The departure from the <b>Gaussian</b> <b>distribution</b> might be the first hint that the picture has been changed – possibly new information has been inserted. In such cases the fast <b>Gaussian</b> <b>distribution</b> test is a very valuable tool. The article describes the phase of the noise (in the picture) extraction and the distribution formation. The second phase of the noise analysis is performed by the neural network. The neural network is trained to recognize the <b>Gaussian</b> <b>distribution</b> of the noise. The trained neural network successfully performs the fast <b>Gaussian</b> <b>distribution</b> test...|$|R
3000|$|This {{stems from}} the fact that the gamma <b>distribution</b> {{approaches}} a <b>Gaussian</b> <b>distribution</b> if the degree of freedom, here 2 NM, is large [18]. Furthermore, the <b>Gaussian</b> <b>distribution</b> tends toward a Dirac delta function as its variance, here 2 NM(E [...]...|$|R
50|$|Hidden Markov {{models can}} model complex Markov {{processes}} where the states emit the observations {{according to some}} probability distribution. One such example is the <b>Gaussian</b> <b>distribution,</b> in such a Hidden Markov Model the states output are represented by a <b>Gaussian</b> <b>distribution.</b>|$|R
40|$|Description This package {{provides}} {{functions for}} the hyperbolic and related distributions. Density, distribution and quantile functions and random number generation are {{provided for the}} hyperbolic distribution, the generalized hyperbolic distribution, the generalized inverse <b>Gaussian</b> <b>distribution</b> and the skew-Laplace distribution. Additional functionality is provided for the hyperbolic <b>distribution,</b> normal inverse <b>Gaussian</b> <b>distribution</b> and generalized inverse <b>Gaussian</b> <b>distribution,</b> including fitting of these distributions to data. Linear models with hyperbolic errors may be fitted using hyperblmFit. License GPL (> = 2...|$|R
3000|$|The {{impulsiveness}} of {{the noise}} can be quantified by its peakedness (measured in units [...] "decibels relative to Gaussian" [...] (dBG) [6]), {{defined in terms of}} kurtosis[21] in relation to the kurtosis of the <b>Gaussian</b> (aka normal) <b>distribution.</b> By definition, the <b>Gaussian</b> <b>distribution</b> has zero dBG peakedness. Impulsive noise would thus have a higher peakedness than the <b>Gaussian</b> <b>distribution</b> (positive dBG). In time domain, high peakedness means a higher occurrence of outliers. In terms of the amplitude distribution of the signal, positive dBG peakedness translates into 'heavier tails' than those of the <b>Gaussian</b> <b>distribution.</b>|$|R
40|$|Different from {{previous}} work that measured robustness its own distribution, measuring robustness with a robust estimator on a generalized <b>Gaussian</b> <b>distribution</b> is introduced here. In detail, an unbiased Maximum Likelihood (ML) variance estimator and a robust variance estimator of the <b>Gaussian</b> <b>distribution</b> with a given censoring value {{are applied to}} the generalized <b>Gaussian</b> <b>distribution</b> that represents <b>Gaussian,</b> Laplace, and Cauchy distributions; then, Mean Square Error (MSE) is calculated to measure robustness. Afterward, how robustness changes is shown because the actual distribution varies over the generalized <b>Gaussian</b> <b>distribution.</b> The results indicate that measuring the MSE of the system {{can be used to}} point out how robust the system is when the system distribution changes...|$|R
40|$|International audienceWe {{study the}} {{optimization}} of a continuous function by its stochastic relaxation, i. e., the optimization {{of the expected}} value of the function itself {{with respect to a}} density in a statistical model. We focus on gradient descent techniques applied to models from the exponential family and in particular on the multivariate <b>Gaussian</b> <b>distribution.</b> From the theory of the exponential family, we reparametrize the <b>Gaussian</b> <b>distribution</b> using natural and expectation parameters, and we derive formulas for natural gradients in both parameterizations. We discuss some advantages of the natural parameterization for the identification of sub-models in the <b>Gaussian</b> <b>distribution</b> based on conditional independence assumptions among variables. <b>Gaussian</b> <b>distributions</b> are widely used in stochastic optimization and in particular in model-based Evolutionary Computation, as in Estimation of Distribution Algorithms and Evolutionary Strategies. By studying natural gradient flows over <b>Gaussian</b> <b>distributions</b> our analysis and results directly apply to the study of CMA-ES and NES algorithm...|$|R
30|$|We {{evaluate}} {{the performance of}} NB to classify the states of unobserved nodes as well as Naive Bayes using kernel density estimation (NBK); kernel density estimation uses multiple (<b>Gaussian)</b> <b>distributions,</b> and is generally more effective than using a single (<b>Gaussian)</b> <b>distribution</b> [1].|$|R
40|$|We give a new {{characterization}} of inverse <b>Gaussian</b> <b>distributions</b> using the regression of a suitable statistic {{based on a}} given random sample. A corollary of this result is a {{characterization of}} inverse <b>Gaussian</b> <b>distribution</b> based on a conditional joint density function of the sample. Application of this corollary as a transformation in the procedure to construct EDF (empirical distribution function) goodness-of-fit tests for inverse <b>Gaussian</b> <b>distributions</b> is also studied. 2000 Mathematics Subject Classification: 62 E 10, 62 F 03. 1. Introduction. ...|$|R
3000|$|... that {{contributes}} to express the output probability distribution b(o|g,c). The output probability b(o|g,c) is simply approximated by a <b>Gaussian</b> <b>distribution.</b> This <b>Gaussian</b> <b>distribution</b> uses a unique context-independent covariance matrix Σ and a context-dependent mean vector. The mean component is obtained by linearly combining μ [...]...|$|R
50|$|In {{the third}} chapter, Taleb {{introduces}} {{the concepts of}} Extremistan and Mediocristan. He uses them as guides to define how predictable the environment one's studying is. Mediocristan environments safely can use <b>Gaussian</b> <b>distribution.</b> In Extremistan environments, a <b>Gaussian</b> <b>distribution</b> is used at one's peril.|$|R
5000|$|This {{distribution}} {{results from}} compounding a <b>Gaussian</b> <b>distribution</b> (normal distribution) with mean [...] and unknown variance, with an inverse gamma distribution {{placed over the}} variance with parameters [...] and [...] In other words, the random variable X is assumed to have a <b>Gaussian</b> <b>distribution</b> with an unknown variance distributed as inverse gamma, and then the variance is marginalized out (integrated out). The reason for the usefulness of this characterization is that the inverse gamma distribution is the conjugate prior distribution of the variance of a <b>Gaussian</b> <b>distribution.</b> As a result, the non-standardized Students t-distribution arises naturally in many Bayesian inference problems. See below.|$|R
40|$|The Normal-Inverse <b>Gaussian</b> <b>distribution</b> arises as a Normal variance-mean {{mixture with}} an Inverse <b>Gaussian</b> mixing <b>distribution.</b> This article deals with Maximum Likelihood {{estimation}} of {{the parameters of the}} Normal-Inverse <b>Gaussian</b> <b>distribution.</b> Due to the complexity of the likelihood, direct maximization is difficult. An EM type algorithm is provided for the Maximum Likelihood {{estimation of the}} Normal-Inverse <b>Gaussian</b> <b>distribution.</b> This algorithm overcomes numerical difficulties occurring when standard numerical techniques are used. An application to a data set concerning the general index of the Athens Stock Exchange is given. Some operating characteristics of the algorithm are discussed. Scale normal mixtures Financial data Hyperbolic distributions Heavy tailed distributions...|$|R
40|$|This thesis {{analyzes}} how the Inverse <b>Gaussian</b> <b>distribution</b> {{compares to}} other statistical distributions for fitting flow data from Newfoundland rivers. Although the 2 -parameter Inverse <b>Gaussian</b> <b>distribution</b> is as flexible as most 3 -parameter distributions, {{it is not}} currently used for statistical analysis of hydrologic data. Theories are available for regression analysis as well as hypothesis testing {{that is based on}} the Inverse Gaussian assumption. [...] A hydrological study of Newfoundland rivers was performed to assess application of the Inverse <b>Gaussian</b> <b>distribution</b> to flow and regional analysis techniques. For high flows, the Inverse <b>Gaussian</b> <b>distribution</b> was compared to the Generalized Extreme Value (GEV), the 3 -Parameter Lognormal (3 PLN), the Extreme Value (EV) and the Lognormal (LN) distributions. For low flows, comparison was made to the 3 -Parameter Weibull (W 3), 2 -Parameter Weibull (W 2), Extreme Value (EV) and the Lognormal (LN) distributions. [...] The analysis confirmed the Inverse <b>Gaussian</b> <b>distribution</b> is a suitable candidate for flood analysis of high flows. However, the distribution does not perform well for low flow analysis. Using the Akaike Information Criterion (AIC) as an indicator of suitability, the Inverse <b>Gaussian</b> <b>distribution</b> is significantly better than both the 2 -parameter and 3 -parameter distributions considered in this analysis for high flows. The results of the study include regional flood frequency curves based on the Inverse <b>Gaussian</b> <b>distribution</b> for two distinct regions within the island of Newfoundland. These curves are suitable for use in addition to and for comparison with other regional flood analysis techniques for Newfoundland streamflow data. [...] There are two main recommendations derived from this study. The first is to apply the Inverse <b>Gaussian</b> <b>distribution</b> to streamflow data in areas other than the island of Newfoundland. Such application of the distribution to both high and low flows of other areas would determine if the suitability for high flows, or the inappropriateness for low flows, is limited to Newfoundland streamflow data. The second main recommendation is to develop an approximation for the inverse of the Inverse <b>Gaussian</b> <b>distribution,</b> similar to the approximation for the more popular <b>Gaussian,</b> or Normal <b>distribution.</b> Such an approximation would enhance the use and capabilities of the Inverse <b>Gaussian</b> <b>distribution</b> in flood frequency analysis, as well as many other statistical applications where the inverse of this robust distribution is required...|$|R
5000|$|It is inefficient (37% efficiency) at <b>Gaussian</b> <b>distributions.</b>|$|R
5000|$|Scaling {{the data}} to a <b>Gaussian</b> <b>distribution</b> (Gaussianization).|$|R
