4|7|Public
40|$|Abstract — Encoder–decoder {{design is}} {{considered}} for a closedloop scalar control system with feedback transmitted over a binary symmetric channel. We propose an iterative procedure which can jointly optimize adaptive encoder–decoder pairs for a certainty equivalence controller. The goal is to minimize a design criterion, in particular, the linear quadratic (LQ) cost function over a finite horizon. The algorithm leads to a practically feasible design of time-varying <b>non-uniform</b> <b>encoding</b> and decoding. Numerical results demonstrate the promising performance obtained by employing the proposed iterative optimization algorithm. I...|$|E
40|$|Abstract. AUV global path {{planning}} often achieves by genetic algorithm, due to genetic algorithm’s local search ability is limited, resulting in two limitations: {{large amounts of}} algorithmic operations to meet the expected effect; there are some peaks after the inheritance in the paths. Thus we propose <b>non-uniform</b> <b>encoding</b> obstacle avoidance operator node deletion operator and smoothing operator. The result proves that the improved Genetic Algorithm finds a comparatively better path, achieves rapid and smooth global {{path planning}} by avoiding obstacle deleting redundant nodes and smooth operation...|$|E
40|$|We study a {{closed-loop}} {{multivariable control}} system with sensor feedback transmitted over a discrete noisy channel. For this problem, we propose a joint {{design of the}} state measurement quantization, protection against channel errors, and control. The proposed algorithm leads to a practically feasible design of time-varying <b>non-uniform</b> <b>encoding</b> and control. Numerical results demonstrate the performance obtained by employing the proposed iterative optimization algorithm. © 2007 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. QC 2012021...|$|E
50|$|This {{standard}} was previously known as KS C 5601. There {{have been several}} revisions of this standard. For example, there were revisions in 1987, 1992, 1998 and 2002.Several computer operating systems encode various versions of this standard several ways. Not all of them encode the standard the same way, like replacing the typical backslash at byte 0x5C with the won currency sign (₩).Some operating systems extend this standard in other <b>non-uniform</b> ways. Possible <b>encoding</b> schemes of KS X 1001 are: EUC-KR, Windows-949 (superset of EUC-KR), ISO-2022-KR and JOHAB. However, the latter two encodings are rarely used.|$|R
40|$|In this letter, we {{consider}} the lossy coding of a non-uniform binary source based on GF(q) -quantized low-density generator matrix (LDGM) codes with check degree dc= 2. By quantizing the GF(q) LDGM codeword, a non-uniform binary codeword can be obtained, which is suitable for direct quantization of the <b>non-uniform</b> binary source. <b>Encoding</b> is performed by reinforced belief propagation, a variant of belief propagation. Simulation {{results show that the}} performance of our method is quite close to the theoretic rate-distortion bounds. For example, when the GF(16) -LDGM code with a rate of 0. 4 and block-length of 1, 500 is used to compress the non-uniform binary source with probability of 1 being 0. 23, the distortion is 0. 091, which is very close to the optimal theoretical value of 0. 074...|$|R
40|$|The rapid {{progress}} {{of computers and}} today's heterogeneous computing environment means computation-intensive signal processing algorithms must be optimized for performance in a machine dependent fashion at an algorithmic level. In this paper, we present a dynamic memory model and associated optimization framework that finds a near-optimal algorithm within a defined search space of algorithms by exploiting the computation- memory tradeoff. By optimal, we mean an algorithm that has the fasting running given the specification of the machine memory hierarchy. We found four successful instantiation of the framework: fast VLC decoding, fast IP address lookup, fast <b>non-uniform</b> scalar quantizer <b>encoding,</b> and fast unconstrained vector quantizer encoding. Experiments show that all four instantiations of framework outperform competing techniques in the literature. 1 Introduction If the computer evolution has matured to a stage where computers are ubiquitous and homogeneous, and improvem [...] ...|$|R
40|$|Encoder-decoder {{design is}} {{considered}} for a closed-loop scalar control system with feedback transmitted over a binary symmetric channel. We propose an iterative procedure which can jointly optimize adaptive encoder-decoder pairs for a certainly equivalence controller. The goal is to minimize a design criterion, in particular, the linear quadratic (LQ) cost function over a finite horizon. The algorithm leads to a practically feasible design of time-varying <b>non-uniform</b> <b>encoding</b> and decoding. Numerical results demonstrate the promising performance obtained by employing the proposed iterative optimization algorithm. © 2006 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. QC 2012021...|$|E
30|$|It must be {{mentioned}} here that although efficient, the MS-VQs {{used in this}} study are not the best quantizers available. For instance, we have not used fully optimized (i.e., using treillis search as in [30]) MS-VQ, but basic (i.e., sequential search) MS-VQ. Also, more sophisticated frame-wise methods have been proposed to obtain transparent LSF quantization at rates lower than the ones required for MS-VQ, but at the cost of increased complexity [35, 36]. Refined versions of split-VQ are also good candidates for improved performances. We restricted ourselves with a relatively simple VQ technique because the goal {{of the present study was}} primarily to show the interest of the long-term approach. Therefore, it is very likely that the performances of the proposed LT coding algorithm can be significantly improved by using high-performance (but more complex) quantizers, 13 since the reduced set of LSF vectors may be quantized with lower ASD/resolution compared to the MS-VQ. In contrast, it seems very difficult to improve the performances of the reference 2 D-transform methods, since we used optimal (<b>non-uniform)</b> quantizers to <b>encode</b> the corresponding 2 D coefficients.|$|R
40|$|Optimal (minimum cost) binary prefix {{codes for}} {{infinite}} sources with geometrically distributed frequencies, e. g., P = {pi(1 - p) }i= 0 ∞<, 0 < p < 1, were first (implicitly) suggested by Golomb {{over thirty years}} ago in the context of run-length encodings. Ten years later Gallager and Van Voorhis exhibited such optimal codes for all values of p. Just recently Merhav, Seroussi and Weinberger extended this further to find optimal binary prefix codes for two-sided geometric distributions. These codes were derived by cleverly "guessing" optimal codes for finite sources, validating these guesses by using the sibling property of Huffman encoding, and then showing that the finite codes converge in a very specific sense to an optimal infinite one. In this thesis we describe the first algorithmic approach to constructing optimal prefix infinite codes. Our approach is to define an infinite weighted graph with the property that the least cost infinite path in the graph corresponds to the optimal code. We then show that even though the graph is infinite, the least-cost infinite path has a repetitive structure and that it is therefore possible to not only find this path but to find it relatively efficiently. This approach will work for even more complicated generalizations of geometric sources where solutions cannot be guessed as well as in extensions of Huffman-coding for which the Huffman algorithm no longer works, e. g., <b>non-uniform</b> cost <b>encoding</b> alphabet characters and/or other restrictions on the codewords. We illustrate our approach by deriving an algorithm for constructing optimal prefix free codes with a geometric source for the telegraph channel. We also implement our algorithm and show what the constructed codes look like in this case...|$|R
40|$|JPEG 2000 Part 10 is a {{new work}} part of the ISO/IEC JPEG Committee dealing with the {{extension}} of JPEG 2000 technologies to three-dimensional data. One of the issues in Part 10 {{is the ability to}} <b>encode</b> <b>non-uniform</b> data grids having variable resolution across its domain. Some parts of the grid can be more finely sampled than others in accordance with some prespecified criteria. Of particular interest to the scientific and engineering communities are variable resolution grids resulting from a process of adaptive mesh refinement of the grid cells. This paper presents the technologies that are currently being developed to accommodate this Part 10 requirement. The coding of adaptive mesh refinement grids with JPEG 2000 works as a two step process. In the first pass, the grid is scanned and its refinement structure is entropy coded. In the second pass, the grid samples are wavelet transformed and quantized. The difference with Part 1 is that wavelet transformation must be done over regions of irregular shape. Results will be shown for adaptive refinement grids with cell-centered or corner-centered samples. It will be shown how the Part 10 coding of an adaptive refinement grid is backwards compatible with a Part 1 decoder...|$|R
40|$|Sorted data {{is usually}} easier to {{compress}} than unsorted permutations {{of the same}} data. This motivates a simple compression scheme: specify the sorted permutation of the data along with {{a representation of the}} sorted data compressed recursively. The sorted permutation can be specified by recording the decisions made by quicksort. If the size of the data is known, then the quicksort decisions describe the data at a rate that is nearly as efficient as the minimal prefix-free code for the distribution, which is bounded by the entropy of the distribution. This is possible even though the distribution is unknown ahead of time. Used in this way, quicksort acts as a universal code in that it is asymptotically optimal for any stationary source. The Shannon entropy is a lower bound when describing stochastic, independent symbols. However, it is possible to <b>encode</b> <b>non-uniform,</b> finite strings below the entropy of the sample distribution by also encoding symbol counts because the values in the sequence are no longer independent once the counts are known. The key insight is that sparse quicksort comparison vectors can also be compressed to achieve an even lower rate when data is highly non-uniform while incurring only a modest penalty when data is random. Comment: preprint, compression by sorting, quicksort as universal code; this version describes the permutation vector and its invers...|$|R

