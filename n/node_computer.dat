20|458|Public
50|$|The {{components}} of a cluster are usually connected to each other through fast local area networks, with each <b>node</b> (<b>computer</b> used as a server) running its own instance of an operating system. In most circumstances, all of the nodes use the same hardware and the same operating system, although in some setups (i.e. using Open Source Cluster Application Resources (OSCAR)), different operating systems {{can be used on}} each computer, or different hardware.|$|E
5000|$|At {{the center}} of each cluster was a star coupler, to which every <b>node</b> (<b>computer)</b> and data storage device in the cluster was {{connected}} by one or two pairs of CI cables. ("CI" [...] stands for Computer Interconnect.) Each pair of cables had a transmission rate of 70 megabits per second, a high speed for that era. Using two pairs gave an aggregate transmission rate of 140 megabits per second, with redundancy in case one cable failed; the star couplers also had redundant wiring for better availability.|$|E
5000|$|Within a vast {{computer}} network, {{the individual}} computers {{on the periphery}} of the network, those that do not also connect other networks, and those that often connect transiently to one or more clouds are called end nodes. Typically, within the cloud computing construct, the individual user / customer computer that connects into one well-managed cloud is called an end node. Since these computers are a part of the network yet unmanaged by the cloud's host, they present significant risks to the entire cloud. This is called the End Node Problem. [...] There are several means to remedy this problem but all require instilling trust in the end <b>node</b> <b>computer.</b>|$|E
40|$|Distributed {{computer}} architectures are well {{accepted in the}} domain of real-time applications. To realize fault-tolerance, fail-silent <b>node</b> <b>computers</b> providing the same service can be clustered into Fault-Tolerant Units (FTUs). Each FTU provides a specified service as long as at least one of its <b>node</b> <b>computers</b> is operational. The communication between these FTUs has to be deterministic, reliable and timely, i. e. there must be a tight upper bound on {{the time it takes to}} send a message from one FTU to the other FTUs. This paper presents a communication system suitable for real-time applications that meets these requirements...|$|R
5000|$|Many {{companies}} issue typical laptops {{and only}} allow those specific computers to remotely connect. For example, the US Department of Defense only allows its remote computers to connect via VPN to its network (no direct Internet browsing) and uses two-factor authentication. [...] Some organizations use server-side tools to scan and/or validate the end <b>node's</b> <b>computer,</b> such as {{communicating with the}} node's TPM.|$|R
40|$|A {{number of}} routing {{protocols}} {{have been proposed}} for mobile ad hoc networks. In this {{paper we propose a}} hybrid multiple zoning scheme minimizing the number of route request messages and nodes involved in the route discovery process by applying proactive routing between high power nodes and reactive routing between other <b>nodes.</b> <b>Computer</b> simulation using NS 2 reveals that the proposed approach can significantly lower routing overhead compared to previous schemes. Also, the improvement gets more significant as the number of nodes in the network increases...|$|R
50|$|In {{local area}} {{networks}} with a star topology, each network host {{is connected to}} a central hub with a point-to-point connection. So {{it can be said}} that every computer is indirectly connected to every other node with the help of the hub. In Star topology, every <b>node</b> (<b>computer</b> workstation or any other peripheral) is connected to a central node called hub, router or switch. The switch is the server and the peripherals are the clients. The network does not necessarily have to resemble a star to be classified as a star network, but all of the nodes on the network must be connected to one central device. All traffic that traverses the network passes through the central hub. The hub acts as a signal repeater. The star topology is considered the easiest topology to design and implement. An advantage of the star topology is the simplicity of adding additional nodes. The primary disadvantage of the star topology is that the hub represents a single point of failure. Since all peripheral communication must flow through the central hub, the aggregate central bandwidth forms a network bottleneck for large clusters.|$|E
40|$|A {{high speed}} MOS digital circuit {{simulation}} program PNAP- 1 (Parallel Network Analysis Program) implemented on a special-purpose parallel computer system LINKS- 1 is described. In LINKS- 1, 64 node computers are linked radially to a root computer {{through a very}} fast memory swapping unit. The task of the root computer is to control <b>node</b> <b>computer</b> and to store waveforms. Each <b>node</b> <b>computer</b> solves assigned subcircuit with standard circuit simulation techniques. PNAP- 1 adopts a block-type waveform relaxation technique as a vehicle of the parallel computation of required waveforms. The slow convergence or more serious non-convergence problem inherent to relaxation-based methods is alleviated by analyzing tightly coupled portions in direct manner in conjunction with exploiting the overall quasi-unidirectional property of MOS digital circuits. A number of experimental results show that PNAP- 1 can achieve a substantial speed improvement in the cost-effective analysis of VLSI circuits...|$|E
30|$|In {{terms of}} {{execution}} times, Hadoop {{proved to be}} a reliable distributed computing platform. The execution times recorded in the experiment showed impressive improvements as the number of nodes increased. We achieved almost a speedup of six when a 19 -node cluster was in place compared to a single <b>node</b> <b>computer.</b> In general, we obtained a linear speedup as we added more computing nodes in the cluster. The overall processing times also showed exponential decrease across the clusters. Besides that, all cluster resources were utilized during feature extraction. The results provided a strong evidence that more resources can be put to use to further shorten the processing times.|$|E
40|$|Learning {{decision}} trees against {{very large}} amounts of data is not practical on single <b>node</b> <b>computers</b> due to the huge amount of calculations required by this process. Apache Hadoop is a large scale distributed computing platform that runs on commodity hardware clusters {{and can be used}} successfully for data mining task against very large datasets. This work presents a parallel decision tree learning algorithm expressed in MapReduce programming model that runs on Apache Hadoop platform and has a very good scalability with dataset size...|$|R
50|$|System {{information}} for <b>computer</b> <b>nodes</b> and clusters.|$|R
40|$|Consider a {{communication}} medium shared among {{a set of}} computer nodes; these <b>computer</b> <b>nodes</b> issue messages that are requested to be transmitted and they must finish their transmission before their respective deadlines. TDMA/SS is a protocol that solves this problem; it is {{a specific type of}} Time Division Multiple Access (TDMA) where a <b>computer</b> <b>node</b> is allowed to skip its time slot and then this time slot can be used by another <b>computer</b> <b>node.</b> We present an algorithm that computes exact queuing times for TDMA/SS in conjunction with Rate-Monotonic (RM) or Earliest- Deadline-First (EDF) ...|$|R
40|$|In this paper, we {{consider}} a multihop {{wireless sensor network}} (WSN) with multiple relay nodes for each hop where the amplify-and-forward (AF) scheme is employed. We present a strategy to jointly design the linear receiver and the power allocation parameters via an alternating optimization approach that maximizes the sum rate of the WSN. We derive constrained maximum sum-rate (MSR) expressions along with an algorithm to compute the linear receiver and the power allocation parameters with the optimal complex amplification coefficients for each relay <b>node.</b> <b>Computer</b> simulations show good performance of our proposed methods in terms of sum rate compared to the method with equal power allocation. Comment: 3 figure...|$|E
40|$|In {{numerous}} {{contexts and}} environments, {{it is necessary}} to identify and assign (potential) experts to subject fields. In the context of an academic journal for computer science (J. UCS), papers and reviewers are classified using the ACM classification scheme. This paper describes a system to identify and present potential reviewers for each category from the entire body of paper’s authors. The topical classification hierarchy is visualized as a hyperbolic tree and currently assigned reviewers are listed for a selected <b>node</b> (<b>computer</b> science category). In addition, a spiral visualization is used to overlay a ranked list of further potential reviewers (high-profile authors) around the currently selected category. This new interface eases the task of journal editors in finding and assigning reviewers. The system is also useful for users who want to find research collaborators in specific research areas...|$|E
40|$|We present DCOOL-NET, a {{scalable}} distributed in-network algorithm for {{sensor network}} localization based on noisy range measurements. DCOOL-NET operates by parallel, collaborative message passing between single-hop neighbor sensors, and involves simple computations at each node. It stems from {{an application of}} the majorization-minimization (MM) framework to the nonconvex optimization problem at hand, and capitalizes on a novel convex majorizer. The proposed majorizer is endowed with several desirable properties and represents a key contribution of this work. It is a more accurate match to the underlying nonconvex cost function than popular MM quadratic majorizers, and is readily amenable to distributed minimization via the alternating direction method of multipliers (ADMM). Moreover, it allows for low-complexity, fast Nesterov gradient methods to tackle the ADMM subproblems induced at each <b>node.</b> <b>Computer</b> simulations show that DCOOL-NET achieves comparable or better sensor position accuracies than a state-of-art method which, furthermore, is not parallel...|$|E
40|$|In this paper, we {{describe}} an effective method of using Self-Organizing Map (SOM) to group websites {{so as to}} eliminate or at least ease up slow speed, {{one of the fundamental}} problems, by using a MapReduce programming model. The proposed MapReduce SOM algorithm has been successfully applied to cluB, which is a typical SOM tool. Performance evaluation shows the proposed SOM algorithm took less time to complete computational processing (i. e. distributed computing) on large data sets in comparison with conventional algorithms, and performance improved by up to 20 percent with increasing <b>nodes</b> (<b>computers)</b> ...|$|R
30|$|To {{implement}} the proposed approach, MySQL database in Ubuntu has been used. Ubuntu is installed over VMware and all involved <b>nodes</b> (<b>computers)</b> are configured with heterogeneity. Factors of heterogeneity are allocated CPU power, allocated RAM and disk space and network bandwidth. To test and generate report python {{is used as}} scripting language. The test bed comprise of 90 nodes for the distributed multi-tenant data base. The processing capability ranges from 500 MHz to 2.4 GHz for processor, 500 MB to 1500 MB for RAM and 10 - 30 GB of free disk space as shown in Table 4.|$|R
40|$|Common {{techniques}} to realize fault-tolerance in distributed real-time systems are the replication of components (<b>node</b> <b>computers)</b> and the interconnection of these components by redundant communication channels. The {{communication between the}} components has to be reliable and predictable in its timing behavior. We describe a communication protocol which is suitable for synchronous distributed architectures. It provides reliable, deterministic, and timely multicast (one-to-many) communication between components {{which is based on}} sending of redundant broadcast messages. This paper presents the results of a study of the reliability of the communication taking into account failures in the communication network and components, as well as their on-line repair...|$|R
40|$|We {{discuss the}} work of the QCDSP {{collaboration}} to build an inexpensive Teraflop scale massively parallel computer suitable for computations in Quantum Chromodynamics (QCD). The computer is a collection of nodes connected in a four dimensional toroidial grid with nearest neighbor bit serial communications. A node is composed of a Texas Instruments Digital Signal Processor (DSP), memory, and a custom made communications and memory controller chip. An 8192 <b>node</b> <b>computer</b> with a peak speed of 0. 4 Teraflops is being constructed at Columbia University for a cost of $ 1. 8 Million. A 12, 288 -node machine with a peak speed of 0. 6 Teraflops is being constructed for the RIKEN Brookhaven Research Center. Other computers have been built including a 50 Gigaflop version for Florida State University. Keywords: parallel, supercomputer, digital signal processor, QCD Introduction The atoms and nuclei of everyday matter are now known to be made up of still tinier particles known as quarks and leptons. [...] ...|$|E
40|$|We {{propose a}} method for the optimal {{scheduling}} of collective data exchanges relying on {{the knowledge of the}} underlying network topology. We introduce the concept of liquid schedules. Liquid schedules ensure the maximal utilization of a network's bottleneck links and offer an aggregate throughput as high as the flow capacity of a liquid in a network of pipes. The collective communication throughput offered by liquid schedules in highly loaded networks may be several times higher than the throughput of topology-unaware techniques. To create a liquid schedule we need to find the smallest partition of all transfers into subsets of mutually non-congesting transfers. The number of combinations of non-overlapping subsets of mutually non-congesting transfers grows exponentially with the number of transfers. We propose several methods to reduce the search space without affecting the solution space. On a real 32 <b>node</b> <b>computer</b> cluster, the measured throughputs of data exchanges scheduled according to our method are very close to the theoretical liquid throughputs. Keywords: Optimal network utilization, collective data exchange, liquid schedules, network topology, topologyaware scheduling...|$|E
40|$|Our flrtt effort {{went into}} {{designing}} and analyzing the the hy percube network aa a msrrage patrlng twitch. We almed at maklng a VLSI chlpr, {{one for each}} node, which should store and forward the mesaages. Message patrlng have some negative effects: Message delays are both unpredlctable and tlgnlflcant, nodes {{have to be able}} to store messages, thereby lncreaslng chip tlze, and finally It was dlrcovered during tlmulatton tests that deadlocks occurred rather frequently. Later a remedy for deadlock was found, but the other negatlve lndlcatort are ttlll valid. The result: we abandoned the message passlng method and turned to line twltchlng. Still there thould be one chip In each node. The hypercube line rwltchlng node has one communlcatlon path to each neighbor (D lines In a D dlmenslonal cube) and two paths to the <b>node</b> <b>computer.</b> The chip should be able to partlclpate In decentralized routing through the cube, and when a connection It establlthed: hold a path through the node. An outllne of the node functlont will be given. A line rwltchlng hypercube network wltl have enormous data tranrmlsslon capacity. In a D dimensional cube with N = 2 ” nodes, at most N channels can be active at the same time. (Remember that each <b>node</b> <b>computer</b> has both an Input port and an output port). 10 MB/set data transfer rate on each channel Is well withln reach. The crltlcal operatlon It to ettabllsh a path through the hypercube. Set up time varies with network rlze and load, clock frequency and actual path through the network. Wlth the tame clock frequency as already mentloned, the normal set up time under light load condltlons Is from 1 to 3 microseconds. An analytls of the path setup process has been carried out. The focu: has been on the probability of belng able to ertabllth a path under a glven load condition. The load is Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specfic permission...|$|E
5000|$|A {{complex system}} is usually {{composed}} of many components and their interactions. Such a {{system can be}} represented by a network where nodes represent the components and links represent their interactions. for example, the INTERNET can be represented as a network composed of <b>nodes</b> (<b>computers)</b> and links (direct connections between computers). Its resilience to failures was studied using percolation theory in.Other examples are social networks, airline networks, biological networks and climate networks.Networks can also fail and recover spontaneously. For modeling this phenomenon see ref.Interacting complex systems can be modeled as networks of networks. For their breakdown and recovery properties see ...|$|R
5000|$|Over 650 <b>computer</b> <b>nodes</b> {{including}} specialist Macs and 7 discreet ICT rooms.|$|R
5000|$|Large scale {{combinatorial}} searches: Keys being candidate {{solutions to}} a problem and each key mapping to the <b>node,</b> or <b>computer,</b> {{that is responsible for}} evaluating them as a solution or not. e.g. Code Breaking ...|$|R
30|$|Hadoop is an {{open source}} {{distributed}} computing platform developed by Apache Software Foundation. It is built to handle large data sets (or big data) on distributed systems (DS). DS {{is a group of}} connected computing nodes (computers) which may be geographically dispersed. By using Hadoop, the workload of a process is spread across the nodes of a DS. Hence, the completion time is reduced [20]. Sarade et al. [13] proposed the implementation of image processing in distributed computing by using Hadoop. They defined the workflow of the system to consist of the following steps: (i) images being uploaded into the Hadoop Distributed File System (HDFS) (ii) image processing through MapReduce (iii) presentation of results. The paper focused on the processing of satellite images particularly on image scaling, feature extraction and image recognition. Similar experimental setup was used in [20] where multiple sets of Hadoop cluster were formed to observe the execution time as the number of computing nodes changed. The results showed that a traditional single <b>node</b> <b>computer</b> took 1967  s to process 800 images while a Hadoop cluster containing 10 nodes took only 253  s. Besides reducing execution times through distributed computing, Hadoop also supports distributed machine learning through Apache Mahout [5].|$|E
40|$|A given {{piece of}} data can be {{represented}} {{in a number of}} ways, and the choice depends on what task should be performed on that data. It is well-known in computer science that if an appropriate data structure is chosen, the algorithms to be used to carry out the task often become relatively obvious. data structure called tree. A tree such as (1) consists of a finite set of nodes and a finite set of edges connecting them with the following two properties: (i) there is one node, called the root, that dominates all the other nodes and (ii) every node other than the root has exactly one node that immediately dominates it. (1) V W X a Y Z b c Since some ordering is almost inevitably imposed on the subtrees of each <b>node,</b> <b>computer</b> science adopts trees that have one more property: (iii) the nodes each node immediately dominates are ordered linearly. The kind of tree adopted widely in generative research of natural languages has property (iii). Moreover, it crucially distinguishes terminal and Miyoko Yasui 2 non-terminal nodes; only the former are subject to PF-interpretation. Because of property (iii), the task of PF-interpretation is trivial. Take (2) for example...|$|E
40|$|Distributed {{integrated}} architectures in {{the automotive}} and avionic domain result in hardware cost reduction, dependability improvements, and improved coordination between application subsystems compared to federated systems. In {{order to support}} safety-critical application subsystems, a distributed integrated architecture needs to support fault-tolerance strategies that enable the continued operation of the system {{in the presence of}} failures. The basis for the implementation and validation of faulttolerance strategies are realistic fault assumptions, which are captured in a fault hypothesis. This paper describes a fault hypothesis for distributed integrated architectures, which takes into account the sharing of the communication and computational resources of a single distributed computer system among multiple application subsystems. Each <b>node</b> <b>computer</b> serves for the execution of multiple jobs. In analogy, the communication network interconnecting the node computers has to support message exchanges of more than one application subsystem. Using a generic system model of a distributed integrated architecture, we argue in favor of a differentiation of fault containment regions for hardware and software faults. Based on these fault containment regions, we discuss the failure modes, the failure rates, the maximum number of failures, and the recovery intervals. In particular, the fault hypothesis describes the assumptions concerning the respective frequencies of transient and permanent failures in consideration of recent semiconductor trends...|$|E
30|$|Traditionally, the {{networks}} were formed using homogenous <b>nodes</b> (<b>computers).</b> However, {{due to the}} emergence of 5 G and the Internet of Things (IoT) paradigms, heterogeneous networks comprising heterogeneous nodes such as sensors, phones, computers and satellites form, thereby providing various ubiquitous services. These wireless ad hoc networks are fully applicable in the field of the IoT. In the IoT, virtual objects, services, processes and devices are considered as nodes that are interconnected through the Internet. Soon, the IoT will amalgamate different technologies wirelessly in which ad hoc networks will play an integral part. Examples of such systems are smart cities, the Internet of connected vehicles (intelligent transportation system), etc. [4, 5].|$|R
50|$|The Tianhe-1A {{system is}} {{composed}} of 112 computer cabinets, 12 storage cabinets, 6 communications cabinets, and 8 I/O cabinets. Each computer cabinet {{is composed of}} four frames, with each frame containing eight blades, plus a 16-port switching board. Each blade is composed of two <b>computer</b> <b>nodes,</b> with each <b>computer</b> <b>node</b> containing two Xeon X5670 6-core processors and one Nvidia M2050 GPU processor. The system has 3584 total blades containing 7168 GPUs, and 14,336 CPUs, managed by the SLURM job scheduler. The total disk storage of the systems is 2 Petabytes implemented as a Lustre clustered file system, and the total memory size {{of the system is}} 262 Terabytes.|$|R
50|$|When {{the company}} {{launched}} and evolved its technology, {{there were no}} off-the-shelf computing systems and integrated software and sound cards. Consequently, all of the hardware from the company's main real-time CPU, all input and output cards, analog-to-digital and digital-to-analog cards {{and all of its}} memory cards were all developed internally, as well as all of the software, certainly a monumental task. The hardware and software of the company's real-time capability was used in other fields completely remote to music, such as the main Dartmouth College campus computing <b>node</b> <b>computers</b> for one of the USA's first campus-wide computing networks, and in medical data acquisition research projects.|$|R
40|$|The {{connection}} of ad-hoc networks to the Internet is typically established via gateways. To start an Internet connection, {{in a first}} step gateways have to be discovered by the mobile nodes within the ad-hoc cluster. Several gateway selection schemes have been proposed that select gateway nodes based on a single Quality of Service parameter, link capacity, link quality, path availability period, or end-to-end delay. Each scheme just focuses on the meant of improve only a single network performance, i. e., network throughput, packet delivery ratio, end-to-end delay, or packet drop ratio. However, none of these schemes improves the overall network performance because they focus on a single Quality of Service path parameter or on set of non- Quality of Service parameters. To improve the overall network performance, {{it is necessary to}} select a gateway with stable path, packet loss rate between two neighbor node should be minimum, a path with the maximum residual load capacity and the minimum latency. In this paper, we propose a gateway selection scheme that considers multiple Quality of Service path parameters such as path availability period, available capacity latency and link quality to select a potential gateway <b>node.</b> <b>Computer</b> simulations show that our gateway selection scheme improves throughput and packet delivery ratio with less per node energy consumption. It also improves the end-to-end delay compared to single Quality of Service path parameter gateway selection schemes...|$|E
40|$|Abstract-The {{connection}} of ad-hoc networks to the Internet is typically established via gateways. To start an Internet connection, {{in a first}} step gateways have to be discovered by the mobile nodes within the ad-hoc cluster. Several gateway selection schemes have been proposed that select gateway nodes based on a single Quality of Service parameter, link capacity, link quality, path availability period, or end-to-end delay. Each scheme just focuses on the meant of improve only a single network performance, i. e., network throughput, packet delivery ratio, end-to-end delay, or packet drop ratio. However, none of these schemes improves the overall network performance because they focus on a single Quality of Service path parameter or on set of non- Quality of Service parameters. To improve the overall network performance, {{it is necessary to}} select a gateway with stable path, packet loss rate between two neighbor node should be minimum, a path with the maximum residual load capacity and the minimum latency. In this paper, we propose a gateway selection scheme that considers multiple Quality of Service path parameters such as path availability period, available capacity latency and link quality to select a potential gateway <b>node.</b> <b>Computer</b> simulations show that our gateway selection scheme improves throughput and packet delivery ratio with less per node energy consumption. It also improves the end-to-end delay compared to single Quality of Service path parameter gateway selection schemes. Keywords- Manet, Quality of Service, Gateway Discovery. 1...|$|E
40|$|Knowing that {{an object}} {{does not belong}} to an {{authorized}} set of objects is an important step in computer system defense. Dr. Stephanie Forrest of the University of New Mexico compared the process of computer system defense to the process used by living organisms to defend against diseases, viruses and other foreign agents. Dr. Forrest 2 ̆ 7 s thesis was to develop a methodology for identifying the self to use intrusion detection to detect non-self-agents. An alternative to this external view is a system that contains its own self-defense mechanism. The project proposed that an internal function could be used to differentiate between self and non-self-objects by creating unique identifiers for computer systems as the human DNA differentiates individuals. This research developed the DNA Self-Defense Methodology where implementation would insert identification data into an object that will identify the object uniquely to the operating system on which it resides. This identification data, denoted as the DNA Pattern, will serve to create a unique copy of the object and create an ownership token between the object and the operating system. The research project then focused on developing an instantiation of the methodology for single <b>node</b> <b>computer</b> systems. Additionally, a proof of concept system was developed to test the functionality of certain features of the methodology. The results of the test demonstrated that, given additional research, practical application of the methodology is feasible...|$|E
40|$|We {{present a}} simple {{algorithm}} for constructing Hamiltonian cycles on trees together with additional edges between leaves, compute {{the number of}} such additional edges required for complete k-ary trees of given depth, show this number is minimal over all Hamiltonian cycles, and present an algorithm that determines the successor of each node in the cycle from the node index alone. We discuss the applications of these algorithms to parallel computing. 1 Introduction Many parallel algorithms need to transfer data in parallel from each <b>node</b> of the <b>computer</b> on which they are running to each other <b>node</b> of the <b>computer</b> in turn. Standard examples are algorithms to compute the cartesian product of two sets, each initially distributed over the <b>nodes</b> of the <b>computer,</b> or the join of two relations, each similarly initially distributed. Such algorithms must transfer data along a Hamiltonian cycle through the <b>nodes</b> of the <b>computer.</b> The questions then arise: which common interconnection topologies for [...] ...|$|R
30|$|We have {{developed}} a software-based coincidence processor that should have adequate performance to process list-mode singles data at near real-time provided that we have enough <b>computer</b> <b>nodes.</b> At a singles data rate of approximately 150 Mcps (equivalent to a 370 -MBq injection evenly distributed in an adult phantom), this can be achieved with 2 <b>computer</b> <b>nodes</b> running dual Intel Xeon E 5 - 2650 v 4 CPUs.|$|R
40|$|In most {{application}} {{scenarios of}} {{wireless sensor networks}} (WSN), sensor nodes are usually deployed randomly {{and do not have}} any knowledge about the network environment or even their ID's at the initial stage of their operations. In this paper, we address the clustering problems with a newly deployed multi-hop WSN where most existing clustering algorithms can hardly be used due to the absence of MAC link connections among the nodes. We propose an effective clustering algorithm based on a random contention model without the prior knowledge of the network and the ID's of <b>nodes.</b> <b>Computer</b> simulations have been used to show the effectiveness of the algorithm with a relatively low complexity if compared with existing schemes...|$|R
