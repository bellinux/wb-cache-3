5223|2385|Public
5|$|Due to Python's {{extensive}} mathematics library, and the third-party library NumPy {{that further}} extends the native capabilities, it is frequently {{used as a}} scientific scripting language to aid in problems such as <b>numerical</b> <b>data</b> processing and manipulation.|$|E
5|$|He {{also sought}} the {{opinions}} {{of many of the}} astronomers to whom he had sent Mysterium, among them Reimarus Ursus (Nicolaus Reimers Bär)—the imperial mathematician to Rudolph II and a bitter rival of Tycho Brahe. Ursus did not reply directly, but republished Kepler's flattering letter to pursue his priority dispute over (what is now called) the Tychonic system with Tycho. Despite this black mark, Tycho also began corresponding with Kepler, starting with a harsh but legitimate critique of Kepler's system; among a host of objections, Tycho took issue with the use of inaccurate <b>numerical</b> <b>data</b> taken from Copernicus. Through their letters, Tycho and Kepler discussed a broad range of astronomical problems, dwelling on lunar phenomena and Copernican theory (particularly its theological viability). But without the significantly more accurate data of Tycho's observatory, Kepler had no way to address many of these issues.|$|E
25|$|Setting (nyāsa/sthāpanā) of the <b>numerical</b> <b>data.</b>|$|E
50|$|<b>Numerical</b> {{univariate}} <b>data</b> consist {{observations that}} are numbers. They are obtained using either interval or ratio scale of measurement. This type of univariate {{data can be}} classified even further into two subcategories: discrete and continuous. A <b>numerical</b> univariate <b>data</b> is discrete if the set of all possible values is finite or countably infinite. Discrete univariate data are usually associated with counting (such {{as the number of}} books read by a person). A <b>numerical</b> univariate <b>data</b> is continuous if the set of all possible values is an interval of numbers. Continuous univariate data are usually associated with measuring (such as the weights of people).|$|R
50|$|Trilinear {{interpolation}} {{is frequently}} used in <b>numerical</b> analysis, <b>data</b> analysis, and computer graphics.|$|R
5000|$|<b>Numerical</b> {{simulation}} <b>data</b> [...] (such as Computational {{fluid dynamics}} or Finite element analysis data) ...|$|R
25|$|In 1518 another Castilian {{opinion was}} {{provided}} by Martin Fernandez de Enciso. Harrisse concluded that Enciso placed his line at 47°24'W on his sphere (7.7% smaller than ours), but at 45°38'W on our sphere using Enciso's <b>numerical</b> <b>data.</b> Enciso also described the coastal features near which the line passed in a very confused manner. Harrisse concluded from this description that Enciso's line could also be {{near the mouth of}} the Amazon between 49° and 50°W.|$|E
25|$|Veblen {{and other}} American institutionalists were {{indebted to the}} German Historical School, {{especially}} Gustav von Schmoller, for the emphasis on historical fact, their empiricism and especially a broad, evolutionary framework of study. Veblen admired Schmoller, but criticized some other leaders of the German school because of their overreliance on descriptions, long displays of <b>numerical</b> <b>data</b> and narratives of industrial development that rested on no underlying economic theory. Veblen tried {{to use the same}} approach with his own theory added.|$|E
25|$|The IMO Performance Standards for radar {{to provide}} a plan display with an {{effective}} display diameter of 180mm, 250mm, or 340mm depending upon the gross tonnage of the vessel. With the diameter parameters already chosen, the manufacturer has then {{to decide how to}} arrange the placement of the digital <b>numerical</b> <b>data</b> and control status indicators. The raster-scan display makes it easier for design engineers in the way auxiliary data can be written.raster from azimuth information digitized.|$|E
5000|$|Exam deliveryOutput {{to paper}} or {{electronic}} format examinationAdherence to standardsImport and export of <b>numerical</b> results <b>data</b> and metadata ...|$|R
40|$|Abstract. Numerical {{simulation}} of natural phenomena is being fostered by {{recent advances in}} powerful high processing computing platforms. Scientists in various areas, such as human cardiovascular system, model a phenomenon being studied {{through a set of}} mathematical equations. As scientists strive to obtain a more realistic simulation, a huge amount of data is produced. Unfortunately, there has been little work on supporting <b>numerical</b> simulation <b>data</b> management, which leaves simulation scientists with huge standard text files and complex analytical programs that eventually extract some meaningful information to validate scientific hypotheses. Moreover, some analytical queries cannot, as well, be represented using none of the scientific query languages. In this context, this paper tries to bridge this gap by raising some issues involved in <b>numerical</b> simulation <b>data</b> analysis. A representation for <b>numerical</b> simulation <b>data</b> is presented that considers a multidimensional model, for dimensional variables, and their corresponding physical quantities. A cloud service to interface with the <b>numerical</b> simulation <b>data</b> manager is proposed and its integration with the Neblina cloud middleware is explored. ...|$|R
5000|$|... maximizing synergies between {{different}} fields contributing to planetary sciences: space observations, earth-based observations, laboratory studies, <b>numerical</b> simulations, <b>data</b> base development; ...|$|R
25|$|For example, an 1871 census in the UK (the {{first of}} its kind, but {{personal}} data from other censuses dates back to 1841 and <b>numerical</b> <b>data</b> back to 1801) found the average male life expectancy as being 44, but if infant mortality is subtracted, males who lived to adulthood averaged 75 years. The present life expectancy in the UK is 77 years for males and 81 for females, while the United States averages 74 for males and 80 for females.|$|E
25|$|Several {{types of}} orders {{can be defined}} from <b>numerical</b> <b>data</b> on the items of the order: a total order results from {{attaching}} distinct real numbers to each item and using the numerical comparisons to order the items; instead, if distinct items are allowed to have equal numerical scores, one obtains a strict weak ordering. Requiring two scores to be separated by a fixed threshold before they may be compared leads {{to the concept of}} a semiorder, while allowing the threshold to vary on a per-item basis produces an interval order.|$|E
25|$|In the 2009 {{test of the}} Programme for International Student Assessment (PISA), a {{worldwide}} evaluation of 15-year-old school pupils' scholastic performance by the OECD, Chinese students from Shanghai achieved the best results in mathematics, science and reading. The OECD also found that even {{in some of the}} very poor rural areas the performance is close to the OECD average. However, controversy has surrounded the high scores achieved by the Chinese students due to the unusual spread of the <b>numerical</b> <b>data,</b> with suggestions that schools were 'gaming' students for the exams. Also while averages across the breadth of other countries are reported, China's rankings are taken from only a few select districts.|$|E
40|$|Abstract. Ontology {{has become}} {{increasingly}} important to software sys-tems. The aim of ontology learning is to ease {{one of the major}} problems in ontology engineering, i. e. the cost of ontology construction. Much of the effort within the ontology learning community has focused on learn-ing from text collections. However, environmental domains often deal with <b>numerical</b> measurement <b>data</b> and, therefore, rely on methods and tools for learning beyond text. We discuss this characteristic using two relations of an ontology for lakes. Specifically, we learn a threshold value from <b>numerical</b> measurement <b>data</b> for ontological rules that classify lakes according to nutrient status. We describe our methodology, highlight the cyclical interaction between data mining and ontologies, and note that the numerical value for lake nutrient status is specific to a spatial and temporal context. The use case suggests that learning from <b>numerical</b> measurement <b>data</b> is a research area relevant to environmental software systems...|$|R
40|$|Part 4 : Semantics and EnvironmentInternational audienceOntology {{has become}} {{increasingly}} important to software systems. The aim of ontology learning is to ease {{one of the major}} problems in ontology engineering, i. e. the cost of ontology construction. Much of the effort within the ontology learning community has focused on learning from text collections. However, environmental domains often deal with <b>numerical</b> measurement <b>data</b> and, therefore, rely on methods and tools for learning beyond text. We discuss this characteristic using two relations of an ontology for lakes. Specifically, we learn a threshold value from <b>numerical</b> measurement <b>data</b> for ontological rules that classify lakes according to nutrient status. We describe our methodology, highlight the cyclical interaction between data mining and ontologies, and note that the numerical value for lake nutrient status is specific to a spatial and temporal context. The use case suggests that learning from <b>numerical</b> measurement <b>data</b> is a research area relevant to environmental software systems...|$|R
40|$|The {{performance}} of the Sub Grid Scale models is studied by simulating a separated flow over a wavy channel. The first and second order statistical moments of the resolved velocities obtained by using Large-Eddy simulations at different mesh resolutions are compared with Direct <b>Numerical</b> Simulations <b>data.</b> The effectiveness of modeling the wall stresses by using local log-law is then tested on a relatively coarse grid. The results exhibit a good agreement between highly-resolved Large Eddy Simulations and Direct <b>Numerical</b> Simulations <b>data</b> regardless the Sub Grid Scale models. However, the agreement is less satisfactory with relatively coarse grid without using any wall models and the differences between Sub Grid Scale models are distinguishable. Using local wall model retuned the basic flow topology and reduced significantly {{the differences between the}} coarse meshed Large-Eddy Simulations and Direct <b>Numerical</b> Simulations <b>data.</b> The results show that the ability of local wall model to predict the separation zone depends strongly on its implementation way...|$|R
500|$|The use of {{mathematics}} {{in the service}} of social and economic analysis dates back to the 17th century. [...] Then, mainly in German universities, a style of instruction emerged which dealt specifically with detailed presentation of data as it related to public administration. [...] Gottfried Achenwall lectured in this fashion, coining the term statistics. [...] At the same time, a small group of professors in England established a method of [...] "reasoning by figures upon things relating to government" [...] and referred to this practice as Political Arithmetick. [...] Sir William Petty wrote at length on issues that would later concern economists, such as taxation, Velocity of money and national income, but while his analysis was numerical, he rejected abstract mathematical methodology. [...] Petty's use of detailed <b>numerical</b> <b>data</b> (along with John Graunt) would influence statisticians and economists for some time, even though Petty's works were largely ignored by English scholars.|$|E
500|$|Conceptual {{development}} for Shadow Dragon and the Blade of Light began in 1987, three {{years prior to}} its release. The concept was first decided upon {{after the completion of}} Famicom Wars, as the team wanted {{to move away from the}} war-based setting of Famicom Wars and create a role-playing experience. The project was first proposed to Nintendo by Kaga with a design document. The document included all the basic elements, including the story, main character and gameplay mechanics. At this stage, the project was called [...] "Battle Fantasy Fire Emblem". The staff never considered the game as a commercial product, being defined by Kaga as a dōjin project that was made on a whim. To make the game accessible to a wide audience, Kaga did his best to avoid emphasizing stats and other <b>numerical</b> <b>data.</b> The game's genre necessitated the extensive use of the Famicom cartridge's memory. Shadow Dragon and the Blade of Light exceeded these limits, so Intelligent Systems used a portion of memory dedicated to saving games to get round this limitation. With Nintendo's help, they created a new chip for the cartridge that could process and display Japanese text.|$|E
500|$|In {{the sixth}} chapter, [...] "Written spells and charms", Merrifield {{discusses}} {{the use of}} the written word in magical contexts. Highlighting archaeological examples from the ancient Graeco-Roman world, he looks at inscriptions on lead tablets that were buried in cemeteries and amphitheatres, both places associated with the dead. Moving on to the use of magic squares, Merrifield highlights various examples of the Sator square in archaeological contexts, before also discussing squares that contained <b>numerical</b> <b>data</b> with astrological significance. He rounds off the chapter with an examination of Post-Medieval curses and charms containing the written word, citing examples that have been found by archaeologists across Britain, hidden inside various parts of buildings. In the seventh chapter, [...] "Charms against witchcraft", he deals with archaeological evidence for a variety of Early Modern and Modern British spells designed to ward off malevolent witchcraft. After briefly discussing the role of holed stone charms, he looks at the evidence for witch bottles, making reference to their relation to beliefs about witches' familiars. Proceeding to focus on 19th- and 20th-century examples, Merrifield discusses the case of James Murrell, an English cunning man, and his involvement with the witch bottle tradition. Merrifield's final chapter, [...] "The ritual of superstition: recognition and potential for study", provides an overview of the entire book, highlighting the evidence of ritual continuity from pre-Christian periods to the present day. Pointing out what he sees as areas of further exploration for archaeologists, he calls for [...] "systematic investigation" [...] of the subject.|$|E
50|$|Input {{data are}} the descriptive, textual {{information}} (address or building name) which the user wants {{to turn into}} <b>numerical,</b> spatial <b>data</b> (latitude and longitude) — {{through the process of}} geocoding.|$|R
5000|$|... {{development}} of <b>numerical</b> methods of <b>data</b> {{processing of results}} of biological experiments; ...|$|R
25|$|HSL and HSV are {{sometimes}} {{used to define}} gradients for data visualization, as in maps or medical images. For example, the popular GIS program ArcGIS historically applied customizable HSV-based gradients to <b>numerical</b> geographical <b>data.</b>|$|R
2500|$|Subspace and {{correlation}} based {{techniques for}} high-dimensional <b>numerical</b> <b>data</b> ...|$|E
2500|$|Some {{members of}} the {{academic}} community began urging the use of SMW on Wikipedia since it was first proposed. In a 2006 paper, Max Völkel et al. wrote {{that in spite of}} Wikipedia's utility, [...] "its contents are barely machine-interpretable. Structural knowledge, e.g. about how concepts are interrelated, can neither be formally stated nor automatically processed. Also the wealth of <b>numerical</b> <b>data</b> is only available as plain text and thus can not be processed by its actual meaning." ...|$|E
2500|$|From their {{beginnings}} in Sumer (now Iraq) around 3500 BC, the Mesopotamian {{people began}} to attempt to record some observations {{of the world with}} <b>numerical</b> <b>data.</b> But their observations and measurements were seemingly taken for purposes other than for elucidating scientific laws. A concrete instance of Pythagoras' law was recorded, as early as the 18th century BC: the Mesopotamian cuneiform tablet Plimpton 322 records a number of Pythagorean triplets (3,4,5) (5,12,13). ..., dated 1900 BC, possibly millennia before Pythagoras, but an abstract formulation of the Pythagorean theorem was not.|$|E
2500|$|GPL Licensed {{multilanguage}} (VBA, C++, Pascal, etc.) <b>numerical</b> {{analysis and}} <b>data</b> processing library.|$|R
5000|$|Amar Bose {{believed}} that traditional measures of audio equipment are {{not relevant to}} perceived audio quality and therefore did not publish the specifications for Bose products, claiming that the ultimate test was the listener's perception of audio quality according to the listener's preferences. In 1968, Bose presented a paper to the Audio Engineering Society titled [...] "On the Design, Measurement and Evaluation of Loudspeakers". In this paper, he rejected <b>numerical</b> test <b>data</b> in favor of [...] "more meaningful measurement and evaluation procedures". This is still the company's philosophy. Many other audio product manufacturers publish <b>numerical</b> test <b>data</b> of their equipment, but Bose does not.|$|R
40|$|The CERA {{metadata}} {{model is}} suitable to describe climate and environmental <b>data,</b> <b>numerical</b> model <b>data</b> {{as well as}} observational data. The metadata system can be extended to incorporate new types of data like satellite data. It matches international standards namely DIF and INFOKLIMA. The CERA system was designed as simply as possible to match the concept of storing data according to the semantic context and as flexibly as necessary to fulfil the requirements of semantic data retrieval and scientific dataprocessing. (orig.) SIGLEAvailable from TIB Hannover / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
2500|$|In the 1990s and {{the early}} decades of the 21st century, statistics-based {{approaches}} to machine learning used techniques related to economics and statistics to allow machines to [...] "guess" [...] to make inexact, probabilistic decisions and predictions based on experience and learning. These programs simulate the way our unconscious instincts are able to perceive, notice anomalies and make quick judgements, similar to what Dreyfus called [...] "sizing up the situation and reacting", but here the [...] "situation" [...] consists of vast amounts of <b>numerical</b> <b>data.</b> These techniques are highly successful and are currently widely used in both industry and academia.|$|E
2500|$|New Media as the Mix Between Existing Cultural Conventions and the Conventions of Software New Media {{today can}} be {{understood}} as the mix between older cultural conventions for data representation, access, and manipulation and newer conventions of data representation, access, and manipulation. The [...] "old" [...] data are representations of visual reality and human experience, and the [...] "new" [...] data is <b>numerical</b> <b>data.</b> The computer is kept out of the key [...] "creative" [...] decisions, and is delegated to the position of a technician. e.g. In film, software is used in some areas of production, in others are created using computer animation.|$|E
2500|$|A {{histogram}} is {{an accurate}} graphical representation {{of the distribution of}} <b>numerical</b> <b>data.</b> It is an estimate of the probability distribution of a continuous variable (quantitative variable) and was first introduced by Karl Pearson. It is a kind of bar graph. To construct a histogram, {{the first step is to}} [...] "bin" [...] the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. [...] The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but are not required to be) of equal size.|$|E
30|$|We thank two {{anonymous}} reviewers whose comments helped {{to improve and}} refine the manuscript significantly. We used the freeware program “Kashmir 3 D” (Sugimoto 2000), and National Land <b>Numerical</b> Information <b>data</b> provided by the Geospatial Information Authority of Japan, to produce Fig.  1.|$|R
40|$|A {{relativistic}} {{extension of}} our pseudo-shifted ℓ-expansion technique {{is presented to}} solve for the eigenvalues of Dirac and Klein-Gordon equations. Once more we show the numerical usefulness of its results via comparison with available <b>numerical</b> integration <b>data.</b> Comment: 20 pages, RevTeX file, revise...|$|R
40|$|Numerical agent models {{often include}} {{a number of}} {{parameters}}. The values of such parameters are usually determined by using some numerical parameter tuning method based on <b>numerical</b> empirical <b>data.</b> However, in many cases no <b>numerical</b> empirical <b>data</b> are available, but properties for dynamic patterns are known that should be fulfilled, as requirements. Classical numerical parameter tuning methods normally cannot work with such dynamic properties, as they can only be true or false. To remedy this, in this paper the notion of approximate satisfaction of dynamic properties is introduced. It adds a numerical measure to the logical notion of satisfaction. By doing this, numerical optimization methods for parameter estimation become applicable to support the design of dynamic agent models for which dynamic properties have been specified as requirements...|$|R
