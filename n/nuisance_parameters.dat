1208|489|Public
25|$|For maximum {{likelihood}} estimations, a model {{may have a}} number of <b>nuisance</b> <b>parameters.</b> For the asymptotic behaviour outlined to hold, the number of <b>nuisance</b> <b>parameters</b> should not increase with the number of observations (the sample size). A well-known example of this case is where observations occur as pairs, where the observations in each pair have a different (unknown) mean but otherwise the observations are independent and normally distributed with a common variance. Here for 2N observations, there are Nnbsp&+nbsp&1 parameters. It is well known that the {{maximum likelihood}} estimate for the variance does not converge to the true value of the variance.|$|E
50|$|The {{difference}} between Barnard's exact test and Fisher's exact test {{is how they}} handle the <b>nuisance</b> <b>parameter(s)</b> of the common success probability when calculating the p-value. Fisher's test avoids estimating the <b>nuisance</b> <b>parameter(s)</b> by conditioning on the margins, an approximately ancillary statistic. Barnard's test considers all possible values of the <b>nuisance</b> <b>parameter(s)</b> and chooses the value(s) that maximizes the p-value.|$|E
50|$|Sometimes it is {{possible}} to find a sufficient statistic for the <b>nuisance</b> <b>parameters,</b> and conditioning on this statistic results in a likelihood which does not depend on the <b>nuisance</b> <b>parameters.</b>|$|E
40|$|Lazar & Mykland (1999) {{showed that}} an {{empirical}} likelihood defined by two estimating equations with a <b>nuisance</b> <b>parameter</b> {{need not be}} Bartlett-correctable. This paper shows that Bartlett correction of empirical likelihood {{in the presence of}} a <b>nuisance</b> <b>parameter</b> depends critically on the way the <b>nuisance</b> <b>parameter</b> is removed when formulating the likelihood for the parameter of interest. We establish in the broad framework of estimating functions that the empirical likelihood is still Bartlett-correctable if the <b>nuisance</b> <b>parameter</b> is profiled out given the value of the parameter of interest. Copyright 2006, Oxford University Press. ...|$|R
40|$|In this article, {{a simple}} {{algorithm}} {{is used to}} maximize a family of optimal statistics for hypothesis testing with a <b>nuisance</b> <b>parameter</b> not defined under the null hypothesis. This arises from genetic linkage and association studies and other hypothesis testing problems. The maximum of optimal statistics over the <b>nuisance</b> <b>parameter</b> space {{can be used as}} a robust test in this situation. Here, we use the maximum and minimum statistics to examine the sensitivity of testing results with respect to the unknown <b>nuisance</b> <b>parameter.</b> Examples from genetic linkage analysis using affected sub pairs and a candidate-gene association study in case-parents trio design are studied. Genetic Analysis, Maximal Statistics, <b>Nuisance</b> <b>Parameter,</b> Robust Test,...|$|R
40|$|Lazar and Mykland (1999) {{presented}} a case which indicated that an empirical likelihood de ned by two estimating equations with a <b>nuisance</b> <b>parameter</b> was not Bartlett correctable. This was a surprising result considering the established cases of Bartlett correction for em-pirical likelihood. This paper shows that Bartlett correction of empirical likelihood {{in the presence}} of a <b>nuisance</b> <b>parameter</b> depends critically on the way the <b>nuisance</b> <b>parameter</b> is removed when formulating the likelihood for the parameter of interest. We show in the broad framework of estimating functions that the empirical likelihood is still Bartlett correctable if the <b>nuisance</b> <b>parameter</b> is proled out given the value of the interested parameter...|$|R
50|$|In statistics, an {{adaptive}} estimator is an estimator in a parametric or semiparametric model with <b>nuisance</b> <b>parameters</b> {{such that the}} presence of these <b>nuisance</b> <b>parameters</b> does not affect efficiency of estimation.|$|E
5000|$|... #Subtitle level 2: Likelihoods that {{eliminate}} <b>nuisance</b> <b>parameters</b> ...|$|E
5000|$|These {{approaches}} are useful because standard likelihood methods can become unreliable or fail entirely {{when there are}} many <b>nuisance</b> <b>parameters</b> or when the <b>nuisance</b> <b>parameters</b> are high-dimensional. This is particularly true when the <b>nuisance</b> <b>parameters</b> can {{be considered to be}} [...] "missing data"; they represent a non-negligible fraction of the number of observations and this fraction does not decrease when the sample size increases. Often these approaches can be used to derive closed-form formulae for statistical tests when direct use of maximum likelihood requires iterative numerical methods. These approaches find application in some specialized topics such as sequential analysis.|$|E
50|$|In statistics, a <b>nuisance</b> <b>{{parameter}}</b> is any parameter {{which is}} not of immediate interest but which must be accounted for {{in the analysis of}} those parameters which are of interest. The classic example of a <b>nuisance</b> <b>parameter</b> is the variance, σ2, of a normal distribution, when the mean, μ, is of primary interest.|$|R
40|$|Godambe's (1960) {{criterion}} of optimality for estimating equations is {{extended to the}} case where prior knowledge is given. The optimality of the estimating equation d£(9) /dO = 0, where £(6) is the posterior density, is shown under certain assumptions. The <b>nuisance</b> <b>parameter</b> case is also analysed. Some key words: Estimating equations; Information; <b>Nuisance</b> <b>parameter.</b> 1...|$|R
40|$|We {{present a}} new and simple method for {{constructing}} a 1 -[alpha] upper confidence limit for [theta] {{in the presence of}} a <b>nuisance</b> <b>parameter</b> vector [psi], when the data is discrete. Our method is based on computing a P-value P{T[less-than-or-equals, slant]t} from an estimator T of [theta], replacing the <b>nuisance</b> <b>parameter</b> by the profile maximum likelihood estimate for [theta] known, and equating to [alpha]. We provide a theoretical result which suggests that, from the point of view of coverage accuracy, this is close to the optimal replacement for the <b>nuisance</b> <b>parameter.</b> We also consider in detail limits for the (i) slope parameter of a simple linear logistic regression, (ii) odds ratio in two-way tables, (iii) ratio of means for two Poisson variables. In all these examples the coverage performance of our upper limit is a dramatic improvement on the coverage performance of the standard approximate upper limits considered. Upper confidence limit Profile maximum likelihood estimator Coverage error <b>Nuisance</b> <b>parameter...</b>|$|R
5000|$|... #Subtitle level 3: The {{case of a}} {{likelihood}} with <b>nuisance</b> <b>parameters</b> ...|$|E
5000|$|... {{credible}} intervals {{and confidence}} intervals treat <b>nuisance</b> <b>parameters</b> in radically different ways.|$|E
50|$|Practical {{approaches}} to statistical analysis treat <b>nuisance</b> <b>parameters</b> somewhat differently in frequentist and Bayesian methodologies.|$|E
40|$|Abstract – The {{problem of}} {{hypothesis}} testing with a <b>nuisance</b> <b>parameter</b> is considered. Two methods for using fuzzy knowledge on the <b>nuisance</b> <b>parameter</b> to test hypotheses are suggested. These methods are neither a pure classical nor a pure Bayesian approach to hypothesis testing, but rather related to both. A few known examples and their applications, which cannot be studied by the parametric statistical methods, are discussed...|$|R
40|$|A {{two-stage}} procedure {{based on}} impulse saturation is suggested to distinguish mean and variance shifts. The resulting zero-mean innovation test statistic has a non standard distribution, with a <b>nuisance</b> <b>parameter.</b> Hence, simulation-based critical values are provided for some cases of interest. Monte Carlo evidence reveals the test has good power properties to discriminate mean and variance shifts identified through the impulse saturation break test. breaks; mean shift; variance shift; impulse saturation; <b>nuisance</b> <b>parameter...</b>|$|R
40|$|AbstractStatistical {{experiments}} {{possess the}} property of adaptivity, if the ignorance of a <b>nuisance</b> <b>parameter</b> does not cause any loss in efficiency. In order to include a large variety of cases, the efficiency is {{measured in terms of}} minimax bounds. It is shown that a necessary and sufficient condition for adaptivity of translation invariant experiments is that the parameter of interest and the <b>nuisance</b> <b>parameter</b> are a. s. independent w. r. t. the posterior distributions...|$|R
50|$|The general {{treatment}} of <b>nuisance</b> <b>parameters</b> can be broadly similar between frequentist and Bayesian approaches to theoretical statistics. It relies on {{an attempt to}} partition the likelihood function into components representing information about the parameters of interest and information about the other (<b>nuisance)</b> <b>parameters.</b> This can involve ideas about sufficient statistics and ancillary statistics. When this partition can be achieved {{it may be possible}} to complete a Bayesian analysis for the parameters of interest by determining their joint posterior distribution algebraically. The partition allows frequentist theory to develop general estimation approaches in the presence of <b>nuisance</b> <b>parameters.</b> If the partition cannot be achieved it may still be possible to make use of an approximate partition.|$|E
50|$|In Bayesian analysis, a {{generally}} applicable approach creates random {{samples from the}} joint posterior distribution of all the parameters: see Markov chain Monte Carlo. Given these, the joint distribution of only the parameters of interest can be readily found by marginalizing over the <b>nuisance</b> <b>parameters.</b> However, this approach {{may not always be}} computationally efficient if {{some or all of the}} <b>nuisance</b> <b>parameters</b> can be eliminated on a theoretical basis.|$|E
50|$|In many cases, the {{likelihood}} {{is a function}} of more than one parameter but interest focuses on the estimation of only one, or at most a few of them, with the others being considered as <b>nuisance</b> <b>parameters.</b> Several alternative approaches have been developed to eliminate such <b>nuisance</b> <b>parameters</b> so that a likelihood can be written as a function of only the parameter (or parameters) of interest; the main approaches being marginal, conditional and profile likelihoods.|$|E
40|$|Deviations {{from the}} center within a robust {{neighborhood}} may naturally be considered an in nite dimensional <b>nuisance</b> <b>parameter.</b> Thus, the semiparametric method may be tried, which is to compute the scores function for the main parameter minus its orthogonal projection on the closed linear tangent space for the <b>nuisance</b> <b>parameter,</b> and then rescale for Fisher consistency. We derive such a semiparametric inuence curve by nonlinear projection on the tangent balls arising in robust statistics...|$|R
40|$|The {{asymptotic}} behavior of multiple decision procedures is studied when the underlying distributions depend on an unknown <b>nuisance</b> <b>parameter.</b> An adaptive procedure must be asymptotically optimal for each {{value of this}} <b>nuisance</b> <b>parameter,</b> and it should not depend on its value. A necessary and sufficient condition {{for the existence of}} such a procedure is derived. Several examples are investigated in detail, and possible lack of adaptation of the traditional overall maximum likelihood rule is discussed...|$|R
50|$|Adaptive {{estimator}} {{estimates the}} parameter of interest equally well regardless whether {{the value of}} the <b>nuisance</b> <b>parameter</b> is known or not.|$|R
5000|$|<b>Nuisance</b> <b>parameters</b> {{should be}} known, or {{estimated}} with high accuracy (an {{example of a}} nuisance parameter would be the standard deviation in a one-sample location test). Z-tests focus on a single parameter, and treat all other unknown parameters as being fixed at their true values. In practice, due to Slutsky's theorem, [...] "plugging in" [...] consistent estimates of <b>nuisance</b> <b>parameters</b> can be justified. However if the sample size is not large enough for these estimates to be reasonably accurate, the Z-test may not perform well.|$|E
50|$|Note {{that the}} {{treatment}} of the <b>nuisance</b> <b>parameters</b> above is often omitted from discussions comparing confidence and credible intervals but it is markedly different between the two cases.|$|E
50|$|The studentized test enjoys optimal {{properties}} as the statistic that is bootstrapped is pivotal (i.e. it {{does not}} depend on <b>nuisance</b> <b>parameters</b> as the t-test follows asymptotically a N(0,1) distribution), unlike the percentile bootstrap.|$|E
40|$|Often for a {{non-regular}} parametric hypothesis, a tractable {{test statistic}} involves a <b>nuisance</b> <b>parameter.</b> A common practice is {{to replace the}} unknown <b>nuisance</b> <b>parameter</b> by its estimator. The validality of such a replacement can only be justified for an infinite sample {{in the sense that}} under appropriate conditions the asymptotic distribution of the statistic under the null hypothesis is unchanged when the <b>nuisance</b> <b>parameter</b> is replaced by its estimator (Crowder M. J. 1990. Biometrika 77 : 499 – 506). We propose a bootstrap method to calibrate the error incurred in the significance level, for finite samples, due to the replacement. Further, we have proved that the bootstrap method provides a more accurate estimator for the unknown actual significance level than the nominal level. Simulations demonstrate the proposed methodology. ...|$|R
40|$|This paper generalizes {{a part of}} {{the theory}} of Z-estimation which has been {{developed}} mainly in the context of modern empirical processes to the case of stochastic processes, typically, semimartingales. We present a general theorem to derive the asymptotic behavior of the <b>nuisance</b> <b>parameter</b> h when the compensator of Ψn is random. As its application, we consider the estimation problem in an ergodic diffusion process model where the drift coefficient contains an unknown, finite-dimensional parameter θ and the diffusion coefficient is indexed by a <b>nuisance</b> <b>parameter</b> h from an infinite-dimensional space. An example for the <b>nuisance</b> <b>parameter</b> space is a class of smooth functions. We establish the asymptotic normality and efficiency of a Z-estimator for the drift coefficient. As another application, we present a similar result also in an ergodic time series model...|$|R
50|$|The term {{nuisance}} {{variable is}} sometimes {{also used in}} more general contexts, simply to designate those variables that are marginalised over when finding a marginal distribution. In particular, the term may sometimes {{be used in the}} context of Bayesian analysis as an alternative to <b>nuisance</b> <b>parameter,</b> given that Bayesian statistics allows parameters to be treated as having probability distributions. However this is usually avoided as the term <b>nuisance</b> <b>parameter</b> has a specific meaning in statistical theory.|$|R
50|$|In some situations, it is {{possible}} to devise a test that properly accounts for the variation in plug-in estimates of <b>nuisance</b> <b>parameters.</b> In the case of one and two sample location problems, a t-test does this.|$|E
50|$|For maximum {{likelihood}} estimations, a model {{may have a}} number of <b>nuisance</b> <b>parameters.</b> For the asymptotic behaviour outlined to hold, the number of <b>nuisance</b> <b>parameters</b> should not increase with the number of observations (the sample size). A well-known example of this case is where observations occur as pairs, where the observations in each pair have a different (unknown) mean but otherwise the observations are independent and normally distributed with a common variance. Here for 2N observations, there are N + 1 parameters. It is well known that the {{maximum likelihood}} estimate for the variance does not converge to the true value of the variance.|$|E
5000|$|... where [...] is the {{observed}} value of [...] and [...] is {{the observed}} value of [...] Exact inferences on [...] based on probabilities and expected values of [...] are possible because its distribution and the observed value are both free of <b>nuisance</b> <b>parameters.</b>|$|E
3000|$|... 1 {{is already}} eliminated. Also, g_ 2 ^(1)=Γ ^(1)g_ 2 ^(0) {{corresponds}} to the last column and the next <b>nuisance</b> <b>parameter</b> u [...]...|$|R
5000|$|Andrews, Donald W. K. & Ploberger, Werner, [...] "Optimal Tests When a <b>Nuisance</b> <b>Parameter</b> Is Present Only Under the Alternative". Econometrica, 62(6), 1994.|$|R
40|$|Statistical {{experiments}} {{possess the}} property of adaptivity, if the ignorance of a <b>nuisance</b> <b>parameter</b> does not cause any loss in efficiency. In order to include a large variety of cases, the efficiency is {{measured in terms of}} minimax bounds. It is shown that a necessary and sufficient condition for adaptivity of translation invariant experiments is that the parameter of interest and the <b>nuisance</b> <b>parameter</b> are a. s. independent w. r. t. the posterior distributions. adaptivity minimax bounds Pitman estimates semi-parametric models...|$|R
