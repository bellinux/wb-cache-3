3452|10000|Public
5|$|Both Amdahl's law and Gustafson's law {{assume that}} the running time of the serial {{part of the program}} is {{independent}} of the <b>number</b> <b>of</b> <b>processors.</b> Amdahl's law assumes that the entire problem is of fixed size so that the total amount {{of work to be done}} in parallel is also independent of the <b>number</b> <b>of</b> <b>processors,</b> whereas Gustafson's law assumes that the total amount of work to be done in parallel varies linearly with the <b>number</b> <b>of</b> <b>processors.</b>|$|E
5|$|The {{controller}} {{is central}} to the correct function of an Xgrid, as this node is responsible for the distribution, supervision and coordination of tasks on agents. The program running on the controller can assign and reassign tasks to handle individual agent failures on demand. The number of tasks assigned to an agent depend on two factors: the number of agents on an Xgrid and the <b>number</b> <b>of</b> <b>processors</b> in each node. The number of agents on an Xgrid determines how the controller will assign tasks. The tasks may be assigned simultaneously for a large number of agents, or queued for a small number of agents. When a node with more than one processor is detected on an Xgrid, the controller may assign one task per processor; this only occurs if the number of agents on the network is lower than the number of tasks the controller has to complete.|$|E
5|$|Protein folding {{does not}} occur in one step. Instead, {{proteins}} {{spend most of their}} folding time, nearly 96% in some cases, waiting in various intermediate conformational states, each a local thermodynamic free energy minimum in the protein's energy landscape. Through a process known as adaptive sampling, these conformations are used by Folding@home as starting points for a set of simulation trajectories. As the simulations discover more conformations, the trajectories are restarted from them, and a Markov state model (MSM) is gradually created from this cyclic process. MSMs are discrete-time master equation models which describe a biomolecule's conformational and energy landscape as a set of distinct structures and the short transitions between them. The adaptive sampling Markov state model method significantly increases the efficiency of simulation as it avoids computation inside the local energy minimum itself, and is amenable to distributed computing (including on GPUGRID) as it allows for the statistical aggregation of short, independent simulation trajectories. The amount of time it takes to construct a Markov state model is inversely proportional to the number of parallel simulations run, i.e., the <b>number</b> <b>of</b> <b>processors</b> available. In other words, it achieves linear parallelization, leading to an approximately four orders of magnitude reduction in overall serial calculation time. A completed MSM may contain tens of thousands of sample states from the protein's phase space (all the conformations a protein can take on) and the transitions between them. The model illustrates folding events and pathways (i.e., routes) and researchers can later use kinetic clustering to view a coarse-grained representation of the otherwise highly detailed model. They can use these MSMs to reveal how proteins misfold and to quantitatively compare simulations with experiments.|$|E
5000|$|The <b>number</b> <b>of</b> <b>processor</b> {{registers}} available may be large, even limitless.|$|R
50|$|The Xen {{hypervisor}} {{has been}} ported to a <b>number</b> <b>of</b> <b>processor</b> families.|$|R
40|$|Implementing {{deterministic}} declarative concurrency using sieves The predominant thread-based {{approach to}} concurrent programming is bug-prone, difficult to reason about, {{and does not}} scale well to large <b>numbers</b> <b>of</b> <b>processors.</b> Sieves provide a simple way of adding deterministic declarative concurrency to imperative programming languages. Sieve programs have a straightforward semantics, are not significantly more difficult to reason about than sequential imperative programs, and should scale to large <b>numbers</b> <b>of</b> <b>processors</b> as well as different processor architectures. 1...|$|R
25|$|In time, as the <b>number</b> <b>of</b> <b>processors</b> increased, {{different}} architectural issues emerged.|$|E
25|$|During the 1980s, as {{the demand}} for {{computing}} power increased, the trend to a much larger <b>number</b> <b>of</b> <b>processors</b> began, ushering {{in the age of}} massively parallel systems, with distributed memory and distributed file systems, given that shared memory architectures could not scale to a large <b>number</b> <b>of</b> <b>processors.</b> Hybrid approaches such as distributed shared memory also appeared after the early systems.|$|E
25|$|Systems with {{a massive}} <b>number</b> <b>of</b> <b>processors</b> {{generally}} take one of two paths: in one approach, e.g., in grid computing the processing power {{of a large number}} of computers in distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, a large <b>number</b> <b>of</b> <b>processors</b> are used in close proximity to each other, e.g., in a computer cluster. In such a centralized massively parallel system the speed and flexibility of the interconnect becomes very important, and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects.|$|E
40|$|Abstract—Full-rate space-time block codes (STBCs) achieve high spectral-efficiency by {{transmitting}} linear {{combinations of}} information symbols through every transmit antenna. However, the coefficients {{used for the}} linear combinations, if not chosen carefully, result in (i) large <b>number</b> <b>of</b> <b>processor</b> bits for the encoder and (ii) high peak-to-average power ratio (PAPR) values. In this letter, we propose {{a new class of}} full-rate STBCs called Integer STBCs (ICs) for multiple-input multiple-output (MIMO) fading channels. A unique property of ICs is the presence of integer coefficients in the code structure which enables reduced <b>numbers</b> <b>of</b> <b>processor</b> bits for the encoder and lower PAPR values. We show that the reduction in the <b>number</b> <b>of</b> <b>processor</b> bits is significant for small MIMO channels, while the reduction in the PAPR is significant for large MIMO channels. We also highlight the advantages of the proposed codes in comparison with the well known full-rate algebraic STBCs...|$|R
40|$|Full-rate space-time block codes (STBCs) achieve high spectral-efficiency by {{transmitting}} linear {{combinations of}} information symbols through every transmit antenna. However, the coefficients {{used for the}} linear combinations, if not chosen carefully, results in (i) large <b>number</b> <b>of</b> <b>processor</b> bits for the encoder and (ii) high peak-to-average power ratio (PAPR) values. In this work, we propose {{a new class of}} full-rate STBCs called Integer STBCs (ICs) for multiple-input multiple-output (MIMO) fading channels. A unique property of ICs is the presence of integer coefficients in the code structure which enables reduced <b>numbers</b> <b>of</b> <b>processor</b> bits for the encoder and lower PAPR values. We show that the reduction in the <b>number</b> <b>of</b> <b>processor</b> bits is significant for small MIMO channels, while the reduction in the PAPR is significant for large MIMO channels. We also highlight the advantages of the proposed codes in comparison with the well known full-rate algebraic STBCs. Comment: 10 pages and 3 figure...|$|R
50|$|In conclusion, {{there is}} no single network {{topology}} which is best for all scenarios. The decision has to be made based on factors like <b>number</b> <b>of</b> <b>processor</b> nodes in the system, bandwidth-latency requirements, cost and scalability.|$|R
25|$|For years, {{processor}} makers delivered {{increases in}} clock rates and instruction-level parallelism, so that single-threaded code executed faster on newer processors with no modification. Now, to manage CPU power dissipation, processor makers favor multi-core chip designs, and software {{has to be}} written in a multi-threaded manner {{to take full advantage}} of the hardware. Many multi-threaded development paradigms introduce overhead, and will not see a linear increase in speed vs <b>number</b> <b>of</b> <b>processors.</b> This is particularly true while accessing shared or dependent resources, due to lock contention. This effect becomes more noticeable as the <b>number</b> <b>of</b> <b>processors</b> increases. There are cases where a roughly 45% increase in processor transistors has translated to roughly 10–20% increase in processing power.|$|E
25|$|The {{air-cooled}} IBM Blue Gene supercomputer architecture trades {{processor speed}} for {{low power consumption}} so that a larger <b>number</b> <b>of</b> <b>processors</b> can be used at room temperature, by using normal air-conditioning. The second-generation Blue Gene/P system has processors with integrated node-to-node communication logic. It is energy-efficient, achieving 371 MFLOPS/W.|$|E
25|$|As the <b>number</b> <b>of</b> <b>processors</b> increases, {{efficient}} {{interprocessor communication}} and synchronization on a supercomputer becomes a challenge. A number of approaches {{may be used}} to achieve this goal. For instance, in the early 1980s, in the Cray X-MP system, shared registers were used. In this approach, all processors had access to shared registers that did not move data back and forth but were only used for interprocessor communication and synchronization. However, inherent challenges in managing a large amount of shared memory among many processors resulted in a move to more distributed architectures.|$|E
50|$|Solace Virtual Message Router {{software}} provides {{message routing}} and persistence functionality. The VMR features a multi-threaded, parallel pipelined architecture, optimized for modern multi-core processor architectures {{so it can}} scale {{in proportion to the}} <b>number</b> <b>of</b> <b>processor</b> cores.|$|R
40|$|We {{propose a}} model for {{describing}} and predicting the parallel performance of a broad class of parallel numerical software on distributed memory architectures. The purpose of this model is to allow reliable predictions {{to be made for}} the performance of the software on large <b>numbers</b> <b>of</b> <b>processors</b> <b>of</b> a given parallel system, by only benchmarking the code on small <b>numbers</b> <b>of</b> <b>processors.</b> Having described the methods used, and emphasized the simplicity of their implementation, the approach is tested on a range of engineering software applications that are built upon the use of multigrid algorithms. Despite their simplicity, the models are demonstrated to provide both accurate and robust predictions across a range of different parallel architectures, partitioning strategies and multigrid codes. In particular, the effectiveness of the predictive methodology is shown for a practical engineering software implementation of an elastohydrodynamic lubrication solver...|$|R
40|$|We {{propose a}} new {{parallel}} performance visualization scheme, {{based on a}} simple moment analysis <b>of</b> <b>processor</b> utilization data. This method combines the scalability advantages of statistical summaries with the more revealing <b>processor</b> utilization information <b>of</b> Gantt charts. It scales well to large <b>numbers</b> <b>of</b> <b>processors,</b> requiring only storage constant in execution time...|$|R
25|$|In {{parallel}} algorithms, {{yet another}} resource {{in addition to}} time and space {{is the number of}} computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial <b>number</b> <b>of</b> <b>processors,</b> then the problem is said to be in the class NC. The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.|$|E
25|$|The {{relatively}} small <b>number</b> <b>of</b> <b>processors</b> in early systems, {{allowed them to}} easily use a shared memory architecture, which allows processors to access a common pool of memory. In the early days a common approach {{was the use of}} uniform memory access (UMA), in which access time to a memory location was similar between processors. The use of non-uniform memory access (NUMA) allowed a processor to access its own local memory faster than other memory locations, while cache-only memory architectures (COMA) allowed for the local memory of each processor to be used as cache, thus requiring coordination as memory values changed.|$|E
25|$|A {{problem is}} {{regarded}} as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such {{as the amount of}} communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the <b>number</b> <b>of</b> <b>processors</b> (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.|$|E
3000|$|This paper {{presents}} the SmartCell as a novel reconfigurable architecture for stream-based applications. It is a coarse-grained architecture that tiles a large <b>number</b> <b>of</b> <b>processor</b> elements with reconfigurable communication fabrics. A prototype with 64 PEs is implemented with TSMC 0.13 [*] [...]...|$|R
50|$|Computer {{software}} {{is said to}} exhibit scalable locality if it can continue to make use <b>of</b> <b>processors</b> that out-pace their memory systems, to solve ever larger problems.This term is a high-performance uniprocessor analog {{of the use of}} scalable parallelism to refer to software for which increasing <b>numbers</b> <b>of</b> <b>processors</b> can be employed for larger problems.|$|R
40|$|ECENT {{advances}} {{in computer technology}} made parallel machines a reality. Massively parallel systems use many general-purpose, inexpensive processing elements to attain computation speed-ups comparable to or better than those achieved by expensive, specialized machines with a small <b>number</b> <b>of</b> fast <b>processors.</b> In such setting, however, {{one would expect to}} see an increased <b>number</b> <b>of</b> <b>processor</b> failures attributable to hardware or software. This may eliminate the potential advantage of parallel computation. We believe that this presents a reliability bottleneck that is among fundamental problems in parallel computation...|$|R
500|$|In {{addition}} to the system cabinet, a Cray-3 system also needed one or two (depending on <b>number</b> <b>of</b> <b>processors)</b> system control pods (or [...] "C-Pods"), [...] square and [...] high, containing power and cooling control equipment.|$|E
500|$|As a {{computer}} system grows in complexity, the {{mean time between failures}} usually decreases. Application checkpointing is a technique whereby the computer system takes a [...] "snapshot" [...] of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large <b>number</b> <b>of</b> <b>processors</b> used in high performance computing.|$|E
2500|$|... /NUMPROC=nnn [...] Sets the <b>number</b> <b>of</b> <b>processors</b> that Windows {{will run}} at startup. With this switch, the user can force a {{multiprocessor}} system {{to use only}} the quantity of processors (number) that you specify. Useful for troubleshooting performance problems and defective CPUs.|$|E
30|$|In {{parallel}} processing, {{performance is}} expected to increase in the same proportion as the increase in <b>number</b> <b>of</b> <b>processor</b> cores. However, this is not always possible in practice, due to scalability limitations in applications, communication limitations generated by programs during execution, and network restrictions.|$|R
50|$|In {{the summer}} of 2009, the XT4 {{cabinets}} were upgraded with quad-core 2.3 GHz Opteron processors with 8 GB memory each. This doubled the <b>number</b> <b>of</b> <b>processor</b> cores to 22,656, and increased total system memory to 45.3 terabytes. Peak performance was increased to 208 teraflops.|$|R
40|$|This paper {{describes}} Nsort's background, presents {{its performance}} sorting a terabyte of data, and compares its performance on an industry-standard benchmark. Nsort performance is presented for file copying and record selection, and sorting with varying <b>numbers</b> <b>of</b> <b>processors,</b> input sizes, key types, key lengths, <b>numbers</b> <b>of</b> keys and record lengths. Backgroun...|$|R
2500|$|The {{heat density}} {{generated}} by a supercomputer has a direct dependence on the processor type used in the system, with more powerful processors typically generating more heat, given similar underlying semiconductor technologies. While early supercomputers used a few fast, closely packed processors that took advantage of local parallelism (e.g., pipelining and vector processing), in time the <b>number</b> <b>of</b> <b>processors</b> grew, and computing nodes could be placed further away,e.g., in a computer cluster, or could be geographically dispersed in grid computing. As the <b>number</b> <b>of</b> <b>processors</b> in a supercomputer grows, [...] "component failure rate" [...] begins to become a serious issue. If a supercomputer uses thousands of nodes, each of which may fail once per year on the average, then the system will experience several node failures each day.|$|E
2500|$|Two {{issues that}} need to be {{addressed}} as the <b>number</b> <b>of</b> <b>processors</b> increases are the distribution of memory and processing. In the distributed memory approach, each processor is physically packaged close with some local memory. The memory associated with other processors is then [...] "further away" [...] based on bandwidth and latency parameters in non-uniform memory access.|$|E
2500|$|The {{limits of}} {{specific}} approaches {{continue to be}} tested, as boundaries are reached through large scale experiments, e.g., in 2011 IBM ended its participation in the Blue Waters petaflops project at the University of Illinois. The Blue Waters architecture {{was based on the}} IBM POWER7 processor and intended to have 200,000 cores with a petabyte of [...] "globally addressable memory" [...] and 10 petabytes of disk space. The goal of a sustained petaflop led to design choices that optimized single-core performance, and hence a lower number of cores. The lower number of cores was then expected to help performance on programs that did not scale well to a large <b>number</b> <b>of</b> <b>processors.</b> The large globally addressable memory architecture aimed to solve memory address problems in an efficient manner, for the same type of programs. Blue Waters had been expected to run at sustained speeds of at least one petaflop, and relied on the specific water-cooling approach to manage heat. In the first four years of operation, the National Science Foundation spent about $200 million on the project. IBM released the Power 775 computing node derived from that project's technology soon thereafter, but effectively abandoned the Blue Waters approach.|$|E
40|$|The {{ability to}} {{tolerate}} faults {{is critical in}} multicomputer employing large <b>numbers</b> <b>of</b> <b>processors.</b> This paper describes a class of fault-tolerant routing algorithms for n-dimensional meshes that can tolerate large <b>numbers</b> <b>of</b> faults without using virtual channels. We show that these routing algorithms prevent livelock and deadlock while remaining highly adaptive...|$|R
40|$|We {{present a}} data {{parallel}} algorithm for radiosity. The algorithm {{was designed to}} take advantage <b>of</b> large <b>numbers</b> <b>of</b> <b>processors.</b> It has been implemented on the Connection Machine CM 2 system and scales linearly in the <b>number</b> <b>of</b> available <b>processors</b> over a wide range. All parts of the algorithm [...] form-factor computation, visibility determination, adaptive subdivision, and linear algebra solution [...] execute in parallel with a completely distributed database. Load balancing is achieved through processor allocation and dynamic data structures which reconfigure appropriately to match the granularity of the required calculations...|$|R
50|$|This section {{compares the}} {{butterfly}} network with other network topologies like linear array, ring, 2-D mesh and hypercube. Note that linear array {{can be considered}} as a 1-D mesh topology. All their relevant parameters are compiled in the below table (‘p’ represents the <b>number</b> <b>of</b> <b>processor</b> nodes).|$|R
