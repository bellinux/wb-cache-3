10|11|Public
40|$|Semi-continuous {{acoustic}} models, {{where the}} output distri-butions for all Hidden Markov Model states {{share a common}} codebook of Gaussian density functions, are a well-known and proven technique for reducing computation in automatic speech recognition. However, {{the size of the}} parameter files, and thus their memory footprint at runtime, can be very large. We demonstrate how <b>non-linear</b> <b>quantization</b> can be com-bined with a mixture weight distribution pruning technique to halve the size of the models with minimal performance overhead and no increase in error rate...|$|E
30|$|In another future study, {{we intend}} to change the minimum {{perceived}} levels of pitch, rate and volume, in order to eliminate the pitfalls {{of the current study}} and to optimize listeners’ performance (e.g., the misunderstanding of bold-italics and bold during their acoustic rendition) or the use of <b>non-linear</b> <b>quantization.</b> Also, the study of acoustic rendition of typographic attributes in a similar methodology using discrete emotions (instead of the dimensional approach) seems very interesting. This would extend the use of the current methodology in systems than can process a reader’s discrete emotions derived from the typographic cues and the TtS systems that only support discrete emotion approach.|$|E
30|$|In {{order to}} ease the {{challenging}} requirement of FH data constraint, various solutions have been proposed such as (i) increasing the FH capacity using single fiber bidirection (SFBD), wavelength division multiplexing (WDM) [19, 20], and time-shared optical networks (TSON) [21, 22]; (ii) decreasing the FH capacity by using compression techniques such as <b>non-linear</b> <b>quantization</b> and IQ data compression with a lossless 2 : 1 compression ratio [23], or using new functional splits between RRU-BBU [15, 24]. Another approach is packet-based network such as Ethernet which is heavily investigated by the recent initiative such as IEEE 1914 working group for next-generation fronthaul interface (NGFI) [25].|$|E
40|$|Small {{compression}} noises, {{despite being}} transparent to human eyes, can adversely affect {{the results of}} many image restoration processes, if left unaccounted for. Especially, compression noises are highly detrimental to inverse operators of high-boosting (sharpening) nature, such as deblurring and superresolution against a convolution kernel. By incorporating the <b>non-linear</b> DCT <b>quantization</b> mechanism into the formulation for image restoration, we propose a new sparsity-based convex programming approach for joint compression noise removal and image restoration. Experimental results demonstrate significant performance gains of the new approach over existing image restoration methods...|$|R
40|$|In {{this paper}} {{we deal with}} the {{construction}} of lower-dimensional manifolds from high-dimensional data which is an important task in data mining, machine learning and statistics. Here, we consider principal manifolds as the minimum of a regularized, <b>non-linear</b> empirical <b>quantization</b> error functional. For the discretization we use a sparse grid method in latent parameter space. This approach avoids, to some extent, the curse of dimension of conventional grids like in the GTM approach. The arising nonlinear problem is solved by a descent method which resembles the expectation maximization algorithm. We present our sparse grid principal manifold approach, discuss its properties and report on the results of numerical experiments for one-, two- and three-dimensional model problems...|$|R
40|$|In this paper, a novel color texture {{classification}} {{approach is}} introduced {{and applied to}} computer-assisted grading of follicular lymphoma from whole-slide tissue samples. The digitized tissue samples of follicular lymphoma were classified into histological grades under a statistical framework. The proposed method classifies the image either into low or high grades based {{on the amount of}} cytological components. To further discriminate the lower grades into low and mid grades, we proposed a novel color texture analysis approach. This approach modifies the gray level co-occurrence matrix method by using a <b>non-linear</b> color <b>quantization</b> with self-organizing feature maps (SOFMs). This is particularly useful for the analysis of H&E stained pathological images whose dynamic color range is considerably limited. Experimental results on real follicular lymphoma images demonstrate that the proposed approach outperforms the gray level based texture analysis. Index Terms — color texture analysis, self-organizing feature maps, computer-aided diagnosis 1...|$|R
40|$|Level {{crossing}} based sampling {{might be}} used as an alternative to Nyquist theory based sampling of a signal. Level crossing based approach take advantage of statistical properties of the signal, providing cues to efficient nonuniform sampling. This paper presents new threshold level allocation schemes for level crossing based nonuniform sampling. Intuitively, it is more reasonable if the information rich regions of the signal are sampled finer and those with sparse information are sampled coarser. To achieve this objective, we proposed <b>non-linear</b> <b>quantization</b> functions which dynamically assign the number of quantization levels depending on the importance of the given amplitude range. Various aspects of proposed techniques are discussed and experimentally validated. Its efficacy is investigated by comparison with Nyquist based samplin...|$|E
40|$|It is {{demonstrated}} that the so-called “unavoidable quantum anomalies ” can be avoided {{in the framework of}} a special <b>non-linear</b> <b>quantization</b> scheme. In this scheme, the quantized hamiltonians are represented by non-linear but homogeneous operators in Hilbert space. The nonlinear terms are of the same order as quantum anomalies, and their role is to cancel anomalies. The quantization method proposed is applicable to integrable classical dynamical systems and the result of quantization is again an integrable (but, generally, non-linear) ”quantum ” system. A simple example is discussed in detail. Irrespective of the existence of possible physical applications, the method provides a constructive way for extending the notion of quantum integrability to non-linear spectral problems and gives a practical tool for buiding completely integrable non-linear spectral equations in Hilbert space. ...|$|E
40|$|Abstract—The paper {{presents}} a <b>non-linear</b> <b>quantization</b> method for detail {{components in the}} JPEG 2000 standard. The quantization step sizes are determined by actual statistics of the wavelet coefficients. Mean and standard deviation are the two statistical parameters used to obtain the step sizes. Moreover, weighted mean of the coefficients lying within the step size is chosen as quantized value- contrary to the uniform quantizer. The empirical results are compared using mean squared error of uniformly quantized and non-linearly quantized wavelet co-efficients w. r. t. original wavelet coefficients. Through empirical results, it has been concluded that for low bit rate compression, the quantization error introduced by uniform quantizer is {{higher than that of}} non-linear quantizer, and thus suggesting the use of non-linear quantizer for low bit rate lossy compression. Index Terms—Mean and standard deviation, quantization, discrete wavelet transform, image compression, image processing, JPEG 2000 standard. I...|$|E
40|$|Variable size matrix {{segmentation}} {{and variable}} size transform {{are used in}} the colour image compression methods based on three dimensional matrix widely discrete cosine transform (3 DMWDCT). However the coefficients have different statistical properties since different areas of an image and difference size blocks have difference energy centralization. 3 D balance quantization based human visual system was proposed in this paper. After dividing the original colour image into variable sized sub-three dimensional matrices according to the activity characteristics of the image, the variable sized 3 DMWDCT is applied to the corresponding sized sub three dimensional matrices. DCT coefficients were 3 D balance quantized through <b>non-linear</b> scalar <b>quantization</b> and entropy coding. Experimental results show that the compression efficiency of the proposed algorithm is better than that of JPEG. Compared with JPEG, the proposed algorithm improves PSNR at most 2 dB, {{and the quality of the}} reconstructed image is improved in subject evaluation too. © 2010 IEEE...|$|R
40|$|Micro-robots, {{unmanned}} aerial vehicles (UAVs), {{imaging sensor}} networks, wireless phones, and other embedded vision systems all require low cost and high-speed implementations of synthetic vision systems capable of recognizing and categorizing objects in a scene. Many successful object recognition systems use dense features extracted on regularly-spaced patches over the input image. The {{majority of the}} feature extraction systems have a common structure composed of a filter bank (generally based on oriented edge detectors or 2 D gabor functions), a <b>non-linear</b> operation (<b>quantization,</b> winner-take-all, sparsification, normalization, and/or point-wise saturation) and finally a pooling operation (max, average or histogramming). For example, the scale-invariant feature transform (SIFT (Lowe, 2004)) operator applies oriented edge filters to a small patch and determines the dominant orientation through a winner-take-all operation. Finally, the resulting sparse vectors are added (pooled) over a larger patch to form local orientation histogram. Some recognition systems use a single stage of feature extractors (Lazebni...|$|R
40|$|Key words: {{control over}} networks, stabilizability, minimum {{feedback}} information, model-based control Summary. In this article {{a class of}} networked control systems called Model-Based Networked Control Systems (MB-NCS) is considered. This control architecture uses an explicit model of the plant {{in order to reduce}} the network traffic while attempting to prevent excessive performance degradation. MB-NCS can successfully address several important control issues in an intuitive and transparent way. In this article the main results of this MB approach are described with examples. Specifically, conditions for the stability of state and output feedback continuous and discrete systems are derived under different scenarios that include delay compensation, constant and time varying update times, <b>non-linear</b> plants and <b>quantization</b> of the feedback signals. In addition, a performance measure for MB-NCS with noise inputs is introduced. ...|$|R
40|$|Part 6 : Circuit DesignInternational audienceSmart {{phones are}} growing rapidly {{with the support}} of Over the Top (OTT) service and {{multimedia}} streaming service as well as existing Internet service. In the conventional Distributed-RAN (D-RAN) structure, a Base Band Unit (BBU) and a Remote Radio Head (RRH) are configured at one cell site. However, C-RAN, which is a new RAN structure, is composed of separated base stations in which BBU and RRH are separated. RRH is left in cell sites where radio signals are transmitted and received, and BBUs are collected and managed in different places. When the bandwidth of the LTE signal is 20  MHz and the 2  ×  2 Multiple Input Multiple Output (MIMO) antenna is used, the CPRI requires 2. 5 Gbps transmission. Since {{there is a limit to}} handle mobile traffic that is continuously increasing with the current transmission rate, several techniques for compressing IQ data before transmission have been proposed. In this paper, we apply the compression technique to remove unnecessary parts in the Most Significant Byte (MSB) of a basic frame after applying the existing Up/Down sampling and <b>Non-linear</b> <b>Quantization.</b> We measured the error vector magnitude (EVM) to measure the compression rate and the quality of the signal after compression to confirm the compression performance. Also, it was confirmed whether the experimental results satisfied the compression requirement of Open Radio Interface (ORI) ...|$|E
40|$|This paper {{presents}} {{a method for}} face recognition using multi-scale Weber local descriptors (WLDs) and multi-level information fusion. Our method introduces the WLD, a novel and robust local descriptor, to describe the facial images and modifies it by a <b>non-linear</b> <b>quantization</b> approach to enhance its discriminative power. Moreover, a multi-scale framework for WLD extraction with multi-level information fusion approaches is provided for face representation and recognition. The proposed method has four main steps: (1) image partition: under given rules, each facial image is uniformly divided into a set of non-overlapped sub-regions; in this way, {{for a set of}} facial images, we therefore have a large pool of this type of sub-regions; (2) feature extraction: in this pool of sub-regions, taking one sub-region as a center, a group of similar ones are chosen for extraction of WLD histogram features; (3) features measurement: these WLD histograms are then fused into a single vector as the feature of the center sub-region. Nearest neighborhood on chi-square is employed for similarity measurement between two sub-regions; and (4) voting: the recognition result of the entire probe (a face in sub-regions) is obtained via a voting function on the recognition result of all its sub-regions. Experimental results demonstrate the effectiveness of the proposed method upon three popular datasets. (C) 2013 Elsevier B. V. All rights reserved...|$|E
40|$|This work {{deals with}} Computer-Generated Rainbow Holograms (CGRHs), which can restore the 3 D images under white light. They {{are devoted to}} include in Diffractive Optically Variable Image Devices (DOVIDs) that are {{currently}} widely used for security needs. CGRHs prevent counterfeiting due {{to the complexity of}} recreation, on the one hand, and allow the simple identification at the first (visual) level of verification, on the other hand. To record it, the Electron Beam Lithography (EBL) is used. As recently proved, this method is a most promising for multi-level optical-digital security devices using chalcogenide glasses as resists. The CGRH computation process is conventionally divided by two parts: synthesizing and recording. On the synthesis stage, firstly, the geometrical and optical constants of recording scheme are determined; secondly, the basic parameters accounting for discretization of Interferogram Data (ID) in hologram plane are defined and, finally, the calculation of the ID - the array of Bipolar Intensity (BI) values - is carried out. This calculation is performed separately in each independent horizontal slice of object space and hologram plane. On the recording stage, suitable quantization parameters are chosen and transformation of ID into the multilevel rectangle data appropriate for EBL is accomplished. The investigations on optimization of synthesis and recording of the multilevel CGRHs of 3 D images integrated in Polygrams are presented here. So the rules for definition of the appropriate discretization parameters were finding out. Advantages of using <b>non-linear</b> <b>quantization</b> that implies condensing of quantization levels near the BI zero were explored. The random deviation of location and direction of elemental hybrid radiating area was applied. Practical applications of the method developed were made using chalcogenide semiconductors of various As-S-Se compositions...|$|E
40|$|In this paper, {{we present}} a wavelet based <b>non-linear</b> interpolative vector <b>quantization</b> scheme for joint {{compression}} and restoration of images; two tasks which are traditionally regarded as having conflicting goals. Vector quantizer codebook training is done using a training set consisting of pairs of the original image and its diffraction-limited counterpart. The designed VQ is then used to compress and simultaneously restore diffraction-limited images. Results from simulations indicate that the image produced at {{the output of the}} decoder is quantitatively and visually superior to the diffraction-limited image at the input to the encoder. We also compare the performance of several wavelet filters in our algorithm. 1. INTRODUCTION Vector quantization(VQ) is a very widely used compression technique in image coding [1]. In recent years, it has also been used to do various kinds of image processing while concurrently achieving compression [2]. VQ has been used to do such diverse image pro [...] ...|$|R
40|$|IEEE Abstract — We {{minimize}} average transmit-power with finite-rate feedback for {{coherent communications}} in a {{wireless sensor network}} (WSN), where sensors communicate with a fusion center (FC) using adaptive modulation and coding over a wireless fading channel. By viewing the coherent WSN setup as a dis-tributed space-time multi-input single-output (MISO) system, we present optimal distributed beamforming and resource allocation strategies when the full (F-) channel state information at the transmitters (CSIT) is available through a feedback channel. We also develop optimal adaptive transmission policies and design optimal quantizers for the finite-rate feedback case where the sensors only have quantized (Q-) CSIT, or, each sensor has F-CSIT of its own link with the FC but only Q-CSIT of other sensors. Numerical results confirm that our novel finite-rate feedback based strategies achieve near-optimal power savings based on even {{a small number of}} feedback bits. Index Terms — Wireless sensor networks, power efficiency, <b>quantization,</b> <b>non-linear</b> optimization, resource allocation, MISO systems. I...|$|R
40|$|Recently, many {{works have}} focused on the {{characterization}} of non-linear dimensionality reduction methods obtained by quantizing linear embeddings, e. g., to reach fast processing time, efficient data compression procedures, novel geometry-preserving embeddings or to estimate the information/bits stored in this reduced data representation. In this work, we prove that many linear maps known to respect the restricted isometry property (RIP) can induce a quantized random embedding with controllable multiplicative and additive distortions with respect to the pairwise distances of the data points beings considered. In other words, linear matrices having fast matrix-vector multiplication algorithms (e. g., based on partial Fourier ensembles or on the adjacency matrix of unbalanced expanders) can be readily used in the definition of fast quantized embeddings with small distortions. This implication is made possible by applying right after the linear map an additive and random "dither" that stabilizes the impact of the uniform scalar quantization operator applied afterwards. For different categories of RIP matrices, i. e., for different linear embeddings of a metric space (K ⊂ R^n, ℓ_q) in (R^m, ℓ_p) with p,q ≥ 1, we derive upper bounds on the additive distortion induced by quantization, showing that it decays either when the embedding dimension m increases or when the distance of a pair of embedded vectors in K decreases. Finally, we develop a novel "bi-dithered" quantization scheme, which allows for a reduced distortion that decreases when the embedding dimension grows and independently of the considered pair of vectors. Comment: Keywords: random projections, <b>non-linear</b> embeddings, <b>quantization,</b> dither, restricted isometry property, dimensionality reduction, compressive sensing, low-complexity signal models, fast and structured sensing matrices, quantized rank-one projections (31 pages...|$|R
40|$|The {{volume of}} radio-astronomical data is a {{considerable}} burden in the processing and storing of radio observations with high time and frequency resolutions and large bandwidths. Lossy compression of interferometric radio-astronomical data is considered to reduce the volume of visibility data and to speed up processing. A new compression technique named "Dysco" is introduced that consists of two steps: a normalization step, in which grouped visibilities are normalized to have a similar distribution; and a quantization and encoding step, which rounds values to a given quantization scheme using a dithering scheme. Several <b>non-linear</b> <b>quantization</b> schemes are tested and combined with different methods for normalizing the data. Four data sets with observations from the LOFAR and MWA telescopes are processed with different processing strategies and different combinations of normalization and quantization. The effects of compression are measured in image plane. The noise added by the lossy compression technique acts like normal system noise. The accuracy of Dysco is depending on the signal-to-noise ratio of the data: noisy data can be compressed with a smaller loss of image quality. Data with typical correlator time and frequency resolutions can be compressed {{by a factor of}} 6. 4 for LOFAR and 5. 3 for MWA observations with less than 1 % added system noise. An implementation of the compression technique is released that provides a Casacore storage manager and allows transparent encoding and decoding. Encoding and decoding is faster than the read/write speed of typical disks. The technique can be used for LOFAR and MWA to reduce the archival space requirements for storing observed data. Data from SKA-low will likely be compressible by the same amount as LOFAR. The same technique can be used to compress data from other telescopes, but a different bit-rate might be required. Comment: Accepted for publication in A&A. 13 pages, 8 figures. Abstract was abridge...|$|E
30|$|In this section, we {{will explore}} the {{performance}} of decoding using sparse recovery based on Equation 20 and the QNC design proposed in Theorem 1. It {{is well known that}} recovery of exactly sparse vectors from an under-determined set of linear measurements can be done with no error, using linear programming [39]. Specifically, theoretical works show that the NP-hard ℓ 0 minimization can be replaced with ℓ 1 minimization without any associated error, when dealing with noiseless measurements [37, 39]. However, when dealing with noisy measurements, ℓ 1 -min recovery does not necessarily offer a minimum mean squared error solution. There is still a lot of work being done to develop practical and near minimum mean squared error recovery algorithms for noisy cases. Sparse recovery from quantized measurements has been recently studied in a number of works [40 – 42]. For instance, the authors in [41] consider the estimation problem of sparse vectors from measurements that are quantized and corrupted by Gaussian noise. The main aspect that differentiates our model from that in [41] is that in our QNC scenario the resulting effective total measurement noises are <b>non-linear</b> functions of <b>quantization</b> noises at each edge.|$|R
40|$|Multi-resolution techniques, and {{especially}} the wavelet transform provide unique benefits in image representation and processing not otherwise possible. While wavelet applications in image compression and denoising have become extremely prevalent, their use in image restoration and super-resolution has not been exploited to the same degree. One issue is the extension 1 -D wavelet transforms into 2 -D via separable transforms versus the non-separability of typical circular aperture imaging systems. This mismatch leads to performance degradations. Image restoration, the inverse problem to image formation, is the first major focus of this research. A new multi-resolution transform is presented to improve performance. The transform is called a Radially Symmetric Discrete Wavelet-like Transform (RS-DWT) and is designed based on the non-separable blurring of the typical incoherent circular aperture imaging system. The results using this transform show marked improvement compared to other restoration algorithms both in Mean Square Error and visual appearance. Extensions to the general algorithm that further improve results are discussed. The ability to super-resolve imagery using wavelet-domain techniques is the second major focus of this research. Super-resolution, the ability to reconstruct object information lost in the imaging process, has been an active research area for many years. Multiple experiments are presented which demonstrate the possibilities and problems associated with super-resolution in the wavelet-domain. Finally, super-resolution in the wavelet domain using <b>Non-Linear</b> Interpolative Vector <b>Quantization</b> is studied {{and the results of}} the algorithm are presented and discussed...|$|R

