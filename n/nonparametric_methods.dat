1122|1884|Public
50|$|Somers’ D plays {{a central}} role in rank {{statistics}} and is the parameter behind many <b>nonparametric</b> <b>methods.</b> It is also used as a quality measure of binary choice or ordinal regression (e.g., logistic regressions) and credit scoring models.|$|E
50|$|The two-sample K-S test {{is one of}} {{the most}} useful and general <b>nonparametric</b> <b>methods</b> for {{comparing}} two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples.|$|E
50|$|Although, currently, more {{software}} {{is available to}} implement parametric than <b>nonparametric</b> <b>methods</b> (because the latter approach is newer), the cited papers by Getz et al. demonstrate that LoCoH methods generally provide more accurate estimates of home range sizes and have better convergence properties as sample size increases than parametric kernel methods.|$|E
40|$|A <b>nonparametric</b> <b>method</b> and a Bayesian {{hierarchical}} modelling method {{are proposed}} {{in this paper}} {{for the detection of}} environmental thresholds. The <b>nonparametric</b> <b>method</b> is based on the reduction of deviance, while the Bayesian method is based on the change in the response variable distribution parameters...|$|R
3000|$|... t {{data from}} a Japanese larch (Larix kaempferi) small, clear specimen. These <b>methods</b> {{included}} a <b>nonparametric</b> <b>method,</b> the projection method of Hayashi, and a proposed method. The estimated length effect parameters (g) by the <b>nonparametric</b> <b>method</b> were 0.0237 and 0.0626 for 50 th and 5 th percentileσ [...]...|$|R
30|$|To detect MRCP, {{a pattern}} {{identification}} unit is required. There {{are two kinds}} of pattern identification unit: One uses a parametric method for which the probability distribution of data is known in advance and another uses a <b>nonparametric</b> <b>method</b> which requires collected data because the probability distribution of data is unknown. We used the <b>nonparametric</b> <b>method</b> because EEG data are different for each patient.|$|R
5000|$|VaR can be {{estimated}} either parametrically (for example, variance-covariance VaR or delta-gamma VaR) or nonparametrically (for examples, historical simulation VaR or resampled VaR). <b>Nonparametric</b> <b>methods</b> of VaR estimation are discussed in Markovich [...] and Novak. A comparison {{of a number of}} strategies for VaR prediction is given in Kuester et al.|$|E
50|$|Lafferty is {{currently}} a full professor at the University of Chicago, and has held visiting positions at the University of California, Berkeley and the University of California, San Diego. His research interests are in statistical machine learning, information retrieval, and natural language processing; focus on computational and statistical aspects of <b>nonparametric</b> <b>methods,</b> high-dimensional data and graphical models.|$|E
5000|$|Recently, the kernel {{approach}} to constructing utilization distributions {{was extended to}} include a number of <b>nonparametric</b> <b>methods</b> such as the Burgman and Fox's alpha-hull method [...] and Getz and Wilmers local convex hull (LoCoH) method. [...] This latter method has now been extended from a purely fixed-point LoCoH method to fixed radius and adaptive point/radius LoCoH methods.|$|E
30|$|The rank-based <b>nonparametric</b> Mann-Kendall <b>method</b> (Mann 1945; Kendall 1975) {{is applied}} to the long term data in this study to detect {{statistically}} significant trends. Sen’s <b>nonparametric</b> <b>method</b> (Sen 1968) was used to estimate the trends of slope in the time series data.|$|R
5000|$|Singular {{spectrum}} analysis is a <b>nonparametric</b> <b>method</b> {{that uses a}} singular value decomposition of the covariance matrix to estimate the spectral density ...|$|R
40|$|We use a nonlinear, <b>nonparametric</b> <b>method</b> to {{forecast}} the unemployment rates. We compare these forecasts to several linear and nonlinear parametric methods {{based on the}} work of Montgomery et al. (1998) and Carruth et al. (1998). Our main result is that, due to the nonlin-earity in the data generating process, the <b>nonparametric</b> <b>method</b> outperforms many other well-known models, even when these models use more information. This result holds for forecasts based on quarterly and on monthly data...|$|R
50|$|Myles Hollander (born March 21, 1941) is an American {{academic}} statistician who {{has made}} research contributions to <b>nonparametric</b> <b>methods,</b> biostatistics, and reliability. He was born in Brooklyn, New York. He is Emeritus and Robert O. Lawton Distinguished Professor of Statistics at Florida State University. He is a Fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the International Statistical Institute.|$|E
50|$|Ait-Sahalia’s {{research}} has {{concentrated on the}} estimation of continuous-time models in financial economics. His primary contributions include the development of <b>nonparametric</b> <b>methods</b> for estimating and testing these models, of expansions to implement maximum-likelihood estimation of arbitrary models using discrete data, and numerous advances in the estimation and testing of models using high frequency data {{with a focus on}} understanding the role and importance of jumps.|$|E
5000|$|The {{simplest}} way of measuring the home range is {{to construct the}} smallest possible convex polygon around the data but this tends to overestimate the range. The best known methods for constructing utilization distributions are the so-called bivariate Gaussian or normal distribution kernel density methods. More recently, <b>nonparametric</b> <b>methods</b> such as the Burgman and Fox's alpha-hull and Getz and Wilmers local convex hull have been used. Software is available for using both parametric and nonparametric kernel methods.|$|E
40|$|Rating {{scales for}} outcome {{variables}} produce categorical data {{which are often}} ordered and measurements from rating scales are not standardized. The {{purpose of this study}} is to apply commonly used and novel methods for paired ordered categorical data to two data sets with different properties and to compare the results and the conditions for use of these models. The two applications consist of a data set of inter-rater reliability and a data set from a follow-up evaluation of patients. Standard measures of agreement and measures of association are used. Various loglinear models for paired categorical data using properties of quasi-independence and quasi-symmetry as well as logit models with a marginal modelling approach are used. A <b>nonparametric</b> <b>method</b> for ranking and analyzing paired ordered categorical data is also used. We show that a deeper insight when it comes to disagreement and change patterns may be reached using the <b>nonparametric</b> <b>method</b> and illustrate some problems with standard measures as well as parametric loglinear and logit models. In addition, the merits of the <b>nonparametric</b> <b>method</b> are illustrated. Agreement:ordinal data; ranking; reliability. rating scales...|$|R
40|$|A <b>nonparametric</b> <b>method</b> and a Bayesian {{hierarchical}} {{modeling method}} are proposed {{in this paper}} {{for the detection of}} environmental thresholds. The <b>nonparametric</b> <b>method</b> is based on the reduction of deviance, while the Bayesian method is based on the change in the response variable distribution parameters. Both methods are tested using macroinvertebrate composition data from a mesocosm experiment conducted in the Everglades wetlands, where phosphorus is the limiting nutrient. Using the percent of phosphorus tolerant species and a dissimilarity index as the response variables, both methods resulted in a similar and well-defined TP concentration threshold, with a distribution function {{that can be used to}} determine the probability of exceeding the threshold...|$|R
40|$|Springer ReferenceInternational audienceThis paper {{deals with}} the {{fundamental}} mathematical tools and the associated computational aspects for constructing the stochastic models of random matrices that appear in the <b>nonparametric</b> <b>method</b> of uncertainties and in the random consti-tutive equations for multiscale stochastic modeling of heterogeneous materials. The explicit construction of ensembles of random matrices, but also the presentation of numerical tools for constructing general ensembles of random matrices are presented {{and can be used}} for high stochastic dimension. The developments presented are illustrated for the <b>nonparametric</b> <b>method</b> for multiscale stochastic mod-eling of heterogeneous linear elastic materials and for the nonparametric stochas-tic models of uncertainties in computational structural dynamics...|$|R
50|$|Several {{methods of}} {{analysis}} are available. Power spectral density (PSD), using parametric or <b>nonparametric</b> <b>methods,</b> provides basic information on the power distribution across frequencies. One {{of the most commonly}} used PSD methods is the discrete Fourier transform.Methods for the calculation of PSD may be generally classified as nonparametric and parametric. In most instances, both methods provide comparable results. The advantages of the <b>nonparametric</b> <b>methods</b> are (1) the simplicity of the algorithm used (fast Fourier transform FFT in most of the cases) and (2) the high processing speed. The advantages of parametric methods are (1) smoother spectral components that can be distinguished independent of preselected frequency bands, (2) easy postprocessing of the spectrum with an automatic calculation of low- and high-frequency power components with an easy identification of the central frequency of each component, and (3) an accurate estimation of PSD even on a small number of samples on which the signal is supposed to maintain stationarity. The basic disadvantage of parametric methods is the need of verification of the suitability of the chosen model and of its complexity (that is, the order of the model).|$|E
50|$|When exact p-values and {{confidence}} intervals are computed under a certain distribution, {{such as the}} normal distribution, then the underlying methods {{are referred to as}} exact parametric methods. The exact methods that do not make any distributional assumptions are referred to as exact <b>nonparametric</b> <b>methods.</b> The latter has the advantage of making fewer assumptions whereas, the former tend to yield more powerful tests when the distributional assumption is reasonable. For advanced methods such as higher-way ANOVA regression analysis, and mixed models, only exact parametric methods are available.|$|E
5000|$|In statistics, a rank {{correlation}} is any of several statistics that measure an ordinal association—the relationship between rankings of different ordinal variables or different rankings {{of the same}} variable, where a [...] "ranking" [...] is the assignment of the ordering labels [...] "first", [...] "second", [...] "third", etc. to different observations of a particular variable. A {{rank correlation}} coefficient measures the degree of similarity between two rankings, {{and can be used}} to assess the significance of the relation between them. For example, two common <b>nonparametric</b> <b>methods</b> of significance that use rank correlation are the Mann-Whitney U test and the Wilcoxon signed-rank test.|$|E
40|$|We compare {{parametric}} and <b>nonparametric</b> estimation <b>methods</b> in {{the context}} of PBPK modeling using simulation studies. We implement a Monte Carlo Markov Chain simulation technique in the parametric method, and a functional analytical approach to estimate the probability distribution function directly in the <b>nonparametric</b> <b>method.</b> The simulation results suggest an advantage for the parametric method when the underlying model can capture the true population distribution. On the other hand, our calculations demonstrate some advantages for a nonparametric approach since it is a more cautious (and hence safer) way to assess the distribution when one does not have sufficient knowledge to assume a population distribution form or parametrization. The parametric approach has obvious advantages when one has significant a priori information on the distributions sought, although when used in the <b>nonparametric</b> <b>method,</b> prior information can also significantly facilitate estimation. Key words: PBPK model; nonlinear mixed effect model; parametric method; nonparametric method; MCMC; Prohorov metric. ...|$|R
30|$|The SVMs {{are mainly}} a <b>nonparametric</b> <b>method,</b> yet some {{parameters}} {{need to be}} tuned before the optimization. In the case of Gaussian kernel, there are two parameters: C, which is the penalty term, and σ, which is {{the width of the}} exponential.|$|R
40|$|We {{will examine}} the {{validity}} of {{means and standard deviations}} as a basis for climate data products. We will explore the conditions under which these two simple statistics are inadequate summaries of the underlying empirical probability distributions by contrasting them with a <b>nonparametric,</b> <b>method</b> called Deterministic Annealing techniqu...|$|R
50|$|Stevens (1946) argued that, {{because the}} {{assumption}} of equal distance between categories does not hold for ordinal data, the use of {{means and standard deviations}} for description of ordinal distributions and of inferential statistics based on means and standard deviations was not appropriate. Instead, positional measures like the median and percentiles, in addition to descriptive statistics appropriate for nominal data (number of cases, mode, contingency correlation), should be used. <b>Nonparametric</b> <b>methods</b> have been proposed as the most appropriate procedures for inferential statistics involving ordinal data, especially those developed for the analysis of ranked measurements. However, use of parametric statistics for ordinal data may be permissible with certain caveats to take advantage of the greater range of available statistical procedures.|$|E
5000|$|The {{analysis}} of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback-Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. [...] Commonly, methods for modeling complex distributions rely on parametric assumptions {{that may be}} unfounded or computationally challenging (e.g. Gaussian mixture models), while <b>nonparametric</b> <b>methods</b> like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings.|$|E
50|$|He has coauthored two {{well-known}} books (Local Polynomial Modeling (1996) and Nonlinear time series: Parametric and <b>Nonparametric</b> <b>Methods</b> (2003)) and authored or coauthored over 170 {{articles on}} finance, economics, computational biology, semiparametric and non-parametric modeling, statistical learning, nonlinear time series, survival analysis, longitudinal data analysis, {{and other aspects}} of theoretical and methodological statistics. He has been consistently ranked as a top 10 highly-cited mathematical scientist. He has received various awards in recognition of his work on statistics, financial econometrics, and computational biology. These include the 2000 COPSS Presidents' Award, given annually to an outstanding statistician under age 40; an invitation to speak at The 2006 International Congress for Mathematicians; the Humboldt Research Award for lifetime achievement in 2006; the Morningside Gold Medal of Applied Mathematics in 2007, honoring triennially an outstanding applied mathematician of Chinese descent; a Guggenheim Fellowship in 2009; the Pao-Lu Hsu Prize (2013), presented every three years by the International Chinese Statistical Association to individuals under the age of 50; and the Guy Medal in Silver (2014), presented once a year by the Royal Statistical Society.|$|E
40|$|We {{propose a}} <b>nonparametric</b> <b>method</b> to {{determine}} the functional form of the noise density in discrete-time stochastic volatility models of financial returns. Our approach suggests that the assumption of Gaussian noise is often adequate, but we do observe deviations from Gaussian noise for some assets, for instance gol...|$|R
40|$|Mode {{clustering}} is a <b>nonparametric</b> <b>method</b> for clustering {{that defines}} clusters as the basins of attraction of a density estimator’s modes. We provide several enhancements to mode clustering: (i) a “soft ” cluster assignment, (ii) {{a measure of}} connectivity between clusters, and (iii) an approach to visualizing the clusters...|$|R
40|$|A <b>nonparametric</b> kernel <b>methods</b> is {{proposed}} and evaluated performance for estimating annual maximum stream flow quantiles. The bandwidth of the estimator is estimated {{by using an}} optimal technique and a cross-validation technique. Results obtained from {{a limited amount of}} real data from Malaysia show that quantiles estimated by <b>nonparametric</b> <b>method</b> using these techniques have small root mean square error and root mean absolute error. Based on correlation coefficient test shown that the nonparametric model approach is accurate, uniform and flexible alternatives to parametric models for flood frequency analysis...|$|R
5000|$|In machine learning, the kernel {{embedding}} of distributions (also {{called the}} kernel mean or mean map) comprises {{a class of}} <b>nonparametric</b> <b>methods</b> in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). [...] A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis. [...] This learning framework is very general and {{can be applied to}} distributions over any space [...] on which a sensible kernel function (measuring similarity between elements of [...] ) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in , discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects. [...] The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf. A review of recent works on kernel embedding of distributions can be found in [...]|$|E
5000|$|Aaron Brown, an author, quant {{and finance}} {{professor}} at Yeshiva and Fordham Universities, said that [...] "the book reads as if Taleb has {{never heard of}} <b>nonparametric</b> <b>methods,</b> data analysis, visualization tools or robust estimation." [...] Nonetheless, he calls the book [...] "essential reading" [...] and urges statisticians to overlook the insults to get the [...] "important philosophic and mathematical truths." [...] Taleb replied in the second edition of The Black Swan that [...] "One {{of the most common}} (but useless) comments I hear is that some solutions can come from 'robust statistics.' I wonder how using these techniques can create information where there is none". While praising the book, Westfall and Hilbe in 2007 complained that Taleb's criticism is [...] "often unfounded and sometimes outrageous." [...] Taleb, writes John Kay, [...] "describes writers and professionals as knaves or fools, mostly fools. His writing is full of irrelevances, asides and colloquialisms, reading like the conversation of a raconteur rather than a tightly argued thesis. But it is hugely enjoyable - compelling but easy to dip into. Yet beneath his rage and mockery are serious issues. The risk management models in use today exclude the very events against which they claim to protect the businesses that employ them. These models import a veneer of technical sophistication... Quantitative analysts have lulled corporate executives and regulators into an illusory sense of security." [...] Taleb felt that academics showed [...] "bad faith" [...] by criticizing a literary book that claimed to be a literary book and by ignoring the empirical evidence provided in his appendix and more technical works. Berkeley statistician David Freedman said that efforts by statisticians to refute Taleb's stance have been unconvincing.|$|E
40|$|We use two {{different}} <b>nonparametric</b> <b>methods</b> {{to determine whether}} there were multiple regimes in U. S. monetary policy over the period 1955 — 2003. We model monetary policy using {{two different}} versions of Taylor’s rule for the nominal interest rate target. By contrast with parametric tests for regime changes, the <b>nonparametric</b> <b>methods</b> we use allow the data to determine the dimensions on which to split the sample for purposes of estimating the coefficients of the Taylor rule. We find evidence for a few structural breaks and consistent agreement between our two <b>nonparametric</b> <b>methods</b> on the dating of those breaks...|$|E
40|$|The {{recognition}} {{rate of the}} typical <b>nonparametric</b> <b>method</b> "k-Nearest Neighbor rule (kNN) " is degraded when the dimensionality of feature vectors is large. Another <b>nonparametric</b> <b>method</b> "linear subspace methods" cannot represent the local distribution of patterns, so {{recognition rate}}s decrease when pattern distribution is not normal distribution. This paper presents a classifier that outputs the class of a test sample by measuring {{the distance between the}} test sample and the average patterns, which are calculated using nearest neighbors belonging to individual categories. A kernel method can be applied to this classifier for improving its recognition rates. The performance of those methods is verified by experiments with handwritten digit patterns and two class artificial ones. Ninth International Workshop on Frontiers in Handwriting Recognition (IWFHR' 04), 26 - 29 Oct. 200...|$|R
40|$|AbstractA <b>nonparametric</b> <b>method</b> for the {{estimation}} of a dose-response relation is described. Up to two quantitative covariates can be handled. Subject only to the constraints of monotonicity an estimation procedure together with an approach for testing the association is described. An application to data of an epidemiological study is presented...|$|R
40|$|This {{scientific}} research considers {{the problem of}} estimating class probabilities for an unknown pattern, which is represented by a feature vector, using a <b>nonparametric</b> <b>method,</b> such as Parzen Windows. This papers contribution is {{the application of the}} nonparametric density estimation approach in the intra-urban land cover classification. Pages: 6869 - 687...|$|R
