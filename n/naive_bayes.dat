4346|31|Public
25|$|In 2015, Zhou et al. {{suggested}} to apply <b>naive</b> <b>Bayes</b> classifier to detect pathological brains.|$|E
25|$|Data mining {{specific}} functionality {{is exposed}} via the DMX query language. Analysis Services includes various algorithms—Decision trees, clustering algorithm, <b>Naive</b> <b>Bayes</b> algorithm, time series analysis, sequence clustering algorithm, linear and logistic regression analysis, and neural networks—for use in data mining.|$|E
25|$|In {{practical}} terms, these complexity results {{suggested that}} while Bayesian networks were rich representations for AI and machine learning applications, {{their use in}} large real-world applications {{would need to be}} tempered by either topological structural constraints, such as <b>naïve</b> <b>Bayes</b> networks, or by restrictions on the conditional probabilities. The bounded variance algorithm was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 1/p(n) where p(n) was any polynomial on the number of nodes in the network n.|$|E
25|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning vector quantization, decision tree analysis, random forests, <b>naive</b> <b>Bayes,</b> logistic regression, kernel regression, artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, covariance matrix self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or information gain (entropy).|$|E
25|$|Even {{though in}} most {{statistical}} classification methods, the neutral class is ignored {{under the assumption}} that neutral texts lie near the boundary of the binary classifier, several researchers suggest that, as in every polarity problem, three categories must be identified. Moreover, it can be proven that specific classifiers such as the Max Entropy and the SVMs can benefit from the introduction of a neutral class and improve the overall accuracy of the classification. There are in principle two ways for operating with a neutral class. Either, the algorithm proceeds by first identifying the neutral language, filtering it out and then assessing the rest in terms of positive and negative sentiments, or it builds a three-way classification in one step. This second approach often involves estimating a probability distribution over all categories (e.g. <b>naive</b> <b>Bayes</b> classifiers as implemented by the NLTK). Whether and how to use a neutral class depends {{on the nature of the}} data: if the data is clearly clustered into neutral, negative and positive language, it makes sense to filter the neutral language out and focus on the polarity between positive and negative sentiments. If, in contrast, the data are mostly neutral with small deviations towards positive and negative affect, this strategy would make it harder to clearly distinguish between the two poles.|$|E
50|$|For {{some types}} of {{probability}} models, <b>naive</b> <b>Bayes</b> classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for <b>naive</b> <b>Bayes</b> models uses the method of maximum likelihood; in other words, one can work with the <b>naive</b> <b>Bayes</b> model without accepting Bayesian probability or using any Bayesian methods.|$|E
5000|$|Classification: Perceptron, SGD classifier, <b>Naive</b> <b>bayes</b> classifier.|$|E
50|$|Averaged one-dependence estimators (AODE) is a {{probabilistic}} classification learning technique. It {{was developed}} to address the attribute-independence problem of the popular <b>naive</b> <b>Bayes</b> classifier. It frequently develops substantially more accurate classifiers than <b>naive</b> <b>Bayes</b> {{at the cost of}} a modest increase in the amount of computation.|$|E
50|$|In the {{statistics}} {{and computer science}} literature, <b>Naive</b> <b>Bayes</b> models are known {{under a variety of}} names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but <b>naive</b> <b>Bayes</b> is not (necessarily) a Bayesian method.|$|E
5000|$|... on document-level: Searching, clustering, and <b>Naive</b> <b>Bayes</b> {{classifier}} ...|$|E
50|$|It can be {{drastically}} simplified {{by assuming}} that the probability of appearance of a word knowing {{the nature of the}} text (spam or not) is independent of the appearance of the other words. This is the <b>naive</b> <b>Bayes</b> assumption and this makes this spam filter a <b>naive</b> <b>Bayes</b> model.|$|E
50|$|Additive {{smoothing}} {{is commonly}} {{a component of}} <b>naive</b> <b>Bayes</b> classifiers.|$|E
5000|$|... #Subtitle level 5: A second example: <b>Naive</b> <b>Bayes</b> {{document}} clustering ...|$|E
5000|$|<b>Naive</b> <b>Bayes</b> {{classifier}} with multinomial or multivariate Bernoulli event models.|$|E
5000|$|Generative: <b>Naive</b> <b>Bayes,</b> Latent Dirichlet Allocation, Probabilistic {{context-free}} grammar, Hidden Markov Models ...|$|E
5000|$|In {{the case}} of {{discrete}} inputs (indicator or frequency features for discrete events), <b>naive</b> <b>Bayes</b> classifiers form a generative-discriminative pair with (multinomial) logistic regression classifiers: each <b>naive</b> <b>Bayes</b> classifier {{can be considered a}} way of fitting a probability model that optimizes the joint likelihood , while logistic regression fits the same probability model to optimize the conditional [...]|$|E
5000|$|<b>Naïve</b> <b>Bayes</b> {{based on}} Bayes {{conditional}} probability rule {{is used for}} performing classification tasks. <b>Naïve</b> <b>Bayes</b> assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed {{when faced with the}} [...] "curse of dimensionality" [...] problem, i.e. when the number of predictors is very high.|$|E
5000|$|The multinomial <b>naive</b> <b>Bayes</b> {{classifier}} {{becomes a}} linear classifier when expressed in log-space: ...|$|E
50|$|In 2015, Zhou et al. {{suggested}} to apply <b>naive</b> <b>Bayes</b> classifier to detect pathological brains.|$|E
5000|$|... {{classification}} and regression: support vector machines, logistic regression, linear regression, decision trees, <b>naive</b> <b>Bayes</b> classification ...|$|E
50|$|In natural {{language}} processing, multinomial LR classifiers {{are commonly used}} {{as an alternative to}} <b>naive</b> <b>Bayes</b> classifiers because they do not assume statistical independence of the random variables (commonly known as features) that serve as predictors. However, learning in such a model is slower than for a <b>naive</b> <b>Bayes</b> classifier, and thus may not be appropriate given {{a very large number of}} classes to learn. In particular, learning in a <b>Naive</b> <b>Bayes</b> classifier is a simple matter of counting up the number of co-occurrences of features and classes, while in a maximum entropy classifier the weights, which are typically maximized using maximum a posteriori (MAP) estimation, must be learned using an iterative procedure; see #Estimating the coefficients.|$|E
5000|$|Very often uses Machine Learning Algorithms, as K-Means, <b>Naive</b> <b>Bayes</b> Filter, C4.5, C5.0, J48, or Random Forest ...|$|E
5000|$|VFDTc (2006) extends VFDT for {{continuous}} data, concept drift, {{and application of}} <b>Naive</b> <b>Bayes</b> classifiers in the leaves.|$|E
50|$|Despite their naive {{design and}} {{apparently}} oversimplified assumptions, <b>naive</b> <b>Bayes</b> classifiers have worked quite well in many complex real-world situations. In 2004, {{an analysis of}} the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of <b>naive</b> <b>Bayes</b> classifiers. Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.|$|E
50|$|Despite {{the fact}} that the {{far-reaching}} independence assumptions are often inaccurate, the <b>naive</b> <b>Bayes</b> classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While <b>naive</b> <b>Bayes</b> often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the <b>naive</b> <b>Bayes</b> classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the <b>naive</b> <b>Bayes</b> classifier are discussed in the literature cited below.|$|E
50|$|Since the <b>Naïve</b> <b>Bayes</b> {{classifier}} {{is simple}} yet effective, {{it is usually}} used as a baseline method for comparison.|$|E
50|$|Contextual {{classification}} of image data {{is based on}} the Bayes minimum error classifier (also known as a <b>naive</b> <b>Bayes</b> classifier).|$|E
5000|$|Given a {{collection}} [...] of labeled samples [...] and unlabeled samples , start by training a <b>naive</b> <b>Bayes</b> classifier on [...]|$|E
5000|$|Convergence is {{determined}} based on improvement {{to the model}} likelihood , where [...] denotes {{the parameters of the}} <b>naive</b> <b>Bayes</b> model.|$|E
5000|$|The {{discussion}} {{so far has}} derived {{the independent}} feature model, that is, the <b>naive</b> <b>Bayes</b> probability model. The <b>naive</b> <b>Bayes</b> classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; {{this is known as}} the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label [...] for some [...] as follows: ...|$|E
50|$|<b>Naive</b> <b>Bayes</b> is {{a simple}} {{technique}} for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It {{is not a single}} algorithm for training such classifiers, but a family of algorithms based on a common principle: all <b>naive</b> <b>Bayes</b> classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A <b>naive</b> <b>Bayes</b> classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.|$|E
50|$|An {{advantage}} of <b>naive</b> <b>Bayes</b> {{is that it}} only requires {{a small number of}} training data to estimate the parameters necessary for classification.|$|E
50|$|Later {{supervised}} learning usually works much better when the raw input data is first translated {{into such a}} factorial code. For example, suppose the final goal is to classify images with highly redundant pixels. A <b>naive</b> <b>Bayes</b> classifier will assume the pixels are statistically independent random variables and therefore fail to produce good results. If the data are first encoded in a factorial way, however, then the <b>naive</b> <b>Bayes</b> classifier will achieve its optimal performance (compare Schmidhuber et al. 1996).|$|E
50|$|Instead of {{decision}} trees, linear {{models have been}} proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and <b>naive</b> <b>Bayes</b> classifiers.|$|E
50|$|In machine learning, <b>naive</b> <b>Bayes</b> {{classifiers}} are {{a family}} of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.|$|E
50|$|Classification: Building a {{model to}} assign items into {{different}} labeled groups. DAAL provides multiple algorithms in this area, including <b>Naïve</b> <b>Bayes</b> classifier, Support Vector Machine, and multi-class classifiers.|$|E
