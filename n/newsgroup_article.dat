1|25|Public
2500|$|Timothy Chow’s article [...] {{is a good}} {{introduction}} to the concepts of forcing that avoids a lot of technical detail. [...] This paper grew out of Chow’s <b>newsgroup</b> <b>article</b> [...] [...] In addition to improved exposition, the Beginner’s Guide includes a section on Boolean-valued models.|$|E
50|$|As {{local area}} {{networks}} and Internet participation proliferated, it became desirable to allow newsreaders {{to be run}} on personal computers connected to local networks. The resulting protocol was NNTP, which resembled the Simple Mail Transfer Protocol (SMTP) but was tailored for exchanging <b>newsgroup</b> <b>articles.</b>|$|R
50|$|This {{construct}} {{displays a}} person's genealogy compactly, {{without the need}} for a diagram such as a family tree. It is particularly useful in situations where one may be restricted to presenting a genealogy in plain text, for example, in e-mails or <b>newsgroup</b> <b>articles.</b> In effect, an ahnentafel is a method for storing a binary tree in an array by listing the nodes (individuals) in level-order (in generation order).|$|R
40|$|This thesis {{introduces}} {{a new approach}} to designing a Message Handling Assistant (MA). It presents a model of an MA and an intention extraction function for text messages, such as emails and <b>Newsgroups</b> <b>articles.</b> Based on a speech act theory and the belief-desire-intention (BDI) theory of rational agency, we define a generic MA. By interpreting intuitive descriptions of the desired behaviours of an MA using the BDI theory and speech act theory, we conjecture that intentions of messages alone provide enough information needed to capture user models and to reason how messages should be processed. To identify intentions of messages written in natural language, we develop a model of an intention extraction function that maps messages to intentions. This function is modelled in two steps. First, each sentence in a message is converted into a tuple (performative, proposition) using a dialogue act classifier. Second, the sender 2 ̆ 7 s intentions are formulated from the tuples using constraints for felicitous human communication. As an investigation of the use of machine learning technologies for designing the intention extraction function, four dialog act classifiers are implemented and evaluated on <b>Newsgroups</b> <b>articles.</b> The thesis also proposes a semantic communication framework, which integrates the agent and Internet technologies for automatic message composing and ontology exchange services...|$|R
5000|$|Newsgroups - {{a list of}} one or more <b>newsgroups</b> {{where the}} <b>article</b> is {{intended}} to appear ...|$|R
40|$|This paper {{presents}} {{a study of}} the similarity between Cantonese and Mandarin spoken and written texts. Spontaneous speech in Cantonese consists of colloquial and filler phrases but it’s keywords similar to Mandarin. We use a statistical tool to extract Cantonese phrases from a spontaneous speech database. We collected using a Wizard-of-Oz setup. More fillers are collected from written Cantonese downloaded from online newsgroups. We quantify the similarity between Cantonese and Mandarin texts by using Zipf's Law of Language Distance [4] and using R 2 regression scores. The scores show that Taiwan and Chinese <b>newsgroup</b> <b>articles</b> are more similar to each other (with 0. 83 R 2 regression score) than with Hong Kong articles (0. 79 and 0. 67), underlining the language difference between Cantonese and Mandarin. 1...|$|R
5000|$|The word sporgery {{was coined}} in the {{newsgroup}} alt.religion.scientology, an Internet newsgroup where people discuss the controversial belief system of Scientology. One {{of the various}} actions of the [...] "war" [...] between Scientology and the Internet involved various individuals who had posted {{more than one million}} forged <b>newsgroup</b> <b>articles</b> to the <b>newsgroup,</b> using the message headers (valid names and e-mail addresses) of articles written by Scientology critics and other legitimate posters, and appending to those headers the bodies of other articles harvested from racist newsgroups. The result was to flood the newsgroup with over one million forged articles that made the other posters appear to be hateful [...] "racist bigots." [...] (Critics accused Scientology of planning and conducting the spam flood, but the organization denied this.) ...|$|R
40|$|Abstract. The {{most common}} model of machine {{learning}} algorithms involves two life-stages, namely the learning stage and the application stage. The cost of human expertise makes difficult the labeling of large sets of data for the training of {{machine learning algorithms}}. In this paper, we propose to challenge this strict dichotomy in the life cycle while addressing the issue of labeling of data. We discuss a learning paradigm called Continuous Learning. After an initial training based on human-labeled data, a Continuously Learning algorithm iteratively trains itself with the result of its own previous application stage and without the privilege of any external feedback. The intuitive motivation and idea of this paradigm are elucidated, followed by explanations on how it differs from other learning models. Finally, empirical evaluation of Continuous Learning applied to the Naive Bayesian Classifier for the classification of <b>newsgroup</b> <b>articles</b> of a well-known benchmark is presented. 1...|$|R
40|$|Abstract. Clustering text {{documents}} {{is a basic}} enabling {{technique in}} a wide variety of Information and Knowledge Management applications. This paper presents an incremental clustering system to organize and manage <b>Newsgroup</b> <b>articles.</b> It serves administrators and readers of a Newsgroup to archive important postings and to get a structured overview on current developments and topics. To be practically applicable, such a system must fulfill two conditions. First, it must be able to process rapidly changing text streams, modifying the cluster structure dynamically by adding, deleting and restructuring clusters. Second, it must consider the user in the incremental process. Severe changes in the organization structure are unacceptable for most users, even if they are optimal from the point of view of an abstract clustering criterion. We propose an approach to model the cost to accommodate to changes in the cluster structure explicitly. Users then may constraint, which changes are acceptable to them...|$|R
40|$|We {{present a}} new {{approach}} to clustering based on the observation that is easier to criticize than to construct. " Our approach of semi-supervised clustering allows a user to iteratively provide feedback to a clustering algorithm. The feedback is incorporated in the form of constraints which the clustering algorithm attempts to satisfy on future iterations. These constraints allow the user to guide the clusterer towards clusterings of the data that the user nds more useful. We demonstrate semi-supervised clustering with a system that learns to cluster news stories from a Reuters data set. Introduction Consider the following problem: you are given 100, 000 text documents (e. g., papers, <b>newsgroup</b> <b>articles,</b> or web pages) and asked to group them into classes or into a hierarchy such that related documents are grouped together. You are not told what classes or hierarchy to use or what documents are related; you have some criteria in mind, but {{may not be able to}} say exactly w [...] ...|$|R
40|$|We propose several {{algorithms}} {{using the}} vector space model to classify the news articles {{posted on the}} NETNEWS according to the newsgroup categories. The baseline method combines the terms of all the <b>articles</b> of each <b>newsgroup</b> in the training set to represent the newsgroups as single vectors. After training, the incoming news articles are classified based on their similarity to the existing newsgroup categories. We propose to use the following techniques to improve the classification performance of the baseline method: (1) use routing (classification) accuracy and the similarity values to refine the training set; (2) update the underlying term structures periodically during testing; and (3) apply k-means clustering to partition the <b>newsgroup</b> <b>articles</b> and represent each newsgroup by k vectors. Our test collection consists of the real news articles and the 519 subnewsgroups under the REC newsgroup of NETNEWS {{in a period of}} 3 months. Our experimental results demonstrate that the technique [...] ...|$|R
40|$|Text {{categorization}} is {{a fundamental}} task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents {{is the presence of}} different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99. 8 % correct classification rate on Usenet <b>newsgroup</b> <b>articles</b> written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80 % correct classificatio [...] ...|$|R
50|$|Sporgery is the {{disruptive}} act of {{posting a}} flood of articles to a Usenet <b>newsgroup,</b> with the <b>article</b> headers falsified so that {{they appear to have}} been posted by others. The word is a portmanteau of spam and forgery, coined by German software developer and critic of Scientology Tilman Hausherr.|$|R
40|$|Abstract We propose several {{algorithms}} {{using the}} vector space mode 1 to classify the news articles {{posted on the}} NETNEWS according to the newsgroup categories. The baseline method combines the terms of all the arti-cles of each newsgroup in the training set to represent the newsgroups as single vectors. After training, the incom-ing news articles are classified based on their similarity to the existing newsgroup categories. We propose to use the following techniques to improve the classification per-formance of the baseline method: (1) use routing (classi-fication) accuracy and the similarity values to refine the training set; (2) update the underlying term structures periodically during testing; and (3) apply k-means clus-tering to partition the <b>newsgroup</b> <b>articles</b> and represent each newsgroup by k vectors. Our test collection con-sists of the real news articles and the 519 subnewsgroups under the REC newsgroup of NETNEWS {{in a period of}} 3 months. Our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage. The technique of periodical updates improves the routing accuracy ranging from 20 % to 100 % but incurs runtime overhead. Finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improve-ment in routing accuracy, ranging from 60 % to lOO%, while causing only slightly higher storage requirements. ...|$|R
50|$|All premiership {{games are}} {{broadcast}} on television, either free-to-air or cable. Online, the ABC, {{as well as}} major <b>newsgroups</b> provide <b>articles</b> on Rugby League, bylined in general by a reporter who is exclusively a sports correspondent. The official publication for the NRL is Big League. Interest in rugby league is highest in the eastern states; as well, many of {{the large number of}} Australian expatriates living and working overseas are avidly interested in the season's games, and are able to ensure that they are kept up-to-date by accessing on-line versions of stories provided by major media organisations.|$|R
40|$|The Conversation Map {{system is}} a Usenet {{newsgroup}} browser that analyzes the text of an archive of newsgroup messages and outputs a graphical interface {{that can be used}} to search and read the messages of the archive. The system incorporates a series of novel text analysis procedures that automatically computes (1) a set of social networks detailing who is responding to and/or citing whom in the newsgroup; (2) a set of “discussion themes ” that are frequently used in the newsgroup archive; and, (3) a set of semantic networks that represent the main terms under discussion and some of their relationships to one another. The text analysis procedures are written in the Per 1 programming language. Their results are recorded as HTML, and the HTML is displayed with a Java applet. With the Java-based graphical interface one can browse a set of Usenet <b>newsgroup</b> <b>articles</b> according to who is “talking ” to whom, what they are “talking ” about, and the central terms and possible emergent metaphors of the conversation. In this paper it is argued that the Conversation Map system is just one example of a new kind of content-based browser that will combine the analysis powers of computational linguistics with a graphical interface to allow network documents and messages to be viewed in ways not possible with today’s, existing, formatbased browsers which do not analyze the contents of the documents or messages. Keywords Content-based browser, social network, social navigation...|$|R
40|$|Relational Databases are universally {{conceived}} as an advance over their predecessors Network and Hierarchical models. Superior in every querying respect, {{they turned out}} to be surprisingly incomplete when modeling transitive dependencies. Almost every couple of months a question how to model a tree in the database surfaces at comp. database. theory <b>newsgroup.</b> This <b>article</b> completes a series of articles exploring Nested Intervals Model. Previous articles introduced tree encoding with Binary Rational Numbers. However, binary encoding grows exponentially, both in breadth and in depth. In this article, we'll leverage Farey fractions in order to overcome this problem. We'll also demonstrate that our implementation scales to a tree with 1 M nodes. Comment: 1 figur...|$|R
40|$|A {{number of}} {{real-world}} domains {{such as social}} networks and e-commerce involve heterogeneous data that describes relations between multiple classes of entities. Understanding the natural structure {{of this type of}} heterogeneous relational data is essential both for exploratory analysis and for performing various predictive modeling tasks. In this paper, we propose a principled multi-way clustering framework for relational data, wherein different types of entities are simultaneously clustered based not only on their intrinsic attribute values, but also on the multiple relations between the entities. To achieve this, we introduce a relation graph model that describes all the known relations between the different entity classes, in which each relation between a given set of entity classes is represented in the form of multi-modal tensor over an appropriate domain. Our multi-way clustering formulation is driven by the objective of capturing the maximal “information ” in the original relation graph, i. e., accurately approximating the set of tensors corresponding to the various relations. This formulation is applicable to all Bregman divergences (a broad family of loss functions that includes squared Euclidean distance, KL-divergence), and also permits analysis of mixed data types using convex combinations of appropriate Bregman loss functions. Furthermore, we present a large family of structurally different multi-way clustering schemes that preserve various linear summary statistics of the original data. We accomplish the above generalizations by extending a recently proposed key theoretical result, namely the minimum Bregman information principle [1], to the relation graph setting. We also describe an efficient multi-way clustering algorithm based on alternate minimization that generalizes a number of other recently proposed clustering methods. Empirical results on datasets obtained from real-world domains (e. g., movie recommendations, <b>newsgroup</b> <b>articles)</b> demonstrate the generality and efficacy of our framework. ...|$|R
40|$|Usenet News is a {{worldwide}} open forum for discussion. It offers groups where users may submit <b>articles.</b> These <b>newsgroups</b> and <b>articles</b> are distributed on the Internet using the "Network News Transfer Protocol" (NNTP). Since {{the number of}} users has increased steadily and only few improvements have been applied to NNTP, NNTP begins to reach its limits. We will explain these limits and will present two strategies to improve the situation. One strategy may be the extension of NNTP. However, this requires a modification of the currently existing news software. The other strategy is the exploitation of caching mechanisms for Usenet News. This is the topic we will concentrate on in the following diploma thesis. We will analyze which areas can profit {{by the use of}} a News Cache and will give a description of our News Cache implementation...|$|R
40|$|This thesis {{presents}} a colloquial language modeling technique for spontaneous Cantonese speech recognition. Cantonese is a language linguistically distinct from standard written Chinese. It {{is different in}} terms of certain vocabulary and word order. Therefore, Cantonese large vocabulary continuous speech recognition (LVCSR) systems need to be trained on Cantonese language model. Since Cantonese is not a written language, {{there is a lack}} of Cantonese text corpus. Moreover, spoken Cantonese tends to be more spontaneous and colloquial than spoken Mandarin. To underline the language difference between Cantonese and Mandarin (standard Chinese), we quantify the similarity between Cantonese and Mandarin texts by applying Zipf's Law of Language Distance and R 2 regression scores to measure common words between the two languages. These scores show that Taiwan and Chinese <b>newsgroup</b> <b>articles</b> are more similar to each other (with a 0. 83 R 2 regression score) than with Hong Kong articles (0. 79 and 0. 67). Any similarity of Cantonese and Mandarin is mostly due to an overlap in content word vocabulary. To collect a spontaneous Cantonese speech corpus, we set up a Wizard-of-Oz database collection system. Spontaneous Cantonese speech consists of colloquial and filler phrases with keywords that are shared between Cantonese and Mandarin. We use a statistical tool to extract colloquial and filler phrases from this database. Additional filler phrases are collected from written Cantonese downloaded from online newsgroups. The extracted filler and colloquial phrases are used for keyword spotting and colloquial Cantonese dictation system. By applying garbage and filler phrase modeling, we obtain 82. 5 % keyword spotting accuracy, this gives a 33 % improvement compared to our baseline system. In order to train a Cantonese language model for our colloquial Cantonese LVCSR system, we adapt our baseline Mandarin language model to Cantonese language model using linear interpolation between large Mandarin text corpus and small amount of Cantonese text corpus. This gives 30 % improvement of character accuracy compared with our baseline system...|$|R
5000|$|The {{receiving}} server {{examines the}} incoming articles. A message is normally discarded if the Message-ID is duplicated by an article already received (i.e., another server sent {{it in the}} meantime), the Date or Expires lines indicate that the article is too old, the header syntax appears to be invalid, the Approved header is missing for a moderated newsgroup, or additional local rules disallow it. Most servers also maintain a list of active newsgroups. If the Newsgroups header of a new article does not match the active list, it may be discarded or placed in a special [...] "junk" [...] <b>newsgroup.</b> Once the <b>article</b> is stored, the server attempts to retransmit it to any servers in its own newsfeed list.|$|R
5000|$|Extreme Tourist Afghanistan was {{critically}} acclaimed by both western and Afghan journalists for its portrayal of Afghans as real {{people and not}} just media stereotypes. In Australia there was extensive praise for the show from national and regional newspapers. The Sydney Morning Herald described the show as [...] "highly recommended [...] ". In the UK's Guardian newspaper the show was quoted in {{an article about the}} Taliban. In the USA, the show gained attention through the McClatchy <b>newsgroup</b> in their <b>article</b> Aussie's Afghan Travel Show a Tribute to Crazy Love. It's been Afghan own reaction to the show that has been, arguably, the most important. The BBC Persian Service has featured the show and interviewed Sabour Bradley following widespread positive discussion about the show amongst Dari and Farsi speakers. Afghan Voice Radio has also interviewed Sabour Bradley.|$|R
40|$|As the world-wide {{computer}} network becomes ubiquitous, new tools have been developed, {{such as the}} World Wide Web (WWW), for the delivery of multimedia hypertext-based documents. Similarly, {{there has been an}} explosion in the amount of email, bulletin boards, and Usenet News available. This has led to a major problem of information overload: we are slowly but surely being overwhelmed by the amount of information available to us. To address this problem, the New Technology Initiative (NTI) of the Joint Information Systems Committee (JISC) is funding the ELF project over a period of two years to develop a tool, the Electronic Learning Facilitator. This tool is aimed at assisting learners to make effective use of the information available. The ELF project started in December 1993, with the goal of producing a tool for installation in UK academic sites. This paper first discusses the new networked information sources that are appearing around the globe, and then analyses the requirements of learners for accessing this information. From the requirements, the paper shows the desirable features of an ELF, such as monitoring the user at work to gain hints as to what he or she is doing, being guided by the user directly, and using information gathered from other ELFs, or from monitoring the various information sources for announcements of new services, updates of information, etc. The main body of the paper illustrates the current progress on the project, outlining the implementation of the ELF- ELF is written using various high-level languages such as the Tool Command Language (TCL) and Perl, as well as the more traditional C++. It consists of various agents, each agent having a specific task to perform, such as monitoring a WWW site for any new documents, or monitoring a Usenet <b>Newsgroup</b> for <b>articles</b> that might be of particular interest to the owner of the agent. Finally, the paper reports on our plans to test users on a prototype...|$|R
40|$|RECOMMENDATION REQUIRES EFFORT Collaborative {{filtering}} {{methods have}} been applied {{to a number of}} domains like books, videos, audio CDs and Usenet news. These systems require some {{effort on the part of}} users before they can generate recommendations (see for example [5] on the 9 ̆ 3 cold-start 9 ̆ 4 problem). How much effort they require depends on domain and application. Some recommender systems for books or videos require rating a specific number of items before they generate the first prediction. If users can save money by not buying the wrong books and videos this effort might pay off. The Usenet news filter GroupLens [2] gathers ratings while users read articles so the process of building the profile and getting recommendations is interleaved. This strategy might lower the threshold for getting started. How much effort a user is willing to make depends not only on the domain but also on the personal value of the recommendation. Users who depend on the information in a newsgroup might make every required effort, but occasional users with a less serious information interest might not. To summarize: The higher the required effort the more potential users will quit. Recommending TV programs has much in common with recommending Usenet news articles. Like news articles TV programs are a daily updating stream unlike the rather stable databases within book and video recommender systems. There is a relatively high number of programs/ <b>newsgroup</b> <b>articles</b> every day. Recommendation is supposed to help users to cope with the flood and to find the few interesting items for them. How much effort are users going to spend on TV recommendations? Interviews showed: Not much. 9 ̆ 3 I don 9 ̆ 2 t watch much TV anyway 9 ̆ 4, 9 ̆ 3 I just want to know what 9 ̆ 2 s on tonight 9 ̆ 4 and so on. It seems that TV doesn 9 ̆ 2 t have much value to people, although statistics show that people do spend a lot of time watching. All users we asked already had experience with printed guides, which gave them a rather clear expectation of what a TV guide should do. Traditionally, both TV and TV guides have been broadcast media. TV makers broadcast, viewers watch. Editors write, readers read. Interactive and collaborative concepts are new here. It is obviously a risky undertaking to try to introduce new techniques in such a 9 ̆ 3 where is the beef 9 ̆ 4 situation. DESIGN GOALS The low disposition of interviewed persons to spend much effort on TV guides was the motivation for our group to try out a new way of gathering ratings. We made two basic design decisions: (a) don 9 ̆ 2 t ask users to do something they don 9 ̆ 2 t have an immediate benefit from and (b) don 9 ̆ 2 t overtax users with concepts unknown to them like rating, relevance or recommendation. Figure 1 gives an overview of our recommendation engine. Before programs reach users they are first globally rated and ranked according to the size-of-the-audience ratings (see below). Then they are selected according to user-individual genre profiles. Personal rating offsets are added. Next, programs are presented to the users as ordered lists or tables. Individual programs in the lists and tables are color-coded to indicate the personalized ratings. From these lists and tables users choose the programs they actually want to watch and collect them in their personal laundry lists. The laundry list serves as a reminder which can be printed out and taken to the VCR or TV set. Preliminary user tests suggest that the laundry list is perceived as a useful tool. To avoid any additional questioning we use the content of users 9 ̆ 2 laundry lists as an input for recommendation generation. Programs in the laundry lists are interpreted as being rated high and recommended to other users. This approach has similarities with the implicit ratings deduced from reading time of <b>newsgroup</b> <b>articles</b> described in [6]. Laundry list content is processed by the two modules sizeof- the-audience rating and opinion leaders. 9 ̆ 3 SIZE OF THE AUDIENCE 9 ̆ 4 The size-of-the-audience rating generates a general ranking of all programs. The rating of an individual program is calculated as the number of times the program occurs in users 9 ̆ 2 laundry lists. The more often a program has been selected for viewing the higher the rating. The size-of-the audience rating can lead users 9 ̆ 2 attention to unexpected events that are not covered by personal profiles. In Germany, for example, the Tour de France came first into the focus of public interest when the German Telekom team was very successful in 1997. Although (or because) it is not personalized, the size-of-the-audience rating can be useful for detecting such events. OPINION LEADERS Since the ratings delivered by the size-of-the-audience ratings are general to all users, so-called opinion leaders are added as a source of more distinct, personal and higher quality recommendation. Opinion leaders are users that publish their otherwise private laundry list along with their names in a public folder. The system offers and recommends opinion leaders like it recommends genres. 9 ̆ 3 You could have found all these programs more easily if you had subscribed to the genre Basketball and the opinion leader Lars Brückner. Do you want to do that now? 9 ̆ 4 How to become an opinion leader? Based on user profiles and laundry lists a program can estimate which users 9 ̆ 2 selections could be of interest for the community. Candidates should a) correlate with many other users, b) cover interest areas not yet covered by enough opinion leaders and c) use the laundry list on a regular basis. If these requirements are fulfilled the system will suggest that a user becomes an opinion leader. Being an opinion leader requires no extra work. But to deserve their name we want them to select more accurately or even to rate or annotate programs. Why should users do the extra work? 9 ̆ 3 Ways to provide compensatory benefits to those group members need to be found 9 ̆ 4 [3, p. 810]. We have to reward them. One way is to organize the opinion leaders in a competitive way and to reward the winners. The success of an opinion leader can be measured in subscriptions, i. e. the number of users that selected them as their favorites. Concept Tour de France: a) display opinion leaders ordered by their success as a leader board. The currently first and most successful one wears the yellow jersey b) reward the best ones by giving awards and prices at the end of each stage c) drop the least successful one when new users apply. DISCUSSION The central element of the suggested no-extra-effort rating system is the laundry list. Certainly: The laundry list is a magnitude less powerful than explicit seven-items scale ratings: It provides only Boolean input, reasons for selecting and not selecting items are ambiguous and it can lead to self-fulfilling prophecies. The applied technique shows similarities to last year 9 ̆ 2 s experiments at MyYahoo to correlate users according to uploaded bookmark lists. On the other hand the laundry list approach frees us from explaining users that they will help generating recommendations using it. The direct utility of the laundry list avoids lots of the trouble of explaining, convincing and persuading users to rate. This solves much of the general problem that many users experience the promised benefit associated with rating as being far away. After all, recommendation is a rather abstract concept and the question is how many first time users are willing to learn enough about it for it to become an incentive. But at the end, those users that want more than the mainstream recommendation of the size-of-the-audience rating do have to spend some effort. The system can suggest opinion leaders, but since these suggestions are based on the less informative laundry lists, users will have to check and fine tune to get equally good results as they would with systems using ratings on seven-items scales. How do opinion leaders relate to those systems where the virtual community is selected by the computer? In those systems one 9 ̆ 2 s virtual community can change by the hour. Opinion leaders are an attempt using active collaborative filtering [5] to provide 9 ̆ 3 names 9 ̆ 4 behind recommendations, like a large set of editors 9 ̆ 6 along with the option to become one. Hill et al. state: 9 ̆ 3 Recommendation and evaluations should come from people, not a black box machine or so-called agent 9 ̆ 4 [4, p. 196]. The opinion leader concept is built on the assumption of role specialization: 9 ̆ 3 Yet there is evidence that people naturally prefer to play distinct producer/consumer roles in the information ecology 9 ̆ 4 [1, p. 60]. It divides users into two distinct groups: A large majority of consumers that contribute only a little to a mass-average and a group of self-selected leaders that are willing to spend more effort on the system for getting social or monetary reward. How far can we get at zero user effort? We need more user tests to find out how much information we can draw out of implicit ratings and which other techniques can be applied to edit them and offer them to users. This will be subject to evaluations that will follow our online debut in May...|$|R
40|$|Automatic text {{document}} classification is {{a fundamental}} problem in machine learning. Given the dynamic nature and the exponential growth of the World Wide Web, one needs the ability to classify not only a massive number of documents, but also documents that belong to wide variety of domains. Some examples of the domains are e-mails, blogs, Wikipedia <b>articles,</b> news <b>articles,</b> <b>newsgroups,</b> online chats, etc. It is {{the difference in the}} writing style that differentiates these domains. Text documents are usually classified using supervised learning algorithms that require large set of pre-labeled data. This requirement, of labeled data, poses a challenge in classifying documents that belong to different domains. Our goal is to classify text documents in the testing domain without requiring any labeled documents from the same domain. Our research develops specialized cross-domain learning algorithms based the distributions over words obtained from a collection of text documents by topic models such as Latent Dirichlet Allocation (LDA). Our major contributions include (1) empirically showing that conventional supervised learning algorithms fail to generalize their learned models across different domains and (2) development of novel and specialized cross-domain classification algorithms that show an appreciable improvement over conventional methods used for cross-domain classification that is consistent for different datasets. Our research addresses many real-world needs. Since massive number of new types of text documents is generated daily, it is crucial {{to have the ability to}} transfer learned information from one domain to another domain. Cross-domain classification lets us leverage information learned from one domain for use in the classification of documents in a new domain...|$|R

