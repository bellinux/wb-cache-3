6|4|Public
40|$|Abstract—Currently {{researchers}} {{interested in}} developing new signal processing algorithms for commercially available cochlear implants {{must rely on}} coding these algorithms in lowlevel assembly language. We propose a Personal Digital Assistant (PDA) based research platform for developing and testing in real-time new signal processing strategies for cochlear implants. Software development can be done either in C or in LabVIEW. The C implementation can be further optimized using Intel’s primitive routines. In this paper, {{we report on the}} real-time implementation of a 16 -channel <b>noiseband</b> vocoder algorithm, which is a similar algorithm used in commercially available implant processors. We further report on EEG recordings on the PDA acquired through a compactflash data acquisition card. I...|$|E
40|$|Objective: To explore {{combined}} acute {{effects of}} frequency shift and compression-expansion on speech recognition, using <b>noiseband</b> vocoder processing. Design: Recognition of vowels and consonants, processed with a <b>noiseband</b> vocoder, {{was measured with}} five normal-hearing subjects, {{between the ages of}} 27 and 35 yr. The speech signal was filtered into 8 or 16 analysis bands and the envelopes were extracted from each band. The carrier noise bands were modulated by the envelopes and resynthesized to produce the processed speech. In the baseline matched condition, the frequency ranges of the corresponding analysis and carrier bands were the same. In the shift only condition, the frequency ranges of the carrier bands were shifted up or down relative to the analysis bands. In the compression and expansion only conditions, the analysis band range was made larger or smaller, respectively, than the carrier band range. By applying the shift to carrier bands and compression or expansion to analysis bands simultaneously, the combined effects of the two spectral distortions on speech recognition were explored. Results: When the spectral distortions of compression-expansion or shift were applied separately, the performance was reduced from the baseline matched condition. However, when the two spectral degradations were applied simultaneously, a compensatory effect was observed; the reduction in performance was smaller for some combinations compared to the reduction observed for each distortion individually. Conclusions: The {{results of the present study}} are consistent with previous vocoder studies with normal-hearing subjects that showed a negative effect of spectral mismatch between analysis and carrier bands on speech recognition. The present results further show that matching the frequency ranges of 1 to 2 kHz, which contain important speech information, can be more beneficial for speech recognition than matching the overall frequency ranges, in certain conditions...|$|E
40|$|Speech {{recognition}} by normal-hearing listeners improves {{as a function}} of the number of spectral channels when tested with a <b>noiseband</b> vocoder simulating cochlear implant signal processing. Speech {{recognition by}} the best cochlear implant users, however, saturates around eight channels and does not improve when more electrodes are activated, presumably due to reduced frequency selectivity caused by channel interactions. Listeners with sensorineural hearing loss may also have reduced frequency selectivity due to cochlear damage and the resulting reduction in the nonlinear cochlear mechanisms. The present study investigates whether such a limitation in spectral information transmission would be observed with hearing-impaired listeners, similar to implant users. To test the hypothesis, hearing-impaired subjects were selected from a population of patients with moderate hearing loss of cochlear origin, where the frequency selectivity would be expected to be poorer compared to normal hearing. Hearing-impaired subjects were tested for vowel and consonant recognition in steady-state background noise of varying levels using a <b>noiseband</b> vocoder and {{as a function of}} the number of spectral channels. For comparison, normal-hearing subjects were tested with the same stimuli at different presentation levels. In quiet and low background noise, performance by normal-hearing and hedring-impaired subjects was similar. In higher background, noise, performance by hearing-impaired subjects saturated around eight channels, while performance by normal-hearing subjects continued to increase up to 12 - 16 channels with vowels, and 10 - 12 channels with consonants., A similar trend was observed for most of the presentation levels at which the normal-hearing subjects were tested. Therefore, it is unlikely that the effects observed with hearing-impaired subjects were due to insufficient audibility or high presentation levels. Consequently, the results with hearing-impaired subjects were similar to previous results obtained with implant users, but only for background noise conditions. (c) 2006 Acoustical Society of America...|$|E
40|$|The {{effect of}} {{interaural}} correlation (ρ) on the loudness for <b>noisebands</b> {{was measured using}} a loudness-matching task in naïve listeners. The task involved a sequence of loudness comparisons for which the intensity of one stimulus in a given comparison was varied using a one-up-one-down adaptive rule. The task provided {{an estimate of the}} level difference (in decibels) for which two stimulus conditions have equal loudness, giving measures of loudness difference in equivalent decibel units (dBequiv). Concurrent adaptive tracks measured loudness differences between ρ = 1, 0, and − 1 and between these binaural stimuli and the monaural case for various <b>noisebands.</b> For all <b>noisebands,</b> monaural stimuli required approximately 6  dB higher levels than ρ = 1 for equal loudness. For most <b>noisebands,</b> ρ = 1 and ρ = − 1 were almost equal in loudness, with ρ = − 1 being slightly louder in the majority of measurements, while ρ = 0 was about 2  dBequiv louder than ρ = 1 or ρ = − 1. However, <b>noisebands</b> with significant high-frequency energy showed smaller differences: for 3745 – 4245  Hz, ρ = 0 was only about 0. 85  dBequiv louder than ρ = ± 1, and for 100 – 5000  Hz it was non-significantly louder (perhaps 0. 7  dBequiv) ...|$|R
40|$|Three {{experiments}} and a computational model explored {{the role of}} within‐channel and across‐channel processes in the perceptual separation of competing, complex, broadband sounds which differed in their interaural phase spectra. In each experiment, two competing vowels, whose first and second formants were represented by two discrete bands of noise, were presented concurrently, for identification. Experiments 1 and 2 showed that listeners {{were able to identify}} the vowels accurately when each was presented to a different ear, but were unable to identify the vowels when they were presented with different interaural time delays (ITDs); i. e. listeners could not group the <b>noisebands</b> in different frequency regions with the same ITD and thereby separate them from bands in other frequency regions with a different ITD. Experiment 3 demonstrated that while listeners were unable to exploit a difference in interaural delay between the pairs of <b>noisebands,</b> listeners could identify a vowel defined by interaurally decorrelated <b>noisebands</b> when the other two <b>noisebands</b> were interaurally correlated. A computational model based upon that of Durlach [J. Acoust. Soc. Am. 32, 1075 – 1076 (1960) ] showed that the results of these and other experiments can be interpreted in terms of a within‐channel mechanism, which is sensitive to interaural decorrelation. Thus the across‐frequency integration which occurs in the lateralization of complex sounds may play little role in segregating concurrent sounds...|$|R
40|$|Modulation masking {{phenomenon}} for maskers {{of different}} spectral and statistical properties {{was investigated in}} this study. Three <b>noisebands</b> centred at 16 Hz were used as modulation maskers, namely: 1) 32 -Hz wide Gaussian noise (GN 32 Hz); 2) 32 -Hz wide low-noise noise (LNN 32 Hz) and 3) 4 -Hz wide low-noise noise (LNN 4 Hz). The GN 32 Hz and LNN 32 Hz were characterized by the same power spectrum density and different probability density functions. Conversely, the LNN 32 Hz and LNN 4 Hz had the same probability density functions, but different power spectra. The results of the measurements indicated that modulation masking was mainly determined by power density spectrum of the modulation masker, however, probably due to peripheral compression, temporal properties of the masker might play also some role in the modulation masking...|$|R
40|$|Normal-hearing (NH) {{listeners}} {{make use}} of context, speech redundancy and top-down linguistic processes to perceptually restore inaudible or masked portions of speech. Previous research has shown poorer perception and restoration of interrupted speech in CI users and NH listeners tested with acoustic simulations of CIs. Three hypotheses were investigated: (1) training with CI simulations of interrupted sentences can teach listeners to use the high-level restoration mechanisms more effectively, (2) phonemic restoration benefit, an increase in intelligibility of interrupted sentences once its silent gaps are filled with noise, can be induced with training, and (3) perceptual learning of interrupted sentences can be reflected in clinical speech audiometry. To test these hypotheses, NH listeners were trained using periodically interrupted sentences, also spectrally degraded with a <b>noiseband</b> vocoder as CI simulation. Feedback was presented by displaying the sentence text and playing back both the intact and the interrupted CI simulation of the sentence. Training induced no phonemic restoration benefit, and learning was not transferred to speech audiometry measured with words. However, a significant improvement was observed in overall intelligibility of interrupted spectrally degraded sentences, with or without filler noise, suggesting possibly better use of restoration mechanisms {{as a result of}} training. (C) 2014 Acoustical Society of America...|$|E
40|$|Recognition of {{periodically}} interrupted sentences (with an interruption rate of 1. 5 Hz, 50 % duty cycle) {{was investigated}} {{under conditions of}} spectral degradation, implemented with a <b>noiseband</b> vocoder, with and without additional unprocessed low-pass filtered speech (cutoff frequency 500 Hz). Intelligibility of interrupted speech decreased with increasing spectral degradation. For all spectral degradation conditions, however, adding the unprocessed low-pass filtered speech enhanced the intelligibility. The improvement at 4 and 8 channels was higher than the improvement at 16 and 32 channels: 19 % and 8 %, on average, respectively. The Articulation Index predicted an improvement of 0. 09, in a scale from 0 to 1. Thus, the improvement at poorest spectral degradation conditions was larger than what would be expected from additional speech information. Therefore, the results implied that the fine temporal cues from the unprocessed low-frequency speech, such as the additional voice pitch cues, helped perceptual integration of temporally interrupted and spectrally degraded speech, especially when the spectral degradations were severe. Considering the vocoder processing as a cochlear implant simulation, where implant users' performance is closest to 4 and 8 -channel vocoder performance, the results support additional benefit of low-frequency acoustic input in combined electric-acoustic stimulation for perception of temporally degraded speech. (C) 2010 Elsevier B. V. All rights reserved...|$|E
40|$|Objective. To {{present a}} {{comprehensive}} {{analysis of the}} feasibility of genetic algorithms (GA) for finding the best fit of hearing aids or cochlear implants for individual users in clinical or research settings, where the algorithm is solely driven by subjective human input. Design: Due to varying pathology, the best settings of an auditory device differ for each user. It is also likely that listening preferences vary at the same time. The settings of a device customized for a particular user can only be evaluated by the user. When optimization algorithms are used for fitting purposes, this situation poses a difficulty for a systematic and quantitative evaluation of the suitability of the fitting parameters produced by the algorithm. In the present study, an artificial listening environment was generated by distorting speech using a <b>noiseband</b> vocoder. The settings produced by the GA for this listening problem could objectively be evaluated by measuring speech recognition and comparing the performance to the best vocoder condition where speech was least distorted. Nine normal-hearing subjects participated in the study. The parameters to be optimized were the number of vocoder channels, the shift between the input frequency range and the synthesis frequency range, and the compression-expansion of the input frequency range over the synthesis frequency range. The subjects listened to pairs of sentences processed with the vocoder, and entered a preference for the sentence with better intelligibility. The GA modified the solutions iteratively according to the subject preferences. The program converged when the user ranked {{the same set of}} parameters as the best in three consecutive steps. The results produced by the GA were analyzed for quality by measuring speech intelligibility, for test-retest reliability by running the GA three times with each subject, and for convergence properties. Results: Speech recognition scores averaged across subjects were similar for the best vocoder solution and for the solutions produced by the GA. The average number of iterations was 8 and the average convergence time was 25. 5 minutes. The settings produced by different GA runs for the same subject were slightly different; however, speech recognition scores measured with these settings were similar. Individual data from subjects showed that in each run, a small number of GA solutions produced poorer speech intelligibility than for the best setting. This was probably a result of the combination of the inherent randomness of the GA, the convergence criterion used in the present study, and possible errors that the users might have made during the paired comparisons. On the other hand, the effect of these errors was probably small compared to the other two factors, as a comparison between subjective preferences and objective measures showed that for many subjects the two were in good agreement. Conclusions: The results showed that the GA was able to produce good solutions by using listener preferences in a relatively short time. For practical applications, the program can be made more robust by running the GA twice or by not using an automatic stopping criterion, and it can be made faster by optimizing the number of the paired comparisons completed in each iteration...|$|E
40|$|Sensitivity to {{differences}} in interaural correlation was measured for 1. 3 -ERB-wide bands of noise using a 2 IFC task at six frequencies: 250, 500, 750, 1000, 1250, and 1500 Hz. The sensitivity index, d′, was measured for discriminations between a number of fixed pairs of correlation values. Cumulative d′ functions were derived for each frequency and condition. The d′ for discriminating any two values of correlation may be recovered from the cumulative d′ function by the difference between cumulative d′’s for these values. Two conditions were employed: the <b>noisebands</b> were either presented in isolation (narrow-band condition) or {{in the context of}} broad, contiguous flanking bands of correlated noise (fringed condition). The cumulative d′ functions showed greater sensitivity {{to differences}} in correlation close to 1 than close to 0 at low frequencies, but this difference was less pronounced in the fringed condition. Also, a more linear relationship was observed when cumulative d′ was plotted {{as a function of the}} equivalent signal-to-noise ratio (SNR) in dB for each correlation value, rather than directly against correlation. The equivalent SNR was the SNR at which the interaural correlation in an NoSπ stimulus would equal the interaural correlation of the noise used in the experiment. The maximum cumulative d′ declined above 750 Hz. This decline was steeper for the fringed than for the narrow-band condition. For the narrow-band condition, the total cumulative d′ was variable across listeners. All cumulative d′ functions were closely fitted using a simple two-parameter function. The complete data sets, averaged across listeners, from the fringed and narrow-band conditions were fitted using functions to describe the changes in these parameters over frequency, in order to produce an interpolated family of curves that describe sensitivity at frequencies between those tested. These curves predict the spectra recovered by the binaural system when complex sounds, such as speech, are masked by noise...|$|R

