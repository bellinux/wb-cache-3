4|21|Public
40|$|For a non-normal Pearsonian population, Clements {{suggests}} {{a method to}} estimate the usual capability indices pC and pkC. Pearn and Kotz apply this method to the pmC and pmkC indices. However these considerations only apply {{to the case of}} symmetrical tolerances. For asymmetrical tolerances, Chen, Lin and Pearn suggest the generalizations ''pC, pkC, pmC, and ''pmkC. In this paper, a geometrical interpretation of these last indices is proposed and, from this approach, new indices are suggested for a <b>non-normal</b> <b>population</b> and for asymmetrical tolerances. Key words: process capability indices, non-normal populations, uncentered targe...|$|E
40|$|In {{this paper}} we {{consider}} {{the distribution of the}} order statistics from the <b>non-normal</b> <b>population</b> with the probability density function f{x) = Jl+^i/ 3 (z) + ̂ i 74 (z) + where A 3, A 4, $(x) and Hj(x) have their usual meanings. By using some results established for the normal case, we determine the exact moments and the asymptotic moments of the non-normal order statistics. The joint distribution of the order statistics is studied and an expression for the product moment of the extreme order statistics is given. 1...|$|E
40|$|For {{a multivariate}} normal population, Kudo (1963), Shorack (1967) and Perlman (1969) derived the {{likelihood}} ratio {{tests of the}} null hypothesis that the mean vector is zero with a one-sided alternative for a known covariance matrix, a partially known covariance matrix and a completely unknown covariance matrix, respectively. Because these tests may be tedious to use, Tang, Gnecco and Geller (1989) developed approximate likelihood ratio tests and Follmann (1996) proposed one-sided modifications of the usual omnibus chi-squared test and Hotelling’s T 2 test. Also, we consider a modification of Follmann’s test (the new test) to include information of off diagonal of covariance matrix, which adjusts for possibly unequal variances. For the <b>non-normal</b> <b>population,</b> Boyett and Shuster (1977) proposed a nonparametric one-sided test and we use their technique to develop nonparametric versions of Perlman’s test, Follmann’s test, the new test and the Tang-Gnecco-Geller test. Following Chongcharoen, Singh and Wright (2002), who considered known and partially known covariance matrices, we study the powers of these one-sided tests for an unknown covariance matrix using Monte Carlo techniques and makerecommendations concerning their use...|$|E
40|$|The fprank command {{tests the}} {{hypothesis}} that two independent groups are sampled from the same population using the robust rank-order test which is a useful alternative of the Mann-Whitney-Wilcoxon test for <b>non-normal</b> <b>populations</b> with unequal variances. This test assumes neither normality, nor equal variances, nor equal shape. rank ordering, robust, <b>non-normal</b> <b>populations...</b>|$|R
40|$|Variable {{importance}} measures {{based on}} discriminant analysis and {{multivariate analysis of}} variance are useful for identifying variables that discriminate between two groups in multivariate group designs. Variable importance measures are developed based on trimmed and Winsorized estimators for describing group differences in multivariate <b>non-normal</b> <b>populations...</b>|$|R
50|$|For many {{statistical}} parameters the jackknife {{estimate of}} variance tends asymptotically {{to the true}} value almost surely. In technical terms one says that the jackknife estimate is consistent. The jackknife is consistent for the sample means, sample variances, central and non-central t-statistics (with possibly <b>non-normal</b> <b>populations),</b> sample coefficient of variation, maximum likelihood estimators, least squares estimators, correlation coefficients and regression coefficients.|$|R
40|$|Title from PDF {{of title}} page (University of Missouri [...] Columbia, viewed on May 29, 2013). The entire thesis text is {{included}} in the research. pdf file; the official abstract appears in the short. pdf file; a non-technical public abstract appears in the public. pdf file. Dissertation advisor: Dr. Douglas SteinleyIncludes bibliographical references. Vita. Ph. D. University of Missouri [...] Columbia 2012. Dissertations, Academic [...] University of Missouri [...] Columbia [...] Psychology. "May 2012 "[ACCESS RESTRICTED TO THE UNIVERSITY OF MISSOURI AT AUTHOR'S REQUEST. ] Growth mixture modeling can be used for two purposes: 1) to identify mixtures of normal sub-groups, and 2) to approximate oddly shaped distributions by a mixture of normal components. Often in applied research this methodology is applied to both of these situations indistinctly using the same fit statistics and likelihood ratio tests. This can lead to the over extraction of latent classes and the attribution of substantive meaning to these spurious classes. The goals of this study were: 1) to investigate the situations in which spurious classes emerge in finite mixture modeling; 2) to explore how separated two multivariate normal populations need to be before they are distinguishable; and 3) {{to examine the effects of}} time invariant covariates in the estimation of the number of latent classes. Four simulation studies were conducted. The first addresses the problem of spurious classes emerging as artifacts of the non-normality of the dependent variables. The second explores the effects of covariates in the estimation of the correct number of latent classes. The third addresses the issue of distinguishing between two classes that overlap. The fourth and last simulation fits one- through four-class solutions to a single <b>non-normal</b> <b>population</b> and compares results. Results show that spurious classes emerge in the data analysis when the population departs from normality even when the non-normality is only present in time invariant covariates, that two populations need to be separated by 2 standard deviations or more to be distinguishable, and that the cat's cradle can be extracted from a single population with skew of 1. 6 and kurtosis of 2...|$|E
40|$|The Pearson skew is {{a measure}} of {{asymmetry}} of a distribution, based on the difference between the mean and the median of a distribution. Here we show how to calculate the Pearson skew, estimate its standard error and the confidence interval. The derivation is based on a population following a normal distribution. Simulations explored the validity of this expression when the normality assumption is met in comparison to when the normality assumption is not met. The standard error of the Pearson skew revealed very robust in case of <b>non-normal</b> <b>populations,</b> compared to the Fisher Skew as presented in Harding, Tremblay and Cousineau (2014) ...|$|R
40|$|Little {{attention}} has been given to the correlation coefficient when data come from discrete or continuous <b>non-normal</b> <b>populations.</b> In this article, we consider the efficiency of two correlation coefficients which are from the same family, Pearson's and Spearman's estimators. Two discrete bivariate distributions were examined: the Poisson and the Negative Binomial. The comparison between these two estimators took place using classical and bootstrap techniques for the construction of confidence intervals. Thus, these techniques are also subject to comparison. Simulation studies were also used for the relative efficiency and bias of the two estimators. Pearson's estimator performed slightly better than Spearman's. Comment: 22 pages with 5 tables and 2 figure...|$|R
40|$|The {{chi-square}} probability {{plot of the}} ordered generalized {{distances from}} the sample mean vector has been suggested for use in checking the normal assumption for a given body of multivariate observations. Here, {{the results of an}} empirical study are presented to illustrate the small and large sample behavior of this plot for samples from both normal and <b>non-normal</b> <b>populations.</b> Our sample sizes range from 10 to 200. As might be expected, samples of 10 tell us almost nothing about multivariate normality, whereas samples of 200 seem very stable except for their few highest points. In general, for samples of size 30 or more, the chi-square plot appears to be an effective graphical aid for assessing multivariate normality...|$|R
40|$|When the {{variance}} of a single population needs to be assessed, the well-known chi-squared test of variance is often used but relies heavily on its normality assumption. For <b>non-normal</b> <b>populations,</b> few alternative tests {{have been developed to}} conduct left tailed hypothesis tests of variance. This thesis outlines a method for generating new test statistics using a saddlepoint approximation. Several novel test statistics are proposed. The type-I error rates and power of each test are evaluated using a Monte Carlo simulation study. One of the proposed test statistics, R_gamma 2, controls type-I error rates better than existing tests, while having comparable power. The only observed limitation is for populations that are highly skewed with heavy-tails, for which all tests under consideration performed poorly...|$|R
40|$|Abstract: From tumor to tumor, {{there is}} a great {{variation}} in the proportion of can cer cells growing and making daughter cells that ultimately metastasize. The differential growth within a single tumor, however, has not been studied exten sively and this may be helpful in predicting the aggressiveness of a particular cancer type. The estimation problem of tumor growth rates from several pop ulations is studied. The baseline growth rate estimator is based on a family of interacting particle system models which generalize the linear birth process as models of tumor growth. These interacting models incorporate the spatial structure of the tumor {{in such a way that}} growth slows down in a crowded system. Approximation-assisted estimation strategy is proposed when initial values of rates are known from the previous study. Some alternative estimators are suggested and the relative dominance picture of the proposed estimator to the benchmark estimator is investigated. An over-riding theme of this article is that the suggested estimation method extends its traditional counterpart to <b>non-normal</b> <b>populations</b> and to more realistic cases...|$|R
40|$|Hotelling's T 2 test {{is known}} to be optimal under multivariate {{normality}} and is reasonably validity-robust when the assumption fails. However, some recently introduced robust test procedures have superior power properties and reasonable type I error control with <b>non-normal</b> <b>populations.</b> These, including the tests due to Tiku & Singh (1982), Tiku & Balakrishnan (1988) and Mudholkar & Srivastava (1999 b, c), are asymptotically valid but are useful with moderate size samples only if the population dimension is small. A class of B-optimal modifications of the stepwise alternatives to Hotellings T 2 introduced by Mudholkar & Subbaiah (1980) are simple to implement and essentially equivalent to the T 2 test even with small samples. In this paper we construct and study the robust versions of these modified stepwise tests using trimmed means instead of sample means. We use the robust one- and two-sample trimmed- t procedures as in Mudholkar et al. (1991) and propose statistics based on combining them. The results of an extensive Monte Carlo experiment show that the robust alternatives provide excellent type I error control and a substantial gain in power. ...|$|R
40|$|From tumor to tumor, {{there is}} a great {{variation}} in the proportion of cancer cells growing and making daughter cells that ultimately metastasize. The differential growth within a single tumor, however, has not been studied extensively and this may be helpful in predicting the aggressiveness of a particular cancer type. The estimation problem of tumor growth rates from several populations is studied. The baseline growth rate estimator is based on a family of interacting particle system models which generalize the linear birth process as models of tumor growth. These interacting models incorporate the spatial structure of the tumor {{in such a way that}} growth slows down in a crowded system. Approximation-assisted estimation strategy is proposed when initial values of rates are known from the previous study. Some alternative estimators are suggested and the relative dominance picture of the proposed estimator to the benchmark estimator is investigated. An over-riding theme of this article is that the suggested estimation method extends its traditional counterpart to <b>non-normal</b> <b>populations</b> and to more realistic cases...|$|R
40|$|We {{consider}} {{the problem of}} multivariate density estimation, using samples from the distribution of interest as well as auxiliary samples from a related distribution. We assume that {{the data from the}} target distribution and the related distribution may occur individually as well as in pairs. Using nonparametric maximum likelihood estimator of the joint distribution, we derive a kernel density estimator of the marginal density. We show theoretically, in a simple special case, that the implied estimator of the marginal density has smaller integrated mean squared error than that of a similar estimator obtained by ignoring dependence of the paired observations. We establish consistency of the marginal density estimator under suitable conditions. We demonstrate small sample superiority of the proposed estimator over the estimator that ignores dependence of the samples, through a simulation study with dependent and <b>non-normal</b> <b>populations.</b> The application of the density estimator in nonparametric classification is also discussed. It is shown that the misclassification probability of the resulting classifier is asymptotically equivalent to that of the Bayes classifier. We also include a data analytic illustration. ...|$|R
40|$|Background: Part of {{the change}} over time of a {{response}} in longitudinal studies {{may be attributed to}} the re¬gression to the mean. The component of change due to regression to the mean is more pronounced in the subjects with extreme initial values. Das and Mulder proposed a nonparametric approach to estimate the regression to the mean. Aim: In this paper, Das and Mulder's method is made data-adaptive for empirical distributions via kernel estimation approaches, while retaining the orig¬inal assumptions made by them. Results: We use the best approaches for kernel density and hazard function estimation in our methods. This makes our approach extremely user friendly for a practitioner via {{the state of the art}} procedures and packages available in statistical softwares such as SAS and R for kernel density and hazard function estimation. We also estimate the standard error of our estimates of regression to the mean via nonparametric bootstrap methods. Finally, our methods are illustrated by analyzing the percent predicted FEV 1 measurements available from the Cystic Fibrosis Foundation's National Patient Registry. Conclusion: The kernel based approach presented in this article is a user-friendly method to assess the regression to the mean in <b>non-normal</b> <b>populations...</b>|$|R
40|$|All {{previously}} derived univariate {{measures of}} credibility are much affected by outlier(s) and their multivariate or hierarchical or regression counterparts {{on the other}} hand, fail {{to cope with the}} observations of <b>non-normal</b> <b>populations.</b> All of them considered traditional measure of risk (that uses the squared error loss function) and variance (or, standard deviation) that fail to capture fully the “true dispersion” of the data for estimating credibility. Moreover, maximum of the credibility estimators are based on normality approximations of the parent population of the data. Attempts have been made here to find new robust measures of credibility which are based on all observations and one dimensional dispersion measures. These measures are useable to any loss distribution irrespective of shape of that distribution (symmetric and/or asymmetric loss distribution) and are less affected by outlier(s) and/or extreme observation(s). Department of Mathematical SciencesExisting measures of greatest accuracy credibility [...] New measures of greatest accuracy credibility [...] Comparative study between Bühlmann's credibility and absolute error loss function credibility for single exposure [...] Comparative study between Bühlmann and Straub's and absolute error loss function credibility for multiple exposures [...] Real life examples. Thesis (M. A. ...|$|R
40|$|Includes bibliographical {{references}} (leaf 45) The often-cited "deep pockets" phenomenon {{holds that}} larger, or richer, firms {{are more susceptible}} to large liability settlements/verdicts, perhaps driven by plaintiffs and their attorneys seeing big firms as attractive targets, or by juries seeing them as more culpable or better able to pay substantial amounts. Explicit tests of insurance data are often complicated by <b>non-normal</b> <b>populations</b> and incomplete data samples (due to truncation or censoring). Here I use an actual sample of left-truncated insurance claims data grouped {{by the size of the}} firm, and develop an approach to assess the reality of the deep pockets phenomenon. First, I describe the challenges of incomplete data, and apply an estimator for the population conditional survival functions, called the Product-Limit Estimator ("PLE"), which is unbiased under left-truncation. Then I develop an unbiased bootstrapping method which provides insight into the sampling distribution of the two estimated survival functions. We use Monte Carlo simulation to test for bias in the PLE and the bootstrapping method, and to plumb the true coverage probability ofthe resulting confidence intervals. The bootstrapped distributions enable us to make probabilistic statements about the tails of these two populations...|$|R
40|$|In {{mathematical}} risk programming models {{the decision}} maker is usually assumed to know the distribution of net returns (when {{other elements of the}} planning problem are deterministic). However, this assumption is only justified when the probability distribution of returns represents degrees of belief of {{the decision maker}}. When sample data obtained from historical information is used, the existence of 2 ̆ 2 estimation error 2 ̆ 2 must be recognized. This study at an assesses the importance of estimation error in risk programming through a repeated sampling experiment. Two population distributions were used, a multivariate normal and a combination of independently distributed lognormals. Samples of various sizes (5, 6, 7, 8, 9, 10, 15, 20, 50 and 99 years) were obtained and several risk programming models (QP, MOTAD, Target MOTAD and 5 versions of the Safety-First model) were run 100 times, using the above samples as inputs. The results indicate that the variability of solution vector is very high, particularly at the sample sizes most commonly used in applied work. Although the variance decreases as the sample size gets larger, the solution is still far from convergence, even with 99 years of data. These results hold for normal and <b>non-normal</b> <b>populations</b> as well. ...|$|R
30|$|In {{order to}} study the {{asymptotic}} error rates of linear, quadratic and logistic rules, Kakaï and Pelz (2010) conducted a Monte Carlo study in two, three and five-group discriminant analysis. The simulation study took into account the overlap of the populations (e= 0.05, e= 0.1, e= 1.5), their common distribution (normal, chi-square with 4, 8 and 12  df) and their heteroscedasticity degree, Γ, measured by {{the value of the}} power function, 1 -β of the homoscedasticity test related to Γ (1 -β = 0.05, 1 -β = 0.4, 1 -β = 0.6, 1 -β = 0.8). They found that the three rules gave similar error rates for normal homoscedastic <b>populations.</b> For <b>non-normal</b> <b>populations,</b> quadratic rule still gave lowest relative error except for two-group where logistic was the best. The quadratic and logistic rules were more influenced by the number of groups irrespective of their lowest relative error. Also linear and quadratic were more influenced by non-normality. The study deviates from Lachenbruch et al. (1977) by focusing on three populations, unequal sample sizes and log-normal distribution for the skewness. Croux (2004) studied the influence of observations on the misclassification probability in quadratic discriminant analysis. They also studied the effect of observations in the training sample on the performance of the associated classification rule. MacFarland (2001) investigated into the exact misclassification probabilities for plug-in normal quadratic functions; the case of equal mean. A stochastic representations for the exact distributions of the “plug-in” quadratic discriminant functions was derived for classifying a newly obtained observation.|$|R
40|$|When n {{replicates}} {{are available}} from a factorial experiment, several methods exist for testing {{the validity of the}} assumption of equal variances within the 2 ̆ 2 cells 2 ̆ 2 or treatment combinations of the experiment. A new test is proposed for variances of random samples believed to be from normal populations. This new test combines both the familiar graphical analysis of means for treatment effects (ANOME) and the analysis of the logarithms of the within-group variances to produce a graphical display of the test for variance homogeneity. To determine robustness of the proposed test against departures from the underlying normality assumption, this new test is also evaluated for <b>non-normal</b> <b>populations.</b> Another analysis-of-means-type test was developed by Wludyka and Nelson which utilizes Dirichlet distributions and specially constructed tables. The new test, proposed herein, has an advantage in that it relies solely on critical values developed for the analysis of- means procedure. As an added simplification, only those critical values corresponding to infinite degrees of freedom are required. A In ANOME analysis of Nelson 2 ̆ 7 s data (used to demonstrate the In ANOVA procedure) yielded the same conclusion. Also, simulation results indicate that when the underlying assumption of normality is not feasible, the In ANOME procedure demonstrated equivalent or superior Type-I error-rate stability and power among tests which rely on that assumption. However, when the underlying assumption of normality is tenable, Bartlett 2 ̆ 7 s test performs the best of all homogeneity-of-variance tests studied in maintaining stable Type-I errors and power...|$|R
40|$|This {{dissertation}} {{introduces a}} novel sequential sampling methodology. The strategy for termination {{associated with this}} methodology is called multiple crossing stopping rule. Properties of the multiple crossing stopping rule are illustrated analytically {{as well as by}} simulations. Comparisons are made between the multiple crossing methodology and some of the existing methodologies, and relative merits are discussed. First, the proposed methodology is discussed in the context of estimating the mean of a normal population with a fixed-width confidence interval. Efficiency, and asymptotic consistency of the multiple crossing methodology is proven. The coverage probabilities are discussed by the means of extensive simulations. A truncation technique is proposed to improve the implementation. A real data implementation of the proposed methodology is discussed with respect to the gas mileage estimates of new vehicle models provided by the Environment Protection Agency (EPA). Next, the multiple crossing methodology is developed to estimate the mean vector of a multivariate normal distribution. Motivation for the proposed methodology is presented by extending a theoretical result by Simons (1968) to the multivariate normal context. A fine-tuned adjustment along the lines of Mukhopadhyay and Datta (1995) is proposed which improves practical implementation remarkably. A software benchmarking exercise based on multiple crossing sequential sampling is illustrated under real-time data gathering. Finally, regression parameters are estimated by a fixed-size confidence region, wherein sampling is based on the multiple crossing methodology. Important theoretical properties are proven. Some characteristics are discussed with the use of simulations. A truncation technique as well as a fine-tuned adjustment is proposed to improve practical usefulness. We conclude by emphasizing that the multiple crossing methodology is a versatile technique that may be easily applied to a variety of other outstanding problems in point estimation, hypothesis testing, and selection and ranking. It may also be extended to analogous problems arising from <b>non-normal</b> <b>populations...</b>|$|R
40|$|Brain {{perfusion}} imaging {{by means}} of 99 mTc-labeled hexamethyl propylene amine oxime (HMPAO) is a well-established Nuclear Medicine diagnostic procedure. The administered dose range recommended by the supplying company and reported in bibliography is rather wide (approximately 9. 5 - 27 mCi). This fact necessitates further quantitative analysis of the technique, so as to minimise patient absorbed dose without compromising the examination diagnostic value. In this study, a quantitative evaluation of the radiopharmaceutical performance for different values of administered dose (10, 15, 20 mCi) was carried out. Subsequently, a generic image quality index was correlated with the administered dose, to produce an overall performance indicator. Through this cost-to-benefit type analysis, the necessity of administration of higher radioactive dose levels in order to perform the specific diagnostic procedure was examined. Materials & methods: The study {{was based on a}} sample of 78 patients (56 administered with 10 mCi, 10 with 15 mCi and 12 with 20 mCi). Some patients were classified as normal, while others presented various forms of pathology. Evaluation of image quality was based on contrast, noise and contrast-to-noise ratio indicators, denoted CI, NI and CNR respectively. Calculation of all indicators was based on wavelet transform. An overall performance indicator (denoted PI), produced by the ratio of CNR by administered dose, was also calculated. Results: Calculation of skewness parameter revealed the normality of CI, NI and non-normality of CNR, PI populations. Application of appropriate statistical tests (analysis of variance for normal and Kruskal-Wallis test for <b>non-normal</b> <b>populations)</b> showed that there is a statistically significant difference in CI (p 0. 05) values. Application of Tukey test for normal populations CI, NI led to the conclusion that CI(10 mCi) = CI(20 mCi) NI(20 mCi), while NI(15 mCi) can not be characterised. Finally, application of non-parametric multiple comparisons showed that CNR(20 mCi) >CNR(10 mCi), while CNR(15 mCi) can not be characterised. Conclusion: Brain perfusion imaging {{by means of}} 99 mTc- HMPAO utilising an administered dose of 20 mCi results in improved image quality, {{on the basis of the}} estimated indicators and for the range of radioactive dose levels examined. Additionally, this image quality improvement is sufficient to justify the increased radiation burden for the patient...|$|R
40|$|Latent {{variable}} modelling is used {{widely in}} applications to economics, social and behavioural sciences. Since the normality-based model fitting procedures are simple and broadly available, and since such procedures are often applied to non-normal data or non-random samples, {{it is important}} to investigate the appropriateness of such practice and to suggest simple remedies. This paper addresses these issues for the analysis of multiple populations. For a very general class of latent variable models, a particular parameterisation is used for meaningful and interpretable analysis of several populations. It turns out that under this parameterisation the large sample statistical inferences based on the assumption of normal and independent populations are valid for virtually any <b>non-normal</b> and dependent <b>populations.</b> This result is also valid when some latent variables are treated as fixed instead of random, or when a group of individuals is measured over several time points longitudinally. structural equation modelling; latent variables; LISREL; fixed variables; non-normal factors; asymptotic robustness; multi-sample methods; dependent populations; panel data; longitudinal data; inference. ...|$|R
40|$|A {{robustness}} {{study to}} investigate the performance of six procedures for comparing two or more groups, under several circumstances, has been carried out. The study was divided in four parts. The first three were Monte-Carlo studies, and the fourth was a study on empirical data. The six procedures were the ANOVA (or its special case, the Student's t) procedure (1), the Satterthwaite (or its special case, the Welch) procedure (2), the bootstrap Satterthwaite (the Welch) procedure (3), the bootstrap percentile procedure for the mean (4) and for the median (5), and the Bonett and Price median procedure (6). The procedures were compared on confidence interval widths and coverage percentages, based on 10. 000 replications. In Studies 1 and 2, samples were drawn from simulated population distributions with the same (non-) normality. The conditions were varied with different <b>non-normal</b> distributions, unequal <b>population</b> variances, and different equal or unequal sample sizes. In Study 1 two samples were compared on the mean or median and in Study 2 four samples were compared on the mean or median with a contrast analysis. For Study 3 simulated cardiovascular data obtained by Van Roon's baroreflex model (1998) were used. In this study, the results of logarithmically transformed data wer...|$|R

