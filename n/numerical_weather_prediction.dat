2928|2425|Public
5|$|Richardson {{envisioned a}} large {{auditorium}} {{of thousands of}} people performing the calculations and passing them to others. However, the sheer number of calculations required was too large to be completed without the use of computers, {{and the size of the}} grid and time steps led to unrealistic results in deepening systems. It was later found, through numerical analysis, that this was due to numerical instability. The first computerised weather forecast was performed by a team composed of American meteorologists Jule Charney, Philip Thompson, Larry Gates, and Norwegian meteorologist Ragnar Fjørtoft, applied mathematician John von Neumann, and ENIAC programmer Klara Dan von Neumann. Practical use of <b>numerical</b> <b>weather</b> <b>prediction</b> began in 1955, spurred by the development of programmable electronic computers.|$|E
5|$|Tropical cyclone {{forecasting}} also {{relies on}} {{data provided by}} numerical weather models. Three main classes of tropical cyclone guidance models exist: Statistical models are based on an analysis of storm behavior using climatology, and correlate a storm's position and date to produce a forecast that {{is not based on}} the physics of the atmosphere at the time. Dynamical models are numerical models that solve the governing equations of fluid flow in the atmosphere; they are based on the same principles as other limited-area <b>numerical</b> <b>weather</b> <b>prediction</b> models but may include special computational techniques such as refined spatial domains that move along with the cyclone. Models that use elements of both approaches are called statistical-dynamical models.|$|E
5|$|During the year, the Japan Meteorological Agency (JMA) issued {{advisories}} on tropical cyclones {{west of the}} International Date Line to the Malay Peninsula, {{and north}} of the equator; {{this was due to}} the agency's status as the official Regional Specialized Meteorological Center, as designated by the World Meteorological Organization in 1989. The JMA issued forecasts and analyses four times a day, beginning at 0000UTC and continuing every six hours. The JMA issued forecasts based on a climatological tropical cyclone forecast model. The agency estimated 10minute sustained winds and barometric pressure based on the Dvorak technique and <b>numerical</b> <b>weather</b> <b>prediction.</b> The JTWC also issued warnings on storms within the basin, operating from Pearl Harbor in Hawaii to represent the interests of the United States Armed Forces in the Indian and Pacific Oceans.|$|E
5000|$|Smagorinsky, J., 1960: A {{primitive}} equation model including condensation processes. In, Proceedings of International Symposium on <b>Numerical</b> <b>Weather</b> <b>Predictions,</b> Japan Meteorological Society, 555.|$|R
40|$|Assimilation of Global Positioning System (GPS) Radio Occultation (RO) {{refractivity}} {{based on}} WRF- 3 DVAR {{is applied to}} <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP) in Hawaii, where limited conventional observations and poor representation of local circulations in global analysis constrain the quality of <b>numerical</b> <b>weather</b> <b>predictions.</b> For a summer trade wind case, with GPS RO refractivity assimi-lated, the trade wind inversion is better predicted. For a winter cold front case, the propagation of the cold front is also better simulated when GPS RO refractivity is assimilated. Furthermore, the moist tongue associated with the cold front is better defined and the vertical profiles of tempera-ture and moisture are largely improved {{when compared to the}} model run without GPS RO assimi-lation...|$|R
40|$|International audienceThe paper {{presents}} an advanced wind forecasting system that uses on-line SCAnA measurements, {{as well as}} <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP) as input, to predict the power production of wind park 8 48 hours ahead. The prediction system integrates models based on adaptive fuzzy-neural networks configured either for short-term (1 - 10 hours) or longterm (1 - 48 hours) forecasting. The paper presents detailed oneyear evaluation results ofthe models on the case study oflreland, where the output of several wind farms is predicted using HIRLAM meteorological forecasts as input A method for the online estimation of confidence intervals of the forecasts is developed together with an appropriate index for assessing online the risk due to the inaccuracy of the <b>numerical</b> <b>weather</b> <b>predictions...</b>|$|R
5|$|The {{basic idea}} of <b>numerical</b> <b>weather</b> <b>prediction</b> is {{to sample the}} state of the fluid at a given time and use the {{equations}} of fluid dynamics and thermodynamics to estimate {{the state of the}} fluid at some time in the future. The main inputs from country-based weather services are surface observations from automated weather stations at ground level over land and from weather buoys at sea. The World Meteorological Organization acts to standardize the instrumentation, observing practices and timing of these observations worldwide. Stations either report hourly in METAR reports, or every six hours in SYNOP reports. Sites launch radiosondes, which rise through the depth of the troposphere and well into the stratosphere. Data from weather satellites are used in areas where traditional data sources are not available. Compared with similar data from radiosondes, the satellite data has the advantage of global coverage, however at a lower accuracy and resolution. Meteorological radar provide information on precipitation location and intensity, which can be used to estimate precipitation accumulations over time. Additionally, if a pulse Doppler weather radar is used then wind speed and direction can be determined.|$|E
25|$|Advanced {{numerical}} {{methods are}} essential in making <b>numerical</b> <b>weather</b> <b>prediction</b> feasible.|$|E
25|$|A few {{aerodynamic}} equations {{are used}} as part of <b>numerical</b> <b>weather</b> <b>prediction.</b>|$|E
40|$|I {{will present}} {{a summary of the}} results {{obtained}} with the backscatter lidar on-board the FAAM research aircraft. This simple instrument has been used in several campaigns, and has contributed successfully to the characterization of volcanic ash, mineral dust, biomass burning aerosols, clouds, and the boundary layer structure. Its datasets have been used in many applications, from <b>numerical</b> <b>weather</b> <b>predictions</b> to the validation of satellite remote sensing...|$|R
50|$|He {{joined the}} then Department of Meteorological Services in 1980. In 1997, {{he became a}} {{national}} TV weather presenter, and received an award for excellence. He {{was the head of}} international relations and protocols and also the general manager, <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP) of the Nigerian Meteorological Agency (NiMet). He is currently the Programme Manager, Offices for Africa and Least Developed Countries at the World Meteorological Organisation in Geneva, Switzerland.|$|R
40|$|The goal is {{to utilize}} {{predictability}} theory and numerical experimentation to identify and {{understand some of the}} dynamical processes which must be modeled more realistically if large-scale <b>numerical</b> <b>weather</b> <b>predictions</b> are to be improved. The emphasis is on the use of relatively simple models to exlore the properties of physically comprehensive general circulation models (GCM's). A global linear quasi-geostrophic model and the Goddard Laboratory for Atmospheric Sciences (GLAS) GCM were used to investigate several mechanisms which are responsible for the decay of large-scale forecast skill in mid-latitude <b>numerical</b> <b>weather</b> <b>predictions.</b> Five-day forecasts for an ensemble of cases were made using First GARP Global Experiment data. It was found that forecast skill depends crucially on the specification of the stationary forcing. A lack of stationary forcing leads to spurious westwad propagation of the ultralong waves. Forecasts made with stationary forcings derived from climatological data are superior to those using forcings inferred from observations immediately preceding the forecast period. Interhemispheric forecast differences were analyzed, and the model errors were compared to errors of a simple persistence-damped-to-climatology scheme and to errors of the GLAS GCM...|$|R
25|$|<b>Numerical</b> <b>weather</b> <b>prediction</b> for {{forecasting}} involves complicated numeric {{computer models}} to predict weather accurately by taking many parameters into account.|$|E
25|$|The GOES {{spacecraft}} also enhance {{operational services}} and improve support for atmospheric science research, <b>numerical</b> <b>weather</b> <b>prediction</b> models, and environmental sensor design and development.|$|E
25|$|<b>Numerical</b> <b>Weather</b> <b>Prediction</b> is a {{main focus}} in {{understanding}} air–sea interaction, tropical meteorology, atmospheric predictability, and tropospheric/stratospheric processes. The Naval Research Laboratory in Monterey, California, developed a global atmospheric model called Navy Operational Global Atmospheric Prediction System (NOGAPS). NOGAPS is run operationally at Fleet Numerical Meteorology and Oceanography Center for the United States Military. Many other global atmospheric models {{are run by}} national meteorological agencies.|$|E
40|$|National audienceNumerous methods {{exist and}} were {{developed}} for global radiation forecasting. The {{two most popular}} types are the <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP) and the predictions using stochastic approaches. We propose to compute a parameter noted  constructed {{in part from the}} mutual information which is a quantity that measures the mutual dependence of two variables. Both of these are calculated with the objective to establish the more relevant method between NWP and stochastic models concerning the current problem...|$|R
40|$|International audienceIn {{this paper}} a dynamic line rating {{experiment}} {{is presented in}} which four machine learning algorithms (Generalized Linear Models, Multivariate Adaptive Regression Splines, Random Forests and Quantile Random Forests) are {{used in conjunction with}} <b>numerical</b> <b>weather</b> <b>predictions</b> to model and predict the ampacity up to 27 hours ahead in two conductor lines located in Northern Ireland. The results are evaluated against reference models and show a significant improvement in performance for point and probabilistic forecasts. The usefulness of probabilistic forecasts in this field is shown through the computation of a safety-margin forecast which can be used to avoid risk situations. With respect to the state of the art, the main contributions of this paper are: an in depth look at explanatory variables and their relation to ampacity, the use of machine learning with <b>numerical</b> <b>weather</b> <b>predictions</b> to model ampacity, the development of a probabilistic forecast from standard point forecasts and a favo urable comparison to standard reference models. These results are directly applicable to protect and monitor transmission and distribution infrastructures, especially if renewable energy sources and/or distributed power generation systems are present...|$|R
40|$|Road {{maintenance}} {{is one of}} the main problems Departments of Transportation face during winter time. Anti-icing, i. e. applying chemicals to the road to prevent ice formation, is often used to keep the roads free of ice. Given the preventive nature of anti-icing, accurate predictions of road ice are needed. Currently, anti-icing decisions are usually based on deterministic weather forecasts. However the costs of the two kinds of error are highly asymmetric because the cost of a road closure due to ice is much greater than that of taking anti-icing measures. As a result, probabilistic forecasts are needed to optimize decisionmaking. We propose two methods for forecasting the probability of ice formation. Starting with deterministic <b>numerical</b> <b>weather</b> <b>predictions,</b> they produce a joint predictive probability distribution of temperature and precipitation. This then yields the probability of ice formation, defined here as the occurrence of precipitation when the temperature is below freezing. In the first method, temperature and precipitation at different spatial locations are treated as conditionally independent given the <b>numerical</b> <b>weather</b> <b>predictions.</b> In the second method, spatial dependence between forecast errors at different locations is modeled. The mode...|$|R
25|$|We can {{utilize the}} output of <b>numerical</b> <b>weather</b> <b>prediction</b> models based on {{physical}} equations describing relationships in the weather system. Their predictive power tends to be less than, or similar to, purely statistical models beyond time horizons of 10–15 days. Ensemble forecasts are especially appropriate for weather derivative pricing within the contract period of a monthly temperature derivative. However, individual members of the ensemble need to be 'dressed' (for example, with Gaussian kernels estimated from historical performance) before a reasonable probabilistic forecast can be obtained.|$|E
25|$|Fortran is a general-purpose, procedural, and {{imperative}} {{programming language}} that is especially suited to numeric computation and scientific computing. Fortran came to dominate this area of programming early on {{and has been in}} continual use for over half a century in computationally intensive areas such as <b>numerical</b> <b>weather</b> <b>prediction,</b> finite element analysis, computational fluid dynamics (CFD), computational physics, and computational chemistry. It {{is one of the most}} popular languages in the area of High-performance computing and programs to benchmark and rank the world's fastest supercomputers are written in Fortran. In 1956, John Backus and a team of researchers at IBM invented the Fortran programming language for the IBM 704 mainframe computer.|$|E
25|$|Mission objective: As {{part of the}} Earth Observation Envelope Programme (EOEP) led by ESA {{to cover}} primary {{research}} objectives, the EarthCARE mission will be the third Earth Explorer Core Mission. The mission will be implemented in collaboration with Japan Aerospace Exploration Agency who will provide one of the core instruments. The EarthCARE mission has been specifically defined with the basic objective of improving the understanding of cloud-aerosol-radiation interactions so as to include them correctly and reliably in climate and <b>numerical</b> <b>weather</b> <b>prediction</b> models. EarthCARE will meet these objectives by measuring simultaneously the vertical structure and horizontal distribution of cloud and aerosol fields together with outgoing radiation over all climate zones. SSTL's role in this mission {{is to provide a}} Multi Spectral Imager (MSI) Instrument by development, manufacturing, testing and operations support during Phase B/C/D/E1.|$|E
40|$|This paper reviews {{different}} {{statistical methods}} {{dedicated to the}} post-processing of <b>Numerical</b> <b>Weather</b> <b>Predictions</b> and Ensemble Forecast. We focus on {{the application of the}} post-processing to problems linked to the production of electricity by eolian devices. The basic idea is to give a concise panorama of the methods commonly used nowadays. We pay a particular attention to the mathematics involved in the methods. We do not compare the methods and do not provide some preferences. Classification. 62 - 02; 62 P 12...|$|R
40|$|The {{processing}} of GPS radio occultation measurements {{for use in}} <b>numerical</b> <b>weather</b> <b>predictions</b> requires a precise orbit determination of the host satellite in near-real-time. Making use of data from the GRAS instrument on Metop-A, the performance of different GPS ephemeris products and processing concepts for near-real time and real-time precise orbit determination is compared. While previous analyses {{have focused on the}} achievable along-track velocity accuracy, the present study contributes a systematic comparison of the resulting estimated bending angles. This enables a more rigorous trade-off of different orbit determination methodologies in relation to the end-user needs for atmospheric science products. It is demonstrated that near-real-time GPS orbit and clock products have reached a sufficient quality to determine the Metop-A along-track velocity with an accuracy of better than 0. 05 mm/s that was formerly only accessible in post-processing. The resulting bending angles are shown to exhibit standard deviation and bias differences of less than 0. 3 % compared to post-processed products up to altitudes of at least 40 km, which is notably better than 1 % accuracy typically assumed for <b>numerical</b> <b>weather</b> <b>predictions</b> in this height regime. Complementary to the analysis of ground-based processing schemes the potential of autonomous on-board orbit determination is investigated for the first time. Using actual GRAS flight data it is shown that a 0. 5 m 3 D rms position accuracy and a 0. 2 mm/s along-track velocity accuracy can in fact be obtained in real-time with the currently available GPS broadcast ephemeris quality. Bending angles derived from the simulated real-time processing exhibit a minor performance degradation above tangent point heights of 40 km but negligible differences with respect to ground based products below this altitude. Onboard orbit determination and, if desired, bending angle computation, can thus enable a further simplification of the ground segment in future radio occultation missions and contribute to reduced product latencies for radio occultation data assimilation in <b>numerical</b> <b>weather</b> <b>predictions...</b>|$|R
40|$|Numerous methods {{exist and}} were {{developed}} for global radiation forecasting. The {{two most popular}} types are the <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP) and the predictions using stochastic approaches. We propose to compute a parameter noted constructed {{in part from the}} mutual information which is a quantity that measures the mutual dependence of two variables. Both of these are calculated with the objective to establish the more relevant method between NWP and stochastic models concerning the current problem. Comment: International Journal of Energy Technology and Policy (2014...|$|R
500|$|In 1978, {{the first}} hurricane-tracking model based on {{atmospheric}} dynamics—the movable fine-mesh (MFM) model—began operating. [...] Within {{the field of}} tropical cyclone track forecasting, despite the ever-improving dynamical model guidance which occurred with increased computational power, {{it was not until}} the 1980s when <b>numerical</b> <b>weather</b> <b>prediction</b> showed skill, and until the 1990s when it consistently outperformed statistical or simple dynamical models. Predictions of the intensity of a tropical cyclone based on <b>numerical</b> <b>weather</b> <b>prediction</b> continue to be a challenge, since statistical methods continue to show higher skill over dynamical guidance.|$|E
500|$|The {{history of}} <b>numerical</b> <b>weather</b> <b>prediction</b> {{began in the}} 1920s {{through the efforts of}} Lewis Fry Richardson, who used {{procedures}} originally developed by Vilhelm Bjerknes to produce by hand a six-hour forecast for the state of the atmosphere over two points in central Europe, taking at least six weeks to do so. [...] It was not until the advent of the computer and computer simulations that computation time was reduced to less than the forecast period itself. The ENIAC was used to create the first weather forecasts via computer in 1950, based on a highly simplified approximation to the atmospheric governing equations. In 1954, Carl-Gustav Rossby's group at the Swedish Meteorological and Hydrological Institute used the same model to produce the first operational forecast (i.e., a routine prediction for practical use). Operational <b>numerical</b> <b>weather</b> <b>prediction</b> in the United States began in 1955 under the Joint <b>Numerical</b> <b>Weather</b> <b>Prediction</b> Unit (JNWPU), a joint project by the U.S. Air Force, Navy and Weather Bureau. [...] In 1956, Norman Phillips developed a mathematical model which could realistically depict monthly and seasonal patterns in the troposphere; this became the first successful climate model. Following Phillips' work, several groups began working to create general circulation models. [...] The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory.|$|E
500|$|In September 1954, Carl-Gustav Rossby {{assembled}} {{an international}} group of meteorologists in Stockholm and produced the first operational forecast (i.e. routine predictions for practical use) {{based on the}} barotropic equation. Operational <b>numerical</b> <b>weather</b> <b>prediction</b> in the United States began in 1955 under the Joint <b>Numerical</b> <b>Weather</b> <b>Prediction</b> Unit (JNWPU), a joint project by the U.S. Air Force, Navy, and Weather Bureau. [...] The JNWPU model was originally a three-layer barotropic model, also developed by Charney. [...] It only modeled {{the atmosphere in the}} Northern Hemisphere. [...] In 1956, the JNWPU switched to a two-layer thermotropic model developed by Thompson and Gates. The main assumption made by the thermotropic model is that while the magnitude of the thermal wind may change, its direction does not change with respect to height, and thus the baroclinicity in the atmosphere can be simulated using the [...] and [...] geopotential height surfaces and the average thermal wind between them. [...] However, due to the low skill showed by the thermotropic model, the JNWPU reverted to the single-layer barotropic model in 1958. [...] The Japanese Meteorological Agency became the third organization to initiate operational <b>numerical</b> <b>weather</b> <b>prediction</b> in 1959. [...] The first real-time forecasts made by Australia's Bureau of Meteorology in 1969 for portions of the Southern Hemisphere were also based on the single-layer barotropic model.|$|E
40|$|Operational medium range flood {{forecasting}} systems are increasingly moving towards {{the adoption of}} ensembles of <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP), known as ensemble prediction systems (EPS), to drive their predictions. We review the scientific drivers of this shift towards such ‘ensemble {{flood forecasting}}’ and discuss several of the questions surrounding best practice in using EPS in flood forecasting systems. We also review the literature evidence of the ‘added value’ of flood forecasts based on EPS and point to remaining key challenges in using EPS successfully...|$|R
40|$|The {{application}} of the Laser Atmospheric Wind Sounder to the EOS and Space Station is proposed. The use of pulsed, CO 2 Doppler lidar to measure wind is described. The design requirements for a Doppler lidar operating in space, {{and the need to}} study the global distribution of naturally occurring atmospheric aerosols are discussed. The space-based Doppler lidar wind data will be useful for improving the skill of <b>numerical</b> <b>weather</b> <b>predictions,</b> for studying large-scale atmospheric circulation and climate dynamics, and for analyzing global biogeochemical and hydrological cycles...|$|R
5000|$|Smagorinsky, J., 1962: <b>Numerical</b> <b>weather</b> {{analysis}} and <b>prediction,</b> by Phillip D. Thompson. Mathematics of Computation, 16(80), 503-505.|$|R
500|$|The {{history of}} <b>numerical</b> <b>weather</b> <b>prediction</b> {{considers}} how current weather conditions as input into mathematical {{models of the}} atmosphere and oceans to predict the weather and future sea state (the process of <b>numerical</b> <b>weather</b> <b>prediction)</b> {{has changed over the}} years. [...] Though first attempted manually in the 1920s, {{it was not until the}} advent of the computer and computer simulation that computation time was reduced to less than the forecast period itself. [...] ENIAC was used to create the first forecasts via computer in 1950, and over the years more powerful computers have been used to increase the size of initial datasets as well as include more complicated versions of the equations of motion. [...] The development of global forecasting models led to the first climate models. [...] The development of limited area (regional) models facilitated advances in forecasting the tracks of tropical cyclone as well as air quality in the 1970s and 1980s.|$|E
500|$|<b>Numerical</b> <b>weather</b> <b>prediction</b> (NWP) uses [...] {{mathematical}} {{models of}} the atmosphere and oceans to predict the weather based on current weather conditions. Though first attempted in the 1920s, {{it was not until}} the advent of computer simulation in the 1950s that numerical weather predictions produced realistic results. [...] A number of global and regional forecast models are run in different countries worldwide, using current weather observations relayed from radiosondes, weather satellites and other observing systems as inputs.|$|E
500|$|Coastal SSTs {{can cause}} {{offshore}} winds to generate upwelling, which can significantly cool or warm nearby landmasses, but shallower waters over a continental shelf are often warmer. [...] Onshore winds {{can cause a}} considerable warm-up even in areas where upwelling is fairly constant, such as the northwest coast of South America. [...] Its values are important within <b>numerical</b> <b>weather</b> <b>prediction</b> as the SST influences the atmosphere above, {{such as in the}} formation of sea breezes and sea fog. [...] It is also used to calibrate measurements from weather satellites.|$|E
40|$|The present paper {{provides}} {{a description of}} four aspects of scatterometer winds and their uses. The theory of wave generation by the wind is considered along with {{an analysis of the}} properties of superobservations, and studies of intermittent versus continuous data assimilation methods for <b>numerical</b> <b>weather</b> <b>predictions</b> which use remotely sensed data. A comparison of the sum of squares versus the maximum likelihood method for recovering the vector winds is also conducted. Questions regarding wind speed, friction velocity, or normal stress are discussed and synoptic scale fields from Seasat-SASS data are examined...|$|R
40|$|We {{investigate}} {{the effects of}} noise specification {{on the quality of}} hydrological forecasts via an advanced data assimilation (DA) procedure using a distributed hydrological model driven by <b>numerical</b> <b>weather</b> <b>predictions.</b> The sequential DA procedure is based on (1) a multivariate rainfall ensemble generator, which provides spatial and temporal correlation error structures of input forcing, and (2) lagged particle filtering to update past and current state variables simultaneously in a lag-time window to consider the response times of internal hydrologic processes. The procedure is evaluated for streamflow forecasting of three flood events in two fast-responding catchments in Japan (Maruyama and Katsura). The rainfall ensembles are derived from ground-based rain gauge observations for the analysis step and <b>numerical</b> <b>weather</b> <b>predictions</b> for the forecast step. The ensemble simulation performs multi-site updating using information from the streamflow gauging network and considers the artificial effects of reservoir release. Sensitivity analysis is performed to assess the impacts of noise specification in DA, comparing a different setup of random state noise and input forcing with/without multivariate conditional simulation (MCS) of rainfall ensembles. The results show that lagged particle filtering (LPF) forced with MCS provides good performance with small and consistent random state noise, whereas LPF forced with Thiessen rainfall interpolation requires larger random state noise to yield performance comparable to that of LPF + MCS for short lead times...|$|R
40|$|International audienceNumerous methods {{exist and}} were {{developed}} for global radiation forecasting. The {{two most popular}} types are the <b>numerical</b> <b>weather</b> <b>predictions</b> (NWP) and the predictions using stochastic approaches. This article presents a methodology for determining the best method to use, according to a rule related to the spatial resolution, temporal step and location. We propose to compute a parameter noted  constructed {{in part from the}} mutual information which is a quantity that measures the mutual dependence of two variables. Both of these are calculated with the objective to establish the more relevant method between NWP and stochastic models concerning the current problem...|$|R
