110|48|Public
50|$|JPF {{includes}} a runtime module system to package such constructs into separate JPF extension projects. A {{number of such}} projects {{are available from the}} main JPF server, including a symbolic execution mode, <b>numeric</b> <b>analysis,</b> race condition detection for relaxed memory models, user interface model checking and many more.|$|E
50|$|HP 39/40 {{series are}} {{graphing}} calculators from Hewlett-Packard, the successors of HP 38G. The series consists of six calculators, which all have algebraic entry modes, and can perform <b>numeric</b> <b>analysis</b> together {{with varying degrees}} of symbolic calculation. All calculators in this series are aimed at high school level students and are characterized by their ability to download (via cable or infra-red) APLETs or E-lessons. These are programs of varying complexity which are generally intended {{to be used in the}} classroom to enhance the learning of mathematics by the graphical and/or numerical exploration of concepts.|$|E
40|$|Submission {{is focused}} on {{completing}} the information system about quality, operation, automatic testing and new evaluating method of vehicle subsystem. <b>Numeric</b> <b>analysis</b> is carried out {{on the base of}} automatic collection and systematic recording of commercial car operation. Proposed new information system about operation and trial process allows verification according to the proposed method. Critical components verified in laboratory conditions are detected by <b>numeric</b> <b>analysis</b> of reliability. Quality level increasing not only for final product, but also related automatic test laboratory for cars is the result of respecting these principles...|$|E
40|$|OBJECTIVE: The {{purpose of}} this {{research}} was to check if the <b>numeric</b> facial <b>analysis</b> can determine facial attractiveness. METHOD: The sample consisted of frontal and lateral standard facial photographs, in natural head position, of 85 Brazilian Caucasian women, without facial plastic surgery report. The sample mean age was 23 years and 9 months. A group of 5 orthodontists, 5 layman and 5 plastic artists classified the photographs according to their own attractiveness graduation in: pleasant, acceptable and not pleasant. The <b>numeric</b> facial <b>analysis</b> was then performed using a computerized method. Linear, proportional and angular measurements were compared among groups. RESULTS: According subjective analysis the sample was consisted of 18. 8 % of pleasant, 70. 6 % of acceptable and 10. 6 % of not pleasant. In most measurements there were no differences among groups. Just in three of them significant statistical difference was observed and in two of them the comparison value was within decision limit. All the differences found were related to the lower third of the face and to facial pattern. CONCLUSION: On the present research, the <b>numeric</b> facial <b>analysis,</b> by itself, was not capable of detecting facial attractiveness, considering that beauty judgment seems to be very personal...|$|R
40|$|Topological {{design of}} {{terrestrial}} networks for communication via satellites is {{studied in the}} paper. Quantitative model of the network cost-analysis minimizing the total transmission and switching cost is described. Several algorithms solving combinatorial problem of the optimal topology design based on binary partitioning, a minimax parametric search and dynamic programming are developed by the author and demonstrated with a <b>numeric</b> example. <b>Analysis</b> of average complexity of the minimax parametric search algorithm is also provided. Comment: Journal of Telecommunications,Volume 1, Issue 1, pp 25 - 35, February 201...|$|R
40|$|Discretization, {{defined as}} a set of cuts over domains of attributes, {{represents}} an important preprocessing task for <b>numeric</b> data <b>analysis.</b> Some Machine Learning algorithms require a discrete feature space but in real-world applications continuous attributes must be handled. To deal with this problem many supervised discretization methods have been proposed but little has been done to synthesize unsupervised discretization methods to be used in domains where no class information is available. Furthermore, existing methods such as (equal-width or equal-frequency) binning, are not well-principled, raising therefore the need for more sophisticated methods for the unsupervised discretization of continuous features. This pape...|$|R
40|$|International audienceThis paper {{contributes}} to a new abstract domain that combines static <b>numeric</b> <b>analysis</b> and points-to analysis. One particularity of this abstract domain lies in its high degree of modularity, {{in the sense that}} the domain is constructed by reusing its combined components as black-boxes. This modularity dramatically eases the proof of its soundness and renders its algorithm intuitive. We have prototyped the abstract domain for analyzing real-world Java programs. Our experimental results show a tangible precision enhancement compared to what is possible by traditional static <b>numeric</b> <b>analysis,</b> and this at a cost that is comparable to the cost of running the numeric and pointer analyses separately...|$|E
40|$|AbstractSubmission is {{concerned}} to complete information system about quality, operation, automatic testing using mechatronic system and new evaluating method of vehicle subsystem. Presentation of statistic significant group of tracing commercial cars is used. Data about operational parameters {{were taken from}} this group. <b>Numeric</b> <b>analysis</b> is realized {{on the base of}} automatic collection and systematic recording of commercial car operation. Proposed new information system about operation and trial process allows verification according to proposed method. Critical components verified in laboratory conditions are detected by <b>numeric</b> <b>analysis</b> of reliability. Trial method is described and also numerically compared in three levels. Quality level increasing not only for final product, but also related automatic test laboratory for cars is the result of these principles respecting. The model of technical system operation is proposed by estimating the level of its elements’ reliability. Application of the repair cycle adaptive structure creates the basis of organizing the preventive maintenance and repair and increasing the operational efficiency of technical systems...|$|E
40|$|Anchorage {{method of}} {{external}} reinforcement for verification anchor areas by experimental analysis. Design and manufacture of trial units for laboratory testing. Running tests of chosen mechanical qualities of trial units. Experimental analysis of trial units in laboratory {{and creation of}} mathematic model by Athena software. Evaluation of experimental analysis and comparing with figures of <b>numeric</b> <b>analysis</b> and statistical calculation. Graphical comparing of results and concluding overall assessment...|$|E
40|$|The paper {{explores the}} {{perspectives}} of pultruded FRP (PFRP) profiles {{in the field of}} masonry building preservation, for ancillary structures or strengthening techniques. The available knowledge about interfaces is briefly summarized; recent experimental results about bolted PFRP-to-masonry joints are cited. A <b>numeric</b> predictive <b>analysis,</b> aimed at evaluating the shear interface behaviour of adhesive PFRP-to-masonry joints, is shown in view of foreseen laboratory tests. The numeric results enlighten a clear influence of compressive loading on the peak shear displacement at varying transfer length. The model, which relies on the assumption of frictional joint behaviour, appears to represent the joint sliding in a satisfactory way...|$|R
5000|$|Ch [...] is a {{proprietary}} cross-platform C and C++ interpreter and scripting language environment, originally designed by Harry H. Cheng as a scripting language for beginners to learn mathematics, computing, numerical <b>analysis</b> (<b>numeric</b> methods), and programming in C/C++. Ch is now developed and marketed by SoftIntegration, Inc. A student edition is freely available. Ch Professional Edition for Raspberry Pi is free for non-commercial use.|$|R
40|$|Abstract During {{the last}} 10 years {{different}} interpretative methods for analysing the effect or importance of input variables on {{the output of}} a feedforward neural network have been proposed. These methods can be grouped into two sets: analysis based on the magnitude of weights; and sensitivity analysis. However, as described throughout this study, these methods present a series of limitations. We have defined and validated a new method, called <b>Numeric</b> Sensitivity <b>Analysis</b> (NSA), that overcomes these limitations, proving to be the procedure that, in general terms, best describes the effect or importance of the input variables on the output, independently of the nature (quantitative or discrete) of the variables included. The interpretative methods {{used in this study}} are implemented in the software program Sensitivity Neural Network 1. 0, created by our team...|$|R
40|$|In {{this paper}} we {{consider}} a delayed nonlinear {{model of the}} dynamics of the immune system against a viral infection that contains wild-type virus and one mutant. A finite response time of the immune system was considered in order which leads to sustained oscillatory behavior as well as chaotic behavior, triggered by the presence of delays. We present a thoroughly <b>numeric</b> <b>analysis</b> and some analytical results...|$|E
40|$|One of {{the current}} {{technologies}} ofelectromagnetic processing of materials proposed forstudy in this work is hardening through inductiveheating. During the past years, there have beendeveloped modeling methods of the hardeningprocesses in which the <b>numeric</b> <b>analysis</b> of theelectromagnetic field is joined with the analysis ofthermal diffusion. The hardening solutions of ascrew with ball are analyzed {{with the help of}} theFLUX program package, resulting the hardeningtime and the distributions corresponding to theelectromagnetic and thermal fields...|$|E
30|$|The pragmatic {{purpose of}} {{chemometrics}} is establishing quantitative relationships between related or logically derived forms {{of one and}} the same product or of a series of products in different states, as well as numeric descriptions of those states connected in physical-chemical processes clearly defined in time, resorting to chemical or instrumental analytical determinations. In order to achieve this goal, the data obtained experimentally are processed through advanced techniques of mathematical statistics, <b>numeric</b> <b>analysis</b> and information theory, employing highly optimized algorithms and software applications.|$|E
40|$|The {{degradation}} of laminated glass {{as a result}} of increased temperature {{has become one of the}} important problem of reconstructions and designs of new glass structures, for instance high-rise buildings that are exposed to the impacts of an intensive heating caused e. g. by sunshine. The temperatures during heating can reach very high values, commonly from 60 to 70 oC. The effect of heating was simulated using the thermal chamber where the glass panes with the size of 120 x 1100 mm were heated. The deformation course under the increase of temperature was continually monitored by a measuring unit. In total six types of foils joining particular layers of glass were examined. In this paper the experimentally gained results are compared with a <b>numeric</b> computer <b>analysis</b> and the particular kinds of interlayers are evaluated using the loss of shear interaction. 1...|$|R
40|$|In {{spite of}} its {{fundamental}} importance, inference {{has not been an}} inherent function of multidimensional models and analytical applications. These models are mainly aimed at <b>numeric</b> (quantitative) <b>analysis</b> where the notions of inference and semantics are not well defined. In this paper we argue that inference can be and should be integral part of multidimensional data models and analytical applications. It is demonstrated how inference can be defined using only multidimensional terms like axes and coordinates as opposed to using logic-based approaches. We propose a novel approach to inference in multidimensional space based on the concept-oriented model of data and introduce elementary operations which are then used to define constraint propagation and inference procedures. We describe a query language with inference operator and demonstrate its usefulness in solving complex analytical tasks. Comment: 19 pages, 14 figures, Full version of the paper published in DATA 2012 conferenc...|$|R
40|$|This paper {{presents}} {{a new approach}} that aims to incorporate prior judgmental forecasts into a statistical forecasting model. The result {{is a set of}} forecasts that are consistent with both the judgment and latest observations. The approach is based on constructing a model with a combined dataset where the expert forecasts and the historical data are described by means of corresponding regression equations. Model estimation is done using <b>numeric</b> Bayesian <b>analysis.</b> Semiparametric methods are used to ensure finding adequate forecasts without any prior knowledge of the specific type of the trend function. The expert forecasts can be provided as estimates of future time series values or as estimates of total or average values over any particular time intervals. Empirical analysis has shown that the approach is operable in practical settings. Compared to standard methods of combining, the approach is more flexible and in empirical comparisons proves to be more accurate...|$|R
40|$|The {{paper is}} focused on a {{comparative}} <b>numeric</b> <b>analysis</b> of the secretory pedicelate (tentacular) trichomes of the upper epidermis of the leaf in 16 Drosera species and of the sessile secretory trichomes belonging to both upper and lower epidermis in the same species. The bigger {{is the number of}} the secretory trichomes, in both epidermises, the more efficient is the plant in capturing and retaining different organisms. This way, the carnivorous plants can completely benefit by the nutritive compounds resulted from the preys...|$|E
40|$|General {{methods are}} {{described}} {{by which the}} mathematical complexities of explicit and exact state equations of robot arms {{can be reduced to}} a simplified and compact state equation representation without introducing significant errors into the robot arm dynamic model. The model reduction methods are based on homogeneous coordinates and on the Langrangian algorithm for robot arm dynamics, and utilize matrix, vector and <b>numeric</b> <b>analysis</b> techniques. The derivation of differential vector representation of centripetal and Coriolis forces which has not yet been established in the literature is presented...|$|E
40|$|This paper investigates and {{analyses}} use of numerical modeling by {{finite element method}} (FEM) at studying of consolidation processes of materials from powder by spark plasma sintering (SPS). Tasks of SPS process optimization is discussed in detail. Examples of <b>numeric</b> <b>analysis</b> of SPS of current conducting and non-conducting materials are given. Numeric modeling of sample sintering with hybrid method when SPS process is combined with hot pressing (HP) process is studied. Also paper presents development prospects of principles of SPS process numeric modeling. Peer Reviewe...|$|E
40|$|The main {{objective}} {{in this paper}} is to describe a framework to characterize and assess the learning of elementary statistical inference. The key constructs of the framework are: populations and samples and their relationships; inferential process; sample sizes; sampling types and biases. To refine and validate this scheme we have taken data from a sample of 49 secondary students sample using a questionnaire with 12 items in three different contexts: concrete, narrative and <b>numeric.</b> Theoretical <b>analysis</b> on the results obtained in this first research phase has permitted us to establish the key constructs described below and determine levels in them. Moreover this has allowed us to determine the students’ conceptions about the inference process and their perceptions about sampling possible biases and their sources. The framework is a theoretical contribution to the knowledge of the inferential statistical thinking domain and for planning teaching in the area...|$|R
40|$|A {{vibration}} {{analysis of}} a structure with joints is performed. The simulation is conducted with finite element software capable of performing a <b>numeric</b> modal <b>analysis</b> with hysteretic damping assumption. The joints are modeled with thin layer elements, representing dissipation and stiffness of the joints. The matrices describing the system consist of the mass, as well as real and complex-valued stiffness matrices. If the eigenvalues of this system are found in one step, due to the mode crossing occurring for the closely spaced modes, it is difficult and time consuming to assign calculated modal damping factors to the corresponding undamped eigenvalues. In order to avoid this problem, an eigenvalue following method is used. The outcome of the solution is the graphical presentation of continuous eigenvalue paths, showing {{the change in the}} eigenvalues from the undamped to the fully damped case. For every undamped eigenvalue exists its equivalent eigenfrequency and damping factor {{that can be used for}} further numerical analysis...|$|R
40|$|We review data analysis, {{pursuing}} the following lines of enquiry: traditional, <b>numeric</b> data <b>analysis,</b> based on graphical means; " data analysis, where the results provide new graphical user interfaces, {{or where the}} results are used to facilitate navigation in information spaces; and newly developed tools and techniques for the processing of image and other signal objects. 1. Data Analysis for Visualization Frequently the analyst must interact with the data. This means that one type of display is made, followed by a dierent visualization of some subset of the data. The term data analysis" is most {{closely associated with the}} name of Tukey (Princeton). Interactive statistics is another term used, and this activity may be supported by computer software. A prime example is the S language (or software environment) originating in ATT Bell Labs, and enhanced as the S-Plus package by MathSoft Inc. (formerly StatSci Inc.). Figures 1, 2 and 3 illustrate complementary view [...] ...|$|R
40|$|We {{present a}} <b>numeric</b> <b>analysis</b> that {{is capable of}} {{reasoning}} about array operations. In particular, the analysis is able to establish that all elements of an array have been initialized ("an array kill"), as well as to discover numeric constraints on values of initialized array elements, and to verify the correctness of comparison-based sorting algorithms. The analysis is based on the combination of canonical abstraction and summarizing numeric domains. We present a prototype implementation of the analysis and discuss our experience with applying this prototype to several kernal examples...|$|E
40|$|Determining the {{inductance}} of {{thin film}} superconducting structures accurately {{is extremely important}} in sub-Terahertz superconducting logic circuits. For <b>numeric</b> <b>analysis,</b> both the structure and ground plane need to be sufficiently segmented. The method of images significantly reduces the number of segments, but {{the selection of the}} position of the reflection plane is critical. The influence of the position of the reflection plane on inductance is investigated, and results presented. A study of segmentation and filamentation is also made, and these results used to calculate the inductance of a complex three-dimensional structure. Conference Pape...|$|E
40|$|The slope {{with soft}} soil layer {{is easier to}} be {{destroyed}} under earthquake than that without soft soil layer. In the paper, <b>numeric</b> <b>analysis</b> is used to analyses the effect of soft soil layer on earthquake response for the slope. In the analysis, the slope is regard as the plane strain model and the finite element method is used. The parameters which influencing the earthquake response of the slope, including the depth, the thickness, the obliquity of soft soil layer are analyzed and the conclusions are valuable for the dynamical analysis of slopes in application...|$|E
40|$|Hard {{real-time}} {{systems have}} stringent timing constraints expressed in units of time. To {{ensure that a}} task finishes within its time-frame, the designer of such a system {{must be able to}} derive upper bounds on the task’s worst-case execu-tion time (WCET). To compute such upper bounds, timing analyses are used. These analyses require that information such as bounds on the maximum numbers of loop iterations are known statically, i. e. during design time. Parametric timing analysis softens these requirements: it yields sym-bolic formulas instead of single numeric values represent-ing the upper bound on the task’s execution time. In this paper, we present a new parametric timing analysis that is able to derive safe and precise results. Our method determines what the parameters of the program are, con-structs parametric loop bounds, takes processor behaviour into account and attains a formula automatically. In the end, we present tests to show that the precision and runtime of our analysis are very close to those of <b>numeric</b> timing <b>analysis.</b> ...|$|R
40|$|This paper first {{provides}} {{a brief overview}} of some frequently encountered real world problems in data analysis. These are problems that have to be solved through data pre-processing so that the nature of the data is better understood and the data analysis is performed more accurately and efficiently. The architecture of a data analysis tool for which a data pre-processing mechanism has been developed and tested is also explained. An example is then given of the use of this data pre-processing mechanism for two purposes: (i) to filter out a set of semiconductor data, and (ii) {{to find out more about}} the nature of these data and make the induction process more efficient. 1. 0 Introduction: Data analysis is now integral to our working lives. It is the basis for investigations in many fields of knowledge, from science to engineering and from management to process control. Data on a particular topic are acquired in the form of symbolic and <b>numeric</b> attributes. <b>Analysis</b> of these data gives [...] ...|$|R
40|$|Up to 50 % of {{patients}} with uveal melanoma develop metastatic disease with poor prognosis. Regional, mainly liver-directed, therapies may induce limited tumor responses but do not improve overall survival. Response rates of meta-static uveal melanoma (MUM) to systemic chemotherapy are poor. Insights into the molecular biology of MUM recently led to investigation of new drugs. In this study, to compare response rates of systemic treatment for MUM we searched Pubmed/Web of Knowledge databases and ASCO website (1980 – 2013) for “metastatic/uveal/melanoma ” and “melanoma/eye. ” Forty studies (one case series, three phase I, five pilot, 22 nonrandomized, and two randomized phase II, one randomized phase III study, data of three expanded access programs, three retrospective studies) with 841 evaluable patients {{were included in the}} <b>numeric</b> outcome <b>analysis.</b> Complete or partial remissions were observed in 39 / 841 patients (overall response rate [ORR] 4. 6 %; 95 % confidence intervals [CI] 3. 3 – 6. 3 %), no responses were observed in 22 / 40 studies. Progression-fre...|$|R
40|$|This paper {{examines}} {{the development of}} evidence-based policies and the complexity associated with its definition. Using correctional education programs on the one hand, and the D. A. R. E program on the other, we demonstrate why evidence based on <b>numeric</b> <b>analysis</b> is inherently limited and limiting. By juxtaposing efforts to reform the justice system in the US, and the recent policies of the Federal government in Canada, we propose a research program to capture not only numeric indices of success and failure, but the costs and consequences for those caught up {{in an era of}} dumb in crime...|$|E
40|$|This paper {{highlights}} analytical {{reasons why}} we believe trade and technology are linked to wage movements in general, and how we should organize our examination of the recent episode of wage and employment erosion in the OECD countries. We start with a graphic tour through the mechanics of general equilibrium theory on trade and wages. This provides a set of implied relationships between wages and factor intensity trends that, together, provide a casual test of the consistency of posited relationships with actual trends. <b>Numeric</b> <b>analysis</b> and {{a review of the}} general equilibrium empirical literature follow the theoretical overview. trade and employment; Trade and Wages; unskilled wages...|$|E
40|$|Abstract—In this paper, a {{pollution}} regulation game model under multiple principal-agent among government, pollutant treatment {{enterprise and}} Small and Medium Enterprises (SMEs) is developed {{to study the}} pollution regulation mechanism under centralized treatment mode. The optimal regulation mechanism is obtained through theoretic and <b>numeric</b> <b>analysis.</b> It is found that with the supervision costs on pollutant emission, government can not achieve the optimal social welfare but the sub-optimal through supervision and regulation; government should charge a fine as big as possible on pollutant treatment enterprise instead of SMEs to reduce the emission of untreated pollutants, and guide the price-making of pollutant treatment enterprise instead of making by itself freely, {{in order to increase}} social welfare...|$|E
40|$|Abstract. With the {{development}} of cloud computing paradigm, energy consumption becomes a big problem. Green cloud computing requires efficient resource management for the purpose of energy saving. Service schedule {{is an important part of}} the resource management to ensure the quality of service. A mathematic model is proposed which can determine service schedule quantity. In this model, taking energy saving and reputation index of service supplier into account, an optimization problem of two targets is simplified under hypothesis in reason. Through <b>numeric</b> calculation and <b>analysis,</b> this paper show there is an optimal point for the advance schedule of service...|$|R
40|$|Clustering is a data {{analysis}} technique, particularly useful {{when there are}} many dimensions and little prior information about the data. Partitional clustering algorithms are efficient, but suffer from sensitivity to the initial partition and noise. We propose here k-Attractors, a partitional clustering algorithm tailored to <b>numeric</b> data <b>analysis.</b> As a pre-processing (initialization) step, it employs maximal frequent itemset discovery and partitioning to define the number of clusters k and the initial cluster “attractors”. During its main phase the algorithm utilizes a distance measure, which is adapted with high precision to the way initial attractors are determined. We applied k-Attractors as well as k-Means, EM and FarthestFirst clustering algorithms to several datasets and compared results. Comparison favored k-Attractors in terms of convergence speed and cluster formation quality in most cases, as it outperforms these 3 algorithms except from cases of datasets with very small cardinality containing only a few frequent itemsets. On the downside, its initialization phase adds an overhead that can be deemed acceptable only when it contributes significantly to the algorithm’s accuracy. 1...|$|R
40|$|International audienceThis paper {{describes}} {{the design and}} implementation of a static analysis tool for certifying Java Card applications, according to security rules defined by the smart card industry. Java Card is a dialect of Java designed for programming multi-application smart cards and the tool, called SawjaCard, has been specialised for the particular Java Card programming patterns. The tool is built around a static analysis engine which uses a combination of <b>numeric</b> and heap <b>analysis.</b> It includes {{a model of the}} Java Card libraries and the Java Card firewall. The tool has been evaluated on a series of industrial applets and is shown to au-tomate a substantial part of the validation process...|$|R
