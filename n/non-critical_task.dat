0|35|Public
5000|$|Defining {{critical}} and <b>non-critical</b> <b>tasks</b> using {{critical path method}} ...|$|R
40|$|Abstract—We {{introduce}} on-demand redundancy, a set {{of architectural}} techniques that leverage the tightly-coupled nature of components in systems-on-chip {{to reduce the cost}} of safety-critical systems. On-demand redundancy eases the assumptions that traditionally segregate the execution of critical and <b>non-critical</b> <b>tasks</b> (NCTs), making resources available for critical tasks at potentially arbitrary points in both space and time, and otherwise freeing resources to execute <b>non-critical</b> <b>tasks</b> when critical tasks are not executing. Relaxed dedication is one such technique that allows <b>non-critical</b> <b>tasks</b> to execute on critical task resources. Our results demonstrate that {{for a wide variety of}} applications and architectures, relaxed dedication is more cost-effective than a traditional approach that employs dedicated resources executing in lockstep. Applied to dual-modular redundancy (DMR), relaxed dedication exposes 73 % more NCT cycles than traditional DMR on average, across a wide variety of usage scenarios. I...|$|R
50|$|IT-enabled sourcing, or eSourcing, uses {{information}} technology {{as a key}} component of service delivery, or as an enabler for delivering services. Often provided remotely eSourcing services range from routine and <b>non-critical</b> <b>tasks</b> that are resource intensive and operational in nature to strategic processes that directly impact revenues.|$|R
50|$|The {{programming}} {{paradigm is}} mostly used for safety critical programs, since the behaviour {{of the program}} is highly deterministic. No external events are allowed to affect the control-flow of the program, the same pattern (i.e., described by the dispatch table) will be repeated time after time. However, idle time of the processor is also highly deterministic, allowing for the scheduling of other <b>non-critical</b> <b>tasks</b> through slack stealing techniques during these idle periods.|$|R
50|$|The recent {{emergence}} of standards-compliant browsers, JavaScript frameworks and high-quality debugging tools have made organized, scalable JavaScript code possible, and the {{emergence of}} Ajax interfaces has made it desirable. Whereas JavaScript was once reserved for relatively simple and <b>non-critical</b> <b>tasks</b> such as form validation and decorative novelties, it is now being used to write large, complex codebases that are often part of a site's core functionality. Run time errors and unpredictable behavior are no longer minor annoyances; they are fatal flaws.|$|R
40|$|We {{present an}} {{approach}} {{for the management}} of highly critical <b>tasks</b> coexisting with <b>non-critical</b> <b>tasks</b> in a single processor or multiprocessor architecture. To prevent error propagation from <b>non-critical</b> to critical <b>tasks,</b> an integrity level is assigned to groups of tasks according to their trustworthiness. Multiple levels of integrity are implemented using spatial and temporal isolation, and mediation via an integrity policy. The integrity policy defines the rules for data flow between integrity levels and resource utilisation by the tasks at different levels. Since the GUARDS project aims to provide generic solutions for a variety of application domains, the described integrity management can be implemented either in a middleware, the operating system or both. In this paper, we show a CORBA-compliant implementation of the integrity policy...|$|R
40|$|Cost {{pressure}} is driving vendors of safety-critical systems to integrate previously distributed systems. One natural approach we have previous introduced is On-Demand Redundancy (ODR), which allows safety-critical and <b>non-critical</b> <b>tasks,</b> traditionally isolated to limit interference, to execute on shared resources. Our prior work {{has shown that}} relaxed dedication (RD), one ODR strategy which allows <b>non-critical</b> <b>tasks</b> (NCTs) to execute on idle critical task resources (CTRs), significantly increases NCT throughput. Unfortunately, there are circumstances under which, {{in spite of this}} opportunity, it is difficult to effectively schedule NCTs. In this paper, we introduce distributed temporal redundancy (DTR), which allows critical tasks, which traditionally execute in lockstep, to execute asynchronously. In doing so, DTR increases scheduling flexibility, resulting in systems that achieve much closer to the optimal NCT throughput than with relaxed dedication alone; in one set of experiments, DTR schedules no less 93 % of the theoretical NCT cycles across a variety of synthetic benchmarks, outperforming RD by over 11 %, on average. Furthermore, by distributing all redundant tasks across different resources, triple-modular redundancy, and therefore fault localization, can be achieved. We demonstrate that this can be accomplished with little additional cost and complexity: in practice, relatively few DTR tasks are in flight simultaneously, limiting the additional buffering needed to support DTR...|$|R
40|$|Mixed-criticality {{systems have}} tasks with {{different}} criticality levels {{running on the}} same hardware platform. Today's DRAM controllers cannot adequately satisfy the often conflicting requirements of tightly bounded worst-case latency for critical tasks and high performance for <b>non-critical</b> real-time <b>tasks.</b> We propose a DRAM memory controller that meets these requirements by using bank-aware address mapping and DRAM command-level priority-based scheduling with preemption. Many standard DRAM controllers can be extended with our approach, incurring no performance penalty when critical tasks are not generating DRAM requests. Our approach is evaluated by replaying memory traces obtained from executing benchmarks on an ARM ISA-based processor with caches, which is simulated on the gem 5 architecture simulator. We compare our approach against previous TDM-based approaches, showing that our proposed memory controller achieves dramatically higher performance for <b>non-critical</b> <b>tasks,</b> without any {{significant impact on the}} worstcase latency of critical tasks. QC 20160510 </p...|$|R
5|$|Symantec has {{released}} {{a special edition}} of Norton Internet Security optimized for netbooks. This is available as download from the Symantec website or in a USB thumb drive. Symantec states that the Netbook edition is optimized for netbooks. The main display is optimized to provide support for the 800 x 480 screen resolution. In addition, <b>non-critical</b> <b>tasks</b> are delayed while the netbook is on battery. Furthermore, the Netbook edition contains complimentary access to Norton's secure online backup and parental control to protect children as they surf the web.|$|R
40|$|Abstract. This paper {{proposes a}} {{combined}} hardware-software approach for a controller design. The {{case of a}} brushless DC (BLDC) motor speed controller is studied. A hardware controller is implemented inside a {{field programmable gate array}} (FPGA) device, together with soft core processors that implement by software <b>non-critical</b> <b>tasks,</b> like liquid crystal display (LCD) interface and serial data communication to a host computer. This way, the control algorithm is executed in hardware, as fast as possible, while the monitoring tasks are performed by the software. Experimental results are provided, showing the working design...|$|R
50|$|Symantec has {{released}} {{a special edition}} of Norton Internet Security optimized for netbooks. This is available as download from the Symantec website or in a USB thumb drive. Symantec states that the Netbook edition is optimized for netbooks. The main display is optimized to provide support for the 800 x 480 screen resolution. In addition, <b>non-critical</b> <b>tasks</b> are delayed while the netbook is on battery. Furthermore, the Netbook edition contains complimentary access to Norton's secure online backup and parental control to protect children as they surf the web.|$|R
40|$|In {{this paper}} {{we present a}} {{low-cost}} Slack-Stealing method, named Fast Slack, to be employed in a Fixed Priority scheduling mechanism. The slack obtained is intended to improve the execution of <b>non-critical</b> <b>tasks</b> without jeopardizing the schedulability of the critical ones. The method is compared with {{the most important one}} in the real-time theory. We show that the complexity of this method is suitable to be implemented online in embedded real-time systems. Besides, we can determine online {{the amount of time that}} the calculus of the slack will take. Key Words: Slack Stealing, Embedded Systems, Real-Time Scheduling...|$|R
30|$|In this {{approach}} software configurations are categorized into two sets 1) A set, denoted by C, of configurations which perform critical tasks (checking for uniqueness, size of index table, search operation, and collisions {{are treated as}} characteristics) 2) A set, denoted by UC, of configurations which perform <b>non-critical</b> <b>tasks</b> (the padding of special symbol in fixed-length records). Initial interaction values of configurations that belong to the set C are assigned to higher values compare to configurations of UC. Since the reliability and performance of software systems depend on successful execution of their critical configurations, we can improve the reliability, performance by provisioning fault tolerant candidates to critical configurations.|$|R
40|$|Abstract. For product {{development}} projects, to void {{the risk of}} project delay, all the tasks should be finished as soon as possible. However, the critical chain method (CCM) requires the <b>non-critical</b> <b>tasks</b> to be executed as late as possible. In this paper, the traditional CCM is revised based {{on the characteristics of}} {{product development}}. Other than the traditional one, the revised CCM is created based on the active plan, that is, all tasks should be executed as early as possible. And then, the schedule scheme of active plan based critical chain method is given, including searching critical chain, no-critical chain and setting buffers. Consulting in the resource constrained project scheduling problem (RCPSP), an optimal model of CCM is built...|$|R
40|$|SIMULATION OF MULTIPLE-DRIFT TUNNEL CONSTRUCTION WITH LIMITED RESOURCES Construction work {{is often}} {{performed}} with limited resources. The optimal dynamic {{allocation of resources}} at simulation runtime sometimes requires that <b>non-critical</b> <b>tasks</b> be held back deliberately and {{not be allowed to}} start so that resources will be available to perform more critical activities later. This is an important issue that has escaped rigorous investigation. For certain projects it may be more expedient to model work at the activity level and not the resource level and embed the routing of resources into precedence relationships. The Hanging Lake Tunneling Project is presented as an example where the estimation of tunnel advance rates for all tunneling alternatives is performed at the activity level and where the allocation of limite...|$|R
40|$|Multithreaded {{processors}} raise {{new opportunities}} {{as well as}} new issues in all application domains. In the context of real-time applications, it has created one major opportunity and one major difficulty. On the one hand, the concurrent execution of multiple threads creates the opportunity to mix, on the same hardware platform, the execution of a complex real-time workload and the execution of non-critical applications. On the other hand, for real-time tasks, timing deadlines must be met and enforced. Hardware resource sharing, inherent to multithreading, hinders the timing analysis of concurrent tasks. Two different objectives are pursued in this work: enforcing timing deadlines for real-time tasks, and achieving the highest possible performance for the non-critical workload. In this paper, we present the PRETI, Partitioned REal-TIme shared cache scheme, a flexible hardwarebased cache partitioning scheme that aims at pursuing these two objectives at the same time. Plainly considering inter-task conflicts on shared caches for real-time tasks yields very pessimistic timing estimates. We remove this pessimism by allocating private cache space for real-time tasks. During the execution of a real-time task, our scheme reserves a fixed number of cache lines per set for the task. Therefore, uniprocessor, i. e. unithread, worst-case execution time (WCET) estimation techniques can be used, resulting in tight WCET estimates. Apart from the private spaces reserved for the real-time tasks currently running, the remaining cache space is shared by all tasks running on the processor, in particular <b>non-critical</b> <b>tasks,</b> enabling high performance for these tasks. While the PRETI cache scheme is of great help to achieving our both objectives, its hardware implementation consists only in a slight modification of the LRU cache replacement policy. Experiments are presented to show that the PRETI cache scheme allows for both guaranteeing the schedulability of a set of real-time tasks with tight timing constraints, and enabling high performance for the <b>non-critical</b> <b>tasks.</b> ...|$|R
40|$|Real-time, {{fault-tolerant}} systems require schedulers {{that provide}} graceful degradation during transient overloads resulting from fault recovery workloads or other system uncertainties. We initially hypothesize that a scheduler {{ideally suited to}} this environment should dispatch tasks using only response time criterion as long as all deadlines can be met, {{and that in the}} presence of overload the best a scheduler can do is semantic-driven load shedding. By temporarily eliminating less important tasks, the more intelligent scheduling algorithms were expected to service the more important tasks well without harming the <b>non-critical</b> <b>tasks</b> unnecessarily. Experimentally we then show this hypothesis to be false. On-line tracking of system load does not provide enough information about future load to effectively trigger semantic-driven load shedding. Simpler algorithms can insure predictable behavior, but sacrifice fault-free schedulability to do so. We show that a compromise can be reached by d [...] ...|$|R
40|$|We {{present the}} system {{architecture}} and {{a prototype of}} Perseus, a secure operating system focusing on personal security management. Nevertheless Perseus allows users to use their favourite applications in a convenient, known way. It is built upon a trusted computing base that is small enough to be formally verified and evaluated according to the Common Criteria or ITSEC. The design includes the services necessary to support post-purchase installation of secure applications by the user. It is flexible enough to run {{on a wide range}} of hardware platforms, which allows PCs or PDAs to be used as general-purpose trusted devices. To support a common binary interface the Perseus system acts as a host that runs an existing operating system as one application (client OS). Moreover, by using the client OS judiciously to perform <b>non-critical</b> <b>tasks,</b> the size of the secure kernel can be significantly reduced compared to a stand-alone secure system...|$|R
40|$|In mobile applications, {{the energy}} {{consumed}} by OS and application tasks primarily comes from limited DC battery source, which imposes an upper {{bound to the}} amount of time available for execution of tasks. To achieve the best Energy-aware Quality-of-Service (EQoS), it is important to prioritize the scheduling of critical <b>tasks</b> over <b>non-critical</b> <b>tasks</b> to improve overall performance while extending the battery life. Using the Combined Static/Dynamic scheduler (CSD) in the EMERALDS operating system [9, 10] as a basis, we developed the Energy-Adaptive CSD (EA-CSD) with an energy-aware scheduling algorithm 1 that executes tasks to achieve effective use of limited energy by favoring low-energy and critical tasks. Our simulation of the EA-CSD shows that battery life can be extended up to about 100 % with varying degrees of performance degradation of up to about 40 %, and the actual values of both are fully customizable by the user through parametric adjustment. ...|$|R
40|$|Multicore {{architectures}} {{are very}} appealing as they offer {{the capability of}} integrating federated architectures, where multiple independent computing elements are devoted to specific tasks, into a single device, allowing significant mass and power savings. Often, the tasks in the federated architectures are responsible for mixed criticalities tasks, i. e. {{some of them are}} mission-/safety-critical real-time tasks, while others are <b>non-critical</b> <b>tasks.</b> When consolidating mixed criticalities tasks on multicore architectures, designers must guarantee that each core does not interfere with the others, introducing side effects not possible in federated architectures. In this paper we propose a hybrid solution based on a combination of known techniques: lightweight hardware redundancy, implemented using smart watchdogs and voter logic, cooperates with software redundancy, implemented using software temporal triple module redundancy for those tasks with low criticality and no real-time requirement, and software triple module redundancy for tasks with high criticality and real-time requirement. To guarantee lack of interference, a hypervisor is used to segregate the execution of each task in a dedicated resource partition. Preliminary experimental results are reported on a prototypical vision-based navigation system...|$|R
40|$|International audienceMultithreaded processors, in {{the context}} of {{real-time}} sys- tems, create the opportunity to mix, on the same hardware platform, the execution of a complex real-time workload and the execution of non-critical applications. But resources sharing, inherent to multithreading, hinders the timing anal- ysis of concurrent tasks. Such analyses are critical to real- time tasks which have timing deadlines that must be met and enforced. In this paper, we present the PRETI, Partitioned REal- TIme shared cache scheme, a flexible, low implementation- overhead, shared cache partitioning scheme. PRETI can preclude inter-task conflicts on shared caches, and their pes- simistic impact on timing estimates, by allocating private cache space to real-time tasks. Therefore, uniprocessor, i. e. unithread, worst-case execution time (WCET) estimation techniques can be used. The remaining cache space, not re- served for currently running real-time tasks, is shared by all tasks running on the processor, in particular the non-critical ones, enabling high performances for these tasks. Experiments are presented to show that the PRETI cache scheme allows for easing and guaranteeing the schedulability of a set of real-time tasks with tight timing constraints, and enabling high performance for the <b>non-critical</b> <b>tasks...</b>|$|R
40|$|Organizations are {{increasingly}} delegating their information technology (IT) intensive business activities to external service providers, {{taking advantage of}} the rapid evolution of the global telecommunications infrastructure. The business processes being outsourced range from routine and <b>non-critical</b> <b>tasks,</b> which are resource intensive and operational, to strategic processes that directly impact revenues. IT-enabled sourcing services include IT-intensive business processes, projects, and tasks that use information technology as an enabler for designing services, coordinating service deployment, and delivering services. Managing and meeting client expectations is a major challenge in IT-enabled sourcing services and examples of failure abound. Failures typically happen throughout the sourcing process, i. e., during requirements specification, contract execution or service completion. The eSourcing Capability Model (e scmSM) contains a set of 93 best practices that address the entire sourcing process, and seeks to aid IT-enabled sourcing service providers in forming, managing and improving sourcing relationships. Each practice in the e scm is associated with a capability level. The five capability levels in the e scm describe an improvement path that progresses from a limited level of capability to deliver a service that meets a client’s particular requirements up to the highest level of enhancing valu...|$|R
40|$|Many {{companies}} are developing robots for the home, including robots specifically for older adults. There is little understanding, however, about the types {{and characteristics of}} tasks that younger and older individuals {{would be willing to}} let a robot perform. In a mailed questionnaire, participants were asked to indicate their willingness to have a robot perform each of 15 robot tasks that required different levels of interaction with the human owner and different levels of task criticality. The responses of 117 older adults (aged 65 - 86) and 60 younger adults (aged 18 - 25) were analyzed. The results indicated that respondents of both groups were more willing to have robots perform infrequent, albeit important, tasks that required little interaction with the human compared to service-type tasks with more required interaction; they were least willing to have a robot perform <b>non-critical</b> <b>tasks</b> requiring extensive interaction between robot and human. Older adults reported more willingness than younger adults in having a robot perform critical tasks in their home. The results suggest that both younger and older individuals are more interested in the benefits that a robot can provide than in their interactive abilities...|$|R
40|$|Abstract—This paper {{presents}} {{a modified version}} of the maximum urgency first scheduling algorithm. The maximum urgency algorithm combines the advantages of fixed and dynamic scheduling to provide the dynamically changing systems with flexible scheduling. This algorithm, however, has a major shortcoming due to its scheduling mechanism which may cause a critical task to fail. The modified maximum urgency first scheduling algorithm resolves the mentioned problem. In this paper, we propose two possible implementations for this algorithm by using either earliest deadline first or modified least laxity first algorithms for calculating the dynamic priorities. These two approaches are compared together by simulating the two algorithms. The earliest deadline first algorithm as the preferred implementation is then recommended. Afterwards, we make a comparison between our proposed algorithm and maximum urgency first algorithm using simulation and results are presented. It is shown that modified maximum urgency first is superior to maximum urgency first, since it usually has less task preemption and hence, less related overhead. It also leads to less failed <b>non-critical</b> <b>tasks</b> in overloaded situations. Keywords—Modified maximum urgency first, maximum urgency first, real-time systems, scheduling. I...|$|R
40|$|AbstractDependability is an {{important}} requirement in hard real-time applications due to the potentially catastrophic consequences of failures. In these systems, fault tolerance mechanisms like temporal redundancy are adopted to improve reliability. Most {{of these types of}} systems are increasingly moving towards integrating critical and non-critical functionalities on the same platform to, e. g., better utilize resources and further reduce cost, and are commonly deployed in environments where errors typically occur in the form of bursts e. g., due to Electro Magnetic Interference (EMI). Consequently, in mixed criticality real-time systems, the designer must guarantee that critical tasks are feasible even under the presence of the error burst, while ensuring the feasibility of the <b>non-critical</b> <b>tasks</b> that are not affected by the burst. We refer to this as Fault Tolerance feasibility (FT-feasibility) of mixed-criticality real-time systems. In this paper, we build on the well established results on Earliest Deadline First (EDF) scheduling, to derive a sufficient test that determines the FT-feasibility of a set of mixed criticality real-time tasks under the assumption that the inter-arrival time between two consecutive error bursts is at least equal to the hyper-period of the taskset...|$|R
40|$|This paper {{concentrates}} on monitoring projects through the PERT and MILESTONES. Complex, multilayered and distributed projects require {{a series of}} activities, some of which must be preferred sequentially and others parallely. This collection of series and parallel tasks can be modeled as a network. PERT is statistical technique applied to such a networks. In this paper we attempted to simulate the PERT networks and graphically represented the tasks along with their inter dependencies. Here project identifies the critical and <b>non-critical</b> <b>tasks</b> and evaluates the critical path to determine which tasks {{have an impact on}} the schedule.    Once a project has advanced to the phase of performance, the focus shifts from the discovery to tracking and reviewing it. MILESTONES are used to track the progress of the project at different stages and the PERT chart, on continual basis. This paper integrated both these techniques for an efficient and easier monitoring. In order to reap the results of the project sooner, we gave a provision to reduce the scheduled completion time with minimum cost burden. This can be achieved by assigning more labor and resources to the various activities.  </p...|$|R
40|$|This study applied panel {{analysis}} {{to determine the}} factors influencing interest margins in Nigeria using bank-specific, sector-specific and macroeconomic data ranging from 2010 :Q 1 to 2014 :Q 2. Based on the Hausman test, a fixed effect model in a generalized form (GLS) was estimated. The result shows that credit risk, growth in loans and advances, staff operating cost, GDP growth, inflation rate and money supply growth are significant determinants of interest margins in Nigeria. Consistent with previous studies, staff cost exerts highest impact on interest margins followed by fixed effects term. Further analysis of the banks' fixed effects reveals that seven banks control about 64 %, which raises a policy concern for banks' supervisors. The result also reveals that banks usually transfer their staff operating costs to customers by either imposing exorbitant lending rates or low deposit rates or both. This study recommends the formulation of strategies for reducing growing banks staff cost {{in the area of}} levels of compensation, employee turnover, redundancy, automation processes and outsourcing of <b>non-critical</b> <b>tasks</b> should be given due attention to ensure efficiency and competitive margin that could spur growth in Nigeria...|$|R
40|$|For many {{parallel}} matrix computations {{the execution}} time is determinedby {{the length of}} the critical path. The baseline approach used to reduce the execution time is to adjust the task granularity. If the tasks are made smaller, {{the length of the}} critical path will decrease and by extension reduce the execution time for the computation. However, reducing the size of the tasks lowers the efficiency and adds parallel overhead. Another way to lessen the impact of the critical path is to parallelize the critical tasks. In theory, this would speed up the execution of the critical path while avoiding the problems associated with a finer granularity for the <b>non-critical</b> <b>tasks.</b> The aim {{of this paper is to}} investigate if an extended approach that incorporates a parallelization of the critical path, in addition to adjusting the granularity, can significantly outperform the baseline approach of simply adjusting the granularity. A comparative study is presented that measures the extended approach’s performance against the baseline approach, using triangular linear solve as the reference computation. The results show that the extended approach can outperform the baseline approach, most notably when there is an abundance of available cores. Redistribution of data and a change in the parallel task model are identified as possible areas of improvement...|$|R
40|$|Wireless sensor nodes are a versatile, generalpurpose {{technology}} {{capable of}} measuring, monitoring and controlling their environment. Even though sensor nodes are becoming ever {{smaller and more}} power efficient, there is one area that is not yet fully addressed; Power Supply Units (PSUs). Standard solutions that are efficient enough for electronic devices with higher power consumption than sensor nodes, such as mobile phones or PDAs, {{may prove to be}} ill suited for the extreme low-power and size requirements often found on wireless sensor nodes. In this paper, a system-level design of a Power Management Architecture (PMA) is presented. The PMA is an integration of PSU hardware and various software components, and is capable of supplying a sensor node with energy from multiple sources, as well as providing status information from the PSU. The heart of the architecture is a context- and power-aware Task manager, which controls when the nodes low-power modes are activated, and is highly integrated with PSU hardware as well as other software components in the system. Its main responsibility is to schedule when energy consuming tasks can be dispatched. Depending on the task priority and system configuration, a task can be either dispatched, discarded or delayed. This approach ensures that only critical tasks will be allowed to use the battery, and that the system will be powered by renewable energy when performing other <b>non-critical</b> <b>tasks.</b> Validerad; 2007; 20070605 (jench...|$|R
40|$|Abstract- The {{design of}} {{real-time}} systems for safety-critical applications depends {{heavily on the}} normal operation of system, in critical conditions. In these applications, among of real-time tasks, a critical task must be immediately scheduled at its arrival time immediately; otherwise, it leads to a system failure and disasters in safety-critical applications. A major problem in real-time systems included critical tasks, is unpredictable arrival of these tasks. To resolve the problem, a kind of scheduler, called “super scheduler”, is employed. The problem can be more complex, in a hierarchical real-time system. A hierarchical real-time system consists of several real-time sub-systems, called “components ”. Hence, using super scheduler for each component of the system, needs to special considerations. On {{the arrival of a}} critical task, the super scheduler preempts the currently running tasks and alters the priority of all existence tasks. When the critical task is completed, the preempted tasks are executed in their new priority order. This guarantees the completion of the critical and almost all other <b>non-critical</b> <b>tasks</b> before their deadlines, and therefore the stability of the component. To guarantee the stability of a hierarchical real-time system, all its components should be stable. This paper presents a model to guarantee the stability of a hierarchical real-time system included a critical task in each component. Moreover, a fault tolerance method has been applied for all components. Evaluation results show that the proposed technique improves the stability of a hierarchical real-time system included critical tasks by decreasing the number of tasks which miss their deadline...|$|R
40|$|The use of {{hardware-based}} {{solutions for}} accelerating real-time and embedded system appli- cations {{is limited by}} the scarceness of hardware resources. By their nature, being limited by the silicon area available, hardware solutions cannot scale in size as easily as their software counter- parts. I assert a hardware-software co-design approach is required to elegantly overcome these limitations. In {{the first part of this}} dissertation, I demonstrate the feasibility of this approach by presenting a new hybrid priority queue architecture, which can be managed in hardware and/or software. As an application of this hybrid architecture, I then present a scalable task scheduler for real-time systems that reduces scheduler processing overhead and improves timing determinism of the scheduler. Performance evaluations of our Field Programmable Gate Array (FPGA) -based system-on-chip prototype shows up to a 90 % reduction in scheduling overhead and a 98 % decrease in scheduler execution time variation, when the scheduler is managed by hardware as compared to software. As recent trends in real-time and embedded systems show, applications of different criticality are being executed on a single hardware platform driven by the need to reduce size, cost and power requirements. In these mixed criticality systems, it is necessary to ensure the <b>non-critical</b> <b>tasks</b> do not interfere with the timing behavior of safety-critical tasks. In the second part of this dissertation, I investigate hardware architectures that are aware of application criticality and can adapt to changing operating conditions to provided better timing guarantees for critical tasks, while improving overall resource utilization. In support of this approach, I present a criticality aware cache architecture for mixed criticality real-time systems. As a part of the proposed cache design, a new cache replacement policy called Least Critical (LC) is presented, where critical tasks’ data is least likely to be evicted from the cache. Experimental results illustrate the impact of the LC cache replacement policy on the response time of critical tasks, and on overall application performance...|$|R
40|$|In {{this paper}} we are {{interested}} in safety-critical distributed sys-tems, composed of heterogeneous processing elements intercon-nected using the TTEthernet protocol. We address hard real-time mixed-criticality applications, which may have different critical-ity levels, and we focus on the optimization of the communica-tion configuration. TTEthernet integrates three types of traffic: Time-Triggered (TT) messages, Event-Triggered (ET) messages with bounded end-to-end delay, also called Rate Constrained (RC) messages, and Best-Effort (BE) messages, for which no timing guarantees are provided. TT messages are transmitted based on static schedule tables, and have the highest priority. RC messages are transmitted if there are no TT messages, and BE traffic has the lowest priority. TT and RC traffic can carry safety-critical mes-sages, while BE messages are <b>non-critical.</b> Mixed-criticality <b>tasks</b> and messages can be integrated onto the same architecture only if there is enough spatial and temporal separation among them. TTEthernet offers spatial separation for mixed-criticality messages through the concept of virtual links, and temporal separation, en-forced through schedule tables for TT messages and bandwidth al-location for RC messages. Given the set of mixed-criticality mes-sages in the system and the topology of the virtual links on which the messages are transmitted, {{we are interested}} to synthesize offline the static schedules for the TT messages, such that the deadlines for the TT and RC messages are satisfied, and the end-to-end delay of the RC traffic is minimized. We have proposed a Tabu Search-based approach to solve this optimization problem. The proposed algorithm has been evaluated using several benchmarks...|$|R
40|$|In {{this thesis}} a versatile, {{scalable}} solution for autonomous navigation of mobile robots is developed. The ability of autonomous navigation {{is essential to}} bring mobile systems from laboratory environments to real life scenarios. The focus is set on the special class of inherently unstable, highly dynamic Micro Aerial Vehicles (MAVs) as the systems cover many constraints and navigation aspects of general mobile robots. These are for example limitations in payload and computational resources, hard realtime requirements in state estimation for control and the required ability of full 3 D motion close to obstacles in cluttered environments. In this thesis, both algorithmic and resulting system architecture aspects are elaborated. Considering algorithms, common state estimation approaches for MAVs use efficient filter- ing techniques to fuse data from Inertial Measurement Units (IMUs) with further comple- mentary, exteroceptive sensors like light-weight cameras. Measurement delays introduced by data processing and communication pipelines are often ignored resulting in a limitation of bandwidth of the state estimator. Furthermore, most estimation approaches are globally metric limiting spatial and (depending on the approach) temporal scalability. Considering system architecture, common designs either ignore inter sensor and system synchroniza- tion issues or depend on specialized hardware. The developed navigation solution tackles these limitations with three main contributions: Firstly, the Local Reference (LR) Inertial Navigation System (INS) algorithm is intro- duced. It {{is based on a}} delayed error state space Kalman Filter. Augmentation techniques are used to process (time delayed) relative poses from multiple odometry measurements as well as (time delayed) absolute state measurements. State augmentation, especially if used for delay compensation, can lead to numerical instability in standard Kalman Filter implementations. Therefore, the square root UD (Upper triangular/Diagonal matrix fac- torization) filter algorithm is extended to integrate augmentation and marginalization in closed, factorized covariance matrix form. Stabilizing an INS by odometry measurements only results in unbounded position and yaw angle errors. This can lead to an increase in unmodeled errors due to violated small error assumptions during linearization and lim- itations in numerical precision. With the LR-INS, uncertainties of unobservable system states can be bounded in an efficient and consistent way. Instead of state estimation in a global frame, the system states are transformed into a local reference frame decreasing state uncertainty. Repeated reference switching makes the hard realtime state estimation spatially and temporally scalable. All operations of LR filtering are directly integrated in closed decomposed covariance form into a square root UD prediction step exploiting its superior numerical properties. The second contribution is the development of a flexible system architecture for au- tonomous navigation of mobile robots considering hardware and software aspects. Es- pecially on inherently unstable systems, the separation of system critical and <b>non-critical</b> <b>tasks</b> in terms of hardware can improve overall system robustness. Furthermore, a dis- tributed system concept enables the transparent exchange of algorithms between com- puter boards and hardware accelerators as for example Field Programmable Gate Ar- rays (FPGAs). In such a configuration, inter sensor and system time synchronization is essential for consistent realtime state estimation with measurement delay compensa- tion. The developed system architecture defines minimal requirements on the underlaying hardware. This enables on the one hand the use of Commercial Off-The-Shelf (COTS) components {{and on the other hand}} a flexible and fast hardware upgrade to the most recent and powerful modules. The third contribution is the demonstration of the entire autonomous navigation solu- tion including stereo vision aided hard realtime state estimation, control, environment mapping, path planning and obstacle avoidance in real life scenario quadrotor flights. Be- sides indoor and outdoor experiments for algorithmic evaluation, autonomous flights in challenging, cluttered environments with indoor/outdoor transitions and in a dusty and gloomy coal mine demonstrate the usability and robustness of the developed solution for autonomous navigation of mobile robots...|$|R
40|$|Nowadays, image {{processing}} is increasingly used in several application fields, such as biomedical, aerospace, or automotive. Within these fields, {{image processing}} {{is used to}} serve both <b>non-critical</b> and critical <b>tasks.</b> As example, in automotive, cameras are becoming key sensors in increasing car safety, driving assistance and driving comfort. They have been employed for infotainment (non-critical), {{as well as for}} some driver assistance tasks (critical), such as Forward Collision Avoidance, Intelligent Speed Control, or Pedestrian Detection. The complexity of these algorithms brings a challenge in real-time image processing systems, requiring high computing capacity, usually not available in processors for embedded systems. Hardware acceleration is therefore crucial, and devices such as Field Programmable Gate Arrays (FPGAs) best fit the growing demand of computational capabilities. These devices can assist embedded processors by significantly speeding-up computationally intensive software algorithms. Moreover, critical applications introduce strict requirements not only from the real-time constraints, but also from the device reliability and algorithm robustness points of view. Technology scaling is highlighting reliability problems related to aging phenomena, and to the increasing sensitivity of digital devices to external radiation events that can cause transient or even permanent faults. These faults can lead to wrong information processed or, in the worst case, to a dangerous system failure. In this context, the reconfigurable nature of FPGA devices can be exploited to increase the system reliability and robustness by leveraging Dynamic Partial Reconfiguration features. The research work presented in this thesis focuses on the development of techniques for implementing efficient and robust real-time embedded image processing hardware accelerators and systems for mission-critical applications. Three main challenges have been faced and will be discussed, along with proposed solutions, throughout the thesis: (i) achieving real-time performances, (ii) enhancing algorithm robustness, and (iii) increasing overall system's dependability. In order to ensure real-time performances, efficient FPGA-based hardware accelerators implementing selected image processing algorithms have been developed. Functionalities offered by the target technology, and algorithm's characteristics have been constantly taken into account while designing such accelerators, in order to efficiently tailor algorithm's operations to available hardware resources. On the other hand, the key idea for increasing image processing algorithms' robustness is to introduce self-adaptivity features at algorithm level, in order to maintain constant, or improve, the quality of results {{for a wide range of}} input conditions, that are not always fully predictable at design-time (e. g., noise level variations). This has been accomplished by measuring at run-time some characteristics of the input images, and then tuning the algorithm parameters based on such estimations. Dynamic reconfiguration features of modern reconfigurable FPGA have been extensively exploited in order to integrate run-time adaptivity into the designed hardware accelerators. Tools and methodologies have been also developed in order to increase the overall system dependability during reconfiguration processes, thus providing safe run-time adaptation mechanisms. In addition, taking into account the target technology and the environments in which the developed hardware accelerators and systems may be employed, dependability issues have been analyzed, leading to the development of a platform for quickly assessing the reliability and characterizing the behavior of hardware accelerators implemented on reconfigurable FPGAs when they are affected by such fault...|$|R

