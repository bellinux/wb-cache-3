0|10000|Public
50|$|The NMIS {{business}} rules engine classifies events on their business impact, {{not just the}} technical nature. The rules engine is extremely powerful; however it can be configured in minutes for a <b>network</b> <b>with</b> a small <b>number</b> of devices to hours for <b>networks</b> <b>with</b> large <b>numbers</b> of devices.|$|R
40|$|Abstract- In this work, {{we study}} {{the problem of}} maximizing lightpath {{reliability}} in optical mesh networks against simultaneous failures of multiple fiber links without using protection schemes. The fiber links belong to shared risk link groups (SRLGs) that have arbitrary failure probabilities. This problem is NP-hard and we propose heuristic algorithms for <b>networks</b> <b>with</b> large <b>numbers</b> of SRLGs as well as optimal solutions for <b>networks</b> <b>with</b> smaller <b>numbers</b> of SRLGs. The solutions are evaluated through simulations...|$|R
30|$|Next, we will {{consider}} the maximum energy consumption of UDNP and EDNP in <b>networks</b> <b>with</b> different <b>number</b> of nodes when both strategies adopt an optimal duty cycle.|$|R
40|$|Limitations of {{capabilities}} of shallow perceptron networks are investigated. Lower bounds are derived for growth of numbers of units and sizes of output weights in networks representing Boolean functions of d variables. It is shown that for large d, almost any randomly chosen Boolean function cannot be tractably represented by shallow perceptron networks, i. e., each its representation requires a <b>network</b> <b>with</b> <b>number</b> of units or sizes of output weights depending on d exponentiall...|$|R
50|$|Bareilly Junction to Sanjay Van; another North-South line, almost {{parallel}} to Yellow Line {{and it will}} also be the longest line on <b>network</b> <b>with</b> most <b>number</b> of stations.|$|R
30|$|To {{the best}} of our knowledge, the energy {{efficiency}} analysis for underlay D 2 D communications in a <b>network</b> <b>with</b> large <b>number</b> of BS antennas has not been carried out before.|$|R
30|$|Theorem 3 : The problem {{formulation}} in (9) {{provides the}} optimum solution for maximizing the operation time for any clustering <b>network</b> <b>with</b> the <b>number</b> of CHs smaller {{than or equal}} to k.|$|R
40|$|Approximation of {{solutions}} of integral equations by <b>networks</b> <b>with</b> kernel units is investigated theoretically. There are derived upper bounds on speed of decrease of errors in approximation {{of solutions}} of Fredholm integral equations by kernel <b>networks</b> <b>with</b> increasing <b>numbers</b> of units. The estimates are obtained for Gaussian and degenerate kernels...|$|R
5000|$|The {{structure}} of the numbering system of the public switched telephone <b>network</b> is geographic, <b>with</b> <b>number</b> portability, as follows: ...|$|R
40|$|We train neural {{networks}} to classify images according to facial expression, gender, and identity. We explore two dimension reduction techniques, PCA and downsampling. We compare two activation functions, the logistic sigmoid and “funny”-tanh and experiment <b>with</b> <b>networks</b> <b>with</b> different <b>numbers</b> of hidden units. ...|$|R
40|$|This comment reexamines Simard et al. 's work in [D. Simard, L. Nadeau, H. Kroger, Phys. Lett. A 336 (2005) 8 - 15]. We {{found that}} Simard et al. {{calculated}} mistakenly the local connectivity lengths Dlocal of networks. The right results of Dlocal are presented and the supervised learning performance of feedforward neural <b>networks</b> (FNNs) <b>with</b> different rewirings are re-investigated in this comment. This comment discredits Simard et al's work by two conclusions: 1) Rewiring connections of FNNs cannot generate <b>networks</b> <b>with</b> small-world connectivity; 2) For different training sets, there {{do not exist}} <b>networks</b> <b>with</b> a certain <b>number</b> of rewirings generating reduced learning errors than <b>networks</b> <b>with</b> other <b>numbers</b> of rewiring. Comment: 8 pages, 5 figure...|$|R
30|$|For {{the number}} of hidden units in each layer, <b>networks</b> <b>with</b> small <b>number</b> of hidden units may not learn enough {{representations}} for future tasks while <b>networks</b> <b>with</b> large <b>numbers</b> of neurons may increase the possibility of overfitting, which cause poor generalization in untrained dataset. From the literature, there is no formula to calculate an exact number of neurons being used, but {{the number of}} neurons within a range is effective in practice. As the input neurons are 1000, number of units for each hidden layer is selected as 500 to avoid both too narrow and too complicated network structures. In addition, the relationship between numbers of hidden units and classification performance of the network are also discussed in the next section.|$|R
5000|$|The small-world {{properties}} can be mathematically {{expressed by}} the slow increase of the average diameter of the <b>network,</b> <b>with</b> the total <b>number</b> of nodes , ...|$|R
40|$|Transmission limitations, {{asymmetric}} {{loads and}} network upgrade strategies are all expected {{to mean that}} different links in a wavelength division multiplexed (WDM) network {{are expected to have}} different numbers of wavelengths. In this paper we examine the performance improvements offered by wavelength converters in WDM networks which have different numbers of wavelengths on different links. We show that wavelength converters can provide more significant performance improvements in <b>networks</b> <b>with</b> differing <b>numbers</b> of wavelengths on each link than in <b>networks</b> <b>with</b> the same <b>number</b> of wavelengths on each link. However, these performance improvements depend on the number of wavelengths on each link, the offered loads and the wavelength assignment scheme used...|$|R
40|$|In {{this paper}} {{a problem of}} {{successive}} changing of individuals states or technical means from some population is considered. This changing is connected as with individuals aging so with appearance of new individuals. The problem is solved using queueing <b>networks</b> <b>with</b> infinite <b>numbers</b> of servers in their nodes...|$|R
40|$|We {{study the}} effect of regularization in an on-line {{gradient-descent}} learning scenario for a general two-layer student <b>network</b> <b>with</b> an arbitrary <b>number</b> of hidden units. Training examples are randomly drawn input vectors labelled by a two-layer teacher <b>network</b> <b>with</b> an arbitrary <b>number</b> of hidden units which may be corrupted by Gaussian output noise. We examine {{the effect of}} weight decay regularization on the dynamical evolution of the order parameters and generalization error in various phases of the learning process, in both noiseless and noisy scenarios...|$|R
40|$|We {{discuss the}} power of {{networks}} of evolutionary processors where only two types of nodes are allowed. We prove that (up to an intersection with a monoid) every recursively enumerable language can be generated by a <b>network</b> <b>with</b> one deletion and two insertion nodes. <b>Networks</b> <b>with</b> an arbitrary <b>number</b> of deletion and substitution nodes only produce finite languages, and for each finite language one deletion node or one substitution node is sufficient. <b>Networks</b> <b>with</b> an arbitrary <b>number</b> of insertion and substitution nodes only generate context-sensitive languages, and (up to an intersection with a monoid) every context-sensitive language can be generated by a <b>network</b> <b>with</b> one substitution node and one insertion node. ...|$|R
40|$|Abstract It is a known {{fact that}} given two rooted binary phylogenetic trees {{the concept of}} maximum acyclic {{agreement}} forests is sufficient to com-pute hybridization <b>networks</b> <b>with</b> minimum hybridization <b>number.</b> In this work we demonstrate by, first, presenting an algorithm and, second, showing its cor-rectness that this concept is also sufficient {{in the case of}} multiple input trees. In detail, we show that for computing hybridization <b>networks</b> <b>with</b> minimum hy-bridization <b>number</b> for multiple rooted binary phylogenetic trees, it is sufficient to consider only maximum acyclic agreement forests instead of acyclic agreement forests of arbitrary size...|$|R
40|$|We {{study the}} effect of two types of noise, data noise and model noise, in an on-line {{gradient-descent}} learning scenario for general two-layer student <b>network</b> <b>with</b> an arbitrary <b>number</b> of hidden units. Training examples are randomly drawn input vectors labeled by a two-layer teacher <b>network</b> <b>with</b> an arbitrary <b>number</b> of hidden units. Data is then corrupted by Gaussian noise affecting either the output or the model itself. We examine {{the effect of}} both types of noise on the evolution of order parameters and the generalization error in various phases of the learning process...|$|R
50|$|Since 1990 Queensland Community Care <b>Network</b> along <b>with</b> a <b>number</b> {{of other}} organisations has been funded {{to manage the}} Community Visitors Scheme in Queensland.|$|R
40|$|International audienceThe {{recovery}} of the causality <b>networks</b> <b>with</b> a <b>number</b> of variables is an important problem that arises in various scientific contexts. For detecting the causal relationships in the <b>network</b> <b>with</b> a big <b>number</b> of variables, the so called Graphical Lasso Granger (GLG) method was proposed. It is widely believed that the GLG-method tends to overselect causal relationships. In this paper, we propose a thresholding strategy for the GLG-method, which we call 2 -levels-thresholding, and we show that with this strategy the variable overselection of the GLG-method may be overcomed. Moreover, we demonstrate that the GLG-method with the proposed thresholding strategy may become superior to other methods that were proposed for the {{recovery of}} the causality networks...|$|R
40|$|Computationally {{efficient}} {{classification system}} architecture is proposed. It utilizes fast tensor-vector multiplication algorithm to apply linear operators upon input signals. The approach is applicable to {{wide variety of}} recognition system architectures ranging from single stage matched filter bank classifiers to complex neural <b>networks</b> <b>with</b> unlimited <b>number</b> of hidden layers. Comment: 7 page...|$|R
5000|$|... a {{stochastic}} simulation, {{typically used}} for discrete systems where events occur probabilistically and which cannot be described directly with differential equations (this is a discrete simulation {{in the above}} sense). Phenomena in this category include genetic drift, biochemical or gene regulatory <b>networks</b> <b>with</b> small <b>numbers</b> of molecules. (see also: Monte Carlo method).|$|R
40|$|Consider {{the multivariate}} nonparametric {{regression}} model. It is shown that estimators based on sparsely connected deep neural <b>networks</b> <b>with</b> ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to log n-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While {{there is a}} lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large <b>networks</b> <b>with</b> <b>number</b> of potential parameters being much bigger than the sample size. The analysis gives some insights why multilayer feedforward neural networks perform well in practice. Interestingly, the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that scaling the <b>network</b> depth <b>with</b> the logarithm of the sample size is natural. Comment: 30 page...|$|R
50|$|Television {{and radio}} are {{provided}} by a system of public-broadcasting organisations (sharing three television and five radio <b>networks)</b> together <b>with</b> a <b>number</b> of commercial channels.|$|R
50|$|On 31 May 1987, the Blauer Enzian {{became part}} of the newly {{introduced}} international EuroCity <b>network,</b> <b>with</b> train <b>number</b> EC20 northbound and EC21 southbound (later renumbered EC12/13). Interesting was the change of throuh coaches among EC Blauer Enzian,EXP Dachstein(Lindau-Graz-Lindau),EC Transalpin(Basel-Wien/Graz/Klagenfurt) in Summer 1990,in Schwarzach St Veit and Bischofshofen.The EC Blauer Enzian conveyed through coaches Dotrmund-Klagenfurt/Graz/Lubjana.|$|R
5000|$|Call Control/Switching: {{call control}} or {{switching}} functionality decides the future course of call {{based on the}} call signalling processing. E.g. switching functionality may decide based on the [...] "called number" [...] that the call be routed towards a subscriber within this operator's <b>network</b> or <b>with</b> <b>number</b> portability more prevalent to another operator's network.|$|R
5000|$|E. Gelenbe, Z.-H. Mao and Y-D. Li [...] "Function {{approximation}} {{by random}} neural <b>networks</b> <b>with</b> a bounded <b>number</b> of layers", 'Differential Equations and Dynamical Systems', 12 (1&2), 143-170, Jan. April 2004.|$|R
40|$|ABSTRACT In this paper, we {{consider}} the computational power of a new variant of networks of evolutionary processors {{which seems to be}} more suitable for a software and hardware implementation. Each processor as well as the data navigating throughout the network are now considered to be polarized. While the polarization of every processor is predefined, the data polarization is dynamically computed by means of a valuation mapping. Consequently, the protocol of communication is naturally defined by means of this polarization. We show that tag systems can be simulated by these <b>networks</b> <b>with</b> a constant <b>number</b> of nodes, while Turing machines can be simulated, in a time-efficient way, by these <b>networks</b> <b>with</b> a <b>number</b> of nodes depending linearly on the tape alphabet of the Turing machine...|$|R
40|$|A {{mathematical}} model {{of transportation and}} distribution of resources with a generalized conservation law is created. For this optimization problem of flow retention within the given limits, algorithms for efficient nonlinear programming are developed. The proposed approach gives the opportunity to solve problems of flow distribution with multitype nonlinear objective functions for <b>networks</b> <b>with</b> any <b>number</b> of closed loops. ??????? ?????????????? ?????? ??????????????? ? ????????????? ???????? ? ?????????? ??????? ??????????. ??? ???? ??????????????? ?????? ????????? ??????? ? ???????????? ???????? ????????? ?????????, ???????????? ?? ??????????? ??????? ??????????? ????????????????. ????????? ??????, ??????? ???? ??????????? ?????? ?????? ????????????? ??????? ? ???????????? ??????????? ???????? ????????? ??? ????? ? ???????????? ??????????? ????????? ??????...|$|R
3000|$|We {{consider}} a <b>network</b> <b>with</b> large <b>number</b> of nodes being deployed uniformly and distributed over a unit area. We use the function H(l) {{as the number}} of hops on a path between two arbitrary nodes x and y such that |x,y| = l is the euclidean distance between these two nodes. According to [31], given a geographical routing protocol, we have [...]...|$|R
40|$|In {{this paper}} we {{introduce}} multi-server queueing systems {{that can be}} considered as extensions of conventional M/M/s queue to fractional number of servers. We show that the extended Erlang's delay function can be used to calculate delay probabilities for such systems. This approach enables the delay analysis in <b>networks</b> <b>with</b> fractional <b>number</b> of servers in the nodes using classical methods...|$|R
40|$|Abstract. Surrogate {{solutions}} of Fredholm integral equations by feedforward {{neural networks}} are investigated theoretically. Convergence of surrogate solutions computable by <b>networks</b> <b>with</b> increasing <b>numbers</b> of computational units to theoretically optimal solutions is proven and upper bounds on rates of convergence are derived. The results {{hold for a}} variety of computational units, they are illustrated by examples of perceptrons and Gaussian radial units. ...|$|R
50|$|In {{the context}} of network science, a sparse network is a <b>network</b> <b>with</b> less <b>number</b> of links than the maximum {{possible}} number of links within the same network. The opposite of the sparse network is dense or complete network. The study of sparse networks {{is a relatively new}} area primarily stimulated by the study of real networks, such as social and computer networks.|$|R
5000|$|A {{high number}} of incidents, from a social and service point of view, have brought the term [...] "RER poubelle" [...] ("RER trash" [...] in French), often used by its users, and even its staff. Assaults are {{frequent}} and unpunctuality is {{the highest in the}} Transilien <b>network,</b> <b>with</b> the <b>number</b> of late trains going from 9.9% to 14.1% between 1994 and 1995.|$|R
40|$|Learning {{from data}} formalized as a {{minimization}} of a regularized empirical error is studied {{in terms of}} approx-imate minimization over sets of functions computable by <b>networks</b> <b>with</b> increasing <b>number</b> of hidden units. There are derived upper bounds on speed of convergence of in-fima achievable over <b>networks</b> <b>with</b> hidden units to the global infimum. The bounds are {{expressed in terms of}} norms tailored to the type of network units and moduli of continuity of regularized empirical error functionals. ...|$|R
