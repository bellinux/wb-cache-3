5|10000|Public
40|$|Quick {{assessment}} of the condition of gearboxes used in helicopters is a safety requirement. One {{of the most widely}} used helicopter on-board-mounted condition monitoring system these days is the Health and Usage Monitoring System. It has been specifically designed to monitor the condition of all safety-critical components operating in the helicopter through calculation of so-called condition indicators (CIs) - signal processing routines designed to output a single number that represents the condition of the monitored component. Among <b>number</b> <b>of</b> <b>available</b> <b>parameters,</b> there is a couple of CIs that over the years of testing have earned a reputation of being the most reliable measures of the gear tooth condition. At the same time, however, it has been observed that in some cases, those techniques do not properly indicate the deteriorating condition with the propagation of a gear tooth fault with the period of operation. Hence, three more robust methods have been suggested, which are discussed in this article...|$|E
40|$|Load {{modeling}} {{plays an}} important role in power system stability analysis and planning studies. The parameters of load models may experience variations in different application situations. Choosing appropriate parameters is critical for dynamic simulation and stability studies in power system. This paper presents a method to select the parameters with good generalization ability based on a given large <b>number</b> <b>of</b> <b>available</b> <b>parameters</b> that have been identified from dynamic simulation data in different scenarios. Principal component analysis is used to extract the major features of the given parameter sets. Reduced feature vectors are obtained by mapping the given parameter sets into principal component space. Then support vectors are found by implementing a classification problem. Load model parameters based on the obtained support vectors are built to reflect the dynamic property of the load. All of the given parameter sets were identified from simulation data based on the New England 10 -machine 39 -bus system, by taking into account different situations, such as load types, fault locations, fault types, and fault clearing time. The parameters obtained by support vector machine have good generalization capability, and can represent the load more accurately in most situations...|$|E
40|$|Development of {{artificial}} neural network (ANN) models using real plant data for the prediction of fresh steam properties from a brown coal-fired boiler of a Slovenian power plant is reported. Input parameters for this prediction were selected from a large <b>number</b> <b>of</b> <b>available</b> <b>parameters.</b> Initial selection was made on a basis of expert knowledge and previous experience. However, the final set of input parameters was optimized with a compromise between smaller number of parameters and higher level of accuracy through sensitivity analysis. Data for training were selected carefully from the available real plant data. Two models were developed, one including mass flow rate of coal and the other including belt conveyor speed {{as one of the}} input parameters. The rest of the input parameters are identical for both models. Both models show good accuracy in prediction of real data not used for their training. Thus both of them are proved suitable for use in real life, either on-line or off-line. Better model out of these two may be decided on a case-to-case basis depending on the objective of their use. The objective of these studies was to examine the feasibility of ANN modeling for coal-based power or combined heat and power (CHP) plants. (c) 2008 Elsevier Ltd. All rights reserved...|$|E
40|$|We {{present a}} design of {{experiments}} (DOE) approach to nanometer design of an analog voltage controlled oscillator (VCO) using CMOS technology. The functional specifications of the VCO optimized in this design are the center frequency and minimization of overall power consumption as well as minimization of power due to gate tunneling current leakage, a component that was not important in previous generations of CMOS technologies but is dominant at 45 nm. Due to the large <b>number</b> <b>of</b> <b>available</b> design <b>parameter</b> (gate oxide thickness and transistor sizes), the concurrent achievement of all optimization goals is difficult. A DOE approach is shown to be very effective and {{a viable alternative to}} standard design exploration in the nanometer regime. ...|$|R
40|$|Tactons (tactile icons) are {{structured}} vibrotactile messages {{which can be}} used for non-visual information presentation. Information can be encoded in a set of Tactons by manipulating <b>parameters</b> <b>available</b> in the tactile domain. One limitation is the <b>number</b> <b>of</b> <b>available</b> usable <b>parameters</b> and research is ongoing to find further effective ones. This paper reports an experiment investigating different techniques (amplitude modulation, frequency, and waveform) for creating texture as a parameter for use in Tacton design. The results of this experiment show recognition rates of 94 % for waveform, 81 % for frequency, and 61 % for amplitude modulation, indicating that a more effective way to create Tactons using the texture parameter is to employ different waveforms to represent roughness. These results will aid designers in creating more effective and usable Tactons...|$|R
40|$|WOSInternational audienceThis paper {{investigates the}} forward {{modeling}} of chirpsonar {{data for the}} quantitative characterization of marine subbottom sediment between 1 and 10 kHz. The forward modeling, based on a transfer function approach, included impacts of layering or impedance mismatch, attenuation, roughness, and transitional layers, i. e., continuous impedance variations. The presented approach provided the best compromise between the <b>number</b> <b>of</b> <b>available</b> geoacoustic <b>parameters</b> from chirp-sonar data and the subbottom modeling accuracy. The forward model was tested on deep-sea chirp-sonar data acquired at a central frequency of 3. 5 kHz. Comparisons between synthetic and experimental seismograms showed good agreement for the first 15 m of buried layers. Performance of the inversion using this forward model was also examined through sensitivity analysis. The results suggested that estimations of layer thickness, impedance, and transitional layer thickness were robust, whereas roughness and attenuation estimations were subject to wavelength and layer thickness conditions...|$|R
40|$|For {{cost savings}} {{and ease of}} operation, nearshore regions have been {{considered}} as ideal regions for deploying wave energy converters (WECs) and wave farms. As the water depths of these regions may be frequently limited to 50 m or less, they {{can be considered as}} being transitional/intermediate to shallow when compared to the wave lengths of interest for wave energy conversion. Since the impact of water depths on propagation of waves is significant, it cannot be ignored in wave energy assessment. According to the basic wave theory, in order to work out accurate wave energy amounts in finite water depth, detailed wave spectral distributions should be given. However, for some practical reasons, there are still some cases where only scatter diagrams and/or the statistical wave parameters are available, whilst the detailed wave spectra are discarded. As a result, the assessments of wave energy and resources are frequently determined by ignoring the effect of water depths or using very simplified approximations. This research paper aims to develop more accurate approximation methods by utilising a <b>number</b> <b>of</b> <b>available</b> <b>parameters</b> such that a better estimate on the wave resource assessment can be achieved even if the detailed wave spectra are not available. As one important goal, the research can provide some important indications on how the measured wave data are effectively presented {{so that they can be}} very useful for assessing the wave energy resource, especially in the cases including the effects of finite water depths...|$|E
40|$|Até o momento não há um estudo que objetivou sintetizar a {{magnitude}} e variação do papel dos invertebrados aquáticos sobre a decomposição do detrito foliar em ambientes aquáticos. Além disso, existe uma falta de conhecimento sobre qual é o impacto geral desses pequenos organismos na decomposição foliar. Além do mais, existe um quantidade de controvérsias sobre como e quais devem ser os fatores mais influentes nesse processo ecológico. O objetivo deste estudo foi através de uma abordagem meta-analítica, entender como estes organismos afetam a decomposição foliar em ecossistemas aquáticos, e como este efeito varia de acordo com aspectos ecológicos e metodológicos. Foram incluídos estudos que avaliaram as taxas de decaimento exponencial na presença e ausência de invertebrados, sendo abordadas apenas as variáveis presentes em mais de 90 % dos estudos do banco de dados. Como esperado, a presença dos invertebrados acelera a decomposição foliar e su{{a magnitude}} do efeito em escala global é consistente. Este efeito não difere em relação ao tipo de ecossistema (lótico ou lêntico), temperatura ou latitude. Portanto, esta análise contradiz o argumento que invertebrados tem um maior papel na decomposição em ecossistemas de altas latitudes enquanto microrganismos deveriam ser mais importantes mais próximo aos trópicos. Não observou-se nenhum viés ou quanto aos métodos de exclusão dos invertebrados ou ainda quanto a diferença entre as malhas. Apesar de ser difícil afirmar que não existem efeitos aditivos da exclusão experimental ou ainda a diferença entre as malhas. Provavelmente outros parâmetros não foram incluídos nesta análise poderiam explicar o efeito dos invertebrados na decomposição foliar, entretanto um insuficiente número de parâmetros disponíveis dificultam mais avaliações. Esta síntese conclui que o papel acelerador dos invertebrados detritívoros deveria ser incluída em modelos biogeoquímicos. Mais especificamente, estes modelos deveriam incorporar parâmetros como a origem do detrito e a duração experimental. There is no study aimed to synthesize {{the magnitude and}} variation of aquatic invertebrates for the decomposition of leaf litter in aquatic ecosystems. Furthermore, {{there is also a}} lack of knowledge about what is the overall impact of these small organisms on leaf decomposition. Moreover, {{there are a number of}} controversies that are the most influential factors on one of the most important processes of aquatic ecology. For example, the influence of latitude and temperature or yet the methodology employed for measuring the effect of the aquatic detritivorous invertebrates. Therefore the aim of our study was, through a meta-analytic review, understand how these organisms affect the decomposition of foliar detritus in aquatic ecosystems, and how this effect varies according to the methodological and environmental parameters most used. Through the evaluation of decomposition rates (k) in the presence and absence of these invertebrates 104 independent studies satisfied our selection criteria and were included in the database. To intention present representativeness of the sample, each used parameter in this analysis need to be present in over 90 % of the selected studies. No publication bias was detected, although we verified a significant decrease in the invertebrate decomposition effect in longer experimental duration, indicating that short experiments reveal higher detritivoria while longer experiments manifest lower mass loss. As expected, these results cast reveal that in general the presence of invertebrates accelerates the decomposition process. Furthermore the global effect size, these effect are consistent and do not differ in relation to aquatic ecosystem types (lotic, lentic or marine), temperature or latitude of the experiments, even presenting a slight increase importance of invertebrates in the decomposition process toward lower latitudes. Therefore, this analysis contradicts the argument that invertebrates have a greater role in the decomposition of aquatic ecosystems at higher latitudes while microorganisms would be more important at minors. Temperature result also contradicts the preview and results of this and other reviews where it could reveal some variation of invertebrate effect. No weakness of invertebrate exclusion methods (mesh bags, electric field or toxic) or even the standardized mesh bag difference (a standardized difference between coarse and fine mesh bags) was found. In contrast to the most of literature, these parameters did not show any significant relationship with the effect size. However should be difficult affirm there is no additive effects of experimental exclusion or also standardized mesh size difference. Conversely, other factors interfere significantly in the decomposition mediated by invertebrates such as detritus origin (allochthonous or autochthonous) and experimental duration. Probably other parameters besides the not included in this analysis could explain the effect of invertebrates on leaf decomposition, however an insufficient <b>number</b> <b>of</b> <b>available</b> <b>parameters</b> hamper this assay. We conclude that this synthesis indicates that the role of detritivorous invertebrates should be included in biogeochemical models, which consider nutrient cycling in temperate or tropical aquatic ecosystems. More specifically these models should incorporate environmental parameters such as detritus origin, as well methodological parameters such as experimental duration...|$|E
50|$|DocBook XSL's stylesheets {{are highly}} configurable. Each of the {{different}} formats has a <b>number</b> <b>of</b> XSLT <b>parameters</b> <b>available</b> for simple customization. For example, the XSL-FO transforms allow the user to define {{the size of the}} pages. Additionally, the XSLT documents themselves are modular; it is possible for the user to add, change, or replace particular levels of functionality. This can allow DocBook XSL to process new documentation tags added to the standard DocBook, or to simply change how the XSLT's generate the resulting format.|$|R
40|$|Since {{the advent}} of {{automation}} {{in the field of}} hematological cell counters there has been a constant refinement of the technology and increase in the <b>number</b> <b>of</b> newer <b>parameters</b> <b>available</b> on CBC analysers. Many novel parameters are being put into routine clinical use and both clinical evaluation and monitoring critically depend on knowledge of laboratory reference ranges. Here, we present reference interval for the Sysmex XE- 2100, with emphasis on the novel or newer research parameters. Blood samples from a total of 122 clinically asymptomatic and apparently healthy subjects were evaluated and a final of 100 subjects (54 -M, 46 -F) were included in the study. A broad spectrum <b>of</b> <b>parameters</b> <b>available</b> with the analyser was assessed and reference ranges for the same evaluated...|$|R
40|$|The {{purpose of}} this work is to propose an {{extension}} to SON of specific use cases within LTE networks. Current wireless networks have become more complex, due to a higher <b>number</b> <b>of</b> base stations, convergence of different technologies (GSM/UMTS/LTE), greater <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> and a multi-vendor environment. It has been proven that SON is a handful resource for mobile operators to decrease costs and save time on dreary optimization tasks, reducing human interaction needed to optimize a network and ensuring quality targets are met. A set of practical use cases are presented, followed by the analysis and actions taken in order to solve them. These solutions are taken as an input for the proposal of automated processes using SON...|$|R
30|$|The use of noninvasive {{ventilation}} (NIV) strongly increased {{during the}} last twenty years, and became a common technique for managing acute and chronic respiratory failures. In response to a growing market, manufacturers propose yearly (if not more often) new ventilators with an increasing <b>number</b> <b>of</b> ventilatory modes and settings. Our objective {{is to help the}} physicians to improve the synchronization between the ventilator pressure cycles and patient breathing cycles, a key feature to optimize comfort and reduce the work of breathing [1]. The <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> in a modern ventilator is indeed significantly greater than in those produced one or two decades ago. Unfortunately, the terminology used to designate these ventilator settings remains not unified and the same abbreviation can sometimes correspond to different quantities, as clearly pointed out long time ago [2]. Such disparities make a comparison between the different ventilators difficult to establish.|$|R
40|$|The {{amount of}} {{collected}} data in many scientific fields is increasing, {{all of them}} requiring a common task: extract knowledge from massive, multi parametric data sets, as rapidly and efficiently possible. This {{is especially true in}} astronomy where synoptic sky surveys are enabling new research frontiers in the time domain astronomy and posing several new object classification challenges in multi dimensional spaces; given the high <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> for each object, feature selection is quickly becoming a crucial task in analyzing astronomical data sets. Using data sets extracted from the ongoing Catalina Real-Time Transient Surveys (CRTS) and the Kepler Mission we illustrate a variety of feature selection strategies used to identify the subsets that give the most information and the results achieved applying these techniques to three major astronomical problems. Comment: 7 pages, to appear in refereed proceedings of Scalable Machine Learning: Theory and Applications, IEEE BigData 201...|$|R
40|$|MRI {{has become}} an {{important}} tool to noninvasively assess global and regional cardiac function, infarct size, or myocardial blood flow in surgically or genetically modified mouse models of human heart disease. Constraints on scan time due to sensitivity to general anesthesia in hemodynamically compromised mice frequently limit the <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> in one imaging session. Parallel imaging techniques to reduce acquisition times require coil arrays, which are technically challenging to design at ultrahigh magnetic field strengths. This work validates {{the use of an}} eight-channel volume phased-array coil for cardiac MRI in mice at 9. 4 T. Two- and three-dimensional sequences were combined with parallel imaging techniques and used to quantify global cardiac function, T 1 -relaxation times and infarct sizes. Furthermore, the rapid acquisition of functional cine-data allowed {{for the first time in}} mice measurement of left-ventricular peak filling and ejection rates under intravenou...|$|R
5000|$|The {{effect is}} caused by {{extremely}} localised fluctuations in surface pressure and humidity, which cause the initial shock wave to distort momentarily and refocus on itself, leading to a double shock wave, each of markedly reduced effect. This has distinct utility {{in the employment of}} air delivered ordnance in close proximity to key urban structures as part of an ongoing influence campaign.The energy of the blast is so great that the pressure and temperature of the gas outside the shock front is negligible compared to the pressure and temperature inside. This substantially reduces the <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> in the problem, leaving only the energy E of the blast, the resting density of the external gas, and the time t since the explosion. With only these three dimensional parameters, it is possible to form other quantities with unique functional dependences. In particular, the only length scale in the problem is ...|$|R
40|$|Abstract — There {{is growing}} {{interest}} in physical-layer key generation schemes that provide very strong or even perfect security in wireless communication systems. One such scheme is reciprocal channel key generation, where two nodes quantize reciprocal channel state information to generate keys. Although the use of multiple-input multiple-output (MIMO) techniques is interesting in this case since the <b>number</b> <b>of</b> random <b>parameters</b> <b>available</b> for rapid key generation is increased, MIMO techniques may also provide more information to an eavesdropper. This work presents a new MIMO measurement campaign performed in LOS and NLOS indoor environments that studies the correlation of the channel between legitimate users with the eavesdropper channel, revealing what key generation rates can be attained in practice and what fraction of generated key bits are safe from eavesdroppers. The effect <b>of</b> eavesdropper separation, <b>number</b> <b>of</b> antennas, eavesdropper advantage, and covariance separability are studied. I...|$|R
40|$|Neural {{networks}} are excellent mapping tools for complex financial data. Their mapping capabilities however {{do not always}} result in good generalizability for financial prediction models. Increasing the <b>number</b> <b>of</b> nodes and hidden layers in a neural network model produces better mapping of the data since the <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> to the model increases. This is determinal to generalizabilitiy of the model since the model memorizes idiosyncratic patterns in the data. A neural network model {{can be expected to}} be more generalizable if the model architecture is made less complex by using fewer input nodes. In this study we simplify the neural network by eliminating input nodes that have the least contribution to the prediction of a desired outcome. We also provide a theoretical relationship of the sensitivity of output variables to the input variables under certain conditions. This research initiates an effort in identifying methods that would improve the generalizability of neural networks in financial prediction tasks by using mergers and bankruptcy models. The result indicates that incorporating more variables that appear relevant in a model does not necessarily improve prediction performance...|$|R
40|$|Asymmetric imaging {{errors are}} {{frequently}} {{the main cause}} for tight tolerances and high demands on manufacture and assembly of optical systems. In order to simultaneously increase robustness and reduce manufacturing cost, desensitization strategies can be applied. Tolerance effects have been included into the optimization function (merit function) by some lens designers to find insensitive designs 1 - 5 and frequently compensators are employed to further improve the performance of assembled lenses. Compensators are limited to a small <b>number</b> <b>of</b> system parameters, but selective assembly of components can extend the <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> for compensation. It can be employed to reduce tolerance effects of disturbed parameters by finding the best matches out {{of a set of}} components. The potential of using tolerance desensitization in conjunction with selective assembly to reduce asymmetric errors in imaging optical systems is investigated. A focus is given on strategies t o find tolerance insensitive design forms under the presence of selective assembly compensators and the selection of suitable parameters for desensitization and measurement...|$|R
40|$|We {{study the}} {{evolution}} of social clusters, in an analogy with physical spin systems, and in detail show {{the importance of the}} concept of the "self" of each agent with quantifiable variable attributes. We investigate the effective influence space around each agent with respect to each attribute, which allows the cutoff of the Hamiltonian dictating the time evolution and suggest that equations similar to those in general relativity for geodesics in distorted space may be relevant in such a context too. We perform in a simple small-world toy system simulations with weight factors for different couplings between agents and their attributes and spin-type flips in either direction from consideration of a utility function, and observe chaotic, highly aperiodic behavior, with also the possibility of punctuated equilibrium-like phenomena. In a realistic large system, because of the very large <b>number</b> <b>of</b> <b>parameters</b> <b>available,</b> we suggest that it would probably almost always be necessary to reduce the problem to simpler systems with a manageable set of coupling matrices, using assumptions of fuzziness or symmetry or some other consideration...|$|R
40|$|Tolerancing {{has long}} been {{identified}} as a crucial part in the development of optical systems. It aims at finding the best balance between quality and cost as tolerances closely tie together manufacturing expenses and performance. Tolerance effects have been included into the optimization function (merit function) by some lens designers to find insensitive designs 1 - 5 and frequently compensators are employed to further improve the performance of assembled lenses. Compensators are limited to a small <b>number</b> <b>of</b> system parameters, but selective assembly of components can extend the <b>number</b> <b>of</b> <b>parameters</b> <b>available</b> for compensation. It can be employed to reduce tolerance effects of disturbed parameters by finding the best matches out of a set of components. In this work we discuss how desensitization and selective assembly can be combined to loosen tolerances and increase as-built performance. The investigations concentrate on tolerance insensitive design forms under the presence of selective assembly compensators. In contrast to desensitizing a given lens or introducing new design means we focus on introducing new assembly strategies into the design procedure and investigate how using selective assembly as a compensator while desensitizing the remaining design parameters can lead to even less sensitive designs...|$|R
40|$|Empirical data {{concerning}} the qualitative and quantitative nature of program dependence is presented {{for a set of}} 20 programs ranging from 600 lines of code to 167, 000 lines of code. The sources of dependence considered are global variables and formal parameters and the targets considered are a program's predicate nodes. The results show that as the <b>number</b> <b>of</b> formal <b>parameters</b> <b>available</b> to a predicate increases, there is a decrease in the proportion of these formal parameters which are depended upon by the predicate. No such correlation was found for global variables. Results from theoretical and actual computation time analysis indicate that the computation of dependence information is practical, suggesting that the analysis may be beneficial to several application areas. The paper also presents results concerning correlations that provide strong evidence that the global and formal dependence sources are independent of one another and that the <b>numbers</b> <b>of</b> globals and formals are independent {{of the size of the}} procedure that contains them. Finally, two visualization techniques for displaying dependence information are introduced. Illustrations show how these visualizations and predicate dependence analysis can assist in activities such as testing, comprehension, and evolution...|$|R
40|$|This is the {{accepted}} manuscript. The final version {{is available at}} [URL] study presents a novel roughness formulation to conceptually account for microtopography and compares it to four existing roughness models from literature. The aim {{is to increase the}} grid size for computational efficiency, while capturing subgrid scale effects with the roughness formulation to prevent the loss in accuracy associated with coarse grids. All roughness approaches are implemented in the Hydroinformatics Modeling System and compared with results of a high resolution shallow water model in three test cases: rainfall-runoff on an inclined plane with sinewave shaped microtopography, ow over an inclined plane with random microtopography and rainfall-runoff in a small natural catchment. Although the high resolution results can not be reproduced exactly by the coarse grid model, e. g. local details of ow processes can not be resolved, overall good agreement between the upscaled models and the high resolution model has been achieved. The proposed roughness formulation generally shows the best agreement of all compared models. It is further concluded that the accuracy increases with the <b>number</b> <b>of</b> calibration <b>parameters</b> <b>available,</b> however the calibration process becomes more difficult. Using coarser grids results in significant speedup in comparison with the high resolution simulation. In the presented test cases the speedup varies from 20 up to 2520, depending on the size and complexity of the test case and the difference in cell sizes. The authors thank the Alexander von Humboldt-Foundation for the Humboldt Research Fellowship granted to Dr. Dongfang Liang...|$|R
40|$|This {{research}} {{focuses on}} reducing the limitations imposed on the repeating topologies in lattice structures that restrict what can be created using the RealiZer Selective Laser Melting (SLM) machine. The creation of regular, randomly perturbed, polar mapped, and random metallic lattice structures using SLM apparatus is reported and discussed in this thesis. It was observed that a new technique was required to generate the slice data files used to control the SLM equipment {{in order to create}} structures that measured significantly more than 10 cells in each axis. The research details the motivations behind the development of the computational methods utilised to develop lattice parts and how the iterations of these methods enabled different areas of research to progress. The limits of the angles from the horizontal that elements could be built are reviewed and scanning techniques are developed that create elements below these values. In order to create horizontal links significant proportions of the machine control software were replaced with software developed {{during the course of the}} research. This is discussed at length along with how the limitations on the <b>number</b> <b>of</b> processing <b>parameters</b> <b>available</b> could be removed and how pauses which let sections of the melt on horizontal links freeze before processing the next section could be used. It is suggested that systems or experimental set ups are developed that allow greater control over the duration of these pauses. This would enable further research into the processing of horizontal links, developing them to the point where they are mechanically consistent and comparable to other links in the structuresEThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|In this project, we {{interest}} {{ourselves with}} the follow-up {{of the growth}} of large Cameroonian cities, whose extension is increasingly large. Considering the non availability of recent maps, the analysis of the synthetic aperture radar (SAR) images seems {{to be a very good}} tool allowing the evaluation of the expansion of the cities with a good accuracy. In this work, the analysis of texture is applied on a SAR ERS- 1 image for determining the urban limits of the city of Yaounde in Cameroon. To obtain an accurate textural classification of a SAR image, it is often necessary to make several tests in order to find the best combination of parameters among a whole of textural parameters. This exercise inevitably leads to a prohibitory calculating times, despite the power of the modern computers. Indeed, if one desires to extract the best pair from parameters of texture among a whole of N parameters, one must examine 2 N possibilities. Time calculating thus growths exponentially with the <b>number</b> <b>of</b> <b>parameters</b> <b>available.</b> To circumvent this difficulty, the techniques of sequential selection of textural parameters have been created, in order to obtain the set of parameters producing the best accuracy of classification, with a less cost of calculation. The two principal techniques utilized within the framework of this study are the technique of progressive sequential elimination and the technique of regressive sequential elimination. In the first case, the whole of parameters to be used is constituted by progressive addition of the parameters filling the criteria of selection. In the second case, all the parameters form part of the initial unit, and the parameters not filling the criteria of selection are successively eliminated until one obtains the desired <b>number</b> <b>of</b> parameters. T [...] ...|$|R
40|$|The use of type Ic Super Luminous Supernovae (SLSN Ic) {{to examine}} the cosmological {{expansion}} introduces a new standard ruler with which to test theoretical models. The sample suitable {{for this kind of}} work now includes 11 SLSN Ic's, which have thus far been used solely in tests involving ΛCDM. In this paper, we broaden the base of support for this new, important cosmic probe by using these observations to carry out a one-on-one comparison between the R_ h=ct and ΛCDM cosmologies. We individually optimize the parameters in each cosmological model by minimizing the χ^ 2 statistic. We also carry out Monte Carlo simulations based on these current SLSN Ic measurements to estimate how large the sample would have to be in order to rule out either model at a ∼ 99. 7 % confidence level. The currently available sample indicates a likelihood of ∼ 70 - 80 % that the R_ h=ct Universe is the correct cosmology versus ∼ 20 - 30 % for the standard model. These results are suggestive, though not yet compelling, given the current limited <b>number</b> <b>of</b> SLSN Ic's. We find that if the real cosmology is ΛCDM, a sample of ∼ 240 SLSN Ic's would be sufficient to rule out R_ h=ct at this level of confidence, while ∼ 480 SLSN Ic's would be required to rule out ΛCDM if the real Universe is instead R_ h=ct. This difference in required sample size reflects the greater <b>number</b> <b>of</b> free <b>parameters</b> <b>available</b> to fit the data with ΛCDM. If such SLSN Ic's are commonly detected in the future, they could be a powerful tool for constraining the dark-energy equation of state in ΛCDM, and differentiating between this model and the R_ h=ct Universe...|$|R
40|$|Summary. The {{mathematical}} model allowed to reproduce and study at qualitative level {{the change of}} berries form {{and the structure of}} the berries layer in the course of drying. The separate berry in the course of drying loses gradually its elasticity, decreases in volume, the peel gathers in folds, there appear internal emptiness. In the course of drying the berries layer decreases in thickness, contacting berries stick strongly with each other due to the coordinated folds of peel appearing, the layer is condensed due to penetration of the berries which have lost elasticity into emptiness between them. The model with high specification describes black currant drying process and therefore has a large <b>number</b> <b>of</b> the <b>parameters</b> <b>available</b> to change. Among them three most important technological parameters, influencing productivity and the drying quality are chosen: the power of microwave radiation P, thickness of the berries layer h, environmental pressure p. From output indicators of the model the most important are three functions from time: dependence of average humidity of the layer on time Wcp (t), dependence of the speed of change of average humidity on time dWcp (t) /dt, dependence of the layer average temperature on time Tср (t). On the standard models classification the offered model is algorithmic, but not analytical. It means that output characteristics of model are calculated with the entrance ones, not by analytical transformations (it is impossible principally for the modeled process), but by means of spatial and temporary sampling and the corresponding calculation algorithm. Detailed research of the microwave drying process by means of the model allows to allocate the following stages: fast heating, the fast dehydration, the slowed-down dehydration, consolidation of a layer of a product, final drying, heating after dehydration...|$|R
40|$|Strongly gravitationally lensed quasar-galaxy systems {{allow us}} to compare {{competing}} cosmologies {{as long as one}} can be reasonably sure of the mass distribution within the intervening lens. In this paper, we assemble a catalog of 69 such systems from the Sloan Lens ACS and Lens Structure and Dynamics surveys suitable for this analysis, and carry out a one-on-one comparison between the standard model, ΛCDM, and the R_ h=ct Universe, which has thus far been favored by the application of model selection tools to other kinds of data. We find that both models account for the lens observations quite well, though the precision of these measurements {{does not appear to be}} good enough to favor one model over the other. Part of the reason is the so-called bulge-halo conspiracy that, on average, results in a baryonic velocity dispersion within a fraction of the optical effective radius virtually identical to that expected for the whole luminous-dark matter distribution modeled as a singular isothermal ellipsoid, though with some scatter among individual sources. Future work can greatly improve the precision of these measurements by focusing on lensing systems with galaxies as close as possible to the background sources. Given the limitations of doing precision cosmological testing using the current sample, we also carry out Monte Carlo simulations based on the current lens measurements to estimate how large the source catalog would have to be in order to rule out either model at a ∼ 99. 7 % confidence level. We find that if the real cosmology is ΛCDM, a sample of ∼ 200 strong gravitational lenses would be sufficient to rule out R_ h=ct at this level of accuracy, while ∼ 300 strong gravitational lenses would be required to rule out ΛCDM if the real Universe were instead R_ h=ct. The difference in required sample size reflects the greater <b>number</b> <b>of</b> free <b>parameters</b> <b>available</b> to fit the data with ΛCDM. We point out that, should the R_ h=ct Universe eventually emerge as the correct cosmology, its lack of any free parameters for this kind of work will provide a remarkably powerful probe of the mass structure in lensing galaxies, and a means of better understanding the origin of the bulge-halo conspiracy...|$|R
40|$|AbstractThe National Risk Assessment Partnership (NRAP) is a {{research}} organization focused on developing methods and tools for long-term quantitative risk assessment for carbon storage. NRAP's approach is to divide the carbon storage system into components—reservoir, wells, seals, groundwater, atmosphere—and to develop reduced order models {{for each of these}} components. These rapid performance models are trained and/or validated against full physics reservoir models (e. g., TOUGH 2, GEM) so that they reproduce similar results but in a fraction of the time of the reservoir model. The different component models can then be combined in an integrated assessment model that can simulate the full system {{in a matter of seconds}} or minutes rather than the days, weeks, or longer that a full physics simulation of the entire system would take. The integrated model can then be run in a Monte Carlo mode to assess the probability of failure of a carbon storage system. In NRAP, part of the focus is on long-term leakage risk, and the rapid performance reservoir models are designed to generate pressures and saturations within the reservoir, and particularly at the reservoir-seal interface, both during injection and for up to 1, 000 years post injection. These pressures and saturations can then be used as inputs to wellbore or seal leakage models to predict rates and volumes of leakage of CO 2 and/or in situ fluids. In the past few years, NRAP researchers have developed and applied a <b>number</b> <b>of</b> different reduced order models to saline-, gas-, and oil- bearing storage fields. These models vary significantly in several respects. They range from lookup tables or response surfaces to models such as polynomial chaos expansion to models that rely on data mining and artificial intelligence techniques. Each of these types of rapid performance models has different strengths and weaknesses, depending on the method used, the reservoir type, and the goals. In all cases, the rapid performance models required a geologic model and at least some traditional reservoir simulation runs for training or validation purposes. Also, in all techniques developed, an initial analysis is performed to reduce the <b>number</b> <b>of</b> input parameters and scenarios needed for the final simulations. The <b>number</b> <b>of</b> reservoir simulation runs needed can vary significantly based on the reduced order model used. The time and sophistication that it takes to develop a reduced order model is another major factor that varies among the different types of models. Some models are easily able to handle a significant <b>number</b> <b>of</b> varying spatial inputs, while others are limited in the <b>number</b> <b>of</b> input <b>parameters</b> <b>available.</b> Additionally, while all of the rapid performance models will run much faster than a reservoir model, there run times can vary from fractions of a second to tens of seconds or longer, depending on the situation being modeled. This paper will describe the different types of reduced order reservoir models used within NRAP. It will also provide a critical assessment of these rapid performance models, discuss under what circumstances different rapid performance models would be most effective, and evaluate their utility in the context of quantitative risk assessment...|$|R
30|$|<b>Number</b> <b>of</b> <b>available</b> customers.|$|R
30|$|The <b>number</b> <b>of</b> <b>available</b> {{frequency}} channels, F.|$|R
3000|$|... c, {{we observe}} the {{performance}} of studied protocols in terms <b>of</b> <b>available</b> licensed channel utilization. When the <b>number</b> <b>of</b> PUs is large in the network, only a small <b>number</b> <b>of</b> licensed channels are available for opportunistic use. The <b>number</b> <b>of</b> requested OBU pairs TE-CAM and CC-VANET can schedule {{is equal to the}} <b>number</b> <b>of</b> <b>available</b> licensed channels. Therefore, when the <b>number</b> <b>of</b> <b>available</b> licensed channels is very low in the network, they can allocate only a small <b>number</b> <b>of</b> requested OBU pairs. On the other hand, HT-CAM is able to schedule a much higher <b>number</b> <b>of</b> OBU pairs with limited <b>number</b> <b>of</b> <b>available</b> licensed channels by spatial reuse. Thus, the utilization <b>of</b> <b>available</b> licensed channels increases with increasing <b>number</b> <b>of</b> PUs in the network.|$|R
3000|$|It is {{observed}} that the <b>number</b> <b>of</b> correctly-identified causative SNPs decreases when the <b>number</b> <b>of</b> <b>available</b> SNPs is large. This {{is to be expected}} because the Bonferroni correction factor is a quadratic function <b>of</b> the <b>number</b> <b>of</b> <b>available</b> SNPs. An increase in the Bonferroni correction factor leads to an increase in the Bonferroni-corrected χ [...]...|$|R
50|$|This {{is a small}} subset <b>of</b> <b>available</b> <b>parameters</b> {{described}} in standards like ASME B46.1 and ISO 4287.Most of these parameters originated from the capabilities of profilometers and other mechanical probe systems. In addition, new measures of surface dimensions have been developed which are more {{directly related to the}} measurements made possible by high-definition optical gauging technologies.|$|R
40|$|In {{the present}} study, Wavelet {{analysis}} of P wave for {{the prediction of}} Atrial Fibrillation after CABG is evaluated. Continuous Wavelet Transform is applied to ECG and Mean/Max parameters are calculated within the P window for different frequency bands. Thus, 24 <b>parameters</b> are <b>available,</b> which, along with the 4 window lengths (corresponding to P wave length of X, Y, Z, V signals), make a pool <b>of</b> <b>available</b> <b>parameters</b> {{to be used for}} classification. Linear regression is used for the classification of the two groups and bootstrapping is applied in order to enhance statistical robustness. The features {{to be used in the}} regression model are selected from the pool <b>of</b> <b>available</b> <b>parameters</b> by use <b>of</b> an iterative procedure. The outcome of the feature selection procedure shows that X and Z-axis features as well as vector-magnitude features are the most important ones for the prediction of Atrial Fibrillation after CABG...|$|R
5000|$|Integrated text search using a <b>number</b> <b>of</b> <b>available</b> search engines.|$|R
25|$|In {{general the}} <b>number</b> <b>of</b> <b>available</b> hosts on a subnet is 2h−2, where h is the <b>number</b> <b>of</b> bits {{used for the}} host portion of the address. The <b>number</b> <b>of</b> <b>available</b> subnets is 2n, where n is the <b>number</b> <b>of</b> bits used for the network portion of the address. This is the RFC 1878 {{standard}} used by the IETF, the IEEE and COMPTIA.|$|R
