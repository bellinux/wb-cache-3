86|1325|Public
40|$|Installation Instructions Opening a CSP File Choosing a Model Viewing the GENET Network Running the Network Customising View Progress Window Viewing the Constraint Graph Recording the Network Using the Mouse Buttons On Line Help GENCSP <b>Random</b> <b>Problem</b> Generator Installation Instructions The {{software}} provided {{includes the}} demonstration program GENET v 1. 0 and the <b>random</b> <b>problem</b> generator GENCSP. The programs {{are based on}} XView toolkit and can run under Openwindows or X 11. The programming language is C++ and therefore a C++ compiler (e. g. GNU C++) is needed to compile the software. The source code for the programs resides in the directories GenetDemo/Genet and GenetDemo/Gencsp respectively. To compile the programs, type make in the GenetDemo/ directory. The subdirectories are recursivelly visited and the executables are compiled and linked. The generated executables appear with the name...|$|E
40|$|In {{this paper}} we {{investigate}} probabilistic completeness and asymptotic optimality of various existing randomized sampling based algorithms such as, probabilistic roadmap methods (PRM) and its many variants. We give new alternate proofs to many such existing theorems regarding probabilistic completeness and asymptotic optimality, in both incremental and independent <b>random</b> <b>problem</b> model framework...|$|E
40|$|The paper {{focuses on}} {{evaluating}} constraint satisfaction search algorithms on application based <b>random</b> <b>problem</b> instances. The application we {{use is a}} well-studied problem in the electric power industry: optimally scheduling preventive maintenance of power generating units within a power plant. We show how these scheduling problems can be cast as constraint satisfaction problems and used to define the structure of randomly generated non-binary CSPs. The <b>random</b> <b>problem</b> instances are then used to evaluate several previously studied algorithms. The paper also demonstrates how constraint satisfaction {{can be used for}} optimization tasks. To find an optimal maintenance schedule, a series of CSPs are solved with successively tighter cost-bound constraints. We introduce and experiment with an "iterative learning" algorithm which records additional constraints uncovered during search. The constraints recorded during the solution of one instance with a certain cost-bound are used again on subsequen [...] ...|$|E
40|$|This chapter covers {{research}} in constraint programming (CP) and related areas involving <b>random</b> <b>problems.</b> Such research {{has played a}} significant role in the development of more efficient and effective algorithms, as well as in understanding the source of hardness in solving combinatorially challenging <b>problems.</b> <b>Random</b> <b>problems</b> have proved useful in a number of different ways. Firstly, they provide a relatively “unbiased ” sample for benchmarking algorithms. In the early days of CP, many algorithms were compared using only a limited sample of problem instances. In some cases, this may have lead to premature conclusions. <b>Random</b> <b>problems,</b> by comparison, permit algorithms to be tested on statistically significant samples of hard problems. However, as we outline in the rest of this chapter, there remain pitfalls waiting the unwary in their use. For example, <b>random</b> <b>problems</b> may not contain structures found in many real world problems, and these structures can make problems much easier or much harder to solve. As a second example, the process of generating <b>random</b> <b>problems</b> may itself be “flawed”, giving problem instances which are not, at least asymptotically, combinatorially hard. <b>Random</b> <b>problems</b> have also provided insight into problem hardness. For example, the influential paper by Cheeseman, Kanefsky and Taylor [12] highlighted the computational difficulty of problems which are on the “knife-edge ” between satisfiability and unsatisfiability [84]. There is even hope within certain quarters that <b>random</b> <b>problems</b> may be one of the links in resolving the P=NP question. Finally, insight into problem hardness provided by <b>random</b> <b>problems</b> has helped inform the design of better algorithms and heuristics. For example, the design of a number of branching heuristics for the Davis Logemann Loveland satisfiability (DPLL) procedure has been heavily influenced by the hardness of <b>random</b> <b>problems.</b> As a second example, the rapid randomization and restart (RRR) strategy [45, 44] was motivated by the discovery of heavy-tailed runtime distributions in backtracking style search procedures on <b>random</b> quasigroup completion <b>problems...</b>|$|R
40|$|We examine phase {{transitions}} in problems derived from real computational problems using {{a wide variety}} of algorithms. These phase transitions resemble those observed with randomly generated problems. Real problems do, however, contain new features (e. g. large scale structures rare in <b>random</b> <b>problems)</b> which can make them significantly harder than <b>random</b> <b>problems.</b> Our results suggest a new methodology for benchmarking algorithms. In addition, they help to identify the location of the really hard real problems. 1 Introduction A conventional method for comparing the performance of algorithms is to use benchmark problems. Since the supply of benchmark problems is usually limited, we may be unable to perform a statistically significant comparison, or to determine accurately how performance depends on problem size and problem difficulty. An alternative approach is to use <b>random</b> <b>problems</b> which are cheap to generate at different <b>problem</b> sizes. Unfortunately <b>random</b> <b>problems</b> are typically easy [...] ...|$|R
2500|$|Karl-Heinz Borgwardt, The Simplex Algorithm: A Probabilistic Analysis, Algorithms and Combinatorics, Volume 1, Springer-Verlag, 1987. (Average {{behavior}} on <b>random</b> <b>problems)</b> ...|$|R
40|$|In this paper, {{a problem}} {{generator}} for the Two-Dimensional Rectangular Single Large Object Placement Problem is presented. The parameters defining this problem are identified and described. The fea-tures {{of the problem}} generator are pointed out, and it is shown how the program {{can be used for}} the generation of reproducible <b>random</b> <b>problem</b> instances. two-dimensional cutting, defect, problem generator...|$|E
40|$|Recent {{progress}} on search and reasoning procedures {{has been driven}} by experimentation on computationally hard problem instances. Hard <b>random</b> <b>problem</b> distributions are an important source of such instances. Challenge problems from the area of finite algebra have also stimulated research on search and reasoning procedures. Nevertheless, the relation of such problems to practical applications is somewhat unclear. Realistic problem instances clearly have more structure than the <b>random</b> <b>problem</b> instances, but, on the other hand, they are not as regular as the structured mathematical problems. We propose a new benchmark domain that bridges the gap between the purely random instances and the highly structured problems, by introducing perturbations into a structured domain. We will show how to obtain interesting search problems in this manner, and how such problems can be used to study the robustness of search control mechanisms. Our experiments demonstrate that the performan [...] ...|$|E
40|$|In recent years, {{there has}} been much {{research}} on local search techniques for solving constraint satisfaction problems, including Boolean satisfiability problems. Some of the most successful procedures combine a form of random walk with a greedy bias. These procedures are quite e#ective in a number of problem domains, for example, constraint-based planning and scheduling, graph coloring, and hard <b>random</b> <b>problem</b> instances. However, i...|$|E
40|$|Abstract. <b>Random</b> <b>problems</b> are a {{good source}} of test suites for {{comparing}} quality of constraint satisfaction techniques. Quasigroup problems are representative of structured <b>random</b> <b>problems</b> that are closer to real-life problems. In this paper, we study generators for Quasigroup Completion Problem (QCP) and Quasigroups with Holes (QWH). We propose an improvement of the generator for QCP that produces a larger number of consistent problems by using propagation through the all-different constraint. We also re-formulate the algorithm for generating QWH...|$|R
30|$|Random variational {{inequality}} theories is {{an important}} part of random function analysis. These topics have attracted many scholars and exports due to the extensive applications of the <b>random</b> <b>problems</b> (see, e.g., [1 – 17]). In 1997, Huang [3] first introduced the concept of random fuzzy mapping and studied the <b>random</b> nonlinear quasicomplementarity <b>problem</b> for <b>random</b> fuzzy mappings. Further, Huang studied the random generalized nonlinear variational inclusions for random fuzzy mappings in Hilbert spaces. Ahmad and Bazán [18] studied a class of random generalized nonlinear mixed variational inclusions for random fuzzy mappings and constructed an iterative algorithm for solving such <b>random</b> <b>problems.</b>|$|R
30|$|On {{the other}} hand, It {{is well known}} that the study of the random {{equations}} involving the random operators in view of their need in dealing with probabilistic models in applied sciences is very important. Motivated and inspired by the recent research works in these fascinating areas, the <b>random</b> variational inequality <b>problems,</b> <b>random</b> quasi-variational inequality <b>problems,</b> <b>random</b> variational inclusion <b>problems</b> and <b>random</b> quasi-complementarity <b>problems</b> have been introduced and studied by Ahmad and Bazán [16], Chang [17], Chang and Huang [18], Cho et al. [19], Ganguly and Wadhwa [20], Huang [21], Huang and Cho [22], Huang et al. [23], and Noor and Elsanousi [24].|$|R
40|$|In this work, we {{extend the}} {{efficiency}} of distributed search in constraint satisfaction networks. Our method adds interleaving and parallelism into distributed backtrack search. Moreover, it has a filtering capacity that makes it open to cooperative work. Experimentations show that 1) the shape of phase transition with <b>random</b> <b>problem</b> can be characterized, 2) important speed-up can be achieved when the distribution of solutions is non uniform...|$|E
40|$|In our new {{learning}} environment {{we have developed}} various JAVA tools, and studied ways to display mathematical expressions effectively on the web. In this paper, we discuss how we adapt MathML and JAVA techniques to develop mathematical tools, for example matrix calculators and the <b>Random</b> <b>Problem</b> Generator (RPG), {{which can be used}} in our linear algebra class. It can be profitably used in other classes as well. ...|$|E
40|$|The {{increasing}} {{scale and}} complexity of computer networks imposes a need for highly flexible management mechanisms. The concept of network virtualization promises to provide this flexibility. Multiple arbitrary virtual networks can be constructed {{on top of a}} single substrate network. This allows network operators and service providers to tailor their network topologies to the specific needs of any offered service. However, the assignment of resources proves to be a problem. Each newly defined virtual network must be realized by assigning appropriate physical resources. For a given set of virtual networks, two questions arise: Can all virtual networks be accommodated in the given substrate network? And how should the respective resources be assigned? The underlying problem is commonly known as the Virtual Network Embedding problem. A multitude of algorithms has already been proposed, aiming to provide solutions to that problem under various constraints. For the evaluation of these algorithms typically an empirical approach is adopted, using artificially created <b>random</b> <b>problem</b> instances. However, due to complex effects of <b>random</b> <b>problem</b> generation the obtained results can be hard to interpret correctly. A structured evaluation methodology that can avoid these effects is currently missing. This thesis aims to fill that gap. Based on a thorough understanding of the problem itself, the effects of <b>random</b> <b>problem</b> generation are highlighted. A new simulation architecture is defined, increasing the flexibility for experimentation with embedding algorithms. A novel way of generating embedding problems is presented which migitates the effects of conventional problem generation approaches. An evaluation using these newly defined concepts demonstrates how new insights on algorithm behavior can be gained. The proposed concepts support experimenters in obtaining more precise and tangible evaluation data for embedding algorithms...|$|E
40|$|AbstractAn {{improvement}} over an earlier feasible directions minimization algorithm is presented. In {{a certain sense}} the new feasible descent cone algorithm is {{shown to be a}} generalization of Rosen's gradient projection method. The algorithm is evaluated for linear programming test problems, and promising results are obtained for <b>random</b> <b>problems</b> and problems involving weight minimization of plane trusses...|$|R
40|$|A recent {{theoretical}} result by Achlioptas et al. {{shows that}} conventional models of <b>random</b> <b>problems</b> are trivially insoluble in the limit. This insolubility {{is due to}} the presence of `flawed variables', variables whose values are all `flawed' (or unsupported). We survey the literature to identify experimental studies that lie within the scope of this result. We then estimate theoretically and measure experimentally the size at which flawed variable can be expected to occur. We also study an alternative model of <b>random</b> <b>problems</b> proposed by Achlioptas et al. to tackle such flaws. We show that flawed values still occur in this model at problem sizes similar to those used in practice. To eliminate flawed values and variables, we introduce a `flawless' generator which puts a limited amount of structure into the conflict matrix. We also show how to introduce structure into the constraint graph. We can thereby generate ensembles of problems that are not trivially insoluble due to the presence of flawed variables, as well as problems that contain structures in their constraint graphs which are rare in purely <b>random</b> <b>problems...</b>|$|R
40|$|Since {{the early}} 2000 s {{physicists}} {{have developed an}} ingenious but non-rigorous formalism called the cavity method to put forward precise conjectures on phase transitions in <b>random</b> <b>problems</b> [Mezard, Parisi, Zecchina: Science 2002]. The cavity method predicts that the satisfiability threshold in the <b>random</b> $k$-SAT <b>problem</b> is $ 2 ^k\ln 2 -\frac 12 (1 +\ln 2) +\epsilon_k$, with $\lim_{k\rightarrow\infty}\epsilon_k= 0 $ [Mertens, Mezard, Zecchina: Random Structures and Algorithms 2006]. This paper contains a proof of that conjecture...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with the <b>random</b> <b>problem</b> containing white noise: [−p(t) u′(t) ]′ + q(t) u(t) = λ∗ϱ(t) u(t) + f(t) + ϑ(t) B(t, ω) αu(0) + α′u′(0) = ξ(ω), βu(1) + β′u′(1) = η(ω), 0 < t < 1 Uniqueness and existence theorems are proved for solutions of this problem with different λ∗, and the Green's function and eigenfunction series are used to describe the solutions...|$|E
40|$|Epistasis {{correlation}} is {{a measure}} that estimates the strength of interactions between problem variables. This paper presents an empirical study of epistasis correlation on {{a large number of}} <b>random</b> <b>problem</b> instances of NK landscapes with nearest neighbor interactions. The results are analyzed with respect to the performance of hybrid variants of two evolutionary algorithms: (1) the genetic algorithm with uniform crossover and (2) the hierarchical Bayesian optimization algorithm...|$|E
40|$|<b>Random</b> <b>problem</b> {{distributions}} {{have played}} a key role in the study and design of algorithms for constraint satisfaction and Boolean satisfiability, as well as in our understanding of problem hardness, beyond standard worst-case complexity. We consider <b>random</b> <b>problem</b> distributions from a highly structured problem domain that generalizes the Quasigroup Completion problem (QCP) and Quasigroup with Holes (QWH), a widely used domain that captures the structure underlying a range of real-world applications. Our problem domain is also a generalization of the well-known Sudoku puzzle: we consider Sudoku instances of arbitrary order, with the additional generalization that the block regions can have rectangular shape, in addition to the standard square shape. We evaluate the computational hardness of Generalized Sudoku instances, for different parameter settings. Our experimental hardness results show that we can generate instances that are considerably harder than QCP/QWH instances of the same size. More interestingly, we show the impact of different balancing strategies on problem hardness. We also provide insights into backbone variables in Generalized Sudoku instances and how they correlate to problem hardness...|$|E
40|$|Several local search {{algorithms}} for propositional satis ability havebeen proposed {{which can}} solve hard <b>random</b> <b>problems</b> beyond {{the range of}} conventional backtracking procedures. In this paper, we explore the impact of focusing search in these procedures on the "unsatisfied variables"; that is, those variables which appear in clauses which are not yet satisfied. For <b>random</b> <b>problems,</b> we show that such a focus reduces the sensitivity to input parameters. We also observe a simple scaling law in performance. For non-random problems, we showthat whilst this focus can improve performance, many problems remain difficult. We speculate that such problems will remain hard for local search unless constraint propagation techniques can be combined with hill-climbing...|$|R
40|$|AbstractThe {{traveling}} salesman {{problem is}} one of the most famous combinatorial problems. We identify a natural parameter for the two-dimensional Euclidean traveling salesman problem. We show that for <b>random</b> <b>problems</b> there is a rapid transition between soluble and insoluble instances of the decision problem at a critical value of this parameter. Hard instances of the traveling salesman problem are associated with this transition. Similar results are seen both with randomly generated problems and benchmark problems using geographical data. Surprisingly, finite-size scaling methods developed in statistical mechanics describe the behaviour around the critical value in <b>random</b> <b>problems.</b> Such phase transition phenomena appear to be ubiquitous. Indeed, we have yet to find an NP-complete problem which lacks a similar phase transition...|$|R
40|$|We {{perform a}} {{comprehensive}} {{theoretical and empirical}} study of the bene ts of singleton consistencies. Our theoretical results help place singleton consistencies within the hierarchy of local consistencies. To determine the practical value of these theoretical results, we measured the cost-effectiveness of pre-processing with singleton consistency algorithms. Our experiments use both <b>random</b> and structured <b>problems.</b> Whilst pre-processing with singleton consistencies is not in general beneficial for <b>random</b> <b>problems,</b> it starts to pay off when randomness and structure are combined, {{and it is very}} worthwhile with structured problems like Golomb rulers. On such problems, pre-processing with consistency techniques as strong as singleton generalized arc-consistency (the singleton extension of generalized arc-consistency) can reduce runtimes. We also show that limiting algorithms that enforce singleton consistencies to a single pass often gives a small reduction in the amount of pruning and improves their cost-effectiveness. These experimental results also demonstrate that conclusions from studies on <b>random</b> <b>problems</b> should be treated with caution...|$|R
40|$|We {{address a}} dynamic {{decision}} problem in which decision makers must pay some costs when they change their decisions along the way. We formalize this problem as Dynamic SAT (DynSAT) with decision change costs, whose {{goal is to}} find a sequence of models that minimize the aggregation of the costs for changing variables. We provide two solutions to solve a specific case of this problem. The first uses a Weighted Partial MaxSAT solver after we encode the entire problem as a Weighted Partial MaxSAT problem. The second solution, which we believe is novel, uses the Lagrangian decomposition technique that divides the entire problem into sub-problems, each of which can be separately solved by an exact Weighted Partial MaxSAT solver, and produces both lower and upper bounds on the optimal in an anytime manner. To compare the performance of these solvers, we experimented on the <b>random</b> <b>problem</b> and the target tracking problem. The experimental results show that a solver based on Lagrangian decomposition performs better for the <b>random</b> <b>problem</b> and competitively for the target tracking problem. ...|$|E
40|$|This paper {{presents}} {{a class of}} NK landscapes with nearest-neighbor interactions and tunable overlap. The considered class of NK landscapes is solvable in polynomial time using dynamic programming; this allows us to generate {{a large number of}} <b>random</b> <b>problem</b> instances with known optima. Several variants of standard genetic algorithms and estimation of distribution algorithms are then applied to the generated problem instances. The results are analyzed and related to scalability theory for selectorecombinative genetic algorithms and estimation of distribution algorithms...|$|E
40|$|Problem {{generators}} are convenient {{tools for}} making {{large numbers of}} problem instances available to objectively evaluate the performance of different algorithms. We suggest that a) problem generators should be used {{only as a last}} resort, and b) if used they should be 'portable', i. e., will generate the same problem instances on different computers, and c) use statistical methodology consistent with good experimental design. We provide a number of rules and tools to use when deciding to use a <b>random</b> <b>problem</b> generator. status: publishe...|$|E
40|$|In {{this paper}} {{we present a}} {{backtracking}} version of the survey propagation algorithm. We show that {{the introduction of the}} simplest form of backtracking greatly improves the ability of the original survey propagation algorithm in solving difficult <b>random</b> <b>problems</b> near the sat-unsat transition. Comment: 9 pages, two figure...|$|R
40|$|Abstract. We {{study the}} {{experimental}} {{consequences of a}} recent theor-etical result by Achlioptas et al. that shows that conventional models of <b>random</b> <b>problems</b> are trivially insoluble in the limit. We survey the lit-erature to identify experimental studies that lie {{within the scope of}} this result. We then estimate theoretically and measure experimentally the size at which problems start to become trivially insoluble. Our results demonstrate that most (but not all) of these experimental studies are luckily unaected by this result. We also study an alternative model of <b>random</b> <b>problems</b> that does not suer from this asymptotic weakness. We show that, at a typical problem size used in experimental studies, this model looks similar to conventional models. Finally, we generalize this model so that we can independently adjust the constraint tightness and density. ...|$|R
40|$|Many <b>random</b> <b>problems</b> of {{engineering}} interest can be {{looked upon as}} examples of continuous Markoff processes. Such processes are completely determined if a certain function, the transition probability, is prescribed. It is shown {{that all of the}} functions of interest in <b>random</b> <b>problems</b> can be derived from the transition probability. Some of the concepts of probability theory and of spectral analysis are reviewed and using these results, the Gaussian white noise function is defined. A new derivation of the Fokker-Planck equation is given which emphasizes the role of the Gaussian white input in the analysis of Markoff processes. The transition probability is the fundamental solution of this equation. It is then shown that the autocorrelation is closely related to the mean motion of a system and can be calculated from the transition probability. This relation can be used, in principle at least, to determine the autocorrelation of nonlinear systems. The Method of Equivalent Linearization for <b>random</b> <b>problems</b> and the First Passage Problem are discussed briefly. These methods are used to solve a number of problems. A discussion of linear systems is presented, and by a similar treatment the solution to a <b>problem</b> in <b>random</b> parametric excitation is given. Next, the first probability density of a class of nonlinear problems is discussed. Finally, the power spectra for two nonlinear systems are calculated. </p...|$|R
40|$|Abstract:- In this paper, we firstly {{introduce}} that CAPTCHA {{technology development}} and the various type of reading CAPTCHA, sound CAPTCHA, graph CAPTCHA at present. The second, we build one kind of the Chinese CAPTCHA model. The third, a new Chinese CAPTCHA system based on AJAX is proposed, which proceed to generate character characteristic based <b>random</b> <b>problem</b> and answer. On its performance, we show that {{the advantages of the}} Chinese CAPTCHA system due to its improved security and the reliability of the technology using it...|$|E
40|$|AbstractDrayage {{operations}} involve transporting {{loaded and}} empty containers in the hinterland of a port. In this paper a full truckload {{vehicle routing problem}} in drayage operations is studied. Either the origin or destination of empty container transport requests is unknown in advance. A two-phase solution algorithm using deterministic annealing is presented to solve the bi-objective problem, minimizing the number of vehicles used and minimizing total distance travelled. Results on <b>random</b> <b>problem</b> instances show that the algorithm is able to find sets of non-dominated solutions of good quality in {{a small amount of}} computation time...|$|E
40|$|AbstractIn this work, we {{consider}} random elliptic interface problems, namely, {{the media in}} elliptic equations have both randomness and interfaces. A Galerkin method using bi-orthogonal polynomials is used to convert the <b>random</b> <b>problem</b> into an uncoupled system of deterministic interface problems. A principle on how to choose the orders of the approximated polynomial spaces is given based on the sensitivity analysis in random spaces, with which the total degree of freedom can be significantly reduced. Then immersed finite element methods are introduced to solve the resulting system. Convergence results are given both theoretically and numerically...|$|E
40|$|We numerically study quantum {{adiabatic}} algorithm for the propositional satisfiability. A {{new class}} of previously unknown hard instances is identified among <b>random</b> <b>problems.</b> We numerically find that the running time for such instances grows exponentially with their size. Worst case complexity of quantum adiabatic algorithm therefore seems to be exponential. Comment: 7 page...|$|R
40|$|In these lectures I {{will present}} an {{introduction}} to the results that have been recently obtained in constraint optimization of <b>random</b> <b>problems</b> using statistical mechanics techniques. After presenting the general results, in order to simplify the presentation I will describe in details only the problems related to the coloring of a random graph. ...|$|R
40|$|In this paper, {{we present}} results dealing with {{properties}} of well-known geometric <b>random</b> <b>problems</b> inthe plane, {{together with their}} motivations. The paper specifically concentrates onthe traveling salesman and minimum spanning tree problems, {{even though most of}} the results apply to other problems such as the Steiner tree problem and the minimum weight matching problem...|$|R
