25|10000|Public
2500|$|A singly linked {{linear list}} is a <b>recursive</b> <b>data</b> <b>structure,</b> because it {{contains}} a pointer to a smaller object of the same type. [...] For that reason, many operations on singly linked linear lists (such as merging two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using iterative commands. [...] While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.|$|E
50|$|Due to {{the nature}} of a binary tree being a <b>recursive</b> <b>data</b> <b>structure,</b> a binary plan allows for {{multiple}} business centers for the MLM to be created, which cannot be done in the other three commonly used network marketing compensation plans, which allows for a more international group of people to potentially become members of the plan.|$|E
5000|$|Consider a <b>recursive</b> <b>data</b> <b>structure</b> like {{a binary}} tree labeled by data of type A.The {{derivative}} {{is computed by}} introducing a recursion variable and we recover the original data structure by finding the fixed point [...] The datatype of the context isBy taking the fixed point [...] we find that a zipper for a tree consists of a [...] "path" [...] and a downward subtree, where a path is a context list of triples consisting of ...|$|E
5000|$|... #Subtitle level 3: <b>Recursive</b> <b>data</b> <b>structures</b> (structural recursion) ...|$|R
40|$|Pointer {{analysis}} {{is essential for}} optimizing and parallelizing compilers. It examines pointer assignment statements and estimates pointer-induced aliases among pointer variables or possible shapes of dynamic <b>recursive</b> <b>data</b> <b>structures.</b> However, previously proposed techniques perform pointer analysis without the knowledge of traversal patterns of dynamic <b>recursive</b> <b>data</b> <b>structures</b> to be constructed. This paper presents an algorithm to identify the traversal patterns of <b>recursive</b> <b>data</b> <b>structures</b> and propagate this information back to those statements that define the <b>data</b> <b>structures.</b> This approach recognizes the DEF/USE relationships between the statements that define and traverse dynamic <b>recursive</b> <b>data</b> <b>structures.</b> The outcome of this technique will be useful for pointer analysis and parallelization. Algorithms to perform pointer analysis and dependence test using the knowledge of traversal patterns will also be presented. 1 Introduction Pointer {{analysis is}} essential for optimizing and p [...] ...|$|R
40|$|The current Modelica Standard (v 3. 3) {{does not}} support dynamic or <b>recursive</b> <b>data</b> <b>structures.</b> For many {{applications}} this constitutes a serious restriction rendering certain implementations either impossible or requires elaborate and unelegant constructs. In this paper we will show that support for dynamic and <b>recursive</b> <b>data</b> <b>structures</b> can be implemented in the Modelica IDE Dymola {{using a variety of}} advanced constructs. This proves the principle viability of the then proposed inclusion of those <b>data</b> <b>structures</b> in the Modelica Standard...|$|R
50|$|A singly linked {{linear list}} is a <b>recursive</b> <b>data</b> <b>structure,</b> because it {{contains}} a pointer to a smaller object of the same type. For that reason, many operations on singly linked linear lists (such as merging two lists, or enumerating the elements in reverse order) often have very simple recursive algorithms, much simpler than any solution using iterative commands. While those recursive solutions can be adapted for doubly linked and circularly linked lists, the procedures generally need extra arguments and more complicated base cases.|$|E
50|$|In {{functional}} programming, fold (also termed reduce, accumulate, aggregate, compress, or inject) {{refers to}} a family of higher-order functions that analyze a <b>recursive</b> <b>data</b> <b>structure</b> and through use of a given combining operation, recombine the results of recursively processing its constituent parts, building up a return value. Typically, a fold is presented with a combining function, a top node of a data structure, and possibly some default values to be used under certain conditions. The fold then proceeds to combine elements of the data structure's hierarchy, using the function in a systematic way.|$|E
40|$|An {{attribute}} grammars {{describes a}} computation over a <b>recursive</b> <b>data</b> <b>structure</b> (syntax tree). The corresponding evaluator {{can be directly}} encoded {{as a set of}} lazy evaluated functions. In this paper we show how this set may be converted into a larger set of strict functions and a collection of new data types. We call this process, which is based on a global data flow analysis, strictification. The resulting set of small functions and the new data types are amenable to further analysis and optimization. Especially the elimination transformation leads to very efficient programs, both in time and space, and which are much more suited for incremental (i. e. memoized) and parallel evaluation...|$|E
40|$|Pointer {{analysis}} {{is essential for}} optimizing and parallelizing compilers. It examines pointer assignment statements and estimates pointer-induced aliases among pointer variables or possible shapes of dynamic <b>recursive</b> <b>data</b> <b>structures.</b> However, previously proposed techniques {{are not able to}} gather useful information or have to give up further optimizations when overall <b>recursive</b> <b>data</b> <b>structures</b> appear to be cyclic even though patterns of traversal are linear. The reason is that these proposed techniques perform pointer analysis without the knowledge of traversal patterns of dynamic <b>recursive</b> <b>data</b> <b>structures</b> to be constructed. This paper proposes an approach, traversal-pattern-aware pointer analysis, that has the ability to first identify the structures specified by traversal patterns of programs from cyclic <b>data</b> <b>structures</b> and then perform analysis on the specified structures. This paper presents an algorithm to perform shape analysis on the structures specified by traversal patterns. T [...] ...|$|R
40|$|Existing {{methods for}} alias {{analysis}} of <b>recursive</b> pointer <b>data</b> <b>structures</b> {{are based on}} two approximation techniques: k-limiting, which blurs distinction between sub-objects below depth k; and store-based (or equivalently location or regionbased) approximations, which blur distinction between elements of <b>recursive</b> <b>data</b> <b>structures.</b> Although notable progress in interprocedural alias analysis has been recently accomplished, very little progress in the precision of analysis of <b>recursive</b> pointer <b>data</b> <b>structures</b> has been seen {{since the inception of}} these approximation techniques by Jones and Muchnick a decade ago. As a result, optimizing, verifying and parallelizing programs with pointers has remained difficult. We present a new parametric framework for analyzing <b>recursive</b> pointer <b>data</b> <b>structures</b> which can express a new natural class of alias information not accessib [...] ...|$|R
40|$|Most {{imperative}} languages only offer arrays as "first-class" <b>data</b> <b>structures.</b> Other <b>data</b> <b>structures,</b> especially <b>recursive</b> <b>data</b> <b>structures</b> such as trees, have to {{be manipulated}} using explicit control of memory, i. e., through pointers to explicitly allocated portions of memory. We believe that this severe limitation is mainly due to historical reasons, and this paper will try and demonstrate that modern analysis techniques, such as data flow analysis, allow {{to cope with the}} compilation problems associated with <b>recursive</b> <b>data</b> <b>structures.</b> As a matter of fact, recursion in the flow of control also is a current open issue in automatic parallelization: to our knowledge, no theory allows the parallelization of, e. g., recursive Pascal programs. This paper uniformly handles both issues. We propose a kernel language that manipulates <b>recursive</b> <b>data</b> <b>structures</b> in an elegant, algebraic way. In this preliminary work, both data- and control- recursive structures are restricted, so that a data flow a [...] ...|$|R
40|$|We {{describe}} a <b>recursive</b> <b>data</b> <b>structure</b> for the uniform handling of permutation groups and matrix groups. This data structure allows the switching between permutation and matrix representations of {{segments of the}} input group, and has wide-ranging applications. It provides a framework to process theoretical algorithms which were considered too complicated for implementation such as the asymptotically fastest algorithms for the basic handling of large-base permutation groups and for Sylow subgroup computations in arbitrary permutation groups. It also facilitates the basic handling of matrix groups. The data structure is general enough for the easy incorporation of any matrix group or permutation group algorithm code; in particular, the library functions of the GAP computer algebra system dealing with permutation groups and matrix groups work with a minimal modification...|$|E
40|$|International audienceThis {{work is a}} part of the SHIVA (Secured Hardware Immune Versatile Architecture) project {{whose purpose}} is to provide a {{programmable}} and reconfigurable hardware module with high level of security. We propose a recursive double-size fixed precision arithmetic called RecInt. Our work can be split in two parts. First we developped a C++ software library with performances comparable to GMP ones. Secondly our simple representation of the integers allows an implementation on FPGA. Our idea is to consider sizes that are a power of 2 and to apply doubling techniques to implement them efficiently: we design a <b>recursive</b> <b>data</b> <b>structure</b> where integers of size 2 ^k, for k>k 0 can be stored as two integers of size 2 ^{k- 1 }. Obviously for k<=k 0 we use machine arithmetic instead (k 0 depending on the architecture) ...|$|E
40|$|Many {{data mining}} tasks (e. g., Association Rules, Sequential Patterns) use complex pointer-based data {{structures}} (e. g., hash trees) that typically suffer from sub-optimal data locality. In the multiprocessor case shared {{access to these}} data structures may also result in false sharing. For these tasks it is commonly observed that the <b>recursive</b> <b>data</b> <b>structure</b> is built once and accessed multiple times during each iteration. Furthermore, the access patterns after the build phase are highly ordered. In such cases locality and false sharing sensitive memory placement of these structures can enhance performance significantly. We evaluate a set of placement policies for parallel association discovery, and show that simple placement schemes can improve execution time {{by more than a}} factor of two. More complex schemes yield additional gains. Introduction Improving the data locality (by keeping data local to a processor's cache) and reducing/eliminating false sharing (by reducing cache coherence [...] ...|$|E
40|$|In {{this paper}} we present {{techniques}} to detect three common patterns of parallelism in C programs that use <b>recursive</b> <b>data</b> <b>structures.</b> These patterns include, function calls that access disjoint sub-pieces of tree-like <b>data</b> <b>structures,</b> pointer-chasing loops that traverse list-like <b>data</b> <b>structures,</b> and array-based loops which operate on {{an array of}} pointers pointing to disjoint <b>data</b> <b>structures.</b> We design dependence tests using {{a family of three}} existing pointer analyses, namely points-to, connection and shape analyses, with special emphasis on shape analysis. To identify loop parallelism, we introduce special tests for detecting loop-carried dependences in the context of <b>recursive</b> <b>data</b> <b>structures.</b> We have implemented the tests in the framework of our McCAT C compiler, and we present some preliminary experimental results...|$|R
5000|$|Sawzall {{supports}} the compound data types lists, maps, and structs. However, {{there are no}} references or pointers. All assignments and function arguments create copies. This means that <b>recursive</b> <b>data</b> <b>structures</b> and cycles are impossible.|$|R
40|$|Abstract. This {{paper is}} {{concerned}} with the integration of <b>recursive</b> <b>data</b> <b>structures</b> with Presburger arithmetic. The integrated theory includes a length function on <b>data</b> <b>structures,</b> thus providing a tight coupling between the two theories, and hence the general Nelson-Oppen combination method for decision procedures is not applicable to this theory, even for the quantifier-free case. We present four decision procedures for the integrated theory depending on whether the language has infinitely many constants and whether the theory has quantifiers. Our decision procedures for quantifier-free theories are based on Oppen's algorithm for acyclic <b>recursive</b> <b>data</b> <b>structures</b> with infinite atom domain. 1 Introduction Recursively defined <b>data</b> <b>structures</b> are essential constructs in programming lan-guages. Intuitively, a <b>data</b> <b>structure</b> is recursively defined if it is partially com-posed of smaller or simpler instances of the same structure. Examples includ...|$|R
40|$|AbstractThis paper {{presents}} a novel set of algorithms for heap abstraction, identifying logically related {{regions of the}} heap. The targeted regions include objects {{that are part of}} the same component structure (<b>recursive</b> <b>data</b> <b>structure).</b> The result of the technique outlined in this paper has the form of a compact normal form (an abstract model) that boosts the efficiency of the static analysis via speeding its convergence. The result of heap abstraction, together with some properties of data structures, can be used to enable program optimizations like static deallocation, pool allocation, region-based garbage collection, and object co-location. More precisely, this paper proposes algorithms for abstracting heap components with the layout of a singly linked list, a binary tree, a cycle, and a directed acyclic graph. The termination and correctness of these algorithms are studied in the paper. Towards presenting the algorithms the paper also presents concrete and abstract models for heap representations...|$|E
40|$|Despite {{the success}} of instruction-level {{parallelism}} (ILP) optimizations in increasing the performance of microprocessors, certain codes remain elusive. In particular, codes containing <b>recursive</b> <b>data</b> <b>structure</b> (RDS) traversal loops have been largely immune to ILP optimizations, due to the fundamental serialization and variable latency of the loop-carried dependence through a pointer-chasing load. To address these and other situations, we introduce decoupled software pipelining (DSWP), a technique that statically splits a single-threaded sequential loop into multiple non-speculative threads, each of which performs useful computation essential for overall program correctness. The resulting threads execute on thread-parallel architectures such as simultaneous multithreaded (SMT) cores or chip multiprocessors (CMP), expose additional instruction level parallelism, and tolerate latency better than the original single-threaded RDS loop. To reduce overhead, these threads communicate using a synchronization array, a dedicated hardware structure for pipelined inter-thread communication. DSWP {{used in conjunction with}} the synchronization array achieves an 11 % to 76 % speedup in the optimized functions on both statically and dynamically scheduled processors. 1...|$|E
40|$|This paper {{presents}} a novel set of algorithms for heap abstraction, identifying logically related {{regions of the}} heap. The targeted regions include objects {{that are part of}} the same component structure (<b>recursive</b> <b>data</b> <b>structure).</b> The result of the technique outlined in this paper has the form of a compact normal form (an abstract model) that boosts the efficiency of the static analysis via speeding its convergence. The result of heap abstraction, together with some properties of data structures, can be used to enable program optimizations like static deallocation, pool allocation, region-based garbage collection, and object co-location. More precisely, this paper proposes algorithms for abstracting heap components with the layout of a singly linked list, a binary tree, a cycle, and a directed acyclic graph. The termination and correctness of these algorithms are studied in the paper. Towards presenting the algorithms the paper also presents concrete and abstract models for heap representations. Comment: 15 page...|$|E
50|$|An {{important}} {{application of}} recursion {{in computer science}} is in defining dynamic <b>data</b> <b>structures</b> such as Lists and Trees. <b>Recursive</b> <b>data</b> <b>structures</b> can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, a static array's size requirements must be set at compile time.|$|R
40|$|If a rewrite-based {{inference}} {{system is}} guaranteed to terminate on the axioms of a theory T and any set of ground literals, then any theorem-proving strategy based on that inference system is a rewrite-based decision procedure for T-satisfiability. In this paper, we consider the class of theories defining <b>recursive</b> <b>data</b> <b>structures,</b> that might appear {{out of reach for}} this approach, because they are defined by an infinite set of axioms. We overcome this obstacle by designing a problem reduction that allows us to prove a general termination result for all these theories. We also show that the theorem-proving strategy decides satisfiability problems in any combination of these theories with other theories decided by the rewrite-based approach. Keywords: Rewrite-based inference systems, <b>recursive</b> <b>data</b> <b>structures...</b>|$|R
40|$|The {{impact of}} {{declarative}} (functional) programming is inhibited {{by the need}} to learn/use new languages. A series of language extensions implemented by preprocessing, which integrates functional programming into the Ada culture, is described. Features include lazy streams, <b>recursive</b> <b>data</b> <b>structures,</b> the abolition of assignment and its replacement by data-flow control constructs...|$|R
40|$|This paper {{presents}} {{techniques to}} model circular lazy programs in a strict, purely functional setting. Circular lazy programs model any algorithm based on multiple traversals over a <b>recursive</b> <b>data</b> <b>structure</b> {{as a single}} traversal function. Such elegant and concise circular programs are defined in a (strict or lazy) functional language and they are transformed into efficient strict and deforested, multiple traversal programs by using attribute grammars-based techniques. Moreover, we use standard slicing techniques to slice such circular lazy programs. We have expressed these transformations as an Haskell library and two tools have been constructed: the HaCirc tool that refactors Haskell lazy circular programs into strict ones, and the OCirc tool that extends Ocaml with circular definitions allowing programmers to write circular programs in Ocaml notation, which are transformed into strict Ocaml programs before they are executed. The first benchmarks of the different implementations are presented and show that for algorithms relying on {{a large number of}} traversals the resulting strict, deforested programs are more efficient than the lazy ones, both in terms of runtime and memory consumption...|$|E
40|$|Recursive {{blocked data}} formats and {{recursive}} blocked BLAS's are introduced {{and applied to}} dense linear algebra algorithms that are typified by LAPACK. The new data formats allow for maintaining data locality {{at every level of}} the memory hierarchy and hence providing high performance on today's memory tiered processors. This new data format is hybrid. It contains blocking parameter which are chosen so that associated submatrices of a block-partitioned A fit into level 1 cache. The recursive part of the data format chooses a linear order of the blocks that maintains a two-dimensional data locality of A in one-dimensional tired memory structure. We argue that, out of the NB factorial choices of ordering the NB blocks, our recursive ordering leads to one of the best. This is because our algorithms are also recursive and will do their computations on submatrices that follow the new <b>recursive</b> <b>data</b> <b>structure</b> definition. This is in analogy with the well known principle that the data structure should be matched to the algorithm. Performance results in support for our recursive approach are also presented...|$|E
40|$|Memoization is a {{familiar}} technique for improving the performance of programs: computed answers are saved {{so that they can}} be reused later instead of being recomputed. In a pure functional language, memoization of a function is complicated by the need to manage the table of saved answers between calls to the function, including recursive calls within the function itself. A lazy <b>recursive</b> <b>data</b> <b>structure</b> can be used to maintain past answers — although achieving an efficient algorithm can require a complex rewrite of the function into a special form. Memoization can also be defined as a language primitive — but to be useful it would need to support a range of memoization strategies. In this paper we develop a technique for modular memoization within a pure functional language. We define monadic memoization mixins that are composed (via inheritance) with an ordinary monadic function to create a memoized version of the function. As a case study, we memoize a recursive-descent parser written using standard parser combinators. A comparison of the performance of different approaches shows that memoization mixins are efficient for a small example. 1...|$|E
5000|$|The latest version, MTL4, is {{developed}} by Peter Gottschling and Andrew Lumsdaine. It contains most of MTL2's functionality and adds new optimization techniques as meta-tuning, e.g. loop unrolling of dynamically sized containers {{can be specified}} in the function call. Platform-independent performance scalability is reached by <b>recursive</b> <b>data</b> <b>structures</b> and algorithms.|$|R
40|$|Data-flow {{analysis}} {{is extremely important}} to detect parallelism. Therefore, the current lack of suitable analysis makes recursion in the control flow still an open problem in automatic parallelization. This paper presents a novel data-flow analysis for imperative recursive programs. The data-flow sources are represented by closed forms expressions, parametrized with an index on the current node in the call tree. The model of our {{analysis is}} based on regular expressions and integer linear programming. We also discuss the implemented software, which is freely distributed. This paper also addresses the dual problem: data-flow analysis of <b>recursive</b> <b>data</b> <b>structures.</b> Moreover, this analysis not only suits automatic parallelization, but has several applications in program checking, optimization of memory usage, etc. Keywords: data-flow analysis, automatic parallelization, compilation, recursion, <b>recursive</b> <b>data</b> <b>structures.</b> 1 Introduction Automatic parallelization has been focusing on nests of [...] ...|$|R
5000|$|AP Computer Science AB {{included}} all {{the topics of}} AP Computer Science A, {{as well as a}} more formal and a more in-depth study of algorithms, <b>data</b> <b>structures,</b> and <b>data</b> abstraction. For example, binary trees were studied in AP Computer Science AB but not in AP Computer Science A. The use of <b>recursive</b> <b>data</b> <b>structures</b> and dynamically allocated structures were fundamental to AP Computer Science AB.|$|R
40|$|Abstract. In {{this paper}} {{we present a}} new {{parallel}} algorithm for data mining of association rules on shared-memory multiprocessors. We study the degree of parallelism, synchronization, and data locality issues, and present optimizations for fast frequency computation. Experiments show that a significant improvement of performance is achieved using our proposed optimizations. We also achieved good speed-up for the parallel algorithm. A lot of data-mining tasks (e. g. association rules, sequential patterns) use complex pointer-based data structures (e. g. hash trees) that typically suffer from suboptimal data locality. In the multiprocessor case shared access to these data structures may also result in false sharing. For these tasks it is commonly observed that the <b>recursive</b> <b>data</b> <b>structure</b> is built once and accessed multiple times during each iteration. Furthermore, the access patterns after the build phase are highly ordered. In such cases locality and false sharing sensitive memory placement of these structures can enhance performance significantly. We evaluate a set of placement policies for parallel association discovery, and show that simple placement schemes can improve execution time {{by more than a}} factor of two. More complex schemes yield additional gains...|$|E
40|$|Recently, the speed-gap between {{processor}} and DRAM has widened. A cache-miss {{can cause a}} stall time of hundreds of processor cycles; so that stall times caused by cache-misses now constitute {{one of the most}} serious bottlenecks in modern computer systems. Therefore it has become imperative to reduce cache misses for speeding up computer systems. Cache misses are mainly caused by insufficient capacity, and can be classified into two classes: (1) cache-misses that occur even when the replacement decisions are correct and (2) cache-misses caused by the wrong replacement decisions. The purpose of this thesis is to propose methods to reduce cache-misses of both types (1) and (2). Two methods that aim to reduce each class of cache-misses are proposed. Type (1) cache misses are reduced by exploiting the redundancy and temporal affinity of data. Two data items are said to have temporal affinity when they are referenced within a short period of time. A static compiler analysis can detect and exploit this affinity in numeric programs. On the other hand, it is difficult for compilers to detect temporal affinity in programs using a <b>recursive</b> <b>data</b> <b>structure,</b> which is used widely in non-numeric programs. We propose a software and hardware cooperativ...|$|E
40|$|As the processor-memory {{performance}} gap increases, {{so does the}} need for aggressive data structure optimizations to reduce memory access latencies. Such optimizations require {{a better understanding of}} the memory behavior of programs. We propose a profiling technique called <b>Recursive</b> <b>Data</b> <b>Structure</b> Profiling to help better understand the memory access behavior of programs that use recursive data structures (RDS) such as lists, trees, etc. An RDS profile captures the runtime behavior of the individual instances of recursive data structures. RDS profiling differs from other memory profiling techniques in its ability to aggregate information pertaining to an entire data structure instance, rather than merely capturing the behavior of individual loads and stores, thereby giving a more global view of a program’s memory accesses. This paper describes a method for collecting RDS profile without requiring any high-level program representation or type information. RDS profiling achieves this with manageable space and time overhead on a mixture of pointer intensive benchmarks from the SPEC, Olden and other benchmark suites. To illustrate the potential of the RDS profile in providing a better understanding of memory accesses, we introduce a metric to quantify the notion of stability of an RDS instance. A stable RDS instance is one that undergoes very few changes to its structure between its initial creation and final destruction, making it an attractive candidate to certain data structure optimizations...|$|E
40|$|AbstractIn {{this paper}} {{we use a}} program logic and {{automatic}} theorem provers to certify resource usage of low-level bytecode programs equipped with annotations describing resource consumption for methods. We have adapted an existing resource counting logic [Aspinall, D., L. Beringer, M. Hofmann, H. -W. Loidl and A. Momigliano, A program logic for resource verification, in: Proceedings of 17 th International Conference on Theorem Proving in Higher Order Logics (TPHOLs 2004), Lecture Notes in Computer Science 3223 (2004), pp. 34 – 49] to fit the first-order setting, implemented a verification condition generator, and tested our approach on programs that contain recursion and deal with <b>recursive</b> <b>data</b> <b>structures.</b> We have successfully applied our framework to programs that did not involve any updates to <b>recursive</b> <b>data</b> <b>structures.</b> But mutation is more tricky because of aliasing of heap. We discuss problems related to this and suggest techniques to solve them...|$|R
40|$|Static Analysis of {{programs}} is indispensable to any software tool, environment, or system that requires compile time {{information about the}} semantics {{of programs}}. With the emergence of languages like C and LISP, Static Analysis of programs with dynamic storage and <b>recursive</b> <b>data</b> <b>structures</b> has become a field of active research. Such analysis is difficult, and the Static Analysis community has recognized the need for simplifying assumptions and approximate solutions. However, even under the common simplifying assumptions, such analyses are harder than previously recognized. Two fundamental Static Analysis problems are May Alias and Must Alias. The former is not recursive (i. e., is undecidable) and the latter is not recursively enumerable (i. e., is uncomputable), even when all paths are executable in the program being analyzed for languages with if-statements, loops, dynamic storage, and <b>recursive</b> <b>data</b> <b>structures.</b> Categories and Subject Descriptors: D. 3. 1 [Programming Languages [...] ...|$|R
40|$|Recently, the {{necessity}} for parallel programming has been increased with the rapid spread of multicore/multiprocessor systems. However, {{it is difficult for}} a programmer to create a highly-effective and high-performance parallel program. So, we are developing the automatic translator from C programs to parallel programs using MPI(Message Passing Interface). In our conventional automatic parallelism analysis of C program with pointer variables, it was able to analyze the data dependencies of only the pointer variables declared explicitly in the code. In this research, we have first applied the Shape analysis to C programs with pointer variables for getting to know the form of the <b>recursive</b> <b>data</b> <b>structures</b> which will be allocated and constructed dynamically at the time of execution. Then, using the result of the analysis, we can analyze the <b>data</b> dependencies of <b>recursive</b> <b>data</b> <b>structures,</b> such as linked list or binary tree structures, and detect more parallelism from C program...|$|R
