20|12|Public
5000|$|The first Windows {{version was}} labeled [...] "2" [...] to {{correspond}} to the Mac version. This included a <b>run-time</b> <b>version</b> of Windows.|$|E
50|$|GNU {{lightning}} is a {{free software}} library for generating assembly language code at <b>run-time.</b> <b>Version</b> 2.0, released in August 2013, supports backends for SPARC (32-bit), x86 (32- and 64-bit), MIPS, ARM, ia64, HPPA and PowerPC (32-bit).|$|E
50|$|This was {{the first}} version of DeskMate that allowed for a <b>run-time</b> <b>version</b> that could be {{distributed}} with applications. This allowed users to use DeskMate applications on their PC's even if they did not have DeskMate installed.|$|E
5000|$|RCS Library Version Functions : Documentation {{for some}} {{functions}} for determining at <b>run-time</b> which <b>version</b> of the RCS library your using.|$|R
40|$|Intrusion {{detection}} systems need {{to maximize}} security while minimizing costs. In this paper, we study {{the problem of}} building cost-sensitive intrusion detection models. We examine the major cost factors: development costs, operational costs, damage costs incurred due to intrusions, and the costs involved in responding to intrusions. We propose cost-sensitive machine learning techniques to produce models that are optimized for user-defined cost metrics. We describe an automated approach for generating efficient <b>run-time</b> <b>versions</b> of these models. Empirical experiments in off-line analysis and real-time detection show that our cost-sensitive modeling and deployment techniques are effective in reducing the overall cost of intrusion detection. ...|$|R
40|$|The limited {{ability of}} compilers to nd the {{parallelism}} in programs is a signi cant {{barrier to the}} use of high performance computers. It forces programmers to resort to parallelizing their programs by hand, adding another level of complexity to the programming task. We show evidence that compilers can be improved, through static and run-time techniques, to the extent that a signi cant group of scienti c programs may be parallelized automatically. Symbolic dependence analysis and array privatization, plus <b>run-time</b> <b>versions</b> of those techniques are shown to be important to the success of this e ort. If we can succeed to parallelize programs automatically, the acceptance and use of large-scale parallel processors will be enhanced greatly...|$|R
50|$|Windows 3.0 was not {{available}} as a <b>run-time</b> <b>version,</b> {{as was the case}} with its predecessors. A limited-use version of Windows 2.x was often bundled with other applications (e.g., Ami Pro) due to the low market penetration of Windows.|$|E
50|$|Aldus Pagemaker 3 for Macintosh {{was shipped}} in April 1988. PageMaker 3.0 for the PC was shipped in May 1988 and {{required}} Windows 2.0, which was bundled as a <b>run-time</b> <b>version.</b> Version 3.01 {{was available for}} OS/2 and took extensive advantage of multithreading for improved user responsiveness.|$|E
5000|$|Program {{documentation}} dated June 15, 1990 {{included a}} <b>run-time</b> <b>version</b> of Microsoft Windows 2.0 which {{would support a}} single application. This was called [...] "Windows Single Application Environment," [...] (SAE). The minimum hardware a user needed was an 80286-based system with 640KB of RAM, running DOS 3.3 or newer, and having an EGA color display.|$|E
40|$|Abstract. CSP++ is an {{open-source}} code synthesis tool {{consisting of}} a translator for a subset of CSPm and a C++ <b>run-time</b> framework. <b>Version</b> 5. 0 now supports Timed CSP operators—timed interrupt, timed timeout, and timed prefix—as well as untimed variants of interrupt and timeout, with only 1 % additional execution and memory overhead, though using interrupts is more costly. We describe the implementation and performance of the new operators, illustrating their use with a robot-vacuum cleaner case study. The tool thus becomes more useful for specifying the behaviour of soft real-time systems, and generating a timing-enabled executable program from its formal model...|$|R
40|$|International audienceWe {{describe}} a middleware solution for automatic <b>run-time</b> process <b>versioning</b> in Business Process Execution Language (BPEL) and then analyse its impact {{in terms of}} scalability and performance. Business processes change in response to business needs, but the deployment of new versions to a BPEL engine must ensure that running instances are not disrupted and can conclude following their original workflows. Our solution is implemented as a standalone component that manages versioning transparently to the process editor, the orchestration engine, the web services used by the process, and the end-user. We have tested it for almost 1 year in the production environment of a telecommunications company, without significant overhead in terms of process invocation time...|$|R
40|$|Dynamic {{evolution}} and adaptation of workflow models due to process (re) engineering activities and dynamic changing situations {{of the real}} process {{is one of the}} most important challenges in workflow management. In this paper, we present an approach for the management of evolving workflow specifications which copes with the evolution of a workflow schema and the dynamic modification of workflow instances. The approach is based on the integrated mod- eling of workflow schema and instance elements, the separated definition of `what to do' and `how to do' in the workflow schema in conjunction with late binding of a workflow at <b>run-time,</b> the <b>versioning</b> of the workflow schema, and capabilities for defining complex workflow migration rules by adopting graph replacement rules. On this basis, we support different propagation /migration strategies as well as local adjustment of instances and their upward propagation. Furthermore, we address the problem of managing consistent configurations of [...] ...|$|R
50|$|Excel 2.0 for Windows, {{which was}} modeled after its Mac GUI-based counterpart, {{indirectly}} expanded the installed {{base of the}} then-nascent Windows environment. Excel 2.0 was released a month before Windows 2.0, and the installed base of Windows was so low {{at that point in}} 1987 that Microsoft had to bundle a <b>run-time</b> <b>version</b> of Windows 1.0 with Excel 2.0. Unlike Microsoft Word, there never was a DOS version of Excel.|$|E
50|$|Automise is a {{commercial}} task automation tool for Microsoft Windows. Developed by VSoft Technologies, Automise offers a {{graphical user interface}} for automating repetitive tasks {{through the creation of}} Automise projects. Automise includes a library of 390 built in actions which can be combined with point-and-click to develop automation projects. Automise automation projects can be run using the Automise IDE, Automise command line executable or the free <b>run-time</b> <b>version</b> called Automise Runner.|$|E
5000|$|PC Magazine {{wrote that}} CP/M-86 [...] "in several ways seems better fitted to the PC" [...] than DOS; however, {{for those who}} did not plan to program in {{assembly}} language, because it cost six times more [...] "CP/M seems a less compelling purchase". It stated that CP/M-86 was strong in areas where DOS was weak, and vice versa, and that the level of application support for each operating system would be most important, although CP/M-86's lack of a <b>run-time</b> <b>version</b> for applications was a weakness.|$|E
40|$|This paper instead {{concentrates}} on structural and conceptual aspects of such systems. Based on an actual implementation, we present {{the architecture of}} a system that provides continuous application profiling, continuous background reoptimization guided by the collected profiling information, and continuous replacement of already running application software by re-optimized versions of the same software. A central trait of our architecture is extensibility. The dynamic optimizer at the heart of our system has a component structure supporting incremental modification in a plug-and-play manner. This is an essential facility for making system-level code generation useful in practice. Without this capability, the "outdated software" problem would merely be shifted downward to the system level: system software manufacturers would be just as disinclined to provide multiple <b>run-time</b> system <b>versions</b> as application software manufacturers are unwilling to provide multiple versions of their products. Among the conceptual issues discussed in the paper are the questions of when to trigger re-optimizations, and which parts of the running software to re-optimize. Categories and Subject Descriptors: D. 3. 4 [Programming Languages]: Processors [...] -Run-tim...|$|R
40|$|We have {{developed}} a facility for run-time optimization of a commodity operating system kernel. This facility {{is a first step}} towards an evolving operating system, one that adapts and changes over time without need for rebooting. Our infrastructure, currently implemented on UltraSPARC Solaris 7, includes the ability to do a detailed analysis of the running kernel's binary code, dynamically insert and remove code patches, and dynamically install new versions of kernel functions. As a first use of this technology, we have implemented a <b>run-time</b> kernel <b>version</b> of the code positioning I-cache optimizations, and obtained noticeable speedups in kernel performance. As a first case study, we performed run-time code positioning on the kernel's TCP read-side processing routine while running a Web client benchmark. We found that the code positioning optimizations reduced this function's execution time by 17. 6 %, resulting in an end-to-end benchmark speedup of 7 %. The primary contributions of this paper are the first run-time kernel implementation of code positioning, and an infrastructure for turning an unmodified commodity kernel into an evolving one. Two further contributions are made in kernel performance measurement. First, we provide a simple and effective algorithm for deriving control flow edge execution counts from basic block execution counts, which contradicts the widely held belief that edge counts cannot be derived from block counts. Second, we describe a means for converting wall time instrumentation-based kernel measurements into virtual (i. e., CPU) time measurements via instrumentation of the kernel's context switch handlers. ...|$|R
40|$|Dynamic {{evolution}} of workflow models due to process (re) engineering activities and dynamic changing situations {{of the real}} process {{is one of the}} most important challenges in workflow management. In this paper, we present an approach for the management of evolving workflow specifications which copes with the {{evolution of}} a workflow schema and the dynamic modification of workflow instances. The approach is based on the integrated modeling of workflow schema and instance ele- ments, the separated definition of `what to do' and `how to do' in the workflow schema, late binding of workflows at <b>run-time,</b> and the <b>versioning</b> of the workflow schema. On this basis, we support lazy, eager, and selective propagation as well as local customization of instances and their upward propagation. Furthermore, we address the problem of managing consistent configurations of the versioned entities of a workflow schema. In our workflow-specific versioning approach, the consistency of the workflow configuratio [...] ...|$|R
40|$|Most {{optimisation}} {{techniques for}} theorem provers for first-order logic rely on static {{analysis of the}} problem statement. For intensional logics, such as static analysis cannot be relied on, since {{it is impossible to}} predict what literals may be introduced by the intensional rules. The current paper shows how to use a dynamic (<b>run-time)</b> <b>version</b> of one well-known static optimisation, and considers its relationship to the use of `relevance checking' in Satchmo. 1 A constructive intensional logic We have shown elsewhere [8, 3] how to extend [6]'s theorem prover Satchmo to cope with [9]'s property theory. Property theory is a highly intensional logic which, roughly speaking, allows you to perform unconstrained quantification over propositions and properties, but places constraints on the conditions under which the Tarski biconditional (xP) :t $ P t=x holds. This language has numerous potential applications: I use it primarily for reasoning about natural language semantics, becaus [...] ...|$|E
40|$|In this paper, a {{lightweight}} hybrid fault tolerant approach for AES, {{which is based}} on the integration of the algorithm based fault tolerant (ABFT) technique and the fault tolerant technique for s-box byte substitution operation is proposed. Two versions of scheme are presented to satisfy different application requirements. The first general version scheme can detect single error for the whole AES process with high efficiency. Another <b>run-time</b> <b>version</b> scheme is used to immediately terminate the error round with no time delay and no computation wasted on the rest rounds for propagating errors. Utilizing the ready-made arithmetic units in AES, single error can be detected by the sender and prevent the misdirected information from sending out. The results of the hardware FPGA implementation and simulation show that the proposed scheme can be integrated both on software and hardware without making many changes to the original AES implementation...|$|E
40|$|Proof Animation is {{a family}} of {{products}} for adding animation to discrete event simulations. Proof is available {{in a variety of}} versions, including an inexpensive, student version, mid-size and unlimited-size commercial versions, a <b>run-time</b> <b>version,</b> and a royalty-free, redistributable demo viewer. Proof is an ASCII-stream-driven, general-purpose animation system which runs on readily available PC hardware. Its vector-based geometry provides a large animation canvas and the ability to zoom in or out, while maintaining crisp, clear images. Proof includes built-in drawing tools and CAD import/export for ease in creating animation layouts. Proofs open architecture makes it ideally suited for serving as a concurrent or post-processed animation engine for models written {{in a wide variety of}} simulation and programming languages. Proofs superior power and performance assure smooth, realistic motion for animations, regardless of their size, complexity, or application. Proof uses Microsofts DirectDraw interface for accessing video hardware. DirectDraw is a built-in component of Windows 98, NT 4, and 2000, and it is available as an add-on for Windows 95. Proof is able to exploit high-performance MMX hardware...|$|E
40|$|Through dynamic linking, Java {{supports}} a novel paradigm for code deployment, which ensures fast program start-up and linking {{with the most}} recent version of code. Thus Java dynamic linking, gives support for software evolution, by supporting a piece of code A which uses a piece of code B, to link at <b>run-time</b> with a <b>version</b> of code B which was created after A was created. Dynamic linking involves loading, verification, resolution and preparation of code. Programmers are normally {{not aware of the}} dynamic linking process. Nevertheless, in some situations, dynamic linking does manifest itself, and affects program execution and the integrity of the virtual machine. Therefore, {{there is a need for}} a description of dynamic linking at the level of the Java source language. We provide such a description, and demonstrate the process in terms of a sequence of source language examples, in which the effects if dynamic linking are explicit. 1...|$|R
40|$|In {{this report}} we present the Pamela <b>Run-Time</b> Library <b>version</b> 1. 3. With this version the RTL has become machine-independent, thus making it {{available}} to the public-domain. We have also measured {{the performance of the}} RTL and tested it in a case-study on the differences between preemptive and non-preemptive scheduling. Contents 1 Introduction 4 2 The portable libraries 6 2. 1 Implementing multitasking in C : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 2. 2 Porting the thread-library : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 9 3 Pamela RTL 1. 3 11 3. 1 The pam stop function : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11 3. 2 Shared resources with pam use : : : : : : : : : : : : : : : : : : : : : : : : : : : : 11 3. 2. 1 Introduction to shared resources : : : : : : : : : : : : : : : : : : : : : : : 12 3. 2. 2 Implementation of pam use : : : : : : : : : : : : : : : : : : : : : : : : : : 14 3. 2. 3 Benchmarking pam use : : : : : : : : : : : : : : [...] ...|$|R
40|$|The {{paper is}} dealing with parallelized {{versions}} of simulated annealing-based heuristics for the classical job shop scheduling problem. The scheduling problem {{is represented by the}} disjunctive graph model and the objective is to minimize the length of longest paths. The problem is formulated for l jobs where each job has to process exactly one task on each of the m machines. The calculation of longest paths is the critical computation step of our heuristics and we utilize a parallel algorithm for this particular problem where we take into account the specific properties of job shop scheduling. In our heuristics, we employ a neighborhood relation which was introduced by Van Laarhoven et al. (Operations Research 40 (1) (1992) 113 – 25). To obtain a neighbor, a single arc from a longest path is reversed and these transition steps always guarantee the feasibility of schedules. We designed two cooling schedules for homogeneous Markov chains and additionally we investigated a logarithmic cooling schedule for inhomogeneous Markov chains. Given O(n 3) processors and a known upper bound Λ=Λ(l,m) for the length of longest paths, the expected <b>run-times</b> of parallelized <b>versions</b> are View the MathML source for the first cooling schedule and View the MathML source for the second cooling schedule, where n=lm is the number of tasks. For the logarithmic cooling schedule, a speed-up of View the MathML source can be achieved. When Markov chains of constant length are assumed, we obtain a polylogarithmic run-time of View the MathML source for the first cooling schedule. The analysis of famous benchmark problems led us to the conjecture that Λ⩽O(l+m) could be a uniform upper bound for the completion time of job shop scheduling problems with l jobs on m machines. Although the number of processors is very large, the particular processors are extremely simple and the parallel processing system is suitable for hardware implementations...|$|R
40|$|GENERAL-PURPOSE CONCURRENT AND POST-PROCESSED ANIMATION WITH PROOF™ Proof Animation ™ is {{a family}} of {{products}} for animating discrete event simulations. Proof is available {{in a variety of}} versions, including an inexpensive, student version, midsize and unlimited-size commercial versions, a <b>run-time</b> <b>version,</b> and a royalty-free, redistributable demo viewer. Proof is an ASCII-stream-driven, general-purpose animation system which runs on readily available PC hardware. Its vector-based geometry provides a large animation canvas and the ability to zoom in or out, while maintaining crisp, clear images. Proof includes built-in drawing tools and CAD import/export for ease in creating animation layouts. Proof’s open architecture makes it ideally suited for serving as a concurrent or post-processed animation engine for models written {{in a wide variety of}} simulation and programming languages. Proof’s superior power and performance assure smooth, realistic motion for animations, regardless of their size, complexity, or application. Proof uses Microsoft’s DirectDraw ™ interface for accessing video hardware. DirectDraw is a built-in component of Windows 98 and Windows 2000, and it is available as an add-on for Windows 95. Proof is able to exploit high-performance MMX hardware. ...|$|E
40|$|Workflows {{specify a}} {{collection}} of tasks that must be executed under the responsibility or supervision of human users. Workflow management systems and workflow-driven applications need to enforce security policies {{in the form of}} access control, specifying which users can execute which tasks, and authorization constraints, such as Separation/Binding of Duty, further restricting the execution of tasks at run-time. Enforcing these policies is crucial to avoid frauds and malicious use, but it may lead to situations where a workflow instance cannot be completed without the violation of the policy. The Workflow Satisfiability Problem (WSP) asks whether there exists an assignment of users to tasks in a workflow such that every task is executed and the policy is not violated. The <b>run-time</b> <b>version</b> of this problem amounts to answering user requests to execute tasks positively if the policy is respected and the workflow instance is guaranteed to terminate. The WSP is inherently hard, but solutions to this problem have a practical application in reconciling business compliance (stating that workflow instances should follow the specified policies) and business continuity (stating that workflow instances should be deadlock-free). Related problems, such as finding execution scenarios that not only satisfy a workflow but also satisfy other properties (e. g., that a workflow instance is still satisfiable {{even in the absence of}} users), can be solved at deployment-time to help users design policies and reuse available workflow models. The main contributions of this thesis are three: 1. We present a technique to synthesize monitors capable of solving the <b>run-time</b> <b>version</b> of the WSP, i. e., capable of answering user requests to execute tasks in such a way that the policy is not violated and the workflow instance is guaranteed to terminate. The technique is extended to modular workflow specifications, using components and gluing assertions. This allows us to compose synthesized monitors, reuse workflow models, and synthesize monitors for large models. 2. We introduce and present techniques to solve a new class of problems called Scenario Finding Problems, i. e., finding execution scenarios that satisfy properties of interest to users. Solutions to these problems can assist customers during the deployment of reusable workflow models with custom authorization policies. 3. We implement the proposed techniques in two tools. Cerberus integrates monitor synthesis, scenario finding, and run-time enforcement into workflow management systems. Aegis recovers workflow models from web applications using process mining, synthesizes monitors, and invokes them at run-time by using a reverse proxy. An extensive experimental evaluation shows the practical applicability of the proposed approaches on realistic and synthetic (for scalability) problem instances...|$|E
40|$|The {{purpose of}} this paper is to {{demonstrate}} the ability of AutoMOD as a flexible modeling tool. Simulation models of material handling systems like those found in automotive paint shops & assembly systems often require many variables, conveyor speeds, and complex logic. These systems have many different variables associated with them. The most important ones are: conveyor speeds, carrier counts, routing information, floating carrier numbers between two control points, active and inactive station information, station operation times, and shift schedules. In order for a simulation analyst to control or experiment with a material handling system, he or she will usually have to vary one or more of these variables. In the past, this has meant that these “hard-coded ” variables would have to be changed one by one through menus for each material handling segment or system. Other methods including the use of AutoStat or AWK could be used to change certain parameters without having to visit the edit menus in AutoMOD. This paper will address how users can use external data files to control these traditionally “hard-coded ” variables, thereby allowing simulation analysts and their customers to drive their material handling systems as they have traditionally used external files to provide process information for their models. PMC’s approach was to create a flexible and robust model that imports all of these system parameters from external data files. This allows for an increase in model efficiency if model experimentation is large and involves many different physical changes. Using this method, a potential customer can modify these parameters for experimentation with a <b>run-time</b> <b>version</b> of AutoMOD simulation software. 1...|$|E
40|$|Subject of Research. The paper {{deals with}} {{development}} outcomes for creation method of one-electron wave functions of complex atoms, relatively simple, symmetrical for all atom electrons and free from hard computations. The accuracy and resource {{intensity of the}} approach are focused on systematic calculations of cross sections and rate constants of elementary processes of inelastic collisions of atoms or molecules with electrons (ionization, excitation, excitation transfer, and others). Method. The method {{is based on a}} set of two iterative processes. At the first iteration step the Schrödinger equation was solved numerically for the radial parts of the electron wave functions in the potential of the atomic core self-consistent field. At the second iteration step the new approximationfor the atomic core field is created that uses found solutions for all one-electron wave functions. The solution optimization for described multiparameter problem is achieved by the use of genetic algorithm. The suitability of the developed method was verified by comparing the calculation results with numerous data on the energies of atoms in the ground and excited states. Main Results. We have created the <b>run-time</b> <b>version</b> of the program for creation of sets of one-electron wave functions and calculation of the cross sections and constants of collisional transition rates in the first Born approximation. The priori available information about binding energies of the electrons for any many-particle system for creation of semi-empirical refined solutions for the one-electron wave functions can be considered at any step of this procedure. Practical Relevance. The proposed solution enables a simple and rapid preparation of input data for the numerical simulation of nonlocal gas discharge plasma. The approach is focused on the calculation of discharges in complex gas mixtures requiring inclusion in the model {{of a large number of}} elementary collisional and radiation processes involving heavy particles in different quantum states...|$|E
40|$|Currently, digital soils {{information}} {{can be obtained from}} many sources. Numerous websites serve digital soils information but often users need their own GIS software and considerable knowledge to view the data. A planned IMS Web Soil Survey site will eliminate the need for GIS software, but will rely on a fairly fast internet connection. The Indiana Natural Resources Conservation Service (NRCS) office has published more than 200 project areas on CD-ROM utilizing a customized <b>run-time</b> <b>version</b> of ArcView accessible to novice computer users and most available computers. Using a customized ArcView application on CD-ROM, the user can access information easily through many unique tools, without needing web access. Future plans for this project include migrating to an ArcGIS format. SoilView History The first effort to provide tools making ESRI-format Soil Survey Geographic Database (SSURGO) data easier to use resulted from discussions with the Illinois National Cooperative Soil Survey (NCSS) staff with input from soil survey users. Their vision was to make this information more easily accessible to the general public. An agreement was made between ESRI and the Illinois Soil Conservation Service (now NRCS) to produce a viewer of soils information that could be distributed freely to the general public. In the mid 1990 s, the Illinois Soil Conservation Service GIS Specialist worked with ESRI programmers to create a viewer using ArcView Data Publishing (AVDP) that would allow users to view digital soils information over an orthophotoquad image base without requiring the user to obtain proprietary GIS software. The name “SoilView ” was coined, and the first county publication using AVDP was distributed on CD-ROM in 1999. The first viewer used a runtime version of ArcView that could be operated independently and distributed freely on a CD-ROM. Initial versions were designed to be installed on the user’s computer hard drive while later versions ran directly from the CD-ROM. Modern versions of SoilView can be run from th...|$|E
40|$|A Driver's Warning Assistant(DWA) {{differs from}} an {{automatic}} co-pilot by only presenting warning {{messages to the}} driver,instead of acting upon the vehicle or the environment. It {{is an open question}} whether combinations of both types of assistance system are feasible with automatic intervention if the driver has been warned without success, or if there is no chance for a warninq message leading to an appropriate action by the driver in due time. In this case difficult questions of task allocation and of responsibility have to be answered. The evaluation has begun and will continue with DWAs dedicated to special driving tasks, e. g. parking, lane keeping, overtaking, crossing. These DWAs have a competence limited to one of the driver's tasks, so that the driver can only expect support for this single task. In a car equipped with such a specialized DWA the driver has to learn the limits of the DWA's competence. Additionally, the case has to be considered in which a specialized DWA has limit ed competence for its task. This means that the support of a driver by the DWA is limited to a subset of all possible situations (e. g. to situations with good sight conditions). Under certain circumstances (e. g. hidden obstacles) the driver might not be supported. It has to be clarified whether this can be learned and will be accepted by the driver. A crucial topic {{in the development of a}} DWA is the timing of warning messages for the driver. The moment of warning on the one hand has to be chosen early enough to enable the driver to react appropriately. This means that the inevitable reaction time, which the driver needs to receive and process the DWA's message and to prepare a reaction, has to be considered. On the other hand the warning message should not be presented too early as this would annoy the driver and result in poor acceptance of the DWA. Therefore, it is important to integrate a time-budget analysis in the development phase and in the <b>run-time</b> <b>version</b> of a DWA, leading t...|$|E
40|$|PC-SEAPAK is a user-interactive {{satellite}} data analysis software package specifically developed for oceanographic research. The program {{is used to}} process and interpret data obtained from the Nimbus- 7 /Coastal Zone Color Scanner (CZCS), and the NOAA Advanced Very High Resolution Radiometer (AVHRR). PC-SEAPAK {{is a set of}} independent microcomputer-based image analysis programs that provide the user with a flexible, user-friendly, standardized interface, and facilitates relatively low-cost analysis of oceanographic {{satellite data}}. Version 4. 0 includes 114 programs. PC-SEAPAK programs are organized into categories which include CZCS and AVHRR level- 1 ingest, level- 2 analyses, statistical analyses, data extraction, remapping to standard projections, graphics manipulation, image board memory manipulation, hardcopy output support and general utilities. Most programs allow user interaction through menu and command modes and also by the use of a mouse. Most programs also provide for ASCII file generation for further analysis in spreadsheets, graphics packages, etc. The CZCS scanning radiometer aboard the NIMBUS- 7 satellite was designed to measure the concentration of photosynthetic pigments and their degradation products in the ocean. AVHRR data is used to compute sea surface temperatures and is supported for the NOAA 6, 7, 8, 9, 10, 11, and 12 satellites. The CZCS operated from November 1978 to June 1986. CZCS data may be obtained free of charge from the CZCS archive at NASA/Goddard Space Flight Center. AVHRR data may be purchased through NOAA's Satellite Data Service Division. Ordering information is included in the PC-SEAPAK documentation. Although PC-SEAPAK was developed on a COMPAQ Deskpro 386 / 20, it can be run on most 386 -compatible computers with an AT bus, EGA controller, Intel 80387 coprocessor, and MS-DOS 3. 3 or higher. A Matrox MVP-AT image board with appropriate monitor and cables is also required. Note that the authors have received some reports of incompatibilities between the MVP-AT image board and ZENITH computers. Also, the MVP-AT image board is not necessarily compatible with 486 -based systems; users of 486 -based systems should consult with Matrox about compatibility concerns. Other PC-SEAPAK requirements include a Microsoft mouse (serial version), 2 Mb RAM, and 100 Mb hard disk space. For data ingest and backup, 9 -track tape, 8 mm tape and optical disks are supported and recommended. PC-SEAPAK has been under development since 1988. Version 4. 0 was updated in 1992, and is distributed without source code. It is available only as a set of 36 1. 2 Mb 5. 25 inch IBM MS-DOS format diskettes. PC-SEAPAK is a copyrighted product with all copyright vested in the National Aeronautics and Space Administration. Phar Lap's DOS_Extender <b>run-time</b> <b>version</b> is integrated into several of the programs; therefore, the PC-SEAPAK programs may not be duplicated. Three of the distribution diskettes contain DOS_Extender files. One of the distribution diskettes contains Media Cybernetics' HALO 88 font files, also licensed by NASA for dissemination but not duplication. IBM is a registered trademark of International Business Machines. MS-DOS is a registered trademark of Microsoft Corporation. HALO 88 is a registered trademark of Media Cybernetics, but the product was discontinued in 1991...|$|E

