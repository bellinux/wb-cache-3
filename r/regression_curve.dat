377|575|Public
5000|$|The {{center of}} the inverse <b>regression</b> <b>curve</b> is located at [...] Therefore, the {{centered}} inverse <b>regression</b> <b>curve</b> is ...|$|E
50|$|In Gaussian process regression, {{also known}} as Kriging, a Gaussian prior is assumed for the <b>regression</b> <b>curve.</b> The errors are assumed to have a multivariate normal {{distribution}} and the <b>regression</b> <b>curve</b> is estimated by its posterior mode. The Gaussian prior may depend on unknown hyperparameters, which are usually estimated via empirical Bayes.|$|E
5000|$|Computing {{the inverse}} <b>regression</b> <b>curve</b> (IR) means {{instead of looking}} for ...|$|E
40|$|The {{contribution}} {{deals with}} determining an influence of normal and tangential {{components of a}} stress on the total damage under combined random loading in tension-pressure and torsion. An experimental program ran with plain and notched tube specimens made out of low carbon steel ČSN 411523. 1. <b>Regression</b> <b>curves</b> were obtained for several rations of standard deviations of the stress components. Basing on the <b>regression</b> <b>curves,</b> curves of equal fatigue lives were constructed. It is possible to use them for calculating asafe life of structures...|$|R
40|$|We {{propose a}} new {{test for the}} {{comparison}} of two <b>regression</b> <b>curves,</b> {{which is based on}} a difference of two marked empirical processes based on residuals. The large sample behaviour of the corresponding statistic is studied to provide a full nonparametric comparison of <b>regression</b> <b>curves.</b> In contrast to most procedures suggested in the literature the new procedure is applicable in the case of different design points and heteroscedasticity. Moreover, it is demonstrated that the proposed test detects continuous alternatives converging to the null at a rate...|$|R
3000|$|... of the SiO 2 -Al 2 O 3 glasses {{increase}} rapidly from 10 to 40  GPa, with no discontinuous {{changes in}} slope. Figure  5 b shows the <b>regression</b> <b>curves</b> of P-V [...]...|$|R
50|$|The {{value of}} σg determines {{the slope of}} the least-squares <b>regression</b> <b>curve.</b>|$|E
5000|$|... #Caption: Graph of a {{logistic}} <b>regression</b> <b>curve</b> showing {{probability of}} passing an exam versus hours studying ...|$|E
5000|$|... 3. Compute {{the mean}} of [...] over all slices, which is a crude {{estimate}} [...] of the inverse <b>regression</b> <b>curve</b> : ...|$|E
30|$|Following {{regression}} analysis of total root biomass against RCD for roots >[*] 1 or >[*] 2  mm in diameter, a t test (two-tailed, unpaired) {{was conducted on}} the slopes of the <b>regression</b> <b>curves</b> to determine if there was a difference in growth characteristics between the two root diameter size classes. In the two-parameter exponential growth equation used (y[*]=[*]a exp (bx), the slope of the regression is represented by b. There was no difference between the slopes of the <b>regression</b> <b>curves</b> (P[*]=[*] 0.867) for the >[*] 1 -mm roots compared with the >[*] 2 -mm roots with respect to root biomass.|$|R
50|$|Plastic pipe {{materials}} {{have always been}} classified {{on the basis of}} long-term pressure testing. The measured failure times {{as a function of the}} stresses in the pipe wall has been demonstrated in so-called <b>Regression</b> <b>Curves.</b>|$|R
30|$|The {{research}} showed among others, {{that the}} <b>regression</b> <b>curves</b> {{could be used}} as a curve classification mean. Moreover, a significant variation was detected on the riders’ behavior when carrying a pillion related to their experience levels.|$|R
50|$|A Method of Determining the <b>Regression</b> <b>Curve</b> When the Marginal Distribution {{is of the}} Normal Logarithmic Type, Annals of Mathematical Statistics, 7:196-201, 1936.|$|E
50|$|The graph {{shows the}} {{probability}} of passing the exam versus {{the number of hours}} studying, with the logistic <b>regression</b> <b>curve</b> fitted to the data.|$|E
5000|$|... {{which is}} a [...] {{dimensional}} curve in [...] In what follows we will consider this centered inverse <b>regression</b> <b>curve</b> {{and we will see}} that it lies on a dimensional subspace spanned by [...]|$|E
40|$|BACKGROUND: Neuroprotective {{effects of}} esmolol in {{laboratory}} and clinical settings have been reported. The {{present study was}} designed to quantitatively evaluate the neuroprotective effects of esmolol using logistic <b>regression</b> <b>curves</b> and extracellular potentials. MATERIALS AND METHODS: In 42 gerbils, bilateral occlusion of common carotid arteries was performed for 3, 5, or 7 minutes (n= 7 in each group). In treated animals, esmolol (200 µg/kg/min) was administered for 90 minutes, 30 minutes before the onset of ischemia. Direct current potentials were measured in the bilateral CA 1 regions, in which histologic evaluation was performed 5 days later. Relations of neuronal damage with ischemic duration and duration of ischemic depolarization were determined using logistic <b>regression</b> <b>curves.</b> RESULTS: There {{was no significant difference in}} onset time between the 2 groups (the control group vs. the esmolol group: 1. 65 ± 0. 46 vs. 1. 68 ± 0. 45 min, P= 0. 76), and significant differences in durations of ischemic depolarization were not observed with any ischemic duration. However, logistic <b>regression</b> <b>curves</b> indicated that esmolol has a neuroprotective effect from 2. 95 to 7. 66 minutes of ischemic depolarization (P< 0. 05), and esmolol prolonged the duration of ischemic depolarization causing 50 % neuronal damage from 4. 97 to 6. 34 minutes (P< 0. 05). Logistic <b>regression</b> <b>curves</b> also indicated that esmolol has a neuroprotective effect from 3. 77 to 7. 74 minutes of ischemic duration (P< 0. 05), and esmolol prolonged the ischemic duration causing 50 % neuronal damage from 4. 26 to 4. 91 minutes (P< 0. 05) ...|$|R
30|$|Classification {{algorithms}} for positron {{emission tomography}} (PET) images support computational treatment planning in radiotherapy. Common clinical practice is based on manual delineation and fixed or iterative threshold methods, the latter of which requires <b>regression</b> <b>curves</b> dependent on many parameters.|$|R
40|$|Abstract: In {{this talk}} we study tests to compare <b>regression</b> <b>curves</b> in two samples, when the design {{variables}} are random. A major problem {{comes from the}} fact that the unknown distributions of the regressors may be unequal. We discuss a new class of score tests and show that they are maximin. The optimal weight function depends on the design distributions, the (conditional) variances of the error variables and on the ratio of the sample sizes. The tests have nontrivial power w. r. t. local alternatives converging to the null hypothesis of equal regression at parametric rates. References [1] Delgado, M. A. (1993). Testing the equality of nonparametric <b>regression</b> <b>curves,</b> Statist. Probab. Lett., 17...|$|R
5000|$|Given this {{condition}} and , {{it is indeed}} true that the centered inverse <b>regression</b> <b>curve</b> [...] is contained in the linear subspace spanned by , where [...] The proof is provided by Duan and Li in Journal of the American Statistical Association (1991).|$|E
5000|$|But before {{seeing that}} this holds true, {{we will have}} {{a look at how the}} inverse <b>regression</b> <b>curve</b> is {{computed}} within the SIR-Algorithm, which will be introduced in detail later. What comes is the [...] "sliced" [...] part of SIR. We estimate the inverse <b>regression</b> <b>curve</b> by dividing the range of [...] into [...] nonoverlapping intervals (slices), to afterwards compute the sample means [...] of each slice. These sample means are used as a crude estimate of the IR-curve, denoted as [...] There are several ways to define the slices, either in a way that in each slice are equally much observations, or we define a fixed range for each slice, so that we then get different proportions of the [...] that fall into each slice.|$|E
50|$|XLfit is a Microsoft Excel-based plug-in which {{performs}} <b>Regression,</b> <b>curve</b> fitting {{and statistical}} analysis. XLfit generates 2D and 3D graphs and analyses data sets produced by {{any type of}} research. XLfit’s curve fitting engine allows linear and non-linear curve fits, smoothing, statistics, weighting and error bars.|$|E
40|$|This paper {{proposes a}} test for the {{equality}} of nonparametric <b>regression</b> <b>curves</b> that {{does not depend on}} the choice of a smoothing number. The test statistic is a weighted empirical process easy to compute. It is powerful under alternatives that converge to the null at a rate n·I 12 • The disturbance [	 distributions are arbitrary and possibly unequal, and conditions on the regressors distribution are very mild. A simulation study demonstrates that the test enjoys good level and power propenies in small samples. We also study extensions to multiple regression, and testing the equality of several <b>regression</b> <b>curves.</b> Key words: Nonparametric testing; weighted empirical process; Donsker's invariance principle; Brownian motion; local alternatives...|$|R
40|$|Averages for the {{projected}} separation, squared radial velocity difference, {{and the product}} of these, are presented for a binary galaxy system of fixed mass and major axis, but any orbital eccentricity. The average of the product varies by a factor about 3 for eccentricities from 0 to 1. 0. For circular orbits, the results agree with those of Page (1952, 1960, 1961), but for linear orbits his mass estimate is too small by a factor 6. The mutual regressions of the velocity and separation on each other are calculated, and are presented {{in such a way}} as to exhibit the relative likelihood of occupation of the different parts of the <b>regression</b> <b>curves.</b> 'Isopleths' for the probability distribution are presented for a few values of the eccentricity to illustrate the underlying cause of pileup at certain parts of the <b>regression</b> <b>curves.</b> It is concluded that previous analyses were inadequate for failing to take into account that the <b>regression</b> <b>curves</b> represent in many cases a distribution that is almost wholly depopulated through most of the range. It is evident that the data are insufficient to draw a firm conclusion about the distribution of eccentricities...|$|R
30|$|Nbv[*]=[*]number {{of blood}} vessel branch points. The IC 50 (concentration {{providing}} 50 % inhibition) of extracts and standards was determinate using <b>regression</b> <b>curves</b> in the linear range of concentrations. The experiments have been repeated {{at least four}} times, {{and the results were}} reproducible.|$|R
5000|$|As just mentioned, the {{centered}} inverse <b>regression</b> <b>curve</b> lies on a dimensional subspace {{spanned by}} [...] (and therefore also the crude estimate we compute). This is {{the connection between}} our Model and Inverse Regression. We shall {{see that this is}} true, with only one condition on the design distribution that must hold. This condition is, that: ...|$|E
50|$|In {{mathematical}} statistics, polynomial {{least squares}} {{refers to a}} broad range of statistical methods for estimating an underlying polynomial that describes observations. These methods include polynomial <b>regression,</b> <b>curve</b> fitting, linear regression, least squares, ordinary least squares, simple linear regression, linear least squares, approximation theory and method of moments. Polynomial least squares has applications in radar trackers, estimation theory, signal processing, statistics, and econometrics.|$|E
5000|$|With high-dimensional data (as p grows), {{the number}} of {{observations}} needed to use local smoothing methods escalates exponentially. Reducing {{the number of}} dimensions makes the operation computable. Dimension reduction aims to show only the most important directions of the data. SIR uses the inverse <b>regression</b> <b>curve,</b> [...] to perform a weighted principal component analysis, with which one identifies the effective dimension reducing directions.|$|E
40|$|<b>Regression</b> <b>curves</b> for {{studying}} trait relationships are developed herein. The adaptive evolution model {{is considered an}} Ornstein-Uhlenbeck system whose parameters are estimated by a novel engagement of generalized least-squares and optimization. Our algorithm is implemented to ecological data. Comment: 14 pages, 3 figure...|$|R
40|$|We updated human {{chorionic}} gonadotropin (hCG) <b>regression</b> <b>curves</b> {{created in}} the eighties after evacuation of complete and partial molar (CM and PM, respectively) pregnancies using modern hCG assays. We created similar curves for patients in need of chemotherapy (gestational trophoblastic neoplasia [GTN]). status: accepte...|$|R
40|$|This paper {{considers}} a two-sample regression model with nonparametric <b>regression</b> <b>curves</b> that differ by a parametric {{transformation in the}} horizontal axis and constructs an efficient estimate of the parameter of the transformation. Semiparametric regression Efficient influence function Least dispersed regular estimator Least squares spline estimators...|$|R
5000|$|... {{which would}} provide a “best” fit for the data points. (Note that a {{straight}} line {{may not be the}} appropriate <b>regression</b> <b>curve</b> for the given data points.) Here the “best” will be understood as in the least-squares approach: such a line that minimizes the sum of squared residuals of the linear regression model. In other words, numbers α and β solve the following minimization problem: ...|$|E
50|$|Consider two models, 1 and 2, where model 1 is 'nested' within model 2. Model 1 is the {{restricted}} model, and Model 2 is the unrestricted one. That is, model 1 has p1 parameters, and model 2 has p2 parameters, where p2 > p1, and for any choice of parameters in model 1, the same <b>regression</b> <b>curve</b> {{can be achieved}} by some choice of the parameters of model 2. The model with more parameters will always be able to fit the data at least as well as the model with fewer parameters. Thus typically model 2 will give a better (i.e. lower error) fit to the data than model 1. But one often wants to determine whether model 2 gives a significantly better fit to the data. One approach to this problem is to use an F-test.|$|E
40|$|By using prior {{information}} about the <b>regression</b> <b>curve</b> we propose new nonparametric regression estimates. We incorporate two types of information. First, we suppose that the <b>regression</b> <b>curve</b> is similar in shape to a family of parametric curves characterized as the solution to a linear differential equation. The <b>regression</b> <b>curve</b> is estimated by penalized least squares with the differential operator defining the smoothness penalty. We discuss in particular growth and decay curves and take a time transformation to obtain a tractable solution. The second type of {{prior information}} is linear equality constraints. We estimate unknown parameters by generalized cross-validation or maximum likelihood and obtain efficient O(n) algorithms to compute the estimate of the <b>regression</b> <b>curve</b> and the cross-validation and maximum likelihood criterion functions...|$|E
40|$|In {{this article}} we {{introduce}} a graphical method for the test of the equality of two <b>regression</b> <b>curves.</b> Our method is based on SiZer (SIgnificant ZERo crossing of the differences) analysis, which is a scale-space visualization tool for statistical inferences. The proposed method does not require any specification of smoothing parameters, it offers a device to compare {{in a wide range}} of resolutions, instead. This enables us to find the differences between two curves that are present at each resolution level. The extension of the proposed method to the comparison of more than two <b>regression</b> <b>curves</b> is also done using residual analysis. A broad simulation study is conducted to demonstrate the sample performance of the proposed tool. Applications with two real examples are also included. ...|$|R
30|$|Results are {{expressed}} as averages[*]±[*]standard deviation throughout. Comparison between groups {{was assessed by}} Student's t-test. For the cell experiments, the linear <b>regression</b> <b>curves</b> and subsequent comparisons in elevation and slope were performed in GraphPad Prism 5 (GraphPad, La Jolla, CA, USA). P < 0.05 was considered significant.|$|R
5000|$|... linear <b>regression</b> and <b>curve</b> fitting using several {{minimization}} techniques ...|$|R
