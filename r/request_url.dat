10|120|Public
5000|$|Content {{resolution}} that maps a <b>request</b> <b>URL</b> to a content node {{in the content}} repository ...|$|E
5000|$|If {{a client}} has link-editing capabilities, it should update all {{references}} to the <b>Request</b> <b>URL.</b>|$|E
5000|$|The request {{follows a}} pattern first {{information}} {{is taken from}} URL endpoint of HTTP. URI is the end point of the <b>request.</b> <b>URL</b> structure follows as: '''''' ...|$|E
5000|$|In Adblock Plus, {{the list}} of blockable content {{includes}} <b>requested</b> <b>URLs</b> ...|$|R
50|$|Generic request {{line and}} header filter {{dropping}} suspicious <b>request</b> <b>URLs</b> or HTTP headers.|$|R
5000|$|... mod_rewrite - rewrites <b>requested</b> <b>URLs</b> {{on the fly}} {{based on}} regular-expressions-based rules and various {{conditions}} ...|$|R
50|$|The adapter {{needs to}} know what {{requests}} {{it is going to}} serve, usually based on some pattern in the <b>request</b> <b>URL,</b> and to where to direct these requests.|$|E
5000|$|When {{sent from}} client to server, ANNOUNCE posts the {{description}} of a presentation or media object identified by the <b>request</b> <b>URL</b> to a server. When sent from server to client, ANNOUNCE updates the session description in real-time. If a new media stream is added to a presentation (e.g., during a live presentation), the whole presentation description should be sent again, rather than just the additional components, so that components can be deleted.|$|E
50|$|Analyzing the target, or {{the voting}} project, {{should be done}} before {{actually}} building the votebot. When handling a voting website for example, one needs to do some webpage analysis on the target, extracting the <b>request</b> <b>URL</b> of the voting action {{as well as some}} HTTP header settings to cheat the website.There are lots of tools which help people to analyze the web, such as Firebug and httpanalyzer. One can trace the voting process of HTTP packages by these tools and find the right voting target and some simple protecting tricks used by websites, such as referrer verification.|$|E
50|$|HTTP {{authorization}} of web <b>request</b> <b>URLs</b> using {{a choice of}} Apache Ant paths or regular expressions.|$|R
25|$|URL Rewrite Module: Provides a {{rule-based}} rewriting {{mechanism for}} changing <b>request</b> <b>URLs</b> {{before they are}} processed by the Web server.|$|R
5000|$|The action takes HTTP <b>requests</b> (<b>URLs</b> {{and their}} methods) and uses that input to {{interact}} with the domain, after which it passes the domains output to one and only one responder.|$|R
5000|$|PUSH_PROMISE frame. This {{frame is}} {{sent by the}} server to the browser to start pushing a resource. It also {{contains}} HTTP headers. However, the kind of headers present in a PUSH_PROMISE frame are headers that would normally be present in a request [...] This {{is different from the}} response headers that a server would normally send. The <b>request</b> <b>URL,</b> for example, is present in the PUSH_PROMISE frame as the HTTP/2-specific :path pseudo-header, as is the :authority pseudo-header to indicate a host. Other headers that may be present in a PUSH_PROMISE and that some browsers use are cache headers, for example, if-none-match.|$|E
50|$|Because HTTPS piggybacks HTTP {{entirely on}} top of TLS, the {{entirety}} of the underlying HTTP protocol can be encrypted. This includes the <b>request</b> <b>URL</b> (which particular web page was requested), query parameters, headers, and cookies (which often contain identity information about the user). However, because host (website) addresses and port numbers are necessarily part of the underlying TCP/IP protocols, HTTPS cannot protect their disclosure. In practice this means that even on a correctly configured web server, eavesdroppers can infer the IP address and port number of the web server (sometimes even the domain name e.g. www.example.org, but not the rest of the URL) that one is communicating with, as well as the amount (data transferred) and duration (length of session) of the communication, though not the content of the communication.|$|E
40|$|Corporations {{that offer}} online trading can achieve a {{competitive}} edge by serving worldwide clients. Nevertheless, online trading faces many obstacles such as the unsecured money orders. Phishing is considered a form of internet crime that {{is defined as the}} art of mimicking a website of an honest enterprise aiming to acquire confidential information such as usernames, passwords and social security number. There are some characteristics that distinguish phishing websites from legitimate ones such as long URL, IP address in URL, adding prefix and suffix to domain and <b>request</b> <b>URL,</b> etc. In this paper, we explore important features that are automatically extracted from websites using a new tool instead of relying on an experienced human in the extraction process and then judge on the features importance in deciding website legitimacy. Our research aims to develop a group of features that {{have been shown to be}} sound and effective in predicting phishing websites and to extract those features according to new scientific precise rules...|$|E
50|$|A program {{receiving}} a query string can ignore part {{or all of}} it. If the <b>requested</b> <b>URL</b> corresponds to a file and not to a program, the whole query string is ignored. However, {{regardless of whether the}} query string is used or not, the whole URL including it is stored in the server log files.|$|R
5000|$|The PURL service {{includes}} a concept known as partial redirection. If a request {{does not match}} a PURL exactly, the <b>requested</b> <b>URL</b> is checked to determine if some contiguous front portion of the PURL string matches a registered PURL. If so, a redirection occurs with {{the remainder of the}} <b>requested</b> <b>URL</b> appended to the target URL. For example, consider a PURL with a URL of [...] with a target URL of [...] An attempt to perform an HTTP GET operation on the URL [...] would result in a partial redirection to [...] The concept of partial redirection allows hierarchies of Web-based resources to be addressed via PURLs without each resource requiring its own PURL. One PURL is sufficient to serve as a top-level node for a hierarchy on a single target server. The new PURL service uses the type [...] "partial" [...] to denote a PURL that performs partial redirection.|$|R
50|$|Assuming the <b>requested</b> <b>URL</b> is acceptable, {{the content}} is then fetched by the proxy. At this point a dynamic filter may be applied {{on the return}} path. For example, JPEG files could be blocked based on fleshtone matches, or {{language}} filters could dynamically detect unwanted language. If the content is rejected then an HTTP fetch error may {{be returned to the}} requester.|$|R
40|$|Many JavaScript {{applications}} perform HTTP {{requests to}} web APIs, {{relying on the}} <b>request</b> <b>URL,</b> HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests {{comply with the requirements}} of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96. 0 %. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87. 9 % for payload data and 99. 9 % for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests. Comment: International Conference on Software Engineering, 201...|$|E
50|$|Each of the {{standard}} HTML reports described above lists only top entries for each item (e.g. top 20 URLs). The actual number of lines {{for each of the}} reports is controlled by configuration. The Webalizer may also be configured to produce a separate report for each of the items, which will list every single item, such as all website visitors, all <b>requested</b> <b>URLs,</b> etc.|$|R
50|$|The {{initial release}} of mod_qos {{was created in}} May 2007 and {{published}} on SourceForge.net as an open source software project. It was able {{to limit the number}} of concurrent HTTP requests for specified resources (path portion of <b>request</b> <b>URLs)</b> on the web server.More features were added and some of them were useful to protect Apache servers against DoS attacks.In 2012, mod_qos was included to the Ubuntu Linux distribution.|$|R
50|$|For {{a static}} <b>request</b> the <b>URL</b> path {{specified}} by the client is relative to the web server's root directory.|$|R
50|$|From Agora 0.7d it was {{possible}} to search some searchable sites by adding the search terms separated by spaces after the URL, but this would not work with forms.Since Agora version 0.8e it {{was possible}} to split the <b>requested</b> <b>URLs</b> into two or more lines. Data compression with uuencoded by gzip or zip was also integrated.Agora version 0.8f determined frames and linked pictures goto and the answer mail get help in these cases.|$|R
2500|$|Between 2004 and 2006, BT Group {{introduced}} its Cleanfeed technology {{which was then}} used by 80% of internet service providers. BT spokesman Jon Carter described Cleanfeed's function as [...] "to block access to illegal Web sites that are listed by the Internet Watch Foundation", and described it as essentially a server hosting a filter that checked <b>requested</b> <b>URLs</b> for Web sites on the IWF list, and returning an error message of [...] "Web site not found" [...] for positive matches.|$|R
5000|$|Pound is a {{lightweight}} open source reverse proxy program and application firewall suitable {{to be used}} as a web server load balancing solution. Developed by an IT security company, it has a strong emphasis on security. The original intent on developing Pound was to allow distributing the load among several Zope servers running on top of ZEO (Zope Extensible Object). However, Pound is not limited to Zope-based installations. Using regular expression matching on the <b>requested</b> <b>URLs,</b> Pound can pass different kinds of requests to different backend server groups. A few more of its most important features: ...|$|R
30|$|The {{main reason}} for {{choosing}} the FIFA logs {{is the fact that}} it provides us, directly or indirectly, with all the information required to evaluate our algorithm: 1) it directly provides us with user driven access patterns that represent the real way users have requested pages from the site; 2) it provides us with sufficient information to obtain the actual site, through which we can obtain crucial information about pages and links; and 3) it is readily available from the Internet with a manageable size. Other traces found online or used in literature often omit important information (such as the <b>requested</b> <b>URLs)</b> by anonymizing the traces or are no longer available.|$|R
5000|$|Google flags {{search results}} {{with the message}} [...] "This site may harm your computer" [...] if the site is known to install {{malicious}} software in the background or otherwise surreptitiously. Google does this to protect users against visiting sites that could harm their computers. For approximately 40 minutes on January 31, 2009, all search results were mistakenly classified as malware and could therefore not be clicked; instead a warning message was displayed and the user was required to enter the <b>requested</b> <b>URL</b> manually. The bug was caused by human error. The URL of [...] "/" [...] (which expands to all URLs) was mistakenly added to the malware patterns file.|$|R
5000|$|The user's browser <b>requests</b> the {{redirect}} <b>URL</b> for {{the identity}} provider, including the application's request ...|$|R
50|$|Multiple {{specifications}} {{provide a}} fall-back when a proxy fails to respond. The browser fetches this PAC file before <b>requesting</b> other <b>URLs.</b> The URL of the PAC file is either configured manually or determined automatically by the Web Proxy Autodiscovery Protocol.|$|R
30|$|The good buckets were {{gathered}} from the Alexa top 3  K websites, which {{are considered to be}} mostly clean. To this end, we visited each website using a crawler (as a Firefox add-on) to record the HTTP traffic triggered by the visit, including network requests, responses, browser events, etc. From the collected traffic, we extracted the HTTP cloud <b>request</b> <b>URLs</b> corresponding to 300 cloud buckets hosted on 20 leading cloud hosting services like Amazon S 3, Google Drive, etc. (see Table 6 in Appendix for the complete list). Note that even though some of them provide CDN service or DDOS protection, they are all provided hosting service to act as cloud repository.|$|R
50|$|Another {{technique}} {{is for the}} owner of an Internet domain name {{to set up the}} domain's DNS entry so that all subdomains are directed to the same server. The operator then sets up the server so that page requests generate a page full of desired Google search terms, each linking to a subdomain of the same site, with the same title as the subdomain in the <b>requested</b> <b>URL.</b> Frequently the subdomain matches the linked phrase, with spaces replaced by underscores or hyphens. Since Google treats subdomains as distinct sites, the effect of many subdomains linking to each other is a boost to the PageRank of those subdomains and of any other site they link to.|$|R
5000|$|Cleanfeed is {{a content}} {{blocking}} system technology {{implemented in the}} UK by BT, Britain's largest Internet provider. It was created in 2003 and went live in June 2004. BT spokesman Jon Carter described Cleanfeed's function as [...] "to block access to illegal Web sites that are listed by the Internet Watch Foundation", and described it as essentially a server hosting a filter that checked <b>requested</b> <b>URLs</b> for Web sites on the IWF list, and returning an error message of [...] "Web site not found" [...] for positive matches. Cleanfeed is a silent content filtering system, which means that Internet users cannot ascertain whether they are being regulated by Cleanfeed, facing connection failures, or the page really does not exist.|$|R
5000|$|Instead of {{creating}} the <b>URL</b> <b>request</b> manually, there are many open source libraries available for most programming languages.|$|R
5000|$|The user's browser <b>requests</b> the {{redirect}} <b>URL</b> {{that goes}} back to the application, including the identity provider's response ...|$|R
40|$|Abstract. HTTP Parameter Pollution (HPP) {{vulnerabilities}} allow at-tackers {{to exploit}} web applications by manipulating the query {{parameters of the}} <b>requested</b> <b>URLs.</b> In this paper, we present Application Request Cache (ARC), a framework for protecting web applications against HPP exploitation. ARC hosts all benign URL schemas, which act as generators of the complete functional set of URLs that compose the application’s logic. For each incoming request, ARC exports the URL, extracts the as-sociated schema, and searches {{for it in the}} set of already known benign schemas. In case the schema is not found, the request is rejected, and the event is recorded. ARC can be transparently integrated with existing web applications without any modifications to the server and client code. It is implemented in Google’s Go language and uses efficient data structures for storing the URL schemas, imposing negligible computational overhead on the web application server. When running on a 4 -core Linux server, ARC can process hundreds of thousands of <b>URL</b> <b>requests</b> per second. A typical URL resolution is in the scale of microseconds...|$|R
50|$|Templates {{are named}} after the classes in database. Roundup {{automatically}} chooses template based on class name <b>requested</b> from <b>URL.</b> Some templates are used for several classes, e.g. _generic.index.html, which allows (authorized) users to change the objects of all classes which lack an own index template.|$|R
