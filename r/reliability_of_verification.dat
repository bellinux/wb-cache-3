5|10000|Public
40|$|In {{the next}} {{generation}} of GPS (Geometrical Product Specification and Verification) standard system, measurement uncertainty and specification uncertainty collectively known as conformance uncertainty. A method to estimate the conformance uncertainty according to {{the next generation}} of GPS standard system is proposed. A case study of straightness is given. Based on the model given in {{the next generation of}} GPS, the calculation equations of the conformance uncertainty for different methods are deduced. The experimental result indicates that the method can not only assure the integrity of the verification result, but also improve the <b>reliability</b> <b>of</b> <b>verification...</b>|$|E
40|$|Abstract. This paper {{presents}} an advanced verification environment based on VMM verification platform architecture which is constructed {{based on an}} object oriented language named System Verilog. The portable, reusable and extensible verification environment, which has a hierarchical structure, randomized excitation and self-check mechanism, efficiently improves the adequacy and <b>reliability</b> <b>of</b> <b>verification</b> and validation efficiency. The environment was used to verify an EEPROM controller which generated the reading and writing timing signals to operate EEPROM. The AHB bus function model-VIP, that used in the environment, reduced the development cycle efficiently. Additionally, the function coverage collected in the test platform gave an intuitive evidence of the verification adequacy and reliability...|$|E
40|$|International audienceEvaluating {{the outcome}} of analog {{simulations}} is a common, mostly manually carried out task in the pre-silicon verification process of mixed-signal ICs. Its non-automated nature makes it an error-prone and time-consuming procedure. For this very reason, we introduce a novel approach for performing this evaluation automatically resulting in significantly reduced turnaround times {{as well as a}} considerably increased <b>reliability</b> <b>of</b> <b>verification</b> results. The presented concept is motivated by an algorithm that is used in optical pattern recognition and is called Earth Mover’s Distance. Furthermore, we compare our approach with already existing algorithms, namely Fréchet Distance and Pearson Coefficient, in order to analyze its capability. Finally, we present a case study in which we prove the algorithm by applying it to the results of a mixed-signal simulation at chip-level demonstrating the efficiency of our approach...|$|E
40|$|The {{specification}} <b>of</b> {{risk and}} <b>reliability</b> acceptance criteria {{is a key}} issue <b>of</b> <b>reliability</b> <b>verifications</b> <b>of</b> new and existing structures. Current target reliability levels in standards appear to have considerable scatter. Critical review of risk acceptance approaches to societal, economic and environmental risk indicates that an optimal design strategy is mostly dominated by economic aspects while human safety aspects need to be verified only in special cases. It is recommended to specify the target levels considering economic optimisation and the marginal life-saving costs principle, as both these approaches {{take into account the}} failure consequences and costs of safety measures...|$|R
40|$|In this paper, a {{probabilistic}} {{measure for}} <b>reliability</b> <b>of</b> speaker <b>verification</b> under noisy acoustic conditions is proposed. A Bayesian network {{is used to}} estimate a probability for verification errors, given the GMM-based speaker verification system output and additional information {{about the level of}} acoustic noise. In particular, the log-likelihood ratio and a signal-to-noise related feature are used to account for the adverse acoustic conditions. The probabilistic measure is subsequently employed in governing a repair sequence of trials for acquiring additional speech presentations which are less likely to lead to unreliable <b>verification.</b> The potential <b>of</b> the proposed method is tested through cross-validation experiments. Finally, the benefits of the repair sequence in terms <b>of</b> <b>verification</b> accuracy is evaluated on a noisy environment speaker verification task. 1...|$|R
30|$|Therefore, {{to improve}} the {{security}} of version data and reduce the storage space of version data, we redesign the base version of version files {{to meet the needs}} of different storage methods of version data in this paper. We call each version file in the version chain the storage node, which can also be shorten to node. Moreover, in order to protect data privacy, when TPV performs the data verification, we apply bilinear maps and homomorphic encryption to guarantee the security <b>of</b> the <b>verification</b> and the <b>reliability</b> <b>of</b> the <b>verification</b> results, and meanwhile, effectively reduce the traffic cost in network communication. The homomorphic encryption generates an encrypted result on ciphertexts, which matches the result of the operations on the plaintext when decrypted. The purpose of homomorphic encryption is to allow computation on encrypted data.|$|R
40|$|This {{study is}} {{concerned}} with the reliability of biometric verification systems when used in forensic applications. In particular, when such systems are subjected to targeted impersonation attacks. The authors expand on the existing work in targeted impersonation, focusing on how best to measure the <b>reliability</b> <b>of</b> <b>verification</b> systems in forensic contexts. It identifies two scenarios in which targeted impersonation effects may occur: (i) the forensic investigation of criminal activity involving identity theft; and (ii) implicit targeting {{as a result of the}} forensic investigation process. Also, the first partial countermeasure to such attacks is presented. The countermeasure uses client-specific Z-score normalisation to provide a more consistent false acceptance rate across all enrolled subjects. This reduces the effectiveness of targeted impersonation without impairing the systems accuracy under random zero-effort attacks...|$|E
40|$|An {{appropriate}} {{combination of}} multiple biometric sensors in-creases the <b>reliability</b> <b>of</b> <b>verification</b> through biometrics. In this pa-per we propose an effective method of fusion of biometrics {{based on a}} dynamic selection of threshold point of fingerprint and iris biometrics towards identifier of an optimal set of rules for fu-sion. The effectiveness of the method has been established us-ing several benchmark databases using Simulated Annealing ap-proach. The selection of a proper set of parameters for SA is a multi-objective decision making optimization problem. Initially the matching scores for individual biometric classifiers are computed. Next, a SA-based procedure is followed to simultaneously optimize the parameters and the fusion rules for fingerprint and iris biomet-rics. An experimental verification of the convergence nature of the simulated annealing method with the worst case behavior for op-timum rule selection is analyzed and a comparative result of the method with the Ant colony optimization technique is also given...|$|E
40|$|Abstract. Fusion is {{a popular}} {{practice}} to increase the <b>reliability</b> <b>of</b> the biometric <b>verification.</b> In this paper, optimal fusion at decision level by AND rule and OR rule is investigated. Both a theoretical analysis and the experimental results are given. Comparisons are presented between fusion at decision level and fusion at matching score level. For our face verification system, decision fusion proves to be a simple, practical, and effective approach, which significantly improves {{the performance of the}} original classifier. ...|$|R
40|$|Fusion is {{a popular}} {{practice}} to increase the <b>reliability</b> <b>of</b> biometric <b>verification.</b> In this paper, we propose an optimal fusion scheme at decision level by the AND or OR rule, based on optimizing matching score thresholds. The proposed fusion scheme will always give an improvement in the Neyman–Pearson sense over the component classifiers that are fused. The theory of the threshold-optimized decision-level fusion is presented, and the applications are discussed. Fusion experiments are done on the FRGC database which contains 2 D texture data and 3 D shape data. The proposed decision fusion improves the system performance, in a way comparable to or better than the conventional score-level fusion. It is noteworthy that in practice, the threshold-optimized decision-level fusion by the OR rule is especially useful in presence of outliers...|$|R
40|$|Reuse is pin-pointed as a {{key factor}} to improve {{productivity}} and <b>reliability</b> <b>of</b> software systems. <b>Verification</b> and validation <b>of</b> software components and the resulting system is important for reuse to be beneficial on a broad industrial basis. This paper suggests a modelling approach which is suitable for <b>reliability</b> certification <b>of</b> modular systems. It discusses a general reliability certification procedure and provides guidelines and opportunities for how to certify software components and also presents some alternatives for certification of modular systems. We conclude that to create reliable systems, we must start certifying the individual constituents of the systems. ...|$|R
40|$|Software {{development}} for NASA missions {{is a particularly}} challenging task. Missions are extremely ambitious scientifically, have very strict time frames, and must be accomplished with a maximum degree <b>of</b> <b>reliability.</b> <b>Verification</b> technologies must therefore be pushed far beyond their current capabilities. Moreover, reuse and adaptation of software architectures and components must be incorporated in software development within and across missions. This paper discusses NASA applications that we are currently investigating from these perspectives...|$|R
40|$|Abstract: Next {{generation}} of Geometrical Product Specifications (GPS) {{is the foundation}} of the technology standards and metrology specifications of mechanical and electric products. GPS estimation of measurement uncertainty can improve the <b>reliability</b> <b>of</b> the <b>verification</b> result. In the indirect measurement, the transfer characteristic is very complex. So, {{it is very difficult to}} estimate the uncertainty in the indirect measurements according to the transfer formula given by GUM. For the indirect measuring of an inside cone angle, the mathematical relation between the measurand and measurement results is established firstly. Then Monte Carlo method was adopted to conduct the sampling and synthesis of measurement uncertainty contributors. At last, the measurement method was evaluated and improved according to Procedure for Uncertainty Management. Experimental result shows that Monte Carlo Simulation method has a good application foreground in the uncertainty estimation and measurement program design...|$|R
40|$|AbstractFirst {{priority}} {{tasks to}} be resolved {{in the development of}} new generation of nuclear data support systems for calculation of fast reactors {{on the basis of the}} unified ABBN- 2020 multi-group system of neutron data retrieved from the ROSFOND files of evaluated neutron data are discussed. Along with the development of advanced system for provision of nuclear data support for neutronics calculations significant attention must be paid to the development and elaboration of methodologies and calculation codes for evaluation and ranging of calculation uncertainties in the calculations of nuclear facilities in order to be able to focus the efforts on the most important directions. Development of the integral unified nuclear data support system and its implementation in the calculation codes will ensure not only the unification of the procedure for nuclear data preparation, which will allow enhancing <b>reliability</b> <b>of</b> their <b>verification,</b> but, as well, will enhance accuracy and <b>reliability</b> <b>of</b> calculation prediction of all the most important characteristics of the reactors under design, will ensure their licensing compliance, competitiveness and independence from foreign products...|$|R
40|$|Zeno runs, where {{infinitely}} many actions {{occur in}} finite time, may inadvertently arise in timed automata specifications. Zeno runs may compromise the <b>reliability</b> <b>of</b> formal <b>verification,</b> and few model-checkers provide {{the means to}} deal with them: this usually {{takes the form of}} liveness checks, which are computationally expensive. As an alternative, we describe here an efficient static analysis to assert absence of Zeno runs on Uppaal networks; this is based on Tripakis's strong non-Zenoness property, and identifies all loops in the automata graphs where Zeno runs may possibly occur. If such unsafe loops are found, we show how to derive an abstract network that over-approximates the loop behaviour. Then, liveness checks may assert absence of Zeno runs in the original network, by exploring the reduced state space of the abstract network. Experiments show that this combined approach may be much more efficient than running liveness checks on the original network...|$|R
40|$|In this paper, the {{features}} that have personal characteristic using pen inclination and pressure information are discussed. Forging a pen inclination and pressure information is difficult {{because it is not}} visible. Four features using invisible information are proposed and their characteristics are discussed. Proposed features calculated by physical vector analysis are verified by SVC 2004 database using DP matching algorithm. As a result, the new feature named Down improves the recognition rate and <b>reliability.</b> Average <b>of</b> correct <b>verification</b> rate is 94. 57 % and variance is 0. 667. </p...|$|R
40|$|Traditionally the {{position}} <b>of</b> <b>reliability</b> {{analysis in the}} design and production process of electronic circuits is a position <b>of</b> <b>reliability</b> <b>verification.</b> A completed design is checked on reliability aspects and either rejected or accepted for production. This paper describes a method to model physical failure mechanisms within components {{in such a way that}} they can be used for reliability optimization, not after, but during the early phase of the design process. Furthermore a prototype of a CAD software tool is described, which can highlight components likely to fail and automatically adjust circuit parameters to improve product reliability...|$|R
40|$|We {{address a}} number of {{limitations}} of Timed Automata and real-time model-checkers, which undermine the <b>reliability</b> <b>of</b> formal <b>verification.</b> In particular, {{we focus on the}} model-checker Uppaal as a representative of this technology. Timelocks and Zeno runs represent anomalous behaviours in a timed automaton, and may invalidate the <b>verification</b> <b>of</b> safety and liveness properties. Currently, model-checkers do not offer adequate support to prevent or detect such behaviours. In response, we develop new meth-ods to guarantee timelock-freedom and absence of Zeno runs, which improve and complement the existent support. We implement these methods in a tool to check Uppaal specifications. The requirements language of model-checkers is not well suited to express sequence and iteration of events, or past computations. As a result, validation problems may arise during verification (i. e., the property that we verify may not accurately reflect the intended require-ment). We study the logic PITL, a rich propositional subset of Interval Temporal Logic, where these requirements can be more intuitively expressed than in model-checkers. However, PITL has a decision procedure with a worst-case non-elementary complexity, which has hampered th...|$|R
40|$|Because of the {{differences}} in education background, accents, etc., different persons have their unique way of pronunciation. This paper exploits the pronunciation characteristics of speakers and proposes a new conditional pronunciation modeling (CPM) technique for speaker verification. The proposed technique aims to establish a link between articulatory properties (e. g., manners and places of articulation) and phoneme sequences produced by a speaker. This is achieved by aligning two articulatory feature (AF) streams with a phoneme sequence determined by a phoneme recognizer, and formulating the probabilities of articulatory classes conditioned on the phonemes as speaker-dependent probabilistic models. The scores obtained from the AF-based pronunciation models are then fused with those obtained from a spectral-based speaker verification system, with the frame-by-frame fused scores weighted by the confidence of the pronunciation models. Evaluations based on the SPIDRE corpus demonstrate that AF-based CPM systems can recognize speakers even with short utterances and are readily combined with spectral-based systems to further enhance the <b>reliability</b> <b>of</b> speaker <b>verification...</b>|$|R
40|$|The {{analysis}} <b>of</b> existing problem <b>reliability</b> and <b>verification</b> <b>of</b> widespread {{electric power}} systems (EPS) simulation tools {{is presented in}} this article. Everything simulation tools {{are based on the}} using of numerical methods for ordinary differential equations. Described the concept <b>of</b> guaranteed <b>verification</b> EPS simulation tools and the structure of its realization are based using the Simulator having properties of continuous, without decomposition three-phase EPS simulation in real time and on an unlimited range with guaranteed accuracy. The information from the Simulator can be verified by using data only quasi-steady-state regime received from the SCADA and such Simulator can be applied as the standard model for verification any EPS simulation tools...|$|R
40|$|With the {{development}} of speech synthesis techniques, automatic speaker verification systems face the serious challenge of spoofing attack. In order to improve the <b>reliability</b> <b>of</b> speaker <b>verification</b> systems, we develop a new filter bank based cepstral feature, deep neural network filter bank cepstral coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The deep neural network filter bank is automatically generated by training a filter bank neural network (FBNN) using natural and synthetic speech. By adding restrictions on the training rules, the learned weight matrix of FBNN is band-limited and sorted by frequency, similar to the normal filter bank. Unlike the manually designed filter bank, the learned filter bank has different filter shapes in different channels, which can capture the differences between natural and synthetic speech more effectively. The experimental results on the ASVspoof { 2015 } database show that the Gaussian mixture model maximum-likelihood (GMM-ML) classifier trained by the new feature performs better than the state-of-the-art linear frequency cepstral coefficients (LFCC) based classifier, especially on detecting unknown attacks...|$|R
40|$|Abstract:- The {{integration}} of analog and/or digital circuits into more fields of application {{has increased the}} demand for <b>reliability</b> <b>verification</b> <b>of</b> a system in the operating phase. It is known that sensor parameter variation in analog circuits leads to faulty results. Determining whether an operation point is to be tolerated needs to be handled in the operation phase. The proposed methods achieve high fault coverage. A temperature measurement system is the example used in this experiment. A comparison is done with other test methods, supply current supervision and the correlation of the output voltage to the supply current. Key-Words:- online test, offline, test, operation phase, Idd supervision...|$|R
30|$|The rapid {{development}} of computer science, cloud computing, Internet of things, and mobile Internet {{have become the}} symbol of the times. The {{rapid development}} of these software technologies has challenged the reliability and hardware performance of self-verification. The performance requirements for hardware ultimately fall on the <b>reliability</b> <b>of</b> hardware integrated circuits. However, in the course <b>of</b> hardware development, <b>verification</b> <b>of</b> the correctness and <b>reliability</b> <b>of</b> integrated chips has been a bottleneck. The verification method based on simulation is widely used in the field <b>of</b> <b>verification,</b> which has irreparable shortcomings, that is, it can only prove that the system is wrong, but it can not prove the correctness of the system design. In software, the <b>verification</b> <b>of</b> <b>reliability</b> is still in the stage of no specific specification. Therefore, it is meaningful to verify the <b>reliability</b> <b>of</b> software or hardware.|$|R
40|$|To {{enhance the}} reality of Connected and Autonomous Vehicles (CAVs) {{kinematic}} simulation scenarios and to guarantee the accuracy and <b>reliability</b> <b>of</b> the <b>verification,</b> a four-layer CAVs kinematic simulation framework, which is composed with road network layer, vehicle operating layer, uncertainties modelling layer and demonstrating layer, is proposed in this paper. Properties of the intersections are defined to describe the road network. A target position based vehicle position updating method {{is designed to simulate}} such vehicle behaviors as lane changing and turning. Vehicle kinematic models are implemented to maintain the status of the vehicles when they are moving towards the target position. Priorities for individual vehicle control are authorized for different layers. Operation mechanisms of CAVs uncertainties, which are defined as position error and communication delay in this paper, are implemented in the simulation to enhance {{the reality of}} the simulation. A simulation platform is developed based on the proposed methodology. A comparison of simulated and theoretical vehicle delay has been analyzed to prove the validity and the creditability of the platform. The scenario of rear-end collision avoidance is conducted to verify the uncertainties operating mechanisms, and a slot-based intersections (SIs) control strategy is realized and verified in the simulation platform to show the supports of the platform to CAVs kinematic simulation and verification...|$|R
40|$|The {{purpose of}} this study is to {{investigate}} the applicability of isotope correlation techniques (ICT) to the Light Water Reactor (LWR) and the Liquid Metal Fast Breeder Reactor (LMFBR) fuel cycles for nuclear material accountancy and safeguards surveillance. The isotopic measurement of the inventory input to the reprocessing phase of the fuel cycle is the primary direct determination that an anomaly may exist in the fuel management of nuclear material. The nuclear materials accountancy gap which exists between the fabrication plant output and the input to the reprocessing plant can be minimized by using ICT at the dissolver stage of the reprocessing plant. The ICT allows a level <b>of</b> <b>verification</b> <b>of</b> the fabricator's fuel content specifications, the irradiation history, the fuel and blanket assemblies management and scheduling within the reactor, and the subsequent spent fuel assembly flows to the reprocessing plant. The investigation indicates that there exist relationships between isotopic concentration which have predictable, functional behavior over a range of burnup. Several cross-correlations serve to establish the initial core assembly-averaged composition. The selection of the more effective functionals will depend not only on the level <b>of</b> <b>reliability</b> <b>of</b> ICT for <b>verification,</b> but also on the capability, accuracy and difficulty of developing measurement methods. The propagation of measurement errors on the correlation functions and respective sensitivities to isotopic compositional changes have been examined and found to be consistent with current measurement methods...|$|R
40|$|Automated <b>Verification</b> <b>of</b> Flight Software), a {{collection}} of tools for analyzing source programs written in FORTRAN and AED is documented. The quality and the <b>reliability</b> <b>of</b> flight software are improved by: (1) indented listings of source programs, (2) static analysis to detect inconsistencies {{in the use of}} variables and parameters, (3) automated documentation, (4) instrumentation of source code, (5) retesting guidance, (6) analysis of assertions, (7) symbolic execution, (8) generation <b>of</b> <b>verification</b> conditions, and (9) simplification <b>of</b> <b>verification</b> conditions. Use <b>of</b> AVFS in the <b>verification</b> <b>of</b> flight software is described...|$|R
40|$|AbstractThe paper {{deals with}} {{optimisation}} of transversal disposition {{of steel and}} concrete composite road bridges {{in terms of the}} number of steel beams. For this purpose, a parametric study was performed, in which a total of 32 superstructures of steel and concrete composite road bridges, with different number of steel beams, different road widths and theoretical spans, were modelled and assessed. The <b>reliability</b> <b>verification</b> <b>of</b> the superstructure has been done according to Eurocodes. A result of parametric study is a comparison of advantages of the double-beam and the four-beam variants in terms of material consumption when considering different transverse dimensions and spans of the bridge. The individual variants are compared on the basis of the consumption of structural steel, concrete and reinforcing steel bars...|$|R
5000|$|... 100% <b>verification</b> <b>of</b> <b>reliability</b> <b>of</b> {{connections}} in initiation network.|$|R
40|$|Static <b>verification</b> <b>of</b> {{a program}} source code {{correctness}} {{is an important}} element <b>of</b> software <b>reliability.</b> Formal <b>verification</b> <b>of</b> software programs involves proving that a program satisfies a formal specification of its behavior. Many languages use both static and dynamic type checking. With such approach, the static type checker verifies everything possible at compile time, and dynamic checks the remaining. The {{current state of the}} Jolie programming language includes a dynamic type system. Consequently, it allows avoidable run-time errors. A static type system for the language has been formally defined on paper but lacks an implementation yet. In this paper, we describe a prototype of Jolie Static Type Checker (JSTC), which employs a technique based on a SMT solver. We describe the theory behind and the implementation, and the process of static analysis. Comment: Modeling and Analysis of Information Systems, 201...|$|R
40|$|The formal {{specification}} and <b>verification</b> <b>of</b> real-time systems are difficult tasks, given the com-plexity {{of these systems}} and the safety-critical role they usually play. Timed Automata, and real-time model-checking, have emerged as powerful tools {{to deal with this}} problem. However, the specifica-tion of urgency in timed automata (essential in most models of interest) may inadvertently cause anomalous behaviours that undermine the <b>reliability</b> <b>of</b> formal <b>verification</b> methods (such as reach-ability analysis). Zeno runs denote executions which may be arbitrarily fast, i. e., executions where an infinite number of events may occur in a finite period of time. Timelocks denote states where no further divergent execution is possible; i. e., where time cannot pass beyond a certain bound. In general, the <b>verification</b> <b>of</b> safety and liveness properties may be meaningless in models where Zeno runs and timelocks may occur, hence the importance of methods to ensure that models are free from such anomalous behaviours. In previous work, we developed methods to detect Zeno runs and Zeno-timelocks (a particular kind of timelocks) in network of timed automata. Later stages of this analysis derived, from the network’s product automaton, reachability formulae that characterise the occurrence of Zeno runs and Zeno-timelocks. Although this simple reachability analysis has a number of advantages over liveness checks (as done in model-checkers such as Uppaal, Kronos and Red), the product automaton is prone to state explosion and so the analysis may not scale well. Here, we refine our previous results by showing that Zeno runs and Zeno timelocks can be characterised by reachability formulae derived from the network’s components, i. e., avoiding the product automaton construction. ...|$|R
40|$|In {{this paper}} the {{advantages}} <b>of</b> <b>verification</b> and validation to support changes {{of an existing}} PLC program are shown. The controller is defined using Signal Interpreted Petri Nets (SIPN) and verification and validation are performed using symbolic model-checking. The main focus {{of this paper is}} to show the process and the benefits <b>of</b> <b>verification</b> and validation for the <b>reliability</b> <b>of</b> the control algorithm when specified changes are to make. This is clarified by the example of a heating tank controller throughout the text. ...|$|R
25|$|Since OPEC {{started to}} set {{production}} quotas {{on the basis}} of reserves levels in the 1980s, many of its members have reported significant increases in their official reserves. There are doubts about the <b>reliability</b> <b>of</b> these estimates, which are not provided with any form <b>of</b> <b>verification</b> that meet external reporting standards. The following table illustrates these rises.|$|R
40|$|International audienceThe {{probabilistic}} model checking provides a precise formalism {{for the performance}} and <b>reliability</b> <b>verification</b> <b>of</b> telecommunication systems modeled by Markov chains. We study a queueing system similar to a Jackson network except that queues have a finite capacity. We propose to study in this paper (state and path) formulas from the Continuous Stochastic Logic (CSL), in order to verify performability properties. Unfortunately, transient and stationary analysis is very complex for multidimensional Markov processes. So we propose to use the stochastic comparisons {{in the sense of}} weak orderings to define bounding processes. Bounding processes are represented by independent M/M/ 1 queues for which transient and stationary distributions can be computed as the product of probability distributions of each queue. We use the increasing set method, and we develop an intuitive formalism based on events to establish weak stochastic comparison...|$|R
40|$|To {{improve the}} <b>reliability</b> <b>of</b> telephone-based speaker <b>verification</b> systems, channel com-pensation is indispensable. However, {{it is also}} {{important}} to ensure that the channel com-pensation algorithms in these systems surpress channel variations and enhance interspeaker distinction. This paper addresses this problem by a blind feature-based transformation ap-proach in which the transformation parameters are determined online without any a priori knowledge of channel characteristics. Specifically, a composite statistical model formed by the fusion of a speaker model and a background model is used to represent the characteristics of enrollment speech. Based on the difference between the claimant’s speech and the com-posite model, a stochastic matching type of approach is proposed to transform the claimant’s speech to a region close to the enrollment speech. Therefore, the algorithm can estimate the transformation online without the necessity of detecting the handset types. Experimental results based on the 2001 NIST evaluation set show that the proposed transformation ap-proach achieves significant improvement in both equal error rate and minimum detection cost as compared to cepstral mean subtraction, Znorm, and short-time Gaussianization...|$|R
40|$|<b>Reliability</b> {{analysis}} <b>of</b> {{structures for}} the purpose of code calibration or <b>reliability</b> <b>verification</b> <b>of</b> specific structures requires that the relevant failure modes are represented and analyzed. For structural timber, sustaining a lifelike load, two failure cases for each failure mode have to be considered. These two cases are: maximum load level exceeding load-carrying capacity, and damage accumulation (caused by the load and its duration) leading to failure. The effect of both load intensity and load duration on the capacity of timber has been an area of large interest over the last decades. Several research projects address this problem and a number of different damage models, based on experimental evidence, have been formulated to describe the phenomenon. Three damage models are analyzed in this study. In the present paper a method for probabilistically modeling the effect of load duration is presented. The method accounts for the overall imprecision of the damage models in comparison to experiments, the variability of the damage models parameters, and the dependency between the parameters. The method is consistent {{in such a way that}} i...|$|R
40|$|OBJECTIVE: To {{develop a}} {{protocol}} {{of care for}} patients with Intra-Aortic Balloon and validate the content of this protocol. METHODS: Study of quantitative and descriptive approach. The methodology followed three steps: development of the instrument; content validity and <b>reliability</b> <b>verification</b> <b>of</b> the protocol {{for the analysis of}} agreement between specialists with greater experience. The study included 48 specialists, including physicians and nurses experienced in patient care in use of balloon. Items considered valid achieved at least 75 % of consensus before the analysis of agreement between evaluators. RESULTS: We evaluated 36 items, of these, 20 were considered valid. The reliability was also verified, using consistency of the responses of more experienced evaluators. Among the items submitted to new statistical analysis by these evaluators, only two were considered valid. CONCLUSION: Based on the content validation, a protocol with 22 items concerning patient care without the use of intra-aortic balloon was developed...|$|R
