2081|1225|Public
25|$|SPSS: Included as {{an option}} in the <b>Regression</b> <b>function.</b>|$|E
5000|$|... {{is called}} the {{smoothing}} parameter because it controls {{the flexibility of the}} LOESS <b>regression</b> <b>function.</b> Large values of [...] produce the smoothest functions that wiggle the least in response to fluctuations in the data. The smaller [...] is, the closer the <b>regression</b> <b>function</b> will conform to the data. Using too small a value of the smoothing parameter is not desirable, however, since the <b>regression</b> <b>function</b> will eventually start to capture the random error in the data.|$|E
50|$|SPSS: Included as {{an option}} in the <b>Regression</b> <b>function.</b>|$|E
40|$|We {{propose the}} {{generalized}} profiling method {{to estimate the}} multiple <b>regression</b> <b>functions</b> {{in the framework of}} penalized spline smoothing, where the <b>regression</b> <b>functions</b> and the smoothing parameter are estimated in two nested levels of optimization. The corresponding gradients and Hessian matrices are worked out analytically, using the Implicit Function Theorem if necessary, which leads to fast and stable computation. Our main contribution is developing the modified delta method to estimate the variances of the <b>regression</b> <b>functions,</b> which include the uncertainty of the smoothing parameter estimates. We further develop adaptive penalized spline smoothing to estimate spatially heterogeneous <b>regression</b> <b>functions,</b> where the smoothing parameter is a function that changes along with the curvature of <b>regression</b> <b>functions.</b> The simulations and application show that the generalized profiling method leads to good estimates for the <b>regression</b> <b>functions</b> and their variances. ...|$|R
40|$|The {{non-reversibility}} of the <b>regression</b> <b>functions,</b> {{the simultaneous}} {{relations between the}} economic phenomena and the so-called paired <b>regression</b> <b>functions</b> are treated {{on the basis of}} cybernetic considerations. It is shown, that the treatment of this type of <b>regression</b> <b>functions</b> requires the existence of simultaneous relations and multicollinearity, which leads, in turn, to based results, when estimating them seperately {{on the basis of the}} classical Least-Squares-Method. Digitalizacja i deponowanie archiwalnych zeszyt√≥w RPEiS sfinansowane przez MNiSW w ramach realizacji umowy nr 541 /P-DUN/ 201...|$|R
40|$|This work {{is devoted}} to seeking methods for {{analysis}} of survival data with the Aalen model under special circumstances. We supposed, that all regres- sion functions and all covariates of the observed individuals were nonnegative and we named this class of models monotone Aalen models. To find estimators of the unknown <b>regression</b> <b>functions</b> we considered three maximum likelihood based approaches, namely the nonparametric maximum likelihood method, the Bayesian analysis using Beta processes as the priors for the unknown cumulative <b>regression</b> <b>functions</b> and the Bayesian analysis using a correlated prior approach, where the <b>regression</b> <b>functions</b> {{were supposed to be}} jump processes with a martingale structure. Powered by TCPDF (www. tcpdf. org...|$|R
5000|$|... where MSE(f) is {{the mean}} squared error of the <b>regression</b> <b>function</b> &fnof;.|$|E
50|$|Many {{techniques}} {{for carrying out}} regression analysis have been developed. Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the <b>regression</b> <b>function</b> is {{defined in terms of}} a finite number of unknown parameters that are estimated from the data. Nonparametric regression refers to techniques that allow the <b>regression</b> <b>function</b> to lie in a specified set of functions, which may be infinite-dimensional.|$|E
5000|$|The {{figure to}} the right shows the {{estimated}} <b>regression</b> <b>function</b> using a second order Gaussian kernel along with asymptotic variability bounds ...|$|E
40|$|Estimation of {{univariate}} <b>regression</b> <b>functions</b> from bounded i. i. d. data is considered. Estimates {{are defined}} by minimizing a complexity penalized residual sum of squares over all piecewise polynomials. The integrated squared error of these estimates achieves for piecewise p-smooth <b>regression</b> <b>functions</b> the rate (ln 2 (n) /n) 2 p/(2 p+ 1). Least squares Regression estimate Rate of convergence Complexity regularization...|$|R
40|$|This paper {{suggests}} an estimator {{of the number}} of jumps of the jump <b>regression</b> <b>functions.</b> The estimator is based on {{the difference between right and}} left onesided kernel smoothers. It is proved to be a. s. consistent. Some results about its rate of convergence are also provided. 1 Introduction Regression analysis is one of the most mature branches in statistics. For a long time, however, its main theory is about the continuous <b>regression</b> <b>functions</b> (c. f. Draper and Smith (1981), Hardle (1990), etc.). Recently, discontinuous <b>regression</b> <b>functions</b> have gotten more and more attention from statisticians all over the world. This, we think, is mainly due to their great application background (e. g. Wahba (1986) used the discontinuous regression model to explore the equi-temperature surfaces of the high sky and the deep ocean). By now, we have found that jump <b>regression</b> <b>functions</b> are discussed in two statistical fields. One is the change-point field, in which statisticians discuss the jump r [...] ...|$|R
40|$|Problems of {{regression}} smoothing and {{curve fitting}} are addressed via predictive infer-ence in a flexible class of mixture models. Multidimensional density estimation using Dirichlet mixture models provides the theoretical basis for semi-parametric regression methods in which fitted <b>regression</b> <b>functions</b> may be deduced as means of conditional predictive distributions. These Bayesian <b>regression</b> <b>functions</b> have features similar to gener-alised kernel regression estimates, but the formal analysis addresses problems of multivari-ate smoothing, parameter estimation, and {{the assessment of}} uncertainties about <b>regression</b> <b>functions</b> naturally. Computations are based on multidimensional versions of existing Markov chain simulation analysis of univariate Dirichlet mixture models. Some key words: Bayesian regression estimation; Dirichlet mixture model; Markov chain simulation; Multivariate density estimation; Smoothing. 1...|$|R
50|$|This {{tradeoff}} {{applies to}} all forms of supervised learning: classification, <b>regression</b> (<b>function</b> fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning.|$|E
5000|$|Outside machine learning, {{overfitting}} is also {{a problem}} in the broad study of regression, including regression done [...] "by hand". In the extreme case, if there are p variables in a linear regression with p data points, the fitted line will go exactly through every point. There is a variety of rules of thumb for the number of observations needed per independent variable, including 10 [...] and 10-15. In the process of regression model selection, the mean squared error of the random <b>regression</b> <b>function</b> can be split into random noise, approximation bias, and variance in the estimate of <b>regression</b> <b>function,</b> and bias-variance tradeoff is often used to overcome overfit models.|$|E
5000|$|Suppose we {{are given}} a <b>regression</b> <b>function</b> [...] {{yielding}} for each [...] an estimate [...] where [...] is the vector of the ith observations on all the explanatory variables. We define the fraction of variance unexplained (FVU) as: ...|$|E
5000|$|It uses {{univariate}} <b>regression</b> <b>functions</b> {{instead of}} their multivariate form, thus effectively dealing with the curse of dimensionality ...|$|R
40|$|International audiencen this paper, {{we address}} the problem of limited {{training}} sets for learning the <b>regression</b> <b>functions</b> in alternate analog test. Typically, a large volume of real data needs to be collected from different wafers and lots {{over a long period of}} time to be able to train the <b>regression</b> <b>functions</b> with accuracy across the whole design space and apply alternate test with high confidence. To avoid this delay and achieve a fast deployment of alternate test, we propose to use the Bayesian model fusion technique that leverages prior knowledge from simulation data and fuses this information with data from few real circuits to draw accurate <b>regression</b> <b>functions</b> across the whole design space. The technique is demonstrated for an alternate test designed for RF low noise amplifiers...|$|R
40|$|We {{study the}} drift of {{stationary}} diffusion processes {{in a time}} series analysis of the autoregression function. A marked empirical process measures {{the difference between the}} nonparametric <b>regression</b> <b>functions</b> of two time series. We bootstrap the distribution of a Kolmogorov-Smirnov-type test statistic for two hypotheses: Equality of <b>regression</b> <b>functions</b> and shifted <b>regression</b> <b>functions.</b> Neither markovian behavior nor Brownian motion error of the processes are assumed. A detailed simulation study finds the size of the new test near the nominal level and a good power for a variety of parametric models. The two-sample result serves to test for mean reversion of the diffusion drift in several examples. The interest rates Euribor, Libor as well as T-Bond yields do not show that stylized feature often modelled for interest rates. ...|$|R
50|$|Although {{polynomial}} regression {{is technically}} {{a special case}} of multiple linear regression, the interpretation of a fitted polynomial regression model requires a somewhat different perspective. It {{is often difficult to}} interpret the individual coefficients in a polynomial regression fit, since the underlying monomials can be highly correlated. For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). Although the correlation can be reduced by using orthogonal polynomials, it is generally more informative to consider the fitted <b>regression</b> <b>function</b> as a whole. Point-wise or simultaneous confidence bands can then be used to provide a sense of the uncertainty in the estimate of the <b>regression</b> <b>function.</b>|$|E
5000|$|Early-stopping {{can be used}} to {{regularize}} non-parametric regression problems {{encountered in}} machine learning. For a given input space, , output space, , and samples drawn from an unknown probability measure, , on , the goal of such problems is to approximate a <b>regression</b> <b>function,</b> , given by ...|$|E
50|$|In summary, {{to ensure}} {{efficient}} inference {{of the regression}} parameters and the <b>regression</b> <b>function,</b> the heteroscedasticity must be accounted for. Variance functions quantify {{the relationship between the}} variance and the mean of the observed data and hence {{play a significant role in}} regression estimation and inference.|$|E
3000|$|..., into SVR individually, two <b>regression</b> <b>functions</b> will be obtained. This {{procedure}} is termed the training phase in machine learning. Then when a radar return from an unknown wall is received, we put its feature vector into the two <b>regression</b> <b>functions</b> {{to obtain the}} estimates of the permittivity and {{the thickness of the}} wall, respectively, and this step is called the test phase. For this application, it also can be termed as an estimation phase.|$|R
40|$|In this dissertation, three example-based single-image super-resolution {{methods and}} a {{benchmark}} study are presented. The three super-resolution methods individually explore domain-specific, {{efficient and effective}} super-resolution solutions. The first method is developed for face images which contain domain-specific content. Test images are decomposed into facial components, edges, and smooth regions to develop adequate upsampling processes independently. Exemplar regions are exploited to transfer high-resolution details to reconstruct high-quality facial components. The second method is designed to generate super-resolution results efficiently for generic images. Multiple <b>regression</b> <b>functions</b> are trained to predict high-resolution patch features from low-resolution ones. By splitting the feature space into numerous subspaces and collecting sufficient exemplars for each subspace, the trained <b>regression</b> <b>functions</b> efficiently generate effective features to reconstruct high-resolution images. The third method integrates <b>regression</b> <b>functions</b> and patch exemplars to fully exploit exemplars to generate high-quality super-resolution images. As <b>regression</b> <b>functions</b> stably estimate high-resolution features and exemplar patches contain rich high-frequency signals, the proposed method uses <b>regression</b> <b>functions</b> to generate a robust intermediate high-resolution image and then finds effective exemplar patches to enrich the high-frequency signals. The benchmark study systematically compares the performance of state-of-the-art super-resolution methods under numerous parameter settings and test images. It investigates the effect of important parameters qualitatively and quantitatively and figures out the effectiveness of many metrics via human subject studies. In summary, this dissertation thoroughly and deeply investigates single-image super-resolution problems and propose solutions using exemplar images...|$|R
40|$|Associated kernels {{have been}} {{introduced}} to improve the classical continuous kernels for smoothing any functional on several kinds of supports such as bounded continuous and discrete sets. This work deals {{with the effects of}} combined associated kernels on nonparametric multiple <b>regression</b> <b>functions.</b> Using the Nadaraya-Watson estimator with optimal bandwidth matrices selected by cross-validation procedure, different behaviours of multiple regression estimations are pointed out according the type of multivariate associated kernels with correlation or not. Through simulation studies, there are no effect of correlation structures for the continuous <b>regression</b> <b>functions</b> and also for the associated continuous kernels; however, there exist really effects of the choice of multivariate associated kernels following the support of the multiple <b>regression</b> <b>functions</b> bounded continuous or discrete. Applications are made on two real datasets. Comment: 19 pages, 2 page...|$|R
50|$|Another {{disadvantage}} of LOESS {{is the fact}} that it does not produce a <b>regression</b> <b>function</b> that is easily represented by a mathematical formula. This can make it difficult to transfer the results of an analysis to other people. In order to transfer the <b>regression</b> <b>function</b> to another person, they would need the data set and software for LOESS calculations. In nonlinear regression, on the other hand, it is only necessary to write down a functional form in order to provide estimates of the unknown parameters and the estimated uncertainty. Depending on the application, this could be either a major or a minor drawback to using LOESS. In particular, the simple form of LOESS can not be used for mechanistic modelling where fitted parameters specify particular physical properties of a system.|$|E
5000|$|Conveniently, {{these models}} are all linear {{from the point}} of view of estimation, since the <b>regression</b> <b>function</b> is linear in terms of the unknown {{parameters}} Œ≤0, Œ≤1, .... Therefore, for least squares analysis, the computational and inferential problems of polynomial regression can be completely addressed using the techniques of multiple regression. This is done by treating x, x2, ... as being distinct independent variables in a multiple regression model.|$|E
50|$|In statistics, {{regression}} analysis is a statistical process for estimating {{the relationships among}} variables. It includes many techniques for modeling and analyzing several variables, when {{the focus is on}} the relationship between a dependent variable and one or more independent variables. More specifically, {{regression analysis}} helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables - that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a function of the independent variables called the <b>regression</b> <b>function.</b> In regression analysis, it is also of interest to characterize the variation of the dependent variable around the <b>regression</b> <b>function</b> which can be described by a probability distribution.|$|E
30|$|The history-based {{performance}} {{model is}} realized {{on the basis}} of knowledge about previously run jobs with a similar configuration and of parameter-specific <b>regression</b> <b>functions.</b> Therefore, the prototype implementation defines <b>regression</b> <b>functions</b> for a subset of parameters of R (resource) and A (application), which are considered in this approach. The prototype focuses on three specific parameters including the number of nodes allocated for a specific job n, the size of the database s, and the number of input files to be compared d and has been evaluated within a case study. Therein we retrieved accurate results on different computing resources [22]. Following this, the performance prediction is based on historical execution time and <b>regression</b> <b>functions.</b> While the approach could be easily extended with support for additional parameters, it is shown that considering only a subset of parameters can results in appropriate estimations. Different types of computing resources implicate changes in the runtime of the application. Therefore, the <b>regression</b> <b>functions</b> are not completely independent of the utilized computing resources (Cloud or cluster resources), but have to be adapted with a weighting factor regarding the allocated computing resources.|$|R
40|$|The paper {{presents}} a generalized regression technique {{centered on a}} superquantile (also called conditional value-at-risk) {{that is consistent with}} that coherent measure of risk and yields more conservatively fitted curves than classical least-squares and quantile regressions. In contrast to other generalized regression techniques that approximate conditional superquantiles by various combinations of conditional quantiles, we directly and in perfect analog to classical regression obtain superquantile <b>regression</b> <b>functions</b> as optimal solutions of certain error minimization problems. We show the existence and possible uniqueness of <b>regression</b> <b>functions,</b> discuss the stability of <b>regression</b> <b>functions</b> under perturbations and approximation of the underlying data, and propose an extension of the coefficient of determination R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the error minimization problems and illustrates the methodology in several numerical examples in the areas of uncertainty quantification, reliability engineering, and financial risk management. ...|$|R
40|$|Consistencies {{and rates}} of {{convergence}} of jump-penalized least squares estimators. (English summary) Ann. Statist. 37 (2009), no. 1, 157 ‚Äì 183. This paper studies the convergence of the jump-penalized least square estimators of the <b>regression</b> <b>functions.</b> The convergence rates of the estimators in the L 2 metric and the Skorokhod metric for the <b>regression</b> <b>functions,</b> and the convergence in the Hausdorff metric for the set of jump functions are both investigated. A simple data-driven parameter selection strategy is also introduced. The results can be extended to higher dimensions...|$|R
5000|$|LOESS, {{originally}} {{proposed by}} Cleveland (1979) and further developed by Cleveland and Devlin (1988), specifically denotes {{a method that}} {{is also known as}} locally weighted polynomial regression. At each point in the range of the data set a low-degree polynomial is fitted to a subset of the data, with explanatory variable values near the point whose response is being estimated. The polynomial is fitted using weighted least squares, giving more weight to points near the point whose response is being estimated and less weight to points further away. The value of the <b>regression</b> <b>function</b> for the point is then obtained by evaluating the local polynomial using the explanatory variable values for that data point. The LOESS fit is complete after <b>regression</b> <b>function</b> values have been computed for each of the [...] data points. Many of the details of this method, such as the degree of the polynomial model and the weights, are flexible. The range of choices for each part of the method and typical defaults are briefly discussed next.|$|E
5000|$|... where [...] is the {{conditional}} distribution at [...] induced by [...]One common choice for approximating the <b>regression</b> <b>function</b> {{is to use}} functions from a reproducing kernel Hilbert space. These spaces can be infinite dimensional, {{in which they can}} supply solutions that overfit training sets of arbitrary size. Regularization is, therefore, especially important for these methods. One way to regularize non-parametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent.|$|E
50|$|The {{residuals}} from a fitted {{model are}} {{the differences between}} the responses observed at each combination values of the explanatory variables and the corresponding prediction of the response computed using the <b>regression</b> <b>function.</b> Mathematically, the definition of the residual for the ith observation in the data set is writtenwith yi denoting the ith response in the data set and xi the vector of explanatory variables, each set at the corresponding values found in the ith observation in the data set.|$|E
40|$|The {{impact of}} {{covariate}} measurement error on quantile <b>regression</b> <b>functions</b> is investigated {{using a small}} variance approximation. The approximation shows how the error contaminated and error free quantile <b>regression</b> <b>functions</b> are related, a key factor being {{the distribution of the}} error free covariate. Exact calculations probe the accuracy of the approximation. The order of the approxiamtion error is unchanged if the error free covariate density is replaced by the error contaminated density. It is then possible to use the approximation to investigate the sensitivity of estimates to varying amounts of measurement error. ...|$|R
3000|$|... respectively. The {{instrumental}} variables {{outputted from}} these functions {{will be used}} to estimate the thickness of the wall later. For now, three <b>regression</b> <b>functions</b> have been trained and established.|$|R
50|$|In addition, use {{is made of}} the {{correlation}} coefficient of all data (Ra), the coefficient of determination or coefficient of explanation, confidence intervals of the <b>regression</b> <b>functions,</b> and Anova analysis.|$|R
