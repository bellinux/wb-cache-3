40|18|Public
50|$|The {{relevance}} feedback information {{needs to be}} interpolated with the original query to improve retrieval performance, such as the well-known <b>Rocchio</b> <b>Algorithm.</b>|$|E
50|$|When {{applied to}} text {{classification}} using tf*idf vectors to represent documents, the nearest centroid classifier {{is known as}} the Rocchio classifier because of its similarity to the <b>Rocchio</b> <b>algorithm</b> for relevance feedback.|$|E
50|$|Relevance {{information}} is utilized {{by using the}} contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query. Relevance feedback is often implemented using the <b>Rocchio</b> <b>Algorithm.</b>|$|E
40|$|Rocchio’s similarity-based {{relevance}} feedback algorithm, {{one of the}} most important query reformation methods in information retrieval, is essentially an adaptive supervised learning algorithm from examples. In practice, <b>Rocchio’s</b> <b>algorithm</b> often uses a fixed query updat-ing factor. When this is the case, we strengthen the linear Ω(n) lower bound obtained in [9] and prove that <b>Rocchio’s</b> <b>algorithm</b> makes Ω(k(n − k)) mistakes in searching for a col-lection of documents represented by a monotone disjunction of k relevant features over the n-dimensional binary vector space { 0, 1 } n, when the inner product similarity measure is used. A quadratic lower bound is obtained when k is linearly proportional to n. We also prove an O(k(n−k) 3) upper bound for <b>Rocchio’s</b> <b>algorithm</b> with the inner product similarity measure in searching for such a collection of documents with a constant query updating factor and a zero classification threshold...|$|R
40|$|Abstract: {{this paper}} {{contains}} {{an overview of}} basic formulations and approaches to text classification. This paper surveys the algorithms used in text categorization: handcrafted rules, decision trees, decision rules, on-line learning, linear classifier, <b>Rocchio’s</b> <b>algorithm,</b> k Nearest Neighbor (kNN), Support Vector Machines (SVM) ...|$|R
30|$|Pseudo-relevance {{feedback}} (PRF) is {{a popular}} method for improving IR effectiveness by using the top-k documents as pseudo-relevance set[18]. One of the best-performing PRF methods on top of BM 25 is an adoption of <b>Rocchio’s</b> <b>algorithm</b> presented in [16], which is able to provide state-of-the-art retrieval effectiveness on standard TREC test collections [16]. BM 25 with PRF is denoted as BM 25 _PRF in this paper.|$|R
5000|$|The <b>Rocchio</b> <b>algorithm</b> often {{fails to}} {{classify}} multimodal classes and relationships. For instance, {{the country of}} Burma was renamed to Myanmar in 1989. Therefore the two queries of [...] "Burma" [...] and [...] "Myanmar" [...] will appear much farther apart in the vector space model, though they both contain similar origins.|$|E
50|$|The {{algorithm}} has a loose {{relationship to}} the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or <b>Rocchio</b> <b>algorithm.</b>|$|E
50|$|Artificial {{imagination}} {{allows us}} to synthesize images {{and to develop a}} new image, whether it is in the database, regardless its existence in the real world. For example, the computer shows results that are based on the answer from the initial query. The user selects several relevant images, and then the technology analyzes these selections and reorganizes the images' ranks to fit the query. In this process, artificial imagination is used to synthesize the selected images and to improve the searching result with additional relevant synthesized images. This technique is based on several algorithms, including the <b>Rocchio</b> <b>algorithm</b> and the evolutionary algorithm. The <b>Rocchio</b> <b>algorithm,</b> locating a query point near relevant examples and far away from irrelevant examples, is simple and works well in a small system where the databases are arranged in certain ranks. The evolutionary synthesis is composed of two steps: a standard algorithm and an enhancement of the standard algorithm. Through feedback from the user, there would be additional images synthesized so as to be suited to what the user is looking for.|$|E
40|$|We {{propose a}} simple Bayesian network-based text classifier, {{which may be}} {{considered}} as a discriminative counterpart of the generative multinomial naive Bayes classifier. The method relies on the use of a fixed network topology with the arcs going form term nodes to class nodes, and also on a network parametrization based on noisy or gates. Comparative experiments of the proposed method with naive Bayes and <b>Rocchio</b> <b>algorithms</b> are carried out using three standard document collections...|$|R
40|$|Information Retrieval, the {{techniques}} of recovering relevant information from a data set, suffers from the term mismatch problem: the indexers and the users do often not use the same vocabulary. Query Expansion is a technique used to solve the term mismatch problem by adding terms from previously identified pieces of information to the original query. In existing IR systems based on the Vector Space Model, the terms in the expanded query are often weighted with <b>Rocchio's</b> <b>algorithm</b> for relevance feedback...|$|R
40|$|We {{describe}} {{a new family}} of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis {{of the family of}} algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters- 21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of <b>Rocchio's</b> <b>algorithm</b> and of the Perceptron algorithm...|$|R
5000|$|The <b>Rocchio</b> <b>algorithm</b> {{is based}} on a method of {{relevance}} feedback found in information retrieval systems which stemmed from the SMART Information Retrieval System which was developed 1960-1964. Like many other retrieval systems, the Rocchio feedback approach was developed using the Vector Space Model. The algorithm {{is based on}} the assumption that most users have a general conception of which documents should be denoted as relevant or non-relevant. [...] Therefore, the user's search query is revised to include an arbitrary percentage of relevant and non-relevant documents as a means of increasing the search engine's recall, and possibly the precision as well. The number of relevant and non-relevant documents allowed to enter a query is dictated by the weights of the a, b, c variables listed below in the Algorithm section.|$|E
40|$|Basic {{theory about}} text {{categorization}} and information retrieval is presented and several important algorithms for text classification are describe in details, {{such as the}} <b>Rocchio</b> <b>Algorithm,</b> TFIDF classifiers and NaÔve Byes Algorithm, etc. An implementation based on <b>Rocchio</b> <b>Algorithm</b> is also discussed and evaluated. It shows that this method is reasonably efficient given fairly small training datasets. However, {{in order to improve}} the performance of text classification algorithms and construct better ones, we should take into extended feature selection such as word sequences into consideration...|$|E
40|$|Information {{filtering}} (IF) systems usually filter {{data items}} by correlating {{a set of}} terms representing the user's interest (a user profile) with similar sets of terms representing the data items. Many techniques can be employed for constructing user profiles automatically, but they usually yield large sets of term. Various dimensionality-reduction techniques can be applied {{in order to reduce}} the number of terms in a user profile. We describe a new terms selection technique including a dimensionality-reduction mechanism which is based on the analysis of a trained artificial neural network (ANN) model. Its novel feature is the identification of an optimal set of terms that can classify correctly data items that are relevant to a user. The proposed technique was compared with the classical <b>Rocchio</b> <b>algorithm.</b> We found that when using all the distinct terms in the training set to train an ANN, the <b>Rocchio</b> <b>algorithm</b> outperforms the ANN based filtering system, but after applying the new dimensionality-reduction technique, leaving only an optimal set of terms, the improved ANN technique outperformed both the original ANN and the <b>Rocchio</b> <b>algorithm...</b>|$|E
40|$|This paper {{presents}} Strathclyde iSchool's (SiS) {{participation in}} the Technological Assisted Reviews in Empirical Medicine Task. For the ranking task, we explored two ways in which assistance to reviewers could be provided during the assessment process: (i) topic models, where we use Latent Dirichlet Allocation to identify topics within the set of retrieved documents, ranking documents by the topic {{most likely to be}} relevant and (ii) relevance feedback, where we use <b>Rocchio's</b> <b>algorithm</b> to update the query model for subsequent rounds of interaction. A third approach combines the topic and relevance feedback to quickly identify the relevant abstracts. For the thresholding task, we apply a score threshold, and exclude documents which did not exceed the threshold given BM 25...|$|R
40|$|Rocchio's similarity-based {{relevance}} feedback algorithm, {{one of the}} most important query reformation methods in information retrieval, is essentially an adaptive learning algorithm from examples in searching for documents represented by a linear classifier. In spite of its popularity in various applications there is little rigorous analysis of its learning complexity in literature. In this paper, we prove for the first time that 2 the learning complexity of <b>Rocchio's</b> <b>algorithm</b> is O (d + d (log d + log n)) over the discretized vector d space { 0, L, n − 1 }, when the inner product similarity measure is used. The upper bound on the learning complexity for searching for documents represented by a monotone linear classifier (q, 0) ...|$|R
40|$|This paper {{presents}} {{the design and}} evaluation of a text categorization method based on the Hierarchical Mixture of Experts model. This model uses a divide and conquer principle to dene smaller categorization problems based on a predened hierarchical structure. The nal classier is a hierarchical array of neural networks. The method is evaluated using the UMLS Metathesaurus as the underlying hierarchical structure, and the OHSUMED test set of MEDLINE records. Comparisons with traditional <b>Rocchio's</b> <b>algorithm</b> adapted for text categorization, {{as well as at}} neural network classi- ers are provided. The results show that the use of the hierarchical structure improves text categorization performance signicantly. 1 Introduction Text categorization, also known as automatic indexing, is the process of algorithmically analyzing an electronic document to assign a set of categories (or index terms) that succinctly describe the content of the document. This assignment can be used for classic [...] ...|$|R
40|$|This work {{investigates the}} {{possible}} applications of functional clustering algorithms in relevance feedback algorithms for interactive search. A {{modified version of}} the <b>Rocchio</b> <b>algorithm</b> for relevance feedback is proposed based off the result of an LGA clustering of the data. The effectiveness of this algorithm is assessed based on simulations using a tiered probabilistic model for user feedback. It is found that the for a poor initial query (one that is far from the users target query in a vector space model), the simulations run with the modified algorithm reach a close vicinity of the target search faster than the standard <b>Rocchio</b> <b>algorithm</b> even when the increased distance between the current query and the options presented is accounted for. A strong inverse relationship is observed between the strength of the linear structures in the data (measured based on the gap statistic of the data set clustered under LGA) and the improvement in search refinement. Data with gap statistic of 0 offers similar performance to the standard <b>Rocchio</b> <b>algorithm</b> once corrected for the increased distance, suggesting that exploiting linear structures in the data can offer more efficient searching...|$|E
40|$|Abstract. Traditional text {{classification}} algorithms have vital {{impact on}} information filtering. However, their performances {{were confined to}} a large extent in terms of the massive data set. This paper proposes an approach using MapReduce-based Rocchio relevance feedback algorithm, which improved the traditional <b>Rocchio</b> <b>algorithm</b> in the MapReduce paradigm, to resolve the problem of massive information filtering. The experiments on Hadoop cluster showed an effective improvement in performance by using the new method...|$|E
40|$|Systems for text retrieval, routing, {{categorization}} {{and other}} IR tasks {{rely heavily on}} linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG {{to be more effective}} than the widely used <b>Rocchio</b> <b>algorithm</b> on several categorization and routing tasks. ...|$|E
40|$|Rocchio’s similarity-based {{relevance}} feedback algorithm, {{one of the}} most important query reformation methods in information retrieval, is essentially an adaptive learning algorithm from examples in searching for documents represented by a linear classifier. Despite its popularity in various applications, there is little rigorous analysis of its learning complexity in literature. In this article, the authors prove for the first time that the learning complexity of <b>Rocchio’s</b> <b>algorithm</b> is O(d � d 2 (log d � log n)) over the discretized vector space { 0, …, n � 1 } d, when the inner product similarity measure is used. The upper bound on the learning complexity for searching for documents represented by a monotone linear classifier () over { 0, …, n � 1 } d can be improved to, at most, 1 � 2 k (n � 1) (log d � log(n � 1)), where k is the number o...|$|R
40|$|International audienceThis paper {{presents}} an original approach to modelling user's information need in text filtering environment. This approach {{relies on a}} specific novelty detection model which allows both accurate learning of user's profile and evaluation of the coherency of user's behaviour during his interaction with the system. Thanks to an online learning algorithm, the novelty detection model is also able to track changes in user's interests over time. The proposed approach has been successfully tested on the Reuters- 21578 benchmark. The experimental results prove that this approach significantly outperforms the well-known <b>Rocchio's</b> learning <b>algorithm...</b>|$|R
40|$|International audienceInformation {{filtering}} {{is one of}} {{the most}} useful and challenging tasks for effective information access. It is concerned with dynamically adapting the distribution of information where both evolving user's interests and new incoming information are taken into account. In this paper, we present an innovative approach to text filtering based on the novelty detection principle. This approach relies on a specific learning model which allows both accurate online learning of user's profile and evaluation of the coherency of user's behaviour during his interaction with the system. We empirically analyse our approach and present experimental results on the Reuters- 21578 benchmark. The obtained results bring out a significant enhancement of performance as compared to the widely used <b>Rocchio's</b> learning <b>algorithm...</b>|$|R
40|$|We {{introduce}} multiple topic tracking (MTT) for iScore {{to better}} recommend news articles for users with multiple interests and to address changes in user interests over time. As {{an extension of}} the basic <b>Rocchio</b> <b>algorithm,</b> traditional topic detection and tracking, and single-pass clustering, MTT maintains multiple interest profiles to identify interesting articles for a specific user given user-feedback. Focusing on only interesting topics enables iScore to discard useless profiles to address changes in user interests and to achieve a balance between resource consumption and classification accuracy. Also by relating a topic’s interestingness to an article’s interestingness, iScore is able to achieve higher quality results than traditional methods such as the <b>Rocchio</b> <b>algorithm.</b> We identify several operating parameters that work well for MTT. Using the same parameters, we show that MTT alone yields high quality results for recommending interesting articles from several corpora. The inclusion of MTT improves iScore’s performance by 9 % in recommending news articles from the Yahoo! News RSS feeds and the TREC 11 adaptive filter article collection. And through a small user study, we show that iScore can still perform well when only provided with little user feedback...|$|E
40|$|Abstract. Contextual {{retrieval}} supports differences amongst {{users in}} their information seeking requests. The Web, {{which is very}} dynamic and nearly universally accessible, is {{an environment in which}} it is increasingly difficult for users to find documents that satisfy their specific information needs. This problem is amplified as users tend to use short queries. Contextual retrieval attempts to address this problem by incorporating knowledge about the user and past retrieval results in the search process. In this paper we explore a feedback technique based on the <b>Rocchio</b> <b>algorithm</b> that significantly reduces demands on the user while maintaining comparable performance on the Reuters- 21578 corpus. ...|$|E
40|$|Exploring digital {{collections}} to {{find information}} {{relevant to a}} user's interests is a challenging task. Algorithms designed to solve this relevant information problem base their relevance computations on user profiles in which representations of the users' interests are maintained. This article presents a new method, based on the classic <b>Rocchio</b> <b>algorithm</b> for text categorization, able to discover user preferences from the analysis of textual descriptions of items in online catalog of e-commerce Web sites. Experiments {{have been carried out}} on several data sets, and results have been compared with those obtained using an inductive logic programming (ILP) approach and a probabilistic one...|$|E
40|$|A {{probabilistic}} {{analysis of}} the <b>Rocchio</b> relevance feedback <b>algorithm,</b> {{one of the most}} popular learning methods from information retrieval, is presented in a text categorization framework. The analysis results in a probabilistic version of the Rocchio classifier and offers an explanation for the TFIDF word weighting heuristic. The Rocchio classifier, its probabilistic variant and a standard naive Bayes classifier are compared on three text categorization tasks. The results suggest that the probabilistic algorithms are preferable to the heuristic Rocchio classifier...|$|R
40|$|For {{the present}} write-up, {{it is assumed}} that the reader is {{familiar}} with the SMART project, originally implemented on an IBM 709 *+ at Harvard [l, 2]. When this information retrieval system was rewritten for a CDC l 66 k at Cornell, the programs available at Harvard could not be used directly. At that time, several design decisions for the Cornell system were made: a. To continue to run the text analysis programs at Harvard [2], and to rewrite only those programs dealing with document abstracts and queries as numeric vectors; b. to concentrate on two major areas at Cornell (the implementation of <b>Rocchio's</b> clustering <b>algorithm</b> [3] and of various relevance feedback algorithms [3 *^ 3) J c. to write these programs to handle collections as large as the 1 ^ 00 document, 225 query Cranfield collection; d. to make the resulting system available for use and fo...|$|R
40|$|We {{evaluate}} {{three different}} relevance feedback (RF) <b>algorithms,</b> <b>Rocchio,</b> Robertson/Sparck-Jones (RSJ) and Bayesian, {{in the context}} of Web search. We use a target-testing experimental procedure whereby a user must locate a specific document. For user relevance feedback, we consider all possible user choices of indicating zero or more relevant documents from a set of 10 displayed documents. Examination of the effects of each user choice permits us to compute an upper-bound on the performance of each RF algorithm. We find that there is a significant variation in the upper-bound performance of the three RF algorithms and that the Bayesian algorithm approaches the best possible...|$|R
40|$|We {{used the}} YFILTER {{filtering}} system for experiments on updating profiles and setting thresholds. We {{developed a new}} method of using language models for updating profiles that is more focused on picking informative/discriminative words for query. The new method was compared with the well-known <b>Rocchio</b> <b>algorithm.</b> Dissemination thresholds were set based on maximum likelihood estimation that models and compensates for the sampling bias inherent in adaptive filtering. Our experimental results suggest that using what kind of distribution to model the scores of relevant and non- relevant documents is corpus dependant. The experimental results also show the sampling bias problem of training data while filtering makes the final profile learned biased. 1...|$|E
40|$|Abstract. Exploring digital {{collections}} to {{find information}} {{relevant to a}} user’s interests is a challenging task. Algorithms designed to solve this relevant information problem base their relevance computations on user profiles in which representations of the users ’ interests are maintained. This paper presents a new method, based on the classical <b>Rocchio</b> <b>algorithm</b> for text categorization, able to discover user preferences from the analysis of textual descriptions of items in online catalogues of e-commerce Web sites. Experiments {{have been carried out}} on a dataset of real users, and results have been compared with those obtained using an Inductive Logic Programming (ILP) approach and a probabilistic one. ...|$|E
40|$|In {{the text}} literature, many topic models were {{proposed}} to represent documents and words as topics or latent topics {{in order to}} process text effectively and accurately. In this paper, we propose LDACLM or Latent Dirichlet Allocation Category Language Model for text categorization and estimate parameters of models by variational inference. As a variant of Latent Dirichlet Allocation Model, LDACLM regards documents of category as Language Model and uses variational parameters to estimate maximum a pos-teriori of terms. In general, experiments show LDACLM model is effective and outperform Naı̈ve Bayes with Laplace smoothing and <b>Rocchio</b> <b>algorithm</b> but little inferior to SVM for text categorization...|$|E
40|$|A {{probabilistic}} {{analysis of}} the <b>Rocchio</b> relevance feedback <b>algorithm,</b> {{one of the most}} popular learning methods from information retrieval, is presented in a text categorization framework. The analysis results in a probabilistic version of the Rocchio classifier and offers an explanation for the TFIDF word weighting heuristic. The Rocchio classifier, its probabilistic variant and a standard naive Bayes classifier are compared on three text categorization tasks. The results suggest that the probabilistic algorithms are preferable to the heuristic Rocchio classifier. This research is sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant F 33615 - 93 - 1 - 1330. The US Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation thereon. Views and conclusions contained in this document are those of the authors and should not be [...] ...|$|R
40|$|Information Retrieval is an {{important}} albeit imperfect component of information technologies. A problem of insufficient diversity of retrieved documents {{is one of the}} primary issues studied in this research. This study shows that this problem leads to a decrease of precision and recall, traditional measures of information retrieval effectiveness. This thesis presents an adaptive IR system based on the theory of adaptive dual control. The aim of the approach is the optimization of retrieval precision after all feedback has been issued. This is done by increasing the diversity of retrieved documents. This study shows that the value of recall reflects this diversity. The Probability Ranking Principle is viewed in the literature as the “bedrock” of current probabilistic Information Retrieval theory. Neither the proposed approach nor other methods of diversification of retrieved documents from the literature conform to this principle. This study shows by counterexample that the Probability Ranking Principle does not in general lead to optimal precision in a search session with feedback (for which {{it may not have been}} designed but is actively used). Retrieval precision of the search session should be optimized with a multistage stochastic programming model to accomplish the aim. However, such models are computationally intractable. Therefore, approximate linear multistage stochastic programming models are derived in this study, where the multistage improvement of the probability distribution is modelled using the proposed feedback correctness method. The proposed optimization models are based on several assumptions, starting with the assumption that Information Retrieval is conducted in units of topics. The use of clusters is the primary reasons why a new method of probability estimation is proposed. The adaptive dual control of topic-based IR system was evaluated in a series of experiments conducted on the Reuters, Wikipedia and TREC collections of documents. The Wikipedia experiment revealed that the dual control feedback mechanism improves precision and S-recall when all the underlying assumptions are satisfied. In the TREC experiment, this feedback mechanism was compared to a state-of-the-art adaptive IR system based on BM- 25 term weighting and the <b>Rocchio</b> relevance feedback <b>algorithm.</b> The baseline system exhibited better effectiveness than the cluster-based optimization model of ADTIR. The main reason for this was insufficient quality of the generated clusters in the TREC collection that violated the underlying assumption...|$|R
40|$|The Rocchio {{relevance}} feedback algorithm {{is one of}} the most popular and widely applied learning methods from information retrieval. Here, a probabilistic analysis of this algorithm is presented in a text categorization framework. The analysis gives theoretical insight into the heuristics used in the <b>Rocchio</b> <b>algorithm,</b> particularly the word weighting scheme and the similarity metric. It also suggests improvements which lead to a probabilistic variant of the Rocchio classi er. The Rocchio classi er, its probabilistic variant, and a naive Bayes classi er are compared on six text categorization tasks. The results show that the probabilistic algorithms are preferable to the heuristic Rocchio classi er not only because they are more well-founded, but also because they achieve better performance. ...|$|E
