960|1760|Public
2500|$|In 1980 Jensen {{published}} a detailed book {{in defense of}} the tests used to measure mental abilities, entitled Bias in Mental Testing. Reviewing this book, psychologist Kenneth Kaye endorsed Jensen's distinction between bias and discrimination. The purpose of tests is to discriminate (that is, reveal actual differences) on the basis of ability; bias constitutes error. Jensen defined any test as biased for a particular group if that group differs significantly from the majority group in the slopes, intercepts, or standard error of the estimates of their <b>regression</b> <b>lines.</b> Most studies found no difference in the <b>regression</b> <b>lines</b> between black and white groups, but those differences that had been found to be biased had overpredicted rather than underpredicted the minority group's performance (for example, grades in Officer Candidate courses). Jensen's conclusion: ...|$|E
2500|$|The {{concept of}} fractal {{dimension}} {{described in this}} article is a basic view of a complicated construct. The examples discussed here were chosen for clarity, and the scaling unit and ratios were known ahead of time. In practice, however, fractal dimensions can be determined using techniques that approximate scaling and detail from limits estimated from <b>regression</b> <b>lines</b> over log vs log plots of size vs scale. Several formal mathematical definitions of different types of fractal dimension are listed below. Although for some classic fractals all these dimensions coincide, in general they are not equivalent: ...|$|E
50|$|The {{slopes of}} the {{different}} <b>regression</b> <b>lines</b> should be equivalent, i.e., <b>regression</b> <b>lines</b> should be parallel among groups.|$|E
3000|$|... where β′ is {{the angle}} of the <b>regression</b> <b>line</b> and b′ is the {{intercept}} of the <b>regression</b> <b>line</b> with the y-axis.|$|R
30|$|For each subject, the first-order <b>regression</b> <b>line</b> was {{computed}} from nine data points for %Rec and %Det during fatiguing contractions of the bilateral MG. The {{rate of change}} in %Rec and %Det {{was defined as the}} slope of the <b>regression</b> <b>line</b> and the initial value as the intercept of the <b>regression</b> <b>line.</b>|$|R
50|$|For example, when {{estimating}} the population mean, this method uses the sample mean; {{to estimate the}} population median, it uses the sample median; to estimate the population <b>regression</b> <b>line,</b> it uses the sample <b>regression</b> <b>line.</b>|$|R
5000|$|Function of {{the angle}} between two {{standardized}} <b>regression</b> <b>lines</b> ...|$|E
50|$|Statistical graphs {{can also}} be generated: bar graphs, line graphs, normal {{distribution}} curves, <b>regression</b> <b>lines.</b>|$|E
5000|$|... #Caption: Comparison of the Theil-Sen {{estimator}} (black) {{and simple}} linear regression (blue) {{for a set of}} points with outliers. Because of the many outliers, neither of the <b>regression</b> <b>lines</b> fits the data well, as measured by the fact that neither gives a very high R2.|$|E
40|$|FIGURE 2. Relationship between {{arm width}} at {{half of it}} (b) and disk radious (r). Empty square: Leptychaster kerguelenensis type. Empty circle: L. mendosus type. Dotted line: <b>regression</b> <b>line</b> fitted to SW Atlantic specimens. Straight line: <b>regression</b> <b>line</b> fitted to Kerguelen and Marion islands specimens...|$|R
30|$|This is the {{estimated}} the <b>regression</b> <b>line.</b>|$|R
30|$|The {{error of}} k, i.e. the {{uncertainty}} of the slope of the <b>regression</b> <b>line,</b> is, due to the number of data points, smaller than the least significant specified digit and therefore negligible. However, the standard deviation of the individual measurements from the <b>regression</b> <b>line</b> is 0.09 μm, corresponding to 2.1 μN.|$|R
5000|$|In 1980 Jensen {{published}} a detailed book {{in defense of}} the tests used to measure mental abilities, entitled Bias in Mental Testing. Reviewing this book, psychologist Kenneth Kaye endorsed Jensens distinction between bias and discrimination. The purpose of tests is to discriminate (that is, reveal actual differences) on the basis of ability; bias constitutes error. Jensen defined any test as biased for a particular group if that group differs significantly from the majority group in the slopes, intercepts, or standard error of the estimates of their <b>regression</b> <b>lines.</b> Most studies found no difference in the <b>regression</b> <b>lines</b> between black and white groups, but those differences that had been found to be biased had overpredicted rather than underpredicted the minority groups performance (for example, grades in Officer Candidate courses). Jensens conclusion: ...|$|E
50|$|Refer to {{the figure}} on the right {{plotting}} the heat capacity {{as a function of}} temperature. In this context, Tg is the temperature corresponding to point A on the curve. The linear sections below and above Tg are colored green. Tg is the temperature at the intersection of the red <b>regression</b> <b>lines.</b>|$|E
5000|$|The {{least squares}} method applied {{separately}} to each segment, by which the two <b>regression</b> <b>lines</b> are made to fit the data set {{as closely as possible}} while minimizing the sum of squares of the differences (SSD) between observed (y) and calculated (Yr) values of the dependent variable, results in the following two equations: ...|$|E
25|$|R2 is a {{statistic}} {{that will give}} some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is {{a statistic}}al measure of how well the <b>regression</b> <b>line</b> approximates the real data points. An R2 of 1 indicates that the <b>regression</b> <b>line</b> perfectly fits the data.|$|R
2500|$|... (the {{point on}} the <b>regression</b> <b>line),</b> while the actual {{response}} would be ...|$|R
25|$|A test {{of whether}} the slope of a <b>regression</b> <b>line</b> differs {{significantly}} from 0.|$|R
50|$|SegReg recognizes {{many types}} of {{relations}} and selects the ultimate type {{on the basis of}} statistical criteria like the significance of the regression coefficients. The SegReg output provides statistical confidence belts of the <b>regression</b> <b>lines</b> and a confidence block for the breakpoint. The confidence level can be selected as 90%, 95% and 98% of certainty.|$|E
50|$|The {{breakpoint}} {{is found}} numerically by adopting a series tentative breakpoints and performing a linear regression at {{both sides of}} them. The tentative breakpoint that provides the largest coefficient of determination (as a parameter for the fit of the <b>regression</b> <b>lines</b> to the observed data values) is selected as the true breakpoint. To assure that the lines at {{both sides of the}} breakpoint intersect each other exactly at the breakpoint, SegReg employs two methods and selects the method giving the best fit.|$|E
50|$|The BUPA liver {{data have}} been studied by various authors, {{including}} Breiman (2001). The data can be found via the classic data sets page {{and there is some}} discussion in the article on the Box-Cox transformation. A plot of the logs of ALT versus the logs of γGT appears below. The two <b>regression</b> <b>lines</b> are those estimated by ordinary least squares (OLS) and by robust MM-estimation. The analysis was performed in R using software made available by Venables and Ripley (2002).|$|E
50|$|A {{trigonometric}} {{representation of}} the orthogonal <b>regression</b> <b>line</b> was given by Coolidge in 1913.|$|R
5000|$|A test {{of whether}} the slope of a <b>regression</b> <b>line</b> differs {{significantly}} from 0.|$|R
50|$|This {{shows the}} role rxy {{plays in the}} <b>regression</b> <b>line</b> of {{standardized}} data points.|$|R
50|$|For uncentered data, {{there is}} a {{relation}} between the correlation coefficient and the angle &phi; between the two <b>regression</b> <b>lines,</b> y = gx(x) and x = gy(y), obtained by regressing y on x and x on y respectively. (Here &phi; is measured within the first quadrant formed around the lines' intersection point if r > 0, or counterclockwise from the fourth to the second quadrant if r < 0.) One can show that if the standard deviations are equal, then r = sec &phi; − tan &phi;, where sec and tan are trigonometric functions.|$|E
50|$|The two <b>regression</b> <b>lines</b> {{appear to}} be very similar (and this is not unusual in a data set of this size). However, the {{advantage}} of the robust approach comes to light when the estimates of residual scale are considered. For ordinary least squares, the estimate of scale is 0.420, compared to 0.373 for the robust method. Thus, the relative efficiency of ordinary least squares to MM-estimation in this example is 1.266. This inefficiency leads to loss of power in hypothesis tests, and to unnecessarily wide confidence intervals on estimated parameters.|$|E
5000|$|The {{concept of}} fractal {{dimension}} {{described in this}} article is a basic view of a complicated construct. The examples discussed here were chosen for clarity, and the scaling unit and ratios were known ahead of time. In practice, however, fractal dimensions can be determined using techniques that approximate scaling and detail from limits estimated from <b>regression</b> <b>lines</b> over log vs log plots of size vs scale. Several formal mathematical definitions of different types of fractal dimension are listed below. Although for some classic fractals all these dimensions coincide, in general they are not equivalent: ...|$|E
5000|$|... is {{estimated}} as {{the slope of}} the <b>regression</b> <b>line</b> for [...] versus [...] where: ...|$|R
30|$|In regression, the R 2 [*]coefficient of {{determination}} is a statistical {{measure of how}} well the <b>regression</b> <b>line</b> approximates the real data points. An R 2 [*]of 1 indicates that the <b>regression</b> <b>line</b> perfectly fits the data. The adjusted R 2 [*]is almost the same as R 2, but it penalizes the statistic as extra variables {{are included in the}} model.|$|R
3000|$|Second, the {{intercept}} of the <b>regression</b> <b>line</b> approximates {{the average}} apparent volume of distribution, [...]...|$|R
5000|$|... #Caption: Illustration of {{regression}} dilution (or attenuation bias) by a {{range of}} regression estimates in Errors-in-variables models. Two <b>regression</b> <b>lines</b> (red) bound the range of linear regression possibilities. The shallow slope is obtained when the independent variable (or predictor) is on the abscissa (x-axis). The steeper slope is obtained when the independent variable is on the ordinate (y-axis). By convention, with the independent variable on the x-axis, the shallower slope is obtained. Green reference lines are averages within arbitrary bins along each axis. Note that the steeper green and red regression estimates are more consistent with smaller errors in the y-axis variable.|$|E
50|$|During the 1950s, Hans Ericsson (Professor of {{microbiology}} at the Karolinska Hospital and Karolinska Institute, Stockholm), {{the scientific}} founder of AB BIODISK, developed {{a method to}} standardize the disc diffusion method and to improve its reproducibility and reliability for clinical susceptibility predictions. The inhibition zone sizes from disc test results were compared to Minimum Inhibitory Concentration (MIC) values based on the reference agar dilution procedure. The correlation between zone sizes and MIC values was then assessed using regression analysis and <b>regression</b> <b>lines</b> were used for extrapolating zone interpretive limits that corresponded to the MIC breakpoint values that defined susceptible, intermediate and resistant categorical results.|$|E
50|$|The {{attached}} figure {{concerns the}} same data {{as shown in the}} blue graph in the infobox {{at the top of this}} page. Here, the wheat crop has a tolerance for soil salinity up to the level of EC=7.1 dS/m instead of 4.6 in the blue figure. However, the fit of the data beyond the threshold is not as well as in the blue figure that has been made using the principle of minimization of the sum of squares of devations of the observed values from the <b>regression</b> <b>lines</b> over the whole domain of explanatory variable X (i.e. maximization of the coefficient of determination), while the partial regression is designed only to find the point where the horizontal trend changes into a sloping trend.|$|E
5000|$|The {{control limit}} lines are {{parallel}} to the <b>regression</b> <b>line</b> rather than the horizontal line.|$|R
30|$|Sa {{reflects}} {{the standard deviation}} of the <b>regression</b> <b>line’s</b> intercept, b quantifies the calibration curve’s slope.|$|R
5000|$|AbsScale: {{assess the}} {{deviations}} of points around the <b>regression</b> <b>line,</b> particularly around the last data points ...|$|R
