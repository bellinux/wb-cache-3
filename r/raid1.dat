13|0|Public
25|$|Mirroring, {{the other}} ZFS RAID option, is {{essentially}} the same as <b>RAID1,</b> allowing any number of disks to be mirrored. Like RAID 1 it also allows faster read and resilver/rebuild speeds since all drives can be used simultaneously and parity data is not calculated separately, and mirrored vdevs can be split to create identical copies of the pool.|$|E
2500|$|For RAID subsystems, data {{integrity}} and fault-tolerance requirements {{also reduce the}} realized capacity. For example, a <b>RAID1</b> array has about half the total capacity {{as a result of}} data mirroring, while a RAID5 array with [...] drives loses [...] of capacity (which equals to the capacity of a single drive) due to storing parity information. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault tolerance. Most RAID vendors use checksums to improve {{data integrity}} at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes, or by using separate 512-byte sectors for the checksum data.|$|E
5000|$|... geom_ccd (legacy volume {{manager with}} RAID0 and {{rudimentary}} <b>RAID1</b> support) ...|$|E
5000|$|Because of {{the very}} nature of <b>RAID1,</b> both disks will be {{subjected}} to the same workload and very closely similar access patterns, stressing them in the same way.|$|E
5000|$|But even {{so there}} can be many common modes: {{consider}} a <b>RAID1</b> where two disks are purchased online and are installed in a computer, {{there can be}} many common modes: ...|$|E
50|$|Storage Spaces is {{a storage}} {{virtualization}} technology which succeeds Logical Disk Manager {{and allows the}} organization of physical disks into logical volumes similar to Logical Volume Manager (Linux), <b>RAID1</b> or RAID5, but at a higher abstraction level.|$|E
50|$|HAST-provided devices appear like disk {{devices in}} the /dev/hast/ {{directory}} in FreeBSD, {{and can be}} used like standard block devices. HAST is similar to a <b>RAID1</b> (mirror) where each RAID component is provided across the network by one cluster node.|$|E
5000|$|The {{latest version}} of Exalogic compute nodes have two Intel E5-2699v3 2.3 GHz Xeon (18-core) {{processors}} and eight 32 GB DDR4 2133 MHz RAM {{for a total of}} 256 GB per node. Two 400 GB SSDs (<b>RAID1)</b> and redundant power supplies ...|$|E
50|$|Since {{support for}} MD {{is found in}} the kernel, there is an issue with using it before the kernel is running. Specifically it will not be present if the boot loader is either (e)LiLo or GRUB legacy. It may not be present for GRUB 2. In order to {{circumvent}} this problem a /boot filesystem must be used either without md support, or else with <b>RAID1.</b> In the latter case the system will boot by treating the <b>RAID1</b> device as a normal filesystem, and once the system is running it can be remounted as md and the second disk added to it. This will result in a catch-up, but /boot filesystems are usually small.|$|E
50|$|Disadvantages include slower write {{performance}} {{than a single}} disk and bottlenecks when multiple drives are written concurrently. However, unRAID allows support of a cache pool which can dramatically speed up the write performance. Cache pool data can be temporarily protected {{in a manner similar}} to <b>RAID1</b> until unRAID moves it to the array based on a schedule set within the software.|$|E
50|$|Normally, PEs simply map {{one-to-one}} to logical extents (LEs). With mirroring, multiple PEs map to each LE. These PEs {{are drawn}} from a physical volume group (PVG), a set of same-sized PVs which act similarly to hard disks in a <b>RAID1</b> array. PVGs are usually laid out so that they reside on different disks or data buses for maximum redundancy.|$|E
50|$|GEOM is {{the main}} storage {{framework}} for the FreeBSD operating system. It is available in FreeBSD 5.0 and higher and provides a standardized way to access storage layers. GEOM is modular and allows for geom modules to connect to the framework. For example, the geom_mirror module will provide <b>RAID1</b> or mirroring functionality to the system. A wide range of modules are already available, and new ones are always in active development by various FreeBSD developers.|$|E
50|$|NetWare 2.x {{implemented}} {{a number of}} features inspired by mainframe and minicomputer systems that were not available in other operating systems of the day. The System Fault Tolerance (SFT) features included standard read-after-write verification (SFT-I) with on-the-fly bad block re-mapping (at the time, disks did not have that feature built in) and software <b>RAID1</b> (disk mirroring, SFT-II). The Transaction Tracking System (TTS) optionally protected files against incomplete updates. For single files, this required only a file attribute to be set. Transactions over multiple files and controlled roll-backs were possible by programming to the TTS API.|$|E

