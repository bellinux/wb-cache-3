206|30|Public
2500|$|A freely {{available}} tool called packJPG {{is based}} on the 2007 paper [...] "Improved <b>Redundancy</b> <b>Reduction</b> for JPEG Files." ...|$|E
50|$|Infomax, in its zero-noise limit, {{is related}} to the {{principle}} of <b>redundancy</b> <b>reduction</b> proposed for biological sensory processing by Horace Barlow in 1961, and applied quantitatively to retinal processing by Atick and Redlich.|$|E
50|$|Today, {{nearly all}} {{commonly}} used video compression methods (e.g., those in standards {{approved by the}} ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial <b>redundancy</b> <b>reduction.</b> The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit {{and the use of}} a discrete wavelet transform (DWT) {{have been the subject of}} some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.|$|E
30|$|Band {{selection}} and sample covariance matrix estimation {{are presented in}} this article to carry out spatial and spectral <b>redundancy</b> information <b>reduction.</b>|$|R
50|$|A {{series of}} cost cutting {{measures}} followed including closure of its London office, <b>redundancies</b> and wage <b>reduction.</b> In early 1851 the EUR directors discussed leasing the operation to the ECR and a proposal was made but {{rejected by the}} ECR.|$|R
40|$|Classical {{transform}} based coders performed excellently {{in reducing}} statistical <b>redundancies.</b> However, <b>reduction</b> of statistical <b>redundancies</b> {{does not always}} correspond to the <b>reduction</b> of psychovisual <b>redundancies.</b> The removal of visually redundant information has potential advantages. First, it ensures that only visually important information is encoded, thus maintaining visual quality. Second, discarding visually redundant information leads to better compression performance. Therefore, incorporating properties of the HVS to coding structures {{is seen as a}} beneficial step in enhancing the visual performance of image coders...|$|R
40|$|Divisive {{normalization}} {{in primary}} visual cortex {{has been linked to}} adaptation to natural image statistics in accordance to Barlow’s <b>redundancy</b> <b>reduction</b> hypothesis. Using recent advances in natural image modeling, we show that the previously studied static model of divisive normalization is rather inefficient in reducing local contrast correlations, but that a simple temporal contrast adaptation mechanism of the half-saturation constant can substantially increase its efficiency. Our findings reveal the experimentally observed temporal dynamics of divisive normalization to be critical for <b>redundancy</b> <b>reduction...</b>|$|E
40|$|International Telemetering Conference Proceedings / October 14 - 16, 1975 / Sheraton Inn, Silver Spring, MarylandA {{real time}} data {{compression}} unit was designed and fabricated for the Radio Astronomv Explorer Lunar Mission Antenna Aspect Camera. The camera takes {{a panoramic view of}} spacecraft, moon, etc. of ± 35 degrees by 360 degrees. This data compressor combined information reduction and <b>redundancy</b> <b>reduction.</b> The information reduction was accomplished by subsampling (used every fourth line) : and the <b>redundancy</b> <b>reduction</b> was accomplished by an adaptive run-length encoder. The adaptive run-length encoder used a zero-order predictor. Two different maximum run-lengths were used with two different data formats. Selection of the operating format depended on the sampled gray level compared to a fixed threshold. Statistical data and images indicate that the <b>redundancy</b> <b>reduction</b> technique yields a compression ratio of 8 : 1. Thus a combined compression ratio of 32 : 1 was obtained on an entire panoramic view...|$|E
40|$|The {{objective}} of independent component analysis is {{to represent a}} set of multidimensional measurement vectors in a basis where the components are statistically independent or as independent as possible. This <b>redundancy</b> <b>reduction</b> between components relates independent component analysis to many other theoretical and practical areas of science that are concerned with information, such as information theory, compression, sensory coding and pattern recognition. One of the first cases where the principle of <b>redundancy</b> <b>reduction</b> was presented was {{the analysis of the}} human visual system. Ever since the first time this idea was brought up, there has existed a need to st [...] ...|$|E
40|$|Abstract-This paper {{investigates the}} <b>redundancy</b> and loss <b>reduction</b> in {{reconfigurable}} antenna structures. We use graph models as tools {{to understand and}} optimize reconfigurable antenna structures. Examples are given and formulas are suggested {{to be used by}} reconfigurable antenna designers to obtain non redundant structures...|$|R
30|$|Other related {{approaches}} are [27, 28]. In [27], authors present the delayed flooding with cumulative neighborhood (DFCN) protocol. DFCN evaluates {{the benefits of}} retransmitting a certain message based on whether vehicles in the neighborhood already received the message in previous occasions. While the protocol focuses on reducing transmission <b>redundancy,</b> this <b>reduction</b> comes {{at the cost of}} a higher latency, which makes it unsuitable for critical emergency applications. A similar approach is taken in [28], where an example of a tree-based protocol applied to vehicular networks is presented. Although the BODYF protocol aims to achieve a higher efficiency in terms of message exchanged, it still inherits the drawback of typical tree-based approaches of having to perform topology maintenance operations.|$|R
40|$|Knowledge {{maintenance}} for Case-Based Reasoning {{systems is}} an important knowledge engineering task despite the avail-ability of initial case knowledge and new cases to extend it. For classification systems {{it is essential that}} different sce-narios for the various classes are well represented and de-cision boundaries are well defined in the case knowledge. A complexity-based competence metric is proposed that iden-tifies redundant and error-causing cases to be deleted. The metric informs amaintenance tool that enables the engineer to experiment and balance conflicting objectives. Complexity-informed maintenance outperforms benchmark algorithms for <b>redundancy</b> and error <b>reduction</b> tasks...|$|R
40|$|Abstract: We {{address the}} data {{reduction}} in time series problem {{through a combination}} of two newly developed algorithms. The first is {{a modified version of the}} Douglas-Peucker Algorithm (DPA) for short-term <b>redundancy</b> <b>reduction.</b> The second is an alternative to the classical statistic methods for long-term <b>redundancy</b> <b>reduction</b> and is based on block sorting. The block sorting technique is inspired from the quite recent Burrows and Wheeler Algorithm (BWA). The novel reduction scheme was applied to the ECG time series using the MITBIH public ECG database. Results show that the novel scheme is highly competitive with respect to the most performant existing techniques (SPIHT, TSVD, CCSP-ORD-VLC and others) ...|$|E
40|$|Static {{real world}} images are {{processed}} by a computationally simple and biologically plausible {{version of the}} recent predictability minimization algorithm for unsupervised <b>redundancy</b> <b>reduction.</b> Without a teacher and without any significant pre-processing, the system automatically learns to generate orientation sensitive edge detectors in the first (semilinear) layer...|$|E
40|$|The {{topic of}} this thesis {{is a new}} {{approach}} to on-board signal processing for SAR (Synthetic Aperture Radar) systems with multiple elevation channels. Multi-channel satellite SAR systems have been proposed to meet the rising demands for increased resolutions and larger imaged areas. A problem with the ractical realization of these systems is the high data rate necessary for transmission of the multi-channel data to the ground. Existing approaches for data reduction are based on digital beamforming (SCORE, Scan On Receive) and a following scalar quantization (BAQ, Block Adaptive Quantization). In this thesis, a more general, information theory-based approach called <b>redundancy</b> <b>reduction</b> is investigated, with the aim of preserving more information and allowing more flexible post-processing of the data. Transform coding is proposed as a practical implementation of <b>redundancy</b> <b>reduction,</b> and simulation results are presented. ...|$|E
40|$|We {{present a}} new method for {{automatic}} implicit induction theorem proving, and its application for the verification of cryptographic protocols. The method {{is based on}} constrained tree grammars and handles non-confluent rewrite systems which are required {{in the context of}} the verification of security protocols because of the non-deterministic behavior of attackers. It also handles axioms between constructor terms which allows us to specify explicit destructors representing cryptographic operators. Constrained tree grammars are used in our procedure both as induction schemes and as oracles for checking validity and <b>redundancy</b> by <b>reduction</b> to an emptiness problem. They also permit to characterize security failure of cryptographic protocols as sets of execution traces corresponding to an attack. This way, we obtain a generic framework for the verification of protocols, in which we can verify reachability properties like confidentiality, but also more complex properties like authentication. We present three case studies which gave very promising results...|$|R
40|$|Storage and {{transportation}} of goods within global supply chains is {{a major cause of}} environmental damage in modern value added processes. Thus, in the past, theory and practice developed several approaches in order to decrease these negative environmental impacts that frequently counteract the traditional efficiency-oriented ambitions. However, in many cases the economic and environmental performance can be improved at the same time. As many activities in logistics and inventory management are related to the treatment of potential uncertainties in the system by establishing <b>redundancies,</b> the <b>reduction</b> of uncertainty has equally a positive impact on both performance measures. To investigate the interrelation between uncertainty and the economic and environmental performance of supply chains, a serial inventory system consisting of a manufacturer who works with overseas suppliers and a carrier is considered, whereas the carrier is able to reduce lead time uncertainty. The relationships between uncertainties and the economic and environmental performance of the considered inventory system are highlighted by a simulation study based on empirical data from an international container shipping supply chain...|$|R
40|$|The {{purpose of}} this {{qualitative}} research was to gain insight on how twenty-one women religious described and gave meaning to their experiences of conflict and anger within the cultural organization of the Congregation de Notre Dame (CND). The conflict and anger norms and taboos of the CND were named and analyzed to determine their effects on how the women religious in that organization handled conflict and expressed anger. ^ Applying the holistic-inductive principles of naturalistic inquiry, one audiotaped indepth interview of 60 - 90 minutes, using a focused interview guide, was carried out with these CND women religious, accessed through purposeful sampling. The criterion for purposeful sampling in the data collection was based on information <b>redundancy.</b> Data <b>reduction,</b> management and analysis were carried out {{by means of the}} constant comparative method, in which categories, patterns and themes emerged from and were grounded in the data. In this process, extensive use was made of verbatim quotes from the participants 2 ̆ 7 audiotaped interviews, through which they recounted and validated their personal experiences of conflict and anger within the CND. ...|$|R
40|$|The <b>Redundancy</b> <b>Reduction</b> Hypothesis by Barlow and Attneave {{suggests}} {{a link between}} the statistics of natural images and the physiologically observed structure and function in the early visual system. In particular, algorithms and probabilistic models like Independent Component Analysis, Independent Subspace Analysis and Radial Factorization, which allow for <b>redundancy</b> <b>reduction</b> mechanism, have been used successfully to generate several features of the early visual system such as bandpass filtering, contrast gain control, and orientation selective filtering when applied to natural images. Here, we propose a new family of probability distributions, called Lp-nested symmetric distributions, that comprises all of the above algorithms for natural images. This general class of distributions allows us to quantitatively asses (i) how well the assumptions made by all of the redundancy reducing models are justified for natural images, (ii) how large the contribution of each of these mechanisms (shape of filters, non-linear contrast gain control, subdivision into subspace) to <b>redundancy</b> <b>reduction</b> is. For ISA, we find that partitioning the space into different subspace only yields a competitive model when applied after contrast gain control. In this case, however, we find that the single filter responses are already almost independent. Therefore, we conclude that a partitioning into subspaces does not considerably improve the model which makes band-pass filtering (whitening) and contrast gain control (divisive normalization) the two most important mechanisms...|$|E
40|$|Abstract—One of {{the major}} {{challenges}} for the home robots is to recognize objects under different transformations and viewpoints. In this paper, we propose a hierarchical neural network for object recognition inspired by two properties of neurons in the visual cortex: invariance of their responses to stimulus transformations and coding the efficient features in images. To achieve the first goal, we used the trace learning rule to develop neurons with invariance responses to stimulus transformations. For the second goal, we used a model of hierarchical <b>redundancy</b> <b>reduction</b> to extract the most efficient features from images. This hierarchical <b>redundancy</b> <b>reduction</b> uses signals from surrounding neurons in each layer to provide the next layer with most informative features. A set of experiments performed on the coil 100 dataset and custom images reveal the high performance of the proposed model for object recognition. I...|$|E
40|$|In {{almost all}} codes of {{practice}} for seismic resistant design of buildings, a behavior factor is used to reduce design base shear. The behavior factor is affected by several parameters such as ductility, overstrength and <b>redundancy</b> <b>reduction</b> factors. There are two common approaches to assess the effects of redundancy {{on the strength of}} a structural system, which are as follows: static pushover analysis and incremental dynamic analysis. The two indices: redundancy strength coefficient and redundancy variation coefficient have been introduced to measure these effects. Simplified methods are developed and presented to calculate these parameters. In this study, the redundancy strength and the redundancy variation parameters are evaluated for the reinforced concrete plane frames with different number of stories, bays and ductility capacities. The investigations indicate that these two parameters are mainly the results of <b>redundancy</b> <b>reduction</b> factors...|$|E
40|$|As {{a result}} of the {{cooperation}} in the Intas 915 project, annotated speech corpora have become available in three different languages for both read and spontaneous speech of some 4 - 5 male and 4 - 5 female speakers per language (6 - 10 minutes per speaker). These data have been used to study the effects of redundancy on acoustic vowel reduction, in terms of vowel duration, F 1 -F 2 distance to a virtual target of reduction, spectral center of gravity, and vowel intensity. It was shown that in all three (typologically different) languages vowel <b>redundancy</b> increases acoustic <b>reduction</b> in the same way. The reduction of redundant vowels seems to be a language universal. 1...|$|R
40|$|Crowdsourced data {{annotation}} is noisier than annotation from trained workers. Previ-ous {{work has}} shown that redundant annota-tions can eliminate the agreement gap be-tween crowdsource workers and trained work-ers. Redundant annotation is usually non-problematic because individual crowdsource judgments are inconsequentially cheap in a class-balanced dataset. However, redundant annotation on class-imbalanced datasets requires many more la-bels per instance. In this paper, using three class-imbalanced corpora, we show that an-notation <b>redundancy</b> for noise <b>reduction</b> is very expensive on a class-imbalanced dataset, and should be discarded for instances receiv-ing a single common-class label. We also show that this simple technique produces an-notations at approximately the same cost of a metadata-trained, supervised cascading ma-chine classifier, or about 70 % cheaper than 5 -vote majority-vote aggregation. ...|$|R
40|$|We {{investigated}} {{the role of}} KNOX genes in legume root nodule organogenesis. Class 1 KNOX homeodomain transcription factors (TFs) are involved in plant shoot development and leaf shape diversity. Class 2 KNOX genes are less characterized, even though an antagonistic function relative to class 1 KNOXs was recently proposed. In silico expression data and further experimental validation identified in the Medicago truncatula model legume three class 2 KNOX genes, belonging to the KNAT 3 / 4 / 5 -like subclass (Mt KNAT 3 / 4 / 5 -like), as expressed during nodulation from early stages. RNA interference (RNAi) -mediated silencing and overexpression studies were used to unravel a function for KNOX TFs in nodule development. Mt KNAT 3 / 4 / 5 -like genes encoded four highly homologous proteins showing overlapping expression patterns during nodule organogenesis, suggesting functional <b>redundancy.</b> Simultaneous <b>reduction</b> of Mt KNAT 3 / 4 / 5 -like genes indeed led to an increased formation of fused nodule organs, and decreased {{the expression of the}} MtEFD (Ethylene response Factor required for nodule Differentiation) TF and its direct target MtRR 4, a cytokinin response gene. Class 2 KNOX TFs therefore regulate legume nodule development, potentially through the MtEFD/MtRR 4 cytokinin-related regulatory module, and may control nodule organ boundaries and shape like class 2 KNOX function in leaf development...|$|R
40|$|Traditional audio coding {{is based}} on a perceptual {{compression}} paradigm that exploits psychoacoustic information to efficiently encode audio signals. Recently, extensive research has been conducted in order to understand how the brain encodes natural signals. These results suggest that the encoding process is very efficient in terms of <b>redundancy</b> <b>reduction</b> of the signal information. It could be that the psychoacoustic effects (such as the masking effect) are only a special case of a more general <b>redundancy</b> <b>reduction</b> mechanism that exists in the auditory pathway. Motivated by this work we propose a new audio coding scheme that {{is based on}} improved sound representation found by Independent Component Analysis. Using a local linear, low rank, non-orthogonal transform, we remove additional redundancies in the signal. At low bitrates this coding scheme gives results superior to a legacy perceptual encoding scheme for different kinds of audio signals...|$|E
40|$|This study {{develops}} a novel hybrid method for outlier detection (HMOD) that combines {{the idea of}} distance based and density based methods. The proposed method has two main advantages {{over most of the}} other outlier detection methods. The first advantage is that it works well on both dense and sparse datasets. The second advantage is that, unlike most other outlier detection methods that require careful parameter setting and prior knowledge of the data, HMOD is not very sensitive to small changes in parameter values within certain parameter ranges. The only required parameter to set is the number of nearest neighbors. In addition, we made a fully parallelized implementation of HMOD that made it very efficient in applications. Moreover, we proposed a new way of using the outlier detection for <b>redundancy</b> <b>reduction</b> in datasets where the confidence level that evaluates how accurate the less redundant dataset can be used to represent the original dataset can be specified by users. HMOD is evaluated on synthetic datasets (dense and mixed “dense and sparse”) and a bioinformatics problem of <b>redundancy</b> <b>reduction</b> of dataset of position weight matrices (PWMs) of transcription factor binding sites. In addition, in the process of assessing the performance of our <b>redundancy</b> <b>reduction</b> method, we developed a simple tool {{that can be used to}} evaluate the confidence level of reduced dataset representing the original dataset. The evaluation of the results shows that our method can be used in a wide range of problems...|$|E
40|$|Divisive {{normalization}} {{has been}} proposed as a nonlinear <b>redundancy</b> <b>reduction</b> mechanism capturing contrast correlations. Its basic function is a radial rescaling of the population response. Because of the saturation of divisive normalization, however, {{it is impossible to}} achieve a fully independent representation. In this letter, we derive an analytical upper bound on the inevitable residual redundancy of any saturating radial rescaling mechanism...|$|E
40|$|Microtechnology has the {{potential}} for a beneficial impact on both launch and space flight operations because of savings in mass, power consumption, volume and low cost manufacturing and testing. Less apparent, but equally valuable, are the advantages in reliability to be gained by increased <b>redundancy</b> and the <b>reduction</b> of complexity in the fabrication process. However, a successful program for the development and insertion of these technologies will need to consider the conservatism of the aerospace community. This is more true of government space programs where success is measured by a lack of launch failures and less true of commercial ventures where success may be measured by other criteria. This paper presents a strategy for evolving microtechnology in space systems that fits both the government's risk avoidance culture, and parallels the expected development of microtechnology applications in space systems...|$|R
40|$|Abstract. We {{developed}} and studied an experimental system, RealTourist, which lets a user {{to plan a}} conference trip {{with the help of}} a remote tourist consultant who could view the tourist’s eye-gaze superimposed onto a shared map. Data collected from the experiment were analyzed in conjunction with literature review on speech and eye-gaze patterns. This inspective, exploratory research identified various functions of gaze-overlay on shared spatial material including: accurate and direct display of partner’s eye-gaze, implicit deictic referencing, interest detection, common focus and topic switching, increased <b>redundancy</b> and ambiguity <b>reduction,</b> and an increase of assurance, confidence, and understanding. This study serves two purposes. The first is to identify patterns that can serve as a basis for designing multimodal human-computer dialogue systems with eye-gaze locus as a contributing channel. The second is to investigate how computer-mediated communication can be supported by the display of the partner’s eye-gaze. ...|$|R
30|$|Research on {{big data}} {{analytics}} is entering {{in the new}} phase called fast data where multiple gigabytes of data arrive in the big data systems every second. Modern big data systems collect inherently complex data streams due to the volume, velocity, value, variety, variability, and veracity in the acquired data and consequently give rise to the 6 Vs of big data. The reduced and relevant data streams {{are perceived to be}} more useful than collecting raw, redundant, inconsistent, and noisy data. Another perspective for big data reduction is that the million variables big datasets cause the curse of dimensionality which requires unbounded computational resources to uncover actionable knowledge patterns. This article presents a review of methods that are used for big data reduction. It also presents a detailed taxonomic discussion of big data reduction methods including the network theory, big data compression, dimension <b>reduction,</b> <b>redundancy</b> elimination, data mining, and machine learning methods. In addition, the open research issues pertinent to the big data reduction are also highlighted.|$|R
40|$|International Telemetering Conference Proceedings / October 14 - 16, 1975 / Sheraton Inn, Silver Spring, MarylandParticles flux {{intensity}} measurements <b>redundancy</b> <b>reduction</b> algorithms are proposed. Accuracy criteria {{consists in}} limiting of a samples relative error maximum value. The algorithms {{are based on}} prediction or interpolation operations with a variable threshold, adaptive to a changing flux intensity. A formula for computation of an adaptive threshold zero order predictor compression ratio is deduced. Computed values show good coincidence with those received by signal and algorithm computer simulation. Adapter threshold zero order predictor (AT-ZOP) and first order interpolator (AT-FOI) algorithms applied to real telemetry data reveal their high efficiency as relating to attainable compression ratios. Algorithms compression ratio comparison results in predictor advantage against interpolator and unsignificantly small predictor loss when preliminary data smoothing is applied. Compression ratios for joint application of background removal [2] and adaptive predictor algorithms are also evaluated. AT-ZOP simplicity and high efficiency allow to recommend it for use in particle flux intensity measurements <b>redundancy</b> <b>reduction</b> systems...|$|E
40|$|In Proc. of IEEE International Conference on Image Processing ICIP' 96 Lausanne, Switzerland, September 1996 {{this paper}} we present an image {{sequence}} coding scheme for very {{low bit rate}} coding {{which is based on}} spatial <b>redundancy</b> <b>reduction</b> via the new edge sensitive subband coding method and temporal <b>redundancy</b> <b>reduction</b> via windowed overlapped block-matching motion compensation. The scheme has the main advantage, that only the significant regions of differenceimages are coded. Thus the computational cost can be kept low. Due to the properties of both the temporal and the spatial coding used, there are no blocking effects in the coded images, and the overall visual performance of the coding scheme is very good. 1. INTRODUCTION The requirements on video source coding algorithms has enormously increased in the past years due to many factors, one of them being the availability of hardware capable of processing vast amounts of visual data in real-time. Some image sequence coding schemes f [...] ...|$|E
40|$|Abstract — In {{corporate}} networks, idle desktop machines {{hardly ever}} sleep, mainly {{for the reason}} that turning off the hosts will affect the internet connection. There are number of systems have been proposed, few of them have mutually considered energy consumption and performance. In this article, we consider the case of file downloading, {{which is one of the}} most common internet applications. A system called College-Net is proposed for saving energy while improving performance by exploiting task migration and <b>redundancy</b> <b>reduction.</b> With this College-Net system, file downloading will not be interrupted even when a PC switches to sleep. In particular, it allows a PC to sleep while continuing to run large file downloads. Moreover, by caching the downloaded files in the server, College-Net suppresses the reduplicate downloading effort and reduces the network traffic. The strategy of <b>redundancy</b> <b>reduction</b> used in College-Net improves the average speed while avoids the energy waste caused by repeated transmissions of downloaded files. The proposed College-Net does not require any hardware additions to end hosts, and can be realized purely by additional software...|$|E
40|$|The aim of {{this paper}} is to display a {{conceptual}} and methodological framework for brand image research by drawing on the discipline of structuralist semiotics. Upon a critical review of existing research from key authors in the brand semiotics literature and through an engagement with the concept of brand image as formulated by key authors in the marketing literature, a semiotic model is furnished for the formation of brand image and brand identity. By drawing on the structuration process of brand image along the three major strata in a brand’s signification trajectory, and the key operations of <b>reduction,</b> <b>redundancy,</b> recurrence, isotopy, homologation, I focus more narrowly on how the chaining [enchaînement] of elements from the three strata is effected with view to addressing how brand image may be operationalised in structuralist semiotic terms vis a vis a brand’s intended positioning, how it may be linked to a brand’s advertising discourse and how the conceptual framework may yield a platform for ongoing brand image analysis and management. 1...|$|R
40|$|Background: The {{logistic}} {{systems are}} very complex socio-technical systems. In this paper the proposal of {{application of the}} hierarchical multi-layers system platform HILS approach for {{the solution of the}} complex vehicle routing problems is presented. The interactive system functional structure was proposed which by intelligent dedicated inter-layers interactions enables the professional solutions of these practical problems. To illustrate these capabilities the complex example of the real-time VRP-SPD-TW routing problem was presented in which upper layers offers the context-related real-time updating network specifications that stimulates the adequate routing parameters and specifications updating for problem solution in optimization layer. At the bottom dispatching control layer the DISCON (Dispatching CONtrol) method from public transport was adopted to logistics applications in which the actual routing is treated as obligatory reference schedule to be stabilized. The intelligence aspects are related among others to HILS based decomposition, context-related trade-offs between routing modifications and corrective dispatching control capabilities e. g. priority or route guidance actions. 	Methods: Decomposition of the vehicle routing problem for the HILS layers tasks creating the ILS system hierarchical structure. Dedicated solution method for the VRP-SPD-TW routing problem. The recognition of the control preferences structure by AHP-Entropy methods. DISCON and PIACON multi-criteria interacting control methods. 	Results: Original formulation and solution of the vehicle routing problem by system-wide approach with essential practical advantages: consistency, lack of <b>redundancy,</b> essential <b>reduction</b> of dimension, dedicated formulation, multi-criteria approach, exploration of the integration and intelligence features supported by the intelligent PIACON-DISCON methods control activities 	Conclusions: The presented proposal creates the professional approach to the solution of the crucial problems in logistics systems with the implementation of modern tools and enabling technologies...|$|R
5000|$|Greybull {{purchased}} 90% of Monarch Holdings Ltd, a UK airline, trading as Monarch, October 25, 2014 {{in return}} for £50m capital commitment the remaining 10% passed to Monarch's pension scheme : Greybull Capital's partner Marc Meyohas said: [...] "We are delighted to acquire Monarch and invest our capital into a very strong brand with great potential" [...] : Greybull's investment secured £125m of capital and liquidity facilities; Restructuring involved reducing its aircraft from 42 to 34, 700 <b>redundancies</b> & wage <b>reductions.</b> [...] The Financial Times reported; Since Greybull bought Monarch, the travel airline has been transformed. Monarch delivering its 1st profit in three years in 2015. Greybull employed Deutsche Bank in April 2016 to 'explore strategic options for Monarch Airlines', including growth opportunities in Europe and selling it. Monarch Airlines seeking in June 2016 to secure £35m either from Greybull or a 3rd party. [...] Amid rumors of imminent bankruptcy in September 2016 Monarch's ATOL aviation insurance was extended for 2 weeks by fresh investment, [...] and by then, £165m of Greybull investment renewed Monarch's annual licence.|$|R
