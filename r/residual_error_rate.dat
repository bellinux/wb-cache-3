16|10000|Public
50|$|Two {{performance}} measures give quality characteristics of an ARQ-M link. These are error rate and throughput. Residual errors can {{be due to}} transpositions of the symbol elements or double errors. The chances that this happens is about 100 to 1000 times less than for a working unprotected link. A log graph of <b>residual</b> <b>error</b> <b>rate</b> against raw error rate shows a steeper line with slope 2 intercepting at 100% errors. If the unprotected 5 unit code had an error rate of 1%, the ARQ-M protected code error rate is 0.0025%.|$|E
40|$|Safety {{integrity}} level (SIL) {{verification of}} functional safety fieldbus communication {{is an essential}} part of SIL verification of safety instrumented system (SIS), and it requires quantifying residual error probability (RP) and <b>residual</b> <b>error</b> <b>rate</b> of function safety communication. The present quantification method of <b>residual</b> <b>error</b> <b>rate</b> uses RP of cyclic redundancy check (CRC) to approximately replace the total RP of functional safety communication. Since CRC only detects data integrity-related errors and CRC has intrinsically undetected error, some other residual errors are not being considered. This research found some residual errors of the present quantification method. Then, this research presents an extended new approach, which takes the found residual errors into account to determine more comprehensive and reasonable RP and <b>residual</b> <b>error</b> <b>rate.</b> From perspective of the composition of safety message, this research studies RPs of those controlling segments (sequence number, time expectation, etc.) to cover the found residual errors beyond CRC detection coverage, and the influences of insertion/masquerade errors and time window on RP are investigated. The results turn out these residual errors, especially insertion/masquerade errors, may have a great influence on quantification of <b>residual</b> <b>error</b> <b>rate</b> and SIL verification of functional safety communication, and they should be treated seriously...|$|E
40|$|Abstract — In video {{streaming}} over multicast networks, error recovery {{is essential to}} alleviate the effect of packet loss. In this paper, we study a feedback-free recovery scheme which combines the strength of FEC (namely, a parity packet can repair any lost packet) and pseudo-ARQ (namely, incremental recovery). To account for the receiver heterogeneity, the server multicasts layered video streams for the receivers to join. For each layer, the receivers may join dynamically additional multicast channels of FEC and pseudo-ARQ packets to recover local losses. In order to offer quality video, we address the following two issues: 1) “Menu creation”: Given a certain maximum error rate after correction (i. e., <b>residual</b> <b>error</b> <b>rate),</b> what is the combination of FEC and pseudo-ARQ packets for the server to send so as to minimize a target receiver’s bandwidth; and 2) “Menu selection”: Given the server’s menu, what’s the combination of FEC and pseudo-ARQ packets for the receiver to join so as to minimize its <b>residual</b> <b>error</b> <b>rate</b> given its local loss probability, bandwidth and loss pattern. We present {{the analysis of the}} scheme and show that our scheme can substantially reduce a receiver’s <b>residual</b> <b>error</b> <b>rate</b> as compared with pure FEC or pure pseudo-ARQ alone (by cutting it as much as half). I...|$|E
50|$|The <b>residual</b> bit <b>error</b> <b>rate</b> (RBER) is a {{receive quality}} metric in digital transmission, {{one of several}} used to {{quantify}} {{the accuracy of the}} received data.|$|R
50|$|When digital {{communication}} systems are being designed, the maximum acceptable <b>residual</b> bit <b>error</b> <b>rate</b> can be used, {{along with other}} quality metrics, to calculate the minimum acceptable {{signal to noise ratio}} in the system. This in turn provides minimum requirements for the physical and electronic design of the transmitter and receiver.|$|R
50|$|The Bidirectional Reliable mode differs in {{many ways}} from the {{previous}} two modes. The most important differences are a more intensive usage of the feedback channel, and a stricter logic at both the compressor and the decompressor that prevents loss of context synchronization between compressor and decompressor, except for very high <b>residual</b> bit <b>error</b> <b>rates.</b>|$|R
40|$|Describes {{the design}} and {{realization}} of an error-correcting codec. For error protection of TV lines the realized codec utilizes a shortened (2196, 2136) BCH-code with 60 bits (2. 7 %) redundancy. The codec provides correction of 4 random errors or correction of one error burst up to 26 bits alternatively; 5 or 6 random errors can be detected. In the case of random errors the <b>residual</b> <b>error</b> <b>rate</b> after error correction is less than 10 - 8 even at 10 - 4 channel error rate...|$|E
40|$|In {{this paper}} {{we present a}} {{statistical}} model used to select coding parameters for a mixed resolution Wyner-Ziv framework implemented using the H. 264 /AVC standard. This paper extends {{the results of a}} previous work for the H. 263 + case to the H. 264 /AVC coder, since the parameters need to be recalculated for the H. 264 case. The proposed correlation estimation mechanism guides the parameter choice process, and also yields the statistical model used for decoding. This mechanism is proposed based on extracting edge information and <b>residual</b> <b>error</b> <b>rate</b> in co-located blocks from the low resolution base layer that is available at both ends. Index Terms — distributed video coding, Wyner-Ziv, parameter estimation 1...|$|E
40|$|Traditional {{methods for}} {{efficient}} text entry {{are based on}} prediction. Prediction requires a constant context-shift between entering text and selecting or verifying the predictions. Previous {{research has shown that}} the advantages offered by prediction are usually eliminated by the cognitive load associated with such context-switching. We present a novel approach that relies on compression. Users are required to compress text using a very simple abbreviation technique that yields an average keystrok reduction of 26. 4 %. Input text is automatically decoded using weighted finite-state transducers, incorporating both word-based and letter-based n-gram language models. Decoding yields a <b>residual</b> <b>error</b> <b>rate</b> of 3. 3 %. User experiments show that this approach yields improved text input speeds...|$|E
40|$|Associative {{memories}} are structures that store data {{in such a}} way that it can later be retrieved given only a part of its content [...] a sort-of error/erasure-resilience property. They are used in applications ranging from caches and memory management in CPUs to database engines. In this work we study associative memories built on the maximum likelihood principle. We derive minimum <b>residual</b> <b>error</b> <b>rates</b> when the data stored comes from a uniform binary source. Second, we determine the minimum amount of memory required to store the same data. Finally, we bound the computational complexity for message retrieval. We then compare these bounds with two existing associative memory architectures: the celebrated Hopfield neural networks and a neural network architecture introduced more recently by Gripon and Berrou...|$|R
40|$|Background Clinical trials {{conducted}} in rural resource-poor settings face special challenges in ensuring {{quality of data}} collection and handling. The variable nature of these challenges, ways to overcome them, and the resulting data quality are rarely reported in the literature. Purpose To provide a detailed example of establishing local data handling capacity for a clinical trial {{conducted in}} a rural area, highlight challenges and solutions in establishing such capacity, and to report the data quality obtained by the trial. MethodsWe provide a descriptive case study of a data system for biological samples and questionnaire data, and the problems encountered during its implementation. To determine the quality of data we analyzed test–retest studies using Kappa statistics of inter- and intra-observer agreement on categorical data. We calculated Technical Errors of Measurement of anthropometric measurements, audit trail analysis was done to assess <b>error</b> correction <b>rates,</b> and <b>residual</b> <b>error</b> <b>rates</b> were calculated by database-to-source document comparison. Results Initial difficulties included the unavailability of experienced research nurses...|$|R
40|$|Abstract — Forward Error Correction (FEC) is {{a common}} {{technique}} for transmitting multimedia streams over the Internet. In this {{paper we propose a}} new approach of adaptive FEC scheme for multimedia applications over the Internet. This adaptive FEC will optimize the redundancy of the generated codewords from a Reed-Solomon (RS) encoder, in–order to save the bandwidth of the channel. The adaptation of the FEC scheme is based on predefined probability equations, which are derived from the data loss rates related to the recovery rates at the clients. The server uses the RTCP reports from clients and the probability equations to approximate the final delivery ratio of the sent packets to the client after applying the adaptive FEC. The server uses the RTCP reports also to predict the next network loss rate using curve fitting technique to generate the optimized redundancy in-order to meet certain <b>residual</b> <b>error</b> <b>rates</b> at the clients...|$|R
40|$|The aim of {{the paper}} is using of {{modelling}} within development of safety–related communication systems presented in the areas where guaranty of safety integrity level is required. In the paper basic principles used {{in the process of}} safety evaluation in closed transmission systems are summarised. Dangerous states of system are mainly caused by random failures of HW within non-trusted transmission system, by electromagnetic interference caused with noise or interferences and by systematic failures within specification of system. Main part of paper describes the simulation of disturbing effects within communication channel via programme Matlab, relations for determination of probability of undetected errors of code words with using block codes and results of <b>residual</b> <b>error</b> <b>rate</b> for Hamming code...|$|E
40|$|AbstractWe {{present a}} {{software}} program simulating data transmission through a noisy channel. This program {{makes it possible to}} simulate simplex or full duplex transmissions of digitalized data, of any type. Through a modelization of the error rate and of the kinds of errors we can simulate transmissions, from submarine ones (error rate about 10 - 2) to transmissions via satellites (error rate about 10 - 5 to 10 - 8). This software operates from one initial file to be transmitted, and from user-entered parameters of simulation. The final results are of two kinds: two files, and several numerical results. The two files are: The initial one transmitted without data protection, and the initial one transmitted with data protection by error correcting codes. The numerical results are: The binary experimental error rate, the binary <b>residual</b> <b>error</b> <b>rate,</b> and the transmission efficiency...|$|E
40|$|In {{this paper}} {{we present a}} rate {{distortion}} analysis and a statistical model in order to select coding parameters for memoryless coset codes, for a spatial scalability based mixed resolution Wyner-Ziv framework. The mixed resolution framework, used in this work, is based on full resolution coding of the key frames and spatial 2 -layer coding of the intermediate non-reference frames where the spatial enhancement layer is Wyner-Ziv coded. The framework enables reduced encoding complexity through reduced spatial-resolution encoding of the non-reference frames. The quantized transform coefficients of the Laplacian residual frame are mapped to cosets {{and sent to the}} decoder. A correlation estimation mechanism that guides the parameter choice process is proposed based on extracting edge information and <b>residual</b> <b>error</b> <b>rate</b> in co-located blocks from the low resolution base layer. Index Terms — Wyner-Ziv, reversed-complexity coding, spatial scalabilit...|$|E
40|$|We {{present a}} hybrid ARQ (HARQ) scheme using single-error {{correcting}} burst-error detecting (SEC-BED) codes to address multiple errors in nanoscale on-chip interconnects. For a given <b>residual</b> flit <b>error</b> <b>rate</b> requirement, the proposed HARQ method yields 20 % energy improvement over other burst error correction schemes. By further integrated with skewed transitions, the proposed HARQ method can efficiently improve the error resilience against burst errors and also reduce delay uncertainty caused by capacitive coupling. The low overhead of our approach makes it suitable for implementation in reliable and energy efficient on-chip communication. 1...|$|R
50|$|Longer {{constraint}} length {{codes are}} more practically decoded {{with any of}} several sequential decoding algorithms, of which the Fano algorithm is the best known. Unlike Viterbi decoding, sequential decoding is not maximum likelihood but its complexity increases only slightly with constraint length, allowing the use of strong, long-constraint-length codes. Such codes {{were used in the}} Pioneer program of the early 1970s to Jupiter and Saturn, but gave way to shorter, Viterbi-decoded codes, usually concatenated with large Reed-Solomon error correction codes that steepen the overall bit-error-rate curve and produce extremely low <b>residual</b> undetected <b>error</b> <b>rates.</b>|$|R
40|$|We {{propose the}} {{combination}} of Orthogonal Frequency Division Multiplexing (OFDM) and Turbo DeCodulation (TDeC) – a multiple Turbo process consisting of iterative demodulation and iterative source-channel decoding – for transmission of correlated source codec parameters over wireless frequency-selective broadband fading channels. As OFDM systems split up frequency-selective fading channels into orthogonal flat-fading channels, individual modulation signal constellations sets with different bit mapping rules can be assigned to subcarriers {{taking into account the}} iterative reception and decoding process. The superior performance of the proposed OFDM-TDeC system is demonstrated by analyzing the achieved <b>residual</b> bit <b>error</b> <b>rate</b> (BER) and parameter signal-to-noise ratio (SNR) ...|$|R
40|$|A {{growing number}} of network {{applications}} {{require the use of}} a reliable multicast protocol to send stream media from a source to a potentially large number of receivers. This work introduces a new technique that integrates word Interleaving, Forward Error Correction (FEC) and Automatic Repeat reQuest (ARQ) to increase reliability. Our laboratory work showed the practical feasibility of a software implementation of the hybrid FEC/ARQ and interleaving scheme. We also present a comparative performance analysis of the hybrid FEC/ARQ scheme and the ARQ only scheme and show the influence of varying sending rates, group sizes, and packet loss probabilities. We evaluate the performance of our hybrid FEC/ARQ scheme and adapt redundancy factor to cope with a high loss rate and large-scale receivers. We demonstrate how appropriate amount of redundancy combined with coding decreases the bandwidth overhead, eliminates NAK implosion, reduces the <b>residual</b> <b>error</b> <b>rate,</b> and helps to meet real-time guarantees...|$|E
40|$|Molecular genetic {{techniques}} have entered {{many areas of}} clinical practice. Public expectations from this technology are understandably high. To maintain confidence in this technology, laboratories must implement the highest standards of quality assurance (QA). External quality assessment (EQA) is recognized as {{an essential component of}} QA. The United Kingdom National External Quality Assessment Service (UKNEQAS) for Molecular Genetics, first set up in 1991, is currently the longest provider of EQA to molecular genetic testing laboratories in the UK, The Netherlands, and Ireland. Errors in the scheme are sporadic events. However, evidence from this and other EQA schemes suggests that a <b>residual</b> <b>error</b> <b>rate</b> persists, which should be taken into account in clinical practice. This EQA scheme has evolved from the respective scientific bodies of the constituent countries and retains a strong emphasis on collective peer review. It is essential that the steps taken to ensure quality in this rapidly expanding field are clear and transparent to participants and public alike. We describe the procedures developed and the governance imposed to monitor and improve analytical and reporting standards in participant laboratories and we compare our experiences with those of equivalent EQA services in the United State...|$|E
40|$|Abstract—Packet loss is {{inevitable}} in video multicast. In this paper, we propose and study an effective feedback-free loss recovery scheme for layered video which combines {{forward error correction}} (FEC) and stream replication. In our scheme, the server multicasts the video in parallel with FEC packets {{and a number of}} replicated delayed (ReD) version of the stream. Receivers autonomously and dynamically join the FEC and ReD streams to repair their losses. On the server side, we analyze and optimize the number of replicated streams and FEC packets to meet a certain residual loss requirement (i. e., error after correction). On the receiver side, we analyze the optimal combination of FEC and ReD packets to minimize its loss. We also present a fast yet accurate approximation algorithm for receiver to make such decision. We show that FEC combined with merely one or two replicated streams can effectively reduce the <b>residual</b> <b>error</b> <b>rate</b> (by as much as 50 %) as compared with pure FEC or replication alone. Both subjective and objective video measures confirm that our recovery scheme achieves much better visual quality. Index Terms—Fast approximation, feedback-free error recovery, forward error correction (FEC), layered video multicast, stream replication. I...|$|E
40|$|The TRANSAT [1] {{architecture}} {{is intended to}} provide efficient Internet access services over DVB-RCS satellite links. One of the key features in the TRANSAT {{architecture is}} the Satellite-Link Aware Communication Protocol (S-LACP) that is employed over a satellite segment to decrease the <b>residual</b> packet <b>error</b> <b>rate</b> down to an acceptable level. In this simulation study, we measure the TCP performance over satellite links, including the effect of various TCP enhancements and the S-LACP protocol. We show that when S-LACP is used to reduce the residual packet rate as seen by the TCP layer, performance improves. Without S-LACP the modeled packet <b>error</b> <b>rate</b> tends to be rather high not allowing a TCP sender to open the congestion window large enough or later on to retain the congestion window fully open...|$|R
40|$|A joint source-channel {{coding scheme}} for {{scalable}} video is developed in this paper. An SNR scalable video coder is used and Unequal Error Protection (UEP) is allowed for each scalable layer. Our {{problem is to}} allocate the available bit rate across scalable layers and, within each layer, between source and channel coding, while minimizing the end-to-end distortion of the received video sequence. The resulting optimization algorithm we propose utilizes universal rate-distortion characteristic plots. These plots show the contribution of each layer to the total distortion {{as a function of}} the source rate of the layer and the <b>residual</b> bit <b>error</b> <b>rate</b> (the <b>error</b> <b>rate</b> that remains after the use of channel coding). Models for these plots are proposed in order to reduce the computational complexity of the solution. Experimental results demonstrate the effectiveness of the proposed approach...|$|R
40|$|The {{purpose of}} this paper is to show through {{laboratory}} results, the performance of the new DLIF (Double Low-IF) receiver architecture. This new approach minimizes the receiver's cost by eliminating the need for RF image-reject and IF-Saw channel filters, avoids some of the shortcomings of direct conversion and single Low-IF methods and can be applied in GSM-like mobile stations. Performance results are presented in terms of Frame <b>Error</b> <b>Rates</b> and <b>Residual</b> Bit <b>Error</b> <b>Rates</b> and can thus be compared with GSM specifications. I INTRODUCTION In this paper detailed test results are provided for an implementation of the Double Low-IF receiver architecture [1] in GSMlike systems. On one hand the tests include classical tests like noise figures, compression points, power consumption etc. On the other hand, we embedded the DLIF chip into a loop-back test configuration using Lucent Sceptre-I baseband components to obtain performance results in terms of frame <b>error</b> <b>rates</b> (FER) and <b>residual</b> bit er [...] ...|$|R
40|$|This paper {{describes}} a design procedure with documentation for embedded software systems aiming at high quality. It starts from process level design. A system is decomposed to several orthogonal processes by data #ow division. Making each block or process mono-functional and repeating the division, {{the system is}} reduced to a hierarchical cluster of Extended Finite State Machines. The design proceeds by converting an abstract level message sequence chart to multi-process message sequence charts, to a state diagram for one trace, and #nally to a state diagram including all traces. Each state transition route is #nally converted to source code. A design of an embedded system is thus partitioned to many small step designs each accompanied with the interfacing documents. Through careful, rigorous and repetitive documentation and check, the overall error rate is decreased. Experiments based on applying this method in developing skills of software design and education are reported. 1. INTRODUCTION With the advances of the computer technology and software systems becoming {{the backbone of the}} modern society, an undetected software error might cause a big disaster. An error in switching software might cause a continental wide telephone tra#c jam, and an error in anti-skid brake software might cause a car accident. The reliabilityofembedded system's software is required to be higher than that of the reliable hardware. It is {{the purpose of this paper}} to report a design method # 6 # for embedded software systems aiming at achieving such reliability. It is generally accepted that software reliability depends upon the <b>residual</b> <b>error</b> <b>rate</b> of the software at the time of ship-out, and the rate depends on the software development work process. In Japan, most of the software developers impro [...] ...|$|E
40|$|The source-channel {{separation}} theorem postulated by Shannon {{has influenced}} {{the design of}} communication systems for multimedia content over the last decades: Source encoding and channel encoding are performed as two separate steps. However, {{the conditions of the}} separation theorem are almost never fulfilled in practical systems; a joint consideration of source and channel coding can thus be of special interest. Such a joint consideration with iterative decoding based on the Turbo principle {{has been found to be}} especially advantageous with regard to the realization of efficient multimedia communication systems. In the first part of this thesis, the concept of Iterative Source-Channel Decoding (ISCD) is fundamentally extended and optimized, especially in view of a possible practical implementation. New design guidelines and optimization criteria lead to a flexible and versatile system design. Special care is taken to optimize the components such that a <b>residual</b> <b>error</b> <b>rate,</b> which shall be as low as possible, results. Besides an extended, iterative receiver architecture leading to an improved exploitation of the correlation between consecutive frames, a simple yet effective stopping criterion is presented. This stopping criterion leads to an ISCD system with incremental redundancy transmission. It is additionally shown how a complexity-reduced ISCD receiver can be designed by employing a novel way of signal quantization. While the first part of this thesis treats the source encoding as given, it is consequently incorporated into the system design in the second part. As a novelty, an efficient method for the compression of parameter sources is introduced. This method shows the advantage of an easy adaptivity to varying transmission conditions. It is additionally shown how the ISCD concept can be applied for decoding multiple descriptions in order to improve the signal reconstruction quality in the presence of bit errors and packet losses. Besides optimized system designs, an innovative concept for the robust packet-based transmission of correlated source signals is presented. All variants and proposals are thoroughly analyzed using theoretical methods, by convergence analysis, or with computer simulations. The contribution of this thesis is the improvement of the error robustness and the spectral efficiency of future digital multimedia communication systems...|$|E
40|$|Nowadays, the Internet connects hospitals, medical {{institutions}} and people together. In this thesis, {{we focus on}} a medical application on ultrasound image processing and transmission. We consider that a pregnant woman takes her ultrasound examination in a local hospital, and the diagnosis and analysis of her ultrasound video (or image sequences) could be done remotely via the Internet. It is also possible for medical professionals and her family members from a distance to simultaneously view her ultrasound video using a multicast-capable network. In order to facilitate clinicians' diagnosis and improve medical information distribution, we address the issues in the clinical system related to the following two components. The first component focuses on automatic detection of fetal head features in ultrasound images. In clinics, fetal biometric measurement is done manually in the qualified ultrasound scanning images, in which some important predefined features exist. We present simple and new methods for automatic detection of these features in a single head ultrasound image. We also describe our system and criteria for ranking multiple images to facilitate clinicians' subsequent image selection and measurements in a sequence of ultrasound images. The {{results show that the}} methods can successfully identify features, and our system's ranking and the clinicians' ranking are well matched (with the hit ratio of 72. 5 %). The second component in the system is to offer quality real-time medical video to users over multicast networks. Since the video packet loss is inevitable, they have to be mostly recovered at clients. We propose and study an effective feedback-free recovery scheme for layered video in which the server multicasts FEC packets and replicated delayed (ReD) version of the streams in parallel with the video layers and the receivers, depending on their local losses, autonomously and dynamically join these layers to minimize their residual error rates (i. e., error after correction). Via analysis, we study the number of replicated streams and FEC packets that the server should provide and the optimal selection of FEC and ReD packets that a receiver should join. We further present fast approximation algorithms for the receiver to allocate the layer bandwidth, FEC and ReD packets, which are useful to those clients of low processing capability. We finally show that combining FEC with merely one or two stream replications can achieve better video quality on subjective experiments and substantially reduce the <b>residual</b> <b>error</b> <b>rate</b> as compared with pure FEC or replication alone (by as much as 50 %) ...|$|E
40|$|The {{so-called}} subband {{identification method}} {{has been introduced}} recently as an alternative method for identification of finite-impulse response systems with large tap sizes. It is known that this method can be more numerically efficient than the classical system identification method. However, no results are available to quantify its advantages. This paper offers a rigorous study {{of the performance of}} the subband method. More precisely, we aim to compare the performance of the subband identification method with the classical (fullband) identification method. The comparison is done in terms of the asymptotic <b>residual</b> <b>error,</b> asymptotic convergence <b>rate,</b> and computational cost when the identification is carried out using the prediction error method, and the optimization is done using the least-squares method. It is shown that by properly choosing the filterbanks, the number of parameters in each subband, the number of subbands, and the downsampling factor, the two identification methods can have compatible asymptotic <b>residual</b> <b>errors</b> and convergence <b>rate.</b> However, for applications where a high order model is required, the subband method is more numerically efficient. We study two types of subband identification schemes: one using critical sampling and another one using oversampling. The former is simpler to use and easier to understand, whereas the latter involves more design problems but offers further computational savings...|$|R
40|$|Subjective quality {{evaluation}} {{is used to}} optimize the produced audiovisual quality from fundamental signal processing algorithms to consumer services. These studies typically follow {{the basic principles of}} controlled psychoperceptual experiments. However, when compromising compression and transmission parameters for consumer services, the ecological validity of conventional {{quality evaluation}} methods can be questioned. To tackle this, we firstly present a novel user-oriented quality evaluation method for mobile television in its usage contexts. Secondly, we present the results of an experiment conducted with 30 participants comparing acceptability and satisfaction of quality as well as goals of viewing in three mobile contexts and under four different <b>residual</b> transmission <b>error</b> <b>rates,</b> when the participants also performed simultaneous assessment tasks. Finally, we compare the results with a previous laboratory experiment. The studied <b>error</b> <b>rates</b> impacted negatively on all measured tasks with some contextual differences. Moreover, the evaluations were more favorable and less discriminate in the mobile contexts compared to the laboratory...|$|R
40|$|Abstract. This paper {{presents}} {{the importance of}} error resilience tools in visual com-munications. Error resilience tools provide a mechanism enabling transmission of visual information over channels with <b>residual</b> random bit <b>errors</b> in the received bit stream. The benefits of using error resilience tools are proven by devising the analytic relationship between the time delay in a transparent channel using Automatic Repeat Request and the Author: OK? equivalent <b>residual</b> bit <b>error</b> <b>rate</b> in a nontransparent channel. The error resilience tools {{make it possible to}} achieve an acceptable visual quality even in the presence of these <b>residual</b> <b>errors.</b> Our work is related and compared to the standardization work of the next-generation still image compression system JPEG 2000. The results show that partial and complete substitution of the quantization and symbol encoding in visual compression systems by robust error resilience tools provides a signif-icant increase in robustness. Three error resilience tools are discussed: (1) substitution of the quantization and symbol encoding by a fixed length coding scheme, (2) substitution by a mixed fixed length coding and variable length coding scheme, and (3) substitution of the variable length coding by reversible variable length coding. Key words: Image compression systems, error resilience, multimedia communications. 1...|$|R
40|$|The present thesis {{deals with}} {{modeling}} of unpredictable disturbances, called channel noise, which occur during {{the transfer of}} information over real transmission channels. A further main subject of the thesis is to develop an appropriate method for an evaluation of different coding techniques which are used to ensure a reliable data transfer. The development of this evaluation scheme {{is based on an}} appropriate channel model describing the unpredictable disturbances. In this thesis, the unpredictable disturbances which occur during the transmission of digital data are interpreted as a sequence of bit errors and are represented by a binary stochastic error process. To model the error process we use a Markovian background process with a finite state space and a transition matrix Q. By means of this approach we are able to model digital transmission channels with memory. Since within this approach the advantages of already known channel models are combined and their disadvantages are widely extenuated this basic approach provides some explicit advantages. To ensure a reliable transmission of information different coding techniques are used which follow different mathematical approaches and are characterized by their different algebraic properties. On the one hand there exist so-called "random error-correcting codes (REC) " which are more suited to correct randomly distributed transmission errors. On the other one there are so-called "burst error-correcting codes (BEC) " which are able to correct so-called burst errors, i. e. errors which are correlated. In this connection, the error-correcting capabilities of different codes depend on their different algebraic properties. Thus, a further objective of this thesis is to develop an appropriate method to compare and evaluate the efficiency of REC versus BEC. In general, as an appropriate measure for the success of an error-correcting code the <b>residual</b> <b>error</b> <b>rate</b> is being used. Based on the channel model proposed {{in the first part of}} the thesis we develop recursive equations to calculate the residual error rates of codes. In this context, we differentiate between REC and BEC; i. e. we consider the advantages of different mathematical approaches of different error-correcting codes in this evaluation of their performance. One important result is that the use of BEC should be preferred if the memory of the considered transmission channel is sufficiently strong. Another result is that by means of these recursive equations an optimal and adequate error-correcting code can be chosen a priori depending on the error statistics of the considered transmission channel...|$|E
40|$|In today's world, {{demands of}} digital {{multimedia}} services are growing tremendously, {{together with the}} development of new communication technologies and investigation of new transmission media. Two common problems encountered in multimedia services are unreliable transmission channels and limited resources. This dissertation investigates advanced source coding and error control techniques, and is dedicated to designing joint source-channel coding schemes for robust image/video transmission. Error resilience properties of JPEG 2000 codestreams are investigated first, and an LDPC-based joint iterative decoding scheme is proposed. Next, a progressive decoding method is presented for still and motion image transmission. The underlying channel codes are created using a Plotkin construction and offer the novel ability of using one long channel codeword to protect an entire image, yet still allowing progressive decoding. Progressive quality improvements occur in two ways: the first is the usual progressive refinement, where image quality is improved as more data are received; the second is that <b>residual</b> <b>error</b> <b>rates</b> of earlier received data are reduced as more data are received. Finally, multichannel systems are studied and an optimal rate allocation algorithm is proposed for parallel transmission of scalable images in multichannel systems. The proposed algorithm selects a subchannel as well as a channel code rate for each packet, based on the signal-to-noise ratios (SNR) of the subchannels. The resulting scheme provides unequal error protection of source bits and significant gains are obtained over equal error protection (EEP) schemes. An application of the proposed algorithm to JPEG 2000 transmission shows the advantages of exploiting differences in SNRs between subchannels. Multiplexing of multiple sources is also considered, and additional gains are achieved by exploiting information diversity among the sources...|$|R
40|$|In this paper, we {{extend our}} {{previous}} work on joint source-channel coding to scalable video transmission over wireless direct-sequence code-division-multiple-access (DS-CDMA) multipath fading chan-nels. An SNR scalable video coder is used and unequal. error pro-tection (UEP) is allowed for each scalable layer. At the reveiver-end an adaptive antenna array Auxiliary-Vector (AV) filter is uti-lized that provides space-time RAKE-type processing and multiple-access interference suppression. The {{choice of the}} AV receiver is dictated by realistic channel fading rates that limit the data record available for receiver adaptation and redesign. Our problem is to allocate the available bit rate of the user of interest between source and channel coding and across scalable layers, while min-imizing the end-to-end distortion of the received video sequence. The optimization algorithm that we propose utilizes universal rate-distortion characteristic (curves that show the contribution of each layer to the total distortion {{as a function of}} the source rate of the layer and the <b>residual</b> bit <b>error</b> <b>rate</b> (the <b>error</b> <b>rate</b> after channel cod-ing). These plots can be approximated using appropriate functions to reduce the computatimal complexity of the solution. 1...|$|R
40|$|This paper {{investigates the}} {{potential}} improvements {{that may be}} obtained {{in terms of the}} secret key transmission rate in a Quantum Key Distribution (QKD) scheme whereby photon-counting detectors are used at the receiver. To take full advantage of such detectors, soft information is generated in the form of Log-Likelihood Ratios (LLRs) using a Bayesian estimator of phase of the signal pulse which is used to carry the information. The technique is general in a sense that any optical communication scheme whereby the received mean photon count is relatively small, but not necessarily below one and uses the polarization state of light to transmit the binary data may benefit from the soft information processing proposed, although in this paper, we have focused on the QKD application. We demonstrate using simulations the significant reduction in the <b>residual</b> Bit <b>Error</b> <b>Rate</b> (BER) and Frame <b>Error</b> <b>Rate</b> (FER) that is achievable using the proposed soft information processing scheme for a given Quantum BER (QBER) on the quantum link, after the information reconciliation process using LDPC Forward Error Correction (FEC) codin...|$|R
40|$|Abstract Accurate and efcient {{generative}} {{models are}} sig-nicant {{for the design}} and performance evaluation of wireless communication protocols as well as error control schemes. In this paper, deterministic processes are utilized to derive {{a new class of}} hard and soft generative models for simulation of digital wireless channels with hard and soft decision outputs, respectively. The proposed deterministic process based generative models (DP-BGMs) are all based on a properly parameterized and sampled deterministic process followed by a threshold detector and two parallel mappers. The target hard and soft error sequences are provided by computer simulations of uncoded enhanced general packet radio service (EGPRS) systems with typical urban (TU) and rural area (RA) channels. Simulation results indicate that the proposed DPBGMs enable us to approximate very closely all the interested burst error statistics of the target hard and soft error sequences. The validity of the suggested DPBGMs is further conrmed by the excellent match of the simulated frame <b>error</b> <b>rates</b> (FERs) and <b>residual</b> bit <b>error</b> <b>rates</b> (RBERs) of coded EGPRS systems obtained from the target and generated error sequences. Index Terms Hard and soft generative models, error models, digital wireless channels, deterministic processes, EGPRS sys-tems. I...|$|R
40|$|Recently we {{investigated}} the potential improvements in key transmission rate in a Quantum Key Distribution (QKD) scheme whereby photon-counting detectors are {{used at the}} receiver. To {{take full advantage of}} such detectors, soft information is generated in the form of Log-Likelihood Ratios (LLRs) using a Bayesian estimator of phase of the signal pulse which is used to carry the information. We achieved significant reduction in the <b>residual</b> Bit <b>Error</b> <b>Rate</b> (BER) and Frame <b>Error</b> <b>Rate</b> (FER) using LDPC codes in the information reconciliation process. In this paper we explore the limits of the achievable performance gains when using photon counting detectors as compared to the case when such detectors are not available. To this end, we find the classical capacity of the Bayesian inference channel clearly showing the potential gains that photon counting detectors can provide {{in the context of a}} realistic cost-effective scheme from an implementation point of view. While there are binary communication schemes that can achieve a higher capacity for a given mean photon count at the receiver compared to the scheme presented here (e. g., the Dolinar receiver), most such schemes are complex and at times unrealistic from an implementation point of vie...|$|R
