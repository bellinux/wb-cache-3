0|10000|Public
40|$|Abstract. We propose {{algorithms}} {{for maintaining}} two variants of kd-trees {{of a set}} of moving points in the plane. A pseudo kd-tree allows the number of points stored in the two children to differ. An overlapping kd-tree allows the bounding boxes of two children to overlap. We show that both of them support range search operations in O(n 1 / 2 +ɛ) time, where ɛ only depends <b>on</b> the <b>approximation</b> <b>precision.</b> When the points move, we use event-based kinetic data structures to update the tree when necessary. Both trees undergo only a quadratic number of events, which is optimal, and the update cost for each event is only polylogarithmic. To maintain the pseudo kd-tree, we make use of algorithms for computing an approximate median level of a line arrangement, which itself is of great interest. We show that the computation of the approximate median level {{of a set of}} lines or line segments, can be done in an online fashion smoothly, i. e., there are no expensive updates for any events. For practical consideration, we study the case when there are speed limit restrictions or smooth trajectory requirements. The maintenance of the pseudo kd-tree, {{as a consequence of the}} approximate median algorithm, can also adapt to those restrictions. ...|$|R
40|$|Abstract. The {{motion path}} of {{grinding}} carriage about workpiece rotation angle in crankshaft tangential point tracing grinding {{is a complex}} curve. For such complicated movement curve, it’s necessary to find a proper path approximation method to meet the high requirement of speed and precision of path interpolation. Sectioned polynomial is effective to approach to complicated path which ensures the <b>approximation</b> <b>precision</b> with low-order polynomial. This paper presents a dynamic sectioned cubic polynomial <b>approximation</b> method based <b>on</b> <b>approximation</b> error which ensures {{the continuity of the}} neighboring sectioned cubic polynomial curves at their boundary points to realize the stable tangential point tracing grinding of crankshaft at high speed...|$|R
40|$|Abstract. This paper {{presents}} an analytically robust, globally convergent approach to managing {{the use of}} approximation models of various fidelity in optimization. By robust global behavior we mean the mathematical assurance that the iterates produced by the optimization algorithm, started at an arbitrary initial iterate, will converge to a stationary point or local optimizer for the original problem. The approach we present {{is based on the}} trust region idea from nonlinear programming and is shown to be provably convergent to a solution of the original high-fidelity problem. The proposed method for managing approximations in engineering optimization suggests ways to decide when the fidelity, and thus the cost, of the approximations might be fruitfully increased or decreased {{in the course of the}} optimization iterations. The approach is quite general. We make no assumptions on the structure of the original problem, in particular, no assumptions of convexity and separability, and place only mild <b>requirements</b> <b>on</b> the <b>approximations.</b> The approximations used in the framework can be of any nature appropriate to an application; for instance, they can be represented by analyses, simulations, or simple algebraic models. This paper introduces the approach and outlines the convergence analysis. 1. Introduction. I...|$|R
40|$|The paper {{presents}} some {{analytical results}} {{pertaining to the}} estimation of variance of the parameters of a three parameter Weibull distribution (3 pW) under type-II censoring. Ageing failure data acquired on an insulating material of considerable application potential {{has been used to}} demonstrate the results. The point estimates of the parameters of failure time distribution are obtained using maximum likelihood estimation method. The true value of the variance of the ML estimates for 3 pW are hard to obtain and the situation becomes more complex when the data is censored. The asymptotic variance can be obtained by taking the inverse of the Fisher information matrix, the computation of which is quite involved in the case of censored 3 -pW data. Approximations are reported in the literature to simplify the procedure. The authors have considered the effects of such <b>approximations</b> <b>on</b> the <b>precision</b> of variance estimates when the sample size is greatly limited by practical difficulties in obtaining the authentic data. A detailed study of the effect of censoring on the ML estimates, under this condition is also presented...|$|R
40|$|The paper {{presents}} the first {{results of our}} project on surface simplification. A new solution. JADE (Just Another DEcimator), is presented. It enhances the Mesh Decimation approach by providing better <b>approximation</b> <b>precision</b> and multiresolution output management. Results are reported on empirical time complexity, approximation quality, and simplification power. Moreover, we show that with a small increase in memory, which is needed to store the mu 1 tiresolution data representation, {{we are able to}} extract at run time any level of detail representation in an extremely efficient way...|$|R
40|$|Abstract. Cryptosystems {{based on}} the {{discrete}} logarithm problem in the infrastructure of a real quadratic number eld [7, 19, 2] are very interesting from a theoretical point of view, because this problem {{is known to be}} at least as hard as, and when considering todays algorithms { as in [11] { much harder than, factoring integers. However it seems that the cryptosystems sketched in [2] have not been implemented yet and consequently it is hard to evaluate the practical relevance of these systems. Furthermore as [2] lacks any proofs regarding the involved <b>approximation</b> <b>precisions,</b> it was not clear whether the second communication round, as required in [7, 19], really could be avoided without substantial slowdown. In this work we will prove a bound for the necessary <b>approximation</b> <b>precision</b> of an exponentiation using quadratic numbers in power product representation and show that the precision given in [2] can be lowered considerably. As the highly space consuming power products can not be applied in environments with limited RAM, we will propose a simple (CRIAD 1 -) arithmetic which entirely avoids these power products. Beside the obvious savings in terms of space this method is also about 30 % faster. Furthermore one may apply more sophisticated exponentiation techniques, which nally result in a ten-fold speedup compared to [2]. ...|$|R
30|$|Proposition  3 {{exhibits}} {{that the}} time-domain convolution theorem, which is fit for infinite-length sequences, {{is no longer}} guaranteed for truncated finite-length sequences. Therefore the spectrum at each frequency for a finite-length sequence computed by the z-transform making use of time-domain convolution theorem is only an approximate formulation. The <b>approximation</b> <b>precision</b> depends <b>on</b> the truncation length and {{the stability of the}} impulse response. Thus the validity of the existing z-transform-based controller design, convergence analysis, and robustness for ILC systems in [28 – 31] needs to be clarified in a rigorous manner.|$|R
40|$|A {{feature of}} new {{economic}} geography model is their mathematical intractability. This intractability {{results from the}} fact that the functional relationship between the indirect utility differential and the state variable cannot be found explicitly. We illustrate three methods that can be utilized to approximate the unknown function. These methods are simple and give a remarkable improvement in the <b>precision</b> of <b>approximation</b> with respect to the commonly utilized Lagrange <b>approximation.</b> <b>Precision</b> of <b>approximation</b> is important in models that feature catastrophic behavior. We apply these methods to the core-periphery model. Naturally, they can be applied to all cases of unknown functional relationships. ...|$|R
40|$|It is {{well known}} that the cubic spline {{function}} has advantages of simple forms, good convergence, approximation, and second-order smoothness. A particular class of cubic spline function is constructed and an effective method to solve the numerical solution of nonlinear dynamic system is proposed based on the cubic spline function. Compared with existing methods, this method not only has high <b>approximation</b> <b>precision,</b> but also avoids the Runge phenomenon. The error analysis of several methods is given via two numeric examples, which turned out that the proposed method is a much more feasible tool applied to the engineering practice...|$|R
40|$|Abstract. Computation time is an {{important}} performance characteristic of computer vision algorithms. This paper shows how existing (slow) binary-valued decision algorithms can be approximated by a trained WaldBoost classifier, which minimises the decision time while guaranteeing predefined <b>approximation</b> <b>precision.</b> The core idea is to take an existing algorithm as a black box performing some useful binary decision task and to train the WaldBoost classifier as its emulator. Two interest point detectors, Hessian-Laplace and Kadir-Brady saliency detector, are emulated to demonstrate the approach. The experiments show similar repeatability and matching score of the original and emulated algorithms while achieving a 70 -fold speed-up for Kadir-Brady detector. ...|$|R
40|$|Three diluents were studied, to {{determine}} {{which is the best}} for the automated immunochemical measurement of specific serum proteins. Nine serum proteins (oroso-mucoid, a 1 -antitrypsin, a 2 -macroglobulin, haptoglobin, transferrin, C 3, lgG, IgA, and 1 gM) were measured in physiological saline (9 g NaCI/liter), tris(hydroxymeth-yl) aminomethane buffer (0. 01 mol/liter; pH 7. 4), and physiological saline containing polyethylene glycol (‘PEG 6000, ” 40 g/liter). Criteria were: reaction rate, analysis rate, carryover between samples, steady-state <b>approximation,</b> <b>precision,</b> and correlation with other methods. Saline containing polyethylene giycol is the best of the three diluents for use in continuous-flow nephelometric analysis of serum proteins. Automated systems in which immunochemica...|$|R
40|$|This paper {{explores the}} {{robustness}} of Guerre, Perrigne and Vuongs (2000) two-step non-parametric estimation procedure in 8 ̆ 5 rst-price, sealed-bid auctions with n (n 1) risk averse bidders. Based <b>on</b> an asymptotic <b>approximation</b> with <b>precision</b> of order O(n 2) of the intractable equilibrium bidding function, we establish the uniform consistency with rates of convergence of Guerre, Perrigne and Vuongs (2000) two-step nonparametric estimator {{in the presence}} of risk aversion. Monte Carlo experiments show that the two-step nonparametric estimator performs reasonably well with a moderate number of bidders such as six...|$|R
40|$|AbstractIn {{order to}} enhance the {{stability}} of RBF neural network under constant changing conditions, we propose a new scheme of forecasting method based on single model structure. The method can avoid inherent defects of double model structure and improve the network's online self-correcting ability. Self-correcting is realized by continuous online learning and structure adjustment according to the changes of monitoring environment. New samples are used to improve the network <b>approximation</b> <b>precision</b> during work time. The neural network shows remarkable adaptability to slow and quick changing environment from simulation result and the minimum error is only 0. 0005 during time 49995 s∼ 50005 s. According to the simulation of memory optimization, system could adjust the structure of hidden layer adaptively...|$|R
40|$|This {{thesis is}} {{a study of the}} degree of uniform linear {{approximation}} with side conditions. The side conditions considered fall into four categories namely: Lagrange interpolatory side conditions imposed <b>on</b> <b>approximation</b> from finite dimensional subspaces of C(T),T compact Hausdorff; Hermite-Birkhoff interpolatory side conditions imposed <b>on</b> <b>approximation</b> by algebraic or trigonometric polynomials on finite intervals; the side condition "increasing to the right" imposed <b>on</b> <b>approximation</b> by algebraic polynomials on finite intervals (the results here are applied to rational <b>approximation</b> <b>on</b> [0,∞)); and generalized monotonicity side conditions imposed <b>on</b> <b>approximation</b> by algebraic polynomials on finite intervals. Jackson type estimates are obtained for the degree of approximation in each case. In addition, for the side conditions of an interpolatory type, best possible asymptotic bounds are found for the ratio of, the degree of approximation with side conditions, to, the degree of unconstrained approximation...|$|R
2500|$|The {{proof is}} based <b>on</b> <b>approximations</b> using {{continued}} fractions.|$|R
2500|$|Workshop <b>on</b> <b>Approximation</b> Algorithms for Combinatorial Optimization Problems (APPROX) ...|$|R
50|$|Bitplane formats may be {{used for}} passing images to Spiking neural networks, or low <b>precision</b> <b>approximations</b> to neural networks/convolutional neural networks.|$|R
40|$|Finite <b>precision</b> <b>approximations</b> of {{discrete}} probability distributions are considered, applicable for distribution synthesis, e. g., probabilistic shaping. Two algorithms are presented that find the optimal M-type approximation Q of a distribution P {{in terms of}} the variational distance | Q-P|_ 1 and the informational divergence D(Q| P). Bounds <b>on</b> the <b>approximation</b> errors are derived and shown to be asymptotically tight. Several examples illustrate that the variational distance optimal approximation can be quite different from the informational divergence optimal approximation. Comment: Submitted to the IEEE Transactions on Information Theor...|$|R
40|$|Abstract—Piecewise-linear {{approximation}} is {{an efficient}} {{solution to the}} problem of modeling the sampling dataset. But the <b>approximation</b> <b>precision</b> of the piecewise linear function has a close correlation with domain partition. In this paper, a triangle partition based construction algorithm is proposed, which uses the techniques similar to what had been applied for minimum weight triangulation problem. The most important characteristics of the algorithm are: we used feature point which is local maximum or local minimum point as the vertices of triangle and then choose each triangle greedily by criteria of minimizing l 2 -distance. By optimizing the positions of all triangular vertices, the proposed algorithms possess a better performance for adapting itself to a concrete approximation problem. As a result, the precision of PWL function defined on this partition is improved greatly. I...|$|R
25|$|<b>On</b> <b>Approximation</b> of set-valued {{functions}} by continuous functions. Colloq. Math. 19 1968 285—293.|$|R
25|$|Jurjen Koksma in 1939 {{proposed}} another classification based <b>on</b> <b>approximation</b> by algebraic numbers.|$|R
50|$|<b>On</b> <b>Approximation</b> of set-valued {{functions}} by continuous functions. Colloq. Math. 19 1968 285—293.|$|R
50|$|Jurjen Koksma in 1939 {{proposed}} another classification based <b>on</b> <b>approximation</b> by algebraic numbers.|$|R
30|$|In fact, these <b>requirements</b> <b>on</b> {{intelligence}} {{start with}} the <b>requirements</b> <b>on</b> information in the school environment.|$|R
50|$|The East Journal <b>on</b> <b>Approximations</b> is {{a journal}} about {{approximation}} theory published in Sofia, Bulgaria.|$|R
5000|$|WAOA, the Workshop <b>on</b> <b>Approximation</b> and Online Algorithms, {{has been}} part of ALGO since 2003.|$|R
50|$|In {{geometric}} optics laws are based <b>on</b> <b>approximations</b> in Euclidean geometry (such as the paraxial approximation).|$|R
50|$|He worked <b>on</b> <b>approximation</b> theory, {{especially}} Padé approximants. Nikishin {{systems of}} functions are named after him. Also named in his honour is the Nikishin-Stein factorisation theorem, {{which is a}} 1970 generalization by Nikishin of the Stein factorisation theorem. Nikishin also did research <b>on</b> rational <b>approximations</b> in number theory and wrote a monograph <b>on</b> such <b>approximations</b> in a unified approach that also treated rational approximations in function spaces.|$|R
5000|$|L. A. Khan, <b>On</b> <b>approximation</b> in {{weighted}} {{spaces of}} continuous vector-valued functions, Glasgow Math. J. 29(1987), 65-68 ...|$|R
40|$|Due to {{the surface}} meshes {{produced}} at increasing complexity in many applications, interest in efficient simplification algorithms and multiresolution representation is very high. An enhanced simplification approach together with a general multiresolution data scheme are presented here. JADE, a new simplification solution based on the Mesh Decimation approach {{has been designed to}} provide both increased <b>approximation</b> <b>precision,</b> based <b>on</b> global error management, and multiresolution output. Moreover, we show that with a small increase in memory, which is needed to store the multiresolution data representation, we are able to extract any level of detail representation from the simplification results in an extremely efficient way. Results are reported on empirical time complexity, approximation quality, and simplification power. Keywords: surface modeling, mesh simplification, bounded approximation error, multiresolution. Address to which proofs should be sent: R. SCOPIGNO, CNUCE [...] Consigl [...] ...|$|R
5000|$|The {{proof is}} based <b>on</b> <b>approximations</b> using {{continued}} fractions. Since , {{there exists a}} [...] such that [...] Therefore ...|$|R
3000|$|... -monotonicity, {{and recall}} some {{investigations}} <b>on</b> <b>approximation</b> solvability {{of a general}} class of nonlinear inclusion problems involving maximal [...]...|$|R
40|$|The {{data type}} also allows to {{evaluate}} real expressions with arbitrary precision. One may either set the mantissae {{length of the}} underlying floating point system and then evaluate the expression with that mantissa length or one may specify an error bound q. The data type then computes an approximation with absolute error at most q. The implementation of the data type real {{is based on the}} LEDA data types integer and bigfloat which are the types of arbitrary precision integers and floating point numbers, respectively. The implementation takes various shortcuts for increased efficiency, e. g., a double approximation of any real number together with an error bound is maintained and tests are first performed <b>on</b> these <b>approximations.</b> A high <b>precision</b> computation is only started when the test <b>on</b> the double <b>approximation</b> is inconclusive...|$|R
30|$|Because {{most of the}} {{techniques}} are based <b>on</b> <b>approximation,</b> uncertainty management {{must be taken into}} consideration which is another challenge.|$|R
40|$|In this paper, using rest bounded {{variation}} sequences {{and head}} bounded variation sequences, some new results <b>on</b> <b>approximation</b> of functions (signals) by almost generalized Nörlund means of their Fourier series are obtained. To {{our best knowledge}} {{this the first time}} to use such classes of sequences <b>on</b> <b>approximations</b> of the type treated in this paper. In addition, several corollaries are derived from our results as well as those obtained previously by others...|$|R
50|$|The {{research}} on truthful job scheduling aims to find upper (positive) and lower (negative) bounds <b>on</b> <b>approximation</b> factors of truthful mechanisms.|$|R
