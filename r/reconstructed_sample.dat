25|336|Public
5000|$|... "Sesame Street" [...] by Blowfly, {{interesting}} {{testimony of}} breakbeat science as the breakbeat is reconstructed from various places with solo drums in the song. Also known as [...] "Helicopter Break" [...] after [...] "The Helicopter Tune" [...] by Deep Blue, {{which is the}} common second-hand source of the <b>reconstructed</b> <b>sample.</b>|$|E
30|$|For {{subspace}} methods, {{the number}} of principal components used for projection is a key parameter for the degree of de-noising. In our framework based on kernel PCA, we empirically observed (see results in Section 6) that {{the number of}} used components has almost no influence. We therefore ignore the projection step and only perform the reconstruction step necessary to determine the sample in input space corresponding to the de-noised sample in feature space. We call this pre-image iterations (PI) for speech enhancement, as the <b>reconstructed</b> <b>sample</b> in input space is called pre-image.|$|E
40|$|When {{transmitting}} a {{sampled signal}} digitally, data and error correction bits must be transmitted {{at least as}} fast as the sampling rate. Typically, each bit is allocated the same transmission time interval, which means the optimal detector yields the same error probability for each bit. An alternative is to vary the bit interval duration according to the bit's contribution to the <b>reconstructed</b> <b>sample.</b> The optimal solution yields significant gains in mean-squared error (several dB) over that provided by equal-duration bit intervals. These gains occurred over a wide range of signal-to-noise ratios. When block error correction is performed, we derive the optimal decoder from a Bayesian viewpoint and show that gains obtain here as well...|$|E
40|$|Abstract—This paper {{provides}} a technical overview {{of a newly}} added in-loop filtering technique, sample adaptive offset (SAO), in High Efficiency Video Coding (HEVC). The key idea of SAO is to reduce sample distortion by first classifying <b>reconstructed</b> <b>samples</b> into different categories, obtaining an offset for eac...|$|R
30|$|Encoder-driven {{rate control}} is a {{sensitive}} process since underestimation leads to failed channel decoding and poorly <b>reconstructed</b> <b>samples</b> while overestimation wastes rate and no longer reduces the distortion level. To counter the effect of over- and underestimation on the overall RD performance, the proposed system includes additional tools.|$|R
40|$|A {{separable}} {{decomposition of}} bidirectional reflectance distributions (BRDFs) {{is used to}} implement arbitrary reflectances from point sources on existing graphics hardware. Two-dimensional texture mapping and compositing operations are used to <b>reconstruct</b> <b>samples</b> of the BRDF at every pixel at interactive rates. A change of variables, the Gram-Schmidt halfangle/difference vector parameterization, improves separability. Two decomposition algorithms are also presented. The singular value decomposition (SVD) minimizes RMS error. The normalized decomposition is fast and simple, using no more space than what {{is required for the}} final representation...|$|R
40|$|There {{has been}} a rapidly growing {{interest}} in three-dimensional micro-structural reconstruction of fuel cell electrodes so as to derive more accurate descriptors of the pertinent geometric and effective transport properties. Due to the limited accessibility of experiments based reconstruction techniques, such as dual-beam focused ion beam-scanning electro microscopy or micro X-Ray computed tomography, within sample micro-structures of the catalyst layers in polymer electrolyte membrane fuel cells (PEMFCs), a particle based numerical model is {{used in this study}} to reconstruct sample microstructure of the catalyst layers in PEMFCs. Then the <b>reconstructed</b> <b>sample</b> structure is converted into the computational grid using body-fitted/cut-cell based unstructured meshing technique. Finally, finite volume methods (FVM) are applied to calculate effective properties on computational sample domains...|$|E
40|$|In {{the past}} few years, {{discriminant}} analysis and manifold learning {{have been widely used}} in feature extraction. Recently, the sparse representation technique has advanced the development of pattern recognition. In this paper, we combine both discriminant analysis and manifold learning with sparse representation technique and propose a novel feature extraction approach named sparsity preserving embedding with manifold learning and discriminant analysis. It seeks an embedded space, where not only the sparse reconstructive relations among original samples are preserved, but also the manifold and discriminant information of both original sample set and the corresponding <b>reconstructed</b> <b>sample</b> set is maintained. Experimental results on the public AR and FERET face databases show that our approach outperforms relevant methods in recognition performance. Department of Computin...|$|E
40|$|Semi-supervised {{learning}} methods using Generative Adversarial Networks (GANs) {{have shown}} promising empirical success recently. Most {{of these methods}} use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by {{the ability of the}} GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i. e., the encoder) which greatly improves in terms of semantic similarity of the <b>reconstructed</b> <b>sample</b> with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure. Comment: NIPS 2017 accepted version, including appendi...|$|E
40|$|Understanding the {{mechanical}} properties of ancient paintings {{is a major}} issue for conservation and restoration. One strategy is to measure {{the mechanical}} properties of reconstructed paints: however the aging process is poorly known, so it is also desirable to measure mechanical properties directly on ancient paint samples. Using nanoindentation, we have characterized submillimetric samples recovered from restoration of two Van Gogh paintings and compared the results with <b>reconstructed</b> paint <b>samples.</b> We demonstrate that the reduced modulus and hardness of historical paints can be measured at a very local scale, even differentiating between each paint layer. Our <b>reconstructed</b> paint <b>samples</b> exhibit elastic moduli comparable to literature values, but the values measured on the two 19 th century paint samples {{are found to be}} significantly larger. Similarly, the compositional dependence of the elastic modulus is consistent with literature results for our <b>reconstructed</b> <b>samples</b> while our preliminary results for ancient samples do not readily fall into the same pattern. These results all point out to a significant impact of long term aging, in a manner which is difficult to predict in our present state of understanding. They demonstrate that nanoindentation is a very adequate tool to improve our knowledge of art paint mechanics and aging...|$|R
40|$|Modern dual-column FIB-SEMs {{are capable}} of {{automatically}} milling and acquiring images {{that can be used}} to <b>reconstruct</b> <b>sample</b> volumes in 3 D. We will demonstrate that this technique, applied to the environmental degradation of materials in nuclear reactors, is capable of revealing features, phases and/or defects in 3 D with nm resolution. In this paper, we have used this technique to characterize surface oxidation and cracking in Zr alloys, the effect of cold work on the oxidation resistance of austenitic steels and crack growth in welded 316 L stainless steel...|$|R
5000|$|... #Caption: Illustration of 4 {{waveforms}} <b>reconstructed</b> from <b>samples</b> {{taken at}} six different rates. Two of the waveforms are sufficiently sampled to avoid aliasing at all six rates. The other two illustrate increasing distortion (aliasing) {{at the lower}} rates.|$|R
40|$|Abstract. We {{consider}} the parametric {{estimation of the}} driving Lévy process of a multivariate continuous-time autoregressive moving average (MCARMA) process, which is observed on the discrete time grid (0, h, 2 h, [...] .). Beginning with a new state space representation, we develop a method to recover the driving Lévy process exactly from a continuous record of the observed MCARMA process. We use tools from numerical analysis and the theory of infinitely divisible distributions to extend this result {{to allow for the}} approximate recovery of unit increments of the driving Lévy process from discrete-time observations of the MCARMA process. We show that, if the sampling interval h = hN is chosen dependent on N, the length of the observation horizon, such that NhN converges to zero as N tends to infinity, then any suitable generalized method of moments estimator based on this <b>reconstructed</b> <b>sample</b> of unit increments has the same asymptotic distribution as the one based o...|$|E
40|$|Accurate {{estimates}} for petrophysical properties {{are very important}} in {{the oil and gas}} industry. For two-phase flow in porous media, experimental techniques for determining these properties can be difficult to set up and can take a very long time. Generalization of these experimental results for the whole reservoir requires adapting boundary and environmental conditions which may require a large num- ber of experimental set-ups, hence emphasizing the need for numerical methods. Several numerical techniques like pore network models have been used in the past to estimate and quantify transport properties like relative permeability in reservoir rocks. These network models are however mostly restricted to quasi-static condi- tions and have many free parameters; thus are not easily predictive. An alternative is to carry out simulations directly on imaged samples of rock micro-structure using lattice Boltzmann techniques which are far more able to handle complex geometrical boundaries than classical computational fluid dynamics techniques. In this thesis, we examine the effect of four initialization techniques namely; the Euclidean Distance Transform, Covering Radius Transform, Capillary Drainage Transform, and Uniform Random distribution on the final fluid distribution in a porous media. These techniques were applied to a <b>reconstructed</b> <b>sample</b> of Fontainebleau sandstone. The convergence of the initial saturations to steady state is quantified by the Minkowski functionals and the capillary drainage transform is shown to be the most stable of the four initialization techniques. Three lattice Boltzmann (Shan-Chen, the Color Gradient, and the Free Energy) approaches are also introduced for two phase simulation in a 2 D geometry. The methods are vali- dated against standard test cases in 2 D. The Shan-Chen solver was then applied to a <b>reconstructed</b> <b>sample</b> of Fontainebleau sandstone for the different initial conditions. Furthermore, the initial conditions are compared with regard to discretisation ef- fects and with regard to speed of convergence. For the simple case of 2 D simulations we also apply a body force to the two phase simulation. In three dimensions, we use a two relaxation time (scheme which is known for its better accuracy and stability) Shan and Chen lattice Boltzmann approach to simulate fluid redistribution for the different initial conditions studied in the thesis...|$|E
40|$|We {{consider}} the parametric {{estimation of the}} driving Lévy process of a multivariate continuous-time autoregressive moving average (MCARMA) process, which is observed on the discrete time grid (0,h, 2 h, [...] .). Beginning with a new state space representation, we develop a method to recover the driving Lévy process exactly from a continuous record of the observed MCARMA process. We use tools from numerical analysis and the theory of infinitely divisible distributions to extend this result {{to allow for the}} approximate recovery of unit increments of the driving Lévy process from discrete-time observations of the MCARMA process. We show that, if the sampling interval h=h_N is chosen dependent on N, the length of the observation horizon, such that N h_N converges to zero as N tends to infinity, then any suitable generalized method of moments estimator based on this <b>reconstructed</b> <b>sample</b> of unit increments has the same asymptotic distribution as the one based on the true increments, and is, in particular, asymptotically normally distributed. Comment: 38 pages, four figures; to appear in Journal of Multivariate Analysi...|$|E
40|$|International audienceReconstruction {{of mixed}} oxides {{resulting}} from the thermal decomposition at 723 K of ZnAl and ZnMgAl layered double hydroxides (LDHs) has been studied performing mechanical stirring or ultrasonic treatments for different periods of time in aqueous media. Thesematerialswere fully characterized by several physico-chemicalmethods including powder X-Ray diffraction and Rietveld refinements, XANES and XASF spectroscopies. The {{results show that the}} crystallinity increases in the LDH samples after the reconstruction. The crystallite sizes of the reconstructed LDHs increase in comparison to the as-prepared LDHs and this effect is improved upon ultrasound treatments. Moreover, <b>reconstructed</b> <b>samples</b> did not completely recover the original structure and ZnO phase was formed at the expense of an amorphous phase present in the as-prepared LDH. The application of ultrasound favors the formation of ZnO. Incorporation ofMg as ternary cation to obtain Mg/Zn/Al LDH decreases the crystallinity and leads to more disordered materials. The Mg/Zn/Al LDHs present lower amount of amorphous phase than ZnAl LDHs which also decreases in the <b>reconstructed</b> <b>samples.</b> But, in contrast to the behavior observed for Zn/Al, ultrasonication alters the crystallinity and increase of the amount of amorphous phasewhen sonication timeincreases. The basicity (number, strength and nature of sites) increases after reconstruction of themixed oxides into the LDH structures {{as well as with the}} sonication time; while the opposite trend was observed for the acid sites. Moreover the basicity is greatly enhanced upon introduction ofMg in the LDH. The changes in the population of acid-base sites affect the behavior of the catalysts in the carbonylation of glycerol...|$|R
40|$|Discrete Fourier Transform (DFT) {{encoding}} {{over the}} real (or complex) field {{has been proposed}} {{as a means to}} <b>reconstruct</b> <b>samples</b> lost in multimedia transmissions over packet-based networks. A collection of simple sample reconstruction (and error detection) algorithms makes DFT codes an interesting candidate. A common problem with DFT code sample reconstruction algorithms is that the quantization associated with practical implementations results in reconstruction errors that are particularly large when lost samples occur in bursts (bursty erasures). Following a survey of DFT decoding algorithms, we present herein the Tandem Filterbank/DFT Code (TFBD). The TFBD code consists of a tandem arrangement of a filterbank and DFT encoder that effectively creates DFT codes along the rows (temporal codevectors) and columns (subband codevectors) of the frame under analysis. The tandem arrangement ensures that subband codevectors (the frame columns) will be DFT codes, and we show how the temporal codevectors (frame rows) can also be interpreted as DFT codes. All the subband and temporal codevectors can be used to <b>reconstruct</b> <b>samples</b> entirely independently of each other. An erasure burst along a particular codevector can then be broken up by <b>reconstructing</b> some lost <b>samples</b> along the remaining orientation; these samples can then be used as received <b>samples</b> in <b>reconstructing</b> the original codevector, a technique that we refer to as pivoting. Expressions related to the performance of the Tandem Filterbank/DFT (TFBD) code, including an expression for the temporal code reconstruction error and for temporal-to-subband pivoting operations, are derived and verified through simulations. The expressions also prove useful in the selection of the many parameters specifying a TFBD encoder. The design process is illustrated for two sample TFBD codes that are then compared to a benchmark DFT code at the same rate. The results show that the TFBD encoder is capable of reconstruction error improvements that are more than four orders of magnitude better than that of the benchmark DFT code...|$|R
40|$|Summary: GENIE {{implements}} {{a statistical}} framework for inferring the demographic {{history of a}} population from phylogenies that have been <b>reconstructed</b> from <b>sampled</b> DNA sequences. The methods are based on population genetic models known collectively as coalescent theory. Availability: GENIE is available fro...|$|R
40|$|To {{ensure the}} actual {{occurrence}} {{of a real}} reasonable feature {{in contrast to a}} fake self manufactured synthetic or <b>reconstructed</b> <b>sample</b> is a significant problem in biometric confirmation, which requires the development of new and efficient protection procedures. In this paper, we present a novel software-based fake detection method {{that can be used in}} multiple biometric systems to detect different types of deceitful access attempts. The objective of the proposed system is to enrich the security of biometric appreciation frameworks, by adding live ness assessment in a fast, user-friendly, and non-intrusive manner, through the use of image worth assessment. The proposed approach presents a very low degree of difficulty, which makes it suitable for real-time applications, using 25 general image quality structures extracted from one image (i. e., the same acquired for authentication purposes) to distinguish between appropriate and impostor samples. The investigational results, obtained on publicly available data sets of fingerprint, iris, and 2 D face, show that the proposed method is highly economical compared with other state-of-the-art attitudes and that the analysis of the common image quality of real biometric samples reveals highly valuable evidence that may be very efficiently used to distinguish them from fake traits...|$|E
40|$|Abstract — To {{ensure the}} actual {{presence}} of a real legiti-mate trait {{in contrast to a}} fake self-manufactured synthetic or <b>reconstructed</b> <b>sample</b> is a significant problem in biometric authentication, which requires the development of new and efficient protection measures. In this paper, we present a novel software-based fake detection method {{that can be used in}} multiple biometric systems to detect different types of fraudulent access attempts. The objective of the proposed system is to enhance the security of biometric recognition frameworks, by adding liveness assessment in a fast, user-friendly, and non-intrusive manner, through the use of image quality assessment. The proposed approach presents a very low degree of complexity, which makes it suitable for real-time applications, using 25 general image quality features extracted from one image (i. e., the same acquired for authentication purposes) to distinguish between legitimate and impostor samples. The experimental results, obtained on publicly available data sets of fingerprint, iris, and 2 D face, show that the proposed method is highly competitive compared with other state-of-the-art approaches and that the analysis of the general image quality of real biometric samples reveals highly valuable information that may be very efficiently used to discriminate them from fake traits. Index Terms — Image quality assessment, biometrics, security, attacks, countermeasures...|$|E
40|$|International audienceWe {{report an}} ecient atom-scale {{reconstruction}} method {{that consists of}} combining the Hybrid Reverse Monte Carlo algorithm (HRMC) with Molecular Dynamics (MD) {{in the framework of}} a simulated annealing technique. In the spirit of the experimentally constrained molecular relaxation technique [Biswas et al., Phys. Rev. B 69, 195207 (2004) ], this modified procedure o↵ers a refined strategy in the field of reconstruction techniques, with special interest for heterogeneous and disordered solids such as amorphous porous materials. While the HRMC method generates physical structures, thanks to the use of energy penalties, the combination with MD makes the method at least one order of magnitude faster than HRMC simulations to obtain structures of similar quality. Furthermore, in order to ensure the transferability of this technique, we provide rational arguments to select the various input parameters such as the relative weight ! of the energy penalty with respect to the structure optimization. By applying the method to disordered porous carbons, we show that adsorption properties provide data to test the global texture of the <b>reconstructed</b> <b>sample</b> but are only weakly sensitive to the presence of defects. In contrast, the vibrational properties such as the phonon density of states are found to be very sensitive to the local structure of the sample...|$|E
50|$|Tomography. In X-ray tomography, one {{of these}} modes is {{combined}} with sample rotation to produce a series of two-dimensional projection images, {{to be used for}} <b>reconstructing</b> the <b>sample’s</b> internal three-dimensional structure. This will be particularly important for observing the morphology of complex nanostructures.|$|R
25|$|In signal {{processing}} and related disciplines, aliasing is an effect that causes different signals to become indistinguishable (or aliases of one another) when sampled. It also {{refers to the}} distortion or artifact that results when the signal <b>reconstructed</b> from <b>samples</b> {{is different from the}} original continuous signal.|$|R
40|$|Abstract—Often the {{adaptive}} interpolation filter {{provides a more}} accurate interpolation technique {{in the formulation of}} the sub-pixel reference frame for motion estimation and compensation. Inspired by its possible advantages, we make use of {{the adaptive}} interpolation filter for efficient lossless intra prediction in H. 264 /AVC. Specifically, four subblocks are firstly formed by sampling pixels in one macroblock/block, and then a hierarchical intra prediction is performed on these subblocks. The first subblock is predicted based on the intra spatial prediction method in H. 264 /AVC, and subsequently it is encoded and reconstructed. The remaining three subblocks are then predicted based on adaptive interpolation filters by using the neighboring <b>reconstructed</b> <b>samples.</b> The residual block is encoded by rate optimization. Experimental results show that the proposed algorithm can improve the compression efficiency of lossless intra coding...|$|R
40|$|To {{ensure the}} actual {{presence}} of a real legitimate trait {{in contrast to a}} fake self-manufactured synthetic or <b>reconstructed</b> <b>sample,</b> is a significant problem in biometric authentication, which requires the development of new and efficient protection measures. In this work we present a novel software-based fake detection method which can be used in multiple biometric systems to detect different types of fraudulent access attempts. The objective of the proposed system is to enhance the security of biometric recognition frameworks, by adding liveness assessment in a fast, user-friendly and non-intrusive manner, through the use of Image Quality Assessment (IQA). The proposed approach presents a very low degree of complexity which makes it suitable for real-time applications, using 25 general image quality features extracted from one image (i. e., the same acquired for authentication purposes) to distinguish between legitimate and impostor samples. The experimental results, obtained on publicly available datasets of fingerprint, iris and 2 -D face, show that the proposed method is highly competitive compared to other state-of-the-art approaches and that the analysis of the general image quality of real biometric samples reveals highly valuable information that may be very efficiently used to discriminate them from fake traits. JRC. G. 6 -Digital Citizen Securit...|$|E
40|$|A {{limestone}} {{sample was}} scanned using computed tomography (CT) and the hydraulic conductivity of the 3 D <b>reconstructed</b> <b>sample</b> was determined using Lattice- Boltzmann methods (LBM) at varying scales. Due {{to the shape}} {{and size of the}} original sample, it was challenging to obtain a consistent rectilinear test sample. Through visual inspection however, 91 mm and 76 mm samples were digitally cut from the original. The samples had porosities of 58 % and 64 % and produced hydraulic conductivity values of K= 13. 5 m/s and K= 34. 5 m/s, respectively. Both of these samples were re-sampled to 1 / 8 and 1 / 64 of their original size to produce new virtual samples at lower resolutions of 0. 542 mm/lu and 1. 084 mm/lu, while still representing the same physical dimensions. The hydraulic conductivity tended to increase slightly as the resolution became coarser. In order to determine an REV, the 91 mm sample was also sub-sampled into blocks that were 1 / 8 and 1 / 64 the size of the original. The results were consistent with analytical expectations such as those produced by the Kozeny-Carman equation. A definitive REV size was not reached, however, indicating the need for a larger sample. The methods described here demonstrate the ability of LBM to test rock structures and sizes not normally attainable...|$|E
40|$|We {{describe}} a new B-meson full reconstruction algorithm {{designed for the}} Belle experiment at the B-factory KEKB, an asymmetric e+e − collider that collected a data sample of 771. 6 × 106 BB ̄ pairs during its running time. To maximize the number of reconstructed B decay channels, it utilizes a hierarchical reconstruc-tion procedure and probabilistic calculus instead of classical selection cuts. The multivariate analysis package NeuroBayes was used extensively to hold the bal-ance between highest possible efficiency, robustness and acceptable consumption of CPU time. In total, 1104 exclusive decay channels were reconstructed, employing 71 neural networks altogether. Overall, we correctly reconstruct one B ± or B 0 candidate in 0. 28 % or 0. 18 % of the BB ̄ events, respectively. Compared to the cut-based classical reconstruction algorithm used at the Belle experiment, this is an improvement in efficiency by roughly a factor of 2, depending on the analysis considered. The new framework also features the ability to choose the desired purity or efficiency of the fully <b>reconstructed</b> <b>sample</b> freely. If the same purity as for the classical full reconstruction code is desired (∼ 25 %), the efficiency is still larger by nearly a factor of 2. If, on the other hand, the efficiency is chosen at a similar level as the classical full reconstruction, the purity rises from ∼ 25 % to nearl...|$|E
40|$|In {{this paper}} {{we present a}} novel {{approach}} for <b>sampling</b> and <b>reconstructing</b> any K-sided convex and bilevel polygon {{with the use of}} exponential splines (E-splines) [1]. The Fourier transform of bilevel polygons, Radon transform and the projection-slice theorem are all utilized to <b>reconstruct</b> <b>sampled</b> bilevel polygons. It will be shown that with K+ 1 projections we are able to perfectly reconstruct a K-sided bilevel polygon from its samples using E-splines as the sampling kernel. By projections we mean line integrals at arbitrary angles tan − 1 (n), where m and n are the indices of the samples. We will also show that the minimum E-spline m order required for perfect reconstruction is N = p. (2 K − 2) where p = max(m, n) needed in order to produce at least K+ 1 projections. ...|$|R
40|$|SUMMARY We have {{identified}} and quantified a Mycobacterium avium subsp. silvaticum (MAS) 1612 insertion sequence in ovine clinical samples, derived {{from two different}} sheep farms with animals showing signs of enteritis; those samples were resulting negative for Mycobacterium avium subsp. paratuberculosis (MAP) IS 900 PCR but IS 1612 presence in the samples, represented by stool and gut tracts, was demonstrated by real time PCR and capillary sequencing methods. The real time PCR detected in <b>reconstructed</b> <b>samples</b> about 100 CFU/g and showed a high linear dynamic range of quantification (102 - 106 IS 1612 copies DNA/reaction) with a good correlation rate (R 2 = 0. 98). Results suggest that real time PCR assay represents a rapid and accurate molecular method to detect/quantify Mycobacterium avium subsp. silvaticum in biological samples. ...|$|R
40|$|A bandlimited signal can be {{recovered}} from its periodic nonuniformly spaced samples provided the average sampling rate {{is at least}} the Nyquist rate. A multirate filter bank structure is used to both model this nonuniform sampling (through the analysis bank) and <b>reconstruct</b> a uniformly <b>sampled</b> sequence (through the synthesis bank). Several techniques for modelling the nonuniform sampling are presented for various cases of sampling. Conditions on the filter bank structure are used to accurately <b>reconstruct</b> uniform <b>samples</b> of the input signal at the Nyquist rate. Several examples and simulation results are presented, with emphasis on forms of nonuniform sampling that {{may be useful in}} mixed-signal integrated circuits...|$|R
40|$|The {{detection}} of a flux of high-energy neutrinos has {{open a new}} observation window on the high-energy Universe. The KM 3 NeT project aims to build a network of neutrino detectors in the Mediterranean Sea: ORCA, to be installed close to the Southern French coast and dedicated mainly to study the neutrino mass hierarchy; ARCA, which represents the northern counterpart of IceCube, the largest neutrino telescope presently in data taking. In the Phase- 2 configuration, ARCA will be made of 230 vertical structures, called Detection Units, each hosting 18 Digital Optical Modules, and will occupy 1 cubic kilometre. Each DOM hosts 31 PMTs for detecting the Cherenkov light emitted along the path of relativistic charged particles in sea water. The sensors are continuously stressed by light sources, such as 40 K beta decay and bioluminescence, in the underwater environment, requiring {{the use of an}} on-line filtering system, called TriDAS, which is also discussed. The ARCA detector is suited {{to be part of a}} global network of observatories that perform multi-messenger astrophysical and astronomy studies. The Astrophysical Neutrino Trigger System, ANTS, presented and described in this thesis, aims to perform a very fast on-line track reconstruction to drive follow-up observations by other observatories in a multi-messenger context, for neutrino induced muons in the the energy range 10 TeV < E < 10 PeV. The capabilities of ANTS, in terms of reconstruction efficiency, purity of the <b>reconstructed</b> <b>sample,</b> angular resolution, and processing speed, are discussed...|$|E
40|$|This work {{extends the}} {{lossless}} data compression technique described in Fast Lossless Compression of Multispectral- Image Data, (NPO- 42517) NASA Tech Briefs, Vol. 30, No. 8 (August 2006), page 26. The original technique {{was extended to}} include a near-lossless compression option, allowing substantially smaller compressed file sizes when {{a small amount of}} distortion can be tolerated. Near-lossless compression is obtained by including a quantization step prior to encoding of prediction residuals. The original technique uses lossless predictive compression and is designed for use on multispectral imagery. A lossless predictive data compression algorithm compresses a digitized signal one sample at a time as follows: First, a sample value is predicted from previously encoded samples. The difference between the actual sample value and the prediction is called the prediction residual. The prediction residual is encoded into the compressed file. The decompressor can form the same predicted sample and can decode the prediction residual from the compressed file, and so can reconstruct the original sample. A lossless predictive compression algorithm can generally be converted to a near-lossless compression algorithm by quantizing the prediction residuals prior to encoding them. In this case, since the <b>reconstructed</b> <b>sample</b> values will not be identical to the original sample values, the encoder must determine the values that will be reconstructed and use these values for predicting later sample values. The technique described here uses this method, starting with the original technique, to allow near-lossless compression. The extension to allow near-lossless compression adds the ability to achieve much more compression when small amounts of distortion are tolerable, while retaining the low complexity and good overall compression effectiveness of the original algorithm...|$|E
40|$|Photons are {{produced}} through various {{processes in the}} particle and nuclei collisions at the LHC, and pose as useful probes {{of the formation of}} a Qaurk-Gluon Plasma. The probability of a photon converting within the central tracking detectors of ALICE {{has been found to be}} approximately 8 - 9 %. The thesis explores the performance of the photon reconstruction through the conversion method, for the High Level Trigger using the central tracking detectors of ALICE. The photon reconstruction method through its conversion will compliment the photon detection performed by the calorimeters. However, due to the large acceptance of the central tracking detectors, the method has proved equally beneficial through the off-line event reconstruction, even though the conversion probability is low. It has therefore been a goal to map the potential performance of an on-line reconstruction of the photons through the conversion method. The performance of the photon reconstruction has been analyzed through the simulation, and subsequent High Level Trigger reconstruction, of highly contrived events and minimum bias p-p collisions at √s = 14 TeV per nucleon pair. The events were embedded with additional Γs in the central pseudo-rapidity range to provide higher statistics. The analysis led to the optimization and further development of the existing code in the High Level Trigger, dedicated to the on-line reconstruction of neutral particles through the detection of their decay products. An overview of the reconstruction algorithm and potential future upgrades is provided. The results obtained for the simulated data is presented along side the optimal off-line results. The final results obtained for the Γ embedded minimum bias p-p collisions for the on-line reconstruction, yielded an average photon conversion reconstruction efficiency of 41. 7 %, with an average purity of the <b>reconstructed</b> <b>sample</b> of 74. 2 %...|$|E
40|$|This paper {{proposes a}} {{post-processing}} technique for reducing tile boundary artifacts which occur in an image {{when it is}} compressed at low bit rate using JPEG 2000 standard. Symmetric extension and difference in quantization accuracy between the tiles of the image are the main factors behind the tile boundary artifacts in JPEG 2000 compressed images. In this paper, we have analyzed the effect of quantization on {{the region of the}} tile boundaries of JPEG 2000 compressed images. The analysis confirms that tiling artifacts are reduced by updating the high pass <b>reconstructed</b> <b>samples</b> lying on the boundary of the image tiles where the artifacts occur. The post-processing is applied on the output of JPEG 2000 coding system and thus it can easily be blended with JPEG 2000 standard. Keywords: JPEG 2000, MSE, PSNR, DWT, boundary artifacts, quantization...|$|R
40|$|Abstract. A {{separable}} {{decomposition of}} bidirectional reflectance distributions(BRDFs) {{is used to}} implement arbitrary reflectances from point sources on existing graphics hardware. Two-dimensional texture mapping and compositing op-erations are used to <b>reconstruct</b> <b>samples</b> of the BRDF at every pixel at interactive rates. A change of variables, the Gram-Schmidt halfangle/difference vector parameterization, improves separability. Two decomposition algorithms are also presented. The singular value decomposition (SVD) minimizes RMS error. The normalized decomposition is fast and simple, using no more space than what is required forthe final representation. 1 Introduction Traditionally hardware renderers only support the Phong lighting model [19] in combi-nation with Gouraud shading. However, the Phong lighting model is strictly empirical and physically implausible. Gouraud shading also tends to undersample the highlightunless a highly tessellated surface is used. In general, surface reflectance can be described using a bidirectional reflectance dis-tribution, or BRDF. The reflectance equation describes the outgoing radianc...|$|R
40|$|Abstract. In {{the last}} decade speech {{processing}} has been applied in commercially available products. One of the key reasons for its success is the identification and use of an underlying set of generic symbols (phonemes) constituting all speech. In this work we follow the same approach, but for the problem of human body gestures. That is, the topic {{of this paper is}} how to define a framework for automatically finding primitives for human body gestures. This is done by considering a gesture as a trajectory and then searching for points where the density of the training data is high. The trajectories are re-sampled to enable a direct comparison between the samples of each trajectory, and enable time invariant comparisons. This work demonstrates and tests the primitive’s ability to <b>reconstruct</b> <b>sampled</b> trajectories. Promising test results are shown for samples from different test persons performing gestures from a small one armed gesture set. ...|$|R
