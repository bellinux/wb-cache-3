1|1654|Public
40|$|The Employment Retention and Advancement (ERA) Demonstration {{programme}} {{is a major}} current welfare-to-work social experiment, {{the largest}} random allocation evaluation ever mounted in Great Britain. This article draws on experience gained in designing the ERA Demonstration to explore the strengths and limitations of social experimentation for policy evaluation and analysis. The focus of the discussion is on {{the reasons for the}} choice of random allocation as a mean of estimating programme impacts, contrasting this approach with the alternatives. The weaknesses of <b>random</b> <b>allocation</b> <b>designs</b> are also examined {{in the light of the}} types of information policy-makers require from evaluations of labour market programmes and social policy demonstrations. The perennial ‘black box’ problem and the difficulties in generalizing from social experiments are given particular prominence...|$|E
40|$|This paper {{describes}} {{an evaluation of}} a skills training programme for long-term unemployed adults in the UK, for which a <b>random</b> <b>allocation</b> <b>design</b> had been ruled out. After an initial survey of programme leavers, a matched comparison group was constructed from administrative records, based on claim dates, sex, age and locality. The close match achieved was partially undermined by subsequent sample attrition, necessitating a re-matching process. Results showed {{a high degree of}} selection into the Training for Work Programme even after matching. Subsequent improvements in administrative data in the UK have increased the viability of this type of design, and where analysis can be carried out entirely with administrative data, the problem of sample attrition is overcome. ...|$|R
40|$|AbstractObjectives. The aim of {{this study}} is to compare the endothelial {{integrity}} of saphenous vein grafts harvested by minimally invasive surgery and veins harvested conventionally for coronary artery bypass surgery in 200 participants who were assigned to interventions by using <b>random</b> <b>allocation.</b> <b>Design.</b> Randomized controlled trial. Methods. Immunocytochemistry with anti-CD 31 antibodies and anti-nitric oxide synthase (NOS) antibodies were employed to identify the endothelial integrity. Results. The CD 31 immunostaining showed that the endothelial cell integrity of the minimally invasive harvested veins was preserved in 82 ± 13 % of the circumference of luminal endothelium, while in conventionally harvested grafts it was reduced to 64 ± 15 % (p= 0. 05). > This was associated with the lack of CD 31 expression in vasa vasorum (10 and 18 %) in both groups, respectively, (p= 0. 02). The NOS immunostaining revealed that the endothelial integrity of the minimally invasive harvested grafts was preserved in 96 ± 4 % of the luminal endothelium circumference as compared to 74 ± 10 % in conventionally harvested grafts (p= 0. 05). The percentage of cases with the lack of NOS expression in all vasa vasorum was 12 and 21 %, in G 1 and G 2, respectively, (p= 0. 02). Conclusion. The endothelial integrity of saphenous vein grafts harvested by minimally invasive surgery is better preserved than with the grafts obtained by the conventional manner. This could play an important role in improving vein graft patency rates...|$|R
40|$|To {{explore the}} {{effectiveness}} of a mentalization-based therapeutic intervention specifically developed for parents in entrenched conflict over their children. To the best of our knowledge, this is the first randomized controlled intervention study in the United Kingdom to work with both parents postseparation, and the first to focus on mentalization in this situation. Using a mixed-methods study design, 30 parents were randomly allocated to either mentalization-based therapy for parental conflict—Parenting Together, or the Parents’ Group, a psycho-educational intervention for separated parents based on elements of the Separated Parents Information Program—part of the U. K. Family Justice System and approximating to treatment as usual. Given the challenges of recruiting parents in these difficult circumstances, the sample size was small and permitted only the detection of large differences between conditions. The data, involving repeated measures of related individuals, was explored statistically, using hierarchical linear modeling, and qualitatively. Significant findings were reported on the main predicted outcomes, with clinically important trends on other measures. Qualitative findings further contributed to the understanding of parents’ subjective experience, pre- and posttreatment. Findings indicate that a larger scale randomized controlled trial would be worthwhile. These encouraging findings shed light on the dynamics maintaining these high-conflict situations known to be damaging to children. We established that both forms of intervention were acceptable to most parents, {{and we were able to}} operate a <b>random</b> <b>allocation</b> <b>design</b> with extensive quantitative and qualitative assessments of the kind that would make a larger-scale trial feasible and productive...|$|R
40|$|We {{studied the}} effect of acute (1 day) and {{subacute}} (16 days) administration of the new antidepressant, nefazodone (400 mg daily), and the selective serotonin re-uptake inhibitor (SSRI), paroxetine (30 mg daily), on the sleep polysomnogram of 37 healthy volunteers using a <b>random</b> <b>allocation,</b> double-blind, placebo-controlled <b>design.</b> Compared to placebo, paroxetine lowered rapid eye movement (REM) sleep and increased REM latency. In addition, paroxetine increased awakenings and reduced Actual Sleep Time and Sleep Efficiency. In contrast, nefazodone did not alter REM sleep and {{had little effect on}} measures of sleep continuity. We conclude that in contrast to typical SSRIs, nefazodone administration has little effect on sleep architecture in healthy volunteers...|$|R
40|$|Adetunji Oladeni Adeniji, 1 Oluseyi Olaboyede A Atanda 21 Department of Obstetrics and Gynaecology, Ladoke Akintola University of Technology, Ogbomoso, Nigeria; 2 Department of Obstetrics and Gynaecology, Ladoke Akintola University of Technology Teaching Hospital, Osogbo, NigeriaBackground: Postoperative pain {{leads to}} patient discomfort, {{decreased}} level of satisfaction, prolonged recovery, and higher health costs. Acute pain control therefore improves {{the overall quality}} of life in patients undergoing cesarean section. Pain relief is a fundamental human right, but there is no gold standard for post–cesarean section pain management. Objective: To compare the efficacy of pentazocine and tramadol used in unimodal and multimodal (in combination with piroxicam) approach, in the management of post–cesarean section pain. Materials and methods: This study employed a <b>random</b> <b>allocation</b> <b>design</b> to compare the effectiveness of intramuscular pentazocine (60 mg) or tramadol (100 mg) as single analgesic agent and in combination with daily intramuscular piroxicam 20 mg, for the management of post–cesarean section pain during the immediate 12 hours after surgery. The primary outcome measure was control of postoperative pain, while the secondary outcome measures were the analgesic agent onset of action, duration of action, patient satisfaction, and maternal and neonatal adverse outcomes. Data obtained were entered into a predesigned sheet and analyzed with the Statistical Package for Social Sciences version 17. Means ± standard deviation (SD) were calculated for the quantitative variables, and the difference between two independent groups was compared using unpaired Student&# 39;s t-test. The level of significance was set at 0. 05. Results: A total of 120 patients were equally and randomly allocated to four study groups – two that received unimodal analgesia (the pentazocine group and the tramadol group) and two that received multimodal analgesia (the pentazocine-piroxicam group and the tramadol-piroxicam group). Among the unimodal groups, tramadol had a faster onset of action, but pentazocine had a longer duration of action and provided better control of pain. Among the multimodal groups, the combination of pentazocine with piroxicam was superior to the tramadol with piroxicam combination, and it was also more effective than pentazocine alone. Conclusion: The multimodal approach of combining pentazocine with piroxicam is a safe, effective, and an acceptable mode of analgesia for post–cesarean section pain management, especially in a resource-constrained setting. Keyword: NSAID, opiods, piroxicam, tramadol, pentazocin...|$|R
40|$|This study {{reports the}} {{quantitative}} effect of students using podcasts in a 1 st year undergraduate exercise physiology module. From {{a cohort of}} 70 students, 50 volunteered and completed the study. Using a pre-post <b>random</b> <b>allocation</b> research <b>design,</b> students were allocated to either a podcast group (PG) or control group (CG) based on a 32 -question multiple-choice exam. The PG then listened to six podcasts over six weeks, while the CG were provided with an exact transcript of the podcasts in printed form to ensure that both groups were provided with the same content. After six weeks, both groups were re-examined using the same test. Data were analysed using the effect size statistic and 90 % confidence intervals. The CG improved their exam performance by 43 %, whereas the PG improved by 46 %. The difference between the groups on the post-test was a mean effect size of 0. 19 (90 %CI: - 0. 16 to 0. 53 [trivial to positively small]). There is almost no chance that the true effect in the population is harmful. The {{results of this study}} suggest that using podcasts provides little quantitative benefit for students over and above written text when learning exercise physiology...|$|R
40|$|Objective To {{summarise}} {{comparisons of}} randomised clinical trials and non-randomised clinical trials, trials with adequately concealed <b>random</b> <b>allocation</b> versus inadequately concealed <b>random</b> <b>allocation,</b> and high quality trials versus low quality trials where {{the effect of}} randomisation could {{not be separated from}} the effects of other methodological manoeuvres...|$|R
5000|$|... {{which is}} {{exponentially}} less than with totally <b>random</b> <b>allocation.</b>|$|R
50|$|Instead of {{just putting}} m balls, it is {{possible}} to consider an infinite process in which, at each time step, a single ball is added and a single ball is taken, such that the number of balls remains constant. For m=n, after a sufficiently long time, with high probability the maximum load is similar to the finite version, both with <b>random</b> <b>allocation</b> and with partially <b>random</b> <b>allocation.</b>|$|R
25|$|Medicine: <b>Random</b> <b>allocation</b> of a {{clinical}} intervention {{is used to}} reduce bias in controlled trials (e.g., randomized controlled trials).|$|R
40|$|AbstractWe analyze <b>random</b> <b>allocation</b> {{applied to}} {{irregular}} and dynamic task-parallel {{programs such as}} branch and bound. The precedence between jobs is revealed on-line, and the processing times of jobs are diverse and unknown before job completion. The objective is to assign jobs to processors and to schedule them to minimize makespan. We show that <b>random</b> <b>allocation</b> achieves makespan close to a natural lower bound. Some empirical experience with irregular parallel applications is reported...|$|R
30|$|Random {{sequence}} generation: All {{the studies}} mentioned <b>random</b> <b>allocation,</b> but none mentioned the detail of sequence generation. Thus, the sequence generation was not clear.|$|R
40|$|Background: This paper {{addresses}} one {{threat to}} the internal validity of a randomized controlled trial (RCT), selection bias. Many authors argue that <b>random</b> <b>allocation</b> is used to ensure baseline equality between study conditions in a given study and that statistically significant differences at pretest mean that randomisation has failed. Purpose: The {{purpose of this study}} was to clarify the role of <b>random</b> <b>allocation</b> in an RCT study. Is the role of <b>random</b> <b>allocation</b> to protect against selection bias? And does it have a further role, namely to ensure baseline equality and the absence of statistically significant differences between study conditions at pretest? Setting: The participants for this study were 229 children in 1 st and 2 nd grade and data were collected as part of an RCT evaluation of a volunteer reading programme piloted in Ireland, Wizards of Words (WoW) ...|$|R
40|$|Randomized {{trials are}} the {{preferred}} tool for patient-oriented research, and their main {{role is to}} enable the transfer of results from basic research to routine application. While the need for randomized trials is evident, conducting these trials is becoming increasingly difficult and complex. This article reviews actual and conflicting issues of clinical trials with respect to gastroenterology. Major problems in trial design are neglect of previous research, inadequate sample size calculations and irrelevant outcome criteria. Significant trial management problems include subversion of <b>random</b> <b>allocation,</b> and the <b>design</b> of systems and procedures that are inefficient, ineffective and inflexible. One of the major challenges in conducting randomized, controlled trials is obtaining informed consent because of the differing perspectives and languages of physicians and patients. Recommendations include practical guidance in obtaining informed consent, feedback of trial results to patients and support of research related to obtaining informed consent. Despite statistical guidance, several critical issues persist with respect to trial analysis. The use of confidence intervals is under-represented, the presentation of baseline data is often omitted and postsubgroup analysis is performed. Another controversial but relevant issue is the intention-to-treat analysis. Despite the formulation of standards, there is consistently poor quality of trial reporting, poor registration of unpublished trials and limited registration of ongoing trials. The authors conclude {{that there is a}} need for more randomized trials in gastroenterology. While the complexity of trial conduction has increased, so have the means of methodological and practical support. Thus, all problems can be professionally tackled, resulting in good clinical research...|$|R
40|$|Simulation-based {{inference}} plays a {{major role}} in modern statistics, and often employs either reallocating (as in a randomization test) or resampling (as in bootstrapping). Reallocating mimics <b>random</b> <b>allocation</b> to treatment groups, while resampling mimics random sampling from a larger population; does it matter whether the simulation method matches the data collection method? Moreover, do the results differ for testing versus estimation? Here we answer these questions in a simple setting by exploring the distribution of a sample difference in means under a basic two group design and four different scenarios: true <b>random</b> <b>allocation,</b> true <b>random</b> sampling, reallocating, and resampling. For testing a sharp null hypothesis, reallocating is superior in small samples, but reallocating and resampling are asymptotically equivalent. For estimation, resampling is generally superior, unless the effect is truly additive. Moreover, these results hold regardless of whether the data were collected by random sampling or <b>random</b> <b>allocation...</b>|$|R
30|$|Ideally, {{the causal}} effect of {{vocational}} education on outcomes {{can be identified}} if the individuals {{were randomly assigned to}} vocational or academic education. While <b>random</b> <b>allocation</b> is unfeasible in the real word, exogenous events (such as reforms of the school system) can produce situations equivalent to <b>random</b> <b>allocation.</b> A particularly suitable reform of vocational education has been exploited for instance by Malamud and Pop-Eleches (2010), who have found that in Romania the causal differential effect of vocational education is close to zero and that the entire difference between raw average outcomes is driven by self-sorting.|$|R
40|$|Abstract. Almost sure limit theorems are {{presented}} for <b>random</b> <b>allocations.</b> A general almost sure limit theorem is proved for arrays of random variables. It {{is applied to}} obtain almost sure versions of the central limit theorem {{for the number of}} empty boxes when the parameters are in the central domain. Almost sure versions of the Poisson limit theorem in the left domain are also proved. AMS 2000 subject classification: 60 F 05, 60 F 15, 60 C 05. Key words and phrases. Almost sure central limit theorem, <b>random</b> <b>allocation.</b> 1...|$|R
40|$|Although {{randomised}} controlled {{trials are}} the reference methodology {{to assess the}} effects of therapeutic interventions, for interventions that naturally occur in groups of individuals <b>random</b> <b>allocation</b> of participants may be inappropriate. In these cases, the unit of <b>random</b> <b>allocation</b> may be the group or cluster, rather than the individual. Clinical trials that randomly allocate groups or clusters of individuals are called cluster randomised trials. This article briefly presents the main implications of cluster randomisation {{with respect to the}} following methodological aspects: generalisability, concealment of allocation, comparability at baseline, blindness, loss of clusters and intra-class correlation...|$|R
30|$|Finally, after {{reducing}} N and n to one, {{the algorithm}} degenerates to <b>random</b> <b>allocation</b> actually. In short, the bat algorithm can get satisfactory results when choosing the appropriate parameters.|$|R
5000|$|File contiguity, {{because many}} {{filesystem}} architectures employ higher I/O speeds if transferring data on contiguous {{areas of the}} storage, whereas <b>random</b> <b>allocation</b> might prevent real-time or better loading performances.|$|R
40|$|Abstract. —Hyperclust {{is a new}} {{clustering}} method {{with which}} we search for hierarchical struc-ture in binary data. These hierarchical trees have the following properties. Any two objects (branches), that are joined at a node, share common characters {{in a manner consistent}} with a <b>random</b> <b>allocation</b> model. This model uses a character pool which is explicitly defined for every node. The test statistic (number of characters shared) follows a hypergeometric probability distribution. Furthermore alternative <b>random</b> <b>allocation</b> models can be tested by using different character pools. Finally by constructing alternative trees incorporating overlapping subsets of objects, we can test whether the local structure within those subsets can be adequately described by a hierarchical model. [Statistical clustering; hierarchical clustering; classification. ] We present a technique for exploring the structure in binary data sets. This method attempts to organize objects into hierarchical trees such that the structure of the trees is consistent with a <b>random</b> <b>allocation</b> model. Unlike most hierarchical clustering method...|$|R
50|$|However, RSD is not Pareto {{efficient}} {{when the}} agents have Von Neumann-Morgenstern utilities over <b>random</b> <b>allocations</b> (lotteries over objects).In fact, there exits no mechanism that satisfies symmetry, truthfulness and Pareto efficiency.|$|R
5000|$|Note {{that for}} , the <b>random</b> <b>allocation</b> process gives only the maximum load of [...] with high probability, so the {{improvement}} {{between these two}} processes is especially visible for large values of [...]|$|R
30|$|In [11], Monte-Carlo {{simulations}} {{were used}} to measure the impact of several low carbon technologies, including EVs and PVs. Similar to our approach, the authors used a realistic LV network with 7 feeders and sampled from realistic profiles for load and for LCTs. Note that the network area examined in our paper is significantly larger, with 44 feeders considered. Their focus is on identifying thermal and voltage problems in different feeders. While they use a <b>random</b> <b>allocation</b> of LCTs, we compare a <b>random</b> <b>allocation</b> with a clustered one using socio-demographic information, although here the focus is the load profile impact.|$|R
5000|$|In 2003, an {{even more}} {{controversial}} admission policy was carried out. [...] Since 2003, the school is still allowed for citywide recruitment, but with an additional preliminary <b>random</b> <b>allocation</b> procedure executed by a computer.|$|R
40|$|<b>Random</b> <b>allocation</b> is an {{important}} feature of experiments designed to assess the effects of interventions: it ensures that, in respect of measured and unmeasured factors of prognostic importance, the comparison groups generated will differ only by chance. It has been asserted that <b>random</b> <b>allocation</b> in experiments to assess the effects of social and educational interventions was introduced at least {{as early as the}} first quarter of the 20 th century. However, because the term 'experiment' and words with the root 'random-' have not been adequately defined and kept apart in these accounts, there is still confusion regarding the studies that have been cited as examples on early randomisation. We examined these putative examples and found that they were not randomised trials. It seems that matching on prognostic variables was the predominant method used to generate comparison groups in social and education intervention studies. The earliest description of a <b>random</b> <b>allocation</b> procedure in a social or educational intervention study that we were able to identify, was published in 1928. Randomised controlled trials/history, Social sciences/history,...|$|R
40|$|In {{this paper}} a novel job {{allocation}} scheme in distributed systems (TAG) is modelled using the Markovian process al-gebra PEPA. This scheme requires no {{prior knowledge of}} job size and {{has been shown to}} be more efficient than round robin and <b>random</b> <b>allocation</b> when the job size distribution is heavy tailed and the load is not high. In this paper the job size dis-tribution is assumed to be of a phase-type and the queues are bounded. Numerical results are derived and compared with those derived from models employing <b>random</b> <b>allocation</b> and the shortest queue strategy. It is shown that TAG can perform well for a range of performance metrics. ...|$|R
50|$|To enter for the 2008 World Championship race, {{athletes were}} {{required}} to qualify through performance at an Ironman or selected Ironman 70.3 race, through Hawaii residency, through a <b>random</b> <b>allocation</b> lottery, or by invitation from the WTC.|$|R
40|$|This paper {{investigates the}} {{performance}} of an orthogonal frequency-division multiplexing (OFDM) -based cognitive radio (CR) spectrum sharing communication system that assumes <b>random</b> <b>allocation</b> and absence of the primary user's (PU) channel occupation information, i. e., no spectrum sensing is employed to acquire information about the availability of unused subcarriers. In case of a single secondary user (SU) in the secondary network, {{due to the lack}} of information of PUs' activities, the SU randomly allocates the subcarriers of the primary network and collide with the PUs' subcarriers with a certain probability. To maintain the quality of service (QoS) requirement of PUs, the interference that SU causes onto PUs is controlled by adjusting SU's transmit power below a predefined threshold, referred to as interference temperature. In this work, the average capacity of SU with subcarrier collisions is employed as performance measure to investigate the proposed <b>random</b> <b>allocation</b> scheme for both general and Rayleigh channel fading models. Bounds and scaling laws of average capacity with respect to the number of SU's, PUs' and available subcarriers are derived. In addition, in the presence of multiple SUs, the multiuser diversity gain of SUs assuming an opportunistic scheduling is also investigated. To avoid the interference at the SUs that might be caused by the <b>random</b> <b>allocation</b> scheme and obtain the maximum sum rate for SUs based on the available subcarriers, an efficient centralized sequential algorithm based on the opportunistic scheduling and <b>random</b> <b>allocation</b> (utilization) methods is proposed to ensure the orthogonality of assigned subcarriers. Comment: To appear in IEEE Transactions on Signal Processin...|$|R
40|$|Well {{distributed}} {{data can}} dramatically improve {{the efficiency and}} effectiveness {{of the use of}} distributed database systems to satisfy geographically dispersed data processing demands. Among several issues related to distribution design in distributed databases, data <b>allocation</b> <b>design</b> is of major importance. Choices of a fragmentation strategy and location of database files are two critical decisions. Thus far, solutions of these design problems, although interdependent, have been attempted separately. Solving both design problems simultaneously in a real design setting is not a trivial task. By formulating typical data <b>allocation</b> <b>design</b> problems, we can analyze the solution space and analytical properties of optimal data <b>allocation</b> <b>design.</b> Based on this, we suggest that clustering data elements into uniform fragments and then allocating these fragments is equivalent to solving the data <b>allocation</b> <b>design</b> as a whole. Such analytical examination of the data <b>allocation</b> <b>design</b> problem has not been attempted by other researchers, but it is essential to provide the theoretical foundation for solving the fragmentation <b>design</b> and fragment <b>allocation</b> <b>design</b> problem. We then extended the research by studying the effect on design issues of such characteristics of distributed processing as database access patterns, network scope, and design objectives. We also propose a generic taxonomy of data <b>allocation</b> <b>design</b> models. We further advance data <b>allocation</b> <b>design</b> skills in the following two directions. The first of these involves developing a design method that guarantees the minimum number of fragments to be considered as units of allocation. This improves upon existing fragment allocation methodologies, which are based on the assumed units of allocation. The second direction involves enhancements in modeling and solution procedures that allow efficient fragment <b>allocation</b> <b>design.</b> Concentration is on information processing environments, which have received little attention in the research literature. We first studied databases connected on local area networks under weak locality of reference. The model proposed is validated by simulation study. We then explored the multiple design objective optimization phase, which involves searching for models where several design objectives are in conflict. We addressed three important design objectives including response time, operating cost and data availability. In conclusion, we submit that the methodology proposed is likely to provide a better understanding of data <b>allocation</b> <b>design</b> problems, the solutions for which are expected to continue providing key design tools as advancing data communication techniques evolve...|$|R
40|$|This paper {{analyzes}} the economic role {{and performance of}} a type of financial institution that is observed worldwide: rotating savings and credit associations. Using a model in which individuals save for an indivisible durable consumption good, the authors study rotating savings and credit associations that distribute funds using <b>random</b> <b>allocation</b> and bidding. Each type of rotating savings and credit association allows individuals without access to credit markets to improve their welfare but, under a reasonable assumption on preferences, <b>random</b> <b>allocation</b> is preferred when individuals have identical tastes. This conclusion need not hold when individuals are heterogeneous. The authors also discuss the sustainability of rotating savings and credit associations given the possibility of default. Copyright 1993 by American Economic Association. ...|$|R
40|$|Abstract Background Typically, {{randomization}} software {{should allow}} users to exert {{control over the}} different aspects of randomization including block design, provision of unique identifiers and control over the format and type of program output. While some of these characteristics have been addressed by available software, {{none of them have}} all of these capabilities integrated into one package. The main objective of the <b>Random</b> <b>Allocation</b> Software project was to enhance the user's control over different aspects of randomization in parallel group trials, including output type and format, structure and ordering of generated unique identifiers and enabling users to specify group names for more than two groups. Results The program has different settings for: simple and blocked randomizations; length, format and ordering of generated unique identifiers; type and format of program output; and saving sessions for future use. A formatted random list generated by this program can be used directly (without further formatting) by the coordinator of the research team to prepare and encode different drugs or instruments necessary for the parallel group trial. Conclusions <b>Random</b> <b>Allocation</b> Software enables users to control different attributes of the <b>random</b> <b>allocation</b> sequence and produce qualified lists for parallel group trials. </p...|$|R
30|$|To obtain {{effective}} CS {{measurements for}} each target, <b>random</b> <b>allocation</b> of pulses {{must ensure that}} the measurements of every target satisfy the two conditions presented in Section  2. Based on this, we propose an algorithm to determine the probability distribution of A.|$|R
30|$|We base our {{adaptive}} power <b>allocation</b> <b>design</b> on {{the following}} optimization problem.|$|R
50|$|To enter for the 2011 World Championship race, age-group {{athletes are}} {{required}} to qualify through a performance at an Ironman or selected Ironman 70.3 race. Entry into the race can also be obtained through a <b>random</b> <b>allocation</b> lottery or through the Ironman’s charitable eBay auction.|$|R
