3061|3|Public
25|$|The {{medium quality}} photo uses only 4.3% of the storage space {{required}} for the uncompressed image, but has little noticeable loss of detail or visible artifacts. However, once a certain threshold of compression is passed, compressed images show increasingly visible defects. See the article on <b>rate–distortion</b> theory for a mathematical explanation of this threshold effect. A particular limitation of JPEG {{in this regard is}} its non-overlapped 8×8 block transform structure. More modern designs such as JPEG 2000 and JPEG XR exhibit a more graceful degradation of quality as the bit usage decreases– by using transforms with a larger spatial extent for the lower frequency coefficients and by using overlapping transform basis functions.|$|E
2500|$|... lossy data compression: allocates bits {{needed to}} {{reconstruct}} the data, within a specified fidelity level measured by a distortion function. This subset of information theory is called <b>rate–distortion</b> theory.|$|E
2500|$|Quality values {{account for}} {{about half of the}} {{required}} disk space in the FASTQ format (before compression), and therefore the compression of the quality values can significantly reduce storage requirements and speed up analysis and transmission of sequencing data. Both lossless and lossy compression are recently being considered in the literature. For example, the algorithm QualComp [...] performs lossy compression with a rate (number of bits per quality value) specified by the user. Based on <b>rate-distortion</b> theory results, it allocates the number of bits so as to minimize the MSE (mean squared error) between the original (uncompressed) and [...] the reconstructed (after compression) quality values. Other algorithms for compression of quality values include SCALCE [...] and Fastqz. Both are lossless compression algorithms that provide an optional controlled lossy transformation approach. For example, SCALCE reduces the alphabet size based on the observation that “neighboring” quality values are similar in general.|$|E
50|$|<b>Rate-distortion</b> theory {{gives an}} {{analytical}} expression {{for how much}} compression can be achieved using lossy compression methods. Many of the existing audio, speech, image, and video compression techniques have transforms, quantization, and bit-rate allocation procedures that capitalize on the general shape of <b>rate-distortion</b> functions.|$|E
5000|$|... #Subtitle level 2: Connecting <b>rate-distortion</b> {{theory to}} channel {{capacity}} ...|$|E
5000|$|<b>Rate-distortion</b> theory: A {{mathematical}} {{basis for}} data compression, Englewood Cliffs, NJ: Prentice-Hall, 1971.|$|E
50|$|<b>Rate-distortion</b> {{theory was}} created by Claude Shannon in his {{foundational}} work on information theory.|$|E
50|$|<b>Rate-distortion</b> theory {{tell us that}} 'no {{compression}} system exists that performs outside the gray area'. The closer a practical {{compression system}} is to the red (lower) bound, the better it performs. As a general rule, this bound can only be attained by increasing the coding block length parameter. Nevertheless, even at unit blocklengths one can often find good (scalar) quantizers that operate at distances from the <b>rate-distortion</b> function that are practically relevant.|$|E
5000|$|Psychovisual <b>Rate-distortion</b> {{optimization}} which {{attempts to}} maintain a similar complexity. The complexity is measured {{using a combination of}} SSD and SATD.|$|E
50|$|<b>Rate-distortion</b> {{optimization}} (RDO) is {{a method}} of improving video quality in video compression. The name refers to the optimization {{of the amount of}} distortion (loss of video quality) against the amount of data required to encode the video, the rate. While it is primarily used by video encoders, <b>rate-distortion</b> optimization can be used to improve quality in any encoding situation (image, video, audio, or otherwise) where decisions have to be made that affect both file size and quality simultaneously.|$|E
50|$|Lossy {{compression}} formats {{suffer from}} generation loss: repeatedly compressing and decompressing the file will {{cause it to}} progressively lose quality. This is in contrast with lossless data compression, where data will not be lost via {{the use of such}} a procedure. Information-theoretical foundations for lossy data compression are provided by <b>rate-distortion</b> theory. Much like the use of probability in optimal coding theory, <b>rate-distortion</b> theory heavily draws on Bayesian estimation and decision theory in order to model perceptual distortion and even aesthetic judgment.|$|E
50|$|An {{analytical}} {{solution to}} this minimization problem {{is often difficult to}} obtain except in some instances for which we next offer two of the best known examples. The <b>rate-distortion</b> function of any source is known to obey several fundamental properties, the most important ones being that it is a continuous, monotonically decreasing convex (U) function and thus the shape for the function in the examples is typical (even measured <b>rate-distortion</b> functions in real life tend to have very similar forms).|$|E
5000|$|After {{defining}} {{these two}} performance metrics for the quantizer, a typical <b>Rate-Distortion</b> formulation for a quantizer design {{problem can be}} expressed {{in one of two}} ways: ...|$|E
5000|$|... lossy data compression: allocates bits {{needed to}} {{reconstruct}} the data, within a specified fidelity level measured by a distortion function. This subset of information theory is called <b>rate-distortion</b> theory.|$|E
5000|$|... where [...] is a {{parameter}} {{related to}} the slope in the <b>rate-distortion</b> curve that we are targeting and thus is related to how much we favor compression versus distortion (higher [...] means less compression).|$|E
50|$|While the {{standard}} does not define a procedure {{as to how}} to perform this form of <b>rate-distortion</b> optimization, the general outline is given in one of its many appendices: For each bit encoded by the EBCOT coder, the improvement in image quality, defined as mean square error, gets measured; this can be implemented by an easy table-lookup algorithm. Furthermore, the length of the resulting code stream gets measured. This forms for each code block a graph in the <b>rate-distortion</b> plane, giving image quality over bitstream length. The optimal selection for the truncation points, thus for the packet-build-up points is then given by defining critical slopes of these curves, and picking all those coding passes whose curve in the <b>rate-distortion</b> graph is steeper than the given critical slope. This method {{can be seen as a}} special application of the method of Lagrange multiplier which is used for optimization problems under constraints. The Lagrange multiplier, typically denoted by λ, turns out to be the critical slope, the constraint is the demanded target bitrate, and the value to optimize is the overall distortion.|$|E
5000|$|<b>Rate-distortion</b> {{optimized}} quantization is {{encountered in}} source coding for [...] "lossy" [...] data compression algorithms, where {{the purpose is}} to manage distortion within the limits of the bit rate supported by a communication channel or storage medium. In this second setting, the amount of introduced distortion may be managed carefully by sophisticated techniques, and introducing some significant amount of distortion may be unavoidable. A quantizer designed for this purpose may be quite different and more elaborate in design than an ordinary rounding operation. It is in this domain that substantial <b>rate-distortion</b> theory analysis is likely to be applied. However, the same concepts actually apply in both use cases.|$|E
50|$|The Blahut-Arimoto algorithm, co-invented by Richard Blahut, is {{an elegant}} {{iterative}} technique for numerically obtaining <b>rate-distortion</b> functions of arbitrary finite input/output alphabet sources and much {{work has been}} done to extend it to more general problem instances.|$|E
50|$|At {{asymptotically}} {{high bit}} rates, the 6 dB/bit approximation is supported for many source pdfs by rigorous theoretical analysis. Moreover, {{the structure of}} the optimal scalar quantizer (in the <b>rate-distortion</b> sense) approaches that of a uniform quantizer under these conditions.|$|E
50|$|Calculating the bit cost is {{made more}} {{difficult}} by the entropy encoders in modern video codecs, requiring the <b>rate-distortion</b> optimization algorithm to pass each block of video {{to be tested}} to the entropy coder to measure its actual bit cost. In MPEG codecs, the full process consists of a discrete cosine transform, followed by quantization and entropy encoding. Because of this, <b>rate-distortion</b> optimization is much slower than most other block-matching metrics, such as the simple sum of absolute differences (SAD) and sum of absolute transformed differences (SATD). As such it is usually used only for the final steps of the motion estimation process, such as deciding between different partition types in H.264/AVC.|$|E
50|$|Schuster holds over 50 U.S. patents {{in fields}} ranging from {{adaptive}} control over video compression to Internet telephony. He is {{the co-author of}} the book <b>Rate-Distortion</b> Based Video Compression (Kluwer Academic Publishers) and has published over 60 peer reviewed journal, proceedings, and book articles.|$|E
5000|$|If {{we assume}} that PX(x) is Gaussian with {{variance}} σ2, and if {{we assume that}} successive samples of the signal X are stochastically independent (or equivalently, the source is memoryless, or the signal is uncorrelated), we find the following analytical expression for the <b>rate-distortion</b> function: ...|$|E
50|$|Compression Rate and Visual Quality: In a {{rendering}} system, {{lossy compression}} {{can be more}} tolerable than for other use cases. Some texture compression libraries, such as crunch, allow the developer to flexibly trade off compression rate vs. visual quality, using methods such as <b>Rate-distortion</b> optimization (RDO).|$|E
5000|$|In the {{definition}} of the <b>rate-distortion</b> function, DQ and D* are the distortion between X and Y for a given QY | X(y | x) and the prescribed maximum distortion, respectively. When we use the mean squared error as distortion measure, we have (for amplitude-continuous signals): ...|$|E
50|$|Richard Blahut, (born June 9, 1937) former {{chair of}} the Electrical and Computer Engineering Department at the University of Illinois at Urbana-Champaign, {{is best known for}} his work in {{information}} theory (e.g. the Blahut-Arimoto algorithm used in <b>rate-distortion</b> theory). He received his PhD Electrical Engineering from Cornell University in 1972.|$|E
50|$|The Blahut-Arimoto algorithm, {{is often}} used to refer to a class of {{algorithms}} for computing numerically either the information theoretic capacity of a channel, or the <b>rate-distortion</b> function of a source. They are iterative algorithms that eventually converge to the optimal solution of the convex optimization problem that is associated with these information theoretic concepts.|$|E
5000|$|Assuming an FLC with [...] levels, the <b>Rate-Distortion</b> {{minimization}} {{problem can}} be reduced to distortion minimization alone.The reduced {{problem can be}} stated as follows: given a source [...] with pdf [...] and the constraint that the quantizer must use only [...] classification regions, find the decision boundaries [...] and reconstruction levels [...] to minimize the resulting distortion ...|$|E
50|$|As {{the above}} {{equations}} show, calculating a <b>rate-distortion</b> function requires the stochastic {{description of the}} input X {{in terms of the}} PDF PX(x), and then aims at finding the conditional PDF QY | X(y | x) that minimize rate for a given distortion D*. These definitions can be formulated measure-theoretically to account for discrete and mixed random variables as well.|$|E
5000|$|This <b>rate-distortion</b> {{function}} holds {{only for}} Gaussian memoryless sources. It {{is known that}} the Gaussian source is the most [...] "difficult" [...] source to encode: for a given mean square error, it requires {{the greatest number of}} bits. The performance of a practical compression system working on - say - images, may well be below the R(D) lower bound shown.|$|E
50|$|The {{analysis}} of quantization involves studying {{the amount of}} data (typically measured in digits or bits or bit rate) {{that is used to}} represent the output of the quantizer, and studying the loss of precision that is introduced by the quantization process (which {{is referred to as the}} distortion). The general field of such study of rate and distortion is known as <b>rate-distortion</b> theory.|$|E
50|$|<b>Rate-distortion</b> {{theory is}} a major branch of {{information}} theory which provides the theoretical foundations for lossy data compression; it addresses the problem of determining the minimal number of bits per symbol, {{as measured by the}} rate R, that should be communicated over a channel, so that the source (input signal) can be approximately reconstructed at the receiver (output signal) without exceeding a given distortion D.|$|E
5000|$|Suppose we have {{a source}} [...] with {{probability}} [...] of any given symbol. We wish to find an encoding [...] that generates a compressed signal [...] from the original signal while minimizing the expected distortion , where the expectation is taken over the joint probability of [...] and [...] We can find an encoding that minimizes the <b>rate-distortion</b> functional locally by repeating the following iteration until convergence: ...|$|E
5000|$|... 1993 Hayder Radha's PhD thesis {{described}} (natural) image representation methods using BSP trees. This {{includes the}} development of an optimal BSP-tree construction framework for any arbitrary input image. This framework is based on a new image transform, known as the Least-Square-Error (LSE) Partitioning Line (LPE) transform. H. Radha's thesis also developed an optimal <b>rate-distortion</b> (RD) image compression framework and image manipulation approaches using BSP trees.|$|E
5000|$|This {{minor planet}} {{was named after}} JPL {{researcher}} Matthew Klimesh (born 1968), developer of the compression algorithm used for handling the vast amount of data obtained by the discovering NEAT program. Since 1996 at JPL's Communications Systems and Research Section, his work includes data compression, <b>rate-distortion</b> theory and channel coding. The official naming citation was published by the Minor Planet Center on 9 May 2001 (...) [...]|$|E
50|$|The {{theoretical}} background of compression {{is provided by}} information theory (which {{is closely related to}} algorithmic information theory) for lossless compression and <b>rate-distortion</b> theory for lossy compression. These areas of study were essentially forged by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference.|$|E
5000|$|In {{the above}} formulation, if the bit rate {{constraint}} is neglected by setting [...] equal to 0, or equivalently {{if it is}} assumed that a fixed-length code (FLC) will be used to represent the quantized data instead of a variable-length code (or some other entropy coding technology such as arithmetic coding that is better than an FLC in the <b>rate-distortion</b> sense), the optimization problem reduces to minimization of distortion [...] alone.|$|E
