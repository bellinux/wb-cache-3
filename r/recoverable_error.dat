2|9|Public
40|$|Recent {{results have}} shown that the turbo {{principle}} which is used in channel coding and almost approaches the Shannon limit, can be used in source coding to obtain very efficient data compression schemes. Compression is achieved by puncturing a parallel concatenated turbo code to the desired rate. An encoding algorithm is presented which removes the redundancy step by step while checking if the encoded bit stream is still <b>recoverable</b> <b>error</b> free. No information about the source distribution is required in the encoding process as it can be estimated with the iterative decoding process...|$|E
40|$|In {{real world}} applications, agents- be they {{software}} agents or autonomous robots- inevitably face erroneous situations {{that have not}} been planned for. Re-planning can sometimes provide solutions to problems, but it is computationally expensive and rarely practical. We argue that replanning is not necessarily the last resort. Instead, during erroneous circumstances, agents should always take advantage of other agents in their environment. In this paper we report on early work that looks at Social Error Recovery as a particular class of exception handling that allows agents to resolve erroneous situations that are beyond their direct control. We also show how the AgentFactory Framework and its language AF-APL have been directly extended to support a basic model of Social Error Recovery. 1. Socially <b>Recoverable</b> <b>Error...</b>|$|E
40|$|Existing {{designs for}} fine-grained, dynamic information-flow control {{assume that it}} is {{acceptable}} to terminate the entire system when an incorrect flow is detected—i. e, they give up availability {{for the sake of}} confidentiality and integrity. This is an unrealistic limitation for systems such as long-running servers. We identify public labels and delayed exceptions as crucial ingredients for making information-flow <b>errors</b> <b>recoverable</b> while retaining the fundamental soundness property of non-interference, and we propose two new error-handling mechanisms that make all <b>errors</b> <b>recoverable.</b> The first mechanism builds directly on these basic ingredients, using not-a-values (NaVs) and data flow to propagate errors. The second mechanism adapts the standard exception model to satisfy the extra constraints arising from information flow control, converting thrown exceptions to delayed ones at certain points. We prove both mechanisms sound. Finally, we describe a prototype implementation of a full-scale language with NaVs and report on our experience building high-availability software components in this setting...|$|R
40|$|Electronic {{devices of}} the latest generations are often {{required}} to operate in harsh environmental conditions, where the temperature may exceed the 100 °C mark. This has {{a negative impact on}} several parameters of the electronic devices, ranging from slow-down and transient <b>recoverable</b> <b>errors</b> to permanent failures and device breakdown. To complicate the picture, electronic components tend to get warmer on their own as they operate, {{due to the fact that}} the power drawn by the devices from the power supply is dissipated by Joule effect. As time passes, heat and temperature management is becoming increasingly problematic, for reasons ranging from economical to technological ones. Packages that are able to sustain high temperatures are very expensive, and so are heat-sinks and cooling systems. In addition, high operating temperatures tend to cause malfunctioning of circuits and components, thus impacting the reliability of the electronic products which incorporate such devices. The development of new, thermal-aware design paradigms can no longer be postponed if the goal is to enable designers to fully exploit the electronic technologies of the future, being those CMOS or alternative to CMOS. This paper presents THERMINATOR, a EU-funded project that aims at providing an answer to these issues; it will address the following major challenges: 1) To devise innovative thermal models usable at different levels of abstraction, and to interface/integrate them into existing simulation and design frameworks. 2) To develop new, thermal-aware design solutions, customized for the different technologies and application domains of interest. 3) To enhance existing EDA solutions by means of thermal-aware add-on tools that will enable designers to address temperature issue...|$|R
40|$|Abstract—Existing {{designs for}} fine-grained, dynamic information-flow control {{assume that it}} is {{acceptable}} to terminate the entire system when an incorrect flow is detected—i. e, they give up availability {{for the sake of}} confidentiality and integrity. This is an unrealistic limitation for systems such as long-running servers. We identify public labels and delayed exceptions as crucial ingredients for making information-flow <b>errors</b> <b>recoverable</b> in a sound and usable language, and we propose two new errorhandling mechanisms that make all <b>errors</b> <b>recoverable.</b> The first mechanism builds directly on these basic ingredients, using not-a-values (NaVs) and data flow to propagate errors. The second mechanism adapts the standard exception model to satisfy the extra constraints arising from information flow control, converting thrown exceptions to delayed ones at certain points. We prove that both mechanisms enjoy the fundamental soundness property of non-interference. Finally, we describe a prototype implementation of a full-scale language with NaVs and report on our experience building robust software components in this setting. Keywords-dynamic information flow control, fine-grained labeling, availability, reliability, error recovery, exception handling, programming-language design, public labels, delaye...|$|R
40|$|This paper {{describes}} a semi-automated procedure for the verification {{of a large}} human-labeled data set containing online handwriting. A number of classifiers trained on the UNIPEN "trainset" is employed for detecting anomalies in the labels of the UNIPEN "devset". Multiple classifiers with different feature sets are used to increase the robustness of the automated procedure {{and to ensure that}} the number of false accepts is kept to a minimum. The rejected samples are manually categorized into four classes: (i) <b>recoverable</b> segmentation <b>errors,</b> (ii) incorrect (<b>recoverable)</b> labels, (iii) well-segmented but ambiguous cases and (iv) unrecoverable segments that should be removed. As a result of the verification procedure, a well-labeled data set is currently being generated, which will be made available to the handwriting recognition community...|$|R
40|$|Grayscale signals can be {{represented}} as sequences of integer-valued symbols. If such a symbol has alphabet { 0, 1, · · ·, 2 B − 1 } it can {{be represented}} by B binary digits. To embed information in these sequences, we are allowed to distort the symbols. The distortion measure that we consider here is squared error, however errors larger than m are not allowed. The embedded message must be <b>recoverable</b> with <b>error</b> probability zero. In this setup there is a so-called “rate-distortion function ” that tells us what the largest embedding rate is, given a certain distortion level and parameter m. First we determine this rate-distortion function for m = 1 and for m →∞. Next we compare the performance of ”low-bits modulation ” to the rate-distortion function for m → ∞. Then embedding codes are proposed based on (i) ternary Hamming codes and on the (ii) ternary Golay code. We show that all these codes are optimal {{in the sense that}} they achieve the smallest possible distortion at a given rate for fixed blocklength for any m...|$|R
40|$|Error {{resilient}} encoding {{in video}} communication {{is becoming increasingly}} important due to data transmission over unreliable channels. In this paper, we propose a new power-aware error resilient coding scheme based on network error probability and user expectation in video communication using mobile handheld devices. By considering both image content and network conditions, we can achieve a fast <b>recoverable</b> and energy-efficient <b>error</b> resilient coding scheme. More importantly, our approach allows system designers to evaluate various operating points in terms of error resilient level and energy consumption {{over a wide range}} of system operating conditions. We have implemented our scheme on an H. 263 video codec algorithm, compared it with the previous AIR, GOP and PGOP codin...|$|R
40|$|Two of {{the major}} {{projects}} in ATM development, SESAR and NextGen, both fore- cast the use of 4 D trajectories as an intermediate phase {{in the development of}} full Performance Based Trajectories. Using 4 D trajectories, the full positional and time coordinates of the aircraft are known throughout the planned trajectory. During approach, when reduced separation minimums are applied, the accuracy of this profile is most important to ensure a safe approach to the runway. One implementation of 4 D approaches is by using Required-Time of Arrival (RTA) to separate aircraft during approach. The latest Flight Management Computers are capable of calculating a flight-path w. r. t. to a RTA. This paper describes the amount of time error that can occur during approaches where an RTA is set at the runway threshold that could still be resolved by increasing or decreasing the speed-profile. The minimum and maximum bounds are referred to as control space. Using simulations, the <b>recoverable</b> time <b>error</b> is calculated. Lateral trajectories from Amsterdam Airport Schiphol, different wind conditions and two different aircraft types were included to investigate different factors influencing the time error, such as aircraft type, speed restrictions and wind. Finally, the paper discusses a new method to control time-based spacing using a closed-loop speed controller. Control & OperationsAerospace Engineerin...|$|R
40|$|Motivation: Serial Analysis of Gene Expression (SAGE) is a {{powerful}} technology for measuring global gene expression, through rapid generation {{of large numbers of}} transcript tags. Beyond their intrinsic value in differential gene expression analysis, SAGE tag collections afford abundant information on {{the size and shape of}} the sample transcriptome and can accelerate novel gene discovery. These latter SAGE applications are facilitated by the enhanced method of Long SAGE. A characteristic of sequencing-based methods, such as SAGE and Long SAGE is the unavoidable occurrence of artifact sequences resulting from sequencing errors. By virtue of their low-random incidence, such tag errors have minimal impact on differential expression analysis. However, to fully exploit the value of large SAGE tag datasets, it is desirable to account for and correct tag artifacts. Results: We present estimates for occurrences of tag errors, and an efficient error correction algorithm. Error rate estimates are based on a stochastic model that includes the Polymerase chain reaction and sequencing error contributions. The correction algorithm, SAGEScreen, is a multi-step procedure that addresses ditag processing, estimation of empirical error rates from highly abundant tags, grouping of similarsequence tags and statistical testing of observed counts. We apply SAGEScreen to Long SAGE libraries and compare error rates for several processing scenarios. Results with simulated tag collections indicate that SAGEScreen corrects 78 % of <b>recoverable</b> tag <b>errors</b> and reduces the occurrences of singleton tags...|$|R
40|$|The {{research}} {{effort is}} a part of the Light Water Reactor Sustainability (LWRS) Program. LWRS is a research and development program sponsored by the Department of Energy, performed in close collaboration with industry to provide the technical foundations for licensing and managing the long-term, safe and economical operation of current nuclear power plants. The LWRS Program serves to help the US nuclear industry adopt new technologies and engineering solutions that facilitate the continued safe operation of the plants and extension of the current operating licenses. The Outage Control Center (OCC) Pilot Project was directed at carrying out the applied research for development and pilot of technology designed to enhance safe outage and maintenance operations, improve human performance and reliability, increase overall operational efficiency, and improve plant status control. Plant outage management is a high priority concern for the nuclear industry from cost and safety perspectives. Unfortunately, many of the underlying technologies supporting outage control are the same as those used in the 1980 ’s. They depend heavily upon large teams of staff, multiple work and coordination locations, and manual administrative actions that require large amounts of paper. Previous work in human reliability analysis suggests that many repetitive tasks, including paper work tasks, may have a failure rate of 1. 0 E- 3 or higher (Gertman, 1996). With between 10, 000 and 45, 000 subtasks being performed during an outage (Gomes, 1996), the opportunity for human error of some consequence is a realistic concern. Although a number of factors exist that can make these <b>errors</b> <b>recoverable,</b> reducing and effectively coordinating the sheer number of tasks to be performed, particularly those that are error prone, has the potential to enhance outage efficiency and safety. Additionally, outage management requires precise coordination of work groups that do not always share similar objectives. Outage managers are concerned with schedule and cost, union workers are concerned with performing work that is commensurate with their trade, and support functions (safety, quality assurance, and radiological controls, etc.) are concerned with performing the work within the plants controls and procedures. Approaches to outage management should be designed to increase the active participation of work groups and managers in making decisions that closed the gap between competing objectives and the potential for error and process inefficiency...|$|R

