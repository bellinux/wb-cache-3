206|0|Public
5000|$|... 1994 - J.P. Morgan <b>RiskMetrics</b> Group, <b>RiskMetrics</b> Technical Document, 1996.|$|E
50|$|In 1998, as client {{demand for}} the group's risk {{management}} expertise exceeded the firm's internal risk management resources, the Corporate Risk Management Department was spun off from J.P. Morgan as <b>RiskMetrics</b> Group with 23 founding employees. The <b>RiskMetrics</b> technical document was revised in 1996. In 2001, it was revised again in Return to <b>RiskMetrics.</b> In 2006, a new method for modeling risk factor returns was introduced (RM2006). On 25 January 2008, <b>RiskMetrics</b> Group listed on the New York Stock Exchange (NYSE: RISK). In June 2010, <b>RiskMetrics</b> was acquired by MSCI.|$|E
50|$|Mr. Levitt {{serves on}} the Board of Directors for <b>RiskMetrics</b> Group.|$|E
50|$|<b>RiskMetrics</b> {{describes}} {{three models}} for modeling {{the risk factors}} that define financial markets.|$|E
50|$|Proxy {{advisory}} firm <b>RiskMetrics</b> had also recommended that Charles River's shareholders {{vote against the}} proposed deal.|$|E
50|$|The <b>RiskMetrics</b> {{variance}} model (also {{known as}} exponential smoother) was first established in 1989, when Sir Dennis Weatherstone, {{the new chairman}} of J.P. Morgan, asked for a daily report measuring and explaining the risks of his firm. Nearly four years later in 1992, J.P. Morgan launched the <b>RiskMetrics</b> methodology to the marketplace, making the substantive research and analysis that satisfied Sir Dennis Weatherstone's request freely available to all market participants.|$|E
50|$|Since {{there are}} three risk {{measures}} covered by <b>RiskMetrics,</b> {{there are three}} incremental risk measures: Incremental VaR (IVaR), Incremental Expected Shortfall (IES), and Incremental Standard Deviation (ISD).|$|E
50|$|In 2007 and 2008 <b>RiskMetrics</b> Group ranked it {{the number}} one law firm for average {{investor}} recovery in securities class actions. G&E is also listed as one of America’s Leading Business Law Firms by Chambers and Partners.|$|E
50|$|MSCI {{formally}} acquired both <b>RiskMetrics</b> Group, Inc. and Measurisk in 2010, {{as well as}} Investment Property Databank in 2012. In 2013 MSCI acquired Investor Force from ICG Group (formerly Internet Capital Group) in 2013. In August 2014, MSCI acquired GMI Ratings.|$|E
50|$|In 2005, ESL was {{established}} in Israel by Daniel Liezrowice as a financial engineering software company. It represented companies such as <b>RiskMetrics</b> (now part of MSCI) and Numerical Algorithms Group. At the same time, ESL developed localized applications around off the shelf financial engineering software.|$|E
50|$|Associated Members include: Deutsche Bundesbank, Deutsche Bank AG, De Nederlandsche Bank, Ernst & Young, Deloitte, White & Case LLP, Covington & Burling LLP, Centre for European Policy Studies, European Association of Law and Economics, <b>RiskMetrics</b> Group, Joseph von Sonnenfels Center for the Study of Public Law and Economics, Bombay Stock Exchange Limited, {{and many}} other notable institutions.|$|E
50|$|In early 2000 Moody's {{acquired}} the Software Products Group of Crowe, Chizek & Co., then the eighth largest accounting and {{consulting firm in}} the U.S., which brought software used by banks to analyze the risk in taking on commercial loans. The same year, MRMS partnered with <b>RiskMetrics</b> to develop software that combined credit risk analysis with portfolio management.|$|E
50|$|Risk {{measurement}} VaR {{was developed}} for this purpose. Development was most extensive at J. P. Morgan, which published the methodology and gave free access to estimates of the necessary underlying parameters in 1994. This {{was the first time}} VaR had been exposed beyond a relatively small group of quants. Two years later, the methodology was spun off into an independent for-profit business now part of <b>RiskMetrics</b> Group (now part of MSCI).|$|E
5000|$|The {{first is}} very similar to the mean-covariance {{approach}} of Markowitz. Markowitz assumed that asset covariance matrix [...] can be observed. The covariance matrix can be used to compute portfolio variance. <b>RiskMetrics</b> assumes that the market is driven by risk factors with observable covariance. The risk factors are represented by time series of prices or levels of stocks, currencies, commodities, and interest rates. Instruments are evaluated from these risk factors via various pricing models. The portfolio itself is assumed to be some linear combination of these instruments.|$|E
5000|$|Abolish the {{practice}} of having a joint chief executive and chairman of board of directors. Install independent bosses to oversee boards instead. Former Walt Disney Co. chief financial officer and director Gary Wilson states he saw [...] "boards transformed overnight from supplicants to independents" [...] when the two roles were separated at companies {{where he was a}} director. As of 2010 only 21 percent of boards were chaired by bona fide independent directors (as opposed to the CEO, an ex-CEO or someone otherwise defined as a company [...] "insider") according to <b>RiskMetrics</b> Group.|$|E
50|$|Milberg LLP (formerly {{known as}} Milberg Weiss LLP and Milberg Weiss Bershad & Schulman LLP) is a US plaintiffs' law firm, {{established}} in 1965 and based in New York City. It has mounted many class action cases {{on behalf of}} investors, and has been recognized as among the leading firms in its field by the National Law Journal, <b>RiskMetrics</b> Group, and Law360. The firm {{and some of its}} partners were charged in 2006 with offering improper inducements to plaintiffs. The case against the firm itself was dismissed in 2008, but that same year four partners pleaded guilty to charges, and many others had already left the firm.|$|E
5000|$|Li {{was born}} as Li Xianglin {{and raised in}} a rural part of China during the 1960s; his family had been {{relocated}} during the Cultural Revolution to a rural village in southern China for [...] "re-education". Li was talented and with hard work he received a master's degree in economics from Nankai University, one of the country’s most prestigious universities. After leaving China in 1987 {{at the behest of}} the Chinese government to learn more about capitalism from the west, he earned an MBA from Laval University in Quebec, and an MMath in Actuarial Science and PhD in statistics from the University of Waterloo in Ontario in 1995 with thesis title An estimating function approach to credibility theory under the supervision of Harry H. Panjer. At this point he changed his name to David X. Li. His financial career began in 1997 at Canadian Imperial Bank of Commerce World Markets division. He eventually moved to New York City and in 2000, he was a partner in J.P. Morgan's <b>RiskMetrics</b> unit. By 2003 he was director and global head of credit derivatives research at Citigroup. In 2004 he moved to Barclays Capital and headed up the credit quantitative analytics team. In 2008 Li moved to Beijing where he works for China International Capital Corporation as head of the risk management department.|$|E
40|$|Academic {{research}} has highlighted the inherent flaws within the <b>RiskMetrics</b> model and demonstrated {{the superiority of}} the GARCH approach in-sample. However, these results do not necessarily extend to forecasting performance. This paper seeks {{answer to the question of}} whether <b>RiskMetrics</b> volatility forecasts are adequate in comparison to those obtained from GARCH models. To answer the question stock index data is taken from 31 international markets and subjected to two exercises, a straightforward volatility forecasting exercise and a Value-at-Risk exceptions forecasting competition. Our results provide some simple answers to the above question. When forecasting volatility of the G 7 stock markets the APARCH model, in particular, provides superior forecasts that are significantly different from the <b>RiskMetrics</b> models in over half the cases. This result also extends to the European markets with the APARCH model typically preferred. For the Asian markets the <b>RiskMetrics</b> model performs well, and is only significantly dominated by the GARCH models for one market, although there is evidence that the APARCH model provides a better forecast for the larger Asian markets. Regarding the Value-at-Risk exercise, when forecasting the 1 % VaR the <b>RiskMetrics</b> model does a poor job and is typically the worst performing model, again the APARCH model does well. However, forecasting the 5 % VaR then the <b>RiskMetrics</b> model does provide an adequate performance. In short, the <b>RiskMetrics</b> model only performs well in forecasting the volatility of small emerging markets and for broader VaR measures. <b>RiskMetrics</b> GARCH Volatility forecasting Value-at-Risk...|$|E
40|$|We {{analyze the}} {{performance}} of <b>RiskMetrics,</b> a widely used methodology for measuring market risk. Based on the assumption of normally distributed returns, the <b>RiskMetrics</b> model completely ignores the presence of fat tails in the distribution function, which is an important feature of financial data. Nevertheless, it was commonly found that <b>RiskMetrics</b> performs satisfactorily well, and therefore the technique has become widely used in the financial industry. We find, however, {{that the success of}} <b>RiskMetrics</b> is the artifact of the choice of the risk measure. First, the outstanding performance of volatility estimates is basically due to the choice of a very short (one-period ahead) forecasting horizon. Second, the satisfactory performance in obtaining Value-at-Risk by simply multiplying volatility with a constant factor is mainly due to the choice of the particular significance level. ...|$|E
40|$|Securities Exchange), Vincent Chang, {{as well as}} Chaoxkest Liu for {{all their}} support with the data {{processing}} and Steven Lin for his technical instruction {{in the application of}} <b>RiskMetrics.</b> We would like to thank SysJust Co., Ltd., which is the agent for <b>RiskMetrics</b> in Taiwan, for providing the <b>RiskMetrics</b> system and all securities firms that made available the compositions of their trading books. We are also grateful for the valuable comments from seminar participants: Jung Chu, Connie Lin, Stanley Kuo, Sam Wang and Da-Bai Sheng at the Taiwan Securities Exchange meetings. All remaining errors are our own responsibility...|$|E
40|$|<b>RiskMetrics</b> was {{unveiled}} by JP Morgan in October of 1994. <b>RiskMetrics</b> is JP Morgan's risk management {{product that is}} based on the bank's methodology for the management of financial risk. The methodology specifies an approach to quantifying market risk for the purpose of managing and controlling financial risk in trading, arbitrage, and investment activities. This paper describes the application of probability and statistics in <b>RiskMetrics</b> with the purpose of identifying problems for further research, attracting statisticians to this line of investigation, and establishing a framework for collaboration with financial economists and managers of financial risk. The discussion centers on the four applications of probability and statistics in <b>RiskMetrics</b> : 1) the statistical analysis of returns in the estimation of market risk, 2) the time series properties and statistical description of volatility, 3) the treatment of risk and optionality, and 4) a methodology for mapping financial instruments that relies on the CIR model for the term structure of interest rates. Included in the paper are suggestions for future research. Also included is a summary of the <b>RiskMetrics</b> product and a glossary of risk management terms. ...|$|E
40|$|AbstractFinancial {{markets are}} not perfect {{and the risk}} cannot be totally eliminated, that's why risk {{reduction}} {{became more and more}} important for the financial markets, since the 2008 financial crisis. The most commonly used tool for risk measure is Value at Risk, being considered a crucial milestone, because it shows the maximum loss in the value of a portfolio asset. The first comprehensive market risk management methodology was developed by JP Morgan in 1994, and was called <b>RiskMetrics,</b> which become extremely popular due to its easy implementation. This paper analyzes the capacity of <b>RiskMetrics</b> in forecasting the high volatility during the financial crisis for the financial Romanian market and {{to see if there is}} some differences regarding the value of decay factor estimated based on squared error loss, the <b>RiskMetrics</b> approach, and the values obtain from implementing the check error loss function in estimating the decay factors. We found that in the case of BET and BET-FI, the <b>RiskMetrics</b> estimations underestimate the decay factor, by attaching a lower weight to the most recent variance. Moreover, we proved that <b>RiskMetrics</b> model was good enough to forecast the volatility on Romanian financial market during the financial crisis period, only if we estimate the decay factor based on check error loss function...|$|E
40|$|This paper {{attempts}} {{to examine the}} impact of merger and acquisition on Value at Risk (VaR) of China Eastern Airline. The VaR is estimated for the whole sample and pre-merger periods by three methods: <b>RiskMetrics,</b> AR-GARCH and Generalized Extreme Value (GEV). The regression-based model reports the highest VaR followed by <b>RiskMetrics</b> and GEV. All models report a low VaR after the 11 June, 2009 merger, indicating a negative impact of merger and acquisition on VaR. ...|$|E
40|$|This article {{provides}} the in-sample estimation and evaluates the out of-sample conditional mean and volatility forecast {{performance of the}} conventional Generalized Autoregressive Conditional Heteroscedasticity (GARCH), Asymmetric Power Autoregressive Conditional Heteroscedasticity (APARCH) and the benchmark <b>RiskMetrics</b> model on the US real estate finance data for the pre-crisis and post-crisis periods in 2008. The empirical {{results show that the}} <b>RiskMetrics</b> model performed satisfactorily in the in-sample estimation but poorly in the out-of-sample forecast. For the post-crisis out-of-sample forecasts, all models naturally performed poorly in conditional mean and volatility forecast. ...|$|E
40|$|The VARLINEX (value at risk linear exponent) {{forecasting}} {{procedure is}} presented in this paper, which explicitly adjusts the forecasts when the loss functions of the forecaster are asymmetric. The theory of order statistics is applied to derive the VARLINEX forecasts and their corresponding confidence intervals, which are distribution-free. An empirical study based on our method is carried out for the S&P 500 returns and compared with the <b>RiskMetrics</b> (TM) and the EVT method. It is found that our method can perform very well in relation to EVT and always performs much better than <b>RiskMetrics</b> (TM) ...|$|E
30|$|The Investor Responsibility Research Center (IRRC; currently, <b>RiskMetrics)</b> reports that, as of 2006, {{large public}} US firms have 9.3 ATPs on average. The {{corresponding}} number in 1996 was 9.7.|$|E
40|$|Financial risk {{management}} {{has become increasingly}} important in the financial industry during the last decade, especially due to the financial disasters in the mid- 1990 s. Value-at-risk (VAR), developed by J. P. Morgan in the late 1980 s, {{has been the most}} widely accepted risk measurement methodology because it provides a single number summary of an institution’s portfolio risk. To promote VAR, Morgan provided <b>RiskMetrics,</b> a VARbased {{risk management}} service, for its clients (borrowers). Later, <b>RiskMetrics</b> was spun off as an independent company providing fee-based services. Building upon the theory of incomplete contracts and other related theories, we develop a game theoretic model to explain why adoption of the free <b>RiskMetrics</b> service stalled and how the change in ownership structure of the service led to wider adoption. In particular, we examine a situation where the borrowers have heterogeneous portfolio riskiness and this differential riskiness affects the value they can gain from the service and the impact of the service provider exploiting their private risk information. Our results suggest that the most important roadblock to borrowers ’ adoption of the free service might have been the potential damage from the service provider’s information exploitation. When the service was spun off, borrowers ’ concern was reduced due to the multi-party independent ownership structure, which led to wider adoption. The spin-off was Morgan’s strategic move to maximize long-term profits from <b>RiskMetrics...</b>|$|E
40|$|Value At Risk (VAR) {{has become}} one of the {{accepted}} norms in financial decision making- not just for risk managers and regulators, but for anyone concerned with the actual numbers produced: traders, fund managers, corporate treasurers, etc [...] Not only is capital set aside for regulatory compliance on the basis of these measures- such things as trading limits or capital allocation decisions may be set. So there has been great motivation to produce VAR measures for a variety of reasons, and this has prompted JP Morgan to spearhead developments in this field. In October 1994 the first version of <b>RiskMetrics</b> ™ was launched, much before any rival product. Since then there have been two significant updates, changing and expanding the methodology to cover all main foreign exchange, money, fixed income and equity markets, and some key commodities. The <b>RiskMetrics</b> ™ Technical Document was developed to further enhance understanding of ways in which product can be used, as well as explaining the statistical methodologies. Recently there has been another important initiative in the development of the product- a partnership with Reuters who have now taken over the production side of <b>RiskMetrics</b> ™, whilst JP Morgan NY remain responsible for the methodology. Reuters plan to offer <b>RiskMetrics</b> ™ volatilities and correlations (expanded to include more markets), and a‘menu ’ of other statisical volatilities and correlations (such as GARCH). Customers will also be able to access the underlying data: Reuters have already spent about £ 100 m on the construction of a huge relational data base, which they now plan to extend so that it includes the <b>RiskMetrics</b> ™ data with special search engines to extract prices, volatilities and correlations relevant to a specific portfolio...|$|E
40|$|Inline {{with the}} third pillar of the Basel accords, {{quantitative}} market risk measurements are investigate and evaluated comparing JP Morgan’s <b>RiskMetrics</b> and Bollerslev’s GARCH with the Peek over Threshold and Block Maxima approaches from the Extreme Value Theory framework. Value-at-Risk and Expected Shortfall (Conditional Value-at-Risk), with 95 % and 99 % confidence, is predicted for 25 years of the OMXS 30. The study finds Bollerslev’s suggested t distribution {{to be a more}} appropriate distributional assumption, but no evidence to prefer the GARCH to the <b>RiskMetrics.</b> The more demanding Extreme Value Theory procedures trail behind as they are found wasteful of data and more difficult to backtest and therefore evaluate. ...|$|E
40|$|Purpose – The {{purpose of}} this paper is to explore risk {{management}} models applied to electric power markets. Several Value-at-Risk (VaR) models are applied to day-ahead forward contract electric power price data to see which, if any, could be best used in practice. Design/methodology/approach – A time-varying parameter estimation procedure is used which gives all models the ability to track volatility clustering. Findings – The <b>RiskMetrics</b> model outperforms the GARCH model for 95 per cent VaR, whereas the GARCH model outperforms <b>RiskMetrics</b> for 99 per cent VaR. Both these models are better at handling volatility clustering than the Stable model. However, the Stable model was more accurate in detecting the numbers of daily returns beyond the VaR limits. The fact that the parsimonious <b>RiskMetrics</b> model performed well suggests that efforts to specify the model dynamics may be unnecessary in practice. Research limitations/implications – The present study provides a starting point for further research and suggests models that could be applied to electricity markets. Originality/value – Electricity markets are a challenge to risk modelers, as they typically exhibit non-Normal return distributions with time-varying volatility. Previous academic research in this area is rather scarce. Exchange rates, Japan, Risk management, Stock markets...|$|E
40|$|Many of the {{concepts}} in theoretical and empirical finance developed over the past decades – including the classical portfolio theory, the Black-Scholes-Merton option pricing model and the <b>RiskMetrics</b> variance-covariance approach to Value at Risk (VaR) – rest upon the assumption that asset returns follo...|$|E
40|$|Since the Basel II accord, {{forecasting}} Value-at-Risk {{become a}} daily task of {{banks and other}} Authorized Deposit-taking Institutions (ADIs). These forecasts are used to determine capital requirements and associated capital costs of ADIs. Methods based on Extreme Value Theory (EVT) showed better performance in terms of unconditional coverage and independence in many comparative studies. In this work we compare, in terms of daily capital requirements and violation penalties under the Basel II accord, {{the performance of a}} new model based on the EVT, with other models based on EVT, GARCH-type models and the <b>Riskmetrics</b> model. We emphasize that with the indexes under study and taking into account the Basel penalty zones, we achieve much better results with this new model than with the well known <b>Riskmetrics</b> model...|$|E
30|$|For the {{corporate}} governance data, we compiled the institutional ownership {{data from the}} Thomson Reuters Institutional (13 F) Holdings (formerly known as CDA/Spectrum database). All other corporate governance variables were first obtained from the <b>RiskMetrics</b> Directors database (formerly known as Investor Responsibility Research Center Takeover Defense database). However, this database does not contain all the governance variables in this study (e.g., data on committee meetings are not available). Also, <b>RiskMetrics</b> only covers S&P 1500 companies beginning in 1996. To complete our corporate governance dataset, we manually collected the remaining data by analysing the proxy statements included in the Direct Edger database. As Direct Edger only incorporates proxy statements commencing from 1992, this effectively limits our governance sample and the related statistical analyses to the period from 1992 to 1997.|$|E
40|$|Accurate {{prediction}} {{of the frequency}} of extreme events is of primary importance in many financial applications such as Value-at-Risk (VaR) analysis. We propose a semi-parametric method for VaR evaluation. The largest risks are modelled parametrically, while smaller risks are captured by the non- parametric empirical distribution function. The semi-parametric method is compared with historical simulation and the J. P. Morgan <b>RiskMetrics</b> technique on a portfolio of stock returns. For predictions of low probability worst outcomes, <b>RiskMetrics</b> analysis underpredicts the VaR while historical simulation overpredicts the VaR. However, the estimates obtained from applying the semi-parametric method are more accurate in the VaR prediction. In addition, an option {{is used in the}} portfolio to lower downside risk. Finally, it is argued that current regulatory environment provides incentives to use the lowest quality VaR method available...|$|E
40|$|This work {{describes}} {{applications of}} probability and statistics in <b>RiskMetrics</b> TM � J P Morgan�s methodology for quantifying market risk. The methodology im� plements an analytical approach to �nancial risk in trading � arbitrage � and in� vestment {{based on the}} statistics of market moves in equities � bonds � currencies and commodities. The public unveiling of <b>RiskMetrics</b> TMin October of 1994 at� tracted widespread interest among regulators � competing �nancial institutions� investment managers � and corporate treasurers � while the available technical doc� umentation o�ers us a unique opportunity for informed statistical research on the {{theory and practice of}} �nancial risk management. For the purpose of identify� ing problems for further research � this discussion focuses on �ve applications of statistics in <b>RiskMetrics</b> TM � which range from data analysis of daily returns and locally Gaussian processes to stochastic volatility models and It�o processes for the term structure of interest rates. The latter problems re�ect the author�s particular interest in stochastic inference for Markov processes and multivariate dependencies. Another important theme of this discussion � however � is devoted to attracting statisticians to the study of �nancial risk management and developing the foundations for collaborative work with �nancial economists and practicing risk managers. For this reason � this is also an expository document that touches several areas of active statistical research with applications to problems of risk management...|$|E
40|$|Accurate {{modelling}} of volatility {{is important}} {{as it relates}} to the forecasting of Value-at-Risk (VaR). The <b>RiskMetrics</b> model to forecast volatility is the benchmark in the financial sector. In an important regulatory innovation, the Basel Committee has proposed the use of an internal method for modelling VaR instead of the strict use of the benchmark model. The aim of this paper is to evaluate the performance of <b>RiskMetrics</b> in comparison to other models of volatility forecasting, such as some family classes of the Generalised Auto Regressive Conditional Heteroscedasticity models, in forecasting the VaR in emerging markets. This paper makes use of the stock market index portfolio, the All-Share Index, as a case study to evaluate the market risk in emerging markets. The paper underlines the importance of asymmetric behaviour for VaR forecasting in emerging markets’ economies...|$|E
