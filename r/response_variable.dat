3000|5088|Public
25|$|In case of {{a single}} regressor, fitted by least squares, R2 is {{the square of the}} Pearson {{product-moment}} correlation coefficient relating the regressor and the <b>response</b> <b>variable.</b> More generally, R2 is the square of the correlation between the constructed predictor and the <b>response</b> <b>variable.</b> With more than one regressor, the R2 can be referred to as the coefficient of multiple determination.|$|E
25|$|Poisson {{regression}} {{and negative}} binomial regression {{are useful for}} analyses where the dependent (<b>response)</b> <b>variable</b> is the count (0,1,2,…) {{of the number of}} events or occurrences in an interval.|$|E
25|$|The threshold, or climatological tipping point, is {{the value}} at which a very small {{increment}} for the control variable (like CO2) produces a large, possibly catastrophic, change in the <b>response</b> <b>variable</b> (global warming).|$|E
3000|$|... 3 Selection of <b>response</b> <b>variables</b> Selection of <b>response</b> <b>variables</b> {{should be}} done {{properly}} so that it provides useful information about the process. Typically in a well-testing problem, a number of points sampled on the pressure derivative plots or a useful transformation of them may be considered as the <b>response</b> <b>variables.</b>|$|R
30|$|For proper functionality, three dynamic <b>response</b> <b>variables</b> are introduced, which are, Approachres_t, MOIDres_t and RobotHoldres_t. The role {{of these}} dynamic <b>response</b> <b>variables</b> {{will be made}} more evident during the {{discussion}} of the services.|$|R
40|$|Abstract. We {{introduced}} a generalised Wishart process (GWP) for modelling input dependent covariance matrices Σ(x), allowing one to model input varying correlations and uncertainties between multiple <b>response</b> <b>variables.</b> The GWP can naturally scale {{to thousands of}} <b>response</b> <b>variables,</b> as opposed to competing multivariate volatility models which are typically intractable for greater than 5 <b>response</b> <b>variables.</b> The GWP can also naturally capture a rich class of covariance dynamics – periodicity, Brownian motion, smoothness, [...] . – through a covariance kernel. ...|$|R
25|$|Hierarchical {{linear models}} (or {{multilevel}} regression) organizes the data into {{a hierarchy of}} regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The <b>response</b> <b>variable</b> might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.|$|E
25|$|Ridge regression, {{and other}} forms of penalized {{estimation}} such as Lasso regression, deliberately introduce bias into the estimation of β {{in order to reduce the}} variability of the estimate. The resulting estimators generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the <b>response</b> <b>variable</b> y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.|$|E
25|$|Two {{hypothesis}} {{tests are}} particularly widely used. First, {{one wants to}} know if the estimated regression equation is any better than simply predicting that all values of the <b>response</b> <b>variable</b> equal its sample mean (if not, it is said to have no explanatory power). The null hypothesis of no explanatory value of the estimated regression is tested using an F-test. If the calculated F-value is found to be large enough to exceed its critical value for the pre-chosen level of significance, the null hypothesis is rejected and the alternative hypothesis, that the regression has explanatory power, is accepted. Otherwise, the null hypothesis of no explanatory power is accepted.|$|E
40|$|A {{detailed}} description is given {{of the quality}} control program used in the photographic laboratory of the NASA-ERTS Ground Data Handling System. The product <b>response</b> <b>variables</b> measured include tone reproduction, resolution, and low spatial frequency noise. In addition to product <b>response</b> <b>variables,</b> certain performance parameters of the laboratory printers and processors are frequently measured {{in order to produce}} consistent duplications of archival photography. A description is given of the operation and use of a densitometer/computer interface which is used to calculate three tone reproduction <b>response</b> <b>variables</b> - film speed, average gradient, and base plus fog density. This procedure eliminates the need for any hand plotting of D log E curves to manually determine <b>response</b> <b>variables...</b>|$|R
40|$|Description This package {{implements}} AO transformations for binary <b>response</b> <b>variables</b> in Generalized Linear Models (GLMs) and bounded <b>response</b> <b>variables</b> in Quantile Regression (QR) models. The Maximum Likelihood Estimate (MLE) of {{the transformation}} parameter is obtained using the profile log likelihood method Depends lqmm, quantre...|$|R
50|$|In a {{narrower}} sense, regression may refer {{specifically to the}} estimation of continuous <b>response</b> <b>variables,</b> {{as opposed to the}} discrete <b>response</b> <b>variables</b> used in classification. The case of a continuous output variable may be more specifically referred to as metric regression to distinguish it from related problems.|$|R
25|$|Second, {{for each}} {{explanatory}} variable of interest, {{one wants to}} know whether its estimated coefficient differs significantly from zero—that is, whether this particular explanatory variable in fact has explanatory power in predicting the <b>response</b> <b>variable.</b> Here the null hypothesis is that the true coefficient is zero. This hypothesis is tested by computing the coefficient's t-statistic, as {{the ratio of the}} coefficient estimate to its standard error. If the t-statistic is larger than a predetermined value, the null hypothesis is rejected and the variable is found to have explanatory power, with its coefficient significantly different from zero. Otherwise, the null hypothesis of a zero value of the true coefficient is accepted.|$|E
25|$|A triple-blind {{study is}} an {{extension}} of the double-blind design; the monitoring committee response variables is not told the identity of the groups. The committee is simply given data for groups A and B. A triple-blind study has the theoretical advantage of allowing the monitoring committee to evaluate the <b>response</b> <b>variable</b> results more objectively. This assumes that appraisal of efficacy and harm, as well as requests for special analyses, may be biased if group identity is known. However, in a trial where the monitoring committee has an ethical responsibility to ensure participant safety, such a design may be counterproductive since in this case monitoring is often guided by the constellation of trends and their directions. In addition, by the time many monitoring committees receive data, often any emergency situation has long passed.|$|E
2500|$|Consider fitting a {{line with}} one {{predictor}} variable. Define i as {{an index of}} each of the n distinct x values, j as an index of the <b>response</b> <b>variable</b> observations for a given x value, and n'i as the number of y values associated with the i th x value. [...] The value of each <b>response</b> <b>variable</b> observation can be represented by ...|$|E
50|$|In {{response}} surface methodology, {{the objective is}} to find the relationship between the input <b>variables</b> and the <b>response</b> <b>variables.</b> The process starts from trying to fit a linear regression model. If the P-value turns out to be low, then a higher degree polynomial regression, which is usually quadratic, will be implemented. The process of finding a good relationship between input and <b>response</b> <b>variables</b> will be done for each simulation test. In simulation optimization, {{response surface}} method can be used to find the best input variables that produce desired outcomes in terms of <b>response</b> <b>variables.</b>|$|R
30|$|Socially {{responsible}} consumption behavior {{had significant}} positive relations with all dependent variables as covariates. In addition, the results exhibited a significant covariate of apparel involvement in all consumer <b>response</b> <b>variables</b> excluding purchase intention. These results confirmed that consumer <b>response</b> <b>variables</b> are indeed sensitive to socially responsible consumption behavior and apparel product involvement.|$|R
40|$|AbstractIn this paper, we {{consider}} the consistency of Cp-type statistics for selecting variables in a normality-assumed linear regression with multiple responses when the dimension of the vector of the <b>response</b> <b>variables</b> may be large. We propose a new consistent Cp-type statistic for which consistency can be achieved whenever the dimension of the <b>response</b> <b>variables</b> vector is fixed or goes to infinity. A high probability of selecting the true subset of explanatory variables can be expected under a moderate sample size when the proposed Cp-type statistic is used to select variables, even {{when there is a}} high-dimensional <b>response</b> <b>variables</b> vector...|$|R
2500|$|Generalized {{linear models}} allow for an {{arbitrary}} link function g that relates {{the mean of}} the <b>response</b> <b>variable</b> to the predictors, i.e. E(y) = g(β′x). The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the [...] range of the linear predictor and the range of the <b>response</b> <b>variable.</b>|$|E
2500|$|... where [...] is the <b>response</b> <b>variable,</b> [...] is the {{explanatory}} variable, εi is a random error term, and [...] and [...] are parameters.|$|E
2500|$|Generalized {{linear models}} (GLMs) are a {{framework}} for modeling a <b>response</b> <b>variable</b> y that is bounded or discrete. [...] This is used, for example: ...|$|E
40|$|A {{sensor failure}} {{will lead to}} sensor {{measurement}} distortion, and may reduce {{the reliability of the}} whole structure analysis. This paper studies the method of monitoring information reconstruction based on the correlation degree. For the faulty sensor, the correlation degree of the normal response of this sensor and the measurements of the other sensors is calculated, which is also called the correlation degree of reconstructed <b>variables</b> and <b>response</b> <b>variables.</b> By comparing the correlation degrees, the <b>response</b> <b>variables,</b> which are needed to establish the correlation model, are determined. The correlation model between the reconstructed <b>variables</b> and the <b>response</b> <b>variables</b> is established by the partial least square method. The value of the correlation degrees between the reconstructed <b>variables</b> and the <b>response</b> <b>variables,</b> the amount of the monitoring data which is used to determine the coefficients of the correlation model, and the number of the <b>response</b> <b>variables</b> are used to discuss the influence factors of the reconstruction error. The stress measurements of structural health monitoring system of Shenzhen Bay Stadium is taken as an example, and the effectiveness of the method is verified and the practicability of the method is illustrated...|$|R
30|$|It {{is usually}} {{difficult}} to realize an optimal {{level of the}} input variables that can result in values close to the ideal or target values {{for all of the}} <b>response</b> <b>variables.</b> The main goal of multi-response optimization is, therefore, to find the settings of the input variables that achieve an optimal compromise in the <b>response</b> <b>variables.</b>|$|R
40|$|In much of {{epidemiological}} {{and clinical}} research, repeated observations of <b>response</b> <b>variables</b> {{and a set}} of covariates are taken from each individual over a certain time. Such researches are commonly referred to as repeated measures studies or longitudinal studies in which the primary objective is to describe the dependence of <b>response</b> <b>variables</b> on time treatment effect...|$|R
2500|$|R2 {{is often}} {{interpreted}} as the proportion of response variation [...] "explained" [...] by the regressors in the model. Thus, R2=1 indicates that the fitted model explains all variability in , while R2=0 indicates no 'linear' relationship (for straight line regression, {{this means that the}} straight line model is a constant line (slope=0, intercept=) between the <b>response</b> <b>variable</b> and regressors). An interior value such as R2=0.7 may be interpreted as follows: [...] "Seventy {{percent of the variance in}} the <b>response</b> <b>variable</b> can be explained by the explanatory variables. The remaining thirty percent can be attributed to unknown, lurking variables or inherent variability." ...|$|E
2500|$|The {{notion of}} a [...] "unique effect" [...] is {{appealing}} when studying a complex system where multiple interrelated components influence the <b>response</b> <b>variable.</b> In some cases, it can literally be interpreted as the causal effect of an intervention that {{is linked to the}} value of a predictor variable. However, {{it has been argued that}} in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the <b>response</b> <b>variable</b> when the predictors are correlated with each other and are not assigned following a study design. A commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.|$|E
2500|$|The very {{simplest}} case of {{a single}} scalar predictor variable x and a single scalar <b>response</b> <b>variable</b> y is known as simple linear regression. [...] The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression. [...] Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased {{in terms of the}} multiple regression model. [...] Note, however, that in these cases the <b>response</b> <b>variable</b> y is still a scalar. Another term multivariate linear regression refers to cases where y is a vector, i.e., the same as general linear regression.|$|E
40|$|Abstract: Multivariate estimators {{proposed}} by Ahmed, Hussin [1] for simultaneous estimation of multiple <b>response</b> <b>variables</b> (Yi) assumes that all <b>response</b> <b>variables</b> depend upon {{same set of}} auxiliary variables (Xi). This is not always feasible, in many situation different <b>response</b> <b>variables</b> may depend on different sets of predictors. In this paper new multivariate estimators for single, two and multiphase phase sampling have been proposed using the concept of Seemingly Unrelated Regression Estimation (SURE) and Mean square errors of the proposed estimators have been derived. Key words: Multivariate estimator • two phase sampling • multiple auxiliary variables • minimum variance • seemingly unrelated regression model • zelner model...|$|R
40|$|This {{paper is}} {{concerned}} with a multivariate growth curve model for observations obtained by simultaneously measuring m <b>response</b> <b>variables</b> at each of p time points, on samples from multiple groups. The objective {{is to develop a}} test for determining wheter the m_ 2 =m-m_ 1 <b>response</b> <b>variables</b> carry no additional information (are redundant) for a comparison between the groups, given the presence of the first m_ 1 <b>response</b> <b>variables.</b> We obtain some equivalent hypotheses for redundancy by extending the technique of Rao (1970). The likelihood ratio (LR) test is descussed. However, because its null distribution is complicated, we propose a more practical approximate test using a conditional LR criterion...|$|R
40|$|This paper {{describes}} {{advances in}} the algorithm development designed to solve a task of optimal polynomial model selection on multivariate data sets in presence of outliers in both explanatory and <b>response</b> <b>variables.</b> On one side novel algorithm, as its ancestor, is based on GMDH-type PNN, which gives him an universal model structure identification abilities thanks to the evolving adaptively synthesized bounded network. And {{on the other side}} the algorithm is enhanced with GM-estimator used for parameter search which allows him achieve robustness to outliers in both explanatory and <b>response</b> <b>variables.</b> Enhanced RPNN demonstrated robustness to outliers in both explanatory and <b>response</b> <b>variables</b> and good accuracy of the automatic structure syntheses...|$|R
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} <b>response</b> <b>variable</b> have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the <b>response</b> <b>variable</b> can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a <b>response</b> <b>variable</b> whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the <b>response</b> <b>variable</b> (e.g. fit the logarithm of the <b>response</b> <b>variable</b> using a linear regression model, which implies that the <b>response</b> <b>variable</b> has a log-normal distribution rather than a normal distribution).|$|E
2500|$|Suppose {{the data}} {{consists}} of n observations {y,x}. Each observation i includes a scalar response yi and a vector of values of p predictors (regressors) xij for j = 1, ..., p. In a linear regression model the <b>response</b> <b>variable</b> is a linear {{function of the}} regressors: ...|$|E
2500|$|In {{order for}} the lack-of-fit sum of squares to differ from the sum of squares of residuals, there must {{be more than one}} value of the <b>response</b> <b>variable</b> for {{at least one of the}} values of the set of {{predictor}} variables. [...] For example, consider fitting a line ...|$|E
40|$|We {{propose a}} semiparametric model for {{regression}} problems involving multiple <b>response</b> <b>variables.</b> The model {{makes use of}} a set of Gaussian processes that are linearly mixed to capture dependencies that may exist among the <b>response</b> <b>variables.</b> We propose an efficient approximate inference scheme for this semiparametric model whose complexity is linear in the number of training data points. We present experimental results in the domain of multi-join...|$|R
40|$|The {{following}} statistical {{study is}} about a series of quantitative <b>response</b> <b>variables</b> monitored three and six months after a breast operation. All patients that underwent a brest cancer operation from January 2003 untill June 2003 have been enrolled in the study. The number of patients observed is 38, but we have excluded from this study a patient for wich {{we do not have}} observed all investigated variables. The main issues involved here are: Descriptive analysis Temporal evolution of phenomena Effects of patient condition on the <b>response</b> <b>variables</b> In the following the three and six months variations of the <b>response</b> <b>variables</b> are labeled as (3) and (6) respectively, while the three to six month variation is marked as (6) (3). ...|$|R
5000|$|... the {{regressor}} matrix and [...] {{the vector}} of <b>response</b> <b>variables.</b> More details {{can be found}} e.g. here ...|$|R
