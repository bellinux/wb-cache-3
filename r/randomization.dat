10000|148|Public
5|$|Experimental research: The {{researcher}} isolates {{a single}} social process and reproduces it {{in a laboratory}} (for example, by creating a situation where unconscious sexist judgements are possible), seeking {{to determine whether or}} not certain social variables can cause, or depend upon, other variables (for instance, seeing if people's feelings about traditional gender roles can be manipulated by the activation of contrasting gender stereotypes). Participants are randomly assigned to different groups that either serve as controls—acting as reference points because they are tested with regard to the dependent variable, albeit without having been exposed to any independent variables of interest—or receive one or more treatments. <b>Randomization</b> allows the researcher to be sure that any resulting differences between groups are the result of the treatment.|$|E
5|$|With current treatments, {{patients}} with low and intermediate risk disease {{have an excellent}} prognosis with cure rates above 90% for low risk and 70–90% for intermediate risk. In contrast, therapy for high-risk neuroblastoma {{the past two decades}} resulted in cures only about 30% of the time. The addition of antibody therapy has raised survival rates for high-risk disease significantly. In March 2009 an early analysis of a Children's Oncology Group (COG) study with 226 high-risk patients showed that two years after stem cell transplant 66% of the group randomized to receive ch14.18 antibody with GM-CSF and IL-2 were alive and disease-free compared to only 46% in the group that did not receive the antibody. The <b>randomization</b> was stopped so all patients enrolling on the trial will receive the antibody therapy.|$|E
5|$|Another {{difficult}} area was feats. Although common feats worked well, with a random {{chance of success}} or failure, uncommon ones would appear to fail more often. To avoid this, <b>randomization</b> {{was replaced by a}} degree of difficulty in accomplishing the feat. Although pen-and-paper falling damage is random, the computer game bases damage on the distance of the fall. The team's biggest challenge was adapting disciplines. The pen-and-paper version may require a little blood that requires a long time to use, or have no blood cost and can be used at will; upgraded disciplines had additional requirements considered too confusing for a computer game. Troika attempted to equalize the disciplines, keeping the effect intact and normalizing the cost, so a first-level power requires one blood point, a second-level two points and so on. To balance the clans, the aristocratic Ventrue were only allowed to feed on noble blood, though this was changed to allow them to feed on lower-class humans, receiving less blood. During character creation, the game had an optional character biography with unique positive and negative characteristics (increasing one ability while limiting another). This was removed from the released game; Activision felt that there was insufficient test time, and removing it was a more stable option.|$|E
50|$|Transparent {{implementation}} of PaX address space layout <b>randomizations</b> and stack smashing protections using ELF shared objects as executables.|$|R
40|$|In {{the paper}} "Randomizations of Scattered Sentences", Keisler showed that if Martin's axiom for aleph one holds, then every {{scattered}} sentence has few separable <b>randomizations,</b> and {{asked whether the}} conclusion could be proved in ZFC alone. We show here {{that the answer is}} "yes". It follows that the absolute Vaught conjecture holds if and only if every L_ω_ 1 ω-sentence with few separable <b>randomizations</b> has countably many countable models. Comment: 9 page...|$|R
30|$|This is the 2 {{rounds of}} <b>randomizations</b> in RSC wrapped into one. Like an {{individual}} round, it also passed all NIST randomness tests (Andrew et al. 2010).|$|R
25|$|The EnKF version {{described}} here involves <b>randomization</b> of data. For filters without <b>randomization</b> of data, see.|$|E
25|$|However {{empirical}} evidence that adequate <b>randomization</b> changes outcomes relative to inadequate <b>randomization</b> {{has been difficult}} to detect.|$|E
25|$|Adaptive biased-coin <b>randomization</b> methods (of which urn <b>randomization</b> is {{the most}} widely known type): In these {{relatively}} uncommon methods, the probability of being assigned to a group decreases if the group is overrepresented and increases if the group is underrepresented. The methods {{are thought to be}} less affected by selection bias than permuted-block <b>randomization.</b>|$|E
40|$|This paper {{considers}} {{the conditions under}} which a monopolist might wish to randomize its pricing. When consumer demands depend on previous decisions by consumers, the magnitude of monopoly profits becomes effectively dependent on the welfare consequences of the monopoly's pricing policy. In these circumstances, differing attitudes towards price gambles between a firm and its customers can imply that randomized pricing is more profitable on average than the best deterministic pricing policy. Sufficient conditions for profitable <b>randomizations,</b> optimal <b>randomizations,</b> and incentive issues are discussed. Copyright 1994 by Blackwell Publishing Ltd. ...|$|R
40|$|We {{characterize}} two {{sorts of}} stochastic choice rules {{in which the}} agent makes current decisions using a forward-looking value function that takes future <b>randomizations</b> into account. Both sorts of rules generalize logistic choice, and are equivalent to it in static problems. The rules di↵er in how the agent views future choice sets and how he views his future <b>randomizations.</b> One rule {{is equivalent to the}} discounted logit used in applied work, and exhibits a “preference for flexibility;” the other is “error-averse” and penalizes the addition of undesirable choices to a menu...|$|R
40|$|This paper {{provides}} axiomatic {{characterizations of}} two sorts of recursive stochastic choice rules, where the agent makes his current decisions using a forward-looking value function {{that takes into}} account his future <b>randomizations.</b> Both of the choice rules we examine generalize logistic choice and are equivalent to it in static problems. The rules differ in how the agent views future choice sets and how he views his future <b>randomizations.</b> One rule is equivalent to the discounted logit used in applied work, and exhibits a “preference for flexibility;” the other is “error-averse” and penalizes the addition of undesirable choices to a menu...|$|R
25|$|As many {{electromagnetic}} attacks, especially SEMA attacks, rely on asymmetric implementations of cryptographic algorithms, {{an effective}} countermeasure {{is to ensure}} that a given operation performed at a given step of the algorithm gives no information on the value of that bit. <b>Randomization</b> of the order of bit encryption, process interrupts, and clock cycle <b>randomization,</b> are all effective ways to make attacks more difficult.|$|E
25|$|<b>Randomization</b> — When {{subjects}} {{are asked to}} choose {{from a variety of}} selections, there is an inherent bias to choose the first selection they are shown. If the order in which they are shown the selections is randomized each time, this bias will be averaged out. The <b>randomization</b> procedures used in the experiment have been criticized for not randomizing satisfactorily.|$|E
25|$|Response-adaptive <b>randomization,</b> {{also known}} as outcome-adaptive randomization: The {{probability}} of being assigned to a group increases if {{the responses of the}} prior patients in the group were favorable. Although arguments have been made that this approach is more ethical than other types of <b>randomization</b> when the probability that a treatment is effective or ineffective increases during the course of an RCT, ethicists have not yet studied the approach in detail.|$|E
40|$|Abstract- It {{follows from}} the Non-Reducibility of the Theorization Problem that an {{arbitrary}} proof cannot be valid on an absolute scale. Thus, in order for an arbitrary proof to be generative, it must be self-referential; but then, {{it must also be}} heuristic if not incomplete as a consequence. By relaxing the validity requirement, heuristic (i. e., relative) proof techniques are enabled. We show that heuristics are search <b>randomizations</b> in spacetime. It is shown how one can develop heuristics, which are <b>randomizations</b> of knowledge. Even more intriguing, it is shown that heuristic proof is to formal proof what fuzzy logic is to formal logic. Simply put, the paper argues for the need to relax the notion of formal proof if AI is to advance...|$|R
40|$|Figure 1 - The {{cumulative}} {{number of}} species (Sobs) for 450 samples collected at both habitat types: A undisturbed forest B secondary forest. Each curve represents the average of 1000 replicate <b>randomizations</b> of samples order. Richness estimator ICE (mean) and average number of singletons are shown with dashed lines...|$|R
5000|$|... 1991-1996: Phase III {{trial with}} two {{sequential}} <b>randomizations</b> for 379 high-risk NB patients {{was carried out}} by the Children's Cancer Group (CCG-3891) which demonstrated improved survival with myeloablative therapy (with total body irradiation) and 13-cis-retinoic acid (Accutane) with 50 patients {{in each of the four}} arms of the study., ...|$|R
25|$|Hyman {{criticized the}} {{ganzfeld}} papers for not describing optimal protocols, nor including the appropriate statistical analysis. He presented {{a factor analysis}} that he said demonstrated a link between success and three flaws, namely: flaws in <b>randomization</b> for choice of target; flaws in <b>randomization</b> in judging procedure; and insufficient documentation. Honorton asked a statistician, David Saunders, to look at Hyman's factor analysis and he concluded {{that the number of}} experiments was too small to complete a factor analysis.|$|E
25|$|Monte Carlo {{methods are}} also a {{compromise}} between approximate <b>randomization</b> and permutation tests. An approximate <b>randomization</b> test {{is based on a}} specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).|$|E
25|$|However, {{no single}} <b>randomization</b> {{procedure}} meets those goals in every circumstance, so researchers must select a procedure {{for a given}} study based on its advantages and disadvantages.|$|E
40|$|A {{bridge in}} a graph is an edge whose removal {{disconnects}} the graph {{and increases the}} number of connected components. We calculate the fraction of bridges {{in a wide range}} of real-world networks and their randomized counterparts. We find that real networks typically have more bridges than their completely randomized counterparts, but very similar fraction of bridges as their degree-preserving <b>randomizations.</b> We define a new edge centrality measure, called bridgeness, to quantify the importance of a bridge in damaging a network. We find that certain real networks have very large average and variance of bridgeness compared to their degree-preserving <b>randomizations</b> and other real networks. Finally, we offer an analytical framework to calculate the bridge fraction, the average and variance of bridgeness for uncorrelated random networks with arbitrary degree distributions. Comment: 18 pages, 10 figure...|$|R
40|$|This {{research}} note II {{introduces a}} way to understand a basic concept of the quantum enigma cipher. The conventional cipher is designed by a mathematical algorithm and its security is evaluated by {{the complexity of the}} algorithm in security analysis and ability of computers. This kind of cipher can be decrypted with probability one in principle by the Brute force attack in which an eavesdropper tries all the possible keys based on the correct ciphertext and some known plaintext. A cipher with quantum effects in physical layer may protect the system from the Brute force attack by means of the quantum no cloning theorem and <b>randomizations</b> based on quantum noise effect. The <b>randomizations</b> for the ciphertext which is the output from the mathematical encryption box is crucial to realize a quantum enigma cipher. Especially, by <b>randomizations,</b> it is necessary to make a substantial difference in accuracy of ciphertext in eavesdropper's observation and legitimate user's observation. The quantum illumination protocol {{can make a difference in}} error performance of the legitimate's receiver and the eavesdropper's receiver. This difference is due to differences in ability of the legitimate's receiver with entanglement and the eavesdropper's receiver without entanglement. It is shown in this note that the quantum illumination can be employed as an element of the most simple quantum enigma cipher. Comment: Submitted to Quantum ICT Research Institute Bulleti...|$|R
40|$|This paper {{considers}} a general optimal auction problem, with many goods {{and with a}} buyer’s utility that can depend non-linearly in his type. We point out that incentive compatibility constraints may be binding even if virtual utilities are strictly increasing in the buyer’s type. More importantly, optimal mechanisms may involve <b>randomizations</b> between different allocations. ...|$|R
25|$|Address space layout <b>randomization</b> (ASLR) is a {{computer}} security feature which involves arranging the positions of key data areas, usually including {{the base of the}} executable and position of libraries, heap, and stack, randomly in a process' address space.|$|E
25|$|The {{ganzfeld}} procedure {{has continued}} to be refined over the years. In its current incarnation, an automated computer system is used to select and display the targets ("digital autoganzfeld"). This overcomes many of the shortcomings of earlier experimental setups, such as <b>randomization</b> and experimenter blindness {{with respect to the}} targets.|$|E
25|$|All of the modules {{required}} by a program are sometimes statically linked and copied into the executable file. This process, {{and the resulting}} stand-alone file, {{is known as a}} static build of the program. A static build may not need any further relocation if virtual memory is used and no address space layout <b>randomization</b> is desired.|$|E
40|$|This paper studies revenue-maximizing {{allocation}} {{mechanisms for}} multiple goods where the buyerís utility can depend non-linearly in his type. We {{point out that}} despite strictly increasing virtual utilities, the allocation rule obtained via pointwise optimization may fail to be increasing and thus it may violate incentive compatibility. More importantly, the revenue maximizing allocation may involve <b>randomizations</b> between di§erent allocations. ...|$|R
40|$|We derive randomization-based {{models for}} {{experiments}} {{with a chain}} of <b>randomizations.</b> Estimation theory for these models leads to formulae for the estimators of treatment effects, their standard errors, and expected mean squares {{in the analysis of}} variance. We discuss the practicalities in fitting these models and outline the difficulties that can occur, many of which do not arise in two-tiered experiments. Publisher PDFPeer reviewe...|$|R
40|$|The {{problem of}} {{controlling}} a Markov chain on a countable state space with ergodic or ’long run average’ cost is {{studied in the}} presence of additional constraints, requiring finitely many (say, m) other ergodic costs to satisfy prescribed bounds. Under extremely general conditions, it is proved that an optimal stationary randomized strategy can be found that requires at most m <b>randomizations.</b> This generalizes a result of Ross...|$|R
25|$|SOD {{may reduce}} free radical damage to skin—for example, to reduce {{fibrosis}} following radiation for breast cancer. Studies {{of this kind}} must be regarded as tentative, however, as there were not adequate controls in the study including a lack of <b>randomization,</b> double-blinding, or placebo. Superoxide dismutase is known to reverse fibrosis, possibly through de-differentiation of myofibroblasts back to fibroblasts.|$|E
25|$|<b>Randomization</b> of {{the virtual}} memory {{addresses}} at which functions and variables can be found can make exploitation of a buffer overflow more difficult, but not impossible. It also forces the attacker to tailor the exploitation attempt to the individual system, which foils the attempts of internet worms. A similar but less effective method is to rebase processes and libraries in the virtual address space.|$|E
25|$|In 1986, Hyman and Honorton {{published}} A Joint Communiqué which {{agreed on}} the methodological problems and on ways to fix them. They suggested a computer-automated control, where <b>randomization</b> and the other methodological problems identified were eliminated. Hyman and Honorton agreed that replication of the studies was necessary before final conclusions could be drawn. They also agreed that more stringent standards were necessary for ganzfeld experiments, and they jointly specified what those standards should be.|$|E
5000|$|Miller-Rabin primality test: a {{probabilistic}} algorithm {{for testing}} whether a given number n is prime or composite. If n is composite, the test will detect n as composite WHP. There {{is a small}} chance that we are unlucky and the test will think that n is prime. But, the probability of error can be reduced indefinitely by running the test many times with different <b>randomizations.</b>|$|R
40|$|Shoval et al. (Reports, 1 June 2012, p. 1157) {{showed how}} {{configurations}} of phenotypes may identify tasks that trade off with each other, using <b>randomizations</b> assuming independence of data points. I {{argue that this}} assumption may not be correct for most and possibly all examples and led to pseudoreplication and inflated significance levels. Improved statistical testing is necessary to assess how the theory applies to empirical data. Peer Reviewe...|$|R
40|$|A {{class of}} multivariate {{distributions}} obtained by Gaussian <b>randomizations</b> of jumps of a Lévy process is studied. Specifically, exact convenient representations of type G distributions, given {{that they are}} of spherical type, are demonstrated. The methodology reveals new ways in extracting families of distributions that may help in understanding various applications that arise in finance. Applications from explicit distributions are also confirmed. Variance mixture of normal distributions Radial functions Lévy measures Series and integral representations...|$|R
