542|1632|Public
6000|$|Redfield's {{voice was}} cuttingly contemptuous {{as he said}} quite calmly: [...] "You're all kinds of asses, you sheepmen. You ought to pay the fee for your cattle with secret joy. So {{long as you can}} get your stock pastured (and in effect guarded) by the Government from June to November for twenty cents, or even fifty cents, per head you're in luck. Mrs. Wetherford is right: we've all been educated in a bad school. Uncle Sam has been too bloomin' lazy to keep any {{supervision}} over his public lands. He's permitted us grass pirates to fight and lynch and burn one another on the high range (to which neither of us had any right), holding back the <b>real</b> <b>user</b> of the land--the farmer. We've played the part of selfish and greedy gluttons so long that we fancy our privileges have turned into rights. Having grown rich on free range, you're now fighting the Forest Service because it is disposed to make you pay for what has been a gratuity. I'm a hog, Gregg, but I'm not a fool. I see the course of empire, and I'm getting into line." ...|$|E
5000|$|BeatBox Technologies (formerly named [...] "ClickCadence LLC") was a {{software}} vendor of <b>real</b> <b>user</b> behavior tracking products. Mercury Interactive acquired BeatBox in 2005 for approximately $14 million in cash, [...] "to extend the <b>real</b> <b>user</b> monitoring capabilities of its BTO software {{and to enhance}} its performance lifecycle offerings.". BeatBox was incorporated into Mercury's <b>Real</b> <b>User</b> Monitor (RUM) product, which {{is now part of}} HP Business Availability Center.|$|E
50|$|Primordial pseudonymous remailers once {{recorded}} {{enough information}} to trace {{the identity of the}} <b>real</b> <b>user,</b> making it possible for someone to obtain the identity of the <b>real</b> <b>user</b> through legal or illegal means. This form of pseudonymous remailer is no longer common.|$|E
5000|$|... 2007 | Elizabeth Brown. <b>Real</b> <b>Users,</b> <b>Real</b> Results: Examining the Limitations of Learning Styles within AEH ...|$|R
40|$|Do {{improvements}} in system performance demonstrated by batch evaluations conJbr the same benefit for <b>real</b> <b>users?</b> We carried out experiments designed {{to investigate this}} question. After identi~ing a weighting scheme that gave maximum improvement over the baseline in a non-interactive evaluation, we used it with <b>real</b> <b>users</b> searching on an instance recall task. Our results showed the weighting scheme giving beneficial results in batch studies {{did not do so}} with <b>real</b> <b>users.</b> Further analysis did identi~ other factors predictive of instance recall, including number of documents saved by the user, document recall, and number of documents seen by the user. 1...|$|R
30|$|Future work focuses {{mainly on}} {{performing}} evaluations involving <b>real</b> <b>users</b> {{and the use}} of parallel programming and heterogeneous computing for computing simultaneous queries in collaborative image retrieval scenarios. We also plan to test the use of our collaborating retrieval approach in real-world search scenarios involving the access of multiple <b>real</b> <b>users</b> to publicly available image collections.|$|R
50|$|Application service {{management}} extends {{the concepts of}} end-user experience management and <b>real</b> <b>user</b> monitoring in that measuring the experience of real users is a critical data point. However, ASM also requires the ability to quickly isolate {{the root cause of}} those slow-downs, thereby expanding the scope of <b>real</b> <b>user</b> monitoring/management.|$|E
5000|$|HP Software Business Availability Center / <b>Real</b> <b>User</b> Management (BAC/RUM) ...|$|E
5000|$|... {{sometimes}} also random <b>real</b> <b>user</b> {{pictures are}} captured (identity fraud) ...|$|E
50|$|By April 2008 {{there were}} more than 1 500 000 {{registered}} users and 780 000 <b>real</b> <b>users</b> of Tlen.pl.|$|R
5000|$|LiveQA Track - Goal: to {{generate}} answers to real questions originating from <b>real</b> <b>users</b> via a live question stream, in real time.|$|R
50|$|ShareTable was {{the first}} {{prototype}} which is deployed {{in the field and}} tested with <b>real</b> <b>users</b> created by Lana Yarosh et al. at 2009.|$|R
50|$|The SOASTA {{platform}} further enables digital {{businesses to}} gain continuous performance insight into <b>real</b> <b>user</b> experiences on mobile and web devices {{in real time}} and at scale, using <b>real</b> <b>user</b> monitoring technology coupled with its mPulse product. SOASTA serves mobile application testing needs through its product TouchTest, which provides functional mobile app testing automation for multi-touch, gesture-based mobile applications.|$|E
5000|$|HP <b>Real</b> <b>User</b> Monitor software: Software that {{provides}} real-time visibility into application performance and availability from the user perspective ...|$|E
50|$|Since version 1.8.1, inspectIT offers {{functionality}} for <b>real</b> <b>user</b> monitoring that records {{user interaction}} with, for example, a website or web-based application.|$|E
3000|$|... values, {{without the}} need to test a text entry method with <b>real</b> <b>users.</b> We could {{furthermore}} observe the effect of dwelling time on upper-bound text entry speed predictions. The value T [...]...|$|R
5000|$|The {{walkthrough}} {{does not}} test <b>real</b> <b>users</b> on the system. The walkthrough will often identify many {{more problems than}} you would find with a single, unique user in a single test session.|$|R
50|$|Laboratory {{experiments}} {{may work}} well for studying a specific aspect of user experience, but holistic user experience is optimally studied {{over a longer period}} of time with <b>real</b> <b>users</b> in a natural environment.|$|R
50|$|<b>Real</b> <b>user</b> {{monitoring}} {{measures the}} performance and availability experienced by actual users, diagnoses individual incidents, and tracks {{the impact of a}} change.|$|E
5000|$|Large tests cover {{three or}} more {{features}} and represent <b>real</b> <b>user</b> scenarios with <b>real</b> <b>user</b> data sources. There is some concern with overall integration of the features, but large tests {{tend to be more}} results-driven, checking that the software satisfies user needs. All three roles are involved in writing large tests and everything from automation to exploratory testing can be the vehicle to accomplish them. The question a large test attempts to answer is, “Does the product operate the way a user would expect and produce the desired results?” ...|$|E
50|$|End-user metrics are {{collected}} through web logs, synthetic monitoring, or <b>real</b> <b>user</b> monitoring. An example is ART (application response time) which provides {{end to end}} statistics that measure Quality of Experience.|$|E
5000|$|OpenSearch Track - Goal: {{to explore}} an {{evaluation}} paradigm for IR that involves <b>real</b> <b>users</b> of operational search engines. For this {{first year of}} the track the task will be ad hoc Academic Search.|$|R
40|$|Empirical spoken dialog {{research}} {{often involves}} {{the collection and}} analysis of a dialog corpus. However, it is not well understood whether and how a corpus of dialogs collected using recruited subjects differs from a corpus of dialogs obtained from <b>real</b> <b>users.</b> In this paper we use Let’s Go Lab, a platform for experimenting with a deployed spoken dialog bus information system, to address this question. Our first corpus is collected by recruiting subjects to call Let’s Go in a standard laboratory setting, while our second corpus consists of calls from <b>real</b> <b>users</b> calling Let’s Go during its operating hours. We quantitatively characterize the two collected corpora using previously proposed measures from the spoken dialog literature, then discuss the statistically significant {{similarities and differences between}} the two corpora with respect to these measures. For example, we find that recruited subjects talk more and speak faster, while <b>real</b> <b>users</b> ask for more help and more frequently interrupt the system. In contrast, we find no difference with respect to dialog structure. ...|$|R
50|$|Telex is a {{research}} anti-censorship system that would allow users to circumvent a censor without alerting the censor to the act of circumvention. It is not ready for <b>real</b> <b>users,</b> but a proof-of-concept mock system exists.|$|R
50|$|It can be {{reassembled}} {{according to}} an application's state machine into end-user activity (for example, into database queries, e-mail messages, and so on.) This kind of technology is common in <b>real</b> <b>user</b> monitoring.|$|E
50|$|Xamarin Test Cloud {{makes it}} {{possible}} to test mobile apps written in any language on real, non-jailbroken devices in the cloud. Xamarin Test Cloud uses object-based UI testing to simulate <b>real</b> <b>user</b> interactions.|$|E
5000|$|Here, {{access is}} {{intended}} to check whether the <b>real</b> <b>user</b> who executed the [...] program would normally be allowed to write the file (i.e., [...] checks the real userid rather than effective userid).|$|E
40|$|An {{unanswered}} {{question in}} information retrieval research is whether improvements in system performance demonstrated by batch evaluations confer the same benefit for <b>real</b> <b>users.</b> We used the TREC- 8 Interactive Track {{to investigate this}} question. After identifying a weighting scheme that gave maximum improvement over the baseline, we used it with <b>real</b> <b>users</b> searching on an instance recall task. Our results showed no improvement; although there was overall average improvement comparable to the batch results, it {{was not statistically significant}} and due to the effect of just one out of the six queries. Further analysis with more queries is necessary to resolve this question...|$|R
30|$|We {{presented}} {{all available}} experiment setting in “System implementation” and “System evaluation” sections. However, the visiting logs of <b>real</b> <b>users</b> cannot be shared since our data policy promised the data {{will only be}} used in this study.|$|R
5000|$|According to Sonico's CEO, Rodrigo Teijeiro, Sonico [...] "managed {{to offer}} a useful, safe and fun social network, with <b>real</b> <b>users</b> and a {{regional}} scope" [...] (Latin America). He assures that usability is the key of their success.|$|R
50|$|The {{combination}} of HP RUM (<b>Real</b> <b>User</b> Monitor) and UD (Universal Discovery) {{enables you to}} discover changes in your cloud environment at the moment it changes, resulting in real time updated views and CI's.|$|E
5000|$|Flow : An {{umbrella}} for user visible performance improvements {{driven by}} a team that works across Gecko components. Currently focused on <b>real</b> <b>user</b> performance improvements on major webapps, primarily the G Suite and Facebook.|$|E
50|$|Privilege {{separation}} is traditionally accomplished by distinguishing a <b>real</b> <b>user</b> ID/group ID from the effective user ID/group ID, using the setuid(2)/setgid(2) and related system calls, which were specified by POSIX. If these are incorrectly positioned, gaps can allow widespread network penetration.|$|E
40|$|There {{are several}} popular IR metrics {{based on an}} {{underlying}} user model. Most of them are parameterized. Usually parameters of these metrics are chosen {{on the basis of}} general considerations and not validated by experiments with <b>real</b> <b>users.</b> Particularly, the parameters of the Expected Reciprocal Rank measure are the normalized parameters of the DCG metric, and the latter are chosen in an ad-hoc manner. We suggest two approaches for adjusting parameters of the ERR model by analyzing <b>real</b> <b>users</b> behaviour: one based on a controlled experiment and another relying on search log analysis. We show that our approaches generate parameters that are largely different from the commonly used parameters of the ERR model...|$|R
40|$|The aim of {{the project}} is to design {{graphical}} user interface, which helps autistic people with time orientation. The devised user interface follows the rules of Structured education. The main components of the mobile application are mobile schedule and digital clock accompanied by a story. Final user interface was tested by <b>real</b> <b>users,</b> who used the experimental application. Developed experimental application {{has been used for}} testing by <b>real</b> <b>users.</b> The tests offer important result: under the supervision, the autistic users are able to utilize the application. The utilisation of clock accompanied by a story is partly verified so far. The important finding is, that after some training, the designed and developed application is exploitable for autistic people...|$|R
50|$|Researchers Meital Ben Sinai, Nimrod Partush, Shir Yadid and Eran Yahav from Israel Technion {{did some}} {{experiments}} in 2014 and wrote an article, “Exploiting Social Navigation”, to discuss about the results. According to the article, attackers can use plenty of machines to fake users’ behavior and fabricate information to mislead other <b>real</b> <b>users.</b> In this case, they attacked a real-time traffic software which {{allows users to}} report traffic news, and broadcasts these messages to others. These researchers used phony users to fabricate traffic information like obstruction or traffic jams and successfully let the system mislead <b>real</b> <b>users</b> with other itineraries. This can cause several problems, as the researchers mentioned. One problem is that <b>real</b> <b>users</b> would take more time and more money to go another longer way compared with the origin way which cost much less. What’s more, this attack may also lead people to some unsafe roads or even nonsexist ways, which causes security-related issues. To solve this shortage of social navigation, they encourage us to verify the users’ identification by checking real name with verification code, or checking users’ behavior with machine learning technologies.|$|R
