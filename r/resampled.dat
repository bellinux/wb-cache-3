1388|5978|Public
5|$|Because of the <b>resampled</b> {{convolution}} method that they describe for computing a numerical approximation of the curve-shortening flow, they call their method the <b>resampled</b> curvature scale space. They observe that this scale space is invariant under Euclidean transformations of the given shape, and assert that it uniquely determines {{the shape and}} is robust against small variations in the shape. They compare it experimentally against several related alternative definitions of a scale space for shapes, and find that the <b>resampled</b> curvature scale space is less computationally intensive, more robust against nonuniform noise, and less strongly influenced by small-scale shape differences.|$|E
25|$|City rendering: An ESRI shapefile {{containing}} a polygonal {{description of the}} building footprints was read in and then the polygons were <b>resampled</b> onto a rectilinear grid, which was extruded into the featured cityscape.|$|E
25|$|Quantum Monte Carlo, {{and more}} {{specifically}} Diffusion Monte Carlo methods can also {{be interpreted as a}} mean field particle Monte Carlo approximation of Feynman-Kac path integrals. The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, but the first heuristic-like and genetic type particle algorithm (a.k.a. <b>Resampled</b> or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984 In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) {{can be traced back to}} 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.|$|E
30|$|The {{previous}} section {{talked about the}} general AMSSDR. Its solution {{is made up of}} three main components: (1) <b>resampling</b> selector; (2) systematic important resampling; and (3) rounding copy <b>resampling.</b> This current section talks about the AMSSDR selector that was taken note of in the {{previous section}}. The <b>resampling</b> selector’s main purpose is to alter the <b>resampling</b> operation between traditional variation <b>resampling</b> and traditional <b>resampling,</b> based on memory adaptation. This allows the operation of <b>resampling</b> in an optimum manner, based on the computing devices’ physical memory requirements. Code 1 illustrates the pseudocode that was used for the <b>resampling</b> selector. Primarily, the pseudocode determines the total quantity of physical memory that is presently used by a computing device. If it is determined to be beyond 1536  MB, the <b>resampling</b> selector will select the <b>resampling</b> rounding. On the other hand, the <b>resampling</b> selector will select the systematic <b>resampling</b> algorithm to serve as its <b>resampling</b> operation. The next section talks about the operation systematic <b>resampling.</b>|$|R
30|$|<b>Resample</b> {{using one}} of the {{standard}} <b>resampling</b> techniques such as residual <b>resampling.</b>|$|R
40|$|One {{possible}} {{approach to}} tackle class imbalance in classification tasks is to <b>resample</b> training dataset, i. e., to drop {{some of its}} elements or to synthesize new ones. There exist several widely-used <b>resampling</b> methods. Recent research showed that selection of <b>resampling</b> method essentially affects quality of classification, which raises <b>resampling</b> selection problem. Exhaustive search for optimal <b>resampling</b> is time-consuming and hence it is of limited use. In this paper, we describe an alternative approach to <b>resampling</b> selection. We follow meta-learning concept to build <b>resampling</b> recommendation systems, i. e., algorithms recommending <b>resampling</b> for datasets {{on the basis of}} their properties...|$|R
25|$|The {{drawbacks}} of {{this technology}} are found in a strong fall-off of the SNR, which {{is proportional to the}} distance from the zero delay and a sinc-type reduction of the depth dependent sensitivity because of limited detection linewidth. (One pixel detects a quasi-rectangular portion of an optical frequency range instead of a single frequency, the Fourier-transform leads to the sinc(z) behavior). Additionally the dispersive elements in the spectroscopic detector usually do not distribute the light equally spaced in frequency on the detector, but mostly have an inverse dependence. Therefore, the signal has to be <b>resampled</b> before processing, which can not take care of the difference in local (pixelwise) bandwidth, which results in further reduction of the signal quality. However, the fall-off is not a serious problem with the development of new generation CCD or photodiode array with a larger number of pixels.|$|E
2500|$|Quantum Monte Carlo, {{and more}} {{specifically}} Diffusion Monte Carlo methods can also {{be interpreted as a}} mean field particle approximation of Feynman-Kac path integrals. The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, but the first heuristic-like and genetic type particle algorithm (a.k.a. <b>Resampled</b> or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984 ...|$|E
2500|$|... "Clock Catcher" [...] has fast, [...] "fierce, [...] " [...] hi-hats. [...] "Pickled!" [...] was {{described}} as a [...] "basstronica odyssey". [...] "Nose Art" [...] combines [...] "raygun squiggles" [...] and [...] "grinding mechanical noises," [...] with [...] "woozy", [...] "symphonic" [...] synths. [...] "Intro//A Cosmic Drama" [...] has [...] "cinematic depth". [...] "Zodiac Shit" [...] has a [...] "heavy, loping" [...] bass thump, vintage synths, syncopated drum solos, and [...] "spooky" [...] strings. [...] "Computer Face//Pure Being" [...] is a [...] "track of almost heavy metal proportions". [...] "...And the World Laughs with You" [...] is a glitchy song, with Yorke's vocals chopped and <b>resampled.</b> [...] "Arkestry" [...] is a jazz composition, which was compared to Sun Ra.|$|E
3000|$|There {{are several}} kinds of <b>resampling</b> methods, and the basic <b>resampling</b> method is called the single {{distribution}} <b>resampling.</b> Furthermore, single distribution <b>resampling</b> is subdivided into two categories called: traditional variation <b>resampling</b> and traditional <b>resampling.</b> Using traditional <b>resampling</b> benefits computing devices that need a single sampling process for every [...] j [...] cycle (for instance, computing devices that only have low memory requirements). On the other hand, the utilisation of traditional variation <b>resampling</b> is beneficial for computing devices that require {{more than a single}} sampling process for every j cycle (for instance, computing devices that have a high memory requirement). Although computing devices that have high memory requirement do not normally have problems with memory consumption, traditional variation <b>resampling</b> is often thought to be more appropriate for usage in this environment because it is faster than traditional <b>resampling</b> [58].|$|R
40|$|We {{describe}} an algorithm for perfect weighted-random <b>resampling</b> {{of a population}} with time complexity O(m + n) for <b>resampling</b> m inputs to produce n outputs. This algorithm is an incremental improvement over standard <b>resampling</b> algorithms. Our <b>resampling</b> algorithm is parallelizable, with linear speedup. Linear-time <b>resampling</b> yields notable performance improvements in our motivating example of Sequentia...|$|R
30|$|<b>Resampling.</b> <b>Resample</b> N {{particles}} on {{the basis}} of the weights obtained in step (b) using a residual <b>resampling</b> algorithm [28].|$|R
2500|$|In {{computational}} physics {{and more specifically}} in quantum mechanics, the ground state energies of quantum systems {{is associated with the}} top of the spectrum of Schrödinger's operators. The Schrödinger equation is the quantum mechanics version of the Newton's second law of motion of classical mechanics (the mass times the acceleration is the sum of the forces). This equation represents the wave function (a.k.a. the quantum state) evolution of some physical system, including molecular, atomic of subatomic systems, as well as macroscopic systems like the universe. The solution of the imaginary time Schrödinger equation (a.k.a. the heat equation) is given by a Feynman-Kac distribution associated with a free evolution [...] Markov process (often represented by Brownian motions) in the set of electronic or macromolecular configurations and some potential energy function. The long time behavior of these nonlinear semigroups is related to top eigenvalues and ground state energies of [...] Schrödinger's operators. [...] The genetic type mean field interpretation of these Feynman-Kac models are termed Resample Monte Carlo, or Diffusion Monte Carlo methods. These branching type evolutionary algorithms are based on mutation and selection transitions. During the mutation transition, the walkers evolve randomly and independently in a potential energy landscape on particle configurations. [...] The mean field selection process (a.k.a. quantum teleportation, population reconfiguration, <b>resampled</b> transition) is associated with a fitness function that [...] reflects the particle absorption in an energy well. Configurations with low relative energy are more likely to duplicate. In molecular chemistry, and statistical physics Mean field particle methods are also used to sample Boltzmann-Gibbs measures associated with some cooling schedule, and to compute their normalizing constants (a.k.a. free energies, or partition functions).|$|E
50|$|The {{basic idea}} of {{bootstrapping}} is that inference about a population from sample data, (sample → population), can be modelled by resampling the sample data and performing inference about a sample from <b>resampled</b> data, (<b>resampled</b> → sample). As {{the population is}} unknown, the true error in a sample statistic against its population value is unknown. In bootstrap-resamples, the 'population' {{is in fact the}} sample, and this is known; hence the quality of inference of the 'true' sample from <b>resampled</b> data, (<b>resampled</b> → sample), is measurable.|$|E
5000|$|The {{bootstrap}} {{can be used}} {{to construct}} confidence intervals for Pearson's correlation coefficient. In the [...] "non-parametric" [...] bootstrap, n pairs (xi, yi) are <b>resampled</b> [...] "with replacement" [...] from the observed set of n pairs, and the correlation coefficient r is calculated based on the <b>resampled</b> data. This process is repeated a large number of times, and the empirical distribution of the <b>resampled</b> r values are used to approximate the sampling distribution of the statistic. A 95% confidence interval for ρ can be defined as the interval spanning from the 2.5th to the 97.5th percentile of the <b>resampled</b> r values.|$|E
3000|$|... {{specific}} computing devices’ memory gives developers several difficulties as {{a result}} of the increased effort and time needed for the development of a particle filter. Thus, one needs a new sequential <b>resampling</b> algorithm that is flexible enough to allow it to be used with various computing devices. Therefore, this paper formulated a new single distribution <b>resampling</b> called the adaptive memory size-based single distribution <b>resampling</b> (AMSSDR). This <b>resampling</b> method integrates traditional variation <b>resampling</b> and traditional <b>resampling</b> in one architecture. The algorithm changes the <b>resampling</b> algorithm using the memory in a computing device. This helps the developer formulate a particle filter without over considering the computing devices’ memory utilisation during the development of different particle filters. At the start of the operational process, it uses the AMSSDR selector to choose an appropriate <b>resampling</b> algorithm (for example, rounding copy <b>resampling</b> or systematic <b>resampling),</b> based on the current computing devices’ physical memory. If one chooses systematic <b>resampling,</b> the <b>resampling</b> will sample every particle for every cycle. On the other hand, if it chooses the rounding copy <b>resampling,</b> the <b>resampling</b> will sample more than one of each cycle’s particle. This illustrates that the method (AMSSDR) being proposed is capable of switching <b>resampling</b> algorithms based on various physical memory requirements. The aim of the authors is to extend this research in the future by applying their proposed method in various emerging applications such as real-time locator systems or medical applications.|$|R
3000|$|The {{preceding}} section {{talked about}} the AMSSDR selector’s operation, which was utilised in the switching of the <b>resampling</b> operation between traditional variation <b>resampling</b> and traditional <b>resampling,</b> the basis of which is memory adaptation. Systematic <b>resampling</b> is the traditional <b>resampling</b> algorithm used, while rounding copy <b>resampling</b> is the traditional variation <b>resampling</b> algorithm implemented. This section {{will talk about the}} operation of systematic <b>resampling</b> (Code 2 can be used to refer to the pseudocode), which may be utilised the physical memory falls below 1.5  GB (1536  MB). First, it will make a sample [...] u_ 1 ∼ U([...] 0, 1 /N) and give a definition to [...] u_i = u_ 1 + ([...] i - 1)/N for i =  2,…N. Lastly, it utilises u [...]...|$|R
40|$|The <b>resampling</b> of discrete-time signals {{where the}} {{underlying}} analog signal is non-bandlimited is considered in this paper. We extend the generalized sampling theory developed {{based on the}} principle of consistency to <b>resampling.</b> Realizing the <b>resampling</b> system has both discrete input and output, the performance of the <b>resampling</b> filter is considered in l 2 instead of the traditionally used L 2. We show that the performance of the <b>resampling</b> system depends on the <b>resampling</b> rate instead of the actual interpolating kernels. The theory can be applied to image processing applications like zooming to provide better response to high frequency components. Since the <b>resampling</b> process is discrete in nature, our filter designed to optimize <b>resampling</b> in l 2 is shown to outperform other techniques designed in L 2. 1...|$|R
5000|$|... #Subtitle level 3: <b>Resampled</b> {{layers of}} sounds {{generated}} by a music workstation ...|$|E
50|$|Cards with ES1370 run {{natively}} at 44 kHz sampling frequency, {{meaning that}} 12, 24, 32 and 48 kHz become <b>resampled.</b> Resampling means lower sound quality, worse synchronization and possibly higher CPU utilization. Cards with ES1371 run at 48 kHz conforming to AC97, so 11, 22 and 44 kHz become <b>resampled.</b> For few soundcards feature multiple quartzes or a PLL, resampling {{is often used}} with all its potential problems.|$|E
50|$|More formally, the {{bootstrap}} {{works by}} treating inference {{of the true}} probability distribution J, given the original data, as being analogous to inference of the empirical distribution of Ĵ, given the <b>resampled</b> data. The accuracy of inferences regarding Ĵ using the <b>resampled</b> data can be assessed because we know Ĵ. If Ĵ is a reasonable approximation to J, then the quality of inference on J can in turn be inferred.|$|E
40|$|The {{asymptotic}} {{distribution of the}} bootstrap sample mean depends on the <b>resampling</b> intensity. This paper explores the sensitivity of that distribution against different <b>resampling</b> intensities. It is generally assumed that small <b>resampling</b> sizes make the bootstrap work. However, we will show that the bootstrap mean can only be highly unstable for small <b>resampling</b> intensities. Our setup considers <b>resampling</b> from a triangular array of row-wise independent and identically distributed random variables satisfying the Central Limit Theorem. Bootstrap sample mean Asymptotic stability Asymptotic distribution Triangular arrays <b>Resampling</b> intensity...|$|R
40|$|AbstractUniform <b>resampling</b> is {{the easiest}} to apply and is a general recipe for all problems, but it may require a large {{replication}} size B. To save computational effort in uniform <b>resampling,</b> balanced bootstrap <b>resampling</b> is proposed to change the bootstrap <b>resampling</b> plan. This <b>resampling</b> plan is effective for approximating {{the center of the}} bootstrap distribution. Therefore, this paper applies it to neural model selection. Numerical experiments indicate {{that it is possible to}} considerably reduce the replication size B. Moreover, the efficiency of balanced bootstrap <b>resampling</b> is also discussed in this paper...|$|R
3000|$|..., {{indicates}} that estimates {{are likely to}} be inaccurate. The key to addressing this is to introduce <b>resampling.</b> The basic idea of <b>resampling</b> is to eliminate samples with low importance weights and replicate samples with larger weights 12. While {{there are a number of}} variants of the <b>resampling</b> algorithm, they all consist of two core stages: calculating how many copies of each sample to generate and generating that number of copies of each sample. The different <b>resampling</b> variants differ in terms of how they calculate the number of copies to generate. We focus here on minimum variance <b>resampling</b> (also known as systematic <b>resampling)</b> which minimises the errors inevitably introduced by the <b>resampling</b> process (and is discussed in more detail in Section 4.6). The use of <b>resampling</b> with SIS is often known as the sampling importance <b>resampling</b> (SIR) filter and has been at the heart of particle filters since their invention [1, 24, 25].|$|R
5000|$|A second {{interpolation}} step is then {{applied to}} obtain the angular <b>resampled</b> signal [...] from the original time domain signal : ...|$|E
5000|$|The Message was <b>resampled</b> for Masta Ace's [...] "Me & The Biz," [...] notably {{featured}} in the video game Grand Theft Auto: San Andreas.|$|E
50|$|In {{investment}} portfolio construction, an investor or analyst {{is faced with}} determining which asset classes, such as domestic fixed income, domestic equity, foreign fixed income, and foreign equity, to invest in and what proportion of the total portfolio should be of each asset class. Harry Markowitz (1959) first described a method for constructing a portfolio with optimal risk/return characteristics. His portfolio optimization method finds the minimum risk portfolio with a given expected return. Because the Markowitz or Mean-Variance Efficient Portfolio is calculated from the sample mean and covariance, which are likely different from the population mean and covariance, the resulting {{investment portfolio}} may allocate too much weight to assets with better estimated than true risk/return characteristics. To account for {{the uncertainty of the}} sample estimates, a financial analyst can create many alternative efficient frontiers based on <b>resampled</b> versions of the data. Each <b>resampled</b> dataset will result in a different set of Markowitz efficient portfolios. These efficient frontiers of portfolios can then averaged to create a <b>resampled</b> efficient frontier. The appropriate compromise between the investor's Risk aversion and desired return will then guide the financial analyst to choose a portfolio from the set of <b>resampled</b> efficient frontier portfolios. Since such a portfolio is different from the Markowitz efficient portfolio it will have suboptimal risk/return characteristics with respect to the sample mean and covariance, but optimal characteristics when averaged over the many possible values of the unknown true mean and covariance. (Michaud, 1998) <b>Resampled</b> Efficiency is covered by U. S. patent #6,003,018, patent pending worldwide. New Frontier Advisors, LLC, has exclusive worldwide licensing rights.|$|E
40|$|Part 1 : Research PapersInternational audienceTo create convincing forged images, {{manipulated}} images {{or parts}} of them are usually exposed to some geometric operations which require a <b>resampling</b> step. Therefore, detecting traces of <b>resampling</b> became an important approach {{in the field of}} image forensics. In this paper, we revisit existing techniques for <b>resampling</b> detection and design some targeted attacks in order to assess their reliability. We show that the combination of multiple <b>resampling</b> and hybrid median filtering works well for hiding traces of <b>resampling.</b> Moreover, we propose an improved technique for detecting <b>resampling</b> using image forensic tools. Experimental evaluations show that the proposed technique is good for <b>resampling</b> detection and more robust against some targeted attacks...|$|R
40|$|This paper {{considers}} {{the effect of}} the <b>Resampling</b> schemes in the behavior of Particle Filter (PF) based robot localizer. The investigated schemes are Multinomial <b>Resampling,</b> Residual <b>Resampling,</b> Residual Systematic <b>Resampling,</b> Stratified <b>Resampling</b> and Systematic <b>Resampling.</b> An algorithm is built in Matlab environment to host these schemes. The performances are evaluated in terms of computational complexity and error from ground truth and the results are reported. The results showed that the localization plan which adopts the Systematic or Stratified <b>Resampling</b> scheme achieves higher accuracy localization while decreasing consumed computational time. However, the difference is not significant. Moreover, a particle excitation strategy is proposed. This strategy achieved significant improvement in the behavior of PF based robot localization...|$|R
40|$|AbstractThe {{asymptotic}} {{distribution of the}} bootstrap sample mean depends on the <b>resampling</b> intensity. This paper explores the sensitivity of that distribution against different <b>resampling</b> intensities. It is generally assumed that small <b>resampling</b> sizes make the bootstrap work. However, we will show that the bootstrap mean can only be highly unstable for small <b>resampling</b> intensities. Our setup considers <b>resampling</b> from a triangular array of row-wise independent and identically distributed random variables satisfying the Central Limit Theorem...|$|R
5000|$|... #Caption: The same image <b>resampled</b> to {{five times}} as many samples in each direction, using Lanczos resampling. Pixelation {{artifacts}} were removed changing the image's transfer function.|$|E
50|$|Under this scheme, a {{small amount}} of (usually {{normally}} distributed) zero-centered random noise is added onto each <b>resampled</b> observation. This is equivalent to sampling from a kernel density estimate of the data.|$|E
50|$|City rendering: An ESRI shapefile {{containing}} a polygonal {{description of the}} building footprints was read in and then the polygons were <b>resampled</b> onto a rectilinear grid, which was extruded into the featured cityscape.|$|E
40|$|In {{this report}} a {{comparison}} is made between four frequently encountered <b>resampling</b> algorithms for particle filters. A theoretical framework is introduced {{to be able}} to understand and explain the differences between the <b>resampling</b> algorithms. This facilitates a comparison of the algorithms based on <b>resampling</b> quality and on computational complexity. Using extensive Monte Carlo simulations the theoretical results are verified. It is found that systematic <b>resampling</b> is favourable, both in <b>resampling</b> quality and computational complexity...|$|R
30|$|The {{preceding}} section {{considered the}} used {{memory of the}} AMSSDR technique. This present section reviews {{the conclusion of the}} suggested method (AMSSDR). Restricting the use of single distribution <b>resampling</b> in case of the memory of specific computing devices causes difficulties for the developer, because of the additional time and effort needed to develop a particle filter. Hence, a new sequential <b>resampling</b> technique is needed, one with an amount which requires memory size depending on the memory requirement of computing devices. In this research, a new single distribution <b>resampling</b> technique has been created, known as AMSSDR, which is based on the combination of traditional <b>resampling</b> technique and traditional variation <b>resampling</b> technique in a <b>resampling</b> architecture. The technique will switch the <b>resampling</b> algorithm which is based on memory in a computer. The execution of this algorithm will facilitate developers to more effortlessly develop a particle filter, with no need to give a big degree of consideration to memory usage in a computing device when including different particle filter development. Initially, in the operational process, the AMSSDR selector will be utilised to select an appropriate <b>resampling</b> algorithm (for instance, systematic <b>resampling</b> technique or rounding copy <b>resampling</b> technique), as per the physical memory available in present computing devices. Following that, the outcomes show that, if systematic <b>resampling</b> is chosen, the <b>resampling</b> will take sample for each particle for each j cycle, while if the rounding copy <b>resampling</b> is selected, the <b>resampling</b> will take samples for more than one unit of each j cycle. Hence, this demonstrates that the suggested method (AMSSDR) is capable of switching and <b>resampling</b> algorithms in various physical memory requirements. The authors of this paper wish to extend this work gradually by implementing their suggested method {{in a number of different}} promising applications (for instance, in medical software [66] or real-time locator systems [67].|$|R
50|$|Sinc <b>resampling</b> {{in theory}} {{provides}} the best possible reconstruction for a perfectly bandlimited signal. In practice, the assumptions behind sinc <b>resampling</b> are not completely met by real-world digital images. Lanczos <b>resampling,</b> an approximation to the sinc method, yields better results. Bicubic interpolation {{can be regarded as}} a computationally efficient approximation to Lanczos <b>resampling.</b>|$|R
