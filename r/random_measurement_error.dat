99|10000|Public
5000|$|Homogeneity {{tests for}} CRMs follow planned {{experimental}} designs. Because the experiment {{is intended to}} test for (or estimate the size of) variation in value between different CRM units, the designs are chosen to allow separation of variation in results due to <b>random</b> <b>measurement</b> <b>error</b> and variation due to differences between units of the CRM. Among the simplest designs recommended for this purpose is a simple balanced nested design (see schematic). [...] Typically 10-30 CRM units are taken from the batch at random; stratified random sampling is recommended so that the selected units are spread across the batch. An equal number of subsamples (usually two or three) is then taken from each CRM unit and measured. Subsamples are measured in random order. Other designs, such as randomized block designs, have also been used for CRM certification.|$|E
40|$|Summary points: - The bias {{introduced}} by <b>random</b> <b>measurement</b> <b>error</b> {{will be different}} {{depending on whether the}} error is in an exposure variable (risk factor) or outcome variable (disease) - <b>Random</b> <b>measurement</b> <b>error</b> in an exposure variable will bias the estimates of regression slope coefficients towards the null - <b>Random</b> <b>measurement</b> <b>error</b> in an outcome variable will instead increase the standard error of the estimates and widen the corresponding confidence intervals, making results less likely to be statistically significant - Increasing sample size will help minimise the impact of measurement error in an outcome variable but will only make estimates more precisely wrong when the error is in an exposure variabl...|$|E
40|$|Estimates of the <b>random</b> <b>measurement</b> <b>error</b> {{contained}} in surface meteorological observations from Voluntary Observing Ships (VOS) {{have been made}} on a 30 ° area grid each month for the period 1970 to 2002. Random measurement errors are calculated for all the basic meteorological variables: surface pressure, wind speed, air temperature, humidity and sea-surface temperature. The random errors vary with space and time, the quality assurance applied {{and the types of}} instrument used to make the observations. The estimates of <b>random</b> <b>measurement</b> <b>error</b> are compared with estimates of total observational error, which includes uncertainty due both to measurement errors and to observational sampling. In tropical regions the measurement error makes a significant contribution to the total observational error in a single observation, but in higher latitudes the sampling error can be much larger...|$|E
40|$|Abstract. <b>Random</b> <b>measurement</b> <b>errors</b> {{in digital}} terrain models are {{important}} to their quality. The present paper describes an untypical use of variograms to compute this kind of <b>measurement</b> <b>errors.</b> After subtracting a low frequency drift from depth/height measurements, a variogram analysis of the residuals is used to derive the <b>random</b> <b>measurement</b> <b>errors</b> of the observations. Numerical experiments on lasercanning and multibeam echo sounder data show how the semivariogram clearly identifies the amount of noise in the original data. The method can run in real time and does not require additional measurements like ground control patches or overlapping survey strips...|$|R
40|$|The {{effect of}} random and {{systematic}} errors {{associated with the}} measurement of normal incidence acoustic impedance in a zero-mean-flow environment was investigated by the transmission line method. The influence of <b>random</b> <b>measurement</b> <b>errors</b> in the reflection coefficients and pressure minima positions was investigated by computing fractional standard deviations of the normalized impedance. Both the standard techniques of random process theory and a simplified technique were used. Over a wavelength range of 68 to 10 cm <b>random</b> <b>measurement</b> <b>errors</b> in the reflection coefficients and pressure minima positions could be described adequately by normal probability distributions with standard deviations of 0. 001 and 0. 0098 cm, respectively. An error propagation technique based on the observed concentration of the probability density functions was found to give essentially the same results but with a computation time of about 1 percent of that required for the standard technique. The results suggest that careful experimental design reduces the effect of <b>random</b> <b>measurement</b> <b>errors</b> to insignificant levels for moderate ranges of test specimen impedance component magnitudes. Most of the observed random scatter {{can be attributed to}} lack of control by the mounting arrangement over mechanical boundary conditions of the test sample...|$|R
40|$|We {{evaluated}} {{the performance of}} two methods for estimating stem volume increment at individual tree level with respect to bias due to <b>random</b> <b>measurement</b> <b>errors.</b> Here, growth is either predicted as the difference between two consecutive volume estimates where single-tree volume functions are applied to data from repeated measurements or by a regression model that is applied to data from a single survey and includes radial increment. In national forest inventories (NFIs), the first method is typically used for permanent plots, the second for temporary plots. The Swedish NFI combines estimates from both plot types to assess growth at national and regional scales and it is, therefore, important that the two methods provide similar results. The accuracy of these estimates is affected by <b>random</b> <b>measurement</b> <b>errors</b> in the independent variables, which may lead to systematic errors in predicted variables due to model non-linearity. Using Taylor series expansion and empirical data from the Swedish NFI we compared the expected bias in stem volume growth estimates for different diameter classes of Scots pine (Pinus sylvestris L.) and Norway spruce (Picea abies (L.) Karst.). Our results indicate that both methods are fairly insensitive to <b>random</b> <b>measurement</b> <b>errors</b> of the size {{that occur in the}} Swedish NFI. The empirical comparison between the two methods showed greater differences for large diameter trees of both pine and spruce. A likely explanation is that the regressions are uncertain because few large trees were available for developing the models...|$|R
40|$|We {{investigate}} the differential impact of alternative combinations of {{horizontal and vertical}} educational mismatches on wages. By using panel data for Belgian graduates, we consider the role of unobserved worker heterogeneity. <b>Random</b> <b>measurement</b> <b>error</b> in both types of mismatches is accounted for by adopting instrumental variable tech-niques. We consistently find that overeducated individuals without field of study mismatch earn less than adequately educated workers with a similar educational back-ground. However, for individuals who are working outside their field of study, such a wage penalty is not always observed once accounting for unobserved heterogeneity and <b>random</b> <b>measurement</b> <b>error.</b> In some cases, field of study mismatch even seems to be financially beneficial to the worker. These findings contribute to our understanding regarding {{the extent to which}} educational mismatches are truly problematic. The results call for policies that focus primarily on combatting vertical mismatches. status: accepte...|$|E
40|$|We {{investigate}} {{the properties of}} exchange rate forecasts with a data set encompassing a broad cross section of currencies. The key finding is that expectations appear to be biased in our sample. This result is robust {{to the possibility of}} <b>random</b> <b>measurement</b> <b>error</b> in the survey measures. Investors would be better off placing less weight on their forecasts or the forward rate, and more on the current spot rate. ...|$|E
3000|$|... is {{the perceptual}} estimate. Sensory {{estimates}} {{are subject to}} two types of error: <b>random</b> <b>measurement</b> <b>error</b> and bias. Thus, estimates of the same object property from different cues usually differ. To reconcile the discrepancy, the nervous system must either combine estimates or choose one, thereby ignoring the other cues. Assuming that each single-cue estimate is unbiased but corrupted by independent Gaussian noise, the statistically optimal strategy for cue combination is a weighted average [7]: [...]...|$|E
50|$|At this stage, {{the angle}} on the {{instrument}} is double the angle of interest between the points. Repeating the procedure causes the instrument to show 4x the angle of interest, with further iterations increasing it to 6x, 8x, and so on. In this way, many measurements can be added together, allowing some of the <b>random</b> <b>measurement</b> <b>errors</b> to cancel out.|$|R
40|$|Our current OMEGA {{experimental}} {{campaign is}} developing the thin shell diagnostic {{for use on}} NIF with the needed accuracy. The thin shell diagnostic {{has the advantage of}} linearity over alternative measurement techniques, so that low-order modes will not corrupt the measurement of high-order modes. Although our <b>random</b> <b>measurement</b> <b>errors</b> are adequate, we need to monitor beam balance and ensure that the thin shells have a uniform thickness...|$|R
40|$|This paper {{adopts a}} Hidden Markov Model {{as a basis}} for {{predicting}} the probabilities in location of soccer robot’s trajectories, develops the corresponding algorithms, and then demonstrates the simplicity of the procedure with simulations. The purpose of the initial presentation is to establish a proper platform for the future comprehensive studies of using Hidden Markov Models to assemble critical observations with uncertainties or <b>random</b> <b>measurement</b> <b>errors</b> in stochastic system modelling and control...|$|R
40|$|Epidemiologic studies {{routinely}} {{suffer from}} bias due to exposure measurement error. In this paper, the authors examine {{the effect of}} measurement error when the exposure variable of interest is constrained by a lower limit. This is an important consideration, since often in epidemiologic studies an exposure variable is constrained by a lower limit such as zero or a nonzero detection limit. In this paper, attenuation of exposure-disease associations is defined within the framework of a classical model of uncorrelated additive error. Then, the special case of nonlinearity due to the effect of a lower threshold is examined. A general model is developed to characterize the effect of <b>random</b> <b>measurement</b> <b>error</b> when there is a lower threshold for recorded values. Findings are illustrated under the assumption that the true exposure follows the lognormal and gamma distributions. The authors show that the direction and magnitude of bias in estimated exposure-response associations depends on the population distribution of the exposure, the magnitude of the recording threshold, the value assigned to below-threshold measurement results, and the variance in the measured exposure due to <b>random</b> <b>measurement</b> <b>error.</b> bias (epidemiology); epidemiologic methods; measurement error; regression analysis Epidemiologists are routinely concerned about the conse-quences of exposure measurement error. Investigators may rely on inaccurate proxy measures of exposure or on infor...|$|E
40|$|Abstract: Casper and Tufis’s article (2003) {{identified}} the noninterchangeability {{problem of the}} three most commonly used democracy indicators. But, they did not identify {{the source of this}} problem and did not provide a right solution either. As demonstrated in this paper, <b>random</b> <b>measurement</b> <b>error</b> or low reliability is what makes the indicators non-interchangeable. The right way, as proposed in this article, to correct this problem is to build a measurement model covering the random measurement errors or to combine the existing indicators into a new and reliable measurement...|$|E
40|$|Eliciting {{expectation}} {{and introducing}} probabilistic questions into surveys have gained important interest. In this study, {{we focus on}} the reliability of students earnings expectations. To what extent is observed log earnings expectations affected by <b>random</b> <b>measurement</b> <b>error</b> (noise) ? A test-retest method is applied and reliability is found to be fairly low; about 0. 59 in 2015 and about 0. 67 in 2016. Particularly homogeneous samples exaggerate problems of measurement error. The analysis show how these measures of reliability can be adjusted to become more suitable to other studies, where different degrees of homogeneity are present...|$|E
40|$|Uncertainties in {{discharge}} determination {{may have}} serious consequences for hydrological modelling and resulting discharge predictions affecting flood and drought risk assessment and decision making. The {{aim of this}} study is to quantify the effect of discharge errors on parameters and performance of a conceptual hydrological model for discharge prediction applied to two catchments. Four error sources in discharge determination are considered: a combination of systematic and <b>random</b> <b>measurement</b> <b>errors</b> without autocorrelation; <b>random</b> <b>measurement</b> <b>errors</b> with autocorrelation; hysteresis in the discharge-water level relation; and effects of an outdated discharge–water level relation. Results show that systematic errors and an outdated discharge–water level relation have a considerable influence on model performance, while other error sources have a small to negligible effect. The effects of errors on parameters are large if the effects on model performance are large, and vice versa. Parameters controlling the water balance are influenced by systematic errors, and parameters related to the shape of the hydrograph are influenced by random errors. Large effects of discharge errors on model performance and parameters should be taken into account when using discharge predictions for risk assessment and decision makin...|$|R
40|$|Conclusions {{about changes}} in {{categorical}} characteristics based on observed panel data can be incorrect when (even a small amount of) <b>measurement</b> <b>error</b> is present. <b>Random</b> <b>measurement</b> <b>errors,</b> referred to as independent classification errors, usually lead to over-estimation of {{the total amount of}} gross change, whereas systematic, correlated errors usually cause underestimation of the transitions. Furthermore, the patterns of true change may be seriously distorted by independent or systematic classification errors. Latent class models and directed log-linear analysis are excellent tools to correct for both independent and correlated <b>measurement</b> <b>errors.</b> An extensive example on labor market states taken from the Survey of Income and Program Participation panel is presented...|$|R
40|$|Because hunger {{has many}} faces, {{it makes sense}} to choose a multidimensional {{approach}} for calculating the Global Hunger Index (GHI). Such an approach has the following advantages: 1. It simultaneously captures various aspects of hunger and undernutrition. 2. The combination of indicators measured independently of each other reduces the impact of <b>random</b> <b>measurement</b> <b>errors.</b> 3. The condensing of information facilitates a quicker overview for decision makers in the public and political arenas. ReportNon-PRIFPRI 2; Theme 7; Subtheme 7. 2; GRP 24; Enhanced food and diet qualityFCN...|$|R
40|$|Aggregate {{rates of}} {{productivity}} growth {{are among the}} most closely watched indicators of economic performance. They are also among the most difficult to measure accurately. This paper explores the sensitivity of such rates to <b>random</b> <b>measurement</b> <b>error</b> using a simple generic model. The model allows for errors in the input and output components of the productivity ratio, with different variances, and for serial and cross correlation of the errors. The effects of the errors are considered {{from the point of view}} of growth rates themselves, changes in growth rates, and comparisons between rates in different countries. productivity; growth rates; measurement error...|$|E
40|$|Geiser (Multitrait-multimethod-multioccasion modeling, 2009) {{recently}} {{presented the}} Correlated State-Correlated (Methods-Minus- 1) [CS-C(M− 1) ] model for analysing longitudinal multitrait-multimethod (MTMM) data. In the present article, the authors discuss {{the extension of}} the CS-C(M− 1) model to a model that includes latent difference variables, called CS-C(M− 1) change model. The CS-C(M− 1) change model allows investigators to study inter-individual differences in intra-individual change over time, to separate true change from <b>random</b> <b>measurement</b> <b>error,</b> and to analyse change simultaneously for different methods. Change in a reference method can be contrasted with change in other methods to analyse convergent validity of change...|$|E
40|$|The {{properties}} of exchange-rate forecasts are investigated, with a data set encompassing a broad {{cross section of}} currencies. Over the entire sample, expectations appear to be biased. This result is robust {{to the possibility of}} <b>random</b> <b>measurement</b> <b>error</b> in the survey measures. There appear to be statistically significant differences in the degree of bias in subgroupings of the data: the bias is lower for the high-inflation countries; the bias is greater for the major currencies studied in earlier papers; and the bias is also greater for the EMS currencies. Copyright 1994 by Ohio State University Press. ...|$|E
40|$|This paper {{describes}} a setup {{with a low}} sensitivity to temperature variations for determining the <b>random</b> <b>measurement</b> <b>errors</b> of a <b>measurement</b> system applying a moving scale. This moving-scale system is developed for advanced equipment such as ultra-precision machine tools and should operate with a measurement uncertainty of 15 nm for a measurement length of 109 mm and temperature variations of 1 °C. Temperature drift is identified as the most contributing source of errors and therefore should be accurately determined. A dedicated setup has been designed for this task. status: publishe...|$|R
40|$|This {{paper is}} {{concerned}} with the estimation of the parameters in a dynamic simultaneous equation model with stationary disturbances under the assumption that the variables are subject to <b>random</b> <b>measurement</b> <b>errors.</b> The conditions under which the parameters are identified are stated. An asymptotically efficient frequency-domain class of instrumental variables estimators is suggested. The procedure consists of two basic steps. The first step transforms the model {{in such a way that}} the observed exogenous variables are asymptotically orthogonal to the residual terms. The second step involves an iterative procedure like that of Robinson [13]. ...|$|R
40|$|Sources of {{variability}} in experimentally derived data include <b>measurement</b> <b>error</b> {{in addition to}} the physical phenomena of interest. This <b>measurement</b> <b>error</b> is a combination of systematic components, originating from the measuring instrument, and <b>random</b> <b>measurement</b> <b>errors.</b> Several novel biological technologies, such as mass cytometry and single-cell RNA-seq, are plagued with systematic errors that may severely affect statistical analysis if the data is not properly calibrated. We propose a novel deep learning approach for removing systematic batch effects. Our method is based on a residual network, trained to minimize the Maximum Mean Discrepancy (MMD) between the multivariate distributions of two replicates, measured in different batches. We apply our method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that it effectively attenuates batch effects. Comment: fixed typ...|$|R
40|$|This paper {{describes}} a new signal processing scheme for the 46. 5 MHz Doppler Beam Swinging wind-profiling radar at Aberystwyth, in the UK. Although the techniques used {{are similar to}} those already described in literature – i. e. the identification of multiple signal components within each spectrum and the use of radial- and time-continuity algorithms for quality-control purposes – it is shown that they must be adapted for the specific meteorological environment above Aberystwyth. In particular they need {{to take into account the}} three primary causes of unwanted signals: ground clutter, interference, and Rayleigh scatter from hydrometeors under stratiform precipitation conditions. Attention is also paid to the fact that short-period gravity-wave activity can lead to an invalidation of the fundamental assumption of the wind field remaining stationary over the temporal and spatial scales encompassed by a cycle of observation. Methods of identifying and accounting for such conditions are described. The <b>random</b> <b>measurement</b> <b>error</b> associated with horizontal wind components is estimated to be 3. 0 – 4. 0 m s − 1 for single cycle data. This reduces to 2. 0 – 3. 0 m s − 1 for data averaged over 30 min. The <b>random</b> <b>measurement</b> <b>error</b> associated with vertical wind components is estimated to be 0. 2 – 0. 3 m s − 1. This cannot be reduced by time-averaging as significant natural variability is expected over intervals of just a few minutes under conditions of short-period gravity-wave activity...|$|E
40|$|This paper {{develops}} {{a method to}} correct for non-random measurement error in a binary indicator of illicit drugs. Our results suggest that estimates {{of the effect of}} self reported prenatal drug use on birth weight are biased upwards by measurement error [...] a finding contrary to predictions of a model of <b>random</b> <b>measurement</b> <b>error.</b> We show that more accurate estimates of the true effect of drug use on birth weight can be obtained by using the predicted probability of falsely reporting drug use. This suggests that out-of-sample information on drug use may improve estimates of the effect of reported drug use in other settings. ...|$|E
40|$|When {{people are}} {{suffering}} from mental issues like depression or anxiety, they can seek help from an intervention against these mental issues. When an intervention is new, researchers typically want to investigate whether the intervention has the desired effect on relevant outcome variables. Also, in recent decades, insurance companies have increased the pressure on mental health care workers to systematically monitor the progression of their clients before, during, and after the intervention, {{in order to avoid}} the funding of ineffective interventions. The psychological scales that are used to measure the mental states of the clients over time are typically indirect, not completely reliable scales for psychological constructs like depression. That is, the observed scores on these scales typically consist of a mixture of true scores and <b>random</b> <b>measurement</b> <b>error.</b> Hence, observed changes in scores collected before and after the intervention may be the result of measurement error rather than true changes over time. This raises the question of how much evidence observed data contain for or against a true intervention effect, as compared to no true intervention effect where observed differences are solely due to <b>random</b> <b>measurement</b> <b>error.</b> In this thesis I developed several hypothesis tests for intervention effects which quantify the relative evidence in data for or against hypotheses regarding intervention effects. The hypothesis tests are so called Bayes factors, which are built from within the Bayesian statistical approach. I developed Bayes factors for mean and trend change after the intervention for single-subject designs as well as group designs...|$|E
40|$|This paper {{presents}} {{a method for}} functional testing of analog circuits, {{on the basis of}} circuit sensitivities. The approach selects the minimum number of measurements that allows a precise prediction of a circuit's functional behavior. A criterion is applied to this predicted behavior to determine if the circuit functions according to specifications. The presented method combines a matrix decomposition technique (the singular value decomposition) with an iterative algorithm to select measurements. The number of measurements is determined {{on the basis of the}} desired precision Of the response prediction and the influence of <b>random</b> <b>measurement</b> <b>errors.</b> Examples demonstrate that the resulting method tests the functional circuit behavior with a high precision, even in the presence of large <b>measurement</b> <b>errors...</b>|$|R
40|$|In this paper, {{the design}} of a test setup for {{determining}} the <b>random</b> <b>measurement</b> <b>errors</b> of a 1 -DOF moving-scale measurement system is presented. A 3 -DOF moving-scale system enables the measurement of the position of a workpiece table moving in three perpendicular directions with minimized Abbe-offset. This allows standard machine tools and CMMs to match or even surpass the accuracy of ultra-precision machines. An error budget indicates the 1 -DOF moving-scale system can reach a measurement uncertainty of 18 nm. Thermal drift is the largest error source and should be accurately determined. A dedicated setup was designed for this task. status: publishe...|$|R
40|$|We {{introduce}} {{a modification of}} the index of increase that works in both deterministic and random environments, and thus allows us to assess monotonicity of functions that are prone to <b>random</b> <b>measurement</b> <b>errors.</b> We prove consistency of the empirical index and show how its rate of convergence is influenced by deterministic and random parts of the data. In particular, the obtained results allow us to determine the frequency at which observations should be taken {{in order to reach}} any pre-specified level of estimation precision. We illustrate the performance of the suggested estimator using simulated data arising from purely deterministic and error-contaminated monotonic and non-monotonic functions. Comment: 28 page...|$|R
40|$|This paper {{presents}} an algorithm for the automatic registration of terrestrial point clouds by match selection using an efficiently conditional sampling method - Threshold-independent BaySAC (BAYes SAmpling Consensus) and employs the error metric of average point- To-surface residual {{to reduce the}} <b>random</b> <b>measurement</b> <b>error</b> and then approach the real registration error. BaySAC and other basic sampling algorithms usually need to artificially determine a threshold by which inlier points are identified, {{which leads to a}} threshold-dependent verification process. Therefore, we applied the LMedS method to construct the cost function that is used to determine the optimum model to reduce the influence of human factors and improve the robustness of the model estimate. Point- To-point and point- To-surface error metrics are most commonly used. However, point- To-point error in general consists of at least two components, <b>random</b> <b>measurement</b> <b>error</b> and systematic error {{as a result of a}} remaining error in the found rigid body transformation. Thus we employ the measure of the average point- To-surface residual to evaluate the registration accuracy. The proposed approaches, together with a traditional RANSAC approach, are tested on four data sets acquired by three different scanners in terms of their computational efficiency and quality of the final registration. The registration results show the st. dev of the average point- To-surface residuals is reduced from 1. 4 cm (plain RANSAC) to 0. 5 cm (threshold-independent BaySAC). The results also show that, compared to the performance of RANSAC, our BaySAC strategies lead to less iterations and cheaper computational cost when the hypothesis set is contaminated with more outliers. </p...|$|E
40|$|Abstract: An {{improved}} adaptive Huber filter {{algorithm is}} presented to model error and mea-surement noise uncertainty. The adaptive algorithm for model error is obtained using an upper bound for the state prediction covariance matrix. The measurement noise is estimated at each filter step by minimizing a criterion function which was original from adaptive Huber filter. A recursive algorithm is provided for solving the criterion function. The proposed adaptive filter algorithm was successfully implemented in relative navigation using global position system for spacecraft formation flying in high earth orbits with real orbit perturbations and non-Gaussian <b>random</b> <b>measurement</b> <b>error.</b> Simulation {{results indicated that the}} proposed adaptive filter per-formed better in robustness and accuracy compared with previous adaptive algorithms...|$|E
40|$|Analyzing {{time series}} data for {{evidence}} of changes in market conditions is a problem central to development of a marketing information system. Examples are tracking field reports on competitors' prices and analyzing market share estimates computed from retail audits and consumer panel reports. These series may involve substantial <b>random</b> <b>measurement</b> <b>error,</b> and any changes, often small relative to this error, are difficult to detect promptly and accurately. One way to approach this problem is with techniques recently developed for handling time series with small stochastic mean shifts as well as random error. Two special procedures are compared in Monte Carlo analyses with a simpler data filtering and control-chart approach; the latter appears the most promising. ...|$|E
40|$|This {{paper is}} a postprint {{of a paper}} {{submitted}} to and accepted for publication in journal Electronics Letters and is subject to Institution of Engineering and Technology Copyright. The copy of record is available at IET Digital Library -[Abstract] It is shown, by means of numerical simulation, that the Moore-Penrose pseudoinverse of a matrix taken from an overdetermined system {{can be applied to}} retrieve the excitation distribution of a planar array of parallel dipoles with faulty elements, by measuring the complex radiated field in its near zone. Failures on voltage (considering mutual coupling) and current of several elements, and systematic or <b>random</b> <b>measurement</b> <b>errors,</b> are considered in the simulation...|$|R
40|$|Two {{sites in}} Canada were {{selected}} for {{detailed analysis of the}} ESMR- 6 / snow relationships. Data were analyzed for February 1976 for site 1 and January, February and March 1976 for site 2. Snowpack water equivalents were less than 4. 5 inches for site 1 and, depending on the month, were between 2. 9 and 14. 5 inches for site 2. A statistically significant relationship was found between ESMR- 6 measurements and snowpack water equivalents for the Site 2 February and March data. Associated analysis findings presented are the effects of <b>random</b> <b>measurement</b> <b>errors,</b> snow site physiolography, and weather conditions on the ESMR- 6 /snow relationship...|$|R
40|$|The Generalized Axiom of Revealed Preference (GARP) can be {{violated}} {{because of}} <b>random</b> <b>measurement</b> <b>errors</b> in the observed quantity data. We study two tests proposed by Varian (1985) and de Peretti (2004), which test GARP within an explicit stochastic framework. Both tests compute adjusted quantity {{data that are}} compliant with GARP. We compare and contrast the two tests in theoretical terms and in an empirical application. The empirical application is based on testing {{a large group of}} monetary assets for the United States over multiple sample periods spanning 1960 - 1992. We found that both tests provided reasonable results and were largely consistent with each other...|$|R
