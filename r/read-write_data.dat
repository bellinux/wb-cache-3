12|27|Public
50|$|HSQLDB has {{two main}} table types used for durable <b>read-write</b> <b>data</b> storage, i.e., if a {{transaction}} {{has been successfully}} committed, it is guaranteed that the data will survive system failure and will keep their integrity.|$|E
40|$|We study {{efficient}} and robust implementations of an atomic <b>read-write</b> <b>data</b> structure over an asynchronous distributed message-passing system made of reader and writer processes, {{as well as}} a number of servers implementing the data structure. We determine the exact conditions under which every read and write involves one round of communication with the servers. These conditions relate the number of readers to the tolerated number of faulty servers and the nature of these failures...|$|E
40|$|Abstract. In this paper, {{we provide}} a {{view on the}} future Web as a Semantic {{read-write}} Web. Given a number of prerequisites for enabling a fully read-write Web for machines, we predict the following. First, datasets and data in general will be driven by machine updates. Second, lots of human-created machine-readable schemas will become obsolete. Third, the Web of Services and the Web of Data will be fully integrated into a Web of <b>read-write</b> <b>Data.</b> Fourth, reasoning on the Web stays mono-tonic and will be optimized for versioned data. Additionally, two appli-cation areas are discussed where the read-write Semantic Web will have a deep impact: the Web of Things and advanced personalisation. ...|$|E
5000|$|AtomGraph Processor, a Java backend for {{building}} declarative, <b>read-write</b> Linked <b>Data</b> applications, supports HATEOAS Linked Data ...|$|R
5000|$|<b>Read-Write</b> Linked <b>data</b> was {{previously}} described using WebDAV andSPARUL [...] by Tim Berners-Lee in his design issues [...] that built upon his 4 rules for linked data [...]|$|R
50|$|LDP {{evolved from}} work at IBM's Rational Product Group for applicationintegration. Starting in 2010, IBM looked at linked data forapplication {{lifecycle}} management and sought what was an alternativemeans for <b>read-write</b> Linked <b>Data.</b>|$|R
40|$|This paper {{describes}} a {{database management system}} (DBMS) modified to use hardware write protection to guard critical DBMS data structures against software errors. Guarding (write-protecting) DBMS data improves software reliability by providing quick detec-tion of corrupted pointers and array bounds overruns. Guarding will be especially helpful in an extensible DBMS since it limits the power of extension code to corrupt unrelated parts of the system. <b>Read-write</b> <b>data</b> structures can be guarded as long as correct software is able to temporarily unprotect the data struc-tures during updates. The paper discusses the effects of three different update models on per-formance, software complexity, and error pro-tection Measurements of a DBMS which uses guarding to protect its buffer pool show two to eleven percent performance degradation in a debit/credit benchmark...|$|E
40|$|We {{present the}} design and {{evaluation}} of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word. The novel aspect of this system {{is that we are}} able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer. We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found <b>read-write</b> <b>data</b> races in a program that allows unsynchronized read access to a global tour bound, and a [...] ...|$|E
40|$|International audienceThe {{multiplication}} of {{the number}} of cores inside embedded systems has raised the pressure on the memory hierarchy. The cost of coherence protocol and the scalability problem of the memory hierarchy is nowadays a major issue. In this paper, a specific data management for read-only data is in-vestigated because these data can be duplicated in several memories without being tracked. Based on analysis of stan-dard benchmarks for embedded systems, we show that read-only data represent 62 % of all the data used by applications and 18 % of all the memory accesses. A specific data path for read-only data is then evaluated by using simulations. On the first level of the memory hierarchy, removing read-only data of the L 1 cache and placing them in another read-only cache improve the data locality of the <b>read-write</b> <b>data</b> by 30 % and decrease the total energy consumption of the first level memory by 5 %...|$|E
40|$|Abstract. In {{this paper}} we {{describe}} {{a novel approach}} to <b>read-write</b> Linked <b>Data</b> design. By combining URI-to-query mapping with SPIN, XSLT, and RDF/POST, many advanced Web application features can be implemented declaratively. Such architecture is standard-compliant and provides a rapid way to build life-science Linked Data apps from reusable components...|$|R
40|$|Abstract. The current Web of Data, {{including}} linked datasets, RDFa content, and GRDDL-enabled microformats is a read-only Web. Although this read-only Web of Data enables data integration, faceted browsing {{and structured}} queries over large datasets, we lack a general concept for a <b>read-write</b> Web of <b>Data.</b> That is, {{we need to}} understand how to create, update and delete RDF data. Starting from the experience we have gathered with Tabulator Redux—a single-triple update system based on a data Wiki—we review necessary components to realize a <b>read-write</b> Web of <b>Data.</b> We propose a form-based editing approach for RDF graphs along with the integration of site-specific APIs. Further, we present a concept of a uniform architecture for a <b>read-write</b> Web of <b>Data,</b> including a demonstration. Eventually, our work reveals issues and challenges of the proposed architecture and discusses future steps...|$|R
5000|$|Linked Data Platform (LDP), a Linked Data {{specification}} {{defining a}} set of integration patterns for building RESTful HTTP services {{that are capable of}} <b>read-write</b> of RDF <b>data.</b>|$|R
40|$|Abstract—Named Data Networking (NDN) is a {{recently}} pro-posed general-purpose network architecture that leverages {{the strengths of}} Internet architecture while aiming to address its weaknesses. NDN names packets rather than end-hosts, and most of NDN’s characteristics are a consequence of this fact. In this paper, {{we focus on the}} packet forwarding model of NDN. Each packet has a unique name which is used to make forwarding decisions in the network. NDN forwarding differs substantially from that in IP; namely, NDN forwards based on variable-length names and has a <b>read-write</b> <b>data</b> plane. Designing and evaluating a scalable NDN forwarding node architecture is a major effort within the overall NDN research agenda. In this paper, we present the concepts, issues and principles of scalable NDN forwarding plane design. The essential function of NDN forwarding plane is fast name lookup. By studying the performance of the NDN reference implementation, known as CCNx, and simplifying its forwarding structure, we identify three key issues in the design of a scalable NDN forwarding plane: 1) exact string matching with fast updates, 2) longest prefix matching for variable-length and unbounded names and 3) large-scale flow maintenance. We also present five forwarding plane design principles for achieving 1 Gbps throughput in software implementation and 10 Gbps with hardware acceleration...|$|E
40|$|Trends in {{conventional}} storage infrastructure motivate {{the development of}} foundational technologies for building a wide-area read-write storage repository capable of providing a single image of a distributed storage resource. The overarching design goals of such an infras-tructure include client performance, global resource utilization, system scalability (providing a single logical view of larger resource and user pools) and application scalability (enabling single applications with large resource requirements). Such a storage infrastructure forms the basis for second generation data-grid efforts underlying massive data handling in high-energy physics, nanosciences, and bioinformatics, among others. This paper describes some of the foundational technologies underlying such a repository, Plethora, for semi-static peer-to-peer (P 2 P) networks implemented on a wide-area Internet testbed. In contrast to many current efforts that focus entirely on unstructured dynamic P 2 P environments, Plethora focuses on semi-static peers with strong network connectivity and a partially persistent network state. In a semi-static P 2 P network, peers are likely to remain par-ticipants in the network {{over long periods of}} time (e. g., compute servers), and are capable of providing reasonably high availability and response-time guarantees. The repository integrates novel concepts in locality enhancing overlay networks, transactional semantics for <b>read-write</b> <b>data</b> coupled with hierarchical versioning, intelligent replication for robustness. While men-tioning approaches to other problems, this paper focuses on the problem of routing data request to blocks, while integrating caching and locality enhancing overlays into a single framework. We show significant performance improvements resulting from our routing techniques...|$|E
40|$|This paper revisits the {{historical}} concept of link-layer, block-oriented storage networking {{in light of}} commoditybased, high-bandwidth local area networks. It examines how advances in data communication technology such as virtual LANs and switched gigabit Ethernet make it relatively simple and inexpensive to re-centralize the storage resources used by clusters of workstations and servers. In particular, the work is motivated by an interest in how to support scalable and efficient access to read-only and private <b>read-write</b> <b>data</b> such as root file systems, swap partitions, log files and static web pages. These techniques complement, but do not replace, higher level distributed file systems whose primary goal is to provide coherent access to shared read/write data. This paper describes the design and implementation of a very simple, link-layer protocol, the Ethernet Block Device (EBD), for accessing remote block devices. It compares the EBD prototype to a locally attached disk and to similar, network-based techniques that use TCP/IP such as the Linux Network Block Device (NBD), as well as higher level distributed file systems such as NFS. Functionally, the implementation is compared with a local disk to determine what restrictions and visible differences exist. The performance evaluation {{is based on a}} series of standard disk and file access benchmarks run on commodity servers and standard networking equipment. The performance results show that for large sequential reads and random reads and writes EBD generally outperforms comparative network storage technologies by 15 % to 30 %, and performance is best when the data set fits into the server's memory. On a benchmark that is very metadata-intensive and does small sequential operations, EBD and NBD perform similarly. On certain [...] ...|$|E
50|$|Other table types allow {{access to}} comma-separated values (CSV) files. These tables can participate, for example, in queries with JOINs and simplify {{spreadsheet}} processing and <b>read-write</b> non-durable in-memory <b>data</b> storage.|$|R
40|$|This paper {{discusses}} {{issues that}} will affect the future development of the Web, either increasing its power and utility, or alternatively suppressing its development. It argues {{for the importance of}} the continued development of the Linked Data Web, and describes the use of linked open data as an important component of that. Second, the paper defends the Web as a read–write medium, and goes on to consider how the <b>read–write</b> Linked <b>Data</b> Web could be achieved...|$|R
40|$|Abstract. The current Web of Data, {{including}} linked datasets, RDFa content, and GRDDL-enabled microformats is a read-only Web. Al-though this read-only Web of Data enables data integration, faceted browsing {{and structured}} queries over large datasets, we lack a general concept for a <b>read-write</b> Web of <b>Data.</b> That is, {{we need to}} understand how to create, update and delete RDF data in a secure, reliable, trust-worthy and scalable way. Attempting to change this situation, this paper reviews available components, presents our vision of a uniform architec-ture for a <b>read-write</b> Web of <b>Data</b> as well as a proof of concept. The paper exposes issues and challenges of the proposed architecture and discusses the next necessary steps. ...|$|R
40|$|The CORAL {{software}} {{is widely used}} at CERN for accessing the data stored by the LHC experiments using relational database technologies. CORAL provides a C++ abstraction layer that supports data persistency for several backends and deployment models, including local access to SQLite files, direct client access to Oracle and MySQL servers, and read-only access to Oracle through the FroNTier web server and cache. Two new components have recently been added to CORAL to implement a model involving a middle tier "CORAL server" deployed close to the database and a tree of "CORAL server proxy" instances, with data caching and multiplexing functionalities, deployed close to the client. The new components are meant to provide advantages for read-only and <b>read-write</b> <b>data</b> access, in both offline and online use cases, {{in the areas of}} scalability and performance (multiplexing for several incoming connections, optional data caching) and security (authentication via proxy certificates). A first implementation of the two new components has been released in the summer 2009 and is presently fully deployed in the ATLAS online system, where the CORAL server and proxy are used to read the configuration and conditions data that are needed by the High Level Trigger, allowing the configuration of a farm of several thousand processes. This {{software is}} the result of a joint development of the CERN IT Department and of the Mainz and SLAC groups of the ATLA S collaboration. This presentation will report on the status of developments and deployment for the CORAL server and proxy components and on their usage in the ATLAS online system during the first year of LHC data taking. The design of the software architecture and the outlook for its eventual use for offline data acccess will also be covered...|$|E
40|$|Without doubt, cloud {{computing}} {{is one of}} the current dominant trends in computing. More and more of our computation is moving into the cloud, owed to the ad-vent of services such as Amazon’s EC 2. Increasingly, there are services and applications with the ultimate goal of moving the entirety of a user’s virtual exis-tence into the cloud, such as eyeOS 1 or Google’s up-coming Chrome OS 2. However, this requires cloud-based data storage (as opposed to computation), to be on par with the convenience and performance of local storage systems. With “classic ” means of local data storage, there are implicit trade-offs being made between availabil-ity and reliability of storage as we attribute different properties to different storage media. There are a number of “cloud storage ” services and backends already, most notably Amazon’s S 3, but also including Rackspace Cloud Files 3 and Mi-crosoft’s SkyDrive 4. Currently, the use cases of these services, and their resale variants, are mostly synchro-nisation and backups, but not full-scale storage of per-sonal data. If, however, a truly universal data storage facility with properties analogous to those of local storage systems is considered – particularly with respect to availability, reliability and consistency guarantees – then it becomes evident that these existing systems do not provide currently the necessary and expected guarantees for <b>read-write</b> <b>data</b> access in a diverse set of use cases. According to Brewer’s conjecture [4], no storage service can ever simultaneously achieve all three properties of consistency, availability and partition tolerance, meaning that any solution will always be a tradeoff. II. RESEARCH OBJECTIVES My proposed research aims to combine and refine the existing concepts of {{cloud computing}} and dis-tributed storage systems by introducing a notion of parametrised data storage in which data is classified ac-cording to the guarantees required. The classification can be automatic or explicitly initiated by the user or an application. It could then be used by a storage system to provide dynamic tradeoffs in the orthogona...|$|E
40|$|Abstract. Enterprises are {{increasingly}} using {{a wide range}} of heteroge-neous information systems for executing and governing their business activities. Even if the adoption of service orientation has improved loose coupling and reusability, applications are still isolated data silos whose integration requires complex transformations and mediations. However, by leveraging Linked Data principles those data silos can now be seamlessly integrated, and this opens the door to new data-driven approaches for Enterprise Application Integration (EAI). In this paper we present LDP 4 j, an open souce Java-based framework for the develop-ment of interoperable <b>read-write</b> Linked <b>Data</b> applications, based on the W 3 C Linked Data Platform (LDP) specification...|$|R
40|$|This work {{addresses}} {{a gap in}} the foundations of computer science. In particular, {{only a limited number of}} models address design decisions in modern Web architectures. The development of the modern Web architecture tends to be guided by the intuition of engineers. The intuition of an engineer is probably more powerful than any model; however, models are important tools to aid principled design decisions. No model is sufficiently strong to provide absolute certainty of correctness; however, an architecture accompanied by a model is stronger than an architecture accompanied solely by intuition lead by the personal, hence subjective, subliminal ego. The Web of Data describes an architecture characterised by key W 3 C standards. Key standards include a semi-structured data format, entailment mechanism and query language. Recently, prominent figures have drawn attention to the necessity of update languages for the Web of Data, coining the notion of <b>Read–Write</b> Linked <b>Data.</b> A dynamicWeb of Data with updates is a more realistic reflection of the Web. An established and versatile approach to modelling dynamic languages is to define an operational semantics. This work provides such an operational semantics for a <b>Read–Write</b> Linked <b>Data</b> architecture. Furthermore, the model is sufficiently general to capture the established standards, including queries and entailments. Each feature is relative easily modelled in isolation; however a model which checks that the key standards socialise is a greater challenge to which operational semantics are suited. The model validates most features of the standards while raising some serious questions. Further to evaluating W 3 C standards, the operational mantics provides a foundation for static analysis. One approach is to derive an algebra for the model. The algebra is proven to be sound with respect to the operational semantics. Soundness ensures that the algebraic rules preserve operational behaviour. If the algebra establishes that two updates are equivalent, then they have the same operational capabilities. This is useful for optimisation, since the real cost of executing the updates may differ, despite their equivalent expressive powers. A notion of operational refinement is discussed, which allows a non-deterministic update to be refined to a more deterministic update. Another approach to the static analysis of <b>Read–Write</b> Linked <b>Data</b> is through a type system. The simplest type system for this application simply checks that well understood terms which appear in the semi-structured data, such as numbers and strings of characters, are used correctly. Static analysis then verifies that basic runtime errors in a well typed program do not occur. Type systems for URIs are also investigated, inspired by W 3 C standards. Type systems for URIs are controversial, since URIs have no internal structure thus have no obvious non-trivial types. Thus a flexible type system which accommodates several approaches to typing URIs is proposed. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Ferro-magnetic {{materials}} are extremely {{important in the}} recording of data and are manufactured {{in the form of}} thin coatings on data carriers or in the <b>read-write</b> heads of <b>data</b> storage instruments. The high storage capacities required can only be achieved providing that the coating system has homogeneous magnetic properties and low internal stresses. These properties can be achieved by the correct control of the processing parameters. Barkhausen noise microscopy offers the possibility of characterising such thin coatings at high resolution in terms of their structure, thickness, internal stresses and heat treated condition. The monstrated with reference to Sendust-coatings and coating systems for MR-sensors...|$|R
40|$|The genome has {{traditionally}} been treated as a Read-Only Memory (ROM) subject to change by copying errors and accidents. In this review, I propose {{that we need to}} change that perspective and understand the genome as an intricately formatted <b>Read-Write</b> (RW) <b>data</b> storage system constantly subject to cellular modifications and inscriptions. Cells operate under changing conditions and are continually modifying themselves by genome inscriptions. These inscriptions occur over three distinct time-scales (cell reproduction, multicellular development and evolutionary change) and involve a variety of different processes at each time scale (forming nucleoprotein complexes, epigenetic formatting and changes in DNA sequence structure). Research dating back to the 1930 s has shown that genetic change is the result of cell-mediated processes, not simply accidents or damage to the DNA. This cell-active view of genome change applies to all scales of DNA sequence variation, from point mutations to large-scale genome rearrangements and whole genome duplications (WGDs). This conceptual change to active cell inscriptions controlling RW genome functions has profound implications for all areas of the life sciences...|$|R
40|$|Abstract: To the {{banknote}} sorting system, {{application of}} large-scale high-performance FPGA {{can help to}} conduct high speed capture and processing of banknote color images. In line with the detection algorithm, the real-time-captured image upon color-space conversion needs to be compared in the four color channels of LGAB with the template images so as {{to work out the}} checking results. With Xilinx soft IP core MPMC, two sets of multiport video interface controllers have been designed, each of which embraces four ways of video <b>data</b> <b>read-write</b> operation; the controller interface parameters can be configured via the DSP interface, with the process flow control based on the image frame synchronizing signal. Tests conducted show that the controllers with flexible configurations boast of reliable performance and now are reputed for proven application to large banknote sorting equipment...|$|R
40|$|Arduino ???????? National Instruments ??? ???????? ?????????????? ???????? ?????????? ? ?????? ????? ??? ?????????? ?? ???????? ????????? ?????????????? ????????? ??? ?????????. ? ?????? ????? ???????????? ????????? ????????????, ?? ???????? ?????????????? ?????? ??????????? ????????? ? ???????????? ??????????? ?? ??????????? ????????? ????? ? ??????? ??? ?????????? ???????????? ?????. ????????????? ???????? ???????? ????????? ???? ? ????? ? ???? ??????? Excel ?????????? ? ????????? ?????????????? ?? ?????? ????? ???????? ???????? ? ???? ???? ?????????? ????????????? ????????????? ??????????. ????????? ???????????????? ??????????? ??????????? ???????????? ??????????????? ????????. ???????? ???? ???? ??????????? ??? ?????????? ?????????? ????????????? ?????????? ?? ????????? ??????? ???? ?????????????? ? ????????? ???????? ?? ??????????? ????????? ?????. In {{this article}} is {{considered}} the option to use the electronic platform Arduino produced by National Instruments Company for construction universal device provided <b>data</b> <b>read-write</b> from analog and digital transducers simultaneously or by turns. In the article is proposed the software which permits synchronous operation of the electronic platform and a personal computer. It is provided data acquisition in a suitable form for further displaying. The proposed code carries out stream input information from transducers and writes it in the Excel file. This device is compatible with any computer software. The experimental researches confirm operability of the proposed device. The device {{can be used for}} temperature measurements with different type transducers and provided storing and processing of the collected data. ? ?????? ?????????? ??????? ????????????? ??????????? ????????? Arduino ???????? National Instruments ??? ?????????? ?????????????? ?????????? ?????????? ? ?????? ?????? ?? ?????????? ? ???????? ????????? ???????????????? ???????????? ???? ??????????. ? ?????? ????? ???????????? ??????????? ???????????, ??????????? ???????????????? ?????? ??????????? ????????? ? ???????????? ??????????? ? ?????????? ????????? ?????? ? ??????? ??? ??????????? ??????????? ?????. ???????????? ????????? ???????????? ????????? ???? ? ?????? ? ???? ??????? Excel ?????????? ? ????????? ???????????????? ? ?????? ?????? ?????????? ??????????? ? ????? ??????????? ???????????? ????????????? ??????????. ??????????? ????????????????? ???????????? ???????????? ????????????????? ????????????? ??????????. ?????????? ????? ???? ???????????? ??? ?????????? ????????????? ????????????? ????????? ??? ?????? ?????????? ???? ???????????????? ? ??????????? ?????????? ? ????????? ?????????? ??????...|$|R
40|$|We {{live in an}} era of {{ever-increasing}} {{abundance of}} data. To cope with the information overload we suffer from every single day, more sophisticated methods are required to access, manipulate, and analyze these humongous amounts of data. By embracing the heterogeneity, which is unavoidable at such a scale, and accepting the fact that the data quality and meaning are fuzzy, more adaptable, flexible, and extensible systems can be built. RESTful services combined with Semantic Web technologies could prove to be a viable path to achieve that. Their combination a 1 lows data integration on an unprecedented sca 1 e and solves some of the problems Web developers are continuously struggling with. This paper introduces a novel approach to create machine-readable descriptions for RESTful services as a first step towards this ambitious goal. It also shows how these descriptions along with analgorithm to translate SPARQL queries to HTTP requests can be used to integrate RESTful services into a global <b>read-write</b> Web of <b>Data...</b>|$|R
40|$|Many future {{applications}} for scalable shared-memory multiprocessors {{are likely to}} have large working sets that overflow secondary or tertiary caches. Two possible solutions to this problem are to add a very large cache called remote cache that caches remote data (NUMA-RC), or organize the machine as a cache-only memory architecture (COMA). This paper tries to determine which solution is best. To compare the performance of the two organizations for the same amount of total memory, we introduce a model of data sharing. The model uses three data sharing patterns: replication, read-mostly migration, and <b>read-write</b> migration. Replication <b>data</b> is accessed in read-mostly mode by several processors, while migration data is accessed largely by one processor at a time. For large working sets, the weight of the migration data largely determines whether COMA outperforms NUMA-RC. Ideally, COMA only needs to fit the replication data in its extra memory; the migration data will simply be s [...] ...|$|R
30|$|According to Solnit 2009 disasters, {{besides being}} devastating, can bring people closer that {{consequently}} creates an altruistic community where people {{help each other}} and fight the ongoing calamity together. Here, we take the example of Haiti earthquake to provide motivation for big data driven crisis response {{and the formation of}} a community powered by digital technology that helped the affected people of the earthquake. In 2010, Haiti faced a devastating earthquake that left a huge number of people dead and many homeless. Humanitarian aid was predominantly delivered in a different manner that time, which was mostly driven by digital technology. The concept of digital humanitarianism (Meier 2014) became popular after this incident. Digital humanitarians (the people who participate or volunteer to deploy technology for the humanitarian aid) used a number of emerging technologies—e.g., <b>read-write</b> web, big <b>data</b> analytics, participatory mapping, crowdsourced translation, social media, and mobile technology (we will discuss these in detail in the upcoming sections of this paper)—to catalyze an effective response after the Haiti disaster.|$|R
40|$|Patient safety {{initiatives}} {{throughout the}} anatomic laboratory and in biorepository laboratories have mandated increasing {{emphasis on the}} need for accurately identifying and tracking biospecimen assets throughout their production lifecycle and for archiving/retrieval purposes. However, increasing production volume along with complex workflow characteristics, reliance on manual production processes, and required asset movement to disparate destinations throughout asset lifecycles continue to challenge laboratory efforts. Radio Frequency Identification (RFID) technology, use of radio waves to communicate data between electronic tags attached to objects and a reader, shows significant potential to facilitate and overcome these hurdles. Advantages over traditional barcode labeling include readability without direct line-of-sight alignment to the reader, ability to read multiple tags simultaneously, higher data storage capacity, faster data transmission rate, and capacity to perform multiple <b>read-writes</b> of <b>data</b> to the tag. Most importantly, use of radio waves decreases the need to manually scan each asset, and at each step, identification or tracking event is needed. Temperature monitoring by on-board sensors and three-dimensional position tracking are additional potential benefits of using RFID technology. To date, barriers to implementation of RFID systems in the anatomic laboratory include increased associated costs of tags and readers, system software, data security concerns, lack of specific data standards for stored information, and potential for technological obsolescence during decades of specimen storage. Novel RFID production techniques and increased production capacity are projected to lower costs of some tags to a few cents each. Potentially, information security concerns can be addressed by techniques such as shielding, data encryption, and tag pseudonyms. Commitment by stakeholder groups to develop RFID tag data standards for anatomic pathology and biorepository laboratories could avoid or mitigate the "islands of data" dilemma presented by barcode usage where there are innumerable standards and a consequent paucity of hardware or software "plug and play" interoperability. Work remains to be done to establish the durability and appropriate shielding of individual tag types for use in harsh laboratory environmental conditions, and for long-term archival storage. Finally, given the requirements for long-term storage of biospecimen assets, consideration should be given to ways of mitigating data isolation due to eventual technological obsolescence of a particular RFID technology or software...|$|R
40|$|Microprocessor {{performance}} has been improving at roughly 60 % per year. Memory access times, however, have improved {{by less than}} 10 % per year. The resulting gap between logic and memory {{performance has}} forced microprocessor designs toward complex and power-hungry architectures that support out-of-order and speculative execution. Multi-cores have successfully delivered performance improvements over the past decade; however, they now face a problem: increased latency of <b>read-write</b> shared <b>data</b> communication. This problem is further exacerbated {{with the emergence of}} unstructured workloads that perform poorly in the traditional Invalidation-based cache-coherent protocols. ^ Improved microprocessor performance has also accelerated growth of mobile devices and tablets has propelled has led it to become the primary computing device. These devices predominately run Web applications that exhibit significantly different qualitative and quantitative characteristics than compared to conventional scientific workloads. These qualitative differences contribute to heavy instruction and data interference behavior in the unified last-level cache (ULLC). This observation combined with the limited capacity of the ULLC contributes to a significant performance bottleneck. ^ In the first part of my thesis, I focus on developing an adaptive run-time scheme to answer the four key questions of read-write sharing: 1) What data is being shared, 2) who is sharing the data, 3) when is the sharing occurs, and 4) how the data should be communicated. Although there exists several related papers that attempt to address this issue, many of them fall short of the desired objective because they only partially address the key concerns, or target very specific sharing patterns: making them unsuitable for unstructured workloads. I present a holistic approach that effectively targets these targets across a diverse set of scientific workloads scalable to 32 cores. ^ In the second part of my thesis, I present a dynamic cache-partitioning scheme aimed at improving the efficiency of the ULLC. I show how conventional techniques such as statically partitioning the cache as well as techniques applied in other domains perform poorly in the context of mobile platforms. I present the key insights specific to the cache inefficiency of mobile platforms that make previous techniques unsuitable, and how our dynamic scheme utilizes these observations to determine prioritizing between instructions and data references. ...|$|R
40|$|This {{research}} has three objectives. The {{first is to}} reveal the archetypes of hero in The Complete Grimm’s Fairy Tales, the second is to reveal the archetypes of hero’s journey in The Complete Grimm’s Fairy Tales {{and the third is}} to reveal the literary methods used to reveal those archetypes. Jung’s archetype is the basis of analysis employed in this research. This research used qualitative approach. Content analysis is used as the technique of analysis of the short stories. The subject of this research is five fairy tales entitled “The Story of the Youth Who Went Forth to Learn What Fear Was”, “The Devil with Three Golden Hairs”, “The Golden Goose”, “The Water of Life” and “The King’s Son Who Feared Nothing”; all of which are compiled in The Complete Grimm’s Fairy Tales written by The Grimm’s Brothers. The data were some sentences and utterances showing the archetypes of hero and hero’s journey. The researcher collected the <b>data</b> using <b>read-write</b> technique. The <b>data</b> analysis dealt with the process of data reducing, data displays and conclusion drawing. To obtain trustworthiness, the researcher used triangulation technique. There are three results of this research. First, there are four archetypes of hero in five fairy tales employed, namely innocent, single-parented, special environment of birth, and noble hearted hero. Second, there are mainly eleven stages among twelve stages of the archetypes of hero’s journey found, namely Ordinary World, Call of Adventure, Meeting with the Mentor, Crossing the First Threshold, Test, Allies and Enemies, Approach to the Inmost Cave, The Ordeal, The Reward, The Road Back, The Resurrection, and Return with the Elixir with the absence of one stage namely Refusal of The Call. Third, the archetypes of hero are mainly revealed by character revelation, mostly through actions and speeches, the archetypes of hero’s journey are presented through plot and setting of time and place...|$|R
40|$|This {{research}} aims (1) {{to identify}} the main female character’s problems in her family related to gender roles; (2) to analyze her struggles in gaining equal roles in her family, and (3) to reveal the significant meanings behind her struggles. This research is a descriptive-qualitative study. The subject {{of this research was}} Steinbeck’s The Grapes of Wrath. The primary data of this research were expressions in words, phrases, clauses, sentences, and paragraph related to gender discrimination and stereotypes in the novel. The supporting data were obtained from some sources, such as books, articles, and journals, related to gender discrimination, women’s stereotypes, and feminism. In collecting <b>data,</b> <b>read-write</b> technique was used. The data analysis was conducted using feminism approach. Trusthwortiness was used to achieve dependability, conformability, transferability, and credibility. The results of this research show three important points. First, there are two gender problems experienced by Ma Joad: gender discrimination and stereotypes. Related to gender discrimination, there are three forms of gender discrimination: being prohibited to help her husband in leading family, being prohibited to share her opinion in family discussion, and being prohibited to take men’s duties. There are also two forms of women’s stereotypes; women are emotional and women are fearful. Second, there are five efforts done by the main character to face gender discrimination and stereotypes; taking an opportunity from Pa’s inability to lead the family, re-considering her husband’s decisions, actively participating in public sphere, having logical reasons in her orders and decisions, and having bravery to challenge men. Third, the significant meanings being her efforts are that women can participate in public sector which proves that women are able to take men’s roles, and women cannot be regarded as subordinate to men...|$|R
40|$|The {{objectives}} of the research are to identify and explain the defense mechanisms experienced by Hedda based on Freud’s psychoanalysis theory, and to identify and explain the motives and types of psychological conflict experienced by Hedda based on the theory of psychological conflict promoted by Atkinson et al. This research is a qualitative content analysis study. The subject {{of this research is}} a play entitled Hedda Gabler by Henrik Ibsen. The data were some phrases, clauses, sentences, and expressions related to the description of defense mechanisms and psychological conflicts. The key instrument of this research was the researcher herself with the concept of defense mechanisms drawn from Freud’s psychoanalysis theory and psychological conflicts drawn from psychological conflict theories promoted by Atkinson et al., all illustrated in the analytical construct. The researcher collected the <b>data</b> using <b>read-write</b> technique. The <b>data</b> analysis was conducted through organizing, preparing the data, reading through all the data, coding the data, giving a description, interrelating description, and interpreting the meaning of the description. To obtain trustworthiness, the researcher used triangulation technique. There are three results of the research. The first result is the description of classified defense mechanisms experienced by Hedda i. e. repression, denial, displacement, projection, reaction formation, fixation regression, and fantasy. Such mechanisms are experienced by Hedda {{in order to deal with}} reality and release her anxiety; they are the response to her psychological conflicts. The second result of the research describes the four pairs of motives representing Hedda’s psychological conflicts namely independence versus dependence motive, intimacy versus isolation motive, cooperation versus competition motive, and impulse expression versus moral standards motive. Most of conflicts between motives fall into three categories i. e. approach – approach conflict, approach – avoidance conflict, and approach – avoidance conflict; this is the third result of the research. All these conflicts contribute to Hedda’s arbitrary behavior and to some forms of defense mechanisms experienced by her...|$|R
40|$|This {{research}} has two objectives. The {{first is to}} demonstrate the archetypes of hero’s journey in Paulo Coelho’s The Alchemist. The second is to uncover literary elements used to reveal those archetypes. The analysis is based on Christopher Vogler’s theory of mythic structure. This research applied the descriptive qualitative method. The content analysis was employed as the technique of the research. The data were some sentences and utterances relating to the archetypes of hero’s journey. The data were collected using <b>read-write</b> technique. The <b>data</b> were analysed relating to the stages of the archetypes of hero’s journey. The data analysis dealt {{with the process of}} data reducing, data displays and conclusion drawing. Triangulation technique was used to obtain trustworthiness. This research reveals two findings. First, based on Christopher Vogler’s theory on mythic structure, 12 stages of the archetypes of hero’s journey that signify the cycle of Separation-Initiation-Return are found in the novel. Those stages are (1) The Ordinary World, (2) The Call to Adventure, (3) Refusal of the Call, (4) Meeting with the Mentor, (5) Crossing the First Threshold, (6) Test-Allies-Enemies, (7) Approach to the Inmost Cave, (8) Supreme Ordeal, (9) Seizing the Reward, (10) The Road Back, (11) Resurrection, and (12) Return with the Elixir. These twelve stages are useful to identify the road-map of the hero’s journey. The whole story of The Alchemist gives more understanding to the archetypes of hero’s journey and vice versa. The stages of the archetypes of hero’s journey help to uncover what quest is being achieved by the hero. The most interesting finding is that the transformation and heroic quality are reflected in the stage of Supreme Ordeal. The transformation is the aim of the hero’s journey. It suggests that the hero changes from the state of innocence to the state of knowledge. Hero’s transformation reflects universal human realization of the essence of life. Second, the archetypes of hero’s journey are presented through the correlation among two narrative intrinsic elements which are plot and setting of time and place. In demonstrating each stage, plot shows the level of tension that reflects six phases: exposition, rising action, complication, climax, falling action, and resolution. Setting demonstrates the time order and the environment that signify the realm of the ordinary world and the special world...|$|R
40|$|The {{objectives}} of the research are to find some aspects of naturalism in Maggie: A Girl of the Streets, to prove and to give information to the readers about how far this novel fulfils {{the requirements of the}} naturalism genre. Structuralism approach is used to answer the problems of this research. This research is a descriptive-qualitative study. The subject of this research is a novel entitled Maggie: A Girl of the Streets written by Stephen Crane. The data were some phrases, clauses, sentences, and discourses related to the characteristics of naturalism genre. The key instrument of this research was the researcher herself with the concepts that there must be some reasons why this novel is categorized as naturalistic novel which were drawn from the theory by Donald Pizer as illustrated in the framework of thinking. The researcher collected the <b>data</b> using <b>read-write</b> technique. The <b>data</b> analysis was conducted through six steps: identifying the data, reading and rereading the whole data, coding and categorizing the data, sorting the data, making the interrelation between the description of the data and the theory, finally making an interpretation based on the researcher’s comprehension about the theory. To obtain trustworthiness, the researcher used triangulation technique. There are two results of the research. The first result is answering the question about the aspects of naturalism which is classified into four aspects; determinism, pessimism, detachment of the story (objectivity), and unpredictable ending. Determinism aspect is divided in two parts; the external and internal forces. The external force is presented by the family condition and the environment (circumstance) and the internal force is presented by passion and instinct. Furthermore, there are four parts which represent pessimism aspect of naturalism namely having lost of hopes; lost hopes for jobs, lost hopes for love, lost hopes for security, and lost hopes for a better future. Meanwhile there are two pairs classified as the data for detachment of the story (objectivity) namely telling the story {{as close as possible to}} reality and creating nameless characters. The unpredictable ending in the end of the story leads the readers in one direction at the beginning and in the middle ultimately drifting towards a completely unexpected course. The second result is answering the question about how far Maggie: A Girl of the Streets can fulfil the requirements of the naturalistic genre. It is answered by juxtaposing the theory from Donald Pizer with other theories of naturalism; Charles Child Walcutt and Emile Zola. Those theories are gathered and classified in order to show that Maggie: A Girl of the Streets fulfills the requirements of the naturalistic genre and it is proved to be a naturalistic novel...|$|R
