66|10000|Public
50|$|EIDORS is an {{open-source}} {{software tool}} box written mainly in MATLAB/GNU Octave designed primarily for image reconstruction from electrical impedance tomography(EIT) data, in a biomedical, industrial or geophysical setting. The name was originally {{an acronym for}} Electrical Impedance Tomography and Diffuse Optical Reconstruction Software. While the name reflects the original intention to cover image <b>reconstruction</b> <b>of</b> <b>data</b> from the mathematically similar near infra red diffuse optical imaging, to date {{there has been little}} development in that area.|$|E
50|$|Stage III is a {{reaction}} against excessive and uncontrolled expenditures {{of time and money}} spent on computer systems, and the major problem for management is the organization of tasks for control of computer operating costs. In this stage, project management and management report systems are organized, which leads to development of programming, documentation, and operation standards. During Stage III, a shift occurs from management of computers to management of data resources. This shift is an outcome of analysis of how to increase management control and planning in expending data processing operations. Also, the shift provides flexibility in data processing that is needed in a case of management’s new controls. The major characteristic of Stage III is <b>reconstruction</b> <b>of</b> <b>data</b> processing operation.|$|E
40|$|Declustered data {{organizations}} in disk arrays (RAIDs) achieve less-intrusive <b>reconstruction</b> <b>of</b> <b>data</b> after a disk failure. We present PDDL, a new data layout for declustered disk arrays. PDDL layouts exist {{for a large}} variety of disk array configurations with a distributed spare disk. PDDL declustered disk arrays have excellent run-time performance under light and heavy workloads. PDDL maximizes access parallelism in the most critical circumstances, namely during <b>reconstruction</b> <b>of</b> <b>data</b> on the spare disk. PDDL occurs minimum address translation overhead compared to all other proposed declustering layouts. ...|$|E
50|$|His current {{research}} interests include computational <b>reconstructions</b> <b>of</b> neuroanatomical <b>data,</b> algorithms for analysis <b>of</b> functional neuroscience <b>data,</b> and genome assembly.|$|R
5000|$|... #Caption: [...] Dynamic {{graphics}} {{are used}} to facilitate understanding of concepts in science, engineering, medicine, education, and business. Computer graphics facilitates the production of images that range in complexity from simple line drawings to three-dimensional <b>reconstructions</b> <b>of</b> <b>data.</b> The evolution <b>of</b> a phenomenon through time and its interactions with other elements can be shown through animation.|$|R
5000|$|It is {{possible}} to use texture mapping hardware to accelerate both the <b>reconstruction</b> <b>of</b> voxel <b>data</b> sets from tomographic scans, and to visualize the results ...|$|R
30|$|The 3 D <b>reconstruction</b> <b>of</b> <b>data</b> from {{temporal}} bone CT {{increases the likelihood}} of identifying traumatically injured structures of the {{temporal bone}} due to the ability to rotate in space and study incudo-malleolar and incudo-stapedial joints in different planes.|$|E
40|$|The Wireless Sensor Network is {{characterized}} by rapid information collection, information transmission with reliability and intelligent information processing. It is applied to monitor the large-scale agriculture production, and then the system will be more unified and yields more. It is effective to promote the productivity with efficient data aggregation, data reconstruction and reduce the overall data transmission and improve network lifetime. The proposed system analyzes the features and functions of wireless sensor network data aggregation and accurate <b>reconstruction</b> <b>of</b> <b>data,</b> based on Matching Pursuit Algorithm for repeat iterative approximation, <b>reconstruction</b> <b>of</b> <b>data</b> and Beam Forming Algorithm for signal processing technique used to control the directionality of the reception or transmission of data...|$|E
30|$|Confederation, cantons, and municipalities: Swiss Federal Statistical Office, Table “Dettes des administrations publiques” (je-f- 18.04. 01.xls) [URL] (accessed October 28, 2016). The <b>reconstruction</b> <b>of</b> <b>data</b> {{with new}} {{standards}} leads {{to having a}} quantitative leap in 1990. The resulting bias is, however, reduced {{by the fact that}} a leap of comparable scale also occurs in the GPD time series in 1990, for similar reasons.|$|E
30|$|We {{first show}} that using an optimal {{transport}} loss for NMF leads to better perceptual <b>reconstruction</b> <b>of</b> voice <b>data.</b> To that end, we evaluated the PEMO-Q score [13] of isolated test voices.|$|R
30|$|Detailed {{descriptions}} <b>of</b> {{the available}} <b>reconstruction</b> options for each protocol, {{taking into account}} the average heart rate and the variability during data acquisition, have been published in a considerable number of studies [11, 14, 31]. <b>Reconstructions</b> <b>of</b> <b>data</b> acquired in the retrospective spiral mode can be performed either using a fraction of the R-R interval or an absolute time-point prior to or following the R-wave of each cardiac cycle, the latter showing some advantages in case of variable heart rates and arrhythmia.|$|R
30|$|The <b>reconstructions</b> <b>of</b> the SPECT/CT <b>data</b> were {{performed}} using the methods described in section 2.2, with TEW scatter correction.|$|R
40|$|We present {{numerical}} simulations {{illustrating the}} <b>reconstruction</b> <b>of</b> <b>data</b> for electromagnetic inverse problems. Two situations are considered: prescribed electric field and measured magnetic {{field at the}} boundary of a medium with cylindrical symmetries, and {{the analysis of the}} far field pattern of a scattered field produced by the incidence of an incoming wave on a set of small inhomogeneities. © 2003 Elsevier Science B. V. All rights reserved. 1...|$|E
40|$|Reliable image {{registration}} {{is a key}} problem within the three-dimensional <b>reconstruction</b> <b>of</b> <b>data</b> obtained by two-dimensional image stacks. Here, a parallel implementation of the so-called elastic matching algorithm approach is presented. This algorithm {{is based on a}} fixpointtype iteration, where in each step a linear system of equations has to be solved. To compute the solution of the linear systems fast fourier techniques are used. The algorithm as well as some numerical examples are presented...|$|E
40|$|This {{bachelor}} thesis {{deals with}} the <b>reconstruction</b> <b>of</b> <b>data</b> from a damaged CD/DVD. The aim {{is to create an}} application which {{will be able to get}} a bitstream from an image (scanned fragments of a disc) acquired by an optical or electron microscope. This thesis also examines a data representation on the disc, process of preparation of specimens of CD and DVD and scanning of these specimens. The algorithm of the created software, its testing and results are described in this thesis as well...|$|E
3000|$|..., where E is {{expressed}} in keV. Before <b>reconstruction</b> <b>of</b> DRRs, CT <b>data</b> must be corrected by multiplication of a correction factor [...]...|$|R
40|$|International audienceWe {{consider}} the inverse problem of optical tomography in the radiative transport regime. We report numerical tests <b>of</b> a direct <b>reconstruction</b> method that {{is suitable for}} use with large <b>data</b> sets. <b>Reconstructions</b> <b>of</b> experimental <b>data</b> obtained from a noncontact optical tomography system are also reported...|$|R
50|$|Unlike the {{protocol}} analyzer, whose main characteristic {{is not the}} <b>reconstruction</b> <b>of</b> the <b>data</b> carried by {{the protocol}}s, Xplico was born expressly with the aim to reconstruct the protocol's application data and {{it is able to}} recognize the protocols with a technique named Port Independent Protocol Identification (PIPI).|$|R
40|$|Pulmonary {{sequestration}} is {{an uncommon}} disease with non-functioning pulmonary tissue and anomalous systemic blood supply. The diagnosis depends on identification of abnormal systemic vessels. Arteriography, CT, MR and Doppler ultrasound can {{been used for}} diagnosis. MDCT is fast, allows high-resolution volumetric imaging that can be obtained during a single breath- hold and single-phase contrast injection. Moreover, volumetric helical imaging allows three- dimensional <b>reconstruction</b> <b>of</b> <b>data,</b> which is useful in the demonstration and characterization of the lesions and also showing vascular structures. Herein, a case of pulmonary sequestration diagnosed by MDCT is reported...|$|E
40|$|Analysis and {{reconstruction}} of range images usually focuses on complex objects completely {{contained in the}} field of view; little attention has been devoted so far to the reconstruction of partially occluded simple-shaped wide areas like parts of a wall hidden behind furniture pieces in an indoor range image. The work in this paper is aimed at such reconstruction. First of all the range image is partitioned and surfaces are fitted to these partitions. A further step locates possibly occluded areas, while a final step determines which areas are actually occluded. The <b>reconstruction</b> <b>of</b> <b>data</b> occurs in this last step. ...|$|E
40|$|The current {{generation}} of UNIX farms at Fermilab are rapidly approaching {{the end of their}} useful life. The workstations were purchased during the years 1991 - 1992 and represented the most cost-effective computing available at that time. Acquisition of new workstations is being made to upgrade the UNIX farms for the purpose of providing large amounts of computing for <b>reconstruction</b> <b>of</b> <b>data</b> being collected at the 1996 - 1997 fixed-target run, as well as to provide simulation computing for CMS, the Auger project, accelerator calculations and other projects that require massive amounts of CPU. 4 refs., 1 fig., 2 tabs...|$|E
40|$|Bachelor {{thesis is}} dealing with {{recognition}} of effectiveness of the sealing wall of earth dam Karolinka by electrical impedance spectrometry method. Sealing wall was installed {{to the body of}} the dam in 2013 during a complex <b>reconstruction.</b> <b>Data</b> <b>of</b> monitoring during 2012, i. e. before the reconstruction, are compared with the data measured in 2015, i. e. after the <b>reconstruction.</b> <b>Data</b> <b>of</b> past month from 2015 are added...|$|R
40|$|The {{functional}} Magnetic Resonance Imaging (fMRI) {{has provided}} us with an approach of revealing the activity of brain. Due to the large amount <b>of</b> <b>data</b> in fMRI studies, feature selection techniques are used to select particular features for classifier. In this project, Spectral Clustering is implemented to construct features to achieve best <b>reconstruction</b> <b>of</b> the <b>data</b> and be most efficient for making predictions...|$|R
30|$|It was {{demonstrated}} that STIR image <b>reconstruction</b> <b>of</b> PET mMR <b>data</b> is possible {{paving the way}} to more advanced models included in the pipeline of 4 D PET reconstruction.|$|R
40|$|The LHCb Silicon Tracker is a silicon micro-strip {{detector}} {{covering a}} sensitive area of 12 m 2 {{with a total}} of 272 k readout channels. The installation of the detector is complete and commissioning is making excellent progress. The detector has recorded first beam-induced events during LHC synchronization tests in August 2008 and in June 2009. These events have allowed the performance to be studied, and adjustments to the operational parameters to be made. In this contribution, we will draw first lessons from the in-situ commissioning of the Silicon Tracker, and present results from the <b>reconstruction</b> <b>of</b> <b>data</b> collected during the LHC synchronization tests...|$|E
40|$|Instant {{messenger}} {{programs such}} as ICQ are often used by hackers and criminals for illicit purposes and consequently the log files from such programs are of interest in a forensic investigation. This paper outlines research {{that has resulted in}} the development of a tool for the extraction of ICQ log file entries. Detailed <b>reconstruction</b> <b>of</b> <b>data</b> from log files was achieved with a number of different ICQ software. There are several limitations with the current design including timestamp information not adjusted for the time zone, data could be altered, and conversations must be manually reconstructed. Future research will aim to address these and other limitations as pointed out in this paper. </p...|$|E
40|$|Eventdisplay is a {{software}} {{package for the}} analysis and <b>reconstruction</b> <b>of</b> <b>data</b> and Monte Carlo events from ground-based gamma-ray observatories such as VERITAS and CTA. It was originally developed as a display tool for data from the VERITAS prototype telescope, but evolved into a full analysis package with routines for calibration, FADC trace integration, image and stereo parameter analysis, response function calculation, and high-level analysis steps. Eventdisplay makes use of an image parameter analysis combined with gamma-hadron separation methods based on multivariate algorithms. An overview of the reconstruction methods and some selected results are presented in this contribution. Comment: 8 pages, 2 figures, in Proceedings of the 35 th International Cosmic Ray Conference (ICRC 2017), Busan (South Korea...|$|E
40|$|In {{this paper}} odd-order B-spline filters are {{proposed}} to reconstruct volumetric data sampled on an optimal Body-Centered Cubic (BCC) grid. To make these filters nearly interpolating, we adapt a previously published framework, {{which is based}} on a discrete frequency-domain prefiltering. It is shown that a BCC-sampled B-spline kernel is not invertible, therefore the interpolation constraint cannot be satisfied by a discrete prefiltering. To remedy this problem, we use a slightly modified discrete Bspline for prefiltering, which is proven to be invertible. Although this modification leads to an approximation, the proposed prefiltered B-spline <b>reconstruction</b> <b>of</b> BCC-sampled <b>data</b> still provides much higher image quality than the interpolating prefiltered B-spline <b>reconstruction</b> <b>of</b> volume <b>data</b> sampled on an equivalent Cartesian Cubic (CC) grid. Furthermore, our method directly supports an efficient implementation on a conventional graphics hardware, unlike the previous reconstruction methods developed for the BCC grid...|$|R
40|$|We present {{progress}} toward using scanned OSSE observations for mapping and sky survey work. To this end, {{we have developed}} a technique for detecting pointlike sources of unknown number and location, given that they appear in a background which is relatively featureless or which can be modeled. The technique, based on the newly developed concept and mean field annealing, is described, with sample <b>reconstructions</b> <b>of</b> <b>data</b> from the OSSE Virgo Survey. The results demonstrate the capability of reconstructing source information without any a priori information about the number and/or location of pointlike sources in the field-of-view. Comment: 15 pages, 4 Postscript figs, AASTeX, uses psfig and epsf. To appear in the Astrophysical Journal, vol. 48...|$|R
40|$|PURPOSE: Demonstrate a novel fast {{method for}} <b>reconstruction</b> <b>of</b> multi-dimensional MR Fingerprinting (MRF) data using Deep Learning methods. METHODS: A neural network (NN) is defined using the TensorFlow {{framework}} and trained on simulated MRF data computed using the Bloch equations. The accuracy <b>of</b> the NN <b>reconstruction</b> <b>of</b> noisy <b>data</b> {{is compared to}} conventional MRF template matching as a function <b>of</b> training <b>data</b> size, and quantified in a both simulated numerical brain phantom data and acquired data from the ISMRM/NIST phantom. The utility of the method is demonstrated in a healthy subject in vivo at 1. 5 T. RESULTS: Network training required 10 minutes and once trained, data reconstruction required approximately 10 ms. <b>Reconstruction</b> <b>of</b> simulated brain <b>data</b> using the NN resulted in a root-mean-square error (RMSE) of 3. 5 ms for T 1 and 7. 8 ms for T 2. The RMSE for the NN trained on sparse dictionaries was approximately 6 fold lower for T 1 and 2 fold lower for T 2 than conventional MRF dot-product dictionary matching on the same dictionaries. Phantom measurements yielded good agreement (R 2 = 0. 99) between the T 1 and T 2 estimated by the NN and reference values from the ISMRM/NIST phantom. CONCLUSION: <b>Reconstruction</b> <b>of</b> MRF <b>data</b> with a NN is accurate, 300 fold faster and more robust to noise and undersampling than conventional MRF dictionary matching. Comment: 21 pages, 7 figure...|$|R
40|$|This book {{highlights}} important {{techniques for}} cellular imaging and covers the basics and applications of electron tomography and related techniques. In addition, it considers practical aspects and broadens the technological focus by incorporating techniques {{that are only}} now becoming accessible (e. g. block face imaging).   The {{first part of the}} book describes the electron microscopy 3 D technique available to scientists around the world, allowing them to characterize organelles, cells and tissues. The major emphasis is on new technologies like scanning transmission electron microscopy (STEM) tomography, though the book also reviews some of the more proven technologies like electron tomography. In turn, the second part is dedicated to the <b>reconstruction</b> <b>of</b> <b>data</b> sets, signal improvement and interpretation...|$|E
40|$|Abstract- This paper {{describes}} LabVIEW simulink for {{serial communication}} and ECG signal reconstruction. In fact, various programming languages {{are available to}} meet the requirements of serial communication and <b>reconstruction</b> <b>of</b> <b>data.</b> However, LabVIEW, altering from habitual programming languages, is interested in this paper. LabVIEW is a programming environment in which programs can be easily created using graphical icons. The aim {{of this paper is to}} support readers with a fundamental understanding of serial communication and reconstruction of ECG signal in LabVIEW. After reconstructing of ECG signal using LabVIEW simulink, it is intended to transmit and receive this ECG signal by using wireless module such as ZigBee and RF module. However, reconstruction of ECG signal is only discussed in this paper...|$|E
40|$|Image {{compression}} {{is useful}} {{because it helps}} to reduce the consumption of expensive resources, such as hard disk space or transmission bandwidth. Many compression techniques are in place {{but there is a}} scope for high compression with good reconstruction of original image. The problem of <b>reconstruction</b> <b>of</b> <b>data</b> from sub signals from each filter channel is removed by lifting scheme because this structure itself assures reversibility. In this paper we are introducing lifting scheme with SPIHT coder for image compression and the results are obtained for compression ratio, PSNR and RMSE. A study in lossless image compression using the lifting scheme is presented. We first suggest why lossless image compression is an important issue and the general idea behind lifting...|$|E
30|$|We have {{implemented}} a protocol for tracking the rigid {{motion of the}} head of a fully conscious rat during a PET scan and performing a motion compensated list-mode <b>reconstruction</b> <b>of</b> the <b>data.</b> Using this technique we have conducted eight rat studies to investigate the effect of isoflurane on the uptake of 18 F-FDG in the brain, by comparing conscious and unconscious scans.|$|R
40|$|FIGURE 10. Volume <b>reconstruction</b> <b>of</b> HRXCT <b>data</b> showing skull <b>of</b> Typhlonectes compressicauda (MW 5820). Views as in Fig. 2. Scale bar = 1 mm. Scan parameters: a {{molybdenum}} {{target set}} at 100 kV and 155 μA; scan {{data were collected}} at 2. 8 frames per second over 3142 projections in 360 ˚; reconstructed voxel size of 12 μm. Abbreviations as in Appendix...|$|R
40|$|Piecewise {{continuous}} <b>reconstruction</b> <b>of</b> real-valued <b>data</b> can be {{formulated in}} terms of non-convex optimisation problems. Both stochastic and deterministic algorithms have been devised to solve them. The simplest such re-construction process is the "weak string". Exact solutions can be obtained for it, and are {{used to determine the}} success or failure of the algorithms under precisely con-trolled conditions. It is concluded that the deterministic algorithm (Graduated Non-Convexity) outstrips stochas-tic (Simulated Annealing) algorithms both in computa-tional efficiency and in problem-solving power. Piecewise continuous <b>reconstruction</b> <b>of</b> real-valued <b>data</b> can be for-mulated {{in terms of}} non-convex optimisation problems. Both stochastic and deterministic algorithms have been devised to solve them. The simplest such reconstruction process is the "weak string". Exact solutions can be ob-tained for it, and are used to determine {{the success or failure of}} the algorithms under precisely controlled con-ditions. It is concluded that the deterministic algorithm (Graduated Non-Convexity) outstrips stochastic (Simu-lated Annealing) algorithms both in computational effi-ciency and in problem-solving power. ...|$|R
