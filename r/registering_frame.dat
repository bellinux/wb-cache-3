0|132|Public
40|$|PosterInternational audienceThis paper {{presents}} {{a study of}} a method for estimating the position and orientation of a photo-shoot in indoor environments for augmented reality applications. Our proposed localization method is based on <b>registered</b> <b>frame</b> data of virtualized reality models, which are photos with known photo-shoot positions and orientations, and depth data. Because <b>registered</b> <b>frame</b> data are secondary product of modeling process, additional works are not necessary to create <b>registered</b> <b>frame</b> data especially for the localization. In the method, a photo taken by a mobile camera is compared to <b>registered</b> <b>frame</b> data for the localization. Since <b>registered</b> <b>frame</b> data are linked with photo-shoot position, orientation, and depth data, 3 D coordinates of each pixel on the photo of <b>registered</b> <b>frame</b> data is available. We conducted experiments with employing five techniques of the estimation for comparative evaluations...|$|R
40|$|A {{method for}} {{computing}} the 3 D camera motion #the ego-motion# in a static scene is introduced, {{which is based}} on computing the 2 D image motion of a single image region directly from image intensities. The computed image motion of this image region is used to register the images so that the detected image region appears stationary. The resulting displacement #eld for the entire scene between the <b>registered</b> <b>frames</b> is affected only by the 3 D translation of the camera. After canceling the e#ects of the camera rotation by using such 2 D image registration, the 3 D camera translation is computed by #nding the focus-of-expansion in the translation-only set of <b>registered</b> <b>frames.</b> This step is followed by computing the camera rotation to complete the computation of the ego-motion...|$|R
50|$|The {{painting}} {{is composed of}} three vertical panels arranged symmetrically, with the two outer sections framing the central one. The middle panel is divided horizontally into two rectangular <b>registers</b> <b>framed</b> by six parallel lines of different colors. The painting's symmetry facilitates {{the reconstruction of the}} damaged part on the left panel.|$|R
40|$|A new {{approach}} is proposed to correct geometric distortion and reduce space and time-variant blur in videos that suffer from atmospheric turbulence. We first <b>register</b> the <b>frames</b> to suppress geometric deformation using a B-spline based non-rigid registration method. Next, a fusion process {{is carried out}} to produce an image from the <b>registered</b> <b>frames,</b> which {{can be viewed as}} being convolved with a space invariant near-diffraction-limited blur. Finally, a blind deconvolution algorithm is implemented to deblur the fused image. Experiments using real data illustrate that this approach is capable of alleviating blur and geometric deformation caused by turbulence, recovering details of the scene and significantly improving visual quality. 1...|$|R
40|$|This paper {{proposes a}} video-based super-resolution {{algorithm}} by high-frequency component compensation. Normalized logarithm distance (NLD) minimization on local shape feature is proposed for image registration. Lost high-frequency information are estimated using the <b>registered</b> <b>frames.</b> By compensating the high-frequency component, the high-resolution images are recovered. The algorithm has lower computational cost than the alternatives. Experimental evaluation verified {{the usefulness of}} the algorithm...|$|R
40|$|A {{method for}} {{computing}} the 3 D camera motion (the ego-motion) in a static scene is introduced, {{which is based}} on computing the 2 D image motion of a single image region directly from image intensities. The computed image motion of this image region is used to register the images so that the detected image region appears stationary. The resulting displacement field for the entire scene between the <b>registered</b> <b>frames</b> is affected only by the 3 D translation of the camera. After canceling the effects of the camera rotation by using such 2 D image registration, the 3 D camera translation is computed by finding the focus-of-expansion in the translation-only set of <b>registered</b> <b>frames.</b> This step is followed by computing the camera rotation to complete the computation of the ego-motion. The presented method avoids the inherent problems in the computation of optical flow and of feature matching, and does not assume any prior feature detection or feature correspondence. 1 Introduction The motion obser [...] ...|$|R
40|$|A robust {{method is}} {{introduced}} for computing the camera motion (the ego-motion) in a static scene. The method {{is based on}} detecting a single planar surface in the scene directly from image intensities, and computing its 2 D motion in the image plane. The detected 2 D motion of the planar surface is used to register the images, so that the planar surface appears stationary. The resulting displacement field for the entire scene in such <b>registered</b> <b>frames</b> is affected only by the 3 D translation of the camera, which is computed by finding the focus-of-expansion in the <b>registered</b> <b>frames.</b> This step is followed by computing the 3 D rotation to complete the computation of the ego-motion. This 3 D motion computation {{is based on a}} motion computation scheme which handles the difficult case when multiple image motions are present. This multiple motion analysis is performed together with object segmentation by using a temporal integration approach...|$|R
40|$|A robust {{method is}} {{introduced}} for computing the camera motion (the ego-motion) in a static scene. The method {{is based on}} detecting a single image region and computing its 2 D motion in the image plane directly from image intensities. The detected 2 D motion of this image region is used to register the images (so that the detected image region appears stationary). The resulting displacement field for the entire scene in such <b>registered</b> <b>frames</b> is affected only by the 3 D translation of the camera. Canceling {{the effects of the}} 3 D rotation of the camera by using such 2 D image registration simplifies the computation of the translation. The 3 D translation of the camera is computed by finding the focus-of-expansion in the translation-only set of <b>registered</b> <b>frames.</b> This step is followed by computing the 3 D rotation to complete the computation of the ego-motion. The presented method avoids the inherent problems in the computation of optical flow and of feature matching, and does not assume any pri [...] ...|$|R
40|$|Abstract—To correct {{geometric}} distortion {{and reduce}} space and time-varying blur, {{a new approach}} is proposed in this paper capable of restoring a single high-quality image from a given image sequence distorted by atmospheric turbulence. This approach reduces the space and time-varying deblurring problem to a shift invariant one. It first <b>registers</b> each <b>frame</b> to suppress geometric deformation through B-spline based non-rigid registration. Next, a temporal regression process is carried out to produce an image from the <b>registered</b> <b>frames,</b> which {{can be viewed as}} being convolved with a space invariant near-diffraction-limited blur. Finally, a blind deconvolution algorithm is implemented to deblur the fused image, generating a final output. Experiments using real data illustrate that this approach can effectively alleviate blur and distortions, recover details of the scene and significantly improve visual quality. Index Terms—Image restoration, atmospheric turbulence, non-rigid image registration, point spread function, sharpness metric. ...|$|R
40|$|Narrative {{painting}} through allegorical figuration For {{the initial}} stages of my master’s project I embarked on a series of works that were a tentative step towards understanding the notion of ‘narrative’ and how this might be embraced within figurative representation. My project employed narratives that were derived from personal recollections of childhood events. These events were recreated through an emotional <b>register,</b> <b>framed</b> by a fictional scenario. I staged scenes with models as a reference point to paint from. This involved the use of costumes and props that were devised to enhance the ‘allegorical’ implications of the narrative...|$|R
40|$|An online {{process is}} {{proposed}} for video registration of dynamic scenes, such as scenes with dynamic textures or with moving objects. This process has three steps: (i) A few frames {{are assumed to}} be already registered. (ii) Using the <b>registered</b> <b>frames,</b> the next new frame is extrapolated. (iii) The actual new <b>frame</b> is <b>registered</b> to the extrapolated frame. Video extrapolation overcomes the bias introduced by dynamics in the scene, even when the dynamic regions cover almost the entire image. It can also overcome not only motion, but also many fluctuations in intensity. The traditional “brightness constancy ” is now replaced with “dynamics constancy”. ...|$|R
40|$|AbstractImprovement of the {{signal-to-noise}} ratio of <b>registered</b> <b>frames</b> {{is necessary in}} scientific, technical and amateur tasks. Dark and light spatial noise portraits of camera's photosensor {{can be used for}} frames quality increase. Earlier method of improvement of the {{signal-to-noise ratio}} of shots by use of light spatial noise portraits of camera's photosensor was proposed. In this paper method of improvement of the signal-to-noise ratio of shots by use of both light and dark spatial noise portraits was analyzed. According to the results of numerical experiments, {{it was found that the}} signal-to-noise ratio can be increased up to 50 ÷ 60 times compared to that one of a single image...|$|R
40|$|In this {{research}} work, an accurate and fast moving object detector that can detect all the moving objects from Unmanned Aerial Platform (UAV) is proposed. Because {{of the distance}} of the UAV to the objects and the movement of the platform, object detection is a challenging task. In order to achieve best results with low error, at first the camera motion has to be estimated so, by using the Rosten and Drummond technique the corners is detected and then by using the corners the camera motion is compensated. After motion compensation, by subtracting the <b>registered</b> <b>frame</b> from the reference frame all the moving objects are detected and extracted...|$|R
30|$|Applications that {{we believe}} are well suited to the joint use of SR and {{compression}} include airborne and satellite imaging [19 – 22]. In these applications, the dominant inter-frame motion {{is the result of}} camera platform motion. Thus, the motion can be well modeled with a global motion model. This allows for accurate sub-pixel motion estimation for super-resolution. There is also a strong need for compression in these applications, in order to store and transmit the acquired data through band-limited channels. We have observed that a video that is well suited to multiframe SR is also likely a good candidate for difference-frame compression. In this case, the correlation between <b>registered</b> <b>frames</b> is exploited for compression, and the high-frequency differences are exploited for SR.|$|R
40|$|Optical {{coherence}} tomography (OCT) is {{an important}} interferometric diagnostic technique which provides cross-sectional views of the subsurface microstructure of biological tissues. However, the imaging quality of high-speed OCT is limited due to the large speckle noise. To address this problem, this paper proposes a multi-frame algorithmic method to denoise OCT volume. Mathematically, we build an optimization model which forces the temporally <b>registered</b> <b>frames</b> to be low rank, and the gradient in each frame to be sparse, under logarithmic image formation and noise variance constraints. Besides, a convex optimization algorithm based on the augmented Lagrangian method is derived to solve the above model. The results reveal that our approach outperforms the other methods {{in terms of both}} speckle noise suppression and crucial detail preservation...|$|R
5000|$|The {{end result}} is a {{connection}} that is working but performs extremely poorly because of the duplex mismatch. Symptoms of a duplex mismatch are connections that seem to work fine with a ping command, but [...] "lock up" [...] easily with very low throughput on data transfers; the effective data transfer rate {{is likely to be}} asymmetrical, performing much worse in the half-duplex to full-duplex direction than the other. In normal half-duplex operations late collisions do not occur. However, in a duplex mismatch the collisions seen on the half-duplex side of the link are often late collisions. The full-duplex side usually will <b>register</b> <b>frame</b> check sequence errors, or runt frames. Viewing these standard Ethernet statistics can help diagnose the problem.|$|R
40|$|A {{new global}} motion {{estimation}} technique for sprite coding systems is presented. The proposed system can accurately <b>register</b> <b>frames</b> to a sprite without referencing to the sprite. This allows the motion estimation process {{to be performed}} in an environment independent {{of the quality of}} the sprite. The frame having the highest resolution of the scene is determined and all other frames can be projected on to the space of this chosen frame such that information loss due to decimation can be avoided. The static sprite is strategically updated, according to the major camera motion during decoding, to alleviate the problem of error propagation from the sprite image. Experimental results indicate that the proposed technique is very accurate and robust, which makes it suitable for MPEG- 4 sprite coding. Department of Electronic and Information Engineerin...|$|R
40|$|Abstract: This paper {{describes}} a methodology for obtaining a high resolution dense point cloud using Kinect (J. Smisek and Pajdla, 2011) and HD cameras. Kinect produces a VGA resolution photograph and a noisy point cloud. But high resolution {{images of the}} same scene can easily be obtained using additional HD cameras. We combine the information to generate a high resolution dense point cloud. First, we do a joint calibration of Kinect and the HD cameras using traditional epipolar geometry (R. Hartley, 2004). Then we use the sparse point cloud obtained from Kinect and the high resolution information from the HD cameras to produce a dense point cloud in a <b>registered</b> <b>frame</b> using graph cut optimization. Experimental results show that this approach can significantly enhance {{the resolution of the}} Kinect point cloud. ...|$|R
40|$|Abstract. An online {{approach}} is proposed for Video registration of dynamic scenes, such as scenes with dynamic textures, moving objects, motion parallax, etc. This approach has three steps: (i) Assume {{that a few}} <b>frames</b> are already <b>registered.</b> (ii) Using the <b>registered</b> <b>frames,</b> the next frame is predicted. (iii) A new video <b>frame</b> is <b>registered</b> to the predicted frame. Frame prediction overcomes the bias introduced by dynamics in the scene, even when dynamic objects cover {{the majority of the}} image. It can also overcome many systematic changes in intensity, and the “brightness constancy ” is replaced with “dynamic constancy”. This predictive online approach can also be used with motion parallax, where non uniform image motion is caused by camera translation in a 3 D scene with large depth variations. In this case a method to compute the camera ego motion is described. ...|$|R
40|$|Abstract. We {{propose a}} dynamic texture feature-based {{algorithm}} for registering two video sequences of a rigid or nonrigid scene taken from two synchronous or asynchronous cameras. We model each video sequence as {{the output of}} a linear dynamical system, and transform the task of <b>registering</b> <b>frames</b> of the two sequences to that of registering {{the parameters of the}} corresponding models. This allows us to perform registration using the more classical image-based features as opposed to space-time features, such as space-time volumes or feature trajectories. As the model parameters are not uniquely defined, we propose a generic method to resolve these ambiguities by jointly identifying the parameters from multiple video sequences. We finally test our algorithm {{on a wide variety of}} challenging video sequences and show that it matches the performance of significantly more computationally expensive existing methods. ...|$|R
50|$|The status {{register}} indicates {{the status of}} the IRQ, DSR and DCD lines, transmitter and receiver data <b>Registers,</b> and overrun, <b>framing</b> and parity error conditions.|$|R
40|$|A {{new global}} motion {{estimation}} technique for sprite coding {{is presented in}} this paper. The proposed system manages to accurately <b>register</b> <b>frames</b> to a sprite without referencing the sprite. This allows motion estimation process to be performed in an environment free from the adverse influence of the sprite which is usually blurred by the erroneous motion estimation. Furthermore, our proposed system can figure out the frame having the highest resolution, base frame, in the video sequence and project all other frames to the space of this base frame. This can avoid the loss of information due to decimation. Moreover, {{the determination of the}} size of the sprite can be conducted before construction of the sprite which avoid clumsy process of manual sprite memory allocation. Experimental results show that our proposed technique manages to estimate global motion in high accuracy and indicate a good robustness against estimation error which is suitable for the sprite coding application in MPEG- 4. 1...|$|R
40|$|This work {{presents}} {{a novel approach}} for robust PCA with total variation regularization for foreground-background separation and denoising on noisy, moving camera video. Our proposed algorithm registers the raw (possibly corrupted) frames of a video and then jointly processes the <b>registered</b> <b>frames</b> to produce a decomposition of the scene into a low-rank background component that captures the static components of the scene, a smooth foreground component that captures the dynamic components of the scene, and a sparse component that can isolate corruptions and other non-idealities. Unlike existing methods, our proposed algorithm produces a panoramic low-rank component that spans the entire field of view, automatically stitching together corrupted data from partially overlapping scenes. The low-rank portion of our robust PCA model {{is based on a}} recently discovered optimal low-rank matrix estimator (OptShrink) that requires no parameter tuning. We demonstrate the performance of our algorithm on both static and moving camera videos corrupted by noise and outliers...|$|R
40|$|Separating {{the direct}} and global {{components}} of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding {{the direct and}} global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain sta-tionary during the image acquisition process. In this pa-per, we develop a motion compensation method that relaxes this condition and allows direct-global separation to be per-formed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is be-ing able to <b>register</b> <b>frames</b> in a video sequence {{to each other in}} the presence of time varying, high frequency active illu-mination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present re-sults on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax. 1...|$|R
50|$|Example 1:Within a {{subroutine}} {{a programmer}} will mainly {{be interested in}} the parameters and the local variables, which will rarely exceed 64 KB, for which one base <b>register</b> (the <b>frame</b> pointer) suffices. If this routine is a class method in an object-oriented language, then a second base register is needed which points at the attributes for the current object (this or self in some high level languages).|$|R
50|$|Paper {{prints were}} an early {{mechanism}} {{to establish the}} copyright of motion pictures by depositing them with the Library of Congress. Thomas Alva Edison’s company was first to <b>register</b> each <b>frame</b> of movie film onto a positive paper print, in 1893. The Library of Congress processed and cataloged each of the films as one photograph, accepting thousands of paper prints of films over a twenty-year period.|$|R
50|$|An embedded-application binary {{interface}} (EABI) specifies standard conventions for file formats, data types, <b>register</b> usage, stack <b>frame</b> organization, {{and function}} parameter passing of an embedded software program, {{for use with}} an embedded operating system.|$|R
40|$|A {{method for}} {{computing}} the 3 D camera motion (the egomotion) in a static scene is introduced, {{which is based}} on initially computing the 2 D image motion of an image region. The computed dominant 2 D parametric motion between two frames is used to register the images so that the corresponding image region appears perfectly aligned between the two <b>registered</b> <b>frames.</b> Such 2 D parametric registration removes all e ects of camera rotation, even for the misaligned image regions. The resulting residual parallax displacement eld between the two region-aligned images is an epipolar eld centered at the FOE (Focus-of-Expansion). The 3 D camera translation is recovered from the epipolar eld. The 3 D camera rotation is recovered from the computed 3 D translation and the detected 2 D parametric motion. The decomposition of image motion into a 2 D parametric motion and residual epipolar parallax displacements avoids many of the inherent ambiguities and instabilities associated with decomposing the image motion into its rotational and translational components, and hence robusti es ego-motion or 3 D structure estimation. I...|$|R
40|$|Videos {{captured}} with hand-held cameras often {{suffer from}} {{a significant amount of}} blur, mainly caused by the inevitable natural tremor of the photographer's hand. In this work, we present an algorithm that removes blur due to camera shake by combining information in the Fourier domain from nearby frames in a video. The dynamic nature of typical videos with the presence of multiple moving objects and occlusions makes this problem of camera shake removal extremely challenging, in particular when low complexity is needed. Given an input video frame, we first create a consistent registered version of temporally adjacent frames. Then, the set of consistently <b>registered</b> <b>frames</b> is block-wise fused in the Fourier domain with weights depending on the Fourier spectrum magnitude. The method is motivated from the physiological fact that camera shake blur has a random nature and therefore, nearby video frames are generally blurred differently. Experiments with numerous videos recorded in the wild, along with extensive comparisons, show that the proposed algorithm achieves state-of-the-art results {{while at the same time}} being much faster than its competitors...|$|R
40|$|An {{accurate}} and computationally very fast multimodal human detector is presented. This 1 D+ 2 D detector fuses 1 D range scan and 2 D image information via an effective geometric descriptor and a silhouette based visual representation within a radial basis function kernel {{support vector machine}} learning framework. Unlike the existing approaches, the proposed 1 D+ 2 D detector does not make any restrictive assumptions on the range scan positions, thus it is applicable {{to a wide range}} of real-life detection tasks. To analyze the discriminative power of the geometric descriptor, a range scan only version, 1 D+, is also evaluated. Extensive experiments demonstrate that the 1 D+ 2 D detector works robustly under challenging imaging conditions and achieves several orders of magnitude performance improvement while reducing the computational load drastically. In addition, a new multi-modal (LIDAR, depth image, optical image) dataset, DontHitMe, is introduced. This dataset contains 40, 000 <b>registered</b> <b>frames</b> and 3, 600 manually annotated human objects. It depicts challenging illumination conditions in indoors and outdoors environments and is publicly available to our community...|$|R
40|$|In {{this work}} {{we aim to}} predict the driver's focus of attention. The goal is to {{estimate}} what a person would pay attention to while driving, and which part of the scene around the vehicle is more critical for the task. To this end we propose a new computer vision model based on a multi-branch deep architecture that integrates three sources of information: raw video, motion and scene semantics. We also introduce DR(eye) VE, the largest dataset of driving scenes for which eye-tracking annotations are available. This dataset features more than 500, 000 <b>registered</b> <b>frames,</b> matching ego-centric views (from glasses worn by drivers) and car-centric views (from roof-mounted camera), further enriched by other sensors measurements. Results highlight that several attention patterns are shared across drivers and can be reproduced to some extent. The indication of which elements in the scene are likely to capture the driver's attention may benefit several applications {{in the context of}} human-vehicle interaction and driver attention analysis. Comment: Submitted to IEEE Transactions on PAM...|$|R
30|$|It {{should be}} noted that SR is most {{beneficial}} for significantly undersampled imaging systems where aliasing is present. For such an imaging system, the individual LR observed frames may not compress well because of high spatial frequency content. However, since a set of frames suitable for SR must overlap in the field of view, these frames are also likely to exhibit inter-frame correlation. Thus, we also consider compression of <b>registered</b> difference <b>frames,</b> as described in the following sub-section.|$|R
40|$|Video {{inpainting}} is {{a process}} of repairing the damaged areas of a video or removing any desired part of a video. Dealing with such problems requires a sturdy image inpainting algorithm along with a regeneration technique for filling the missing parts of a video sequence recorded from a static camera. Many automatic techniques for video inpainting are available but most of them are computationally intensive and fail to repair the damaged areas. Also, from the remaining video inpainting algorithms a masked video sequence has to be provided. To overcome this problem, inpainting process is carried out by using the background registration method which is proposed in this paper. The video is first converted into distinct image frames and the first <b>frame</b> is <b>registered.</b> Second, the edges of an object to be removed are detected by comparing the <b>registered</b> <b>frame</b> with each succeeding frame of the video. Next, a masked frame is generated for each time frame. Then the inpainting process is performed separately for each and every time frame of the images. Next, these processed image frames are displayed sequentially, so that it appears as a video...|$|R
40|$|Over {{the next}} five years, the U. S. Census Bureau will {{evaluate}} three potential enhancements to its business register. These new enhancements will improve the overall coverage, quality and usefulness of the business register for internal users in the Economic Programs Directorate at the U. S. Census Bureau, external data users, and for data providers by limiting the amount of respondent burden. The three enhancements are described as follows: o First, the Census Bureau {{is working with the}} Bureau of Labor Statistics to craft an agreement to exchange information to supplement each agency’s decentralized <b>register</b> <b>frame.</b> These two agencies conducted a formal Register Comparison Project and, as an outgrowth of this project, the Census Bureau would like to obtain from BLS information about smaller multi-unit companies. Availability of this information will provide continuous breakouts of multi-unit companies; reduce annual mailings for the Report of Organization Survey, which keeps the register up-to-date; and refine geographic level data for products generated from the register. o Second, the Census Bureau would like to internally convert to a common identification scheme for current survey collections and tie that common ID to the business register. Because o...|$|R
40|$|Figure 1 : Two misregistered frames due to user {{motion in}} a projector-based AR {{experiment}} {{are shown in}} (a) and (c). Our approach observes the augmented imagery and corrects any visible misregistration. The corresponding <b>registered</b> <b>frames</b> are shown in (b) and (d). The typical registration process in augmented reality (AR) consists of three independent consecutive stages: static calibration, dynamic tracking, and graphics overlay. The {{result is that the}} real-virtual registration is “open loop”—inaccurate calibration or tracking leads to misregistration that is seen by the users but not the system. To cope with this, we propose a general approach to “close the loop” in the displayed appearance by using the visual feedback of registration for pose tracking to achieve accurate registration. Specifically, a model-based method is introduced to simultaneously track and augment real objects in a closed-loop fashion, where the model is comprised of the combination of the real object to be tracked and the virtual object to be rendered. This method is applicable to paradigms including video-based AR, projector-based AR, and diminished reality. Both qualitative and quantitative experiments are presented to demonstrate the feasibility and effectiveness of our approach...|$|R
40|$|This work {{concerns}} {{quality improvement}} of auto-fluorescence retinal images by averaging of non-rigidly reg-istered images. The necessity {{of using the}} elastic spatial transformation model is documented {{as well as the}} need for similarity criterion capable of dealing with the non-homogenous and variable illumination of retinal images. The presented multilevel registration algorithm provides parameters of primarily affine and then B-spline free-form spatial transformation optimal with respect to the mutual information similarity criterion. The registration was tested on three modeled image sets of 100 images. The difference of artificially introduced pre-deformation displacement field and the displacement field found by our algorithm clearly showed the ability to compensate for the diverse modeled distortions. Further, the registration algorithm was used for improving quality of realistic retinal images using averaging of <b>registered</b> <b>frames</b> of image sequences. The whole method was verified by processing of 16 time series of real images. The gain in signal to noise ratio in the averaged registered images with respect to individual frame reach the expected about 4 dB, without introducing a visible blur. The final im-age was substantially less blurred than the non-registered averaged image, which is documented by comparison of the autocorrelation functions of both images. 1...|$|R
