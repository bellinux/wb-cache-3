23|317|Public
50|$|Reviews of {{the service}} have been mixed, with some early {{responses}} complimentary about the ability to access and search the large data sets. However, there have been complaints of the excessive cost and the general policy of the British Library allowing a private company {{the rights to the}} newspapers. One writer noted that: The BNA demonstrates what happens to our cultural heritage when there is no political will for public investment. The nineteenth-century newspaper press was one of the period’s greatest achievements but, rather than celebrate it, opening it up and giving it back to the nation, the British Library have been forced to sell it off. The search Interface has also been criticised for problems in identifying where the searched terms are on the <b>retrieved</b> <b>pages,</b> and in the unreliability of the web interface, with bugs preventing images loading and regular crashes.|$|E
40|$|International audienceThrough data {{collection}} tools, the Pagerank, inlink {{and the total}} number of <b>retrieved</b> <b>pages</b> of Agridata are computed. By considering website traffic as the flag of network influence levels, the paper analyze correlations between website traffic and Pagerank, inlink and the number of <b>retrieved</b> <b>pages.</b> Finally, the feasubily to regard the web link analysis as a factor to measure network influence of agricultural websites is discussed...|$|E
40|$|The World Wide Web is a {{rich source}} of {{information}} and continues to expand in size and complexity. To capture the features of the Web at a higher level to realise the information classification and efficient retrieval on the Web is becoming a challenge task. One natural way is to exploit the linkage information among the Web pages. Previous work such as HITS in this area is based on a set of <b>retrieved</b> <b>pages</b> to get a Web community that is a bunch of pages related to the query topics. Since the set of <b>retrieved</b> <b>pages</b> may contain many unrelated pages (noise pages) to the query topics, the obtained Web community sometimes is unsatisfactory. In this paper, we propose an innovative algorithm to eliminate noise pages from the set of <b>retrieved</b> <b>pages</b> and improve its quality. This improvement will enable existing community construction algorithms to construct good quality Web page communities. The proposed algorithm reveals and takes advantage of the relationships among concerned Web pages at a deeper level. The numerical experiment results show the effectiveness and feasibility of the algorithm. This algorithm could also be used solely to filter unnecessary Web pages and reduce the management cost and burden of Web-based data management systems. The ideas in the algorithm can also be applied to other hyperlink analysis...|$|E
50|$|The {{client-side}} {{content is}} generated on the client's computer. The web browser <b>retrieves</b> a <b>page</b> from the server, then processes the code {{embedded in the}} page (typically written in JavaScript) and displays the <b>retrieved</b> <b>page's</b> content to the user.|$|R
50|$|Internet Channel uses an {{internet}} connection (set in the Wii Settings) to <b>retrieve</b> <b>pages</b> {{directly from a}} web site's HTTP or HTTPS server, not through a network of proxy servers as in Opera Mini products. Internet Channel is capable of rendering most web sites {{in the same manner}} as its desktop counterpart by using Opera's Medium Screen Rendering technology.|$|R
40|$|This paper {{proposes a}} method for <b>retrieving</b> Web <b>pages</b> {{according}} to objects described in them. To achieve that goal, ontologies extracted from HTML tables are used as queries. The system <b>retrieves</b> Web <b>pages</b> containing the type of objects described by a given ontology. We propose a simple and efficient algorithm for this task and show its performance on real-world Web sites. ...|$|R
40|$|In the Web search process {{people often}} {{think that the}} hardest work {{is done by the}} search engines or by the {{directories}} which are entrusted with finding the Web pages. While this is partially true, a not less important part of the work is done by the user, who has to decide which page is relevant from the huge set of <b>retrieved</b> <b>pages.</b> In this paper we present a graphical visualisation tool aimed at helping users to determine the relevance of a Web page with respect to its structure. Such tool can help the user in the often tedious task of deciding which page is relevant enough to deserve a visit...|$|E
40|$|Despite of the {{popularity}} of global search engines, people still suffer from low accuracy of site search. The primary reason lies in the difference of link structures and data scale between global Web and website, which leads' to failures of traditional re-ranking methods' such as HITS, PageRank and DirectHit. This' paper proposes a novel re-ranking method based on user logs within websites. With the help of website taxonomy, we mine for generalized association rules and abstract access patterns of different levels'. Mining results' are subsequently used to re-rank the <b>retrieved</b> <b>pages.</b> One {{of the advantages of}} our mining algorithm is that it resolves the diversity problem of user's' access behavior and discovers' general patterns. Experiment shows that the proposed method outperforms keyword-based method by 15 % and DirectHit by 13 % respectively...|$|E
40|$|Ankara : The Department of Computer Engineering and the Institute of Engineering and Science of Bilkent University, 2007. Thesis (Master's) [...] Bilkent University, 2007. Includes bibliographical {{references}} leaves 93 - 97. Many {{search engines}} use a two-step process to retrieve from the web pages {{related to a}} user’s query. In the first step, traditional text processing is performed to find all pages matching the given query terms. Due to the massive size of the web, this step can result in thousands of <b>retrieved</b> <b>pages.</b> In the second step, many search engines sort the list of <b>retrieved</b> <b>pages</b> according to some ranking criterion to make it manageable for the user. One popular way to create this ranking is to exploit additional information inherent in the web due to its hyperlink structure. One successful and well publicized link-based ranking system is PageRank, the ranking system used by the Google search engine. The dynamically changing matrices reflecting the hyperlink structure of the web and used by Google in ranking pages are not only very large, {{but they are also}} sparse, reducible, stochastic matrices with some zero rows. Ranking pages amounts to solving for the steady-state vectors of linear combinations of these matrices with appropriately chosen rank- 1 matrices. The most suitable method of choice for this task appears to be the power method. Certain improvements have been obtained using techniques such as quadratic extrapolation and iterative aggregation. In this thesis, we propose iterative methods based on various block partitionings, including those with triangular diagonal blocks obtained using cutsets, for the computation of the steady-state vector of such stochastic matrices. The proposed iterative methods together with power and quadratically extrapolated power methods are coded into a software tool. Experimental results on benchmark matrices show {{that it is possible to}} recommend Gauss-Seidel for easier web problems and block Gauss-Seidel with partitionings based on a block upper triangular form in the remaining problems, although it takes about twice as much memory as quadratically extrapolated power method. Noyan, Gökçe NilM. S...|$|E
5000|$|HtmlUnit [...] - [...] {{headless}} browser {{that can}} be used for <b>retrieving</b> web <b>pages,</b> web scraping, and more.|$|R
40|$|Abstract: The {{studies in}} the {{literature}} show that about 40 % of the current Internet traffic and bandwidth consumption is due to web crawlers that <b>retrieve</b> <b>pages</b> for indexing by the different search engines. This traffic and bandwidth consumption will increase in future due to the exponential growth of the web. This paper addresses the problem of bandwidth consumption by introducing an efficient indexing system based on mobile crawlers. The proposed system employs mobile agents to crawl the pages. These mobile agent based crawlers <b>retrieve</b> the <b>pages,</b> process them, compare their data to filter out pages that are not modified after last crawl, and then compress them before sending them to the search engine for indexing. The experimental results of the proposed system are very encouraging...|$|R
40|$|We {{estimate}} that approximately 40 % of current Internet traffic {{is due to}} Web crawlers <b>retrieving</b> <b>pages</b> for indexing. We address this problem by introducing an efficient indexing system based on active networks. Our approach employs strategically placed active routers that constantly monitor passing Internet traffic, analyze it, and then transmit the index data to a dedicated back-end repository. Our simulations have shown that active indexing is up to 30 % more efficient than the current crawler-based techniques. I...|$|R
40|$|Abstract. Trust is a {{necessary}} concept to realize the Semantic Web. But how can we build a “Web of Trust”? We first argue that a small “Web of Trust ” for each community is very essential to realize a huge “Web of Trust. ” Then, we focus on an academic community as a microcosm of a “Web of Trust ” and show a web mining approach to generate a social network automatically. Each edge is added using the number of <b>retrieved</b> <b>pages</b> by a search engine which includes both persons ’ names. Moreover, each edge is given a label, such as “coauthors” or “members of the same project, ” by applying classification rules to the page content. The relation of persons such as “coauthor” or “same laboratory ” can be described by an RDF format. Finally, using the social network, we calculate authoritativeness of a node as a social trust and an individual trust. ...|$|E
40|$|Conventional wisdom {{holds that}} queries to {{information}} retrieval systems will yield more relevant results if they contain multiple topic-related terms and use Boolean and phrase operators to enhance interpretation. AlthoughstudieshaveshownthattheusersofWeb-based searchenginestypicallyentershort,term-basedqueries and rarely use search operators, little information exists concerning {{the effects of}} term and operator usage on the relevancy of search results. In this study, search engine users formulated queries on eight search topics. Each query was submitted to the user-specified search engine, and relevancy ratings for the <b>retrieved</b> <b>pages</b> were assigned. Expert-formulated queries were also submittedandprovidedabasisforcomparingrelevancy ratings across search engines. Data analysis based on our research model of the term and operator factors affecting relevancy was then conducted. The results showthatthedifferenceinthenumberoftermsbetween expert and nonexpert searches, the percentage of matching terms between those searches, and the erroneous use of nonsupported operators in nonexpert searchesexplainmostofthevariationintherelevancyof search results. These findings highlight the need for designing search engine interfaces that provide greater support {{in the areas of}} term selection and operator usage...|$|E
40|$|In despite of the {{popularity}} of current search engines, people still suffer search failure and lots of non-relevant results when finding some specific information from a specific website. This is because the site search performance is not satisfying as the whole Web search. This paper analyzes the specialty of site search compared with traditional Web search, and the non-applicability of link-based re-ranking techniques such as HITS and PageRank. In this paper, we propose to use log mining to improve the site search performance. With the help of website taxonomy, a generalized association rule mining technique is applied to users ’ log to abstract the user’s access patterns at different levels, and the mining results are then applied to re-ranking the <b>retrieved</b> <b>pages.</b> Our mining algorithm tackles the diversity problem of user’s access behavior and mines out general patterns. The experimental results show that our proposed method outperforms keyword-based method by 15 % and DirectHit by 13 % respectively...|$|E
40|$|Knowing {{how easily}} pages within a website can be {{retrieved}} using the site’s search functionality provides crucial {{information to the}} site designer. If {{the system is not}} <b>retrieving</b> particular <b>pages</b> then the system or information may need to be changed to ensure that visitors to the site have the best chance of finding the relevant information. In this demo paper, we present a Page Retrievability Calculator, which estimates the retrievability of a page for a given search engine. To estimate the retrievability, instead of posing all possible queries, we focus on issuing only those likely to <b>retrieve</b> the <b>page</b> and use them to obtain an accurate approximation. We can also rank the queries associated with the page to show the site designer what queries are most likely to <b>retrieve</b> the <b>pages</b> and at what rank. With this application we can now explore how {{it might be possible to}} improve the site or content to improve the retrievability...|$|R
5000|$|Sizinkiler - Çatlak Yumurtalar Wikipedia <b>Page.</b> <b>Retrieved</b> 14 November 2015.|$|R
5000|$|Archive.is, {{an archive}} site which stores {{snapshots}} of web <b>pages.</b> It <b>retrieves</b> one <b>page</b> at a time, but unlike WebCite, it includes Web 2.0 {{sites such as}} Google Maps and Twitter.|$|R
40|$|Searching online {{provides}} {{you with}} a wealth of information, {{but not all of}} it will be useful or of the highest quality. Search engines are distributed programs that dive into the World Wide Web to find relevant information for a given search query. Their fundamental components are: the crawlers, the indexer module, the collection analysis module, the query engine, and the ranking module. Many of today’s search engines use a traditional text process to retrieve pages related to a user’s query. Traditional text processing is done to find all documents using the query terms, or related to the query terms by semantic meaning. With the massive size of the web, this result in thousands of <b>retrieved</b> <b>pages</b> may or may not related to the query. The main function of the ranking module is to sort the search results by relevance or importance using information retrieval (IR) algorithms. There were two kinds of methods in information retrieval, based o...|$|E
40|$|The {{exponential}} {{growth of the}} Internet makes the universal search engines try to address the scalability limitations with vast amounts of hardware and network resources and by distributing the crawling process across users, queries, or even client computers. Furthermore it makes increasingly difficult to discover topic relevant {{information that can be}} used in specialized portals and on-line search. To tackle this issue the focused web crawlers are emerging. They are a kind of crawlers that dynamically browse the Internet by choosing the most promising links in order to try to maximize the relevancy of the <b>retrieved</b> <b>pages</b> and thus saving significantly bandwidth and time. In this master thesis, an algorithm survey is done to find the best alternatives in the literature. With the best options (Graphic Context, Best-First and History Path) and two new approaches (proposed in this paper) an automatic and manual analysis is performed with two different document topic classifier algorithms: SVM and String-Matching. It will be shown that any of the alternatives in the literature outperforms the Best-First algorithm and just the tw...|$|E
40|$|When {{searching the}} web, {{it is often}} {{possible}} {{that there are too}} many results available for ambiguous queries. Text snippets, extracted from the <b>retrieved</b> <b>pages,</b> are an indicator of the pages' usefulness to the query intention and can be used to focus the scope of search results. In this paper, we propose a novel method for automatically extracting web page snippets that are highly relevant to the query intention and expressive of the pages' entire content. We show that the usage of semantics, as a basis for focused retrieval, produces high quality text snippet suggestions. The snippets delivered by our method are significantly better in terms of retrieval performance compared to those derived using the pages' statistical content. Furthermore, our study suggests that semantically-driven snippet generation {{can also be used to}} augment traditional passage retrieval algorithms based on word overlap or statistical weights, since they typically differ in coverage and produce different results. User clicks on the query relevant snippets can be used to refine the query results and promote the most comprehensive among the relevant documents. Comment: In proceedings of SIGIR 2007 Workshop on Focused Retrieval. 8 page...|$|E
2500|$|... Dublin: The James Joyce Center (2006). [...] Archived <b>page</b> <b>retrieved</b> 2013-05-24.|$|R
50|$|UGA Institute of Higher Education faculty <b>page</b> <b>Retrieved</b> May 18, 2006.|$|R
5000|$|The Beat Club's Official MySpace <b>page.</b> <b>Retrieved</b> on August 8, 2007.|$|R
40|$|The study {{examined}} the possibility of constructing business profiles (specifically, product profiles) based on keyword patterns on various types of Websites including the company 2 ̆ 7 s own Website, blog sites, and Websites that have particular keywords and also have hyperlinks pointing the company 2 ̆ 7 s Websites. To test the proposed methods, we selected China 2 ̆ 7 s four major oil companies and two other companies that have related products. We collected data about these companies from these three Web sources and analyzed the numbers of <b>retrieved</b> <b>pages</b> to construct business profiles. The business profiles constructed were checked against business information collected from other sources such as company annual reports and company newsletters to determine the correctness of the profiles and thus {{the usefulness of the}} proposed methods. We found that the method of analyzing frequency distribution of product keywords on company 2 ̆ 7 s own Website worked very well. The method of searching for blogs with a company name and product keywords was useful in knowing recent changes and developments of the company. The method of searching for inlinks with keywords provided some information but the overall result is not as good...|$|E
40|$|We {{present and}} {{evaluate}} an implemented system {{with which to}} rapidly and easily build intelli-gent software agents for Web-based tasks. Our design is centered around two basic functions: ScoreThisLink and ScoreThisPage. If given highly accurate such functions, standard heuristic search would lead to ecient retrieval of useful information. Our approach allows users to tailor our system’s behavior by providing approximate advice about the above functions. This advice is mapped into neural network implementations of the two functions. Subsequent reinforcements from the Web (e. g., dead links) and any ratings of <b>retrieved</b> <b>pages</b> that the user wishes to provide are, respectively, used to rene the link- and page-scoring functions. Hence, our architecture pro-vides an appealing middle ground between non-adaptive agent programming languages and sys-tems that solely learn user preferences from the user’s ratings of pages. We describe our internal representation of Web pages, the major predicates in our advice language, how advice is mapped into neural networks, and the mechanisms for rening advice based on subsequent feedback. We also present a case study where we provide some simple advice and specialize our general-purpose system into a -page nder". An empirical study demonstrates that our approach leads to a more eective home-page nder {{than that of a}} leading commercial Web search site...|$|E
40|$|A crawler is {{a program}} that {{retrieves}} and stores pages from the Web, commonly for a Web search engine. A crawler often has to download {{hundreds of millions of}} pages {{in a short period of}} time and has to constantly monitor and refresh the downloaded pages. Once the crawler has downloaded a significant number of pages, it has to start revisiting the downloaded pages in order to refresh the downloaded collection. Due to resource constraints, search engines usually have difficulties keeping the entire local repository synchronized with the web. Given the size of web today and inherent resource constraints: re-crawling too frequently leads to wasted bandwidth, re-crawling too infrequently brings down the quality of the search engine. In this paper a hybrid approach is build on the basis of which a web crawler maintains the <b>retrieved</b> <b>pages</b> “fresh” in the local collection. Towards this goal the concept of Page rank and Age of a web page is used. As higher page rank means that more number of users are visiting that very web page and that page has higher link popularity. Age of web page is a measure that indicates how outdated the local copy is. Using these two parameters a hybrid approach is proposed that can identify important pages at the early stage of a crawl, and the crawler re-visit these important pages with higher priority. </p...|$|E
2500|$|Ward, Vicki[...] Vanity Fair (January 2004). (11 <b>pages.).</b> <b>Retrieved</b> February 6, 2007.|$|R
5000|$|EURAC - European Air Chiefs' Conference Official Web <b>Page,</b> <b>Retrieved</b> on 2013-02-08 ...|$|R
5000|$|Portsmouth Naval and Defence Heritage. <b>Page</b> <b>retrieved</b> at 11.30am 29 July 2005.|$|R
40|$|Effective {{representation}} of Web search results {{remains an open}} problem in the Information Retrieval community. For ambiguous queries, a traditional approach is to organize search results into groups (clusters), one for each meaning of the query. These groups are usually constructed according to the topical similarity of the retrieved documents, {{but it is possible}} for documents to be totally dissimilar and still correspond to the same meaning of the query. To overcome this problem, we exploit the thematic locality of the Web— relevant Web pages are often located close {{to each other in the}} Web graph of hyperlinks. We estimate the level of relevance between each pair of <b>retrieved</b> <b>pages</b> by the length of a path between them. The path is constructed using multi-agent beam search: each agent starts with one Web page and attempts to meet as many other agents as possible with some bounded resources. We test the system on two types of queries: ambiguous English words and people names. The Web appears to be tightly connected; about 70 % of the agents meet with each other after only three iterations of exhaustive breadth-first search. However, when heuristics are applied, the search becomes more focused and the obtained results are substantially more accurate. Combined with a content-driven Web page clustering technique, our heuristic search system significantly improves the clustering results. ...|$|E
40|$|Accurate topical {{classification}} of user queries allows for increased effectiveness and efficiency in general-purpose Web search systems. Such classification becomes critical {{if the system}} must route queries to a subset of topic-specific and resource-constrained back-end databases. Successful query classification poses a challenging problem, as Web queries are short, thus providing few features. This feature sparseness, coupled with the constantly changing distribution and vocabulary of queries, hinders traditional text classification. We attack this problem by combining multiple classifiers, including exact lookup and partial matching in databases of manually classified frequent queries, linear models trained by supervised learning, and a novel approach based on mining selectional preferences from a large unlabeled query log. Our approach classifies queries without using external sources of information, such as online Web directories or the contents of <b>retrieved</b> <b>pages,</b> making it viable for use in demanding operational environments, such as large-scale Web search services. We evaluate our approach using a large sample of queries from an operational Web search engine and show that our combined method increases recall by nearly 40 % over the best single method while maintaining adequate precision. Additionally, we compare our results to those from the 2005 KDD Cup and find that we perform competitively despite our operational restrictions. This suggests {{it is possible to}} topically classify {{a significant portion of the}} query stream without requiring external sources of information, allowing for deployment in operationally restricted environments...|$|E
40|$|This paper {{proposes a}} set of {{measures}} to evaluate search engine functionality over time. When coming to evaluate the performance of Web search engines, the evaluation criteria used in traditional information retrieval systems (precision, recall, etc.) are not sufficient. Web search engines operate in a highly dynamic, distributed environment, therefore it becomes necessary to assess search engine performance not just at a single point in time, but over a whole period. The size of a search engine's database is limited, {{and even if it}} grows, it grows more slowly than the Web. Thus the search engine has to decide whether and to what extent to include new pages in place of pages that were previously listed in the database. The optimal solution is that all new pages are listed, and no old ones are removed- but this of course is usually unachievable. The proposed metrics that evaluate search engine functionality in presence of dynamic changes include the percentage of newly added pages, and the percentage of the removed pages, which still exist on the Web. The percentage of non-existent pages (404 errors, nonexistent server, etc.) out of the set of <b>retrieved</b> <b>pages</b> indicates the timeliness of the search engine. The ideas in this paper elaborate on some of the measures introduced in a recently published paper (Bar-Ilan, 2002). I'd like {{to take advantage of the}} opportunity to discuss the problem of search engine evaluation in dynamic environments with the participants of the Web Dynamics Workshop...|$|E
25|$|This page draws {{heavily on}} the French Wikipedia <b>page</b> , <b>retrieved</b> 12 April 2005.|$|R
5000|$|Fareham Borough Council page on Fort Nelson. <b>Page</b> <b>retrieved</b> at 12.20pm 29 July 2005.|$|R
2500|$|Linda Gramatky Smith, Hardie Gramatky's Story, gramatky.com (Ken and Linda Gramatky Smith. 20 <b>pages.</b> <b>Retrieved</b> 2007-03-17.|$|R
