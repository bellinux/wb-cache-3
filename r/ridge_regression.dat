1336|52|Public
25|$|The finite-dimensional {{case was}} expounded by Arthur E. Hoerl, {{who took a}} {{statistical}} approach, and by Manus Foster, who interpreted this method as a Wiener–Kolmogorov (Kriging) filter. Following Hoerl, it is known in the statistical literature as <b>ridge</b> <b>regression.</b>|$|E
25|$|In penalized regression, 'L1 penalty' and 'L2 penalty' {{refer to}} penalizing either the L1 norm of a solution's vector of {{parameter}} values (i.e. {{the sum of}} its absolute values), or its L2 norm (its Euclidean length). Techniques which use an L1 penalty, like LASSO, encourage solutions where many parameters are zero. Techniques which use an L2 penalty, like <b>ridge</b> <b>regression,</b> encourage solutions where most parameter values are small. Elastic net regularization uses a penalty term that is a combination of the L1 norm and the L2 norm of the parameter vector.|$|E
25|$|In {{some cases}} the (weighted) normal {{equations}} matrix XTX is ill-conditioned. When fitting polynomials the normal equations matrix is a Vandermonde matrix. Vandermonde matrices become increasingly ill-conditioned as {{the order of the}} matrix increases. In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate. Various regularization techniques can be applied in such cases, the most common of which is called <b>ridge</b> <b>regression.</b> If further information about the parameters is known, for example, a range of possible values of , then various techniques can be used to increase the stability of the solution. For example, see constrained least squares.|$|E
40|$|This paper {{addresses}} a model order reduction technique based on <b>Ridge’s</b> <b>regression</b> for power amplifier (PA) behavioral models {{to be used}} in digital predistortion (DPD) linearization applications. Commonly, the DPD parameters’ extraction is performed by means of a least squares (LS) <b>regression.</b> With <b>Ridge’s</b> <b>regression,</b> the coefficients of the DPD are extracted defining a weighted cost function aimed at minimizing not only the mean square error, but also including a regularization term based on the square of the Euclidean norm of the coefficients’ vector. Taking advantage of this regularization and following a given criterium explained in this paper, it is possible to select the most significant basis functions of the DPD model and thus, not only improving the ovedetermined matrix problem, but also reducing the model’s order and consequently the computational complexity of the DPD linearizer. Peer ReviewedPostprint (published version...|$|R
40|$|This paper {{considers}} Bayesian regression {{with normal}} and double exponential priors as forecasting methods based on large panels of time series. We show that, empirically, these forecasts are {{highly correlated with}} principal component forecasts and that they perform equally well {{for a wide range}} of prior choices. Moreover, we study the asymptotic properties of the Bayesian regression under Gaussian prior under the assumption that data are quasi collinear to establish a criterion for setting parameters in a large cross-section. Bayesian VAR; large cross-sections; Lasso regression; principal components; <b>ridge</b> <b>regressions...</b>|$|R
25|$|Hoerl AE, 1962, Application of <b>ridge</b> {{analysis}} to <b>regression</b> problems, Chemical Engineering Progress, 1958, 54–59.|$|R
25|$|<b>Ridge</b> <b>regression,</b> {{and other}} forms of penalized {{estimation}} such as Lasso regression, deliberately introduce bias into the estimation of β {{in order to reduce the}} variability of the estimate. The resulting estimators generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.|$|E
2500|$|One of {{the prime}} {{differences}} between Lasso and <b>ridge</b> <b>regression</b> is that in <b>ridge</b> <b>regression,</b> as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause {{more and more of}} the parameters to be driven to zero. This is an advantage of Lasso over <b>ridge</b> <b>regression,</b> as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas <b>Ridge</b> <b>regression</b> never fully discards any features. Some feature selection techniques are developed based on the LASSO including Bolasso which bootstraps samples, [...] and FeaLect which analyzes the regression coefficients corresponding to different values of [...] to score all the features.|$|E
2500|$|Tikhonov regularization, {{named for}} Andrey Tikhonov, {{is the most}} {{commonly}} used method of regularization of ill-posed problems. [...] In statistics, the method is known as <b>ridge</b> <b>regression,</b> in machine learning it is known as weight decay, and with multiple independent discoveries, it is also variously known as the Tikhonov–Miller method, the Phillips–Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg–Marquardt algorithm for non-linear least-squares problems.|$|E
40|$|In {{this paper}} we {{evaluate}} exchange rate predictability {{using a new}} framework developed by Giacomini and White (2004). In this new framework we test for conditional predictive ability rather than for unconditional predictive ability, {{which has been the}} usual approach thus far. Using several shrinkage based forecasting methods, including new methods proposed here, we evaluate conditional predictability of five bilateral exchange rates at differing horizons. Our results indicate that for most currencies a random walk would not be the best forecasting method in a real time forecasting exercise, at least for some predictive horizons. We also show that our proposed shrinkage methods in general perform on par with Bayesian shrinkage and <b>ridge</b> <b>regressions,</b> and sometimes they even perform better. ...|$|R
40|$|Entity {{disambiguation}} is {{an important}} step in many information retrieval applications. This paper proposes new research for entity disambiguation with the focus of name disambiguation in digital libraries. In particular, pairwise similarity is first learned for publications that share the same author name string (ANS) and then a novel Hierarchical Agglomerative Clustering approach with Adaptive Stopping Criterion (HACASC) is proposed to adaptively cluster a set of publications that share a same ANS to individual clusters of publications with different author identities. The HACASC approach utilizes a mixture of kernel <b>ridge</b> <b>regressions</b> to intelligently determine the threshold in clustering. This obtains more appropriate clustering granularity than non-adaptive stopping criterion. We conduct a large scale empirical study with a dataset of more than 2 million publication record pairs to demonstrate the advantage of the proposed HACASC approach...|$|R
40|$|It {{is known}} that the common factors in a large panel of data can be {{consistently}} estimated by the method of principal components, and principal components can be constructed by iterative least squares regressions. Replacing least squares with <b>ridge</b> <b>regressions</b> {{turns out to have}} the effect of shrinking the singular values of the common component and possibly reducing its rank. The method is used in the machine learning literature to recover low-rank matrices. We study the procedure from the perspective of estimating a minimum-rank approximate factor model. We show that the constrained factor estimates are biased but can be more efficient in terms of mean-squared errors. Rank consideration suggests a data-dependent penalty for selecting the number of factors. The new criterion is more conservative in cases when the nominal number of factors is inflated by the presence of weak factors or large measurement noise. The framework is extended to incorporate a priori linear constraints on the loadings. We provide asymptotic results {{that can be used to}} test economic hypotheses...|$|R
2500|$|In some {{contexts}} a regularized {{version of}} the least squares solution may be preferable. Tikhonov regularization (or <b>ridge</b> <b>regression)</b> adds a constraint that , the L2-norm of the parameter vector, is not greater than a given value. Equivalently, it may solve an unconstrained minimization of the least-squares penalty with [...] added, where [...] is a constant (this is the Lagrangian form of the constrained problem). [...] In a Bayesian context, this is equivalent to placing a zero-mean normally distributed prior on the parameter vector.|$|E
2500|$|Linear {{regression}} models are often fitted using the least squares approach, {{but they may}} also be fitted in other ways, such as by minimizing the [...] "lack of fit" [...] in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares loss function as in <b>ridge</b> <b>regression</b> (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms [...] "least squares" [...] and [...] "linear model" [...] are closely linked, they are not synonymous.|$|E
2500|$|Linearity. [...] This {{means that}} {{the mean of the}} {{response}} variable is a linear combination of the parameters (regression coefficients) and the predictor variables. [...] Note that this assumption is much less restrictive than it may at first seem. [...] Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. [...] The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. [...] This trick is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method. [...] In fact, models such as polynomial regression are often [...] "too powerful", in that they tend to overfit the data. [...] As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process. [...] Common examples are <b>ridge</b> <b>regression</b> and lasso regression. [...] Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, <b>ridge</b> <b>regression</b> and lasso regression can both be viewed as [...] special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.) ...|$|E
40|$|AbstractNewborn {{screening}} {{programs for}} severe metabolic disorders using tandem mass spectrometry are widely used. Medium-Chain Acyl-CoA dehydrogenase deficiency (MCADD) {{is the most}} prevalent mitochondrial fatty acid oxidation defect (1 : 15, 000 newborns) {{and it has been}} proven that early detection of this metabolic disease decreases mortality and improves the outcome. In previous studies, data mining methods on derivatized tandem MS datasets have shown high classification accuracies. However, no machine learning methods currently have been applied to datasets based on non-derivatized screening methods. A dataset with 44, 159 blood samples was collected using a non-derivatized screening method as part of a systematic newborn screening by the PCMA screening center (Belgium). Twelve MCADD cases were present in this partially MCADD-enriched dataset. We extended three data mining methods, namely C 4. 5 decision trees, logistic <b>regression</b> and <b>ridge</b> logistic <b>regression,</b> with a parameter and threshold optimization method and evaluated their applicability as a diagnostic support tool. Within a stratified cross-validation setting, a grid search was performed for each model {{for a wide range of}} model parameters, included variables and classification thresholds. The best performing model used <b>ridge</b> logistic <b>regression</b> and achieved a sensitivity of 100 %, a specificity of 99. 987 % and a positive predictive value of 32 % (recalibrated for a real population), obtained in a stratified cross-validation setting. These results were further validated on an independent test set. Using a method that combines <b>ridge</b> logistic <b>regression</b> with variable selection and threshold optimization, a significantly improved performance was achieved compared to the current state-of-the-art for derivatized data, while retaining more interpretability and requiring less variables. The results indicate the potential value of data mining methods as a diagnostic support tool...|$|R
40|$|In {{this thesis}} {{advanced}} regression methods {{are applied to}} discuss and investigate highly relevant research questions {{in the areas of}} finance and economics. In the field of credit risk the thesis investigates a hierarchical model which allows to obtain a consensus score, if several ratings are available for each firm. Autoregressive processes and random effects are used to model both a correlation structure between and within the obligors in the sample. The model also allows to validate the raters themselves. The problem of model uncertainty and multicollinearity between the explanatory variables is addressed in the other two applications. Penalized regressions, like bridge regressions, are used to handle multicollinearity while model averaging techniques allow to account for model uncertainty. The second part of the thesis makes use of Bayesian elastic nets and Bayesian Model Averaging (BMA) techniques to discuss long-term economic growth. It identifies variables which are significantly related to long-term growth. Additionally, it illustrates the superiority of this approach in terms of predictive accuracy. Finally, the third part combines <b>ridge</b> <b>regressions</b> with BMA to identify macroeconomic variables which are significantly related to aggregated firm failure rates. The estimated results deliver important insights for e. g., stress-test scenarios. (author's abstract...|$|R
40|$|This study {{presents}} a rapid multiple incremental and decremental mechanism based on Weight-Error Curves (WECs) for support-vector analysis. Recursion-free computation is proposed for predicting the Lagrangian multipliers of new samples. This study examines Ridge Support Vector Models, subsequently devising a recursion-free function derived from WECs. With the proposed function, {{all the new}} Lagrangian multipliers can be computed at once without using any gradual step sizes. Moreover, such a function relaxes a constraint, where the increment of new multiple Lagrangian multipliers {{should be the same}} in the previous work, thereby easily satisfying the requirement of KKT conditions. The proposed mechanism no longer requires typical bookkeeping strategies, which compute the step size by checking all the training samples in each incremental round. Comment: Ridge support vector machine (Ridge SVM), <b>Ridge</b> support vector <b>regression</b> (<b>Ridge</b> SVR), multiple incremental learning, multiple decremental learning, online learning, batch learning, cloud computing, big data analysis, data analytic...|$|R
2500|$|Bayesian linear {{regression}} applies {{the framework of}} Bayesian statistics to {{linear regression}}. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β {{are assumed to be}} random variables with a specified prior distribution. [...] The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) <b>ridge</b> <b>regression</b> or lasso regression. [...] In addition, the Bayesian estimation process produces not a single point estimate for the [...] "best" [...] values of the regression coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity. [...] This can be used to estimate the [...] "best" [...] coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.|$|E
5000|$|Regression: {{least squares}}, <b>ridge</b> <b>regression,</b> least angle regression, elastic net, kernel <b>ridge</b> <b>regression,</b> support vector {{machines}} (SVM), partial least squares (PLS) ...|$|E
5000|$|When , elastic net becomes <b>ridge</b> <b>regression,</b> whereas [...] {{it becomes}} Lasso. [...] Elastic Net penalty {{function}} doesn't {{have the first}} derivative at 0 and it is strictly convex [...] taking the properties both lasso and <b>ridge</b> <b>regression.</b>|$|E
30|$|Actually, {{this is the}} Sylvester equation, with B_v {{the unknown}} {{parameter}} matrix to be determined. The equation can be solved efficiently (Bartels and Stewart 1972). Similar to the estimation of <b>Ridge</b> and Lasso <b>regression,</b> we used a nested threefold cross-validation to determine λ _ 1, λ _ 2 in the range (10 ^- 5, 10 ^ 5) on a log scale.|$|R
40|$|Logistic {{regression}} analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach {{that uses a}} Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach {{to a range of}} document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or <b>ridge</b> logistic <b>regression</b> combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results. KEY WORDS: Information retrieval; Lasso; Penalization; Ridge regression; Support vector classifier; Variable selection...|$|R
40|$|ABSTRACT. This paper tries ®rst to {{introduce}} and motivate {{the methodology of}} multivariate calibration. Next a review is given, mostly avoiding technicalities, of the somewhat messy theory of the subject. Two approaches are distinguished: the estimation approach (controlled calibration) and the prediction approach (natural calibration). Among problems discussed are the choice of estimator, the choice of con®dence region, methodology for handling situations with more variables than observations, near-collinearities (with countermeasures like <b>ridge</b> type <b>regression,</b> principal components regression, partial least squares regression and continuum regression), pretreatment of data, and cross-validation vs true prediction. Examples discussed in detail concern estimation {{of the age of}} a rhinoceros from its horn lengths (low-dimensional), and nitrate prediction in waste-water from high-dimensional spectroscopic measurements...|$|R
5000|$|One of {{the prime}} {{differences}} between Lasso and <b>ridge</b> <b>regression</b> is that in <b>ridge</b> <b>regression,</b> as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause {{more and more of}} the parameters to be driven to zero. This is an advantage of Lasso over <b>ridge</b> <b>regression,</b> as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas <b>Ridge</b> <b>regression</b> never fully discards any features. Some feature selection techniques are developed based on the LASSO including Bolasso which bootstraps samples, [...] and FeaLect which analyzes the regression coefficients corresponding to different values of [...] to score all the features.|$|E
5000|$|Simple Least-Squares Linear Regression (and <b>Ridge</b> <b>Regression)</b> ...|$|E
5000|$|As {{discussed}} above, lasso can set coefficients to zero, while <b>ridge</b> <b>regression,</b> {{which appears}} superficially similar, cannot. This {{is due to}} the difference {{in the shape of the}} constraint boundaries in the two cases. Both lasso and <b>ridge</b> <b>regression</b> can be interpreted as minimizing the same objective function ...|$|E
40|$|The paper {{provides}} a proof of {{consistency of the}} <b>ridge</b> estimator for <b>regressions</b> where the number of regressors tends to infinity. Such result is obtained without assuming a factor structure. A Monte Carlo study suggests that shrinkage autoregressive models can lead to very substantial advantages compared to standard autoregressive models. An empirical application focusing on forecasting inflation and GDP growth in a panel of countries confirms this finding. Shrinkage, Forecasting...|$|R
40|$|Accurate model {{representation}} of land–atmosphere carbon fluxes {{is essential for}} climate projections. However, the exact responses of carbon cycle processes to climatic drivers often remain uncertain. Presently, knowledge derived from experiments, complemented by a steadily evolving body of mechanistic theory, provides the main basis for developing such models. The strongly increasing availability of measurements may facilitate new ways of identifying suitable model structures using machine learning. Here, we explore the potential of gene expression programming (GEP) to derive relevant model formulations {{based solely on the}} signals present in data by automatically applying various mathematical transformations to potential predictors and repeatedly evolving the resulting model structures. In contrast to most other machine learning regression techniques, the GEP approach generates readable models that allow for prediction and possibly for interpretation. Our study is based on two cases: artificially generated data and real observations. Simulations based on artificial data show that GEP is successful in identifying prescribed functions, with the prediction capacity of the models comparable to four state-of-the-art machine learning methods (random forests, support vector machines, artificial neural networks, and kernel <b>ridge</b> <b>regressions).</b> Based on real observations we explore the responses of the different components of terrestrial respiration at an oak forest in south-eastern England. We find that the GEP-retrieved models are often better in prediction than some established respiration models. Based on their structures, we find previously unconsidered exponential dependencies of respiration on seasonal ecosystem carbon assimilation and water dynamics. We noticed that the GEP models are only partly portable across respiration components, the identification of a general terrestrial respiration model possibly prevented by equifinality issues. Overall, GEP is a promising tool for uncovering new model structures for terrestrial ecology in the data-rich era, complementing more traditional modelling approaches...|$|R
40|$|Order {{selection}} of linear regression models has been thoroughly researched in the statistical community for some time. Different shrinkage {{methods have been}} proposed, such as the <b>Ridge</b> and Lasso <b>regression</b> methods. Especially the Lasso regression has won fame because {{of its ability to}} set less important parameters exactly to zero. However, these methods do not take dynamical systems into account, where the regressors are ordered via the time lag. To this end, a modified variant of the nonnegative garrote method will be analyzed...|$|R
5000|$|In practice, {{it may be}} {{necessary}} to regularize the whitened STA, since whitening amplifies noise along stimulus dimensions that are poorly explored by the stimulus (i.e., axes along which the stimulus has low variance). A common approach to this problem is <b>ridge</b> <b>regression.</b> The regularized STA, computed using <b>ridge</b> <b>regression,</b> can be written ...|$|E
5000|$|... #Caption: Forms of the {{constraint}} regions for lasso and <b>ridge</b> <b>regression.</b>|$|E
5000|$|This can be {{compared}} to <b>ridge</b> <b>regression,</b> where the objective is to minimize ...|$|E
40|$|Accurate {{modelling}} of land-atmosphere carbon fluxes {{is essential}} for future climate projections. However, the exact responses of carbon cycle processes to climatic drivers often remain uncertain. Presently, knowledge derived from experiments complemented with a steadily evolving body of mechanistic theory provides the main basis for developing the respective models. The strongly increasing availability of measurements may complicate the traditional hypothesis driven path to developing mechanistic models, but it may facilitate new ways of identifying suitable model structures using machine learning as well. Here we explore the potential to derive model formulations automatically from data based on gene expression programming (GEP). GEP automatically (re) combines various mathematical operators to model formulations that are further evolved, eventually identifying the most suitable structures. In contrast to most other machine learning regression techniques, the GEP approach generates models that allow for prediction and possibly for interpretation. Our study is based on two cases: artificially generated data and real observations. Simulations based on artificial data show that GEP is successful in identifying prescribed functions with the prediction capacity of the models comparable to four state-of-the-art machine learning methods (Random Forests, Support Vector Machines, Artificial Neural Networks, and Kernel <b>Ridge</b> <b>Regressions).</b> The case of real observations explores different components of terrestrial respiration at an oak forest in south-east England. We find that GEP retrieved models are often better in prediction than established respiration models. Furthermore, {{the structure of the}} GEP models offers new insights to driver selection and interactions. We find previously unconsidered exponential dependencies of respiration on seasonal ecosystem carbon assimilation and water dynamics. However, we also noticed that the GEP models are only partly portable across respiration components; equifinality issues possibly preventing the identification of a "general" terrestrial respiration model. Overall, GEP is a promising tool to uncover new model structures for terrestrial ecology in the data rich era, complementing the traditional approach of model building...|$|R
40|$|International audienceThe <b>ridge</b> {{logistic}} <b>regression</b> {{has successfully}} {{been used in}} text categorization problems {{and it has been}} shown to reach the same performance as the Support Vector Machine but with the main advantage of computing a probability value rather than a score. However, the dense solution of the ridge makes its use unpractical for large scale categorization. On the other side, LASSO regularization is able to produce sparse solutions but its performance is dominated by the ridge when the number of features is larger than the number of observations and/or when the features are highly correlated. In this paper, we propose a new model selection method which tries to approach the ridge solution by a sparse solution. The method first computes the ridge solution and then performs feature selection. The experimental evaluations show that our method gives a solution which is a good trade-off between the ridge and LASSO solutions...|$|R
40|$|Liu linear {{regression}} model building analysis is frequently applied for fitting model to near mullicollinearity data sets. When atypical observations {{exist in a}} data set, they may exert undue influence on {{the result of the}} analysis. This paper studies an assessment of the minor perturbation of the Liu estimator in the <b>ridge</b> type linear <b>regression</b> model using Cook's (1986) method to detect anomalous observations in the data set when mean squared error is known or unknown, and perturbation of individual explanatory variable. Example based on Longley data are used for illustration...|$|R
