364|2183|Public
25|$|Perera {{initially}} {{tested positive}} for a banned substance and was recalled from the New Zealand tour in December 2015. His urine sample was provided for a <b>random</b> <b>test</b> by the International Cricket Council during the home series against Pakistan in May 2015. He said that he took some medicine for a leech bite. After the A sample became positive, Kusal himself {{with the help of}} the Sri Lanka Cricket Board asked for the testing of the B sample, which was tested at Qatar. On 25 December 2015, the results of the B sample came back and it revealed that Kusal was positive for the banned substance. Meanwhile, Sports Minister Dayasiri Jayasekara informed that there is a conspiracy behind this scandal is to withdraw Kusal from 2016 ICC World Twenty20 tournament due to his effectiveness in the format. If the B sample was positive, he was likely to face a four-year ban.|$|E
5000|$|... {{directed}} <b>random</b> <b>test</b> generation - f.ex. [...] "feedback-directed <b>random</b> <b>test</b> generation" [...] or [...] "adaptive random testing" ...|$|E
50|$|<b>Random</b> <b>test</b> {{generators}} {{range in}} scope from simple scripts and parameterized macros {{that can be}} created {{in a matter of}} weeks to full featured systems requiring extensive software development. <b>Random</b> <b>test</b> generators are most often created by the designing organizations.|$|E
40|$|Previous {{studies showed}} that {{adaptive}} <b>random</b> <b>testing</b> is an effective alternative to <b>random</b> <b>testing</b> method, but requires additional overheads to evenly spread test cases. Mirroring was introduced to reduce the overheads of adaptive <b>random</b> <b>testing.</b> This paper is the follow-up work to a previous study on the integration of mirroring and adaptive <b>random</b> <b>testing,</b> namely the mirror adaptive <b>random</b> <b>testing.</b> It studies characteristics and effectiveness of mirror adaptive <b>random</b> <b>testing</b> in depth, and provides guidelines on how to apply mirror adaptive <b>random</b> <b>testing</b> in practice...|$|R
40|$|Adaptive <b>random</b> <b>testing</b> is an {{enhancement}} of <b>random</b> <b>testing.</b> Previous studies on adaptive <b>random</b> <b>testing</b> assumed {{that once a}} failure is detected, testing is terminated and debugging is conducted immediately. It {{has been shown that}} adaptive <b>random</b> <b>testing</b> normally uses fewer <b>test</b> cases than <b>random</b> <b>testing</b> for detecting the first software failure. However, under many practical situations, testing should not be withheld after the detection of a failure. Thus, it is important to investigate the effectiveness with respect to the detection of multiple failures. In this paper, we compare adaptive <b>random</b> <b>testing</b> and <b>random</b> <b>testing</b> under various scenarios and examine whether adaptive <b>random</b> <b>testing</b> is still able to use fewer <b>test</b> cases than <b>random</b> <b>testing</b> to detect multiple software failures. Our study delivers some interesting results and highlights a number of promising research projects...|$|R
40|$|<b>Random</b> <b>testing</b> {{is a basic}} {{software}} testing technique {{that can be used}} to assess the software reliability as well as to detect software failures. Adaptive <b>random</b> <b>testing</b> has been proposed to enhance the failure-detection capability of <b>random</b> <b>testing.</b> Previous studies have shown that adaptive <b>random</b> <b>testing</b> can use fewer <b>test</b> cases than <b>random</b> <b>testing</b> to detect the first software failure. In this paper, we evaluate and compare the performance of adaptive <b>random</b> <b>testing</b> and <b>random</b> <b>testing</b> from another perspective, that of code coverage. As shown in various investigations, a higher code coverage not only brings a higher failure-detection capability, but also improves the effectiveness of software reliability estimation. We conduct a series of experiments based on two categories of code coverage criteria: structure-based coverage, and fault-based coverage. Adaptive <b>random</b> <b>testing</b> can achieve higher code coverage than <b>random</b> <b>testing</b> with the same number of test cases. Our experimental results imply that, in addition to having a better failure-detection capability than <b>random</b> <b>testing,</b> adaptive <b>random</b> <b>testing</b> also delivers a higher effectiveness in assessing software reliability, and a higher confidence in the reliability of the software under test even when no failure is detected...|$|R
5000|$|... undirected <b>random</b> <b>test</b> {{generation}} - with no heuristics {{to guide}} its search ...|$|E
50|$|However, <b>Random</b> <b>Test</b> Data Generation {{is usually}} {{used as a}} {{benchmark}} as it has the lowest acceptable rate of generating test data.|$|E
50|$|<b>Random</b> <b>test</b> data {{generation}} {{is probably the}} simplest method for generation of test data. The advantage {{of this is that}} {{it can be used to}} generate input for any type of program. Thus to generate test data we can randomly generate a bit stream and let it the represent the data type needed. However, <b>random</b> <b>test</b> data generation does not generate quality test data as it does not perform well in terms of coverage. Since the data generated is based solely on probability it cannot accomplish high coverage as the chances of it finding semantically small faults is quite low.|$|E
40|$|Abstract. In this paper, we {{introduce}} {{an enhanced}} form of <b>random</b> <b>testing</b> called Adaptive <b>Random</b> <b>Testing.</b> Adaptive <b>random</b> <b>testing</b> seeks to distribute test cases more evenly within the input space. It {{is based on}} the intuition that for non-point types of failure patterns, an even spread of test cases is more likely to detect failures using fewer test cases than ordinary <b>random</b> <b>testing.</b> Experiments are performed using published programs. Results show that adaptive <b>random</b> <b>testing</b> does outperform ordinary <b>random</b> <b>testing</b> significantly (by up to as much as 50 %) for the set of programs under study. These results are very encouraging, providing evidences that our intuition is likely to be useful in improving the effectiveness of <b>random</b> <b>testing.</b> ...|$|R
40|$|Recently, Adaptive <b>Random</b> <b>Testing</b> through Iterative Partitioning (IP-ART) {{has been}} {{proposed}} as a <b>random</b> <b>testing</b> method that {{is more effective than}} pure <b>Random</b> <b>Testing.</b> Besides this, {{it is supposed to be}} equally effective as very good <b>random</b> <b>testing</b> techniques, namely Distance- Based Adaptive <b>Random</b> <b>Testing</b> and Restricted <b>Random</b> <b>Testing,</b> while only having between linear and quadratic runtime. In the present paper, it is investigated what influence the ratio of width and height of a rectangular input domain has on the effectiveness of various Adaptive <b>Random</b> <b>Testing</b> methods. Based on our findings, an improved version of IP-ART is proposed. The effectiveness of the new method is also analyzed for various ratios of width and height of the input domain...|$|R
40|$|Abstract—The {{drawback}} {{of classical}} software <b>random</b> <b>testing</b> is low efficiency to find failure-causing inputs, {{because it requires}} {{a large number of}} test cases compared to a family of partition testing. This paper proposes a software <b>random</b> <b>testing</b> scheme based on Markov chain Monte Carlo (MCMC) method. In this paper, we propose a probability model to represent the activities for finding failures in software testing. In experiments, we compare effectiveness of MCMC <b>random</b> <b>testing</b> with both ordinary <b>random</b> <b>testing</b> and adaptive <b>random</b> <b>testing</b> in real program sources. These results provide that MCMC <b>random</b> <b>testing</b> can drastically improve the effectiveness of software testing. I...|$|R
50|$|The South Africa Soccer Association {{assessed}} {{a two-year}} ban on Okétola in January 2009, {{as a result}} of a doping violation. A <b>random</b> <b>test</b> after a league match turned up a positive result for amphetamines, a prohibited substance.|$|E
50|$|<b>Random</b> <b>test</b> {{generators}} (often abbreviated RTG or ISG for Instruction Stream Generator) are a type {{of computer}} software that is used in functional verification of microprocessors. Their primary use lies in providing input stimulus to a device under test.|$|E
50|$|Trace {{amounts of}} cocaine {{were found in}} Flachi's system in a <b>random</b> <b>test</b> after a 0-2 away loss against to F.C. Internazionale Milano on 28 January 2007. On 31 May, he was {{suspended}} for 16 months and, later, the ban was increased to two years. As a result, his contract with Sampdoria was canceled.|$|E
40|$|Abstract—In this paper, {{we propose}} a {{probabilistic}} approach to finding failure-causing inputs based on Bayesian estimation. According to our probabilistic insights of software testing, the test case generation algorithms are developed by Markov chain Monte Carlo (MCMC) methods. Dissimilar to existing <b>random</b> <b>testing</b> schemes such as adaptive <b>random</b> <b>testing,</b> our approach can also utilize the prior knowledge on software testing. In experiments, we compare effectiveness of our MCMC-based <b>random</b> <b>testing</b> with both ordinary <b>random</b> <b>testing</b> and adaptive <b>random</b> <b>testing</b> in real program sources. These results indicate {{the possibility that}} MCMC-based <b>random</b> <b>testing</b> can drastically improve the effectiveness of software testing...|$|R
40|$|<b>Random</b> <b>testing</b> {{techniques}} have been extensively used in reliability assessment, {{as well as}} in debug testing. When used to assess software reliability, <b>random</b> <b>testing</b> selects test cases based on an operational profile; while in the context of debug <b>testing,</b> <b>random</b> <b>testing</b> often uses a uniform distribution. However, generally neither an operational profile nor a uniform distribution is chosen from the perspective of maximizing the effectiveness of failure detection. Adaptive <b>random</b> <b>testing</b> has been proposed to enhance the failure detection capability of <b>random</b> <b>testing</b> by evenly spreading test cases over the whole input domain. In this paper, we propose a new test profile, which is different from both the uniform distribution, and operational profiles. The aim of the new test profile is to maximize the effectiveness of failure detection. We integrate this new test profile with some existing adaptive <b>random</b> <b>testing</b> algorithms, and develop a family of new <b>random</b> <b>testing</b> algorithms. These new algorithms not only distribute test cases more evenly, but also have better failure detection capabilities than the corresponding original adaptive <b>random</b> <b>testing</b> algorithms. As a consequence, they perform better than the pure <b>random</b> <b>testing...</b>|$|R
40|$|The F-measure - {{the number}} of {{distinct}} test cases to detect the first program failure - is an effectiveness measure for debug testing strategies. We show that for <b>random</b> <b>testing</b> with replacement, the F-measure will be distributed according to the geometric distribution. A simulation study examines the distribution of two adaptive <b>random</b> <b>testing</b> methods, to study how closely their sampling distributions approximate the geometric distribution, revealing that in the worst case scenario, the sampling distribution for adaptive <b>random</b> <b>testing</b> {{is very similar to}} <b>random</b> <b>testing.</b> Our results have provided an answer to a conjecture that adaptive <b>random</b> <b>testing</b> is always a more effective alternative to <b>random</b> <b>testing,</b> with reference to the F-measure. We consider the implications of our findings for previous studies conducted in the area, and make recommendations to future studies...|$|R
5000|$|His {{career was}} notable for the {{numerous}} fines and suspensions he had because of various violations, and in 2003, {{he was suspended}} indefinitely (later reduced to eight months) by then PBA Commissioner Noli Eala after he was tested positive for an illegal substance in a <b>random</b> <b>test.</b> [...] The suspension was later lifted after he completed the rehab program.|$|E
50|$|After drugs {{testing was}} {{introduced}} to the sport in 2006, after the UK Open in what was the eighth <b>random</b> <b>test</b> in the British-based organisation by agency UK Sport, Green became the first darts player to test positive for drugs. As a result of a positive test for marijuana, he was: forced to return his £4,000 prize money from the UK Open; fined £2,000; and banned from competition for eight weeks.|$|E
50|$|In 2010, the U.S. Anti-Doping Agency banned him for 2 {{years for}} use of {{synthetic}} testosterone and modafinil.In August 2011, David Clinger was issued a lifetime doping ban by the U.S. Anti-Doping Agency (USADA) after testing positive during a <b>random</b> <b>test</b> while serving a prior doping ban. Clinger accepted responsibility for using clenbuterol for performance-enhancing purposes, USADA said. The second violation prompted USADA to issue the lifetime competition ban.|$|E
40|$|Testing {{plays an}} {{important}} role in software development. The purpose of testing is to "search for errors" or "prove that the software works". In order to achieve this purpose, there are many dierent testing methods using dierent classied criteria. <b>Random</b> <b>testing</b> is an input-domain-based and often black-box method. In <b>random</b> <b>testing,</b> test cases are selected randomly from the entire input domain. Historically, there are strong disagreements about its value. In this thesis, we propose a new approach to improve <b>random</b> <b>testing</b> { <b>random</b> <b>testing</b> with log le analysis. It uses <b>random</b> <b>testing</b> as the main strategy in test case generation, and incorporates automatic test result checking and code coverage measures into the testing process. It provides a way to solve the problems associated with <b>random</b> <b>testing,</b> and also provides a way of automatic testing...|$|R
40|$|Adaptive <b>Random</b> <b>Testing</b> subsumes a {{class of}} {{algorithms}} that detect the first failure with less <b>test</b> cases than <b>Random</b> <b>Testing.</b> The present paper shows that a “reference method ” {{in the field of}} Adaptive <b>Random</b> <b>Testing</b> is not effective for higher dimensional input domains and clustered failure-causing inputs. The reason for this behavior is explained, and a modified method is proposed and analyzed...|$|R
40|$|Software {{product lines}} are the common trend in {{software}} development which helps {{in reducing the}} development cost. Mostly the interaction faults {{are very difficult to}} identify during the process of debugging. By the use of combinatorial testing a set of features can be identified and all small combinations can be verified to a certain level only. By introducing <b>random</b> <b>testing</b> can improve the accuracy and ratio of t-wise fault detection. Through <b>random</b> <b>testing</b> can acquire a higher level of improvements over the combinatorial testing which will be under the budgetary limit of the product. <b>Random</b> <b>testing</b> can provide minimum guarantees on the probability of fault detection at any interaction level using the set of theories. For example, <b>random</b> <b>testing</b> becomes even more effective as the number of features increases and converges toward equal effectiveness with combinatorial testing. Given that combinatorial testing entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which <b>random</b> <b>testing</b> may outperform combinatorial testing in large systems. Furthermore, in common situations where test budgets are constrained and unlike combinatorial <b>testing,</b> <b>random</b> <b>testing</b> can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then <b>random</b> <b>testing</b> can fare arbitrarily worse than combinatorial testing. Index Terms: Combinatorial <b>testing,</b> <b>random</b> <b>testing,</b> t-wise faul...|$|R
50|$|Many {{employers}} or occupations {{have their}} own rules and BAC limits; for example, the United States Federal Railroad Administration has a 0.04% limit for train crew. Certain large corporations {{have their own}} rules; for example, Union Pacific Railroad has their own BAC limit of 0.02% that, if violated during a <b>random</b> <b>test</b> or a for-cause test—for example, after a traffic accident—can result in termination of employment with no chance of future rehire.|$|E
50|$|If a {{fault is}} only {{revealed}} {{by a small}} percentage of the program input it is said to be a semantically small fault. For example, of a semantically small fault consider the following code:void test(char x,char y) { if (x==y) printf("Equal"); else printf("Not Equal");}It is easy to see that the probability of execution of the first statement is significantly lesser than that of the second statement. As the structures in it grow complex so does the probability of its execution. Thus, such semantically small faults are hard to find using <b>random</b> <b>test</b> data generation.|$|E
50|$|Recovery {{mechanisms}} are {{ways in which}} the systems can recover from failures. These recovery mechanisms should be well designed, meaning that they are reliable, effective and efficient. These systems should be proactive in testing and verifying the behavior of the recovery mechanisms so should there be a real failure it is certain that these mechanisms will do what they are designed to do and aid in the recovery of the system. These verifications should be performed even in production level equipment as this type of equipment is the most vital to have up. There are two methods for performing these tests and both of these should be used. The first method is directed tests in which the tests are set up and executed. The other method is a <b>random</b> <b>test</b> in which they occur without warning.|$|E
40|$|Abstract—This paper compares {{partition}} <b>testing</b> and <b>random</b> <b>testing</b> on {{the assumption}} that program failure rates are not known with certainty before testing and are, therefore, modeled by random variables. It is shown that under uncertainty, partition testing compares more favorably to <b>random</b> <b>testing</b> than suggested by prior investigations concerning the deterministic case: The restriction to failure rates that are known with certainty systematically favors <b>random</b> <b>testing.</b> In particular, we generalize a result by Weyuker and Jeng stating equal fault detection probabilities for partition <b>testing</b> and <b>random</b> <b>testing</b> in the case where the failure rates in the subdomains defined by the partition are equal. It turns out that for independent random failure rates with equal expectation, the case above is a boundary case (the worst case for partition testing), and the fault detection probability of partition testing can be up to k times higher than that of <b>random</b> <b>testing,</b> where k is the number of subdomains. Also in a related model for dependent failure rates, partition testing turns out to be consistently better than <b>random</b> <b>testing.</b> The dominance can also be verified for the expected (weighted) number of detected faults as an alternative comparison criterion. Index Terms—Decisions under uncertainty, fault detection, partition <b>testing,</b> program <b>testing,</b> <b>random</b> <b>testing,</b> software testing. ————————— — F ——————————...|$|R
40|$|Test case {{generation}} is {{a path to}} identify the solution in software <b>testing.</b> Adaptive <b>random</b> <b>testing</b> is an enhancement of <b>random</b> <b>testing</b> {{to improve the quality}} of fault-revealing. The research focuses on software adaptive <b>random</b> <b>testing</b> based on Matrix called Partitioned Block based Adaptive <b>Random</b> <b>Testing.</b> It compares the performance of PBART with the existing Adaptive <b>random</b> <b>testing</b> using <b>random</b> samples of <b>test</b> cases which are drawn from blocks of distinct partitions. Partition testing defines as a block of test cases partitioned into set of all test cases. Thereby it has prompted to investigate the performance of <b>random</b> <b>testing</b> that can be improved by taking the patterns of failure-causing inputs which utilizes the prior knowledge and the information of the test cases. The proposed algorithm PB –ART performs the testing of program structure and load the source code to matrix with scenarios, method flows and data values. In numerical experiments, the approach examines effectiveness of PB-ART with ordinary adaptive <b>random</b> <b>testing.</b> There exist three measures for evaluating the effectiveness of a testing technique namely P-measure, E-measure and F-measure. Moreover F-measure is intuitively more appealing to testers and more realistic and informative from a practical point of view. Therefore, F-measure is chosen for measuring testing techniques in this research work...|$|R
40|$|Abstract. This paper {{proposes a}} {{software}} <b>random</b> <b>testing</b> {{scheme based on}} Markov chain Monte Carlo (MCMC) method. The significant issue of software testing is {{how to use the}} prior knowledge of experienced testers and the information obtained from the preceding test outcomes in making test cases. The concept of Markov chain Monte Carlo <b>random</b> <b>testing</b> (MCMCRT) is based on the Bayes approach to parametric models for software testing, and can utilize the prior knowledge and the information on preceding test outcomes for their parameter estimation. In numerical experiments, we examine effectiveness of MCMCRT with ordinary <b>random</b> <b>testing</b> and adaptive <b>random</b> <b>testing...</b>|$|R
50|$|Perera {{initially}} {{tested positive}} for a banned substance and was recalled from the New Zealand tour in December 2015. His urine sample was provided for a <b>random</b> <b>test</b> by the International Cricket Council during the home series against Pakistan in May 2015. He said that he took some medicine for a leech bite. After the A sample became positive, Kusal himself {{with the help of}} the Sri Lanka Cricket Board asked for the testing of the B sample, which was tested at Qatar. On 25 December 2015, the results of the B sample came back and it revealed that Kusal was positive for the banned substance. Meanwhile, Sports Minister Dayasiri Jayasekara informed that there is a conspiracy behind this scandal is to withdraw Kusal from 2016 ICC World Twenty20 tournament due to his effectiveness in the format. If the B sample was positive, he was likely to face a four-year ban.|$|E
5000|$|On November 11, the WBC {{were told}} by VADA that Stiverne had tested {{positive}} on a drug test. It was said that the banned substance was methylhexaneamine, which {{is also known as}} dimethylamylamine or 'DMAA'. Povetkin's camp confirmed the fight will still go ahead. Stiverne claimed he ingested a post-workout supplement called SUPERPHARM without knowing it included dimethylamylamine, but under VADA’s rules an athlete is responsible for whatever goes into their body. The WBC took into account that it was Stiverne’s first offence when making its ruling and fined him $75,000. Just 20 hours before the fight was to take place, the WBC withdrew it's sanction of the fight stating Povetkin had failed another drug test, this time for Ostarine. The test was taken of December 6. Stiverne later made a statement to tell everyone he will be heading home to Las Vegas {{and did not want to}} fight if the sanction was off, as that was the whole reason for him training and taking the fight in Russia. [...] Following the cancellation, Stiverne's promoter Don King stated he would be filing a lawsuit against World of Boxing promoter Andrey Ryabinsky, just as Ryabinsky did against King when Lebedev pulled out of his scheduled rematch against Guillermo Jones, after Jones tested positive for a second time. At that time, Ryabinsky was awarded $1.6m in damages. On December 23, Ryabinsky stated that Povetkin's sample from December 13 came back negative. The test were he had tested positive contained 0,00000000001g traces of ostarine. A previous <b>random</b> <b>test</b> in November also came back negative.|$|E
40|$|This paper {{discusses}} a Genetic Algorithm-based {{method of}} generating test vectors for detecting faults in combinational circuits. The GA-based approach combines {{the merits of}} two techniques {{that have been used}} previously for generating test vectors - the directed search approach and the <b>random</b> <b>test</b> method. We employ a variant of the traditional GA, the Adaptive GA (AGA), to improve the effi cacy of the genetic search. Two cost functions that are used for assessing the quality of the vectors are discussed. The performance of the AGA-based test generation approach has been evaluated using ISCAS- 85 benchmark circuits. In our approach, the number of vectors that need to be simulated for detecting all detectable faults is significantly smaller than that required for a <b>random</b> <b>test</b> method. Even when optimized input distributions are used to generate the <b>random</b> <b>test</b> vectors, the AGA sustains its superior performance over the <b>random</b> <b>test</b> method...|$|E
40|$|Abstract—A {{substantial}} amount of work has shed light on whether <b>random</b> <b>testing</b> is actually a useful testing technique. Despite its simplicity, several successful real-world applications {{have been reported in}} the literature. Although it is not going to solve all possible <b>testing</b> problems, <b>random</b> <b>testing</b> appears to be an essential tool in the hands of software testers. In this paper, we review and analyze the debate about <b>random</b> <b>testing.</b> Its benefits and drawbacks are discussed. Novel results addressing general questions about <b>random</b> <b>testing</b> are also presented, such as how long does <b>random</b> <b>testing</b> need, on average, to achieve testing targets (e. g., coverage), how does it scale, and how likely is it to yield similar results if we rerun it on the same testing problem (predictability). Due to its simplicity that makes the mathematical analysis of <b>random</b> <b>testing</b> tractable, we provide precise and rigorous answers to these questions. Results show that there are practical situations in which <b>random</b> <b>testing</b> is a viable option. Our theorems are backed up by simulations and we show how they can be applied to most types of software and testing criteria. In light of these results, we then assess the validity of empirical analyzes reported in the literature and derive guidelines for both practitioners and scientists...|$|R
40|$|Abstract—Random {{testing is}} a low cost {{strategy}} {{that can be applied}} {{to a wide range of}} testing problems. While the cost and straightforward application of <b>random</b> <b>testing</b> are appealing, these benefits must be evaluated against the reduced effectiveness due to the generality of the approach. Recently, a number of novel techniques, coined Adaptive <b>Random</b> <b>Testing,</b> have sought to increase the effectiveness of <b>random</b> <b>testing</b> by attempting to maximize the testing coverage of the input domain. This paper presents the novel application of an evolutionary search algorithm to this problem. The results of an extensive simulation study are presented in which the evolutionary approach is compared against the Fixed Size Candidate Set (FSCS), Restricted <b>Random</b> <b>Testing</b> (RRT), quasi-random testing using the Sobol sequence (Sobol), and <b>random</b> <b>testing</b> (RT) methods. The evolutionary approach was found to be superior to FSCS, RRT, Sobol, and RT amongst block patterns, the arena in which FSCS, and RRT have demonstrated the most appreciable gains in testing effectiveness. The results among fault patterns with increased complexity were shown to be similar to those of FSCS, and RRT; and showed a modest improvement over Sobol, and RT. A comparison of the asymptotic and empirical runtimes of the evolutionary search algorithm, and the other testing approaches, was also considered, providing further evidence that the application of an evolutionary search algorithm is feasible, and within the same order of time complexity as the other adaptive <b>random</b> <b>testing</b> approaches. Index Terms—Adaptive <b>random</b> <b>testing,</b> automated testing, evolutionary computing and genetic algorithms, <b>random</b> <b>testing,</b> software testing, test generation, test strategies...|$|R
3000|$|General {{prioritization}} techniques: Techniques {{defined in}} Section 1. We {{will consider the}} following short-names {{for the sake of}} simplicity: Optimal, Random, ART_Jac (Adaptive <b>Random</b> <b>Testing</b> with Jaccard distance), ART_Man (Adaptive <b>Random</b> <b>Testing</b> with Manhattan distance), Fixed_Weights, and Stoop; [...]...|$|R
