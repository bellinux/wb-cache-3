10|10000|Public
5000|$|... 'drafts' is a {{directory}} (d), {{the owner of}} which {{has the right to}} read (r) write (w) and execute (x): rwx, group members have (r--), meaning read only, and others have (r--), meaning <b>read</b> <b>only</b> <b>access.</b>|$|E
50|$|The core element for {{collaboration}} in Folio Cloud {{is the team}} room - a team room is a protected area in the cloud for storing data. Users may have <b>read</b> <b>only</b> <b>access,</b> change access or full control within a team room.|$|E
50|$|The pilot went live on 6 November 2006, with Lancashire, West Yorkshire and Merseyside {{contributing}} and viewing images. Greater Manchester, North Wales, Devon and Cornwall, British Transport Police (BTP) North Eastern Region, {{as well as}} one of the Metropolitan Police specialist {{units and}} eBorders had <b>read</b> <b>only</b> <b>access</b> to the system.|$|E
5000|$|Up to 64 words {{relative}} to the constant pointer (<b>read</b> <b>only,</b> word <b>access</b> <b>only)</b> ...|$|R
50|$|It was {{announced}} that on March 20, 2011 the service would be shut down leaving <b>only</b> <b>read</b> <b>access</b> to the website active.|$|R
50|$|Simple File Sharing {{disables}} granular {{local and}} network sharing permissions. It shares the item with the Everyone {{group on the}} network with <b>read</b> <b>only</b> or write <b>access,</b> without asking for a password but forcing Guest user permissions.|$|R
40|$|This work {{considers}} the materialized view maintenance in a distributed environment where the collaboration of the operational systems {{is limited to}} granting <b>read</b> <b>only</b> <b>access</b> on the selected relational tables. In addition, due to possibly large volumes of source relational tables involved in the view maintenance process and consequently enormous overhead associated with data extracting and transporting, the efficiency issues have been taken into the consideration and some optimization techniques are included in this paper...|$|E
40|$|This work {{considers}} extracting delta in a {{distributed environment}} where the collaboration from highly autonomous operational database management systems {{is limited to}} granting <b>read</b> <b>only</b> <b>access</b> {{on a set of}} selected relational tables. In addition, due to possibly large volumes of remote source data involved, and consequently enormous overhead associated with data transfer, efficiency issues must been taken into consideration. This research is based on the observation that usually, there is a large amount of static data in relational tables. It is proposed to use some statistical techniques in data warehouse system to help us understand the essential characteristics of remote data. Based on the characteristics of remote data, some optimisation techniques are presented...|$|E
40|$|This work {{considers}} extracting delta in a {{distributed environment}} where the collaboration from highly autonomous operational database management systems {{is limited to}} granting <b>read</b> <b>only</b> <b>access</b> {{on a set of}} selected relational tables. Because of inherently huge volume of data in data warehouse system, it is critical to minimise communication costs as much as possible. Based on the observation that usually, two consecutive snapshots are not very different, a statistical-based group hash method is developed to minimise the volumes of data required to complete the data extraction. In addition, to relax the assumption that the changes to remote data are only caused by random events, we define a progression pattern to describe data changes with temporal regularities and also propose a method for progression pattern discovery...|$|E
50|$|The 10-bit {{program counter}} is {{accessible}} as R2. <b>Reads</b> <b>access</b> <b>only</b> the low bits, and writes clear the high bits. An {{exception is the}} TBL instruction, which modifies the low byte while preserving bits 8 and 9.|$|R
5000|$|VOS {{supports}} write, read, execute, and null {{access to}} all files, directories and devices (although directories and files have slightly different access lists). Access can be assigned to users, groups, or the world. <b>Only</b> <b>read</b> <b>access</b> is required to run an executable program, provided that the user has [...] "execute" [...] privileges for the directory in which that program sits.|$|R
40|$|We {{introduce}} a new model and a description for objects which can move around on a cellular grid. In the movement part of the model the objects specify {{the direction in which}} they want to move. The conflict, which occurs when objects collide or when alternative objects want to move to the same free cell, is resolved through the conflict resolution part of the model. We present an extension to the cellular description language CDL which implements the moving objects and we give example programs. This extension CDL++ is automatically converted into a two [...] phased CDL program. 1 Motivation The cellular automata (CA) computation model is based on the principles ffl Locality: Cells <b>read</b> <b>only</b> their neighbour states and change their state depending on this information ffl Massive Parallelism: All cells execute their local rule in parallel. To be more precise, the locality principle means that a cell has <b>only</b> <b>read</b> <b>access</b> to their neighbours and <b>only</b> write <b>access</b> within itself. Thinking in th [...] ...|$|R
40|$|This work {{considers}} maintaining materialized view in a {{distributed environment}} where the collaboration from highly autonomous operational database management systems {{is limited to}} granting <b>read</b> <b>only</b> <b>access</b> {{on a set of}} selected relational tables. In addition, due to possibly large volumes of remote source data involved in the view maintenance process, and consequently enormous overhead associated with data transfer, efficiency issues must been taken into the consideration. This proposal is based on the observation that usually, there is a large amount of static data in relational tables. It is proposed to use some statistical techniques at data warehouse system to helps us understand the essential characteristics of raw data at remote sites. Based on the characteristics of raw data, some optimization techniques are presented. © 2012 ICST...|$|E
40|$|Cloud {{computing}} {{has recently}} received considerable attention both in industry and academia. Due {{to the great}} success {{of the first generation}} of Cloud-based services, providers have to deal with larger and larger volumes of data. Quality of service agreements with customers require data to be replicated across data centers in order to guarantee a high degree of availability. In this context, Cloud Data Management has to address several challenges, especially when replicated data are concurrently updated at different sites or when the system workload and the resources requested by clients change dynamically. Mostly independent from recent developments in Cloud Data Management, Data Grids have undergone a transition from pure file management with <b>read</b> <b>only</b> <b>access</b> to more powerful systems. In our recent work,we have developed the Re:GRIDiT protocol for managing data in the Grid which provides concurrent access to replicated data at different sites without any global component and supports the dynamic deployment of replicas. Since it is independent from the underlying Grid middleware, it can be seamlessly transferred to other environments like the Cloud. In this paper, we compare Data Management in the Grid and the Cloud, briefly introduce the Re:GRIDiT protocol and show its applicability for Cloud Data Management...|$|E
40|$|Through joint {{national}} and international efforts ail information system was developed during the last years to store the variety of geological and environmental data in a consistent form, and to provide access on data essential for the scientific community involved in Global Change research. Based on the discussions and recommendations of scientists from jnstitutes contributing to marine sciences, the information system, PANGAEA (PaleoNetwork for Geological and Environmental Data), was developed at the Alfred Wegener Institute for Polar, and Marine Research (AWI) funded by the German Ministry of Education, Science, Research · and Technology (BMBF). PANGAEA provides a new powerful scientific tool, which is innovative and unique so far. The first PANGAEA subsystem SEPAN (Sediment and Paleoclimate Data Network) is operational since 1996 and is used within a growing network of marine research institutes in Germany. The PANGAEA-data management comprises the collection of analytical data including all related meta-iIiformation, data set error checking and publication with long-'term banking of data. Retrieval, downloading, and visualization functionality is included as well as support for working groups in data handling. The efficiency of handling, sharing and integrating data is supported by an easy to use graphical interface which also offers complex search and access functionality. Problems arising from the great variety of parameters, methods, calibrations, and interpretations in the field of. environmental. Investigations are solved through a flexible and simple data model. The structure of the data model reflects the standard processing steps for environmental data. Different projects are working in selected areas or on different,campaigns to take samples or to measure environmental parameter. From each site analytical data are produced. Lists including standardized meta-information (e. g. references, definition of parameter, method) {{are connected to the}} main data fields. Any site oriented data f a specific scientific field can be, stored in a consistent format. The user is able to extract data sets for specific requirements in nearly any useful combination of metadata and analytical data via comprehensive retrievals. Data can be exported as text or plotted with one of the graphic tools as a geographical map,or as a plot versus time or elevation. All data sets are copyright protected within a hierarchicc;tl protection system. PANGAEA uses client/server technology through the Intranet/Internet. The main server, located in a computer center, is connected through the Internet with the different external institutes/projects. To improve access speed, all meta-information is mirrored on local servers. Only on request of anatytical data the main server is accessed through the Internet. Using the PANGAEA proprietary software gives full access to the functionality of the system; <b>read</b> <b>only</b> <b>access</b> on published data is provided through the WorldWideWeb. Tests of the client/server connection through the Internet have shown that this system can be used throughout whole Europe. The PANGAEA system is introduced to the scientific community since 1995 on {{national and}} international workshops and symposia and was offered as the data management system for different projects and institutions. It is now part of the data management plan of five EU proposals in the fields of MAST and 'Climate and Environment', of three special research projects (SFB) of Germany, dealing with marine research and is established in Germany to provide management and longterm storage of data for several institutes connected to the PANGAEA Network...|$|E
40|$|Graduation date: 1983 A process-resource graph is a {{directed}} graph with m resource nodes and n process nodes. A request edge is directed from {{a process to}} a resource. An assignment edge is directed from a resource to a process. A cycle in the process-resource graph is a necessary and sufficient condition for a deadlock to exist. This paper presents a method to find the probability of deadlock for shared <b>access</b> (<b>read</b> <b>only)</b> and exclusive <b>access</b> (read/write). Two formulae are given, one to count {{the total number of}} graphs, and another to count the total number of graphs which have at least one cycle. The probability of deadlock is then calculated by dividing the total number of graphs into the number of graphs with cycle...|$|R
50|$|Tuxera NTFS for Mac allows macOS {{computers}} {{to read and}} write NTFS partitions. By default, macOS provides <b>only</b> <b>read</b> <b>access</b> to NTFS partitions. The latest version of the driver is 2015.1, released in October 2015. With the introduction of System Integrity Protection (SIP) by Apple in OS X El Capitan, usage of third-party software in Disk Utility is no longer possible. As a workaround, Tuxera NTFS for Mac ships together with Tuxera Disk Manager to facilitate the format and maintenance of NTFS volumes in macOS. Currently the software supports 13 languages: Arabic, Simplified and Traditional Chinese, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish and Turkish. The software supports 64-bit kernels, including OS X El Capitan. It supports NTFS extended attributes and works with virtualization and encryption solutions including Parallels Desktop and VMware Fusion.|$|R
40|$|We {{introduce}} a new model for objects which can move around on a cellular grid. The model consists of two phases, the movement phase and the conflict resolution phase. In the movement part of the description objects specify their desired direction. The conflict, which occurs when alternative objects {{want to move to}} the same free cell, is resolved in the conflict resolution part. The cellular description language CDL was extended to CDL++ in order to describe moving objects. This extension is automatically converted into a two [...] phased CDL program. 1 Introduction In the cellular automaton (CA) model cells are located on a regular grid. The next state of a cell is computed by the local rule depending on the cell state and the cell states of the neighbour cells. All cells may execute their local rule in parallel, because they can act independently from each other. The locality principle of the CA means that a cell has <b>only</b> <b>read</b> <b>access</b> to its neighbours and <b>only</b> write <b>access</b> with [...] ...|$|R
40|$|This {{thesis is}} devoted to the {{development}} of methods and tools to design hybrid distributed data warehouses. The analysis of the existing research aimed at solving this problem has shown the lack of the combined logical and physical data distribution in the data warehouse, based on both the data characteristics and query statistics. The thesis proposes the introduction of the concept of multibase data warehouses, as well as their models and inter-level transitions. Logical data distribution is performed based on the data structuredness with the purpose to choose models to represent the source data. Physical data distribution combines placement of data among the nodes and routing data replication, both operations are provided based on the minimal cost criterion. Multibase data warehouse concept is introduced. Multibase data warehouses consist of three levels: the data level, nodes level and data stores level. The data may be stored in the data stores, which are subject to data storage optimization, and the data sources that store the data loaded in the data stores and provide an extra level of data redundancy to protect the data warehouse from data retrieval failures. The nodes are classified into the central nodes that hold geo-independent data, and regional ones that store geo-dependent data. The data stores in nodes are classified into the primary and secondary ones, with the primary stores used for read and write access, and the secondary ones ? for <b>read</b> <b>only</b> <b>access.</b> Based on the the concept of multibase data warehouses, the process of their building envisions application of hybrid distributed data warehousing an adaptive technology. It starts with the source analysis and then continues with the conceptual, logical and physical design phases. Conceptual design phase is done based on E/R model with the additional ?split entity? and ?merge entity? operations to support partly-structured data. To categorize the data into areas, relational model has been extended with ?the linked set building? operation. Logical design phase determines the logical model, which describes the entity-to-data-store-element and the link-to-data-store-relation correspondence. To decide which model of data store will be chosen, the structuredness is analyzed, so the data is classified into the structured, semi-structured and partly-structured data. The models to store data are chosen based on the structuredness class and minimal cost criteria. To load the data properly, the procedures of converting data from the models of sources to the model of warehouse and vice versa are described. Physical design phase envisions distribution of data between the nodes and routes data replication based on the minimal cost criteria as well. Minimal cost criteria used in this paper ensures accounting for the data placement, processing and replication cost. The data placement and replication configuration, which provides minimal cost are found using the modified genetic algorithm. This algorithm uses block-based crossover and mutation with ad hoc adjusted probabilities, as well as the preliminary stage of forming initial population based on the artificial bee colony (ABC) method, which provides for approaching the sought solution and reducing the overall time needed to find it. The Information technology for building distributed hybrid data warehouses which utilizes the proposed models and methods was developed. To implement this technology in public finance of Ukraine, the generic system of public finance management is proposed. It has the levels of data storage, processing and delivery. The multibase data warehouse and its data access services are used for the data storage. The data can be accessed using standard SQL notation, regardless of the node, where the data have been stored and the model represented, and the distributed data services provide for the data usage with the acceptable connection latency regardless of user location that helps to maintain query processing time within the preset limit. Processing of data is done by applications that use the data in the warehouse. Different applications may use the same or different data depending on the business processes and the data warehouse will adapt to the resulting data usage. The data are delivered through both the Web services and sites, as well as non-Web thin clients and thick clients due to the diversity of existing applications and new developments. The examples of Public Finance Management System, ?Transparent Budget? information system, as well as several e-learning and scientific resources demonstrate efficiency of the information technology due to the increase of query performance and decrease of total cost of storing and processing data in the warehouse. The software used for implementation of this information technology is Multibase Data Warehouse Management System (MDWMS), which consists of querying agents, database management agents, data placement and data warehouse description modules, MDWMS Configurator and Client applications. These modules have been designed to manage databases based on the proposed method and to support the environment for the user and applications access. User applications connect to the data through the software interface (MDWMS API) and the end users use MDW Client for direct data access. Having provided such access, user applications can use the data, regardless of their location and the actual data model, where they have been stored. The statistics is checked on a regular basis to find out whether the requirements to the data warehouse performance and reliability are met. If these requirements are not met, the repeated data warehouse building is performed based on the current data in the warehouse and data sources, query statistics, so that after such building has been completed, the set requirements will be met. ? ??????????????? ?????? ?????? ?????????? ??????-???????????? ?????? ?????????? ?????????????? ???????? ?????? ?????????? ???? ? ?????? ??????? ?????? ? ?????????? ?????????? ???????? ? ?????????. ??????????? ?????? ?????? ?????????? ?????????????? ???????? ?????? ?????????? ????, ?????????? ???????????? ??????? ???? ??????. ?????????? ?????????? ? ?????????????? ?????????? ?????????? ?????????????? ???????? ?????? ?????????? ????. ??????? ??????? ????????????? ???????? ??????, ??????????? ??????????????, ?????????? ? ?????????? ?????? ????? ???????? ? ???????? ???????????? ?????????. ??????? ?????????? ?????? ? ????????? ? ??????? ???????? ??????????? ?????? ? ????????, ? ????? ?????? ??????? ??? ????????????? ??????. ?????????? ?????? ????? ?????? ????????? ? ???????? ?????????? ?????? ???????????? ??? ?????? ???????? ????????? ? ????????????????? ????????????? ?????????. ?? ????????? ???????????? ??????? ? ??????? ??????????? ?????????????? ?????????? ?????????? ?????????????? ???????? ?????? ?????????? ????, ??????? ?????? ???????????? ??????? ??????. ????????? ?????????? ????????? ??? ?????????? ?????????????? ? ?????????????-????????????? ?????? ??? ???????????? ???????? ???????. ?????????? ????????? ???????????, ??? ??? ???????? ???????? ???????????...|$|E
40|$|Cellular Processing {{is based}} on the {{computational}} model cellular automata and includes an appropriate software environment. The model is massively parallel because all cells can compute their local rule in parallel. The paper gives an overview of the developed software and hardware support. The language CDL allows to describe complex rules with record states in a concise and readable form. CDL++ is an extension for moving objects with automatic conflict resolution. The CEPRA family of configurable coprocessors with FPGA technology was developed to significantly accelerate the computation. Compilers are available which transform CDL into C code or JAVA code for the software simulators. A hardware compiler synthesises logic code to be loaded into the FPGAs. I. CELLULAR PROCESSING Cellular Processing is an attractive and a simple massive parallel processing model based on cellular automata (CA). In a CA the cells are located in a regular grid. The cells have <b>only</b> <b>read</b> <b>access</b> to the state [...] ...|$|R
2500|$|... the {{interface}} provided for {{reading and writing}} the memory is different (NOR allows random-access for <b>reading,</b> NAND allows <b>only</b> page <b>access)</b> ...|$|R
5000|$|Hard disks {{and some}} USB-sticks. Supported file systems are FAT12, FAT16, FAT32 (long names support), NTFS (partially, <b>read</b> <b>only),</b> ext2/ext3/ext4 (partially, <b>read</b> <b>only),</b> XFS (partially, <b>read</b> <b>only)</b> and CDFS ...|$|R
40|$|The paper {{describes}} the principle structures of a Control Center with its Projects. The modeling {{of the reality}} {{is the next step}} towards a configuration management system. The reason for an own system instead of a COTS product is discussed together with the reason for an object oriented approach. To have a web based user interface within a server/client solution is after our opinion state of the art. The normal user is familiar with all kind of Web browsers and is not forced to learn a new user interface. The Configuration Management System also clearly distinguish between the different rules a user can have on application level. A user can be responsible for a subsystem but has at the same time <b>only</b> <b>read</b> <b>access</b> to other subsystems. And an other user class can <b>only</b> <b>read</b> in all subsystems. A central point is the Configuration Identification list - called CIDL - which allows a special view on some kind of information within the configuration database. If e. g. somebody is interested only in software items introduced since a time point she/he will get a list of the selected attributes. All the configuration items are hierarchical structured according to the users wish...|$|R
2500|$|Rabbi Rothstein {{also argued}} {{that only a few}} of the medieval {{commentators}} held that a woman could intrinsically read all the aliyot, that most held they could <b>read</b> <b>only</b> some and some major authorities held they could <b>read</b> <b>only</b> the last one. He argued that the authorities who held a woman could <b>read</b> <b>only</b> the last aliyah [...] "carry greater weight" [...] than the authorities who held they could read more: ...|$|R
5000|$|Mount the {{filesystem}} {{in either}} read write or <b>read</b> <b>only</b> mode. Explicitly defining a file system as rw can alleviate some problems in file systems that default to <b>read</b> <b>only,</b> {{as can be}} the case with floppies or NTFS partitions.|$|R
2500|$|... •Samuel Beckett—evident {{influence}} in 'Epilog: <b>Read</b> <b>Only</b> Memory.' ...|$|R
40|$|This {{paper will}} {{demonstrate}} the stored compiled macro {{facility in the}} multi-user clinical data management environment of Cancer and Leukemia Group B (CALGB). CALGB is a large cancer research cooperative group. Clinical data is gathered from hundreds of institutions {{and sent to the}} CALGB statistical center, where it is stored in a single database and processed by dozens of statisticians, programmers and data managers. At any given time CALGB coordinates more than 80 actively accruing clinical trials. This setting necessitates the need for streamlining data cleaning, data queries, and reports. The compiled stored macro facility permits all production level macros to be stored in one common location. In the multi-user setting it is crucial that all changes to the stored compiled macros in the macro catalog are authorized. The stored compiled macro facility permits a small group of users to control the content of the macro library, while a larger group of users has <b>only</b> <b>read</b> <b>access.</b> Step-by-step instructions on how to create and use compiled stored macros will be presented. Several examples from the CALGB statistical center will be used to demonstrate the flexibility and efficiency of the facility including multilevel macro programs which allow for easy customization of stored programs...|$|R
50|$|Sung <b>read</b> <b>only</b> the Bible and a daily newspaper.|$|R
50|$|Usenet {{support was}} <b>read</b> <b>only</b> because the server was anonymous.|$|R
50|$|A {{simple method}} of {{ensuring}} data provenance in computing is to mark a file as <b>read</b> <b>only.</b> This allows {{the user to}} view {{the contents of the}} file, but not edit or otherwise modify it. <b>Read</b> <b>only</b> can also in some cases prevent the user from accidentally or intentionally deleting the file.|$|R
5000|$|... #Subtitle level 3: An example <b>Read</b> <b>Only</b> right moving Turing machine ...|$|R
50|$|The {{cyberpunk}} adventure <b>Read</b> <b>Only</b> Memories depicts Sutro Tower from a distance.|$|R
5000|$|... /* This tells flex to <b>read</b> <b>only</b> one {{input file}} */%option noyywrap ...|$|R
5000|$|CE-160 7.6 KB <b>read</b> <b>only</b> {{memory module}} with CR2032 battery data backup ...|$|R
5000|$|The {{events of}} 2064: <b>Read</b> <b>Only</b> Memories take place from December 21-24 ...|$|R
5000|$|... {{this is a}} <b>read</b> <b>only</b> operation, {{does not}} pose any {{concurrency}} issues ...|$|R
