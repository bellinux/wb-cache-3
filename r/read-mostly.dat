43|0|Public
40|$|Abstract — DotSlash is an {{automated}} web hotspot rescue system. This paper presents DotSlash Qcache services that allow {{a web site}} to use on-demand distributed query result caching to greatly reduce the workload at <b>read-mostly</b> databases. DotSlash Qcache services complement DotSlash rescue services; together they provide a comprehensive solution to address different bottlenecks at multi-tier web sites. I...|$|E
30|$|Mordvinova et al. (2009) {{showed that}} RAID arrays of USB flash storage drives can be {{purchased}} for less cost, compared with SSDs or HDDs, but like SD cards they usually provide a poor performance for random write operations. Therefore, USB flash storage drives are a useful option mainly for <b>read-mostly</b> applications like storing the content of web servers and for CPU bound applications.|$|E
40|$|Many future {{applications}} for scalable shared-memory multiprocessors {{are likely to}} have large working sets that overflow secondary or tertiary caches. Two possible solutions to this problem are to add a very large cache called remote cache that caches remote data (NUMA-RC), or organize the machine as a cache-only memory architecture (COMA). This paper tries to determine which solution is best. To compare the performance of the two organizations for the same amount of total memory, we introduce a model of data sharing. The model uses three data sharing patterns: replication, <b>read-mostly</b> migration, and read-write migration. Replication data is accessed in <b>read-mostly</b> mode by several processors, while migration data is accessed largely by one processor at a time. For large working sets, the weight of the migration data largely determines whether COMA outperforms NUMA-RC. Ideally, COMA only needs to fit the replication data in its extra memory; the migration data will simply be s [...] ...|$|E
40|$|Recently, a ?column store? {{system called}} CStore has shown {{significant}} performance benefits by utilizing storage optimizations for a <b>read-mostly</b> query workload. The {{authors of the}} C-Store paper compared their optimized column store to a commercial row store RDBMS that is optimized for a mixture of reads and writes, which obscures the relative benefits of row and column stores. In this paper, we describe two storage optimizations for a row store architecture given a <b>read-mostly</b> query workload ? ?super tuples? and ?column abstraction. ? We implement both our optimized row store and C-Store in a common framework in order to perform an ?apples-to-apples? comparison of the optimizations in isolation and combination. We also develop a detailed cost model for sequential scans tobreak down time spent into three categories ? disk I/O, iteration cost, and local tuple reconstruction cost. We conclude that, while the C-Store system offers tremendous performance benefits for scanning {{a small fraction of}} columns from a table, our optimized row store provides disk storage savings, reduced sequential scan times, and low additional CPU overheads while requiring only evolutionary changes to a standard row store...|$|E
40|$|Cloudera Impala is a modern, {{open-source}} MPP SQL en-gine architected {{from the}} ground up for the Hadoop data processing environment. Impala provides low latency and high concurrency for BI/analytic <b>read-mostly</b> queries on Hadoop, not delivered by batch frameworks such as Apache Hive. This paper presents Impala from a user’s perspective, gives an overview of its architecture and main components and briefly demonstrates its superior performance compared against other popular SQL-on-Hadoop systems. 1...|$|E
40|$|Recently, a “column store ” {{system called}} C-Store has shown {{significant}} performance benefits by utilizing storage optimizations for a <b>read-mostly</b> query workload. The {{authors of the}} C-Store paper compared their optimized column store to a commercial row store RDBMS that is optimized for a mixture of reads and writes, which obscures the relative benefits of row and column stores. In this paper, we describe two storage optimizations for a row store architecture given a <b>read-mostly</b> query workload – “super tuples ” and “column abstraction. ” We implement both our optimized row store and C-Store in a common framework in order to perform an “applesto-apples” comparison of the optimizations in isolation and combination. We also develop a detailed cost model for sequential scans to break down time spent into three categories – disk I/O, iteration cost, and local tuple reconstruction cost. We conclude that, while the C-Store system offers tremendous performance benefits for scanning {{a small fraction of}} columns from a table, our optimized row store provides disk storage savings, reduced sequential scan times, and low additional CPU overheads while requiring only evolutionary changes to a standard row store...|$|E
40|$|Bitmap indices {{have become}} popular access methods for data {{warehouse}} applications and decision support systems with {{large amounts of}} <b>read-mostly</b> data. This paper could arrive a number of results such as; Bitmap Index highly improves the performance of Query Answering in Data Warehouses, It highly increases the efficiency of Complex Query processing through using bitwise operations (AND, OR). A prototype of Data Warehouse “STUDENTS DW ” has been built according to the conditions of W. Inomn of Data Warehouses. This prototype is built for student's information...|$|E
40|$|Although {{real-time}} {{operating systems}} and applications {{have been available}} for multicore systems for some years, shared-memory parallel systems still pose some severe challenges for real-time algorithms, particularly {{as the number of}} CPUs increases. These challenges can take the form of lock contention, memory contention, conflicts/restarts for lockless algorithms, as well as many others. One technology that has been recently added to the real-time arsenal is a preemptable implementation of read-copy update (RCU), which permits deterministic read-side access to <b>read-mostly</b> data structures, {{even in the face of}} concurrent updates. In some cases, updates may also be carried out in a deterministic manner. ...|$|E
40|$|Layered {{clustering}} offers cluster-like {{load balancing}} for unmodified NFS or CIFS servers. Read {{requests sent to}} a busy server can be offloaded to other servers holding replicas of the accessed files. This paper explores a key design question for this approach: which files should be replicated ? We find that the popular policy of replicating readonly files offers little benefit. A policy that replicates readonly portions of <b>read-mostly</b> files, however, implicitly coordinates with client cache invalidations and thereby allows almost all read operations to be offloaded. In a read-heavy trace, 75 % of all operations and 52 % of all data transfers can be offloaded...|$|E
40|$|DB 2 with BLU Acceleration deeply {{integrates}} innovative {{new techniques}} for defining and processing column-organized tables that speed <b>read-mostly</b> Business Intelligence queries by 10 to 50 times and improve compression by 3 to 10 times, compared to traditional row-organized tables, with-out {{the complexity of}} defining indexes or materialized views on those tables. But DB 2 BLU {{is much more than}} just a col-umn store. Exploiting frequency-based dictionary compres-sion and main-memory query processing technology from the Blink project at IBM Research- Almaden, DB 2 BLU per-forms most SQL operations – predicate application (even range predicates and IN-lists), joins, and grouping – on the compressed values, which can be packed bit-aligned so densely that multiple values fit in a register and can be pro...|$|E
40|$|Memory {{technologies}} {{are divided into}} two categories. The first category, nonvolatile memories, are traditionally used in read-only or <b>read-mostly</b> applications because of limited write endurance and slow write speed. These memories are derivatives of read only memory (ROM) technology, which includes erasable programmable ROM (EPROM), electrically-erasable programmable ROM (EEPROM), Flash, and more recent ferroelectric non-volatile memory technology. Nonvolatile memories are able to retain data {{in the absence of}} power. The second category, volatile memories, are random access memory (RAM) devices including SRAM and DRAM. Writing to these memories is fast and write endurance is unlimited, so they are most often used to store data that change frequently, but they cannot store data in the absence of power. Nonvolatile memory technologies with better future potential are FRAM, Chalcogenide, GMRAM, Tunneling MRAM, and Silicon-Oxide-Nitride-Oxide-Silicon (SONOS) EEPROM...|$|E
40|$|Read-copy update (RCU) allows lock-free {{read-only}} {{access to data}} structures that are concurrently modified on SMP systems. Despite the concurrent modifications, {{read-only access}} requires neither locks nor atomic instructions, and can often be written as if the data were unchanging, in a “CS 101 ” style. RCU is typically applied to <b>read-mostly</b> linked structures that the read-side code traverses unidirectionally. Previous work has shown no clear best RCU implementation for all measures of performance. This paper combines ideas from several RCU implementations {{in an attempt to}} create an overall best algorithm, and presents a RCU-based implementation of the System V IPC primitives, improving performance by more than an order of magnitude, while increasing code size by less than 5 % (151 lines). This implementation has been accepted into the Linux 2. 5 kernel...|$|E
40|$|Abstract—The {{problems}} of synchronization overhead, contention, and deadlock can pose serious challenges to those designing and implementing parallel programs. Therefore, many researchers have proposed parallel update disciplines that greatly reduce these problems in restricted but commonly occurring situations, for example, <b>read-mostly</b> data structures [7, 10, 11, 13, 18]. However, these proposals rely either on garbage collectors [10, 11], termination of all processes currently using the data structure [13], or expensive explicit tracking of all processes accessing the data structure [7, 18]. These mechanisms are inappropriate in many cases, such as within many operating-system kernels and server applications. This paper proposes a novel and extremely efficient mechanism, called read-copy update, and compares its performance {{to that of}} conventional locking primitives under conditions of both low and high contention in read-intensive data structures. Index Terms—Shared memory, mutual exclusion, reader-writer locking, performance, contention. ...|$|E
40|$|Existing {{techniques}} (e. g., RCU) {{can achieve}} good multi-core scaling for <b>read-mostly</b> data, but for update-heavy data structures only special-purpose techniques exist. This paper presents OpLog, a general-purpose library supporting good scalability for update-heavy data struc-tures. OpLog achieves scalability by logging each update in a low-contention per-core log; it combines logs only when required by a {{read to the}} data structure. OpLog achieves generality by logging operations without having to understand them, to ease application to existing data structures. OpLog can further increase performance if the programmer indicates which operations can be combined in the logs. An evaluation shows how to apply OpLog to three update-heavy Linux kernel data structures. Measurements on a 48 -core AMD server show that the result significantly improves {{the performance of the}} Apache web server and the Exim mail server under certain workloads. ...|$|E
40|$|OLAP {{technology}} is indispensable {{for the analysis}} of historical data. Using this technology for decision support requires fast access to aggregated data. To support this, we propose the materialization of aggregates in a multidimensional index-structure. Applying this materialization to the R -tree shows significant performance improvements for range queries that compute aggregates. To determine the speed-up analytically we suggest a model that estimates the performance gain achieved by adding materialized data to an already existing index-structure. Experiments show high accuracy of the presented models. 1 Introduction Large sets of multidimensional data are stored in data warehouses when used for decision support purposes. The logical data structure is a data cube, a conceptual model is described in [GL 97]. These data warehouse systems are <b>read-mostly</b> environments. Typically, the data is updated during some time when the system is not accessible for the user. There is a great nee [...] ...|$|E
40|$|Best-e↵ort {{hardware}} transactional memory (HTM) allows complex {{operations to}} execute atomically and in parallel, {{so long as}} hardware bu↵ers do not overflow, and conflicts are not encountered with concurrent operations. We describe a programming technique and compiler support to reduce both overflow and conflict rates by partitioning common operations into <b>read-mostly</b> (planning) and write-mostly (completion) operations, which then execute separately. The completion operation remains transactional; planning can often occur in ordinary code. High-level (semantic) atomicity for the overall operation is ensured by passing an application-specific validator object between planning and completion. Transparent composition of partitioned operations is made possible through fully-automated compiler support, which migrates all planning operations out of the parent transaction while respecting all program data flow and dependences. For both micro- and macro-benchmarks, experiments on IBM z-Series and Intel Haswell machines demonstrate that partitioning can lead to dramatically lower abort rates and higher scalability. ...|$|E
40|$|Bitmap {{indices are}} popular multi-dimensional data {{structures}} for accessing <b>read-mostly</b> data such as data warehouse (DW) applications, decision support systems (DSS) and {{on-line analytical processing}} (OLAP). One of their main strengths is that they provide good performance characteristics for complex adhoc queries and an efficient combination of multiple index dimensions in one query. Considerable research {{work has been done}} in the area of finite (and low) attribute cardinalities. However, additional complexity is imposed on the design of bitmap indices for high cardinality or even non-discrete attributes, where different optimisation techniques than the ones proposed so far have to be applied. In this paper we discuss the design and implementation of bitmap indices for High-Energy Physics (HEP) analysis, where the potential search space consists of hundreds of independent dimensions. A single HEP query typically covers 10 to 100 dimensions out of the whole search space. I [...] ...|$|E
40|$|High Energy Physics collaborations {{are faced}} with {{exponentially}} growing data sets due to recent technological advances and need of better statistics for rare processes. The collaborations are large and geographically distributed along with their computing and data storage resources. The Nile Project, a collaborative effort between physics and computer science is building distributed computing tools to address the physics demands of the next millenium. One aspect of Nile is to provide efficient and highly available data services. In this article, we present a high-performance data server that leverages from typical physics analysis use patterns: <b>read-mostly</b> statistics collection on a subset of object attributes. The simple but flexible design of the server incorporates just enough intelligence to perform useful optimizations without slowing down the critical path to data access. Among {{the features of the}} data server are aggressive prefetching, support for horizontal and vertic [...] ...|$|E
40|$|Data {{warehouses}} {{are used}} to store large amounts of data. This data is often used for On-Line Analytical Processing (OLAP). Short response times are essential for on-line decision support. Common approaches to reach this goal in <b>read-mostly</b> environments are the precomputation of materialized views {{and the use of}} index structures. In this paper, a framework is presented to evaluate different index structures analytically depending on nine parameters for the use in a data warehouse environment. The framework is applied to four different index structures to evaluate which structure works best for range queries. We show that all parameters influence the performance. Additionally, we show why bitmap index structures use modern disks better than traditional tree structures and why bitmaps will supplant the tree based index structures in the future. 1 Introduction Data warehouse and OLAP applications differ very much from the traditional database applications. Traditional da [...] ...|$|E
40|$|The <b>read-mostly</b> {{environment}} of data warehousing {{makes it possible}} to use more complex indexes to speed up queries than in situations where concurrent updates are present. The current paper presents a short review of current indexing technology, including row-set representation by Bitmaps, and then introduces two approaches we call Bit-Sliced indexing and Projection indexing. A Projection index basically materializes all values of a column in RID order, and a Bit-Sliced index essentially takes an orthogonal bit-by-bit view of the same data. While some of these concepts started with the MODEL 204 product, and both Bit-Sliced and Projection indexing are now fully realized in Sybase IQ, this is the first rigorous examination of such indexing capabilities in the literature. We compare algorithms that become feasible with these variant index types against algorithms using more conventional indexes. The analysis demonstrates important performance advantages for variant indexes in some types [...] ...|$|E
40|$|Bitmap {{indexing}} {{has been}} {{touted as a}} promising approach for processing complex adhoc queries in <b>read-mostly</b> environments, like those of decision support systems. Nevertheless, only few possible bitmap schemes have been proposed {{in the past and}} very {{little is known about the}} space-time tradeoff that they offer. In this paper, we present a general framework to study the design space of bitmap indexes for selection queries and examine the disk-space and time characteristics that the various alternative index choices offer. In particular, we draw a parallel between bitmap indexing and number representation in different number systems, and define a space of two orthogonal dimensions that captures a wide array of bitmap indexes, both old and new. Within that space, we identify (analytically or experimentally) the following interesting points: (1) the time-optimal bitmap index; (2) the space-optimal bitmap index; (3) the bitmap index with the optimal space-time tradeoff (knee); and (4) the ti [...] ...|$|E
40|$|Byzantine {{fault-tolerant}} (BFT) replication {{has enjoyed}} a series of performance improvements, but remains costly due to its replicated work. We eliminate this cost for <b>read-mostly</b> workloads through Prophecy, a system that interposes itself between clients and any replicated service. At Prophecy’s core is a trusted sketcher component, designed to extend the semi-trusted load balancer that mediates access to an Internet service. The sketcher performs fast, load-balanced reads when results are historically consistent, and slow, replicated reads otherwise. Despite its simplicity, Prophecy provides {{a new form of}} consistency called delay-once consistency. Along the way, we derive a distributed variant of Prophecy that achieves the same consistency but without any trusted components. A prototype implementation demonstrates Prophecy’s high throughput compared to BFT systems. We also describe and evaluate Prophecy’s ability to scale-out to support large replica groups or multiple replica groups. As Prophecy is most effective when state updates are rare, we finally present a measurement study of popular websites that demonstrates a large proportion of static data. ...|$|E
40|$|Coherent wide-area data caching {{can improve}} the {{scalability}} and responsiveness of distributed services such as wide-area file access, database and directory services, and content distribution. However, distributed services differ widely {{in the frequency of}} read/write sharing, the amount of contention between clients for the same data, and their ability to make tradeoffs between consistency and availability. Aggressive replication enhances the scalability and availability of services with <b>read-mostly</b> data or data that need not be kept strongly consistent. However, for applications that require strong consistency of writeshared data, you must throttle replication to achieve reasonable performance. We have developed a middleware data store called Swarm designed to support the widearea data sharing needs of distributed services. To support the needs of diverse distributed services, Swarm provides: (i) a failure-resilient proximity-aware data replication mechanism that adjusts the replication hierarchy based on observed network characteristics and node availability, (ii) a customizable consistency mechanism that allows applications to specify allowable consistency-availability tradeoffs, and (iii) a contention-aware cachin...|$|E
40|$|In recent years, column stores (or C-stores for short) {{have emerged}} as a novel {{approach}} to deal with <b>read-mostly</b> data warehousing applications. Experimental evidence suggests that, for certain types of queries, the new features of C-stores result in orders of magnitude improvement over traditional relational engines. At the same time, some C-store proponents argue that C-stores are fundamentally different from traditional engines, and therefore their benefits cannot be incorporated into a relational engine short of a complete rewrite. In this paper we challenge this claim and show {{that many of the}} benefits of C-stores can indeed be simulated in traditional engines with no changes whatsoever. We then identify some limitations of our ?pure-simulation? approach for the case of more complex queries. Finally, we predict that traditional relational engines will eventually leverage most of the benefits of C-stores natively, as is currently happening in other domains such as XML data. Comment: CIDR 200...|$|E
40|$|Bitmap {{indices are}} {{efficient}} multi-dimensional index data structures for handling complex adhoc queries in <b>read-mostly</b> environments. They {{have been implemented}} in several commercial database systems but are only well suited for discrete attribute values which are very common in typical business applications. However, many scientific applications usually operate on floating point numbers and cannot {{take advantage of the}} optimisation techniques offered by current database solutions. We thus present a novel algorithm called Generic RangeEval for processing one-sided range queries over floating point values. In addition, we present a cost model for predicting the performance of bitmap indices for high-dimensional search spaces. We verify our analytical results by a detailed experimental study, and show that the presented bitmap evaluation algorithm scales well also for high-dimensional search spaces requiring only a fairly small index. Because of its simple arithmetic structure, the cost model could easily be integrated into a query optimiser for deciding whether the current multi-dimensional query shall be answered by means of a bitmap index or better by sequentially scanning the data values, without using an index at all. (19 refs) ...|$|E
40|$|Traditional operating-system locking designs tend {{to either}} be very complex, result in poor concur-rency, or both. These {{traditional}} locking designs {{fail to take}} advantage of the event-driven nature of op-erating systems, which process many small, quickly completed units of work, in contrast to CPU-bound software such as scientic applications. This event-driven nature can often be exploited by splitting up-dates into the two phases: 1) carrying out enough of each update for new operations to see the new state, while still allowing existing operations to proceed on the old state, then: 2) completing the update after all active operations have completed. Common-case code can then proceed without disabling interrupts or acquiring any locks to protect against the update code, which simplies locking protocols, improves uniprocessor performance, and increases scalability. Examples of the application of these techniques in-clude maintaining <b>read-mostly</b> data structures, such as routing tables, avoiding the need for existence locks (and hence avoiding locking hierarchies with the attendant deadlock issues), and dealing with un-usual situations like module unloading. ...|$|E
40|$|There {{has been}} renewed {{interest}} in column-oriented database architectures in recent years. For <b>read-mostly</b> query workloads such as those found in data warehouse and decision support applications, ``column-stores'' {{have been shown to}} perform particularly well relative to ``row-stores. '' In order for column-stores to be readily adopted as a replacement for row-stores, however, they must present the same interface to client applications as do row stores, which implies that they must output row-store-style tuples. Thus, the input columns stored on disk must be converted to rows {{at some point in the}} query plan, but the optimal point at which to do the conversion is not obvious. This problem can be considered as the opposite of the projection problem in row-store systems: while row-stores need to determine where in query plans to place projection operators to make tuples narrower, column-stores need to determine when to combine single-column projections into wider tuples. This paper describes a variety of strategies for tuple construction and intermediate result representations and provides a systematic evaluation of these strategies...|$|E
40|$|The Blink project’s ambitious goal is {{to answer}} all Business Intelligence (BI) queries in mere seconds, {{regardless}} of the database size, with an extremely low total cost of ownership. Blink is a new DBMS aimed primarily at <b>read-mostly</b> BI query processing that exploits scale-out of commodity multi-core processors and cheap DRAM to retain a (copy of a) data mart completely in main memory. Additionally, it exploits proprietary compression technology and cache-conscious algorithms that reduce memory bandwidth consumption and allow most SQL query processing to be performed on the compressed data. Blink always scans (portions of) the data mart in parallel on all nodes, without using any indexes or materialized views, and without any query optimizer to choose among them. The Blink technology has thus far been incorporated into two IBM accelerator products generally available since March 2011. We are now working on {{the next generation of}} Blink, which will significantly expand the “sweet spot ” of the Blink technology to much larger, disk-based warehouses and allow Blink to “own ” the data, rather than copies of it. ...|$|E
40|$|Abstract — DotSlash is an {{automated}} web hotspot rescue system, which allows different web sites {{to form a}} mutual-aid community, and use spare capacity in the community to relieve web hotspots experienced by any individual site. DotSlash rescue services enable a web site to build an adaptive distributed web server system on the fly and replicate application programs dynamically, which relieve a spectrum of bottlenecks ranging from access network bandwidth to web servers and application servers. This paper presents DotSlash Qcache services that allow a web site to use on-demand distributed query result caching to greatly reduce the workload at <b>read-mostly</b> databases. The novelty of this work is that our query result caching is on demand and operated based on load conditions, which offers good data consistency for normal load and good scalability with relaxed data consistency under heavy load. DotSlash Qcache services complement DotSlash rescue services; together they provide a comprehensive solution to address different bottlenecks at multitier web sites. Experiments show that using DotSlash a web site can increase its maximum request rate supported {{by a factor of}} 10 for the RUBBoS read-only mix. I...|$|E
40|$|Part 6 : Replication and CachingInternational audienceCaching is an {{important}} technique in scaling storage for high-traffic web applications. Usually, building caching mechanisms involves significant effort from the application developer to maintain and invalidate data in the cache. In this work we present CacheGenie, a caching middleware which {{makes it easy for}} web application developers to use caching mechanisms in their applications. CacheGenie provides high-level caching abstractions for common query patterns in web applications based on Object-RelationalMapping (ORM) frameworks. Using these abstractions, the developer does {{not have to worry about}} managing the cache (e. g., insertion and deletion) or maintaining consistency (e. g., invalidation or updates) when writing application code. We design and implement CacheGenie in the popular Django web application framework, with PostgreSQL as the database backend and memcached as the caching layer. To automatically invalidate or update cached data, we use triggers inside the database. CacheGenie requires no modifications to PostgreSQL or memcached. To evaluate our prototype, we port several Pinax web applications to use our caching abstractions. Our results show that it takes little effort for application developers to use CacheGenie, and that CacheGenie improves throughput by 2 - 2. 5 × for <b>read-mostly</b> workloads in Pinax...|$|E
40|$|Abstract: The <b>read-mostly</b> {{environment}} of data warehousing {{makes it possible}} to use more complex indexes to speed up queries than in situations where concurrent updates are present. The current paper presents a short review of current indexing technology, including row-set repre-sentation by Bitmaps, and then introduces two approaches we call Bit-Sliced indexing and Projection indexing. A Projection index basically materializes all values of a column in RID or-der, and a Bit-Sliced index essentially takes an orthogonal bit-by-bit view of the same data. While some of these concepts started with the MODEL 204 product, and both Bit-Sliced and Projection indexing are now fully realized in Sybase IQ, this is the first rigorous examination of such indexing capabilities in the literature. We compare algorithms that become feasible with these variant index types against algorithms using more conventional indexes. The analysis demonstrates important performance advantages for variant indexes in some types of SQL aggre-gation, predicate evaluation, and grouping. The paper concludes by introducing a new method whereby multi-dimensional Group By queries, reminiscent of OLAP or Datacube queries but with more flexibility, can be very efficiently performed. 1. Introduct io...|$|E
40|$|Abstract. Caching is an {{important}} technique in scaling storage for high-traffic web applications. Usually, building caching mechanisms involves significant effort from the application developer to maintain and invalidate data in the cache. In this work we present CacheGenie, a caching middleware which {{makes it easy for}} web application developers to use caching mechanisms in their applications. CacheGenie provides high-level caching abstractions for common query patterns in web applications based on Object-Relational Mapping (ORM) frameworks. Using these abstractions, the developer does {{not have to worry about}} managing the cache (e. g., insertion and deletion) or maintaining consistency (e. g., invalidation or updates) when writing application code. We design and implement CacheGenie in the popular Django web application framework, with PostgreSQL as the database backend and memcached as the caching layer. To automatically invalidate or update cached data, we use triggers inside the database. CacheGenie requires no modifications to PostgreSQL or memcached. To evaluate our prototype, we port several Pinax web applications to use our caching abstractions. Our results show that it takes little effort for application developers to use CacheGenie, and that CacheGenie improves throughput by 2 – 2. 5 × for <b>read-mostly</b> workloads in Pinax. ...|$|E
40|$|This paper {{describes}} TidyFS, {{a simple}} and small distributed file system that provides the abstractions necessary for data parallel computations on clusters. In recent {{years there has been}} an explosion of interest in computing using clusters of commodity, shared nothing computers. Frequently the primary I/O workload for such clusters is generated by a distributed execution engine such as MapReduce, Hadoop or Dryad, and is high-throughput, sequential, and <b>read-mostly.</b> Other large-scale distributed file systems have emerged to meet these workloads, notably the Google File System (GFS) and the Hadoop Distributed File System (HDFS). TidyFS differs from these earlier systems mostly by being simpler. The system avoids complex replication protocols and read/write code paths by exploiting properties of the workload such as the absence of concurrent writes to a file by multiple clients, and the existence of end-to-end fault tolerance in the execution engine. We describe the design of TidyFS and report some of our experiences operating the system over the past year for a community of a few dozen users. We note some advantages that stem from the system’s simplicity and also enumerate lessons learned from our design choices that point out areas for future development. ...|$|E
40|$|Network file systems offer a powerful, {{transparent}} interface for accessing remote data. Unfortunately, {{in current}} network file systems like NFS, clients fetch {{data from a}} central file server, inherently limiting the system’s ability to scale to many clients. While recent distributed (peer-topeer) systems have managed to eliminate this scalability bottleneck, they are often exceedingly complex and provide non-standard models for administration and accountability. We present Shark, a novel system that retains {{the best of both}} worlds—the scalability of distributed systems with the simplicity of central servers. Shark is a distributed file system designed for largescale, wide-area deployment, while also providing a dropin replacement for local-area file systems. Shark introduces a novel cooperative-caching mechanism, in which mutually-distrustful clients can exploit each others ’ file caches to reduce load on an origin file server. Using a distributed index, Shark clients find nearby copies of data, even when files originate from different servers. Performance results show that Shark can greatly reduce server load and improve client latency for read-heavy workloads both in the wide and local areas, while still remaining competitive for single clients in the local area. Thus, Shark enables modestly-provisioned file servers to scale to hundreds of <b>read-mostly</b> clients while retaining traditional usability, consistency, security, and accountability. ...|$|E
40|$|Caching is an {{important}} technique in scaling storage for high-traffic web applications. Usually, building caching mechanisms involves significant effort from the application developer to maintain and invalidate data in the cache. In this work we present CacheGenie, a caching middleware which {{makes it easy for}} web application developers to use caching mechanisms in their applications. CacheGenie provides high-level caching abstractions for common query patterns in web applications based on Object-RelationalMapping (ORM) frameworks. Using these abstractions, the developer does {{not have to worry about}} managing the cache (e. g., insertion and deletion) or maintaining consistency (e. g., invalidation or updates) when writing application code. We design and implement CacheGenie in the popular Django web application framework, with PostgreSQL as the database backend and memcached as the caching layer. To automatically invalidate or update cached data, we use triggers inside the database. CacheGenie requires no modifications to PostgreSQL or memcached. To evaluate our prototype, we port several Pinax web applications to use our caching abstractions. Our results show that it takes little effort for application developers to use CacheGenie, and that CacheGenie improves throughput by 2 - 2. 5 × for <b>read-mostly</b> workloads in Pinax. ACM/IFIP/USENIX 12 th International Middleware Conference, Lisbon, Portugal, December 12 - 16, 2011. ProceedingsQuanta Computer (Firm...|$|E
40|$|Databases and {{knowledge}} representation both have decades of history but to date {{exchange of ideas}} and techniques between these disciplines has been limited. The intuition {{that there would be}} value in greater cooperation has not failed to occur to researchers on either side, after all, both sides deal with data. From this, we have seen deductive databases emerge, as well as more recently ‘ database friendly ’ profiles of OWL. In this position paper we will examine what, in the most concrete terms, is needed in order to bring leading edge database technology together with expressive querying and reasoning. This draws on our experience in building Virtuoso, one of today’s leading RDF stores. Following this, we argue for the creation of benchmarks and challenges that in fact do reflect reality and facilitate open and fair comparison of products and technologies. Data integration is often mentioned as the motivating use case for RDF. Database research has {{over the past few years}} produced great advances for business intelligence, i. e. complex queries and <b>read‐mostly</b> workloads. These advances are typified by compressed columnar storage and architecture‐conscious execution models, mostly based on the idea of always processing multiple sets of values in each operation (vectoring). With these techniques, raw performance with relatively simple schemas an...|$|E
