1|33|Public
40|$|Venous {{thromboembolism}} {{that cause}} blood clotting in blood vessels, prevent blood circulation, depending {{on changes in}} {{one or more of the}} coagulation factors II, VII, IX and X. Patients who have had a blood clot or cardiovascular diseases are treated with oral anti-vitamin K (Warfarin®) to reducing and prevent relapse. Warfarin is also used as a preventive treatment before the disease. An overdose of Warfarin® may cause bleeding-complications and low dose cause blood clotting. The dosage of the drug is controlled by measuring prothrombin in plasma. The aim of this study was to investigate if prothrombin-complex value changes due to <b>re-spinning</b> and re-analysis after six hours. Fitty whole blood samples from warfarin-treated patients were divided into three subgroups, those with protrombinkomplex-values of 2 - 4 (n= 20), > 4 (n= 15) and < 2 (n= 15). The samples were centrifugated and measured (Method A), re-centrifugated and measured (Method B) or re-analysed after six hours (Method C). All results were compared in a Bland-Altman plot as follows: Method B vs. Method A and Method C vs. Method A. The scatter graph yielded a strong correlation between Method A and Method B (R 2 = 0. 9984) and Method A and Methods C (R 2 = 0. 9977). The results from t-test showed a significance level (p< 0. 001) for both analyses (statistical significance=p< 0. 05). In this study we showed that prothrombin complex value ware stable after re-centrifugation and re-measurement after six hours. Statistical calculations yielded a strong correlation between the methods (A, B, C), and there was no significance difference between the methods...|$|E
50|$|Hardware problems: Sporadic {{hardware}} failures start {{showing up}} in the field due to the new effect. Postmanufacturing redesign and hardware <b>re-spins</b> are required to get the chip to function.|$|R
50|$|Last release: Fedora 12 Re-spins-20100303 {{released}}, March 19, 2010. These <b>Re-Spin</b> ISOs {{are based}} on the officially released Fedora 12 installation media and include all updates released as of March 3, 2010.|$|R
50|$|Playtech {{signed on}} for the {{development}} of a 40-line branded slot game based on Fox Broadcasting Company's animated sitcom, American Dad. Released online in May 2017, Stan Smith, Francine, Hayley, Klaus and Roger are featured along with special game bonuses including <b>re-spins,</b> free spins and 'Wheels and the Legman'.|$|R
50|$|Although {{usually a}} barnacle-implemented change is {{incorporated}} into a new fabrication cycle circuit before production, occasionally there are final-assembly barnacles. In such cases it is determined to be less expensive to add a barnacle to a final, shipping product rather than <b>re-spin</b> the circuit to ship without these interventions left in place.|$|R
50|$|Development Cost: Development cost of 90-nm ASIC/SoC design tape-out {{is around}} $20 million, with a mask set costing over $1 million alone. Development costs of 45-nm designs are {{expected}} to top $40 million. With increasing cost of mask sets, and the continuous decrease of IC size, minimizing the number of <b>re-spins</b> {{is vital to the}} development process.|$|R
50|$|Running a SoC {{design on}} FPGA {{prototype}} is a reliable {{way to ensure}} that it is functionally correct. This is compared to designers only relying on software simulations to verify that their hardware design is sound. About a third of all current SoC designs are fault-free during first silicon pass, with nearly half of all <b>re-spins</b> caused by functional logic errors. A single prototyping platform can provide verification for hardware, firmware, and application software design functionality before the first silicon pass.|$|R
5000|$|In {{the film}} ABCs of Death 2, 3 {{characters}} are seen playing Russian roulette {{in the scene}} [...] "R is for Roulette." [...] They play a variant where they do not <b>re-spin</b> the chamber between pulls so {{one of them would}} certainly get the bullet and avoid being captured by the Nazi regime. The [...] "winner" [...] (who is certain to get the bullet) decides to shoot his lover in an act of mercy before they are all eventually captured.|$|R
50|$|If {{the wheel}} lands on BUST, the chef forfeits their {{winnings}} and leaves with nothing. On PUSH, the chef earns no additional money and leaves with their original winnings. Landing on any ADD $XXXX space adds that amount to their winnings. The ? Space adds a mystery amount to the chef's winnings, {{up to an}} additional $10,000. A DOUBLE space doubles the winnings of the chef, and the JACKPOT space triples the chef's winnings, up to a possible top winnings of $30,000. SPIN AGAIN forces the chef to <b>re-spin</b> the wheel.|$|R
40|$|Next-generation {{nano-scale}} RF-IC designs have {{an unprecedented}} complexity and performance that will inevitably lead to costly <b>re-spins</b> {{and loss of}} market opportunities. In order to cope with this, {{the aim of the}} CHAMELEON RF project is to develop methodologies and prototype tools for a comprehensive and highly accurate analysis of complete functional IC blocks. These blocks operate at RF frequencies of up to 60 GHz. The results achieved in the CHAMELEON RF project are presented {{on the basis of the}} simulation of a real-life test case, an LNA for 24 GHz car radar applications, compared to measurements...|$|R
40|$|Since pre-silicon {{functional}} verification {{is insufficient to}} detect all design errors, <b>re-spins</b> are often needed due to malfunctions that escape into the silicon. This paper presents an automated software solution {{to analyze the data}} collected during silicon debug. The proposed methodology analyzes the test sequences to detect suspects in both the spatial and the temporal domain. A set of software debug techniques are proposed to analyze the acquired data from the hardware testing and provide suggestions for the setup of the test environment in the next debug session. A comprehensive set of experiments demonstrate its effectiveness in terms of run-time and resolution. 1...|$|R
40|$|Producing a {{functionally}} correct {{integrated circuit}} {{is becoming increasingly}} difficult. No matter how careful a designer is, {{there will always be}} integrated circuits that are fabricated, but do not operate as expected. Providing a means to effectively debug these integrated circuits is vital to help pin-point problems and reduce the number of <b>re-spins</b> required to create a correctly-functioning chip. In this paper, we show that programmable logic cores (PLCs) and flexible networks can provide this debugging capability. We present an architecture and example implementation. We show that the area overhead of this proposed architecture would be well below 10 % for many target ICs. 1...|$|R
50|$|Reluctantly, {{believing that}} any other {{similarly}} senescent star will offer no better option, the people accede to the new captain's order to land on Earth {{despite the lack of}} an atmosphere. However, this is not a haphazard decision by the new captain, who is an astrophysicist. He is convinced that there is a remnant atmosphere frozen on the dark side, and uses the ship's drive to begin to <b>re-spin</b> Earth. (The drive really is powerful enough to achieve this within a reasonable timescale, if only the planet's crust can withstand the seismic stresses without catastrophic earthquakes.) This will, he hopes, convert this frozen gas back to a breathable atmosphere.|$|R
40|$|In {{this paper}} we present {{the design of}} a SoC {{baseline}} platform with a Leon 2 CPU. An Advanced Encryption Standard (AES) module and a reconfigurable core form the IP blocks that are attached to the SoC through AMBA bus. The reconfigurable core is inserted into the design using tools developed by DAFCA, Inc. (Design Automation for Flexible Chip Architectures) for post-silicon debugging and verification. Hence, a <b>re-spin</b> may be avoided and the time-to-market will be reduced. SoC is a major revolution in IC design where the whole functionality of a system is placed on a single chip. Its advantages include high performance, shorter design cycle time and space efficiency, whereas the challenges includ...|$|R
40|$|This paper {{presents}} a cell-based modeling and design platform for high-frequency analog ICs to shorten design cycle {{time and to}} minimize the risk for mask <b>re-spin.</b> Based on a pre-characterized analog sub-circuit cell library, which contains not only active devices and passive components but also routing interconnects. This methodology systematically alleviates modeling inaccuracy at high frequencies due to the difference in the layout between device test structures and actual circuit implementation. By exploiting the modularity in analog circuits at the sub-circuit level, the proposed design platform achieves a balance between design flexibility and modeling accuracy compared. The macro modeling techniques the sub-circuit cells will be described along with measurement results from a characterization test chip. © 2008 IEEE...|$|R
40|$|Overheating {{has been}} {{acknowledged}} as a ma-jor issue in testing complex SOCs. Several power constrained system-level DFT solutions (power con-strained test scheduling) have been recently proposed to tackle this problem. However, {{as it will}} be shown in this paper, imposing a chip-level maximum power constraint doesn’t necessarily avoid local overheating due to the non-uniform distribution of power across the chip. This paper proposes a new approach for dealing with overheating during test, by embedding thermal awareness into test scheduling. The proposed approach facilitates rapid generation of thermal-safer test schedules without requiring time-consuming ther-mal simulations. This is achieved by employing a low-complexity test session thermal model used to guide the test schedule generation algorithm. This approach reduces {{the chances of a}} design <b>re-spin</b> due to poten-tial overheating during test. 1...|$|R
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceOverheating has been {{acknowledged}} as {{a major issue in}} testing complex SOCs. Several power constrained system-level DFT solutions (power constrained test scheduling) have recently been proposed to tackle this problem. However, as it will be shown in this paper, imposing a chip-level maximum power constraint doesn't necessarily avoid local overheating due to the non-uniform distribution of power across the chip. This paper proposes a new approach for dealing with overheating during test, by embedding thermal awareness into test scheduling. The proposed approach facilitates rapid generation of thermal-safer test schedules without requiring time-consuming thermal simulations. This is achieved by employing a low-complexity test session thermal model used to guide the test schedule generation algorithm. This approach reduces the chances of a design <b>re-spin</b> due to potential overheating during test...|$|R
40|$|International audienceLaser {{attacks are}} an {{effective}} threat against secure integrated circuits, {{due to their}} capability to inject very precise hardware faults. Evaluating the effect of such attacks from RTL descriptions provides designers a means to increase the security level of an IC, early in the design stage and without the need to perform multiple design <b>re-spins.</b> An RTL laser fault model that attempts to model the locality of laser attacks, early in the design flow, has already been proposed in our previous works. The current work presents detailed results on the validation of this model, with respect to layout information for multiple designs. Furthermore we perform a statistical analysis on the RTL predictions, in order to calculate {{the percentage of the}} fault space generated using only RTL information that actually corresponds to local faults according to layout information...|$|R
40|$|SoC (System on a Chip) is a {{powerful}} solution to increase system performance and to reduce system cost, in particular for digital consumer products. SoCs for DVD, DTV, DSC, DVC, and cellular phone are current main LSI products for the digital consumer market. However current SoC has tough issues. 1. 1 Increase of development time and cost System complexity has increased along with increase of transistor number. Perfect system verification {{in a short time}} is vital. Furthermore physical level verification has become tough, due to the signal integrity and reliability issue. Mask cost becomes much expensive. Reduction of <b>re-spin</b> is needed. These increase the development time and cost. 1. 2 Increase of kind of devices and processes Current scaled CMOS device and process has tough tradeoffs, not every performance will increase with scaling. The most seriou...|$|R
40|$|Next-generation {{nano-scale}} RFIC designs have {{an unprecedented}} complexity and performance that will inevitably lead to costly <b>re-spins</b> {{and loss of}} market opportunities. In order to cope with this, efficient and accurate models of interconnects, integrated inductors, the substrate and devices, together with their mutual interactions, need to be developed. The key idea is that integrated devices {{can no longer be}} treated in isolation as the EM interactions due to proximity effects are becoming more relevant in the behavior of the complete system. EM simulations must also address these interactions, so new procedures and models able to be included in coupled simulation must be developed. But these simulations may become very expensive as the complexity of the system increases, so model order reduction techniques able to treat these coupling effects are necessary in order to obtain a better performance. In this work some solutions for efficient simulation of such problems are introduced...|$|R
40|$|Software and {{hardware}} systems are often built without detailed documentation. The correctness {{of these systems}} can only be verified {{as well as the}} specifications are written. The lack of sufficient specifications often leads to misses of critical bugs, design <b>re-spins,</b> and time-to-market slips. In this paper, we address this problem by mining specification dynamically from simulation traces. Given an execution trace, we mine recurring temporal behaviors in the trace that match a set of pattern templates. Subsequently, we synthesize them into complex patterns by merging events in time and chaining the patterns using inference rules. We specifically designed our algorithm to make it highly efficient and meaningful for digital circuits. In addition, we propose a pattern-mining diagnosis framework where specifications mined from error-labeled traces are used to automatically pinpoint the sources of error. In this work, we focus on traces from digital circuits, but any ordered trace of events is amenable to this analysis. We demonstrate the effectiveness of our approach on industrial-size examples. ...|$|R
40|$|Functional {{errors and}} bugs {{inadvertently}} introduced at the RTL {{stage of the}} design process {{are responsible for the}} largest fraction of silicon IC <b>re-spins.</b> Thus, comprehensive func- tional verification is the key to reduce development costs and to deliver a product in time. The increasing demands for verification led to an increase in FPGA-based tools that perform emulation. These tools can run at much higher operating frequencies and achieve higher coverage than simulation. However, an important pitfall of the FPGA tools is that they suffer from limited internal signal observability, as only a small and preselected set of signals is guided towards (embedded) trace buffers and observed. This paper proposes a dynamically reconfigurable network of multiplexers that significantly enhance the visibility of internal signals. It allows the designer to dynamically change the small set of internal signals to be observed, virtually enlarging the set of observed signals significantly. These multiplexers occupy minimal space, as they are implemented by the FPGA’s routing infrastructure...|$|R
40|$|Effective system {{verification}} requires good specifications. The lack {{of sufficient}} specifications {{can lead to}} misses of critical bugs, design <b>re-spins,</b> and time-to-market slips. In this paper, we present a new technique for mining temporal specifications from simulation or execution traces of a digital hardware design. Given an execution trace, we mine recurring temporal behaviors in the trace that match a set of pattern templates. Subsequently, we synthesize them into complex patterns by merging events in time and chaining the patterns using inference rules. We specifically designed our algorithm to make it highly efficient and meaningful for digital circuits. In addition, we propose a pattern-mining diagnosis framework where specifications mined from correct and erroneous traces are used to automatically localize an error. We demonstrate the effectiveness of our approach on industrial-size examples by mining specifications from traces of over a million cycles {{in a few minutes}} and use them to successfully localize errors of different types to within module boundaries...|$|R
40|$|This paper presents, for {{the first}} time, a {{cell-based}} modeling and design platform for RFICs aiming to shorten design cycle time by eliminating iterations between schematic and post-layout simulations and {{to minimize the risk}} for costly mask <b>re-spin.</b> Based on a pre-characterized RF sub-circuit cell library, which contains not only active devices and passive components but also routing interconnects, this methodology systematically alleviates the common RF model inaccuracy due to layout discrepancies between actual circuits and device model test structures. By exploiting the modularity in RF circuits at the sub-circuit level, the proposed design platform achieves a balance between circuit design flexibility and device model accuracy compared to the conventional approach of using precharacterizing single transistors. This paper describes the implementation of the parameterized sub-circuit cell layout and the macro modeling techniques for a 0. 13 -μm CMOS RF sub-circuit cell library. Measurement results from a characterization test chip for validating the macro circuit models are presented...|$|R
40|$|This paper {{addresses}} the challenges associated with RF system design and verification {{in the presence}} of detailed circuit-level impairments and physical implementation parasitics. Distortion, compression, noise, memory effects, {{as well as a number}} of other non-ideal effects, can dramatically degrade system performance. Many of these effects are difficult and expensive to extract and model at the system level. Once these RF systems are physically implemented, signals across the chip through the package and board are distorted and delayed by interconnect parasitics. In addition, disparate platforms for RF system, circuit, IC, and package/PCB design and verification limit the designer's ability to deal concurrently with these effects across these domains. Poor tool integration and the inability to take these effects into consideration early in the design cycle are responsible for a large number of expensive product development <b>re-spins</b> and delays. Solutions to some of these challenges are emerging and will be discussed. Other challenges are still only understood as proposed research topics. Ideas for their potential resolution will be presented herein as well. 4 page(s...|$|R
40|$|Shrinking process node sizes {{allow the}} {{integration}} of more and more functionality into a single chip design. At the same time, the mask costs to manufacture a new chip increases steadily. For the industry this cost increase can be absorbed by selling more chips. Furthermore, new innovative chip designs have a higher risk. Therefore, the industry only changes small parts of a chip design between different generations to minimize their risks. Thus, new innovative chip designs can only be realized by research institutes, which {{do not have the}} cost restrictions and the pressure from the markets as the industry. Such an innovative research project is EXTOLL, which is developed by the Computer Architecture Group of the University of Heidelberg. It is a new interconnection network for High performance Computing, and targets the problems of existing interconnection networks commercially available. EXTOLL is optimized for a high bandwidth, a low latency, and a high message rate. Especially, the low latency and high message rate become more important for modern interconnection networks. As the size of networks grow, the same computational problem is distributed to more nodes. This leads to a lower data granularity and more smaller messages, that have to be transported by the interconnection network. The problem of smaller messages in the interconnection network is addressed by this thesis. It develops a new network protocol, which is optimized for small messages. It reduces the protocol overhead required for sending small messages. Furthermore, the growing network sizes introduce a reliability problem. This is also addressed by the developed efficient network protocol. The smaller data granularity also increases the need for an efficient barrier synchronization. Such a hardware barrier synchronization is developed by thesis, using a new approach of integrating the barrier functionality into the interconnection network. The masks costs to manufacture an ASIC make it difficult for a research institute to build an ASIC. A research institute cannot afford <b>re-spin,</b> because of the costs. Therefore, there is the pressure to make it right the first time. An approach to avoid a <b>re-spin</b> is the functional verification in prior to the submission. A complete and comprehensive verification methodology is developed for the EXTOLL interconnection network. Due to the structured approach, it is possible to realize the functional verification with limited resources in a small time frame. Additionally, the developed verification methodology is able to support different target technologies for the design with a very little overhead. ...|$|R
40|$|Functional {{complexity}} in analog, mixed-signal, and RF (A/RF) designs has risen {{dramatically in the}} past few years. Today’s simple A/RF functional block such as an RF receiver or power management unit can have hundreds to thousands of control bits. A/RF designs implement many modes of operation for different standards, power saving modes, and calibration. Increasingly, catastrophic failures in chips are due to functional bugs, and not due to missed performance specifications. Common errors include inverted signals, incorrect bit ordering in a bus, mis-wired analog bias lines, and incorrect power-up/power-down sequencing of blocks. This paper gives an overview of an A/RF verification methodology that focuses on catching functional errors in A/RF design. Using extensive use of Verilog-AMS and AMS Designer in an automated and parallel regression based manner, we will describe how errors can be caught throughout the design process and certainly prior to manufacturing. We will give an overview of the verification flow, project timelines, and discuss how testbenches and models can be written. Examples will be given using Verilog-AMS. Using this systematic verification methodology will help eliminate chip <b>re-spins.</b> Additionally, this methodology provide...|$|R
40|$|Consumer {{security}} devices are becoming ubiquitous, from pay-TV through mobile phones, PDA, prepayment gas meters to smart cards. There are many ongoing research {{efforts to keep}} these devices secure from opponents who try to retrieve key information by observation or manipulation of the chip’s components. In common industrial practise, it is after the chip has been manufactured that security evaluation is performed. Due to design time oversights, however, weaknesses are often revealed in fabricated chips. Furthermore, post manufacture security evaluation is time consuming, error prone and very expensive. This evokes the need of design time security evaluation techniques {{in order to identify}} avoidable mistakes in design. This thesis proposes a set of design time security evaluation methodologies covering the well-known non-invasive side-channel analysis attacks, such as power analysis and electromagnetic analysis attacks. The thesis also covers the recently published semi-invasive optical fault injection attacks. These security evaluation technologies examine the system under test by reproducing attacks through simulation and observing its subsequent response. The proposed design time security evaluation methodologies can be easily implemented into the standard integrated circuit design flow, requiring only commonly used EDA tools. So it adds little non-recurrent engineering (NRE) cost to the chip design but helps identify the security weaknesses at an early stage, avoids costly silicon <b>re-spins,</b> and helps succeed in industrial evaluation for faster time-to-market. ...|$|R
40|$|Producing a {{functionally}} correct {{integrated circuit}} {{is becoming increasingly}} difficult. No matter how careful a designer is, {{there will always be}} integrated circuits that are fabricated, but do not operate as expected. Providing a means to effectively debug these integrated circuits is vital to help pin-point problems and reduce the number of <b>re-spins</b> required to create a correctly-functioning chip. In this paper, we show that programmable logic cores (PLCs) and flexible networks can provide this debugging capability. We elaborate on our PLC based debug infrastructure and summarize our current research. We address issues such as defining the debug architecture and debug methodology, determining the expected area overhead, optimizing the interconnect topology, creating a high throughput multi-frequency on-chip network and building efficient interfaces between the PLC and fixed-function logic. Finally, we outline a number of directions for ongoing research in this area. Debugging fixed function ICs presents a significant challenge. It is difficult to observe or control signals within fixed-function ICs. After fabrication, the chip cannot be easily modified to provide these signals to output pins where they can be observed. Even if the signals were identified before fabrication, providing external access to these signals at design-time is problematic since I/O resources are often limited and do not operate at the high speeds that would be required. 1...|$|R
40|$|Embedded {{memories}} {{have become}} {{integral part of}} any system on chip (SOC) occupying about 60 % of the chip area in most cases. Memories which often are delivered as Intellectual Property (IP), if defective can affect the time-to-market of a chip because {{of the amount of}} time spent in debugging or even worse may result in a <b>re-spin.</b> Hence it is of utmost importance to ensure that the memory is bug free during design phase itself. This session will focus on developing an exhaustive, reusable and configurable test-environment to verify the behavioral memory models. The session shall also provide information on the traditional methods of verification and their problems. It will discuss in detail about developing an automated testbench and its advantages in terms of its functional coverage and reusability. Specman (4. 3. 4) based verification has been employed for developing an automated test environment for behavioral memory models. This methodology was used to develop a shared test environment which can verify different types of memories like single-port, dual-port and rom. With the introduction of Specman for verification 45 defects (for memories in one technology) were identified in the behavioral memory models, which were not caught by traditional Verilog/VHDL testbenches so resulting in reducing the cost of non-quality to very large extent. Using this methodology not only ensures that the customer gets high quality memory models but also provides a plug and play memory eVC which can be used during system level verification. 2 1...|$|R
40|$|Continuous {{improvements}} in the VLSI domain have enabled the integration of billions of transistors on the same die operating at frequencies in the gigahertz range. These advancements have brought upon the era of system-on-chip (SoC). Traditionally, analog ICs has been prone to device noise while digital ICs have typically not been the prime concern being considered as relatively immune to noise. With faster transition times and denser integration, the scenario wherein digital ICs {{were considered to be}} immune to noise has changed significantly. Drastic changes in the physical design of an IC and increase in the operating frequencies has immensely changed the classical understanding of noise in the new age complex ICs. Switching noise specifically has become a dominating criteria for high performance digital and mixed signal ICs. Voltage variations on the power/ground nodes of a circuit is a type of switching noise affecting digital and mixed-signal ICs. Therefore, power integrity (PI) has become a critical challenge that must be addressed at the system level considering the parasitic effects of package and board. In this work, a die, package and board modeling and co-simulation methodology is presented which can be easily integrated into a standard VLSI design flow. This methodology involves breaking down the system in multiple components and generating models for each component to observe individual performance. System level response can be seen by combining them together. This approach has been successfully exploited to guarantee the power integrity on an industrial design. This approach becomes successful in providing a systematic and a widely reusable method to estimate integrity issues before fabrication, thus exhibiting its worthiness as a design step in avoiding failures and <b>re-spins...</b>|$|R
40|$|Continued {{technology}} scaling {{has enabled}} the tremendous growth that semi-conductor industry has witnessed in last half century. However, as the technology scales in {{into the deep}} submicron era, variability in device parameters and operating conditions is emerging as {{a major threat to}} this continued growth. In the face of increasing variability, traditional design approaches that use typical or worst case values can lead to yield loss or increased cost and time to market, causing significant revenue loss in either case. State-of-the-art system-level analysis and design methodologies do not take the impact of variability into account and hence it is natural to question their effectiveness in the presence of variability. We believe that it is imperative that the impact of variability be considered during system level design. Analysis and design techniques that are cognizant of variability can help provide designers valuable feed- back about the impact of variations early on in the design cycle and hence, facilitate better design decisions at the system-level while preventing expensive design <b>re-spins.</b> The specific contributions on this thesis include: (i) system-level variability-aware power analysis methodology while considering the impact of manufacturing-induced variations in effective channel length and operation- induced variations in on-chip temperature, (ii) variation- aware system-level shutdown based power management techniques, (iii) variability-aware voltage level selection to improve the number of chips meeting power and performance targets, and (iv) a methodology for system- level performance analysis under variability, and various architecture level and application level techniques to enable performance recovery in systems affected by variations. Experiments conducted on various Systems-on- chip designs demonstrate that variation-aware design techniques enable significant improvements in overall energy dissipation and performance characteristics. In particular, the resulting distributions are more favorable in terms of reducing the revenue loss due to variations. We believe that the approaches outlined in this thesis are useful with the existing design show with the current technologies as well as in future systems by facilitating research and development in variation-aware application- level and architecture level technique...|$|R
40|$|The {{continued}} device scaling {{trend and}} the aggressive integrated circuit design style have shifted the major device failure mechanism from stuck-at fault types to marginal failures induced by timing uncertainty and signal noise. The production test methodologies currently employed by industry, however, are still {{based on the}} traditional structural test schemes {{that focus on the}} detection of permanent defects, failing to account for emerging failure mechanisms in nanometer scale designs. The inability of current test methodologies in adapting to the failure mechanism shift imposes critical challenges to the IC providers, mainly observed as significant product quality degradation and yield loss. To make things worse, the marginal failures result in highly ambiguous failure syndromes, invalidating traditional assumptions employed in silicon debugging. The degraded test quality and yield, combined with inaccurate failure diagnosis, lead to a lengthened design-fabrication-debugging cycle needed for ramping up the yield and quality for final production, significantly slowing down the time-to-market and boosting the overall product cost. Maintaining high quality yet low cost production test for nanometer scale integrated circuits necessitates a comprehensive examination of marginal failure scenarios while minimizing yield loss. Reducing the time-to-market cycle relies on an accurate identification of marginal failure locations and causalities to pinpoint the design and fabrication weaknesses that have gross quality impact. The challenges, though, are the resolution to the paradox between overscreening and underscreening that are simultaneously taking place in today's industrial testing practice, and the extraction of sensible diagnostic signals from highly ambiguous fault behaviors of marginal failures. The presented thesis work overcomes these challenges through the proposition of an innovative marginal failure aware test and diagnosis scheme, capable of thoroughly targeting the functional mode failure scenarios with a low cost structural test platform and the accurate identification of failure-induced feature change in large volume test data. A comprehensive production ramp-up flow, constructed based on the proposed test and diagnosis schemes, is furthermore presented to guide the silicon debugging, test optimization, and yield/quality learning activities, so as to minimize the time-to-market. From a technical point of view, this thesis work analyzes the power ground noise in functional and testing modes and its impact on circuit timing robustness, with a focus on the differentiation of the functional mode timing failures from the pure testing mode ones, thus enabling a clear decomposition of the noise treatment strategies for different operation scenarios. A set of tightly-coupled approaches, including 1) noise resilience in testing related circuitry for overscreening minimization, 2) approximation of worst-case functional mode noise in structural testing for marginal timing failure detection, and 3) diagnosis of noise- induced timing failure diagnosis in scan paths and scan clock trees for design optimization, are presented to attain the overall goal of high yield, low test escape rate, and fast silicon <b>re-spin.</b> These techniques are developed with the consideration of enabling a seamless adaptation of industrial flows by delivering maximal compatibility to mainstream design-for-testability architectures and testing platforms employed in nanometer scale designs. The successful incorporation of these techniques will significantly expedite the silicon production ramp-up process with highly reduced risk and cos...|$|R
40|$|Specification is {{the first}} and arguably the most {{important}} step for formal verification and correct-by-construction synthesis. These tasks require understanding precisely a design's intended behavior, and thus are only effective if the specification is created right. For example, much of the challenge in bug finding lies in finding the specification that mechanized tools can use to find bugs. It {{is extremely difficult to}} manually create a complete suite of good-quality formal specifications, especially given the enormous scale and complexity of designs today. Many real-world experiences indicate that poor or the lack of sufficient specifications can easily lead to misses of critical bugs, and in turn design <b>re-spins</b> and time-to-market slips. This dissertation presents research that mitigates this manual and error-prone process through automation. The overarching theme is specification mining - the process of inferring likely specifications by observing a design's behaviors. We explore formalisms and algorithms to mine specifications from different sources, and demonstrate that the mined specifications are useful if not essential for a variety of applications such as verification, diagnosis and synthesis. The first part of the dissertation presents two approaches to mine specifications dynamically from simulation or execution traces. The first approach offers a simple but effective template-based remedy to the aforementioned problem. The second approach presents a novel formalism of specification mining based on the notion of sparse coding, which can learn latent structures in an unsupervised setting, and thus are not restricted by predefined templates. Additionally, we show that the mined specifications from both approaches can be used to localize bugs effectively. In {{the second part of the}} dissertation, we study the problem of synthesis from temporal logic specifications. This synthesis approach offers an attractive proposition - one can automatically construct a functionally correct system from its behavioral description. The downside, however, is that it completely relies on the user to not only specify the intended behaviors of the system but also the assumptions on the environment. The latter is especially tricky in practice as environment assumptions are often implicit knowledge and seldom documented. We propose a framework that learns assumptions from the counterstrategies of an unrealizable specification to systematically guide it towards realizability. We further show that, the proposed counterstrategy-guided assumption mining approach enables the automatic synthesis of a new class of semi-autonomous controllers, called human-in-the-loop (HuIL) controllers. A crucial component of such a controller is an advisory that determines when to switch control from the autonomous controller to the human operator. We formalize the criteria that characterize a HuIL controller, by taking into account of human factors such as response time, and describe how to construct the advisory using assumption mining. Human inputs are still critical in specification. In the last part of this dissertation, we describe two efforts on broadening the scope of specification mining with creative use of human inputs. The first is the design of a crowdsourced specification mining game called CrowdMine. The main idea of CrowdMine is to transform a design's traces into images and leverage the human ability to recognize patterns in images to assist the process of mining specifications. The second effort examines the feasibility of converting natural language specifications to formal specifications, with a focus on how specification mining encapsulated in a natural language processing (NLP) layer may assist non-expert users of formal methods at the requirement stage of a design...|$|R

