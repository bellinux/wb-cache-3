12|1371|Public
5000|$|... where 'U' {{denotes the}} set of top 'N' users that are most similar to user 'u' who <b>rated</b> <b>item</b> 'i'. Some {{examples}} of the aggregation function includes: ...|$|E
5000|$|Second, {{the system}} executes a {{recommendation}} stage. It uses the most similar items to a user's already-rated items {{to generate a}} list of recommendations. Usually this calculation is a weighted sum or linear regression. This form of recommendation is analogous to [...] "people who rate item X highly, like you, also tend to rate item Y highly, and you haven't <b>rated</b> <b>item</b> Y yet, so you should try it".|$|E
50|$|Basically, {{these methods}} use an item profile (i.e., {{a set of}} {{discrete}} attributes and features) characterizing the item within the system. The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors {{using a variety of}} techniques. Simple approaches use the average values of the <b>rated</b> <b>item</b> vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers, cluster analysis, decision trees, and artificial neural networks in order to estimate the probability that the user is going to like the item.|$|E
40|$|Abstract. Recommender systems play an {{important}} role in supporting people getting items they like. One type of recommender systems is userbased collaborative filtering. The fundamental assumption of user-based collaborative filtering is that people who share similar preferences for common items behave similar in the future. The similarity of user preferences is computed globally on common <b>rated</b> <b>items</b> such that partial preference similarities might be missed. Consequently, valuable ratings of partially similar users are ignored. Furthermore, two users may even have similar preferences but the set of common <b>rated</b> <b>items</b> is too small to infer preference similarity. We propose first, an approach that computes user preference similarities based on learned user preference models and second, we propose a method to compute partial user preference similarities based on partial user model similarities. For users with few common <b>rated</b> <b>items,</b> we show that user similarity based on preferences significantly outperforms user similarity based on common <b>rated</b> <b>items.</b> ...|$|R
50|$|In this case, {{the average}} {{difference}} in ratings between item B and A is (2+(-1))/2=0.5. Hence, on average, <b>item</b> A is <b>rated</b> above <b>item</b> B by 0.5. Similarly, the average difference between item C and A is 3. Hence, if {{we attempt to}} predict the rating of Lucy for item A using her <b>rating</b> for <b>item</b> B, we get 2+0.5 = 2.5. Similarly, {{if we try to}} predict her <b>rating</b> for <b>item</b> A using her <b>rating</b> of <b>item</b> C, we get 5+3=8.|$|R
50|$|The Front Row {{interface}} lacks some iTunes functionality, including <b>rating</b> <b>items,</b> {{checking the}} account balance, adding {{funds to the}} account, synchronizing {{from more than one}} computer, full Internet radio support, and games.|$|R
5000|$|Some {{goods and}} {{services}} are [...] "zero-rated". The zero rate is treated like a positive rate of tax calculated at 0%. Supplies subject to the zero rate are still [...] "taxable supplies", that is, they count as having VAT charged on them. In the UK, examples include most food, books, medications, and certain kinds of transport. The zero rate is not featured in the EU Sixth Directive as it was intended that the minimum VAT rate throughout Europe would be 5%. However, zero-rating remains in some member states, most notably the UK and Ireland, as a legacy of pre-EU legislation. These member states have been granted a derogation to continue existing zero-rating but cannot add new goods or services.An EU Member State may uplift their domestic zero rate to a higher rate, for example to 5% or 20%, however, EU VAT rules do not allow a reversal back to the Zero rate once it has been given up. Interestingly, Member States may institute a reduced rate on a previously zero <b>rated</b> <b>item</b> even where EU law does not provide for a reduced rate, however if a Member State makes an increase from a zero rate to the prevalent standard rate, they may not then decrease down to a reduced rate unless specifically provided for in EU VAT Law (Annexe III of EU Dir 2006/112 list sets out where a reduced rate is permissible).|$|E
30|$|Table S 1 compares {{satisfaction}} survey {{scores between}} healthy volunteers and critically ill patients (see Additional file 4). Patients {{scored higher than}} volunteers for relaxation (p =  0.002) and system interaction (p =  0.004) and lower for boredom (p =  0.041). Additional file 4 : Table S 2 reports the scores on the acceptance survey for ICU personnel, including the overall mean as well as mean scores for physicians, nurses, and physiotherapists. The lowest <b>rated</b> <b>item</b> was Compatibility with physical infrastructure and ICU facilities, and the highest rated items were Compatibility with pharmacologic treatment and Compatibility with physiotherapy.|$|E
30|$|Updates or deletions of {{existing}} rating data are possible. For example, say the user has <b>rated</b> <b>item</b> a and b beforehand. When {{it comes to}} updating, he/she can notify the cloud {{of the difference between}} the new pair-wise rating deviation and the previous one and flag it to the cloud that it is an update. The process of rating update is described in algorithm 5. Similarly, for the delete operation, the additive inverse of the previous deviation, i.e. −δa,b is sent by the user to the cloud signifying a deletion. The process of rating deletion is also described in algorithm 5.|$|E
3000|$|... where ϕa,b is {{the count}} of the users who have <b>rated</b> both <b>items</b> while δi,a,b=ri,a−ri,bis the {{deviation}} of the <b>rating</b> of <b>item</b> a from that of item b both given by user i.|$|R
5000|$|A user expresses {{his or her}} {{preferences}} by <b>rating</b> <b>items</b> (e.g. books, {{movies or}} CDs) of the system. These ratings {{can be viewed as}} an approximate representation of the user's interest in the corresponding domain.|$|R
40|$|Abstract. In a {{recommender}} system where users <b>rate</b> <b>items</b> we predict the <b>rating</b> of <b>items</b> users have not rated. We define a rating graph containing users and items as vertices and ratings as weighted edges. We extend {{the work of}} [1] that uses the resistance distance on the bipartite rating graph incorporating negative edge weights into the calculation of the resistance distance. This algorithm is then compared to other rating prediction algorithms using data from two rating corpora. ...|$|R
40|$|Background: The Dependence Scale (DS) was {{designed}} to assess levels of patient need for care due to deficits typical of Alzheimer’s disease (AD). This study examined content validity of the DS based on input from patients, caregivers, and clinicians. Methods: Qualitative interviews with experts, patients, and caregivers were used to collect information {{on the concept of}} dependence and to assess content validity. Results: Nine clinicians <b>rated</b> <b>item</b> relevance ‘‘high’ ’ with consensus on the primacy of functional abilities and dependence in the measurement of AD progression. Twenty-two US, 11 UK, and 14 informal caregivers from Spain participated in focus groups; 18 patients participated in 3 separate focus groups. Discussion supported DS hierarchy of dependence, capture of mild-to-severe dependence, suitability of response options, and short recall time frame. Conclusions: Clinicians, caregivers, and patients support content validity of the DS in mild-to-moderate AD. The DS may be valuable to capture dependence within future clinical dementia trials...|$|E
40|$|Collaborative {{filtering}} is {{a popular}} technique to infer users' preferences on new content based on the collective information of all users preferences. Recommender systems then use this information to make personalized suggestions to users. When users accept these recommendations it creates a feedback loop in the recommender system, and these loops iteratively influence the collaborative filtering algorithm's predictions over time. We investigate whether {{it is possible to}} identify items affected by these feedback loops. We state sufficient assumptions to deconvolve the feedback loops while keeping the inverse solution tractable. We furthermore develop a metric to unravel the recommender system's influence on the entire user-item rating matrix. We use this metric on synthetic and real-world datasets to (1) identify {{the extent to which the}} recommender system affects the final rating matrix, (2) rank frequently recommended items, and (3) distinguish whether a user's <b>rated</b> <b>item</b> was recommended or an intrinsic preference. Our results indicate that it is possible to recover the ratings matrix of intrinsic user preferences using a single snapshot of the ratings matrix without any temporal information. Comment: Neural Information Processing Systems, 201...|$|E
40|$|Most {{recommendation}} systems {{require some}} form of user feedback such as ratings {{in order to make}} personalized propositions of items. Typically ratings are unidimensional in the sense of consisting of a scalar value that represents the user’s appreciation for the <b>rated</b> <b>item.</b> Multi-criteria ratings allow users to express more differentiated opinions by allowing separate ratings for different aspects or dimensions of an item. Recent approaches of multi-criteria recommender systems are able to exploit this multifaceted user feedback and make personalized propositions that are more accurate than recommendations based on unidimensional rating data. However, most proposed multi-criteria recommendation algorithms simply exploit the fact that a richer feature space allows building more accurate predictive models without considering the semantics and available domain expertise. This paper contributes on the latter aspects by analyzing multi-criteria ratings from the major etourism platform, TripAdvisor, and structuring raters ’ overall satisfaction {{with the help of a}} Penalty-Reward Contrast analysis. We identify that several a-priori user segments significantly differ in the way overall satisfaction can be explained by multi-criteria rating dimensions. This finding has implications for practical algorithm development that needs to consider different user segments...|$|E
3000|$|The Q-LES-Q is a self-report {{measure that}} assesses {{subjective}} {{quality of life}} (i.e., physical health, subjective feelings, leisure activities, and social relationships) over the previous week. Participants <b>rate</b> <b>items</b> {{on a scale from}} a 5 -point scale that ranges from “very poor” to “very good.” [...]...|$|R
5000|$|If a user <b>rated</b> several <b>items,</b> the {{predictions}} are simply combined using a weighted average where {{a good choice}} for the weight is the number of users having <b>rated</b> both <b>items.</b> In the above example, we would predict the following rating for Lucy on item A: ...|$|R
3000|$|Teachers had {{no missing}} values on any item. Missing values in student {{questionnaires}} ranged between [...]. 1 and 2.2 % per <b>rating</b> <b>item,</b> with Little’s Chi square test {{indicating that they}} were missing completely at random (χ 2  =  1197.683; df =  1181; p = . 361). These items were imputed with the expectation–maximization algorithm.|$|R
30|$|Association {{rules are}} based on the market-basket model, where, in this case, we put all items rated by the same user into a basket and regard ratings as binary only (i.e., rated/not rated). For every ordered pair of items (i, j), we then {{evaluate}} a simple algorithm inspired by the Apriori algorithm [31] and rank all items by how much more likely an item is to be consumed if another item was consumed. Specifically, we compute the fraction of co-ratings of i and j over the total ratings of i (i.e., the fraction users who rated both i and j, out of those who rated i). Let U_i be the set of users who <b>rated</b> <b>item</b> i. We can then compute this as as |U_i ∩ U_j|/|U_i|. This is also known as the confidence of an association rule. To compensate for the popularity of j, we then divide by the fraction of users who did not rate i but still rated j. Let U_i be the set of users who did not rate item i. We can then divide by |U_i ∩ U_j|/|U_i| to counter the effect of highly popular items that are likely to be co-rated with every item, but would not be very useful as a recommendation. We then take the top-N items most likely to be co-rated with an item by this measure.|$|E
40|$|The study compares three {{methods for}} {{establishing}} cut-off scoreS that effect {{a compromise between}} absolute cut-offs based on item difficulty and relative cut-ffs based on expected passing rates. Each method coordinates these two_types of information. differently. The Beuk method_obtains judges ' estimates of an absolute cut-off and an expected passing rate, and constructs a cutting line whose slope is {{the ratio of the}} absolute and relative standard deviations and which passes through the point of _ mean absolute/relative cut-off. The judges can be either test-oriented or-examinee-oriented depending on whether they show greater agreement (small standard deviations) on the absolute or relative cut-offs. The Hofstee method draws a cutting line_through two extreme points: (1) maximum cut-off, minimum failure point; and (2) minimum cut-off, maximum failure point. The DeGruijter method is similr to the Beuk method, but uses confidence estimates for the absolute and relative cut-offs to define a criterion ellipse. These methods_were applied to two,tests from a certification program. Judges <b>rated</b> <b>item</b> difficulty by the Angoff_method and estimated a desirable passing rate. All three compromise_methods brought the cut-off two points below the absolute level, in line with an_acceptable passing _ rate. This study suggests that further research into all three of the compromise methods is needed. (LPG) Reproductions supplied by EDRS are the bes from the original documen hat can be mad...|$|E
40|$|Abstract. Watching {{television}} {{tends to}} be a social activity. So, adaptive television needs to adapt to groups of users rather than to individual users. In this paper, we discuss different strategies for combining individual user models to adapt to groups, some of which are inspired by Social Choice Theory. In a first experiment, we explore how humans select a sequence of items for a group to watch, based on data about the individuals’ preferences. The results show that humans use some of the strategies such as the Average Strategy (a. k. a. Additive Utilitarian), the Average Without Misery Strategy and the Least Misery Strategy, and care about fairness and avoiding individual misery. In a second experiment, we investigate how satisfied people believe they would be with sequences chosen by different strategies, and how their satisfaction corresponds with that predicted by a number of satisfaction functions. The results show that subjects use normalization, deduct misery, and use the ratings in a non-linear way. One of the satisfaction functions produced reasonable, though not completely correct predictions. According to our subjects, the sequences produced by five strategies give satisfaction to all individuals in the group. The results also show that subjects put more emphasis than expected on showing the best <b>rated</b> <b>item</b> to each individual (at a cost of misery for another individual), and that the ratings of the first and last items in the sequence are especially important. In a final experiment, we explore the influence viewing an item can have on the ratings of other items. This is important for deciding the order in which to present items. The results show an effect of both mood and topical relatedness...|$|E
3000|$|Multi-criteria {{decision}} making (MCDM) methods have recently become popular [2, 11, 23] for implementing recommender systems that use multiple criteria rather than traditional single criteria. The traditional recommender <b>rates</b> <b>items</b> based on (R: users [...] × items → ratings), while MCDM may involve (R: users × items × contexts × rates × time → ratings).|$|R
3000|$|In this paper, a {{fractional}} differential {{model of}} HIV infection of CD 4 + T-cells is investigated. We shall consider this model, which includes full logistic growth {{terms of both}} healthy and infected CD 4 + T-cells, time delay <b>items,</b> and cure <b>rate</b> <b>items.</b> A more appropriate method is given to ensure that both equilibria are asymptotically stable for [...]...|$|R
40|$|It is {{a common}} {{practice}} among Web 2. 0 services to allow users to <b>rate</b> <b>items</b> on their sites. In this paper, we first point out the flaws of the popular methods for user-rating based ranking of items, and then argue that two well-known Information Retrieval (IR) techniques, namely the Probability Ranking Principle and Statistical Language Modelling, provide a simple but effective solution to this problem...|$|R
40|$|This study {{presents}} an innovative method {{for reducing the}} number of <b>rating</b> scale <b>items</b> without predictability loss. The "area under the re- ceiver operator curve method" (AUC ROC) is used to implement in the RatingScaleReduction package posted on CRAN. Several cases have been used to illustrate how the stepwise method has reduced the number of <b>rating</b> scale <b>items</b> (variables). Comment: 20 pages, 6 figure...|$|R
40|$|A common {{approach}} to designing Recommender Systems (RS) consists of asking users to explicitly <b>rate</b> <b>items</b> {{in order to}} collect feedback about their preferences. However, users {{have been shown to}} be inconsistent and to introduce a non-negligible amount of natural noise in their ratings that affects the accuracy of the predictions. In this paper, we present a novel approach to improve RS accuracy by reducing the natural noise in the input data via a preprocessing step. In order to quantitatively understand the impact of natural noise, we first analyze the response of common recommendation algorithms to this noise. Next, we propose a novel algorithm to denoise existing datasets by means of re-rating: i. e. by asking users to <b>rate</b> previously <b>rated</b> <b>items</b> again. This denoising step yields very significant accuracy improvements. However, re-rating all items in the original dataset is unpractical. Therefore, we study the accuracy gains obtained when re-rating only some of the ratings. In particular, we propose two partial denoising strategies: data and user-dependent denoising. Finally, we compare the value of adding a rating of an unseen item vs. re-rating an item. We conclude with a proposal for RS to improve the quality of their user data and hence their accuracy: asking users to re-rate items might, in some circumstances, be more beneficial than asking users to <b>rate</b> unseen <b>items...</b>|$|R
40|$|To {{develop an}} {{assertion}} scale for adolescents 27 items were tentatively selected from 112 items {{that appeared in}} previous studies and they were revised for Japanese adaptation. One hundred and thirty-four college students, Junior college students, and technical school students <b>rated</b> the <b>items</b> on a four-point rating scale, and simultaneously <b>rated</b> <b>items</b> of four related scales such as aggression and social skills as well (Study 1). On {{the basis of the}} results of some statistical analyses of the data, the items of the assertion scale were revised. One hundred and twenty-one college students rated 25 revised assertion items on a 5 point-rating scale, and simultaneously <b>rated</b> <b>items</b> of several related scales as well (Study 2). The factor analysis revealed that there were two factors, each of which consisted of 8 items. These factors were labeled as Relation-Formation factor and Persuasion-Negotiation factor, respectively. The reliability (Cronbach's a was. 80 for Relation-Formation and. 71 for Persuasion-Negotiation, test-retest correlation was. 79 for total score) and validity (r =. 69 with the SSRS assertion scale) of the revised scale were verified. A Pearson correlation of the assertion scale with a shyness scale was r =　-. 59, and that with a public self-consciousness scale was r = -. 15. The usefulness and adaptability of this assertion scale in line with the future assertion training were discussed...|$|R
40|$|This study {{assessed}} {{individual differences}} in the value that college students placed on communication skills exhibited by same-sex peers. Participants (N = 410) <b>rated</b> <b>items</b> tapping eight different communication skills for their importance in same-sex relationships. The skills included ego support, conflict management, comforting, referential ability, conversational skill, regulative skill, narrative ability, and persuasive skill. Interpersonal cognitive complexity was assessed through Crockett 2 ̆ 7 s (1965) Role Category Questionnaire...|$|R
50|$|Visual {{analogue}} scale (also {{called the}} Continuous rating {{scale and the}} graphic rating scale) - respondents <b>rate</b> <b>items</b> by placing a mark on a line. The line is usually labeled at each end. There are sometimes a series of numbers, called scale points, (say, from zero to 100) under the line. Scoring and codification is difficult for paper-and-pencil scales, but not for computerized and Internet-based visual analogue scales.|$|R
30|$|IKDC-SKF {{examines}} 19 {{items on}} 5 -point Likert (items 1, 4, 5, 7, 8 & 9 a-i), 0 – 10 <b>rating</b> (<b>items</b> 2, 3, 10 a & 10 b) or dichotomous scales (item 6). The overall score {{is based on}} 18 items (item 10 a is not included) and ranges from 0 to 100 points with higher points corresponding to less symptoms, better function and {{a higher level of}} sports activity [13].|$|R
30|$|The {{numerical}} {{analysis showed that}} as rate of holding cost, setup cost and deterioration rate increases, the percentage improvement in optimal total cost with substitution over without substitution also increases and if we increase the shortage cost or cost of substitution, the percentage improvement decreases in total optimal cost with substitution over without substitution while percentage improvement becomes constant when substitution <b>rate</b> of <b>item</b> 1 becomes equal or more from substitution <b>rate</b> of <b>item</b> 2.|$|R
40|$|Given {{two sets}} of objects Set of users and set of items Observe labeled object pairs ruj = 5 ⇔ User u gave <b>item</b> j a <b>rating</b> of 5 Predict labels of unobserved pairs How will user u <b>rate</b> <b>item</b> k? Examples <b>Rating</b> {{prediction}} in collaborative filtering How will user u rate movie j? Click prediction in web search Will user u click on URL j? Link prediction in a social networ...|$|R
40|$|This paper {{presents}} a flexible mixture model (FMM) for collaborative filtering. FMM extends existing partitioning/clustering algorithms for collaborative filtering by clustering both users and items together simultaneously without assuming that each user and item should only {{belong to a}} single cluster. Furthermore, {{with the introduction of}} `preference' nodes, the proposed framework is able to explicitly model how users <b>rate</b> <b>items,</b> which can vary dramatically, even among the users with similar tastes on items...|$|R
30|$|Cohort 2 {{received}} a new itemised questionnaire (Additional file 1) that retained the highly <b>rated</b> <b>items</b> from cohort 1 ’s first questionnaire, consolidating closely related items, with new questions formulated {{to capture the}} themes from the qualitative analysis. Questions were grouped to represent CoI presences. Thirteen students from cohort 2 (30 students) completed this questionnaire electronically, representing a 43 % response rate, that might be skewed. Findings from cohort 2 are {{compared with those of}} cohort 1.|$|R
40|$|Learning from Pairs Given {{two sets}} of objects Set of users and set of items Observe labeled object pairs User u gave <b>item</b> j a <b>rating</b> ruj of 5 Predict labels of unobserved pairs How will user u <b>rate</b> <b>item</b> k? Examples Movie rating {{prediction}} in collaborative filtering How will user u rate movie j? Click prediction in web search Will user u click on URL j? Link prediction in a social network Is user u friends with user j...|$|R
5000|$|Increase {{of central}} excise duty <b>rates</b> for <b>items</b> {{attracting}} the <b>rate</b> of 4% to a mean rate of 8% with exceptions like food items and drugs.|$|R
