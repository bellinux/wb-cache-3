4233|6067|Public
5000|$|... #Caption: A {{schematic}} {{illustration of}} the stellar locus <b>regression</b> <b>method</b> of photometric calibration in astronomy.|$|E
50|$|In 1757, Roger Joseph Boscovich {{developed}} a <b>regression</b> <b>method</b> {{based on the}} L1 norm and therefore implicitly on the median.|$|E
5000|$|... #Caption: Cumulative Gumbel {{distribution}} {{fitted to}} maximum one-day October rainfalls in Surinam by the <b>regression</b> <b>method</b> with added confidence band using cumfreq ...|$|E
50|$|Support {{for various}} <b>regression</b> <b>methods.</b>|$|R
40|$|Both {{equation}} {{discovery and}} <b>regression</b> <b>methods</b> aim at inducing models of numerical data. While the equation discovery methods are usually evaluated {{in terms of}} comprehensibility of the induced model, the emphasis of the <b>regression</b> <b>methods</b> evaluation is on their predictive accuracy. In this paper, we present Ciper, an efficient method for discovery of polynomial equations and empirically evaluate its predictive performance on standard regression tasks. The evaluation shows that polynomials compare favorably to linear and piecewise regression models, induced by the existing state-of-the-art <b>regression</b> <b>methods,</b> in terms of degree of fit and complexity. ...|$|R
40|$|We propose Bayesian {{extensions}} of two nonparametric <b>regression</b> <b>methods</b> which are kernel and mutual $k$-nearest neighbor <b>regression</b> <b>methods.</b> Derived based on Gaussian process models for regression, the extensions provide distributions for target value estimates and the framework {{to select the}} hyperparameters. It is shown that both the proposed methods asymptotically converge to kernel and mutual $k$-nearest neighbor <b>regression</b> <b>methods,</b> respectively. The simulation {{results show that the}} proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set. Comment: 8 page...|$|R
5000|$|... 2. Using a single-nucleotide polymorphisms (SNP) <b>regression</b> <b>method</b> to {{quantify}} the contribution of additive, dominance, and imprinting variance to the total genetic variance ...|$|E
50|$|Linear regression: The {{simplest}} <b>regression</b> <b>method.</b> Fitting {{a linear}} equation {{to model the}} relationship between dependent variables (things to be predicted) and explanatory variables (things known).|$|E
5000|$|A {{statistical}} {{estimate of}} beta is calculated by a <b>regression</b> <b>method.</b> For a given asset and a benchmark, {{the goal is}} to find an approximate formula ...|$|E
40|$|Regression is a {{statistical}} approach for modelling {{the relationship between}} a response variable y and one or several explanatory variables x. Various types of <b>regression</b> <b>methods</b> are extensively applied for the analysis of data from literarily all fields of quantitative research. For example, multiple linear regression, logistic regression, and Cox proportional hazards models have been the main basic statistical tools in medical research for decades. In the last 20 – 30 years, the regression toolbox has been supplied with numerous extensions, like, for example, generalized additive models, <b>regression</b> <b>methods</b> for repeated measurements, and <b>regression</b> <b>methods</b> for high-dimensional data, to mention some...|$|R
40|$|Configurable {{software}} systems allow {{users to}} form configurations by selecting and deselecting features. The process of configuration creation may directly affect {{performance of the}} system in a non-linear way because of possible complex feature interactions. Understanding the correlation between feature selection and performance is important for stakeholders to acquire a desirable program variant. In this work we try to infer this correlation between system configuration and performance, using small samples of already measured configurations, without additional effort to detect feature interactions. We carry out {{a case study of}} several <b>regression</b> <b>methods</b> for solving this problem: regression trees, bagging of regression trees, random forests and support vector machines. All <b>regression</b> <b>methods</b> have their parameters tuned in automatic fashion by using Sobol sampling. To evaluate the prediction accuracy of the <b>regression</b> <b>methods,</b> the case study is performed using six real-world configurable software systems from different application domains and written in different programming languages. We show that bagging outperforms all other <b>regression</b> <b>methods</b> in most of the cases for all configurable systems, sampling sizes and parameter settings. We analyse the sensitivity of different <b>regression</b> <b>methods</b> and show that the most stable ones are regression trees and bagging...|$|R
40|$|We {{study the}} {{suitability}} of lasso-type penalized regression techniques when applied to macroeconomic forecasting with high-dimensional datasets. We consider performance of the lasso-type methods when the true DGP is a factor model, contradicting the sparsity assumption underlying penalized <b>regression</b> <b>methods.</b> We also investigate how the methods handle unit roots and cointegration in the data. In an extensive simulation study we find that penalized <b>regression</b> <b>methods</b> are morerobust to mis-specification than factor models estimated by principal components, even if the underlying DGP is a factor model. Furthermore, the penalized <b>regression</b> <b>methods</b> are demonstrated to deliver forecast improvements over traditional approaches when applied to non-stationary data containing cointegrated variables, despite a deterioration of the selective capabilities. Finally, we also consider an empirical application to a large macroeconomic U. S. dataset and demonstrate that, in line with our simulations, penalized <b>regression</b> <b>methods</b> attain the best forecast accuracy most frequently...|$|R
50|$|Eslamian, S. S., Ghasemizadeh, M., Biabanaki, M. and M. Talebizadeh, 2010, A {{principal}} component <b>regression</b> <b>method</b> for estimating low flow index, Water Resources Management, Vol. 24, No. 11, 2553-2566.|$|E
5000|$|... the <b>regression</b> <b>method,</b> linearizing the {{probability}} distribution through transformation and determining the parameters from a linear regression of the transformed Pc (obtained from ranking) on the transformed X data.|$|E
5000|$|This {{method is}} {{implemented}} by obtaining a [...] consistent estimator of [...] and then deriving an estimator of [...] from the nonparametric regression of [...] on [...] using an appropriate nonparametric <b>regression</b> <b>method.</b>|$|E
40|$|Over {{the last}} decade, {{the number and}} sophistication of methods used to do {{regression}} on complex datasets have increased substantially. Despite this, our literature review found that research that explores the impact of heteroscedasticity on many widely used modern <b>regression</b> <b>methods</b> appears to be sparse. Thus, our research seeks to clarify the impact that heteroscedasticity has on the predictive effectiveness of modern <b>regression</b> <b>methods.</b> In order to achieve this objective, we begin by analyzing the ability of ten different modern <b>regression</b> <b>methods</b> to predict outcomes for three medium-sized data sets that each feature heteroscedasticity. We then use insights provided from this work to develop a simulation model and design an experiment that explores the impact that various factors have on prediction accuracy of our ten different <b>regression</b> <b>methods.</b> These factors include linearity, sparsity, the signal to noise ratio, the number of explanatory variables, {{and the use of}} a variance stabilizing transformation...|$|R
50|$|<b>Regression</b> <b>methods</b> {{continue}} to be an area of active research. In recent decades, new methods {{have been developed for}} robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, <b>regression</b> <b>methods</b> accommodating various types of missing data, nonparametric <b>regression,</b> Bayesian <b>methods</b> for <b>regression,</b> regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression.|$|R
5000|$|Nonparametric {{regression}} and semiparametric <b>regression</b> <b>methods</b> {{have been}} developed based on kernels, splines, and wavelets.|$|R
50|$|In {{statistics}} and, in particular, in {{the fitting}} of linear or logistic regression models, the elastic net is a regularized <b>regression</b> <b>method</b> that linearly combines the L1 and L2 penalties of the lasso and ridge methods.|$|E
50|$|The {{development}} of single-nucleotide polymorphisms (SNPs) mapping helps {{to explore the}} genetic variation of complex traits at individual loci. Researchers can quantify the contribution of additive, dominance, and imprinting variance to the total genetic variance by using a SNP <b>regression</b> <b>method.</b>|$|E
5000|$|Nonparametric multiplicative {{regression}} (NPMR) {{is a form}} of nonparametric regression {{based on}} multiplicative kernel estimation. Like other regression methods, the goal is to estimate a response (dependent variable) based on one or more predictors (independent variables). NPMR can be a good choice for a <b>regression</b> <b>method</b> if the following are true: ...|$|E
5000|$|Forecasting ragweed pollen {{characteristics}} with nonparametric <b>regression</b> <b>methods</b> {{over the}} most polluted areas in Europe (co-author, 2011) ...|$|R
30|$|The data {{analysis}} {{described in this}} report suffers from the two common threats to validity that apply to effort estimation [20]. First, the conclusions {{are based on the}} data that SAP collects about fixing vulnerabilities in its software. Changes to the data collection processes, such as changes to the attributes of the collected data, could impact the predictions and the viability of producing predictions in the first place. Second, the conclusions of this study are based on the <b>regression</b> <b>methods</b> we used, i.e., LR, RPART, and NNR. There are many other single and ensemble <b>regression</b> <b>methods</b> that we did not experiment with. We note that performance issues due {{to the size of the}} datasets inhibit us from using random forest [23] and boosting [23], two ensemble <b>regression</b> <b>methods.</b>|$|R
30|$|As the {{deviation}} distribution is nonlinear {{and may be}} related to various factors, conventional <b>regression</b> <b>methods</b> may be ineffective.|$|R
50|$|<b>Regression</b> <b>method,</b> using a {{transformation}} of the cumulative distribution function so that a linear relation is found between the cumulative probability and {{the values of the}} data, which may also need to be transformed, depending on the selected probability distribution. In this method the cumulative probability needs to be estimated by the plotting position.|$|E
50|$|In statistics, an {{additive}} model (AM) is a nonparametric <b>regression</b> <b>method.</b> It was suggested by Jerome H. Friedman and Werner Stuetzle (1981) {{and is an}} essential part of the ACE algorithm. The AM uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a p-dimensional smoother. Furthermore, the AM is more flexible than a standard linear model, while being more interpretable than a general regression surface at the cost of approximation errors. Problems with AM include model selection, overfitting, and multicollinearity.|$|E
5000|$|Solving the {{simplified}} {{error function}} to determine an [...] pair {{can be done}} with alternating optimization, where first a random [...] is used to project [...] in to 1D space, and then the optimal [...] is found to describe the relationship between that projection and the residuals via your favorite scatter plot <b>regression</b> <b>method.</b> Then if [...] is held constant, assuming [...] is once differentiable, the optimal updated weights [...] can be found via the [...] Gauss-Newton method - a quasi-Newton method in which the part of the Hessian involving the second derivative is discarded. To derive this, first [...] Taylor expand , then plug the expansion back in to the simplified error function [...] and do some algebraic manipulation {{to put it in the}} form ...|$|E
5000|$|Models of {{detection}} and alert systems (Serfling <b>regression</b> <b>methods,</b> Costagliola D. and coll [...], Am. J. Public Health, on 1991) ...|$|R
3000|$|Comparisons of GDC MRTWO {{results were}} made to four washout models; two {{different}} <b>regression</b> <b>methods</b> of fitting biexponentials, and two different <b>regression</b> <b>methods</b> of fitting single gamma variates. One of these latter, Tk-GV; the Tikhonov regularized gamma variate herein adaptively minimizes the relative error of β or alternatively the minimum relative error of plasma clearance [12, 13, 32]. Briefly, Tk-GV is an approximate inverse solution to Eq. (4). That is for C(t)[*]=[*]GDfastTk-GVWO, then Tk-GVWO[*]=[*]GDfast [...]...|$|R
40|$|We {{present a}} {{detailed}} investigation on the temperature-dependence of polarized and unpolarised Raman spectra for various laboratory water, fresh and saltwater samples. We {{have identified the}} spectral parameters which are most sensitive to water temperature, and compared these between the water samples from different locations. We have then applied linear <b>regression</b> <b>methods</b> and multiple linear <b>regression</b> <b>methods</b> to investigate the accuracy with which water temperature can be estimated from Raman spectral data. 10 page(s...|$|R
5000|$|Blau and Duncan both {{analyzed}} status attainment {{within a}} wide framework {{by using a}} basic mobility model. They {{thought that it would}} be easiest to analyze if they examined the process by which men move up and down the social ladder in their family of origin to adult positions in a hierarchy of occupations. [...] "Rather than depicting the father-son relationship in a cross-tabular form, the <b>regression</b> <b>method</b> made it possible to approximate the process by which the son's status was attained" [...] Schoon, Ingrid (2008), A Transgenerational Model of Status Attainment: the Potential Mediating Role of School Motivation and Education, National Institute Economic Review, pp. 72-81. This model was coined the Wisconsin model. It focused on the processes of individuals developing personal qualities, such as motivation or skills at a given task that lead to educational achievement and eventually to positions on the occupational ladder. Acknowledgments between education, family status, and young people's ability levels as well as their motivations and aspirations were all contributive to one's status attainment.|$|E
50|$|Most hydroceles {{appearing}} {{in the first year}} of life seldom require treatment as they resolve without treatment. Hydroceles that persist after the first year or occur later in life require treatment through open operation for removing surgically, as these may have little tendency towards <b>regression.</b> <b>Method</b> of choice is open operation under general or spinal anesthesia, which is sufficient in adults. General anesthesia is the choice in children. Local infiltration anesthesia is not satisfactory because it cannot abolish abdominal pain due to traction on the spermatic cord. If a testicular tumor is suspected, a hydrocele must not be aspirated as malignant cells can be disseminated via the scrotal skin to its lymphatic field. This is excluded clinically by ultrasonography. If a tumor is not present, the hydrocele fluid can be aspirated with a needle and syringe. Clear straw-colored fluid contains mostly albumin and fibrinogen. If the fluid is allowed to drain in a collecting vessel, it does not clot but can be coagulated if small amounts of blood come in contact with the damaged tissue. In long standing cases, hydrocele fluid may be opalescent with cholesterol and may contain crystals of tyrosine and a palpable normal testis confirms the diagnosis; other wise surgical exploration of testis is needed.|$|E
50|$|ResProx uses a {{collection}} of 25 different protein structure features (such as torsion angle distributions, hydrogen bonding, packing volume, cavities, Molprobity measures) {{that were used in}} a Support Vector <b>Regression</b> <b>method</b> to maximize the correlation between the predicted resolution and the observed X-ray resolution on a set of 2400 protein structures with known X-ray resolution. The exact details of the algorithm are provided in a paper published by Dr. Wishart and colleagues. After training and appropriate validation on independent tests sets, this SVR model is able to estimate the resolution of solved X-ray structures with a correlation coefficient of 0.92, mean absolute error of 0.28 Angstroms. This is about 15-30% better than existing methods. This is shown in Figure 1. Because the performance of the ResProx method is so high and because it only needs coordinate data to generate an estimate of the equivalent X-ray resolution, it is ideally suited to be applied to NMR structures. Interestingly when NMR structures are analyzed by ResProx, the average NMR structure has an equivalent X-ray resolution of 2.8 Angstroms—which is relatively poor (Fig. 2). This is in agreement with qualitative observations regarding the overall quality and precision of NMR structures. As seen in Figure 2, a very small number NMR structures exhibit a resolution equivalent to < 1.0 Angstroms—but these are rare.|$|E
40|$|Despite {{the variety}} of robust <b>{{regression}}</b> <b>methods</b> that have been developed, current regression formulations are either NP-hard, or allow unbounded response to even a single leverage point. We present a general formulation for robust regression—Variational M-estimation—that unifies a number of robust <b>regression</b> <b>methods</b> while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates {{the effectiveness of the}} new estimation approach compared to standard methods. ...|$|R
50|$|Many {{different}} semiparametric <b>regression</b> <b>methods</b> {{have been}} proposed and developed. The most popular methods are the partially linear, index and varying coefficient models.|$|R
40|$|This paper {{contains}} some {{remarks on the}} so-called “relevant subspaces” useful when data reduction, near-collinearity of prediction problems are to be dealt with. The presentation is mainly based on a geometrical point of view. The consequences of relevant subspaces on the most common linear <b>regression</b> <b>methods</b> are analysed and a new method is developed to extract relevant subspaces. An experiment based on simulations is proposed to verify if the forecasting ability of some <b>regression</b> <b>methods</b> is influenced by relevant components...|$|R
