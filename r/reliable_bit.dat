20|84|Public
40|$|In this paper, {{we study}} a gossip {{algorithm}} for distributed averaging over a wireless sensor network. The usual assumption is that, through properly chosen codes, the physical layer {{is reduced to}} a set of <b>reliable</b> <b>bit</b> pipes for the distributed averaging algorithm. However, with a new channel coding technique, computation coding, we can exploit the interference property of the wireless medium for efficient averaging. This then provides a new abstraction for the physical layer: reliable linear equations instead of <b>reliable</b> <b>bit</b> pipes. The “neighborhood gossip” algorithm operates modularly on top of this abstraction. We will show that for certain regimes, such an approach can lead to energy savings that are exponential in the network size and time savings that are polynomial...|$|E
30|$|The {{complexity}} of the proposed algorithm mostly depends on the {{complexity of}} the FPCs. The main FPCs complexity comes from two ways: the computation of the extrinsic information in the reliable part and the adaptation of the parity-check matrix. Since only some bits in the boundary will be switched from the reliable part to the unreliable part in the adaptation of the parity-check matrix, we can use the rule of partial <b>reliable</b> <b>bit</b> updating proposed in [13] to reduce the complexity in the bit reliabilities update part.|$|E
40|$|Power spectra for binary {{phase-shift}} keyed (PSK) and frequency-shift keyed (FSK) signals with combined sinusoidal {{amplitude modulation}} by the bit rate clock are presented. Such signal formats have application in 'burst' communication systems wherein <b>reliable</b> <b>bit</b> synchronization {{is provided with}} relative receiver simplicity. Effects of the clock amplitude modulation index and phase are considered, and corresponding power spectra are illustrated. Further, effects of the clock parameters on the rate of spectral rolloff are given and {{compared with those of}} conventional PSK and FSK spectra...|$|E
3000|$|Perform Gauss-Jordan {{elimination}} on {{the matrix}} G, {{with respect to}} the positions of the k most <b>reliable</b> <b>bits,</b> to obtain a systematic generator matrix G [...]...|$|R
40|$|An {{iterative}} soft input soft output (SISO) decoding algorithm for Reed-Solomon (RS) codes {{using their}} binary image representations is presented. The novelty of the iterative algorithm is in reducing a submatrix {{corresponding to the}} less <b>reliable</b> <b>bits</b> in the binary parity check matrix of the RS code to a sparse nature before each decoding iteration. The propose...|$|R
40|$|In this paper, {{we present}} a novel secure iris {{verification}} system, where a transformed version of the iris template instead of the plain reference is stored for protecting the sensitive biometric data. An Error Correcting Code (ECC) technique is adopted to perform the comparison in the transformed domain. A two-segment method is proposed to execute the feature verification, where a Bose-Chaudhuri-Hochquenghem (BCH) code of a random bit-stream is introduced to eliminate the considerable differences between the features extracted from different scans of irises. A <b>reliable</b> <b>bits</b> selection process during the iris feature generation stage reduces the system error rate from 6. 0 % to 0. 8 %. The appropriate size of the set of <b>reliable</b> <b>bits</b> is determined by investigating the best match between the associated error correct cutting edge and the actual verification accuracy. Index Terms — iris, personal verification, data security 1...|$|R
40|$|We {{present a}} Kullback–Leibler (KL) control {{treatment}} of the fundamental problem of erasing a bit. We introduce notions of reliability of information storage via a reliability timescale τ r, and speed of erasing via an erasing timescale τ e. Our problem formulation captures the tradeoff between speed, reliability, and the KL cost required to erase a bit. We show that rapid erasing of a <b>reliable</b> <b>bit</b> costs at least log 2 - log 1 - e - τ e τ r > log 2, which goes to 1 2 log 2 τ r τ e when τ r > > τ e...|$|E
40|$|In {{our earlier}} work [Appl. Phys. Lett. 92, 022509 (2008) ], we {{proposed}} nonvolatile vortex {{random access memory}} (VRAM) based on the energetically stable twofold ground state of vortex-core magnetizations as information carrier. Here we experimentally demonstrate reliable memory bit selection and low-power-consumption recording in a two-by-two vortex-state dot array. The bit selection and core switching is made by flowing currents along two orthogonal addressing electrode lines chosen among the other crossed electrodes. Tailored pulse-type rotating magnetic fields are used for efficiently switching a vortex core only {{at the intersection of}} the two orthogonal electrodes. This robust mechanism provides <b>reliable</b> <b>bit</b> selection and information writing operations in a potential VRAM device...|$|E
40|$|This paper {{presents}} {{a method for}} shaping the transmit pulse of a molecular signal such that the diffusion channel's response is a sharp pulse. The impulse response of a diffusion channel is typically characterised as having an infinitely long transient response. This can cause severe inter-symbol-interference, and reduce the achievable <b>reliable</b> <b>bit</b> rate. We achieve the desired chemical channel response by poisoning the channel with a secondary compound, such that it chemically cancels aspects of the primary information signal. We use two independent methods {{to show that the}} chemical concentration of the information signal should be ∝ δ(t) and that of the poison signal should be ∝ t- 3 / 2...|$|E
3000|$|... 1 Since G {{is a full}} rank matrix by definition, its k rows being linearly {{independent}} vectors {{of length}} n>k, the Gauss-Jordan elimination always succeeds. However, in case the k columns of G corresponding to the initial k most reliable positions are not linearly independent, {{it may be necessary}} to replace some of the most <b>reliable</b> <b>bits</b> in v [...]...|$|R
40|$|This paper {{presents}} a soft decision decoding algorithm for Reed Solomon (RS) codes using their binary image representations. The novelty {{of the proposed}} decoding algorithm is in reducing the submatrix corresponding to the less <b>reliable</b> <b>bits</b> to a sparse nature prior to each decoding iteration. Simulation {{results show that the}} new method provides significant gain over hard decision decoding (HDD) and compares favorably with other popular soft decision decoding methods [1]...|$|R
3000|$|... is singular, we replace some {{elements}} of B with the indices of less <b>reliable</b> information <b>bits</b> until H [...]...|$|R
40|$|We {{present a}} KL-control {{treatment}} of the fundamental problem of erasing a bit. We introduce notions of "reliability" of information storage via a reliability timescale τ_r, and "speed" of erasing via an erasing timescale τ_e. Our problem formulation captures the tradeoff between speed, reliability, and the Kullback-Leibler (KL) cost required to erase a bit. We show that rapid erasing of a <b>reliable</b> <b>bit</b> costs at least 2 - (1 - e^-τ_e/τ_r) > 2, which goes to 1 / 22 τ_r/τ_e when τ_r>>τ_e. Comment: 14 pages, 3 figures. Conference version: Unconventional Computation and Natural Computation (2015), pp. 192 [...] 201, Springer International Publishing. Changes: Section 4 is substantially expanded {{with a discussion of}} possible physical meanings for the KL-cost functio...|$|E
40|$|Abstract—This paper {{presents}} {{a method for}} shaping the transmit pulse of a molecular signal such that the diffusion channel’s response is a sharp pulse. The impulse response of a diffusion channel is typically characterised as having an infinitely long transient response. This can cause severe inter-symbol-interference, and reduce the achievable <b>reliable</b> <b>bit</b> rate. We achieve the desired chemical channel response by poisoning the channel with a secondary compound, such that it chemically cancels aspects of the primary information signal. We use two independent methods {{to show that the}} chemical concentration of the information signal should be / (t) and that of the poison signal should be / t 3 / 2. I...|$|E
40|$|The optimum {{decoding}} of component {{codes in}} block coded modulation (BCM) schemes requires {{the use of}} the log-likelihood ratio (LLR) as the signal metric. An approximation to the LLR for the least <b>reliable</b> <b>bit</b> (LRB) in an 8 -PSK modulation based on planar equations with fixed point arithmetic is developed that is both accurate and easily realizable for practical BCM schemes. Through an error power analysis and an example simulation it is shown that the approximation results in 0. 06 dB in degradation over the exact expression at an E(sub s) /N(sub o) of 10 dB. It is also shown that the approximation can be realized in combinatorial logic using roughly 7300 transistors. This compares favorably to a look up table approach in typical systems...|$|E
40|$|Abstract — This paper {{presents}} a soft decision decoding algorithm for Reed Solomon (RS) codes using their binary image representations. The novelty {{of the proposed}} decoding algorithm is in reducing the submatrix corresponding to the less <b>reliable</b> <b>bits</b> to a sparse nature prior to each decoding iteration. Simulation {{results show that the}} new method provides significant gain over hard decision decoding (HDD) and compares favorably with other popular soft decision decoding methods [1]. I...|$|R
40|$|Adaptive Demodulation (ADM) is a newly {{proposed}} rate-adaptive {{system which}} operates without requiring Channel State Information (CSI) at the transmitter (unlike adaptive modulation) by using adaptive decision region boundaries at {{the receiver and}} encoding the data with a rateless code. This paper addresses the design and performance of an ADM scheme for two common differentially coherent schemes: M-DPSK (M-ary Differential Phase Shift Keying) and M-DAPSK (M-ary Differential Amplitude and Phase Shift Keying) operating over AWGN and Rayleigh fading channels. The optimal method for determining the most <b>reliable</b> <b>bits</b> for a given differential detection scheme is presented. In addition, simple (near-optimal) implementations are provided for recovering the most <b>reliable</b> <b>bits</b> from a received pair of differentially encoded symbols for systems using 16 -DPSK and 16 - DAPSK. The new receivers offer the advantages of a rate-adaptive system, without requiring CSI at the transmitter and a coherent phase reference at the receiver. Bit error analysis for the ADM system in both cases is presented along with numerical results of the spectral efficiency for the rate-adaptive systems operating over a Rayleigh fading channel. Comment: 25 pages, 11 Figures, submitted to IEEE Transactions on Communications, June 1, 201...|$|R
30|$|In {{a typical}} NUM formulation, user's {{utilities}} are static, predetermined functions, associated to specific services or service classes, emphasizing {{mainly on the}} support of non-real-time users' long-term requirements. Therefore, users' utilities mainly define a continuous relationship between their service performance and their actual achieved throughput (i.e., goodput that reflects the number of <b>reliable</b> <b>bits</b> transmitted) over the wireless opportunistic CDMA paradigm, while considering long-term user's fairness issues [16], minimum performance requirements [17] and/or appropriate constraints imposed by the devices' physical limitations [18].|$|R
40|$|Abstract—Physical unclonable {{functions}} (PUFs) and biomet-rics {{are inherently}} noisy. When used in practice as cryptographic key generators, {{they need to}} be combined with an extraction technique to derive <b>reliable</b> <b>bit</b> strings (i. e., cryptographic key). An approach based on an error correcting code was proposed by Dodis et al. and is known as a fuzzy extractor. However, this method appears to be difficult for non-specialists to implement. In our recent study, we reported the results of some exam-ple implementations using PUF data and presented a detailed implementation diagram. In this paper, we describe a more efficient implementation method by replacing the hash function output with the syndrome from the BCH code. The experimental results show that the Hamming distance between two keys vary according to the key size and information-theoretic security has been achieved...|$|E
40|$|A novel {{detector}} for multiple-input multiple-output (MIMO) communications is presented. The algorithm {{belongs to}} the class of the lattice detectors, i. e. it finds a reduced complexity {{solution to the problem}} of finding the closest vector to the received observations. The algorithm achieves optimal maximum-likelihood (ML) performance in case of two transmit antennas, at the same time keeping a complexity much lower than the exhaustive search-based ML detection technique. Also, differently from the state-of-art lattice detector (namely sphere decoder), the proposed algorithm is suitable for a highly parallel hardware architecture and for a <b>reliable</b> <b>bit</b> soft-output information generation, thus making it a promising option for real-time high-data rate transmission. Comment: To appear in Proc. of "Forty-Third Annual Allerton Conference on Communication, Control, and Computing" (Sept. 28 th- 30 th 2005). 13 pages 2 figures (4 plots...|$|E
40|$|Biometric {{authentication}} is {{a convenient}} and increasingly reli-able {{way to prove}} one’s identity. Iris scanning in particular {{is among the most}} accurate biometric authentication technologies currently available. However, despite their extremely high accuracy under ideal imaging conditions, existing iris recognition methods degrade when the iris images are noisy or the enrollment and verification imaging conditions are substantially different. To address this issue and enable iris recognition on less-than-ideal images, we introduce a weighted majority voting technique applicable to any biometric authentication system using bitwise comparison of enrollment-time and verification-time biometric templates. In a series of experiments with the CASIA iris database, we find that the method outperforms existing majority voting and <b>reliable</b> <b>bit</b> selection techniques. Our method is a simple and efficient means to improve upon the accu-racy of existing iris recognition systems. Index Terms — Biometrics, iris recognition, weighted majority voting 1...|$|E
3000|$|However, we {{can test}} whether {{the product of}} several most likely minimal polynomials is a factor of the {{generator}} polynomial to increase the successful recognition rate, because according to the adaptive processing of the parity-check matrices, the more parity equations we consider, the more {{we are able to}} construct a parity matrix which is parsed on less <b>reliable</b> <b>bits.</b> For the convenience of automatic recognition using computer programs, we propose the procedure including the following steps to estimate the optimal parity-check matrix: [...]...|$|R
40|$|International audienceIn this letter, {{we propose}} a {{probabilistic}} {{approach to the}} generation of test patterns in the Chase Algorithm (CA) denoted as the Stochastic Chase Algorithm (SCA). We compare the performance of SCA with the regular CA for different Reed-Solomon codes. Simulation {{results show that the}} probabilistic nature of the SCA helps in providing a more efficient test pattern generation. SCA avoids the use of least <b>reliable</b> <b>bits</b> selection and reduces the number of candidate codewords up to 60 % for the same decoding performance...|$|R
40|$|Abstract—In this letter, {{we propose}} a {{probabilistic}} {{approach to the}} generation of test patterns in the Chase Algorithm (CA) denoted as the Stochastic Chase Algorithm (SCA). We compare the performance of SCA with the regular CA for different Reed-Solomon codes. Simulation {{results show that the}} probabilistic nature of the SCA helps in providing a more efficient test pattern generation. SCA avoids the use of least <b>reliable</b> <b>bits</b> selection and reduces the number of candidate codewords up to 60 % for the same decoding performance. Index Terms—Stochastic decoding, Chase algorithm, soft decision decoding, Reed-Solomon codes...|$|R
40|$|Abstract. We {{propose a}} class of finite state systems of {{synchronizing}} distributed processes, where processes make assumptions at local states {{about the state of}} other processes in the system. This constrains the global states of the system to those where assumptions made by a process about another are compatible with the commitments offered by the other at that state. We model examples like <b>reliable</b> <b>bit</b> transmission and sequence transmission protocols in this framework and discuss how assumption-commitment structure facilitates compositional design of such protocols. We prove a decomposition theorem which states that every protocol specified globally as a finite state system can be decomposed into such an assumption compatible system. We also present a syntactic characterization of this class using top level parallel composition. Keywords. Assumption-commitment; automata theory; concurrency theory; verification; decomposition. 1. The assumption-commitment framework Compositionality is a desired criterion for verification methodologies, particularly fo...|$|E
30|$|Much of the {{previous}} work on energy efficient systems concentrates on network optimisation and scheduling policies. Macro-cell size reduction for better energy efficiency is investigated in [16], with positive results. Of course, reducing the cell-sizes means {{increasing the number of}} BSs in an area, which is generally rejected due to the enhanced infrastructure expenses. In [17], game-theoretic approaches are utilised to, minimise the cost per <b>reliable</b> <b>bit</b> sent in energy constrained networks. However, it is seen that there is a clear tradeoff between energy and spectral efficiency, and hence the energy-efficient resource allocations tend to be spectrally inefficient. This is further highlighted in [18], where an analytical model determines the optimal energy-spectral efficiency tradeoff for the downlink in OFDMA networks. In this article, however, we present an ICIC technique which utilises interfering link gains to not only provide interference mitigation and spectral efficiency gains in the uplink, but also generate large energy savings.|$|E
40|$|Landauer's {{principle}} {{states that}} the erasure of one bit of information requires the free energy kT ln 2. We argue that {{the reliability of the}} bit erasure process is bounded by the accuracy inherent in the statistical state of the energy source (`the resources') driving the process. We develop a general framework describing the `thermodynamic worth' of the resources with respect to <b>reliable</b> <b>bit</b> erasure or good cooling. This worth turns out to be given by the distinguishability of the resource's state from its equilibrium state {{in the sense of a}} statistical inference problem. Accordingly, Kullback-Leibler relative information is a decisive quantity for the `worth' of the resource's state. Due to the asymmetry of relative information, the reliability of the erasure process is bounded rather by the relative information of the equilibrium state wit respect to the actual state than by the relative information of the actual state with respect to the equilibrium state (which is the free energy up to constants). Comment: 18 pages, Revte...|$|E
3000|$|Upon {{receiving}} the {{signal from the}} AWGN channel and demodulating it into the sequence y, find the k most <b>reliable</b> received <b>bits</b> and collect them in a length-k vector v [...]...|$|R
40|$|Abstract — Compared with traditonal hard Bose-Chaudhuri-Hochquenghem (BCH) decoders, soft BCH decoders {{provide better}} error-correcting {{performance}} but much higher hardware complexity. In this brief, an improved soft BCH decoding algorithm {{is presented to}} achieve both competitive hardware complexity and better error-correcting performance by dealing with least <b>reliable</b> <b>bits</b> and compensating one extra error outside the least reliable set. For BCH (255, 239; 2) and (255, 231; 3) codes, our proposed soft BCH decoders can achieve up to 0. 75 -dB coding gain with one extra error compensation and 5 % less complexity than the traditional hard BCH decoders. Index Terms — Bose-Chaudhuri-Hochquenghem (BCH) codes, error-correction coding, soft decoding. I...|$|R
40|$|Adaptive Demodulation (ADM) is a {{new rate}} {{adaptive}} technique wherein the receiver demodulates only the most <b>reliable</b> <b>bits</b> and treats unreliable bits as erasures. This paper derives the optimum and simple near-optimum receivers for an ADM system operating without a coherent phase reference, where differential encoding is assumed at the transmitter (using 16 -DPSK and 16 -DAPSK). The new receivers offer the advantages of a rate-adaptive system, without requiring channel state information at the transmitter or a coherent phase reference at the receiver. Bit error analysis for the ADM system in both cases is presented along with numerical results of the spectral efficiency for the rate adaptive systems operating over a Rayleigh fading channel. I...|$|R
40|$|We {{consider}} {{the challenge of}} operating a <b>reliable</b> <b>bit</b> that can be rapidly erased. We find that both erasing and reliability times are non-monotonic in the underlying friction, leading to a trade-off between erasing speed and bit reliability. Fast erasure is possible {{at the expense of}} low reliability at moderate friction, and high reliability comes at the expense of slow erasure in the underdamped and overdamped limits. Within a given class of bit parameters and control strategies, we define 'optimal' designs of bits that meet the desired reliability and erasing time requirements with the lowest operational work cost. We find that optimal designs always saturate the bound on the erasing time requirement, but can exceed the required reliability time if critically damped. The non-trivial geometry of the reliability and erasing time scales allows us to exclude large regions of parameter space as suboptimal. We find that optimal designs are either critically damped or close to critical damping under the erasing procedure...|$|E
30|$|Teoh and Kim [40] {{applied a}} {{randomized}} dynamic quantization transformation to binarize fingerprint features extracted from a multichannel Gabor filter. Feature vectors of 375 bits are extracted and Reed-Solomon codes {{are applied to}} construct the fuzzy commitment scheme. The transformation comprises a non-invertible projection based on a random matrix derived from a user-specific token. It is required that this token is stored on a secure device. Similar schemes based on the feature extraction of BioHashing [41] (discussed later) have been presented in [42, 43]. Tong et al. [44] proposed a fuzzy extractor scheme based on a stable and order invariant representation of biometric data called Fingercode reporting inapplicable performance rates. Nandakumar [45] applies a binary fixed-length minutiae representation obtained by quantizing the Fourier phase spectrum of a minutia set in a fuzzy commitment scheme, where alignment is achieved through focal point of high curvature regions. In [46] a fuzzy commitment scheme based on face biometrics is presented in which real-valued face features are binarized by simple thresholding followed by a <b>reliable</b> <b>bit</b> selection to detect most discriminative features. Lu et al. [47] binarized principal component analysis (PCA) based face features which they apply in a fuzzy commitment scheme.|$|E
40|$|We {{consider}} the technologically relevant costs of operating a <b>reliable</b> <b>bit</b> {{that can be}} erased rapidly. We find that both erasing and reliability times are non-monotonic in the underlying friction, leading to a trade-off between erasing speed and bit reliability. Fast erasure is possible {{at the expense of}} low reliability at moderate friction, and high reliability comes at the expense of slow erasure in the underdamped and overdamped limits. Within a given class of bit parameters and control strategies, we define "optimal" designs of bits that meet the desired reliability and erasing time requirements with the lowest operational work cost. We find that optimal designs always saturate the bound on the erasing time requirement, but can exceed the required reliability time if critically damped. The non-trivial geometry of the reliability and erasing time-scales allows us to exclude large regions of parameter space as sub-optimal. We find that optimal designs are either critically damped or close to critical damping under the erasing procedure. Comment: 37 pages, 22 figures, Section 4 shortened by moving proofs to the Supplementary Information, Section A. 2 added to the Supplementary Information to introduce accuracy of erasure, small clarifications added throughout, To appear in the Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science...|$|E
30|$|The first {{variation}} {{inspired by}} [13] can further improve the performance by running the proposed algorithm several times each {{time with the}} same initial LLRs from the channel but a different grouping of the less <b>reliable</b> <b>bits.</b> This {{is based on the}} fact that some bits with their |LLR|s close to those in the unreliable set B are also of the wrong sign and vice versa. Each time the proposed algorithm is run, a different estimate of codeword may be obtained due to the different parity-check matrix. The decoder keeps all the returned codewords in a list and chooses the one that minimizes Euclidean distance from the received vector of the channel. This variation can significantly improve the asymptotic performance of polar codes.|$|R
40|$|Abstract—This paper {{provides}} a soft BCH decoder using error magnitudes {{to deal with}} least <b>reliable</b> <b>bits.</b> With soft information from the previous decoder defined in digital video broadcasting (DVB), the proposed soft BCH decoder provides much lower complexity and latency than the traditional hard BCH decoder while still maintaining performance. The proposed error locator evaluator architecture evaluates error locations without Chien search, leading to high throughput. Börck-Pereyra error magnitudes solvers (BP-EMS) is presented to improve decoding efficiency and hardware complexity. The experimental result reveals that our proposed soft (32400, 32208) BCH decoder defined in DVB-S 2 system can save 50. 0 % gate-count and achieve 314. 5 Mbps in standard CMOS 90 nm technology. Index Terms—Error correction coding, Bose-Chaudhuri-Hochquenghem (BCH) codes, Digital Video Broadcasting. I...|$|R
40|$|An {{iterative}} {{algorithm is}} presented for soft input soft output (SISO) decoding of Reed-Solomon (RS) codes. The proposed algorithm {{works at the}} bit level using the binary parity check matrix associated with the RS code. The novelty in the proposed algorithm is in reducing a submatrix of the binary parity check matrix that corresponds to less <b>reliable</b> <b>bits</b> to a sparse nature before each decoding iteration. The proposed algorithm can be geometrically interpreted as a gradient descent with an adaptive potential function. This adaptive procedure {{is crucial to the}} convergence behavior of the gradient descent algorithm and, therefore, significantly improves the decoding performance. Simulation results show that the proposed decoding algorithm and its variations provide significant gain over hard decision decoding (HDD) and compares favorably with other popular soft decision decoding methods both in terms of error performance and complexity...|$|R
