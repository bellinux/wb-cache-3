57|236|Public
25|$|A slow read attack sends {{legitimate}} {{application layer}} requests, but reads responses very slowly, thus trying to exhaust the server's connection pool. It {{is achieved by}} advertising {{a very small number}} for the TCP Receive Window size, {{and at the same time}} emptying clients' TCP <b>receive</b> <b>buffer</b> slowly, which causes a very low data flow rate.|$|E
50|$|A {{session is}} {{set up by the}} client sending a DSIOpenSession, which will include the size of the <b>receive</b> <b>buffer</b> the client has for packets (called the request quantum, {{typically}} 1024 bytes). The server acknowledges the request and returns the size of its data <b>receive</b> <b>buffer</b> (typically 256k on Mac OS X Leopard).|$|E
5000|$|RFC 5041 Direct Data Placement over Reliable Transports is layered over MPA/TCP or SCTP. It defines how {{received}} {{data can be}} directly placed into an upper layer protocol's <b>receive</b> <b>buffer</b> without intermediate buffers.|$|E
30|$|The {{number of}} {{neighbours}} of a mote {{is greater than}} the number of frames that the <b>receiving</b> <b>buffer</b> of the CC 2420 chip can hold.|$|R
40|$|Low-latency {{transmit}} {{and receive}} queues — IEEE 802. 3 x-compliant flow-control support with software-controllable thresholds — Caches up to 64 packet descriptors {{in a single}} burst — Programmable host memory <b>receive</b> <b>buffers</b> (256 B to 16 KB) and cache line size (16 B to 256 B) — Wide, optimized internal data path architecture — 64 KB configurable Transmit and <b>Receive</b> FIFO <b>buffer...</b>|$|R
40|$|AbstractBased on {{the impact}} {{investigation}} of both bandwidth and <b>receiving</b> <b>buffer</b> on concurrent multi-path transmission (CMT) schemes of Stream Control Transmission Protocol (SCTP), the authors present an adaptive policy on how to effectively apply CMT- SCTP schemes under various network conditions. Moreover, an optimal fast SACK (FACK) scheme is proposed too in this paper in order to accelerate the feedback of SACKs and further find out varieties of both available bandwidth and loss rate as soon as possible. Simulation {{results show that the}} proposed scheme could improve the traffic efficiency of the CMT scheme when <b>receiving</b> <b>buffer</b> is bounded and network bandwidth gets smaller...|$|R
50|$|One {{cause of}} bit {{slippage}} is overflow of a <b>receive</b> <b>buffer</b> {{that occurs when}} the transmitter's clock rate exceeds that of the receiver. This causes one or more bits to be dropped for lack of storage capacity.|$|E
50|$|On {{the other}} hand, a {{non-blocking}} socket returns whatever {{is in the}} <b>receive</b> <b>buffer</b> and immediately continues. If not written correctly, programs using non-blocking sockets are particularly susceptible to race conditions due to variances in network link speed.|$|E
50|$|Slow Read attack sends {{legitimate}} {{application layer}} requests but reads responses very slowly, thus trying to exhaust the server's connection pool. Slow reading {{is achieved by}} advertising {{a very small number}} for the TCP Receive Window size {{and at the same time}} by emptying clients' TCP <b>receive</b> <b>buffer</b> slowly. That naturally ensures a very low data flow rate.|$|E
40|$|This paper {{studies the}} buffer {{reduction}} schemes for the Transport Protocol TCP. By using network simulations, {{it shows that}} a TCP connection requires large number of <b>receive</b> <b>buffers</b> to achieve high throughput. However, it also shows that theses buffers are not utilized efficiently. To improve utilization of the buffers this paper proposes common <b>receive</b> <b>buffers</b> for multiple TCP connections. Evaluation by simulations proves that the common buffer scheme can {{reduce the number of}} required buffers. Although segment losses are observed, its effect to the total throughput is allowable. This paper also studies outstanding buffers for the sending side of TCP and shows that large amount of outstanding buffers are held in provision for retransmissions. To mitigate this problem this paper proposes utilization of secondary storage for the outstanding buffers. Although extra delay is introduced for retransmissions, its effect to the total performance is limited...|$|R
50|$|Packet {{switching}} features {{delivery of}} {{variable bit rate}} data streams, realized as sequences of packets, over a computer network which allocates transmission resources as needed using statistical multiplexing or dynamic bandwidth allocation techniques. As they traverse network nodes, such as switches and routers, packets are <b>received,</b> <b>buffered,</b> queued, and transmitted (stored and forwarded), resulting in variable latency and throughput depending on the link capacity and the traffic load on the network.|$|R
40|$|Abstract—Digital Video Broadcasting for Handheld {{terminals}} (DVB-H) is assuming an ever growing {{importance for}} digital video transmission over wireless terminals. In such a context, Time Slicing has been implemented {{to achieve a}} better power saving and manage handover. Specifically, a generic user transmits bursts of data, interspaced by time periods in which no data are transmitted. In this paper, to improve time sliced multiservice transmission effectiveness, the Variable Burst Time (VBT) algorithm is presented and discussed. It dynamically varies the whole set of stream Burst Durations according to input stream data, available channel bandwidth, <b>receiving</b> <b>buffer</b> size and eventually stream priority. Burst Durations are derived by the minimization of a Total Loss Function (TLF) representing the amount of losses of the whole service set. Numerical results show the VBT effectiveness if compared with the time sliced transmission recommended in the DVB guidelines, for different numbers, types and quality degrees of VBR streams, <b>receiving</b> <b>buffer</b> sizes and stream priorities. This suggests that VBT could be efficiently exploited for transmission of VBR streams in DVB-H systems...|$|R
5000|$|TCP Window Scaling is {{implemented}} in Windows since Windows 2000. It is enabled by default in Windows Vista / Server 2008 and newer, {{but can be}} turned off manually if required.Windows Vista and Windows 7 have a fixed default TCP <b>receive</b> <b>buffer</b> of 64 kB, scaling up to 16 MB through [...] "autotuning", limiting manual TCP tuning over long fat networks.|$|E
50|$|A flit (flow control units/digits) is a {{unit amount}} of data when the message is {{transmitting}} in link-level. The flit can be accepted or rejected at the receiver side base on the flow control protocol {{and the size of}} the <b>receive</b> <b>buffer.</b> The mechanism of link-level flow control is allowing the receiver to send a continuous signals stream to control if it should keep sending flits or stop sending flits. When a packet is transmitted over a link, the packet will need to be split into multiple flits before the transmitting begin.|$|E
50|$|Another {{feature of}} L2 {{is the ability}} for an L2 {{transmitter}} to know whether there is buffer space for the data frame at the receiving end. This again relies on L2 control frames (AFC) which allow a receiver to tell the peer's transmitter how much buffer space is available. This allows the receiver to pause the transmitter if needed, thus avoiding <b>receive</b> <b>buffer</b> overflow. Control frames are unaffected by L2 flow control: they can be sent {{at any time and}} the L2 receiver is expected to process these at the speed at which they arrive.|$|E
40|$|PCI Express * (PCIe*) — 64 -bit address master {{support for}} systems using more than 4 GB of {{physical}} memory — Programmable host memory <b>receive</b> <b>buffers</b> (256 bytes to 16 KB) — Intelligent interrupt generation features to enhance driver performance — Descriptor ring management hardware for transmit and receive software controlled reset (resets {{everything except the}} configuration space) — Message Signaled Interrupts (MSI and MSI-X) — Configurable receive and transmit data FIFO, programmable in 1 KB increment...|$|R
40|$|For {{the problem}} of low code gain of the {{concatenation}} of Reed Solomon (RS) codes and convolutional codes (CC), the concatenated scheme of RS+CC and Luby Transform (LT) codes with an improved decoding algorithm is proposed. The improved decoding algorithm for LT codes decreases the <b>receiving</b> <b>buffer</b> consumption and improves the successful decoding probability and the real-time property of LT codes at some low computational cost. Simulations and analyses are made to evaluate {{the performance of the}} concatenated scheme in terms of the error correction capacity and cost...|$|R
50|$|The Virtual Machine Queue (VMQ) is a {{hardware}} virtualization {{technology for the}} efficient transfer of network traffic (such as TCP/IP, iSCSI or FCoE) to a virtualized host OS. VMQ technology was patented in 2010 by Daniel Baumberger of Intel Corp. A VMQ capable NIC can use DMA to transfer all incoming frames that should be routed to a receive queue to the <b>receive</b> <b>buffers</b> that are allocated for that queue. The miniport driver can indicate all of the frames that are in a receive queue in one receive indication call.|$|R
50|$|A modern {{practice}} has evolved to divide hardware interrupt handlers into front-half and back-half elements. The front-half (or first level) receives the initial interrupt {{in the context}} of the running process, does the minimal work to restore the hardware to a less urgent condition (such as emptying a full <b>receive</b> <b>buffer)</b> and then marks the back-half (or second level) for execution in the near future at the appropriate scheduling priority; once invoked, the back-half operates in its own process context with fewer restrictions and completes the handler's logical operation (such as conveying the newly received data to an operating system data queue).|$|E
50|$|At {{the link}} layer the {{protocol}} {{relies on the}} use of credits in order to signal whether data can be transmitted or not. Receivers always issue credits to the transmitter before the transmitter is allowed to send any data. A link can give 8, 32, or 64 credits, but it should never issue more than 127 credits which guarantees that the other side can always record credits using 7 bit counters. Conversely, if large numbers of credits are given out the link needs to have large receive buffers; the minimum size of a <b>receive</b> <b>buffer</b> is 8 tokens (72 bits).|$|E
5000|$|In {{order to}} {{minimize}} {{the number of times}} received data is copied, the <b>receive</b> <b>buffer</b> for payload data is received directly into a page aligned disk buffer. If the connection is encrypted, the buffer is decrypted in-place. The buffer is then moved into the disk cache without being copied. Once all the blocks for a piece have been received, or the cache needs to be flushed, all the blocks are passed directly to writev (...) to flush them in a single syscall. This means a single copy into user space memory, and a single copy back into kernel memory.|$|E
30|$|Once a node <b>receives</b> the <b>buffer</b> update from a {{group member}} or the leader, it will record the latest buffer {{availability}} of these nodes included in the received update.|$|R
40|$|The FlexRay {{controller}} {{supports a}} single communication channel, {{and can be}} configured for either Channel A or Channel B. The Cyclic Redundancy Check (CRC) seed used to generate the Header CRC is different for Channel A and for Channel B. Therefore, connecting the FlexRay controller that is configured for Channel A to Channel B in the cluster (or vice-versa) is not allowed. Configurable Payload Length The Max Payload that can be supported by the controller during synchronous or aysnchronous transmission is user-configurable, depending upon specific design requirements. Configurable number of Transmit Buffers The number of transmit buffers in the FlexRay controller can be user-configured from {{a minimum of two}} to a maximum of 128 in powers of two. The block RAM resource utilization varies, {{based on the number of}} Transmit buffers and the configurable payload length. Configurable number of <b>Receive</b> <b>Buffers</b> The number of <b>Receive</b> <b>buffers</b> in the FlexRay controller can be user-configured from a minimum of two to a maximum of 128 in powers of two. The block RAM resource utilization varies, based on the number of <b>Receive</b> <b>buffers</b> and the configurable payload length. Configurable Receive FIFO depth The depth of the Receive FIFO in the FlexRay controller can be user-configured from a minimum of two to a maximum of 128 in powers of two. The block RAM resource utilization varies, based on the depth of the Receive FIFO and the configurable payload length. Frame ID, Cycle Counter and Message ID based Receive Filtering The FlexRay controller supports filtering of receive messages based on Frame ID, cycle counter and message ID. Any or all of these combinations can be used for filtering the receive messages. Applications The FlexRay controller is typically used in automotive networked applications, offering high fault tolerant synchronous and asynchronous transfer capabilities. It can function as a stand-alone FlexRay controller or it can be integrated with other Xilinx LogiCORE and EDK cores to build a variety of embedded systems. The FlexRay solution allows for flexible partioning between software and hardware functions by providing a customized, scalable controller that off loads network specific overhead from the host processor. This is ideal for applications demanding maximum host application performance...|$|R
50|$|Early {{implementations}} used <b>receive</b> <b>buffers</b> {{that were}} only 3 bytes deep, and a send buffer with a single byte. This meant that the real-world performance was limited by the host platform's ability to continually empty the buffers into its own memory. With network-like communications the SCC itself could cause the remote sender to stop transmission when the buffers were full, and thereby prevent data loss while the host was busy. With conventional async serial this was not possible; on the Macintosh Plus this limited RS-232 performance to about 9600 bit/s or less, and as little as 4800 bit/s on earlier models.|$|R
5000|$|Internally, the QL {{comprised}} the CPU, two ULAs, (ZX8301 and ZX8302) and an Intel 8049 microcontroller (known as the IPC, or [...] "Intelligent Peripheral Controller"). The ZX8301 or [...] "Master Chip" [...] implemented the video display generator and also provided DRAM refresh. The ZX8302, or [...] "Peripheral Chip", interfaced to the RS-232 ports (transmit only) Microdrives, QLAN ports, real-time clock and the 8049 (via a synchronous serial link). The 8049 (included at late {{stage in the}} QL's design, the ZX8302 originally being intended to perform its functions) ran at 11 MHz and acted as a keyboard/joystick interface, RS-232 <b>receive</b> <b>buffer</b> and audio generator.|$|E
5000|$|The sender transmits {{the rest}} of the message using Consecutive Frames. Each Consecutive Frame has a one byte PCI, with a four bit type (type = 2) {{followed}} by a 4-bit sequence number. The sequence number starts at 1 and increments with each frame sent (1, 2,..., 15, 0, 1,...), with which lost or discarded frames can be detected.Each consecutive frame starts at 0, initially for the first set of data in the first frame will be considered as 0th data. So the first set of CF(Consecutive frames) start from [...] "1". There afterwards when it reaches [...] "15", will be started from [...] "0". The 12 bit length field (in the FF) allows up to 4095 bytes of user data in a segmented message, but in practice the typical application-specific limit is considerably lower because of <b>receive</b> <b>buffer</b> or hardware limitations.|$|E
40|$|Abstract In {{heterogeneous}} {{wireless network}} environments, network connections of a device {{may have a}} significant differential. This paper studies the effect of path bandwidth differential {{on the performance of}} fast retransmission strategies in Multi-homing environments. It identifies that fast retransmission on an alternate path may cause <b>receive</b> <b>buffer</b> blocking when path bandwidth differential is significant and the <b>receive</b> <b>buffer</b> is limited. A theoretical model is proposed for selecting retransmission path during the fast retransmission phase, which is based on <b>receive</b> <b>buffer</b> and path conditions. The model is verified through simulations with various path differentials. ...|$|E
40|$|Abstract. The default {{messaging}} {{model for}} the OpenFabrics “Verbs” API is to consume <b>receive</b> <b>buffers</b> in order—regardless of the actual incoming message size—leading to inefficient registered memory usage. For example, many small messages can consume large amounts of registered memory. This paper introduces a new transport protocol in Open MPI implemented using the existing OpenFabrics Verbs API that exhibits efficient registered memory utilization. Several real-world applications were run at scale with the new protocol; results show that global network resource utilization efficiency increases, allowing increased scalability—and larger problem sizes—on clusters which can increase application performance in some cases. ...|$|R
5000|$|Frame delay {{variations}} (FDV): Also {{known as}} packet jitter, {{this is a}} measurement of the variations in the time delay between packet deliveries. As packets travel through a network to their destination, they are often queued and sent in bursts to the next hop. There may be prioritization at random moments also resulting in packets being sent at random rates. Packets are therefore received at irregular intervals. The direct consequence of this jitter is stress on the <b>receiving</b> <b>buffers</b> of the end nodes where buffers can be overused or underused when there are large swings of jitter.|$|R
5000|$|The Linux kernel packet {{scheduler}} is configured {{using the}} userspace CLI utility called [...] (short for [...] "traffic control"). As the default queuing discipline, the packet scheduler uses a FIFO implementation called pfifo_fast, although systemd since its version 217 changes the default queuing discipline to fq_codel. The packet scheduler {{is an integral}} part of the Linux kernel's network stack and manages the ring buffers of all NICs, by working on the layer 2 of the OSI model and handling Ethernet frames, for example. It manages the transmit and <b>receive</b> <b>buffers</b> of all NICs installed in a computer.|$|R
40|$|This paper {{presents}} a preliminary {{investigation into the}} impact of TCP's advertised <b>receive</b> <b>buffer</b> size and timer granularity on TCP performance over erroneous links in a LEO satellite environment. Conducted simulations include over 200 different combinations of TCP flavor, advertised <b>receive</b> <b>buffer</b> size, timer granularity, and bit error rate. Results show that TCP can only approximate the variable propagation delay for unsaturated links and that the minimum timer granularity which prevents a premature expiration of the RTO depends on the advertised <b>receive</b> <b>buffer</b> size. Low BERs do not influence TCP's capability to track the variable propagation delay in contrast to high BERs. Final {{results indicate that the}} relative performance degradation of TCP over erroneous links does not depend on the ability to estimate the variable propagation delay...|$|E
40|$|Abstract In {{heterogeneous}} network environments, the network connections of a multi-homed device may have significant bandwidth differential. For a multihomed transmission protocol designed for network failure tolerance, such as SCTP, path selection algorithms for data transmission drastically affect performance. This article studies {{the effect of}} path bandwidth differential {{on the performance of}} retransmission strategies in multi-homing environments. It identifies that fast retransmission on an alternate path may cause <b>receive</b> <b>buffer</b> blocking when path bandwidth differential is significant and the <b>receive</b> <b>buffer</b> is limited. A theoretical model is proposed for selecting retransmission path during the fast retransmission phase, based on <b>receive</b> <b>buffer</b> and path conditions. From these observations and analysis results, this article proposes that path selection strategies for transmitting new data and retransmitted data should be decoupled. A new path selection scheme is proposed and evaluated through SCTP simulations. Keywords Multi-homing, Transmission Protocol...|$|E
40|$|IFIPIn {{heterogeneous}} network environments, the network connections of a multi-homed device may have significant bandwidth differential. For a multi-homed transmission protocol designed for network failure tolerance, such as SCTP, path selection algorithms for data transmission drastically affect performance. This article studies {{the effect of}} path bandwidth differential {{on the performance of}} retransmission strategies in multi-homing environments. It identifies that fast retransmission on an alternate path may cause <b>receive</b> <b>buffer</b> blocking when path bandwidth differential is significant and the <b>receive</b> <b>buffer</b> is limited. A theoretical model is proposed for selecting retransmission path during the fast retransmission phase, based on <b>receive</b> <b>buffer</b> and path conditions. From these observations and analysis results, this article proposes that path selection strategies for transmitting new data and retransmitted data should be decoupled. A new path selection scheme is proposed and evaluated through SCTP simulations...|$|E
30|$|All groups {{except for}} the saline group {{required}} preparation of the HA/TCP carrier for injection. The procedure is as follows: first, 0.2  g HA/TCP (particle size < 0.53  µm) was mixed with 100  µl of the appropriate rhBMP- 2 dose. The mixture was allowed to stand for 15  min to allow the rhBMP- 2 to bind to the HA/TCP carrier. After 15  min, 100  µl of buffer solution was added and mixed. The mixture was then drawn into a 1 -ml syringe ready for injection into the femoral canal. Note that the HA/TCP control group <b>received</b> <b>buffer</b> solution twice in place of rhBMP- 2. All groups received approximately 0.1  ml injections into the femur.|$|R
40|$|OBJECTIVE: To {{test the}} {{hypothesis}} that ibuprofen administration during modified muscle use reduces muscle necrosis and invasion by select myeloid cell populations. METHODS: Rats were subjected to hindlimb unloading for 10 days, after which they experienced muscle reloading by normal weight-bearing to induce muscle inflammation and necrosis. Some animals received ibuprofen by intraperitoneal injection 8 h prior to the onset of muscle reloading, and then again at 8 and 16 h following the onset of reloading. Other animals <b>received</b> <b>buffer</b> injection at 8 h prior to reloading and then ibuprofen at 8 and 16 h following the onset of reloading. Control animals <b>received</b> <b>buffer</b> only at each time point. Quantitative immunohistochemical {{analysis was used to}} assess the presence of necrotic muscle fibers, total inflammatory infiltrate, neutrophils, ED 1 + macrophages and ED 2 + macrophages at 24 h following the onset of reloading. RESULT: Administration of ibuprofen beginning 8 h prior to reloading caused significant reduction in the concentration of necrotic fibers, but increased the concentration of inflammatory cells in muscle. The increase in inflammatory cells was attributable to a 2. 6 -fold increase in the concentration of ED 2 + macrophages. Animals treated with ibuprofen 8 h following the onset of reloading showed no decrease in muscle necrosis or increase in ED 2 + macrophage concentrations. CONCLUSION: Administration of ibuprofen prior to increased muscle loading reduces muscle damage, but increases the concentration of macrophages that express the ED 2 antigen. The increase in ED 2 + macrophage concentration and decrease in necrosis may be mechanistically related because ED 2 + macrophages have been associated with muscle regeneration and repair...|$|R
40|$|Abstract-The {{de facto}} {{requirement}} in traditional telephone networks is to restore failures in 50 milliseconds or less. The same standard has been assumed in data networks. In {{this study we}} consider the reaction of TCP to a failure in a continental-scale network. Our goal is {{to determine whether there}} are particular values for outage duration at which file transfer times increase markedly. Such values would indicate significant objectives for the restoration of networks carrying TCP traffic. For SACK and NewReno TCP, we find that a restoration objective of 600 ms to 1 s is appropriate. In addition, we also find that <b>receive</b> <b>buffers</b> can be sized at 2 rτ to maximize link utilization and resilience. Index Terms-TCP, resilience, data network. I...|$|R
