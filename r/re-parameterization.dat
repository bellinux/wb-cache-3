122|23|Public
2500|$|The {{renormalization}} group {{concept of}} the second kind {{is associated with the}} nature of the mathematical computation used to discover the set of parameters [...] In its essence, the calculation starts with some specific form of a theory with cutoff [...] and derives a corresponding theory with a smaller cutoff, in the sense of more restrictive, say [...] After <b>re-parameterization</b> using the cutoff as a unit, one obtains a new theory of similar type but with new terms. This means that the starting theory with cutoff [...] should also contain such new terms for its form to be consistent with the presence of a cutoff. Eventually, one can find a set of terms that reproduces itself up to changes in the coefficients of the required terms. These coefficients evolve with the number of steps one makes, in each and every step reducing the cutoff by factor of two and rescaling variables. One could use other factors than two, but two is convenient.|$|E
50|$|ProbOnto {{knowledge}} base stores such <b>re-parameterization</b> formulas {{to allow for}} a correct translation of models between tools.|$|E
5000|$|ProbOnto is a {{knowledge}} base and [...] ontology of probability distributions. ProbOnto 2.5 (released on January 16, 2017) contains over 150 uni- and multivariate distributions and alternative parameterizations, more than 220 relationships and <b>re-parameterization</b> formulas, supports encoding of empirical and univariate mixture distributions.|$|E
40|$|We present two Decision Support Systems [DSSs] as {{decision}} aids {{to health}} planners {{in evaluating the}} expenditures needed to launch and sustain the 80 / 20 policy initiative of the IOM. The core DSS is called the Dynamic Change DSS and {{it is assumed that}} there is unrestricted activity in the parameter set. The other DSS is called the SWAP DSS; it assumes stasis in the final total number of individuals in the service population and so only a swap between the service classes is permitted. The SWAP is a benchmark to the Dynamic Change information set which is the launching pad to the What-If <b>re-parameterizations</b> that may be needed to address sustainability. The interactive model for these What-If <b>re-parameterizations</b> is an e-Delphi Workspace. These DSSs, the Pilot-Test parameterized versions, and the respective manuals are available from the corresponding author at no charge and with no restriction on their use...|$|R
50|$|ProbOnto {{stores in}} Version 2.5 over 220 {{relationships}} between univariate distributions with <b>re-parameterizations</b> {{as a special}} case, see figure. While this form of relationships is often neglected in literature, and the authors concentrate one a particular form for each distribution, they are crucial from the interoperability point of view. ProbOnto focuses on this aspect and features more than 15 distributions with alternative parameterizations.|$|R
40|$|We {{find that}} (1) several stable spatio-temporal {{representations}} co-exist {{in a given}} cell which permits identification and selection of different motor programs to operate the external device, and (2) these multiple representations can be extracted from the multi-electrode neuron spike patterns reflecting various spatial <b>re-parameterizations</b> compatible with the ones imposed by the external mechanical device. A neural theoretical formulation {{in terms of a}} Hodgkin-Huxley excitatory and inhibitory neural ring network is used [5] to model multi-electrode spiking statistics, explicitly considering the separation of different motor dynamical times...|$|R
50|$|Let's {{consider}} the situation when one {{would like to}} run a model using two different optimal design tools, e.g. PFIM and PopED. The former supports the LN2, the latter LN7 parameterization, respectively. Therefore, the <b>re-parameterization</b> is required, otherwise the two tools would produce different results.|$|E
50|$|The Milne {{model was}} a special-relativistic cosmological model {{proposed}} by Edward Arthur Milne in 1935. It is mathematically {{equivalent to a}} special case of the FLRW model {{in the limit of}} zero energy density (in other words, an empty universe) and it obeys the cosmological principle. The Milne model is also similar to Rindler space, a simple <b>re-parameterization</b> of flat Minkowski space.|$|E
5000|$|The {{renormalization}} group {{concept of}} the second kind {{is associated with the}} nature of the mathematical computation used to discover the set of parameters [...] In its essence, the calculation starts with some specific form of a theory with cutoff [...] and derives a corresponding theory with a smaller cutoff, in the sense of more restrictive, say [...] After <b>re-parameterization</b> using the cutoff as a unit, one obtains a new theory of similar type but with new terms. This means that the starting theory with cutoff [...] should also contain such new terms for its form to be consistent with the presence of a cutoff. Eventually, one can find a set of terms that reproduces itself up to changes in the coefficients of the required terms. These coefficients evolve with the number of steps one makes, in each and every step reducing the cutoff by factor of two and rescaling variables. One could use other factors than two, but two is convenient.|$|E
40|$|Abstract. This paper {{illustrates}} {{and extends}} an efficient framework, called the square-root-elastic (SRE) framework, for studying shapes of closed curves, that {{was first introduced}} in [2]. This framework combines the strengths of two important ideas- elastic shape metric and path-straightening methods- for finding geodesics in shape spaces of curves. The elastic metric allows for optimal matching of features between curves while path-straightening ensures that the algorithm results in geodesic paths. This paper extends this framework by removing two important shape preserving transformations: rotations and <b>re-parameterizations,</b> by forming quotient spaces and constructing geodesics on these quotient spaces. These ideas are demonstrated using experiments involving 2 D and 3 D curves. ...|$|R
40|$|Abstract. We {{address the}} problem of {{segmenting}} an image into a previ-ously unknown number of segments from the perspective of graph parti-tioning. Specifically, we consider minimum multicuts of superpixel affin-ity graphs in which all affinities between non-adjacent superpixels are negative. We propose a relaxation by Lagrangian decomposition and a constrained set of <b>re-parameterizations</b> for which we can optimize ex-actly and efficiently. Our contribution is to show how the planarity of the adjacency graph can be exploited if the affinity graph is non-planar. We demonstrate the effectiveness of this approach in user-assisted image segmentation and show that the solution of the relaxed problem is fast and the relaxation is tight in practice. ...|$|R
40|$|This paper {{illustrates}} {{and extends}} an efficient framework, called the square-root-elastic (SRE) framework, for studying shapes of closed curves, that {{was first introduced}} in [2]. This framework combines the strengths of two important ideas - elastic shape metric and path-straightening methods - for finding geodesics in shape spaces of curves. The elastic metric allows for optimal matching of features between curves while path-straightening ensures that the algorithm results in geodesic paths. This paper extends this framework by removing two important shape preserving transformations: rotations and <b>re-parameterizations,</b> by forming quotient spaces and constructing geodesics on these quotient spaces. These ideas are demonstrated using experiments involving 2 D and 3 D curves...|$|R
40|$|We analyze {{a common}} feature of a nontrivial {{fractional}} flux periodicity in two-dimensional systems. We demonstrate that an addition of fractional flux can be absorbed into <b>re-parameterization</b> of quantum numbers. For an exact fractional periodicity, all the electronic states undergo the <b>re-parameterization,</b> whereas for an approximate periodicity valid {{in a large}} system, only the states near the Fermi level {{are involved in the}} <b>re-parameterization.</b> Comment: 4 pages, 1 figure, minor changes, final version to appear in J. Phys. Soc. Jp...|$|E
30|$|The {{extension}} of <b>re-parameterization</b> to mixture experiments {{with more than}} four components is very simple.|$|E
3000|$|... }],[*]x[*]>[*] 0,[*]λ[*]>[*] 0,[*]α,[*]β[*]>[*] 0 after {{appropriate}} <b>re-parameterization.</b> Reliability {{characteristics and}} parameter {{estimation of the}} above particular cases are also discussed in detail by Al-Masoud (2013).|$|E
40|$|We {{address the}} problem of {{segmenting}} an image into a previously unknown number of segments from the perspective of graph partitioning. Specifically, we consider minimum multicuts of superpixel affinity graphs in which all affinities between non-adjacent superpixels are negative. We propose a relaxation by Lagrangian decomposition and a constrained set of <b>re-parameterizations</b> for which we can optimize exactly and efficiently. Our contribution is to show how the planarity of the adjacency graph can be exploited if the affinity graph is non-planar. We demonstrate the effectiveness of this approach in user-assisted image segmentation and show that the solution of the relaxed problem is fast and the relaxation is tight in practice...|$|R
40|$|Abstract. We {{consider}} {{the task of}} computing shape statistics and classification of 3 D anatomical structures (as continuous, parameterized surfaces) under a Rie-mannian framework. This task requires a Riemannian metric that allows: (1) <b>re-parameterizations</b> of surfaces by isometries, and (2) efficient computations of geodesic paths between surfaces. These tools allow for computing Karcher means and covariances (using tangent PCA) for shape classes, and a probabilistic clas-sification of surfaces into disease and control classes. In a separate paper [13], we introduced a mathematical representation of surfaces, called q-maps, and we used the L 2 metric on the space of q-maps to induce a Riemannian metric on the space of parameterized surfaces. We also developed a path-straightening al-gorithm for computing geodesic paths [14]. This process requires optimal <b>re-parameterizations</b> (deformations of grids) of surfaces and achieves a superior alignment of geometric features across surfaces. The resulting means and co-variances are better representatives of the original data and lead to parsimonious shape models. These two moments specify a normal probability model on shape classes, which are then used for classifying test shapes. Through improved ran-dom sampling and a higher classification performance, we demonstrate the suc-cess of this model over some past methods. In addition to toy objects, we use the Detroit Fetal Alcohol and Drug Exposure Cohort data to study brain structures and present classification results for the Attention Deficit Hyperactivity Disorder cases and controls in this study. We find that using the mean and covariance struc-ture of the given data, {{we are able to}} attain a 88 % classification rate, which is an improvement over a previously reported result of 82 % on the same data...|$|R
40|$|We {{introduce}} {{a framework for}} analyzing symmetry of 2 D and 3 D objects using elastic deformations of their boundaries. The basic idea is to define spaces of elastic shapes and to compute shortest (geodesic) paths between the objects and their reflections using a Riemannian structure. Elastic matching, based on optimal (nonlinear) <b>re-parameterizations</b> of curves, provides a better registration of points across shapes, {{as compared to the}} previously-used linear registrations. A crucial step of orientation alignment, akin to finding planes of symmetry, is performed as a search for shortest geodesic paths. This framework is fully automatic and provides: a measure of asymmetry, the nearest symmetric shape, the optimal deformation to make an object symmetric, and the plane of symmetry for a given object. Anglai...|$|R
40|$|Hidden {{truncation}} (HT) and additive component (AC) are two {{well known}} paradigms of generating skewed distributions from known symmetric distribution. In case of normal distribution {{it has been}} known that both the above paradigms lead to Azzalini’s (1985) skew normal distribution. While the HT directly gives the Azzalini’s (1985) skew normal distribution, the one generated by AC also leads to the same distribution under a <b>re-parameterization</b> proposed by Arnold and Gomez (2009). But no such <b>re-parameterization</b> which leads to exactly the same distribution by these two paradigms {{has so far been}} suggested for the skewed distributions generated from symmetric logistic and Laplace distributions. In this article, an attempt has been made to investigate numerically as well as statistically the closeness of skew distributions generated by HT and AC methods under the same <b>re-parameterization</b> of Arnold and Gomez (2009) in the case of logistic and Laplace distributions...|$|E
40|$|A {{new method}} to obtain {{explicit}} <b>re-parameterization</b> that preserves the curve degree and parametric domain {{is presented in}} this paper. The <b>re-parameterization</b> brings a curve {{very close to the}} arc length parameterization under L 2 norm but with less segmentation. The <b>re-parameterization</b> functions we used are C 1 continuous piecewise rational linear functions, which provide more flexibility and can be easily identified by solving a quadratic equation. Based on the outstanding performance of Mobius transformation on modifying pieces with monotonic parametric speed, we first create a partition of the original curve, in which the parametric speed of each segment is of monotonic variation. The values of new parameters corresponding to the subdivision points are specified a priori as the ratio of its cumulative arc length and its total arc length. C 1 continuity conditions are imposed to each segment, thus, with respect to the new parameters, the objective function is linear and admits a closed-form optimization. Illustrative examples are also given to assess the performance of our new method...|$|E
40|$|AbstractThe {{weighted}} low-rank approximation {{problem in}} general has no analytical solution {{in terms of}} the singular value decomposition and is solved numerically using optimization methods. Four representations of the rank constraint that turn the abstract problem formulation into parameter optimization problems are presented. The parameter optimization problem is partially solved analytically, which results in an equivalent quadratically constrained problem. A commonly used <b>re-parameterization</b> avoids the quadratic constraint and makes the equivalent problem a nonlinear least squares problem, however, it might be necessary to change this <b>re-parameterization</b> during the iteration process. It is shown how the cost function can be computed efficiently in two special cases: row-wise and column-wise weighting...|$|E
40|$|We {{introduce}} {{a framework for}} analyzing symmetry of 3 D anatomical structures using elastic deformations of their boundaries (surfaces). The basic idea is to define a space of parameterized surfaces and to compute geodesic paths between the objects and their arbitrary reflections using a Riemannian structure. Elastic matching, based on opti-mal (non-linear) <b>re-parameterizations</b> (grid deformations) of surfaces, provides a better registration of points across shapes, {{as compared to the}} commonly-used linear regis-trations. A crucial step of orientation alignment, akin to finding planes of symmetry, is performed as a search for shortest geodesic paths. This framework is fully automatic and provides a measure of symmetry, the nearest symmet-ric shape and the optimal deformation to make an object symmetric. We demonstrate this framework through multi-ple toy examples on simple and complicated surfaces. We also explore the use of symmetry analysis in differentiating between healthy and subjects with Attention Deficit Hyper-activity Disorder. 1...|$|R
40|$|Ó The Author(s) 2011. This {{article is}} {{published}} with open access at Springerlink. com Abstract Quantitative genetics {{stems from the}} theoretical models of genetic effects, which are <b>re-parameterizations</b> of the genotypic values into parameters of biological (genetic) relevance. Different formulations of genetic effects are adequate to address different subjects. We thus need to generalize and unify them under a common framework for enabling researchers to easily transform genetic effects between different biological meanings. The Natural and Orthogonal Interactions (NOIA) model of genetic effects has been developed to achieve this aim. Here, we further implement the statistical formulation of NOIA with multiple alleles under Hardy–Weinberg departures (HWD). We show that our developments are straightforwardly connected to the decomposition of the genetic variance and we point out several emergent properties of multiallelic quantitative genetic models, {{as compared to the}} biallelic ones. Further, NOIA entails a natural extension of one-locus developments to multiple epistatic loci under linkage equilibrium. Therefore, we present an extension of the orthogona...|$|R
40|$|Let k> 2. We {{prove that}} the cotangent bundles of {{oriented}} homotopy (2 k- 1) -spheres S and S' are symplectomorphic only if the classes defined by S and S' agree up to sign in the quotient group of oriented homotopy spheres modulo those which bound parallelizable manifolds. We also show that if the connect sum of real projective space of dimension (4 k- 1) and a homotopy (4 k- 1) -sphere admits a Lagrangian embedding in complex projective space, then twice the homotopy sphere framed bounds. The proofs build on previous work of Abouzaid and the authors, in combination with a new cut-and-paste argument, which also gives rise to some interesting explicit exact Lagrangian embeddings into plumbings. As another application, we show that there are <b>re-parameterizations</b> of the zero-section in the cotangent bundle of a sphere which are not Hamiltonian isotopic (as maps, rather than as submanifolds) to the original zero-section. Comment: 19 pages, no figures. Version 2 : Theorem 1. 4 added. Version 3 : Clarified attributions, typographical correction...|$|R
40|$|The {{weighted}} low-rank approximation {{problem in}} general has no analytical solution {{in terms of}} the singular value decomposition and is solved numerically using optimization methods. Four representations of the rank constraint that turn the abstract problem formulation into parameter optimization problems are presented. The parameter optimization problem is partially solved analytically, which results in an equivalent quadratically constrained problem. A commonly used <b>re-parameterization</b> avoids the quadratic constraint and makes the equivalent problem a nonlinear least squares problem, however, it might be necessary to change this <b>re-parameterization</b> during the iteration process. It is shown how the cost function can be computed efficiently in two special cases: row-wise and column-wise weighting. (c) 2006 Elsevier Inc. All rights reserved. status: publishe...|$|E
40|$|Statistical shape models (SSMs) are a {{well-established}} tool in medical image analysis. The most challenging part of SSM construction, which cannot be solved trivially in 3 D, is {{the establishment of}} corresponding points, so-called landmarks. A popular approach for solving the correspondence problem is to minimize a groupwise objective function using the optimization by <b>re-parameterization</b> approach. To this end, several objective functions, optimization strategies and <b>re-parameterization</b> functions have been proposed. While previous evaluation studies focused mainly on the objective function, we provide a detailed evaluation of different correspondence methods, objective functions, <b>re-parameterization,</b> and optimization strategies. Moreover and contrary to previous works, we use distance measures that compare landmark shape vectors to the original input shapes, thus adequately accounting for correspondences which undersample certain regions of the input shapes. Additionally, we segment binary expert segmentations to benchmark SSMs constructed from different correspondences. This new evaluation technique overcomes limitations of the correspondence based evaluation and allows for directly quantifying {{the influence of the}} correspondence on the expected segmentation accuracy. From our evaluation results we identify pitfalls of the current approach and derive practical recommendations for implementing a groupwise optimization pipeline...|$|E
40|$|Model-based invariants are {{relations}} between model parameters and image measurements, which {{are independent of}} the imaging parameters. Such relations are true for all images of the model. Here we describe an algorithm which, given L independent model-based polynomial invariants describing some shape, will provide a linear <b>re-parameterization</b> of the invariants. This <b>re-parameterization</b> has the properties that: (i) it includes the minimal number of terms, and (ii) the shape terms are the same in all the model-based invariants. This final representation has 2 main applications: (1) it gives new representations of shape in terms of hyperplanes, which are convenient for object recognition; (2) it allows the design of new linear shape from motion algorithms. In addition, we use this representation to identify object classes that have universal invariants...|$|E
40|$|Quantitative {{genetics}} {{stems from}} the theoretical models of genetic effects, which are <b>re-parameterizations</b> of the genotypic values into parameters of biological (genetic) relevance. Different formulations of genetic effects are adequate to address different subjects. We thus need to generalize and unify them under a common framework for enabling researchers to easily transform genetic effects between different biological meanings. The Natural and Orthogonal Interactions (NOIA) model of genetic effects has been developed to achieve this aim. Here, we further implement the statistical formulation of NOIA with multiple alleles under Hardy–Weinberg departures (HWD). We show that our developments are straightforwardly connected to the decomposition of the genetic variance and we point out several emergent properties of multiallelic quantitative genetic models, {{as compared to the}} biallelic ones. Further, NOIA entails a natural extension of one-locus developments to multiple epistatic loci under linkage equilibrium. Therefore, we present an extension of the orthogonal decomposition of the genetic variance to multiple epistatic, multiallelic loci under HWD. We illustrate this theory with a graphical interpretation and an analysis of published data on the human acid phosphatase (ACP 1) polymorphism...|$|R
40|$|In {{this paper}} we {{introduce}} a novel Riemannian framework for shape analysis of parameterized surfaces. We derive a distance function between any two surfaces that is invariant to rigid motion, global scaling, and reparametrization. It {{is the last}} part that presents the main difficulty. Our {{solution to this problem}} is twofold: (1) we define a special representation, called a q-map, to represent each surface, and (2) we develop a gradient-based algorithm to optimize over different <b>re-parameterizations</b> of a surface. The second step is akin to deforming the mesh on a fixed surface to optimize its placement. (This is different from the current methods that treat the given meshes as fixed.) Under the chosen representation, with the L 2 metric, the action of the re-parametrization group is by isometries. This results in, to our knowledge, the first Riemannian distance between parameterized surfaces to have all the desired invariances. We demonstrate this framework with several examples using some toy shapes, and real data with anatomical structures, and cropped facial surfaces. We also successfully demonstrate clustering and classification of these objects under the proposed metric. 1...|$|R
40|$|We {{consider}} generic curves in R^ 2, i. e. generic C^ 1 functions f : S^ 1 → R^ 2. We analyze these curves {{through the}} persistent homology groups of a filtration induced on S^ 1 by f. In particular, {{we consider the}} question whether these persistent homology groups uniquely characterize f, at least up to reparameterizationsof S^ 1. We give a partially positive answer to this question. More precisely, we prove that f = g ◦ h, where h : S^ 1 → S^ 1 is a C^ 1 -diffeomorphism, {{if and only if}} the persistent homology groups of s ◦ f and s ◦g coincide, for every s belonging to the group S generated by reflections in the coordinate axes. Moreover, for a smaller set of generic functions, we show that f and g are close to each other in themax-norm (up to <b>re-parameterizations)</b> if and only if, for every s in S, the persistent Betti number functions of s ◦ f and s ◦ g are close to each other, with respect to a suitable distance...|$|R
40|$|The {{algorithm}} for {{surface modeling}} and volume grid generation using parametric Non-Uniform Rational B-splines (NURBS) geometric representation are presented. The enhanced <b>re-parameterization</b> algorithm which can yield a desired physical distribution on the curve, surface, and volume is also presented. This approach bridges {{the gap between}} computer aided design surface/volume definition and surface/volume grid generation...|$|E
40|$|Abstract—This paper {{introduces}} a square-root velocity (SRV) representation for analyzing shapes of curves in Euclidean spaces under an elastic metric. Due to this SRV representation the elastic metric simplifies to the L 2 metric, the <b>re-parameterization</b> group acts by isometries, {{and the space}} of unit length curves becomes the unit sphere. The shape space of closed curves is quotient space of (a submanifold of) the unit sphere, modulo rotation and <b>re-parameterization</b> groups, and we find geodesics in that space using a path-straightening approach. These geodesics and geodesic distances {{provide a framework for}} optimally matching, deforming and comparing shapes. These ideas are demonstrated using: (i) Shape analysis of cylindrical helices for studying protein backbones, (ii) Shape analysis of facial curves for recognizing faces, (iii) A wrapped probability distribution for capturing shapes of planar closed curves, and (iv) Parallel transport of deformations for predicting shapes from novel poses. Index Terms—Elastic curves, Riemannian shape analysis, elastic metric, Fisher-Rao metric, square-root representations, path-straightening method, elastic geodesics, parallel transport, shape models. F...|$|E
40|$|Rational re-parameterizations of a {{polynomial}} curve that preserve the curve degree and [0, 1] parameter domain {{are characterized by}} a single degree of freedom. The “optimal ” <b>re-parameterization</b> in this family (that comes closest under the L 2 norm to arc-length parameterization) can be identified by solving a quadratic equation, but may exhibit too much residual parametric speed variation for motion control and other applications. Closer approximations to arc-length parameterizations require more flexible <b>re-parameterization</b> functions, such as piecewise-polynomial/rational forms. We show that, for fixed nodes, the optimal piecewise-rational parameterization of the same degree is defined by a simple recursion relation, and we analyze its convergence to the arc-length parameterization. With respect to the new curve parameter, this representation is only of C 0 continuity, although the smoothness and geometry of the curve are unchanged. A C 1 parameterization {{can be obtained by}} using continuity conditions, rather than optimization, to fix certain free parameters, but the objective function is then highly non-linear and does not admit a closed-form optimization. Empirical results fro...|$|E
40|$|Abstract—Current {{techniques}} for shape analysis tend to seek invariance to similarity transformations (rotation, translation and scale), but certain imaging situations require invariance to larger groups, such as affine or projective groups. Here {{we present a}} general Riemannian framework for shape analysis of planar objects where metrics and related quantities are invariant to affine and projective groups. Highlighting two possibilities for representing object boundaries – ordered points (or landmarks) and parameterized curves – we study different combinations of these representations (points and curves) and transformations (affine and projective). Specifically, we provide solutions to {{three out of four}} situations and develop algorithms for computing geodesics and intrinsic sample statistics, leading up to Gaussian-type statistical models, and classifying test shapes using such models learned from training data. In the case of parameterized curves, we also achieve the desired goal of invariance to <b>re-parameterizations.</b> The geodesics are constructed by particularizing the path-straightening algorithm to geometries of current manifolds and are used, in turn, to compute shape statistics and Gaussian-type shape models. We demonstrate these ideas using a number of examples from shape and activity recognition. Index Terms—Affine shape analysis, projective shape analysis, path straightening method, geodesic computation, shape models F...|$|R
40|$|We {{argue that}} full surface {{correspondence}} (registration) and optimal deformations (geodesics) are two related problems and propose {{a framework that}} solves them simultaneously. We build on the Riemannian shape analysis of anatomical and star-shaped surfaces of Kurtek et al. and focus on articulated complex shapes that undergo elastic deformations and that may contain missing parts. Our core contribution is the re-formulation of Kurtek et al. 's approach as a constrained optimization over all possible <b>re-parameterizations</b> of the surfaces, using a sparse set of corresponding landmarks. We introduce a landmark-constrained basis, which we use to numerically solve this optimization and therefore establish full surface registration and geodesic deformation between two surfaces. The length of the geodesic provides a measure of dissimilarity between surfaces. The advantages of this approach are: (1) simultaneous computation of full correspondence and geodesic between two surfaces, given a sparse set of matching landmarks (2) ability to handle more comprehensive deformations than nearly isometric, and (3) the geodesics and the geodesic lengths can be further used for symmetrizing 3 D shapes and for computing their statistical averages. We validate the framework on challenging cases of large isometric and elastic deformations, and on surfaces with missing parts. We also provide multiple examples of averaging and symmetrizing 3 D models...|$|R
40|$|This paper {{addresses}} visualization {{issues of}} the Terrestrial Planet Finder Mission[2]. The goal of this mission is to search for chemical signatures of life in distant solar systems using five satellites flying in formation to simulate a large telescope. To design and visually verify such a delicate mission one has to analyze and interact with many different 3 D spacecraft trajectories, which is often difficult in 2 D. We employ a novel trajectory design approach using invariant manifold theory, which is best understood and utilized in an immersive setting. The visualization also addresses multi-scale {{issues related to the}} vast differences in distance, velocity, and time at different phases of the mission. Additionally, the parameterization and coordinate frames used for numerical simulations may not be suitable for direct visualization. Relative motion presents a more serious problem where the patterns of the trajectories can only be viewed in particular rotating frames. Some of these problems are greatly relieved by using interactive, animated stereo 3 D visualization in a semi-immersive environment such as a Responsive Workbench. Others were solved using standard techniques such as a stratify approach with multiple windows to address the multiscale issues, <b>re-parameterizations</b> of trajectories and associated 2 D manifolds and relative motion of the camera to "evoke" the desired patterns...|$|R
