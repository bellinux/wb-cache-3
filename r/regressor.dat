1943|3100|Public
25|$|There {{may be some}} {{relationship}} between the regressors. For instance, the third <b>regressor</b> may be {{the square of the}} second <b>regressor.</b> In this case (assuming that the first <b>regressor</b> is constant) we have a quadratic model in the second <b>regressor.</b> But this is still considered a linear model because it is linear in the βs.|$|E
25|$|In case of {{a single}} <b>regressor,</b> fitted by least squares, R2 is {{the square of the}} Pearson {{product-moment}} correlation coefficient relating the <b>regressor</b> and the response variable. More generally, R2 is the square of the correlation between the constructed predictor and the response variable. With more than one <b>regressor,</b> the R2 can be referred to as the coefficient of multiple determination.|$|E
25|$|The theorem {{can be used}} to {{establish}} a number of theoretical results. For example, having a regression with a constant and another <b>regressor</b> is equivalent to subtracting the means from the dependent variable and the <b>regressor</b> and then running the regression for the demeaned variables but without the constant term.|$|E
40|$|This {{paper is}} {{concerned}} primarily with the asymptotic {{distribution of the}} least squares estimator in a linear equation with stochastic <b>regressors.</b> We prove a central limit theorem dealing with a sequence of products of random variables. The theorem is then applied to show asymptotic normality of the least squares estimator {{in a wide variety}} of cases, including: a) autoregressive <b>regressors,</b> b) moving average <b>regressors,</b> c) lagged dependent variable <b>regressors.</b> The results are generalized to handle Aitken estimation with stochastic <b>regressors,</b> and instrumental variable estimation in simultaneous equation models...|$|R
40|$|We study {{statistical}} {{properties of}} coefficient {{estimates of the}} partially linear regression model when some or all <b>regressors,</b> in the unknown part of the model, are discrete. The method does not require smoothing in the discrete variables. Unlike when there are continuous <b>regressors.</b> when all <b>regressors</b> are discrete independence between <b>regressors</b> and regression errors is not required. We also give some guidance on how to implement the estimate when there are both continuous and discrete <b>regressors</b> in the unknown part of the model. Weights employed in this paper seem straightforwardly applicable to other semiparametric problems...|$|R
40|$|A {{system of}} multivariate semiparametric {{nonlinear}} time series models is studied with possible dependence structures and nonstationarities in the parametric and nonparametric components. The parametric <b>regressors</b> may be endogenous while the nonparametric <b>regressors</b> {{are assumed to}} be strictly exogenous. The parametric <b>regressors</b> may be stationary or nonstationary and the nonparametric <b>regressors</b> are nonstationary integrated time series. Semiparametric least squares (SLS) estimation is considered and its asymptotic properties are derived. Due to endogeneity in the parametric <b>regressors,</b> SLS is not consistent for the parametric component and a semiparametric instrumental variable (SIV) method is proposed instead. Under certain regularity conditions, the SIV estimator of the parametric component is shown to have a limiting normal distribution. The rate of convergence in the parametric component depends on the properties of the <b>regressors.</b> The conventional √ n rate may apply even when nonstationarity is involved in both sets of <b>regressors...</b>|$|R
25|$|Sometimes {{one of the}} regressors can be a {{non-linear}} {{function of}} another <b>regressor</b> or of the data, as in polynomial regression and segmented regression. The model remains linear {{as long as it}} is linear in the parameter vector β.|$|E
25|$|The exogeneity {{assumption}} {{is critical for}} the OLS theory. If it holds then the <b>regressor</b> variables are called exogenous. If it doesn't, then those regressors that are correlated with the error term are called endogenous, and then the OLS estimates become invalid. In such case the method of instrumental variables {{may be used to}} carry out inference.|$|E
25|$|Using {{relatively}} recent night light data and electricity consumption {{in comparison with}} Gross County Product, the informal sector of the local economy in Veracruz state is shown to have grown {{during the period of}} the Fox Administration though the regional government remained PRI. The assumption that the informal economy of Mexico is a constant 30% of total economic activity is not supported at the local level. The small amount of local spatial autocorrelation that was found suggests a few clusters of high and low literacy rates amongst municipios in Veracruz but not enough to warrant including an I-statistic as a <b>regressor.</b> Global spatial autocorrelation is found especially literacy at the macro-regional level which is an area for further research beyond this study.|$|E
40|$|Abstract _ We study {{statistical}} {{properties of}} coefficient {{estimates of the}} partially linear regression model when some or all <b>regressors.</b> in the unknown part of the model. are discrete. The method does not require smoothing in the discrete variables. Unlike when there are continuous <b>regressors.</b> when all <b>regressors</b> are discrete independence between <b>regressors</b> and regression errors is not required. We also give some guidance on how to implement the estimate when there are both continuous and discrete <b>regressors</b> in the unknown part of the model. Weights employed in this paper seem straightforwardly applicable to other semiparametric problems...|$|R
40|$|A {{system of}} vector semiparametric {{nonlinear}} time series models is studied with possible dependence structures and nonstationarities in the parametric and nonparametric components. The parametric <b>regressors</b> may be endogenous while the nonparametric <b>regressors</b> are strictly exogenous. The parametric <b>regressors</b> may be stationary or nonstationary and the nonparametric <b>regressors</b> are nonstationary time series. Semiparametric least squares (SLS) estimation is considered and its asymptotic properties are derived. Due to endogeneity in the parametric <b>regressors,</b> SLS {{is not consistent}} for the parametric component and a semiparametric instrumental variable least squares (SIVLS) method is proposed instead. Under certain regularity conditions, the SIVLS estimator of the parametric component is shown {{to be consistent with}} a limiting normal distribution. Interestingly, the rate of convergence in the parametric component depends on the properties of the <b>regressors.</b> It has been shown that the conventional rate is still achievable even when nonstationarity is involved in both the <b>regressors.</b> Dynamic simultaneous equation, endogeneity, exogeneity, non-stationarity, partially linear model, vector semiparametric regression...|$|R
40|$|International audienceAccurate {{detection}} of facial landmarks {{is very important}} for many applications like face recognition or analysis. In this paper we describe an efficient detector of facial landmarks based on a cascade of boosted <b>regressors</b> of arbitrary number of levels. We define as many <b>regressors</b> as landmarks and we train them separately. We describe how the training is conducted for the series of <b>regressors</b> by supplying training samples centered on the predictions of the previous levels. We employ gradient boosted regression and evaluate three different kinds of weak elementary <b>regressors,</b> each one based on Haar features: non parametric <b>regressors,</b> simple linear <b>regressors</b> and gradient boosted trees. We discuss trade-offs between the number of levels and the number of weak <b>regressors</b> for optimal detection speed. Experiments performed on three datasets suggest that our approach is competitive compared to state-of-the art systems regarding precision, speed as well as stability of the prediction on video streams...|$|R
25|$|In statistics, {{ordinary}} {{least squares}} (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, {{with the goal of}} minimizing the sum of the squares {{of the differences between the}} observed responses (values of the variable being predicted) in the given dataset and those predicted by a linear function of a set of explanatory variables. Visually this is seen as the sum of the squared vertical distances between each data point in the set and the corresponding point on the regression line – the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a single <b>regressor</b> on the right-hand side.|$|E
2500|$|As a rule, the {{constant}} term is always {{included in the}} set of regressors X, say, by taking x'i1=1 for all [...] The coefficient β1 corresponding to this <b>regressor</b> is called the intercept.|$|E
2500|$|If {{the data}} matrix X {{contains}} only two variables, a constant and a scalar <b>regressor</b> xi, {{then this is}} called the [...] "simple regression model". This case is often considered in the beginner statistics classes, as it provides much simpler formulas even suitable for manual calculation. The parameters are commonly denoted as : ...|$|E
40|$|This paper {{establishes}} that <b>regressors</b> in {{the models}} with censored dependent variables {{need not be}} bounded for the standard asymptotic results to apply. Thus, <b>regressors</b> that grow monotonically with the observation index may be acceptable. It also purports to provide an upper bound on {{the rate at which}} <b>regressors</b> may grow. ...|$|R
5000|$|If P <b>regressors</b> are {{selected}} from a set of K > P, the Cp statistic for that particular set of <b>regressors</b> is defined as: ...|$|R
40|$|We {{consider}} {{the problem of}} estimating a nonparametric regression model containing categorical <b>regressors</b> only. We investigate the theoretical properties of least squares cross-validated smoothing parameter selection, establish the rate of convergence (to zero) of the smoothing parameters for relevant <b>regressors,</b> and {{show that there is}} a high probability that the smoothing parameters for irrelevant <b>regressors</b> converge to their upper bound values, thereby automatically smoothing out the irrelevant <b>regressors.</b> A small-scale simulation study shows that the proposed cross-validation-based estimator performs well in finite-sample settings. ...|$|R
2500|$|... {{autocorrelation}} {{that was}} found suggests a few clusters {{of high and low}} literacy rates amongst municipios in Veracruz but not enough to warrant including an I-statistic as a <b>regressor.</b> Global spatial autocorrelation is found especially literacy at the macro-regional level which is an area for further research beyond this study. Improved literacy bolsters both the informal and formal economies in Veracruz indicating policies designed to further literacy are vital for growing the regional economy.|$|E
2500|$|When {{only one}} {{dependent}} variable is being modeled, a scatterplot will suggest {{the form and}} strength {{of the relationship between}} the dependent variable and regressors. It might also reveal outliers, heteroscedasticity, and other aspects of the data that may complicate the interpretation of a fitted regression model. [...] The scatterplot suggests that the relationship is strong and can be approximated as a quadratic function. OLS can handle non-linear relationships by introducing the <b>regressor</b> HEIGHT2. [...] The regression model then becomes a multiple linear model: ...|$|E
2500|$|The {{variance}} of this estimator {{is equal to}} , which does not attain the Cramér–Rao bound of [...] However it was shown {{that there are no}} unbiased estimators of σ2 with variance smaller than that of the estimator s2. If we are willing to allow biased estimators, and consider the class of estimators that are proportional to the sum of squared residuals (SSR) of the model, then the best (in the sense of the mean squared error) estimator in this class will be , which even beats the Cramér–Rao bound in case when there is only one <b>regressor</b> (...) [...]|$|E
40|$|Fixed effects (FE) in panel {{data models}} overlap {{each other and}} {{prohibit}} {{the identification of the}} impact of "constant" <b>regressors.</b> Think of <b>regressors</b> that are constant across countries in a country-time panel with time FE. The traditional approach is to drop some FE and constant <b>regressors</b> by normalizing their impact to zero. We introduce "untangling normalization", meaning that we orthogonalize the FE and, if present, the constant <b>regressors.</b> The untangled FE are much easier to interpret. Moreover, the impact of constant <b>regressors</b> can now be estimated, and the untangled FE indicate to what extent the estimates reflect the true value. Our untangled estimates are a linear transformation of the traditional, zero-normalized estimates; no new estimation is needed. We apply the approach to a gravity model for OECD countries' exports to the US. The constant <b>regressors</b> US GDP, world GDP and the US effective exchange rate explain 90 % of the time FE, making the latter redundant, so the estimated impacts indeed reflect the true value...|$|R
40|$|Hassler [Hassler, U., 1996. Spurious {{regressions}} when stationary <b>regressors</b> are included, Economics Letters, 50, 25 – 31] {{shows that}} t- and F-tests for zero restrictions on I(0) <b>regressors</b> in equations with an I(1) dependent variable do not diverge to infinity asymptotically. He concludes {{that there is}} no spurious significance for these <b>regressors.</b> Using Monte Carlo simulation we demonstrate that spurious correlation generally occurs in such regressions...|$|R
40|$|This paper formulates {{a mixture}} model for {{modeling}} unobserved heterogeneity of explanatory mechanism. Our model allows for {{different sets of}} <b>regressors</b> and/or different interactions among the same <b>regressors</b> in different regression regimes. The model is demonstrated with particular interest to the censored dependent variable. A two-step procedure is proposed for model identification. The {{first step is to}} identify the number of regression regimes with each regime, including all <b>regressors.</b> The second step is to select <b>regressors</b> in the regression regimes. The results of our simulation studies suggest that the procedure works well. Two microeconometric applications are provided. ...|$|R
50|$|There {{may be some}} {{relationship}} between the regressors. For instance, the third <b>regressor</b> may be {{the square of the}} second <b>regressor.</b> In this case (assuming that the first <b>regressor</b> is constant) we have a quadratic model in the second <b>regressor.</b> But this is still considered a linear model because it is linear in the βs.|$|E
50|$|In case of {{a single}} <b>regressor,</b> fitted by least squares, R2 is {{the square of the}} Pearson {{product-moment}} correlation coefficient relating the <b>regressor</b> and the response variable. More generally, R2 is the square of the correlation between the constructed predictor and the response variable. With more than one <b>regressor,</b> the R2 can be referred to as the coefficient of multiple determination.|$|E
50|$|The theorem {{can be used}} to {{establish}} a number of theoretical results. For example, having a regression with a constant and another <b>regressor</b> is equivalent to subtracting the means from the dependent variable and the <b>regressor</b> and then running the regression for the demeaned variables but without the constant term.|$|E
40|$|This paper {{addresses}} {{the problem of}} endogenous <b>regressors</b> due {{to the presence of}} unobserved heterogeneity, when this is correlated with the <b>regressors,</b> and caused by regressors' measurement errors. A simple two-stage testing procedure is proposed for the identification of the underlying cause of correlation between <b>regressors</b> and the error term. The statistical performance of the resulting sequential test is assessed using simulated data. Copyright 2007 Blackwell Publishing Ltd. ...|$|R
40|$|ABSTRACT. In {{the present}} work, a {{dependence}} {{analysis of the}} determination coefficient {{and the number of}} <b>regressors</b> was carried out on a set of 65 mycotoxins. The simple and multiple regression techniques were used to identify the linear relationship between retention times and molecular descriptors calculated with HyperChem from the optimized 3 D structures. The highest number of <b>regressors</b> to be used in investigating the retention times as function of mycotoxins investigated properties must be equal to 5. As far as the dependence between number of <b>regressors</b> and determination coefficient was concerned, the analysis revealed that the best relationship is linear if the cutoff is set at 4 or 5 <b>regressors</b> while exponential for more than 5 <b>regressors.</b> The results must be verified on other classes of compounds and other dependent or independent variables in order to be extrapolated...|$|R
40|$|This paper {{presents}} and discusses procedures for estimating regression curves when <b>regressors</b> are discrete and applies them to semiparametric inference problems. We show that pointwise root-n-consistency and global consistency of regression curve estimates are achieved without employing any smoothing, even for discrete <b>regressors</b> with unbounded support. These results still hold when smoothers are used, under much weaker conditions than those required with continuous <b>regressors.</b> Such estimates {{are useful in}} semiparametric inference problems. We discuss in detail the partially linear regression model and shape-invariant modelling. We also provide some guidance on estimation in semiparametric models where continuous and discrete <b>regressors</b> are present. The paper also includes a Monte Carlo study...|$|R
5000|$|The {{solution}} of LS-SVM <b>regressor</b> will be obtained after we construct the Lagrangian function: ...|$|E
5000|$|This {{model is}} {{identifiable}} in two cases: (1) either the latent <b>regressor</b> x* is not normally distributed, (2) or x* has normal distribution, but neither εt nor ηt are divisible by a normal distribution. That is, the parameters α, β can be consistently estimated {{from the data}} set [...] without any additional information, provided the latent <b>regressor</b> is not Gaussian.|$|E
5000|$|In this case, the {{coefficient}} on the <b>regressor</b> {{of interest is}} given by [...] Substituting for : ...|$|E
40|$|The present paper {{addresses}} the selection-of-regressors issue into a general discrimination framework. We show how this framework {{is useful in}} unifying various procedures for selecting <b>regressors</b> and helpful in understanding the different strategies underlying these procedures. We review selection of <b>regressors</b> in linear, nonlinear and nonparametric regression models. In each case we successively consider model selection criteria and hypothesis testing procedures. Selection of <b>regressors,</b> Discrimination, JEL Classification: Primary C 52 : Secondary C 20,...|$|R
40|$|This thesis {{investigates the}} finite sample {{performance}} of the fully modified OLS estimator for cointegrating polynomial regressions (CPR),developed by Wagner and Hong (2016), including stationary <b>regressors</b> to the model. To be precise, this thesis considers regressions including deterministic variables, integrated processes, powers of integrated processes and stationary variables as explanatory variables and stationary errors. The errors are allowed to be serially correlated and the <b>regressors</b> are allowed to be endogenous except for the stationary <b>regressors</b> where both cases, i. e. predetermined or endogenous stationary <b>regressors,</b> are examined in this thesis. The basis for the finite sample performance investigation is a simulation study which shows that the assumption of allowing endogeneity of the stationary <b>regressors</b> {{can not be made}} as in this case the FM-OLS estimator seem to be not consistent anymore and statistical inference is no longer feasible for every level of serial correlation and endogeneity...|$|R
40|$|Abstract. Sparse {{regression}} is {{the problem}} of selecting a parsimonious subset of all available <b>regressors</b> for an efficient prediction of a target variable. We consider a general setting in which both the target and <b>regressors</b> may be multivariate. The <b>regressors</b> are selected by a forward selection procedure that extends the Least Angle Regression algorithm. Instead of the common practice of estimating each target variable individually, our proposed method chooses sequentially those <b>regressors</b> that allow, on average, the best predictions of all the target variables. We illustrate the procedure by an experiment with artificial data. The method is also applied to the task of selecting relevant pixels from images in multidimensional scaling of handwritten digits. ...|$|R
