26|63|Public
2500|$|Typical <b>RGB</b> <b>input</b> {{devices are}} color TV and video cameras, image scanners, and digital cameras. Typical RGB output devices are TV sets of various {{technologies}} (CRT, LCD, plasma, OLED, quantum dots, etc.), computer and mobile phone displays, video projectors, multicolor LED displays and large screens such as JumboTron. [...] Color printers, {{on the other}} hand are not RGB devices, but subtractive color devices (typically CMYK color model).|$|E
50|$|The monitor {{supports}} 15/24 and 31 kHz {{with up to}} 65,535 {{colors and}} functions as a cable-ready television (NTSC-J standard) with composite video input. It was an excellent monitor for playing JAMMA compatible arcade boards due to its analog <b>RGB</b> <b>input</b> and standard-resolution refresh timing.|$|E
50|$|Typical <b>RGB</b> <b>input</b> {{devices are}} color TV and video cameras, image scanners, and digital cameras. Typical RGB output devices are TV sets of various {{technologies}} (CRT, LCD, plasma, OLED, Quantum-Dots etc.), computer and mobile phone displays, video projectors, multicolor LED displays and large screens such as JumboTron. Color printers, {{on the other}} hand are not RGB devices, but subtractive color devices (typically CMYK color model).|$|E
5000|$|QuickLogic (Ultra Low Power Sensor Hubs, {{extremely}} low powered, low density SRAM-based FPGAs, Display bridges MIPI & <b>RGB</b> <b>inputs,</b> MIPI, <b>RGB</b> and LVDS outputs) ...|$|R
50|$|The {{original}} specification defined pin 16 as a {{high frequency}} (up to 3 MHz) signal that blanked the composite video. The <b>RGB</b> <b>inputs</b> were always active and the signal 'punches holes' in the composite video. This {{could be used to}} overlay subtitles from an external Teletext decoder.|$|R
30|$|Convert the <b>input</b> <b>RGB</b> image into a grey image.|$|R
50|$|The JVC D-VHS deck {{released}} in the UK, was not a bitstream recorder, although it did have a FireWire input. Instead it is best {{thought of as a}} digital recorder for traditional analog inputs such as domestic analog TV and digiboxes for digital broadcasts. The deck was able to record D-VHS signals onto S-VHS tapes, which made it a cost-effective source of high quality domestic recordings (USA version also allows use of cheaper S-VHS media). Pictures were noticeably superior to S-VHS and were essentially transparent when compared to an off-air source. Using the LS3 mode, approximately 17.25 hours of digital video could be stored on a S-VHS E-240. The deck's biggest shortcomings were the lack of a DV output and, perhaps more crucially, the lack of <b>RGB</b> <b>input</b> via the SCART connector. NTSC versions had component outputs.|$|E
50|$|There {{are five}} {{variations}} of the S3TC algorithm (named DXT1 through DXT5, referring to the FourCC code assigned by Microsoft to each format), each designed for specific types of image data. All convert a 4×4 block of pixels to a 64-bit or 128-bit quantity, resulting in compression ratios of 6:1 with 24-bit <b>RGB</b> <b>input</b> data or 4:1 with 32-bit RGBA input data. S3TC is a lossy compression algorithm, resulting in image quality degradation, an effect which is minimized by the ability to increase texture resolutions while maintaining the same memory requirements. Hand-drawn cartoon-like images do not compress well, nor do normal map data, both of which usually generate artifacts. ATI's 3Dc compression algorithm is a modification of DXT5 designed to overcome S3TC's shortcomings with regard to normal maps. id Software worked around the normalmap compression issues in Doom 3 by moving the red component into the alpha channel before compression and moving it back during rendering in the pixel shader.|$|E
50|$|The Teleputer was a {{range of}} {{computers}} that were suffixed with a number. Only the Teleputer 1 and Teleputer 3 were manufactured and sold. The teleputer 1 was a very simple device and only worked as a teletex terminal, whereas the Teleputer 3 was a z80 based micro computer. It ran {{with a pair of}} single sided 5¼ inch floppy disk drive; a 20Mb Hard disk drive version was available {{towards the end of the}} product's life. The operating system was CP/M or a proprietary variant CP*, and the unit was supplied with a suite of applications, consisting of a word processor, spreadsheet, database and a semi-compiled basic programming language. The display supplied with the unit (both the Teleputer 1 and 3) was a modified Rediffusion 14 inch portable colour television, with the tuner circuitry removed and being driven by a <b>RGB</b> <b>input.</b> The unit had a 64Kb onboard memory which could be expanded to 128Kb with a plug in card. Graphics were the standard videotext (or teletext) resolution and colour, but a high resolution graphic card was also available. A 75/1200 baud modem was fitted as standard (could also run at 300/300 and 1200/1200), and connected to the telephone via an old style round telephone connector. In addition an IEEE interface card could be fitted. On the back of the unit there was a RS232 and Centronic connections and on the front was the connector for the keyboard.|$|E
40|$|This paper {{introduces}} Conditional Regressive Random Forest (CRRF), a novel {{method that}} combines a closed-form Conditional Random Field (CRF), using learned weights, and a Regressive Random Forest (RRF) that employs adaptively selected expert trees. CRRF {{is used to}} estimate a depth image of hand given stereo <b>RGB</b> <b>inputs.</b> CRRF uses a novel superpixel-based regression framework that {{takes advantage of the}} smoothness of the hand’s depth surface. A RRF unary term adaptively selects different stereo-matching measures as it implicitly determines matching pixels in a coarse-to-fine manner. CRRF also includes a pair-wise term that encourages smoothness between similar adjacent superpixels. Experimental results show that CRRF can produce high quality depth maps, even using an inexpensive RGB stereo camera and produces state-of-the-art results for hand depth estimation...|$|R
40|$|Occlusion edges {{in images}} which {{correspond}} to range discontinuity {{in the scene}} {{from the point of}} view of the observer are an important prerequisite for many vision and mobile robot tasks. Although they can be extracted from range data however extracting them from images and videos would be extremely beneficial. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and <b>RGB</b> <b>inputs.</b> The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications. Comment: 7 pages, double column, IEEE HPEC 2015 Conferenc...|$|R
40|$|Figure 1 : A {{color image}} (Left) often reveals {{important}} visual details missing from a luminance-only image (Middle). Our Color 2 Gray algorithm (Right) maps visible color changes to grayscale changes. Image: Impressionist Sunrise by Claude Monet, courtesy of Artcyclopedia. com. Visually important image features often disappear when color images are converted to grayscale. The algorithm introduced here reduces such losses {{by attempting to}} preserve the salient features of the color image. The Color 2 Gray algorithm is a 3 -step process: 1) convert <b>RGB</b> <b>inputs</b> to a perceptually uniform CIE L ∗ a ∗ b ∗ color space, 2) use chrominance and luminance differences to create grayscale target differences between nearby image pixels, and 3) solve an optimization problem designed to selectively modulate the grayscale representation {{as a function of}} the chroma variation of the source image. The Color 2 Gray results offer viewers salient information missing from previous grayscale image creation methods...|$|R
3000|$|However, {{before going}} any further, {{we would like}} to point out that the {{illumination}} error and noise models are applied on the <b>RGB</b> <b>input</b> images before transforming these into the corresponding representations. For the sake of readability, the used notation is explained here again. We consider the (discrete) image to be a mapping [...]...|$|E
40|$|This paper {{describes}} an application framework to perform high quality upsampling on depth maps captured from a low-resolution and noisy 3 D time-of-flight (3 D-ToF) camera {{that has been}} coupled with a high-resolution RGB camera. Our framework is inspired by recent work that uses nonlocal means filtering to regularize depth maps {{in order to maintain}} fine detail and structure. Our framework extends this regularization with an additional edge weighting scheme based on several image features based on the additional high-resolution <b>RGB</b> <b>input.</b> Quantitative and qualitative results show that our method outperforms existing approaches for 3 D-ToF upsampling. We describe the complete process for this system, including device calibration, scene warping for input alignment, and even how the results can be further processed using simple user markup. 1...|$|E
40|$|A general {{methodology}} for noise reduction and contrast enhancement in very noisy image data with low dynamic range is presented. Video footage recorded in very dim light is especially targeted. Smoothing kernels that automatically {{adapt to the}} local spatio-temporal intensity structure in the image sequences are constructed {{in order to preserve}} and enhance fine spatial detail and prevent motion blur. In color image data, the chromaticity is restored and demosaicing of raw <b>RGB</b> <b>input</b> data is performed simultaneously with the noise reduction. The method is very general, contains few user-defined parameters and has been developed for efficient parallel computation using a GPU. The technique has been applied to image sequences with various degrees of darkness and noise levels, and results from some of these tests, and comparisons to other methods, are presented. The present work has been inspired by research on vision in nocturnal animals, particularly the spatial and temporal visual summation that allows these animals to see in dim light...|$|E
3000|$|... 1. Image preprocessing: First, {{bilinear}} interpolation is used {{to adjust}} {{the size of the}} input image to N[*]×[*]N (256 [*]×[*] 256 in this paper). Then, the <b>input</b> <b>RGB</b> color image is converted to the CIEL*a*b* color model representation.|$|R
50|$|Input {{data can}} come from device sources like digital cameras, image {{scanners}} or any other measuring devices. Those inputs can be either monochrome (in which case only the response curve needs to be calibrated, though in a few select cases one must also specify the color or spectral power distribution that that single channel corresponds to) or specified in multidimensional color - most commonly in the three channel <b>RGB</b> model. <b>Input</b> data is in most cases calibrated against a profile connection space (PCS).|$|R
30|$|Furthermore a few initial {{remarks about}} the choice of input {{parameters}} for the algorithms {{have to be made}} for clarification at this stage. All of the five algorithms except for CROSS use grayscale images as the input to disparity calculation. Whereas the other algorithms could have been adapted to use a colour input only CROSS used an <b>RGB</b> colour <b>input</b> in the original work [4]. BLOCK and NOMD both use the same SAD window size in order to prevent this difference from influencing the comparison.|$|R
40|$|Abstract—We {{address the}} problem of {{estimating}} the pose of humans using RGB image input. More specifically, we are using a random forest classifier to classify pixels into joint-based body part categories, much similar to the famous Kinect pose estimator [11], [12]. However, we are using pure <b>RGB</b> <b>input,</b> i. e. no depth. Since the random forest requires a large number of training examples, we are using computer graphics generated, synthetic training data. In addition, we assume that we have access to a large number of real images with bounding box labels, extracted for example by a pedestrian detector or a tracking system. We propose a new objective function for random forest training that uses the weakly labeled data from the target domain to encourage the learner to select features that generalize from the synthetic source domain to the real target domain. We demonstrate on a publicly available dataset [6] that the propose...|$|E
40|$|Hyperspectral imaging is a {{promising}} tool for applications in geosensing, cultural heritage and beyond. However, compared to current RGB cameras, existing hyperspectral cameras are severely limited in spatial resolution. In this paper, we introduce a simple new technique for reconstructing a very high-resolution hyperspectral image from two readily obtained measurements: A lower-resolution hyperspectral image and a high-resolution RGB image. Our approach {{is divided into}} two stages: We first apply an unmixing algorithm to the hyperspectral input, to estimate a basis representing reflectance spectra. We then use this representation in conjunction with the <b>RGB</b> <b>input</b> to produce the desired result. Our approach to unmixing is motivated by the spatial sparsity of the hyperspectral input, and casts the unmixing problem as the search for a factorization of the input into a basis and a set of maximally sparse coefficients. Experiments show that this simple approach performs reasonably well on both simulations and real data examples. 1...|$|E
40|$|Scene {{understanding}} for autonomous vehicles is {{a challenging}} computer vision task, with {{recent advances in}} convolutional neural networks (CNNs) achieving results that notably surpass prior traditional feature driven approaches. However, limited work investigates the application of such methods either within the highly unstructured off-road environment or to RGBD input data. In this work, we take an existing CNN architecture designed to perform semantic segmentation of RGB images of urban road scenes, then adapt and retrain it to perform the same task with multichannel RGBD images obtained under a range of challenging off-road conditions. We compare two different stereo matching algorithms and five different methods of encoding depth information, including disparity, local normal orientation and HHA (horizontal disparity, height above ground plane, angle with gravity), to create a total of ten experimental variations of our dataset, {{each of which is}} used to train and test a CNN so that classification performance can be evaluated against a CNN trained using standard <b>RGB</b> <b>input...</b>|$|E
40|$|This paper {{presents}} a unique spectral modelling approach suitable for digital camera image processing. The proposed vector model combines both {{the magnitude and}} directional characteristics of the vectorial <b>RGB</b> color <b>inputs</b> and enforces orientation constraints in an orthogonal color system to produce outputs which can simultaneously match luminance and chrominance characteristics of the captured image. Thus, the model enhances the performance of commonly used practical demosaicking solutions. Moreover, spectral models previously used in demosaicking {{can be seen as}} special cases of the proposed vector model...|$|R
40|$|Abstract — In this paper, {{we propose}} {{a simple but}} {{effective}} shadow removal method using a single input image. We first derive a 2 -D intrinsic image from a single RGB camera image based solely on colors, particularly chromaticity. We next present a method to recover a 3 -D intrinsic image based on bilateral filtering and the 2 -D intrinsic image. The luminance contrast in regions with similar surface reflectance due to geometry and illumination variances is effectively reduced in the derived 3 -D intrinsic image, while the contrast in regions with different surface reflectance is preserved. However, the intrinsic image contains incorrect luminance values. To obtain the correct luminance, we decompose the <b>input</b> <b>RGB</b> image and the intrinsic image. Each image is decomposed into a base layer and a detail layer. We obtain a shadow-free image by combining the base layer from the <b>input</b> <b>RGB</b> image and the detail layer from the intrinsic image such that {{the details of the}} intrinsic image are transferred to the <b>input</b> <b>RGB</b> image from which the correct luminance values can be obtained. Unlike previous methods, the presented technique is fully automatic and does not require shadow detection. Index Terms — Bilateral filter, chromaticity, intrinsic image. I...|$|R
40|$|Motion {{representation}} plays a {{vital role}} in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatio-temporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by <b>RGB</b> <b>inputs</b> achieves a competitive accuracy of 93. 3 % on UCF- 101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96. 0 % accuracy on UCF- 101. The code will be available online...|$|R
40|$|We {{introduce}} {{a new approach to}} intrinsic image decomposition, the task of decomposing a single image into albedo and shading components. Our strategy, which we term direct intrinsics, is to learn a convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch. Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms. The large-scale synthetic ground-truth of the MPI Sintel dataset {{plays a key role in}} training direct intrinsics. We demonstrate results on both the synthetic images of Sintel and the real images of the classic MIT intrinsic image dataset. On Sintel, direct intrinsics, using only <b>RGB</b> <b>input,</b> outperforms all prior work, including methods that rely on RGB+Depth input. Direct intrinsics also generalizes across modalities; it produces quite reasonable decompositions on the real images of the MIT dataset. Our results indicate that the marriage of CNNs with synthetic training data may be a powerful new technique for tackling classic problems in computer vision. Comment: International Conference on Computer Vision (ICCV), 201...|$|E
40|$|In the {{proposed}} work, classification of diseased and undiseased arecanut have been determined using texture features of Local Binary Pattern (LBP), Haar Wavelets, GLCM and Gabor. This {{work has been}} carried out in two stages. In the first stage, LBP have been applied on each color component of HSI and YCbCr color models and histogram of LBP is generated. The statistical method correlation is used to measure the distance between histogram of training set and query sample and obtained a success rate of 92. 00 %. We have not achieved better results in the first stage. In the second stage, texture features of Haar wavelets, GLCM and Gabor have been used. In this stage, <b>RGB</b> <b>input</b> arecanut image is transformed to HSI and YCbCr color models and texture features are extracted from each color component. Subset of texture features with high degree of discrimination power has been identified empirically based on combination of texture features. The kNN classifier gave a success rate of 100 % for discriminative subset of texture features...|$|E
40|$|We {{address the}} problem of {{estimating}} the pose of humans using RGB image input. More specifically, we are using a random forest classifier to classify pixels into joint-based body part categories, much similar to the famous Kinect pose estimator [11], [12]. However, we are using pure <b>RGB</b> <b>input,</b> i. e. no depth. Since the random forest requires a large number of training examples, we are using computer graphics generated, synthetic training data. In addition, we assume that we have access to a large number of real images with bounding box labels, extracted for example by a pedestrian detector or a tracking system. We propose a new objective function for random forest training that uses the weakly labeled data from the target domain to encourage the learner to select features that generalize from the synthetic source domain to the real target domain. We demonstrate on a publicly available dataset [6] that the proposed objective function yields a classifier that significantly outperforms a baseline classifier trained using the standard entropy objective [10]. Comment: 6 page...|$|E
40|$|Logvinenko’s {{color atlas}} theory {{provides}} a structure {{in which a}} complete set of color-equivalent material and illumination pairs can be generated to match any given <b>input</b> <b>RGB</b> color. In chromaticity space, the set of such pairs forms a 2 -dimensional manifold embedded in a 4 -dimensional space. For singleilluminant scenes, the illumination for different <b>input</b> <b>RGB</b> values must be contained in all the corresponding manifolds. The proposed illumination-estimation method estimates the scene illumination based on calculating the intersection of the illuminant components of the respective manifolds through a Hough-like voting process. Overall, the performance on the two datasets for which camera sensitivity functions are available is comparable to existing methods. The advantage of the formulating the illumination-estimation in terms of manifold intersection is that it expresses the constraints provided by each available RGB measurement within a sound theoretical foundation...|$|R
50|$|JFIF {{usage of}} JPEG {{supports}} Y′CbCr where Y′, CB and CR {{have the full}} 8-bit range of 0...255. Below are the conversion equations expressed to six decimal digits of precision. (For ideal equations, see ITU-T T.871.)Note that for the following formulae, the range of each <b>input</b> (<b>R,G,B)</b> is also the full 8-bit range of 0...255.|$|R
40|$|We {{present a}} machine {{learning}} technique to recognize ges-tures and estimate metric depth of hands for 3 D interaction, relying only on monocular <b>RGB</b> video <b>input.</b> We aim to en-able spatial interaction with small, body-worn devices where rich 3 D input is desired but {{the usage of}} conventional depth sensors is prohibitive due to their power consumption and size. We propose a hybrid classification-regression approach to learn and predict a mapping of RGB colors to absolute, metric depth in real time. We also classify distinct hand ges-tures, allowing {{for a variety of}} 3 D interactions. We demon-strate our technique with three mobile interaction scenarios and evaluate the method quantitatively and qualitatively. Author Keywords mobile interaction; gesture recognition; machine learnin...|$|R
40|$|We {{address the}} task of {{estimating}} the 6 D pose of known rigid objects, from RGB and RGB-D input images, in scenarios where the objects are heavily occluded. Our main contribution is a new modular processing pipeline. The first module localizes all known objects in the image via an existing instance segmentation network. The next module densely regresses the object surface positions in its local coordinate system, using an encoder-decoder network. The third module is purely a geometry-based algorithm to output the final 6 D object poses. While the first two modules are learned from data, {{and the last one}} not, we believe that this is the best of both worlds: geometry-based and learning-based algorithms for object 6 D pose estimation. This is validated by achieving state-of-the-art results for <b>RGB</b> <b>input</b> and a slight improvement over state-of-the-art for RGB-D input. However, in contrast to previous work, we achieve these results with the same pipeline for RGB and RGB-D input. Furthermore, to obtain these results, we give a second contribution of a new 3 D occlusion-aware and object-centric data augmentation procedure...|$|E
40|$|Spatiotemporal feature {{learning}} in videos {{is a fundamental}} and difficult problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from <b>RGB</b> <b>input</b> in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF 101, and HMDB 51, demonstrating that SMART blocks obtain an evident improvement over 3 D convolutions for spatiotemporal {{feature learning}}. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods. Comment: Code & models available at [URL]...|$|E
40|$|Figure 1 : Calibration-free realtime facial {{performance}} capture on highly occluded subjects {{using an}} RGB-D sensor. We introduce a realtime facial tracking system {{specifically designed for}} performance capture in unconstrained settings using a consumer-level RGB-D sensor. Our framework provides uninterrupted 3 D facial tracking, even {{in the presence of}} extreme occlusions such as those caused by hair, hand-to-face gestures, and wearable accessories. Anyone’s face can be instantly tracked and the users can be switched without an extra calibration step. During tracking, we explicitly segment face regions from any occluding parts by detecting outliers in the shape and appearance input using an exponentially smoothed and user-adaptive tracking model as prior. Our face segmentation combines depth and <b>RGB</b> <b>input</b> data and is also robust against illumination changes. To enable continuous and reliable facial feature tracking in the color channels, we synthesize plausible face textures in the occluded regions. Our tracking model is personalized on-the-fly by progressively refining the user’s identity, expressions, and texture with reliable samples and temporal filtering. We demonstrate robust and high-fidelity facial tracking {{on a wide range of}} subjects with highly incomplete and largely occluded data. Our system works in everyday environments and is fully unobtrusive to the user, impacting consumer AR applications and surveillance. 1...|$|E
40|$|In this paper, an {{improved}} thresholding approach based on neutrosophic sets (NSs) and adaptive thresholding is proposed. This {{is applied to}} degraded historical documents imaging and its performance evaluated. The <b>input</b> <b>RGB</b> image is transformed into the NS domain, which is described using three subsets, namely the percentage of truth in a subset, the percentage of indeterminacy in a subset, {{and the percentage of}} falsity in a subset...|$|R
3000|$|The goal of {{clustering}} is {{to minimize}} square sum mean (Kc)[*]=[*]∑_k= 1 ^Kmean (c_k) [...] of distance. The clustering method {{in this paper}} is to replace the original values of the same kind of pixels with their defined values (colors) and take each color component of the <b>RGB</b> as the <b>input</b> parameter to replace the pixels of the same kind of the original image. The resulting categories are displayed on an image without displaying them one by one.|$|R
40|$|A {{backward}} spectral characterization for Liquid Crystal Display {{by the use}} of {{rule for}} the maximum peak of spectral radiation curves changing with the digital input values is proposed; this new model is developed based on forward spectral characterization. It deals with estimation of <b>RGB</b> used as <b>input</b> to the digital display from known spectral radiation curves. We first investigate the rule for the peak of spectral radiation curves changing with the digital input values of primaries; then the initial digital <b>input</b> <b>RGB</b> are calculated based on that rule using the known spectral radiation curves ρ 0. Third, RGB are inputted into forward spectral characterization model and the corresponding spectral radiation curves ρ 1 are predicted. Last, RGB are modified according to the difference between predicted ρ 1 and known ρ 0, until this difference satisfied the prediction accuracy of the inverse characterization model. The inverse model has the advantage of using the same model for both forward and inverse color space transformation. This improves the accuracy of the color space transformation and reduces the source of errors. Results for 3 devices are shown and discussed; the accuracy of this model is considered sufficient for many applications...|$|R
