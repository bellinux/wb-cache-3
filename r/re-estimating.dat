139|1027|Public
50|$|In {{computer}} science, the inside-outside {{algorithm is}} a way of <b>re-estimating</b> production probabilities in a probabilistic context-free grammar. It was introduced James K. Baker in 1979 as a generalization of the forward-backward algorithm for parameter estimation on hidden Markov models to stochastic context-free grammars. It is used to compute expectations, for example as part of the expectation-maximization algorithm (an unsupervised learning algorithm).|$|E
5000|$|He {{arrived at}} Island II around noon, delayed by a {{contrary}} marea (usually translated as [...] "tide", although [...] "breeze" [...] is possible), <b>re-estimating</b> the distance as seven leagues rather than five. Island II had a coastline facing Guanahani that ran north-south for five leagues, and another coastline ("which I followed", says Columbus) that ran east-west {{for more than}} ten leagues.|$|E
3000|$|... 5 <b>Re-estimating</b> {{without the}} 18  year olds showed no {{economically}} or {{statistically significant differences}} in results.|$|E
30|$|The {{results of}} Ohlson {{original}} and <b>re-estimated</b> models are also reported in Table  11. In the original model, all the variables were significant except CLCA, INTWO and constant {{whereas in the}} <b>re-estimated</b> model all the variables are significant except SIZE, TLTA, FUTL and INTWO. The coefficients which are significant in both the original and <b>re-estimated</b> models are WCTA, OENEG, NITA, and CHIN. In case of WCTA, the original coefficient was - 1.43 and <b>re-estimated</b> coefficient is - 5.216 which is significantly different. For OENEG the original coefficient was - 1.72 and <b>re-estimated</b> coefficient is 2.836 which is different in value {{as well as in}} sign. There is huge difference in the value of NITA coefficient for original (- 2.37) and <b>re-estimated</b> (- 29.676) model. In case of CHIN, the original (- 0.5) and <b>re-estimated</b> (1.73) coefficient are not only different in value but also in sign. The result shows the coefficients of Ohlson (1980) model is sensitive to time period and not stable.|$|R
30|$|The result shows {{there is}} {{significant}} difference in the coefficients of original and <b>re-estimated</b> model except RETA. In case of RETA the original (1.4) and <b>re-estimated</b> (1.464) coefficients is found to be very close. For WCTA original coefficient was 1.2, and it ranks third with respect to relative importance of the variable to contribute in the overall index. In the <b>re-estimated</b> model the coefficient (0.076) significantly changes but still its ranks third in term of its relative importance in the overall index. In the original model EBITA, coefficient was 3.3, and it ranks first to contribute in the overall index, whereas <b>re-estimated</b> coefficient (-. 063) becomes negative and ranks fifth. In case of BVEBVD, the original coefficient was 0.6 and <b>re-estimated</b> coefficient is 3.474 which is significantly different. For SLTA, the original coefficient was 0.99 and <b>re-estimated</b> coefficient is 0.028. The * indicates the statistical significance of F-statistic in the difference of mean. For both the Altman original and <b>re-estimated</b> models, the F-statistics is significant, meaning that both the groups defaulted and non-defaulted have significantly different means. The finding suggests the coefficients of Altman (1968) model are not stable, and they are sensitive to time periods.|$|R
30|$|Begley et al. (1996) <b>re-estimates</b> {{and compares}} {{performance}} of original Altman’s and Ohlson’s models using US 1980 ’s data. The major finding {{of the study}} suggests Altman’s and Ohlson’s model outperforms <b>re-estimated</b> model. Both the <b>re-estimated</b> model have higher classification errors. Out of four contesting models, Ohlson’s original model outperforms other three contesting models. In line with Begley, Boritz et al. (2007) studying bankruptcy in Canada founds predictive accuracy of Altman’s and Ohlson’s original models are higher than <b>re-estimated</b> model. They also compared the accuracy of models developed for Canadian firms, namely, Springate (1978), Altman and Levallee (1980), and Legault and Veronneau (1986). The study concludes the Canadian models are being simpler and requiring less data. All models have stronger performance with the original coefficients than the <b>re-estimated</b> coefficients.|$|R
30|$|Step 3. <b>Re-estimating</b> the CSI {{by using}} the {{proposed}} channel estimation in Section “ST-based channel estimation”.|$|E
30|$|Excluding the few {{observations}} that violate concavity, and <b>re-estimating</b> the two models {{does not affect}} our elasticity estimates.|$|E
3000|$|... 1 by Equation (58) {{to update}} the models q (X^(0)_ 0 | θ _ 0), q (X^(1)_ 1 | θ _ 1) and also <b>re-estimating</b> the {{boundary}} w, e.g. by a FDA method based on the updated models.|$|E
30|$|This section covers re-{{estimation}} of Altman, Ohlson and Zmijewski models using estimation {{sample of}} 130 Indian firms consisting {{equal numbers of}} defaulted and non-defaulted firms. The statistical methodologies are the same used in the original models and discussed in section 2. The stability of the coefficients of original models is tested by comparing it from <b>re-estimated</b> models. The original and <b>re-estimated</b> coefficients are reported in Table  11. The coefficients of original and <b>re-estimated</b> models are compared to test the stability of coefficients to the time periods and change in the financial conditions. The overall predictive accuracy of model is tested on estimation and holdout sample to test whether change in coefficients (<b>re-estimated)</b> with recent data set improves the predictive accuracy of the model. The newly proposed model is compared with original and <b>re-estimated</b> models. By overall predictive accuracy, ROC, long-range accuracy test and the method to model bankruptcy, it is summarised that the newly proposed model for Indian manufacturing sectors outperforms other competitive models.|$|R
5000|$|Perform {{polynomial}} interpolation and <b>re-estimate</b> {{positions of}} the local extrema.|$|R
30|$|Finally, {{the result}} of Zmijewski model is again {{reported}} in Table  11. In the original model, all the coefficients are significant whereas in the <b>re-estimated</b> model all the variables are significant except CACL (Current assets to current liabilities). Rest other coefficients preserve similar sign but different in the magnitude. In case of TLTA, the original coefficients were 5.7 and <b>re-estimated</b> is 0.586 which is significantly different in magnitude. For NITL the original coefficients was - 4.5 and <b>re-estimated</b> co-efficient is - 13.797. The constant term in both the original (- 4.3) and <b>re-estimated</b> (- 1.222) is different in values. The result shows the coefficients of Zmijewski (1984) model is sensitive to time period and not stable. The results of newly proposed model are reported in the last column of Table  11. All the variables are significant except intercept.|$|R
3000|$|... 8 <b>Re-estimating</b> {{the model}} for the DNB 1995 sample only on workers ages 29 through 45 (presumably a random sample of those who would reach 43 through 59 in 2009, the average year in which members of the 2006 – 10 sample were observed), the {{estimated}} impact of height is even larger than it was for this cohort in 2009.|$|E
40|$|This paper {{examines}} {{the validity of}} Rudd and Whelan’s (2006) critiques of Gali and Gertler’s (1999) hybrid Phillips curve (HYPC) by <b>re-estimating</b> the HYPC using full information maximum likelihood (FIML). We also estimate HYPC with the constraint that the weights for the sum of forward looking and backward looking expectations should be unity. Our results support Rudd and Whelan’s conclusion that the weight for forward looking expectations is insignificant. ...|$|E
3000|$|When {{looking at}} PISA results, {{signs of the}} {{estimates}} are all consistent. The reduced significance of coefficients {{can be explained by}} a loss of precision due to a smaller sample size, as <b>re-estimating</b> the model on the reduced sample without PISA scores produces very similar results (results omitted). Concerning the reduced magnitude of the logit coefficient, a possible explanation could be the unobserved nature of PISA scores by employers: Mueller and Wolter [...]...|$|E
5000|$|... 1948:Ralph Alpher and Robert Herman <b>re-{{estimate}}</b> Gamow's estimate at 5 K.|$|R
2500|$|... 1960s – Robert Dicke <b>re-estimates</b> a {{microwave}} background radiation temperature of 40K ...|$|R
5000|$|... 1960s - Robert Dicke <b>re-estimates</b> a {{microwave}} background radiation temperature of 40 K ...|$|R
40|$|Abstract: The {{presence}} of autocorrelation in a regression model requires {{the use of}} the generalized least-squares technique in estimating the regression coefficients. To study the diagnostics of such a model it is therefore necessary to take account of the autocorrelation while <b>re-estimating</b> the parameters after the deletion of an observation. In this paper we look into this problem assuming that the disturbances follow a first-order autoregressive process. Key words: Autocorrelation, autoregressive process, regression diagnos-tics...|$|E
40|$|In the {{framework}} of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to reliably estimate bilin-gual term correspondences for low frequency terms. Considering such a situation, this paper proposes to complementarily use much larger monolingual Web documents collected by search engines, {{as a resource for}} reliably <b>re-estimating</b> bilingual term correspon-dences. We experimentally show that, using a suf-ficient number of monolingual Web documents, it is quite possible to have reliable estimate of bilin-gual term correspondences for those low frequency terms. ...|$|E
40|$|This paper {{examines}} {{the direction and}} the magnitude of bias in the technological progress measurement due to imperfect competition and short-run fixed costs. We show that, if capital growth exceeds non-capital input growth, the traditional measure underestimates the true technological growth if the pure profit is on the average positive. We then measure the actual magnitude of this bias by <b>re-estimating</b> sectoral technological progress in the Japanese industries. We find that the traditional measurement underestimates the technological progress by about one third between 1962 and 1974. ...|$|E
30|$|Given the probabilities, we {{use them}} to <b>re-estimate</b> the mixture {{parameters}} α(t) and β(t).|$|R
5000|$|... 1938:Nobel Prize winner (1920) Walther Nernst <b>re-estimates</b> {{the cosmic}} ray {{temperature}} as 0.75 K.|$|R
5000|$|... 1960s:Robert Dicke <b>re-estimates</b> a MBR (microwave {{background}} radiation) {{temperature of}} 40 K (ref: Helge Kragh).|$|R
40|$|Optical {{tomography}} (OT) recovers the cross-sectional {{distribution of}} optical parameters inside a highly scattering medium from {{information contained in}} measurements that are performed on {{the boundaries of the}} medium. The image reconstruction problem in OT can be considered as a large-scale optimization problem, in which an appropriately defined objective functional needs to be minimized. Most of earlier work is based on a forward model based iterative image reconstruction (MOBIIR) method. In this method, a Taylor series expansion of the forward propagation operator around the initial estimate, assumed to be close to the actual solution, is terminated at the first order term. The linearized perturbation equation is solved iteratively, <b>re-estimating</b> the first order term (or Jacobian) in each iteration, until a solution is reached. In this work we consider a nonlinear reconstruction problem, which has the second order term (Hessian) in addition to the first order. We show that in OT the Hessian is diagonally dominant and in this work an approximation involving the diagonal terms alone is used to formulate the nonlinear perturbation equation. This is solved using conjugate gradient search (CGS) without <b>re-estimating</b> either the Jacobian or the Hessian, resulting in reconstructions better than the original MOBIIR reconstruction. The computation time in this case is reduced by a factor of three...|$|E
30|$|While we find mixed {{evidence}} of a relationship between return migration and retirement status among the immigrant population as a whole, our theoretical model indicates that the link between retirement status and return migration should be the strongest among immigrants who are closest to retirement age. We investigate this issue by <b>re-estimating</b> the third specification above allowing the relationship between return migration rates and retirement status to depend on how close immigrants are to qualifying for the Australian Age Pension. 18 For conciseness we will refer to this as the retirement age.|$|E
40|$|The {{problem of}} {{modelling}} round trip times (RTT) {{on the internet}} is examined. Following recent work to break such RTTs down into their components it is noted that in many cases the size of the packets has a signicant eect on the RTTs. To take account of this a model is proposed where the time includes a component that equals a and forward bandwidth" (SFB) times the sum of the forward and reverse packet lengths plus a random component caused by queueing and other network delays. A technqiue for estimating the SFB is described. This uses a weighted linear regression over the minimum times on the RTTs binned on packet size. The accuracy of the resulting estimates is veri- ed by subsampling the original data and <b>re-estimating</b> the parameters. Examination of the data shows that the technique above does not model some connections adequately. A more complex model is introduced which uses dierent SFBs in the forward and resverse directions. The two SFB parameters are estimated using a three dimensional linear regression on the minimum of rectangular bins on the two packet sizes. The bins are generated using a quad-tree subdivision algorithm. The accuracy of the resulting estimates is veri- ed by subsampling the original data and <b>re-estimating</b> the parameters. Keywords| bandwidth, passive measurement, network delays, server delays, HTTP, TCP, store and forward bandwidth I...|$|E
3000|$|Given this estimate, we {{may then}} <b>re-estimate</b> the camera {{positions}} by projecting the measurement vectors y [...]...|$|R
30|$|<b>Re-estimate</b> {{the leaf}} values and HMM {{transition}} parameters {{based on the}} alignments from four and most recent DTAMs.|$|R
30|$|On the contrary, {{there are}} ample of studies {{questioning}} {{construct validity of}} the models to original models towards the change in time periods and financial conditions. Grice and Ingram (2001) analysed the sensitivity of Altman’s Z-score model for US companies. The study suggests the coefficients of the models are sensitive to the change in the financial environment and time period. The <b>re-estimated</b> model with the most recent information give better predictive accuracy. Grice and Dugan (2001) conducted study on US companies founds predictive accuracy of <b>re-estimated</b> Altman’s and Ohlson’s model is higher than the original models. Timmermans (2014) analysed the sensitivity of Altman’s, Ohlson’s and Zmijewski’s models on US companies. The major finding of the study suggests the <b>re-estimated</b> model have a higher predictive accuracy than the original models. Avenhuis (2013) conducted study on Dutch companies. The study <b>re-estimates</b> and compares performance of Altman’s, Ohlson’s and Zmijewski’s original models. The major finding of the study suggests re-estimation of model with specific and bigger sample give better predictive accuracy.|$|R
30|$|Finally, we also {{investigated}} the longer run implications of job loss {{for mental health}} by <b>re-estimating</b> our main specifications including a measure of job loss lagged one period. 9 In almost all cases, the substantive effect of recent job loss remains virtually unchanged {{and there is no}} evidence of long-term mental health effects of spousal and parental job loss. The exception is that the negative impact of husbands’ recent job loss with women’s mental health is statistically significant and somewhat larger in absolute magnitude, while lagged job loss is also associated with a significant reduction in women’s mental health.|$|E
40|$|How many labeled {{examples}} {{are needed to}} estimate a classifier’s per-formance on a new dataset? We study the case where data is plentiful, but labels are expensive. We show that by making a few reasonable assump-tions {{on the structure of}} the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evalu-ation (SPE), is based on a generative model for the classifier’s confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by <b>re-estimating</b> the class-conditional confidence distributions. ...|$|E
40|$|Data {{revisions}} routinely {{introduced by}} the World Bank can lead to significant revisions in empirical results. We show this by <b>re-estimating</b> our aggregate indicator for predicting the 1997 Asian crisis utilizing the 1999 and 2004 updates of the 1996 World Bank data and comparing these results to those we obtained (this Journal, 2000) for predicting the same event using the original, unrevised, 1996 World Bank data. Since most data-gathering organizations routinely revise their data, this may represent a much greater problem for policy makers than might be recognized. Copyright Springer Science + Business Media, Inc. 2005 data revisions, financial crisis, emerging markets, warning indicators,...|$|E
5000|$|All {{pairs of}} {{sequences}} x,y {{from the set}} of all sequences [...] are now <b>re-estimated</b> using all intermediate sequences z: ...|$|R
40|$|So-called “spurious regression” {{relationships}} between random-walk (or strongly autoregressive) variables are generally accompanied by clear signs of severe autocorrelation in their residuals. A conscientious researcher would therefore not end an investigation {{with such a}} result, but would likely <b>re-estimate</b> with an autocorrelation correction. Simulations show, for several typical cases, that the test-rejection statistics for the <b>re-estimated</b> relationships {{are very close to}} the true values, so do not yield results of the spurious type. ...|$|R
3000|$|... thus, it will <b>re-estimate</b> Gcon(i) Δ times. Between two {{consecutive}} executions, {{there could be}} at most Δ time steps on M [...]...|$|R
