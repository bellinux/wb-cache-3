171|6721|Public
2500|$|Furthermore, a basic intuition that Silver {{drew from}} his {{analysis}} of the 2008 Democratic party primary elections was that the voting history of a state or Congressional district provided clues to current voting. This is what allowed him to beat all the pollsters in his forecasts in the Democratic primaries in North Carolina and Indiana, for example. Using such information allowed Silver to come up with estimates of the vote preferences even in states for which there were few if any polls. For his general election projections for each state, in addition to relying on the available polls in a given state and [...] "similar states," [...] Silver estimated a [...] "538 regression" [...] using historical voting information along with demographic characteristics of the states to create an estimate that he treated as a separate poll (equivalent to the actually available polls from that state). This approach helped to stabilize his projections, because if there were few if any polls in a given state, the state forecast was largely determined by the 538 <b>regression</b> <b>estimate.</b>|$|E
40|$|AbstractLet (X, Y) be an Rd×R-valued {{regression}} pair, whereXhas a density andYis bounded. Ifni. i. d. {{samples are}} drawn from this distribution, the Nadaraya–Watson kernel <b>regression</b> <b>estimate</b> in Rdwith Hilbert kernelK(x) = 1 /‖x‖dis shown to converge weakly for all such regression pairs. We also show that strong convergence cannot be obtained. This is particularly interesting as this <b>regression</b> <b>estimate</b> {{does not have a}} smoothing parameter...|$|E
40|$|Let (X,Â Y) be an d [...] valued {{regression}} pair, whereXhas a density andYis bounded. Ifni. i. d. {{samples are}} drawn from this distribution, the Nadaraya-Watson kernel <b>regression</b> <b>estimate</b> in dwith Hilbert kernelK(x) = 1 /||x||dis shown to converge weakly for all such regression pairs. We also show that strong convergence cannot be obtained. This is particularly interesting as this <b>regression</b> <b>estimate</b> {{does not have a}} smoothing parameter. regression function estimation, kernel estimate, convergence, bandwidth selection, Nadaraya-Watson estimate, nonparametric estimation...|$|E
3000|$|... 11 In our {{descriptive}} tables, we pool {{whites and}} Asians, though in our <b>regression</b> <b>estimates</b> below we separate them (whites are the omitted group and Asians are indicated by a dummy variable). Though Asians {{earn more than}} whites, even controlling for education and achievement, their numbers {{are too small to}} generate major inconsistencies between the earlier descriptive results and our <b>regression</b> <b>estimates.</b>|$|R
50|$|This {{model is}} what is used to fit to data {{in order to get}} <b>regression</b> <b>estimates.</b>|$|R
30|$|The <b>regression</b> <b>estimates</b> for {{stage at}} {{diagnosis}} were consistently {{higher for the}} two methods that identified influential costs—DFBETA and Cook’s distance—than for the full cohort. Thus, {{it is possible that}} the DFBETA and Cook’s distance methods identified many cases with low or middle costs that were influential in addition to some influential high-cost records, which would increase the <b>regression</b> <b>estimates</b> and gradually increase estimated costs from lower-stage to higher-stage colon cancer.|$|R
40|$|Zhou (2010) {{introduced}} a multivariate Wilcoxon <b>regression</b> <b>estimate</b> which possesses some nice properties: computational ease, asymptotic normality and high efficiency. However, it {{is sensitive to}} the leverage points. To circumvent this problem, we propose a weighted multivariate Wilcoxon <b>regression</b> <b>estimate.</b> Under some regularity conditions, the asymptotic normality is established. We further study the robustness of the proposed estimate through the influence function. By properly choosing the weight functions, our {{results show that the}} corresponding estimate can have bounded influence function on both response and covariates. Multivariate regression Wilcoxon Rank estimate Influence function...|$|E
40|$|Bagging is {{a simple}} way to combine {{estimates}} in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each bootstrap sample, and decide by combining. By bagging an n-sample, the crude nearest neighbor <b>regression</b> <b>estimate</b> is turned out into a consistent weighted nearest neighbor <b>regression</b> <b>estimate,</b> which is amenable to statistical analysis. Letting the resampling size k_n grows with n in such a manner that k_n→∞ and k_n/n→ 0, it is shown that this estimate achieves optimal rates of convergence, independently from the fact that resampling is done with or without replacement...|$|E
40|$|This thesis {{examines}} local polynomial regression. Local polynomial regression {{is one of}} non-parametric {{approach of}} data fitting. This particular method is based on repetition of fitting data using weighted least squares estimate of {{the parameters of the}} polynomial model. The aim of this thesis is therefore revision of some properties of the weighted least squares estimate used in linear regression model and introduction of the non-robust method of local polynomial regression. Some statistical properties of the local polynomial <b>regression</b> <b>estimate</b> are derived. Conditional bias and conditional variance of the local polynomial <b>regression</b> <b>estimate</b> are then approximated using Monte Carlo method and compared with theoretical results. Powered by TCPDF (www. tcpdf. org...|$|E
30|$|The {{method that}} {{combined}} the DFBETA threshold of 0.15 and qualified box-plot outliers produced <b>regression</b> <b>estimates</b> {{that were very}} {{similar to those of the}} full cohort. A possible explanation might be that, at only 13, the number and value of influential observations we identified using the combined criteria was too small to induce a large change in the model. This method is robust as it uses a combination of outlying and influential criteria and yields results that are consistent with the <b>regression</b> <b>estimates</b> for the full cohort.|$|R
5000|$|Let bA and bB be {{respectively}} the <b>regression</b> <b>estimates</b> of βA and βB. Then, since the average value of residuals in a linear regression is zero, we have: ...|$|R
40|$|We {{propose a}} new {{multiple}} imputation technique for imputing squares. Current methods yield either unbiased <b>regression</b> <b>estimates</b> or preserve data relations. No method, however, seems to deliver both, which limits {{researchers in the}} implementation of regression analysis in the presence of missing data. Besides, current methods only work under a missing completely at random (MCAR) mechanism. Our method for imputing squares uses a polynomial combination. The proposed method yields both unbiased <b>regression</b> <b>estimates,</b> while preserving the quadratic relations in the data for both missing at random and MCAR mechanisms. © The Author(s) 2013...|$|R
30|$|All {{regression}} estimates with a {{significance level}} (p value) {{of less than}} 0.05 implies a confidential level that is higher than 95  %. In other words, if a <b>regression</b> <b>estimate</b> has a significance level of 0.05 or lower we can be at least 95  % sure that the independent variable is either positively or negatively associated with the dependent variable. If the significance level is lower than 0.01 we can be at least 99  % sure. Taken together, the lower the significance level (p value) the higher the confidence interval. A low significance level is accordingly indicative of a robust <b>regression</b> <b>estimate</b> (for further readings and assumptions about confidential intervals in statistics and econometrics, see for instance Wooldridge 2006).|$|E
40|$|Forecasting current quarter GDP is a {{permanent}} task inside the central banks. Many models are known and proposed to solve this problem. Thanks to new results on the asymptotic normality of the multivariate k-nearest neighbor <b>regression</b> <b>estimate,</b> we propose an interesting and new approach to solve in particular the forecasting of economic indicators, included GDP modelling. Considering dependent mixing data sets, we prove the asymptotic normality of multivariate k-nearest neighbor <b>regression</b> <b>estimate</b> under weak conditions, providing confidence intervals for point forecasts. We introduce an application for economic indicators of euro area, and compare our method with other classical ARMA-GARCH modelling. Multivariate k-nearest neighbor, asymptotic normality of the regression, mixing time series, confidence intervals, forecasts, economic indicators, Euro area. ...|$|E
40|$|International audienceBagging is {{a simple}} way to combine {{estimates}} in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor <b>regression</b> <b>estimate</b> is turned into a consistent weighted nearest neighbor <b>regression</b> <b>estimate,</b> which is amenable to statistical analysis. Letting the resampling size k_n grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented...|$|E
40|$|Dimensional {{studies of}} Congressional voting {{have found a}} single {{dominant}} "ideological" dimension, while <b>regression</b> <b>estimates</b> find that constituency variables and party are dominant. Koford (1989 b) recalibrated the dimensional studies, and found that several dimensions are important. This study reviews those findings, and then considers additional reasons why dimensional studies might understate the number of dimensions. It then examines the <b>regression</b> <b>estimates</b> for biases that overstate the number of dimensions. Overall, fewer dimensions are found than seem consistent with {{the wide variety of}} constituents' preferences on issues. Copyright 1990 Blackwell Publishers Ltd [...] ...|$|R
5000|$|Whereas, ȳ is {{the overall}} sample mean for yi, ŷi is the <b>regression</b> <b>estimated</b> mean for {{specific}} set of k independent (explanatory) variables and n is the sample size.|$|R
40|$|It is {{well-known}} {{that the rate}} of convergence of nonparametric <b>regression</b> <b>estimates</b> depends on the dimension of the covariate when it is finite dimensional. When the covariate is infinite dimensional, as it happens in the case of functional data, the derivation of convergence rates for nonparametric <b>regression</b> <b>estimates</b> is a challenging problem that has received attention in the recent literature. We derive the optimum convergence rates for a wide class of kernel <b>regression</b> <b>estimates</b> with infinite dimensional covariates. In our setup, the covariate is a random element in a complete separable metric space, and the function to be estimated takes values in a separable Banach space. The small ball probability function in the covariate space plays a critical role in determining the asymptotic variance of kernel estimates. Unlike what happens in the case of finite dimensional covariates, the optimal asymptotic orders of the bias and the variance of a nonparametric estimate are not same for infinite dimensional covariates...|$|R
40|$|We {{define a}} robust {{procedure}} to "correct" a <b>regression</b> <b>estimate</b> along the directions in predictor {{space where the}} fit is worse. When is the least median of squares estimate, the "corrected estimate" has a smaller maximum asymptotic bias under contamination, and a much better finite-sample behavior than...|$|E
40|$|Here {{we study}} the finite {{dimensional}} distribution {{of a process}} based on the Yang <b>regression</b> <b>estimate.</b> In getting information about the local behaviour of this estimate, our result allows solving the handicap of its asymptotical independence at finitely many points. Yang estimate regression function finite dimensional distribution...|$|E
40|$|This short paper {{points out}} the fact that, for a large class of multidimensional {{probability}} distributions with bounded support, every estimate of the regression can be modified {{in order to give}} an estimate of the edge of the support. Edge estimate Multivariate distribution <b>Regression</b> <b>estimate</b> Support...|$|E
40|$|We {{introduce}} strongly consistent {{estimates of}} the conditional expectation of Y given X when Y is censored on the right by R and min(Y,R) is left censored. This investigation extends the results available for Y only right censored. <b>Regression</b> <b>estimates</b> Kernel estimates Partitioning estimates Nearest neighbor estimates Twice censored data...|$|R
30|$|We first {{describe}} the download profile of MOEG through July 2011. Subsequently OLS <b>regression</b> <b>estimates</b> with DID and RDD specifications are presented. Then {{the role of}} Internet search engines is briefly discussed.|$|R
5000|$|... #Caption: Illustration of {{regression}} dilution (or attenuation bias) by a {{range of}} <b>regression</b> <b>estimates</b> in Errors-in-variables models. Two regression lines (red) bound the range of linear regression possibilities. The shallow slope is obtained when the independent variable (or predictor) is on the abscissa (x-axis). The steeper slope is obtained when the independent variable is on the ordinate (y-axis). By convention, with the independent variable on the x-axis, the shallower slope is obtained. Green reference lines are averages within arbitrary bins along each axis. Note that the steeper green and red <b>regression</b> <b>estimates</b> are more consistent with smaller errors in the y-axis variable.|$|R
40|$|The paper {{deals with}} the {{approximation}} of the best deterministic choice (minimizing mean integrated squared error) of the parameter in naive kernel and cubic partitioning regression estimation by cross-validation. The result is essentially distribution-free. <b>Regression</b> <b>estimate</b> Naive kernel Cubic partition Mean integrated squared error (MISE) Best deterministic parameter choice Cross-validation...|$|E
30|$|Linear <b>regression</b> <b>estimate</b> {{imputation}} {{is one of}} {{the single}} imputation method used the surviving creature characteristicswhen the variable with missing value has correlation with explanatory variable (time) and the series of data values follow linear trend [7]. However, socio-economic data expect to have some trends but may not exactly linear in time. Hence, applying this method may enhance correlation and under estimate the standard error of the regression coefficients by under estimating the variance of the imputed variables [8]. So with this consideration simple linear <b>regression</b> <b>estimate</b> is used to impute when the missing data is at the beginning (t_ 1) or/and at the end (t_n). However, missing values at the internal part were treated by comparing this method for minimum error with other exact estimation methods discussed in “Linear interpolation”, “Linear spline interpolation” and “Lagrange polynomial interpolation” sections.|$|E
40|$|For {{predicting}} the roundwood {{potential in the}} forest region of East Kalimantan, roughly two kinds of data are required. These includes sample data and population data. Sample data collected are the area or size of several selected forest region and {{the volume of the}} corresponding roundwood production, while population data include the total forest area of East Kalimantan. The prediction methods applied are ratio estimate and <b>regression</b> <b>estimate.</b> Using ratio estimate, the volume of roundwood production potential predicted was 8 093 575 cu. m (7 032 723 to 9 154 427 cu. m). On the other hand, with the application of <b>regression</b> <b>estimate,</b> the uolume of roundwood production potential was 8 505 603 cu. m. (7 889 057 to 9 122 149 cu. m.). Examination of the residuals indicated that assumption of normality of data was satisfie...|$|E
40|$|Abstract. Pricing of American options can be {{achieved}} by solving optimal stopping problems. This in turn can be done by computing so-called continuation values, which we represent as regression functions defined by the aid of a cash flow for the next few time periods. We use Monte Carlo to generate data and apply nonparametric least squares <b>regression</b> <b>estimates</b> to estimate the continuation values from these data. The parameters of the <b>regression</b> <b>estimates</b> and of the underlying regression problems are chosen data-dependent. Results concerning consistency and rate of convergence of these estimates are presented, and the resulting pricing of American options is illustrated by the aid of simulated data. 1...|$|R
40|$|WP 03 / 2012; This paper {{presents}} a new user-written STATA command called ivtreatreg for {{the estimation of}} five different (binary) treatment models with and without idiosyncratic (or heterogeneous) average treatment effect. Depending on the model specified by the user, ivtreatreg provides consistent estimation of average treatment effects both under the hypothesis of “selection on observables” and “selection on unobservables” by using Ordinary Least Squares (OLS) regression in the first case, and Intrumental-Variables (IV) and Selection-model (à la Heckman) in the second one. Conditional on a pre-specified subset of exogenous variables x – thought of as driving the heterogeneous response to treatment – ivtreatreg calculates for each model the Average Treatment Effect (ATE), the Average Treatment Effect on Treated (ATET) and the Average Treatment Effect on Non-Treated (ATENT), {{as well as the}} estimates of these parameters conditional on the observable factors x, i. e., ATE(x), ATET(x) and ATENT(x). The five models estimated by ivtreatreg are: Cf-ols (Control-function <b>regression</b> <b>estimated</b> by OLS), Direct- 2 sls (IV <b>regression</b> <b>estimated</b> by direct two-stage least squares), Probit- 2 sls (IV <b>regression</b> <b>estimated</b> by Probit and two-stage least squares), Probit-ols (IV two-step <b>regression</b> <b>estimated</b> by Probit and ordinary least squares), and Heckit (Heckman two-step selection model). An extensive treatment of the conditions under which previous methods provide consistent estimation of ATE, ATET and ATENT can be found, for instance, in Wooldgrige (2002, Chapter 18). The value added of this new STATA command is that it allows for a generalization of the regression approach typically employed in standard program evaluation, by assuming heterogeneous response to treatment...|$|R
50|$|In 1977, with Clifford Patlak, Albert Gjedde {{described}} the Gjedde-Patlak plot, {{also known as}} Multitime Graphical Analysis (MTGA),. The MTGA linearizes irreversible brain uptake of tracers {{in a manner that}} enables <b>regression</b> <b>estimates</b> to be made of uptake rates.|$|R
40|$|A broad {{class of}} multidimensional {{probability}} distributions {{is shown to}} have large samples which can be almost surely encompassed by a sequence of deterministic close fitting surfaces. Based on polar regression, a general method is proposed to estimate these surfaces. Almost surely stable extreme value Asymptotical location Elliptically contoured distribution Isobar Multivariate distribution <b>Regression</b> <b>estimate...</b>|$|E
40|$|AbstractWe {{design a}} data-dependent metric in Rd {{and use it}} to define the k-nearest neighbors of a given point. Our metric is {{invariant}} under all affine transformations. We show that, with this metric, the standard k-nearest neighbor <b>regression</b> <b>estimate</b> is asymptotically consistent under the usual conditions on k, and minimal requirements on the input data...|$|E
40|$|In {{this paper}} a simple linear {{regression}} model with independent and symmetric, but nonidentically distributed errors is considered. Asymptotic properties of the rank <b>regression</b> <b>estimate</b> defined in Jaeckel (1972) are studied. We show that the studied estimator is consistent and asymptotically normally distributed. The cases of bounded and unbounded score functions are examined separately...|$|E
40|$|We present multivariate penalized {{least squares}} <b>regression</b> <b>estimates.</b> We use Vapnik-Chervonenkis theory and bounds on the {{covering}} numbers to analyze convergence of the estimates. We show strong {{consistency of the}} truncated versions of the estimates without any conditions on the underlying distribution...|$|R
40|$|This paper {{presents}} a continuous time {{model of a}} firm that can dynamically adjust both its capital structure and its investment choices. In the model we endogenize the investment choice as well as firm value, which are both determined by an exogenous price process that describes the firm's product market. Within {{the context of this}} model we explore cross-sectional as well as time-series variation in debt ratios. We pay particular attention to interactions between financial distress costs and debtholder/equityholder agency problems and examine how the ability to dynamically adjust the debt ratio affects the deviation of actual debt ratios from their targets. <b>Regressions</b> <b>estimated</b> on simulated data generated by our model are roughly consistent with actual <b>regressions</b> <b>estimated</b> in the empirical literature. Copyright 2007, Oxford University Press. ...|$|R
30|$|Similarly, a {{spending}} model that uses last year’s income and this year’s income as explanatory variables should be {{equivalent to a}} model that uses last year’s income and the change in income from last year to this year. Multiple <b>regression</b> <b>estimates</b> will not be affected; stepwise estimates might.|$|R
