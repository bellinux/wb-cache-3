4|17|Public
40|$|For X {{a finite}} {{subset of the}} circle and for 0 X which maps each point to the {{clockwise}} furthest element of X within angular distance less than 2 pi r. We study the discrete dynamical system on X generated by f_r, and especially its expected behavior when X is a large random set. We show that, as |X| -> infinity, the expected fraction of periodic points of f_r tends to 0 if r is irrational and to 1 /q if r = p/q is rational with p and q coprime. These results are obtained via more <b>refined</b> <b>statistics</b> of f_r which we compute explicitly in terms of (generalized) Catalan numbers. The motivation for studying f_r comes from Vietoris-Rips complexes, a geometric construction used in computational topology. Our results determine how much one can expect to simplify the Vietoris-Rips complex of {{a random sample of}} the circle by removing dominated vertices. Comment: Advances in Applied Mathematics, 201...|$|E
40|$|This paper {{develops}} {{more accurate}} tests {{for lack of}} spatial correlation than ones based on the usual central limit theorem. We test nullity of the lag parameter in a pure spatial autoregression based on least squares and Gaussian maximum likelihood estimates. In each case, depending on assumptions on the spatial weight matrix, the rate of convergence of the estimate can be slower than the square root of n, where n is sample size. Correspondingly, the error in the normal approximation can be larger than the usual parametric order. This provides particularly strong motivation for employing instead <b>refined</b> <b>statistics</b> which entail closer approximations. These are based on (formal) Edgeworth expansions. In Monte Carlo simulations we demonstrate that the new tests (and one based on a bootstrap, {{which is expected to}} have similar properties) outperform one based on the usual normal approximation in small and moderate samples. The new tests are also applied in two empirical examples...|$|E
40|$|International audienceA new {{characterization}} method of the nanoporous structure of activated carbons (ACs) is proposed, based on mathematical morphology analysis of high resolution {{transmission electron microscopy}} (TEM) images. It produces <b>refined</b> <b>statistics</b> describing the shape, size and orientation of the defective graphene sheets seen edge on as individual fringes on TEM images. It also provides some quantitative information regarding their spatial arrangement. Especially, assemblages composed of 2 - 4 nearly parallel fringe fragments could be detected, which were relevant of some partial stacking of the defective graphene sheets. Such assemblages were possibly locally oriented along a common direction to form large continuous domains. To prove {{the ability of the}} image analysis tool to reveal distinctive features and degrees of disorder of the AC structures, a set of various commercial carbon adsorbents was characterized. The measured effective spaces separating the individual fringes, the stacks and the continuous domains were examined and compared with the porosity data derived from 77 K-N 2 adsorption isotherms. Consistency between the two sets of data was assessed and interpreted by considering the N 2 diffusional limitations resulting from the micropore network connectivity...|$|E
40|$|The {{use of the}} {{percolation}} {{model from}} lattice statistics to represent the distribution of trapping times for air bubbles in polar ice is discussed. A number of relevant techniques and results from the statistical mechanics of phase transitions are reviewed in order to propose suitable approaches for performing a more <b>refined</b> lattice <b>statistics</b> analysis of bubble trapping. 1...|$|R
40|$|Statistics play an {{important}} role in influencing the plans produced by a query optimizer in a relational database management system. Traditionally, query optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this proposal, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present a first attempt for efficiently building and <b>refining</b> <b>statistics.</b> We introduce STHoles, a “workload-aware” histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. This proposal contributes to the broader goal of automating management of statistics for relational database management systems...|$|R
40|$|Atmospheric {{propagation}} at frequencies {{within the}} THz domain are deeply {{affected by the}} inﬂuence of the composition and phenomena of the troposphere. This paper {{is focused on the}} estimation of ﬁrst order statistics of total attenuation under non-rainy conditions at 100 GHz. With this purpose, a yearly meteorological database from Madrid, including radiosoundings, SYNOP observations and co-site rain gauge, have been used in order to calculate attenuation due to atmospheric gases and clouds, as well as to introduce and evaluate a rain detection method. This method allows to ﬁlter out rain events and <b>reﬁne</b> the <b>statistics</b> of total attenuation under the scenarios under study. It is expected that the behavior of the statistics would be closest to the ones obtained by experimental techniques under similar conditions...|$|R
40|$|Much of today’s {{success in}} Information Retrieval (IR) {{comes from a}} hard approach: {{employing}} blazingly fast machines, ever more <b>refined</b> <b>statistics,</b> and increasingly powerful classification schemes. In recent years, however, the hard approach has entered a phase of diminishing returns. This paper explores a softer alternative which, we argue, {{is still in the}} phase of increasing returns. As the quality of an IR system is ultimately decided by its users, the approach starts from how these users structure information. Interestingly, for this approach many useful principles are readily available in the psychological literature. We illustrate the approach with three examples. The first applies the cognitive status of ‘complex nominals’ to improve search results by automatically constructing specialized queries. The second shows how the connection between language and imagery at the ‘basic level ’ can be used for multimedia retrieval on the World Wide Web. The final example employs the notion of ’semantic space ’ to make retrieval more effective especially for large scale corpora. In each example the results were substantial. The cases we studied illustrate how an approach to information retrieval based on cognitive principles can lead to significant, immediate, and fundamental results. It shows how prolific the application of cognitive science to the core of IR can be, and we believe that both disciplines stand to benefit from this approach...|$|E
40|$|Appreciation of the {{importance}} of statistics literacy for citizens of a democracy has resulted in an increasing number of degree programs making statistics courses mandatory for university students. Unfortunately, empirical evidence suggests that students in nonmathematical disciplines (e. g., social sciences) regard statistics courses as the most anxiety-inducing course in their degree programs. Although a literature review exists for statistics anxiety, it was done more than a decade ago, and newer studies have since added findings for consideration. In this article, we provide a current review of the statistics anxiety literature. Specifically, related variables, definitions, and measures of statistics anxiety are reviewed with the goal of <b>refining</b> the <b>statistics</b> anxiety construct. Antecedents, effects, and interventions of statistics anxiety are also reviewed to provide recommendations for statistics instructors and for a new research agenda...|$|R
40|$|We present {{exponential}} {{generating function}} analogues to two classical identities involving the ordinary generating {{function of the}} complete homogeneous symmetric functions. After a suitable specialization the new identities reduce to identities involving {{the first and second}} order Eulerian polynomials. The study of these identities led us to consider a family of symmetric functions associated with a class of permutations introduced by Gessel and Stanley, known in the literature as Stirling permutations. In particular, we define certain type statistics on Stirling permutations that <b>refine</b> the <b>statistics</b> of descents, ascents and plateaux and we show that their refined versions are equidistributed, generalizing a result of Bóna. The definition of this family of symmetric functions extends to the generality of r-Stirling permutations. We discuss some occurrences of these symmetric functions in the cases of r= 1 and r= 2. Comment: 33 pages, 14 figures, Theorems 2. 4 and 2. 5 have been proved by the author in arXiv: 1608. 00715 and arXiv: 1408. 5415 using techniques in poset topolog...|$|R
40|$|AstraLux is the Lucky Imaging {{camera for}} the Calar Alto 2. 2 -m {{telescope}} and the 3. 5 -m NTT at La Silla. It allows nearly diffraction limited imaging in the SDSS i' and z' bands of objects as faint as i'= 15. 5 mag with minimum technical effort. One {{of the ongoing}} AstraLux observing programs is a binarity survey among late-type stars with spectral types K 7 to M 8, covering more than 1000 targets on {{the northern and southern}} hemisphere. The survey is designed to <b>refine</b> binarity <b>statistics</b> and especially the dependency of binarity fraction on spectral type. The choice of the SDSS i' and z' filters allows to obtain spectral type and mass estimates for resolved binaries. With an observing efficiency of typically 6 targets per hour we expect to complete the survey in mid- 2009. Selected targets will be followed up astrometrically and photometrically, contributing to the calibration of the mass-luminosity relation at the red end of the main sequence and at visible wavelengths. Comment: 4 pages, 3 figures. To appear in proceedings of Cool Stars 15 conference, St. Andrews, 200...|$|R
40|$|The Ehrhart {{polynomial}} of a lattice polytope P encodes {{information about}} the number of integer lattice points in positive integral dilates of P. The h^∗-polynomial of P is the numerator polynomial of the generating function of its Ehrhart polynomial. A zonotope is any projection of a higher dimensional cube. We give a combinatorial description of the h^∗-polynomial of a lattice zonotope in terms of <b>refined</b> descent <b>statistics</b> of permutations and prove that the h^∗-polynomial of every lattice zonotope has only real roots and therefore unimodal coefficients. Furthermore, we present a closed formula for the h^∗-polynomial of a zonotope in matroidal terms which is analogous to a result by Stanley (1991) on the Ehrhart polynomial. Our results hold not only for h^∗-polynomials but carry over to general combinatorial positive valuations. Moreover, we give a complete description of the convex hull of all h^∗-polynomials of zonotopes in a given dimension: it is a simplicial cone spanned by refined Eulerian polynomials. Comment: 20 pages, 2 figures; Corollary 4. 5 and Proposition 4. 11 added in v 2; minor changes, accepted for publication in Trans. Amer. Math. Soc. ...|$|R
40|$|This {{publication}} {{provides information}} and statistical {{data on a}} variety of crude oils and <b>refined</b> petroleum products. <b>Statistics</b> on crude oil costs and refined petroleum products sales are presented. Data on crude oil include the domestic purchase price, the free on board and landed cost of imported crude oil, and the refiners` acquisition cost of crude oil. Refined petroleum product sales data include motor gasoline, distillates, residuals, aviation fuels, kerosene, and propane. Monthly statistics on purchases of crude oils and sales of petroleum products are presented in five sections: (1) summary statistics, (2) crude oil prices, (3) prices of petroleum products, (4) volumes of petroleum products, and (5) prime supplier sales volumes of petroleum products for local consumption. 50 tabs...|$|R
40|$|In this thesis, I apply {{simulation}} {{techniques to}} investigate three questions in population biology, which focus on movement and natural selection. The first model assesses the theoretical implications of long-range dispersal in species invasions, identifying an important {{interaction between the}} representation of a finite population {{and the rate of}} population spread. The second investigates the genetic impact of movement distortions among domestic animals due to human economic activity, suggesting that the marketing of animals could fundamentally impact their genetic variation and distribution. My third model considers the problem of detecting evidence of positive natural selection in the genome, <b>refining</b> and testing <b>statistics</b> designed to identify which genes have offered a reproductive advantage in the past using population genetic data. These three simulation studies use very different approaches, and, separately, identify the critical and practical importance of assumptions frequently encountered in population models. Such assumptions - infinite population size, unbiased migration, and constant recombination rate - each lead to interesting properties of model behaviour, and may be relevant to interpretation and prediction in real world problems...|$|R
40|$|In {{this paper}} we treat {{the black hole}} horizon as a {{physical}} boundary to the spacetime and study its dynamics following from the Gibbons-Hawking-York boundary term. Using the Kerr black hole as an example we derive an effective action that describes, in the large wave number limit, a massless Klein-Gordon field living on the average location of the boundary. Complete solutions {{can be found in}} the small rotation limit of the black hole. The formulation suggests that the boundary can be treated in the same way as any other matter contributions. In particular, the angular momentum of the boundary matches exactly with that of the black hole, suggesting an interesting possibility that all charges (including the entropy) of the black hole are carried by the boundary. Using this as input, we derive predictions on the Planck scale properties of the boundary. Comment: References added, nature of boundary stress tensor clarified, discussion of <b>statistics</b> <b>refined</b> and a mistake with Hawking temperature corrected, 16 pages; version to appear in journa...|$|R
40|$|This paper {{investigates the}} soybean-oil “crush ” spread, {{that is the}} profit margin gained by {{processing}} soybeans into soyoil. Soybeans form a large proportion (over 1 / 5 th) of the agricultural output of US farmers and the profit margins gained will therefore have a wider impact on the US economy in general. The paper uses a number of techniques to forecast and trade the soybean crush spread. A fair value cointegration model {{is used as a}} benchmark against more sophisticated models such as a MultiLayer Perceptron, Recurrent Networks and Higher Order Neural Networks. These are then used to trade the spread, the implementation of a number of filtering techniques as used in Dunis et al. (2005) are utilised to further <b>refine</b> the trading <b>statistics</b> of the models. The results show the best in-sample model to be the MLP with a correlation filter, this also proves to be the best out-of-sample model in terms of a measure of risk adjusted return. Further it is noted that the best general architecture for trading the spread is the Higher Order Neural Network despite shorter computational times when compared with MultiLayer Perceptrons and Recurrent Networks...|$|R
40|$|How {{does one}} {{identify}} and then prioritize relatively minor improvements and modifications {{which might be}} made with limited funding {{for the sake of}} safety and accident reduction on the Interstate Highway System in Kentucky? First, one must know how to identify places or situations which a~e more hazardous than-others. Second, one must know how to rank hazards according to severity and the potential effectiveness of corrective measures. One must know what is normal in terms of accident statistics in order to identify what is abnormal when scanning accident data files. Indeed, one must have criteria and guidelines to follow. The report submitted herewith brings known factors together in order and organization which will be the basis for a manual of step-by-step procedures. The program will evolve therefrom. Provisions for updating will emerge in the form of <b>refined</b> criteria and <b>statistics</b> and from possible, further availability of designated funds. It is planned to extend the guidelines and criteria to cover similar application to the remaining systems of highways in the state. The report at hand was reviewed by the Project Advisory Committee on December 7; only editorial changes have been made. The membership of the Committee in addition to Research people is: F. R...|$|R
40|$|The {{extensible}} mark-up language (XML) {{is gaining}} widespread {{use as a}} format for data exchange and storage on the World Wide Web. Queries over XML data require accurate selectivity estimation of path expressions in order to optimize query execution plans. Selectivity estimation of XML path expression is usually done based on summary statistics about {{the structure of the}} underlying XML repository. All previous methods require an off-line scan of the XML repository to collect the statistics. In this paper, we propose XPathLearner 4, a method for estimating selectivity of the most commonly used types of path expressions without looking at the XML data. XPathLearner gathers and <b>refines</b> the required <b>statistics</b> using query feedback in an on-line manner and is especially suited to queries in Internet scale applications since the underlying XML repository is either inaccessible or too large to be scanned in its entirety. Besides the on-line property, our method also has two other novel features: (a) XPathLearner is workload aware in collecting the statistics and thus can be more accurate than the more costly off-line method under tight memory constraints, and (b) XPathLearner automatically adjusts the statistics using query feedback when the underlying XML data change. We show empirically the estimation accuracy of our method using the XMark synthetic data set and several real data sets...|$|R
40|$|The {{current status}} of some nucleon isovector observables, the vector charge, g_V, axial charge, g_A, quark {{momentum}} fraction, 〈 x 〉_u-d, and quark helicity fraction, 〈 x 〉_Δ u - Δ d, calculated using recent RBC/UKQCD 2 + 1 -flavor dynamical domain-wall fermions (DWF) lattice QCD ensembles are reported: with Iwasaki gauge action at inverse lattice spacing, a^- 1, of about 1. 7 GeV, linear lattice extent, L, of about 2. 7 fm, pion mass, m_π, of about 420 and 330 MeV, and with Iwasaki×DSDR gauge action at a^- 1 of about 1. 4 GeV, L of about 4. 6 fm, and m_π of about 250 and 170 MeV. The calculations have been <b>refined</b> with enhanced <b>statistics,</b> in particular through successful application of the all-mode-averaging (AMA) technique for the 170 - and 330 -MeV ensembles. As a result, the precision agreement seen in the charge ratio, g_A/g_V, for 420 -MeV and 250 -MeV ensembles that share the finite-size scaling parameter m_π L of about 5. 8 is more significant with new values of 1. 17 (2) and 1. 18 (4) respectively. We also studied the dependence on the source-sink separation in the lightest ensemble of 170 -MeV, by comparing the cases with the separation of about 1. 0 and 1. 3 fm and did not see any dependence: contamination from the excited states are well under control in our choice of source and sink smearing. The axial charge, g_A and the ratio, g_A/g_V, shows a long-range autocorrelation that extends {{the entire range of}} configurations that were so far analyzed, almost 700 hybrid Molecular Dynamics time, in the lightest ensemble of m_π= 170 MeV. The other observables do not show any autocorrelation with the interval of 16 trajectories. Comment: 7 pages, 12 figures, a talk presented at the 31 st International Symposium on Lattice Field Theory, LATTICE 2013, July 29 - August 3, 2013, Mainz, German...|$|R
40|$|A problem often {{encountered}} in multidimensional NMR spectroscopy {{is that an}} existing chemical shift list of a protein has {{to be used to}} assign an experimental spectrum but does not fit sufficiently well for a safe assignment. A similar problem occurs when temperature or pressure series of n-dimensional spectra are to be evaluated automatically. Two slightly different algorithms, AUREMOL-SHIFTOPT 1 and AUREMOL-SHIFTOPT 2 have developed here that fulfill this task. Their performance is analyzed employing a set of simulated and experimental two-dimensional and three-dimensional spectra obtained from three different proteins. Peak probability and atom type based weighted averaging is introduced {{in order to reduce the}} influence of the wrong assignment during the assignment process. Chemical shift prediction programs often use a single energy minimized structure as input, but ensemble averaging of chemical shifts gives better prediction values irrespective of the prediction method. This is in agreement with the fact that proteins in solution occur in multiple conformational states in fast exchange on the chemical shift time scale. However, in contrast to the real conditions in solution at ambient temperatures, the chemical shift prediction methods seems optimal to predict the lowest energy ground state structure that is only weakly populated under these conditions. An analysis of the data shows that a chemical shift prediction can be used as measure to define the minimum size of the structural bundle required for a faithful description of the structural ensemble. Reliable homo- and heteronuclear chemical shift distributions are required for the automated assignment procedures. However, the statistics derived from the Biological Magnetic Resonance Bank (BMRB) is not clean and is not structurally unbiased. Therefore, <b>refined</b> chemical shift <b>statistics</b> was created from a structural database of non-homologous proteins (Nh 3 D) that comprises 806 different three-dimensional structures. The chemical shift data base was created by calculating the resulting chemical shifts with the prediction programs SHIFTS and SHIFTX. Analysis of the obtained data set shows that unbiased chemical shift statistics improves the a priori probability values for resonance assignment, removes ambiguities in assignment to certain level and helps to make stereochemical assignments...|$|R
40|$|Understanding how {{communication}} between brain areas evolves to support dynamic function remains a fundamental challenge in neuroscience. One {{approach to this}} question is functional connectivity analysis, in which statistical coupling measures are employed to detect signatures of interactions between brain regions. Because the brain uses multiple communication mechanisms at different temporal and spatial scales, and because the neuronal signatures of communication are often weak, powerful connectivity inference methodologies require continued development specific to these challenges. Here we address the challenge of inferring task-related functional connectivity in brain voltage recordings. We first develop a framework for detecting changes in statistical coupling that occur reliably in a task relative to a baseline period. The framework characterizes the dynamics of connectivity changes, allows inference on multiple spatial scales, and assesses statistical uncertainty. This general framework is modular and applicable {{to a wide range}} of tasks and research questions. We demonstrate the flexibility of the framework in the second part of this thesis, in which we <b>refine</b> the coupling <b>statistics</b> and hypothesis tests to improve statistical power and test different proposed connectivity mechanisms. In particular, we introduce frequency domain coupling measures and define test statistics that exploit theoretical properties and capture known sampling variability. The resulting test statistics use correlation, coherence, canonical correlation, and canonical coherence to infer task-related changes in coupling. Because canonical correlation and canonical coherence are not commonly used in functional connectivity analyses, we derive the theoretical values and statistical estimators for these measures. In the third part of this thesis, we present a sample application of these techniques to electrocorticography data collected during an overt reading task. We discuss the challenges that arise with task-related human data, which is often noisy and underpowered, and present functional connectivity results in the context of traditional and contemporary within-electrode analytics. In two of nine subjects we observe time-domain and frequency-domain network changes that accord with theoretical models of information routing during motor processing. Taken together, this work contributes a methodological framework for inferring task-related functional connectivity across spatial and temporal scales, and supports insight into the rapid, dynamic functional coupling of human speech...|$|R
40|$|In {{analysis}} of structural information for transmembrane (TM) proteins it is ideal {{to work with}} a three-dimensional (3 D) structure. This is not always possible as determining an accurate 3 D structure can be challenging and expensive as pursuing one can take large amounts of time. Sequence analysis is often used as a surrogate to determine a subset of information regarding secondary and tertiary protein structure given the primary structure (an amino acid sequence). Using Equilibrative Nucleoside Transporter member 1 (ENT 1) as a model, the objective of predicting secondary and tertiary structure given primary structure is attempted through computational (in silico) methods. The in silico methods include the use of a pipeline of programs spanning both custom made and ready built software. A set of 2034 homologous protein sequences are first obtained from the initial human ENT 1 (hENT 1) sequence through a BLASTp. This sequence information is then processed to acquire information on variation, conservation, and hydrophobicity through topology prediction, hydropathic moment plots and variation moment plots. Comparing these results with data acquired from Glycerol- 3 -Phosphate Transporter (GlpT), a protein with a known 3 D structure of 3. 3 Angstrom resolution is done to assess evidence of evolutionary origin. This in turn allows estimation on the reliability of predictions to be made on aspects of the secondary and tertiary structure for hENT 1. The results show that within predicted TM alpha helical regions that there is some level of correlation between the variation of the amino acids within the alpha helical TM region and its orientation towards the membrane. This can be further <b>refined</b> by gathering <b>statistics</b> on other known proteins with a 3 D structure for their relationships in TM regions to hydrophobicity and variability. This will aid in secondary and tertiary structure predictions of other TM proteins given further refinement and additional data. In addition, the sequence conservation information obtained should prove to be robust and allow for a large number of sequences to be analyzed to determine conservation of amino acids given a reference protein. Ideally this information will provide aid in determining interesting amino acids for experiments to be done on hENT 1. Vain tiivistelmä. Opinnäytteiden arkistokappaleet ovat luettavissa Helsingin yliopiston kirjastossa. Hae HELKA-tietokannasta ([URL] only. The paper copy of the whole thesis is available for reading room use at the Helsinki University Library. Search HELKA online catalog ([URL] avhandlingens sammandrag. Pappersexemplaret av hela avhandlingen finns för läsesalsbruk i Helsingfors universitets bibliotek. Sök i HELKA-databasen ([URL]...|$|R

