5|22|Public
40|$|A {{study was}} made to {{determine}} improved values and definitions {{to be used for}} thermal environmental design parameters for a spacecraft in near-earth orbit. An algorithm was used to derive a total earth thermal radiation based on a mathematical relationship. Several albedo and earth thermal <b>radiation</b> <b>grid</b> maps were produced on seven track digital magnetic tape. Each map contained the values obtained during a 24 hour period over the entire earth. The output statistics are summarized, and the data processing program is described...|$|E
40|$|An {{algorithm}} is described for selecting a grid subset for calculating radiative transport. The subset spacing {{is determined by}} using the variation in aerothermal properties across the full grid of the shock layer. Results show that a <b>radiation</b> <b>grid</b> subset of 15 to 20 points {{can be used for}} viscous-shock layer calculations where approximately 50 grid points are required to define the aerothermal profiles. Results are presented for various planetary entry conditions with and without mass injection to demonstrate both the validity and utility of the algorithm...|$|E
40|$|Polarisation {{analysis}} of synchrotron THz radiation {{was carried out}} with a standard stretched polyethylene polariser and revealed that the linearly polarised (horizontal) component contributes up to 22 +/- 5 % to the circular polarised synchrotron emission extracted by a gold-coated mirror with a horizontal slit inserted near a bending magnet edge. Comparison with theoretical predictions shows a qualitative match with dominance of the edge <b>radiation.</b> <b>Grid</b> polarisers 3 D-printed out of commercial acrilic resin were tested for the polariser function and showed spectral regions where the dichroic ratio DR > 1 and < 1 implying importance of molecular and/or stress induced anisotropy. Metal-coated 3 D-printed THz optical elements can find a range of applications in intensity and polarisation control of THz beams. Comment: 8 figure...|$|E
30|$|As the TNPC varies {{based on}} {{sensitivity}} variables chosen, in this analysis, multiple optimizations are performed {{to measure the}} effects of the uncertainties such as wind speed, solar <b>radiations,</b> <b>grid</b> tariffs, and fuel prices on the system performance. Hence, the HPS configuration has to be chosen to tolerate all these situations.|$|R
40|$|Before a hydrocode {{is used to}} {{investigate}} a question of scientific interest, it should be tested against analogous laboratory experiments and problems with analytical solutions. The <b>Radiation</b> Adaptive <b>Grid</b> Eulerian (RAGE) hydrocode[1], developed by Los Alamos National Laboratory (LANL) and Science Applications International Corporation (SAIC) [2, 3] {{has been subjected to}} many tests during its development. [4, 5] We extend and review this work, emphasizing tests relevant to impact cratering into volatile-rich targets...|$|R
40|$|We have modeled laser-induced {{transient}} current waveforms in <b>radiation</b> coplanar <b>grid</b> detectors. Poisson's equation has been solved by {{finite element method}} and currents induced by photo-generated charge were obtained using Shockley-Ramo theorem. The spectral response on a radiation flux has been modeled by Monte-Carlo simulations. We show 10 × improved spectral resolution of coplanar grid detector using differential signal sensing. We model the current waveform dependence on doping, depletion width, diffusion and detector shielding and their mutual dependence is {{discussed in terms of}} detector optimization. The numerical simulations are successfully compared to experimental data and further model simplifications are proposed. The space charge below electrodes and a non-homogeneous electric field on a coplanar grid anode are found to be the dominant contributions to laser-induced {{transient current}} waveforms. Comment: 7 pages, 8 figures, regular articl...|$|R
40|$|The {{comprehensive}} Earth-system model {{developed at}} the European Centre for Medium-Range Weather Forecasts (ECMWF) in cooperation with Météo-France forms {{the basis for the}} center’s data assimilation and forecasting activities. All the main applications required are available through one computer software system called the Integrated Forecasting System (IFS). IFS consists of several components: an atmospheric general circulation model, an ocean wave model, a land surface model and perturbation models for the data assimilation and forecast ensembles, producing forecasts from days to week and months ahead. When running any high resolution model, keeping the computational costs within operational limits is an ongoing challenge. In particular, thermal radiation computations in IFS are expensive, taking approximately 10 percent of total compute time. Even this current level of cost is achieved by imposing limitations. ECMWF runs the radiation scheme only once every forecast hour and uses a <b>radiation</b> <b>grid</b> resolution which is coarser than that of the model grid. As scalability will become more and more challenging on future supercomputers, ECMW...|$|E
40|$|The {{aim of this}} {{experiment}} was {{to find out how}} the results change for the upright-pedal route by using different parameters. At the beginning, several German clinics for horses were visited and the blocks to do the upright pedal route were measured according to a certain model. Consequently, the angle of the block for the upright-pedal route which were used as well as the distance between the ground and the top of the horse’s hoof were determined. For my own tests, 16 feet of horses for slaughter from unknown origins and genesis were used. The different parameters were the three angles of the block (55 degrees, 60 degrees and 65 degrees) and the alignment of the x-ray beam in four different positions for each of the angles. These four different positions are: directly on the coronet, 2 cm above, 2 cm below, and inclined by 5 degrees and directed towards the coronet. First, a movable block which can do different angles was built using Plexiglas. Not only the pilot test but also the main tests were conducted with this block. The question whether it is possible to do without a conventional scatter <b>radiation</b> <b>grid</b> by using the Grödel-technique and the computed radiography was tried to answer in the pilot tests. Here, the distance of 15 cm between the film and the object was determined with which it was possible to take a sharp picture {{while at the same time}} being able to recognize details. This distance of 15 cm between the film and the object was used for all upright-pedal routes in the main tests as well. In the following, of each foot four upright-pedal routes with different directions of the x-ray beam were taken from three different angles. The pictures of the main tests were analyzed according to different parameters. These parameters correspond to the criteria in the guidelines for X-Ray interpretations 07. The classification according to these guidelines was also taken as analysis criteria. In the sond part, four out of the 16 feet were selected randomly and the same upright-pedal roots as mentioned above were taken again with a scatter <b>radiation</b> <b>grid</b> and a high quality film. These upright-pedal routes were developed in an analog way and were analyzed equally as the digitally developed ones. At the end, several navicular bones were isolated from the surrounding tissue and x-rays taken again from different positions with different block angles. With a certain setting of the movable upright-pedal route, the position of the navicular bone in the equine foot could be determined by using a tangent. This was possible because a latero-medial projection of the distal limb had been taken in each angle position before. The different analysis parameters were also taken from these upright-pedal routes, which were all developed digitally. Deviations of the different parameters were analyzed statistically. Consequently, the different upright-pedal routes of a foot with the direction towards the x-ray beam in one of the three angle settings were compared to those of the corresponding upright-pedal routes of the isolated navicular bones. The difference and deviations were recorded. It turned out that the best upright-pedal routes were those of the intact feet. These intact feet had the most agreement with the upright-pedal routes of the isolated navicular bones; sometimes they even provided better information than the others. The different analyzed parameters of the upright-pedal routes with analogue radiation were compared to those with digital radiography. As a result, it was shown that it was possible to get high quality upright-pedal routes for a purchase examination with digital radiography using the Grödel-technique. In regard to the structure of the navicular bones one could even get a better resolution in details with digital radiography. The analysis of the digital radiography in regard to the isolated navicular bones showed an ambiguous result statistically. This was due to the fact that the analysis referred particularly to one single parameter and not to the general impression of the upright-pedal route. Therefore, a subjective analysis of the individual pictures was necessary; this is the same as is usually the case with a purchase examination. The statistic and the subjective analyses were finally compared and differences noted. In brief, the best results were gained when the angle of the block to do the upright-pedal route was 60 degrees, directing at the x-ray beam and placed 2 cm above the coronet. In addition, using the Grödel-technique and a distance between film and object of 15 cm in computed radiography were the best conditions and prerequisites of all those under discussion...|$|E
40|$|Introduction With digital {{radiography}} (DR), radiographers receive immediate feedback on detector exposure {{in the form}} of the exposure index (EI). Purpose To identify whether radiographers were meeting the manufacturer-recommended EI (MREI) value ranges for routine chest, abdomen and pelvis X-ray examinations under a variety of conditions and to examine factors that impacted upon EI levels. Methods Data on 5000 adult X-ray examinations were collected, including: EI values, examination parameters, patients’ gender, dates of birth, dates and times of examinations, grid usages and the presence of metallic implants or prostheses. Descriptive statistics were used to summarise data sets and Mann–Whitney U testing was performed to establish causal agents for EI value variations. Results EI values were often outside the MREI ranges. EI value variations were identified, with significantly higher EI values recorded for female compared with male patients for all manufacturers (P ≤ 0. 04), indicating higher detector exposures except for Philips direct {{digital radiography}} (DDR), where increased EI values indicated lower exposures (P= 0. 01). Higher EI values were also noted for the following variables: out of hours radiography (P ≤ 0. 02), the absence of secondary <b>radiation</b> <b>grids</b> with Philips DDR chest X-rays (P = 0. 03), younger patients with Siemens DDR chest X-rays (P < 0. 0001) and higher kVp for Carestream computed radiography (CR) chest X-rays (P = 0. 02). Significantly lower median EI values were demonstrated for Carestream CR chest X-rays when a metallic implant or prosthesis was present (P = 0. 02). Discussion Non adherence to MREIs has been demonstrated. EI value discrepancies have been identified as being dependent on patient gender, time/day of exposure, grid usage, the presence of a metallic implants or prostheses, patient age and use of ≥ 100 kVp. The value of careful retrospective evaluation of EI databases has been highlighted. Access is restricted to staff and students of the University of Sydney. UniKey credentials are required. Non university access may be obtained by visiting the University of Sydney Library...|$|R
40|$|The element {{efficiency}} of a phased array is {{the ratio of}} the radiated-to-available power of a single element, when only that element is excited. We relate this element efficiency to the output noise power generated by a quasi-optical grid amplifier array. Both electromagnetic and thermodynamic derivations are presented. These ideas are used to predict the total noise power and noise <b>radiation</b> pattern of <b>grid</b> arrays. The results are also extended to show that the output noise temperature of the entire array will be the same as the output noise temperature of a single element...|$|R
40|$|Spatially {{fractionated}} <b>radiation</b> therapy (or <b>grid)</b> using megavoltage x-rays is {{a relatively}} new method of treating bulky (> 8 cm) malignant tumors. Unlike the conventional approach in which the entire tumor is targeted with a nearly uniform <b>radiation</b> field, in <b>grid</b> the incident <b>radiation</b> is collimated with a special grid collimator. As such, only the volume under the open areas of the grid receives direct irradiation from the incident beam; the rest only sees scattered radiation and hence receives significantly less dose. Those regions seeing less dose serve as regrowth areas for normal tissues, thus reducing the normal tissue complication probability after the treatment. Although the grid dose distribution in a tumor is non-uniform, the regression of tumor mass has exhibited uniform regression clinically. Protons have two advantages over megavoltage x-rays which are typically used for grid: (1) protons scatter less in tissue, and (2) they have a fixed range in tissue (the Bragg peak) {{that can be used to}} target a tumor. The goal of this thesis is to computationally and experimentally assess the feasibility of grid using clinical proton beams. The proton pencil beams at the Provision Cancer Center in Knoxville, Tennessee, are used to create an array of beams mimicking the arrangement of beams in grid therapy. The dose distributions at various depths in a solid-water phantom are obtained computationally by the Monte Carlo code MCNP and validated by RayStation experimental Gafchromic film EBT 3. The results are compared with those of the grid using megavoltage x-rays. M. S...|$|R
40|$|A new {{algorithm}} for modeling {{radiative transfer}} in inhomogeneous three-dimensional media is described. The spherical harmonics discrete ordinate method uses a spherical harmonic angular representation to reduce memory use and time computing the source function. The radiative transfer equation is integrated along discrete ordinates through a spatial grid {{to model the}} streaming of <b>radiation.</b> An adaptive <b>grid</b> approach, which places additional points where they are most needed to improve accuracy, is implemented. The solution method {{is a type of}} successive order of scattering approach or Picard iteration. The model computes accurate radiances or fluxes in either the shortwave or longwave regions, even for highly peaked phase functions. Broadband radiative transfer is computed efficiently with a k distribution. The results of validation tests and examples illustrating the efficiency and accuracy of the algorithm are shown for simple geometries and realistic simulated clouds. 1...|$|R
40|$|Satellite and gridded {{meteorological}} {{data can be}} used to estimate evaporation (E) from land surfaces using simple diagnostic models. Two satellite datasets indicate a positive trend (first time derivative) in global available energy from 1983 to 2006, suggesting that positive trends in evaporation may occur in "wet" regions where energy supply limits evaporation. However, decadal trends in evaporation estimated from water balances of 110 wet catchments ((E) over bar (wb)) do not match trends in evaporation estimated using three alternative methods: 1) (E) over bar (MTE), a model-tree ensemble approach that uses statistical relationships between E measured across the global network of flux stations, meteorological drivers, and remotely sensed fraction of absorbed photosynthetically active radiation; 2) (E) over bar (Fu), a Budyko-style hydrometeorological model; and 3) (E) over bar (PML), the Penman-Monteith energy-balance equation coupled with a simple biophysical model for surface conductance. Key model inputs for the estimation of (E) over bar (Fu) and (E) over bar (PML) are remotely sensed <b>radiation</b> and <b>gridded</b> meteorological fields and it is concluded that these data are, as yet, not sufficiently accurate to explain trends in E for wet regions. This provides a significant challenge for satellite-based energy-balance methods. Trends in (E) over bar (wb) for 87 "dry" catchments are strongly correlated to trends in precipitation (R- 2 = 0. 85). These trends were best captured by (E) over bar (Fu), which explicitly includes precipitation and available energy as model inputs...|$|R
40|$|We {{present a}} simple method to solve spherical {{harmonics}} moment systems, {{such as the}} the time-dependent $P_N$ and $SP_N$ equations, of radiative transfer. The method, which works for arbitrary moment order $N$, makes use of the specific coupling between the moments in the $P_N$ equations. This coupling naturally induces staggered grids in space and time, which in turn {{give rise to a}} canonical, second-order accurate finite difference scheme. While the scheme does not possess TVD or realizability limiters, its simplicity allows for a very efficient implementation in Matlab. We present several test cases, some of which demonstrate that the code solves problems with ten million degrees of freedom in space, angle, and time within a few seconds. The code for the numerical scheme, called StaRMAP (Staggered <b>grid</b> <b>Radiation</b> Moment Approximation), along with files for all presented test cases, can be downloaded so that all results can be reproduced by the reader. Comment: 28 pages, 7 figures; StaRMAP code available at [URL]...|$|R
40|$|Abstract. We {{present a}} simple method to solve spherical {{harmonics}} moment systems, {{such as the}} the time-dependent PN and SPN equations, of radiative transfer. The method, which works for arbitrary moment order N, makes use of the specific coupling between the moments in the PN equations. This coupling naturally induces staggered grids in space and time, which in turn {{give rise to a}} canonical, second-order accurate finite difference scheme. While the scheme does not possess TVD or realizability limiters, its simplicity allows for a very efficient implementation in Matlab. We present several test cases, some of which demonstrate that the code solves problems with ten million degrees of freedom in space, angle, and time within a few seconds. The code for the numerical scheme, called StaRMAP (Staggered <b>grid</b> <b>Radiation</b> Moment Approximation), along with files for all presented test cases, can be downloaded so that all results can be reproduced by the reader. 1...|$|R
40|$|Conventional {{radiology}} {{is performed}} {{by means of}} digital detectors, with various types of technology and different performance in terms of efficiency and image quality. Following {{the arrival of a}} new digital detector in a radiology department, all the staff involved should adapt the procedure parameters to the properties of the detector, in order to achieve an optimal result in terms of correct diagnostic information and minimum radiation risks for the patient. The aim {{of this study was to}} develop and validate a software capable of simulating a digital X-ray imaging system, using graphics processing unit computing. All radiological image components were implemented in this application: an X-ray tube with primary beam, a virtual patient, noise, scatter <b>radiation,</b> a <b>grid</b> and a digital detector. Three different digital detectors (two digital radiography and a computed radiography systems) were implemented. In order to validate the software, we carried out a quantitative comparison of geometrical and anthropomorphic phantom simulated images with those acquired. In terms of average pixel values, the maximum differences were below 15 %, while the noise values were in agreement with a maximum difference of 20 %. The relative trends of contrast to noise ratio versus beam energy and intensity were well simulated. Total calculation times were below 3 seconds for clinical images with pixel size of actual dimensions less than 0. 2 mm. The application proved to be efficient and realistic. Short calculation times and the accuracy of the results obtained make this software a useful tool for training operators and dose optimisation studies...|$|R
40|$|SLAM (Short-term Local scale Ammonia {{transport}} Model) {{has been}} developed to calculate the ammonia concentrations in a multiple source area on a short term (hour) and local scale (100 m up to 15 km). In SLAM the dispersion in the surface layer is modelled using a description given by Gryning et al. (1987). The model also takes into account dry deposition and chemical conversion of ammonia {{as well as a}} background concentration caused by remote sources outside the model area. The meteorological input parameters are hourly averaged values of wind speed, wind direction and its standard deviation, temperature and global <b>radiation.</b> The <b>grid</b> setting is limited to 75 x 75 grids within an area of 15 x 15 km. Also extra receptor points can be specified. Basically the number of sources is unlimited but will be restricted by computing time. A sensitivity analysis of several model parameters (except emission strength) showed that wind speed, standard deviation of the lateral wind global radiation, source height and height at which the concentration was calculated, were the most important factors influencing the calculated concentrations. Uncertainties in emissions are directly translated into to model results and are expected to be the largest uncertainty source. SLAM largely overestimated measured concentrations when the wind velocity was 1. 5 m s- 1 SLAM calculated a 10 - 15 % higher yearly averaged concentration. Differences between measured and calculated diurnal concentration patterns were 10 - 30 %. From the comparison with mobile measurements it was concluded that the spatial distribution of the ammonia concentration in time by SLAM was reasonable represented. Large deviations between the calculated and measured concentrations could be attributed to a strong variation in local emissions not represented in the inventory. SLAM can easily be adjusted or extended to describe the concentration of other gases and aerosols emitted from multiple source areas. ...|$|R
40|$|A {{methodology}} {{is given}} that converts an existing finite volume radiative transfer method that requires input of local absorption coefficients {{to one that}} can treat a mixture of combustion gases and compute the coefficients on the fly from the local mixture properties. The Full-spectrum k-distribution method is used to transform the radiative transfer equation (RTE) to an alternate wave number variable, g. The coefficients in the transformed equation are calculated at discrete temperatures and participating species mole fractions that span {{the values of the}} problem for each value of g. These results are stored in a table and interpolation is used to find the coefficients at every cell in the field. Finally, the transformed RTE is solved for each g and Gaussian quadrature is used to find the radiant heat flux throughout the field. The present implementation is in an existing cartesian/cylindrical grid radiative transfer code and the local mixture properties are given by a solution of the National Combustion Code (NCC) on the same grid. Based on this work the intention is to apply this method to an existing unstructured <b>grid</b> <b>radiation</b> code which can then be coupled directly to NCC...|$|R
40|$|II. Indications for use The SenoScan ® Full-Field Digital Mammography System is a {{dedicated}} mammography system intended to produce radiographic {{images of the}} human breast {{for the purpose of}} diagnostic and screening mammography. The SenoScan ® Full-Field Digital Mammography system is intended {{to be used in the}} same clinical applications as traditional film-based mammographic systems. III. Device Description The SenoScan ® Full-Field Digital Mammography System (SenoScan®) includes a digital image receptor combined with a conventional mammographic X-ray device except for the collimator. The system consists of a dual filament x-ray tube, slit-collimator, x-ray generator, support arm, support assembly, compression device and CCD digital detector with readout equipment. The arm assembly consists of an X-ray tube on one end and a digital detector on the other end. The entire arm assembly pivots at the focal spot of the x-ray tube as the detector scans from left to right. The slit-collimator shapes the beam and limits the x-ray beam to the 1 -cm active width of the detector. This results in images with very little scatter and no dose penalty associated with absorption of primary <b>radiation</b> by a <b>grid...</b>|$|R
40|$|We {{describe}} {{an extension of}} the Enzo code to enable the direct numerical simulation of inhomogeneous reionization in large cosmological volumes. By direct we mean all dynamical, radiative, and chemical properties are solved self-consistently on the same mesh, as opposed to a postprocessing approach which coarse-grains the radiative transfer. We do, however, employ a simple subgrid model for star formation, which we calibrate to observations. The numerical method presented is a modification of an earlier method presented in Reynolds et al. Radiation transport is done in the grey flux-limited diffusion (FLD) approximation, which is solved by implicit time integration split off from the gas energy and ionization equations, which are solved separately. This results in a faster and more robust scheme for cosmological applications compared to the earlier method. The FLD equation is solved using the hypre optimally scalable geometric multigrid solver from LLNL. By treating the ionizing <b>radiation</b> as a <b>grid</b> field as opposed to rays, our method is scalable with respect to the number of ionizing sources, limited only by the parallel scaling properties of the radiation solver. We test the speed and accuracy of our approach on a number of standar...|$|R
40|$|The {{provision}} of an adequate network of urban infrastructures {{is essential to}} create clean and energy-efficient urban mobility systems. However, the urban infrastructure to support sustainable mobility can produce a substantial environmental burden if no life cycle environmental criteria are applied in its design and management. This paper demonstrates the potential to support energy-efficient and CO 2 -free pedestrian and electric bike (e-bike) mobility through the ecological design (eco-design) of urban elements. An eco-design approach is applied to reconceptualize a conventional pergola toward an eco-product (solar pergola). The solar pergola generates surplus photovoltaic electricity that provides a multifunctional character. According to the end-use of this energy, different scenarios are analyzed for robust decision-making.; The deployment of solar pergolas can contribute to save from 2, 080 kg to over 47, 185 kg of CO 2 eq. and from 350, 390 MJ to over 692, 760 MJ eq. in 10 years, depending on the geographic emplacement (solar <b>radiation</b> and electricity <b>grid</b> system). These savings are equivalent to charging 2 - 9 e-bikes per day using clean energy.; Instead of maximizing infrastructure deployment to shift to environmentally friendly modes of mobility, the implementation of multifunctional urban elements represents a key area of action {{in the context of}} smart city development. Preprin...|$|R
40|$|We {{calculate}} numerically {{the collapse}} of slowly rotating, non-magnetic, massive molecular clumps, which conceivably {{could lead to the}} formation of massive stars. Because radiative acceleration on dust grains plays a critical role in the clump's dynamical evolution, we utilize a wavelength-dependent radiation transfer and a three component dust model: amorphous carbon particles, silicates and "dirty ice"-coated silicates. We do not spatially resolve the innermost regions of the molecular clump and assume that all material in the innermost grid cell accretes onto a single object. We introduce a semi-analytical scheme for augmenting existing evolution tracks of pre-main sequence protostars by including the effects of accretion. By considering an open outermost boundary, an arbitrary amount of material could, in principal, be accreted onto this central star. However, for the three cases considered (30, 60, and 120 solar masses originally within the computation <b>grid),</b> <b>radiation</b> acceleration limited the final masses to 31. 6, 33. 6, and 42. 9 solar masses, respectively, for wavelength-dependent radiation transfer and to 19. 1, 20. 1, and 22. 9 solar masses for comparison simulations with grey radiation transfer. We demonstrate that massive stars can in principle be formed via accretion through a disk. We conclude with the warning that a careful treatment of radiation transfer is a mandatory requirement for realistic simulations of the formation of massive stars. Comment: 39 pages, 13 figures, 4 tables, AASTEX v 5. 0, accepted by Ap...|$|R
40|$|We have updated our {{publicly}} available dust radiative transfer code (HOCHUNK 3 D) to include new emission processes and various 3 -D geometries appropriate for forming stars. The 3 -D geometries include warps and spirals in disks, accretion hotspots {{on the central}} star, fractal clumping density enhancements, and misaligned inner disks. Additional axisymmetric (2 -D) features include gaps in disks and envelopes, "puffed-up inner rims" in disks, multiple bipolar cavity walls, and iteration of disk vertical structure assuming hydrostatic equilibrium. We include the option for simple power-law envelope geometry, which combined with fractal clumping, and bipolar cavities, {{can be used to}} model evolved stars as well as protostars. We include non-thermal emission from PAHs and very small grains, and external illumination from the interstellar <b>radiation</b> field. The <b>grid</b> structure was modified to allow multiple dust species in each cell; based on this, a simple prescription is implemented to model dust stratification. We describe these features in detail, and show example calculations of each. Some of the more interesting results include the following: 1) Outflow cavities may be more clumpy than infalling envelopes. 2) PAH emission in high-mass stars may be a better indicator of evolutionary stage than the broadband SED slope; and related to this, 3) externally illuminated clumps and high-mass stars in optically thin clouds can masquerade as YSOs. 4) Our hydrostatic equilibrium models suggest that dust settling is likely ubiquitous in T Tauri disks, in agreement with previous observations. Comment: 44 pages, 31 figures, 1 table, accepted for publication in ApJ...|$|R
40|$|We {{describe}} {{an extension of}} the Enzo code to enable fully-coupled radiation hydrodynamical simulation of inhomogeneous reionization in large ∼ (100 Mpc) ^ 3 cosmological volumes with thousands to millions of point sources. We solve all dynamical, radiative transfer, thermal, and ionization processes self-consistently on the same mesh, as opposed to a postprocessing approach which coarse-grains the radiative transfer. We do, however, employ a simple subgrid model for star formation which we calibrate to observations. Radiation transport is done in the grey flux-limited diffusion (FLD) approximation, which is solved by implicit time integration split off from the gas energy and ionization equations, which are solved separately. This results in a faster and more robust scheme for cosmological applications compared to the earlier method. The FLD equation is solved using the hypre optimally scalable geometric multigrid solver from LLNL. By treating the ionizing <b>radiation</b> as a <b>grid</b> field as opposed to rays, our method is scalable with respect to the number of ionizing sources, limited only by the parallel scaling properties of the radiation solver. We test the speed and accuracy of our approach on a number of standard verification and validation tests. We show by direct comparison with Enzo's adaptive ray tracing method Moray that the well-known inability of FLD to cast a shadow behind opaque clouds has a minor effect on the evolution of ionized volume and mass fractions in a reionization simulation validation test. We illustrate an application of our method to the problem of inhomogeneous reionization in a 80 Mpc comoving box resolved with 3200 ^ 3 Eulerian grid cells and dark matter particles. Comment: 32 pages, 23 figures. ApJ Supp accepted. New title and substantial revisions re. v...|$|R
40|$|The {{slope and}} {{aspect of a}} vegetated surface {{strongly}} affects the amount of solar radiation intercepted by that surface. Solar radiation is the dominant component of the surface energy balance and influences ecologically critical factors of microclimate, including near-surface temperatures, evaporative demand and soil moisture content. It also determines the exposure of vegetation to photosynthetically active and ultra-violet wavelengths. Spatial variation in slope and aspect is therefore a key determinant of vegetation pattern, species distribution and ecosystem processes in many environments. Slope and aspect angle may vary considerably over distances of a few metres, and fine-scale species' distribution patterns frequently follow these topographic patterns. The availability of suitable microclimate at such scales may be critical for the response of species distributions to climatic change at much larger spatial scales. However, quantifying the relevant microclimatic gradients is not straightforward, as the potential variation in solar radiation flux under clear-sky conditions is modified by local and regional variations in cloud cover, and interacts with long-wave radiation exchange, local meteorology and surface characteristics. We tested simple models of near-surface temperature and potential evapotranspiration driven by meteorological data with the incoming solar radiation flux adjusted for topography against measurements of temperature and soil moisture at two chalk grassland field sites in contrasting regional climates of the United Kingdom. We then estimated the cumulative distribution function of three key ecological variables (monthly temperature sums above 5 and 30 degrees C, plus potential evapotranspiration) across areas of complex topography at each site using two separate approaches: a spatially explicit and a spatially implicit method. The spatially explicit method uses digital elevation models of the sites to calculate the solar <b>radiation</b> at each <b>grid</b> cell and hence determines the spatial distribution of environmental variables. The second, less computationally intensive, method uses estimated statistical distributions of slope and aspect within the field sites to calculate {{the proportion of the}} surface area of each site predicted to exceed a given threshold of temperature sum or potential evapotranspiration. The spatially implicit model reproduces the range of the explicit model reasonably well but is limited by the parameterisation of slope and aspect, underlining the importance of variation in topography in determining the microclimatic conditions of a site...|$|R
40|$|We {{calculate}} numerically {{the collapse}} of slowly rotating, nonmagnetic, massive molecular clumps of masses 30, 60, and 120 Stellar Mass, which conceivably {{could lead to the}} formation of massive stars. Because radiative acceleration on dust grains plays a critical role in the clump's dynamical evolution, we have improved the module for continuum radiation transfer in an existing two-dimensional (axial symmetry assumed) radiation hydrodynamic code. In particular, rather than using "gray" dust opacities and "gray" radiation transfer, we calculate the dust's wavelength-dependent absorption and emission simultaneously with the radiation density at each wavelength and the equilibrium temperatures of three grain components: amorphous carbon particles. silicates, and " dirty ice " -coated silicates. Because our simulations cannot spatially resolve the innermost regions of the molecular clump, however, we cannot distinguish between the formation of a dense central cluster or a single massive object. Furthermore, we cannot exclude significant mass loss from the central object(s) that may interact with the inflow into the central grid cell. Thus, with our basic assumption that all material in the innermost grid cell accretes onto a single object. we are able to provide only an upper limit to the mass of stars that could possibly be formed. We introduce a semianalytical scheme for augmenting existing evolutionary tracks of pre-main-sequence protostars by including the effects of accretion. By considering an open outermost boundary, an arbitrary amount of material could, in principal, be accreted onto this central star. However, for the three cases considered (30, 60, and 120 Stellar Mass originally within the computation <b>grid),</b> <b>radiation</b> acceleration limited the final masses to 3 1. 6, 33. 6, and 42. 9 Stellar Mass, respectively, for wavelength-dependent radiation transfer and to 19. 1, 20. 1, and 22. 9 Stellar Mass. for the corresponding simulations with gray radiation transfer. Our calculations demonstrate that massive stars can in principle be formed via accretion through a disk. The accretion rate onto the central source increases rapidly after one initial free-fall time and decreases monotonically afterward. By enhancing the nonisotropic character of the radiation field, the accretion disk reduces the effects of radiative acceleration in the radial direction - a process we call the "flashlight effect. " The flashlight effect is further amplified in our case by including the effects of frequency-dependent radiation transfer. We conclude with the warning that a careful treatment of radiation transfer is a mandatory requirement for realistic simulations of the formation of massive stars...|$|R
40|$|This {{dissertation}} {{describes the}} development and validation of a methodology for estimating the consequences of accidental dust explosions in complex geometries. The approach adopted entails the use of results from standardized tests in 20 -litre explosion vessels as input to the combustion model in a computational fluid dynamics (CFD) code, and the subsequent validation of the model system by comparing with results from laboratory and large-scale experiments. The PhD project includes dedicated laboratory experiments designed to explore selected aspects of flame propagation in dust clouds, and to reveal {{similarities and differences between}} flame propagation in gaseous mixtures and mechanical suspensions of combustible powder in air. The research project represents a continuation of numerous efforts by various research groups, where the key underlying problem has been the scaling of results obtained in laboratory tests for predicting the consequences of dust explosion scenarios in industry. The traditional approach to the scaling problem entails the use of empirical correlations, typically represented as nomographs or formulas in relevant safety standards. It is generally accepted that empirical correlations may work reasonably well for simple geometries, such as isolated process vessels and silos. The need for more sophisticated methods arises for accident scenarios that involve complex geometrical boundary conditions, such as flame propagation in connected vessel systems and secondary dust explosions inside buildings. The European Commission (EC) supported the Dust Explosion Simulation Code (DESC) project under the Fifth Framework Programme. The goal was to develop and validate a CFD code for simulating industrial dust explosions in complex geometries. To this end, GexCon created the CFD code DESC (Dust Explosion Simulation Code) by modifying the existing CFD code FLACS (FLame ACceleration Simulator), originally developed for simulating gas explosions in congested offshore geometries. The specific contributions from the candidate with respect {{to the development of the}} CFD software is limited to the methodology for estimating combustion parameters for a given dust sample from experimental results, the validation of the resulting model system against experimental data, and general participation in the R&D team during the development process. The modelling of particle-laden flow and heterogeneous combustion in the CFD code DESC involves several simplifying assumptions. The flow model assumes thermal and kinetic equilibrium between the dispersed particles and the continuous phase, and the k-ε turbulence model in FLACS remains unchanged for multiphase flows. The empirical correlation for the turbulent burning velocity in dust clouds originates from experiments with premixed combustion in gaseous mixtures. The fraction of dust that takes part in the combustion reactions, as function of the nominal dust concentration, is estimated from the explosion pressures measured in a constant volume explosion vessel. The thermodynamic data available in FLACS limit the application area to materials containing the elements carbon, hydrogen, oxygen, nitrogen and sulphur. The simplifications limit the application area of DESC to certain classes of materials, and flame propagation in dust clouds with relatively high reactivity. DESC do not contain models for simulating phenomena such as agglomeration, gravitational settling, and selective separation of particles in flow through cyclones or along other curved paths. In spite of the simplicity of the model system, the results from the validation work show that the CFD code DESC can describe the course of dust explosions in relatively complex geometries with reasonable accuracy relative to the inherent spread in the experimental results. The results obtained for silo explosions reproduce trends observed for variation in vent area and ignition position from various experiments. Results obtained for flame propagation sustained by dust dispersion from a layer indicate that the empirical model for dust lifting in DESC is suitable for the purpose. Results obtained for dust explosions vented through ducts reproduce the experimental trends fairly well. Simulations of dust explosions in a system of two vented vessels connected by a pipe with a 90 ° bend indicate that the DESC can reproduce relatively complex chains of events, including dust lifting from a layer. The results for the connected vessel system also demonstrate how sensitive the results can be with respect to modest changes in the initial and boundary conditions. Finally, simulations of explosion experiments in elongated vessels with repeated obstacles reproduce the experimental trends fairly well. Although the results from the validation work indicate that CFD simulations can become a valuable tool for consequence modelling and design of industrial facilities, the modelling in DESC requires further improvements. An essential improvement entails fundamental changes to the numerical solver to reduce in the influence of the grid resolution on the results from the simulations. In the current versions of FLACS and DESC, simulation of explosion scenarios is subject to strict grid guidelines. The current versions of both codes use a structured Cartesian grid, with limited possibilities for local grid refinement. This poses a particular challenge for DESC, since the grid resolution required to resolve complex internal geometries on a structured Cartesian grid varies significantly from case to case. The long-term solution to these challenges will presumably entail the use of adaptive mesh refinement (AMR), and this is outside the scope of the present work. The model system may also benefit from various other improvements, such as turbulent burning velocity correlations specifically developed for dust explosions, an explicit model for turbulent flame thickness, <b>radiation</b> models, local <b>grid</b> refinement in the region where ignition occurs, reduced dependence on empirical input to the model system, and in general more realistic modelling of particleladen flow and heterogeneous combustion. There is, however, a fine balance between the level of detailed information that must be specified in the model, and the applicability and user-friendliness of the model system. For most industrial applications of a CFD tool for dust explosions, there are significant inherent uncertainties associated with initial and boundary conditions. Dust explosion experiments in transparent balloons show that the initial phase of flame propagation in turbulent dust clouds can progress in a distributed manner, with very limited energy output. This observation may explain some of the challenges associated with the analysis of pressure-time histories from 20 -litre explosion vessels for dust explosions when using a weak ignition source. Experiments in a 3. 6 -m flame acceleration tube demonstrate the importance of explosion-generated turbulence for dust explosions, and illustrate the challenge associated with poor repeatability in dust explosion experiments. The results obtained for propane-air mixtures in the same apparatus indicate that FLACS under-predicts the rate of combustion for turbulent flame propagation in fuel-rich propane-air mixtures. The CFD code DESC represents a significant step forward for process safety related to dust explosions in the process industry. There is, however, significant room for further improvements to the model system, and dedicated experiments will play an important role for the future development of the code. Improved safety in the process industry requires reliable and well-documented consequence models, and future development of DESC should include an integrated framework for model validation, including verification and testing. </p...|$|R

