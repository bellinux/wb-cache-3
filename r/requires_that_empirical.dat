1|10000|Public
40|$|Small-scale aerial {{photographs}} and high-resolution satellite images, available for Ethiopia {{since the second}} half of the twentieth century as for most countries, allow only the length of gullies to be determined. Understanding the development of gully volumes therefore <b>requires</b> <b>that</b> <b>empirical</b> relations between gully volume (V) and length (L) are established in the field. So far, such V–L relations have been proposed for a limited number of gullies/environments and were especially developed for ephemeral gullies. In this study, V–L relations were established for permanent gullies in northern Ethiopia, having a total length of 152 km. In order to take the regional variability in environmental characteristics into account, factors that control gully cross-sectional morphology were studied from 811 cross-sections. This indicated that the lithology and the presence of check dams or low-active channels were the most important controls of gully cross-sectional shape and size. Cross-sectional size could be fairly well predicted by their drainage area. The V–L relation for the complete dataset was V= 0 562 L 1381 (n = 33, r 2 = 0 94, with 34 9 % of the network having check dams and/or being low-active). Producing such relations for the different lithologies and percentages of the gully network having check dams and/or being low-active allows historical gully development from historical remote sensing data to be assessed. In addition, gully volume was also related to its catchments area (A) and catchment slope gradient (Sc). This study demonstrates that V–L and V–ASc relations can be very suitable for planners to assess gully volume, but that the establishment of such relations is necessarily region-specificstatus: publishe...|$|E
40|$|Abstract. Recent {{developments}} in Austrian-market-process economics are discussed, and, despite the continuing difficulties {{of communicating with}} mainstream economics, some causes for optimism are discerned. Looking to a useful future for Austrian economics will <b>require</b> <b>that</b> further <b>empirical</b> work in applying its insights be done. The question of placing {{the burden of proof}} in policy discussions is examined. JEL classifications: A 1, B...|$|R
25|$|While for {{bodies in}} their own {{thermodynamic}} equilibrium states, the notion of temperature <b>requires</b> <b>that</b> all <b>empirical</b> thermometers must agree as to which of two bodies is the hotter or that {{they are at the}} same temperature, this requirement is not safe for bodies that are in steady states though not in thermodynamic equilibrium. It can then well be <b>that</b> different <b>empirical</b> thermometers disagree about which is the hotter, and if this is so, then {{at least one of the}} bodies does not have a well defined absolute thermodynamic temperature. Nevertheless, any one given body and any one suitable empirical thermometer can still support notions of empirical, non-absolute, hotness and temperature, for a suitable range of processes. This is a matter for study in non-equilibrium thermodynamics.|$|R
40|$|Abstract — In this paper, we {{consider}} queue-length stability in wireless networks under a general class of arrival processes <b>that</b> only <b>requires</b> <b>that</b> the <b>empirical</b> average converges {{to the actual}} average polynomially fast. We present a scheduling policy, sequential maximal scheduling, and use novel proof techniques {{to show that it}} attains 2 of the maximum 3 stability region in tree-graphs under primary interference constraints, for all such arrival processes. For degree bounded networks, the computation time of the policy varies as the the logarithm of the network size. Our results are a significant improvement over previous results that attain only 1 of the maximum throughput region even for graphs that have a 2 simple path topology, in similar computation time under stronger (i. e., Markovian) assumptions on the arrival process. I...|$|R
40|$|This paper {{begins with}} an {{argument}} that most measure development in the social sciences, with its reliance on correlational techniques as a tool, falls short of the requirements for constructing meaningful, unidimensional measures of human attributes. By demonstrating how rating scales are ordinal-level data, we argue the necessity of converting these to equal-interval units to develop a measure that is both qualitatively and quantitatively defensible. This <b>requires</b> <b>that</b> the <b>empirical</b> results and theoretical explanation are questioned and adjusted at {{each step of the}} process. In our response to the reviewers, we describe how this approach was used to develop the Game Engagement Questionnaire (GEQ), including its emphasis on examining a continuum of involvement in violent video games. The GEQ is an empirically sound measure focused on one player characteristic that may be important in determining game influence...|$|R
40|$|Buildings {{are subject}} to change of {{requirements}} during their periods of use. In the last decades it has been mostly assumed that the rate of change is increasing. Exact description of the type of change or the amount of change is not always investigated. Requirements change knowledge in currently used buildings is lacking. A method is <b>required</b> <b>that</b> identifies <b>empirical</b> building change knowledge. Identification of changes in buildings has the objective of examining if the empirical change could be useful in forecast change in newly designed buildings. The rest of the paper is organized as follows: Section 1 overviews general knowledge and building knowledge. Section 2 identifies and structures requirement change knowledge. Element change knowledge is structured and described in section 3. A change measurement method is introduced in section 4. Empirical analysis application to the change method is explained in section 5. A tool prototype in section 6 shows how change knowledge is captured from existing buildings and made useful to forecast change in new designs...|$|R
50|$|Analytical {{reasoning}} {{refers to}} the ability to look at information, be it qualitative or quantitative in nature, and discern patterns within the information. Analytical reasoning involves deductive reasoning with no specialised knowledge, such as: comprehending the basic structure of a set of relationships; recognizing logically equivalent statements; and inferring what could be true or must be true from given facts and rules. Analytical reasoning is axiomatic in that its truth is self-evident. In contrast, synthetic reasoning <b>requires</b> <b>that</b> we include <b>empirical</b> observations, which are always open to doubt. The specific terms “analytic” and “synthetic” themselves were introduced by Kant (1781) {{at the beginning of his}} Critique of Pure Reason.|$|R
40|$|The need to {{implement}} strategies that promote knowledge management {{in order to}} transcend the management of information in a society marked by change problem solving context {{and the influence of}} technology in scientific innovation processes. The methodology used for the conceptual cartography in order to organize information collected, primary and secondary sources were recovered mainly from academic google. The results show four key aspects in the management of knowledge from socioformation. As a consecuence we have the basis of a key concept in the transformation of education in Latin America <b>that</b> <b>requires</b> <b>empirical</b> studies to consolidate in future pedagogical reforms...|$|R
40|$|In a {{previous}} paper (Technical Note No. 605), a theory was developed <b>that</b> <b>required</b> an <b>empirical</b> relation to calculate sound pressures {{for the higher}} harmonics. Further investigation indicated that the modified theory agrees with experiment and <b>that</b> the <b>empirical</b> relation was due to an interference phenomenon peculiar to the test arrangement used. Comparison is made between the test results for a two-blade arrangement and the theory. The comparison is made for sound pressures in {{the plane of the}} revolving blades for varying values of tip velocity. Comparison is also made at constant tip velocity for all values of azimuth angle B. A further check is made between the theory and the experimental results for the fundamental of a four-blade arrangement with blades of the same dimensions as those used in the two-blade arrangement...|$|R
40|$|A {{system of}} {{analytical}} and numerical two-dimensional mixer/ejector nozzle models <b>that</b> <b>require</b> minimal <b>empirical</b> input {{has been developed}} and programmed for use in conceptual and preliminary design. This report contains a user's guide describing {{the operation of the}} computer code, DREA (Differential Reduced Ejector/mixer Analysis), that contains these mathematical models. This program is currently being adopted by the Propulsion Systems Analysis Office at the NASA Glenn Research Center. A brief summary of the DREA method is provided, followed by detailed descriptions of the program input and output files. Sample cases demonstrating the application of the program are presented...|$|R
40|$|In {{spite of}} the great effort {{made in the last}} decades to improve our {{understanding}} of stellar evolution, significant uncertainties remain due to our poor knowledge of some complex physical processes <b>that</b> <b>require</b> an <b>empirical</b> calibration, such as the efficiency of the interior mixing related to convective overshoot. Here we review the impact of convective overshoot on the evolution of stars during the main Hydrogen and Helium burning phases. Comment: Proc. of the workshop "Asteroseismology of stellar populations in the Milky Way" (Sesto, 22 - 26 July 2013), Astrophysics and Space Science Proceedings, (eds. A. Miglio, L. Girardi, P. Eggenberger, J. Montalban...|$|R
40|$|Abstract—Feature-oriented {{software}} {{development is a}} promising paradigm to implement variable software. One advantage is that crosscutting concerns can be modularized, {{which in turn has}} a positive effect on program comprehension. However, benefits for program comprehension are mostly based on plausibility arguments and theoretical discussions. This is not sufficient, since program comprehension is an internal cognitive process <b>that</b> <b>requires</b> <b>empirical</b> evaluation. Up to today, there are only few <b>empirical</b> studies <b>that</b> evaluate the effect of feature-oriented {{software development}} on program comprehension. With our work, we aim at filling this gap and providing sound empirical evidence about the effect of featureoriented software development on program comprehension. Keywords-program comprehension, feature-oriented software development, variable software I...|$|R
40|$|Modern {{consciousness}} {{studies are}} in a healthy state, with many progressive empirical programmes in cognitive science, neuroscience and related sciences, using relatively conventional third-person research methods. However not all the problems of consciousness can be resolved in this way. These problems may be grouped into problems <b>that</b> <b>require</b> <b>empirical</b> advance, those <b>that</b> <b>require</b> theoretical advance, and those <b>that</b> <b>require</b> a re-examination {{of some of our}} pre-theoretical assumptions. I give examples of these, and focus on two problems—what consciousness is, and what consciousness does—that require all three. In this, careful attention to conscious phenomenology and finding an appropriate way to relate first-person evidence to third-person evidence appears to be central to progress. But we may also need to reexamine what we take to be “natural facts ” about the world, and how we can know them. The same appears to be true for a trans-cultural understanding of consciousness that combines classical Indian phenomenological methods with the third-person methods of Western science...|$|R
40|$|This paper {{considers}} low-complexity coded multiple-input-multiple-output transmission in Rayleigh channels with {{correlation between}} antennas {{at both the}} transmitter and receiver. We consider statistical beamforming (SB) and spatial multiplexing (SM) with a zero-forcing receiver. We calculate the link-level capacity of both schemes with bit-interleaved coded modulation and derive accurate closed-form approximations to the bit error rate. We then show how the resulting expressions can. be used in an adaptive algorithm to select the best combination of code rate, modulation format, and transmission scheme (SB or SM) {{in order to maximize}} throughput. Unlike other mode-switching schemes <b>that</b> <b>require</b> <b>empirical</b> lookup tables, this approach applies to any correlation scenario. Numerical studies are used to demonstrate the performance as a function of signal-to-noise ratio and correlation parameters...|$|R
40|$|Abstract—This paper {{considers}} low-complexity coded multipleinput–multiple-output transmission in Rayleigh channels with {{correlation between}} antennas {{at both the}} transmitter and receiver. We consider statistical beamforming (SB) and spatial multiplexing (SM) with a zero-forcing receiver. We calculate the link-level capacity of both schemes with bit-interleaved coded modulation and derive accurate closed-form approximations to the bit error rate. We then show how the resulting expressions {{can be used in}} an adaptive algorithm to select the best combination of code rate, modulation format, and transmission scheme (SB or SM) in order to maximize throughput. Unlike other mode-switching schemes <b>that</b> <b>require</b> <b>empirical</b> lookup tables, this approach applies to any correlation scenario. Numerical studies are used to demonstrate the performance as a function of signal-to-noise ratio and correlation parameters. Index Terms—Adaptive coding, modulation, multiple-input– multiple-output (MIMO) communications, spatially correlated channels. I...|$|R
40|$|Three {{distinct}} clusters {{were identified}} from a survey {{study of a}} sample of 127 unit coordinators from a regional Australian University. The clusters emerged after a survey that explored perceptions of pedagogical practices that incorporated the use of Information Communication and Technology (ICT). The key components of the survey were based on seven constructs derived from the Technological Pedagogical and Content Knowledge (TPACK). For future investigations of TPACK application in university contexts, a three-cluster configuration of teacher-practitioners is proposed <b>that</b> <b>requires</b> <b>empirical</b> confirmation. Alongside the theorised clusters of university lecturers according to their perceived engagement with ICT, several layers of technology policy disconnect have also been discovered. The relevance {{of the findings of}} the inquiry and their implications on universities that conduct ICT intensive courses are also discussed, especially in relation to improving teaching practices...|$|R
40|$|The Pathways {{articles}} to date {{were intended to}} engagefaculty in teaching, learning, and assessment, espe-cially in large enrollment courses. The challenge for many faculty who have changed their courses is {{to determine if the}} innovations actually improve student learning. This leads some faculty towards research models <b>that</b> <b>require</b> <b>empirical</b> evidence based on student assessment data. Here we describe a framework for research on scientific teaching. Articles in subsequent months will provide practical advice for faculty who are interested in class-room research. We will use constructivist theories of how people learn (ie existing knowledge is used to build new knowledge; Bransford et al. 1999) to explore questions about how students actively gain meaningful understand-ing (Ausubel 2000). We also provide examples of research strategies and how one might gather evidence to asses...|$|R
40|$|Progress in {{the study}} of emotion <b>requires</b> <b>that</b> the field move toward consensus. To this end, we {{undertake}} an attempt to integrate one version of appraisal theory and one version of psychological construction theory, based on the following points of agreement: (a) the to-be-explained phenomena are episodes classified by most people as emotional, (b) these episodes consist of various component, none of which can be identified as the emotion, (c) the components are not predetermined by an affect program, but constructed on the spot based on several sources of information, and (d) research should study relations among components instead of relations among emotions and components. Our attempt also reveals differences and issues <b>that</b> <b>require</b> further <b>empirical</b> testing. Psychological construction has been vague about how the components are connected, whereas appraisal theories have ventured concrete hypotheses about relations between specific appraisal and specific other components. More empirical research is needed, however, to test these and alternative hypotheses, and to contrast the role of appraisal with the role of other types of information processing. status: publishe...|$|R
50|$|Direct {{acquaintance}} only {{refers to}} the individual’s direct access to some aspect of her/his experience, whereas knowledge by acquaintance <b>requires</b> <b>that</b> the individual have a belief about it. Russell and other acquaintance theorists assert that not only does acquaintance make knowledge possible; it makes thinking itself possible. This assertion {{is based on the}} epistemic principle <b>that</b> <b>empirical</b> experience is the source of properly simple concepts.|$|R
40|$|Some {{evidence}} {{in the area of}} make-buy decisions for new technologies suggests that {{it is a good idea}} for a company to pursue a fairly rigorous ''make'' policy in the early days of a potentially disruptive innovation. Other studies prescribe exactly the opposite, promoting instead a ''buy'' strategy. This paper seeks to bridge the gap between these perspectives by suggesting that both strategies are valid, but that they are most successfully applied in different market environments. The ''make'' prescription may be more suited to either extremely fast or extremely slow rates of technological change, while a ''buy'' strategy might be more appropriate in market sectors where technologies evolve at a medium pace. This paper highlights the importance of industry clockspeed and supplier relationships in make-buy decisions for new technologies, and puts forward two new hypotheses <b>that</b> <b>require</b> <b>empirical</b> testing...|$|R
40|$|International audienceToday's {{multi-core}} era places significant {{demands on}} an optimizing compiler, which must parallelize programs, exploit memory hierarchy, and leverage the ever-increasing SIMD capabilities of modern processors. Existing model-based heuristics for performance optimization used in compilers {{are limited in}} their ability to identify profitable parallelism/locality trade-offs and usually lead to sub-optimal performance. To address this problem, we distinguish optimizations for which effective model-based heuristics and profitability estimates exist, from optimizations <b>that</b> <b>require</b> <b>empirical</b> search to achieve good performance in a portable fashion. We have developed a completely automatic framework in which we focus the empirical search on the set of valid possibilities to perform fusion/code motion, and rely on model-based mechanisms to perform tiling, vectorization and parallelization on the transformed program. We demonstrate the effectiveness of this approach in terms of strong performance improvements on a single target as well as performance portability across different target architectures...|$|R
40|$|Thesis (Ph. D.) [...] Boston UniversityA {{survey of}} the {{literature}} shows that group psychotherapy with the psychoses, since its inception thirty-five years ago, has assumed a position of major importance {{as a method of}} treatment. Despite the widespread use of group therapy, research has not kept pace with the clinical use of this technique. 	The need for experimentation in this area is recognized. There is general agreement on many fundamental issues <b>that</b> <b>require</b> <b>empirical</b> investigation. In accordance with this consensus regarding the need for basic research, it seemed possible to investigate the following three problems: (l) the effectiveness of group psychotherapy; (2) the relative effectiveness of two different therapeutic approaches; and (3) the influence {{of the role of the}} leader upon the group process. The focus in this study of group therapy was on the development of interaction as it is related to the style of leadership...|$|R
30|$|Selection {{of solvent}} is another {{important}} consideration in microwave extraction. Microwaves are effective on materials that have high dielectric properties, an intrinsic property of the material <b>that</b> <b>requires</b> <b>empirical</b> measurement but is mostly influenced by the moisture liquid/solid mixture content and spatial distribution {{of the water and}} other polar/ionic compound in the matrix. The dielectric properties of materials are defined in terms of their relative complex permittivity. For a solvent/matrix to heat up rapidly under the microwave radiation, it has to have a high dielectric constant, associated with the potential for electrical energy storage in the material, and a high dielectric loss which is related to the electrical energy dissipation in the material [101]. The heating of a dielectric material in the presence of an electromagnetic field is based on intermolecular friction that arises via ionic conduction and dipolar rotation [102]. N-hexane is widely used as solvent for extraction with other commonly used solvents such as isopropanol, methanol, ethanol, acetone and water [89, 90, 103, 104].|$|R
40|$|Click on the DOI link {{to access}} the article (may not be free). In this article, we provide a {{historical}} overview of the Object Relations Inventory (ORI) and related methods {{for the assessment of}} object relations constructed by Sidney Blatt and colleagues (e. g., Blatt, Bers, & Schaffer, 1992; Blatt, Wein, Chevron, & Quinlan, 1979; Diamond, Kaslow, Coonerty, & Blatt, 1990). We clarify terminology that has been used inconsistently in the literature, especially by way of differentiating the methods used to collect descriptions of significant figures, such as the ORI and its predecessor, the Parental Description (PD) task, and the rating scales that Blatt and colleagues constructed to rate those descriptions. We provide a tabular summary of empirical studies of the measure and offer a critical review of those aspects of the instrument <b>that</b> <b>require</b> further <b>empirical</b> investigation and methodological rigor...|$|R
40|$|Abstract. Jonathan Weisberg {{claims that}} certain {{probability}} assessments constructed by Jeffrey conditioning resist subsequent revision {{by a certain}} type of after-the-fact defeater of the reasons supporting those assessments, and that such conditioning is thus “inherently anti-holistic. ” His analysis founders, however, in applying Jeffrey conditioning to a partition for which an essential rigidity condition clearly fails. Applied to an appropriate partition, Jeffrey conditioning is amenable to revision by the sort of after-the-fact defeaters considered by Weisberg in precisely the way that he demands. Key words: defeater, holism, Jeffrey conditioning, rigidity. 1. Holism Denied. Confirmational holism <b>requires</b> <b>that</b> a belief’s <b>empirical</b> justification be sensitive to background belief. For Bayesian epistemology this entails among other things that your probability assessments be amenable to revision in response to the discovery of after-thefact defeaters of reasons supporting those assessments. Jonathan Weisberg (2009) thinks that a certain class of probability assessments constructed by Jeffrey conditioning resist such revision, and that such conditioning is thus “inherently anti-holistic. ” Typifying such cases is, he claims, the following example: Suppose that E asserts that a certain jelly bean is red, and F assert...|$|R
40|$|Decoupled {{payments}} {{have emerged}} {{as an alternative to}} traditional agricultural subsidies that are coupled to production decisions, in order to minimize the distorting impacts of domestic agricultural policy. Economic theory suggests that, when farmers face imperfections in key markets such as that for financial services, even lump-sum subsidies may affect agricultural output. This paper explores these issues by developing and solving a deterministic dynamic optimization model for a credit-constrained representative corn farming household in the United States. The model is parameterized using data from the USDAs ARMS database. Simulations for different levels of DP and other parameters were conducted, and three effects on agricultural output stemming from DP were found: an expanding and temporary liquidity effect; an expanding and permanent credit supply effect; and a contracting and permanent land price effect. The magnitude and direction of the final net effect depends on several factors <b>that</b> <b>require</b> further <b>empirical</b> research. Agricultural Finance,...|$|R
40|$|The {{problem of}} testing for nonhomogeneous white noise (i. e. {{independently}} but possibly nonidentically distributed observations, with a common, specified or unspecified, median) against alternatives of serial dependence is considered. This problem includes as {{a particular case}} the important problem of testing for heteroscedastic white noise. When {{the value of the}} common median is specified, invariance arguments suggest basing this test on a generalized version of classical runs: the generalized runs statistics. These statistics yield a run-based correlogram concept with exact (under the hypothesis of nonhomogeneous white noise) p-values. A run-based portmanteau test is also provided. The local powers and asymptotic relative efficiencies (AREs) of run-based correlograms and the corresponding run-based tests with respect to their traditional parametric counterparts (based on classical correlograms) are investigated and explicitly computed. In practice, however, the value of the exact median of the observations is seldom specified. For such situations, we propose two different solutions. The first solution is based on the classical idea of replacing the unknown median by its empirical counterpart, yielding aligned runs statistics. The asymptotic equivalence between exact and aligned runs statistics is established under extremely mild assumptions. These assumptions do not <b>require</b> <b>that</b> the <b>empirical</b> median consistently estimates the exact one, so that the continuity properties usually invoked in this context are totally helpless. The proofs we are giving are of a combinatorial nature, and related to the so-called Banach match box problem. The second solution is a finite-sample, nonasymptotic one, yielding (for fixed n) strictly conservative testing procedures, irrespectively of the underlying densities. Instead of the empirical median, a nonparametric confidence interval for the unknown median is considered. Run-based correlograms can be expected to play the same role in the statistical analysis of time series with nonhomogeneous innovation process as classical correlograms in the traditional context of second-order stationary ARMA series. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Fair {{trade is}} {{often viewed as}} an {{alternative}} to free trade that reduces global inequality and poverty. This paper examines whether fair trade is truly an alternative to the free market, and as a consequence whether it can effectively advance gender equality and alleviate the poverty of women in less developed countries (LDCs). First, neoclassical economics and trade liberalization policies are reviewed. The paper then examines how fair trade seeks to correct market imperfections, thereby making the free market more efficient in distributing wealth. The ability of fair trade to address the central issues related to trade liberalization and women in LDCs is discussed, and the gendered structures of fair trade identified. Whether fair trade can provide gender equality within global capitalist structures is a theoretical matter <b>that</b> <b>requires</b> further <b>empirical</b> inquiry. Suggestions for future research, informed by feminist theories of the political economy, are provided. Copyright © 2009 John Wiley & Sons, Ltd and ERP Environment. ...|$|R
40|$|Numerous current efforts seek {{to improve}} the {{representation}} of ecosystem ecology and vegetation demographic processes within Earth System Models (ESMs). These developments are widely viewed as {{an important step in}} developing greater realism in predictions of future ecosystem states and fluxes. Increased realism, however, leads to increased model complexity, with new features raising a suite of ecological questions <b>that</b> <b>require</b> <b>empirical</b> constraints. Here, we review the developments that permit the representation of plant demographics in ESMs, and identify issues raised by these developments that highlight important gaps in ecological understanding. These issues inevitably translate into uncertainty in model projections but also allow models to be applied to new processes and questions concerning the dynamics of real-world ecosystems. We argue that stronger and more innovative connections to data, across the range of scales considered, are required to address these gaps in understanding. The development of first-generation land surface models as a unifying framework for ecophysiological understanding stimulated much research into plant physiological traits and gas exchange. Constraining predictions at ecologically relevant spatial and temporal scales will require a similar investment of effort and intensified inter-disciplinary communication. This article is protected by copyright. All rights reserved...|$|R
40|$|Fosgerau and Karlstrom [The {{value of}} reliability. Transportation Research Part B, Vol. 43 (8 – 9), pp. 813 – 820, 2010] {{presented}} a derivation {{of the value}} of travel time variability (VTTV) with a number of desirable properties. This definition of the VTTV depends on certain properties of the distribution of random travel times <b>that</b> <b>require</b> <b>empirical</b> verification. This paper therefore provides a detailed empirical investigation of the distribution of travel times on an urban road. Applying a range of nonparametric statistical techniques to data giving minute-by-minute travel times for a congested urban road over a period of five months, we show that the standardized travel time is roughly independent of the time of day as required by the theory. Except for the extreme right tail, a stable distribution seems to fit the data well. The travel time distributions on consecutive links seem to share a common stability parameter such that the travel time distribution for a sequence of links is also a stable distribution. The parameters of the travel time distribution for a sequence of links can then be derived analytically from the link level distributions. ...|$|R
40|$|This paper {{analyzes}} {{three questions}} about teachers understanding of teaching statistics {{as part of}} a study that addresses the relationship between the pedagogical knowledge of statistics in primary teachers in Chile and its impact on students learning. This work is part of a study that looks for evidences of the association between teacher Pedagogical Content Knowledge and the effectiveness in teaching statistics that is focuses in 4 th and 7 th grade. Thirty-one teachers from different schools of Valparaíso district were tested after reading a short text referred to non typical values and properties mean. They answered the questions showing a rigid understanding of statistic, as part of mathematics, like a formal discipline. This account evidences primary teachers’ privilege procedural approach than understanding problem solving approaches. BACKGROUND At present, in Chile, diverse presumptions exist in particular about the main variables that affect the efficiency of the statistic education. This ambiguity complicates the Educative Systems <b>that</b> <b>requires</b> <b>empirical</b> support to favor the consistency and the relevance of tools for the improvement of education and the teacher evaluation and promotion. Beyond the stable or in the long term changeable variables like socio-economical level o...|$|R
40|$|Introduction: There {{is limited}} {{research}} addressing he experiences {{of patients in}} inpatient rehabilitation (rehabilitation), who often spend long periods in hospital, and the nursing approaches utilised. Aim: Based on evidence that Motivational Interviewing (MI) may improve nursing practice, this was a pilot study evaluating the feasibility of training rehabilitation nurses in MI and measuring patient experience. Method: Nurses underwent training and supervision focusing on MI spirit. Quantitative and qualitative measures were taken pre-training, two months post-training and eight months post training. Expert-by-experience research assistants facilitated patients’ participation in the study. Results: This study showed that training rehabilitation nurses in MI was feasible and relevant to their work. Patients participated in interviews and focus groups with support and potential improvements <b>that</b> <b>require</b> further <b>empirical</b> investigation in patient experience were found following the MI training. Discussion: This pilot study establishes the feasibility of a larger study addressing efficacy. Tentative qualitative findings question whether interactions between nurses and patients are valued in rehabilitation and support MI as a promising skill-set for rehabilitation nurses...|$|R
40|$|ABSTRACT: The simple questionWhat is {{empirical}} success?turns {{out to have}} {{a surprisingly}} intricate answer. The paper begins with the point <b>that</b> <b>empirical</b> success cannot be equated with goodness-of-fit without making some kind of distinction between meritorious fit and fudged fit. The proposal <b>that</b> <b>empirical</b> success is adequately defined by Akaikes Information Criterion (AIC) is analyzed in this light. What is called cross-validated fit is proposed as a further improvement. But it still leaves something out. The final proposal is <b>that</b> <b>empirical</b> success has a hierarchical structure that commonly emerges from the agreement of independent measurements of theoretically postulated quantities...|$|R
40|$|The {{tremendous}} {{changes in}} world society {{during the past}} several decades raise a number of important questions <b>that</b> <b>require</b> new <b>empirical</b> evidence and theoretical explanation for studies of rationalization. World society theory treats rationalization as a cultural process. This theory argues that a rational world culture, which originated from western culture, has universal influence. Based on this idea, this thesis proposes the standardized social measurement as one of the main embodiments of the rational world culture. Thus, participation in global social survey infrastructure (GSSI) is used to measure the diffusion of scientific rationality in world society. Based on this measurement, a descriptive analysis on the global participation in GSSI and an event history analysis on the history of global participation are conducted to describe this process and provide insights into its causes. Results of descriptive analysis show the national participation in GSSI is extensive, but highly unbalance even throughout its expansion in recent years. Event history models show that countries with closer connections to world culture are at greater risk of joining the GSSI. Both results support the argument of world society theory. Rationalization in contemporary world is driven by the world culture through diffusion...|$|R
30|$|In our analyses, we use {{a priori}} EOP from C 04. This means <b>that</b> <b>empirical</b> CPO {{corrections}} are included in our analysis already.|$|R
50|$|The {{most common}} {{versions}} of philosophy of science accept <b>that</b> <b>empirical</b> measurements are always approximations—they do not perfectly represent {{what is being}} measured.|$|R
