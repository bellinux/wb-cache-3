12|56|Public
5000|$|<b>Record</b> <b>identifier</b> field—identifying {{the record}} and {{assigned}} by the organization that creates the record. The <b>record</b> <b>identifier</b> field has tag 001.|$|E
5000|$|Travelers who {{apply for}} redress through TRIP are {{assigned}} a <b>record</b> <b>identifier</b> called a [...] "Redress Control Number". Airline reservations systems allow passengers {{who have a}} Redress Control Number to enter it when making their reservation.|$|E
50|$|The {{operating}} system uses a four byte relative track and record (TTR) for some access methods and for others an eight-byte extent-bin-cylinder-track-record block address, or MBBCCHHR, Channel programs address DASD using a six byte seek address (BBCCHH) and a five byte <b>record</b> <b>identifier</b> (CCHHR).|$|E
50|$|Healthcare Identifiers Regulations 2010 {{regulates the}} collection, use and {{disclosure}} of healthcare <b>record</b> <b>identifiers</b> and information.|$|R
50|$|An archive {{requires}} stable <b>identifiers</b> for core <b>records,</b> {{but not for}} extensions. For {{any kind}} of shared data it is therefore necessary {{to have some sort}} of local <b>record</b> <b>identifiers.</b> It’s good practice to maintain - with the original data - identifiers that are stable over time and are not being reused after the record is deleted. If you can, please provide globally unique identifiers instead of local ones.|$|R
40|$|PJM) {{submitted}} an executed interconnection {{service agreement}} (ISA) entered into among PJM, Green Energy Partners/Stonewall LLC (Green Energy) and Virginia Electric and Power Company (DVP). 2 You state that this ISA is nonconforming {{because it contains}} changes in Schedule E of Attachment O of the PJM Tariff, which contains the explanation {{and support for the}} Monthly Charge. Pursuant to the authority delegated to the Director, Division of Electric Power Regulation – East, under 18 C. F. R. § 375. 307, your submittal is accepted for filing, effective May 8, 2014, as requested. 1 The tariff record filed in Docket No. ER 14 - 2116 - 000 is rejected as moot. In the future, PJM should use Associated Filing and <b>Record</b> <b>Identifiers</b> at the <b>record</b> level when amending a tariff record in a pending proceeding. ...|$|R
50|$|Identifiers.org URIs {{have been}} {{developed}} since 2011 as a resolvable version of the MIRIAM identifiers, developed since 2005, which were of a URN form, and not directly resolvable. Identifiers.org URIs are similar to PURLs, albeit providing alternative resolutions for collections with several instances. They are also similar to DOIs, but provide human readable collection names, and re-use the <b>record</b> <b>identifier</b> assigned by the data provider.|$|E
5000|$|The {{database}} {{is part of}} {{a system}} that includes topographic map names and bibliographic references. The names of books and historic maps that confirm the feature or place name are cited. Variant names, alternatives to official federal names for a feature, are also recorded. Each feature receives a permanent, unique feature <b>record</b> <b>identifier,</b> sometimes called the GNIS identifier. The database never removes an entry, [...] "except in cases of obvious duplication." ...|$|E
50|$|PFIF allows {{different}} repositories {{of missing}} person data to exchange and aggregate their records. Every record {{has a unique}} identifier, which indicates the domain name of the original repository where the record was created. The unique <b>record</b> <b>identifier</b> is preserved as the record is copied from one repository to another. For example, any repository that receives {{a copy of a}} given person can publish a note attached to that person, and even as the note and person are copied to other repositories, they remain traceable to their respective original sources.|$|E
50|$|The {{purpose of}} the TreeBASE project is to provide stable <b>records</b> and <b>identifiers</b> for these data, so that other workers can refer to their {{deposited}} data in their publication, and other scientists can locate the data {{and use them to}} verify the original research or to include or extend them in further analyses.|$|R
40|$|Previous {{estimates}} of the size and composition of the U. S. homeless population {{have been based on}} cross-sectional survey methodologies. National enumeration efforts have yielded point-prevalence estimates ranging from 0. 11 to 0. 25 percent of the population. This study reports data from shelter databases in Philadelphia and New York City that <b>record</b> <b>identifiers</b> for all persons admitted and so make possible unduplicated counts of users. Unduplicated counts of shelter users yield annual rates for 1992 of about 1 percent for both cities and rates near 3 percent over three years in Philadelphia (1990 – 92) and over five years (1988 – 92) in New York City. The annual rates are three times greater than rates documented by point-prevalence studies. Shelter bed turnover rates are reported, as are average monthly first admission and readmission counts over a two-yea...|$|R
50|$|In October 2009 MusicIP was {{acquired}} by AmpliFIND. Some time after the acquisition, the MusicDNS service began having intermittent problems. Since {{the future of the}} free identification service was uncertain, a replacement for it was sought. The Chromaprint acoustic fingerprinting algorithm, the basis for AcoustID identification service, was started in February 2010 by a long-time MusicBrainz contributor Lukáš Lalinský. While AcoustID and Chromaprint are not officially MusicBrainz projects, they are closely tied with each other and both are open source. Chromaprint works by analyzing the first two minutes of a track, detecting the strength in each of 12 pitch classes, storing these 8 times per second. Additional post-processing is then applied to compress this fingerprint while retaining patterns. The AcoustID search server then searches from the database of fingerprints by similarity and returns the AcoustID identifier along with MusicBrainz <b>recording</b> <b>identifiers</b> if known.|$|R
50|$|The Identifiers.org URIs are {{perennial}} identifiers, that specify at {{once the}} data collection, using the namespaces of the Registry, and the <b>record</b> <b>identifier</b> within the collection {{in the form of}} a unique resolvable URI. The Identifiers.org resolving system is built upon the information stored in the MIRIAM Registry, which is a database that stores namespaces assigned to commonly used data collections (databases and ontologies) for the Life Sciences. It transforms an Identifiers.org URI into the various URLs leading to the various instances of the record identified by the URI.|$|E
3000|$|Differential {{operation}} is employed to obtain differential updates between the previous {{content and the}} current content. During synchronization process, content will be handled by both Master subsystem and Slave subsystem. Particular content is transformed into synchronization table which (for {{the rest of this}} paper, will be referred to as synctable [...]). Synctable comprises number of records depending on the content size with every records containing (ID, content, hash) tuple information. ID is <b>record</b> <b>identifier,</b> content contains transformed records from corresponding Moodle tables, and finally hash contains MD 5 hashes of content.|$|E
40|$|Report Number: USGS-OFR- 83 - 430; OSTI ID: 5053222; Legacy ID: DE 84900998; ON: DE 84900998 GEOTHERM {{sample file}} {{contains}} 34 records for Hawaii. The high average ambient air temperature {{found on the}} Hawaiian Islands required fluid samples to have a temperature of at least 30 /sup 0 /C to be included. A computer-generated index is found in appendices A of this report. The index give one line summaries of each GEOTHERM record describing the chemistry of geothermal springs and wells in the sample file for Hawaii. The index is found in appendix A (p. is sorted by county and {{by the name of}} the source. Also given are well number (when appropriate), site type (spring, well, fumarole), latitude, longitude (both use decimal minutes), GEOTHERM <b>record</b> <b>identifier,</b> and temperature (/sup 0 /C). In conducting a search of Appendix A, site names are quite useful for locating springs or wells for which a specific name is commonly used, but sites which do not have specific names are more difficult to locate...|$|E
40|$|This manual is {{intended}} {{as a guide to}} Nuclear Science References (NSR) compilers. The basic conventions followed at the National Nuclear Data Center (NNDC), which are compatible with the maintenance and updating of and retrieval from the Nuclear Science References (NSR) file, are outlined. In Section H, the structure of the NSR file such as the valid <b>record</b> <b>identifiers,</b> <b>record</b> contents, text fields as well as the major TOPICS for which are prepared are enumerated. Relevant comments regarding a new entry into the NSR file, assignment of, generation of and linkage characteristics are also given in Section II. In Section III, a brief definition of the Keyword abstract is given followed by specific examples; for each TOPIC, the criteria for inclusion of an article as an entry into the NSR file as well as coding procedures are described. Authors preparing Keyword abstracts either to be published in a Journal (e. g., Nucl. Phys. A) or to be sent directly to NNDC (e. g., Phys. Rev. C) should follow the illustrations in Section III. The scope of the literature covered at the NNDC, the categorization into Primary and Secondary sources, etc., is discussed in Section IV. Useful information regarding permitted character sets, recommended abbreviations, etc., is given under Section V as Appendices...|$|R
40|$|This paper {{describes}} XDB-IPG, an {{open and}} extensible database architecture that supports efficient and flexible integration of heterogeneous and distributed information resources. XDB-IPG provides a novel “schema-less ” database approach using a document-centered object-relational XML database mapping. This enables structured, unstructured, and semi-structured information to be integrated without requiring document schemas or translation tables. XDB-IPG utilizes existing international protocol standards of the World Wide Web Consortium Architecture Domain and the Internet Engineering Task Force, primarily HTTP, XML and WebDAV. Through {{a combination of these}} international protocols, universal database <b>record</b> <b>identifiers,</b> and physical address data types, XDB-IPG enables an unlimited number of desktops and distributed information sources to be linked seamlessly and efficiently into an information grid. XDB-IPG has been used to create a powerful set of novel information management systems for a variety of scientific and engineering applications. Introduction – The Information Grid The Information Power Grid (IPG) is the National Aeronautics and Space Administration’s (NASA) project for providing seamless access to distributed information resources regardless of location [29]. The project addresses three major categories of distributed resources: 1) hardware resources, suc...|$|R
40|$|The {{accuracy}} {{of a general}} practice data system has been measured and has improved considerably over three years. It is difficult to identify all the factors contributing to this change, but an overall effort to {{emphasize the importance of}} <b>recording</b> patient <b>identifiers</b> correctly has been effective. No directly comparable error rates have been reported elsewhere; consequently, relative accuracy cannot be known. Administrators and managers of data systems are urged to determine levels of error and disseminate this information...|$|R
40|$|This paper {{discusses}} {{the adaptation of}} the Scene of Crime Information System developed within an EPSRC-funded project, to the collection of data within the ImageCLEF track of the Cross Language Evaluation Forum 2003. The adaptations necessary {{to participate in this}} activity are detailed, and initial results are briefly presented. 1. ImageCLEF Collection ImageCLEF is concerned with the retrieval of images from a specific collection by the captions associated to those images, and is running in relation to an EPSRC-funded project at Sheffield University (Eurovision, GR/R 56778 / 01). The image collection consists of around 28, 133 images from the photographic collection provided by St Andrews University Library (Clough et al. 2003). The 28133 images are each referred to and annotated by a single text file, and the full set of annotations are contained within one SGML-based document 1. Each annotation comprises identifiers to the text file and the image files (DOCNO, SMALL_IMG, LARGE_IMG), the caption of the image (HEADLINE), a set of categories that have been assigned to this image (CATEGORIES), a database <b>record</b> <b>identifier</b> (RECORD_ID) and an unlabelled chunk of text describing the image, denoted below in italics...|$|E
40|$|The central {{concept in}} species {{identification}} by molecular methods is {{to match the}} DNA sequence of unknown sample to a reference sequence through DNA sequence similarity searches. The limitation {{of this process is}} the lack of authenticated reference sequence databases. For instance, in the GenBank, the biggest database available, the sequences are accepted without any standard protocols or quality control, raising doubts over their suitability for food inspection application. Barcoding may be a tool for species identification because its aim is the production of COI (Cytochrome Oxidase subunit I) reference sequences using standard protocols. Every specimen record will not gain formal barcode status until seven data elements are placed in: species name, voucher data, collector <b>record,</b> <b>identifier</b> of specimen, COI sequence of at least 500 bp, polymerase chain reaction primers used to generate the amplicon and trace files. BOLD (Barcode of Life Data System) employs several tools to identify data anomalies or low quality records. Recently this system has been adopted in Italy too to verify the compliance of prepared fishery products with species declared in label. The authors introduce a discussion about a possible simplification of the Italian official list of seafood. especially for species of the same commercial value...|$|E
40|$|Updated: 25 October 2017 This Excel {{data file}} (compatible with Excel 2007 and later versions) is an extract of my working Greek New Testament {{database}} which I use for statistical and data analysis. It {{originated in the}} early 20002 ̆ 7 s from UBS 3 data files in beta code obtained from CCAT, and has since been evolving through countless changes and corrections. A flat-file table display such as Excel 2007 + is the best format suitable for Autofilter and VBA applications, without involving a more complex XML format. The file itself may be opened with Excel 2007 or later versions, or with the freeware spreadsheet packages OpenOffice Calc 3. 3 or later, or LibreOffice Calc 3. 3. 3 or later. Values for the individual Greek words are presented in a stripped Latin character transliteration, see 2 ̆ 2 FullWord 2 ̆ 2 entry below. Each record in the database contains the following fields:ID - numeric <b>record</b> <b>identifier,</b> 1 through 138, 019 Ref - reference address in format 00. ABC_ 11 : 22. 33, where 00 is the book index, ABC is {{the two or three}} letter book abbreviation, 11 is the chapter number, 22 the verse number, and 33 the word number; all numeric segments less than 10 are padded with a leading zeroFullWord - full word stripped of all diacritics/accents and in compact SBL style transliteration, to prevent machine reverse engineering back to copyrighted textLemma - dictionary form of the word in Greek unicode (UTF- 8) charactersCap - numeric indicator of capitalization, 2 for both lemma and word, 1 for word only (UBS 3 criteria) TC - text critical indicator, 1 for single brackets, 2 for double brackets, 3 for single brackets within double bracketed textIPQ - three character indicator for Indent-Punctuation-Quotation; the first two positions are largely unused, but a number in the third position indicates the word is quoted from Old Testament text, based on UBS criteriaPoS - one or two character indicator for part of speech: Adjective, Conjunction, aDverb, Interjection, Noun, Preposition, pRonoun-Articular, pRonoun-Demonstrative, pRonoun-indeFinite, pRonoun-Interrogative, pRonoun-Personal/Possessive, pRonoun-Relative, pRronoun-Xreciprocal, Verb, X-particlePerson - numeric indicator 1, 2, or 3 for first, second or third personTense - single character indicator for tense of verb forms: Aorist, Future, Imperfect, Present, X-perfect, Y-pluperfectVoice - single character indicator for voice of verb forms: Active, Middle, PassiveMood - single character indicator for mood of verb forms: D-imperative, Indicative, iNfinitive, Optative, Participle, SubjunctiveCase - single character indicator for case endings: Accusative, Dative, Genitive, Nominative, VocativeNumber - single character for number: Singular, PluralGender - single character for gender: Masculine, Feminine, NeuterExtra - miscellaneous one or two character indicator: Comparative, Superlative, Historical Present (HP indicators are complete only for the gospels of Mark, Luke, and John), and Alpha-Privative; multiple indicators may be present for a single record, separated by a commaSylls - numeric syllable count of the full word; initial iota followed by another vowel is always counted as a separate syllableChars - numeric (Greek-) character count of the full word, which may differ from character count of the transliterated form displayed in the FullWord field [...] Transliteration scheme (after SBL) for FullWord field:α=a, β=b, γ=g or γ=n before γ/κ/ξ/χ, δ=d, ε=e, ζ=z, η=ē, θ=th, ι=i, κ=k, λ=l, μ=m, ν=n, ξ=x, ο=o, π=p, ρ=r, σ/ς=s, τ=t, υ=u in diphthongs αυ/ευ/ου/υι otherwise υ=y, φ=ph, χ=ch, ψ=ps, ω=ōRough breathing on initial vowel or diphthong = initial 2 ̆ 2 h 2 ̆ 2 at beginning of word, except for initial ῥ=rh; medial ρρ = rrh...|$|E
2500|$|... "Philosophy of Science: Observation and Interpretation", Voice of America {{broadcast}} from 1964 (<b>recorded</b> 1963), ARC <b>Identifier</b> 106673, Local <b>Identifier</b> 306-FORUM-EN-L-T-6456-I, <b>Record</b> Group 306: Records of the U.S. Information Agency, 1900 - 1992 (cited at , retrieved by searching for [...] "philosophy science observation interpretation") ...|$|R
50|$|In {{programming}} languages {{the association}} of identifiers to a value is termed a definition. Program text is structured using block constructs and definitions can be local to a block. The localized association of an identifier to a value established by a definition is termed a binding and the region of program text in which a binding is effective is known as its scope.The computational state is kept using two components: the environment, used as a <b>record</b> of <b>identifier</b> bindings, and the store, which {{is used as a}} record of the effects of assignments.|$|R
40|$|The CRM and ERP {{systems are}} usually {{obtained}} and implemented separately and at different times. Quite {{often they are}} purchased from different sellers and producers, and implemented by different teams. As a rule, the CRM and ERP systems contain separate databases even if {{they come from the}} same manufacturer. Such separately kept databases also lead to separate basic <b>records</b> (<b>identifiers),</b> which primarily relate to business partners, items and services. This may create problems with updating and maintaining consistency of the data within the information system of a company. The CRM and ERP systems usually overlap in certain segments of business processes (e. g., orders, order confirmations, quotations, etc.), thus potentially creating redundant information and documents. More often than not, the CRM and ERP also differ in terms of technology, both by their vertical architectures and with regard to the DBMS and API support. The objective {{of this paper is to}} generate an ERP-CRM integration data model by way of optimising the relevant processes and costs, and to provide details about the processes of integration of the logical and physical data models. The structural integration of the ERP-CRM databases provide the integration services that ensure all the necessary functionalities in various interface logics and technologies with regard to software solutions and applications given, or used for local adaptations of the existing ERP and CRM applications...|$|R
40|$|NVocD is a dataset of the {{identifier}} name declarations {{and vocabulary}} found in 60 FLOSS Java projects where the source code structure is <b>recorded</b> and the <b>identifier</b> name vocabulary is made directly available, offering advantages for identifier name research over conventional source code models. The dataset {{has been used}} to support a range of research projects from identifier name analysis to concept location, and provides many opportunities to researchers...|$|R
40|$|Integration of {{disparate}} information from electronic health records, clinical data warehouses, birth certificate registries {{and other public}} health information systems offers great potential for clinical care, public health practice, and research. Such integration, however, depends on correctly matching patient-specific <b>records</b> using demographic <b>identifiers.</b> Without standards for these <b>identifiers,</b> <b>record</b> linkage is complicated by issues of structural and semantic heterogeneity. Objectives: Our objectives were to develop and validate an ontology to: 1) identify components of identity and events subsequent to birth that result in creation, change, or sharing of identity information; 2) develop an ontology to facilitate data integration from multiple healthcare and public health sources; and 3) validate the ontology’s ability to model identity-changing events over time. Methods: We interviewed domain experts in area hospitals and public health programs and developed process models describing the creation and transmission of identity information among various organizations for activities subsequent to a birth event. We searched for existing relevant ontologies. We validated the content of our ontology with simulated identity information conforming to scenarios identified in our process models...|$|R
50|$|The field {{record type}} is an {{abbreviation}} {{for the type}} of information stored in the last field, record data. The type also provides the name of each record. For example, an address <b>record,</b> having the <b>identifier</b> A for IPv4 and AAAA for IPv6, maps the domain name from the first field to an IP address in the record data, and a mail exchanger record (MX) specifies the Simple Mail Transfer Protocol (SMTP) mail host for a domain.|$|R
40|$|Record linkage is the {{procedure}} of bringing together information from {{two or more}} records that are believed {{to belong to the}} same entity. The linking of a pair of <b>records</b> without <b>identifier</b> should be based on attributes both records have in common. The probabilistic framework described by Fellegi and Sunter (1969) can be used for record linkage. This record linkage framework classifies pairs of records as links, non-links or possible links based on a comparison of the attributes found in both records. In this thesis, the framework is studied and methods to estimate the parameters of the framework. The framework is applied to link privacy preserved police and hospital road accident records. The linkage of these data sources is of great interest in the field of road safety research. Applied mathematicsElectrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|The goal of {{this project}} is a service based {{solution}} that utilizes parallel and distributed processing algorithms to solve the transitive closure problem for a large dataset. A dataset may be view conceptually as a table in a database, with a physical structure representing a file containing a sequence of records and fields. Two records {{are said to be}} transitively related if and only if they are directly related due to sharing of one or more specific fields, or a sequence may be made from one record to the other under the condition that all intermediate entries are related the immediate previous and subsequent entry. The transitive closure problem is to cluster the records in a dataset into groups such that all transitively related records are in one group. An approach to solve this problem is to divide the task into two separate problems. The first of these problems is the processing of the dataset, and thus generating a set of pairs. Each of these pairs would include two <b>record</b> <b>identifiers,</b> and these pairs would exist if and only if these two records were directly related. The second of these problems is to use the record pairs to cluster the records into transitive closures. The current software solution solves this second sub problem through the reading of record pairs, produced by a different software solution, and writes the completed results of the transitive closure problem to a file. This thesis studies how to enhance the current software solution {{in such a way that}} it becomes a 2 ̆ 2 service 2 ̆ 2. The study includes designing, implementing, testing, and evaluating the enhanced solution. The service model identifies an aspect that would potentially benefit from restructuring or addition of functionality. A current issue is the lack of an ability to fetch transitive closure from within the solution upon the completion of a job, and is thus limited in itsdirect use with other processes or applications...|$|R
30|$|Genes {{identified}} by computational methods alone {{may prove to}} be false positives when confirmed by experimental evidence, thus making it necessary to retire the locus. In such cases, all the <b>records</b> and corresponding <b>identifiers</b> should be preserved with a flag OBSOLETE and never DELETED from data repositories. The flag OBSOLETE ensures that the same identifiers are not used again for a new locus, thus avoiding a situation that would lead to confusion and, if required, makes it possible for an obsolete gene to still be referenced.|$|R
40|$|A {{recent and}} {{original}} class of routing algorithms in large and mobile ad hoc networks is the Age and Position Based (APB) approach. A routing algorithm is APB if nodes base {{the decision of}} the next-hop node on: (1) an estimation of the destination’s position and, (2) the current node’s coordinates. In APB algorithms, contrary to traditional approaches where the data transmission phase comes after the location phase, route discovery is performed during packet forwarding. Each node maintains a local database that <b>records</b> the <b>identifier</b> and an estimation of the location of other nodes in the topology. Each location is associated with an age, which gives the time elapsed since the last time it has been updated. The age gives the accuracy of the estimation. Clearly, the lower the age, the better the estimation of the node’s location. The path followed by a packet toward a destination is the concatenation of jump...|$|R
40|$|Pointer Swizzling {{has been}} {{recognized}} as an efficient persistent storage concept for object stores. Here, we show how the classical <b>Record</b> (Tuple) <b>Identifiers</b> (RID, TID) scheme can be combined with run-time pointer swizzling to avoid copying of persistent objects within main memory. The paper also points out advantages, disadvantages, and open problems of this new approach. The details are explained {{in the context of}} the complex object database ESCHER which puts emphasis on the visual interface requiring fast navigation in the object tree. Keywords Persistent storage, pointer swizzling, complex objects 1 Introduction ESCHER is a complex object database for the eNF 2 data model [5], an extension of the classical nonfirst normal-form data model [13, 18]. ESCHER became operational in 1989 as a database editor [11, 23, 25] featuring a tabular representation of hierarchical objects. Interaction is achieved by means of so-called fingers which is a generalization of the well-known cursor [...] ...|$|R
40|$|This manual is {{intended}} {{as a guide for}} Nuclear Science References (NSR) compilers. The basic conventions followed at the National Nuclear Data Center (NNDC), which are compatible with the maintenance and updating of and retrieval from the Nuclear Science References (NSR) file, are outlined. The NSR database originated at the Nuclear Data Project (NDP) at Oak Ridge National Laboratory as part of a project for systematic evaluation of nuclear structure data. 1 Each entry in this computer file corresponds to a bibliographic reference that is uniquely identified by a Keynumber and is describable by a Topic and Keywords. It has been used since 1969 to produce bibliographic citations for evaluations published in Nuclear Data Sheets. Periodic additions to the file were published as the ''Recent References'' issues of Nuclear Data Sheets prior to 2005. In October 1980, the maintenance and updating of the NSR file became the responsibility of the NNDC at Brookhaven National Laboratory. The basic structure and contents of the NSR file remained unchanged during the transfer. In Chapter 2, the elements of the NSR file such as the valid <b>record</b> <b>identifiers,</b> <b>record</b> contents, and text fields are enumerated. Relevant comments regarding a new entry into the NSR file and assignment of a keynumber are also given in Chapter 2. In Chapter 3, the format for keyword abstracts is given followed by specific examples; for each TOPIC, the criteria for inclusion of an article as an entry into the NSR file as well as coding procedures are described. Authors preparing Keyword abstracts either to be published in a Journal (e. g., Nucl. Phys. A) or to be sent directly to NNDC (e. g., Phys. Rev. C) should follow the illustrations in Chapter 3. The scope of 1 See W. B. Ewbank, ORNL- 5397 (1978). the literature covered at the NNDC, the categorization into Primary and Secondary sources, etc., is discussed in Chapter 4. Useful information regarding permitted character sets, recommended abbreviations, etc., is given in the Appendices. The NSR database has been in existence for decades, and responsibility for its upkeep has passed through many hands. Those familiar with the contents of NSR will note that not all of the formats and conventions discussed in this manual have always been adhered to. In recent years, however, these conventions have been followed fairly consistently, and it is expected that the preparation of new entries will follow these guidelines. The most up-to-date information about NSR contents and policies {{can be found at the}} NSR web site: [URL] This manual is an update to BNL-NCS- 51800 (Rev. 08 / 96) by S. Ramavataram and C. L. Dunford. Discussions with Mark Kellett of the IAEA are gratefully acknowledged, as are comments and suggestions from the NNDC staff and members of the U. S. Nuclear Data Program. This manuscript has been authored by Brookhaven Science Associates, LLC under Contract No. DE-AC 02 - 98 CH 1 - 886 with the U. S. Department of Energy...|$|R
40|$|The European Nucleotide Archive (ENA; &#x 22;[URL] is a {{comprehensive}} repository for public nucleotide sequence data from nearly four hundred thousand taxonomic nodes. Together with partners in the International Nucleotide Sequence Database Collaboration (INSDC; EBI, NCBI and DDBJ) we provide {{a broad spectrum of}} sequences, from raw reads (Sequence Read Archive data class), assembled contigs (Whole Genome Shotgun data class), assemblies of EST transcripts (Transcriptome Shotgun Assembly data set), to partial or complete assembled nucleic acid molecules with functional annotation derived from direct and third party experimental evidence (Standard and TPA data classes, respectively). Resources beyond ENA, such as RNA and protein databases, genome collections and model organism services, use data stored and presented at ENA as both source and underlying supporting evidence for their records. Integration of the growing wealth of molecular information is a great challenge that brings opportunities for ENA to serve as a bioinformatics data information hub, allowing, through its provision of permanent identifiers for sequence and project <b>records,</b> community-recognized <b>identifiers</b> for navigation across databases. |$|R
30|$|Cryptographic {{keys and}} {{anonymous}} transactions make blockchain vulnerable to account takeover and digital identity theft because an identity is protected only by its private key. Loss {{of a key}} means loss of identity on the network. One solution is to build an identity and reputation system using a blockchain that records “fingerprint” events (Baxter 2016; Nordseth 2016). Instead of <b>recording</b> the personal <b>identifiers</b> customarily used offline (e.g., social security number, birth certificate, passport), the blockchain may track life events (e.g., births, schooling, acquiring student loans, opening bank accounts, buying cars, or purchasing homes). These events recorded in the irreversible identity blockchain become a digital identity {{that is difficult to}} steal because it is unforgeable, time-stamped, and publically monitored.|$|R
40|$|Little {{is known}} about the role played by {{emergency}} departments in cases of partner violence. To find out what happens and what works, for whom and in what circumstances, this research employs a mixed-method case series approach to explore pathways through emergency care. One element of the study is a retrospective medical record case review, which will be used to compare health service management of emergency department contacts arising from partner violence. However, conducting partner violence research in health systems in England is methodologically challenging. This poster focuses on the problems of sample design to ‘find cases’ for a medical record case review of a largely ‘hidden’ population. Medical <b>record</b> case <b>identifiers</b> or clinical codes for partner violence are not normally applied in emergency department health systems. The poster illustrates the clustered, multi-phase, random sample design which has been developed for the retrospective medical record review of cases of partner violence in emergency departments in Lancashire. There are, however, limitations in the sample design that stem from health data originating from individual pathology (physical injury from assault) and incidence (episodes of pathology), which collectively may obscure prevalence and wider health consequences of partner violence. Furthermore, this may perpetuate bias towards particular characteristics of ‘cases’ in health, limiting the view to extreme partner violence ‘types’...|$|R
