10000|9709|Public
5|$|A {{stochastic}} {{process can be}} classified in different ways, for example, by its state space, its index set, or the dependence among the <b>random</b> <b>variables.</b> One common way of classification is by the cardinality of the index set and the state space.|$|E
5|$|Convolution {{is used to}} add two {{independent}} <b>random</b> <b>variables</b> defined by distribution functions. Its usual definition combines integration, subtraction, and multiplication. In general, convolution is useful {{as a kind of}} domain-side addition; by contrast, vector addition is a kind of range-side addition.|$|E
5|$|A {{stochastic}} or {{random process}} {{can be defined as}} a collection of <b>random</b> <b>variables</b> that is indexed by some mathematical set, meaning that each random variable of the stochastic process is uniquely associated with an element in the set. The set used to index the <b>random</b> <b>variables</b> is called the index set. Historically, the index set was some subset of the real line, such as the natural numbers, giving the index set the interpretation of time. Each random variable in the collection takes values from the same mathematical space known as the state space. This state space can be, for example, the integers, the real line or -dimensional Euclidean space. An increment is the amount that a stochastic process changes between two index values, often interpreted as two points in time. A stochastic process can have many outcomes, due to its randomness, and a single outcome of a stochastic process is called, among other names, a sample function or realization.|$|E
5000|$|If X1 is gamma (α1, 1) <b>random</b> <b>variable</b> and X2 is a gamma (α2, 1) <b>random</b> <b>variable</b> then X1/(X1 + X2) is a beta(α1, α2) <b>random</b> <b>variable.</b> More generally, if X1is gamma(α1, β1) <b>random</b> <b>variable</b> and X2 is gamma(α2, β2) <b>random</b> <b>variable</b> then β2 X1/(β2 X1 + β1 X2) is a beta(α1, α2) <b>random</b> <b>variable.</b>|$|R
5000|$|If X is a {{standard}} normal <b>random</b> <b>variable</b> and U is a chi-squared <b>random</b> <b>variable</b> with ν degrees of freedom, then [...] is a Students t (ν) <b>random</b> <b>variable.</b>|$|R
30|$|Uncertain <b>random</b> <b>variable</b> {{and chance}} theory were {{introduced}} to model the uncertain random compound systems. As the same as <b>random</b> <b>variable</b> and uncertain variable, expected value is the average of uncertain <b>random</b> <b>variable</b> {{in the sense of}} chance measure and the variance of uncertain <b>random</b> <b>variable</b> provides a degree of spread of the distribution around its expected value. In this paper, a series of formulas were built to obtain the variance of uncertain <b>random</b> <b>variable.</b>|$|R
5|$|Although less used, the {{separability}} {{assumption is}} considered more general because every stochastic process has a separable version. It {{is also used}} {{when it is not}} possible to construct a stochastic process in a Skorokhod space. For example, separability is assumed when constructing and studying random fields, where the collection of <b>random</b> <b>variables</b> is now indexed by sets other than the real line such as -dimensional Euclidean space.|$|E
5|$|Logarithms {{also occur}} in {{log-normal}} distributions. When the logarithm of a random variable has a normal distribution, the variable {{is said to}} have a log-normal distribution. Log-normal distributions are encountered in many fields, wherever a variable is formed as the product of many independent positive <b>random</b> <b>variables,</b> for example in the study of turbulence.|$|E
25|$|This is {{the notion}} of pointwise {{convergence}} of sequence of functions extended to sequence of <b>random</b> <b>variables.</b> (Note that <b>random</b> <b>variables</b> themselves are functions).|$|E
30|$|A <b>random</b> <b>variable</b> X {{is called}} a (centered) Gaussian mixture if there exists a {{positive}} <b>random</b> <b>variable</b> V and a standard Gaussian <b>random</b> <b>variable</b> Z, independent of V, such that X=dVZ.|$|R
5000|$|If [...] is an -valued <b>random</b> <b>variable</b> whose {{probability}} distribution on [...] is a tight measure then [...] {{is said to}} be a separable <b>random</b> <b>variable</b> or a Radon <b>random</b> <b>variable.</b>|$|R
3000|$|A <b>random</b> <b>variable</b> {{that can}} assume only a finite or countably {{infinite}} number of values {{is known as a}} discrete <b>random</b> <b>variable,</b> while a variable which can assume each and every value within some interval is called a continuous <b>random</b> <b>variable.</b> The distribution function of a <b>random</b> <b>variable</b> X, denoted by [...]...|$|R
25|$|The normal {{distribution}} {{is an important}} example where the inverse transform method is not efficient. However, there is an exact method, the Box–Muller transformation, which uses the inverse transform to convert two independent uniform <b>random</b> <b>variables</b> into two independent normally distributed <b>random</b> <b>variables.</b>|$|E
25|$|Much as {{the central}} limit theorem {{requires}} certain kinds of <b>random</b> <b>variables</b> to have as a focus of convergence the Gaussian distribution and express white noise, the Tweedie convergence theorem requires certain non-Gaussian <b>random</b> <b>variables</b> to express 1/f noise and fluctuation scaling.|$|E
25|$|The Randomized Dependence Coefficient is a {{computationally}} efficient, copula-based {{measure of}} dependence between multivariate <b>random</b> <b>variables.</b> RDC is invariant {{with respect to}} non-linear scalings of <b>random</b> <b>variables,</b> is capable of discovering {{a wide range of}} functional association patterns and takes value zero at independence.|$|E
50|$|If X1 is {{a normal}} (μ1, σ) <b>random</b> <b>variable</b> and X2 {{is a normal}} (μ2, σ) <b>random</b> <b>variable,</b> then X1 + X2 is a normal (μ1 + μ2, σ + σ) <b>random</b> <b>variable.</b>|$|R
2500|$|Function of <b>random</b> <b>variable,</b> {{distribution}} of {{a function of}} a <b>random</b> <b>variable</b> ...|$|R
5000|$|A {{reciprocal}} <b>random</b> <b>variable</b> is the exponential of {{a uniform}} <b>random</b> <b>variable.</b>|$|R
25|$|In {{probability}} theory, {{the continuous}} mapping theorem states that continuous functions are limit-preserving {{even if their}} arguments are sequences of <b>random</b> <b>variables.</b> A continuous function, in Heine’s definition, is such a function that maps convergent sequences into convergent sequences: if xn → x then g(xn) → g(x). The continuous mapping theorem states that this will also be true if we replace the deterministic sequence {xn} with a sequence of <b>random</b> <b>variables</b> {Xn}, and replace the standard notion of convergence of real numbers “→” {{with one of the}} types of convergence of <b>random</b> <b>variables.</b>|$|E
25|$|Rothschild {{published}} an autobiography, <b>Random</b> <b>Variables</b> in 1984.|$|E
25|$|Concentration {{inequality}} – {{a summary}} of tail-bounds on <b>random</b> <b>variables.</b>|$|E
5000|$|A {{rectangular}} <b>random</b> <b>variable</b> is {{the floor}} of a uniform <b>random</b> <b>variable.</b>|$|R
5000|$|A {{geometric}} <b>random</b> <b>variable</b> is {{the floor}} of an exponential <b>random</b> <b>variable.</b>|$|R
5000|$|Function of <b>random</b> <b>variable,</b> {{distribution}} of {{a function of}} a <b>random</b> <b>variable</b> ...|$|R
25|$|The {{concept of}} almost sure {{convergence}} {{does not come from}} a topology on the space of <b>random</b> <b>variables.</b> This means there is no topology on the space of <b>random</b> <b>variables</b> such that the almost surely convergent sequences are exactly the converging sequences with respect to that topology. In particular, there is no metric of {{almost sure convergence}}.|$|E
25|$|Categorical distribution: for {{discrete}} <b>random</b> <b>variables</b> with {{a finite}} set of values.|$|E
25|$|The {{reason this}} gives a stable {{distribution}} {{is that the}} characteristic function for the sum of two <b>random</b> <b>variables</b> equals {{the product of the}} two corresponding characteristic functions. Adding two <b>random</b> <b>variables</b> from a stable distribution gives something with the same values of α and β, but possibly different values of μ and c.|$|E
5000|$|... where Y is a {{logistic}} <b>random</b> <b>variable,</b> X is a half-logistic <b>random</b> <b>variable.</b>|$|R
5000|$|A Weibull (1, β) <b>random</b> <b>variable</b> is an {{exponential}} <b>random</b> <b>variable</b> with mean β.|$|R
5000|$|The {{conditional}} {{variance of}} a <b>random</b> <b>variable</b> Y given another <b>random</b> <b>variable</b> X is ...|$|R
25|$|Then Z0 and Z1 are {{independent}} <b>random</b> <b>variables</b> {{with a standard}} normal distribution.|$|E
25|$|Benford's law – Result of {{extension}} of CLT to product of <b>random</b> <b>variables.</b>|$|E
25|$|Remark 4. For multidimensional <b>random</b> <b>variables,</b> their {{expected}} value is defined per component, i.e.|$|E
5000|$|... κ(Xi : i ∈ B | Y) is a {{conditional}} cumulant {{given the}} value of the <b>random</b> <b>variable</b> Y. It is therefore a <b>random</b> <b>variable</b> in its own right - a function of the <b>random</b> <b>variable</b> Y.|$|R
3000|$|Lemma 1 {{gives the}} {{relationships}} between the <b>random</b> <b>variable</b> X and the <b>random</b> <b>variable</b> T. These relationships can be used to generate random samples from X by using T. For example, one can simulate the <b>random</b> <b>variable</b> X which follows the distribution of T - N{exponential} family in (2.7) by first simulating <b>random</b> <b>variable</b> T from the PDF f [...]...|$|R
2500|$|The second view is the {{probabilistic}} view: the <b>random</b> <b>variable</b> [...] {{depends upon}} the <b>random</b> <b>variable</b> , which depends upon , which {{depends upon the}} <b>random</b> <b>variable</b> [...] This view is most commonly encountered {{in the context of}} graphical models.|$|R
