4657|2077|Public
25|$|El-Manzalawy, Y., Dobbs, D., and Honavar, V. (2012). Predicting {{protective}} bacterial antigens using <b>random</b> <b>forest</b> classifiers.. ACM Conference on Bioinformatics and Computational Biology pp.426–433, 2012.|$|E
2500|$|In February 2011, Alfa Aesar {{released}} over 10,000 melting {{points of}} compounds from their catalog as open data. These data have been curated and are freely available for download. These data {{have been used}} to create a <b>random</b> <b>forest</b> model for melting point prediction which is now available as a free-to-use webservice. Highly curated and open melting point data are also available from Nature Precedings. High quality data mined from patents and also models developed with these data [...] were published by Tetko et al.|$|E
5000|$|As part {{of their}} construction, <b>random</b> <b>forest</b> {{predictors}} naturally lead to a dissimilarity measure between the observations. One can also define a <b>random</b> <b>forest</b> dissimilarity measure between unlabeled data: {{the idea is to}} construct a <b>random</b> <b>forest</b> predictor that distinguishes the “observed” data from suitably generated synthetic data.The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A <b>random</b> <b>forest</b> dissimilarity can be attractive because it handles mixed variable types well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The <b>random</b> <b>forest</b> dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the [...] "Addcl 1" [...] <b>random</b> <b>forest</b> dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The <b>random</b> <b>forest</b> dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.|$|E
40|$|<b>Random</b> <b>forests,</b> {{introduced}} by Leo Breiman in 2001, {{are a very}} effective statistical method. The complex mechanism of the method makes theoretical analysis difficult. Therefore, a simplified version of <b>random</b> <b>forests,</b> called purely <b>random</b> <b>forests,</b> which can be theoretically handled more easily, has been considered. In this paper we introduce a variant {{of this kind of}} <b>random</b> <b>forests,</b> that we call purely uniformly <b>random</b> <b>forests.</b> In the context of regression problems with a one-dimensional predictor space, we show that both random trees and <b>random</b> <b>forests</b> reach minimax rate of convergence. In addition, we prove that compared to <b>random</b> trees, <b>random</b> <b>forests</b> improve accuracy by reducing the estimator variance by a factor of three fourths...|$|R
50|$|In machine learning, kernel <b>random</b> <b>forests</b> {{establish}} {{the connection between}} <b>random</b> <b>forests</b> and kernel methods. By slightly modifying their definition, <b>random</b> <b>forests</b> can be rewritten as kernel methods, which are more interpretable and easier to analyze.|$|R
40|$|<b>Random</b> <b>Forests</b> is {{a useful}} data mining tool that is quite popular in finding {{variable}} importance. However, many people don’t {{make use of the}} <b>Random</b> <b>Forests</b> results in interactive graphs. Partly, this is because software packages that can do interactive graphs can’t handle large data sets and those that use <b>Random</b> <b>Forests</b> have large data sets or many variables. A new software package in R, known as iPlots eXtreme, that is still in development makes it simple to explore large data sets interactively. I have created a function, called irfplot (interactive <b>random</b> <b>forests</b> plot) that specifically uses <b>Random</b> <b>Forests</b> to produce interactive graphs that are more informative than using raw values. I will use the interactive <b>Random</b> <b>Forests</b> plot that I’ve created to explore the nutrition data set from the Cache County Memory Study...|$|R
5000|$|Bandit Forest algorithm: a <b>random</b> <b>forest</b> {{is built}} and {{analyzed}} w.r.t the <b>random</b> <b>forest</b> built knowing the joint distribution of contexts and rewards.|$|E
5000|$|Leo Breiman was {{the first}} person to notice the link between <b>random</b> <b>forest</b> and kernel methods. He pointed out that random forests which are grown using i.i.d random vectors in the tree {{construction}} are equivalent to a kernel acting on the true margin. Lin and Jeon [...] established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed <b>Random</b> <b>Forest</b> Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and <b>random</b> <b>forest.</b> He also gave explicit expressions for kernels based on centred <b>random</b> <b>forest</b> and uniform <b>random</b> <b>forest,</b> two simplified models of <b>random</b> <b>forest.</b> He named these two KeRFs by Centred KeRF and Uniform KeRF,and proved upper bounds on their rates of consistency.|$|E
50|$|In statistics, {{jackknife}} variance {{estimates for}} <b>random</b> <b>forest</b> {{are a way}} to estimate the variance in <b>random</b> <b>forest</b> models, in order to eliminate the bootstrap effects.|$|E
40|$|Abstract—We {{introduce}} and validate Spatiotemporal Relational <b>Random</b> <b>Forests,</b> {{which are}} <b>random</b> <b>forests</b> created with spatiotemporal relational probability trees. We {{build on the}} documented success of <b>random</b> <b>forests</b> by bringing spatiotemporal capabilities to the trees, enabling them to identify critical spatial, temporal, and spatiotemporal features in the data. We validate our results on simulated data and realworld convectively-induced turbulence data from a commercial airline flying in the continental United States. Keywords-Spatiotemporal data mining, Relational learning, <b>Random</b> <b>forests,</b> Turbulence I...|$|R
40|$|This {{paper is}} {{a comment on}} the survey paper by Biau and Scornet (2016) about <b>random</b> <b>forests.</b> We focus {{on the problem of}} {{quantifying}} the impact of each ingredient of <b>random</b> <b>forests</b> on their performance. We show that such a quantification is possible for a simple pure forest, leading to conclusions that could apply more generally. Then, we consider "hold-out" <b>random</b> <b>forests,</b> which are a good middle point between "toy" pure forests and Breiman's original <b>random</b> <b>forests...</b>|$|R
40|$|This article {{addresses}} current methodological {{research on}} non-parametric <b>Random</b> <b>Forests.</b> It provides a brief intellectual history of <b>Random</b> <b>Forests</b> that covers CART, boosting and bagging methods. It then introduces the primary methods by which researchers can visualize results, {{the relationships between}} covariates and responses, and the out-of-bag test set error. In addition, the article considers current research on universal consistency and importance tests in <b>Random</b> <b>Forests.</b> Finally, several uses for <b>Random</b> <b>Forests</b> are discussed, and available software is identified...|$|R
5000|$|Tree growth {{step of the}} <b>random</b> <b>forest</b> machine {{learning}} technique.|$|E
5000|$|A <b>random</b> <b>forest</b> {{classifier}} is {{a specific}} type of bootstrap aggregating ...|$|E
5000|$|Regularized trees, e.g. regularized <b>random</b> <b>forest</b> {{implemented}} in the RRF package ...|$|E
40|$|International audienceRandom forests, {{introduced}} by Leo Breiman in 2001, {{are a very}} effective statistical method. The complex mechanism of the method makes theoretical analysis difficult. Therefore, simplified versions of <b>random</b> <b>forests,</b> called purely <b>random</b> <b>forests,</b> which can be theoretically handled more easily, have been considered. In this paper we study the variance of such forests. First, we show a general upper bound which emphasizes {{the fact that a}} forest reduces the variance. We then introduce a simple variant of purely <b>random</b> <b>forests,</b> that we call purely uniformly <b>random</b> <b>forests.</b> For this variant and in the context of regression problems with a one-dimensional predictor space, we show that both random trees and <b>random</b> <b>forests</b> reach minimax rate of convergence. In addition, we prove that compared to <b>random</b> trees, <b>random</b> <b>forests</b> improve accuracy by reducing the estimator variance by a factor of three fourths...|$|R
40|$|<b>Random</b> <b>forests</b> {{are known}} to be good for data mining of {{classification}} tasks, because <b>random</b> <b>forests</b> are robust for datasets having insufficient information possibly with some errors. But applying <b>random</b> <b>forests</b> blindly may not produce good results, and a dataset in the domain of rotogravure printing is one of such datasets. Hence, in this paper, some best classification accuracy based on clever application of <b>random</b> <b>forests</b> to predict the occurrence of cylinder bands in rotogravure printing is investigated. Since <b>random</b> <b>forests</b> could generate good results with an appropriate combination of parameters like the number of randomly selected attributes for each split and the number of trees in the forests, an effective data mining procedure considering the property of the target dataset by way of trial <b>random</b> <b>forests</b> is investigated. The effectiveness of the suggested procedure is shown by experiments with very good results...|$|R
40|$|Random Forests™ is {{reported}} {{to be one of the}} most accurate classification algorithms in complex data analysis. It shows excellent performance even when most predictors are noisy and the number of variables is much larger than the number of observations. In this thesis <b>Random</b> <b>Forests</b> was applied to a large-scale lung cancer case-control study. A novel way of automatically selecting prognostic factors was proposed. Also, synthetic positive control was used to validate <b>Random</b> <b>Forests</b> method. Throughout this study we showed that <b>Random</b> <b>Forests</b> can deal with large number of weak input variables without overfitting. It can account for non-additive interactions between these input variables. <b>Random</b> <b>Forests</b> can also be used for variable selection without being adversely affected by collinearities. ^ <b>Random</b> <b>Forests</b> can deal with the large-scale data sets without rigorous data preprocessing. It has robust variable importance ranking measure. Proposed is a novel variable selection method in context of <b>Random</b> <b>Forests</b> that uses the data noise level as the cut-off value to determine the subset of the important predictors. This new approach enhanced the ability of the <b>Random</b> <b>Forests</b> algorithm to automatically identify important predictors for complex data. The cut-off value can also be adjusted based on the results of the synthetic positive control experiments. ^ When the data set had high variables to observations ratio, <b>Random</b> <b>Forests</b> complemented the established logistic regression. This study suggested that <b>Random</b> <b>Forests</b> is recommended for such high dimensionality data. One can use <b>Random</b> <b>Forests</b> to select the important variables and then use logistic regression or <b>Random</b> <b>Forests</b> itself to estimate the effect size of the predictors and to classify new observations. ^ We also found that the mean decrease of accuracy is a more reliable variable ranking measurement than mean decrease of Gini. ...|$|R
5000|$|... #Subtitle level 3: Relation between {{infinite}} KeRF and infinite <b>random</b> <b>forest</b> ...|$|E
5000|$|... #Subtitle level 4: Survival <b>random</b> <b>forest</b> models {{using the}} randomForestSRC package ...|$|E
5000|$|An {{alternative}} {{to building a}} single survival tree is to build many survival trees, where each tree is constructed using {{a sample of the}} data, and average the trees to predict survival. This is the method underlying the survival <b>random</b> <b>forest</b> models. Survival <b>random</b> <b>forest</b> analysis is available in the R package [...] "randomForestSRC".|$|E
40|$|The final {{publication}} {{is available}} at Springer: [URL] audienceThis paper is a comment on the survey paper by Biau and Scornet (2016) about <b>random</b> <b>forests.</b> We focus {{on the problem of}} quantifying the impact of each ingredient of <b>random</b> <b>forests</b> on their performance. We show that such a quantification is possible for a simple pure forest, leading to conclusions that could apply more generally. Then, we consider " hold-out " <b>random</b> <b>forests,</b> which are a good middle point between " toy " pure forests and Breiman's original <b>random</b> <b>forests...</b>|$|R
40|$|<b>Random</b> <b>forests</b> are {{a type of}} {{ensemble}} method {{which makes}} predictions by combining the results of several independent trees. However, the theory of <b>random</b> <b>forests</b> has long been outpaced by their application. In this paper, we propose a novel <b>random</b> <b>forests</b> algorithm based on cooperative game theory. Banzhaf power index is employed to evaluate the power of each feature by traversing possible feature coalitions. Unlike the previously used information gain rate of information theory, which simply chooses the most informative feature, the Banzhaf power index {{can be considered as}} a metric of the importance of each feature on the dependency among a group of features. More importantly, we have proved the consistency of the proposed algorithm, named Banzhaf <b>random</b> <b>forests</b> (BRF). This theoretical analysis takes a step towards narrowing the gap between the theory and practice of <b>random</b> <b>forests</b> for classification problems. Experiments on several UCI benchmark data sets show that BRF is competitive with state-of-the-art classifiers and dramatically outperforms previous consistent <b>random</b> <b>forests.</b> Particularly, it is much more efficient than previous consistent <b>random</b> <b>forests.</b> Comment: arXiv admin note: text overlap with arXiv: 1302. 4853 by other author...|$|R
40|$|<b>Random</b> <b>forests</b> {{have proved}} to be very {{effective}} classifiers, which can achieve very high accuracies. Although a number of papers have discussed the use of fuzzy sets for coping with uncertain data in decision tree learning, fuzzy <b>random</b> <b>forests</b> have not been particularly investigated in the fuzzy community. In this paper, we first propose a simple method for generating fuzzy decision trees by creating fuzzy partitions for continuous variables during the learning phase. Then, we discuss how the method can be used for generating forests of fuzzy decision trees. Finally, we show how these fuzzy <b>random</b> <b>forests</b> achieve accuracies higher than two fuzzy rule-based classifiers recently proposed in the literature. Also, we highlight how fuzzy <b>random</b> <b>forests</b> are more tolerant to noise in datasets than classical crisp <b>random</b> <b>forests...</b>|$|R
50|$|Regularized trees {{naturally}} handle numerical and categorical features, {{interactions and}} nonlinearities. They are invariant to attribute scales (units) and insensitive to outliers, and thus, require little data preprocessing such as normalization. Regularized <b>random</b> <b>forest</b> (RRF) is {{one type of}} regularized trees. The guided RRF is an enhanced RRF which is guided by the importance scores from an ordinary <b>random</b> <b>forest.</b>|$|E
50|$|The randomForestSRC package {{includes}} an example survival <b>random</b> <b>forest</b> analysis using {{the data set}} pbc. This data is from the Mayo Clinic Primary Biliary Cirrhosis (PBC) trial of the liver conducted between 1974 and 1984. In the example, the <b>random</b> <b>forest</b> survival model gives more accurate predictions of survival than the Cox PH model. The prediction errors are estimated by bootstrap re-sampling.|$|E
5000|$|ALGLIB {{contains}} {{a modification of}} the <b>random</b> <b>forest</b> in C#, C++, Pascal, VBA. GPL 2+ ...|$|E
40|$|Discuss {{approaches}} to combine techniques used by ensemble learning methods. Randomness {{which is used}} by Bagging and <b>Random</b> <b>Forests</b> is introduced into Adaboost to get robust performance under noisy situation. Declare that when the randomness introduced into AdaBoost equals to 100, the proposed algorithm {{turns out to be}} a <b>Random</b> <b>Forests</b> with weight update technique. Approaches are discussed to improve the performance of <b>Random</b> <b>Forests</b> with weight update technique introduced...|$|R
40|$|Buffer Overflows are {{a common}} type of network {{intrusion}} attack that continue to plague the networked community. Unfortunately, this type of attack is not well detected with current data mining algorithms. This research investigated the use of <b>Random</b> <b>Forests,</b> an ensemble technique that creates multiple decision trees, and then votes for the best tree. The research Investigated <b>Random</b> <b>Forests</b> 2 ̆ 7 effectiveness in detecting buffer overflows compared to other data mining methods such as CART and Naïve Bayes. <b>Random</b> <b>Forests</b> was used for variable reduction, cost sensitive classification was applied, and each method 2 ̆ 7 s detection performance compared and reported along with the receive operator characteristics. The experiment was {{able to show that}} <b>Random</b> <b>Forests</b> outperformed CART and Naïve Bayes in classification performance. Using a technique to obtain Buffer Overflow most important variables, <b>Random</b> <b>Forests</b> was also able to improve upon its Buffer Overflow classification performance...|$|R
30|$|<b>Random</b> <b>forests</b> {{algorithm}} is trained on these ‘input features’ {{to predict the}} MCS, m_r,n^t, such that the sum-goodput is maximized. Essentially, we deploy the <b>random</b> <b>forests</b> as multi-class classifier, where the output variable m_r,n^t has multiple values, or classes.|$|R
5000|$|... 2. T. Jo, J. Cheng. Improving Protein Fold Recognition by <b>Random</b> <b>Forest.</b> BMC Bioinformatics. 15(S11):S14, 2014. paper ...|$|E
5000|$|Very often uses Machine Learning Algorithms, as K-Means, Naive Bayes Filter, C4.5, C5.0, J48, or <b>Random</b> <b>Forest</b> ...|$|E
5000|$|Thus <b>random</b> <b>forest</b> {{estimates}} satisfy, for all , [...] Random regression forest has two {{level of}} averaging, first over the samples {{in the target}} cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the <b>random</b> <b>forest</b> methods and compensate the misestimation, Scornet defined KeRF by ...|$|E
40|$|<b>Random</b> <b>forests</b> were {{introduced}} as a machine learning tool in Breiman (2001) and have since {{proven to be}} very popular and powerful for high-dimensional regression and classification. For regression, <b>random</b> <b>forests</b> give an accurate approximation of the conditional mean of a response variable. It is shown here that <b>random</b> <b>forests</b> provide information about the full conditional distribution of the response variable, {{not only about the}} conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of <b>random</b> <b>forests.</b> Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power...|$|R
40|$|International audienceThe <b>random</b> <b>forests</b> {{method is}} one of the most {{successful}} ensemble methods. However, <b>random</b> <b>forests</b> do not have high performance when dealing with very-high-dimensional data in presence of dependencies. In this case one can expect that there exist many combinations between the variables and unfortunately the usual <b>random</b> <b>forests</b> method does not effectively exploit this situation. We here investigate a new approach for supervised classification with a huge number of numerical attributes. We propose a random oblique decision trees method. It consists of randomly choosing a subset of predictive attributes and it uses SVM as a split function of these attributes. We compare, on 25 datasets, the effectiveness with classical measures (e. g. precision, recall, F 1 -measure and accuracy) of <b>random</b> <b>forests</b> of <b>random</b> oblique decision trees with SVMs and <b>random</b> <b>forests</b> of C 4. 5. Our proposal has significant better performance on very-high-dimensional datasets with slightly better results on lower dimensional datasets...|$|R
40|$|Description For tree {{ensembles}} such as <b>random</b> <b>forests,</b> regularized <b>random</b> <b>forests</b> and gradi-ent boosted trees, {{this package}} provides functions for: extracting, measuring and prun-ing rules; selecting a compact rule set; summarizing rules into a learner; calculating fre-quent variable interactions; formatting rules in latex code...|$|R
