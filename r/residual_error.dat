1313|1904|Public
25|$|Term I {{accounts}} for past {{values of the}} SP-PV error and integrates them over time to produce the I term. For example, {{if there is a}} residual SP-PV error after the application of proportional control, the integral term seeks to eliminate the <b>residual</b> <b>error</b> by adding a control effect due to the historic cumulative value of the error. When the error is eliminated, the integral term will cease to grow. This will result in the proportional effect diminishing as the error decreases, but this is compensated for by the growing integral effect.|$|E
500|$|... {{with the}} <b>residual</b> <b>error</b> being of order a−(1/6) + ε, where ε is infinitesimal. The {{constant}} C (Porter's Constant) in this formula equals ...|$|E
2500|$|Both {{ab initio}} and semi-empirical {{approaches}} involve approximations. [...] These range from simplified {{forms of the}} first-principles equations that are easier or faster to solve, to approximations limiting {{the size of the}} system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. [...] For example, most ab initio calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. [...] In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. [...] In practice, however, it is impossible to eliminate all approximations, and <b>residual</b> <b>error</b> inevitably remains. [...] The goal of computational chemistry is to minimize this <b>residual</b> <b>error</b> while keeping the calculations tractable.|$|E
30|$|After {{calculating the}} <b>residual</b> <b>errors</b> ɛ(t)′ {{of the model}} AR(13), the <b>residual</b> <b>errors</b> ɛ(t)′ needed to be {{verified}} whether they were the white noise. Finally, the test function of the white noise in MATLAB was used and the <b>residual</b> <b>errors</b> ɛ(t)′ of the model AR(13) were verified to be the white noise. Thus the <b>residual</b> <b>errors</b> ɛ(t)′ of the model AR(13) could replace the <b>residual</b> <b>errors</b> ɛ(t) of the model ARMA(6, 4).|$|R
30|$|In Eq. (8), the <b>residual</b> <b>errors</b> ɛ(t) were {{unknown in}} the model ARMA (6, 4), so the time series fitting <b>residual</b> <b>errors</b> δ (t)^' were not {{calculated}} by the linear derivation. However, if the model order w was large enough, the model AR(w) could approximately substitute the model ARMA(p, q). And the <b>residual</b> <b>errors</b> ɛ(t)′ of the model AR(w) could also replace the unknown <b>residual</b> <b>errors</b> ɛ(t) in the model ARMA (6, 4).|$|R
30|$|After the <b>residual</b> <b>errors</b> δ(t)′ of {{time series}} {{analysis}} in Eq. (12) were calculated, the <b>residual</b> <b>errors</b> δ(t)′ needed to be verified whether they were the white noise. Finally, the test function of the white noise in MATLAB was used to verify that the <b>residual</b> <b>errors</b> δ(t)′ of the {{time series analysis}} were the white noise. Thus, combining with the <b>residual</b> <b>errors</b> δ(t)′ of the time series analysis and the fitting value of the multiple linear model yM(t), the comprehensive compensation model yC(t) of the thermal errors of the wear-depth detecting system was obtained.|$|R
2500|$|Around 1900 low thermal {{expansion}} materials were developed which, when used as pendulum rods, made elaborate temperature compensation unnecessary. These were only used in {{a few of the}} highest precision clocks before the pendulum became obsolete as a time standard. In 1896 Charles Édouard Guillaume invented the nickel steel alloy Invar. This has a CTE of around 0.5 µin/(in·°F), resulting in pendulum temperature errors over 71°F of only 1.3 seconds per day, and this <b>residual</b> <b>error</b> could be compensated to zero with a few centimeters of aluminium under the pendulum bob (this {{can be seen in the}} Riefler clock image above). Invar pendulums were first used in 1898 in the Riefler regulator clock [...] which achieved accuracy of 15 milliseconds per day. Suspension springs of Elinvar were used to eliminate temperature variation of the spring's restoring force on the pendulum. Later fused quartz was used which had even lower CTE. These materials are the choice for modern high accuracy pendulums.|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between <b>residual</b> <b>error</b> and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
5000|$|Restriction - downsampling the <b>residual</b> <b>error</b> to a coarser grid.|$|E
5000|$|The <b>residuals</b> (<b>error</b> terms) {{should be}} {{normally}} distributed [...] ~ [...]|$|R
30|$|The {{white noise}} with the normal {{distribution}} characteristics is {{the premise of}} the time series model, that is, the multi-element fitting <b>residual</b> <b>errors</b> δ (t) should obey the normal distribution. The existing function of the normal distribution test in MATLAB was used to get that the multi-element fitting <b>residual</b> <b>errors</b> δ (t) obeyed the normal distribution.|$|R
30|$|The {{white noise}} {{verification}} of the <b>residual</b> <b>errors</b> ɛ(t)′ {{of the model}} AR(13).|$|R
50|$|The RPSM {{is used to}} seek a {{multiple}} fractional power series (MFPS) solution to fractional differential equations. This procedure {{can be achieved by}} substituting its MFPS expansion among its truncated <b>residual</b> <b>error</b> function. From the resulting equation a recursion formula for the computation of the coefficients is derived, while the coefficients in the MFPS expansion can be computed recursively by recurrent fractional differentiation of the truncated <b>residual</b> <b>error</b> function by means of the symbolic computation software used.|$|E
5000|$|... {{with the}} <b>residual</b> <b>error</b> being of order a−(1/6) + ε, where ε is infinitesimal. The {{constant}} C (Porter's Constant) in this formula equals ...|$|E
5000|$|... #Caption: Cross-polarization {{cancelling}} scheme involving equalization on {{the main}} path, XPIC filtering on the cross-polarized component and decision (slicing) with computation of <b>residual</b> <b>error</b> ...|$|E
30|$|The normal {{distribution}} {{testing of the}} multi-element fitting <b>residual</b> <b>errors</b> δ (t) is done as follows.|$|R
40|$|We {{will ask}} the {{question}} of whether or not the Regge calculus (and two related simplicial formulations) is a consistent approximation to General Relativity. Our criteria will be based on the behaviour of <b>residual</b> <b>errors</b> in the discrete equations when evaluated on solutions of the Einstein equations. We will show that for generic simplicial lattices the <b>residual</b> <b>errors</b> can not be used to distinguish metrics which are solutions of Einstein’s equations from those that are not. We will conclude that either the Regge calculus is an inconsistent approximation to General Relativity or that it is incorrect to use <b>residual</b> <b>errors</b> in the discrete equations as a criteria to judge the discrete equations. 1...|$|R
30|$|Finally, {{obtain the}} {{comparison}} score as the <b>residual</b> <b>errors</b> {{to compute the}} performance of the overall system.|$|R
5000|$|... if the {{function}} to minimize is {{for example the}} mean power the <b>residual</b> <b>error,</b> [...] , the adapting gradient algorithm prescribes that the coefficients are updated after every time step [...] as: ...|$|E
50|$|This {{process of}} adding terms {{continues}} untilthe change in <b>residual</b> <b>error</b> {{is too small}} to continueor until the maximum number of terms is reached.The maximum number of termsis specified by the user before model building starts.|$|E
50|$|In {{order to}} use Peirce's criterion, one must first {{understand}} the input and return values. Regression analysis (or the fitting of curves to data) results in residual errors (or {{the difference between}} the fitted curve and the observation points). Therefore, each observation point has a <b>residual</b> <b>error</b> associated with a fitted curve. By taking the square (i.e., <b>residual</b> <b>error</b> raised to the power of two), residual errors are expressed as positive values. If the squared error is too large (i.e., due to a poor observation) it can cause problems with the regression parameters (e.g., slope and intercept for a linear curve) retrieved from the curve fitting.|$|E
40|$|Estimation of {{parameter}} and predictive {{uncertainty of}} hydrologic models has traditionally relied on several simplifying assumptions. <b>Residual</b> <b>errors</b> are often {{assumed to be}} independent and to be adequately described by a Gaussian probability distribution {{with a mean of}} zero and a constant variance. Here we investigate to what extent estimates of parameter and predictive uncertainty are affected when these assumptions are relaxed. A formal generalized likelihood function is presented, which extends the applicability of previously used likelihood functions to situations where <b>residual</b> <b>errors</b> are correlated, heteroscedastic, and non?Gaussian with varying degrees of kurtosis and skewness. The approach focuses on a correct statistical description of the data and the total model residuals, without separating out various error sources. Application to Bayesian uncertainty analysis of a conceptual rainfall?runoff model simultaneously identifies the hydrologic model parameters and the appropriate statistical distribution of the <b>residual</b> <b>errors.</b> When applied to daily rainfall?runoff data from a humid basin we find that (1) <b>residual</b> <b>errors</b> are much better described by a heteroscedastic, first?order, auto?correlated error model with a Laplacian distribution function characterized by heavier tails than a Gaussian distribution; and (2) compared to a standard least?squares approach, proper representation of the statistical distribution of <b>residual</b> <b>errors</b> yields tighter predictive uncertainty bands and different parameter uncertainty estimates that are less sensitive to the particular time period used for inference. Application to daily rainfall?runoff data from a semiarid basin with more significant <b>residual</b> <b>errors</b> and systematic underprediction of peak flows shows that (1) multiplicative bias factors can be used to compensate for some of the largest errors and (2) a skewed error distribution yields improved estimates of predictive uncertainty in this semiarid basin with near?zero flows. We conclude that the presented methodology provides improved estimates of parameter and total prediction uncertainty and should be useful for handling complex <b>residual</b> <b>errors</b> in other hydrologic regression models as well. Water ManagementCivil Engineering and Geoscience...|$|R
40|$|Improved clock filter {{algorithm}} reduces network jitter Operating system kernel modifications achieve {{time resolution}} of 1 ns and frequency resolution of. 001 PPM using NTP and PPS sources. With kernel modifications, <b>residual</b> <b>errors</b> are reducec {{to less than}} 2 μs RMS with PPS source and less than 20 μs over a 100 -Mb LAN. New optional interleaved on-wire protocol minimizes errors due to output queueing latencies. With this protocol and hardware timestamps in the NIC, <b>residual</b> <b>errors</b> over a LAN {{can be reduced to}} the order of PPS signal. Using external oscillator or NIC oscillator as clock source, <b>residual</b> <b>errors</b> can be reduced to the order of IEEE 1588 PTP. Optional precision timing sources using GPS, LORAN-C and cesium clocks...|$|R
3000|$|... {{as close}} as {{possible}} to ρ. This can be realized by reducing the difference between the successive <b>residual</b> <b>errors</b> [...]...|$|R
50|$|Fig. 1. Counter-scanned {{images of}} porous alumina (AFM, 128×128 pixels): (a) direct and (b) counter {{images of the}} first pair; (c) direct and (d) counter images of the second pair. Drift induced error makes 25%. (e) Corrected image, <b>residual</b> <b>error</b> makes 0.1%.|$|E
50|$|ANOVA is {{considered}} to be a special case of linear regression which in turn is a special case of the general linear model. All consider the observations to be the sum of a model (fit) and a <b>residual</b> (<b>error)</b> to be minimized.|$|E
5000|$|The <b>residual</b> <b>error</b> is denoted en and {{is defined}} as en = xn &minus; sn (see the {{corresponding}} block diagram). The Wiener filter is designed so as to minimize the mean square error (MMSE criteria) which can be stated concisely as follows: ...|$|E
3000|$|... had a {{distribution}} of approximately χ^ 2 (Z-h_j^μ-p_j^μ), where Z {{is the first}} autocorrelation of the μth component model <b>residual</b> <b>errors,</b> r [...]...|$|R
3000|$|Step 2   Apply an {{optimization}} technique {{to determine the}} sub-optimal adjustable parameters of NNM {{in such a way}} that the <b>residual</b> <b>errors</b> are minimized.|$|R
30|$|The data {{stationarity}} is {{the basis}} of time series analysis, so the stationarity of the multi-element fitting <b>residual</b> <b>errors</b> δ (t) should be verified first.|$|R
50|$|It {{should also}} be noted that, due to {{limitations}} of precision during the numeric conversion between color models (notably if this conversion is not linear or uses non integer weights), additional roundoff errors may occur that should be taken into account into the <b>residual</b> <b>error.</b>|$|E
50|$|Both {{ab initio}} and semi-empirical {{approaches}} involve approximations. These range from simplified {{forms of the}} first-principles equations that are easier or faster to solve, to approximations limiting {{the size of the}} system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all. For example, most ab initio calculations make the Born-Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation. In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced. In practice, however, it is impossible to eliminate all approximations, and <b>residual</b> <b>error</b> inevitably remains. The goal of computational chemistry is to minimize this <b>residual</b> <b>error</b> while keeping the calculations tractable.|$|E
50|$|Mathematical {{formulas}} or models called algorithms may {{be applied}} to the data to identify relationships among the variables, such as correlation or causation. In general terms, models may be developed to evaluate a particular variable in the data based on other variable(s) in the data, with some <b>residual</b> <b>error</b> depending on model accuracy (i.e., Data = Model + Error).|$|E
3000|$|The three-parameter GRNN model, {{based on}} ([...] C_v, f_ 0,V_S 30), is very {{powerful}} to predict actual AF, with <b>residual</b> <b>errors</b> less than 15 [...]...|$|R
30|$|Pulse periods taking non-integer {{multiples}} {{values of}} τ_min then have intrinsic conflict with implementation in discretized time via digital control, {{giving rise to}} <b>residual</b> <b>errors.</b>|$|R
30|$|The <b>residual</b> <b>errors</b> of the μth {{component}} model were determined as {{the difference between}} the actual and predicted values for point k[*]+[*]q: a_j,k+q^μ=s_j,k+q^μ, actual-s_j,k+q^μ, predict.|$|R
