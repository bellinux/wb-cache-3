1|22|Public
40|$|Triglycidyl isocyanurate (TGIC) {{is mainly}} used in polyester-based powder paints, {{but also in}} laminates, {{insulating}} varnishes, coatings and adhesives. Several cases of contact allergy to TGIC have been reported during the last 10 years. Contact allergy to TGIC has developed in a factory producing the chemical, in a factory producing powder paints containing TGIC, and in industries using powder coating. In this paper, we report a man who developed a work-related dermatitis when working on the painting of metal frames. He was exposed to polyester powder pigments containing TGIC. When patch tested, he was negative to TGIC (prepared in 1988) 3 x and positive to polyester powder pigment. Only when a new test preparation of fresh TGIC powder was tested, was a positive reaction obtained. Chemical analyses showed {{that there was no}} TGIC in the test preparation from 1988, and that in the TGIC powder from 1988, there was only 30 % of the expected amount of TGIC. The investigations, clinical and chemical, strongly indicate degradation of TGIC in the test preparation and powder. Both substances and the test preparations made from them may change over time. Therefore, if a false-negative reaction due to a test preparation is strongly suspected, we recommend a re-test of the patient with a new test preparation of fresh material. As a general <b>rule,</b> <b>patch</b> testing should be performed with fresh substances and test preparations made from them, unless their stability and durability are known. status: publishe...|$|E
5000|$|... #Caption: A non-primary {{road sign}} near Bristol shows Guildford <b>Rules</b> <b>patches.</b>|$|R
5000|$|... #Caption: Black-on-white {{regional}} {{road sign}} in Irish and English, showing Guildford <b>Rules</b> <b>patching</b> for the N75 and the M8 ...|$|R
5000|$|According to article 13 of IBJJF <b>rules,</b> <b>patches</b> may {{be placed}} on the gi in one of {{thirteen}} different locations: ...|$|R
40|$|Thermal errors {{can have}} {{significant}} effects on CNC machine tool accuracy. The errors usually come from thermal deformations {{of the machine}} elements created by heat sources within the machine structure or from ambient change. The performance of a thermal error compensation system inherently depends on the accuracy and robustness of the thermal error model. In this paper, Adaptive Neuro Fuzzy Inference System (ANFIS), Artificial Neural Network (ANN) and Particle Swarm Optimization (PSO) techniques were employed to design four thermal prediction models: ANFIS by dividing the data space into <b>rule</b> <b>patches</b> (ANFIS-Scatter partition model); ANFIS by dividing the data space into rectangular sub-spaces (ANFIS-Grid partition model); ANN with a back-propagation algorithm (ANN-BP model) and ANN with a PSO algorithm (ANN-PSO model). Grey system theory was also used to obtain the influence ranking of the input sensors on the thermal drift of the machine structure. Four different models were designed, based on the higher-ranked sensors on thermal drift of the spindle. According to the results, the ANFIS models are superior {{in terms of the}} accuracy of their predictive ability; the results also show ANN-BP to have a relatively good level of accuracy. In all the models used in this study, the accuracy of the results produced by the ANFIS models was higher than that produced by the ANN models...|$|R
40|$|International audienceWe {{propose a}} new {{approach}} to construct selective and reduced integration rules for isogeometric analysis based on NURBS elements. The notion of an approximation space that approximates the target space is introduced. We explore the use of various approximation spaces associated with optimal patch-wise numerical quadratures that exactly integrate the polynomials in approximation spaces with the minimum number of quadrature points. <b>Patch</b> <b>rules</b> exploit the higher continuity of spline basis functions. The tendency of smooth spline functions to exhibit numerical locking in nearly-incompressible problems when using a full Gauss–Legendre quadrature is alleviated with selective or reduced integration. Stability and accuracy of the schemes are examined analyzing the discrete spectrum in a generalized eigenvalue problem. We propose a local algorithm, which is robust and computationally efficient, to compute element-by-element the quadrature points and weights in <b>patch</b> <b>rules.</b> The performance of the methods is assessed on several numerical examples in two-dimensional elasticity and Reissner–Mindlin shell structures...|$|R
30|$|However, {{encryption}} {{keys and}} login credentials can be breached, exposing the network {{to all kinds}} of threats. Furthermore, the prevention capabilities of firewalls and anti-viruses are limited by the prescribed set of <b>rules</b> and <b>patches.</b> Hence, it is imperative to include a second line of defense that can detect early symptoms of cyber-threats and react quickly enough before any damage is done. Such systems are commonly referred to as Intrusion Detection/Prevention Systems (IDS/IPS). IDSs monitor the network for signs of malicious activities and can be broadly classified into two categories—Misuse- and Anomaly-based systems. While the former rely on signatures of known attacks, the latter is based on the notion that intrusions exhibit a behavior that is quite distinctive from normal network behavior. Hence, the general objective of anomaly-based IDSs is to define the “normal behavior” in order to detect deviations from this norm.|$|R
40|$|International audienceRegion {{matching}} - finding conjugate regions {{on a pair}} {{of images}} - plays a fundamental role in computer vision. Indeed, such methods have numerous applications such as indexation, motion estimation or tracking. In the vast literature on the subject, several dissimilarity measures have been proposed {{in order to determine the}} true match for each region. In this paper, under statistical hypothesis of similarity, we provide an improved decision <b>rule</b> for <b>patch</b> matching based on significance tests and the statistical inequality of McDiarmid. The proposed decision rule allows to validate or not the similarity hypothesis and so to automatically detect matching outliers. The approach is applied to motion estimation and object tracking on noisy video sequences. Note that the proposed framework is robust against noise, avoids the use of statistical tests and may be related to the a contrario approach...|$|R
40|$|Isogeometric {{analysis}} {{based on}} NURBS (Non-Uniform Rational B-Splines) as basis functions preserves the exact geometry but {{suffers from the}} drawback of a rectangular grid of control points in the parameter space, which renders a purely local refinement impossible. This paper demonstrates how this difficulty can be overcome by using T-splines instead. T-splines allow the introduction of so-called T-junctions, which are related to hanging nodes in the standard FEM. Obeying a few straightforward <b>rules,</b> rectangular <b>patches</b> in the parameter space of the T-splines can be subdivided and thus a local refinement becomes feasible while still preserving the exact geometry. Furthermore, it is shown how state-of-the-art a posteriori error estimation techniques can be combined with refinement by T-Splines. Numerical examples underline the potential of isogeometric analysis with T-splines and give hints for further developments. Key words: adaptivity, a posteriori error estimation, isogeometric analysis, NURBS, CAD...|$|R
40|$|Additive fuzzy {{systems can}} filter {{impulsive}} noise from signals. Alpha-stable statistics model the impulsiveness as a parametrized family of probability density functions or unit-area bell curves. The bell-curve parameter ct ranges through the interval (0, 2] {{and gives the}} Gaussian bell curve when 0 t = 2 and gives the Cauchy bell curve when ~t = 1. The impulsiveness grows as ~ falls and the bell curves have thicker tails. Only the Gaussian statistics have finite variances or finite higher moments. An additive fuzzy system can learn ellipsoidal fuzzy <b>rule</b> <b>patches</b> from a new pseudo-covariation matrix or measure of alpha-stable covariation. Mahalanobis distance gives a joint set function for the learned if-part fuzzy sets of the if-then rules. The joint set function preserves input correlations that factored set functions ignore. Competitive learning tunes the local means and pseudo-covariations of the alpha-stable statistics and thus tunes the fuzzy rules. Then the covariation rules can both predict nonlinear signals in impulsive noise and filter the impulsive noise in time-series data. The fuzzy system filtered such noise better than did a benchmark radial basis neural network. 1. Filtering impulsive noise Impulsive noise is not Gaussian. The bell-curve density of impulsive noise has thicker tails than a Gaussian bell curve has. The thicker tails give rise to more frequent bursts of noise. The Cauchy density f(x) = 1 /n(l+x 2) has this property and so do all alpha-stable probability densities [17] for index parameter ~ in 0 < ~t < 2. The Cauchy density is the special case when ct = 1. The thicker polynomial tails give an infinite variance and give infinite higher-order moments. The lower-order fractional moments are still finite. The Gaussian density is the special case of an alpha-stable density when ct = 2. It is unique in this family because it has exponential tails and has finite variance and higher-order moments. Fig. 1 shows how alpha-stable noise grows more impulsive as the index ~t falls from 2 to 1...|$|R
40|$|Adaptive isogeometric {{analysis}} by local h-refinement with T-splines. (English summary) Comput. Methods Appl. Mech. Engrg. 199 (2010), no. 5 - 8, 264 – 275. Summary: “Isogeometric analysis based on non-uniform rational B-splines (NURBS) as basis functions preserves the exact geometry but {{suffers from the}} drawback of a rectangular grid of control points in the parameter space, which renders a purely local refinement impossible. This paper demonstrates how this difficulty can be overcome by using T-splines instead. T-splines allow the introduction of so-called T-junctions, which are related to hanging nodes in the standard FEM. Obeying a few straightforward <b>rules,</b> rectangular <b>patches</b> in the parameter space of the T-splines can be subdivided and thus a local refinement becomes feasible while still preserving the exact geometry. Furthermore, it is shown how state-of-the-art a posteriori error estimation techniques can be combined with refinement by T-splines. Numerical examples underline the potential of isogeometric analysis with T-splines and give hints for further developments. ...|$|R
40|$|We {{report the}} case of a 32 -year-old woman who {{presented}} with pain recurrence 20 months after she underwent a C 5 C 6 metal-on-metal total replacement. Plan radiographs demonstrated a modification of the shape of the vertebral bodies making the prosthesis more protruding. Then, infection has been <b>ruled</b> out and <b>patch</b> testing revealed a strongly positive reaction for chromium and cobalt. The prosthesis has been removed and a fusion achieved using a cage filled with bone graft. She has been immediately and fully relieved from her pain. We report the radiological signs that enabled early diagnosis and treatment allowing favorable outcome...|$|R
40|$|Copula {{duration}} (t) decreases, and {{proportional rate}} of sperm transfer (c) increases, with larger male body size in dung flies, so their dimensionless product (c. t) is approximately constant (approximately 2. 2). The most recent copulating male fertilizes about 89 % {{of the eggs}} laid (= 1 - e(-c. t) = 1 - e(- 2. 2)), independent of his body size. The conditions under which natural selection favors this phenotypic invariance are studied with fitness optimization models. The dimensionless <b>rules</b> for optimal <b>patch</b> residence times are then generalized to cover phenotypic variation in other foraging cases...|$|R
40|$|Abstract. Reverse {{engineering}} {{has been}} widely recognized as a central step in a product cycle of designing and manufacturing. However, major problems with current reverse engineering technology are the inefficient surface reconstruction process, lack of digitizing accuracy control in the data digitization process, and bottlenecks resulted from huge amounts of digitized surface points in the surface modeling process. Moreover, under this limitation, modern concurrent engineering concepts are difficult {{to be applied to}} obtain optimal product process. This paper presents a case study on the reverse designing of a plastic part. The study applies a developed reverse engineering approach to reconstruct surface [...] a series of advanced algorithms, including segmentation, simplification, extraction, fitting curves and reconstruction surface. Two orthogonal curves, which are fitted from the digitized points, are taken as base curves of a <b>ruled</b> surface <b>patch.</b> All surface patches are merged into a whole surface by trimming and chamfering. A CAD model is constructed based on the whole surface. The approach integrates surface digitizing and modeling processes of a plastic part into surface reconstruction process. Using the approach, accurate product CAD model can be efficiently generated and the product design cycle of plastic parts can be successfully linked...|$|R
40|$|This {{specification}} {{defines the}} JSON merge patch format and processing <b>rules.</b> The merge <b>patch</b> format is primarily {{intended for use}} with the HTTP PATCH method {{as a means of}} describing a set of modifications to a target resource’s content. Status of This Memo This is an Internet Standards Track document. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the Internet Engineering Steering Group (IESG). Further information on Internet Standards is available in Section 2 of RFC 5741. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained a...|$|R
40|$|Background: We are {{interested}} in understanding if metacommunity dynamics contribute to the persistence of complex spatial food webs subject to colonization-extinction dynamics. We study persistence {{as a measure of}} stability of communities within discrete patches, and ask how do species diversity, connectance, and topology influence it in spatially structured food webs. Methodology/Principal Findings: We answer this question first by identifying two general mechanisms linking topology of simple food web modules and persistence at the regional scale. We then assess the robustness of these mechanisms to more complex food webs with simulations based on randomly created and empirical webs found in the literature. We find that linkage proximity to primary producers and food web diversity generate a positive relationship between complexity and persistence in spatial food webs. The comparison between empirical and randomly created food webs reveal that the most important element for food web persistence under spatial colonization-extinction dynamics is the degree distribution: the number of prey species per consumer is more important than their identity. Conclusions/Significance: With a simple set of <b>rules</b> governing <b>patch</b> colonization and extinction, we have predicted that diversity and connectance promote persistence at the regional scale. The strength of our approach is that it reconciles the effect of complexity on stability at the local and the regional scale. Even if complex food webs are locally prone t...|$|R
40|$|A {{method for}} {{detecting}} collision of manipulator links is presented {{and used to}} avoid collision in a robotics environment in which manipulator links and obstacles are represented by complex solids. Surface boundary representation methods are used to envelop each solid with a known surface (<b>ruled,</b> double-curved, surface <b>patches,</b> and planar surfaces). Negative entities are defined on surfaces having discontinuities. Based on this representation scheme, a collision detection method is developed that incorporates complex solids. The collision avoidance algorithm is based on checking the surfaces of solid pairs of bodies for interference. The intersection curve (or branch) is calculated and checked for existence inside the boundary of entities on the two surfaces. Each segment of the intersecting curve is analyzed by introducing a new method for determining {{the existence of a}} point inside the boundary of an entity. This method is based upon computing the circulation along a closed contour o [...] ...|$|R
40|$|A neural {{fuzzy system}} can learn an agent {{profile of a}} user when it samples user {{question-answer}} data. A fuzzy system uses if-then rules to store and compress the agent's knowledge of the user's likes and dislikes. A neural system uses training data to form and tune the rules. The profile is a preference map or a bumpy utility surface defined over the space of search objects. <b>Rules</b> define fuzzy <b>patches</b> that cover the surface bumps as learning unfolds and as the fuzzy agent system gives a finer approximation of the profile. The agent system searches for preferred objects with the learned profile and with a new fuzzy measure of similarity. The appendix derives the supervised learning law that tunes this matching measure with fresh sample data. We test the fuzzy agent profile system on object spaces of flowers and sunsets and test the fuzzy agent matching system on an object space of sunset images. Rule explosion and data acquisition impose fundamental limits on the system designs...|$|R
40|$|We {{present an}} inference-based surface {{reconstruction}} algorithm that {{is capable of}} both identifying objects of interest amongst a cluttered scene, as well as reconstructing solid model representations even in the presence of occluded surfaces. As the underlying complexity of laser scanned environments increases, accurately reconstructing complete high-fidelity models from point cloud data becomes an increasingly more difficult problem. As the number of objects and the degree of clutter incorporated in these scenes increases, issues such as occlusion and partially visible surface features become more prevalent. For simulation purposes, there is a great need for accurate reconstructions of discrete solid models of each object residing in the scene. Our proposed approach incorporates a predictive modeling framework that uses a core set of prior knowledge, and applies this knowledge to the iterative identification and construction process. Our approach uses a local to global matching and fitting process driven by a set of rules developed from prior models. High quality surface patches, also obtained from the prior models, are then incrementally fitted and used to guide the reconstruction of each object. Through the use of these <b>rules</b> and <b>patches,</b> this approach is capable of reconstructing a solid representation even if a major portion of the object’s structure is occluded. To demonstrate the effectiveness of our approach, we provide the results of the algorithm executed on several datasets of increasing size and complexity. Categories and Subject Descriptors I. 3. 5 [Computer Graphics]: Computational Geometry and Object Modeling – boundary representations, curve, surface...|$|R
40|$|Patch {{departure}} was {{studied in}} experimentally naive, laboratory-reared bumble bees, Bombus impatiens (Cresson 1863) foraging from artifi cial umbels (rings of eight wells in blocks of Plexiglas, each well containing 0 – 4 µL of 30 % sucrose solution). Bees from three colonies probed {{an average of}} about ten wells (all available wells plus two revisits to emptied wells) before departing. On the empty rings, bees probed signifi cantly more wells before departing if they had previously visited a fi lled ring. The numbers of wells probed on fi lled rings varied with the volume of nectar, with the maximum associated with 2 µL, and fewer on average associated with both 1 µL and 4 µL. In tests with triplets of rings containing one empty ring and two others fi lled with sucrose solution, the numbers of wells probed on the empty rings depended on both the volumes of nectar in previous rings and their order of encounter. The results did not support the threshold departure rules (e. g, ‘depart after fi nding two empty fl owers’) that have been proposed as the proximal mechanism of bee patch departure. Instead, the mechanism by which the bumble bees decided to depart patches seemed to be a complex function of experience in recently encountered patches. Key words: Bombus impatiens (Cresson 1863) – departure <b>rules</b> – food <b>patch</b> – foragin...|$|R
40|$|A neural {{fuzzy system}} can learn an agent {{profile of a}} user when it samples user {{question}} -answer data. A fuzzy system uses if-then rules to store and compress the agent's knowledge of the user's likes and dislikes. A neural system uses training data to form and tune the rules. The profile is a preference map or a bumpy utility surface defined over the space of search objects. <b>Rules</b> define fuzzy <b>patches</b> that cover the surface bumps as learning unfolds and as the fuzzy agent system gives a finer approximation of the profile. The agent system searches for preferred objects with the learned profile and with a new fuzzy measure of similarity. The appendix derives the supervised learning law that tunes this matching measure with fresh sample data. We test the fuzzyagent profile system on object spaces of flowers and sunsets and test the fuzzy agent matching system on an object space of sunset images. Rule explosion and data acquisition impose fundamental limits on the system designs. 1 Smart Agents: Profile Learning and Object Matching How can we teach an agent what we like and dislike? How can an agent search new databases on our behalf? These are core questions for both human agents and intelligent software agents. We explore these questions with the joint tools of fuzzy rule-based systems and neural learning. These tools exploit the filter and set-theoretic structure of agent search. An intelligent agent {{can act as a}} smart database filter (Grosky, 1994; Maes, 1994). The agent can search a database or a space of objects on behalf of its user. The agent can find and retrieve objects that the user likes. Or the agent can find and then ignore or delete objects that the user does not like. Or it can perform some mix of both. The agent acts as a filter because it maps a set of o [...] ...|$|R
40|$|Estimating the {{exposure}} of honeybees to pesticides on a landscape scale requires models of their spatial foraging behaviour. For this purpose, we developed a mechanistic, energetics-based model for a single day of nectar foraging in complex landscape mosaics. Net energetic efficiency determined resource patch choice. In one version of the model a single optimal patch was selected each hour. In another version, recruitment of foragers was simulated and several patches could be exploited simultaneously. Resource availability changed during the day due to depletion and/or intrinsic properties of the resource (anthesis). The model accounted {{for the impact of}} patch distance and size, resource depletion and replenishment, competition with other nectar foragers, and seasonal and diurnal patterns in availability of nectar-providing crops and wild flowers. From the model we derived simple <b>rules</b> for resource <b>patch</b> selection, e. g., for landscapes with mass-flowering crops only, net energetic efficiency would be proportional to the ratio of the energetic content of the nectar divided by distance to the hive. We also determined maximum distances at which resources like oilseed rape and clover were still energetically attractive. We used the model to assess the potential for pesticide exposure dilution in landscapes of different composition and complexity. Dilution means a lower concentration in nectar arriving at the hive compared to the concentration in nectar at a treated field and can result from foraging effort being diverted away from treated fields. Applying the model for all possible hive locations over a large area, distributions of dilution factors were obtained that were characterised by their 90 -percentile value. For an area for which detailed spatial data on crops and off-field semi-natural habitats were available, we tested three landscape management scenarios that were expected to lead to exposure dilution: providing alternative resources than the target crop (oilseed rape) in the form of (i) other untreated crop fields, (ii) flower strips of different widths at field edges (off-crop in-field resources), and (iii) resources on off-field (semi-natural) habitats. For both model versions, significant dilution occurred only when alternative resource patches were equal or more attractive than oilseed rape, nearby and numerous and only in case of flower strips and off-field habitats. On an area-base, flower strips were more than one order of magnitude more effective than off-field habitats, the main reason being that flower strips had an optimal location. The two model versions differed in the predicted number of resource patches exploited over the day, but mainly in landscapes with numerous small resource patches. In landscapes consisting of few large resource patches (crop fields) both versions predicted the use of a small number of patches...|$|R

