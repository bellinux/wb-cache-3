326|299|Public
2500|$|The {{equilibrium}} <b>row</b> <b>vector</b> π must be annihilated by {{the rate}} matrix Q: ...|$|E
2500|$|The outer {{product of}} the column vector [...] by the <b>row</b> <b>vector</b> [...] yields an [...] matrix : ...|$|E
2500|$|When D is a 1×1 matrix, B is {{a column}} vector, and C is a <b>row</b> <b>vector</b> then ...|$|E
5000|$|... #Caption: The <b>row</b> <b>vectors</b> of a matrix. The row {{space of}} this matrix is the vector space {{generated}} by linear combinations of the <b>row</b> <b>vectors.</b>|$|R
40|$|An {{iterative}} method of computing {{the capacity of}} a discrete memoryless channel, whose channel matrix has m <b>row</b> <b>vectors</b> and is of rank t, has been proposed independently by Arimoto (1972) and Blahut (1972). The amount of computation involved depends upon {{the size of the}} channel matrix used. It is shown that it is sufficient to use a set of t linearly independent <b>row</b> <b>vectors</b> as channel matrix in the computation of the capacity of a discrete memoryless channel. For the case m > t, a criterion for selecting a set of t linearly independent <b>row</b> <b>vectors</b> as channel matrix is presented...|$|R
3000|$|... 1 [*]≠[*] 0), which signify {{that any}} column <b>vectors</b> or <b>row</b> <b>vectors</b> from Pn and H are linearly independent. Therefore the SKP {{measurement}} matrix Φ {{is also a}} linear independent system between <b>row</b> <b>vectors.</b> The correlation among the resulting measurements is reduced, and the unique distribution of the SKP measurement matrix facilitates its implementation.|$|R
2500|$|These {{numbers are}} often {{arranged}} into a column vector or <b>row</b> <b>vector,</b> particularly {{when dealing with}} matrices, as follows: ...|$|E
2500|$|... {{which can}} also be {{interpreted}} as a matrix multiplication (i.e., a column vector times a <b>row</b> <b>vector</b> equals a matrix).|$|E
2500|$|Observe {{that each}} row {{has the same}} {{distribution}} as this {{does not depend on}} starting state. The <b>row</b> <b>vector</b> π may be found by solving ...|$|E
2500|$|... "Covariant tensors are <b>row</b> <b>vectors</b> {{that have}} indices that are below (co-below-row)." ...|$|R
5000|$|Place the <b>row</b> <b>vectors</b> into {{a single}} matrix X of {{dimensions}} n × p.|$|R
30|$|In the following, all vectors of {{consecutive}} {{values in}} time are <b>row</b> <b>vectors,</b> while vectors of simultaneous values taken {{at a given}} time instant are column vectors. Matrices are organized accordingly.|$|R
2500|$|The {{same point}} [...] can be {{represented}} either by a column vector [...] or a <b>row</b> <b>vector</b> [...] Rotation matrices can either pre-multiply column vectors (...) , or post-multiply row vectors (...) [...] However, [...] produces a rotation {{in the opposite direction}} with respect to [...] Throughout this article, rotations produced on column vectors are described by means of a pre-multiplication. To obtain exactly the same rotation (i.e. the same final coordinates of point [...] ), the <b>row</b> <b>vector</b> must be post-multiplied by the transpose of [...] (...) [...]|$|E
2500|$|... with [...] being a <b>row</b> <b>vector,</b> {{such that}} all {{elements}} in [...] {{are greater than}} 0 and [...] = 1. From this, π may be found as ...|$|E
2500|$|For a finite-dimensional vector space, using a fixed {{orthonormal}} basis, {{the inner}} product {{can be written}} as a matrix multiplication of a <b>row</b> <b>vector</b> with a column vector: ...|$|E
50|$|The null {{space of}} matrix A is {{the set of}} all vectors x for which Ax = 0. The product of the matrix A and the vector x can be written {{in terms of the}} dot product of vectors:where r1,&#8239;...&#8239;,&#8239;rm are the <b>row</b> <b>vectors</b> of A. Thus Ax = 0 if and only if x is {{orthogonal}} (perpendicular) to each of the <b>row</b> <b>vectors</b> of A.|$|R
50|$|In particular, the <b>row</b> <b>vectors</b> of A are a {{basis for}} the null space of the {{corresponding}} matrix.|$|R
5000|$|Note {{the use of}} <b>row</b> <b>vectors</b> for point {{coordinates}} {{and that}} the matrix is written on the right.|$|R
2500|$|A {{stationary}} distribution π is a (<b>row)</b> <b>vector,</b> whose entries are non-negative and sum to 1, is unchanged by {{the operation of}} transition matrix P on it and so is defined by ...|$|E
2500|$|In an -dimensional Hilbert space, [...] can {{be written}} as a [...] <b>row</b> <b>vector,</b> and [...] (as in the {{previous}} section) is an [...] matrix. Then the bra [...] can be computed by normal matrix multiplication.|$|E
2500|$|The {{distribution}} over {{states can}} be written as a stochastic <b>row</b> <b>vector</b> [...] with the relation [...] So if at time [...] the system is in state , then three time periods later, at time [...] the distribution is ...|$|E
50|$|These two (linearly independent) <b>row</b> <b>vectors</b> {{span the}} <b>row</b> space of A, a plane {{orthogonal}} to the vector (−1,−26,16)T.|$|R
3000|$|... m is the {{resulting}} matrix of total impact multipliers {{with the same}} dimensions as the f matrix (usually <b>row</b> <b>vectors).</b>|$|R
30|$|Where c is {{the set of}} the M <b>row</b> <b>vectors</b> {{selected}} from U matrix. Φ is the final measurement matrix [8].|$|R
2500|$|This {{corresponds}} to transforming a column vector (element of V) to a <b>row</b> <b>vector</b> (element of V*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism [...] "depends on {{the choice of}} basis".|$|E
2500|$|... (a) For {{the case}} of column vector c and <b>row</b> <b>vector</b> r, each with m components, the formula allows quick {{calculation}} of the determinant of a matrix that differs from the identity matrix by a matrix of rank 1: ...|$|E
2500|$|... where κ is a scalar and u is a 1 by n matrix. Any <b>row</b> <b>vector</b> u {{satisfying}} {{this equation}} {{is called a}} left eigenvector of A and κ is its associated eigenvalue. Taking the transpose of this equation, ...|$|E
3000|$|... [...]. Thereby, the <b>row</b> <b>vectors</b> of Hred,dshould be {{as short}} as {{possible}} and close to orthogonal. Applying the additional unimodular matrix [...]...|$|R
5000|$|Notice {{that we are}} {{now using}} a column vector while the forward probabilities used <b>row</b> <b>vectors.</b> We can then work {{backwards}} using: ...|$|R
30|$|Then, two {{children}} are generated by combining their parent’s genes. In particular, a crossover point is first chosen randomly {{at a certain}} column of the two given chromosomes. Next, in order to form the first offspring, all the <b>row</b> <b>vectors</b> before the crossover point of the first matrix will combine with the <b>row</b> <b>vectors</b> after the crossover point of the second matrix. The second offspring is generated in the opposite way. Herein, the crossover process is illustrated.|$|R
2500|$|The outer product [...] is {{equivalent}} to a matrix multiplication uvT, provided that u is represented as a [...] column vector and v as a [...] column vector (which makes vT a <b>row</b> <b>vector).</b> For instance, if [...] and , then ...|$|E
2500|$|If [...] is a vector-valued random variable, with {{values in}} [...] {{and thought of}} as a column vector, then a natural {{generalization}} of variance is [...] where [...] and [...] is the transpose of [...] and so is a <b>row</b> <b>vector.</b> [...] The result is a positive semi-definite square matrix, commonly referred to as the variance-covariance matrix (or simply as the covariance matrix).|$|E
2500|$|The general {{formulation}} of covariance and contravariance refer {{to how the}} components of a coordinate vector transform under a change of basis (passive transformation). [...] Thus let V be a vector space of dimension n over the field of scalars S, and let each of [...] and [...] be a basis of V. to V. [...] Regarding f as a <b>row</b> <b>vector</b> whose entries are {{the elements of the}} basis, the associated linear isomorphism is then [...] Also, let the change of basis from f to f′ be given by ...|$|E
50|$|To simplify writing column vectors in-line {{with other}} text, {{sometimes}} they are written as <b>row</b> <b>vectors</b> with the transpose operation applied to them.|$|R
5000|$|Coordinate vectors of finite-dimensional vector spaces can be {{represented}} by matrices as column or <b>row</b> <b>vectors.</b> In the above notation, one can writeor ...|$|R
5000|$|Row {{reduction}} {{does not}} change the span of the <b>row</b> <b>vectors,</b> i.e. the reduced matrix has the same row space as the original.|$|R
