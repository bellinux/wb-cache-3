3|10|Public
40|$|Considerations for time, cost, and audit {{comfort in}} {{completing}} audit engagements have gradually led to {{extensive use of}} analytical review (AR) in external audits by large accounting firms in Hong Kong. AR is now being used by auditors as "attention‐directing" at the planning stage, "test‐reducing" at the fieldwork stage, and an overall <b>reasonableness</b> <b>check</b> in arriving at the true‐and‐fair decision at the final review stage. Greater reliance on AR has long been predicted in the auditing literature as an inevitable future trend. Empirical findings on auditors attesting to the extensive use of AR in Hong Kong are simply visible manifestations of the profession's adjustment to the global movement. School of Accounting and Financ...|$|E
40|$|Human beings {{make and}} usually detect errors routinely. The same mental {{processes}} that allow humans {{to cope with}} novel problems {{can also lead to}} error. Bill Rouse has argued that errors are not inherently bad but their consequences may be. He proposes the development of error-tolerant systems that detect errors and take steps to prevent the consequences of the error from occurring. Research should be done on self and automatic detection of random and unanticipated errors. For self detection, displays should be developed that make the consequences of errors immediately apparent. For example, electronic map displays graphically show the consequences of horizontal flight plan entry errors. Vertical profile displays should be developed to make apparent vertical flight planning errors. Other concepts such as energy circles could also help the crew detect gross flight planning errors. For automatic detection, systems should be developed that can track pilot activity, infer pilot intent and inform the crew of potential errors before their consequences are realized. Systems that perform a <b>reasonableness</b> <b>check</b> on flight plan modifications by checking route length and magnitude of course changes are simple examples. Another example would be a system that checked the aircraft's planned altitude against a data base of world terrain elevations. Information is given in viewgraph form...|$|E
40|$|As {{congestion}} management strategies begin {{to put more}} emphasis on person trips than vehicle trips, the need for vehicle occupancy data has become more critical. The traditional methods of collecting these data include the roadside windshield method and the carousel method. These methods are labor-intensive and expensive. An alternative to these traditional methods is {{to make use of}} the vehicle occupancy information in traffic accident records. This method is cost effective and may provide better spatial and temporal coverage than the traditional methods. However, this method is subject to potential biases resulting from under- and over-involvement of certain population sectors and certain types of accidents in traffic accident records. ^ In this dissertation, three such potential biases, i. e., accident severity, driver’s age, and driver’s gender, were investigated and the corresponding bias factors were developed as needed. The results show that although multi-occupant vehicles are involved in higher percentages of severe accidents than are single-occupant vehicles, multi-occupant vehicles in the whole accident vehicle population were not overrepresented in the accident database. On the other hand, a significant difference was found between the distributions of the ages and genders of drivers involved in accidents and those of the general driving population. ^ An information system that incorporates adjustments for the potential biases was developed to estimate the average vehicle occupancies (AVOs) for different types of roadways on the Florida state roadway system. A <b>reasonableness</b> <b>check</b> of the results from the system shows AVO estimates that are highly consistent with expectations. In addition, comparisons of AVOs from accident data with the field estimates show that the two data sources produce relatively consistent results. ^ While accident records can be used to obtain the historical AVO trends and field data can be used to estimate the current AVOs, no known methods have been developed to project future AVOs. Four regression models for the purpose of predicting weekday AVOs on different levels of geographic areas and roadway types were developed as part of this dissertation. The models show that such socioeconomic factors as income, vehicle ownership, and employment have a significant impact on AVOs. ...|$|E
40|$|A {{medical data}} base {{is only as}} good as the data it contains, and the process of {{collecting}} medical information and converting it to machine-readable form is fraught with opportunities for error. An analysis of the regularities in medical data permits the development of validity and <b>reasonableness</b> <b>checks</b> which can be automated. We examine the regularities to be found in common elements of medical data and suggest checks based on these regularities. Particular attention is given to constructing checks which lend themselves to automation...|$|R
50|$|An RNP system utilises its {{navigation}} sensors, system architecture, {{and modes}} of operation to satisfy the RNP navigation specification requirements. It must perform the integrity and <b>reasonableness</b> <b>checks</b> of the sensors and data, and it may provide a means to deselect specific types of navigation aids to prevent reversion to an inadequate sensor. RNP requirements may limit the modes of operation of the aircraft, e.g. for low RNP, where flight technical error (FTE) is a significant factor, and manual flight by the crew may not be allowed. Dual system/sensor installations may also be required depending on the intended operation or need.|$|R
40|$|Abstract. The {{services}} offered by Internet Data Centers involve {{the provision of}} storage, bandwidth and computational resources. A common business model is to charge consumers on a pay-per-use basis where they periodically pay for the resources they have consumed (as opposed to a fixed charge for service provision). The pay-per-use model {{raises the question of}} how to measure resource consumption. Currently, a widely used accounting mechanism is provider-side accounting where the provider unilaterally measures the consumer’s resource consumption and presents the latter with a bill. A serious limitation of this approach is that it does not offer the consumer sufficient means of performing <b>reasonableness</b> <b>checks</b> to verify that the provider is not accidentally or maliciously overcharging. To address the problem the paper develops bilateral accounting models where both consumer and provider independently measure resource consumption, verify the equity of the accounting process and try to resolve potential conflicts emerging from the independently produced results. The paper discusses the technical issues involved in bilateral accounting...|$|R
40|$|Effluents {{from point}} sources (industries, communities) and diffuse inputs {{introduce}} pollutants {{into the water}} of the river Rhine and cause a basic contaminant load. The aim is to establish a biological warning system to detect increased toxicity in addition to the already existing chemical-physical monitoring system. To cover a wide range of biocides, continuous working biotests at different trophic levels (bacteria, algae, mussels, water fleas, fishes) have been developed and proved. These are checked out for sensitivity against toxicants, reaction time, validity of data and practical handling under field conditions at the rivers Rhine or Main respectively after being developed for laboratory using. For this purpose a mobile laboratory container is available to test the biomonitors at different places. Tests within one group (e. g. algae tests or fish tests) are compared for their suitability for monitoring the Rhine. The measured data and alarm signals of each test are transmitted to a central computer. They are evaluated and proved for alarm events. If there is an alarm message by one of the biotests at the Rhine or Main, <b>reasonableness</b> <b>checks</b> and checks for synchronous alarms are done by the container staff. (orig.) SIGLEAvailable from TIB Hannover: RN 8908 (94 - 170) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Umwelt, Naturschutz und Reaktorsicherheit, Bonn (Germany) DEGerman...|$|R
30|$|A common cloud {{forensic}} model {{proposed by}} researchers is ‘Cloud-Forensic-as-a-Service’ where consumers have to access it {{as a service}} to collect forensic data from cloud environments. The ‘Cloud-Forensic-as-a-Service’ model {{raises the question of}} how it collects digital evidence pertaining to an incident which occurred in the cloud. Currently, types of ‘Cloud-Forensic-as-a-Service’ systems in the literature show that the system is controlled and implemented by the cloud provider, where they unilaterally define the type of evidence that can be collected by the system. A serious limitation of this approach {{is that it does not}} offer the consumer sufficient means of performing <b>reasonableness</b> <b>checks</b> to verify that the provider is not accidentally or maliciously contaminating the evidence. To address the problem, the paper proposes a conceptual bilateral Cloud-Forensic-as-a-Service model where both consumers and providers can independently collect, verify the equity of the forensic analysis process and try to resolve potential disputes emerging from the independently collected results. The authors have developed a cloud forensic process model to lead common and significant aspects of a bilateral Cloud-Forensics-as-a-Service model. The paper explicitly discusses the concept of a bilateral Cloud-Forensic-as-a-Service model.|$|R
30|$|In {{light of}} the above discussion, we propose {{the notion of a}} Consumer–centric Resource Accounting Model for a cloud resource. We say that an {{accounting}} model is weakly consumer–centric if all the data that the model requires for calculating billing charges can be queried programmatically from the provider. Further, we say that an accounting model is strongly consumer–centric if all the data that the model requires for calculating billing charges can be collected independently by the consumer (or a TTP); in effect, this means that a consumer (or a TTP) should {{be in a position to}} run their own measurement service. We contend that it is in the interest of the providers to make the accounting models of their services at least weakly consumer–centric. Strongly consumer–centric models should prove even more attractive to consumers as they enable consumers to incorporate independent consistency or <b>reasonableness</b> <b>checks</b> as well as raise alarms when apparent discrepancies are suspected in consumption figures; furthermore, innovative charging schemes can be constructed by consumers that are themselves offering third party services. Strongly consumer–centric accounting models have the desirable property of openness and transparency, since service users are in a position to verify the charges billed to them.|$|R
40|$|Automated {{generation}} of test cases {{is a prerequisite}} for fast testing. Whereas the research has addressed the creation of individual test points, test trajectoiy generation has attracted limited. In simple terms, a test trajectoiy is defined as a series of data points, with each (possibly multidimensional) point relying upon the value(s) of previous point(s). Software systems that use data trajectories as inputs include closed-loop process controllers. For these systems, software testers can either handcraft test trajectories, use input trajectories from older versions of the system or, perhaps, collect test data in a high fidelity system simulator. While these are valid approaches, they are expensive and time-consuming, especially if the assessment goals require substantial number of tests. In this paper, we propose a framework for expanding a small, conventionally developed set of test trajectories into a large set suitable, for example, for system safe 0 assurance. In the core of this framework is statistical regression analysis. The regression analysis builds a relationship between controllable independent variables and closely correlated dependent variables, which represent test trajectories. By perturbing the independent variables, new test trajectories can be generated automatically. Automated test trajectory generation has been applied in the safety assessment of a fault tolerant flight control system. We compare the performance of simple linear regression, multiple linear regression, and autoregressive techniques. The peiformance meti-ics include the speed of test generation and the percentage of “acceptable” trajectories, measured by the domain specific <b>reasonableness</b> <b>checks.</b> 1...|$|R
50|$|In an {{organization}} complex logic is usually segregated into simpler logic across multiple processes. <b>Reasonableness</b> DQ <b>checks</b> on such complex logic yielding to a logical result {{within a specific}} range of values or static interrelationships (aggregated business rules) may be validated to discover complicated but crucial business processes and outliers of the data, its drift from BAU (business as usual) expectations, and may provide possible exceptions eventually resulting into data issues. This check may be a simple generic aggregation rule engulfed by large chunk of data {{or it can be}} a complicated logic on a group of attributes of a transaction pertaining to the core business of the organization. This DQ check requires high degree of business knowledge and acumen. Discovery of reasonableness issues may aid for policy and strategy changes by either business or data governance or both.|$|R
40|$|For new {{access to}} space systems with {{challenging}} mission requirements, effective implementation of integrated system health management (ISHM) must be available {{early in the}} program to support the design of systems that are safe, reliable, highly autonomous. Early ISHM availability is also needed to promote design for affordable operations; increased knowledge of functional health provided by ISHM supports construction of more efficient operations infrastructure. Lack of early ISHM inclusion in the system design process could result in retrofitting health management systems to augment and expand operational and safety requirements; thereby increasing program cost and risk due to increased instrumentation and computational complexity. Having the right sensors generating the required data to perform condition assessment, such as fault detection and isolation, {{with a high degree}} of confidence is critical to reliable operation of ISHM. Also, the data being generated by the sensors needs to be qualified to ensure that the assessments made by the ISHM is not based on faulty data. NASA Glenn Research Center has been developing technologies for sensor selection and data validation as part of the FDDR (Fault Detection, Diagnosis, and Response) element of the Upper Stage project of the Ares 1 launch vehicle development. This presentation will provide an overview of the GRC approach to sensor selection and data quality validation and will present recent results from applications that are representative of the complexity of propulsion systems for access to space vehicles. A brief overview of the sensor selection and data quality validation approaches is provided below. The NASA GRC developed Systematic Sensor Selection Strategy (S 4) is a model-based procedure for systematically and quantitatively selecting an optimal sensor suite to provide overall health assessment of a host system. S 4 can be logically partitioned into three major subdivisions: the knowledge base, the down-select iteration, and the final selection analysis. The knowledge base required for productive use of S 4 consists of system design information and heritage experience together with a focus on components with health implications. The sensor suite down-selection is an iterative process for identifying a group of sensors that provide good fault detection and isolation for targeted fault scenarios. In the final selection analysis, a statistical evaluation algorithm provides the final robustness test for each down-selected sensor suite. NASA GRC has developed an approach to sensor data qualification that applies empirical relationships, threshold detection techniques, and Bayesian belief theory to a network of sensors related by physics (i. e., analytical redundancy) in order to identify the failure of a given sensor within the network. This data quality validation approach extends the state-of-the-art, from red-lines and <b>reasonableness</b> <b>checks</b> that flag a sensor after it fails, to include analytical redundancy-based methods that can identify a sensor in the process of failing. The focus of this effort is on understanding the proper application of analytical redundancy-based data qualification methods for onboard use in monitoring Upper Stage sensors...|$|R
40|$|Aim of {{work was}} to develop {{strategic}} foundations of physical rehabilitation in the surgical treatment of patients with lumbar and sacral vertebrogenic compression syndromes. Methods are applied in our research: analysis of literature and theoretical research methods (analysis, interpretation and synthesis) of scientific and manual literature on the study. Abstraction (or idealization, or schematization) is an allocation of the essential foundations. There was an experimenting with schemes (as a development of their content, <b>check</b> <b>reasonableness</b> and practicality) during the development of concepts, practical models and physical rehabilitation programs. Materials: researching performed {{on the basis of}} analysis of an medical data of 542 patients, that treated on the base of SO “Institute of Neurosurgery named after acad. A. P. Romodanov NAMS of Ukraine” and rehabilitation department of the SO “Institute of Traumatology and Orthopedics NAMS of Ukraine”. Results: effectiveness of the surgical treatment depends on the timely, differentiated, adequate application of physical rehabilitation which should be based on the principles of continuity and duration of effects, which requires developing a plan for physical rehabilitation. A plan which developing, should be {{based on the results of}} the rehabilitation`s diagnosis and including the need for rehabilitation and the rehabilitation prognosis. Conclusions: the developed integrated assessment of rehabilitation potential and the need to determine the amount and intensity of physical rehabilitation...|$|R

