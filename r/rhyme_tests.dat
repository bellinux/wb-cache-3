10|35|Public
40|$|Speech {{intelligibility}} is {{the most}} important parameter of speech quality. In the contribution a new objective intelligibility assessment of speech processing algorithms, e. g., speech coding, synthesis, enhancement, conversion, is proposed. It is based on automatic speech recognition of <b>rhyme</b> <b>tests.</b> The idea is illustrated by comparison with listening evaluation of Czech <b>rhyme</b> <b>tests</b> and is applied for intelligibility assessment of Czech text-to-speech system and voice conversion...|$|E
40|$|This thesis {{examines}} {{speech intelligibility}} and multi-lingual communication, {{in terms of}} acoustics and perceptual factors. More specifically, the work focused {{on the impact of}} room acoustic conditions on the speech intelligibility of four languages representative {{of a wide range of}} linguistic properties (English, Polish, Arabic and Mandarin). Firstly, diagnostic <b>rhyme</b> <b>tests</b> (DRT), phonemically balanced (PB) word lists and phonemically balanced sentence lists have been compared under four room acoustic conditions defined by their speech transmission index (STI = 0. 2, 0. 4, 0. 6 and 0. 8). The results obtained indicated that there was a statistically significant difference between the word intelligibility scores of languages under all room acoustic conditions, apart from the STI = 0. 8 condition. English was the most intelligible language under all conditions, and differences with other languages were larger when conditions were poor (maximum difference of 29...|$|E
40|$|Due to the {{stability}} against the external noise, bone-conducted (BC) speech seems {{better to be}} used instead of noisy air-conducted speech in an extremely noisy environment. However the quality of bone-conducted speech is very low and restoring bone-conducted speech is a challenged topic in speech signal processing field. As the main issue to improve the BC speech, many studies try to model and resolve the degradation when the signal is conducted through bone transduction. In previous study, we proposed a linear prediction (LP) based blind-restoration model. In this paper, we therefore completely evaluated the proposed model in comparison with other models {{to find out whether}} our proposed model could adequately improve voice quality and the intelligibility of BC speech, using objective measures (LSD, MCD, and LCD) and carrying out Japanese word-intelligibility tests (JWITs), Vietnamese word-intelligibility tests (VWITs) and Modified <b>Rhyme</b> <b>Tests</b> (MRTs) for English. The results of experiments on different languages, i. e. Japanese, English and Vietnamese proved the practicability of blind-BC restoration...|$|E
40|$|The authors propose {{standards}} {{for evaluating the}} intelligibility of coded, synthesised or distorted Chinese speech over a wired or wireless voice channel in a communication system. The standards {{are based on the}} English equivalent diagnostic <b>rhyme</b> <b>test</b> (DRT) and modified <b>rhyme</b> <b>test</b> (MRT). The relative features of Chinese phonetics are outlined and the underlying principles of choosing phonetic units for the proposed standards, the Chinese diagnostic <b>rhyme</b> <b>test</b> (CDRT) and the Chinese modified <b>rhyme</b> <b>test</b> (CMRT), are presente...|$|R
40|$|It is {{becoming}} crucial to accurately estimate and monitor speech quality in various ambient environments to guarantee high quality speech communication. This practical hands-on book shows speech intelligibility measurement methods {{so that the}} readers can start measuring or estimating speech intelligibility of their own system. The book also introduces subjective and objective speech quality measures, and describes in detail speech intelligibility measurement methods. It introduces a diagnostic <b>rhyme</b> <b>test</b> which uses <b>rhyming</b> word-pairs, and includes: An investigation into the effect of word familiarity on speech intelligibility. Speech intelligibility measurement of localized speech in virtual 3 -D acoustic space using the <b>rhyme</b> <b>test.</b> Estimation of speech intelligibility using objective measures, including the ITU standard PESQ measures, and automatic speech recognizers...|$|R
30|$|To {{validate}} the method, an experiment {{was performed in}} real environments with various noise and reverberation conditions. The speech transmission index, spectrogram, log-spectral distortion measure, short-time objective intelligibility measure, and modified <b>rhyme</b> <b>test</b> were {{used to compare the}} performance. The objective and subjective evaluation results illustrate that the method can effectively improve the speech intelligibility of I-PA systems in noisy and reverberant environments.|$|R
40|$|This {{empirical}} article, is a {{case study}} that looks at how a lively girl with Down syndrome, together with her mother, father and grandmother, experiences the CCTs known as REFLECT, which was developed for the <b>RHYME</b> <b>tests</b> in 2013. Different from its predecessors, ORFI and WAVE, REFLECT has RFID tags, a type of technology that requires that participants scan one CCT onto another to activate the music through the RFID reader. Data were recorded via video observations of the family while they explored REFLECT, and an interview {{was done with the}} family immediately following their second experience with the platform. The question Stensæth asks is as follows: How does one family experience REFLECT, and how might their musicking with REFLECT potentially enhance their quality of life? The empirical part of this study will elaborate upon the methods and results, while the discussion and conclusion will apply certain theoretical perspectives to the whole enterprise...|$|E
40|$|The {{acoustic}} worship ambience in {{the church}} is defined through the derived Acoustic Worship Indices (AWI), namely Sacred Factor (SaF), Intelligibility Factor (InF) and Silence Factor (SiF). The Speech Intelligibility {{in the church}} was evaluated through RASTI tests for different sound source locations (altar [SA], pulpit [SB] and high altar [SC]) and for different postures (sitting [SIT] and standing [STAND]) and the Modified <b>Rhyme</b> <b>Tests</b> based Subjective Speech Intelligibility (SSI) scores for different source locations (SA, SB and SC) and for different languages (English [ENG] and Konkani [KONK]). SaF was found to regress significantly on SSI [SA] (R 2 = 0. 88) and RASTI[SIT] (R 2 = 0. 92); InF regressed the best on SSI[SC] (R 2 = 0. 84) and SSI[ENG] (R 2 = 0. 89) while SiF was best predicted through RASTI[STAND] (R 2 = 0. 93) and SSI[ENG] (R 2 = 0. 60). The possibility of a predictable connection between speech intelligibility and acoustic worship ambience can help enhance the liturgical functions in the church...|$|E
40|$|In this paper, we {{investigated}} {{the influence of}} stereo coding on Japanese speech localized in 3 -D virtual space. We encoded localized speech using Joint Stereo and Parametric Stereo modes within the HE-AAC (High-Efficiency Advanced Audio Coding) encoder at identical data rates. First, the sound quality of the localized speech signal was checked using MUSHRA subjective tests. The result showed that the speech quality for Joint Stereo is higher than Parametric Stereo when localized at 45 (where 0 refers to localization {{directly in front of}} the listener) by 20 to 30 MUSHRA score points. The scores for Joint Stereo were relatively proportional to bit rate. However, Parametric Stereo scores were not proportional to bit rate, and remained fairly constant with bit rate. Next, the Japanese word intelligibility tests were conducted using the Japanese Diagnostic <b>Rhyme</b> <b>Tests</b> (JDRT). Test speech was localized in front, while competing noise were localized at various angles. The result showed that speech could not be separated from the noise for Joint Stereo when the noise was in located in the frontal region, from 45 to 45, and intelligibility degrades significantly. However at other azimuth, the intelligibility improves dramatically. On the other hand, intelligibility with Parametric Stereo remained constant, at about 70 to 80 %. 1...|$|E
40|$|This work {{introduces}} a phonetically balanced modi-fied <b>rhyme</b> <b>test</b> (MRT) for evaluating Catalan speech intelligibility. The proposal {{complies with the}} stan-dard MRT restrictions, besides yielding phonetic balanced word ensembles {{so as to avoid}} biasing the test to scarcely representative phonemes. Hence, it allows testing the intelligibility of any communica-tion system delivering Catalan speech by means of a unique phonetic meaningful comparison framework...|$|R
40|$|International audienceVisible speech {{movements}} were motion captured and parameterized. Coarticulated targets were extracted from VCVs and modeled to generate arbitrary German utterances by target interpolation. The system {{was extended to}} synthesize English utterances by a mapping to German phonemes. An evaluation {{by means of a}} modified <b>rhyme</b> <b>test</b> reveals that the synthetic videos of isolated words increase the recognition scores from 27 % to 47. 5 % when added to audio only presentatio...|$|R
40|$|An {{amplitude}} control technique has been employed {{for use with}} analog voice communication systems, which improves low-level phoneme reception and eliminates the received noise between words and syllables. Tests were conducted on a narrow-band frequency-modulation simplex voice communication channel employing the {{amplitude control}} technique. Presented for both the modified <b>rhyme</b> word <b>tests</b> and the phonetically balanced word tests {{are a series of}} graphical plots of the tests' score distribution, mean, and standard deviation as a function of received carrier-to-noise power density ratio. At low received carrier-to-noise power density ratios, a significant improvement in the intelligibility was obtained. A voice intelligibility improvement of more than 2 dB was obtained for the modified <b>rhyme</b> <b>test</b> words, and a voice intelligibility improvement in excess of 4 dB was obtained for the phonetically balanced word tests...|$|R
40|$|Presented at the 15 th International Conference on Auditory Display (ICAD 2009), Copenhagen, Denmark, May 18 - 22, 2009 In this paper, we {{investigated}} {{the influence of}} stereo coding on Japanese speech localized in 3 -D virtual space. We encoded localized speech using Joint Stereo and Parametric Stereo modes within the HE-AAC (High-Efficiency Advanced Audio Coding) encoder at identical data rates. First, the sound quality of the localized speech signal was checked using MUSHRA subjective tests. The result showed that the speech quality for Joint Stereo is higher than Parametric Stereo when localized at 45 (where 0 refers to localization {{directly in front of}} the listener) by 20 to 30 MUSHRA score points. The scores for Joint Stereo were relatively proportional to bit rate. However, Parametric Stereo scores were not proportional to bit rate, and remained fairly constant with bit rate. Next, the Japanese word intelligibility tests were conducted using the Japanese Diagnostic <b>Rhyme</b> <b>Tests</b> (JDRT). Test speech was localized in front, while competing noise were localized at various angles. The result showed that speech could not be separated from the noise for Joint Stereo when the noise was in located in the frontal region, from 45 to 45, and intelligibility degrades significantly. However at other azimuth, the intelligibility improves dramatically. On the other hand, intelligibility with Parametric Stereo remained constant, at about 70 to 80 %...|$|E
40|$|This paper {{describes}} a pilot research project {{which seeks to}} assess speakers ’ intuitions about the phonological oppositions in their speech, using minimal pair and <b>rhyme</b> <b>tests.</b> As Wells (1982 : 73, 78 - 80) discusses, lexical incidence (distribution) {{is one of the}} primary ways in which accents of a language differ from each other, and this method is designed to reveal these distributions without recourse to phonetic analysis. It is my intention to investigate whether native speaker intuitions of this sort can provide a coherent picture of the geographical distribution of phonological oppositions in the British Isles, and whether they can provide us with crucial data for understanding the progression and interaction of processes such as standardisation (Milroy 2001), dialect levelling (Kerswill 2003) and the spread of mergers at the expense of distinctions (Herzog’s Principle, Labov 1994). In this study, speakers from a range of locations in Britain and Ireland are asked to judge whether pairs of words rhyme (e. g. cut and foot) or are pronounced the same (e. g. cot and caught) in their speech, and are asked to indicate whether this is always/usually the case, sometimes the case, or never/rarely the case for them. In addition, a range of social information is gathered for each speaker in order to contextualise the results. The data ar...|$|E
40|$|Classrooms {{acoustics}} {{can affect}} students´ speech intelligibility and learning performance depending on its background noise level and/or reverberation. Speech intelligibility is usually assessed in real classrooms through a subjective approach, by performing speech intelligibility tests, or through an objective approach, by evaluating speech transmission index (STI) from impulse response, speech and noise level measurements. Acoustic simulation technique {{makes it possible}} to assess acoustical conditions for speech reception in virtual environments, thus allowing for predicting intelligibility before a classroom is built or renovated. However, in order to obtain reliable results, the simulation model needs to be calibrated and validated with in-situ measurements. The aim of this work is to compare tests performed in-situ on a group of people, with tests performed on the same people by reproducing the auralized test signal through headphones, in terms of intelligibility scores (IS), response times (RT), listening efficiency values (DE) and related STI values. Simulations have been carried out using the room acoustic software Odeon version 14. 01. The investigation focused on a university classroom, {{which is part of the}} Classroom Spaces Living Lab of the Free University of Bozen-Bolzano, currently equipped with devices for monitoring energy and indoor comfort conditions, as well as detailed external weather conditions. By exploiting the bilingual context in South Tyrol, Diagnostic <b>Rhyme</b> <b>Tests</b> (DRT) in the Italian language were administered to both Italian and German native speaker students, the latter with an Italian level at least equal to B 2, according to the common European framework of reference for languages. In this way, speech reception performance of the two groups has been investigated and compared...|$|E
40|$|Visible speech {{movements}} were motion captured and parameterized. Coarticulated targets were extracted from VCVs and modeled to generate arbitrary German utterances by target interpolation. The system {{was extended to}} synthesize English utterances by a mapping to German phonemes. An evaluation {{by means of a}} modified <b>rhyme</b> <b>test</b> reveals that the synthetic videos of isolated words increase the recognition scores from 27 % to 47. 5 % when added to audio only presentation. Index Terms: talking head, intelligibility, evaluation 1...|$|R
40|$|The {{present study}} investigates {{the effect of}} {{computer-assisted}} intervention targeting phonological processing skill in Hong Kong kindergarteners who learn English {{as a second language}} (ESL). Thirty children received eighteen sessions of 45 minutes phonological processing skill intervention over nine weeks. Children in the experimental group (n = 15) received computer-assisted intervention, while the control group (n = 15) received intervention by traditional (pencil-and-paper) teaching approach. Following a pretest – posttest – retention test design, participants were assessed for their phonological processing skill changes by alliteration test, blending test, phoneme segmentation <b>test</b> and <b>rhyme</b> <b>test</b> before the intervention, after the intervention and 10 weeks after the intervention. The results indicated that children in the experimental group outperformed than those in the control group in all subtests during the posttest. A retention test which held 10 weeks after the intervention revealed that significant gains in the experimental group had only maintained in the alliteration <b>test</b> and <b>rhyme</b> <b>test.</b> To conclude, findings suggest that computer-assisted intervention has striking effect on enhancing children’s phonological processing skill in short term, yet, the long-term positive effect may not necessarily cover all aspects of the phonological processing skill. The current study may be of importance in explaining the feasibility and universality of computer-assisted intervention, as well as providing a better understand of how information technology could enhance children’s learning in phonological processing skill for stakeholders in the educational field. published_or_final_versionLibrary and Information ManagementMasterMaster of Science in Library and Information Managemen...|$|R
40|$|Current speech {{synthesis}} systems generally require large and carefully annotated speech corpora for their development. However, for many languages these resources are not available. This paper describes a speech generation algorithm based on monophone subword units for minimal reliance on such databases. The system {{is based on}} the source-filter speech production framework, and includes a linear prediction based vocal tract model as well as an excitation model. An interpolation algorithm is presented to allow coarticulation between monophone units to be modelled. The excitation model includes a method for dealing with voiced and partiallyvoiced sounds based on a Gaussianity measure applied to the excitation spectrum. Promising first results were obtained when evaluating the intelligibility of the developed system's South African English speech output using the modified <b>rhyme</b> <b>test</b> and semantically unpredictable sentences. Articl...|$|R
30|$|Despite {{the fact}} that {{objective}} methods like RMS distance between measured and predicted facial feature points or accumulated color differences of pixels {{can be applied to}} data-driven approaches, visual speech synthesis is meant to be perceived by humans. Therefore, subjective evaluation is crucial in order to assess the quality in a reasonable manner. All submissions to this special issue were required to include a subjective evaluation. In general, subjective evaluation comprises the selection of the task for the viewers, the material—that is, the text corpus to be synthesized—and the presentation mode(s). Two tasks were included within the LIPS Challenge: one to measure intelligibility and one to assess the perceived quality of the lip synchronization. For the former task subjects were asked to transcribe an utterance, and for the latter task they were asked to rate the audiovisual coherence of audible speech articulation and visible speech movements on an MOS scale. The material to be synthesized consisted of 42 semantically unpredictable sentences (SUSs). Compared to single words used, for example, in <b>rhyme</b> <b>tests</b> or logatome tests, SUSs offer the advantage that they are well formed complete sentences constructed from real words. Furthermore, the effect of context is minimized as the keywords to be identified cannot be predicted from one another. As the evaluation should focus on articulatory movements, the subjects were presented with {{the lower half of the}} face only. This avoids distractions from mouth movements by, for example, staring or blinking eyes. All synthesized videos were to be synchronized to the given auditory speech as a prerequisite. In addition to the lip-synched audiovisual sequences, subjects were presented with the (degraded) audio alone to assess any gain in intelligibility provided by the systems. Likewise the natural video was included to access the expected upper-bound on performance. Video only was not included as SUSs are virtually impossible to lip-read. In total 30 SUSs were presented for intelligibility testing (degraded to 5 [*]dB SNR using babble noise), and 12 SUSs were presented without degradation for rating the audiovisual synchrony.|$|E
40|$|Sensorineural {{hearing loss}} is {{associated}} with widening of the auditory filters, leading to increased spectral masking and degraded speech perception. Multi-band frequency compression {{can be used for}} reducing the effect of spectral masking. The speech spectrum is divided into a number of bands and spectral samples in each of these bands are compressed towards the band center, by a constant compression factor. In the present study, we have investigated the effectiveness of the scheme for different compression factors, in improving the speech perception. Evaluation of the scheme using the modified <b>rhyme</b> <b>test</b> showed maximum improvement in recognition scores for compression factor of 0. 6 : about 17 % for the normal-hearing subjects under simulated hearing loss, and 6 - 21 % for the subjects with moderate to severe sensorineural hearing loss...|$|R
40|$|International audienceVisible speech {{movements}} were optically motion captured and parameterized {{by means of}} a guided PCA. Co-articulated consonantal targets were extracted from VCVs, vocalic targets were extracted from these VCVs and from sustained vowels. Targets were selected or combined to derive target sequences for phone chains of arbitrary German utterances. Parameter trajectories for these utterances are generated by interpolating targets through linear to quadratic functions that reflect the degree of co-articulatory influence. Videos of test words embedded in a carrier sentence were rendered from parameter trajectories for an evaluation {{in the form of a}} <b>rhyme</b> <b>test</b> in noise. Results show that the synthetic videos - although intelligible only somewhat above chance level when played alone - significantly increase the recognition scores from 45. 6 % in audio alone presentation to 60. 4 % in audiovisual presentation...|$|R
40|$|This {{study was}} {{designed}} to combine two areas of research, each of which has produced significant results, in an effort to find better ways to prepare children for reading. First is the training of preschoolers in specific reading readiness skills. ^ In the early grades, phoneme segmentation skills and reading skills are highly correlated. There is evidence of a causal relationship and further evidence that phoneme segmentation skills can be taught, resulting in enhanced reading acquisition (Bradley 2 ̆ 6 Bryant, 1985). ^ Recent research has found a correlation between an early metalinguistic skill, rhyming, and reading acquisition (Bradley 2 ̆ 6 Bryant, 1985). Since the metalinguistic skills of rhyming, syllable segmentation and phoneme segmentation develop sequentially (Liberman, et al., 1980), it seems likely that they build one upon the other. This study focuses on developing rhyming skills of preschool children. ^ The second area of research is the training of parents in techniques which will enhance their children 2 ̆ 7 s development. This approach has been effective in language, cognitive and social development. ^ This study involves working with mothers of Head Start children. Mothers of one experimental group were to read to their children from selected books which emphasize rhyme and were taught a strategy for enhancing rhyming skills. Mothers in a second experimental group were to read to their children from selected books that focus on a story. Pre and post <b>tests</b> of <b>rhyming</b> skills were administered to Head Start children whose mothers were in the above groups and to a comparison group of Head Start children. <b>Rhyme</b> <b>test</b> scores were analyzed by Analysis of Covariance. ^ Although supplied with certain books and reading lists, mothers in both experimental groups independently expanded their reading to include books in both rhyme and story. Children in both experimental groups had significantly greater gain on this <b>rhyme</b> <b>test</b> than children in the Control Group. ...|$|R
5000|$|One {{example of}} this is empirically shown, specifically, in a study by Morris and {{associates}} (1977) using semantic and rhyme tasks. In a standard recognition test, memory was better following semantic processing compared to rhyme processing (the levels-of-processing effect). However, in a <b>rhyming</b> recognition <b>test,</b> memory was better for those who engaged in rhyme processing compared to semantic processing.|$|R
40|$|Visible speech {{movements}} were optically motion captured and parameterized {{by means of}} a guided PCA. Co-articulated consonantal targets were extracted from VCVs, vocalic targets were extracted from these VCVs and from sustained vowels. Targets were selected or combined to derive target sequences for phone chains of arbitrary German utterances. Parameter trajectories for these utterances are generated by interpolating targets through linear to quadratic functions that reflect he degree of co-articulatory influence. Videos of test word embedded in a carrier sentence were rendered from parameter trajectories for an evaluation {{in the form of a}} <b>rhyme</b> <b>test</b> with carrier sentence in noise. Results show that the synthetic videos – although intelligible only somewhat above chance level when played alone – significantly increase the recognition scores from 45. 6 % in audio alone presentation to 60. 4 % in audiovisual presentation. Index Terms: talking head, intelligibility, evaluation, trajectory generatio...|$|R
40|$|This is a publisher’s {{version of}} an article {{published}} in Annals of Otology, Rhinology & Laryngology published by Annals Publishing Company. This version is reproduced with permission from Annals Publishing Company. [URL] processors extracting either the fundamental frequency (F 0) alone, or the fundamental frequency combined with second formant information (F 0 -F 2), have been evaluated on a totally deaf patient using a multiple-channel cochlear implant. A closed set test using 16 spondees and a modified <b>rhyme</b> <b>test</b> showed that for electrical stimulation alone the F 0 -F 2 speech processor was significantly better than the F 0 processor. The open set tests using phonetically balanced words and Central Institute for the Deaf everyday sentences showed that for electrical stimulation alone and electrical stimulation combined with lipreading, the results with the F 0 -F 2 speech processor were all significantly better than with the F 0 processor. Information transmission for consonant speech features was also better when using the F 0 -F 2 processor. Open Acces...|$|R
40|$|The {{published}} Chinese diagnostic <b>rhyme</b> <b>test</b> (CDRT) {{has been}} proposed as a standard method for evaluating the intelligibility of coded, synthesised or distorted Chinese speech over wired or wireless voice channels in a communication system. Inheriting {{the philosophy of the}} English context DRT test, the CDRT only assessed six groups of phonetic attribute distortion. However, pitch distortion is a highly important element for evaluating intelligibility of Chinese speech, which is characterised by Chinese tones, an important element of Chinese language which is absent in western languages. An extension to the CDRT named 'CDRT-tone' for the intelligibility test of Chinese tones is proposed. The relative features of Chinese tones are outlined and the principles of choosing phonetic units for the proposed standard, CDRT-tone, are presented. Then the validity of the CDRT-tone is discussed. Finally, CDRT-tone was applied to evaluate the pitch performance of the GSM regular pulse excitation with a long-term prediction (RPE-LTP) coding algorithm...|$|R
30|$|The {{modified}} <b>rhyme</b> <b>test</b> (MRT) [33] {{was used}} for subjective and realistic evaluation of the speech intelligibility in a noise and reverberation environment. The MRT database contains a total of 2700 audio source files, including five males and four females reading 300 words. The 300 words read by each person are divided into 50 six-word groups of rhyming or similar-sounding English words, such as “same,” “name,” “game,” “tame,” “came,” and “fame.” Each word is a monosyllable of the form consonant-vowel-consonant (CVC), and the six words in each list differ in only the leading or trailing consonant. In this listening test, a total of 640 audio source files (4 RTs × 4 SNRs ×[*] 4 algorithms ×[*] 10 groups) {{were randomly selected from}} the database, modified using the four different of methods, and degraded by factory noise-II at SNRs of −[*] 10, −[*] 5, 0, and 5  dB in different reverberation conditions. This procedure was performed in the experiment described in Section 3, and the processed audio files were recorded by a laptop as test speech for the subjective evaluation.|$|R
40|$|HMM-based {{synthesized}} {{voices are}} intelligible but not natural especially in limited data condition because of over smoothing speech spectra in time-frequency domain. Improving naturalness {{is a critical}} problem of HMM-based speech synthesis. One solution for the problem is using voice conversion techniques to convert over-smoothed spectra to natural spectra. Although conventional conversion techniques transform speech spectra to natural ones to improve naturalness, they cause unexpected distortions on acceptable intelligibility of synthesized speech. The aim of the paper is to improve naturalness without violating intelligibility of synthesized speech employing an asymmetric bilinear model (ABM) to separate intelligibility and naturalness. In the paper, an ABM was implemented on modulation spectrum domain of Mel-cepstral coefficient (MCC) sequence to enhance fine structure of spectral parameter trajectory generated from HMMs. Subjective evaluations carried out on English data confirm that the achieved naturalness of proposed method is competitive with other methods in large data condition and outperform other methods in limited data condition. Moreover, modified <b>rhyme</b> <b>test</b> (MRT) shows that acceptable intelligibility of synthesized speech is well-preserved with proposed method...|$|R
40|$|Hidden Markov model (HMM) -based {{synthesized}} {{voices are}} intelligible but not natural especially under limited-data conditions due to over-smoothed speech spectra. Improving naturalness {{is a critical}} problem of HMM-based speech synthesis. One solution is to use voice conversion techniques to convert over-smoothed spectra to natural spectra. Although conventional conversion methods transform speech spectra to natural ones to improve naturalness, they cause unexpected distortions in the intelligibility of synthesized speech. The aim {{of the study is}} to improve naturalness without reducing the intelligibility of synthesized speech by employing our novel asymmetric bilinear model (ABM) to separate the intelligibility and naturalness of synthesized speech. In the study, our ABM was implemented on the modulation spectrum domain of Mel-cepstral coefﬁcient (MCC) sequences to enhance the ﬁne structure of spectral parameter trajectory generated from HMMs. Subjective evaluations carried out on English data conﬁrmed that the achieved naturalness of the method using the ABM involving singular value decomposition (SVD) was competitive with other methods under large-data conditions and outperformed other methods under limited-data conditions. Moreover, modiﬁed <b>rhyme</b> <b>test</b> (MRT) showed that the intelligibility of synthesized speech was well preserved with our method...|$|R
40|$|ICSLP 2000 : the 6 th International Conference on Spoken Language Processing, October 16 - 20, 2000, Beijing, China. The goal of {{this paper}} is to locate and {{understand}} the fine fundamental problems that exist in the representation of speech sounds by a very high quality speech analysis/synthesis engine namely STRAIGHT. The approach followed here is the evaluation of this system using subjective measures. We use the diagnostic <b>rhyme</b> <b>test</b> (DRT) to evaluate the intelligibility of speech analysed and synthesised by this system for various analysis frame-rates. Consequently we catagorise the fine problems and suggest possible improvements. The results from the DRT have indicated that STRAIGHT can produce speech with an average DRT score of 95 between 1 - 5 ms analysis frame-rate. In addition, a set of subjective quality measures using MOS and MNRU tests have been conducted. These tests have been carried out for three different versions of the STRAIGHT system: versions 17, 23 and 30. The DRT has been carried out using version 23 only. Based on the subjective evaluation results, a discussion of possible improvements to the STRAIGHT system is given...|$|R
50|$|In 1967, Carroll and Sapon {{authored}} the Modern Language Aptitude Test - Elementary (EMLAT; more recently, MLAT-E). This was {{an adaptation}} of the adult version of the MLAT intended for younger students (grades 3 through 6). The MLAT-E is broken down into four parts, three of which are modified versions of the MLAT’s Part 3 - Hidden Words, Part 4 - Words in Sentences and Part 1 - Number Learning. It also includes a new section called Finding <b>Rhymes,</b> which <b>tests</b> the subject’s ability to hear speech sounds.|$|R
40|$|BACKGROUND: Patients with Parkinson's disease {{commonly}} {{suffer from}} speech and voice difficulties such as impaired articulation and reduced loudness. Speech and language therapy (SLT) aims {{to improve the}} intelligibility of speech with behavioural treatment techniques or instrumental aids. OBJECTIVES: To compare the efficacy and effectiveness of novel SLT techniques versus a standard SLT approach to treat Parkinsonian speech problems. SEARCH METHODS: We identified relevant, published prior to 11 (th) April 2011, by electronic searches of numerous literature databases including CENTRAL, MEDLINE and CINAHL, as well as handsearching relevant conference abstracts and examining reference lists in identified studies and other reviews. SELECTION CRITERIA: Only randomised controlled trials (RCT) of one type of {{speech and language therapy}} versus another were included. DATA COLLECTION AND ANALYSIS: Two review authors independently extracted data and resolved differences by discussion. MAIN RESULTS: Six trials involving 159 patients satisfied the inclusion criteria. Data could not be analysed from one trial due to changes in patient numbers and from a second because the data provided were not in a usable format. All trials reported intelligibility measures but a statistically significant result was only reported for the diagnostic <b>rhyme</b> <b>test</b> used in the study of Lee Silverman Voice Treatment -LOUD (LSVT-LOUD) versus a modified version of this therapy (LSVT-ARTIC). In this case a difference of 12. 5 points (95...|$|R
40|$|This study aims to {{investigate}} whether unbalanced Chinese-English bilingual children’s phonological awareness skills are limited to language experience, and whether these skills are improved after {{a short period of}} articulation training with L 2 (English) tongue twisters. Sixty kindergarten children in Taipei whose English proficiency was lower than their mother tongue, Chinese, participated in a series of tests. They were divided into two age groups with an average age of 5; 3 and 6; 3 respectively. An English proficiency test was first administrated to understand these children’s command of English. Then, phonological awareness pre-tests in Chinese and English were used to tap these children’s phonological awareness in both languages. <b>Tests</b> include onset/ <b>rhyme</b> detection <b>test,</b> onset deletion <b>test,</b> and onset/ <b>rhyme</b> substitution <b>tests.</b> Based on the causal link between articulation and phonological awareness, an English articulation training was given to the experimental group of children after the pre-tests, to examine whether enhanced English phonological awareness skills transfer to Chinese. Results showed that phonological awareness acquired in L 1 were also found in L 2. A period of articulation training in English led to an improvement of these children’s performances in both English and Chinese, which implies a backward transfer from weaker L 2 to stronger L 1. Cross-language transfers in phonological awareness abilities also imply that an abstract underlying capacity facilitates language processing across languages...|$|R
40|$|Speech {{intelligibility}} of {{two types}} of vocoders was measured using the modified <b>rhyme</b> <b>test.</b> One type of vocoder, a continuous variable slope delta (CVSD), was a waveform encoder. The other type, an advanced multi-band excitation (AMBE), was a parametric encoder. In the first experiment, clear speech was processed through the vocoders. Intelligibility was measured in a control condition, i. e. without vocoding, with each type alone and with two vocoders in tandem. AMBE and CVSD performed similarly, 92. 6 and 90. 4 %, respectively. CVSD-to-AMBE {{had little effect on}} intelligibility, measured at 89. 2 %. However, AMBEto-CVSD had a large degrading effect on intelligibility. The AMBE-to-CVSD direction scored about 81. 7 % intelligibility with clear, unaltered speech signals. The asymmetry between waveform-to-parametric and parametric-to-waveform encoders underscores the non-linear nature of tandem vocoders on intelligibility. When vocoders of the same type were in tandem, there was no additional effect on intelligibility. The double CVSD condition yielded 92. 2 % intelligibility and the double AMBE condition yielded 91 %. The deleterious effects of speech clipping were measured in a second experiment, as these are ubiquitous in military radio transmission systems. The AMBE parametric vocoder performed at the 88 % level in isolation and at 84 % when tandemed with the CVSD waveform vocoder. Alternative methods of encoding speech signals are being explored to improve speech intelligibility performance in military communication systems...|$|R
40|$|The {{recruitment}} phenomenon, that is, {{the reduced}} dynamic range between threshold and uncomfortable level, {{is attributed to}} the loss of instantaneous dynamic compression on the basilar membrane. Despite this, hearing aids commonly use slow-acting dynamic compression for its compensation, because this was found to be the most successful strategy in terms of speech quality and intelligibility rehabilitation. Former attempts to use fast-acting compression gave ambiguous results, raising the question as to whether auditory-based recruitment compensation by instantaneous compression is in principle applicable in hearing aids. This study thus investigates instantaneous multiband dynamic compression based on an auditory filterbank. Instantaneous envelope compression is performed in each frequency band of a gammatone filterbank, which provides a combination of time and frequency resolution comparable to the normal healthy cochlea. The gain characteristics used for dynamic compression are deduced from categorical loudness scaling. In speech intelligibility tests, the instantaneous dynamic compression scheme was compared against a linear amplification scheme, which used the same filterbank for frequency analysis, but employed constant gain factors that restored the sound level for medium perceived loudness in each frequency band. In subjective comparisons, five of nine subjects preferred the linear amplification scheme and would not accept the instantaneous dynamic compression in hearing aids. Four of nine subjects did not perceive any quality differences. A sentence intelligibility test in noise (Oldenburg sentence test) showed little to no negative effects of the instantaneous dynamic compression, compared to linear amplification. A word intelligibility test in quiet (one-syllable <b>rhyme</b> <b>test)</b> showed that the subjects benefit from the larger amplification at low levels provided b...|$|R
40|$|One of {{the most}} {{significant}} current discussions has led to the hypothesis that domain-specific training programs alone are not enough to improve reading achievement or working memory abilities. Incremental or Entity personal conceptions of intelligence may be assumed to be an important prognostic factor to overcome domain-specific deficits. Specifically, incremental students tend to be more oriented toward change and autonomy and are able to adopt more efficacious strategies. This study aims at examining the effect of personal conceptions of intelligence to strengthen the efficacy of a multidimensional intervention progrm in order to improve decoding abilities and working memory. Participants included two children (M age= 10 years) with developmental dyslexia and different conceptions of intelligence. The children were tested on a whole battery of reading and spelling tests commonly used in the assessment of reading disabilities in Italy. Afterwards, they were given a multimedia test to measure motivational factors such as conceptions of intelligence and achievement goals. The children took part in the T. I. R. D. Multimedia Training for the Rehabilitation of Dyslexia (Rappo and Pepi, 2010) reinforced by specific units to improve verbal working memory for 3 months. This training consisted of specific tasks to rehabilitate both visual and phonological strategies (sound blending, word segmentation, alliteration <b>test</b> and <b>rhyme</b> <b>test,</b> letterr ecognition, digraph recognition,trigraph recognition, and word recognition as samples of visual tasks) and verbal working memory (rapid words and non- words recognition). Posttest evaluations showed that the child holding the incremental theory of intelligence improved more than the child holding a static representaton. On the whole this study highlights the importance of treatment programs in which both specificity of deficits and motivational factors are both taken into account. There is a need to plan multifaceted intervention programs based on a transverse approach, considering both cognitive and motivational factors...|$|R
