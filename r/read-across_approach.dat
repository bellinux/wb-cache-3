10|25|Public
30|$|In summary, {{the overall}} REACH {{criteria}} for applying a <b>read-across</b> <b>approach</b> are met.|$|E
40|$|The current {{international}} guidance {{documents for}} performing a group-based {{assessment of the}} possible risks caused by chemical substances with comparable structures need further elucidation {{if they are to}} be properly used. This was the result of RIVM research on the workability of guidance documents. This research was prompted by the upcoming European legislation on production, trade and use of chemical substances (REACH), which will come into force in mid- 2007. This legislation stipulates that about 30, 000 chemical substances are to be assessed on their possible risks. Several non-animal methods such as QSARs, in vitro methods and the category or <b>read-across</b> <b>approach</b> have been developed to assess this large number of chemicals. The physico-chemical and toxicological properties are only known for a minority of all chemical substances. These may for example include skin irritation, water solubility and degradation in the environment. The category or <b>read-across</b> <b>approach</b> uses available substance information to be able to assess chemical substances with comparable structures for which only few data are available. The current guidance document for this approach can be used as a basis, but several points still need more attention in the further development of the guidance document for REACH. One point of particular interest is establishing a clear definition of the categories for use in the <b>read-across</b> <b>approach</b> to avoid comparing unequal data. Data substantiated and clearly presented for this approach highly define the usefulness of this new system...|$|E
40|$|Toxicity of {{chemicals}} is traditionally assessed using animal studies. For reasons of ethics, economy and legislation predictive alternatives {{have to be}} used, whenever possible. Nevertheless, the development of alternatives to animal testing has lagged behind. One alternative in risk assessment {{of chemicals}} is read-across. Here, toxicity data of a tested compound is used to predict the toxicity of a "similar" non-tested compound. As {{one of the main}} routes of exposure to chemicals is the inhalation route, our project combines the <b>read-across</b> <b>approach</b> with genomic data of in vitro and ex vivo respiratory assays. First, we selected three categories of chemicals (vicinale halogenide, aromates, and ester) and defined one lead compound and two read-across in each category. Categories were chosen based on potentially different mode of action. Secondly, repeated exposure of chemicals was performed on three days for three hours daily in rat and human precision-cut lung slices (PCLS). The cytotoxicity of chemicals was assessed in PCLS by LDH and WST- 1 assay. At least one substance in each category showed cytotoxic effect in dose dependent manner. The chemicals did not show increased sensitivity upon repeated exposure. Human PCLS were less sensitive to chemicals compared to rat PCLS maybe due to differences in metabolic activity. This study shows differences of rodent and human metabolism. Although toxicity profiling is based on rodent exposure data, species diversity can be studied ex vivo for better predictively in human. Moreover, results of the project will be used to evaluate the <b>read-across</b> <b>approach</b> based on the tested chemicals...|$|E
30|$|Furthermore, the {{grouping}} of substances and <b>read-across</b> <b>approaches</b> were developed for filling data gaps in registrations of chemicals. <b>Read-across</b> <b>approaches</b> utilize weight-of-evidence approaches {{to make use}} of shared properties of an untested compound to a known compound. While sharing many characteristics of a QSAR, <b>read-across</b> <b>approaches</b> do not seek a mathematical formula for larger parts of the chemical universe but instead are based on “local” similarity and shared properties of chemicals [36, 37]. In addition to the similarity in the structure and physicochemical properties, biological data are also used to compare biological similarity among chemicals in <b>read-across</b> <b>approaches.</b> Good <b>Read-Across</b> Practices (GRAPs) were created and developed by the European Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH) legislation {{as a result of the}} broad use of <b>read-across</b> <b>approaches</b> and the need to establish standards [36, 38]. The GRAP collaboration formed to further this approach and addresses aspects such as regulatory acceptability, the use of biological support data [37], and the applicability to nanomaterials or complex mixtures. The major advantage is that such an approach can actually be formally validated and uncertainties with any prediction can be quantified. The emergence of professional tools and services promises a much broader use of computational approaches both for REACH registration and other similar legislations worldwide, as well as for Green Toxicology practices.|$|R
30|$|Since {{hydrophobicity}} {{does not}} drive all bioaccumulation processes, other information {{has to be}} considered, e.g. surface activity, structural alerts, high log KOA (octanol/air partition coefficient) {{as an indicator of}} a possible bioaccumulation in air-breathing organisms or <b>read-across</b> <b>approaches</b> from structurally related substances.|$|R
40|$|ABSTRACT: Traditional <b>read-across</b> <b>approaches</b> {{typically}} rely on {{the chemical}} similarity principle to predict chemical toxicity; however, the accuracy of such predictions is often inadequate due to the underlying complex mechanisms of toxicity. Here, {{we report on the}} development of a hazard classification and visualization method that draws upon both chemical structural similarity and comparisons of biological responses to chemicals measured in multiple short-term assays (“biological ” similarity). The Chem-ical−Biological <b>Read-Across</b> (CBRA) <b>approach</b> infers each compound’s toxicity from both chemical and biological analogues whose similarities are determined by the Tanimoto coefficient. Classification accuracy of CBRA was compared to that of classical RA and other methods using chemical descriptors alone or in combination with biological data. Different types of adverse effects (hepatotoxicity, hepatocarcinogenicity, mutagenicity, and acute lethality) were classified using several biological data types (gene expression profiling and cytotoxicity screening). CBRA...|$|R
40|$|Zebrafish embryos {{were exposed}} to {{different}} organotin compounds during very early development (< 100. h post fertilization). Morphology, histopathology and swimming activity (in a motor activity test) were the endpoints analyzed. DBTC was, by far, the most embryotoxic compound at all time points and endpoints studied. In fact, we observed a clear concordance between the effects observed in our zebrafish embryo model, and those observed with these compounds in full rodent in vivo studies. All organotin compounds classified as developmental (neuro) toxicants in vivo, were correctly classified in the present assay. Together, our results support the ZET model as a valuable tool for providing biological verification for a grouping and a <b>read-across</b> <b>approach</b> to developmental (neuro) toxicity. © 2013 Elsevier Inc...|$|E
40|$|Background: Methods {{that provide}} a measure of {{chemical}} similarity are strongly relevant in several fields of chemoinformatics as they allow to predict the molecular behavior and fate of structurally close compounds. One common application of chemical similarity measurements, {{based on the principle}} that similar molecules have similar properties, is the <b>read-across</b> <b>approach,</b> where an estimation of a specific endpoint for a chemical is provided using experimental data available from highly similar compounds. Results: This paper reports the comparison of multiple combinations of binary fingerprints and similarity metrics for computing the chemical similarity in the context of two different applications of the read-across technique. Conclusions: Our analysis demonstrates that the classical similarity measurements can be improved with a generalizable model of similarity. The proposed approach has already been used to build similarity indices in two open-source software tools (CAESAR and VEGA) that make several QSAR models available. In these tools, the similarity index plays a key role for the assessment of the applicability domain. Pubblicat...|$|E
40|$|Here {{we present}} and {{evaluate}} {{a framework for}} estimating concentrations of pharmaceuticals over time in wildlife feeding at wastewater treatment plants (WWTPs). The framework is composed {{of a series of}} predictive steps involving the estimation of pharmaceutical concentration in wastewater, accumulation into wildlife food items, uptake by wildlife with subsequent distribution into, and elimination from, tissues. As many pharmacokinetic parameters for wildlife are unavailable for the majority of drugs in use, a <b>read-across</b> <b>approach</b> was employed using either rodent or human data on absorption, distribution, metabolism and excretion (ADME). Comparison of the different steps in the framework, against experimental data for the scenario where birds are feeding on a WWTP contaminated with fluoxetine, showed that: estimated concentrations in wastewater treatment works were lower than measured concentrations; concentrations in food could be reasonably estimated if experimental bioaccumulation data are available; and that read-across from rodent data worked better than human to bird read-across. The framework provides adequate predictions of plasma concentrations and of elimination behavior in birds, but yields poor predictions of distribution in tissues. We believe the approach holds promise, but {{it is important that we}} improve our understanding of the physiological similarities and differences between wild birds and domesticated laboratory mammals used in pharmaceutical efficacy/safety trials, so that the wealth of data available can be applied more effectively in ecological risk assessments...|$|E
40|$|A {{round-robin}} {{exercise was}} conducted within the CALEIDOSLIFE project. The participants {{were invited to}} assess the hazard posed by a substance, applying in silico methods and <b>read-across</b> <b>approaches.</b> The exercise was based on three endpoints: mutagenicity, bioconcentration factor and fish acute toxicity. Nine chemicals were assigned for each endpoint and the participants were invited to complete a specific questionnaire communicating their conclusions. The interesting aspect of this exercise is the justification behind the answers more than the final prediction in itself. Which tools were used? How did the approach selected affect the final answer...|$|R
30|$|A minimum tonnage of the {{manufactured}} or imported {{substance is}} necessary to trigger registration (1 t/a). The extent of mandatory information is reduced for on-site isolated (OSII) and transported isolated intermediates (TII). The toxicological information requirements increase with increasing tonnage bands only. From an occupational point of view, the profile of use of a substance and information about high exposures and wide dispersive professional uses would serve as more effective triggers. <b>Read-across</b> <b>approaches</b> and waiving of studies can also be justified. The registration requirements of REACH exclude chemical agents not intentionally produced or those present in the workplace for reasons not directly associated with occupation [7].|$|R
40|$|EC FP 7 NANoREG (Grant Agreement NMP 4 -LA- 2013 - 310584) Free PMC Article: [URL] {{the growing}} numbers of {{nanomaterials}} (NMs), {{there is a great}} demand for rapid and reliable ways of testing NM safety—preferably using in vitro approaches, to avoid the ethical dilemmas associated with animal research. Data are needed for developing intelligent testing strategies for risk assessment of NMs, based on grouping and <b>read-across</b> <b>approaches.</b> The adoption of high throughput screening (HTS) and high content analysis (HCA) for NM toxicity testing allows the testing of numerous materials at different concentrations and on different types of cells, reduces the effect of inter-experimental variation, and makes substantial savings in time and cost. info:eu-repo/semantics/publishedVersio...|$|R
40|$|The {{number and}} variety of {{engineered}} nanoparticles have been growing exponentially. Since the experimental evaluation of nanoparticles causing public health concerns is expensive and time consuming, efficient computational tools are amongst the most suitable approaches to identifying potential negative impacts, to the {{human health and the}} environment, of new nanomaterials before their production. However, developing computational models complimentary to experiments is impossible without incorporating consistent and high quality experimental data. Although there are limited available data in the literature, one may apply read-across techniques that seem to be an attractive and pragmatic alternative way of predicting missing physico-chemical or toxicological data. Unfortunately, the existing methods of read-across are strongly dependent on the expert's knowledge. In consequence, the results of estimations may vary dependently on personal experience of expert conducting the study and as such cannot guarantee the reproducibility of their results. Therefore, it is essential to develop novel read-across algorithm(s) that will provide reliable predictions of the missing data without the need to for additional experiments. We proposed a novel quantitative <b>read-across</b> <b>approach</b> for nanomaterials (Nano-QRA) that addresses and overcomes a basic limitation of existing methods. It is based on: one-point-slope, two-point formula, or the equation of a plane passing through three points. The proposed Nano-QRA approach is a simple and effective algorithm for filling data gaps in quantitative manner providing reliable predictions of the missing data. © The Royal Society of Chemistry...|$|E
40|$|SEURAT- 1 is a European {{public-private}} research consortium that {{is working}} towards animal-free testing of chemical compounds {{and the highest}} level of consumer protection. A research strategy was formulated around harnessing knowledge about toxicological modes-of-action and an organisational model was developed that combines crowd-sourcing with individual excellence. The proof of the initiative will be in demonstrating the applicability of the concepts on which SEURAT- 1 is built. This is done on three levels: (i) Theoretical prototypes for adverse outcome pathways are formulated based on knowledge already available in the scientific literature on investigating the toxicological mode-of-actions leading to adverse outcomes; (ii) adverse outcome pathway descriptions are used as a guide for the formulation of case studies to further elucidate the theoretical model and to develop integrated testing strategies for the prediction of certain toxicological effects; (iii) further case studies target the application of knowledge gained within SEURAT- 1 in the context of safety assessment. The ultimate goal would be to perform ab initio predictions based on a complete understanding of toxicological mechanisms. In the near-term it is more realistic that data from innovative testing methods will support read-across arguments. Both scenarios, the ab initio prediction and the <b>read-across</b> <b>approach,</b> are addressed with respective case studies for improved safety assessment. A conceptual framework for a rational integrated assessment strategy emerged from the efforts of designing the case studies and is discussed in the context of international developments focussing on alternative approaches for evaluating chemicals using the new 21 st century tools for toxicity testing. JRC. I. 5 -Systems Toxicolog...|$|E
40|$|AbstractThe aquatic {{environment}} is polluted {{with thousands of}} chemicals. It is currently unclear which of these pose a significant threat to aquatic biota. The typical exposure scenario is now represented by a widespread blanket of contamination composed of myriads of individual pollutants—each typically present at a low concentration. The synthetic steroids, 17 α-ethinylestradiol and levonorgestrel, have been widely reported {{to be present in}} the {{aquatic environment}} in the low ng to sub-ng/l range. They are widely used in contraceptive formulations, both individually and in combination. Our research employed the fathead minnow (Pimephales promelas) 21 day ‘pair-breeding’ assay to assess reproductive output when pairs of fish were exposed to the single chemicals at low environmentally relevant concentrations, and then to a binary mixture of them. A variety of endpoints were assessed, including egg production, which was inhibited in a concentration-dependent manner by both the individual chemicals and the mixture. Significant, sex specific effects were also seen with both chemicals, at differing levels of biological organisation. Plasma concentrations of EE 2 and levonorgestrel were predicted {{and in the case of}} levonorgestrel measured, and compared with the human therapeutic plasma concentrations (<b>Read-Across</b> <b>approach)</b> to support the interpretation of the results. A novel quantitative method was developed for the data analysis, which ensured a suitable endpoint for the comparative mixture assessment. This approach compares the reproductive performance from individual pairs of fish during chemical exposure to its pre-treatment performance. The responses from the empirical mixture study were compared to predictions derived from the single substance data. We hypothesised combined responses which were best described by the concept of concentration addition, and found no clear indications against this additivity expectation. However, the effect profiles support the current knowledge that both compounds act in different ways to reduce egg production in fish, and suggest that probably response addition (also called Independent action) is the more appropriate mixture model in this case...|$|E
40|$|Physicochemical {{properties}} of chemicals affect their exposure, toxicokinetics/fate and hazard, and for nanomaterials, the variation of these properties {{results in a}} wide variety of materials with potentially different risks. To limit the amount of testing for risk assessment, the information gathering process for nanomaterials needs to be efficient. At the same time, sufficient information to assess the safety of human health and the environment should be available for each nanomaterial. Grouping and <b>read-across</b> <b>approaches</b> can be utilised to meet these goals. This article presents different possible applications of grouping and read-across for nanomaterials within the broader perspective of the MARINA Risk Assessment Strategy (RAS), as developed in the EU FP 7 project MARINA. Firstly, nanomaterials can be grouped based on limited variation in physicochemical properties to subsequently design an efficient testing strategy that covers the entire group. Secondly, knowledge about exposure, toxicokinetics/fate or hazard, for example via properties such as dissolution rate, aspect ratio, chemical (non-) activity, can be used to organise similar materials in generic groups to frame issues that need further attention, or potentially to read-across. Thirdly, when data related to specific endpoints is required, read-across can be considered, using data from a source material for the target nanomaterial. Read-across could be based on a scientifically sound justification that exposure, distribution to the target (fate/toxicokinetics) and hazard of the target material are similar to, or less than, the source material. These grouping and <b>read-across</b> <b>approaches</b> pave the way for better use of available information on nanomaterials and are flexible enough to allow future adaptations related to scientific developments. JRC. I. 4 -Nanobioscience...|$|R
30|$|Based on {{the recent}} experience, reasons for {{non-compliance}} can be various. One example of non-compliance with legal provisions is that the exemptions from the registration requirements are not correctly understood. A very common {{problem is that the}} identity of the substance under consideration is often not clear or even incorrect [10]. Furthermore, the definition of intermediates and the term ‘strictly controlled conditions’ (SCC) is interpreted differently between the authorities and some companies [11]. To avoid unjustified toxicological tests, <b>read-across</b> <b>approaches</b> are made or certain toxicological tests are waived for unjustified reasons. Furthermore, the compilation of registration dossiers is performed very differently by each SIEF and different registrants may comply differently in a way not readily understandable for competent authorities or the downstream users. Non-compliance with the legal framework will occur when these assumptions, and estimations are not disclosed and/or not justified and when the technical dossier and the CSR are inconsistent. A non-exhaustive summary of these discrepancies is summarised in column 3 of Additional file 1.|$|R
40|$|The use of {{so-called}} “in chemico” methodology - abiotic assays that measure chemical reactivity - is gaining ground as relevant and reliable means of toxicity prediction. In this report we explain {{the basis of}} the in chemico approach to toxicity prediction and we review the studies that have developed the concept and its practical application since the 1930 s, with special attention being paid to studies aimed at the development of Quantitative Structure-Activity Relationship (QSAR) models and <b>read-across</b> <b>approaches.</b> The studies covered in this review are limited to non-enzymatic experiments and to nucleophiles up to 50 amino acids. The main applications identified are related to the assessment of skin sensitisation, aquatic toxicity and hepatotoxicity. Various experimental measures of nucleophile depletion or adduct formation have been proposed as chemical reactivity descriptors, but no single protocol has emerged as the most generally useful. It is concluded that in chemico approaches provide a promising means of toxicity prediction within their applicability domains and should be further developed and investigated as alternative methods to animal testing, especially when used in the context of integrated testing strategies based on the use of multiple non-animal methods. JRC. DG. I. 6 -Systems toxicolog...|$|R
40|$|Nanomaterials are {{currently}} regulated essentially under the chemicals legislations {{in the different}} OECD countries, possibly with additional data required in specific areas of legislation within a country. The OECD concluded, {{based on the findings}} of the OECD Working Party on Manufactured Nanomaterials, that the methods and approaches developed for managing the safety of chemicals can by and large be applied to nanomaterials, though paying special attention to a number of issues, for example, sample preparation. Nanomaterials, due to their specific properties which may change during their life cycle from cradle-to-grave, pose specific challenges to the risk assessment. A systematic testing of all different forms of NMs in all use/release scenarios is impossible as it would be too time consuming and costly. In addition, modern toxicology requires the reduction and optimization of in vivo tests (especially in higher vertebrates) and recommends in vitro and in silico models in integrated (intelligent) testing approaches. As the development and validation of in vitro and in silico assays for NMs still lags behind conventional chemicals, currently the scaling, grouping, and <b>read-across</b> <b>approach</b> to existing information from different forms (bulk or other nanoforms) seem to be the most promising approach and frameworks have been proposed by some stakeholders. Any hypothesis for grouping and read-across needs to be supported and confirmed by data, for which some testing (in a tiered testing strategy) may still be required. (Occupational) inhalation of NMs appears to be the main focus area in relation to the risk (and thereby hazard and exposure) of NMs. Test guidelines for inhalation toxicity are thus attracting specific attention and considerations are made in relation to increasing the number of endpoints to be examined in such guideline studies. At the same time, several efforts are made to model exposure to NMs via different routes by in vitro assays as realistically as possible by testing them in different and more complex cell models and exposure types. Also sample preparation and dosimetry, including characterization of the NMs during the study, is of outmost importance. Similarly, for exposure estimation, most attention is given to the potential for inhaling airborne NMs. Exposure models are not yet validated for NMs, and models for inhalation exposure seem in particular to need further re-development before being applicable to NM. Additional efforts are needed if other metrics than mass are relevant and should be predicted. In relation to measurements, a suite of techniques are available, from relatively cheap particle counters giving quick, but rough and not specific results, to expensive off-line analysis of samples, which can provide insight into elemental composition and particle characteristics. Thus, as for hazards, intelligent exposure assessment strategies are needed. Further, the overall exposure database is very scarce compared to hazard investigations and most exposure data relate to upstream NM’s manufacturing and laboratory work. Risk assessment needs to address the challenges of handling data from alternative methods and the potentially higher uncertainty associated, due to fewer data or lower reliability. In principle, current risk assessment frameworks provide sufficient flexibility to use data from alternative methods; however, further experience and consensus building is needed in relation to NMs. Weight of evidence approaches based on expert judgment are being suggested in risk assessment at different levels and could be extended with an increased level of confidence and validation. While appropriate methods for hazard/exposure and risk characterization are still under development and approval, {{it is important to consider}} and manage the potential risk of NMs already in their development phase. Control banding is one such ways to manage risk of nanomaterials, “safety by design” can contribute to a lower risk from both the hazard side by considering those properties that most likely contribute to biological effects, and from the exposure side by minimizing or excluding exposure. JRC. F. 2 -Consumer Products Safet...|$|E
30|$|Green Toxicology {{offers many}} {{advantages}} in the practices {{and application of}} Green Chemistry, which are discussed in this paper. The Green Toxicology principles outlined by Maertens et al. [1] and Fig.  1 {{provide a framework for}} designing chemicals that are safer for humans and the environment by utilizing new and innovative predictive toxicological tools and strategies. This paper discusses some of the aspects of Green Toxicology with respect to improving the integration of Green Toxicology with Green Chemistry practices to produce safer, less harmful products. The integration of new testing methods and strategies in product development, testing and regulation stages are presented with examples of the applications of in vitro, omic, and in silico methods. Other tools for Green Toxicology, including the reduction of animal testing, alternative test methods, and <b>read-across</b> <b>approaches</b> are also discussed. Examples of lessons that can be learned from past activities are also discussed with respect to reducing current and future risks (e.g. late lessons from early warning; precautionary principle; [16 – 18]). This paper also examines some of the stages of product development, regulation, use and disposal that can or have benefited from the incorporation of Green Toxicology practices. In addition, some of the most relevant aspects, advances and limitations of the emergence of Green Toxicology from the perspective of different industry and research groups will be discussed.|$|R
40|$|International audienceBiomonitoring using {{birds of}} prey as sentinelspecies has been mooted {{as a way to}} {{evaluate}} the success ofEuropean Union directives that are designed to protectpeople and the environment across Europe from industrialcontaminants and pesticides. No such pan-European evaluation currently exists. Coordination of such large scalemonitoring would require harmonisation across multiplecountries of the types of samples collected and analysedmatrices vary in the ease with which they can be collected and the information they provide. We report the first everpan-European assessment of which raptor samples arecollected across Europe and review their suitability forbiomonitoring. Currently, some 182 monitoring programmes across 33 European countries collect a variety ofraptor samples, and we discuss the relative merits of eachfor monitoring current priority and emerging compounds. Of the matrices collected, blood and liver are used mostextensively for quantifying trends in recent and longerterm contaminant exposure, respectively. These matricesare potentially the most effective for pan-European biomonitoring but are not so widely and frequently collected as others. We found that failed eggs and feathersare the most widely collected samples. Because of thisubiquity, they may provide the best opportunities forwidescale biomonitoring, although neither is suitable for allcompounds. We advocate piloting pan-European monitoring of selected priority compounds using these matricesand developing <b>read-across</b> <b>approaches</b> to accommodateany effects that trophic pathway and species differences inaccumulation may have on our ability to track environmental trends in contaminants...|$|R
30|$|Green Toxicology {{refers to}} the {{application}} of predictive toxicology in the sustainable development and production of new less harmful materials and chemicals, subsequently reducing waste and exposure. Built upon the foundation of “Green Chemistry” and “Green Engineering”, “Green Toxicology” aims to shape future manufacturing processes and safe synthesis of chemicals in terms of environmental and human health impacts. Being {{an integral part of}} Green Chemistry, the principles of Green Toxicology amplify the role of health-related aspects for the benefit of consumers and the environment, in addition to being economical for manufacturing companies. Due to the costly development and preparation of new materials and chemicals for market entry, it is no longer practical to ignore the safety and environmental status of new products during product development stages. However, this is only possible if toxicologists and chemists work together early on in the development of materials and chemicals to utilize safe design strategies and innovative in vitro and in silico tools. This paper discusses some of the most relevant aspects, advances and limitations of the emergence of Green Toxicology from the perspective of different industry and research groups. The integration of new testing methods and strategies in product development, testing and regulation stages are presented with examples of the application of in silico, omics and in vitro methods. Other tools for Green Toxicology, including the reduction of animal testing, alternative test methods, and <b>read-across</b> <b>approaches</b> are also discussed.|$|R
40|$|With {{the growing}} numbers of {{nanomaterials}} (NMs), {{there is a great}} demand for rapid and reliable ways of testing NM safety—preferably using in vitro approaches, to avoid the ethical dilemmas associated with animal research. Data are needed for developing intelligent testing strategies for risk assessment of NMs, based on grouping and <b>read-across</b> <b>approaches.</b> The adoption of high throughput screening (HTS) and high content analysis (HCA) for NM toxicity testing allows the testing of numerous materials at different concentrations and on different types of cells, reduces the effect of inter-experimental variation, and makes substantial savings in time and cost. HTS/HCA approaches facilitate the classification of key biological indicators of NM-cell interactions. Validation of in vitro HTS tests is required, taking account of relevance to in vivo results. HTS/HCA approaches are needed to assess dose- and time-dependent toxicity, allowing prediction of in vivo adverse effects. Several HTS/HCA methods are being validated and applied for NM testing in the FP 7 project NANoREG, including Label-free cellular screening of NM uptake, HCA, High throughput flow cytometry, Impedance-based monitoring, Multiplex analysis of secreted products, and genotoxicity methods—namely High throughput comet assay, High throughput in vitro micronucleus assay, and γH 2 AX assay. There are several technical challenges with HTS/HCA for NM testing, as toxicity screening needs to be coupled with characterization of NMs in exposure medium prior to the test; possible interference of NMs with HTS/HCA techniques is another concern. Advantages and challenges of HTS/HCA approaches in NM safety are discussed...|$|R
40|$|Read-across, i. e., filling {{toxicological}} data gaps by relating to similar chemicals for which test data are available, is usually done based on chemical similarity. Besides structure and physico-chemical properties, biological similarity based on biological data adds extra strength to this process. In the simplest case, chemically similar substances also show similar test results in relevant in vitro assays. This is a well-established method for the read-across of, e. g., genotoxicity assays. Larger datasets of biological and toxicological properties of hundreds {{and thousands of}} substances are becoming available, enabling big data <b>approaches</b> in <b>read-across</b> studies. In the context of developing Good Read-Across Practice guidance, a number of case studies using various big data sources were evaluated to assess the contribution of biological data to enriching read-across. An example is given for the US EPA's ToxCast dataset which allows read-across for high quality uterotrophic assays for estrogenic endocrine disruption. Similarly, an example is given for REACH registration data that enhances read-across for acute toxicity studies. A different approach is taken using omics data to establish biological similarity: Examples are given for in vitro stem cell models and short-term in vivo repeated dose studies in rats used to support read-across and category formation. These preliminary biological data-driven read-across studies show the way towards the generation of new <b>read-across</b> <b>approaches</b> that can inform chemical safety assessment. Funding Details: T 32 ES 007141, NIEHS, National Institute of Environmental Health Science...|$|R
40|$|Biomonitoring using {{birds of}} prey as {{sentinel}} species has been mooted {{as a way to}} evaluate the success of European Union directives that are designed to protect people and the environment across Europe from industrial contaminants and pesticides. No such pan-European evaluation currently exists. Coordination of such large scale monitoring would require harmonisation across multiple countries of the types of samples collected and analysed-matrices vary in the ease with which they can be collected and the information they provide. We report the first ever pan-European assessment of which raptor samples are collected across Europe and review their suitability for biomonitoring. Currently, some 182 monitoring programmes across 33 European countries collect a variety of raptor samples, and we discuss the relative merits of each for monitoring current priority and emerging compounds. Of the matrices collected, blood and liver are used most extensively for quantifying trends in recent and longer-term contaminant exposure, respectively. These matrices are potentially the most effective for pan-European biomonitoring but are not so widely and frequently collected as others. We found that failed eggs and feathers are the most widely collected samples. Because of this ubiquity, they may provide the best opportunities for widescale biomonitoring, although neither is suitable for all compounds. We advocate piloting pan-European monitoring of selected priority compounds using these matrices and developing <b>read-across</b> <b>approaches</b> to accommodate any effects that trophic pathway and species differences in accumulation may have on our ability to track environmental trends in contaminants. </p...|$|R
40|$|International audienceWith {{the growing}} numbers of {{nanomaterials}} (NMs), {{there is a great}} demand for rapid and reliable ways of testing NM safety—preferably using in vitro approaches, to avoid the ethical dilemmas associated with animal research. Data are needed for developing intelligent testing strategies for risk assessment of NMs, based on grouping and <b>read-across</b> <b>approaches.</b> The adoption of high throughput screening (HTS) and high content analysis (HCA) for NM toxicity testing allows the testing of numerous materials at different concentrations and on different types of cells, reduces the effect of inter-experimental variation, and makes substantial savings in time and cost. HTS/HCA approaches facilitate the classification of key biological indicators of NM-cell interactions. Validation of in vitro HTS tests is required, taking account of relevance to in vivo results. HTS/HCA approaches are needed to assess dose- and time-dependent toxicity, allowing prediction of in vivo adverse effects. Several HTS/HCA methods are being validated and applied for NM testing in the FP 7 project NANoREG, including Label-free cellular screening of NM uptake, HCA, High throughput flow cytometry, Impedance-based monitoring, Multiplex analysis of secreted products, and genotoxicity methods—namely High throughput comet assay, High throughput in vitro micronucleus assay, and γH 2 AX assay. There are several technical challenges with HTS/HCA for NM testing, as toxicity screening needs to be coupled with characterization of NMs in exposure medium prior to the test; possible interference of NMs with HTS/HCA techniques is another concern. Advantages and challenges of HTS/HCA approaches in NM safety are discussed. For further resources related to this article, please visit the WIREs website...|$|R
40|$|Asthma is {{commonly}} treated with inhalable glucocorticosteroids, including beclomethasone dipropionate (BDP). This is a synthetic prodrug which is metabolized {{to the more}} active monopropionate (BMP) and free beclomethasone in humans. To evaluate potential effects of residual drugs on fish, we conducted a 14 day flow-through exposure experiment with BDP and beclomethasone using rainbow trout, and analyzed effects on plasma glucose, hepatic glutathione and catalase activity together with water and body concentrations of the BDP, BMP and beclomethasone. We also analyzed hepatic gene expression in BDP-exposed fish by microarray and quantitative PCR. Beclomethasone (up to 0. 65 μg/L) was not taken up in the fish while BDP (0. 65 and 0. 07 μg/L) resulted in accumulation of both beclomethasone, BMP and BDP in plasma, reaching levels up {{to those found in}} humans during therapy. Accordingly, exposure to 0. 65 μg/L of BDP significantly increased blood glucose as well as oxidized glutathione levels and catalase activity in the liver. Exposure to beclomethasone or the low concentration of BDP had no effect on these endpoints. Both exposure concentrations of BDP resulted in significantly higher transcript abundance of phosphoenolpyruvate carboxykinase involved in gluconeogenesis, and of genes involved in immune responses. As only the rapidly metabolized prodrug was potent in fish, the environmental risks {{associated with the use of}} BDP are probably small. However, the observed physiological effects in fish of BDP at plasma concentrations known to affect human physiology provides valuable input to the development of <b>read-across</b> <b>approaches</b> in the identification of pharmaceuticals of environmental concern...|$|R
40|$|SummaryThere is {{an urgent}} need to {{establish}} a fundamental understanding of the mechanisms of nanomaterial (NM) interaction with living systems and the environment, in order for regulation of NMs {{to keep pace with}} their increasing industrial application. Identification of critical properties (physicochemical descriptors) that confer the ability to induce harm in biological systems is crucial, enabling both prediction of impacts from related NMs (via quantitative nanostructure–activity relationships (QNARs) and <b>read-across</b> <b>approaches)</b> and development of strategies to ensure these features are avoided or minimised in NM production in the future (“safety by design”). A number of challenges to successful implementation of such a strategy exist, including: (i) the lack of widely available systematically varied libraries of NMs to enable generation of sufficiently robust datasets for development and validation of QNARs; (ii) the fact that many physicochemical properties of pristine NMs are inter-related and thus cannot be varied systematically in isolation from others (e. g. increasing surface charge may impact on hydrophobicity, or changing the shape of a NM may introduce defects or alter the atomic configuration of the surface); and (iii) the effect of ageing, transformation and biomolecule coating of NMs under environmental or biological conditions. A novel approach to identify interlinked physicochemical properties, and on this basis identify overarching descriptors (axes or principle components) which can be used to correlate with toxicity is proposed. An example of the approach is provided, using three principle components which we suggest can be utilised to fully describe each NM, these being the intrinsic (inherent) properties of the NM, composition (which we propose as a separate parameter) and extrinsic properties (interaction with media, molecular coronas etc.) ...|$|R
40|$|This {{article is}} an output from a {{workshop}} on Setting Best Practices on Raptor Contaminant Monitoring Activities in Europe, {{one of the}} activities of the EURAPMON network ([URL] www. eurapmon. net) hosted in Murcia in May–June 2013. [...] et al. Biomonitoring using birds of prey as sentinel species has been mooted as a way to evaluate the success of European Union directives that are designed to protect people and the environment across Europe from industrial contaminants and pesticides. No such pan-European evaluation currently exists.  Coordination of such large scale monitoring would require harmonisation across multiple countries of the types of samples collected and analysed-matrices vary in the ease with which they can be collected and the information they provide. We report the first ever pan-European assessment of which raptor samples are collected across Europe and review their suitability for biomonitoring. Currently, some 182 monitoring programmes across 33 European countries collect a variety of raptor samples, and we discuss the relative merits of each for monitoring current priority and emerging compounds. Of the matrices collected, blood and liver are used most extensively for quantifying trends in recent and longer-term contaminant exposure, respectively. These matrices are potentially the most effective for pan-European biomonitoring but are not so widely and frequently collected as others. We found that failed eggs and feathers are the most widely collected samples. Because of this ubiquity, they may provide the best opportunities for widescale biomonitoring, although neither is suitable for all compounds. We advocate piloting pan-European monitoring of selected priority compounds using these matrices and developing <b>read-across</b> <b>approaches</b> to accommodate any effects that trophic pathway and species differences in accumulation may have on our ability to track environmental trends in contaminants. The work undertaken for this review was also supported by EURAPMON. Peer Reviewe...|$|R
40|$|The {{high-quality}} in vivo preclinical safety data {{produced by}} the pharmaceutical industry during drug development, which follows numerous strict guidelines, are mostly not available in the public domain. These safety data are sometimes published as a condensed summary for the few compounds that reach the market, {{but the majority of}} studies are never made public and are often difficult to access in an automated way, even sometimes within the owning company itself. It is evident from many academic and industrial examples, that useful data mining and model development requires large and representative data sets and careful curation of the collected data. In 2010, {{under the auspices of the}} Innovative Medicines Initiative, the eTOX project started with the objective of extracting and sharing preclinical study data from paper or pdf archives of toxicology departments of the 13 participating pharmaceutical companies and using such data for establishing a detailed, well-curated database, which could then serve as source for <b>read-across</b> <b>approaches</b> (early assessment of the potential toxicity of a drug candidate by comparison of similar structure and/or effects) and training of predictive models. The paper describes the efforts undertaken to allow effective data sharing intellectual property (IP) protection and set up of adequate controlled vocabularies) and to establish the database (currently with over 4000 studies contributed by the pharma companies corresponding to more than 1400 compounds). In addition, the status of predictive models building and some specific features of the eTOX predictive system (eTOXsys) are presented as decision support knowledge-based tools for drug development process at an early stage. The research leading to these results has received support from the Innovative Medicines Initiative Joint Undertaking under grant agreement n° 115002 (eTOX), resources of which are composed of financial contribution from the European Union’s Seventh Framework Programme (FP 7 / 2007 - 2013) and EFPIA companies’ in kind contributions. The authors would like to formally acknowledge the contribution to the eTOX project of all scientists and other staff involve...|$|R
40|$|In {{order to}} {{efficiently}} and effectively assess {{the risks of}} large numbers of existing chemicals and new chemical entities, there is an increasing emphasis in the regulatory setting on the use of so-called ¿non-testing¿ methods, either as a supplement to, or as a substitute for, traditional testing methods. In particular, alternatives to animal methods are being developed to reduce the need for animal testing in pharmacology and toxicology. Non-testing methods are {{based on the premise that}} the properties (including physicochemical properties and biological activities) of a chemical depend on its intrinsic nature and can be directly predicted from its molecular structure or inferred from the properties of similar compounds whose activities are known. Non-testing methods include a range of predictive approaches, including Structure-Activity Relationships (SARs), Quantitative Structure Activity Relationships (QSARs), chemical grouping and read-across, and computer-based tools based on the use of one or more of these approaches. The main question for the assessor when applying non-testing methods for regulatory purposes concerns the usefulness of the approach, which can be broken down into the practical applicability of the method and the adequacy of the predictions. Considerable progress has been made at the EU and international levels to develop a harmonised framework for assessing and documenting non-testing methods and their predictions. Exactly how this framework is applied in practice will depend on the provisions of the specific legislation (e. g. chemicals, pesticides, biocides, cosmetics) and the context in which the non-testing data are being used (including, for example, whether a traditional testing method is being replaced, whether additional, supporting data are available, and the consequences of making an inaccurate prediction). The general framework leaves largely open the difficult question of how to determine the adequacy of predicted data, and there is a considerable need to develop detailed guidance on how the predictions generated by non-testing methods can be translated into regulatory decisions. This chapter introduces the conceptual basis of SARs and QSARs, collectively referred to as (Q) SARs, as well as the related approach of chemical grouping (category formation) and read-across within chemical groups (categories). The current international framework for (Q) SAR models and predictions is then described (a similar framework has been developed for category and <b>read-across</b> <b>approaches).</b> The practical applicability of this framework. is illustrated by focussing on a checklist of 10 key questions, with respect to some well known software tools and their predictions of genotoxicity of two case study compounds. The purpose of these case studies is to highlight some of the scientific issues that need to be considered, as well the difficulties encountered. This leads into a discussion of what is needed to provide further guidance on the assessment of prediction adequacyJRC. I. 5 -Systems Toxicolog...|$|R
40|$|This report {{reviews the}} use of {{stepwise}} testing approaches for the prediction of skin and eye irritation and corrosion in a regulatory context. It is published as a companion report to the "Review of Literature-Based Models for Skin and Eye Irritation and Corrosion", an ECB report which reviewed the state-of-the-art of in silico and in vitro dermal and ocular irritation and corrosion human health hazard endpoints. In the former review, the focus was placed on reviewing alternative in silico approaches to assess acute local toxic effects, such as QSARs, SARs, chemical categories, and <b>read-across</b> and analogue <b>approaches.</b> Special {{emphasis was placed on}} literature-based (Q) SAR models for skin and eye irritation and corrosion and expert systems. In the present review, the emphasis is on different schemes (testing strategies) that have been conceived for the integrated use of different approaches, including in silico, in vitro and in vivo methods. JRC. I. 3 -Toxicology and chemical substance...|$|R
40|$|Fish may {{be exposed}} to an array of {{pharmaceuticals}} that are discharged into the aquatic environment, paralleling advances in medical knowledge, research and technology. Pharmaceuticals by their nature are designed to target specific receptors, transporters, or enzymes. Nuclear receptors (NRs) are often {{a key component of}} the therapeutic mechanism at play, and many of these are conserved among vertebrates. Consequently, fish may be affected by environmental pharmaceutical exposure, however there has been relatively little characterisation of NRs in fish compared with in mammals. In this thesis common carp (C. carpio) were exposed to selected pharmaceuticals in vitro and in vivo to investigate effects centred on the pregnane X receptor (PXR) and peroxisome proliferator-activated receptor alpha (PPARα), two key NRs involved in organism responses to pharmaceutical exposure. The PXR acts as a xenosensor, modulating expression of a number of xenobiotic metabolising enzymes (XMEs) in mammals. In a primary carp hepatocyte model it was shown that expression of a number of XMEs was altered on exposure to rifampicin (RIF), as occurs in mammals. This response was repressed by addition of ketoconaozle (KET; PXR-antagonist), indicating possible PXR involvement. The genes analysed showed up-regulation on exposure to ibuprofen (IBU) and clofibric acid (CFA), but not clotrimazole (CTZ) or propranolol (PRP). The lack of response to mammalian PXR-agonist CTZ was unexpected. In contrast, the same XME genes were found to be up-regulated in vivo after 10 days of exposure of carp to CTZ, although this response occurred only for a relatively high exposure concentration. CTZ was found to concentrate in the plasma (with levels up to 40 times higher than the water). Development and application of a reporter gene assay to measure PXR activation in carp (cPXR) and human PXR showed CTZ activation of cPXR, supporting data from the in vivo studies. Furthermore, activation was seen at concentrations as low as 0. 01 μM. Interestingly RIF did not induce a response in the cPXR reporter gene assay, contrasting with the hepatocyte culture work. Taken together, the data presented here suggests divergence in the PXR pathway between mammals and fish in terms of ligand activation and downstream gene targets. PPARα was investigated in carp in vivo using CFA as a mammalian PPARα-agonist. Overall the resulting data suggested a broadly similar role for this NR in lipid homeostasis in fish as for mammals, with a number of PPARα-associated genes and acyl-coA oxidase (ACOX 1) activity up-regulated in response to CFA exposure. A number of XMEs were also up-regulated by CFA (in vivo and in vitro), potentially extending the role of PPARα in fish (carp) to regulation of xenobiotic metabolism. The work presented has provided further characterisation of PXR and PPARα in fish. Elucidation of these pathways is vital to provide meaningful data in terms of establishing toxicity and mechanism-of-action data for pharmaceuticals and other compounds in fish, to allow validation of <b>read-across</b> <b>approaches</b> and ultimately aid in their environmental risk assessment. In vitro approaches are attractive ethically, financially and can provide useful mechanistic characterisation of compounds and the primary hepatocyte model and reporter gene assays used here show potential for the screening of pharmaceutical compounds in fish. However, further understanding of the metabolism of drugs and chemicals in fish is required to establish the true value of these methods for informing on possible effects in fish, in vivo. BBSRCAstraZenec...|$|R
40|$|The use of high animal {{numbers and}} the {{suffering}} of animals in experiments such as repeated dose toxicity studies (RDT) demand for alternative test approaches. This {{can be done by}} grouping of chemicals to reduce in vivo studies. Therefore, <b>read-across</b> (RAX) <b>approach</b> is purposed to reduce animal numbers as one or several tested source compound(s) are used to predict the toxicity of “similar” non-tested target compound(s). Similar compounds shall share structural- and physico-chemical properties as well as a similar mode of action. RAX approach can be used for filling data gaps in human risk assessment. This study was designed to evaluate the use of data from ex vivo experiments such as rat or human precision-cut lung slices (PCLS). Three RAX-categories were tested, namely vicinal halogenides, naphthalene derivatives and vinyl esters. Each RAX category was selected based on shared structural properties and similar toxicological effects in RDT studies extracted from the FhGRepDose database ([URL] Repeated chemical exposure of rat and human PCLS was performed on three days for three hours daily. The cytotoxicity of chemicals was assessed by LDH and WST- 1 assay. Ex vivo IC 50 values were calculated by sigmoidal curve fitting. These values were correlated to the public available in vivo LD 50 values. Cytotoxic effect was assessed in dose dependent manner. Vicinal halogenides and naphthalene derivatives were less toxic in human lung sections compared to rat sections, whereas the vinyl esters showed the comparable cytotoxicity in both species. Linear regression analysis of public available LD 50 values to the obtained ex vivo IC 50 values showed good linear correlation. Thus only one substance group showed similar cytotoxicity in both tested species, whereas two other groups revealed interspecies diversity based on cytotoxicity endpoint. This first evaluation of rat and human and PCLS show that ex vivo chemical testing inhuman lung sections result in a promising approach for toxicity profiling as no human in vivo reference data exist...|$|R
40|$|We {{describe}} and illustrate a workflow for chemical safety assessment that completely avoids animal testing. The workflow, which was developed within the SEURAT- 1 initiative, {{is designed to}} be applicable to cosmetic ingredients as well as to other types of chemicals, e. g. active ingredients in plant protection products, biocides or pharmaceuticals. The aim of this work was to develop a workflow to assess chemical safety without relying on any animal testing, but instead constructing a hypothesis based on existing data, in silico modelling, biokinetic considerations and then by targeted non-animal testing. For illustrative purposes, we consider a hypothetical new ingredient x as a new component in a body lotion formulation. The workflow is divided into tiers in which points of departure are established through in vitro testing and in silico prediction, as the basis for estimating a safe external dose in a repeated use scenario. The workflow includes a series of possible exit (decision) points, with increasing levels of confidence, based on the sequential application of the Threshold of Toxicological (TTC) <b>approach,</b> <b>read-across,</b> followed by an “ab initio” assessment, in which chemical safety is determined entirely by new in vitro testing and in vitro to in vivo extrapolation by means of mathematical modelling. We believe that this workflow could be applied as a tool to inform targeted and toxicologically relevant in vitro testing, where necessary, and to gain confidence in safety decision making without the need for animal testing. JRC. F. 3 -Chemicals Safety and Alternative Method...|$|R

