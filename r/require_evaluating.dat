47|2853|Public
5000|$|Assignments are statements, not expressions, and {{therefore}} {{cannot be used}} in situations that <b>require</b> <b>evaluating</b> the assignment operation.|$|E
5000|$|An {{additional}} {{argument for}} exponentiation being right-associative {{is that the}} superscript inherently behaves {{as a set of}} parentheses; e.g. in the expression [...] the addition is performed before the exponentiation despite there being no explicit parentheses [...] wrapped around it. Thus given an expression such as , it makes sense to <b>require</b> <b>evaluating</b> the full exponent [...] of the base [...] first.|$|E
5000|$|The Maclaurin series {{coefficients}} of F increase in absolute value {{until they reach}} their maximum at the 40th term of -1.753. By the 109th term they have dropped below one in absolute value. Taking the first 1000 terms suffices to give a very accurate value for for [...] However, this would <b>require</b> <b>evaluating</b> a polynomial of degree 1000 either using rational arithmetic with the {{coefficients of}} large numerator or denominator, or using floating point computations of over 100 digits. An alternative {{is to use the}} inverse Mellin transform defined above and numerically integrate. Neither approach is computationally easy.|$|E
50|$|A {{quotient}} filter requires 10-25% {{more space}} than a comparable Bloom filter but is faster because each access <b>requires</b> <b>evaluating</b> {{only a single}} hash function.|$|R
3000|$|Note {{that the}} {{adaptive}} parametric implementations differ mainly in parameter estimation, they share identical steps in signal whitening and calculating the test statistic. The CG-PGLRT-NSCM detector is slightly {{more complex than}} the CG-PGLRT-SCM detector since it <b>requires</b> <b>evaluating</b> [...]...|$|R
5000|$|FIPS 140-1 <b>required</b> <b>evaluated</b> {{operating}} systems that referenced the Trusted Computer System Evaluation Criteria (TCSEC) classes C2, B1 and B2. However, TCSEC {{is no longer}} in use and has been replaced by the Common Criteria. Consequently, FIPS 140-2 now references the Common Criteria.|$|R
5000|$|On each iteration, {{the most}} {{time-consuming}} {{task is to}} select β. We {{know that there are}} B possible values, so we can find β using [...] comparisons. Each comparison will <b>require</b> <b>evaluating</b> [...] In the kth iteration, y has k digits, and the polynomial can be evaluated with [...] multiplications of up to [...] digits and [...] additions of up to [...] digits, once we know the powers of y and β up through [...] for y and n for β. β has a restricted range, so we can get the powers of β in constant time. We can get the powers of y with [...] multiplications of up to [...] digits. Assuming n-digit multiplication takes time [...] and addition takes time , we take time for each comparison, or time [...] to pick β. The remainder of the algorithm is addition and subtraction that takes time , so each iteration takes [...] For all k digits, we need time [...]|$|E
3000|$|..., {{this search}} does not <b>require</b> <b>evaluating</b> cross {{products}} of different workloads; each workload is only optimized once per value of [...]...|$|E
30|$|Location based {{services}} also <b>require</b> <b>evaluating</b> how {{to automatically}} configure position and size, like by using user input, {{to increase the}} performance {{in terms of both}} privacy and utility [23].|$|E
40|$|Increasing {{anthropogenic}} {{noise pollution}} <b>requires</b> <b>evaluating</b> its potential effects on fishes and defining mitigation measures. Managers often request a single, simple index representing {{the impact of}} noise. A dB scale for fish species analogous to the dB(A) scale for humans was suggested (Nedwell et al. 2004 a) ...|$|R
30|$|Concerning {{the first}} problem, {{we have to}} account that there are 206 {{sovereign}} states recognized by the United Nations 4, and users can {{have more than one}} nationality. Therefore, computing ⟦p⟧E(q) <b>requires</b> <b>evaluating</b> 2205 queries (i.e., the queries that can be constructed from the initial query {(nat,BE)}), which is clearly infeasible.|$|R
40|$|Transportation {{policy making}} often <b>requires</b> <b>evaluating</b> a {{proposed}} discrete change, {{whether it be}} a physical investment or a new set of operating rules. Some proposals, like the rail tunnel under the English channel, are one-time capital investments with long-lasting effects. Others, like congestion pricing proposed for The Netherlands, require major behavioral and political groundwork...|$|R
40|$|Computational Grids {{normally}} {{deal with}} large computationally intensive problems on small data sets. In contrast, Data Grids mostly deal with large computational problems {{that in turn}} <b>require</b> <b>evaluating</b> and mining large amounts of data. Replication is {{regarded as one of}} the major optimisation techniques for providing fast data access...|$|E
40|$|International audienceAdvanced {{context-aware}} telecom services <b>require</b> <b>evaluating</b> rules encompassing {{local or}} remote, raw or abstract context situations. The meteoric rise of smartphones {{raises the question}} of potential deployment of terminal-based contextaware services versus a network-centric one. In this paper we compare these two approaches through the analysis of typical context-aware services...|$|E
40|$|Attempting {{to measure}} the costs of a crisis like that of Walkerton without {{recognizing}} the illness and loss of life it caused is both unreasonable and unre-alistic. But how can we measure those human costs? To place values on the human costs would <b>require</b> <b>evaluating</b> in tangible terms the intangible wort...|$|E
40|$|Abstract—The kernel trick enables {{learning}} of non-linear decision functions {{without having to}} explicitly map the original data to a high dimensional space. However, at test time, it <b>requires</b> <b>evaluating</b> the kernel with {{each one of the}} support vectors, which is time consuming. We propose a novel approach for learning non-linear support vector machine (SVM) corresponding to commonly use...|$|R
40|$|Abstract. We {{construct}} both randomizable {{and strongly}} existentially unforgeable structure-preserving signatures for messages consisting of many group elements. To sign a message consisting of N = mn group el-ements {{we have a}} verification key size of m group elements and signatures contain n+ 2 elements. Verification of a signature <b>requires</b> <b>evaluating</b> n+ 1 pairing product equations. We also investigate the case of fully structure-preserving signatures where it is required that the secret signing key consists of group elements only. We show a variant of our signature scheme allowing the signer to pick part of the verification key {{at the time of}} signing is still secure. This gives us both randomizable and strongly existentially unforgeable fully structure-preserving signatures. In the fully structure preserving scheme the verification key is a single group element, signatures contain m+n+ 1 group elements and verification <b>requires</b> <b>evaluating</b> n+ 1 pairing product equations...|$|R
30|$|Additional {{data are}} <b>required</b> to <b>evaluate</b> {{intermediate}} levels of oxygenation target and potential impact in this population.|$|R
40|$|Computational Grids process large, {{computationally}} intensive {{problems on}} small data sets. In contrast, Data Grids process large computational problems {{that in turn}} <b>require</b> <b>evaluating,</b> mining and producing large amounts of data. Replication, creating geographically disparate identical copies of data, is {{regarded as one of}} the major optimisation techniques for reducing data access costs. In this paper...|$|E
30|$|This {{technique}} performs as many iterations {{as necessary}} to find a stationary point such that its slice is of size zero. As expected, {{the choice of the}} step size is critical because too small values would <b>require</b> <b>evaluating</b> the target function too many times to generate the slices, while too high values could potentially lead the search far away from the targeted maximum.|$|E
40|$|Many tasks <b>require</b> <b>evaluating</b> a specied boolean {{expression}} # over {{a set of}} probabilistic tests where we know the probability that each test will succeed, and also the cost of performing each test. A strategy species when to perform which test, towards determining the overall outcome of #. This paper investigates the challenge of nding the strategy with the minimum expected cost...|$|E
30|$|The discretized {{model in}} the form of {{ordinary}} differential equations (ODEs) (Equations 9 to 11) may have very large dimensions and be expensive to solve. The MCMC method <b>requires</b> <b>evaluating</b> repeatedly the solution of the forward model (many thousands or even millions of times). Hence, these simulations can be a computationally expensive undertaking. In such situations, the reduced-order model is needed to approximate the large-scale model, which allows efficient simulations.|$|R
40|$|<b>Evaluating</b> complex propositions <b>requires</b> <b>evaluating</b> trothvalues and {{assigning}} modal operators. Previous research {{suggested that}} evaluating truth-values {{may be the}} key to assigning modal operators. This study placed 111 third and fifth grade children in one of three training conditions: no training, training troth-value assignment, and training trothvalue and modal operator assignment. The results indicate that truth-value assignment training is sufficient to significantly improve children's evaluations of complex propositions...|$|R
40|$|We {{perform a}} {{stability}} {{analysis of a}} Monte Carlo method for simulating the Compton scattering of photons by free electron in high energy density applications and develop time-step limits that avoid unstable and oscillatory solutions. Implementing this Monte Carlo technique in multi physics problems typically <b>requires</b> <b>evaluating</b> the material temperature at its beginning-of-time-step value, {{which can lead to}} this undesirable behavior. With a set of numerical examples, we demonstrate the efficacy of our time-step limits...|$|R
40|$|Abstract. Computational Grids {{normally}} {{deal with}} large computationally intensive problems on small data sets. In contrast, Data Grids mostly deal with large computational problems {{that in turn}} <b>require</b> <b>evaluating</b> and mining large amounts of data. Replication is {{regarded as one of}} the major optimisation techniques for providing fast data access. Within this paper, several replication algorithms are studied. This is achieved using the Grid simulator: OptorSim. OptorSim provides a modular framework within which optimisation strategies can be studied under different Grid configurations. The goal is to explore the stability and transient behaviour of selected optimisation techniques. ...|$|E
30|$|Once a risk {{analysis}} is performed and the calculated risks are evaluated against the criteria defined for this context, risks within tolerable thresholds {{need to be}} evaluated against the ALARP principle. As previously discussed, this would <b>require</b> <b>evaluating</b> the reduction of risk given potential mitigation strategies to further reduce risk, {{and the costs of}} these strategies (capital and operational costs). The feasibility for further risk reduction would become dependent of the available operational budget and the population’s perception of risk. Balanced decisions will render minimum residual risks within tolerable thresholds, without overstressing available resources, and optimized through evaluation of different options through ACSSL or UCSSL comparisons.|$|E
40|$|International audienceApplications {{that deal}} with time-series data often <b>require</b> <b>evaluating</b> complex {{statistics}} for which each time series is essentially one data point. When only a few time series are available, bootstrap methods are used to generate additional samples {{that can be used}} to evaluate empirically the statistic of interest. In this work a novel bootstrap method is proposed, which is shown to have some asymptotic consistency guarantees under the only assumption that the time series are stationary and ergodic. This contrasts previously available results that impose mixing or finite-memory assumptions on the data. Empirical evaluation on simulated and real data, using a practically relevant and complex extrema statistic is provided...|$|E
50|$|The {{performance}} verification {{and execution}} of a real-time scheduling algorithm is performed by {{the analysis of the}} algorithm execution times. Verification for the performance of a real-time Scheduler will require testing the scheduling algorithm under different test scenarios including the worst-case execution time. These testing scenarios include worst case and unfavorable cases to assess the algorithm performance. The time calculations required for the analysis of scheduling systems <b>requires</b> <b>evaluating</b> the algorithm at the code level.|$|R
50|$|Under a {{particular}} EMC test standard, a specific LISN type is <b>required</b> for <b>evaluating</b> and characterizing {{the operation of}} the EUT.|$|R
30|$|Research is <b>required</b> to <b>evaluate</b> the faking {{effect on}} {{indirect}} personality assessment, namely through {{the tools that}} aim to select non-academic characteristics.|$|R
40|$|Traditionally, {{effects that}} <b>require</b> <b>evaluating</b> multidimensional {{integrals}} for each pixel, such as motion blur, depth of field, and soft shadows, suffer from noise {{due to the}} variance of the high-dimensional integrand. In this paper, we describe a general reconstruction technique that exploits the anisotropy in the temporal light field and permits efficient reuse of samples between pixels, multiplying the effective sampling rate by a large factor. We show that our technique can be applied in situations that are challenging or impossible for previous anisotropic reconstruction methods, and that it can yield good results with very sparse inputs. We demonstrate our method for simultaneous motion blur, depth of field, and soft shadows...|$|E
40|$|Raising Holstein calves {{continues}} to be a big challenge • Proper quality and quantity of colostrum cannot be overlooked • Raising milk and feed prices <b>require</b> <b>evaluating</b> use of alternative milk replacer components and/or alternative feeding practices • Pasteurization capability may be viable for dairy producers; however, calf ranches have fewer opportunities for such technologies. Evaluation of effects of antibiotics on changes in the morphology of the digestive tract and antibiotic resistance is warranted • Concentrates are needed for ruminal development; however, roughage source and level need to be reevaluated for overall health and performance • Special dietary considerations may impact overall performance of calves • Effects of environment of performance of calves remains to be determine...|$|E
40|$|Computational grids process large, {{computationally}} intensive {{problems on}} small data sets. In contrast, data grids process large computational problems {{that in turn}} <b>require</b> <b>evaluating,</b> mining and producing large amounts of data. Replication, creating geographically disparate identical copies of data, is {{regarded as one of}} the major optimization techniques for reducing data access costs. In this paper, several replication algorithms are discussed. These algorithms were studied using the Grid simulator: OptorSim. OptorSim provides a modular framework within which optimization strategies can be studied under different Grid configurations. The goal is to explore the stability and transient behaviour of selected optimization techniques. We detail the design and implementation of OptorSim and analyze various replication algorithms based on different Grid workloads...|$|E
5000|$|For all but {{the richest}} societies, the cost and time <b>required</b> to <b>evaluate</b> the large set of {{interventions}} required may be prohibitive.|$|R
2500|$|Lastly, {{he found}} it ironic that the Court had relied on the {{plurality}} standard in O'Connor, since its discussion would make it even harder to assess what the operating realities of the workplace were. [...] "Any rule that <b>requires</b> <b>evaluating</b> whether a given gadget is a 'necessary instrumen for self-expression, even self-identification,' on top of assessing {{the degree to which}} 'the law's treatment of [...] evolve,' ... is (to put it mildly) [...] unlikely to yield objective answers." ...|$|R
40|$|AbstractThe use of {{rectangular}} isoparametric {{elements in}} {{finite element analysis}} of second-order boundary-value problems <b>requires</b> <b>evaluating</b> integrals of rational polynomial functions. Gaussian quadrature formulas are currently the most popular method of obtaining approximations to the exact integrals. A new method is described in which the isoparametric finite element function spaces are approximated. The resulting integrals can be evaluated exactly, avoiding the computational expense of the Gaussian quadrature schemes, particularly the 27 point formula used in three-dimensional elements...|$|R
