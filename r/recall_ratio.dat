62|63|Public
5000|$|... (1) Naming. Naming is a {{decentralized}} and name-stable global object (Gnode) management system. Naming supports locating objects by {{the global}} unique identifier with the feature of low latency and high success ratio; Naming also supports object searching based-on attribute-match with the feature of low latency and high <b>recall</b> <b>ratio.</b> Naming is a fundamental component in VegaGOS to construct the whole system. As a reusable component, Naming forms a global layer of virtual names {{to solve the problem}} of non-stable of physical address and tight coupling between applications and resources.|$|E
40|$|Abstract. <b>Recall</b> <b>ratio</b> and Precision ratio are two {{important}} indices {{of the effect}} evaluation of computer information retrieval. This paper analyzed factors impacting <b>Recall</b> <b>ratio</b> and Precision ratio, then discussed the improving of computer information retrieval efficiency from retrieval approach, database selecting and retrieval pattern, etc...|$|E
40|$|There are {{two types}} of {{keywords}} used as metadata: controlled terms and free terms. Free terms have the advantage that metadata creators can freely select keywords, but there also exists a disadvantage that the information retrieval <b>recall</b> <b>ratio</b> might be reduced. The <b>recall</b> <b>ratio</b> can be improved by using controlled terms. But creating and maintaining controlled vocabularies has an enormous cost. In addition, many existing controlled vocabularies are published in formats less suitable for programming. We introduce a JavaScript library called “covo. js” that enables us to make use of controlled vocabularies as metadata for the organization of web pages...|$|E
40|$|Restricted AccessPhenomal {{growth has}} been {{observed}} in both the number of bibliographic citations {{and the number of}} commercially available bibliographic online databases. The wide choice of online databases available makes their selection a rather difficult task for solving a given information retrieval problem. Moreover, when the search topic is either multidisciplinary or interdisciplinary in nature, and there are more databases falling within the scope of the search, then the searcher has to shoulder the responsibility of selecting the most suitable database for his patron in order to optimize the utility of search results. In this study, we have used analysis of variance (ANOVA) - a statistical technique for studying the variability of <b>recall</b> <b>ratios</b> among different databases for a given set of search topics. Another statistical technique called Duncon multiple range test (DMRT) is used for pair wise comparison of <b>recall</b> <b>ratios</b> and significance thereo...|$|R
40|$|Critera {{by which}} {{literature}} search services may be evaluated, are quality, time and cost criteria. Evaluation judgements {{are far from}} being reliable, if not based on these criteria. Relevance, {{which is the most}} important quality criteria, is discussed briefly. Precision and <b>Recall</b> <b>ratios,</b> which reflect the numerical value and measure of Relevance, are discussed in some detail. These two criteria frequently used to judge the performance of searches. Significance of Precision and Recall is described and definitions are given. It is considered that these two ratios are the most important performance criteria among all, for they measure Relevance, and Relevance is the most significant component in search effectiveness. For this reason, the present study is limited mainly to the topics of Precision and Recall. However, the text includes some brief description of other criteria as well. In the present study, it is stated as a conclusion that Precision and <b>Recall</b> <b>ratios</b> should be used as Relevancy criteria in judging the performance of literature searches, otherwise all the quality judgements on searches will be false and unreliable...|$|R
40|$|This is an {{investigation}} of information retrieval performance of Turkish search engines with respect to precision, normalized recall, coverage and novelty ratios. We defined seventeen query topics for Arabul, Arama, Netbul and Superonline. These queries were carefully selected to assess the capability of a search engine for handling broad or narrow topic subjects, exclusion of particular information, identifying and indexing Turkish characters, retrieval of hub/authoritative pages, stemming of Turkish words, correct interpretation of Boolean operators. We classified each document in a retrieval output as being ”relevant” or ”nonrelevant” to calculate precision and normalized <b>recall</b> <b>ratios</b> at various cut-off points for each pair of query topic and search engine. We found the coverage and novelty ratios for each search engine. We also tested how search engines handle meta-tags and dead links. Arama {{appears to be the}} best Turkish search engine in terms of average precision and normalized <b>recall</b> <b>ratios,</b> and the coverage of Turkish sites. Turkish characters (and stemming as well) still cause bottlenecks for Turkish search engines. Superonline and Netbul make use of the indexing information in metatag fields to improve retrieval results...|$|R
40|$|This article {{proposes a}} novel {{framework}} for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware {{convolutional neural network}} (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, <b>recall</b> <b>ratio,</b> and F 1 score on the Google-news data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, <b>recall</b> <b>ratio,</b> and F 1 score. For Brown Corpus, our framework obtains the best F 1 score and almost equivalent precision rate and <b>recall</b> <b>ratio</b> over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F 1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells’ impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word 2 vec, GloVe, and sent 2 vec embeddings and report their performance differences...|$|E
40|$|This paper {{proposes a}} novel {{framework}} for detecting redundancy in supervised sentence categorisation. Unlike traditional singleton neural network, our model incorporates character-aware {{convolutional neural network}} (Char-CNN) with character-aware recurrent neural network (Char-RNN) to form a convolutional recurrent neural network (CRNN). Our model benefits from Char-CNN in that only salient features are selected and fed into the integrated Char-RNN. Char-RNN effectively learns long sequence semantics via sophisticated update mechanism. We compare our framework against the state-of-the-art text classification algorithms on four popular benchmarking corpus. For instance, our model achieves competing precision rate, <b>recall</b> <b>ratio,</b> and F 1 score on the Google-news data-set. For twenty-news-groups data stream, our algorithm obtains the optimum on precision rate, <b>recall</b> <b>ratio,</b> and F 1 score. For Brown Corpus, our framework obtains the best F 1 score and almost equivalent precision rate and <b>recall</b> <b>ratio</b> over the top competitor. For the question classification collection, CRNN produces the optimal recall rate and F 1 score and comparable precision rate. We also analyse three different RNN hidden recurrent cells' impact on performance and their runtime efficiency. We observe that MGU achieves the optimal runtime and comparable performance against GRU and LSTM. For TFIDF based algorithms, we experiment with word 2 vec, GloVe, and sent 2 vec embeddings and report their performance differences. Comment: Conference paper accepted at IEEE SMARTCOMP 2017, Hong Kon...|$|E
40|$|This paper {{describes}} {{and compares}} {{the use of}} methods based on N-grams (specifically trigrams and pentagrams), together with five features, to recognise the syntactic and semantic categories of numeral strings representing money, number, date, etc., in texts. The system employs three interpretation processes: word N-grams construction with a tokeniser; rule-based processing of numeral strings; and N-gram-based classification. We extracted numeral strings from 1, 111 online newspaper articles. For numeral strings interpretation, we chose 112 (10 %) of 1, 111 articles to provide unseen test data (1, 278 numeral strings), and used the remaining 999 articles to provide 11, 525 numeral strings for use in extracting N-gram-based constraints to disambiguate meanings of the numeral strings. The word trigrams method resulted in 83. 8 % precision, 81. 2 % <b>recall</b> <b>ratio,</b> and 82. 5 % in F-measurement ratio. The word pentagrams method resulted in 86. 6 % precision, 82. 9 % <b>recall</b> <b>ratio,</b> and 84. 7 % in F-measurement ratio...|$|E
40|$|As the {{information}} is increasingly moved from printed media to the electronic equivalents in today's world, the Internet has become an indispensable information source with its proliferating contents and accessibility {{from all over the}} world. In this context, such questions as the performances of the search engines and if they can meet the users' needs are studied. In this study, information retrieval performances of the five search engines (Alta Vista, Excite, HotBot, Infoseek and the Northern Light) are evaluated in terms of precision, recall, differences between the first 10 and first 20 retrieved results and overlap. The evaluation is based on the first 20 retrieved results of 1 1 queries. As a result of evaluation, {{it was found that the}} average precision performances of Alta Vista, Excite, HotBot and Infoseek are around 50 % while it is 64 % for the Northern Light. However, no statistical significance in terms of average precision ratio among the search engines is observed. On the other hand, the average relative <b>recall</b> <b>ratios</b> of these engines vary between 14 % and 31 % (Infoseek 14 %, HotBot 21 %, Excite 23 %, Alta Vista 24 %, Northern Light 31 %). Tests showed that there is a statistical significance between Infoseek and the Northern Light in terms of their average relative <b>recall</b> <b>ratios.</b> The overlap among the relevant documents which are retrieved by all five search engines is 11 %. In conclusion, the average recall and precision performances of search engines are low. Search engines are similar to each other in terms of their average precision and <b>recall</b> <b>ratios</b> with the exception of Infoseek and the Northern Light which they differ in average recall performances. Also, it is observed that the overlap ratio among the retrieved results by all five search engines is low, as they index different documents in different subject areas...|$|R
40|$|Abstract. This is an {{investigation}} of information retrieval performance of Turkish search engines with respect to precision, normalized recall, coverage and novelty ratios. We defined seventeen query topics for Arabul, Arama, Netbul and Superonline. These queries were carefully selected to assess the capability of a search engine for handling broad or narrow topic subjects, exclusion of particular information, identifying and indexing Turkish characters, retrieval of hub/authoritative pages, stemming of Turkish words, correct interpretation of Boolean operators. We classified each document in a retrieval output as being ”relevant ” or ”nonrelevant ” to calculate precision and normalized <b>recall</b> <b>ratios</b> at various cut-off points for each pair of query topic and search engine. We found the coverage and novelty ratios for each search engine. We also tested how search engines handle meta-tags and dead links. Arama {{appears to be the}} best Turkish search engine in terms of average precision and normalized <b>recall</b> <b>ratios,</b> and the coverage of Turkish sites. Turkish characters (and stemming as well) still cause bottlenecks for Turkish search engines. Superonline and Netbul make use of the indexing information in metatag fields to improve retrieval results. ...|$|R
40|$|This paper {{proposes a}} {{content-based}} audio information retrieval and indexing technique based on wavelet transform. The presented approach uses multiresolution decomposition {{property of the}} discrete wavelet transform to analyze audio data. The wavelet decomposition of an audio signal highly resembles to its decomposition in sound octaves. A hierarchical indexing scheme is constructed using statistical properties of the wavelet coefficients such as zero-crossing rate, mean, and standard deviation at multiple scales. The performance of the proposed systems is experimentally evaluated using 4 18 different audio clips. The prototype system yields very high <b>recall</b> <b>ratios</b> (higher than 70 %) for sample queries with diverse audio characteristics. 1...|$|R
40|$|Objective: To {{estimate}} {{the ability of}} parents to recall the injuries of their children. Design: Comparison of parent recall with computerized medical records. Setting: A health maintenance organization in Washington State during 2003. Subjects: Parents of children younger than 6 years. Main outcome measures: The ratio of recalled injuries to injuries in computerized data. Results: Telephone interviews were completed with a parent of 1672 young children who had computerized medical data {{for at least one}} injury in the last year. Counting the three most recent treated injuries, the 1672 children had 1896 separate new injuries in the year before interview and parents recalled 1150 of these: <b>recall</b> <b>ratio</b> 0. 61 (95 % confidence interval (CI) 0. 58 to 0. 63). The <b>recall</b> <b>ratio</b> decreased from 0. 82 (95 % CI 0. 79 to 0. 85) for injuries one day before interview to 0. 37 (95 % CI 0. 32 to 0. 40) at 365 days before interview. For 341 major injuries the <b>recall</b> <b>ratio</b> was 0. 80 (95 % CI 0. 76 to 0. 84), for 202 minor injuries treated in an emergency department or hospital it was 0. 77 (95 % CI 0. 71 to 0. 82), for 597 minor injuries treated in urgent care it was 0. 70 (95 % CI 0. 65 to 0. 73), and for 756 minor injuries treated in a clinic it was 0. 43 (95 % CI 0. 39 to 0. 47). Conclusions: Recall decreased with time. Recall was best for major injuries, intermediate for minor injurie...|$|E
30|$|The {{figure in}} each column in Tables  2, 3 {{represents}} {{the number of}} data whose respective labels were estimated for the sensor data considered to be unlabeled. Italics numbers represent the number of data whose estimated results are correct. In addition, the <b>recall</b> <b>ratio</b> represents {{the percentage of the}} data correctly estimated for labels. The precision factor represents the ratio of correct answers for the estimated results.|$|E
40|$|This thesis {{put forward}} a method used to {{calculate}} query similarity of webpage search results based on Web comprehending. According to users’ query input, this method can use Web comprehending technology to display the important web pages closer to users’ query in {{the first page of}} the list, make users more satisfied with the response of search engine, running after <b>recall</b> <b>ratio</b> and ensure precision at the same time. </p...|$|E
5000|$|It is {{possible}} to interpret precision and <b>recall</b> not as <b>ratios</b> but as probabilities: ...|$|R
40|$|International audienceWe {{address a}} {{particular}} case of video genre classification, namely the classification of animated movies. This task is achieved using two categories of content descriptors, temporal and color based, which are adapted to this particular content. Temporal descriptors, like rhythm or action, are quantifying {{the perception of the}} action content at different levels. Color descriptors are determined using color perception which is quantified in terms of statistics of color distribution, elementary hues, color properties (e. g. amount of light colors, cold colors, etc.) and color relationship. The potential of the proposed descriptors to the classification task has been proved through experimental tests conducted on more than 159 hours of video footage. Despite the high diversity of the video material, the proposed descriptors achieve an average precision and <b>recall</b> <b>ratios</b> up to 90...|$|R
40|$|We {{compared}} the information retrieval performances of some popular search engines (namely, Google, Yahoo, AlltheWeb, Gigablast, Zworks and AltaVista and Bing/MSN) {{in response to}} a list of ten queries, varying in complexity. These queries were run on each search engine and the precision and response time of the retrieved results were recorded. The first ten documents on each retrieval output were evaluated as being ‘relevant’ or ‘non-relevant’ for evaluation of the search engine’s precision. To evaluate response time, normalised <b>recall</b> <b>ratios</b> were calculated at various cut-off points for each query and search engine. This study shows that Google appears to be the best search engine in terms of both average precision (70 %) and average response time (2 s). Gigablast and AlltheWeb performed the worst overall in this study...|$|R
40|$|Abstract. This paper {{describes}} {{and compares}} {{the use of}} methods based on N-grams (specifically trigrams and pentagrams), together with five features, to recognise the syntactic and semantic categories of numeral strings representing money, number, date, etc., in texts. The system employs three interpretation processes: word N-grams construction with a tokeniser; rule-based processing of numeral strings; and N-gram-based classification. We extracted numeral strings from 1, 111 online newspaper articles. For numeral strings interpretation, we chose 112 (10 %) of 1, 111 articles to provide unseen test data (1, 278 numeral strings), and used the remaining 999 articles to provide 11, 525 numeral strings for use in extracting N-gram-based constraints to disambiguate meanings of the numeral strings. The word trigrams method resulted in 83. 8 % precision, 81. 2 % <b>recall</b> <b>ratio,</b> and 82. 5 % in F-measurement ratio. The word pentagrams method resulted in 86. 6 % precision, 82. 9 % <b>recall</b> <b>ratio,</b> and 84. 7 % in F-measurement ratio. Keywords: numeral strings, N-grams, named entity recognition, natural language processing. ...|$|E
40|$|This paper {{proposes a}} word sense {{disambiguation}} (WSD) method using bilingual corpus in English-Chinese machine translation system. A mathematical model is constructed to disambiguate word in terms of context phrasal collocation. A rules learning algorithm is proposed, and an application algorithm of the learned rules is also provided, which can increase the <b>recall</b> <b>ratio.</b> Finally, an analysis is given by an experiment on the algorithm. Its application gives an increase of 10 % in precision...|$|E
40|$|Abstract-As a basic {{prosodic}} unit, the prosodic word {{influences the}} naturalness and the intelligibility greatly. Although {{the relation between}} the lexicon word and the prosodic word has been widely discussed, the prosodic word prediction still can not reach a high precision and recall. In this paper, the study shows the lexical features are more efficient in prosodic word prediction. Based on careful analysis on the mapping relationship and the difference between the lexicon words and the prosodic words, this paper proposes two methods to predict prosodic words from the lexicon words sequence. The first is a statistic probability model, which efficiently combines the local POS and word length information. Experiments show that by choosing appropriate threshold this statistic model can reach a high precision and high <b>recall</b> <b>ratio.</b> Another is an SVM-based method. In this SVM classifier, an efficient feature is introduced. Besides the POS and the word length features, the in-word-probability (IWP) is used. IWP means the probability of a lexicon word to be in a prosodic word. The precision and the <b>recall</b> <b>ratio</b> are improved greatly after using SVM classifier and the IWP feature. I...|$|E
3000|$|... is the <b>recall,</b> the <b>ratio</b> {{between the}} number of pixels in the {{intersection}} of the detected and the real scratch and the number of pixels in the real scratch. When it tends to 1, the detected scratch covers the whole real scratch, but it gives no information about pixels outside [...]...|$|R
3000|$|... is the {{throughput}} {{achieved at}} the RS-SS link and c 2 is {{the capacity of}} users of type 2. <b>Recall</b> that the <b>ratio</b> [...]...|$|R
40|$|Feature {{extraction}} is a {{key technology}} {{in the process of}} the 3 D model retrieval, and the retrieval result is determined by the quality of the features. This paper structures one function (θ, φ, H) which is used the spatial position (θ, φ) and the mean curvature (H) of the 3 D model surface point. Through making harmonic analysis for the function, a group of partial features which have rotation invariance are obtained. And then taking advantage of the entire features which are weighted by the distance from the surface points to the model barycenter, a new group of feature vectors are obtained. So the new features obtain both partial and entire features. The experimental results show that using the new features are better than only using partial or entire features in the <b>recalling</b> <b>ratio</b> and precision ratio...|$|R
40|$|Abstract. Conventional Web image {{search engines}} can return reason-ably {{accurate}} results for queries containing concrete terms, but the re-sults are less accurate for queries containing only abstract terms, such as “spring ” or “peace. ” To improve the <b>recall</b> <b>ratio</b> without drastically degrading the precision ratio, {{we developed a}} method that replaces an abstract query term given by a user {{with a set of}} concrete terms and that uses these terms in queries input into Web image search engines. Concrete terms are found for a given abstract term by making use of social tagging information extracted from a social photo sharing system, such as Flickr. This information is rich in user impressions about the objects in the images. The extraction and replacement are done by (1) collecting social tags that include the abstract term, (2) clustering the tags in accordance with the term co-occurrence of images, (3) selecting concrete terms from the clusters by using WordNet, and (4) identifying sets of concrete terms that are associated with the target abstract term by using a technique for association rule mining. Experimental results show that our method improves the <b>recall</b> <b>ratio</b> of Web image searches. ...|$|E
40|$|AbstractMost {{of current}} {{ontology}} mapping methods can not treat different mapping tasks {{in different ways}} referred to {{the features of the}} input ontology. And they combine different features of ontology without full consideration of the influences on mapping results caused mapping features. In view of the above questions, this paper proposes mapping method which can use entropy decision-making method to determine the combined weight of the different features of ontology. Experiments show that this method can maintain the stability and the commonality, and improve the <b>recall</b> <b>ratio</b> and the precision ratio at the same time...|$|E
40|$|Abstract. In this paper, {{we present}} an {{automatic}} system for bloody image detection on the internet. A quick blood model in HIS color space is employed {{to identify the}} blood regions and then the fractal dimension and the information entropy of these regions are extracted. At last all the features are fed to a SVM-classifier to tell whether the image is bloody or not. Our experiments on real-world web image data indicate that this system can detect the bloody images with high <b>recall</b> <b>ratio</b> and high precision ratio...|$|E
40|$|Investigation of {{end-user}} searching at the New York Hospital-Cornell University Medical Center (NYH-CUMC) {{revealed that}} 8 % of the physicians surveyed were end users, 63 % {{were interested in}} learning to search, and 29 % were not interested. When training sessions were offered at the Burke Rehabilitation Center, an affiliated institution, 50 % of the medical staff attended at least one class, but only 7 % of the total staff became frequent searchers. Analysis of the precision and <b>recall</b> <b>ratios</b> of searches conducted by five end users at HYH-CUMC indicated that the best results were obtained by end users who had been taught to search by experienced librarian-searchers. The quality of end user searches {{did not appear to}} be affected by the "friendliness" of the systems used, the frequency of searching habits, or the length of time that an end user had been searching...|$|R
40|$|International audienceIn {{this paper}} we tackle {{the issue of}} {{highlighting}} action content in animated movies, which proves to be a valuable information for retrieving movies in content-based video indexing systems. We use the hypothesis that action is in general related to a high frequency of video transitions, and adapt it to {{the constraints of the}} animation domain. First, we perform a video temporal segmentation by detecting cuts, fades, dissolves and specific color effects. Second, we analyze the movie rhythm, in terms of shot changes over a time unit. We target several action categories, namely: hot action, regular action and low action. This constitutes the action groundtruth. Finally, we employ a four step algorithm (thresholding, merging, pruning, restoring complementarity) to highlight movie parts according to the previously determined groundtruth. The efficiency of our approach was tested on several pre-labeled animated movies, achieving precision and <b>recall</b> <b>ratios</b> of more than 70 %...|$|R
40|$|International audienceIn this paper, {{we propose}} an {{audio-visual}} approach to video genre categorization. Audio information is extracted at block-level, {{which has the}} advantage of capturing local temporal information. At temporal structural level, we asses action contents with respect to human perception. Further, color perception is quantified with statistics of color distribution, elementary hues, color properties and relationship of color. The last category of descriptors determines statistics of contour geometry. An extensive evaluation of this multi-modal approach based on more than 91 hours of video footage is presented. We obtain average precision and <b>recall</b> <b>ratios</b> within [87 % − 100 %] and [77 % − 100 %], respectively,nwhile average correct classification is up to 97 %. Additionally, movies displayed according to feature-based coordinates in a virtual 3 D browsing environment tend to regroup with respect to genre, which has potential application with real content-based browsing systems...|$|R
40|$|Most {{of current}} {{ontology}} mapping methods can not treat different mapping tasks {{in different ways}} referred to {{the features of the}} input ontology. And they combine different mapping strategies without full consideration of the influences on mapping results caused mapping strategies. In view of the above questions, this paper proposes a dynamic mapping policy selection method which can pre-select mapping strategy and use entropy decision-making method to determine the combined weight of the selected strategy. Experiments show that this method can maintain the stability and the commonality, and improve the <b>recall</b> <b>ratio</b> and the precision ratio at the same time...|$|E
40|$|International audienceTo support {{users to}} quickly access {{information}} they need from the agricultural library’s vast information and to improve the low intelligence query service, a model for intelligent library information retrieval was constructed. The semantic web mode was introduced and the information retrieval framework was designed. The model structure consisted of three parts: Information data integration, user interface and information retrieval match. The key method supporting retrieval was designed. The traditional semantic similarity algorithm was improved according to its shortages. An algorithm based on semantic distance was designed and tested. The results can improve the <b>recall</b> <b>ratio</b> and precision of information retrieval, improving information retrieval performance...|$|E
40|$|In {{response}} to the demand of the logistics industry’s application, a retrieval algorithm for logistics bill is proposed which combines the local feature with the global feature of images, which solved the problem of rotation positioning and applied {{to a set of}} practical courier receipts retrieval system. By using the scale invariance principle of the local features combined with Zernike invariant moments of the global features, we can quickly calculate the image rotating angle and make the exact match. Experimental results show that this method not only keeps the well precision and <b>recall</b> <b>ratio</b> ability of SIFT features, but also reduces the counting times which are required by fine matching...|$|E
40|$|For {{a six-month}} period in 1965 the University of Colorado Medical Center Library {{conducted}} a demonstration project in which MEDLARS tapes {{received from the}} National Library of Medicine were searched on a Honeywell 800 computer located in Denver. Eighty-seven search requests were received {{from members of the}} UCMC staff during this period and successfully run. The average relevance and <b>recall</b> <b>ratios</b> were each in the neighborhood of 80 percent. Cost figures are given; it is calculated that an average search through two years' tapes costs about $ 200. The concept of the “search-month” is introduced, and it is shown that the average machine time per search-month will vary from 0. 67 to 2 minutes. It is concluded that the technical and economic feasibility of decentralized search centers using a centrally prepared input has been demonstrated. Data are presented on the basis of which the performance of the system may be judged...|$|R
30|$|<b>Recall</b> is the <b>ratio</b> of {{the number}} of {{correctly}} detected clones by the proposed tool to the total number of actual clones in the project by reference values.|$|R
40|$|We {{present a}} new {{approach}} to extracting information from unstructured documents based on an application ontology that describes a domain of interest. Starting with such an ontology, we formulate rules to extract constants and context keywords from unstructured documents. For each unstructured document of interest, we extract its constants and keywords and apply a recognizer to organize extracted constants as attribute values of tuples in a generated database schema. To make our approach general, we fix all the processes and change only the ontological description for a different application domain. In experiments we conducted on two di#erent types of unstructured documents taken from the Web, our approach attained <b>recall</b> <b>ratios</b> in the 80 % and 90 % range and precision ratios near 98 %. Keywords: unstructured data, semistructured data, information extraction, information structuring, ontology. 1 Introduction A relation in a structured database consists of a set of n- tuples. Each n-tupl [...] ...|$|R
