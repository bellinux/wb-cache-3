2|26|Public
40|$|Randomised {{techniques}} allow {{very big}} language models {{to be represented}} succinctly. However, being batch-based they are unsuitable for modelling an unbounded stream of language whilst maintaining a constant error rate. We present a novel randomised language model which uses an online perfect hash function to efficiently deal with unbounded text streams. Translation experiments over a text stream show that our online <b>randomised</b> <b>model</b> matches the performance of batch-based LMs without incurring the computational overhead associated with full retraining. This opens up the possibility of randomised language models which continuously adapt to the massive volumes of texts published on the Web each day. ...|$|E
40|$|Objective The {{purpose of}} this paper is to examine {{potential}} threats to generalisability of the results of a multicentre randomised controlled trial using data from A Very Early Rehabilitation Trial (AVERT). Design AVERT is a prospective, parallel group, assessor-blinded randomised clinical trial. This paper presents data assessing the generalisability of AVERT. Setting Acute stroke units at 44 hospitals in 8 countries. Participants The first 20   000 patients screened for AVERT, of whom 1158 were recruited and <b>randomised.</b> <b>Model</b> We use the Proximal Similarity Model, which considers the person, place, and setting and practice, as a framework for considering generalisability. As well as comparing the recruited patients with the target population, we also performed an exploratory analysis of the demographic, clinical, site and process factors associated with recruitment. Results The demographics and stroke characteristics of the included patients in the trial were broadly similar to population-based norms, with the exception that AVERT had a greater proportion of men. The most common reason for non-recruitment was late arrival to hospital (ie, 24  h). Overall, being older and female reduced the odds of recruitment to the trial. More women than men were excluded for most of the reasons, including refusal. The odds of exclusion due to early deterioration were particularly high for those with severe stroke (OR= 10. 4, p&# 60; 0. 001, 95...|$|E
40|$|A simple metric is {{presented}} for the accurate prediction of path delay variability during the automated synthesis of digital VLSI circuits. This allows circuit variability {{to be assessed}} at early stages within the design process with minimal computational effort, as extensive Monte Carlo or SSTA runs are not required. This paper introduces the metric and investigates its effectiveness. The final predictions of path delay variability {{are found to be}} within 10 % of measured path delay variability, with an average error of 3 %, for a series of test paths synthesised from <b>randomised</b> <b>models</b> of a 130 nm technology library. These <b>randomised</b> <b>models</b> are generated from a 3 D atomistic simulator and provide more accuracy than traditional Monte Carlo simulation runs...|$|R
40|$|We {{consider}} {{the use of}} <b>randomised</b> forward <b>models</b> and log-likelihoods within the Bayesian approach to inverse problems. Such random approximations to the exact forward model or log-likelihood arise naturally when a computationally expensive model is approximated using a cheaper stochastic surrogate, as in Gaussian process emulation (kriging), or {{in the field of}} probabilistic numerical methods. We show that the Hellinger distance between the exact and approximate Bayesian posteriors is bounded by moments of the difference between the true and approximate log-likelihoods. Example applications of these stability results are given for <b>randomised</b> misfit <b>models</b> in large data applications and the probabilistic solution of ordinary differential equations. Comment: 20 page...|$|R
40|$|Thirty-three wheat {{breeding}} {{trials were}} conducted from 1994 to 1996 in the Northern Grains Region (QLD and Northern NSW) of Australia {{to evaluate the}} influence of experimental designs and spatial analyses on the estimation of genotype effects for yield {{and their impact on}} selection decisions. The relative efficiency of the alternative designs and analyses was best measured by the average standard error of difference between line means. Both more effective designs and spatial analyses significantly improved the efficiency relative to the <b>randomised</b> complete block <b>model,</b> with the preferred model (which combined the design information and spatial trends) giving an average relative efficiency of 138 % over all 33 trials. When the Czekanowski similarity coefficient was used, none of the studied models were in full agreement with the <b>randomised</b> complete block <b>model</b> in the selection of the top lines. The agreement was influenced by selection proportions. Hence, the use of these methodologies can impact on the selection decisions in plant breeding...|$|R
40|$|We {{consider}} a multitype epidemic model {{which is a}} natural exten- sion of the <b>randomised</b> Reed-Frost epidemic <b>model.</b> The main result is the derivation of an asympotic Gaussian limit theorem for the ¯nal size of the epidemic. The method of proof is simpler, and more direct, than is used for similar results elsewhere in the epidemics literature. In particular, the results are specialised to epidemics upon extensions of the Bernoulli random graph...|$|R
40|$|Abstract. The satisfiability {{problem is}} widely used in {{research}} on combinatorial search and for industrial applications such as verification and planning. Real world search problem benchmarks are not plentiful, yet understanding search algorithm behaviour {{in the real world}} domain is highly important. This work justifies and investigates a <b>randomised</b> satisfiability problem <b>model</b> with modular properties akin to those observed in real world search problem domains. The proposed problem model provides a reliable benchmark which highlights pitfalls and advantages with various satisfiability search algorithms. ...|$|R
40|$|We {{investigate}} {{a new approach}} for SMT system training within the streaming model of computation. We develop and test incrementally retrainable models which, given an incoming stream of new data, can efficiently incorporate the stream data online. A naive approach using a stream would use an unbounded amount of space. Instead, our online SMT system can incorporate information from unbounded incoming streams and maintain constant space and time. Crucially, {{we are able to}} match (or even exceed) translation performance of comparable systems which are batch retrained and use unbounded space. Our approach is particularly suited for situations when there is arbitrarily large amounts of new training material and we wish to incorporate it efficiently and in small space. The novel contributions of this thesis are: 1. An online, <b>randomised</b> language <b>model</b> that can model unbounded input streams in constant space and time. 2. An incrementally retrainable translation model for both phrase-based and grammarbased systems. The model presented is efficient enough to incorporate nove...|$|R
40|$|Immunoglobulin A (IgA) {{is unique}} amongst {{antibodies}} {{in being able}} to form polymeric structures that may possess important functions in the pathology of specific diseases. IgA also forms complexes with other plasma proteins, the IgA 1 -human serum albumin (HSA) complex (IgA 1 -HSA) being typical. We have purified this complex using a novel two-step purification based on thiophilic chromatography and gel filtration, and characterised this. HSA is linked covalently to the tailpiece of IgA 1 by a disulphide bond between Cys 471 in IgA 1 and Cys 34 in HSA. IgA 1 -HSA binds to IgA receptors on neutrophils and monocytes, and elicits a respiratory burst that is comparable in magnitude to that of monomeric IgA 1. The solution arrangement of IgA 1 -HSA was identified by X-ray scattering and ultracentrifugation. The radius of gyration RG of 7. 5 (G 0. 3) nm showed that IgA 1 -HSA is more extended in solution than IgA 1 (RG of 6. 1 – 6. 2 nm). Its distance distribution function P(r) showed two peaks that indicated a well-separated solution structure similar to that for IgA 1, and a maximum dimension of 25 nm, which is greater than that of 21 nm for IgA 1. Sedimentation equilibrium showed that the IgA 1 :HSA stoichiometry is 1 : 1. Sedimentation velocity resulted in a sedimentation coefficient of 6. 4 S and a frictional ratio of 1. 87, which is greater than that of 1. 56 for IgA 1. The constrained modelling of the IgA 1 -HSA structure using known structures for IgA 1 and HSA generated 2432 conformationally <b>randomised</b> <b>models</b> of which 52 gave good scattering fits. The HSA structure was located {{at the base of the}} Fc fragment in IgA 1 in an extended arrangement. Such a structure accounts for the functional activity of IgA 1 - HSA, and supports our previous modelling analysis of the IgA 1 solution structure. The IgA 1 -HSA complex may suggest the potential for creating a new class of targeted therapeutic reagents based on the coupling of IgA 1 to carrier proteins...|$|R
40|$|A diffusion-based coupled oxidation, {{intergranular}} {{damage and}} multisite <b>randomised</b> crack growth <b>model</b> for environmentally assisted oxidation/carburisation and creep time dependent material is proposed. A combined grain boundary and grain mesh structure is employed for simulating surface hardening and intergranular cracking {{resulting from a}} surface gas/solid carbon diffusion and bulk creep interaction by assuming variations in their strength ratios. Using 316 H properties at 550 ° C the predicted surface intergranular cracks, due to both carburisation and creep, and subsequent crack growth are analysed {{in terms of their}} rupture and failure strains are compared to as received 316 H data to validate the model...|$|R
40|$|Language {{models are}} {{probability}} distributions over {{a set of}} unilingual natural language text used in many natural language processing tasks such as statistical machine trans-lation, information retrieval, and speech processing. Since more well-formed training data means a better model and the increased availability of text via the Internet, the size of language modelling n-gram data sets have grown exponentially the past few years. The latest data sets available can no longer fit on a single computer. A recent investi-gation reported first known use of a probabilistic data structure to create a <b>randomised</b> language <b>model</b> capable of storing probability information for massive n-gram sets in {{a fraction of the}} space normally needed. We report and compare the properties of lossy language models using two probabilistic data structures: the Bloom filter and lossy dictionary. The Bloom filter has exceptional space requirements and only one-sided, false positive error returns but it is computationally slow in scale which is a potential drawback for a structure being queried millions of times per sentence. Lossy dictionar-ies have low space requirements and are very fast but with two-sided error that return...|$|R
40|$|Sample size {{determination}} is {{an essential}} component in public health survey designs on sensitive topics (e. g. drug abuse, homosexuality, induced abortions and pre or extramarital sex). Recently, non-randomised models {{have been shown to}} be an efficient and cost effective design when comparing with <b>randomised</b> response <b>models.</b> However, sample size formulae for such non-randomised designs are not yet available. In this article, we derive sample size formulae for the non-randomised triangular design based on the power analysis approach. We first consider the one-sample problem. Power functions and their corresponding sample size formulae for the one- and two-sided tests based on the large-sample normal approximation are derived. The performance of the sample size formulae is evaluated in terms of (i) the accuracy of the power values based on the estimated sample sizes and (ii) the sample size ratio of the non-randomised triangular design and the design of direct questioning (DDQ). We also numerically compare the sample sizes required for the randomised Warner design with those required for the DDQ and the non-randomised triangular design. Theoretical justification is provided. Furthermore, we extend the one-sample problem to the two-sample problem. An example based on an induced abortion study in Taiwan is presented to illustrate the proposed methods. © The Author(s), 2011. link_to_subscribed_fulltex...|$|R
40|$|Objective: To {{evaluate}} effectiveness, {{safety and}} cost-effectiveness of Computerised Clinical Decision Support (CCDS) for paramedics attending {{older people who}} fall. Design: Cluster trial <b>randomised</b> by paramedic; <b>modelling.</b> Setting: 13 ambulance stations in two UK emergency ambulance services. Participants: 42 of 409 eligible paramedics, who attended 779 older patients for a reported fall. Interventions: Intervention paramedics received CCDS on Tablet computers to guide patient care. Control paramedics provided care as usual. One service had already installed electronic data capture. Main Outcome Measures: Effectiveness: patients referred to falls service, patient reported {{quality of life and}} satisfaction, processes of care. Safety: Further emergency contacts or death within one month. Cost-Effectiveness: Costs and quality of life. We used findings from published Community Falls Prevention Trial to model cost-effectiveness. Results: 17 intervention paramedics used CCDS for 54 (12. 4...|$|R
40|$|Neural {{networks}} {{have been widely}} used as predictive models to fit data distribution, and they could be implemented through learning a collection of samples. In many applications, however, the given dataset may contain noisy samples or outliers which may result in a poor learner model in terms of generalization. This paper contributes to a development of robust stochastic configuration networks (RSCNs) for resolving uncertain data regression problems. RSCNs are built on original stochastic configuration networks with weighted least squares method for evaluating the output weights, and the input weights and biases are incrementally and randomly generated by satisfying with a set of inequality constrains. The kernel density estimation (KDE) method is employed to set the penalty weights for each training samples, so that some negative impacts, caused by noisy data or outliers, on the resulting learner model can be reduced. The alternating optimization technique is applied for updating a RSCN model with improved penalty weights computed from the kernel density estimation function. Performance evaluation is carried out by a function approximation, four benchmark datasets and a case study on engineering application. Comparisons to other robust <b>randomised</b> neural <b>modelling</b> techniques, including the probabilistic robust learning algorithm for neural networks with random weights and improved RVFL networks, indicate that the proposed RSCNs with KDE perform favourably and demonstrate good potential for real-world applications. Comment: 14 page...|$|R
40|$|The {{problem of}} how to acquire a model of a {{physical}} robot, which is fit for evolution of controllers that can subsequently be used to control that robot, is considered in the context of racing a radio-controlled toy car around a <b>randomised</b> track. Several <b>modelling</b> techniques are compared, and the specific properties of the acquired models that influence the quality of the evolved controller are discussed. As we aim to minimise the amount of domain knowledge used, we further investigate the relation between the assumptions about the modelled system made by particular modelling techniques and the suitability of the acquired models as bases for controller evolution. We find that none of the models acquired is good enough on its own, and that a key to evolving robust behaviour is to evaluate controllers simultaneously on multiple models during evolution. Examples of successfully evolved racing control for the physical car are analysed...|$|R
40|$|Balls-into-bins {{games for}} uniform bins {{are widely used}} to <b>model</b> <b>randomised</b> load {{balancing}} strategies. Recently, balls-into-bins games have been analysed {{under the assumption that}} the selection probabilities for bins are not uniformly distributed. These new models are motivated by properties of many peer-to-peer (P 2 P) networks. In this paper we consider scenarios in which non-uniform selection probabilities help to balance the load among the bins. While previous evaluations try to find strategies for identical bins, we investigate heterogeneous bins where the “capacities” of the bins might differ significantly. We look at the allocation of m balls into n bins of total capacity C where each ball has d random bin choices. For such heterogeneous environments we show that the maximum load remains bounded by lnln(n) /ln(d) +O(1) w. h. p. if the number of balls m equals the total capacity C. Further analytical and simulative results show better bounds and values for the maximum loads in special cases...|$|R
40|$|Aims. To {{investigate}} {{the effects of}} using bromazepam on the relative power in alpha while performing a typing task. Bearing in mind the particularities of each brain hemisphere, our hypothesis was that measuring the relative power would allow its to {{investigate the}} effects of bromazepam oil specific areas of the cortex. More, specifically, we expected to observe different patterns of powers in sensory-motor integration, attention and activation processes. Subjects and methods. The sample {{was made up of}} 39 subjects (15 males and 24 females) {{with a mean age of}} 30 +/- 10 years. The control (placebo) and experimental (3 mg and 6 mg of bromazepam) groups were trained ill the typing task with a <b>randomised</b> double-blind <b>model.</b> Results. A three-way ANOVA and Scheffe test were used to analyse interactions between the factors condition and moment, and between condition and sector Conclusions. The doses used ill this study facilitated motor performance of the typing task. Ill this study, the use of the drug did not prevent learning of the task, but it did appear to concentrate mental effort on more restricted and specific aspects of typing. It also seemed to influence the rhythm and effectiveness of the operations performed during mechanisms related to the encoding and storage often, information. Likewise, a predominance of activity was observed in the left (dominant) frontal area in the 3 mg bromazepam group, which indicates that this close of the drug affords the subject a greater degree of directionality of cortical activity for planning and performing the task. [REV NEUROL 2009; 49 : 295 - 9...|$|R
40|$|The Moran process, as {{studied by}} Lieberman, Hauert and Nowak, is a <b>randomised</b> {{algorithm}} <b>modelling</b> {{the spread of}} genetic mutations in populations. The algorithm runs on an underlying graph where individuals correspond to vertices. Initially, one vertex (chosen u. a. r.) possesses a mutation, with fitness r> 1. All other individuals have fitness 1. During {{each step of the}} algorithm, an individual is chosen with probability proportional to its fitness, and its state (mutant or non-mutant) is passed on to an out-neighbour which is chosen u. a. r. If the underlying graph is strongly connected then the algorithm will eventually reach fixation, in which all individuals are mutants, or extinction, in which no individuals are mutants. An infinite family of directed graphs is said to be strongly amplifying if, for every r> 1, the extinction probability tends to 0 as the number of vertices increases. Lieberman et al. proposed two potentially strongly-amplifying families - superstars and metafunnels. Heuristic arguments have been published, arguing that there are infinite families of superstars that are strongly amplifying. The same has been claimed for metafunnels. In this paper, we give the first rigorous proof that there is an infinite family of directed graphs that is strongly amplifying. We call the graphs in the family "megastars". When the algorithm is run on an n-vertex graph in this family, starting with a uniformly-chosen mutant, the extinction probability is roughly $n^{- 1 / 2 }$ (up to logarithmic factors). We prove that all infinite families of superstars and metafunnels have larger extinction probabilities (as a function of n). Finally, we prove that our analysis of megastars is fairly tight - there is no infinite family of megastars such that the Moran algorithm gives a smaller extinction probability (up to logarithmic factors). Comment: minor update...|$|R
40|$|Abstract Background In {{addition}} to their antimicrobial activity, antibiotics modulate cellular host defence. Granulocyte-colony stimulating factor (G-CSF) is also a well known immunomodulator; however {{little is known about}} the interactions of G-CSF with antibiotics. We investigated in septic rats the effects of two antibiotic combinations with G-CSF. Methods In two clinic <b>modelling</b> <b>randomised</b> trials (CMRTs), male Wistar rats were anesthetized, given antibiotic prophylaxis, had a laparotomy with peritoneal contamination and infection (PCI), and were randomly assigned (n = 18 rats/group) to: 1) PCI only; 2) PCI+antibiotic; and, 3) PCI+antibiotic+G-CSF prophylaxis (20 μg/kg, three times). This sequence was conducted first with 10 mg/kg coamoxiclav, and then with ceftriaxone/metronidazole (Cef/met, 10 / 3 mg/kg). In additional animals, the blood cell count, migration and superoxide production of PMNs, systemic TNF-α and liver cytokine mRNA expression levels were determined. Results Only the combination coamoxiclav plus G-CSF improved the survival rate (82 vs. 44 %, p Conclusion There are substantial differences in the interaction of antibiotics with G-CSF. Therefore, the selection of the antibiotic for combination with G-CSF in sepsis treatment should be guided not only by the bacteria to be eliminated, but also by the effects on antimicrobial functions of PMNs and the cytokine response. </p...|$|R
40|$|This study {{analyses}} {{the financial}} risk faced by representative mixed-enterprise farm businesses in four regions of south-eastern Australia. It uses discrete stochastic programming to optimise the ten-year cash flow margins produced by these farms operating three different farming systems. Monte Carlo analysis {{is used to}} produce a risk profile for each scenario, derived from multiple runs of this optimised <b>model,</b> <b>randomised</b> for commodity prices and decadal growing season rainfall since 1920. This analysis shows that {{the performance of the}} enterprise mixes at each site is characterised more by the level of variability of possible outcomes than by the mean values of financial outputs. It demonstrates that relying on mean values for climate and prices disguises the considerable risks involved with cropping in this area. Diversification into a Merino sheep enterprise marginally reduced the probability of financial loss at all sites. This study emphasises the fact that the variability, or risk, associated with all scenarios far exceeds the likely change in cash margins due to innovation and good management. It further shows that farm managers should give a higher priority to adopting innovations which reduce costs, rather than increase productivity, in order to reduce risk. Further analysis shows that the current static measures of financial performance (gross margins, profit and cash margins) do not characterise the risk-adjusted performance of the various farming systems and almost certainly result in a flawed specification of best-practice farm management in south-eastern Australia. Farm Management,...|$|R
40|$|Background: Older people {{participate}} in exercise programmes {{to reduce the}} risk of falls but no study has investigated a specific balance strategy training intervention presented in a workstation format for small groups. Objective: To determine whether a specific balance strategy training programmeme delivered in a workstation format was superior to a community based exercise class programme for reducing falls. Design: A <b>randomised</b> controlled trial <b>model.</b> Setting: Neurological Disorders, Ageing and Balance Clinic, Department of Physiotherapy, The University of Queensland. Subjects: 73 males and females over 60 years, living independently in the community and who had fallen in the previous year were recruited. Methods: All subjects received a falls risk education booklet and completed an incident calendar for the duration of the study. Treatment sessions were once a week for 10 weeks. Subject assessment before and after intervention and at 3 months follow-up included number of falls, co-morbidities, medications, community services and activity level, functional motor ability, clinical and laboratory balance measures and fear of falling. Results: All participants significantly reduced the number of falls (P < 0. 000). The specific balance strategy intervention group showed significantly more improvement in functional measures than the control group (P= 0. 034). Separate group analyses indicated significantly improved performance in functional motor ability and most clinical balance measures for the balance group (P < 0. 04). The control group only improved in TUG and TUGcog. Conclusions: The results provide evidence that all participants achieved a significant reduction in falls. Specific balance strategy training using workstations is superior to traditional exercise classes for improving function and balance...|$|R
40|$|In {{this thesis}} we {{consider}} sequential probabilistic programs. Such programsare {{a means to}} <b>model</b> <b>randomised</b> algorithms in computer science. They facilitatethe formal analysis of performance and correctness of algorithms or securityaspects of protocols. We develop an operational semantics for probabilistic programs and showit to be equivalent to the expectation transformer semantics due to McIver andMorgan. This {{connection between the two}} kinds of semantics provides a deeperunderstanding of the behaviour of probabilistic programs and is instrumental totransfer results between communities that use transition systems such as Markovdecision processes to reason about probabilistic behaviour and communities thatfocus on deductive verification techniques based on expectation transformers. As a next step, we add the concept of observations and extend both semanticsto facilitate the calculation of expectations which are conditioned on the fact thatno observation is violated during the program’s execution. Our main contributionhere is to explore issues that arise with non-terminating, non-deterministic or infeasibleprograms and provide semantics that are generally applicable. Additionally,we discuss several program transformations to facilitate the understandingof conditioning in probabilistic programming. In {{the last part of the}} thesis we turn our attention to the automated verificationof probabilistic programs. We are interested in automating inductive verificationtechniques. As usual the main obstacle in program analysis are loopswhich require either the calculation of fixed points or the generation of inductiveinvariants for their analysis. This task, which is already hard for standard, i. e. non-probabilistic, programs, becomes even more challenging as our reasoningbecomes quantitative. We focus on a technique to generate quantitative loop invariantsfrom user defined templates. This approach is implemented in a softwaretool called Prinsys and evaluated on several examples...|$|R
40|$|The Transtheoretical Model (TTM) {{proposes that}} stage {{matching}} improves {{the effectiveness of}} behaviour change interventions, such as for smoking cessation. It also proposes that standard smoking cessation interventions are matched to the relatively few smokers in the preparation stage and will not assist the majority of smokers, {{who are in the}} precontemplation or contemplation stages. This study tested the hypothesis that stage-matched interventions increase movement through the stages relative to interventions not stage-matched. It also tested the hypothesis that the relative effectiveness of stage-matched interventions is greater for people in precontemplation or contemplation (stage-matched for TTM but not for control) than for people in preparation (where both intervention and control were stage-matched). A total of 2471 UK adult smokers were randomised to either control or TTM-based self-help intervention and followed up 12 months after beginning the programme. Content analysis of the intervention and control self-help interventions examined whether control interventions were action-oriented, meaning they emphasised the processes of change relevant for preparation and action. Participants in the TTM arm were slightly more likely to make a positive move in stage, but this was not significant. There was no evidence that the TTM-based intervention was more effective for participants in precontemplation or contemplation than for participants in preparation. There was no evidence that TTM-based interventions were effective in this trial. The control intervention advocated process use appropriate for all stages and was not action-orientated. Stage matching does not explain the modest effects of TTM-based interventions over control interventions observed in some trials. These effects may instead have occurred because TTM-based interventions were more intensive than control interventions. Smoking cessation Transtheoretical <b>Model</b> <b>Randomised</b> controlled trial Stage matching UK Intervention...|$|R
40|$|In {{this thesis}} we {{consider}} {{two sets of}} combinatorial structures defined on an Eulerian graph: the Eulerian orientations and Euler tours. We {{are interested in the}} computational problems of counting (computing the number of elements in the set) and sampling (generating a random element of the set). Specifically, we are interested in the question of when there exists an efficient algorithm for counting or sampling the elements of either set. The Eulerian orientations of a number of classes of planar lattices are of practical significance as they correspond to configurations of certain models studied in statistical physics. In 1992 Mihail and Winkler showed that counting Eulerian orientations of a general Eulerian graph is #P-complete and demonstrated that the problem of sampling an Eulerian orientation can be reduced to the tractable problem of sampling a perfect matching of a bipartite graph. We present a proof that this problem remains #Pcomplete when the input is restricted to being a planar graph, and analyse a natural algorithm for generating random Eulerian orientations of one of the afore-mentioned planar lattices. Moreover, we make some progress towards classifying the range of planar graphs on which this algorithm is rapidly mixing by exhibiting an infinite class of planar graphs for which the algorithm will always take an exponential amount of time to converge. The problem of counting the Euler tours of undirected graphs has proven to be less amenable to analysis than that of Eulerian orientations. Although it has been known for many years that the number of Euler tours of any directed graph can be computed in polynomial time, until recently very little was known about the complexity of counting Euler tours of an undirected graph. Brightwell and Winkler showed that this problem is #P-complete in 2005 and, apart from a few very simple examples, e. g., series-parellel graphs, there are no known tractable cases, nor are there any good reasons to believe the problem to be intractable. Moreover, despite several unsuccessful attempts, there has been no progress made on the question of approximability. Indeed, this problem was considered {{to be one of the}} more difficult open problems in approximate counting since long before the complexity of exact counting was resolved. By considering a <b>randomised</b> input <b>model,</b> we are able to show that a very simple algorithm can sample or approximately count the Euler tours of almost every d-in/d-out directed graph in expected polynomial time. Then, we present some partial results towards showing that this algorithm can be used to sample or approximately count the Euler tours of almost every 2 d-regular graph in expected polynomial time. We also provide some empirical evidence to support the unproven conjecture required to obtain this result. As a sideresult of this work, we obtain an asymptotic characterisation of the distribution of the number of Eulerian orientations of a random 2 d-regular graph. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|To {{evaluate}} effectiveness, {{safety and}} cost-effectiveness of Computerised Clinical Decision Support (CCDS) for paramedics attending {{older people who}} fall. Cluster trial <b>randomised</b> by paramedic; <b>modelling.</b> 13 ambulance stations in two UK emergency ambulance services. 42 of 409 eligible paramedics, who attended 779 older patients for a reported fall. Intervention paramedics received CCDS on Tablet computers to guide patient care. Control paramedics provided care as usual. One service had already installed electronic data capture. Effectiveness: patients referred to falls service, patient reported {{quality of life and}} satisfaction, processes of care. Further emergency contacts or death within one month. Costs and quality of life. We used findings from published Community Falls Prevention Trial to model cost-effectiveness. 17 intervention paramedics used CCDS for 54 (12. 4 %) of 436 participants. They referred 42 (9. 6 %) to falls services, compared with 17 (5. 0 %) of 343 participants seen by 19 control paramedics [Odds ratio (OR) 2. 04, 95 % CI 1. 12 to 3. 72]. No adverse events were related to the intervention. Non-significant differences between groups included: subsequent emergency contacts (34. 6 % versus 29. 1 %; OR 1. 27, 95 % CI 0. 93 to 1. 72); quality of life (mean SF 12 differences: MCS - 0. 74, 95 % CI - 2. 83 to + 1. 28; PCS - 0. 13, 95 % CI - 1. 65 to + 1. 39) and non-conveyance (42. 0 % versus 36. 7 %; OR 1. 13, 95 % CI 0. 84 to 1. 52). However ambulance job cycle time was 8. 9 minutes longer for intervention patients (95 % CI 2. 3 to 15. 3). Average net cost of implementing CCDS was £ 208 per patient with existing electronic data capture, and £ 308 without. Modelling estimated cost per quality-adjusted life-year at £ 15, 000 with existing electronic data capture; and £ 22, 200 without. Intervention paramedics referred twice as many participants to falls services with no difference in safety. CCDS is potentially cost-effective, especially with existing electronic data capture. ISRCTN Register ISRCTN 10538608...|$|R
40|$|A thesis {{submitted}} in fulfilment of {{the requirements}} for the degree of Doctor of Philosophy in the Department of Computing, Faculty of Science and Engineering, Macquarie University". " 2014 ". Bibliography: pages 75 - 82. 1. Introduction [...] 2. Linking operational and denotational semantics [...] 3. Conditional probabilities and expectations [...] 4. Automated analysis [...] Chapter 5. Conclusion and future work [...] Appendices. In this thesis we consider sequential probabilistic programs. Such programs are a means to <b>model</b> <b>randomised</b> algorithms in computer science. They facilitate the formal analysis of performance and correctness of algorithms or security aspects of protocols. We develop an operational semantics for probabilistic programs and show it to be equivalent to the expectation transformer semantics due to McIver and Morgan. This {{connection between the two}} kinds of semantics provides {{a deeper understanding of the}} behaviour of probabilistic programs and is instrumental to transfer results between communities that use transition systems such as Markov decision processes to reason about probabilistic behaviour and communities that focus on deductive verification techniques based on expectation transformers. As a next step, we add the concept of observations and extend both semantics to facilitate the calculation of expectations which are conditioned on the fact that no observation is violated during the program's execution. Our main contribution here is to explore issues that arise with non-terminating, non-deterministic or infeasible programs and provide semantics that are generally applicable. Additionally, we discuss several program transformation to facilitate the understanding of conditioning in probabilistic programming. In the last part of the thesis we turn our attention to the automated verification of probabilistic programs. We are interested in automating inductive verification techniques. As usual the main obstacle in program analysis are loops which require either the calculation of fixed points or the generation of inductive invariants for their analysis. This task, which is already hard for standard, i. e. non-probabilistic, programs, becomes even more challenging as our reasoning becomes quantitative. We focus on a technique to generate quantitative loop invariants from user defined templates. This approach is implemented in a software tool called Prinsys and evaluated on several examples. Mode of access: World wide web 1 online resource (x, 85 pages) illustrations (some coloured...|$|R
40|$|Background: Complications such as {{surgical}} site {{infection and}} post-operative adhesion formation following abdominal surgery are common {{and there has}} been a call for new approaches to reduce these complications. Insufflation of humidified-warm CO 2 to protect the abdominal cavity during open abdominal surgery has been proposed as a promising new therapeutic invention; however mechanisms of action have not yet been fully investigated. After first setting out to develop a controlled animal model, this research has tested the hypotheses that humidified-warm CO 2 can increase tissue oxygen partial pressure and that humidified-warm CO 2 can reduce loss of peritoneal mesothelium that may be caused by exposure of the peritoneum to the dry operating room. Methods: A systematic review was conducted to determine the current state of knowledge regarding the effect of gaseous exposure of the peritoneum during abdominal surgery on loss of peritoneal mesothelium. A rat model was then developed to allow the examination of gaseous exposure of the peritoneum during open abdominal surgery. Using that <b>model,</b> <b>randomised</b> cross-over trials were conducted to investigate the effect of humidified-warm CO 2 on sub-peritoneal tissue oxygen partial pressure, and to elucidate the relative effect of dry versus humidified-warm CO 2. Finally, a controlled trial was conducted to determine whether exposure of the peritoneum to the operating room environment during open abdominal surgery causes sufficient desiccation to result in loss of peritoneal mesothelium, and furthermore, whether any mesothelial loss can be reduced by the insufflation of humidified-warm CO 2. Findings: The systematic review of the literature revealed that the creation of pneumoperitoneum for laparoscopic surgery can cause loss of areas of the mesothelial layer of the peritoneum. The degree of disruption appears to be reduced at lower pneumoperitoneum pressures and by the reduction of desiccation via warming and humidification of the insufflation gas. However, the systematic review revealed a gap in current knowledge regarding understanding of the effect of gaseous exposure during open abdominal surgery, including whether desiccation of the peritoneum during open abdominal surgery is sufficient to damage the peritoneal mesothelium. Therefore, a controlled rat model was developed to investigate the effect of gaseous exposure during open abdominal surgery. Key properties of the model were: assurance of adequate exposure of the peritoneum; modelling of operating room air flow; creation of a CO 2 environment within the constraints of the model; appropriate mechanical ventilation and anaesthetic/analgesic management protocol; measurement of tissue oxygen partial pressure; optimisation of protocols to ensure protection of the mesothelium during tissue fixation and processing. Results of investigations utilising the model showed firstly that insufflation of humidified-warm CO 2 caused an immediate and clinically significant increase in tissue oxygen partial pressure. Two subsequent sets of randomised cross-over trials showed that exposure to CO 2 and exposure to humidity/warmth individually increased PtO 2 and that these effects were additive. Finally, the trial investigating mesothelial loss showed that simply exposing the peritoneum to standardised operating room airflow during open abdominal surgery led to significant loss of peritoneal mesothelium that was prevented by insufflation of humidified-warm CO 2. Key methods that were utilised during this research were: design of a rat model and manipulation of conditions during experimental use; endotracheal intubation; blood gas measurement; filtering and displaying signals for monitoring of heart-rate, body temperature and pulse oximetry during general anaesthesia; measurement of tissue oxygen partial pressure; scanning electron microscopy; light microscopy; fluorescent microscopy; biochemical assay. Conclusions: In a carefully designed rat model of open abdominal surgery, humidifiedwarm CO 2 insufflation increased tissue oxygen partial pressure and prevented loss of peritoneal mesothelium. These are important mechanisms in the prevention of postoperative complications. Humidified-warm CO 2 insufflation during abdominal surgery may have important clinical implications...|$|R

