4|10000|Public
40|$|Earthquakes in {{a nuclear}} {{installation}} can overload a piping system which is not flexible enough. These loads can be forces, moments and stresses working on the pipes or equipments. If the load is too large and exceed the allowable limits, the piping and equipment can be damaged and lead to overall system operation failure. The load received by piping systems can be reduced by making adequate piping flexibility, so all the loads can be transmitted homogenously throughout the pipe without load concentration at certain point. In this research the analysis of piping stress has been conducted to determine the size of loads that occured in the piping of primary cooling system of TRIGA 2000 Reactor, Bandung if an earthquake happened in the reactor site. The analysis was performed using Caesar II software-based finite element method. The ASME code B 31. 1 arranging the design of piping systems for power generating system (Power Piping Code) was used as <b>reference</b> <b>analysis</b> <b>method.</b> Modeling of piping systems {{was based on the}} cooling piping that has already been installed and the existing data reported in Safety Analysis Reports (SARs) of TRIGA 2000 reactor, Bandung. The quake considered in this analysis i...|$|E
40|$|Three {{methods for}} the {{determination}} of chloramines in water were compared using pH-buffered nanopure water and natural organic matter (NOM) solutions. We investigated whether the N,N-diethyl-p-phenylenediamine (DPD) colorimetric method and/or an adapted indophenol method (Hach MonochlorF) are suitable for determining the concentration of monochloramine in drinking water. Membrane introduction mass spectrometry (MIMS) {{was used as a}} <b>reference</b> <b>analysis</b> <b>method</b> to determine the different chloramine species in water. All methods measured monochloramine accurately in Nanopure water, but the DPD colorimetric method measured higher residuals (inorganic and organic chloramines) than MonochlorF or MIMS when in the presence of NOM due to organic chloramines. The indophenol method (MonochlorF) accurately detected only monochloramine and not other chloramine forms. Overall, the monochloramine concentration measured by MonochlorF was comparable with the MIMS results. A combined chlorine residual approach by the DPD colorimetric method does not differentiate between monochloramine and organic chloramines. Therefore, DPD colorimetric methods can overestimate disinfection efficacy in chloraminated water systems because of interference from organic chloramines that have no or poor bactericidal ability. Compared with the DPD colorimetric method, MonochlorF is a better choice for chloraminated water systems. (C) 2007 Elsevier Ltd. All rights reserved...|$|E
40|$|This is an open-access article {{distributed}} {{under the}} terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Within the HarvestPlus program there are many collaborators currently using X-Ray Fluorescence (XRF) spectroscopy to measure Fe and Zn in their target crops. In India, five HarvestPlus wheat collaborators have laboratories that conduct this analysis and their throughput has increased significantly. The benefits of using XRF are its ease of use, minimal sample preparation and high throughput analysis. The lack of commercially available calibration standards has led to a need for alternative calibration arrangements for many of the instruments. Consequently, the majority of instruments have either been installed with an electronic transfer of an original grain calibration set developed by a preferred lab, or a locally supplied calibration. Unfortunately, neither of these methods has been entirely successful. The electronic transfer is unable to account for small variations between the instruments, whereas the use of a locally provided calibration set is heavily reliant on the accuracy of the <b>reference</b> <b>analysis</b> <b>method,</b> which is particularly difficult to achieve when analyzing low levels of micronutrient. Consequently, we have developed a calibration method that uses non-matrix matched glass disks. Here we present the validation of this method and show this calibration approach can improve the reproducibility and accuracy of whole grain wheat analysis on 5 different XRF instruments across the HarvestPlus breeding program...|$|E
30|$|The {{distribution}} of estimated regional BPND values using the two <b>reference</b> tissue <b>analysis</b> <b>methods</b> from tau and Aβ PET scans over all subjects {{was compared to}} assess the performance of each method.|$|R
40|$|A {{quantitative}} methodology {{based on}} in-line UV/Vis absorption spectroscopy and on-line evaporative light scattering detection for supercritical fluid extraction is proposed. The method {{was applied to}} the extraction of carotenoids, chlorophyll A, ergosterol and total lipids from microalgae. One regression technique and two curve resolution techniques were applied on the absorption spectroscopy data and evaluated, namely classical least squares, multivariate curve resolution by alternating least squares and parallel factor analysis (PARAFAC 2). The two former both generated useful models, furthermore multivariate curve resolution also successfully enabled estimation of both spectra and concentration profiles of the analytes. The integrated extraction profiles of each analyte were compared with analysis of the collected fractions using <b>reference</b> <b>analysis</b> <b>methods</b> Precision, in regards to quantification of the analytes in the eluent, was better using in-line measurements compared to off-line measurements by UV/Vis absorption spectroscopy, supercritical fluid chromatography with mass spectrometry and liquid chromatography with UV/Vis detection...|$|R
40|$|The use of “simplified ” (<b>reference</b> stress) <b>analysis</b> <b>methods</b> is {{discussed}} and illustrated for primary load high temperature design. Elastic methods {{are the basis}} of the ASME Section III, Subsection NH primary load design procedure. There are practical drawbacks with this current NH approach, particularly for complex geometries and temperature gradients. The paper describes an approach which addresses these difficulties through the use of temperature-dependent elastic, perfectly-plastic analysis. Traditionally difficulties associated with discontinuity stresses, inelastic strain concentrations and multiaxiality are addressed. A procedure is identified to provide insight into how this approach could be implemented. Though preliminary in nature, it is intended to provide a basis for further development and eventual Code adaptation. * Notice: This submission was sponsored by a contractor of th...|$|R
40|$|Earthquakes in {{a nuclear}} {{installation}} can overload a piping system which is not flexible enough. These loads can be forces, moments and stresses working on the pipes or equipments. If the load is too large and exceed the allowable limits, the piping and equipment can be damaged and lead to overall system operation failure. The load received by piping systems can be reduced by making adequate piping flexibility, so all the loads can be transmitted homogenously throughout the pipe without load concentration at certain point. In this research the analysis of piping stress has been conducted to determine the size of loads that occured in the piping of primary cooling system of TRIGA 2000 Reactor, Bandung if an earthquake happened in the reactor site. The analysis was performed using Caesar II software-based finite element method. The ASME code B 31. 1 arranging the design of piping systems for power generating system (Power Piping Code) was used as <b>reference</b> <b>analysis</b> <b>method.</b> Modeling of piping systems {{was based on the}} cooling piping that has already been installed and the existing data reported in Safety Analysis Reports (SARs) of TRIGA 2000 reactor, Bandung. The quake considered in this analysis is the earthquake that occurred due to the Lembang fault, since it has the Peak Ground Acceleration (PGA) in the Bandung TRIGA 2000 reactor site. The analysis results showed that in the static condition for sustain and expansion loads, the stress fraction in all piping lines does not exceed the allowable limit. However, during operation moment, in dynamic condition, the primary cooling system is less flexible at sustain load, ekspansi load, and combination load and the stress fraction have reached 95, 5 %. Therefore a pipeline modification (rerouting) is needed to make pipe stress does not exceed the allowable stress. The pipeline modification was carried out by applied a gap of 3 mm in the X direction of the support at node 25 and eliminate the support at the node 30, also a gap of 3 mm was applied in X and Z directions of the support at the node 155. The axial force (FY) that occurred in the pump outlet nozzle (dia. 4 in.) of PriPump line have also exceeded the allowable limit that lead to the pump nozzle failure during an earthquake of Lembang fault. The modifications is necessary to be applied on the cooling system for PriPump line so the nozzle would not receive the force that exceed the allowable limits. The modification can be done by removing the support at node 105 and node 135 so the primary cooling system piping of Bandung TRIGA 2000 reactor would be safe to operate during an earthquake originated from Lembang fault...|$|E
40|$|Abstract—This paper {{describes}} the tool {{support for a}} framework for performing statistical WCET analysis of real-time embedded systems by using bootstrapping sampling and Extreme Value Theory (EVT). To be specific, bootstrapping sampling is used to generate timing traces, which not only fulfill the requirements given by statistics and probability theory, but also are robust {{to use in the}} context of estimating the WCET of programs. Next, our proposed statistical inference uses EVT to analyze such timing traces, and computes a WCET estimate of the target program, pertaining to a given predictable probability. The evaluation results show that our proposed method could have the potential of being able to provide a tighter upper bound on the WCET estimate of the programs under analysis, when compared to the estimates given by the <b>referenced</b> WCET <b>analysis</b> <b>methods.</b> I...|$|R
40|$|This chapter {{describes}} <b>reference</b> <b>analysis,</b> a <b>method</b> {{to produce}} Bayesian inferential statements which only {{depend on the}} assumed model and the available data. Statistical information theory is used to define the reference prior function as a mathematical description of that situation where data would best dominate prior knowledge about the quantity of interest. Reference priors are not descriptions of personal beliefs; they are proposed as formal consensus prior functions {{to be used as}} standards for scientific communication. Reference posteriors are obtained by formal use of Bayes theorem with a reference prior. Reference prediction is achieved by integration with a reference posterior. Reference decisions are derived by minimizing a reference posterior expected loss. An information theory based loss function, the intrinsic discrepancy, may be used to derive reference procedures for conventional inference problems in scientific investigation, such as point estimation, region estimation and hypothesis testing...|$|R
40|$|The dry-extract {{system for}} (near) {{infrared}} (DESIR) technique was implemented using reflectance near-infrared spectroscopy in context of detection of contact pesticide residues on fruit. Based on chemical structure, spectra features and regression statistics for PLSR models, a product containing metiram and pyraclostrobin was chosen from six pesticides for further consideration. Regression models based on spectra of dry extracts of aqueous solutions and either acetone or water washes of contaminated fruit were encouraging (RMSECV of approximately 0. 03 - 0. 06 mg a. i.). This level of analytical performance {{would support the}} use of the technique as a rapid screening tool, with suspect samples then subject to the <b>reference</b> GC-MS <b>analysis</b> <b>method.</b> However, the PLSR model performance was poor across populations of fruit, suggesting that matrix changes in the solvent wash between sets of fruit is problematic. Further work is required to establish whether sufficient variation can be built into a calibration set to overcome this issue, without degrading model performance {{to the point where it}} loses practical application...|$|R
40|$|In {{the last}} decade, {{nonlinear}} static procedures {{emerged as the}} <b>reference</b> practice-oriented <b>analysis</b> <b>method</b> for structures under seismic actions. These procedures are generally based upon two key steps: (i) performing a nonlinear static (pushover) analysis under lateral forces and (ii) evaluating the seismic demand through the so-called capacity spectrum obtained {{at the end of}} the previous step. This paper specifically deals with the latter and proposes a parametric comparison between two wide classes of methods formally based on conceptually alternative approaches for defining the demand spectra in nonlinear static analyses of dissipative structures. First of all, the paper outlines the main conceptual aspects of such alternative methods and reports their key operational details. Particularly, it proposes a dimensionless formulation of their relationships and demonstrates that both methods are based upon a small and almost common set of dimensionless parameters. Therefore, a wide parametric analysis is proposed to compare the ductility demand obtained by means of these two alternative methods with the aim of describing and quantifying the different predictions obtained by applying such methods. Finally, the results of nonlinear time-history analyses are proposed for assessing the two aforementioned methods...|$|R
40|$|This paper {{reports on}} the role of family members in {{everyday}} diabetes self-care and in diabetic crises. It is based on qualitative data drawn from 45 semi-structured interviews {{with a wide range of}} people with an established diagnosis of Type 1 or Type 2 diabetes, who were admitted to hospital for urgent or emergency treatment in connection with their diabetes. The interviews were carried out in two contrasting sites in the United Kingdom in 2009 – 2010, transcribed and analysed thematically with particular <b>reference</b> to framework <b>analysis</b> <b>methods.</b> We found that family involvement in self-care was common, and the role of family and friends was especially important when the person with diabetes needed urgent help. We comment on the diversity of family members who assisted regularly or dealt with crises, the importance of taking account of the complexities of family life, including reciprocal care, and the particular problems faced by people without family support. Finally, we make recommendations for further research and for improvements in existing services...|$|R
30|$|For {{the data}} {{presented}} here, arterial input functions were not available and therefore we considered SRTM results from full dynamic data as the reference standard based on previous studies [29, 30]. Comparison of the two dynamic <b>analysis</b> <b>methods</b> (SRTM and <b>reference</b> Logan graphical <b>analysis)</b> showed that they produced similar results with <b>reference</b> Logan graphical <b>analysis</b> slightly underestimating the BPND values.|$|R
30|$|To {{investigate}} {{the relationship between}} the dynamic and static measures, the coefficient of determination (R 2) was calculated for all atlas regions across all subjects between BPND values, obtained from SRTM and <b>reference</b> Logan graphical <b>analysis</b> <b>methods,</b> and SUVR values of several 20  min time windows. The relationship between these two outcome measures started to stabilize from scan windows of ~[*] 40 – 60  min p.i. for [18 F]AV 45 and ~[*] 80 – 100  min p.i. for [18 F]AV 1451. The linear regression analysis of the regional BPND and SUVR values for the above mentioned time windows showed a high correlation between the two measures using both dynamic methods for both Aβ (SUVR 40 [*]−[*] 60 [*]=[*] 0.97 [*]×[*]BPND_SRTM[*]−[*] 1.2, R 2 SRTM = 0.75; SUVR 40 [*]−[*] 60 [*]=[*] 1.11 [*]×[*]BPND_Logan[*]−[*] 1.3, R 2 Logan[*]=[*] 0.7; p[*]<[*] 0.01) and tau (SUVR 80 [*]−[*] 100 [*]=[*] 1.028 [*]×[*]BPND_SRTM − 1.13; R 2 SRTM[*]=[*] 0.88; SUVR 80 [*]−[*] 100 [*]=[*] 1.27 [*]×[*]BPND_Logan − 1.25; R 2 Logan[*]=[*] 0.71; p[*]<[*] 0.01) PET scans.|$|R
40|$|The aim of {{this paper}} is to specify the Critical Success Factors (CSFs) for Online Distance Learning (ODL) in Higher Education (HE). Research {{methodology}} was analyzing and synthesizing the literature review. The literatures were reviewed to determine items relevant to online learning success as implementation, criteria and indicator. A total of 19 papers, published during 2000 - 2012, were selected from Chulalongkorn University <b>reference</b> databases. Data <b>analysis</b> <b>method</b> was using one of the popular analysis techniques for qualitative research works or the content analysis. The results on the CSFs for ODL can be grouped into 5 factors: (1) institutional management, (2) learning environment, (3) instructional design, (4) services support and (5) course evaluation. Each of these 5 factors includes several important elements that can assist to enhance efficiency of online learning courses in higher education institutions. It is a concrete approach to lead functions of an online institute or course in all levels to the same directions for achieving the success of the institute‘s vision, and make staffs and executives know what they have to do for the success of online distance learning...|$|R
40|$|Funding {{provided}} by the National Development Plan, Research Stimulus Fund administrated by DAFM (RSF – 07 526). Near infrared reflectance spectroscopy (NIRS) has become the routine method of assessing forage quality on grass evaluation and breeding programmes. NIRS requires predictive calibration models that relate spectral data to reference values developed using a calibration set (Burns et al. 2013). The samples that form the calibration set influence the accuracy and reliability of these models {{and need to be}} representative of samples that will likely be analysed (Shenk and Westerhaus, 1991; Burns et al. 2014). Analysing samples from the calibration set using reference techniques has a significant cost and time associated and needs to be considered {{in the context of the}} desired accuracy and robustness of calibration models. Calibration selection techniques can therefore maximise the accuracy and robustness of calibration models whilst reducing the number of samples requiring <b>reference</b> <b>analysis.</b> One such <b>method</b> is principal component analysis (PCA; Shenk and Westerhaus, 1991) whereby Shetty et al. (2012) reported that the number of samples could be reduced by up to 80 % with a minimal loss in accuracy of calibration model. PCA selects representative calibration sub-sets through plotting all the samples in hyper-dimensional space, based on spectral data, and a sample is selected to represent a local neighbourhood cluster of samples for <b>reference</b> <b>analysis.</b> The aim of this research was to assess the accuracy of NIRS calibration models for buffering capacity, in vitro dry matter digestibility (DMD) and water soluble carbohydrate (WSC) content developed using calibration sub-sets selected by PCA...|$|R
40|$|Characterization of {{measurement}} uncertainty {{in terms of}} root sums of squares of both un-known systematic as well as random error components is given meaning {{in the sense of}} predic-tion intervals. Both types of errors are commonly encountered with industrial hygiene air monitoring of hazardous substances. Two extreme types {{of measurement}} methods are pre-sented for illustrating how confidence levels may be ascribed to prediction intervals defined by such uncertainty values. In the case of method calibration at each measurement, systematic error or bias may enter from a biased calibrant. At another extreme, a single initial method evaluation may leave residual bias owing to random error in the evaluation itself or to the use of a biased <b>reference</b> <b>method.</b> <b>Analysis</b> is simplified through new simple approximations to probabilistic limits (quantiles) on the magnitude of a non-central Student t-distributed ran-dom variable. Connection is established between traditional confidence limits, accuracy meas-ures in the case of bias minimization and an uncertainty measure...|$|R
40|$|Near {{infrared}} reflectance spectroscopy (NIRS) {{has become the}} routine method of assessing forage quality on grass evaluation and breeding programmes. NIRS requires predictive calibration models that relate spectral data to reference values developed using a calibration set (Burns et al. 2013). The samples that form the calibration set influence the accuracy and reliability of these models {{and need to be}} representative of samples that will likely be analysed (Shenk and Westerhaus, 1991; Burns et al. 2014). Analysing samples from the calibration set using reference techniques has a significant cost and time associated and needs to be considered {{in the context of the}} desired accuracy and robustness of calibration models. Calibration selection techniques can therefore maximise the accuracy and robustness of calibration models whilst reducing the number of samples requiring <b>reference</b> <b>analysis.</b> One such <b>method</b> is principal component analysis (PCA; Shenk and Westerhaus, 1991) whereby Shetty et al. (2012) reported that the number of samples could be reduced by up to 80 % with a minimal loss in accuracy of calibration model. PCA selects representative calibration sub-sets through plotting all the samples in hyper-dimensional space, based on spectral data, and a sample is selected to represent a local neighbourhood cluster of samples for <b>reference</b> <b>analysis.</b> The aim of this research was to assess the accuracy of NIRS calibration models for buffering capacity, in vitro dry matter digestibility (DMD) and water soluble carbohydrate (WSC) content developed using calibration sub-sets selected by PCA. peer-reviewedFunding provided by the National Development Plan, Research Stimulus Fund administrated by DAFM (RSF – 07 526). Department of Agriculture, Food and the Marin...|$|R
40|$|This study {{intended}} to analyze comparatively the evolution and the structural changes in sheep production in Rio Grande do Sul and Uruguay, being the international wool crisis {{used as a}} <b>reference</b> point. The <b>analysis</b> <b>method</b> {{was based on an}} econometrics time series, and the analysis began with the estimation of models that used linear and semi logarithmic regression. The estimation of the models proved that there were structural changes in sheep production in these regions, and this estimation used the wool crisis as a point of reference. In Rio Grande do Sul after 1990, the variables of sheep stock, wool and sheep meat presented a negative variable in their posted annual growth rates, as they decreased by 5. 9 %, 5. 6 % and 5. 6 %, respectively. The negative growth rates in Uruguay for the same variables in the same period were 6. 1 %, 5. 6 % and 0. 9 %, respectively. The data models indicate that there was no return to a balanced situation after the changes caused by the crisis. Therefore, the sheep market was permanently affected, which dynamically determined the evolution of sheep production and was defined by changes and uncertainty...|$|R
40|$|Standard {{methods for}} {{predicting}} bone's mechanical response from quantitative computer tomography (qCT) scans are mainly based on classical h-version finite element methods (FEMs). Due to the low-order polynomial approximation, {{the need for}} segmentation and the simplified approach to assign a constant material property to each element in h-FE models, these often compromise the accuracy and efficiency of h-FE solutions. Herein, a non-standard method, the finite cell method (FCM), is proposed for predicting the mechanical response of the human femur. The FCM is free of the above limitations associated with h-FEMs and is orders of magnitude more efficient, allowing its use {{in the setting of}} computational steering. This non-standard method applies a fictitious domain approach to simplify the modeling of a complex bone geometry obtained directly from a qCT scan and takes into consideration easily the heterogeneous material distribution of the various bone regions of the femur. The fundamental principles and properties of the FCM are briefly described in relation to bone analysis, providing a theoretical basis for the comparison with the p-FEM as a <b>reference</b> <b>analysis</b> and simulation <b>method</b> of high quality. Both p-FEM and FCM results are validated by comparison with an in vitro experiment on a fresh-frozen femur...|$|R
40|$|A turbidimetric flow-injection {{system was}} {{developed}} for the determination of total nitrogen and potassium in vegetable samples using a single spectrophotometer as detector. As a precipitating agent, 3. 0 % (w/v) sodium tetraphenylboron solution prepared in 2. 0 % (w/v) poly(vinyl alcohol) was used. A gas diffusion process {{was included in the}} manifold to separate ammonium ions {{from the rest of the}} sample and to allow paired analysis. Total nitrogen and potassium determinations were carried out on the solutions remaining in the acceptor and donor streams, respectively. Results obtained were precise (relative standard deviations < 2. 1 and 1. 6 % for total N (< 25 mg g− 1) and K (< 55 mg g− 1) determinations, respectively) and in agreement with those of <b>reference</b> <b>methods.</b> <b>Analysis</b> can be carried out at a rate of up to 35 samples per hour (corresponding to 70 determinations per hour) within concentration range 87 – 430 mg N–NH+ 4 l− 1 and 78 – 390 mg K+ l− 1 for the total nitrogen and potassium determinations, respectively...|$|R
40|$|Dependence {{analysis}} and dependence information are critical components of many optimizing and parallelizing compilers. Recently, {{there has been}} an increased interest in extending dependence analysis to also cover Fortran 95 array section <b>references.</b> This <b>analysis</b> can then be used in making advanced scalarization, loop fusion, and array contraction decisions. This paper reviews the methods of performing array section dependence analysis, discusses a simplied <b>analysis</b> <b>method,</b> and then evaluates the eectiveness of the simplied method...|$|R
40|$|The goal of our {{research}} is to understand how ideas propagate, combine and are created in large social networks. In this work, we look at a sample of relevant scientific publications {{in the area of}} high-frequency analog circuit design and their citation distribution. A novel aspect of our work {{is the way in which}} we categorize citations based on the reason and place of it in a publication. We created seven citation categories from general domain references, references to specific methods used in the same domain problem, <b>references</b> to an <b>analysis</b> <b>method,</b> <b>references</b> for experimental comparison and so on. This added information allows us to define two new measures to characterize the creativity (novelty and usefulness) of a publication based on its pattern of citations clustered by reason, place and citing scientific group. We analyzed 30 publications in relevant journals since 2000 and their about 300 citations, all in the area of high-frequency analog circuit design. We observed that the number of citations a publication receives from different scientific groups matches a Levy type distribution: with a large number of groups citing a publication relatively few times, and a very small number of groups citing a publication a large number of times. We looked at the motifs a publication is cited differently by different scientific groups...|$|R
3000|$|Material <b>Analysis</b> <b>Methods</b> (e.g. {{elemental}} <b>analysis</b> <b>methods,</b> qualitative <b>analysis</b> <b>methods,</b> spectroscopic methods) [...]...|$|R
40|$|We develop <b>reference</b> <b>analysis</b> for matrix-variate dynamic {{models with}} unknown {{observation}} covariance matrices. Bayesian algorithms for forecasting, estimation, and filtering are derived. This work extends the existing theory of <b>reference</b> <b>analysis</b> for univariate dynamic linear models, {{and thus it}} proposes {{a solution to the}} specification of the prior distributions for a very wide class of time series models. Subclasses of our models include the widely used multivariate and matrix-variate regression models...|$|R
5000|$|Software {{architecture}} <b>analysis</b> <b>method,</b> {{precursor to}} architecture tradeoff <b>analysis</b> <b>method</b> ...|$|R
40|$|Software {{architecture}} {{is important in}} software development, which is vital to software quality, so the software <b>analysis</b> <b>methods</b> are becoming the research hotspot. In this article we advanced a scenario-based software architecture <b>analysis</b> <b>method</b> based on SAAM (Software Architecture <b>Analysis</b> <b>Method)</b> and ATAM (Architecture Tradeoff <b>Analysis</b> <b>Method).</b> By making scenario library and utility tree, this method will offer a lightweight way to evaluate and analyze software architectures. © 2014 WIT Press. Software {{architecture is}} important in software development, which is vital to software quality, so the software <b>analysis</b> <b>methods</b> are becoming the research hotspot. In this article we advanced a scenario-based software architecture <b>analysis</b> <b>method</b> based on SAAM (Software Architecture <b>Analysis</b> <b>Method)</b> and ATAM (Architecture Tradeoff <b>Analysis</b> <b>Method).</b> By making scenario library and utility tree, this method will offer a lightweight way to evaluate and analyze software architectures. © 2014 WIT Press...|$|R
40|$|Liao’s Homotopy <b>analysis</b> <b>method</b> is discussed. The authors {{introduced}} {{the new approach}} on which they claim makes the calculation of Homotopy <b>analysis</b> <b>method</b> easy. The analytical solutions for non-linear reaction diffusion equations using Homotopy <b>analysis</b> <b>method</b> and new Homotopy <b>analysis</b> <b>method</b> were discussed with examples. We also prove analytically that the methods given by Liao {{is equivalent to the}} present method...|$|R
40|$|Program {{analyses}} and optimization of Java programs require reference information that determines the in-stances {{that may be}} accessed through dereferences. Ref-erence information can be computed using reference anal-ysis. This paper presents a set of studies that evaluate the precision of some existing approaches for identify-ing instances and for computing reference information in a <b>reference</b> <b>analysis.</b> The studies use dynamic refer-ence information collected during run-time as a lower bound approximation to the precise reference informa-tion. The studies measure the precision of an existing approach by comparing the information computed using the approach with the lower bound approximation. The paper also presents case studies that attempt to identify the cases under which an existing approach is not eec-tive. The presented studies provide information that may guide the usage of existing <b>reference</b> <b>analysis</b> tech-niques {{and the development of}} new <b>reference</b> <b>analysis</b> techniques. 1...|$|R
40|$|This paper {{proposes a}} fast Fourier {{transforms}} (FFT) -based spectral <b>analysis</b> <b>method</b> for the dynamic analysis of linear discrete dynamic systems which have non-proportional viscous damping and {{are subjected to}} non-zero initial conditions. To evaluate the proposed FFT-based spectral <b>analysis</b> <b>method,</b> the forced vibration of a three degree-of-freedom (DOF) system is considered as an illustrative problem. The accuracy of the proposed FFT-based spectral <b>analysis</b> <b>method</b> is evaluated by comparing the forced vibration responses obtained by the present FFT-based spectral <b>analysis</b> <b>method</b> with those obtained by using the well-known Runge-Kutta <b>method</b> and modal <b>analysis</b> <b>method...</b>|$|R
40|$|Abstract: In {{this method}} new {{approach}} on Liao’s Homotopy <b>analysis</b> <b>method</b> is discussed. The authors introduced {{the new approach}} on which they claim makes the calculation of Homotopy <b>analysis</b> <b>method</b> easy. The analytical solutions for non-linear reaction diffusion equations using Homotopy <b>analysis</b> <b>method</b> and new Homotopy <b>analysis</b> <b>method</b> were discussed with examples. We also prove analytically that the methods given by Liao {{is equivalent to the}} present method...|$|R
40|$|Aim To examine {{performance}} in the UK National External Quality Assessment Scheme (UKNEQAS) for toxoplasma serology for evidence of discrepant results {{as compared with the}} predistribution and postdistribution results supplied by the toxoplasma <b>reference</b> laboratories. <b>Methods</b> <b>Analysis</b> of {{performance in}} the toxoplasma IgG and IgM schemes was made for the period 1994 e 2008 to look for trends in performance. Results For the IgG scheme, a mean of 98 % of participants obtained the correct result for detection of toxoplasma-specific antibody. The most common problem was failure to detect low levels of antibody. In some cases this was the result of participants deviating from the manufacturer’s instructions and using higher cut-off levels. For the IgM scheme, an average of 95 % of participants obtained the correct result for toxoplasma antibody detection. The most common problem was the failure of some enzyme immunoassay kits to detect specific toxoplasma IgM antibody, which was detected by the more sensitive immunosorbent agglutination assay. Conclusions Performance standards in the UKNEQAS toxoplasma serology schemes were high. The problems encountered have highlighted the importance of detecting low levels of antibody, adhering to the kit manufacturer’s instructions and selecting an appropriate assay for the clinical situation...|$|R
40|$|Purpose Quantitative {{estimates}} of dopamine transporter availability, determined with [123 I]FP-CIT SPECT, {{depend on the}} SPECT equipment, including both hardware and (reconstruction) software, which limits their use in multicentre research and clinical routine. This study tested a dedicated reconstruction algorithm {{for its ability to}} reduce camera-specific intersubject variability in [123 I]FP-CIT SPECT. The secondary aim was to evaluate binding in whole brain (excluding striatum) as a <b>reference</b> for quantitative <b>analysis.</b> <b>Methods</b> Of 73 healthy subjects from the European Normal Control Database of [123 I]FP-CIT recruited at six centres, 70 aged between 20 and 82 years were included. SPECT images were reconstructed using the QSPECT software package which provides fully automated detection of the outer contour of the head, camera-specific correction for scatter and septal penetration by transmission-dependent convolution subtraction, iterative OSEMreconstruction including attenuation correction, and camera-specific Bto kBq/ml^ calibration. LINK and HERMES reconstruction were used for head-to-head comparison. The specific striatal [123 I]FP-CIT binding ratio (SBR) was computed using the Southampton method with binding in the whole brain, occipital cortex or cerebellum as the reference. The correlation between SBR and age was used as the primary quality measure. Results The fraction of SBR variability explained by age was highest (1) with QSPECT, independently of the reference region, and (2) with whole brain as the reference, independently of the reconstruction algorithm. Conclusion QSPECT reconstruction appears to be useful for reduction of camera-specific intersubject variability of [123 I]FP-CIT SPECT in multisite and single-site multicamera settings. Whole brain excluding striatal binding as the reference provides more stable quantitative estimates than occipital or cerebellar binding...|$|R
30|$|Correlation analysis. Correlation is the {{law that}} exists between two or more {{variables}} in a certain sense. Its purpose is to explore the hidden correlation in the dataset. The commonly used correlation <b>analysis</b> <b>methods</b> include Pearson correlation coefficient <b>method,</b> canonical correlation <b>analysis</b> <b>method,</b> and principal component <b>analysis</b> <b>method.</b>|$|R
40|$|AbstractScreening out {{sensitivity}} prediction indexes fit for {{the coal}} mine is very important technical work in coal and gas outburst prediction process. The feasibility that principal component <b>analysis</b> <b>method</b> {{been used in the}} coal and gas outburst prediction index screening is discussed. Through using gray correlation <b>analysis</b> <b>method</b> and principal component <b>analysis</b> <b>method</b> to <b>analysis</b> a batch of measured data of a coal mine, the validity of the latter is proved. Through the contrast of process between the gray correlation analysis and the principal component <b>analysis</b> <b>method,</b> the vantage of principal component <b>analysis</b> <b>method</b> is obvious, that is the better maneuverability, simplicity and easier to extend...|$|R
30|$|In the literature, {{there are}} several <b>analysis</b> <b>methods</b> for {{different}} purposes (e.g., optimization, performance, and impact of change); however, normally those <b>analysis</b> <b>methods</b> are focused on a particular problem and {{they are based on}} different metamodels (e.g., workload of human resources in business processes using BPMN as modeling language, for a commercial enterprise). Then, the <b>analysis</b> <b>method</b> algorithm might be very specialized making it neither applicable nor reusable in other enterprise models. The lack of re-usability becomes an important issue because, when the analyst decides to perform an existing <b>analysis</b> <b>method</b> that was not designed for the enterprise model that is being analyzed, there is the need to modify the <b>analysis</b> <b>method</b> increasing the effort and cost of the analysis process. Moreover, due to <b>analysis</b> <b>methods</b> are taken from multiple references; {{there is a lack of}} uniformity, structure, and characterization.|$|R
