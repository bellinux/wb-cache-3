0|104|Public
5000|$|Fetch the {{prediction}} for the addresses of {{the targets of}} branches in that <b>run</b> of <b>instructions</b> ...|$|R
5000|$|The IBM 1130's [...] "cycle steal" [...] {{is really}} DMA because the CPU clock is stopped during memory access. Several I/O {{controllers}} access RAM this way. They self-arbitrate via a fixed priority scheme. Most controllers deliberately pace RAM access to minimize {{impact on the}} system's ability to <b>run</b> <b>instructions,</b> but others, such as graphic video adapters, operate at higher speed and may slow down the system.|$|R
40|$|This paper {{presents}} a novel compiler for Esterel, a concurrent synchronous imperative language. It generates fast, small object code by compiling away concurrency, producing a single C function requiring no operating system support for threads. It translates an Esterel program into an acyclic concurrent controlflow graph from which code is synthesized that <b>runs</b> <b>instructions</b> in an order respecting inter-thread communication. Exceptions and preemption constructs become conditional branches. Variables save control stat...|$|R
40|$|This paper {{examines}} differential multithreading (dMT) as {{an attractive}} organization for increasing throughput in simple, small-scale, pipelined processors like {{those used in}} embedded environments. dMT copes with pipeline stalls due to hazards and data- and instruction-cache misses by using duplicated pipeline registers to <b>run</b> <b>instructions</b> from an alternate thread. Results show that dMT boosts throughput substantially and can in fact replace dynamic branch prediction or {{can be used to}} reduce the sizes of the instruction and data caches. This report expands upon [1] by presenting extended results for additional dMT configurations...|$|R
5000|$|F.B.I. (Future Beat <b>Instructions)</b> <b>Run</b> by DJ Preach (2005-2008) ...|$|R
50|$|In {{machines}} {{where this}} recurrence takes two cycles, the machine loses one full cycle of fetch after every predicted taken branch. As predicted branches happen every 10 instructions or so, this can force a substantial drop in fetch bandwidth. Some machines with longer instruction cache latencies {{would have an}} even larger loss. To ameliorate the loss, some machines implement branch target prediction: given the address of a branch, they predict the target of that branch. A refinement of the idea predicts {{the start of a}} sequential <b>run</b> of <b>instructions</b> given the address of the start of the previous sequential <b>run</b> of <b>instructions.</b>|$|R
50|$|Traditional {{processors}} {{examine and}} <b>run</b> <b>instructions</b> {{from a list}} of instructions stored in memory, known as a program. As memory runs much more slowly than the CPU, in order to maximize performance, modern CPU designs include a number of high-speed memory elements to temporarily store values and avoid the delays inherent to talking to main memory. One of the key advances in the RISC concept was to reduce the complexity of the instructions, and use those savings in the associated circuitry to include more processor registers, the fastest form of memory. This technique quickly reached its limits, and since the 1990s modern CPUs have added increasing amounts of CPU cache to increase local storage, although cache is slower than registers.|$|R
50|$|Scouting and {{simultaneous}} multithreading (SMT) both use hardware threads {{to fight}} the memory wall. With scouting, the scout thread <b>runs</b> the <b>instructions</b> from the same instruction stream as the instruction that causes the pipeline stall. In the case of SMT, the SMT thread executes instruction in another context.|$|R
50|$|FFmpeg {{encompasses}} software implementations {{of video}} and audio compressing and decompressing algorithms. These can be compiled and <b>run</b> on diverse <b>instruction</b> sets.|$|R
50|$|When {{modifying the}} {{operating}} parameters of a CPU, such as temperature, humidity, overclocking, underclocking, overvolting, and undervolting, {{it may be}} necessary to verify if the new parameters (usually CPU core voltage and frequency) are suitable for heavy CPU loads. This is done by running a CPU-intensive program for extended periods of time, to test whether the computer hangs or crashes. CPU stress testing is also referred to as torture testing. Software that is suitable for torture testing should typically <b>run</b> <b>instructions</b> that utilise the entire chip rather than only a few of its units. Stress testing a CPU over the course of 24 hours at 100% load is, in most cases, sufficient to determine that the CPU will function correctly in normal usage scenarios such as in a desktop computer, where CPU usage typically fluctuates at low levels (50% and under).|$|R
25|$|Textbooks on the Nanai language, fairy tales, {{and artistic}} {{literature}} {{are used in}} Nanai language teaching. Sometimes teachers took the initiative to use oral folklore as well. However, {{there is a shortage}} of teaching and auxiliary materials, as well as difficulty in motivating students. Nanai language textbooks follow the model of Russian language textbooks aimed at native speakers, rather than emphasising instruction in the language itself, and in the theoretical/practical grammar. This model is not adequate for the situation of heritage language preservation. Moreover, the existing language teaching materials are oriented predominantly (or only) towards the development of reading habits; however, the number of publications in the Nanaian language does not exceed one-two ten, mostly collections of folklore or artistic works of the historical-biographical genre, publishing in limited print <b>runs.</b> <b>Instruction</b> in spoken language is not conducted sufficiently and is not reinforced by teaching aids.|$|R
40|$|Abstract—A {{software}} product line (SPL) {{is a family}} of related programs, {{each of which is}} uniquely defined by a combination of features. Testing an SPL requires running each of its programs, which may be computationally expensive as the number of programs in an SPL is potentially exponential in the number of features. It is also wasteful since instructions common to many programs must be repeatedly executed, rather than just once. To reduce this waste, we propose the idea of shared execution, which <b>runs</b> <b>instructions</b> just once for a set of programs until a variable read yields multiple values, causing execution to branch for each value until a common execution point that allows shared execution to resume. Experiments show that shared execution can be faster than conventionally running each program from start to finish, despite its overhead. Keywords-product lines; feature oriented programming; testing; shared execution; dynamic analysis. I...|$|R
50|$|Textbooks on the Nanai language, fairy tales, {{and artistic}} {{literature}} {{are used in}} Nanai language teaching. Sometimes teachers took the initiative to use oral folklore as well. However, {{there is a shortage}} of teaching and auxiliary materials, as well as difficulty in motivating students. Nanai language textbooks follow the model of Russian language textbooks aimed at native speakers, rather than emphasising instruction in the language itself, and in the theoretical/practical grammar. This model is not adequate for the situation of heritage language preservation. Moreover, the existing language teaching materials are oriented predominantly (or only) towards the development of reading habits; however, the number of publications in the Nanaian language does not exceed one-two ten, mostly collections of folklore or artistic works of the historical-biographical genre, publishing in limited print <b>runs.</b> <b>Instruction</b> in spoken language is not conducted sufficiently and is not reinforced by teaching aids.|$|R
5000|$|CPU Sim, a Java {{application}} {{that allows the}} user to design and create an instruction set and then <b>run</b> programs of <b>instructions</b> from the set through simulation ...|$|R
5000|$|At {{the lowest}} level, {{executable}} code consists of machine language instructions specific {{to an individual}} processor—typically a central processing unit (CPU). A machine language consists of groups of binary values signifying processor instructions that change {{the state of the}} computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also (indirectly) cause something to appear on a display of the computer system—a state change which should be visible to the user. The processor carries out the instructions in the order they are provided, unless it is instructed to [...] "jump" [...] to a different instruction, or is interrupted (by now multi-core processors are dominant, where each core can <b>run</b> <b>instructions</b> in order; then, however, each application software runs only on one core by default, but some software has been made to run on many).|$|R
50|$|If {{these two}} {{assembly}} pseudocode <b>instructions</b> <b>run</b> in a pipeline, after fetching and decoding the second instruction, the pipeline stalls, {{waiting until the}} result of the addition is written and read.|$|R
50|$|Another approach, {{based on}} trace-based {{simulation}} does not <b>run</b> every <b>instruction,</b> but <b>runs</b> a trace file which store important program events only. This approach loses some flexibility and accuracy compared to cycle-accurate simulation mentioned above {{but can be}} much faster. The generation of traces often consumes considerable amounts of storage space and can severely impact the runtime of applications if large amount of data are recorded during execution.|$|R
50|$|Most {{instructions}} on most CPU architectures are sequential instructions.Because most instructions are sequential instructions, CPU designers often add features that deliberately sacrifice {{performance on the}} other instructions—branch instructions—in order to make these sequential <b>instructions</b> <b>run</b> faster.|$|R
50|$|Another {{benefit is}} that VHDL allows the {{description}} of a concurrent system. VHDL is a dataflow language, unlike procedural computing languages such as BASIC, C, and assembly code, which all <b>run</b> sequentially, one <b>instruction</b> at a time.|$|R
5000|$|To {{simplify the}} circuit {{structure}} {{and reduces the}} hardware cost of quantum computers (proposed to <b>run</b> the MIPS32 <b>instruction</b> set) a 50 GHz superconducting [...] "4-bit bit-slice arithmetic logic unit (ALU) for 32-bit rapid single-flux-quantum microprocessors was demonstrated." ...|$|R
50|$|Pin {{performs}} instrumentation {{by taking}} control of the program just after it loads into the memory. Then just-in-time recompiles (JIT) small sections of the binary code using pin just before it is <b>run.</b> New <b>instructions</b> to perform analysis are added to the recompiled code. These new instructions come from the Pintool. A large array of optimization techniques are used to obtain the lowest possible running time and memory use overhead. As of June 2010, Pin's average base overhead is 30 percent (without running a pintool).|$|R
5|$|The CIA has {{a branch}} campus in Napa County, California. The primary campus in St. Helena {{is known as}} the Culinary Institute of America at Greystone; the {{satellite}} campus in the city of Napa {{is known as the}} Culinary Institute of America at Copia. The campus runs associate degree programs as well as certificate programs, continuing education courses, custom classes, conferences, and seminars including the Worlds of Flavor International Conference & Festival each year. The Rudd Center for Professional Wine Studies <b>runs</b> wine <b>instruction</b> classes and a certification program for wine professionals.|$|R
50|$|Based on Flynn's Multiple-Instruction-Multiple-Data Streams terminology, this {{category}} spans {{a wide spectrum}} of architectures in which processors execute multiple instruction sequences on (potentially) dissimilar data streams without strict synchronization. Although both instruction and data streams can be different for each processor, they need not be. Thus, MIMD architectures can run identical programs that are in various stages at any given time, <b>run</b> unique <b>instruction</b> and data streams on each processor or execute a combination of each these scenarios. This category is subdivided further primarily on the basis of memory organization.|$|R
50|$|The CIA has {{a branch}} campus in Napa County, California. The primary campus in St. Helena {{is known as}} the Culinary Institute of America at Greystone; the {{satellite}} campus in the city of Napa {{is known as the}} Culinary Institute of America at Copia. The campus runs associate degree programs as well as certificate programs, continuing education courses, custom classes, conferences, and seminars including the Worlds of Flavor International Conference & Festival each year. The Rudd Center for Professional Wine Studies <b>runs</b> wine <b>instruction</b> classes and a certification program for wine professionals.|$|R
5000|$|A {{multi-core}} processor is {{a single}} computing component with two or more independent actual processing units (called [...] "cores"), which are units that read and execute program instructions. The instructions are ordinary CPU instructions (such as add, move data, and branch), but the single processor can <b>run</b> multiple <b>instructions</b> on separate cores at the same time, increasing overall speed for programs amenable to parallel computing. Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or CMP), or onto multiple dies in a single chip package.|$|R
5000|$|OoOE {{processors}} fill these [...] "slots" [...] in {{time with}} other instructions that are ready, then re-order the results at the end {{to make it appear}} that the instructions were processed as normal. The way the instructions are ordered in the original computer code is known as program order, in the processor they are handled in data order, the order in which the data, operands, become available in the processor's registers. Fairly complex circuitry is needed to convert from one ordering to the other and maintain a logical ordering of the output; the processor itself <b>runs</b> the <b>instructions</b> in seemingly random order.|$|R
50|$|The basic 77-68 {{comprised}} an 8-inch square {{printed circuit}} board accommodating the microprocessor, Static RAM of 256 8 bit words and the bare essentials in terms of input/output and timing logic to make a working computer. The processor <b>ran</b> with an <b>instruction</b> cycle time of around 1.25 microseconds with most instructions executing in 3 to 7 microseconds.|$|R
40|$|Results are {{presented}} {{of a study}} conducted with a digital simulation model {{being used in the}} design of the Automatically Reconfigurable Modular Multiprocessor System (ARMMS), a candidate computer system for future manned and unmanned space missions. The model simulates the activity involved as instructions are fetched from random access memory for execution in one of the system central processing units. A series of model <b>runs</b> measured <b>instruction</b> execution time under various assumptions pertaining to the CPU's and the interface between the CPU's and RAM. Design tradeoffs {{are presented}} in the following areas: Bus widths, CPU microprogram read only memory cycle time, multiple instruction fetch, and instruction mix...|$|R
50|$|Hardware level works upon dynamic {{parallelism}} whereas, {{the software}} level works on static parallelism.Dynamic parallelism means the processor decides at <b>run</b> time which <b>instructions</b> to execute in parallel, whereas static parallelism means the compiler decides which instructions to execute in parallel. The Pentium processor {{works on the}} dynamic sequence of parallel execution but the Itanium processor works on the static level parallelism.|$|R
50|$|The Prime 400 ran at 0.5 MIPS, had a main {{store of}} up to 8 MB and 160 MB of disc storage. The name PRIMOS was now used for the {{operating}} system and the P400 ran PRIMOS 4. It <b>ran</b> a V-mode <b>instruction</b> set, along with the S-mode and R-mode instructions. It had a segmented virtual memory architecture, somewhat similar to Multics.|$|R
5000|$|The goal of SCI, {{and other}} {{contemporary}} projects, was {{nothing less than}} full machine intelligence. [...] "The machine envisioned by SC", according to Alex Roland and Philip Shiman, [...] "would <b>run</b> ten billion <b>instructions</b> per second to see, hear, speak, and think like a human. The degree of integration required would rival that achieved by the human brain, the most complex instrument known to man." ...|$|R
2500|$|Close {{was then}} {{selected}} {{to play for}} England in the third Test match at Old Trafford against the touring New Zealand cricket team; in this game, Close became, and to date remains, England's youngest-ever Test player, aged 18 years and 149 days. He came in to bat when the England innings needed quick <b>runs,</b> his <b>instruction</b> from Freddie Brown, the captain, being to [...] "have {{a look at a}} couple and then give it a go". Close duly played two balls back to the bowler, then hit out for the boundary, only to be caught in the outfield for a score of nought. He had previously taken one wicket for 39 runs during the first New Zealand innings.|$|R
5000|$|In modern machines, {{the time}} to fetch a {{variable}} from the data cache is often several times longer than the time needed for basic ALU operations. A program runs faster without stalls if its memory loads can be started several cycles before the instruction that needs that variable. Complex machines can do this with a deep pipeline and [...] "out-of-order execution" [...] that examines and <b>runs</b> many <b>instructions</b> at once. Register machines can even do this with much simpler [...] "in-order" [...] hardware, a shallow pipeline, and slightly smarter compilers. The load step becomes a separate instruction, and that instruction is statically scheduled much earlier in the code sequence. The compiler puts independent steps in between.|$|R
50|$|The LINC-8 {{contained}} one PDP-8 CPU and one LINC CPU, partially emulated by the PDP-8. At any one time, {{the computer}} was in either 'LINC mode' or 'PDP-8 mode' - both processors could not <b>run</b> in parallel. <b>Instructions</b> were provided to switch between modes. In the LINC-8, all interrupts were handled by the PDP-8 CPU, and programs that relied on the interrupt architecture of the LINC could not be run.|$|R
50|$|In recent years, {{the speed}} of the CPU has grown many times in {{comparison}} to the access speed of the main memory. Care needs to be taken {{to reduce the number of}} times main memory is accessed in order to maintain performance. If, for instance, every <b>instruction</b> <b>run</b> in the CPU requires an access to memory, the computer gains nothing for increased CPU speed—a problem referred to as being memory bound.|$|R
50|$|Not all {{problems}} can be attacked {{with this sort of}} solution. Including these types of instructions necessarily adds complexity to the core CPU. That complexity typically makes other <b>instructions</b> <b>run</b> slower—i.e., whenever it is not adding up many numbers in a row. The more complex instructions also add to the complexity of the decoders, which might slow down the decoding of the more common instructions such as normal adding.|$|R
