1506|409|Public
25|$|In analog-to-digital {{conversion}} a quantization error occurs. This error is either due to rounding or truncation. When the original signal {{is much larger}} than one least significant bit (LSB), the quantization error is not significantly correlated with the signal, and has an approximately uniform distribution. The <b>RMS</b> <b>error</b> therefore follows from the variance of this distribution.|$|E
5000|$|High {{accuracy}} of surface form testing (wavefront <b>RMS</b> <b>error</b> 0.125 nm).|$|E
5000|$|For {{nearly all}} models the <b>r.m.s.</b> <b>error</b> in zonal- and annual-mean surface air {{temperature}} is small compared with its natural variability.|$|E
40|$|The paper {{establishes}} <b>rms</b> <b>errors</b> in Nimbus 6 {{satellite data}} from discrepancies between satellite and rawinsonde data, compares <b>rms</b> <b>errors</b> in Nimbus 6 {{with those in}} rawinsonde data, and examines the capability of Nimbus 6 satellite data to depict such quantities as horizontal gradients of basic variables and computed variables such as vorticity and lapse rate of temperature. The capability of Nimbus 6 data is illustrated by comparisons of <b>rms</b> <b>errors</b> in computed variables obtained through a propagation-of-error method with average and near-extreme values of the variables obtained from two of NASA's Atmospheric Variability Experiments (AVE II and AVE IV). It is shown that the <b>rms</b> <b>errors</b> in Nimbus 6 basic variables (temperature, mixing ratio, geopotential height, and wind speed) are between 2 and 12 {{times larger than the}} corresponding errors in rawinsonde data. For all variables examined, the magnitude of the <b>rms</b> <b>errors</b> are smaller than the near-extreme values of the variables determined from AVE II and AVE IV data...|$|R
40|$|The {{results are}} {{reported}} of {{research into the}} effects on system operation of signal quantization in a digital control system. The investigation considered digital controllers (filters) operating in floating-point arithmetic in either open-loop or closed-loop systems. An error analysis technique is developed, and is implemented by a digital computer program {{that is based on}} a digital simulation of the system. As an output the program gives the programing form required for minimum system quantization errors (either maximum of <b>rms</b> <b>errors),</b> and the maximum and <b>rms</b> <b>errors</b> that appear in the system output for a given bit configuration. The program can be integrated into existing digital simulations of a system...|$|R
30|$|Our SV {{candidate}} {{model for}} IGRF- 11 (valid {{for the period}} 2010.0 – 2015.0 and centered in 2012.5) is a rounded (to the nearest 0.01 nT/yr) version of this model, up to degree/order 8, with associated <b>rms</b> <b>errors.</b>|$|R
5000|$|... {{where the}} {{learning}} rate [...] {{is taken to}} be 0.3. The training is performed with one pass through the 100 training points. The <b>rms</b> <b>error</b> is 0.15.|$|E
5000|$|The {{relative}} {{merits of}} various models {{can be inferred}} from figure 1. Simulated patterns that agree well with observations will lie nearest the point marked [...] "observed" [...] on the x-axis. These models have relatively high correlation and low RMS errors. Models lying on the dashed arc have the correct standard deviation (which indicates that the pattern variations are of the right amplitude). In figure 1 {{it can be seen}} that models A and C generally agree best with observations, each with about the same <b>RMS</b> <b>error.</b> Model A, however, has a slightly higher correlation with observations and has the same standard deviation as the observed, whereas model C has too little spatial variability (with a standard deviation of 2.3 mm/day compared to the observed value of 2.9 mm/day). Of the poorer performing models, model E has a low pattern correlation, while model D has variations that are much larger than observed, in both cases resulting in a relatively large (~3 mm/day) centered <b>RMS</b> <b>error</b> in the precipitation fields. Note also that although models D and B have about the same correlation with observations, model B simulates the amplitude of the variations (i.e., the standard deviation) much better than model D, resulting in a smaller <b>RMS</b> <b>error.</b>|$|E
50|$|Quantizers, or analog-to-digital converters, can use lattices to {{minimise}} {{the average}} root-mean-square error. Most quantizers {{are based on}} the one-dimensional integer lattice, but using multi-dimensional lattices reduces the <b>RMS</b> <b>error.</b> The Leech lattice is a good solution to this problem, as the Voronoi cells have a low second moment.|$|E
40|$|An error {{analysis}} is presented for cloud-top pressure and cloud-amount retrieval using infrared sounder data. <b>Rms</b> and bias <b>errors</b> are determined for instrument noise (typical of the HIRS- 2 instrument on Tiros-N) and for uncertainties in the temperature profiles and water vapor profiles {{used to estimate}} clear-sky radiances. Errors are determined {{for a range of}} test cloud amounts (0. 1 - 1. 0) and cloud-top pressures (920 - 100 mb). <b>Rms</b> <b>errors</b> vary by an order of magnitude depending on the cloud height and cloud amount within the satellite's field of view. Large bias errors are found for low-altitude clouds. These bias errors are shown to result from physical constraints placed on retrieved cloud properties, i. e., cloud amounts between 0. 0 and 1. 0 and cloud-top pressures between the ground and tropopause levels. Middle-level and high-level clouds (above 3 - 4 km) are retrieved with low bias and <b>rms</b> <b>errors...</b>|$|R
40|$|We {{present a}} {{comparison}} between Gaussian processes (GPs) and artificial neural networks (ANNs) as methods for determining photometric redshifts for galaxies, given training-set data. In particular, we compare their degradation in performance as the training-set size is degraded in ways which might be caused by the observational limitations of spectroscopy. Using publicly available regression codes, we find that performance with large, complete training sets is very similar, although the ANN achieves slightly smaller <b>rms</b> <b>errors.</b> Training sets with brighter magnitude limits than the test data do not strongly affect the performance of either algorithm, until the limits are so severe that they remove {{almost all of the}} high-redshift training objects. Similarly, the introduction of a plausible number (up to 10 per cent) of inaccurate redshifts into the training set has little effect on either method. However, if the size of the training set is reduced by random sampling, the <b>rms</b> <b>errors</b> of both methods increase, but they do so to a lesser extent and in a much smoother manner for the case of GP regression; for the example presented ANNZ has <b>rms</b> <b>errors</b> ∼ 20 per cent worse than GP regression in the small training-set limit. Also, when training objects are removed at redshifts 1. 3 < z < 1. 7, to simulat...|$|R
40|$|Digital {{images of}} the {{intertidal}} region were used to map shorelines and the intertidal bathymetry along four geomorphically and hydrodynamically distinct coastlines in the United States, United Kingdom, The Netherlands, and Australia: Mapping methods, each of which was originally designed to perform well at {{only one of the}} four sites, were applied to all four sites, and the results were compared to direct topographic surveys. It was determined that the <b>rms</b> <b>errors</b> of image-derived versus directly surveyed elevations depended on the prevailing hydrodynamic conditions as well as differences {{in each of the four}} different mapping methods. Before these differences were accounted for, <b>rms</b> <b>errors</b> ranged from 0. 3 to 0. 7 m. An empirical correction model that computed local estimates of setup, swash, and surf beat amplitudes reduced errors by about 50 %, with residual <b>rms</b> <b>errors</b> ranging between 0. 1 and 0. 4 m. The model required tuning only one parameter that determined how each method was affected by swash at infragravity and incident wave frequencies. In environments where all methods successfully identify shorelines, the methods can be used somewhat interchangeably. The diversity of methods is advantageous in situations where one or more methods are likely to fail (e. g., lack of color imagery, high degree of alongshore variability). This remote sensing methodology can be applied to the shoreline and inter-tidal mapping problem across diverse nearshore environments...|$|R
50|$|In analog-to-digital {{conversion}} a quantization error occurs. This error is either due to rounding or truncation. When the original signal {{is much larger}} than one least significant bit (LSB), the quantization error is not significantly correlated with the signal, and has an approximately uniform distribution. The <b>RMS</b> <b>error</b> therefore follows from the variance of this distribution.|$|E
50|$|From this, {{the optimal}} control model (Pew & Baron, 1978) {{developed}} {{in order to}} model a human operator's ability to internalize system dynamics and minimize objective functions, such as root mean square (<b>RMS)</b> <b>error</b> from the target. The optimal control model also recognizes noise in the operator's ability to observe the error signal, and acknowledges noise in the human motor output system.|$|E
5000|$|... {{where the}} {{learning}} rate [...] is again {{taken to be}} 0.3. The training is performed with one pass through the 100 training points. The <b>rms</b> <b>error</b> on a test set of 100 exemplars is 0.084, smaller than the unnormalized error. Normalization yields accuracy improvement. Typically accuracy with normalized basis functions increases even more over unnormalized functions as input dimensionality increases.|$|E
25|$|In fixed-point arithmetic, the finite-precision errors {{accumulated}} by FFT algorithms are worse, with <b>rms</b> <b>errors</b> {{growing as}} O(√N) for the Cooley–Tukey algorithm (Welch, 1969). Moreover, even achieving this accuracy requires {{careful attention to}} scaling to minimize loss of precision, and fixed-point FFT algorithms involve rescaling at each intermediate stage of decompositions like Cooley–Tukey.|$|R
40|$|We analyze high {{signal-to-noise}} spectrophotometric observations acquired {{simultaneously with}} TWIN, a double-arm spectrograph, from 3400 to 10400 Å of three star-forming {{regions in the}} HII galaxy SDSS J 165712. 75 + 321141. 4. We have measured four line temperatures: Te([OIII]), Te([SIII]), Te([OII]), and Te([SII]), with high precision, <b>rms</b> <b>errors</b> of order 2...|$|R
50|$|In fixed-point arithmetic, the finite-precision errors {{accumulated}} by FFT algorithms are worse, with <b>rms</b> <b>errors</b> {{growing as}} O(√N) for the Cooley-Tukey algorithm (Welch, 1969). Moreover, even achieving this accuracy requires {{careful attention to}} scaling to minimize loss of precision, and fixed-point FFT algorithms involve rescaling at each intermediate stage of decompositions like Cooley-Tukey.|$|R
50|$|Quantitative {{comparisons}} between different eyes and conditions are usually made using RMS (root mean square). To measure RMS {{for each type}} of aberration involves squaring the difference between the aberration and mean value and averaging it across the pupil area. Different kinds of aberrations may have equal RMS across the pupil but have different effects on vision, therefore, <b>RMS</b> <b>error</b> is unrelated to visual performance. The majority of eyes have total RMS values less than 0.3 µm.|$|E
5000|$|The sample Taylor diagram {{shown in}} Figure 1 {{provides}} {{a summary of}} the relative skill with which several global climate models simulate the spatial pattern of annual mean precipitation. Eight models, each represented by a different letter on the diagram, are compared, and the distance between each model and the point labeled “observed” is a measure of how realistically each model reproduces observations. For each model, three statistics are plotted: the Pearson correlation coefficient (gauging similarity in pattern between the simulated and observed fields) is related to the azimuthal angle (blue contours); the centered <b>RMS</b> <b>error</b> in the simulated field is proportional to the distance from the point on the x-axis identified as “observed” (green contours); and the standard deviation of the simulated pattern is proportional to the radial distance from the origin (black contours). It is evident from this diagram, for example, that for Model F the correlation coefficient is about 0.65, the <b>RMS</b> <b>error</b> is about 2.6 mm/day and the standard deviation is about 3.3 mm/day. Model F’s standard deviation is clearly greater than the standard deviation of the observed field (indicated by the dashed contour at radial distance 2.9 mm/day).|$|E
50|$|Zernike polynomials {{are usually}} {{expressed}} in terms of polar coordinates (ρ,θ), where ρ is radial coordinate and θ is the angle. The advantage of expressing the aberrations in terms of these polynomials includes the fact that the polynomials are independent of one another. For each polynomial the mean value of the aberration across the pupil is zero and the value of the coefficient gives the <b>RMS</b> <b>error</b> for that particular aberration (i.e. the coefficients show the relative contribution of each Zernike mode to the total wavefront error in the eye). However these polynomials have the disadvantage that their coefficients are only valid for the particular pupil diameter they are determined for.|$|E
40|$|Abstract. Metallicity {{calibrations}} of low-resolution pa-rameters {{are potentially}} useful for (at least) two problems: {{the properties of}} moving groups, and the supermetallicity problem in K giants. In this paper, metallicity calibra-tions are derived for six sets of parameters. One of these parameters is the DDO CN index CN. This parameter and three others are calibrated for use with evolved G and K stars. Two additional sets of low-resolution param-eters are calibrated for use with G and K dwarfs. The calibrations are derived by comparing the input data with two catalogs of homogenized high-dispersion results from diverse authors (see Taylor 1995, 1999 a). Using <b>rms</b> <b>errors</b> that are given in the catalogs, intrinsic <b>rms</b> <b>errors</b> are de-rived for metallicities deduced from the calibrations. The errors {{turn out to be}} comparable to those that apply for averaged high-dispersion results...|$|R
40|$|The {{method of}} linear {{covariance}} analysis {{is applied to}} the study of an aerospace vehicle integrated guidance-and-navigation system. This requires that the rms trajectory deviations of the vehicle be computed along with the <b>rms</b> <b>errors</b> of the navigation system. The technique is used to examine the performance of the shuttle orbiter vehicle's guidance-and-navigation system during the terminal phase of the landing. Essentially the same rms trajectory deviations were obtained from a single covariance run as from a 30 -sample Monte-Carlo run set. Of particular interest in the study was the result that rms trajectory deviations for the integrated-system were generally significantly larger than the <b>rms</b> navigation <b>errors...</b>|$|R
40|$|Statistics such as {{average power}} density pattern, {{variance}} of the power density pattern and variance of the beam pointing error are related to hardware parameters such as transmitter <b>rms</b> phase <b>error</b> and <b>rms</b> amplitude <b>error.</b> Also a limitation on spectral width of the phase reference for phase control was established. A 1 km diameter transmitter appears feasible provided the total <b>rms</b> insertion phase <b>errors</b> of the phase control modules does not exceed 10 deg, amplitude errors do not exceed 10 % rms, and the phase reference spectral width does not exceed approximately 3 kHz. With these conditions the expected radiation pattern is virtually {{the same as the}} error free pattern, and the <b>rms</b> beam pointing <b>error</b> would be insignificant (approximately 10 meters) ...|$|R
5000|$|The FAA funded a {{four-year}} TAMDAR impact study that was concluded in January 2009. The {{study was conducted}} by the Global Systems Division (GSD) of NOAA under an FAA contract to ascertain {{the potential benefits of}} including TAMDAR data to the 3D-Var Rapid Update Cycle (RUC) model, which was the current operational aviation-centric model run by National Centers for Environmental Prediction (NCEP). Two parallel versions of the model were run with the control withholding the TAMDAR data. The results of this study concluded that significant gains in forecast skill were achieved with the inclusion of the data despite using 3D-Var assimilation methods. The reduction in 30-day running mean <b>RMS</b> <b>error</b> averaged throughout the contiguous United States within the boundary layer for model state variables were: ...|$|E
5000|$|The {{first step}} is {{identifying}} and tracking features. A feature is a specific point in the image that a tracking algorithm can lock onto and follow through multiple frames (SynthEyes calls them blips). Often features are selected because they are bright/dark spots, edges or corners depending on the particular tracking algorithm. Popular programs use template matching based on NCC score and <b>RMS</b> <b>error.</b> What is important is that each feature represents a specific point {{on the surface of}} a real object. As a feature is tracked it becomes a series of two-dimensional coordinates that represent the position of the feature across a series of frames. This series is referred to as a [...] "track". Once tracks have been created they can be used immediately for 2D motion tracking, or then be used to calculate 3D information.|$|E
40|$|This paper {{reviews the}} {{development}} of a root mean squared error (RMS) flaw sizing criterion. The effect of using an <b>RMS</b> <b>error</b> criterion for the sizing performance of the PISC II [1 - 3] inspection teams is investigated. A crucial part of establishing an <b>RMS</b> <b>error</b> sizing criterion is the decomposition of the components in the <b>RMS</b> <b>error</b> into the sum of the squares of the mean error, the standard error of measurement, and the slope error [4, 5] (See Appendix A). This decomposition assists in arriving at an <b>RMS</b> <b>error</b> sizing criterion that is equivalent to the criteria on slope, correlation coefficient, and mean of deviation, i. e., ε...|$|E
40|$|Rapid {{neutron capture}} or `$r$-process' {{nucleosynthesis}} {{may be responsible}} for half the production of heavy elements above iron on the periodic table. Masses {{are one of the most}} important nuclear physics ingredients that go into calculations of $r$-process nucleosynthesis as they enter into the calculations of reaction rates, decay rates, branching ratios and Q-values. We explore the impact of uncertainties in three nuclear mass models on $r$-process abundances by performing global monte carlo simulations. We show that root-mean-square (<b>rms)</b> <b>errors</b> of current mass models are large so that current $r$-process predictions are insufficient in predicting features found in solar residuals and in $r$-process enhanced metal poor stars. We conclude that the reduction of global <b>rms</b> <b>errors</b> below $ 100 $ keV will allow for more robust $r$-process predictions. Comment: 5 pages, 3 figures, invited talk at the 15 th International Symposium on Capture Gamma-Ray Spectroscopy and Related Topics (CGS 15), to appear in EPJ Web of Conference...|$|R
40|$|In this paper, {{accurate}} {{tree stand}} height retrieval is demonstrated using C-band Shuttle Radar Topography Mission (SRTM) height and ancillary data. The tree height retrieval algorithm {{is based on}} modeling uniform tree stands with a single layer of randomly oriented vegetation particles. For such scattering media, the scattering phase center height, as measured by SRTM, {{is a function of}} tree height, incidence angle, and the extinction coefficient of the medium. The extinction coefficient for uniform tree stands is calculated as a function of tree height and density using allometric equations and a fractal tree model. The accuracy of the proposed algorithm is demonstrated using SRTM and TOPSAR data for 15 red pine and Austrian pine stands (TOPSAR is an airborne interferometric synthetic aperture radar). The algorithm yields root-mean-square (<b>rms)</b> <b>errors</b> of 2. 5 - 3. 6 m, which is a substantial improvement over the 6. 8 - 8. 3 -m <b>rms</b> <b>errors</b> from the raw SRTM minus National Elevation Dataset Heights...|$|R
40|$|Ten-day mean {{surface level}} air-temperature from SSMI precipitable water (SSMI-Ta) has been derived and {{compared}} with the temperature from two ocean data buoys (Buoy-Ta) of Japan Meteorological Agency (JMA) {{for a period of}} six months (July–December, 1988). Statistical relations between air-temperature and mixing ratio, using data from ocean data buoys are used to derive air-temperature from mixing ratio, obtained from SSMI precipitable water. For getting the mixing ratio from precipitable water, regional mixing ratio-precipitable water relations have been used, instead of global relation proposed by Liu (1986). The <b>rms</b> <b>errors</b> (standard deviation of the difference between SSMI-Ta and Buoy-Ta) for two buoy locations are found to be 1. 15 and 1. 12 °C, respectively. Surface level temperature for the two buoy locations are also derived using direct regression relation between Buoy-Ta and precipitable water. The <b>rms</b> <b>errors</b> of the SSMI-Ta, in this case are found to be reduced to 1. 0 °C. 1...|$|R
40|$|During the August-September 2003 Autonomous Ocean Sampling Network-II ex-periment, the Harvard Ocean Prediction System (HOPS) and Error Subspace Sta-tistical Estimation (ESSE) {{system were}} {{utilized}} in real-time to forecast physical elds and uncertainties, assimilate various ocean measurements (CTD, AUVs, glid-ers and SST data), provide suggestions for adaptive sampling, and guide dynamical investigations. The qualitative {{evaluations of the}} forecasts showed {{that many of the}} surface ocean features were predicted, but that their detailed positions and shapes were less accurate. The root-mean-square errors of the real-time forecasts showed that the forecasts had skill out to 2 days. Mean one day forecast temperature <b>RMS</b> <b>error</b> was 0. 26 C less then persistence <b>RMS</b> <b>error.</b> Mean two day forecast tempera-ture <b>RMS</b> <b>error</b> was 0. 13 C less then persistence <b>RMS</b> <b>error.</b> Mean one or two da...|$|E
30|$|The {{algorithm}} proceeded {{as long as}} such an <b>RMS</b> <b>error</b> decreased. When the <b>RMS</b> <b>error</b> {{increased with}} respect to the previous step in the iteration, the algorithm was stopped and the estimated flow and production term at the previous step (i.e., the ones for which the RMS was minimum) were considered.|$|E
30|$|As expected, as σaccel increases, {{there is}} a {{deterioration}} in the <b>RMS</b> <b>error</b> performance. However, the ratio between the <b>RMS</b> <b>error</b> performance of the suboptimal ReDif-PF tracker and the benchmark optimal DcPF/CbPFb algorithms remains approximately constant (close to a factor of two) along the simulation period for all three different values of σaccel employed.|$|E
40|$|The optical {{constants}} of Hg xCd 1 -xTe as {{a function}} of energy and composition x are modeled over a wide spectral range from 1. 5 to 6 eV. The model employed represents an extension of Adachi's model and incorporates the adjustable broadening function rather than the conventional Lorentzian one. In this way, greater flexibility of the model is achieved, enabling us to obtain an excellent agreement with the experimental data. The relative <b>rms</b> <b>errors</b> obtained for all compositions are below 2. 5 % for the real part and below 6 % for the imaginary part of the index of refraction. The lowest <b>rms</b> <b>errors</b> are obtained for x = 0 (0. 6 % for the real part and 0. 7 % for the imaginary part of the index of refraction), and the highest for the x = 0. 91 (2. 4 % for the real part and 5. 8 % for the imaginary part). © 1999 American Institute of Physics. published_or_final_versio...|$|R
40|$|Temperature {{forecasts}} {{issued by}} Denver's four primary television stations and two primary newspapers were recorded {{every day for}} twelve consecutive months in 1995 - 1996. The daily forecasts included predictions of maxi-mum and minimum temperatures for one to five days (Day-l to Day- 5) in the future. The predictions were veri-fied using official observations of the National Weather Service. The media forecasts were compared with each other, with the observations, with predictions from the National Weather Service Medium Range Forecast Model Output Statistics (MRF-MOS), and with "no-skill " fore-casts based on persistence and climatology. Typically, the media forecasts differed little from each other or from the MRF-MOS, but very large departures occasionally occurred, especially for longer-range projections. Root mean square (<b>rms)</b> <b>errors</b> of daily maximum temperature forecasts increased from 5. 5 °F for Day-l to 10. 9 °F for Day- 5 for the six-media average. Although differences among the media forecasts were generally small, one media team had slightly smaller <b>rms</b> <b>errors</b> for temperature forecasts for maximum and for minimum temperatures {{for most of the}} five individual projections {{as well as for the}} five days combined. Skill score statistics indicated that the media outperformed persistence and climatology temperature forecasts by wide margins for Day-I. These margins nar-rowed substantially with increasing projection. The media-average skill score with respect to climatology for predicting daily high temperatures dropped from 0. 60 for Day-l to 0. 20 for Day- 5. The media-average temperature forecast <b>rms</b> <b>errors</b> were slightly smaller than those of the MRF-MOS for the short-range projections, but the MRF-MOS outperformed all media temperature forecasters by Day- 5. Beyond Day-I, persistence gave the poorest fore-casts by far...|$|R
40|$|We propose {{an optical}} {{performance}} monitoring technique for simultaneous monitoring of optical signal-to-noise ratio (OSNR), chromatic dispersion (CD), and polarization-mode dispersion (PMD) using an {{artificial neural network}} trained with asynchronous amplitude histograms (AAHs). Simulations are conducted to demonstrate the technique for both 40 -Gb/s return-to-zero differential quadrature phase-shift keying (RZ-DQPSK) and 40 -Gb/s noneturn-to-zero 16 quadrature amplitude modulation (16 -QAM) systems. The OSNR, CD, and PMD monitoring range and root-mean-square (<b>rms)</b> <b>errors</b> are 10 - 30 and 0. 43 dB, 0 - 400 and 9. 82 ps/nm, and 0 - 10 and 0. 92 ps, respectively, for RZ-DQPSK systems. For 16 -QAM system, the monitoring range and <b>rms</b> <b>errors</b> are 10 - 30 and 0. 2 dB, 0 - 400 and 9. 66 ps/nm, and 0 - 30 and 0. 65 ps for OSNR, CD, and PMD, respectively. As the generation of AAH does not require any clock or timing recovery, the proposed technique {{can serve as a}} low-cost option to realize in-service multiparameter monitoring for the next-generation transparent optical networks...|$|R
