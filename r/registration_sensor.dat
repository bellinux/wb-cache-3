4|83|Public
30|$|Even though {{numerous}} palm pose can {{be captured}} and acquired from motion sensing devices, {{there are still}} some theoretically interesting problems, space <b>registration,</b> <b>sensor</b> data filtering, and dynamic response.|$|E
40|$|Abstract. This paper {{introduces}} the Sensapp platform, a semantic and OGCbased sensor application platform to enable users to register, annotate, search, visualize, and compose OGC-based sensors {{and services for}} creating addedvalue services and applications. Functionalities of Sensapp such as sensor <b>registration,</b> <b>sensor</b> data visualization, visual composition and generation of executable service compositions are presented through the demo...|$|E
40|$|Abstract. In {{this paper}} we {{describe}} a virtual ultrasound imaging {{system for the}} simulation of ultrasound guided needle insertion procedures, {{which is designed to}} improve the early training stages of interventional radiology trainees. A pair of calibrated magnetic 3 D motion sensors are used to capture the position and orientation of a mock ultrasound probe and needle, whilst emulational ultrasound images are generated in real-time from a labelled volumetric data set that is non-rigidly aligned to a physical model of human body. To achieve a realistic simulation of ultrasound imaging, a set of volumetric textures are constructed to represent the typical appearance of ultrasound images, and an alpha blending method is applied to produce the radial blurring effect. The procedures of volumetric <b>registration,</b> <b>sensor</b> calibration, construction of texture bank, and image rendering, are presented. ...|$|E
40|$|SemMOB enables dynamic <b>registration</b> of <b>sensors</b> via mobile devices, search, {{and near}} {{real-time}} inference over sensor observations in ad-hoc mobile environments (e. g., fire fighting). We demonstrate SemMOB {{in the context}} of an emergency response use case that requires automatic and dynamic <b>registrations</b> of <b>sensor</b> devices and annotation of sensor observations, decoding of latitude-longitude information in terms of human sensible names, fusion and abstraction of sensor values using background knowledge, and their visualization using mash-up...|$|R
40|$|ABSTRACT. One of {{the main}} reasons in forming a sensor network is to combine the {{information}} seen from different sensors to produce a single integrated picture that is an accurate representation of the scene of interest. An often overlooked problem in network design is the proper <b>registration</b> of the <b>sensors</b> in the network. <b>Sensor</b> <b>registration</b> {{can be seen as the}} process of removing (accounting for) non-random errors, or biases, in the sensor data. Without properly accounting for these errors, the quality of the composite picture can, and oftentimes does, degrade. In this paper, we present an approach for solving the <b>sensor</b> <b>registration</b> problem, based on a new continuous meta-heuristic, when not all data is seen by all sensors, and the correspondence of data seen by the different sensors is not known a priori. Considering a real problem from the defense industry, we show this approach performs better than other approaches in the literature. 1...|$|R
40|$|One of {{the major}} {{problems}} in multiple sensor surveillance systems is inadequate <b>sensor</b> <b>registration.</b> We propose {{a new approach to}} <b>sensor</b> <b>registration</b> based on layered neural networks. The nonparametric nature of this approach enables many different kinds of sensor biases to be solved. As part of the implementation we develop some modifications to the common network training algorithm to tackle the inherent randomness in all components of the training set. Manuscript received October 28, 1997; revised February 10, 1999; released for publication November 24, 1999. Refereeing of this contribution was handled by S. Shrier. IEEE Log No. T-AES/ 36 / 1 / 02588...|$|R
40|$|During {{the last}} ten years, the {{availability}} of images acquired from unmanned aerial vehicles (UAVs) has been continuously increasing due to the improvements and economic success of flight and sensor systems. From our point of view, reliable and automatic image-based change detection may contribute to overcoming several challenging problems in military reconnaissance, civil security, and disaster management. Changes within a scene {{can be caused by}} functional activities, i. e., footprints or skid marks, excavations, or humidity penetration; these might be recognizable in aerial images, but are almost overlooked when change detection is executed manually. With respect to the circumstances, these kinds of changes may be an indication of sabotage, terroristic activity, or threatening natural disasters. Although image-based change detection is possible from both ground and aerial perspectives, in this paper we primarily address the latter. We have applied an extended approach to change detection as described by Saur and Kruger, 1 and Saur et al. 2 and have built upon the ideas of Saur and Bartelsen. 3 The commercial simulation environment Virtual Battle Space 3 (VBS 3) is used to simulate aerial "before" and "after" image acquisition concerning flight path, weather conditions and objects within the scene and to obtain synthetic videos. Video frames, which depict the same part of the scene, including "before" and "after" changes and not necessarily from the same perspective, are registered pixel-wise against each other by a photogrammetric concept, which is based on a homography. The pixel-wise registration is used to apply an automatic difference analysis, which, to a limited extent, is able to suppress typical errors caused by imprecise frame <b>registration,</b> <b>sensor</b> noise, vegetation and especially parallax effects. The primary concern {{of this paper is to}} seriously evaluate the possibilities and limitations of our current approach for image-based change detection with respect to the flight path, viewpoint change and parametrization. Hence, based on synthetic "before" and "after" videos of a simulated scene, we estimated the precision and recall of automatically detected changes. In addition and based on our approach, we illustrate the results showing the change detection in short, but real video sequences. Future work will improve the photogrammetric approach for frame registration, and extensive real video material, capable of change detection, will be acquired...|$|E
40|$|A new least-squares {{procedure}} is presented which fuses {{data from a}} standard perspective sensor (CCD camera, FLIR sensor, etc.) and a range sensor (LADAR) based upon corresponding features identified on a 3 D object model and in each image. The algorithm solves for both the pose estimate of the object relative to the <b>sensors</b> and the <b>registration</b> between <b>sensors.</b> This model-based coregistration process is being developed to support future work recognizing modeled 3 D objects in scenes imaged by both optical (FLIR and CCD) and range (LADAR) sensors. Coregistration results are presented for both synthetic and real world tests. The algorithm requires an initial pose and <b>sensor</b> <b>registration</b> estimate. Tests on controlled synthetic data show it is robust with respect to substantial errors in initial translation and orientation errors up to roughly 45 degrees. 1 Introduction The work presented here {{is part of a}} larger project developing new ways of identifying modeled 3 D objects in range, IR an [...] ...|$|R
40|$|We {{propose a}} range-free {{localization}} {{framework in which}} a network of randomly deployed acoustic sensors can passively use natural acoustic phenomena within its environment to localize itself. We introduce a novel approach for <b>registration</b> of <b>sensors</b> observations which takes advantage of a clustering technique on triplets of associated observations. A Bayesian filtering method is employed to incrementally improve system state estimation as more observations become available. To {{the best of our}} knowledge this is the first work done on rangefree passive acoustic localization. Simulation experiments of the proposed algorithms are presented. 1...|$|R
40|$|Abstract. A {{publicly}} available MATLAB GUI-based resource for analysis and mapping of near infrared topographic neuroimaging measures is described. The code supports <b>sensor</b> <b>registration,</b> Level 1 and 2 GLM-based statistical parametric mapping and hyperscanning, among other features. OCIS codes: (170. 2655) Functional monitoring and imaging; (200. 3050) Information processing; (110. 1758) Computational imaging 1. Introduction: Acces...|$|R
30|$|<b>Registration</b> between <b>sensor</b> sets was {{performed}} on all imagery regardless of within-sensor registration determination. This process also required {{the use of an}} anchor image. To ensure that all imagery was aligned consistently across sensors, the anchor image for between-sensor registration was the first “up” image taken in the low resolution visible set. This provided a basis for defining not only the position of the Landolt C (as in within-sensor registration) but also the location, size, and proportion of the stimulus in relation to the image frame. Between-sensor registration used the projective method allowing for shifts of the imagery to match any difference in viewing angle, thus equating all stimulus locations regardless of camera geometry. This method was applied to all stimuli following any already-completed within-sensor registration.|$|R
40|$|Abstract. This paper investigates {{one problem}} arising from {{ubiquitous}} sensing: can {{the position of}} a set of randomly placed sensors be automatically determined even if they do not have an overlapping field of view. (If the view overlapped, then standard stereo auto-calibration can be used.) This paper shows that the problem is solveable. Distant moving features allow accurate orientation <b>registration.</b> Given the <b>sensor</b> orientations, nearby linearly moving features allow full pose registration, up to a scale factor...|$|R
40|$|As the {{development}} of 3 D <b>sensors,</b> <b>registration</b> of 3 D data (e. g. point cloud) coming from different kind of sensor is dispensable and shows great demanding. However, point cloud <b>registration</b> between different <b>sensors</b> is challenging because of the variant of density, missing data, different viewpoint, noise and outliers, and geometric transformation. In this paper, we propose a method to learn a 3 D descriptor for finding the correspondent relations between these challenging point clouds. To train the deep learning framework, we use synthetic 3 D point cloud as input. Starting from synthetic dataset, we use region-based sampling method to select reasonable, large and diverse training samples from synthetic samples. Then, we use data augmentation to extend our network be robust to rotation transformation. We focus our work on more general cases that point clouds coming from different sensors, named cross-source point cloud. The experiments show that our descriptor is not only able to generalize to new scenes, but also generalize to different sensors. The results demonstrate that the proposed method successfully aligns two 3 D cross-source point clouds which outperforms state-of-the-art method...|$|R
40|$|The work {{reported}} here demonstrates how to automatically compute {{the position and}} attitude of a targeting reflective alignment concept (TRAC) camera relative to the robot end effector. In the robotics literature {{this is known as}} the <b>sensor</b> <b>registration</b> problem. The registration problem is important to solve if TRAC images need to be related to robot position. Previously, when TRAC operated {{on the end of a}} robot arm, the camera had to be precisely located at the correct orientation and position. If this location is in error, then the robot may not be able to grapple an object even though the TRAC sensor indicates it should. In addition, if the camera is significantly far from the alignment it is expected to be at, TRAC may give incorrect feedback for the control of the robot. A simple example is if the robot operator thinks the camera is right side up but the camera is actually upside down, the camera feedback will tell the operator to move in an incorrect direction. The automatic calibration algorithm requires the operator to translate and rotate the robot arbitrary amounts along (about) two coordinate directions. After the motion, the algorithm determines the transformation matrix from the robot end effector to the camera image plane. This report discusses the TRAC <b>sensor</b> <b>registration</b> problem...|$|R
40|$|Navigation in {{neurovascular}} interventions {{is currently}} hindered {{by the fact}} that the vessel infrastructure and the instruments are only shown simultaneously in a single real-time image during the use of a roadmap. An image guidance system based on a single C-arm is proposed, which will enable a 3 D-roadmap showing a blended image of a 3 D-rotational angiography and a realtime fluoroscopy image. The images are combined using machine-based <b>registration,</b> employing <b>sensors</b> mounted on the patient table and the C-arm. The setup of the system and its implications for the interventional procedures are described. The feasibility of the system is discussed with respect to the desired accuracy of matching and speed. The 3 D-roadmap is expected to enhance 3 D-insight for the interventionist and will facilitate instrument navigation. Implementation of the system will lead to a reduction both of the X-ray dosage and of the use of contrast agen...|$|R
40|$|Abstract In this paper, a new {{multi-sensor}} calibration approach, called iterative {{registration and}} fusion (IRF), is presented. The key {{idea of this}} approach is to use surfaces reconstructed from multiple point clouds to enhance the registration accuracy and robustness. It calibrates the relative position and orientation of the spatial coordinate systems among multiple sensors by iteratively registering the discrete 3 D sensor data against an evolving reconstructed B-spline surface, which results from the Kalman filterbased multi-sensor data fusion. Upon each <b>registration,</b> the <b>sensor</b> data gets closer to the surface. Upon fusing the newly registered sensor data with the surface, the updated surface represents the sensor data more accurately. We prove that such an iterative registration and fusion process is guaranteed to converge. We further demonstrate in experiments that the IRF can result in more accurate and more stable calibration than many classical point cloud registration methods...|$|R
40|$|Abstract — This paper {{provides}} {{an introduction to}} sensor fusion techniques for target tracking. It presents an overview of common filtering techniques that are effective for moving targets as well as methods of overcoming problems specific to target tracking, such as measurement-to-track association and <b>sensor</b> <b>registration.</b> The computational demand of such algorithms is discussed and various practices, including dis-tributed processing of target tracks and sensor management, are proposed to help reduce this demand. Final comments include a discussion of applications and implementation issues specific to the presented scenarios. I...|$|R
40|$|Based on {{information}} fusion by multi-sensor tracking system, registration algorithm for heterogeneous spatiotemporal measurements is discussed. In the paper, principles of <b>sensor</b> <b>registration</b> for out-ofsequence measurement are introduced, the IMM based registration algorithm for multi-lag OOSM is proposed. The method that judging how old negative-time measurement {{could be used}} to retrodict the state is presented. On the condition of DAP (data association problem), the algorithm of how to obtain a strict chronological sequence of measurement is discussed. Finally, the conclusions and further works are addressed and some future problems are given...|$|R
40|$|A {{system for}} live high quality surface {{reconstruction}} using a single moving depth camera on a commodity hardware is presented. High accuracy and real-time frame rate {{is achieved by}} utilizing graphics hardware computing capabilities via OpenCL and by using sparse data structure for volumetric surface representation. Depth sensor pose is estimated by combining serial texture registration algorithm with iterative closest points algorithm (ICP) aligning obtained depth map to the estimated scene model. Aligned surface is then fused into the scene. Kalman filter is used to improve fusion quality. Truncated signed distance function (TSDF) stored as block-based sparse buffer is used to represent surface. Use of sparse data structure greatly increases accuracy of scanned surfaces and maximum scanning area. Traditional GPU implementation of volumetric rendering and fusion algorithms were modified to exploit sparsity to achieve desired performance. Incorporation of texture <b>registration</b> for <b>sensor</b> pose estimation and Kalman filter for measurement integration improved accuracy and robustness of scanning process...|$|R
40|$|Image <b>registration</b> for <b>sensor</b> fusion is a {{valuable}} technique to acquire 3 D and colour information for a scene. Nevertheless, this process normally relies on feature-matching techniques, which is a drawback for combining sensors that {{are not able to}} deliver common features. The combination of ToF and RGB cameras is an instance that problem. Typically, the fusion of these sensors is based on the extrinsic parameter computation of the coordinate transformation between the two cameras. This leads to a loss of colour information because of the low resolution of the ToF camera, and sophisticated algorithms are required to minimize this issue. This work proposes a method for <b>sensor</b> <b>registration</b> with non-common features and that avoids the loss of colour information. The depth information is used as a virtual feature for estimating a depth-dependent homography lookup table (Hlut). The homographies are computed within sets of ground control points of 104 images. Since the distance from the control points to the ToF camera are known, the working distance of each element on the Hlut is estimated. Finally, two series of experimental tests have been carried out in order to validate the capabilities of the proposed method. The authors acknowledge funding from the European commission in the 7 th Framework Programme (CROPS Grant Agreement No. 246252) and partial funding under ROBOCITY 2030 -III-CM project (Robótica aplicada a la mejora de la calidad de vida de los ciudadanos. Fase III; S 2013 /MIT- 2748), funded by Programa de Actividades I+D en la Comunidad de Madrid and cofunded by Structural Funds of the EU. Héctor Montes also acknowledges support from Universidad Tecnológica de Panamá. We acknowledge support by the CSIC Open Access Publication Initiative through its Unit of Information Resources for Research (URICI). CHF 1, 620 APC fee funded by the EC FP 7 Post-Grant Open Access PilotPeer reviewe...|$|R
40|$|Abstract — Pipe {{inspection}} is {{a critical}} activity in gas production facilities and many other industries. In this paper, we contribute a stereo visual odometry system for creating high resolution, sub-millimeter maps of pipe surfaces. Such maps provide both 3 D structure and appearance {{information that can be}} used for visualization, cross <b>registration</b> with other <b>sensor</b> data, inspection and corrosion detection tasks. We present a range of optical configuration and visual odometry techniques that we use to achieve high accuracy while minimizing specular reflections. We show empirical results from a range of datasets to demonstrate the performance of our approach. I...|$|R
40|$|In {{this paper}} we {{investigate}} a problem arising in decentralized <b>registration</b> of <b>sensors.</b> The application we consider involves a heterogeneous collection of sensors - some sensors have on-board Global Positioning System (GPS) capabilities while others do not. All sensors have wireless communications capability but the wireless communication has limited effective range. Sensors can communicate only with other sensors that are within a fixed distance of each other. Sensors with GPS capability are self-registering. Sensors without GPS capability are {{less expensive and}} smaller but they must compute estimates of their location using estimates of the distances between themselves and other sensors within their radio range. GPS-less sensors may be several radio hops away from GPS-capable <b>sensors</b> so <b>registration</b> must be inferred transitively. Our approach to solving this registration problem involves minimizing a global potential or penalty function by using only local information, determined by the radio range, available to each sensor. The algorithm we derive is a special case of a more general methodology we have developed called "Emergence Engineering"...|$|R
40|$|Abstract [...] In this paper, {{we argue}} that {{biological}} vision and electronic image acquisition share common principles despite their vastly different implementations. These shared principles {{are based on the}} need to acquire and a common set of input stimuli as well as the need to generalize from the acquired images. Two related principles are discussed in detail, namely multiple parallel image representations and the use of dedicated local memory in various stages of acquisition and processing. We review relevant literature in visual neuroscience and image systems engineering to support our argument. Particularly, the paper discusses multiple capture image acquisition, with applications such as dynamic range, field-of-view or depth-of-field extension. Finally, as an example, a novel multiple-capture-single-image CMOS sensor is presented. This sensor has been developed at Stanford and it illustrates both principles that, we believe, are shared among biological vision and image acquisition. Index Terms [...] image sensor, image acquisition, multiple capture, multiple representations, image mosaicking, image <b>registration,</b> CMOS <b>sensor,</b> dynamic range, digital camera, memory architecture, visual pathways, human vision...|$|R
40|$|This paper {{focuses on}} data fusion {{algorithms}} and flight {{results from a}} multi-sensor obstacle detection and tracking system based on radar/electro-optical (EO) fusion and aimed at UAS non-cooperative collision avoidance. The system {{was developed in the}} framework of a research project carried out by the Italian Aerospace Research Center and the Department of Aerospace Engineering of the university of Naples “Federico II”. It was then installed onboard an optionally piloted flying laboratory of Very Light Aircraft category, and an extensive flight test campaign with a single intruder aircraft was carried out to evaluate the capability of the tracking system to support autonomous collision avoidance. Solutions adopted for the on-board software are discussed regarding obstacle detection in EO images, space/time <b>registration</b> of <b>sensors</b> information, tuning of the tracking filter, handling of sensors’ latency, and inclusion of navigation measurements. Potential of radar/EO tracking in terms of achievable accuracy in estimating intruder position and velocity is compared with standalone radar tracking performance. It is demonstrated that the increase in angular accuracy and data rate provided by the EO sensors improves collision detection performance...|$|R
40|$|Thesis (M. S.) [...] Wichita State University, College of Engineering, Dept. of Electrical and Computer Science Engineering"December 2007. "Image {{registration}} {{is the process}} of integrating data from different coordinate systems into one coordinate system. It is of great interest in video surveillance because of its capability to combine images and generate a larger view of the area under observation, while retaining all the information in the images. This thesis proposes a novel method for registering images obtained from low resolution visual sensor networks by using change detection as a tool for image <b>registration.</b> Two <b>sensors</b> with overlapping fields of view are used to capture images at regular intervals. The images differences are found and significant change is identified using a random threshold. This significant change forms the basis of identifying control points. Once the control points are identified then, the reference image is transformed with respect to the base image using affine transformation. The transformed image and base image are stitched together to obtain the registered image. Experiments using the proposed scheme have shown that precise registration can be achieved...|$|R
40|$|An {{important}} {{prerequisite for}} successful multisensor integration {{is that the}} data from the reporting sensors are transformed to a common reference frame free of systematic or registration bias errors. If not properly corrected, registration errors can seriously degrade the global surveillance system performance. The absolute <b>sensor</b> <b>registration</b> (or grid-locking) process aligns remote data coming from sensors to an absolute reference frame. In this paper we consider a multi-target scenario and we address the problem of jointly estimating registration errors involved in the absolute grid-locking problem with two radars. A linear Least Squares (LS) estimator is derived and its statistical performance compared to the hybrid Cramér-Rao lower bound (HCRLB) ...|$|R
40|$|In this paper, online {{multisensor}} {{data fusion}} algorithm using CORBA event channel is proposed, {{in order to}} deal with simplifying problem in <b>sensor</b> <b>registration</b> and fusion for vehicle’s state estimation. The networked based navigation concept for Autonomous Ground Vehicle (AGV) using several sensors is presented. A simulation of various application scenarios are considered by choosing several parameters of UKF, i. e. weighting constant for sigma points and square root matrix. Normalized mean-square error (MSE) of Monte Carlo simulations are computed and reported in the simulation results. Furthermore, the middleware infrastructure based on Open Control Platform (OCP) to support the interconnection between the whole filter structures also reported...|$|R
40|$|International audienceMulti-sensor {{data fusion}} plays an {{essential}} role in most robotic applications. Appropriate registration of information from different sensors is a fundamental requirement in multi-sensor data fusion. Registration requires significant effort particularly when sensor signals do not have direct geometric interpretations, observer dynamics are unknown and occlusions are present. In this paper, we propose Mutual Information (MI) based <b>sensor</b> <b>registration</b> which exploits the effect of a common cause in the observed space on the sensor outputs that does not require any prior knowledge of relative poses of the observers. Simulation results are presented to substantiate the claim that the algorithm is capable of registering the sensors in the presence of substantial observer dynamics...|$|R
40|$|Digital image {{registration}} {{is very important}} in many applications, such as medical imagery, robotics, visual inspection, and remotely sensed data processing. NASA's Mission To Planet Earth (MTPE) program will be producing enormous Earth global change data, reaching hundreds of Gigabytes per day, that are collected form different spacecrafts and different perspectives using many sensors with diverse resolutions and characteristics. The analysis of such data requires integration, therefore, accurate registration of these data. Image registration is defined as the process which determines the most accurate relative orientation between two or more images, acquired at the same or different times by different or identical <b>sensors.</b> <b>Registration</b> can also provide the absolute orientation between an image and a map. 1...|$|R
40|$|In {{this paper}} {{we present a}} new method for the <b>registration</b> of {{multiple}} <b>sensors</b> applied to a mobile robotic inspection platform. Our main technical challenge is automating the integration process for various multimodal inputs, such as depth maps, and multi-spectral images. This task is approached through a unified framework based on a new registration criterion that can be employed for both 3 D and 2 D datasets. The system embedding this technology reconstructs 3 D models of scenes and objects that are inspected by an autonomous platform in high security areas. The models are processed and rendered with corresponding multi-spectral textures, which greatly enhances both human and machine identification of threat objects...|$|R
40|$|Summary. Multi-sensor {{data fusion}} plays an {{essential}} role in most robotic applications. Appropriate registration of information from different sensors is a fundamental requirement in multi-sensor data fusion. Registration requires significant effort particularly when sensor signals do not have direct geometric interpretations, observer dynamics are unknown and occlusions are present. In this paper, we propose Mutual Information (MI) based <b>sensor</b> <b>registration</b> which exploits the effect of a common cause in the observed space on the sensor outputs that does not require any prior knowledge of relative poses of the observers. Simulation results are presented to substantiate the claim that the algorithm is capable of registering the sensors in the presence of substantial observer dynamics. ...|$|R
40|$|RML) is {{a popular}} {{methodology}} for estimating unknown static parameters in state-space models. We describe how a completely decentralized version of RML can be implemented in dynamic graphical models through the propagation of suitable messages that are exchanged between neighbouring nodes of the graph. The resulting algorithm {{can be interpreted as}} a generalization of the celebrated belief propagation algorithm to compute likelihood gradients. This algorithm is applied to solve the <b>sensor</b> <b>registration</b> and localisation problem for sensor networks. An exact implementation is given for dynamic linear Gaussian models without loop. If loops are present, a loopy version of the algorithm is described. For non-linear non Gaussian scenarios, a Sequential Monte Carlo (SMC) or particle filter implementation is sketched...|$|R
40|$|An Enhanced Vision System (EVS) {{utilizing}} multi-sensor {{image fusion}} is currently under {{development at the}} NASA Langley Research Center. The EVS will provide enhanced images of the flight environment to assist pilots in poor visibility conditions. Multi-spectral images obtained from a short wave infrared (SWIR), a long wave infrared (LWIR), and a color visible band CCD camera, are enhanced and fused using the Retinex algorithm. The images from the different sensors {{do not have a}} uniform data structure: the three sensors not only operate at different wavelengths, but they also have different spatial resolutions, optical fields of view (FOV), and boresighting inaccuracies. Thus, in order to perform image fusion, the images must first be co-registered. Image registration is the task of aligning images taken at different times, from different sensors, or from different viewpoints, so that all corresponding points in the images match. In this paper, we present two methods for registering multiple multi-spectral images. The first method performs <b>registration</b> using <b>sensor</b> specifications to match the FOVs and resolutions directly through image resampling. In the second method, registration is obtained through geometric correction based on a spatial transformation defined by user selected control points and regression analysis...|$|R
40|$|To improve {{environmental}} monitoring, {{the availability}} of great coverage of spatio -temporal data in an interoperable way is crucial for its integration into environmental models, for example, to compute fire danger models. To produce up-to-date and accurate results those models need {{the availability of}} data with high temporal and spatial resolution. Thus, {{it is promising to}} consider the increasing number of insitu sensors providing observations of our environment in real-time. Today, interoperable access to such spatio-temporal data is achieved by Geospatial Information Infrastructures (GIIs). From a technical point of view GIIs provide this data through standards-based Web service interfaces. While those Web service interfaces already enable the interoperable discovery and retrieval of sensor observations, the functionality to publish sensor observations is still an arduous task. Hence, in this paper, we present an approach to improve the <b>registration</b> of <b>sensors</b> and the publication of their observations via standards- based Web service interfaces. We evaluate our approach by extending a standards-based GII and by applying the developed approach to the example of integrating in-situ weather observations into the European Forest Fire Information System for assessing fire danger in SpainEuroGEOSS - European FP 7 Project n. 22648...|$|R
40|$|The {{fusion of}} {{images from the}} visible and long-wave {{infrared}} (thermal) portions of the spectrum produces images that have improved face recognition performance under varying lighting conditions. This is because long-wave infrared images {{are the result of}} emitted, rather than reflected, light and are therefore less sensitive to changes in ambient light. Similarly, 3 D and 2. 5 D images have also improved face recognition under varying pose and lighting. The opacity of glass to long-wave infrared light, however, means that the presence of eyeglasses in a face image reduces the recognition performance. This thesis presents the design and performance evaluation of a novel camera system which is capable of capturing spatially registered visible, near-infrared, long-wave infrared and 2. 5 D depth video images via a common optical path requiring no spatial <b>registration</b> between <b>sensors</b> beyond scaling for differences in sensor sizes. Experiments using a range of established face recognition methods and multi-class SVM classifiers show that the fused output from our camera system not only outperforms the single modality images for face recognition, but that the adaptive fusion methods used produce consistent increases in recognition accuracy under varying pose, lighting and with the presence of eyeglasses...|$|R
40|$|Many multi-object {{estimation}} problems {{require additional}} estimation of model or sensor parameters {{that are either}} common to all objects or related to unknown characterisation {{of one or more}} sensors. Important examples of these include <b>registration</b> of multiple <b>sensors,</b> estimating clutter profiles, and robot localisation. Often these parameters are estimated separately to the multi-object estimation process, which can lead to systematic errors or overconfidence in the estimates. These parameters can be estimated jointly with the multi-object process based only on the sensor data using a single-cluster point process model. This paper presents novel results for joint parameter estimation and multi-object filtering based on a single-cluster second-order Probability Hypothesis Density (PHD) and Cardinalised PHD (CPHD) filter. Experiments provide a comparison between the discussed approaches using different likelihood functions...|$|R
