8|25|Public
50|$|Keylight 4.1 {{introduced}} the Keylight Ambassador. It {{was the first}} GRC platform to allow for both SAML and LDAP integration, the first to perform bulk tasks on data records, including data edits, workflow and <b>record</b> <b>deletion,</b> {{and the first to}} create ad-hoc reports on historic content. Keylight 4.1 also added support for syslog data collection.|$|E
50|$|There {{are three}} types of control records used {{to keep track of}} insertions and deletions applied in {{different}} deltas. They are insertion control <b>record,</b> <b>deletion</b> control record and end control record. Whenever a user changes some part of the text, a control record is inserted surrounding the change. These control records are stored in the body along with the original source code or text records.|$|E
30|$|Data mining {{technology}} is a very common computer technology, which has been widely used in many fields because of its superior performance. The method of talent management data cleaning in wireless sensor networks is studied based on data {{mining technology}}. The research status of data mining technology is first introduced at home and abroad, and the specific application forms of wireless sensor networks are analyzed. Then, the structure characteristics of wireless sensor networks are introduced, and a data cleansing technology is proposed based on clustering model. A cluster-based replication <b>record</b> <b>deletion</b> algorithm is proposed, and finally, the accuracy of data cleansing methods is verified. The {{results show that the}} research method of this paper is correct and effective.|$|E
50|$|A {{database}} (.DBF) file {{is composed}} of a header, data <b>records,</b> <b>deletion</b> flags, and an end-of-file marker. The header contains information about the file structure, and the records contain the actual data. One byte of each record is reserved for the deletion flag.|$|R
5000|$|Deleted files : To {{prevent the}} unintentional {{restoration}} of files {{that have been}} intentionally deleted, a <b>record</b> of the <b>deletion</b> must be kept.|$|R
40|$|The initial {{database}} search cost deteriorates due to <b>record</b> additions, <b>deletions</b> and updates performed during the system operation. Previous studies proposed strategies for selecting the optimum maintenance points by making various {{assumptions about the}} rate of database deterioration, the database planning period, the maintenance interval, etc. However, the studies did not express the optimum maintenance points as a function o...|$|R
40|$|Abstract:- The {{frequent}} pattern tree (FP-tree) is {{an efficient}} data structure for association-rule mining without generation of candidate itemsets. It, however, needed to process all transactions in a batch way. In addition to record insertion, <b>record</b> <b>deletion</b> is also commonly seen in real-application. In this paper, we propose {{the structure of}} prelarge trees for efficiently handling deletion of records based {{on the concept of}} pre-large itemsets. Due to the properties of pre-large concepts, the proposed approach does not need to rescan the original database until a number of records have been deleted. The proposed approach can thus achieve a good execution time for tree construction especially when a small number of records are deleted each time. Experimental results also show that the proposed approach has a good performance for incrementally handling deleted records...|$|E
40|$|The {{use of any}} modern {{computer}} system leaves unintended traces of expired data and remnants of users ’ past activities. In this paper, we investigate the unintended persistence of data stored in database systems. This data can be recovered by forensic analysis, and it {{poses a threat to}} privacy. First, we show how data remnants are preserved in database table storage, the transaction log, indexes, and other system components. Our evaluation of several real database systems reveals that deleted data is not securely removed from database storage and that users have little control over the persistence of deleted data. Second, we address the problem of unintended data retention by proposing a set of system transparency criteria: data retention should be avoided when possible, evident to users when it cannot be avoided, and bounded in time. Third, we propose specific techniques for secure <b>record</b> <b>deletion</b> and log expunction that increase the transparency of database systems, making them more resistant to forensic analysis...|$|E
40|$|The Frequent-Pattern-tree (FP tree) is an {{efficient}} data structure for association-rule mining without generation of candidate itemsets. It {{was used to}} represent a database into a tree structure which stored only frequent items. It, however, needed to process all transactions in a batch way. In the past, Hong etal. thus proposed {{an efficient}} incremental mining algorithm for handling newly inserted transactions. In addition to record insertion, <b>record</b> <b>deletion</b> from databases is also commonly seen in real-applications. In this paper, we thus attempt to modify the FP-tree construction algorithm for efficiently handling deletion of records. A fast updated FP-tree (FUFP-tree) structure is used, which makes the tree update process become easier. An FUFP-tree maintenance algorithm for the deletion of records is also proposed for reducing the execution time in reconstructing the tree when records are deleted. Experimental results also show that the proposed FUFP-tree maintenance algorithm for deletion of records runs faster than the batch FP-tree construction algorithm for handling deleted records and generates nearly the same tree structure as the FP-tree algorithm. The proposed approach can thus achieve a good trade-off between execution time and tree complexity. ...|$|E
40|$|We {{develop new}} {{algorithms}} {{for the management}} of transactions in a page-shipping client-server database system in which the physical database is organized as a sparse B-tree index. Our starvation-free fine-grained locking protocol combines adaptive callbacks with key-range lock-ing and guarantees repeatable-read-level isolation (i. e., serializability) for transactions containing any number of <b>record</b> insertions, <b>record</b> <b>deletions,</b> and key-range scans. Partial and total roll-backs of client transactions are performed by the client. Each structure modification such as a page split or merge is defined as an atomic action that affects only two levels of the B-tree and is logged using a single redo-only log record, so that the modification never needs to be un-done during transaction rollback or restart recovery. The steal-and-no-force buffering policy is applied by the server when flushing updated pages onto disk and by the clients when shipping updated data pages to the server, while pages involved in a structure modification are forced to the server when the modification is finished. The server performs the restart recovery from client and system failures using an ARIES/CSA-based recovery protocol. Our algorithms avoid access-ing stale data but allow a data page to be updated by one client transaction and read by many other client transactions simultaneously, and updates may migrate from a data page to anothe...|$|R
40|$|Includes bibliographical {{references}} (page 83) A {{method of}} file organization which combines fast access, ease of maintenance, and sequential ordering is proposed for a dynamic random access file. This method, termed the Gemini Algorithm, {{is a combination}} of the scatter storage (hashing) technique for the data file and a B+tree index organization. During query and data file record modification processes, only the hash component of the Gemini algorithm is utilized. The B+tree component of Gemini algorithm is utilized solely in the periodic processing. During data file <b>record</b> insertion and <b>deletion,</b> both the hash and B+tree components must be maintained. The major premise behind the development of the Gemini algorithm is that additional overhead can be allowed for data file <b>record</b> insertion and <b>deletion</b> as these processes are relatively minor segment of total file interaction. (See more in text...|$|R
40|$|Neuronal {{release of}} {{noradrenaline}} is {{primarily responsible for}} the contraction of prostatic smooth muscle in all species, and this forms {{the basis for the}} use of 1 -adrenoceptor antagonists as pharmacotherapies for benign prostatic hyperplasia. Previ-ous studies in mice have demonstrated that a residual nonad-renergic component to nerve stimulation remains after 1 -ad-renoceptor antagonism. In the guinea pig and rat prostate and the vas deferens of guinea pigs, rats, and mice, ATP is the mediator of this residual contraction. This study investigates the mediator of residual contraction in the mouse prostate. Whole prostates from wild-type, 1 A-adrenoceptor, and P 2 X 1 -purinoceptor knockout mice were mounted in organ baths, and the isometric force that tissues developed in response to elec-trical field stimulation or exogenously applied agonists was <b>recorded.</b> <b>Deletion</b> of the P 2 X 1 purinoceptor did not affect nerve-mediated contraction. Furthermore, the P 2 -purinoceptor antagonist suramin (30 M) failed to attenuate nerve-mediated contractions in wild-type, 1 A-adrenoceptor, or P 2 X 1 -purino-ceptor knockout mice. Atropine (1 M) attenuated contraction in prostates taken from wild-type mice. In the presence of prazosin (0. 3 M) or guanethidine (10 M), or in prostates taken from 1 A-adrenoceptor knockout mice, residual nerve-medi-ated contraction was abolished by atropine (1 M), but not suramin (30 M). Exogenously administered acetylcholine elic-ited reproducible concentration-dependent contractions of the mouse prostate that were atropine-sensitive (1 M), but not prazosin-sensitive (0. 3 M). Acetylcholine, but not ATP, medi-ates the nonadrenergic component of contraction in the mouse prostate. This cholinergic component of prostatic contraction is mediated by activation of muscarinic receptors...|$|R
30|$|Combining large {{databases}} often encounters {{problems such}} as incorrect data entry, different schemas, or inconsistent abbreviation forms. These problems will cause the merged database to have multiple records that represent the same entity but have slightly different attribute values, which creates an inconsistent data. After cleaning and preprocessing, some simple errors in the database are cleared. However, because the object to be processed is a large database, the amount of data to be faced is very large, so it still contains a lot of errors and inconsistent data. The accuracy metric used in this paper is a pure clustering comparison. The definition of pure clustering refers to that all records contained in a cluster represent the same entity. The experimental method is {{used to evaluate the}} data in the large-scale database. The goal of the accuracy in the measurement process is the entire database, not just one data in the database. When the data is recorded using pure clustering, the representation of the records is the same. If the records are in different forms, this clustering form is not a pure clustering, indicating that the clustering method is inaccurate. The use of cluster-based replication <b>record</b> <b>deletion</b> algorithm can largely solve the problem of data inconsistency. This method can reduce the amount of data processing and improve the efficiency of data processing.|$|E
40|$|There {{is always}} a trade off {{between the number of}} cases to be stored in the case library of a Case-Based Expert System and the {{performance}} of retrieval efficiency. The larger the case library, the more the problem space covered, however, it would also downgrade the system performance if the number of cases grows to an unacceptable high level. In this paper, an approach of maintaining the size of a Case-Based Expert System is proposed. The main idea is using the fuzzy class membership value of each record, determined by a trained Neural Network, to guide the <b>record</b> <b>deletion.</b> These fuzzy membership values are used to calculate the case density of each record, and a deletion policy can then be used to determine the percentage of record to be deleted. Using this approach, we could maintain the size of the Case-base without loosing significant amount of information. A testing Case-base consists of 214 records is used as an illustrative example of our approach, the Neural Network software NEURALWORKS PROFESSIONAL II/PLUS 0 is used to develop the Neural Network. It was shown that it could reduce the size of the case library by 28 % if we select those records that have an overall class membership of over 0. 8 and case density over 0. 95. Future work includes integrating adaptation rules for building deletion policy. 1...|$|E
40|$|The initial {{database}} search cost deteriorates due to <b>record</b> additions, <b>deletions</b> and updates performed during the system operation. Previous studies proposed strategies for selecting the optimum maintenance points by making various {{assumptions about the}} rate of database deterioration, the database planning period, the maintenance interval, etc. However, the studies did not express the optimum maintenance points {{as a function of}} the database physical structure. The paper assumes (a) that the rate of additions to the database equals the rate of deletions for each database record type, and (b) that maintenance is performed for the entire database at fixed time intervals and it shows how the maintenance points may be determined for various primitive physical database structures. The results are subsequently ultilized to select the optimum maintenance points for arbitrary database structures...|$|R
40|$|The {{purpose of}} this study is to {{describe}} linguistic differences between written scripts and the oral performances of those scripts in Baghdadi Colloquial Arabic dramatic discourse. The data involves 10 Biblical narratives written in a dramatized format with the intent of being performed. The scriptwriters’ goal was to create texts that were as similar to natural speech as possible. However, in spite of this goal, certain changes occurred throughout the stories when performed by mother tongue Baghdadi Arabic speakers. Although this study <b>records</b> all <b>deletions,</b> additions and substitutions in each of the ten stories, it will highlight three types of changes: the deletion of the connective wa ‘and’, the addition of repeated words and phrases, and diglossically motivated substitutions. These changes represent involvement strategies employed by the actors to accommodate the increased need for textual and interpersonal cohesion in the speaker-hearer dimension when changing the mode from writing to speaking...|$|R
40|$|In {{clinical}} trials, Microsoft Excel {{is widely}} used to document programming requirements. Those documents are frequently updated over time. It is quite time consuming and often challenging for programmers to keep track the changes of the requirement documents. As a result, the requirement changes might not be communicated efficiently among the team and the implementation of those changes might be delayed or overseen. The Excel Manager is a SAS macro with {{the integration of the}} following techniques. The Dynamic Data Exchange (DDE) is used to import Excel worksheets into SAS. The table-driven programming technique is applied to pass metadata from the source to the subsequent programs. The COMPARE and REPORT procedures are used to extract the version change information and generate color-coded RTF reports to reflect the changes of the Excel file for <b>records</b> addition, <b>deletion,</b> and update. The Visual Basic script is developed to send email reminder to users via Simple Mail Transfer Protocol (SMTP) server for new version release...|$|R
40|$|My paper covers {{two aspects}} of MARC: 1) the MARC Distribution Service and 2) the MARC users themselves. The MARC Distribution Service is the {{arrangement}} by which the MARC data are sent off every week {{to each of the}} users. Each weekly issue of MARC is complete on one 300 -foot reel of magnetic computer tape. Thus each user receives one reel of tape containing the MARC data for that week and a printed listing showing the LC card order numbers in the shipment, their status (new, correction or deletion), and the number of new, correction and <b>deletion</b> <b>records,</b> plus a total record count. published or submitted for publicatio...|$|R
40|$|International audienceMotivated by {{applications}} to maintaining confidentiality {{and efficiency of}} encrypted data access in cloud computing, we uncovered an inherent confidentiality weakness in databases outsourced to cloud servers, even when encrypted. To address this weakness, we formulated a new privacy notion for outsourced databases and (variants of) a classical record length optimization problem, whose solutions achieve the new privacy notion. Our algorithmic investigation resulted {{in a number of}} exact and approximate algorithms,for arbitrary input distributions, and in the presence of <b>record</b> additions and <b>deletions.</b> Previous work only analyzed an unconstrained variant of our optimization problem for specific input distributions, with no attention to running time or database updates...|$|R
50|$|Laravel 4, codenamed Illuminate, was {{released}} in May 2013. It was made as a complete rewrite of the Laravel framework, migrating its layout into a set of separate packages distributed through Composer, which serves as an application-level package manager. Such a layout improved the extendibility of Laravel 4, which was paired with its official regular release schedule spanning six months between minor point releases. Other new features in the Laravel 4 release include database seeding for the initial population of databases, support for message queues, built-in support for sending different types of email, and support for delayed <b>deletion</b> of database <b>records</b> called soft <b>deletion.</b>|$|R
5000|$|Scott 4 was {{released}} late in 1969, {{the same year}} as Scott 3. It was credited as being by Noel Scott Engel. It failed to chart and as a result, The album was deleted soon after release. [...] In July 1972, the British music paper, Melody Maker, reported that a cutprice LP issued by Virgin <b>Records</b> was facing <b>deletion</b> because, ironically, it was too popular. Faust's The Faust Tapes, then at number 18 in Melody Makers chart, actually cost more to produce than its selling price (49p) and so Virgin lost supposedly £2,000 on sales of 60,000. [...] It has since been argued that this move was merely a publicity stunt by Virgin's owner, Richard Branson.|$|R
40|$|International audienceDevice {{drivers are}} {{essential}} components of any operating system (OS). They specify the communication protocol {{that allows the}} OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73. 5 %, 85. 3 %, and 88. 2 % of the correct recommendations {{are located in the}} Top- 1, Top- 2, and Top- 5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of <b>record</b> access, <b>deletion</b> of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history...|$|R
40|$|Simple {{computer}} algorithms {{for processing}} source program and text data files {{in order to}} extract change detection, version control, version history, and current status information are described. These algorithms presuppose {{that it is possible}} to attach to each record of the source files a 6 -character code, placed within delimiters that will cause the compiler, or other using program, to ignore this code field. The code contains a 2 -character code for a character-by-character position-sensitive checksum of the record, another for the record number in the file, and a third for the data on which the encoding took place. Once the source file has been thus encoded, it is possible to detect the following transactions on the file since the most recent version coding; (1) addition of new records (having no version code), (2) modification of existing <b>records,</b> (3) <b>deletion</b> of a number of records, (4) movement and/or duplication of existing records, and (5) modification and duplication of records. In addition, it is possible to extract a version history of the number of records created or modified by date. A special file listing program is described which prints the file records without showing the version codes, but places a "change bar" at the right margin whenever a change is detected. The program also provides a list of changed pages and a version history...|$|R
40|$|Abstract. In {{this paper}} we present new {{concurrent}} and re-coverable B-link-tree algorithms. Unlike previous algorithms, ours maintain {{the balance of}} the B-link tree at all times, so that a logarithmic time bound for a search or an update operation is guaranteed under arbitrary sequences of <b>record</b> insertions and <b>deletions.</b> A database transaction can contain any number of operations of the form “fetch the first (or next) matching record”, “insert a record”, or “delete a record”, where database records are identified by their primary keys. Repeatable-read-level isolation for transactions is guaranteed by key-range locking. The algorithms apply the write-ahead logging (WAL) protocol and the steal and no-force buffering policies for in-dex and data pages. Record inserts and deletes on leaf pages of a B-link tree are logged using physiological redo-undo log records. Each structure modification such as a page split o...|$|R
40|$|Abstract — Most {{existing}} anonymization {{work has}} been done on static datasets, which have no update and need only onetime publication. Recent studies consider anonymizing dynamic datasets with external updates: the datasets are updated with <b>record</b> insertions and/or <b>deletions.</b> This paper addresses a new problem: anonymous re-publication of datasets with internal updates, where the attribute values of each record are dynamically updated. This is an important and challenging problem for attribute values of records are updating frequently in practice and existing methods are unable to deal with such a situation. We initiate a formal study of anonymous re-publication of dynamic datasets with internal updates, and show the invalidation of existing methods. We introduce theoretical definition and analysis of dynamic datasets, and present a general privacy disclosure framework that is applicable to all anonymous republication problems. We propose a new counterfeited generalization principle called m-Distinct to effectively anonymize datasets with both external updates and internal updates. We also develop an algorithm to generalize datasets to meet m-Distinct. The experiments conducted on real-world data demonstrate the effectiveness of the proposed solution. I...|$|R
40|$|CLIMATICA, is a {{software}} for management of climatic data. The software has different features like {{the possibility of}} control and validation of data, the conversion of date formats and of the units of the variables, the calculation of statistical indexes. CLIMATICA can perform management operations on climatic dataset (fusion of more datasets by rows or by columns, reduction of the dataset dimensionality with estimation of statistics), on metadata (input and modification), on variables (deletion, generation and re-calculation) and on <b>records</b> (search, sorting, <b>deletion</b> and input). There are also procedures for interpolation and reconstruction of missing data using multiple linear regression, non-linear regression, neural networks and moving statistics (moving average, etc.). CLIMATICA also includes the weather generator Climak for the estimation of climatic parameters from historical data and for the generation of meteorological variables to evaluate risks or to create climatic scenarios. The management of datasets can be made either by dialog windows or by commands, which also allows the automation of procedures. CLIMATICA presents a wide flexibility that permits to be adapted to specific requirements...|$|R
40|$|Social data often contain missing information. The {{problem is}} inevitably severe when {{analysing}} historical data. Conventionally, researchers analyse complete <b>records</b> only. Listwise <b>deletion</b> not only reduces the {{effective sample size}} but also may result in biased estimation, depending on the missingness mechanism. We analyse household types by using population registers from ancient China (618 - 907 AD) by comparing a simple classification, a latent class model of the complete data and a latent class model of the complete and partially missing data assuming four types of ignorable and non-ignorable missingness mechanisms. The findings show that either a frequency classification or a latent class analysis using the complete records only yielded biased estimates and incorrect conclusions {{in the presence of}} partially missing data of a non-ignorable mechanism. Although simply assuming ignorable or non-ignorable missing data produced consistently similarly higher estimates of the proportion of complex households, a specification {{of the relationship between the}} latent variable and the degree of missingness by a row effect uniform association model helped to capture the missingness mechanism better and improved the model fit. Copyright 2004 Royal Statistical Society. ...|$|R
40|$|Abstract: When a file {{is loaded}} into a {{direct access storage device}} using key-to-address transformations, {{the number and}} size of storage blocks can be selected. In this study, a {{selection}} that minimizes the combined cost of storage space and accesses to the storage device is determined for the case where no <b>record</b> additions or <b>deletions</b> occur after loading. The analysis {{is based on the assumption}} that for a given set of keys, a transformation exists that gives a uniform probability distribution over the available addresses. Under this assumption, formulas are derived for the average number of overflow records and for the average number of accesses required to retrieve a record. Given these formulas, the costs are expressed as a function of storage used, number of accesses, cost per unit of storage, and cost per access. Minima are computed for a range of block sizes and operational conditions. The results seem to indicate that current file design practices are abundant with storage space. Finally, the results are condensed in an easy to use approximate formula...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedTo create a record in a database, one uses the INSERT command. However, in the Multi-Backend Database System (MBDS), the insert command only inserts one record at a time. When creating very large databases consisting of thousands or millions of records, {{the use of}} the INSERT command is a time-consuming process. Once a database is created, some of the records of the database may be tagged for deletion. MBDS uses the DELETE command to tag these records. Over some period of time, those <b>records</b> tagged for <b>deletion</b> should be physically removed from the database. Hence, removing tagged records is in essence creating new databases from untagged records. In this thesis, we present a methodology to efficiently create very large databases in gigabytes on parallel computers and to reorganize them when they have been tagged for deletion. Specifically, we design a utility program to by-pass the system's INSERT command, to load the data of the database directly onto disks and create all the necessary base data and meta data of the database. [URL] United States Nav...|$|R
5000|$|The World Wide Web and HTTP {{are based}} on a number of request methods or 'verbs', {{including}} POST and GET as well as PUT, DELETE, and several others. Web browsers normally use only GET and POST, but RESTful online apps make use of many of the others. POST's place in the range of HTTP methods is to send a representation of a new data entity to the server so that it will be stored as a new subordinate of the resource identified by the URI. For example, for the URI , POST requests might be expected to represent new customers, each including their name, address, contact details and so on. Early website designers strayed away from this original concept in two important ways. First, there is no technical reason for a URI to textually describe the web resource subordinate to which POST data will be stored. In fact, unless some effort is made, the last part of a URI will more likely describe the web application's processing page and its technology, such as [...] Secondly, given most web browsers' natural limitation to use only GET or POST, designers felt the need to re-purpose POST to do many other data submission and data management tasks, including the alteration of existing <b>records</b> and their <b>deletion.</b>|$|R
40|$|OBJECTIVE: To {{study the}} {{occurrence}} of congenital aural atresia in patients with a deletion of the long arm of chromosome 18 (18 q- deletion or de Grouchy syndrome). STUDY DESIGN AND PATIENTS: This retrospective study presents {{an overview of the}} otologic findings in 33 Dutch and Belgian patients with a deletion of 18 q. MATERIALS AND METHODS: Detailed information on otorhinolaryngological findings was obtained from otorhinolaryngologists and audiologic centers. Data about medical and developmental history and phenotype were collected from physical examination by a clinical geneticist, by interviewing parents, and by reviewing medical and developmental <b>records.</b> Determination of <b>deletion</b> breakpoints was established by routine karyotyping, prometaphase studies, and/or fluorescence in-situ hybridization (FISH). RESULTS: Twenty out of 33 patients (61 %) with a deletion 18 q had congenital aural atresia (CAA) ranging from narrow external auditory canals to meatal atresia type IIB. Fifteen patients (45 %) had conductive hearing impairment (range: 30 dB- 70 dB). Twelve of these 15 patients (80 %) received hearing aids, which resulted in improved hearing but not in speech development. CAA was found only in patients with a distal deletion of 18 q (including band 18 q 22. 3 or 18 q 23) and not in patients with more proximal 18 q deletions. CONCLUSION: In patients with narrow ear canals or meatal atresia and unexplained mental retardation, chromosomal analysis is indicated. If de Grouchy syndrome is diagnosed in a young patient, auditory examination and surveillance are highly recommended...|$|R

