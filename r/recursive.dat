10000|5|Public
5|$|The <b>recursive</b> {{version is}} based on the {{equality}} of the GCDs of successive remainders and the stopping condition gcd(r'N−1, 0) = r'N−1.|$|E
5|$|This is {{precisely}} the <b>recursive</b> definition of 0X and S'X.|$|E
5|$|This <b>recursive</b> {{formulation}} of addition {{was developed by}} Dedekind as early as 1854, and he would expand upon it in the following decades. He proved the associative and commutative properties, among others, through mathematical induction.|$|E
5|$|Writers {{have placed}} the story within several {{different}} genres, including science fiction, a subgenre {{of science fiction}} called <b>recursive</b> science fiction, and fantasy. Masters of the Occult author Daniel Cohen noted the book contributed to Hubbard's reception among influential science fiction authors of the 1940s. It is regarded as classic science fiction by The Houghton Mifflin Dictionary of Biography in its entry on Hubbard, {{as well as by}} writer James Gunn, and publications including the Daily News of Los Angeles, and Chicago Sun-Times. Writers have placed Typewriter in the Sky within the Golden Age of Science Fiction. Authors Mike Resnick and Robert J. Sawyer classed the story within the science fiction subgenre <b>recursive</b> science fiction, and writer Gary Westfahl wrote that Hubbard may have been influenced by the 1921 Luigi Pirandello play within the <b>recursive</b> fantasy subgenre, Six Characters in Search of an Author. The book is listed in Fantasy: The 100 Best Books, and Rivals of Weird Tales: 30 Great Fantasy and Horror Stories from the Weird Fiction Pulps placed it among the best quality fantasy writing of the 20th century. Writers characterized the overarching theme within the book as dealing with an individual caught between two different worlds.|$|E
5|$|This tiling has four-way {{rotational}} symmetry around {{each of its}} squares. When {{the ratio of the}} side lengths of the two squares is an irrational number such as the golden ratio, its cross-sections form aperiodic sequences with a similar <b>recursive</b> structure to the Fibonacci word. Generalizations of this tiling to three dimensions have also been studied.|$|E
25|$|Any ordinal {{smaller than}} a <b>recursive</b> ordinal is itself <b>recursive,</b> so the set of all <b>recursive</b> ordinals forms a certain (countable) ordinal, the Church-Kleene ordinal (see below).|$|E
25|$|The {{converse}} is not true, as {{not every}} provably total function is primitive <b>recursive.</b> Indeed, one can enumerate all the primitive <b>recursive</b> functions and define a function en such {{that for all}} n, m: en(n,m) = f'n(m), where f'n is the n-th primitive <b>recursive</b> function (for k-ary functions, this will be set to f'n(m,m...m)). Now, g(n) = en(n,n)+1 is provably total but not primitive <b>recursive,</b> by a diagonalization argument: had there been a j such that g = f'j, we would have got g(j) = en(j,j)+1 = f'j (j)+1= g(j)+1, a contradiction. (It {{should be noted that}} the Gödel numbers of all primitive <b>recursive</b> functions can be enumerated by a primitive <b>recursive</b> function, though the primitive <b>recursive</b> functions' values cannot).|$|E
25|$|Exponentiation and primality testing are {{primitive}} <b>recursive.</b> Given primitive <b>recursive</b> functions e, f, g, and h, {{a function}} that returns {{the value of}} g when e≤f {{and the value of}} h otherwise is primitive <b>recursive.</b>|$|E
25|$|The {{function}} that takes m to Ackermann(m,m) is a unary total <b>recursive</b> {{function that}} is not primitive <b>recursive.</b>|$|E
25|$|Most of the {{functions}} normally studied in number theory are primitive <b>recursive.</b> For example, addition and division, the factorial and exponential function, and the function which returns the nth prime are all primitive <b>recursive.</b> So are many approximations to real-valued functions. In fact, {{it is difficult to}} devise a total <b>recursive</b> function that is not primitive <b>recursive,</b> although some are known (see the section on Limitations below).|$|E
25|$|In computability theory, the Ackermann function, {{named after}} Wilhelm Ackermann, {{is one of}} the {{simplest}} and earliest-discovered examples of a total computable function that is not primitive <b>recursive.</b> All primitive <b>recursive</b> functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive <b>recursive.</b>|$|E
25|$|The {{primitive}} <b>recursive</b> {{functions are}} closely related to mathematical finitism, and are used in several contexts in mathematical logic where a particularly constructive system is desired. Primitive <b>recursive</b> arithmetic (PRA), a formal axiom system for the natural numbers and the primitive <b>recursive</b> functions on them, is often used for this purpose.|$|E
25|$|Just as {{algorithms}} on <b>recursive</b> {{data types}} can naturally be given by <b>recursive</b> functions, algorithms on mutually <b>recursive</b> data structures can be naturally given by mutually <b>recursive</b> functions. Common examples include algorithms on trees, and <b>recursive</b> descent parsers. As with direct recursion, tail call optimization is necessary if the recursion depth is large or unbounded, such as using mutual recursion for multitasking. Note that tail call optimization in general (when the function called {{is not the}} same as the original function, as in tail-recursive calls) may be more difficult to implement than the special case of tail-recursive call optimization, and thus efficient implementation of mutual tail recursion may be absent from languages that only optimize tail-recursive calls. In languages such as Pascal that require declaration before use, mutually <b>recursive</b> functions require forward declaration, as a forward reference cannot be avoided when defining them.|$|E
25|$|The {{iteration}} of {{the inner}} loop of the algorithm for v=4 makes a <b>recursive</b> call to the algorithm with R={4}, P={3,5,6}, and X=Ø (although vertex 2 belongs to the set X in the outer call to the algorithm, {{it is not a}} neighbor of v and is excluded from the subset of X passed to the <b>recursive</b> call). This <b>recursive</b> call will end up making three second-level <b>recursive</b> calls to the algorithm that report the three cliques {3,4}, {4,5}, and {4,6}. Then, vertex 4 is added to X and removed from P.|$|E
25|$|The {{operations}} of addition, multiplication and exponentiation are all examples of primitive <b>recursive</b> ordinal functions, and more general primitive <b>recursive</b> ordinal functions {{can be used}} to describe larger ordinals.|$|E
25|$|<b>Recursive</b> {{functions}} are not supported in Brook+ because all function calls are inlined at compile time. Using CAL, functions (<b>recursive</b> or otherwise) {{are supported to}} 32 levels.|$|E
25|$|Fractals can be {{computed}} (up {{to a given}} resolution) by <b>recursive</b> functions. This {{can sometimes}} be done more elegantly via mutually <b>recursive</b> functions; the Sierpiński curve is a good example.|$|E
25|$|The {{study of}} which {{mathematical}} constructions can be effectively performed {{is sometimes called}} <b>recursive</b> mathematics; the Handbook of <b>Recursive</b> Mathematics (Ershov et al. 1998) covers many of the known results in this field.|$|E
25|$|For {{the most}} part the papers contain {{mathematics}} beyond the undergraduate level—in particular the primitive <b>recursive</b> functions and mu <b>recursive</b> functions presented elegantly in Kleene (1952) and less in depth, but useful nonetheless, in Boolos-Burgess-Jeffrey (2002).|$|E
25|$|Divide-and-conquer {{algorithms}} {{are naturally}} implemented as <b>recursive</b> procedures. In that case, the partial sub-problems {{leading to the}} one currently being solved are automatically stored in the procedure call stack. A <b>recursive</b> function is a function that calls itself within its definition.|$|E
25|$|The current {{terminology}} {{was coined}} by Rózsa Péter (1934) after Ackermann had proved in 1928 that the function which today {{is named after}} him was not primitive <b>recursive,</b> an event which prompted the need to rename what until then were simply called <b>recursive</b> functions.|$|E
25|$|As with {{directly}} <b>recursive</b> functions, a wrapper function may be useful, {{with the}} mutually <b>recursive</b> functions defined as nested functions within its scope {{if this is}} supported. This is particularly useful for sharing state across a set of functions without having to pass parameters between them.|$|E
25|$|Overlapping sub-problems {{means that}} the space of sub-problems must be small, that is, any <b>recursive</b> {{algorithm}} solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the <b>recursive</b> formulation for generating the Fibonacci series: F'i = F'i1 + F'i2, with base case F1=F2=1. Then F43 =F42+F41, and F42 =F41+F40. Now F41 is being solved in the <b>recursive</b> sub-trees of both F43 as well as F42. Even though {{the total number of}} sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive <b>recursive</b> solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once.|$|E
25|$|In theory, {{authoritative}} {{name servers}} are {{sufficient for the}} operation of the Internet. However, with only authoritative name servers operating, every DNS query must start with <b>recursive</b> queries at the root zone of the Domain Name System and each user system would have to implement resolver software capable of <b>recursive</b> operation.|$|E
25|$|A <b>recursive</b> {{algorithm}} {{is one that}} invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using <b>recursive</b> implementation. Every <b>recursive</b> version has an equivalent (but possibly more or less complex) iterative version, and vice versa.|$|E
25|$|There {{exists a}} <b>recursive</b> isoperimetric {{function}} f(n) for (∗).|$|E
25|$|Example. We take f(x) as the S(x) defined above. This f is a 1-ary {{primitive}} <b>recursive</b> function. And so is g(x) = S(x). So h(x) {{defined as}} f(g(x)) = S(S(x)) is a primitive <b>recursive</b> 1-ary function too. Informally speaking, h(x) is the function that turns x into x+2.|$|E
25|$|When Post {{defined the}} notion of a simple set as an r.e. set with an {{infinite}} complement not containing any infinite r.e. set, he started to study the structure of the recursively enumerable sets under inclusion. This lattice became a well-studied structure. <b>Recursive</b> sets can be defined in this structure by the basic result that a set is <b>recursive</b> if and only if the set and its complement are both recursively enumerable. Infinite r.e. sets have always infinite <b>recursive</b> subsets; but on the other hand, simple sets exist but do not have a coinfinite <b>recursive</b> superset. Post (1944) introduced already hypersimple and hyperhypersimple sets; later maximal sets were constructed which are r.e. sets such that every r.e. superset is either a finite variant of the given maximal set or is co-finite. Post's original motivation in the study of this lattice was to find a structural notion such that every set which satisfies this property is neither in the Turing degree of the <b>recursive</b> sets nor in the Turing degree of the halting problem. Post did not find such a property and the solution to his problem applied priority methods instead; Harrington and Soare (1991) found eventually such a property.|$|E
25|$|These {{examples}} reduce {{easily to}} a single <b>recursive</b> function by inlining the forest function in the tree function, which is commonly done in practice: directly <b>recursive</b> functions that operate on trees sequentially process {{the value of the}} node and recurse on the children within one function, rather than dividing these into two separate functions.|$|E
25|$|H. Rogers, 1967. Theory of <b>recursive</b> {{functions}} and effective computability. McGraw-Hill.|$|E
25|$|If {{efficiency}} {{is not a}} concern, computing factorials is trivial from an algorithmic point of view: successively multiplying a variable initialized to 1 by the integers up to n (if any) will compute n!, provided the result fits in the variable. In functional languages, the <b>recursive</b> definition is often implemented directly to illustrate <b>recursive</b> functions.|$|E
25|$|Similarly we {{can apply}} other {{differencing}} formulas in a <b>recursive</b> manner.|$|E
25|$|General <b>recursive</b> {{formulas}} {{also exist}} for the volume of an -ball.|$|E
25|$|Iterative {{algorithms}} can {{be implemented}} by means of <b>recursive</b> predicates.|$|E
25|$|<b>Recursive</b> {{definitions}} {{had been}} used more or less formally in mathematics before, but the construction of primitive recursion is traced back to Richard Dedekind's theorem 126 of his Was sind und was sollen die Zahlen? (1888). This work {{was the first to}} give a proof that a certain <b>recursive</b> construction defines a unique function.|$|E
25|$|For every m, the {{function}} h(n) = f(m,n) is primitive <b>recursive.</b>|$|E
