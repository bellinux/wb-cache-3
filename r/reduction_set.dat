21|866|Public
2500|$|The {{ignition}} system {{was similar to}} that of the Model T,with a flywheel-mounted low-tension magneto and trembler coils. The ignition timing was manually advanced or retarded with the spark advance lever mounted near the steering column, which rotated the timer. The cooling was by thermosiphon. (In later decades, a high-tension magneto and a water pump would be added.) The transmission was a three-speed spur gear (the three forward speeds ranged from approximately 2 to 6mph). A worm drive <b>reduction</b> <b>set</b> and a differential made up the rear. The design of the rear was patented for its ease of manufacture and service. Brakes were not provided on early Fordsons, as high-ratio worm sets generally transmitted rotation in one direction only, from the worm element to the gear element, because of the high power loss through friction. To stop the tractor, the driver depressed the clutch.|$|E
5000|$|Immigration {{advocacy}} groups welcomed this effort but expressed skepticism that the timetable for backlog <b>reduction</b> <b>set</b> by USCIS and FBI is realistic. On September 9, 2008, the CIS Ombudsman announced that USCIS and FBI {{have met the}} goal of processing all FBI name checks pending {{for more than two}} years by July 2008.|$|E
5000|$|The {{ignition}} system {{was similar to}} that of the Model T,with a flywheel-mounted low-tension magneto and trembler coils. The ignition timing was manually advanced or retarded with the spark advance lever mounted near the steering column, which rotated the timer. The cooling was by thermosiphon. (In later decades, a high-tension magneto and a water pump would be added.) The transmission was a three-speed spur gear (the three forward speeds ranged from approximately 2 to 6 mph). A worm drive <b>reduction</b> <b>set</b> and a differential made up the rear. The design of the rear was patented for its ease of manufacture and service. Brakes were not provided on early Fordsons, as high-ratio worm sets generally transmitted rotation in one direction only, from the worm element to the gear element, because of the high power loss through friction. To stop the tractor, the driver depressed the clutch.|$|E
30|$|In {{reviewing}} all trials, {{the number}} of trials performed was sufficient with the allowable error for the resulting dust <b>reductions</b> <b>set</b> at ±[*] 5 %.|$|R
40|$|International audienceA new {{algorithm}} of attribute reduction {{based on}} boolean matrix computation is proposed in this paper. The method compresses the valid information stored in table into a binary tree, {{at the same}} time deleting the invalid information and sharing a branch about the same prefix information. Some relative concepts such as local core attributes, local attribute reduction and global core attributes, global attribute reduction are introduced. The conclusions that the global core set is the union of all local core sets and the global attribute <b>reduction</b> <b>sets</b> are the union of respective local attribute <b>reduction</b> <b>sets</b> are proved. The attribute reduction steps of the algorithm are presented. At last, The correctness and effectiveness of the new algorithm are also shown in experiment and in an example...|$|R
50|$|During {{his time}} on Vancouver City Council, Ladner was a {{sustainability}} and liveability advocate {{on issues such as}} transportation, green buildings, energy use, and waste reduction. He championed greenhouse gas <b>reduction,</b> <b>setting</b> a goal of making Vancouver carbon neutral by 2030. An avid cyclist, Ladner introduced the first motion in favour of a bike sharing program.|$|R
40|$|The forward-greedy {{algorithm}} {{based on}} the neighborhood rough set is a simple and effective method for the attribute reduction. However, it is non-incremental reduction method, which limits its application in the dynamic decision system. In this article, an improved incremental attribute reduction algorithm is proposed by introducing concept of the relative positive region, which can update the original <b>reduction</b> <b>set</b> quickly and handle both the incremental attributes and the incremental samples. Finally, the correctness and effectiveness of the proposed algorithm are demonstrated by examples and experiments on 5 standard data sets from UCI...|$|E
40|$|Knowledge {{reduction}} {{is the core}} content in rough set theory. Start searching knowledge <b>reduction</b> <b>set</b> from the core {{is a kind of}} very effective algorithm. In this paper, firstly, a new approach to calculate the core of consistent covering decision system is proposed based on covering probability distribution function is a single-point distribution. Its amount of calculation is less than algorithm in paper [12]. Secondly, algorithms to calculate the smallest relative reduction and all relative reductions of consistent covering decision system are designed by the core while algorithms in paper [12] can only calculate the core and the smallest relative reduction. Finally, an illustrative example is provided to verify the efficiency of the algorithms...|$|E
40|$|Abstract—SAR image {{compression}} {{is very important}} in reducing the costs of data storage and transmission in relatively slow channels. In this paper, we propose a compression scheme driven by texture analysis, homogeneity mapping and speckle noise reduction within the wavelet framework. The image compressibility and interpretability are improved by incorporating speckle reduction into the compression scheme. We begin with the classical set partitioning in hierarchical trees (SPIHT) wavelet compression scheme, and modify it to control the amount of speckle reduction, applying different encoding schemes to homogeneous and nonhomogeneous areas of the scene. The results compare favorably with the conventional SPIHT wavelet and the JPEG compression methods. Index Terms—Image texture, synthetic aperture radar (SAR), SAR image data compression, speckle <b>reduction,</b> <b>set</b> partitioning in hierarchical trees (SPIHT), wavelets. I...|$|E
40|$|In this paper, the {{parameterization}} value <b>reduction</b> of soft <b>sets</b> and its algorithm {{in decision}} making are studied and described. It is based on parameterization <b>reduction</b> of soft <b>sets.</b> The {{purpose of this study}} is to investigate the inherited disadvantages of parameterization <b>reduction</b> of soft <b>sets</b> and its algorithm. The algorithms presented in this study attempt to reduce the value of least parameters from soft set. Through the analysis, two techniques have been described. Through this study, it is found that parameterization <b>reduction</b> of soft <b>sets</b> and its algorithm has yielded a different and inconsistency in suboptimal result. ...|$|R
25|$|Under Joint Implementation (JI) a {{developed}} country with relatively {{high costs of}} domestic greenhouse <b>reduction</b> would <b>set</b> up a project in another {{developed country}}.|$|R
40|$|AbstractIn this paper, {{we focus}} our {{discussion}} on the parameterization <b>reduction</b> of soft <b>sets</b> and its applications. First we {{point out that the}} results of soft <b>set</b> <b>reductions</b> offered in [1] are incorrect. We also observe that the algorithms used to first compute the reduct-soft-set and then to compute the choice value to select the optimal objects for the decision problems in [1] are not reasonable and we illustrate this with an example. Finally, we propose a reasonable definition of parameterization <b>reduction</b> of soft <b>sets</b> and compare it with the concept of attributes <b>reduction</b> in rough <b>sets</b> theory. By using this new definition of parameterization reduction, we improve the application of a soft set in a decision making problem found in [1]...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis will demonstrate Peru's inability to physically operate and politically control {{large sections of}} the country, {{is the result of}} eroded internal state sovereignty. The decline of Peru's internal sovereignty is a function of economic, ethnic, and social clevages which have remained virtually unchanged since the Spanish Conquest of the Inca in 1533. As a result, Peru evolved into a polarized society, which is ethnically and culturall divided, with a substantially wide margin existing between state authority and rural social autonomy. This marginalization of state sovereignty has facilitated the emergence and growth of the Shining Path insurgency, which has coupled with the expanding cocaine trade. Together these two processes have accelerated the erosion of sovereignty in Peru. Given this reality, the policy goals of supply <b>reduction</b> <b>set</b> forth by the 1992 National Drug Control Strategy remain unattainable in Peru, and have little prospect for success. Lieutenant Commander, United States Nav...|$|E
40|$|Abstract—Traditional {{rough set}} theory is only {{suitable}} for dealing with discrete variables and need data preprocessing. Neighborhood rough sets overcome these shortcomings {{with the ability to}} directly process numeric data. This paper modifies the attribute reduction method based on neighborhood rough sets, in which the attribute importance is combined with information entropy to select the appropriate attributes. When multiple attributes have the same importance degree, compare the information entropy of these attributes. Put the attribute having the minimal entropy into the <b>reduction</b> <b>set,</b> so that the reduced attribute set is better. Then we introduce this attribute reduction method to improve spectral clustering and propose NRSR-SC algorithm. It can highlight the differences between samples while maintaining the characteristics of data points to make the final clustering results closer to the real data classes. Experiments show that, NRSR-SC algorithm is superior to traditional spectral clustering algorithm and FCM algorithm. Its clustering accuracy is higher, and has strong robustness to the noise in high-dimensional data. Index Terms—neighborhood rough sets, information entropy, attribute reduction, spectral clustering I...|$|E
40|$|Abstract—In some {{scenarios}} {{there are}} ways of conveying information with many fewer, even exponentially fewer, qubits than possible classically. Moreover, some of these methods have a very simple structure—they involve only few message exchanges between the communicating parties. It is therefore natural to ask whether every classical protocol may be transformed to a “simpler ” quantum protocol—one that has similar efficiency, but uses fewer message exchanges. We show that for any constant k, {{there is a problem}} such that its k + 1 message classical communication complexity is exponentially smaller than its k message quantum communication complexity. This, in particular, proves a round hierarchy theorem for quantum communication complexity, and implies, via a simple reduction, an (N 1 =k) lower bound for k message quantum protocols for Set Disjointness for constant k. Enroute, we prove information-theoretic lemmas, and define a related measure of correlation, the informational distance, that we believe may be of significance in other contexts as well. Index Terms—Average encoding theorem, entanglement-assisted communication, Hellinger distance, informational distance, pointer jumping, quantum communication complexity, quantum information theory, round complexity, round <b>reduction,</b> <b>set</b> disjointness...|$|E
40|$|Abstract — We {{investigate}} {{the use of}} inexact solves in a Krylov-based model <b>reduction</b> <b>setting</b> and present the resulting perturbation effects on the underlying model reduction problem. We show that for a good selection of interpolation points, Krylov-based model reduction is robust {{with respect to the}} perturbations due to inexact solves. On the other hand, when the interpolation points are poorly selected, these perturbations are magnified through the model reduction process. Finally, we incorporate inexact solves for the Krylov-based optimal H 2 approximation. The result is an effective optimal model reduction algorithm applicable in realistic large-scale settings. I...|$|R
40|$|In {{this paper}} we first {{point out that}} the result of <b>reductions</b> of soft <b>sets</b> in [P. K. Maji et al., 2002] is {{incorrect}} and the definition of reduct-soft-set in [P. K. Maji et al., 2002] is not reasonable which can be illustrated by an example. Then we propose a reasonable definition of parameterization <b>reduction</b> of soft <b>sets</b> by employing the idea of knowledge <b>reduction</b> in rough <b>set</b> theory. At the end of this paper we improve the application of soft sets in a decision making problem as mentioned in [P. K. Maji et al., 2002]. Department of ComputingRefereed conference pape...|$|R
40|$|Recent studies {{make use}} of cortex {{proportion}} as a proxy measurement {{for the impact of}} artefact transport on assemblage formation. Cortex in these studies is measured on an ordinal scale and analysed in relation to mechanical measurements of artefact size. Here we report on the use of a 3 D laser scanner to obtain precise measurements from experimental lithic <b>reduction</b> <b>sets.</b> High-resolution measurements of cortex area are compared to measurements obtained through ordinal approximations of cortex proportion and mechanical approximations of artefact size. While the ordinal and mechanical measurements result in considerable error for individual artefacts, estimates improve significantly as sample size increases...|$|R
40|$|For data mining, {{reducing}} the unnecessary redundant attributes which {{was known as}} attribute reduction (AR), in particular, reducts with minimal cardinality, is an important preprocessing step. In the paper, by a coding method of combination subset of attributes set, a novel search strategy for minimal attribute reduction based on rough set theory (RST) and fish swarm algorithm (FSA) is proposed. The method identifies the core attributes by discernibility matrix firstly and all the subsets of noncore attribute sets with the same cardinality were encoded into integers as the individuals of FSA. Then, the evolutionary direction of the individual {{is limited to a}} certain extent by the coding method. The fitness function of an individual is defined based on the attribute dependency of RST, and FSA was used to find the optimal set of reducts. In each loop, if the maximum attribute dependency and the attribute dependency of condition attribute set are equal, then the algorithm terminates, otherwise adding a single attribute to the next loop. Some well-known datasets from UCI were selected to verify this method. The experimental results show that the proposed method searches the minimal attribute <b>reduction</b> <b>set</b> effectively and it has the excellent global search ability...|$|E
40|$|Several {{algorithms}} {{exist to}} address the issues concerning parameter reduction of soft sets. The most recent concept of Normal Parameter Reduction (NPR) is introduced, which overcomes the problem of suboptimal choice and added parameter set of soft sets. However, the algorithm involves {{a great amount of}} computation. In this thesis, a New Efficient Normal Parameter Reduction algorithm (NENPR) of soft sets is proposed based on the new theorems, which have been proved and presented. The proposed technique can be carried out without parameter important degree and decision partition. As a result, it can involve relatively less computation, compared with the algorithm of NPR. The experimental results are analyzed and comparisons are done with three real-life datasets and ten synthetic generated datasets. The computational complexity is described {{in terms of the number}} of entry access, the number of parameter importance degree access and oriented-parameter access, and the number of candidate parameter <b>reduction</b> <b>set.</b> From these experimental results, some conclusions can be drawn that NENPR improves the number of entry access, the number of parameter importance degree access and oriented-parameter access, the number of candidate parameter <b>reduction</b> <b>set</b> and the executing time of NPR averagely up to 95. 21 %, 52. 45 %, 53. 58 % and 60. 02 % through three real-life datasets and ten synthetic generated datasets, respectively. Sum up, NENPR provides the better solutions for capturing the normal parameter reduction compared with NPR. An interval-valued fuzzy soft set is a special case of a soft set by combining the interval-valued fuzzy set and soft set. However, up to the present, the previous work has not involved parameter reduction of the interval-valued fuzzy soft sets. In this thesis, four new parameter reductions of the interval-valued fuzzy soft sets are proposed: Optimal Choice Considered Parameter Reduction (OCCPR), Invariable Rank of Decision Choice Considered Parameter Reduction (IRDCCPR), Standard Parameter Reduction (SPR) and Approximate Standard Parameter Reduction (ASPR). The related heuristic algorithms are given. In order to show the high efficiency of the proposed four algorithms, comparisons and analysis for decision making between OCCPR, IRDCCPR, ASPR, SPR and directly Interval-Valued Fuzzy Soft Sets based Fuzzy Decision Making algorithm (IVFSS-FDM) with three real-life datasets and ten synthetic generated datasets are made. Average percent of improvement of four proposed algorithms compared with IVFSS-FDM on the executing time concerning all of datasets are 80. 28 %, 56. 37 %, 47. 44 %, 10 %, respectively. From these experimental results, conclusions can be drawn that our four proposed algorithms have much higher efficiency compared with directly IVFSS-FDM for decision making and four approaches have the respective merits and demerits. Therefore these proposed methods can be applied into the different situations...|$|E
40|$|Nepal {{being the}} mountainous country has {{difficult}} terrain. Land {{is the main}} source of livelihood. The limited arable land with sparse and diverse utilization is not meeting even the subsistence agriculture practice {{of the majority of the}} people. The population growth resulting unlimited human activities, unplanned land uses, misuse of land-based resources and uneconomic labour investment has increasing poverty and causing serious problems in nation building. Unless the proper balance between land and people is maintained poverty reduction is not possible. Proper Land Management based on local spatial knowledge will enhance the economic growth of the poor and minimize the unequilibrium of overall prosperity of municipalities and villages in the districts. Indeed, the government has realized devolution approach for decentralized planning and has enacted and enforced Local Governance Act 1999; which reflects management of lands to some extent. In reality, it is not functioning well due to lack of commitment and various constraints. Decentralized Land Management model, though ambitious, will hopefully promote economic and social welfare in the locality and uplift the quality of life of the Nepalese people. The significant of spatial planning in the framework of decentralized approach is envisioned so that the people will feel sustainable economic growth and balanced eco-system. The real life outcomes will certainly support the national goal of poverty <b>reduction</b> <b>set</b> forth by the current tenth five years plan (2003 - 2008 AD) ...|$|E
5000|$|... #Subtitle level 3: The <b>reduction</b> into a <b>set</b> of {{equations}} with binary variables ...|$|R
30|$|When {{looking more}} closely into the learned {{surfaces}} of all subjects, {{more than half}} of the subjects who preferred learned over default settings experienced a significantly sloping surface over the relevant acoustic range. The black dots on the surface of Figure 11 denote the sounds that have been used in the stimulus of the second lab test. From the position of these dots, we observe that during the second lab test, subject 12 experienced a noise reduction that changed considerably with the type of sound. We conjecture that the preference with respect to the default noise <b>reduction</b> <b>setting</b> is partly caused by the personalized environmental steering of the gain depth parameter.|$|R
5000|$|Enantiopure, planar chiral {{chromium}} arene complexes can be synthesized using several strategies. Diastereoselective complexation of a chiral, non-racemic arene to chromium is {{one such}} strategy. In the example in equation (5), enantioselective Corey-Itsuno <b>reduction</b> <b>sets</b> up a diastereoselective ligand substitution reaction. After complexation, the alcohol is reduced with triethylsilane.(5)A second strategy involves enantioselective ortho-lithiation and in situ quenching with an electrophile. Isolation of the lithium arene and subsequent treatment with TMSCl led to lower enantioselectivities.(6)Site-selective conjugate addition to chiral aryl hydrazone complexes {{can also be used}} for the enantioselective formation of planar chiral chromium arenes. Hydride abstraction neutralizes the addition product, and treatment with acid cleaves the hydrazone.(7) ...|$|R
40|$|In Europe over 70 % of {{citizens}} live in urban areas, and projections forecast an increase to nearly 80 % by 2030. Densely populated cities increase strains on energy, transportation, resources, housing and public spaces needs, thus calling for new solutions. Such ‘smart” solutions {{have to be}} efficient and sustainable as well as capable of generating economic prosperity and social wellbeing. It is estimated that road transport contributes to about one fifth of the total carbon dioxide emissions in Europe, growing by nearly 23 % between 1990 and 2010. This calls for major changes for future mobility, as outlined by EC White Paper 2011, resulting in a significant de-carbonisation of transport to reach the 60 % greenhouse gas emissions <b>reduction</b> <b>set</b> for 2050. In this context, big data {{has the potential of}} supporting and driving the process of re-definition of the current transport policies towards more effective and wiser implementation of the actions that need to be undertaken to meet the long-term goals of the Union. This paper presents the Transport Technology and Mobility Assessment (TEMA) platform, developed to harness the potential of big data in transport policy support. The presented applications provide the reader with an overview of the potential of the platform, aiming at stimulating discussions and collaborations between scientists, data owners and policy makers for the development of smart, sustainable and inclusive transport policies. JRC. C. 4 -Sustainable Transpor...|$|E
40|$|As {{part of the}} MASCOT/WOTRO {{multinational}} team {{conducting the}} maternal health literature mapping, four Latin American researchers were particularly interested in analysing information specific to their region. The mapping started with 45, 959 papers uploaded from MEDLINE, CINAHL, Embase, LILACAS, PopLINE, PsycINFO and Web of Knowledge. From these, 4175 full texts were reviewed and 2295 papers were subsequently included. Latin America experienced an average maternal mortality decline of 40 % between 1990 and 2013. Nevertheless, the region's performance was below the global average and short of the 75 % <b>reduction</b> <b>set</b> in Millennium Development Goal 5 for 2015. The main outcomes show that research on maternal health in the countries where the most impoverished populations {{of the world are}} living is not always aligned with their compelling needs. From another perspective, the review made it possible to recognize that research funding as well as the amount of scientific literature produced concentrate on issues that are not necessarily among the main causes of maternal deaths. Even though research on maternal health in Latin America has grown from an average of 92. 5 publications per year in 2000 - 2003 to 236. 7 between 2008 and 2012, it's not satisfactorily keeping pace with other regions. In conclusion, it is critical to effectively orient research funding and production to respond to the health needs of the population. At the same time, {{there is a need for}} innovative mechanisms to strengthen the production and uptake of scientific evidence that can properly inform public health decision making...|$|E
40|$|Churn is {{perceived}} as the behaviour of a customer to leave or to terminate a service. This behaviour causes the loss of profit to companies because acquiring new customer incurred high investment for advertisements and promotions compared to retaining existing ones. Thus, {{it is necessary to}} consider an efficient classification model to reduce the rate of churn. In the traditional approach of classification modelling, it do not produce straightforward result interpretation. Therefore, identifying the best classification model to reduce the rate of churn is indeed a challenging task. The main objective of this thesis is to propose a new classification model based on the Rough Set Theory to classify customer churn. This research utilized the Knowledge Discovery in Database (KDD) process involving data pre-processing, data discretization, attribute reduction, rule generation, classification process, as well as data analysis, using the Rough Set toolkit. The Rough Set theory elements consist of indiscernibility relation, lower and upper approximations, as well as <b>reduction</b> <b>set.</b> Those elements are applied to classify customer chum from uncertain and imprecise dataset. The results of the proposed model are compared with a few established existing approaches. The results of the study show that the proposed classification model outperformed the existing models and contributes to significant accuracy improvement. The model is tested using dataset form local telecommunication company which achieves 90. 32 %. In conclusion, the results proved that the classification model based on Rough Set Theory had been capable to classify customer chum compared to the existing model...|$|E
50|$|The White House {{clarified}} that Trump {{will end}} the implementation of carbon <b>reduction</b> targets <b>set</b> by former President Barack Obama and that the withdrawal will be done {{in accordance with the}} years-long exit process spelled out in the accord.|$|R
5000|$|Emissions will be {{monitored}} {{for two years}} to help <b>set</b> <b>reduction</b> targets in consultation with industry.|$|R
50|$|On December 4, 2008, Matthews {{introduced}} Ontario's Poverty Reduction Strategy {{as chair}} of the Cabinet Committee on Poverty Reduction. The long-term <b>reduction</b> plan <b>set</b> a target {{to reduce the number of}} children living in poverty by 25 per cent over 5 years.|$|R
40|$|Different {{protozoa}} and metazoa populations {{develop in}} the activated sludge wastewater treatment processes and are highly dependent on the operating conditions. In the current work the protozoa and metazoa groups and species most frequent in wastewater treatment plants were studied, mainly the flagellate, sarcodine, and ciliate protozoa {{as well as the}} rotifer, gastrotrichia, and oligotrichia metazoa. The work is centered on the survey of the wastewater treatment plant conditions by protozoa and metazoa population using image analysis, discriminant analysis (DA), and neural networks (NNs) techniques, and its main objective was set on the evaluation of the importance of raw data pre-processing techniques in the final results. The main pre-processing techniques herein studied were the raw parameters <b>reduction</b> <b>set</b> by a joint cross-correlation and decision trees (DTs) procedure and two data normalization techniques: logarithmic normalization and standard deviation normalization. Regarding the parameters reduction methodology, the use of a joint DTs and correlation analysis (CA) procedure resulted in 28 and 30 % reductions in terms of the initial parameters set for the stalked and non-stalked microorganisms, respectively. Consequently, the use of the reduced parameters set {{has proven to be a}} suitable starting point for both the DA and NNs methodologies, although for the DA an initial logarithmic normalization step is advisable. For the NNs analysis a standard deviation normalization procedure could be considered for the non-stalked microorganisms regarding the operating parameters assessment. National Council of Scientific and Technological Development of Brazil (CNPq) BI-EURAM III ALFA co-operation project (European Commission) (UFPE, Recife, Brasil) Fundação para a Ciência e a Tecnologia (FCT...|$|E
40|$|The term {{carbon dioxide}} capture and storage (CCS) refers {{to a range of}} {{technologies}} that can reduce CO 2 emissions from fossil fuels enabling the continued use of this fuel type without compromising the security of electricity supply. The technologies applicable to CCS differ in many key aspects; the stage of the electricity generation process at which the CO 2 is captured, the CO 2 capture process, efficiency, availability and matureness of the technology. The integration of these technologies into power plants results in a reduction in power generation efficiency, which remains one of the major issues for the commercial implementation of CCS. Among the possible technologies, the focus of this thesis is on post-combustion capture as it is a known technology, is readily available and it can be retrofitted to existing power plants. This thesis is concerned with the development of new carbon capture processes that require less energy for CO 2 separation and are, at the same time, more environmentally friendly. Prior to the development of any new process, {{the current state of the}} art needs to be analysed and updated in order to set realistic targets for the new technology and benchmark the potential of the newly developed processes. Therefore, part of the work of this thesis is a thorough benchmarking exercise in which updated baselines for the performance of conventional post-combustion capture processes are given. The new process concepts developed in this thesis are based on the combination of enhanced absorption and enhanced desorption, two effects encountered in capture processes that are based on precipitating amino acid solvents. For this purpose, the conceptual design methodology has been followed with a specific target of energy <b>reduction</b> <b>set</b> to (at least) 30 % of a conventional MEA process...|$|E
40|$|The {{goal of this}} {{research}} is the proper tailoring of the civil tiltrotor's composite wing-box structure leading to a minimum-weight wing design. With focus on the structural design, the wing's aerodynamic shape and the rotor-pylon system are held fixed. The initial design requirement on drag <b>reduction</b> <b>set</b> the airfoil maximum thickness-to-chord ratio to 18 percent. The airfoil section is the scaled down version of the 23 percent-thick airfoil used in V- 22 's wing. With the project goal in mind, the research activities began with an investigation of the structural dynamic and aeroelastic characteristics of the tiltrotor configuration, and the identification of proper procedures to analyze and account for these characteristics in the wing design. This investigation led to a collection of more than thirty technical papers on the subject, some of which have been referenced here. The review of literature on the tiltrotor revealed the complexity of the system in terms of wing-rotor-pylon interactions. The aeroelastic instability or whirl flutter stemming from wing-rotor-pylon interactions is found to be the most critical mode of instability demanding careful consideration in the preliminary wing design. The placement of wing fundamental natural frequencies in bending and torsion relative to each other and relative to the rotor 1 /rev frequencies is found to have a strong influence on the whirl flutter. The frequency placement guide based on a Bell Helicopter Textron study is used in the formulation of frequency constraints. The analysis and design studies are based on two different finite-element computer codes: (1) MSC/NASATRAN and (2) WIDOWAC. These programs are used in parallel with the motivation to eventually, upon necessary modifications and validation, use the simpler WIDOWAC code in the structural tailoring of the tiltrotor wing. Several test cases were studied for the preliminary comparison of the two codes. The results obtained so far indicate a good overall agreement between the two codes...|$|E
40|$|This paper {{explores the}} {{prospects}} for a global carbon market as the centrepiece of any serious attempt to reach the ambitious goal for greenhouse gas (GHG) <b>reductions</b> <b>set</b> by climate scientists. My aim is to clarify the extent to which we know what policy might best support global decarbonisation. I begin by discussing what we might mean by a global carbon market and its theoretical properties. I proceed to discuss the EU Emissions Trading System experience and the recent experience with the Australian carbon tax. Next, I assess the evolving carbon market initiatives in the US and in China. In the conclusion, I apply some principles of 'good' energy policy making to {{the prospects for}} a successful global carbon market...|$|R
30|$|<b>Reduction</b> of the <b>set</b> of KCs, {{by leaving}} out the liver KC or both {{the liver and}} {{vertebrae}} KC.|$|R
5000|$|Dominating set is {{the problem}} of {{selecting}} a set of vertices (the dominating set) in a graph such that all other vertices are adjacent to at least one vertex in the dominating set. The Dominating set problem was shown to be NP complete through a <b>reduction</b> from <b>Set</b> cover.|$|R
