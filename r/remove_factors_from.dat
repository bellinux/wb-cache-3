0|10000|Public
40|$|This short note first {{develops}} a general formalism for globally <b>removing</b> a <b>factor</b> <b>from</b> an obstruction theory. This formalism is then applied {{to give a}} construction of a reduced obstruction theory on the moduli of maps from a curve to a surface satisfying a technical condition. This condition was recently identified in work of Kool and Thomas. Comment: 10 page...|$|R
5000|$|Jeff {{believes}} that “Business communication has changed, and email, still the principal method of communication in business, hasn’t kept up" [...] thus Zula was planned to <b>remove</b> the Email <b>factor</b> <b>from</b> intensive day-to-day work.|$|R
40|$|Flat space {{cosmology}} spacetimes are exact time-dependent solutions of 3 -dimensional gravity theories, such as Einstein gravity or topologically massive gravity. We {{exhibit a}} novel kind of phase transition between these cosmological spacetimes and the Minkowski vacuum. At sufficiently high temperature (rotating) hot flat space tunnels into a universe described by flat space cosmology. Comment: 5 pp, v 2 : <b>removed</b> Tolman <b>factor</b> <b>from</b> the discussion and corrected footnote [40]; essentially matches published version in PR...|$|R
40|$|We prove certain {{endpoint}} restriction {{estimates for}} the paraboloid over finite fields in three and higher dimensions. Working in the bilinear setting, {{we are able to}} pass from estimates for characteristic functions to estimates for general functions while avoiding the extra logarithmic power of the field size which is introduced by the dyadic pigeonhole approach. This allows us to <b>remove</b> logarithmic <b>factors</b> <b>from</b> the estimates obtained by Mockenhaupt and Tao in three dimensions and those obtained by Iosevich and Koh in higher dimensions. Comment: 15 page...|$|R
40|$|Abstract. The {{purpose of}} this study is to explore how student {{competition}} using the tit-for-tat strategy could be remedied with a minimum design change in order to support student to collaborate constructively in a computer supported-collaborative learning system called SWoRD (Scaffolded Writing and Review-ing in the Discipline) [5], a reciprocal peer reviewing of writing system. We identified a factor for the tit-for-tat that causes learners to compete each other, and <b>removed</b> the <b>factor</b> <b>from</b> the interface. The results show when with the in-terface improvement the tit-for-tat strategy was restrained in the SWoRD sys-tem, which helped the learners constructively respond to peer comments...|$|R
2500|$|If, on {{the other}} hand, we wish to answer an interventional question: [...] "What is the {{probability}} that it would rain, given that we wet the grass?" [...] {{the answer would be}} governed by the post-intervention joint distribution function [...] obtained by <b>removing</b> the <b>factor</b> [...] <b>from</b> the pre-intervention distribution. As expected, the probability of rain is unaffected by the action: [...]|$|R
40|$|Antirabies {{vaccines}} {{produced from}} infected brains of adult mammals {{have always had}} the potentiality of causing post-vaccinal paralysis or allergic encephalitis in man. Attempts in recent years either to <b>remove</b> the paralytic <b>factor</b> <b>from</b> brain-tissue vaccines or to use as the virus source infected tissue other than nervous tissue (e. g., chick embryos) have usually resulted in a substantial reduction of the specific antirabies potency...|$|R
50|$|PI3K-dependent Akt {{activation}} can {{be regulated}} through the tumor suppressor PTEN, which works essentially as {{the opposite of}} PI3K mentioned above. PTEN acts as a phosphatase to dephosphorylate PIP3 back to PIP2. This <b>removes</b> the membrane-localization <b>factor</b> <b>from</b> the Akt signaling pathway. Without this localization, the rate of Akt activation decreases significantly, as {{do all of the}} downstream pathways that depend on Akt for activation.|$|R
40|$|Using Weil descent, we give bounds for {{the number}} of {{rational}} points on two families of curves over finite fields with a large abelian group of automorphisms: Artin-Schreier curves of the form y^q-y=f(x) with f∈[x], on which the additive group acts, and Kummer curves of the form y^q- 1 /e=f(x), which have an action of the multiplicative group ^. In both cases we can <b>remove</b> a √(q) <b>factor</b> <b>from</b> the Weil bound when q is sufficiently large...|$|R
40|$|Response data (loss cost, claim {{frequency}} or claim severity) {{are often}} pre-adjusted with known factors and directly analyzed with generalized linear models (GLM). This paper {{shows that the}} exposure weights should also be adjusted if the Tweedie distribution with log link is used in such direct analysis. An advantage of the direct analysis over GLM offsetting is that {{the structure of the}} original dataset may be simplified significantly after <b>removing</b> the known <b>factors.</b> Direct analysis is a convenient tool for directly modeling loss ratio and for <b>removing</b> known territory <b>factors</b> <b>from</b> the dataset. Implementation in EMBLEM and SAS is discussed, and a computationally efficient SAS macro is provided for Tweedie models...|$|R
40|$|Abstract. In {{this paper}} we propose a new log-chromaticity 2 -D colour space, an {{extension}} of previous approaches, which succeeds in <b>removing</b> confounding <b>factors</b> <b>from</b> dermoscopic images: (i) the effects of the particular camera characteristics for the camera system used in forming RGB images; (ii) the colour of the light used in the dermoscope; (iii) shading induced by imaging non-flat skin surfaces; (iv) and light intensity, removing the effect of light-intensity falloff toward the edges of the dermoscopic image. In the context of a blind source separation of the underlying colour, we arrive at intrinsic melanin and hemoglobin images, whose properties are then used in supervised learning to achieve excellent malignant vs. benign skin lesion classification. In addition, we propose using the geometric-mean of colour for skin lesion segmentation based on simple greylevel thresholding, with results outperforming the state of the art. ...|$|R
40|$|In this paper, we {{put forward}} a new {{pre-processing}} scheme for automatic analysis of dermoscopic images. Our contributions are two-fold. First, we present a procedure, an extension of previous approaches, which succeeds in <b>removing</b> confounding <b>factors</b> <b>from</b> dermoscopic images: these include shading induced by imaging non-flat skin surfaces {{and the effect of}} light-intensity falloff toward the edges of the dermoscopic image. This procedure is shown to facilitate the detection and removal of artifacts such as hairs as well. Second, we present a novel simple yet effective greyscale conversion approach that is based on physics and biology of human skin. Our proposed greyscale image provides high separability between a pigmented lesion and normal skin surrounding it. Finally, using our pre-processing scheme, we perform segmentation based on simple grey-level thresholding, with results outperforming the state of the art. 1...|$|R
40|$|Screening {{designs are}} used {{in the early stages of}} {{industrial}} and computer exper-iments to find the most important input factors affecting a system’s output. They provide an economical way to <b>remove</b> unimportant <b>factors</b> <b>from</b> further, potentially costly, experimentation. However, when an experiment has a large number of con-trol factors and limited number of available runs, it is infeasible to run a traditional screening design. In these situations, experimenters can use supersaturated designs. A supersaturated design is a fractional factorial design that can screen a set of k factors in n runs, where k> n − 1. Unfortunately, they do not always provide definitive results. Improper and incomplete analysis of supersaturated designs can cause an experimenter to misclassify active factors and waste resources in subsequent experiments. In light of these concerns, this research investigates how to construc...|$|R
50|$|Practically speaking, ECM is {{considered}} a special purpose factoring algorithm as it is most suitable for finding small factors. , {{it is still the}} best algorithm for divisors not greatly exceeding 20 to 25 digits (64 to 83 bits or so), as its running time is dominated {{by the size of the}} smallest factor p rather than by the size of the number n to be factored. Frequently, ECM is used to <b>remove</b> small <b>factors</b> <b>from</b> a very large integer with many factors; if the remaining integer is still composite, then it has only large factors and is factored using general purpose techniques. The largest factor found using ECM so far has 83 digits and was discovered on 7 September 2013 by R. Propper. Increasing the number of curves tested improves the chances of finding a factor, but they are not linear with the increase in the number of digits.|$|R
40|$|In vitro protein-synthesizing {{systems from}} Escherichia coli can be {{categorized}} as either chloramphenicol-sensitive or chloramphenicol-insensitive. The chloramphenicol-sensitive systems {{used in this study}} required the presence of <b>factors</b> <b>removed</b> <b>from</b> ribosomes with 1. 0 m NH 4 Cl when chromatographically purified ribosomes were used for amino acid incorporation. These ribosomal wash factors inhibited but did not eliminate amino acid incorporation in chloramphenicol-insensitive systems. For both systems, addition of increasing amounts of the ribosomal wash factors increased the sensitivity to chloramphenicol inhibition...|$|R
30|$|Electrostatic attraction, {{covalent}} binding, and hydrophobic absorption are spontaneous {{processes to}} assemble and disassemble the molecules of gold nanoparticles (GNP). This dynamic {{change can be}} performed {{in the presence of}} ions, such as NaCl or charged molecules. Current research encompasses the GNP in mediating non-biofouling and investigating the molecular attachment and detachment. Experiments were performed with different sizes of GNP and polymers. As a proof of concept, poly(ethylene glycol)-b-poly(acrylic acid), called PEG-PAAc, attachment and binding events between factor IX and <b>factor</b> IX-bp <b>from</b> snake venom were demonstrated, and the variations with these molecular attachment on GNP were shown. Optimal concentration of NaCl for GNP aggregation was 250  mM, and the optimal size of GNP used was 30  nm. The polymer PEG-PAAc (1  mg/ml) has a strong affinity to the GNP as indicated by the dispersed GNP. The concentration of 5800  nM of factor IX was proved to be optimal for dispersion of GNP, and at least 100  nM of factor IX-bp was needed to <b>remove</b> <b>factor</b> IX <b>from</b> the surface of GNP. This study delineates the usage of unmodified GNP for molecular analysis and downstream applications.|$|R
40|$|Relations {{between the}} Friedmann observables of the {{expanding}} Universe and the Dirac observables in the generalized Hamiltonian approach are {{established for the}} Friedmann cosmological model of the Universe with the field excitations imitating radiation. A full separation of the physical sector from the gauge one is fulfilled by the method of the gaugeless reduction in which the gravitational part of the energy constraint is considered as a new momentum. We show that this reduction <b>removes</b> an infinite <b>factor</b> <b>from</b> the Hartle – Hawking functional integral, provides the normalizability of the Wheeler – DeWitt wave function, clarifies its relation to the observational cosmology, and picks out a conformal frame of Narlikar...|$|R
40|$|We study Witten’s open {{string field}} {{theory in the}} {{presence}} of a constant B field. We construct the string field theory in the operator formalism and find that, compared to the ordinary theory with no B field, the vertices in the resulting theory has an additional factor. The factor makes the zero modes of strings noncommutative. This is in agreement with the results in the first-quantized formulation. We also discuss background independence of the purely cubic action derived from the above string field theory and then find a redefinition of string fields to <b>remove</b> the additional <b>factor</b> <b>from</b> the vertex. Furthermore, we briefly discuss the supersymmetric extension of our string field theory. Decembe...|$|R
40|$|We study Witten's open {{string field}} {{theory in the}} {{presence}} of a constant B field. We construct the string field theory in the operator formalism and find that, compared to the ordinary theory with no B field, the vertices in the resulting theory has an additional factor. The factor makes the zero modes of strings noncommutative. This is in agreement with the results in the first-quantized formulation. We also discuss background independence of the purely cubic action derived from the above string field theory and then find a redefinition of string fields to <b>remove</b> the additional <b>factor</b> <b>from</b> the vertex. Furthermore, we briefly discuss the supersymmetric extension of our string field theory. Comment: 22 pages, lanlmac, references adde...|$|R
40|$|We {{investigated}} {{the effects of}} fractionated sera obtained from cancer patients by double filtration plasmapheresis (DFPP) plus antitumor agents on murine pulmonary metastasis. Fractions of the sera, in combination with natural human tumor necrosis factors (nTNF) and cyclophosphamide (Cy), were systemically administered to Lewis lung carcinoma-bearing mice. When the second filtrate (a plasma fraction containing substances composed of smaller molecular weight compounds) combined with low-dose nTNF (1, 000 U/kg) and Cy (250 micrograms/kg) was administered to the mice, the degree of metastasis was significantly suppressed compared with the control group (p less than 0. 01). In contrast, the discarded fluid (a plasma fraction containing larger molecular weight compounds) combined with the same doses of nTNF and Cy caused little inhibition of metastasis. Also, the discarded fluid significantly suppressed natural killer activity compared with normal sera (p less than 0. 01). The results suggested that DFPP combined with nTNF and Cy is an efficient procedure to <b>remove</b> immunosuppressive <b>factors</b> <b>from</b> the sera of cancer-bearing hosts, to enhance the host antitumor immunity, and to suppress tumor proliferation. </p...|$|R
40|$|Previous {{reports have}} shown that rough microsomes treated with high salt (Warren and Dobberstein, 1978, Nature, 273 : 569 - 571) or proteases (Walter et al., 1979, Proc. Nati. Acad. Sci. U. S. A., 76 : 1, 795) are unable to vectorially {{translocate}} nascent proteins. Readdition of the high salt or protease extracts restored activity to such inactive rough microsomes. A detailed study was carried out to determine how this factor interacts with the rough microsomal membrane. Proteolytic cleavage {{was found to be}} necessary but not sufficient to <b>remove</b> this <b>factor</b> <b>from</b> the membrane. A subsequent treatment with high salt had to be carried out. Endogenous (pancreatic) protease could effect the required cleavage, but low levels of trypsin, clostripain, or elastase were far more efficient. Several proteases were not effective. The minimum level of salt (after proteolysis) required to solubilize the active factor was - 200 mM KCI. Salt extracts prepared by treatment with one of the effective proteases were capable of restoring activity to inactive microsomes produced by treatment with one of the others...|$|R
40|$|Channel {{interference}} {{factor for}} the identification results is prevalent among the existing speaker recognition algorithm. In order to improve {{the accuracy of the}} system, in this paper, feature warping is used to compensate the channel factor of Mel-Frequency Cepstral Coefficient (MFCC) features. Then, factor analysis technique is applied to deal with the channel factors of the speaker's Gaussian Mixture Model (GMM). In the endpoint detection phase of speech of this recognition system, the GMM for speech modeling is built to accurately determine the beginning and end points of the speech segment, and then the features after feature warping are used to establish speaker GMM. Using factor analysis technique to fit the differences between the speaker characteristics space and the channel space, the algorithm <b>removes</b> channel <b>factor</b> <b>from</b> GMM, and then extracts GMM super-vectors as input of the Support Vector Machine (SVM) to obtain recognition results. Experimental results show that the combination of channel compensation technique and SVM can obtain better recognition rate, and ensure the robustness of the system. Department of Electrical Engineerin...|$|R
40|$|The {{biometric}} system {{is based on}} human’s behavioral and physical characteristics. Among all of these, iris has unique structure, higher accuracy and it can remain stable over a person’s life. Iris recognition is the method by which system recognize a person by their unique identical feature found in the eye. Iris recognition technology includes four subsections as, capturing of the iris image, segmentation, extraction of the needed features and matching. This paper is a detail description of eyelids, eyelashes detection technique and Hough transform method applied on iris image. Generally, eyelids and eyelashes are noise factors in the iris image. To increase {{the accuracy of the}} system we must have to <b>remove</b> these <b>factors</b> <b>from</b> the iris image. Eyelashes detection algorithm can be used for detecting eyelids and eyelashes. To improve the overall performance of the iris recognition system, we can use canny edge detection algorithm [12]. Then, Hough Transform can be applied on these images to identify the circles of specific radii and lines on iris image [14]...|$|R
40|$|The {{definitive}} {{version is}} available at www. blackwell-synergy. comBackground: For day case laparoscopic cholecystectomy programmes, studies suggest that overnight admission may be predicted by the following factors: gall bladder wall thickness, patient age over 55 years and previous sphincterotomy. This study investigated the effect of relaxing selection for a day surgery laparoscopic cholecystectomy programme, by <b>removing</b> these <b>factors</b> <b>from</b> the exclusion criteria. Methods: Between September 2002 and April 2003, patients for elective laparoscopic cholecystectomy were considered for day surgery subject to standard criteria. For the initial part of the programme, patients were additionally excluded according to the risk factors mentioned above. Results: Thirty-three patients underwent intended day case procedures. The first 16 were selected according to the more rigorous criteria. The latter 17 were significantly older, with a significantly higher incidence of gall bladder wall thickening. There were seven admissions, three in the former {{part of the study}} and four in the latter. Conclusion: The exclusion criteria described are not necessary for a good same-day discharge rateMatthew S. Metcalfe, Emma J. Mullin and Guy J. Madder...|$|R
40|$|A {{review of}} some recent {{well-documented}} cases of intolerance in the cosmology field illustrates {{a common problem}} in science. Many relate to the Big Bang theory, such as the case of Geoffrey and Margaret Burbidge and Halton Arp. None of the accounts involved Intelligent Design advocates or creationists. This selection <b>removes</b> this compounding <b>factor</b> <b>from</b> the evaluation, but the cases have direct relevance to both Intelligent Design and creationism because both groups face the same resistance. It was concluded that it is critical for science to advance that, new ideas must be evaluated on the evidence and not because they challenge established science. This problem has persisted during {{the entire history of}} science, the most well known example being Galileo...|$|R
40|$|We {{consider}} N × N Hermitian random matrices {{with independent}} identically distributed entries (Wigner matrices). The matrices are normalized {{so that the}} average spacing between consecutive eigenvalues is of order 1 /N. Under suitable assumptions {{on the distribution of}} the single matrix element, we first prove that, away from the spectral edges, the empirical density of eigenvalues concentrates around the Wigner semicircle law on energy scales η ≫ N − 1. This result establishes the semicircle law on the optimal scale and it <b>removes</b> a logarithmic <b>factor</b> <b>from</b> our previous result [6]. We then show a Wegner estimate, i. e. that the averaged density of states is bounded. Finally, we prove that the eigenvalues of a Wigner matrix repel each other, in agreement with the universality conjecture...|$|R
5000|$|Blind {{signature}} schemes can {{be implemented}} using a number of common public key signing schemes, for instance RSA and DSA. To perform such a signature, the message is first [...] "blinded", typically by combining it in some way with a random [...] "blinding factor". The blinded message is passed to a signer, who then signs it using a standard signing algorithm. The resulting message, along with the blinding factor, can be later verified against the signer's public key. In some blind signature schemes, such as RSA, it is even possible to <b>remove</b> the blinding <b>factor</b> <b>from</b> the signature before it is verified. In these schemes, the final output (message/signature) of the blind signature scheme is identical {{to that of the}} normal signing protocol.|$|R
40|$|It {{has been}} shown {{previously}} that ribosomal <b>factors</b> <b>removed</b> <b>from</b> chick muscle ribosomes by a high salt wash are required for both the binding of muscle mRNA to ribosomes and mRNA-directed synthesis of myosin on reticulocyte ribosomes. These factors can be separated into several components on DEAE-cellulose. A factor eluting between 0. 18 and 0. 25 M KCl is responsible for binding mRNA to the ribosome. This binding factor shows specificity in the recognition of mRNA, as has been demonstrated by the preferential binding of muscle and globin mRNA to ribosomes which contain their respective binding factors. Similarly, globin mRNA is preferentially translated when reticulocyte factors are present on the ribosome...|$|R
30|$|Approximately 1, 300 survey {{invitation}}s {{were sent}} in rounds between December 2012 and August 2013. The e-mail invitation described {{the goal of}} the survey: conducting a census of people using SCALE-UP style instruction. Thus, people who filled out the survey associated themselves with SCALE-UP use, or at least acknowledged that the reform influenced their teaching. Three reminders were sent to non-respondents, and individuals could elect to be removed from the list if they thought the survey was irrelevant. In the end, 812 surveys were started with 84 % of these respondents completing the entire survey. For this study, responses were only retained from respondents at American higher education institutions, leaving a sample of 659, a better than 50 % response rate. SCALE-UP originated in the US so we chose to focus on universities with a similar cultural context. In other countries, different social and cultural norms could complicate the way SCALE-UP was implemented and we wanted to <b>remove</b> cultural <b>factors</b> <b>from</b> adding an additional variable to the study. Furthermore, since 84 % of respondents came from the US, we decided to focus on getting a detailed understanding of domestic implementations.|$|R
40|$|We {{present a}} novel method for {{classifying}} emotions from static facial images. Our approach leverages {{on the recent}} success of Convolutional Neural Networks (CNN) on face recognition problems. Unlike the settings often assumed there, far less labeled data is typically available for train-ing emotion classification systems. Our method is therefore designed {{with the goal of}} simplifying the problem domain by <b>removing</b> confounding <b>factors</b> <b>from</b> the input images, with an emphasis on image illumination variations. This, in an ef-fort {{to reduce the amount of}} data required to effectively train deep CNN models. To this end, we propose novel transfor-mations of image intensities to 3 D spaces, designed to be invariant to monotonic photometric transformations. These are applied to CASIA Webface images which are then used to train an ensemble of multiple architecture CNNs on mul-tiple representations. Each model is then fine-tuned with limited emotion labeled training data to obtain final clas-sification models. Our method was tested on the Emotion Recognition in the Wild Challenge (EmotiW 2015), Static Facial Expression Recognition sub-challenge (SFEW) and shown to provide a substantial, 15. 36 % improvement over baseline results (40 % gain in performance) ...|$|R
40|$|Determinate {{sentencing}} and {{sentencing guidelines}} {{have resulted in}} prison population increases in several states. Initial results of the Minnesota Sen-tencing Guidelines showed greater uniformity and less sentence disparity, with no significant prison population increases. Public and political pressure led the 1989 legislature to increase sentences for drug offenses, sexual assault and homicide. This study examines some of the effects which those changes have had on corrections. Initial results show dramatic increases in drug of-fender admissions; total admissions exceed the projected growth; and cor-rections officers express concern about crowding, prison gangs and security problems. Determinate sentencing and sentencing guidelines have evolved as the most viable options for sentencing reform in the American judicial system. The myriad examples of sentencing disparity evident with indeterminate sen-tencing raise serious questions of justice and fairness. The very foundation on which it is based-the rehabilitative ideal-is no longer viewed as a realistic and justifiable goal of sentencing. Sentencing guidelines have been proposed and implemented in several states and the Federal judicial system. The sentencing reforms have varied objectives, but they all generally aim at attaining more predictability and uniformity, and at <b>removing</b> extralegal <b>factors</b> <b>from</b> the sentencing decision (for reviews of determinate sentencing...|$|R
40|$|The aim of {{root canal}} {{treatment}} is to <b>remove</b> virulence <b>factors</b> <b>from</b> this system. Cleaning and shaping {{of the root}} canal are at the outmost importance in endodontic treatment. Canal irrigation {{during the process of}} cleaning and shaping can lead to the elimination of microorganism, which are not removable through physical methods. Moreover, during the preparation of root canal, manually and by rotary instruments, the smear layer is created that must be eliminated by irrigants. In the present review article, irrigants were investigated in terms of chemical and biological features and their eff ective and safe ways of usage, along with some information that have been proposed on recent developments of root canal solutions. Furthermore, this topic has been studied regarding its eff ect on microorganisms and smear layer. In the present article, a review has been conducted through libraries, PubMed, ISI Web of science, Scopus websites, and Google using keywords such as endodontic treatment, intracanal irrigant, anti-bacterial, chlorhexidine, smear layer, and sodium hypochlorite. Diff erent materials have been introduced as root canal irrigants. Although sodium hypochlorite is the most common material used in the endodontic treatment against roo...|$|R
40|$|Mating-type a {{cells of}} the yeast Saccharomyces cerevisiae that had been {{specifically}} arrested in the G 1 phase of the cell cycle by alpha factor, an oligopeptide pheromone made by alpha cells, recovered and resumed cell division {{after a period of}} inhibition which was dependent on the concentration of alpha factor used. These treated a cells were more resistant to alpha factor than untreated a cells, but lost their resistance upon further cell division. However, cells arrested for 6 h were no more resistant to alpha factor than cells arrested for only 2. 5 h. Mating-type a strains could inactivate or <b>remove</b> alpha <b>factor</b> <b>from</b> the culture fluid, but two a sterile (nonmating) mutants and an a/alpha diploid strain could not. These results suggest that a cells have a mechanism, which may involve uptake or inactivation of alpha <b>factor,</b> for recovering <b>from</b> alpha <b>factor</b> arrest. However, the results do not distinguish between a recovery mechanism which is constitutive and one which is induced by alpha factor. The loss of alpha factor activity during recovery appeared to be primarily cell contact mediated, although an extracellular, diffusible inhibitor of alpha factor that is labile or that functions stoichiometrically could not be ruled out...|$|R
40|$|An {{effective}} way to control for cross-section correlation when conducting a panel unit root test is to <b>remove</b> the common <b>factors</b> <b>from</b> the data. However, there remain many ways to use the defactored residuals to construct a test. In this paper, we use the panel analysis of nonstationarity in idiosyncratic and common components (PANIC) residuals to form two new tests. One estimates the pooled autoregressive coefficient, and one simply uses a sample moment. We establish their large-sample properties using a joint limit theory. We find that when the pooled autoregressive root is estimated using data detrended by least squares, the tests have no power. This result holds regardless of how the data are defactored. All PANIC-based pooled tests have nontrivial power {{because of the way}} the linear trend is removed. ...|$|R
40|$|High index {{differential}} algebraic equations (DAEs) are ordinary {{differential equations}} (ODEs) with constraints and arise frequently from many mathematical models of physical phenomenons and engineering fields. In this paper, we generalize {{the idea of}} differential elimination with Dixon resultant to polynomially nonlinear DAEs. We propose a new algorithm for index reduction of DAEs and establish the notion of differential algebraic elimination, which can provide the differential algebraic resultant of the enlarged system of original equations. To make use of structure of DAEs, variable pencil technique is given to determine the termination of differentiation. Moreover, we also provide a heuristics method for <b>removing</b> the extraneous <b>factors</b> <b>from</b> differential algebraic resultant. The experimentation shows that the proposed algorithm outperforms existing ones for many examples taken from the literature. Comment: 19 pages, 1 figur...|$|R
40|$|We {{consider}} N× N Hermitian random matrices {{with independent}} identically distributed entries (Wigner matrices). The matrices are normalized {{so that the}} average spacing between consecutive eigenvalues is of order 1 /N. Under suitable assumptions {{on the distribution of}} the single matrix element, we first prove that, away from the spectral edges, the empirical density of eigenvalues concentrates around the Wigner semicircle law on energy scales η≫ N^- 1. This result establishes the semicircle law on the optimal scale and it <b>removes</b> a logarithmic <b>factor</b> <b>from</b> our previous result ESY 2. We then show a Wegner estimate, i. e. that the averaged density of states is bounded. Finally, we prove that the eigenvalues of a Wigner matrix repel each other, in agreement with the universality conjecture. Comment: 35 pages, LateX fil...|$|R
