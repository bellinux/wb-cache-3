4|72|Public
50|$|Periodic {{maintenance}} actions {{control risk}} of operational failure. This relies on invasive procedures that renders a system inoperable {{for a brief}} period while users <b>run</b> <b>manual</b> diagnostic or preventative procedures. The following are a few examples.|$|E
50|$|The {{project began}} with the glass negatives from the {{departments}} of the German Archaeological Institute at Athens, Istanbul and Cairo (Emagines1) in 2006. Negatives from Rome, Madrid, the headquarters in Berlin and the Eurasia-Department of the DAI were added during the second project phase in 2008 (Emagines2). These negatives mostly contain historical images of sculpture, topography and architecture, much of which no longer exists in its former condition. Furthermore, the negatives are severely threatened by physical decomposition and destruction due to environmental influences, natural catastrophes like earthquakes and (in the long <b>run)</b> <b>manual</b> use.|$|E
40|$|This paper {{presents}} Yasper, a {{tool for}} modeling, analyzing and simulating workflow systems, based on Petri nets. Yasper puts Petri net modeling {{in the hands of}} business analysts and software architecture designers. They can specify systems in familiar terms (XOR choice, workflow, cases, roles, processing time and cost), and can directly <b>run</b> <b>manual</b> and automatic simulations on the resulting models to analyze correctness and performance. Yasper was designed to cooperate with other tools, such as Petri net analyzers, and off-the-shelf software for data (color) handling and forms handling...|$|E
50|$|Gen 4 also <b>runs</b> a <b>manual</b> {{throttle}} body.|$|R
40|$|Abstract. This paper {{summarizes}} {{the work done}} at the State University of New York at Buffalo (UB) in the GeoCLEF 2006 track. The approach presented uses pure IR techniques (indexing of single word terms as well as word bigrams, and automatic retrieval feedback) to try to improve retrieval performance of queries with geographical references. The main purpose of this work is to identify the strengths and shortcomings of this approach so that {{it serves as a}} basis for future development of a geographical reference extraction system. We submitted four runs to the monolingual English task, two automatic <b>runs</b> and two <b>manual</b> <b>runs,</b> using the title and description fields of the topics. Our official results are above the median system (auto= 0. 2344 MAP, manual= 0. 2445 MAP). We also present an unofficial run that uses title description and narrative which shows a 10 % improvement in results with respect to our baseline <b>runs.</b> Our <b>manual</b> <b>runs</b> were prepared by creating a Boolean query based on the topic description and manually adding terms from geographical resources available on the web. Although the average performance of the <b>manual</b> <b>run</b> is comparable to the automatic runs, a query by query analysis shows significant differences among individual queries. In general, we got significant improvements (more that 10 % average precision) in 8 of the 25 queries. However, we also noticed that 5 queries in the <b>manual</b> <b>runs</b> perform significantly below the automatic runs. ...|$|R
50|$|Before {{arriving}} at San Jose State, Smith <b>ran</b> at <b>Manual</b> Arts High School in Los Angeles, finishing {{third in the}} 220 yard dash at the CIF California State Meet in 1966.|$|R
40|$|Robot of WS_NOTO efesien se didesain {{possible}} and {{as simple as}} possible, where manual robot of didesain can be controlled by human being and of kontruksi made manual robot framework of almunium so that robot weight don’t exceed weight which have been determined. Motor of DC weared to move robot of needed reduce at motor, to be motor have bigger torsi. Control system at manual robot use mosfet driver and relay driver to control motor rotation, motor driver controlled by buttom push controlled by operator. Where operator <b>run</b> <b>manual</b> robot by using controling buttom push move robot, so that robot earn block(balok builder similar of horse poultice) and put down at tower (pillar situating of block builder) ...|$|E
60|$|The {{distinction}} here drawn is {{so great}} and obvious that for proof of the German girl's case we need better evidence than Coleridge's rumour of a rumour, cited, as it is, by Hamilton, Maudsley, Carpenter, Du Prel, and the common <b>run</b> of <b>manuals.</b>|$|R
50|$|The {{highest class}} at the Emo Speedway is the WISSOTA Modifieds. This class is similar in {{appearance}} as a WISSOTA Midwest Modified {{with the exception of}} some cosmetic differences. However, the WISSOTA Modified class has a minimum weight of 2450 lbs (1,111 kg) and can <b>run</b> a <b>Manual</b> Transmission, a Spec Engine (Turning out up to 600-650 hp), and an alcohol fuel option.|$|R
40|$|IR {{experiments}} typically {{use test}} collections for evaluation. Such test collections are formed by judging {{a pool of}} documents retrieved {{by a combination of}} automatic and <b>manual</b> <b>runs</b> for each topic. The proportion of relevant documents found for each topic depends on the diversity across each of the runs submitted and the depth to which runs are assessed (pool depth). <b>Manual</b> <b>runs</b> are commonly believed to reduce bias in test collections when evaluating new IR systems. In this work, we explore alternative approaches to improving test collection reliability. Using fully automated approaches, we are able to recognise a large portion of relevant documents that would normally only be found through <b>manual</b> <b>runs.</b> Our approach combines simple fusion methods with machine learning. The approach demonstrates the potential to find many more relevant documents than are found using traditional pooling approaches. Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment coverage is maintained across the entire document collection...|$|R
40|$|In this paper, we {{describe}} our experiments for TRECVID 2004 for the Search task. In the interactive search task, we developed {{two versions of}} a video search/browse system based on the Físchlár Digital Video System: one with text- and image-based searching (System A); the other with only image (System B). These two systems produced eight interactive runs. In addition we submitted ten fully automatic supplemental <b>runs</b> and two <b>manual</b> <b>runs.</b> A. 1, Submitted Runs: • DCUTREC 13 a_{ 1, 3, 5, 7 } for System A, four interactive runs based on text and image evidence. • DCUTREC 13 b_{ 2, 4, 6, 8 } for System B, also four interactive runs based on image evidence alone. • DCUTV 2004 _ 9, a <b>manual</b> <b>run</b> based on filtering faces from an underlying text search engine for certain queries. • DCUTV 2004 _ 10, a <b>manual</b> <b>run</b> based on manually generated queries processed automatically. • DCU_AUTOLM{ 1, 2, 3, 4, 5, 6, 7 }, seven fully automatic runs based on language models operating over ASR text transcripts and visual features. • DCUauto_{ 01, 02, 03 }, three fully automatic runs based on exploring the benefits of multiple sources of text evidence and automatic query expansion. A. 2, In the interactive experiment {{it was confirmed that}} text and image based retrieval outperforms an image-only system. In th...|$|R
50|$|Rocrail is a {{proprietary}} software {{to control a}} model train layout from one or more computers. Users can run trains directly from a computer, or have some <b>run</b> automatically with <b>manual</b> control for any others.|$|R
30|$|Table  8 {{presents}} the evaluation results {{based on the}} automatic and <b>manual</b> <b>runs</b> submitted by WSU-IR [2] in the TREC 2015 CDS Task A. wsuirdaa and wsuirdma in Table  8 are the submitted automatic and <b>manual</b> <b>runs,</b> respectively, and are used as our strong baselines. wsuirdaa+SEM_d_Para-D^k_PRF and wsuirdaa+SEM_d_Sum-D^k_PRF correspond to applying Paragraph Embeddings and Term Summation, respectively, the same to wsuirdma+SEM_d_Para-D^k_PRF and wsuirdma+SEM_d_Sum-D^k_PRF. According to the results, {{we can see that}} there are still statistically significant improvements over the strong baselines wsuirdaa and wsuirdma in most cases when applying both Paragraph Embeddings and Term Summation, indicating the effectiveness of our proposed semantic relevance score.|$|R
50|$|In modern practice, {{cryptographic}} {{engineering is}} deployed in crypto systems. Like most engineering design, these are wholly human creations. Most crypto systems are computer software, either embedded in firmware or running as ordinary executable files under an operating system. In some system designs, the cryptography <b>runs</b> under <b>manual</b> direction, in others, it is run automatically, {{often in the}} background. Like other software design, and unlike most other engineering, there are few external constraints.|$|R
5000|$|Hate People Like Us (remix of PLU by Coil, Negativland, Death in June, Barbed, Christoph Heemann, Bruce Gilbert, Stock, Hausen & Walkman, Rehberg & Bauer, Mika Vainio, Boyd Rice, Dummy <b>Run,</b> Farmers <b>Manual</b> and Sons of Silence. 2CD {{features}} additionally Cyclobe, Req1, V/VM, Sniper, Mr Rotorvator, Felix Kubin, Xper. Xr, Venoz TKS, Katy Brown and Dr P Li Khan. There was {{an additional}} remix by Andy Votel/Badly Drawn Boy, excluded for legal reasons. *Caciocavallo/CAD1 (CD) and Staalplaat/STCD126 (2CD) 1999 2CD ...|$|R
40|$|Information {{retrieval}} test collections traditionally use {{a combination}} of automatic and <b>manual</b> <b>runs</b> to create a pool of documents to be judged. The quality of the final judgments produced for a collection {{is a product of the}} variety across each of the runs submitted and the pool depth. In this work, we explore fully automated approaches to generating a pool. By combining a simple voting approach with machine learning from documents retrieved by automatic runs, we are able to identify a large portion of relevant documents that would normally only be found through <b>manual</b> <b>runs.</b> Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment coverage is maintained across complete document collections...|$|R
50|$|Despite {{being an}} {{interpreted}} language, a well-designed MAPPER run can achieve reasonable performance, because the commands {{of the language}} are relatively powerful and invoke pre-compiled functions. For example, Search, Sort, Match, and similar bulk-processing functions are single commands in both the MAPPER <b>run</b> language and <b>manual</b> functions.|$|R
50|$|Rocrail is an {{open source}} project that can control a model train layout from one or more computers. Users can run trains {{directly}} from their computer or have the it run the trains automatically. Some of the trains can be set to <b>run</b> automatically allowing <b>manual</b> control for others.|$|R
40|$|We {{describe}} {{our fourth}} participation, that includes two high-level feature extraction <b>runs,</b> and one <b>manual</b> search <b>run,</b> to the TRECVID video retrieval evaluation. All of these runs {{have used a}} system trained on the common development collection. Only visual information, consisting of color, texture and edge-based low-level features, was used. ...|$|R
40|$|AbstractLearning Factories are {{implemented}} for multiple applications offering opportunities for practical research and training. The majority of learning factories {{is designed for}} very special purposes as training of lean methods and manufacturing logistics. The bime developed and installed an adaptable Learning Factory called BERTHA that is capable to <b>run</b> various <b>manual</b> assembly scenarios typical for special-purpose machine assembly for example. The paper introduces the Learning Factory BERTHA, its current application scenarios and concepts to support participants and trainers to evaluate learning outcomes. Finally, the paper describes one sequence of a scenario {{and the application of}} evaluation concepts...|$|R
40|$|Abstract. Software {{performance}} prediction {{methods are}} typically vali-dated by taking an appropriate software system, performing both perfor-mance predictions and performance measurements for that system, and comparing the results. The validation includes manual actions, {{which makes it}} feasible only for {{a small number of}} systems. To significantly increase the number of systems on which software perfor-mance prediction methods can be validated, and thus improve the valida-tion, we propose an approach where the systems are generated together with their models and the validation <b>runs</b> without <b>manual</b> intervention. The approach is described in detail and initial results demonstrating both its benefits and its issues are presented. Key words: performance modeling, performance validation, MDD...|$|R
25|$|Alexander Cockburn was an {{opponent}} of conspiracism and particularly {{in regard to the}} 9/11 conspiracy theories interpreted its rise {{as a sign of the}} decline of the American Left. At CounterPunch Cockburn and St. Clair <b>ran</b> articles by <b>Manual</b> Garcia, a physicist, on the events of September 11, 2001, challenging the conspiracy theories that have been circling since the attacks.|$|R
40|$|In TREC- 8, we {{participated in}} the {{automatic}} and manual tracks for category A {{as well as the}} small web track. This year, we first ensured that our baseline matched the effectiveness achieved by other teams using the same ranking techniques. We then introduced some experimental improvements. We investigated differences among the top TREC participants from past years and corrected some minor variations in our system. For the automatic runs, we included a baseline run (iit 99 au 1) and an experimental run (iit 99 au 2) that used a concept-based expansion technique. The automatic runs used the required title plus description (`short') query versions. The experimental run used relevance feedback with a high-precision first pass to select terms and then a high-recall final pass. For <b>manual</b> <b>runs,</b> we used predefined concept lists with terms from the concept lists combined in different ways. The <b>manual</b> <b>run</b> focused on using phrases and proper nouns in the query. In the small web-track we submitted one [...] ...|$|R
40|$|In {{this paper}} we {{describe}} work done {{as part of}} the TREC- 5 benchmarking exercise by a team from Dublin City University. In TREC- 5 we had three activities as follows:. Our ad hoc retrieval submissions employ Query Space Reduction techniques which attempt to minimise the amount of data processed by an IR search engine during the retrieval process. We submitted four runs for evaluation, two automatic and two manual with one automatic <b>run</b> and one <b>manual</b> <b>run</b> employing our Query Space Reduction techniques. The paper reports our findings in terms of retrieval effectiveness and also in terms of the savings we make in execution time.. Our submission to the multi-lingual track (Spanish) in TREC- 5 involves evaluating the performance of a new stemming algorithm for Spanish developed by Martin Porter. We submitted three runs for evaluation, two automatic, and one manual, involving a manual expansion of terms from the top- 10 retrieved documents.. Character shape coding (CSC) is a technique for [...] ...|$|R
40|$|Two English {{automatic}} ad-hoc runs {{have been}} submitted: pircsAAS uses short and pircsAAL employs long topics. Our new avtf*ildf term weighting {{was used for}} short queries. 2 -stage retrieval were performed. Both automatic runs are {{much better than the}} overall automatic average. Two <b>manual</b> <b>runs</b> are based on short topics: pircsAM 1 employs double weighting for user-selected query terms and pircsAM 2 additionally extends these queries with new terms chosen manually. They perform about average compared with the the overall <b>manual</b> <b>runs.</b> Our two Chinese automatic ad-hoc runs are: pircsCw, using short-word segmentation for Chinese texts and pircsCwc, which additionally includes single characters. Both runs are much better than average, but pircsCwc has a slight edge over pircsCw. In routing a genetic algorithm is used to select suitable subsets of relevant documents for training queries. Out of an initial random population of 15, the best subset (based on average precision) was employed to train t [...] ...|$|R
50|$|Other {{manufacturers}} and companies partitioned and formatted disks manually, then used file copy utilities or archiving utilities, such as tar or zip to copy files. It {{is not sufficient}} simply to copy all files from one disk to another, because there are special boot files or boot tracks which must be specifically placed for an operating system to <b>run,</b> so additional <b>manual</b> steps were required.|$|R
40|$|The {{goal of the}} TREC 2012 Medical Records Track was {{to search}} medical record {{documents}} to identify patients as possible candidates for clinical studies based on diagnosis, age, and other attributes. For TREC 2012, the Oregon Health & Science University (OHSU) group experimented with both manual and automated techniques. We used a derivative of Lucene to build an interactive retrieval system that can process queries {{in one of two}} ways. Users can manually specify Boolean queries whose terms may include words as well as ICD- 9 codes. Alternatively, the system features an automated query parser that transforms free-text queries into structured Boolean queries. The query parser is built on top of MetaMap and the UMLS Metathesaurus. We submitted both automatic runs (which relied solely on the automated query parser) as well as <b>manual</b> <b>runs</b> consisting of queries built by an expert clinician. Overall, our automated query parser performed below the mean of other groups, although there were individual topics for which it performed very well. This irregular performance was in part due to our parser’s tendency to over-specify queries, leading to reduced recall. There were, however, several topics for which our parser performed very well, suggesting that our fundamental approach has merit. In contrast, our <b>manual</b> <b>runs</b> performed very well, scoring second-best among official <b>manual</b> <b>runs.</b> With further modification of the manual queries, we were able to achieve even better performance. Query of electronic health records for the use case of identifying patients as candidates for clinical studies still requires manual query development, at least until better automated methods can be developed that outperform them...|$|R
40|$|A {{computer}} program (INHYD) {{was developed for}} intraply hybrid composite design. A users manual for INHYD is presented. In INHYD embodies several composite micromechanics theories, intraply hybrid composite theories, and an integrated hygrothermomechanical theory. The INHYD can be run in both interactive and batch modes. It has considerable flexibility and capability, which the user can exercise through several options. These options are demonstrated through appropriate INHYD <b>runs</b> in the <b>manual...</b>|$|R
40|$|We {{describe}} {{our third}} participation, that includes one high-level feature extraction <b>run,</b> and two <b>manual</b> and one interactive search runs, to the TRECVID video retrieval evaluation. All of these runs {{have used a}} system trained on the common development collection. Only visual and textual information were used where visual information consisted of color, texture and edge-based low-level features and textual information consisted of the speech transcript provided in the collection...|$|R
40|$|Facilities for {{hyperbaric}} {{oxygen therapy}} that are suitable for animal experimental research are scarce. In this paper, the authors introduce a hyperbaric oxygen chamber that was developed specifically for animal experimental purposes. The hyperbaric oxygen chamber was designed to meet a number of criteria regarding safety and ease of use. The hyperbaric oxygen chamber conforms to 97 / 23 /EC (Pressure Equipment Directive), Conformity Assessment Module G Product Group 1. It provides easy access, and can be <b>run</b> in <b>manual</b> mode, semi-automatic mode and full-automatic mode. Sensors for pressure level, oxygen level, temperature, humidity and carbon dioxide level allow full control. This state-of-the-art hyperbaric oxygen chamber for animal experimental purposes permits {{the investigation of the}} biological mechanisms through which {{hyperbaric oxygen therapy}} acts at a fundamental level...|$|R
5000|$|Norris {{provided}} CERL with machines {{on which}} to develop their system in the late 1960s. In 1971 {{he set up a}} new division within CDC to develop PLATO [...] "courseware", and eventually many of CDC's own initial training and technical <b>manuals</b> <b>ran</b> on it. In 1974 PLATO was running on in-house machines at CDC headquarters in Minneapolis, and in 1976 they purchased the commercial rights in exchange for a new CDC Cyber machine.|$|R
40|$|In this paper, we {{describe}} our experiments for TRECVID 2004 for the Search task. In the interactive search task, we developed {{two versions of}} a video search/browse system based on the Físchlár Digital Video System: one with text- and image-based searching (System A); the other with only image (System B). These two systems produced eight interactive runs. In addition we submitted ten fully automatic supplemental <b>runs</b> and two <b>manual</b> <b>runs.</b> A. 1, Submitted Runs: • DCUTREC 13 a_ 1, 3, 5, 7 for System A, four interactive runs based on text and image evidence. • DCUTREC 13 b_ 2, 4, 6, 8 for System B, also four interactive runs based on image evidence alone. • DCUTV 2004 _ 9, a <b>manual</b> <b>run</b> based on filtering faces from an underlying text search engine for certain queries. • DCUTV 2004 _ 10, a <b>manual</b> <b>run</b> based on manually generated queries processed automatically. • DCU_AUTOLM 1, 2, 3, 4, 5, 6, 7, seven fully automatic runs based on language models operating over ASR text transcripts and visual features. • DCUauto_ 01, 02, 03, three fully automatic runs based on exploring the benefits of multiple sources of text evidence and automatic query expansion. A. 2, In the interactive experiment {{it was confirmed that}} text and image based retrieval outperforms an image-only system. In the fully automatic runs, DCUauto_ 01, 02, 03, it was found that integrating ASR, CC and OCR text into the text ranking outperforms using ASR text alone. Furthermore, applying automatic query expansion to the initial results of ASR, CC, OCR text further increases performance (MAP), though not at high rank positions. For the language model-based fully automatic runs, DCU_AUTOLM 1, 2, 3, 4, 5, 6, 7, we found that interpolated language models perform marginally better than other tested language models and that combining image and textual (ASR) evidence was found to marginally increase performance (MAP) over textual models alone. For our two <b>manual</b> <b>runs</b> we found that employing a face filter disimproved MAP when compared to employing textual evidence alone and that manually generated textual queries improved MAP over fully automatic runs, though the improvement was marginal. A. 3, Our conclusions from our fully automatic text based runs suggest that integrating ASR, CC and OCR text into the retrieval mechanism boost retrieval performance over ASR alone. In addition, a text-only Language Modelling approach such as DCU_AUTOLM 1 will outperform our best conventional text search system. From our interactive runs we conclude that textual evidence is an important lever for locating relevant content quickly, but that image evidence, if used by experienced users can aid retrieval performance. A. 4, We learned that incorporating multiple text sources improves over ASR alone and that an LM approach which integrates shot text, neighbouring shots and entire video contents provides even better retrieval performance. These findings will influence how we integrate textual evidence into future Video IR systems. It was also found that a system based on image evidence alone can perform reasonably and given good query images can aid retrieval performance...|$|R
40|$|In this paper, we {{describe}} our experiments for TRECVID 2004 for the Search task. In the interactive search task, we developed {{two versions of}} a video search/browse system based on the Físchlár Digital Video System: one with text- and image-based searching (System A); the other with only image (System B). These two systems produced eight interactive runs. In addition we submitted ten fully automatic supplemental <b>runs</b> and two <b>manual</b> <b>runs.</b> A. 1, Submitted Runs: • DCUTREC 13 a_{ 1, 3, 5, 7 } for System A, four interactive runs based on text and image evidence. • DCUTREC 13 b_{ 2, 4, 6, 8 } for System B, also four interactive runs based on image evidence alone. • DCUTV 2004 _ 9, a <b>manual</b> <b>run</b> based on filtering faces from an underlying text search engine for certain queries. • DCUTV 2004 _ 10, a <b>manual</b> <b>run</b> based on manually generated queries processed automatically. • DCU_AUTOLM{ 1, 2, 3, 4, 5, 6, 7 }, seven fully automatic runs based on language models operating over ASR text transcripts and visual features. • DCUauto_{ 01, 02, 03 }, three fully automatic runs based on exploring the benefits of multiple sources of text evidence and automatic query expansion. A. 2, In the interactive experiment {{it was confirmed that}} text and image based retrieval outperforms an image-only system. In the fully automatic runs, DCUauto_{ 01, 02, 03 }, it was found that integrating ASR, CC and OCR text into the text ranking outperforms using ASR text alone. Furthermore, applying automatic query expansion to the initial results of ASR, CC, OCR text further increases performance (MAP), though not at high rank positions. For the language model-based fully automatic runs, DCU_AUTOLM{ 1, 2, 3, 4, 5, 6, 7 }, we found that interpolated language models perform marginally better than other tested language models and that combining image and textual (ASR) evidence was found to marginally increase performance (MAP) over textual models alone. For our two <b>manual</b> <b>runs</b> we found that employing a face filter disimproved MAP when compared to employing textual evidence alone and that manually generated textual queries improved MAP over fully automatic runs, though the improvement was marginal. A. 3, Our conclusions from our fully automatic text based runs suggest that integrating ASR, CC and OCR text into the retrieval mechanism boost retrieval performance over ASR alone. In addition, a text-only Language Modelling approach such as DCU_AUTOLM 1 will outperform our best conventional text search system. From our interactive runs we conclude that textual evidence is an important lever for locating relevant content quickly, but that image evidence, if used by experienced users can aid retrieval performance. A. 4, We learned that incorporating multiple text sources improves over ASR alone and that an LM approach which integrates shot text, neighbouring shots and entire video contents provides even better retrieval performance. These findings will influence how we integrate textual evidence into future Video IR systems. It was also found that a system based on image evidence alone can perform reasonably and given good query images can aid retrieval performance...|$|R
5000|$|HSV also {{released}} a HSV Senator Signature SV08 which is released {{in a limited}} <b>run</b> of 20 <b>manual</b> and 30 automatic units. This model featured lower paint-outs, sill plates and extra chrome accents on the side mirrors and door handles. It was powered by a V8 engine developing 317 kW mated to a new Tremec TR6060 gearbox and had 20-inch [...] "Pentagon" [...] wheels, Magnetic Ride Control suspension system with Sport mode and Park Assist system.|$|R
40|$|In TREC- 7, we {{participated in}} both the {{automatic}} and manual tracks for category A. For the automatic runs, we included a baseline run and an experimental run that filtered relevance feedback using proper nouns. The baseline run used the short query versions and term thresholding {{to focus on the}} most meaningful terms. The experimental run used the long queries (title, description and narrative) with relevance feedback that filtered for proper nouns. Information extraction tools were used to identify proper nouns. For <b>manual</b> <b>runs,</b> we used predefined concept lists with terms from the concept lists combined in different ways. The <b>manual</b> <b>run</b> focused on using phrases and proper nouns in the query. We continued to use the NCR/Teradata DBC- 1012 Model 4 parallel database machine as the primary platform and added an implementation on Sybase IQ. We again used the relational database model implemented with unchanged SQL. In addition, we enhanced our system by implementing new stop word lists for terms and selecting phrases based on association scores. Our results, while not dramatic, indicate that further work in merging information extraction and information retrieval is warranted. 1...|$|R
40|$|Our {{experiments}} focused {{this year}} on the ad-hock task of the Terabyte track. We experimented with WAND, a document-ata-time evaluation algorithm we developed recently. Our results demonstrate the superiority of WAND over traditional term-atime strategy while searching over a large collection such as gov 2. We demonstrate how Web expansion can be successfully applied to significantly improve search results. In addition, we describe several schemes for creating manual queries, following this year’s goal to enrich the pool of results by <b>manual</b> <b>runs.</b> ...|$|R
