0|1846|Public
40|$|In {{the model}} of choice by <b>sequential</b> <b>procedures,</b> the {{successive}} application of criteria gradually reduces the set of alternatives to a unique element, which is the one actually chosen. We offer a simple property, Independence of One Irrelevant Alternative, and show it is equivalent to choice by <b>sequential</b> <b>procedures.</b> Our property is instrumental for the understanding of choice by <b>sequential</b> <b>procedures,</b> and provides a novel tool with which to study how other behavioral concepts relate to it. We show that the notions of rationalizability by game trees, agenda rationalizability, and choice functions exhibiting a status-quo bias are special cases of choice by <b>sequential</b> <b>procedures...</b>|$|R
40|$|This article {{considers}} using <b>sequential</b> <b>procedures</b> {{to determine}} the amount of survey effort required in a line transect survey in order to achieve a certain precision level in estimating the abundance of a biological population. <b>Sequential</b> <b>procedures</b> are constructed for both parametric and nonparametric animal abundance estimators. The criterion used to derive the stopping rules is the width of confidence intervals for the animal abundance. For each estimator considered, we develop stopping rules based on the asymptotic distributions and the bootstrap. A sequential analysis on an aerial survey of the southern bluefin tuna indicates substantial saving of survey effort can be made by employment of the proposed <b>sequential</b> <b>procedures.</b> This savings of survey effort is also observed in a simulation study designed to evaluate the empirical performance of the proposed <b>sequential</b> <b>procedures.</b> link_to_subscribed_fulltex...|$|R
40|$|We {{propose a}} rule of decision-making, the <b>sequential</b> <b>procedure</b> guided by routes, and show that three {{influential}} boundedly rational choice models can be equivalently understood as special cases of this rule. In addition, the <b>sequential</b> <b>procedure</b> guided by routes is instrumental in showing that the three models are intimately related. We show that choice with a status-quo bias is a refinement of rationalizability by game trees, which, in turn, is also a refinement of sequential rationalizability. Thus, we provide a sharp taxonomy of these choice models, and show that they all {{can be understood as}} choice by <b>sequential</b> <b>procedures...</b>|$|R
40|$|We {{continue}} our survey of methods for constructing confidence intervals for steady-state means via simulation by studying <b>sequential</b> <b>procedures</b> which determine {{the length of}} the simulation {{during the course of the}} run. Our goal is to provide the simulation practitioner with some guidance as to which published procedures might actually perform well in practice. Empirical results for a variety of stochastic models with known steady-state means suggest that <b>sequential</b> <b>procedures</b> by Fishman and by Law and Carson provide good performance relative to the criterion probability of coverage. simulation, confidence invervals, stopping rules, <b>sequential</b> <b>procedures,</b> steady-state simulations...|$|R
40|$|An {{accelerated}} <b>sequential</b> <b>procedure</b> {{is proposed}} {{to save the}} number of sampling operations we would have observed had the purely <b>sequential</b> <b>procedures</b> been used. The method is outlined {{as it pertains to}} the point estimation of the unknown normal mean. We supplement our theoretical findings with simulations to study the moderate sample size performance of the proposed procedure...|$|R
40|$|Threshold {{estimation}} with <b>sequential</b> <b>procedures</b> is justifiable on the {{surmise that}} the index {{used in the}} so-called dynamic stopping rule has diagnostic value for identifying when an accurate estimate has been obtained. The performance of five types of Bayesian <b>sequential</b> <b>procedure</b> was compared here to that of an analogous fixed-length procedure. Indices for use in <b>sequential</b> <b>procedures</b> were: (1) {{the width of the}} Bayesian probability interval, (2) the posterior standard deviation, (3) the absolute change, (4) the average change, and (5) the number of sign fluctuations. A simulation study was carried out to evaluate which index renders estimates with less bias and smaller standard error at lower cost (i. e. lower average number of trials to completion), in both yes–no and two-alternative forced-choice (2 AFC) tasks. We also considered the effect of the form and parameters of the psychometric function and its similarity with themodel function assumed in the procedure. Our results show that <b>sequential</b> <b>procedures</b> do not outperform fixed-length procedures in yes–no tasks. However, in 2 AFC tasks, <b>sequential</b> <b>procedures</b> not based on sign fluctuations all yield minimally better estimates than fixed-length procedures, although most of the improvement occurs with short runs that render undependable estimates and the differences vanish when the procedures run for a number of trials (around 70) that ensures dependability. Thus, none of the indices considered here (some of which are widespread) has the diagnostic value that would justify its use. In addition, difficulties of implementation make <b>sequential</b> <b>procedures</b> unfit as alternatives to fixed-length procedures...|$|R
5000|$|Lower bounds for the {{expected}} {{sample size and}} the average risk of a <b>sequential</b> <b>procedure,</b> 1960 ...|$|R
40|$|In {{this study}} we propose a <b>sequential</b> <b>procedure</b> for {{hypothesis}} testing on the Cpk process capability index. We compare {{the properties of the}} sequential test with the performances of non-sequential tests by performing an extensive simulation study. The results indicate that the proposed <b>sequential</b> <b>procedure</b> makes it possible to save a large amount of sample size, which can be translated into reduced costs, time and resources...|$|R
40|$|Determining {{the correct}} sample size {{is of utmost}} {{importance}} in study design. Large samples yield classifiers or parameters with more precision and conversely, samples that are too small yield unreliable results. Fixed sample size methods, {{as determined by the}} specified level of error between the obtained parameter and population value, or a confidence level associated with the estimate, have been developed and are available. These methods are extremely useful when {{there is little or no}} cost (consequences of action), financial and time, involved in gathering the data. Alternatively, <b>sequential</b> sampling <b>procedures</b> have been developed specifically to obtain a classifier or parameter estimate that is as accurate as deemed necessary by the researcher, while sampling the least number of observations required to obtain the specified level of accuracy. This dissertation discusses a <b>sequential</b> <b>procedure,</b> derived using Martingale Limit Theory, which had been developed to train a classifier with the minimum number of observations to ensure, with a high enough probability, that the next observation sampled has a low enough probability of being misclassified. Various classification methods are discussed and tested, with multiple combinations of parameters tested. Additionally, the <b>sequential</b> <b>procedure</b> is tested on microarray data. Various advantages and shortcomings of the <b>sequential</b> <b>procedure</b> are pointed out and discussed. This dissertation also proposes a new <b>sequential</b> <b>procedure</b> that trains the classifier to such an extent as to accurately estimate the Bayes error with a high probability. The <b>sequential</b> <b>procedure</b> retains all of the advantages of the previous method, while addressing the most serious shortcoming. Ultimately, the <b>sequential</b> <b>procedure</b> developed enables the researcher to dictate how accurate the classifier should be and provides more control over the trained classifier. Dissertation (MSc) [...] University of Pretoria, 2014. StatisticsRestricte...|$|R
40|$|We {{consider}} a sequential problem of selling K identical assets over the finite time horizon with a fixed number of offers per time period and no recall of past offers. The {{objective is to}} find an optimal <b>sequential</b> <b>procedure</b> which maximizes the total expected revenue. In this paper, we derive an effective number of stoppings for an optimal <b>sequential</b> <b>procedure</b> for the selling problem with independent observations. 5 page(s...|$|R
40|$|Two {{solutions}} {{based on}} two-stage sampling {{are given to}} the problem of obtaining a confidence interval having a specified width 2 d for the parameter of the uniform (0,[theta]) density. When d/ 0 is small, they yield smaller sample sizes than that of Graybill and Connell (1964). An adaptive <b>sequential</b> <b>procedure</b> is also developed and compared with the two-stage procedures. Uniform density parameter Fixed-width interval estimation Two-stage <b>sequential</b> <b>procedures...</b>|$|R
3000|$|Once {{the minimum}} and maximum FAR values are specified, the pAUC measure for each {{procedure}} is fixed {{and will not}} vary {{as a function of}} which theoretical model of memory strengths is assumed to be true. In that sense, pAUC is a purely empirical measure of discriminability. In Fig. 5, it is visually obvious that pAUC for the simultaneous procedure is greater than the pAUC for the <b>sequential</b> <b>procedure</b> over the FAR range of 0 to [...]. 038 (the maximum FAR for the <b>sequential</b> <b>procedure).</b> This is true even though, as ordinarily computed, the DR for the <b>sequential</b> <b>procedure</b> (8.0) is larger than the DR for the simultaneous procedure (7.3). The pROC software uses a bootstrap procedure to determine if the apparent difference in the two pAUC values is statistically significant.|$|R
40|$|The {{sequential}} probability ratio test (SPRT) is {{a hypothesis}} testing procedure, which evaluates data {{as it is}} collected. The original SPRT was developed by Wald for one-parameter families of distributions and later extended by Bartlett to account for nuisance parameters. We adapt Bartlett's SPRT to Generalized Linear Mixed Models (GLMM), in which the observations are non-identitically and non-independently distributed and illustrate the approach taken with two applications. In the first application, we incorporate a Poisson GLMM into <b>sequential</b> <b>procedure</b> to design a multicenter randomized clinical trial that compares two preventive treatments for surgical site infections. In the second application, we incorporate a Negative Binomial spatial GLMM into <b>sequential</b> <b>procedure</b> to design a pest assessment protocol. We also consider a generative spatial model {{in the context of}} <b>sequential</b> <b>procedures</b> as an alternative to spatial GLMMs...|$|R
40|$|For {{parameters}} of stationary processes with zero mean and spectral density, <b>sequential</b> <b>procedures</b> are proposed for constructing fixed size confidence ellipsoidal regions for unknown parameters using a minimum contrast estimator. The confidence ellipsoids are {{shown to be}} asymptotically consistent and the associated stopping rules are shown to be asymptotically efficient as {{the size of the}} region becomes small when the assumed parametric model is correct. Monte Carlo simulations are given to investigate the performance of our proposed <b>sequential</b> <b>procedures.</b> ...|$|R
40|$|This paper {{discusses}} {{implementation of}} a <b>sequential</b> <b>procedure</b> to estimate the steady-state density of a stochastic process. The procedure computes sample densities at certain points and uses Lagrange interpolation to estimate the density f(x). Even though the proposed <b>sequential</b> <b>procedure</b> is a heuristic, it does have strong basis. Our empirical {{results show that the}} procedure gives density estimates that satisfy a pre-specified precision requirement. An experimental performance evaluation demonstrates the validity of using the procedure to estimate densities. ...|$|R
40|$|We study a k-stage > <b>sequential</b> {{estimation}} <b>procedure</b> {{which includes}} the three-stage procedure of Hall (1981) as a special case. With a suitable value of k, the k-stage procedure not only can be as efficient as the fully <b>sequential</b> <b>procedure</b> of Anscombe, Chow and Robbins in terms of sample size, but also requires at most k sampling operations. For the problem of constructing a fixed width confidence interval for the mean of a normal population with unknown variance, the three-stage procedure of Hall always needs a few more observations than the fully <b>sequential</b> <b>procedure.</b> The five-stage procedure, however, requires almost {{the same number of}} observations as the fully sequential procedur...|$|R
40|$|A {{truncated}} <b>sequential</b> <b>procedure</b> {{is constructed}} for estimating the drift coefficient {{at a given}} state point based on discrete data of ergodic diffusion process. A nonasymptotic upper bound is obtained for a pointwise absolute error risk. The optimal convergence rate and a sharp constant in the bounds are found for the asymptotic pointwise minimax risk. As a consequence, the efficiency is obtained of the proposed <b>sequential</b> <b>procedure.</b> Comment: Published at [URL] in the Bernoulli ([URL] by the International Statistical Institute/Bernoulli Society ([URL]...|$|R
3000|$|... (higher for the <b>sequential</b> <b>procedure</b> in this {{hypothetical}} example), {{the data}} would support that model even though empirical discriminability (pAUC) {{goes in the}} opposite direction.|$|R
40|$|We give an {{elementary}} {{proof of a}} theorem of Bechhofer, Kiefer, and Sobel concerning their <b>sequential</b> <b>procedure</b> PB* for selecting the highest probability in binomial or multinomial trials. Our proof uses the change of measure approach. An error in the original proof is pointed out and corrected. <b>sequential</b> selection <b>procedures</b> binomial trials change of measure...|$|R
40|$|Human {{genetic linkage}} studies have the {{objective}} of testing whether disease genes are linked to genetic markers based on family genetic data. Sometimes, these studies require many years of recruiting informative families and large amount of funds. One way to reduce the required sample size for such studies is to use <b>sequential</b> testing <b>procedures.</b> In this paper, we investigate two group sequential tests for homogeneity in binomial mixture models that are commonly used in genetic linkage analysis. We conduct Monte Carlo simulations to examine the performance of the group <b>sequential</b> <b>procedures.</b> The results show that the proposed group <b>sequential</b> <b>procedures</b> can save, on average, substantial sample size and detect linkage with almost the same power as their nonsequential counterparts. ...|$|R
40|$|The {{problem to}} select the best {{population}} in some specified sense from several assigned populations {{in the light of}} samples drawn from them is very important in practical situations. An experimenter who is faced with this problem may select the best population according to a certain statistical procedure in view of the informations supplied by samples drawn from these populations. In this connection several different statistical procedures have been introduced by many authors such as Bahadur [2], Bahadur and Robbins [3], Bechhofer [4], [5], Bechhofer and Blumenthal [6], Bechhofer, Dunnett and Sobel [7], Dunnett [8], Fabian [9], Girshick [10], Gupta and Sobel [11], Mosteller [13] and Paulson [14]～[18] Taylor and David [25], Truax [26]. They have discussed this problem from several different viewpoints such as slippage aspect grouping aspect and ranking aspect. Some of them appealed to <b>sequential</b> multiple decision <b>procedures</b> which were not necessary closed. On the other hand authors such as Armitage [1], Schneiderman [21], Schneiderman and Armitage [22], Sobel and Wald [23], Sobel [24] appealed to <b>restricted</b> or closed <b>sequential</b> <b>procedures</b> which, however, were not necessarily multiple decision procedure. It was Paulson [19], [20] who presented a class of closed <b>sequential</b> multiple decision <b>procedures</b> for a set of significance level a in $ 0 < alpha < 1 $ and for a certain configuration of population means for which the probability by which the best population (having the largest means) among several normal populations is selected is larger than the prescribed value $ 1 -alpha $. The object {{of this paper is to}} generalize the results of Paulson [19], [20] in two directions. In the first place we shall be concerned with a more general class of population distributions, that is, one parameter exponential distributions. In the second place we shall discuss with a configuration of population parameters which are more general than that which Paulson [19], [20] did substantially consider. Such a generalized configuration of population parameters is indeed both subtle and necessary in our generalized set-up dealing with one parameter exponential distribution. It is shown in this paper that the essential aspect of our closed <b>sequential</b> statistical <b>procedures</b> (CSSP) can be established in the frame of additive family of sufficient statistics whose notion was introduced by Kitagawa [12]. It is also noted that the restrictions on population distributions are required to establish our results. However these restrictions are so mild that we can easily obtain various examples regarding normal distributions, $ x^ 2 $-distributions, Poisson distributions and binomial distributions...|$|R
40|$|Detailed {{analyses}} and comparisons of urban travel forecasts obtained {{by applying the}} state-of-practice <b>sequential</b> <b>procedure</b> and the solution of a combined network equilibrium model are presented. The <b>sequential</b> <b>procedure</b> for solving the trip distribution, mode choice and assignment problems with feedback is the current practice in most transportation planning agencies, although its important limitations are well known. The solution of a combined model, in contrast, results from a single mathematical formulation, which ensures a well-converged and consistent result. Using a real network, several methods for solving the <b>sequential</b> <b>procedure</b> with feedback are compared to {{the solution of the}} combined model ESTRAUS. The results of these methods are shown to have various levels of instability. The paper concludes with a call for a new paradigm of travel forecasting practice based on an internally consistent model formulation that can be solved to a level of precision suitable for comparing alternative scenarios...|$|R
5000|$|... demonstrating <b>sequential</b> <b>procedures</b> {{of testing}} {{underlying}} assumptions of parametric tests, commonly recommended in textbooks and statistics software user manuals, [...] "increases {{the rate of}} Type I error"; ...|$|R
40|$|We {{present a}} new fully <b>sequential</b> <b>procedure</b> for {{selecting}} {{the best among}} {{a finite number of}} simulated systems. While many fully <b>sequential</b> selection <b>procedures</b> make a decision based on pairwise comparison, the new procedure compares systems in a group of three and uses some properties of a bivariate Brownian motion process exiting a circle or an ellipse for its derivation. 1...|$|R
40|$|In {{this article}} the result on the {{conditional}} probability of correct selection of Liu (1993) {{is used to}} construct a <b>sequential</b> <b>procedure</b> with elimination for selecting the population with the largest mean among k normal populations with a common known variance. Comparisons with some other <b>sequential</b> selection <b>procedures</b> on the expected total sample sizes are made both asymptotically and by simulations...|$|R
40|$|PERFORMANCE EVALUATIONS OF COMPARISON-WITH-A-STANDARD PROCEDURES We {{investigate}} {{the performance of}} a heuristic <b>sequential</b> <b>procedure</b> to compare a finite number of designs with respect to a single standard. The goal is to identify the best design, and if the chosen best design is not the standard, to determine whether the chosen best design is better than the standard. We give preferential status to the standard because there are costs and time involved in replacing the standard. We accomplish this goal by extending indifference-zone selection procedures. An experimental performance evaluation demonstrates the validity and efficiency of our <b>sequential</b> <b>procedures.</b> ...|$|R
40|$|The {{problem of}} minimum risk point {{estimation}} under squared-error loss function (SELF) for the parameter {{associated with the}} generalized life distributions, is considered. The failure of fixed sample size <b>procedure</b> is established. <b>Sequential</b> <b>procedure</b> using uniformly minimum variance unbiased estimator (UMVUE) at both the stopping and estimation stages is developed and the second-order approximations are derived. The regret of the <b>sequential</b> <b>procedure</b> is obtained and the condition under which the regret may be negative is discussed. Finally, an improved estimator is proposed and its dominance over the UMVUE (in terms of having smaller risk) is also established...|$|R
40|$|Sequential {{methods are}} used in statistics, where {{only a limited number}} of {{observations}} has to be made to obtain a reliable result. In this thesis we present basic <b>sequential</b> <b>procedures</b> that {{are used in}} the linear regression model. In the first chapter we introduce the linear regression model. In the second chapter we present different sequential methods. We compare these methods with each other and determine the advantages and disadvantages of individual <b>sequential</b> <b>procedures.</b> In the third and fourth chapter we cons- truct interval estimates for regression parameters. Additionally, in the fourth chapter we construct tests for regression parameters. ...|$|R
40|$|Abstract: Fully <b>sequential</b> indifference-zone {{selection}} <b>procedures</b> {{have been}} proposed in the simulation literature to select the system with the best mean performance {{from a group of}} simulated systems. However, the existing <b>sequential</b> indifference-zone <b>procedures</b> allocate an equal number of samples to the two systems in comparison even if their variances are drastically different. In this paper we propose new fully <b>sequential</b> indifference-zone <b>procedures</b> that allocate samples according to the variances. We show that the procedures work better than several existing <b>sequential</b> indifference-zone <b>procedures</b> when variances of the system...|$|R
40|$|In {{this paper}} we apply a {{two-stage}} sequential design to item calibration problems under a three-parameter logistic model assumption. The measurement errors of {{the estimates of}} the latent trait levels of examinees are considered in our <b>procedure.</b> Moreover, a <b>sequential</b> <b>procedure</b> is employed to guarantee that the estimates of the parameters reach a prescribed accuracy criterion when the iteration is stopped, which fully takes the advantage of sequential design. Statistical properties of both the item parameter estimates and the <b>sequential</b> <b>procedure</b> are discussed. We compare {{the performance of the}} proposed method with that of the procedures based on some conventional designs using numerical studies...|$|R
40|$|In {{this paper}} we {{consider}} the problem of constructing a set of fixed-width simultaneous confidence intervals for all-pairwise differences {{of the means of}} k independent normal populations with a common unknown variance. A <b>sequential</b> <b>procedure</b> is necessary to achieve this goal. For the pure <b>sequential</b> <b>procedure</b> considered in this paper, the second order approximations to the confidence level and the expected sample size are given. A recursive method for the exact calculations of the confidence level and the expected sample size is also provided. Comparisons between the second order approximations and the exact calculations are carried out to indicate when the approximations are reasonably accurate...|$|R
40|$|The use of batch {{means is}} a {{well-known}} technique for estimating the variance of mean point estimators computed from a simulation experiment. This paper discusses implementation of a <b>sequential</b> <b>procedure</b> to determine the batch size for constructing confidence intervals for a simulation estimator of the steady-state mean of a stochastic process. Our quasi-independent (QI) procedure increases the batch size progressively until {{a certain number of}} essentially i. i. d. samples are obtained. We show that our <b>sequential</b> <b>procedure</b> gives valid confidence intervals. The only (mild) assumption is that the correlations of the stochastic process output sequence die off. An experimental performance evaluation demonstrates the validity of the QI procedure...|$|R
40|$|This paper {{develops}} a theory and methodology for estimation of Gini index such that both {{cost of sampling}} and estimation error are minimum. Methods in which sample size is fixed in advance, cannot minimize estimation error and sampling cost at the same time. In this article, a purely <b>sequential</b> <b>procedure</b> is proposed which provides {{an estimate of the}} sample size required to achieve a sufficiently smaller estimation error and lower sampling cost. Characteristics of the purely <b>sequential</b> <b>procedure</b> are examined and asymptotic optimality properties are proved without assuming any specific distribution of the data. Performance of our method is examined through extensive simulation study...|$|R
40|$|In this paper, we {{consider}} sequential {{estimation of the}} location parameter based on the midrange {{in the presence of}} an unknown scale parameter when the underlying distribution has a bounded support. The estimation is done under squared loss plus cost of sampling. Stopping rules based on the range are proposed and are shown to be asymptotically efficient. The risks of the <b>sequential</b> <b>procedures</b> are compared with the Robbins <b>sequential</b> estimation <b>procedure</b> based on the sample mean. The former are shown to be asymptotically more efficient than the latter {{in the sense of the}} sample size when the density function changes sharply at the end points of the support. Koike (2007) observed a similar asymptotic superiority of the <b>sequential</b> estimation <b>procedure</b> based on the midrange in the <b>sequential</b> interval estimation <b>procedure</b> under the same condition...|$|R
3000|$|... [...]) {{is greater}} for {{sequential}} lineups {{than it is}} for simultaneous lineups, the sequential ROC data nevertheless fall closer to the diagonal line of chance performance (i.e., pAUC is lower for the <b>sequential</b> <b>procedure,</b> and parametric A [...]...|$|R
30|$|The {{analysis}} {{is based on}} a <b>sequential</b> <b>procedure</b> in which models of increasing complexity were tested. In order to demonstrate the logic and procedures of DiD we use this small example to present the results from each of the major steps.|$|R
