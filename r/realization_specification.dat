7|28|Public
40|$|In this paper, we {{introduce}} {{a novel approach}} for high level syn-Lhesis for DSP algorithms. Two features are provided by the approach: completeness and correctness. A given algorithm will be represented in a new developed language termed Algorithm Specification Language (ASL). ASL has the abilit,y to describe any general algorithm. An automatic procedure is used to transform an ASL representation into a specific <b>realization</b> <b>specification</b> using a correctness preserving set of transformations. The realization format is based on representing the digital architectures by another developed language called <b>Realization</b> <b>Specification</b> Language(RSL). Logic Programming {{is used as a}} user interface for the synthesis pro-cedure. 1...|$|E
40|$|During fall 2009, the Eindhoven University of Technology {{started the}} {{development}} of the Lupo EL (Electric Lightweight) research vehicle. The vehicle is fully operational now, allowed to drive on the public road and used in several research projects. This paper will focus more in-depth on the vehicle development and the research performed. The following subjects will be addressed: vehicle <b>realization,</b> <b>specification</b> and performance, data acquisition systems and control systems...|$|E
40|$|The major {{drawback}} {{of reported}} high level synthesis techniques is their limited applicability {{to a specific}} class of algoriehms without extendibility to general algorithms {{and the lack of}} a formal approach to prove the correctness of the such techniques. In this paper, we introduce a novel approach for high level synthesis from p-recursive alg+ rithms. Two features are provided by the approach: com-pleteness and correctness. Completeness means the ability to use the approach for any general algorithm. Correctness is achieved by using a set of transformations that are proved to be correct. A formal framework for the synthesis procedure has been developed which can be easily automated. A given algorithm will be represented in a new developed language termed Algorithm Specification Language (ASL). ASL has the ability to describe any gen-eral algorithm. An automatic procedure is used to transform an ASL representation into a specific <b>realization</b> <b>specification</b> using a correctness preserving set of transfor-mations. The realization format is based on representing the digital architectures by a <b>Realization</b> <b>Specification</b> Language(RSL). 1...|$|E
5000|$|... "FRAND" [...] {{intellectual}} property licensing: OMA members that own {{intellectual property}} rights (e.g. patents) on technologies that {{are essential to the}} <b>realization</b> of a <b>specification</b> agree in advance to provide licenses to their technology on [...] "fair, reasonable and non-discriminatory" [...] terms to other members.|$|R
40|$|We derive {{a closed}} form {{portfolio}} optimization rule for an investor who is diffident about mean return and volatility estimates, {{and has a}} CRRA utility. Confidence is here represented using ellipsoidal uncertainty sets for the drift, given a (compact valued) volatility <b>realization.</b> This <b>specification</b> affords a simple and concise analysis, as the agent becomes observationally equivalent to one with constant, worst case parameters. The result {{is based on a}} max–min Hamilton–Jacobi–Bellman–Isaacs PDE, which extends the classical Merton problem and reverts to it for an ambiguity-neutral investor...|$|R
40|$|International audienceSystems are {{generally}} described by using many various languages for their <b>specification,</b> <b>realization,</b> implementation, etc. In this context, the paper explains how generic datatypes and higher-order functions {{can be used}} to model languages and to pass more easily and formally from one language to another. As an application the paper present a (meta) model for temporal properties, states-transitions models, and implementation, and the transformations between them...|$|R
40|$|A {{complete}} {{design and}} implementation of a cell library has been accomplished in this work. This cell library supports a formal high level synthesis framework. The library contains the logic level models of all primitive functions of a <b>Realization</b> <b>Specification</b> Language (RSL). Modular design methodology is employed to support the expandability of basic cells. Examples of a formal adder, multiplier, inner-product and matrix-matrix multiplier are presented. Advisor: Prof. Sadiq M. Sait, Co-Advisors: Dr. Khalid M. Elleithy and Dr. Samir Abdul Jauwad...|$|E
40|$|In {{this paper}} {{we present a}} {{complete}} design and implementation of a CMOS cell library which supports a formal high level synthesis framework. The library contains the logic level models and VLSI layouts of all primitive functions of the <b>Realization</b> <b>Specification</b> Language (RSL) [1] {{as well as some}} commonly used functions which are also built using these basic functions. Modular design methodology is employed to support the expandibility of the basic cells. Example of a formal matrix-matrix multiplayer is presented to illustrate the application of the cell library...|$|E
40|$|Sadly {{we cannot}} post {{a copy of}} this {{proceedings}} as we are only allowed to post a pre-print copy, which we do not have. To access this article use the information from the WorldCat link to below to request the article through inter-library loan from your local library. The major drawback of reported high level synthesis techniques is their limited applicability to a specific class of algorithms without extendibility to general algorithms and the lack of a formal approach to prove the correctness of the such techniques. In this paper, we introduce a novel approach for high level synthesis from μ-recursive algorithms. Two features are provided by the approach: completeness and correctness. Completeness means the ability to use the approach for any general algorithm. Correctness is achieved by using a set of transformations that are proved to be correct. A formal framework for the synthesis procedure has been developed which can be easily automated. A given algorithm will be represented in a new developed language termed Algorithm Specification Language (ASL). ASL has the ability to describe any general algorithm. An automatic procedure is used to transform an ASL representation into a specific <b>realization</b> <b>specification</b> using a correctness preserving set of transformations. The realization format is based on representing the digital architectures by a <b>Realization</b> <b>Specification</b> Language (RSL). [URL]...|$|E
50|$|SOMA {{identifies}} services, component boundaries, flows, compositions, {{and information}} through complementary techniques which include domain decomposition, goal-service modeling and existing asset analysis.The service lifecycle in SOMA {{consists of the}} phases of identification, <b>specification,</b> <b>realization,</b> implementation, deployment and management in which the fundamental building blocks of SOA are identified then refined and implemented in each phase. The fundamental building blocks of SOA consist of services, components, flows and related to them, information, policy and contracts.|$|R
50|$|SOMA is an {{end-to-end}} SOA {{method for}} the identification, <b>specification,</b> <b>realization</b> {{and implementation of}} services (including information services), components, flows (processes/composition). SOMA builds on current techniques {{in areas such as}} domain analysis, functional areas grouping, variability-oriented analysis (VOA) process modeling, component-based development, object-oriented analysis and design and use case modeling. SOMA introduces new techniques such as goal-service modeling, service model creation and a service litmus test to help determine the granularity of a service.|$|R
40|$|The goal of {{the thesis}} is the {{definition}} of a specification environment for Complex Distributed Software Applications. More precisely, the aim is to investigate all the issues connected to the <b>realization</b> of executable <b>specifications</b> for such applications, using new technologies rising in the fields of Logic Programming, Multi-Agent Systems and Distributed Software Engineering. In this context, we want to realize a set of tools {{that can be used for}} implementing and testing reliable software prototypes that solve real-world problems...|$|R
40|$|Model Driven Engineering (MDE) {{promotes}} {{the use of}} models as primary artefacts of a software development process, {{as an attempt to}} handle complexity through abstraction, e. g. to cope with the evolution of execution platforms. MDE follows a stepwise approach, by prescribing to develop abstract models further improved to integrate little by little details relative to the final deployment platforms. Thus, the application of an MDE process results in various models residing at various levels of abstraction. Each one of these models is expressed in a modeling language, in which one may find appropriate concepts for the abstraction level considered. Many advocate to use the right (modeling) language for the right purpose. This means that it is sometimes better approach to use small languages specific to the considered domain and abstraction level, than to use general purpose languages (e. g. UML) when they do not perfectly fit the (modeling) needs. As a matter of fact, an MDE development process, which involves many different domains and abstraction levels, should also involve a large variety of modeling languages. Project managers who want to apply an MDE process need to deal with this language proliferation to such an extent that, in the long run, one may infer that language engineers can become major actors of software development teams. We believe that comprehensive modeling language management facilities may considerably alleviate that MDE drawback. Such facilities may include modeling language definition, extension, adaptation, or composition. To define a (modeling) language, one needs to define its abstract syntax, its semantics, and one or more concrete syntaxes. This thesis focuses on concrete syntax definition for modeling languages, when the abstract syntax is given {{in the form of a}} metamodel. We will provide solutions both for textual and graphical concrete syntaxes. Some of our experiences in building textual languages (as MTL, a model transformation language), and graphical languages (as Netsilon, a web-application modeler) has shown that a lot of work was spent in implementing interface using traditional techniques, be it a text processor generated from a compiler compiler specification, or a modeler making use of modern 2 D graphical libraries. Indeed, abstract and concrete syntax were implemented in a disconnected way, and it was then necessary to assemble them, which became rapidly clumsy while abstract syntax evolved. We built our solution to concrete syntax definition as companions of the abstract syntax. The definition of concrete syntax we propose here made it possible to build automatic tools able to analyze or synthesize models from/to text, and to create graphical modelers. We will present a metamodel for textual concrete syntax definition to construct constructive reversible grammars. We will also propose a technique for graphical concrete syntax definition following a two-step process: specification and <b>realization.</b> <b>Specification</b> is a restrictive approach in which a metamodel defines a graphical concrete syntax. Both relations with abstract syntax and spatial relationships are expressed by means of constraints. The realization step proposes a way to provide the concrete syntax tree a meaning, by attributing it a graphical appearance, and by expressing possible user interactions. The structure of the document is the following. After introducing in deeper details the problem and the general structure of the solution we propose, we will take a tour of MDE, text and graph grammars. Then, we will present Netsilon as an example of an MDE tool to MDE development, which required both the definition of a graphical and a textual modeling language. The two following sections will present the solutions we propose for textual and graphical concrete syntax definition, respectively. Final remarks and possible improvements, especially regarding reusability in general of MDE meta-artifacts (like metamodels or model transformations), and of concrete syntax in particular, will conclude the document...|$|E
40|$|We {{present a}} {{declarative}} approach to modeling multimedia presentations using an action-based temporal language called Alan. This kind of specifications {{can be built}} incrementally, as knowledge is gained, until the model encompasses all requirements of the presentation, including non-Markovian behaviors. This means there is no gap between the specification of a system and its ultimate <b>realization,</b> since the <b>specification</b> is the implementation. By posing queries to the model, one is {{able to use it}} during the verification and validation phases of the development cycle...|$|R
40|$|The {{validation}} process comprises the activities required {{to insure the}} agreement of system <b>realization</b> with system <b>specification.</b> A preliminary validation methodology for fault tolerant systems documented. A general framework for a validation methodology is presented along {{with a set of}} specific tasks intended for the validation of two specimen system, SIFT and FTMP. Two major areas of research are identified. First, are those activities required to support the ongoing development of the {{validation process}} itself, and second, are those activities required to support the design, development, and understanding of fault tolerant systems...|$|R
40|$|Abstract — In most cases, a {{bandpass}} filter characteristic is obtained {{by using a}} lowpass-to-bandpass frequency transformation on a known lowpass transfer function. This frequency transformation controls the location of passband edges and transfer zero frequencies completely. Using the “Vlach-Chebyshev approximation ” [1] however, {{we are able to}} specify the (Chebyshev) passband limits directly, together with a free choice of transfer zero locations in the stopband. In this way it is possible to design bandpass transfer functions that cannot be obtained from lowpass functions by a frequency transformation. We think this method to be the only (and not very well known) analytical method to obtain such bandpass characteristics. We show how we designed wave digital <b>realizations</b> from the <b>specification,</b> through a VHDL description and synthesis into...|$|R
40|$|We present Matisse, a {{concurrent}} object-oriented {{system specification}} language, well-suited for protocol processing applications used in telecom networks. An industrial application used in ATM networks is introduced. From this case study, we derive the requirements {{that must be}} supported by Matisse. Matisse is the entry point for the methodology presented in [6], that bridges the gap between system specification and synthesis tools commercially available. In contrast to the system specification languages currently used in industry, Matisse is implementation-independent and permits the exploration of different embedded hardware/software <b>realizations.</b> Keywords System <b>specification,</b> hardware/software codesign, protocol processing applications, system synthesis and object-oriented languages. 1 INTRODUCTION Modern telecom systems are rapidly increasing in design complexity. Telecom network applications include protocol processing systems for broadband networks [9], wireless infrastructur [...] ...|$|R
40|$|Part 4 : ModelingInternational audienceModel Driven Architecture (MDA) {{is based}} on models and {{distinguish}} between a system functionality specification and this <b>specification</b> <b>realization</b> on a given technological platform. MDA consists of four models: CIM (Computation Independent Model), PIM (Platform Independent Model), PSM (Platform Specific Model) and code model, all these are parts of the MDA transformation line: CIM->PIM->PSM->code. A PIM model has to be created using a language which is able to describe a system from various points of view, system behavior, system’s business objects, system actors, system use cases and so on. Current paper discusses the application of two-hemisphere model for construction of UML class diagram {{as a part of}} PIM. Several solutions for determination of elements of class diagram from two-hemisphere model are currently researched and described in the paper. As well as application of the transformations by example of insurance problem domain are presented in the paper...|$|R
40|$|Observability and {{reachability}} {{are important}} concepts for formal software development. While observability concepts {{are used to}} specify the required observable behavior of a program or system, reachability concepts are {{used to describe the}} underlying data in terms of datatype constructors. In this paper we first reconsider the observational logic institution which provides a logical framework for dealing with observability. Then we develop in a completely analogous way the constructor-based logic institution which formalizes a novel treatment of reachability. Both institutions are tailored to capture the semantically correct <b>realizations</b> of a <b>specification</b> from either the observational or the reachability point of view. We show that there is a methodological and even formal duality between both frameworks. In particular, we establish a correspondence between observer operations and datatype constructors, observational and constructor-based algebras, fully abstract and reachable algebras, and observational and inductive consequences of specifications. The formal duality between the observability and reachability concepts is established in a category-theoretic setting...|$|R
40|$|One can {{attempt to}} solve the problem of {{establishing}} the correctness of some software w. r. t. a formal specification at the semantical level. For this purpose, the semantics of an algebraic specification should be the class of all algebras which correspond to the correct <b>realizations</b> of the <b>specification.</b> We approach this goal by defining an observational satisfaction relation which is less restrictive than the usual satisfaction relation. The idea is that the validity of an equational axiom should depend on an observational equality, instead of the usual equality. We show that it is not reasonable to expect an observational equality to be a congruence, hence we define an observational algebra as an algebra equipped with an observational equality which is an equivalence relation but not necessarily a congruence. Since terms may represent computations, our notion of observation depends on a set of observable terms. From a careful case study it follows that this requires to take into acco [...] ...|$|R
40|$|Bharucha, and Bigand (2000) {{show that}} {{listeners}} exposed {{to only a}} few minutes of stimuli organized according to inherent rules of an artificial grammar successfully distinguish between stimuli that obey and disobey the rules. The present study considers the extent to which subjects learn rules of a musical tradition with which they have had no contact. Although master musicians have differed in detail for centuries concerning the rules or conventions of particular North Indian (Hindustani) rags, within Bhatkande's (1966) monumental anthology the examples of rag Alhaiya Bilawal are sufficiently regular in their melodic progressions to provide a basis for inferring quite precise specifications of what happens, or tends to happen, in particular <b>realizations,</b> and these <b>specifications</b> also accord with Bhatkande's explicit prescriptions. Of importance to the present study are the following regularities: i) Relative to a tambura drone comprising C and G, each melody employs all and only the tones C D E F G A B-flat and/or B...|$|R
40|$|AbstractObservability and {{reachability}} {{are important}} concepts for formal software development. While observability concepts {{are used to}} specify the required observable behavior of a program or system, reachability concepts are {{used to describe the}} underlying data in terms of datatype constructors. In this paper we first reconsider the observational logic institution which provides a logical framework for dealing with observability. Then we develop in a completely analogous way the constructor-based logic institution which formalizes a novel treatment of reachability. Both institutions are tailored to capture the semantically correct <b>realizations</b> of a <b>specification</b> from either the observational or the reachability point of view. We show that there is a methodological and even formal duality between both frameworks. In particular, we establish a correspondence between observer operations and datatype constructors, observational and constructor-based algebras, fully abstract and reachable algebras, and observational and inductive consequences of specifications. The formal duality between the observability and reachability concepts is established in a category-theoretic setting...|$|R
40|$|To {{establish}} the correctness of some software w. r. t. its formal specification is {{widely recognized as}} a difficult task. A first simplification is obtained when the semantics of an algebraic specification {{is defined as the}} class of all algebras which correspond to the correct <b>realizations</b> of the <b>specification.</b> A software is then declared correct if it corresponds to some algebra of this class. We approach this goal by defining an observational satisfaction relation which is less restrictive than the usual satisfaction relation. Based on this notion we provide an institution for observational specifications. The idea is that the validity of an equational axiom should depend on an observational equality, instead of the usual equality. We show that it is not reasonable to expect an observational equality to be a congruence. We define an observational algebra as an algebra equipped with an observational equality which is an equivalence relation but not necessarily a congruence. We assume th [...] ...|$|R
40|$|In {{planning}} traffic-dependent {{controls for}} traffic signal systems repetitive subproblems frequently occur. Solutions {{which are used}} {{again and again in}} the same or deviating form exist for such subproblems at many planning offices and producers of traffic signal systems. However as a rule these differ considerably in terms of their realization, application width and parameters. Present solutions for subproblems with traffic signal systems are frequently not known generally, are documented in various forms and depth, are too specialized to one certain purpose or characterized highly by the style of the specific manufacturer, so that subsequent modifications by other processors may be difficult. The main objective of this research and development project was therefore to develop primary requirements for definition of the function and description of the treatment of universally applicable software modules for traffic-dependent traffic signal controls, with the intention of creating a basis for solving traffic-related problems with virtual independence from the planner or manufacturer and ensure uniform treatment in the development of traffic signal programs. This research and development project is therefore intended to create the basis for modularization and standardization in order to establish a 'library' with appropriate modules in the future. Analysis of the problems specified showed that a significant step toward a solution would be to differentiate between <b>specification</b> and <b>realization</b> of a module. In contrast to module <b>realization,</b> module <b>specification</b> allows formulation of the function with virtual independence from the planner/manufacturer, thereby allowing potential users a better decision in terms of selection, application or new development of a module. (orig.) Summary in FrenchSIGLEAvailable from TIB Hannover: ZA 4681 (769) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|The use of Laplacian {{smoothing}} splines (LSS) with generalized {{cross validation}} (GCV) {{to choose the}} smoothing parameter for the objective analysis problem is investigated. Simulated 500 mb pressure height fields are approximated from first-guess data with spatially correlated errors and observed values having independent errors. It is found that GCV does not allow LSS to adapt to variations in individual <b>realizations,</b> and that <b>specification</b> of a single suitable smoothing parameter value for all realizations leads to smaller rms error overall. While the tests were performed {{in the context of}} data from a meteorology problem, it is expected the results carry over to data from other sources. A comparison shows that significantly better approximations can be obtained using LSS applied in a unified manner to both first-guess and observed values rather that in a correction to first-guess scheme (as in Optimum Interpolation) when the firstguess error has low spatial correlation. funded by the Naval Environmental Prediction Research Facility, Monterey, CA under Program Element 611 53 N, Project (none), "Interpolation of Scattered Meteorological Data"[URL]...|$|R
40|$|In the {{development}} of the planning tool »CoaTway« (»Computer-aided strategic way for the planning of paint shops«) practice-relevant algorithms for data collection and processing, requirement <b>specification,</b> <b>realization</b> of ABC, trend and cost-benefit analyses, as well as for a link-up of check lists (e. g., tests, experiments) have been developed and modelled within an interactive system for the use by the paint user. The planning and prognosis system CoaTplan (»Computer-aided technology assessment for the planning of paint shops«) has been integrated as a module into the planning tool. In a further step, a strategic approach in the planning for innovative coating processes for the use in CoaTway by integrating knowledge-based search algorithms for allocation tasks with single technologies has been worked out. With the present experience of the Fraunhofer IPA in planning projects, for ten selected companies planning-relevant data has been collected and evaluated by the already existing method for the survey of the actual state. On the basis of the data acquisition, {{the development}} and modelling of practice-relevant algorithms took place. These were evaluated within the use cases in the selected companies. Thus, by CoaTway new efficient and ecological ways may be found without losing financial means and time in testing inapt technologies...|$|R
40|$|Young {{children}} who learn {{a second language}} (L 2) are able to attain native pronunciation norms. However, L 2 learners beyond childhood rarely rid themselves of foreign accent. Various hypotheses and models have been offered to explain such age-related differences. Few of these explanations have {{addressed the issue of}} perception/production asymmetries. Neufeld's research has demonstrated that some older learners evidence native-like knowledge of phonological distinctions at the perceptual level, while unable to reproduce these distinctions in output. This asymmetry led him to propose his performance-based Pre- and Post-articulatory Verification model. This model assumes that, although native-like phonological representations may exist in the learner's L 2 system, last-second morphophonological and phonetic adjustment may not take place because of a developmentally induced shift in focus from low- to high-level linguistic processing, i. e. to content and form. This thesis elaborates upon Neufeld's ideas by centering on articulatory <b>realization</b> of phonetic <b>specifications</b> derived in the ultimate stage of sentence planning. It is suggested that, in order to meet real-time constraints, frequent and well-practiced articulatory sequences are eventually encoded as rapidly accessible routines. These routines are packaged instructions which translate phonetic representations into articulatory goals. This extension of Neufeld's model seeks to explain much of foreign accent in adolescent and adult L 2 learners as the result of entrenched erroneous motor routines...|$|R
40|$|Abstract. Information {{technology}} (IT) is a {{means to}} an end, yet many IT projects assign primacy to technical development and attend comparatively less to the organizational change effort that is required to attain a good fit between organization and IT system. This entails a risk of not capturing the benefits of the deployed system. Effects-driven IT development aims to counter this risk by providing an instrument for managing IT projects through a sustained focus on the effects desired from the use of the IT system. A sustained focus on effects entails that the <b>specification,</b> <b>realization,</b> and assessment of effects become central systems-development activities. In this chapter, we describe the six empirical projects we have conducted in our work on effects-driven IT development during the period 2004 – 2011 and we discuss the experiences gained so far. The empirical projects indicate that the desired effects can be specified and measured, though we have mixed experiences with ensuring that effects are measured. An effects hierarchy has been devised and appears suitable for working with effects at different levels of abstraction. A key challenge with which we still have insufficient experience concerns how a partnership with close relations between a customer and a vendor can be established. Finally, we have yet to address whether and how to incorporate an effects-driven approach in the contractual regulation of IT projects...|$|R
40|$|The {{software}} development paradigm propounded by Semantic Designs, Inc. envisions a design-centric perspective rather than today’s all too prevalent code-centric viewpoint. The Design Maintenance vision mandates notions {{of design and}} designing that are both formal and practicable. Our notion of a formal design entails three interrelated parts: specification (the artifact’s functionality and performance goals); realization (including architectural design choices and ultimately, code); and rationale (justifying the <b>realization</b> of the <b>specification).</b> By practicable, we mean {{that each of the}} three parts of our formal notion of design are: 1) manipulable by both machine and software engineer; 2) scaleable for real, industrial-size systems (10 ’s of MSLOC); 3) able to accommodate systems that are realized using multiple domain specific languages (ranging from high-level specification languages to implementations comprised of multiple target-execution languages); and 4) capable of continuous, incremental enhancement and extension. The implementation of this vision is called the Design Maintenance System ™ or DMS™. The current release is DMS 1. 0 Reengineering Toolkit, which provides infrastructure for Domain Specific Language (DSL) engineering as well as language neutral components that enable construction of large-scale, mixed-language specification analysis, transformation and synthesis tools. Original research and development for DMS was funded by an award from the Department of Commerce, NIST ATP under the Component-Based Software Initiative (NIST Cooperative Agreement Number 70 NANB 5 H 1165) ...|$|R
40|$|AbstractTo {{establish}} the correctness of some software w. r. t. its formal specification is {{widely recognized as}} a difficult task. A first simplification is obtained when the semantics of an algebraic specification {{is defined as the}} class of all algebras which correspond to the correct <b>realizations</b> of the <b>specification.</b> A software is then declared correct if some algebra of this class corresponds to it. We approach this goal by defining an observational satisfaction relation which is less restrictive than the usual satisfaction relation. Based on this notion we provide an institution for observational specifications. The idea is that the validity of an equational axiom should depend on an observational equality, instead of the usual equality. We show that it is not reasonable to expect an observational equality to be a congruence. We define an observational algebra as an algebra equipped with an observational equality which is an equivalence relation but not necessarily a congruence. We assume that two values can be declared indistinguishable when it is impossible to {{establish the}}y are different using some available observations. This is what we call the Indistinguishability Assumption. Since term observation seems sufficient for data type specifications, we define an indistinguishability relation on the carriers of an algebra w. r. t. the observation of an arbitrary set of terms. From a careful case study it follows that this requires {{to take into account the}} continuations of suspended evaluations of observation terms. Since our indistinguishability relation is not transitive, it is only an intermediate step to define an observational equality. Our approach is motivated by several examples...|$|R
40|$|Fourth Conference on Interdisciplinary Musicology, Graz, 2004 (CIMA 04) Studies by Bigand and Barrouillet (1996), Perruchet, Bigand, and Benoit-Gonin (1997), Bigand, Perruchet, and Boyer (1998),Tillmann, Bharucha, and Bigand (2000) {{show that}} {{listeners}} exposed {{to only a}} few minutes of stimuli organized according to inherent rules of an artificial grammar successfully distinguish between stimuli that obey and disobey the rules. The present study considers the extent to which subjects learn rules of a musical tradition with which they have had no contact. Although master musicians have differed in detail for centuries concerning the rules or conventions of particular North Indian (Hindustani) rags, within Bhatkande's (1966) monumental anthology the examples of rag Alhaiya Bilawal are sufficiently regular in their melodic progressions to provide a basis for inferring quite precise specifications of what happens, or tends to happen, in particular <b>realizations,</b> and these <b>specifications</b> also accord with Bhatkande's explicit prescriptions. Of importance to the present study are the following regularities: i) 	Relative to a tambura drone comprising C and G, each melody employs all and only the tones C D E F G A B-flat and/or B, i. e., 7 - 35 or 8 - 23 in Forte's numbering (1973) [...] the second understood as a 'chromatic' version of the first (Rahn 1991); ii) 	Among all the melodies, each possible stepwise progression between two of these tones occurs, except between B-flat and B. The study tested the hypothesis that after 15 minutes of exposure to Alhaiya Bilawal subjects who had not previously encountered classical North Indian music would correctly distinguish between instances of the rag and examples that diverged...|$|R
40|$|The {{proliferation}} of modern wireless networks increases demand for high capacity and throughput {{in order to}} provide faster, more robust, efficient and broadband services to end users. Mobile WiMAX and LTE are examples of such networks in which for some cases they have exposed limited connectivity due to harsh environment. Relay stations are preferred to overcome problems of weak or no access for such network devices, that are placed in specific positions to maintain high quality of data transfer at low cost and provide the required connectivity anywhere anytime. These stations should be equipped with an antenna system capable of establishing communication between base station (backhaul link) and end users (access link). This thesis focuses on the design and development of a new antenna system that is suitable for a relay-based wireless network. Planar geometries of microstrip patch antennas are utilized. The antenna system comprises two antenna modules: a new design of a single antenna for access link and a new design of an antenna array for backhaul link <b>realization.</b> Both antenna <b>specifications</b> are compatible with the IEEE 802. 16 j protocol standard. Hence, relay station should be capable of pointing its radiation pattern to the base station antenna, thus to achieve the desired radiation pattern of the relay station, a new beam-forming module is proposed, designed and developed to generate the proper radiation pattern. The beam-forming module incorporating digital phase shifters and attenuator chips is fabricated and tested. The optimization process using the Least Mean Square (LMS) algorithm is considered in this study to assign the proper phase and amplitude that is necessary to each radiation element excitation current, to produce the desired steered radiation pattern. A comprehensive study on the coupling effects for several relative positions between two new backhaul and access link antenna elements is performed. Two new antenna configurations for coupling reduction are tested and the simulated and measured results in terms of antenna radiation performances were compared and commented...|$|R
40|$|The {{success of}} today’s Internet can partly be {{attributed}} to the design of the layered protocol stack. This design organizes communication protocols, that establish the rules of communication between different communicating entities, in hierarchical layers. These layers are strictly separated and offer only limited interfaces among adjacent layers. Essentially, protocols at each layer have a very specific task and they need to fulfill this task independently. Although this self-contained design of protocols worked well in wired networks, several problems appeared with the emergence of wireless and mobile communication. A prominent example is TCP’s performance drop in wireless networks as it misinterprets packet loss, due to poor link conditions, as congestion in the network. In principle, the missing knowledge of higher layers about volatile wireless conditions and in case of lower layers about higher layer requirements leads to misinterpretation and misbehavior causing suboptimal performance. A promising concept that addresses the lack of information availability is the cross- layer design paradigm which in fact circumvent the rules of strict layer separation and allows the interaction across non-adjacent layers. Many specific solutions, i. e., problem-oriented and tailor-made implementations, have demonstrated the utility of this paradigm by highlighting adaptivity advantages and performance improvements of applications and protocols. But a typical consequence of the very specific focus of the tailor-made solutions was the violation of software engineering principles such as maintainability and extensibility which are the major driving factors for the success and proliferation of software in general. As a result of this observation, a few static cross-layer architectures have been proposed that facilitate systematic design and the integration of several specific solutions. Unfortunately, in static cross-layer architectures the cross-layer coordination algorithms are deeply embedded into the operating system (OS) and are realized at compile-time. This static and deep integration into the OS has several drawbacks. First, the design of cross-layer coordination algorithms requires relevant expertise to understand and modify protocols residing in the OS. Second, the experimen- tation with cross-layer coordination algorithms is tedious since their modification requires a recompilation. Third, coordination algorithms are always active even if not needed. Finally, application developers who know best about their application requirements and constraints are prevented from specifying and providing their own set of cross-layer coordination algorithms. In this thesis, we present Crawler, a flexible cross-layer architecture that allows the <b>specification,</b> <b>realization,</b> and adaptation (i. e., addition, removal and modification) of cross-layer coordination algorithms at runtime. Based on the detection of underlying environmental changes, Crawler allows to automatically load the adequate set of coordination algorithms. It alleviates the problem of complicated access to relevant application, protocol, and system information by enabling a declarative and abstract way to describe cross-layer coordination algorithms and by providing a unified interface to inject such abstractions into the system. The generic design of this unified interface further enables the extensive experimentation with diverse compositions of cross-layer coordination algorithms and their adaptions. Moreover, the interface allows applications to provide own coordination algorithms, to share information with the system and system monitoring. In this context, we classify problems such as conflicts when adding multiple cross-layer coordination algorithms and support developers to tackle them. In general, we enable an unprecedented degree of flexibility and convenience to monitor, experiment and run several cross-layer coordination algorithms. To further support the developer while experimenting, we even allow to remotely add, remove, and modify cross-layer coordination algorithms and their monitoring. We demonstrate the usability of Crawler for monitoring and experimentation with cross-layer coordination algorithms in five diverse use cases from different areas of wireless networking such as manipulating TCP behavior, VoIP codec switching, jamming detection and reaction...|$|R

