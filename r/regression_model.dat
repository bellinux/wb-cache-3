10000|10000|Public
5|$|Subsequently, Denis Sargan and Alok Bhargava {{extended}} {{the results for}} testing if the errors on a <b>regression</b> <b>model</b> follow a Gaussian random walk (i.e., possess a unit root) against the alternative {{that they are a}} stationary first order autoregression.|$|E
25|$|The {{least squares}} estimators are point {{estimates}} of the linear <b>regression</b> <b>model</b> parameters β. However, generally {{we also want to}} know how close those estimates might be to the true values of parameters. In other words, we want to construct the interval estimates.|$|E
25|$|The {{capital asset}} pricing model uses linear {{regression}} {{as well as}} the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear <b>regression</b> <b>model</b> that relates the return on the investment to the return on all risky assets.|$|E
40|$|This paper {{considers}} series estimators of additive interactive <b>regression</b> (AIR) <b>models.</b> AIR <b>models</b> are nonparametric <b>regression</b> <b>models</b> that generalize additive <b>regression</b> <b>models</b> {{by allowing}} interactions between different regressor variables. They place more {{restrictions on the}} regression function, however, than do fully nonparametric <b>regression</b> <b>models.</b> By doing so, they attempt to circumvent the curse of dimensionality that afflicts the estimation of fully non-parametric <b>regression</b> <b>models.</b> ...|$|R
2500|$|C24 	Truncated and Censored <b>Models</b> • Switching <b>Regression</b> <b>Models</b> • Threshold <b>Regression</b> <b>Models</b> ...|$|R
5000|$|C24 Truncated and Censored <b>Models</b> • Switching <b>Regression</b> <b>Models</b> • Threshold <b>Regression</b> <b>Models</b> ...|$|R
25|$|Single index models {{allow some}} degree of {{nonlinearity}} {{in the relationship between}} x and y, while preserving the central role of the linear predictor β′x as in the classical linear <b>regression</b> <b>model.</b> Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant.|$|E
25|$|In Dempster–Shafer theory, or {{a linear}} belief {{function}} in particular, a linear <b>regression</b> <b>model</b> may be {{represented as a}} partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.|$|E
25|$|The {{coefficient}} of partial determination {{can be defined}} as the proportion of variation that cannot be explained in a reduced model, but {{can be explained by the}} predictors specified in a full(er) model.]", The American Statistician, Volume 48, Issue 2, 1994, pp. 113–117. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified <b>regression</b> <b>model.</b>|$|E
40|$|Abstract. Factorization Machines (FM) are a {{new model}} class that {{combines}} the advantages of polynomial <b>regression</b> <b>models</b> with factorization <b>models.</b> Like polynomial <b>regression</b> <b>models,</b> FMs are a general model class working with any real valued feature vector as input for the prediction of real-valued, ordinal or categorical dependent variables as output. However, in contrast to polynomial <b>regression</b> <b>models,</b> FMs replace two-way and all other higher order interaction effects by their factorized analogues. The factorization of higher order interactions enables efficient parameter estimation even for sparse datasets where just a few or even no observations for those higher order effects are available. Polynomial <b>regression</b> <b>models</b> without this factorization fail. This work discusses the relationship of FMs to polynomial <b>regression</b> <b>models</b> and the conceptual difference between factorizing and non-factorizing model parameters of polynomial <b>regression</b> <b>models.</b> Additionally, we show that the model equation of factorized polynomial <b>regression</b> <b>models</b> can b...|$|R
40|$|Piecewise linear <b>regression</b> <b>models</b> {{are very}} {{flexible}} models for modeling the data. If the piecewise linear <b>regression</b> <b>models</b> are matched against the data, then the parameters {{are generally not}} known. This paper studies the problem of parameter estimation of piecewise linear <b>regression</b> <b>models.</b> The method used to estimate the parameters of piecewise linear <b>regression</b> <b>models</b> is Bayesian method. But the Bayes estimator {{can not be found}} analytically. To overcome these problems, the reversible jump MCMC (Marcov Chain Monte Carlo) algorithm is proposed. Reversible jump MCMC algorithm generates the Markov chain converges to the limit distribution of the posterior distribution of the parameters of piecewise linear <b>regression</b> <b>models.</b> The resulting Markov chain is used to calculate the Bayes estimator for the parameters of piecewise linear <b>regression</b> <b>models...</b>|$|R
40|$|Title: "Regression {{models and}} their teaching" Author: Mgr. Marian Rybář Department: Department of Mathematics Education Abstract: <b>Regression</b> <b>models</b> and their outputs {{have a huge}} {{utilization}} {{not only in the}} field of medicine, science and managerial decision-making but also in many other fields. From the math didactics point of view is a question of intelligible explanation of <b>regression</b> <b>models</b> to non- math students very topical and exploitable theme. The main problem is an understandable explanation of this topic to statistics schooling members with minimal math application. These people aren't usually math or technical branch alumni, but they use <b>regression</b> <b>models</b> in their work. This work tries to suggest simply and clearly explanation of <b>regression</b> <b>models</b> on particular examples. Practical examples show the effects of the most common mistakes that are made by laymen in application of <b>regression</b> <b>models.</b> The main output of this work is a set of examples {{that could be used in}} <b>regression</b> <b>models</b> schooling. Keywords: <b>regression</b> <b>models,</b> <b>regression</b> analysis, didactics, teachin...|$|R
25|$|There {{are several}} {{different}} frameworks {{in which the}} linear <b>regression</b> <b>model</b> can be cast {{in order to make}} the OLS technique applicable. Each of these settings produces the same formulas and same results. The only difference is the interpretation and the assumptions which have to be imposed in order for the method to give meaningful results. The choice of the applicable framework depends mostly on the nature of data in hand, and on the inference task which has to be performed.|$|E
25|$|Sizing of a UV {{system is}} {{affected}} by three variables: flow rate, lamp power, and UV transmittance in the water. Manufacturers typically developed sophisticated Computational Fluid Dynamics (CFD) models validated with bioassay testing. This involves testing the UV reactor's disinfection performance with either MS2 or T1 bacteriophages at various flow rates, UV transmittance, and power levels {{in order to develop}} a <b>regression</b> <b>model</b> for system sizing. For example, this is a requirement for all drinking water systems in the United States per the EPA UV Guidance Manual.|$|E
25|$|A {{number of}} studies have {{suggested}} that subtle behavioral or personality changes may occur in infected humans, and infection with the parasite has recently been associated with a number of neurological disorders, particularly schizophrenia and bipolar disorder. A 2015 study also found cognitive deficits in adults to be associated with joint infection by both T. gondii and Helicobacter pylori in a <b>regression</b> <b>model</b> with controls for race-ethnicity and educational attainment. Although a causal relationship between latent toxoplasmosis with these neurological phenomena has not yet been established, preliminary evidence suggests that T. gondii infection can induce some of the same alterations in the human brain as those observed in mice.|$|E
50|$|These {{and other}} censored <b>regression</b> <b>models</b> are often {{confused}} with truncated <b>regression</b> <b>models.</b> Truncated <b>regression</b> <b>models</b> {{are used for}} data where whole observations are missing so that the values for the dependent and the independent variables are unknown. Censored <b>regression</b> <b>models</b> are used for data where only the value for the dependent variable (hours {{of work in the}} example above) is unknown while the values of the independent variables (age, education, family status) are still available.|$|R
40|$|This chapter {{reviews the}} {{literature}} on variable selection in nonparametric and semiparametric <b>regression</b> <b>models</b> via shrinkage. We highlight recent developments on simultaneous variable selection and estimation through the methods of least absolute shrinkage and selection operator (Lasso), smoothly clipped absolute deviation (SCAD) or their variants, but restrict our attention to nonparametric and semiparametric <b>regression</b> <b>models.</b> In particular, we consider variable selection in additive models, partially linear models, functional/varying coefficient models, single index <b>models,</b> general nonparametric <b>regression</b> <b>models,</b> and semiparametric/nonparametric quantile <b>regression</b> <b>models...</b>|$|R
40|$|In {{this paper}} we {{summarize}} the main points of beta <b>regression</b> <b>models</b> under Bayesian perspective, including {{a presentation of}} the Bayesianbetareg R-package, used to fit the beta <b>regression</b> <b>models</b> under a Bayesian approach. Finally, beta <b>regression</b> <b>models</b> are fitted to a reading score database using, respectively, the Bayesianbetareg and betareg R-Packages for Bayesian and classic perspectives...|$|R
25|$|In statistics, {{ordinary}} {{least squares}} (OLS) or linear least squares is a method for estimating the unknown parameters in a linear <b>regression</b> <b>model,</b> {{with the goal of}} minimizing the sum of the squares {{of the differences between the}} observed responses (values of the variable being predicted) in the given dataset and those predicted by a linear function of a set of explanatory variables. Visually this is seen as the sum of the squared vertical distances between each data point in the set and the corresponding point on the regression line – the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side.|$|E
2500|$|A <b>regression</b> <b>model</b> is {{a linear}} one when the model {{comprises}} a linear {{combination of the}} parameters, i.e., ...|$|E
2500|$|Incremental {{validity}} {{is usually}} assessed using multiple regression methods. A <b>regression</b> <b>model</b> with other variables is fitted {{to the data}} first and then the focal variable {{is added to the}} model. [...] A significant change in the R-square statistic (using an F-test to determine significance) is interpreted as an indication that the newly added variable offers significant additional predictive power for the dependent variable over variables previously included in the <b>regression</b> <b>model.</b>|$|E
40|$|Frontier <b>regression</b> <b>models</b> seek {{to explain}} boundary, {{frontier}} or optimal behavior rather than average behavior as in ordinary <b>regression</b> <b>models.</b> Ordinary <b>regression</b> {{is one of}} the most important tools for data mining. Frontier models may be desirable alternatives in many circumstances. In this chapter, we discuss frontier <b>regression</b> <b>models</b> and compare their interpretations to ordinary <b>regression</b> <b>models.</b> Occasional contact with stochastic frontier estimation models is also made, but we concentrate primarily on pure ceiling or floor frontier models. We also propose some guidelines for when to choose between them...|$|R
40|$|Mixture of <b>regression</b> <b>models</b> {{are used}} for {{modeling}} when the data are a mixture of subgroups to allow for heterogeneity of the population. In the presence of outliers, standard mixture of linear <b>regression</b> <b>models</b> fail. To study mixture of linear <b>regression</b> <b>models</b> in this setting, we introduce a class of robust estimators, called S-estimators. We investigate their breakdown point in mixtur...|$|R
40|$|This work proposes joint {{modeling}} of parameters in the biparametric exponential family, including heteroscedastic linear regression (non linear regression) models; with joint {{modeling of}} the mean and precision (the variance) parameters; beta <b>regression</b> <b>models,</b> longitudinal date analysis (including modeling of the covariance matrix) and hierarchical models. This work presents results of the classic approach to fit <b>regression</b> <b>models</b> for both mean and precision parameters in biparametric exponential family of distributions, which includes Bayesian methods for fitting the proposed models. And also extensions of the Bayesian methods to fit nonlinear <b>regression</b> <b>models.</b> Finally, proposes to use a Bayesian approach for modeling the covariance matrix in normal <b>regression</b> <b>models</b> when the observations are not independent. This document includes the following chapters: Chapter 1 is a introduction. Chapter 2 presents a summary of generalized linear models and the classical and Bayesian approaches to the parameters estimation, presenting the Fisher score method and a Bayesian approach using the Metropolis-Hastings algorithm. In Chapter 3, the heteroscedastic normal linear <b>regression</b> <b>models</b> are considered, including summaries of the classic method and Bayesian method proposed to fit these models. Chapter 4 {{is an extension of}} Chapter 3, which studies the <b>regression</b> <b>models</b> in the biparametric exponential family of distribution for mean and precision parameters. The following examples are included. 1. Gamma <b>regression</b> <b>models</b> with <b>regression</b> structures in the mean and precision (variance). 2. Beta <b>regression</b> <b>models</b> with <b>regression</b> structures in both mean and dispersion parameter. Several simulation studies were performed to illustrate these models and the proposed Bayesian methods. Chapter 5 discusses normal nonlinear heteroskedastic <b>regression</b> <b>models.</b> Chapter 6 include a Bayesian proposal to fit longitudinal <b>regression</b> <b>models,</b> where <b>regression</b> structures are assumed for the mean and the variance-covariance matrix of observations with Normal distribution (longitudinal data) Chapter 7 presents an extension of the methodology proposed in the previous chapters for adjusting hierarchical models. ...|$|R
2500|$|When {{only one}} {{dependent}} variable is being modeled, a scatterplot will suggest {{the form and}} strength {{of the relationship between}} the dependent variable and regressors. It might also reveal outliers, heteroscedasticity, and other aspects of the data that may complicate the interpretation of a fitted <b>regression</b> <b>model.</b> [...] The scatterplot suggests that the relationship is strong and can be approximated as a quadratic function. OLS can handle non-linear relationships by introducing the regressor HEIGHT2. [...] The <b>regression</b> <b>model</b> then becomes a multiple linear model: ...|$|E
2500|$|A fitted linear <b>regression</b> <b>model</b> {{can be used}} to {{identify}} the relationship between a single predictor variable x'j and the response variable y when all the other predictor variables in the model are [...] "held fixed". Specifically, the interpretation of β'j is the expected change in y for a one-unit change in x'j when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to x'j. This is sometimes called the unique effect of x'j on y. In contrast, the marginal effect of x'j on y can be assessed using a correlation coefficient or simple linear <b>regression</b> <b>model</b> relating only x'j to y; this effect is the total derivative of y with respect to x'j.|$|E
2500|$|Suppose {{the data}} {{consists}} of n observations {y,x}. Each observation i includes a scalar response yi and a vector of values of p predictors (regressors) xij for j = 1, ..., p. In a linear <b>regression</b> <b>model</b> the response variable is a linear {{function of the}} regressors: ...|$|E
40|$|Abstract: This study relates {{negative}} binomial and generalized Poisson <b>regression</b> <b>models</b> {{through the}} mean-variance relationship, and suggests {{the application of}} these models for overdispersed or underdispersed count data. In addition, this study relates zero-inflated negative binomial and zero-inflated generalized Poisson <b>regression</b> <b>models</b> through the mean-variance relationship, and suggests the application of these zero-inflated models for zero-inflated and overdispersed count data. The negative binomial and generalized Poisson <b>regression</b> <b>models</b> were fitted to the Malaysian OD claim count data, whereas the zero-inflated negative binomial and zero-inflated generalized Poisson <b>regression</b> <b>models</b> were fitted to the German healthcare count data...|$|R
40|$|This paper {{investigates the}} {{applicability}} of using artificial neural network (ANN) and multilinear <b>regression</b> <b>models</b> to predict urban stormwater quality at unmonitored catchments. Models were constructed using logarithmically transformed environmental data. Violation of the assumption of data independence lead to the inclusion of insignificant variables when a straightforward stepwise regression was applied. To overcome this problem, cross validation {{was used to determine}} when to stop adding variables. <b>Regression</b> <b>models</b> calibrated using event mean concentration (EMC) as the dependent variable were more accurate than those using event load. <b>Regression</b> <b>models</b> developed on a regional subset of data were more accurate than the models developed on the entire data set. Even though <b>regression</b> and ANN <b>models</b> yielded similar predictions, <b>regression</b> <b>modelling</b> was considered to be a more applicable approach. Compared to ANN <b>models,</b> <b>regression</b> <b>models</b> were faster to construct and apply, more transparent and less likely to overfit the limited data...|$|R
40|$|Abstract—A hybrid {{neural network}} <b>regression</b> <b>models</b> with {{unsupervised}} fuzzy clustering is proposed for clustering nonparametric <b>regression</b> <b>models</b> for datasets. In the new formulation, (i) the performance {{function of the}} neural network <b>regression</b> <b>models</b> is modified such that the fuzzy clustering weightings can be introduced in these network models; (ii) the errors of these network models are feed-backed into the fuzzy clustering process. This hybrid approach leads to an iterative procedure to formulate neural network <b>regression</b> <b>models</b> with optimal fuzzy membership values for each object such that the overall error of the neural network <b>regression</b> <b>models</b> can be minimized. Our testing results show that this hybrid algorithm NN-FC can handle cases that the K-means and Fuzzy C-means perform poorly. The overall training errors drop down rapidly and converge {{with only a few}} iterations. The clustering accuracy in testing period is consistent with these drops of errors and can reach up to about 100 % for some problems that the other classical fuzzy clustering algorithms perform poorly with about accuracy of 60 % only. Our algorithm can also build <b>regression</b> <b>models,</b> which has the advantage of the NN component, being non-parametric and thus more flexible than the fuzzy c-regression. Index Terms—Neural network, <b>regression</b> <b>models,</b> fuzzy clustering, fuzzy performance function. I...|$|R
2500|$|Errors-in-variables models (or [...] "measurement error models") {{extend the}} {{traditional}} linear <b>regression</b> <b>model</b> {{to allow the}} predictor variables X to be observed with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.|$|E
2500|$|... {{is called}} the error term, {{disturbance}} term, or noise. This variable captures all other factors which influence the dependent variable y'i other than the regressors x'i. The relationship between the error term and the regressors, for example whether they are correlated, is a crucial step in formulating a linear <b>regression</b> <b>model,</b> as it will determine the method to use for estimation.|$|E
2500|$|Given a {{data set}} [...] of n {{statistical}} units, a linear <b>regression</b> <b>model</b> {{assumes that the}} relationship between the dependent variable yi and the p-vector of regressors xi is linear. This relationship is modeled through a disturbance term or error variable εi — an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors. Thus the model takes the form ...|$|E
40|$|Although {{databases}} used in {{many organizations}} {{have been found to}} contain errors, {{little is known about the}} effect of these errors on predictions made by linear <b>regression</b> <b>models.</b> The paper uses a real-world example, the prediction of the net asset values of mutual funds, to investigate the effect of data quality on linear <b>regression</b> <b>models.</b> The results of two experiments are reported. The first experiment shows that the error rate and magnitude of error in data used in model prediction negatively affect the predictive accuracy of linear <b>regression</b> <b>models.</b> The second experiment shows that the error rate and the magnitude of error in data used to build the model positively affect the predictive accuracy of linear <b>regression</b> <b>models.</b> All findings are statistically significant. The findings have managerial implications for users and builders of linear <b>regression</b> <b>models...</b>|$|R
40|$|Collapsibility {{with respect}} to a measure of {{association}} implies that the measure of association can be obtained from the marginal model. We first discuss model collapsibility and collapsibility {{with respect to}} regression coefficients for linear <b>regression</b> <b>models.</b> For parallel <b>regression</b> <b>models,</b> we give simple and different proofs of some of the known results and obtain also certain new results. For random coefficient <b>regression</b> <b>models,</b> we define (average) A-collapsibility and obtain conditions under which it holds. We consider Poisson regression and logistic <b>regression</b> <b>models</b> also, and derive conditions for collapsibility and A-collapsibility, respectively. These results generalize some of the results available in the literature. Some suitable examples are also discussed. (C) 200...|$|R
40|$|In the {{presence}} of nuisance parameters, we discuss a one-parameter Bayesian analysis based on a pseudo-likelihood assuming a default prior distribution for the parameter of interest only. Although this way to proceed cannot always be considered as orthodox in the Bayesian perspective, it is of interest to evaluate whether the use of suitable pseudo-likelihoods may be proposed for Bayesian inference. Attention is focused {{in the context of}} <b>regression</b> <b>models,</b> in particular on inference about a scalar regression coefficient in various multiple <b>regression</b> <b>models,</b> i. e. scale and <b>regression</b> <b>models</b> with non-normal errors, non-linear normal heteroscedastic <b>regression</b> <b>models,</b> and log-linear models for count data with overdispersion. Some interesting conclusions emerge...|$|R
