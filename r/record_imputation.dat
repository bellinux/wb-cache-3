1|9|Public
40|$|This Technical Paper {{describes}} the processes {{for creating a}} key input dataset for the Regional Dimensions project. This input dataset is the so-called HES Linkage File, which feeds into the 'reweighting' process for converting a set of national household weights into small-area (SLA) household weights. The processes described here prepare the selected variables from the ABS 1998 - 99 Household Expenditure Survey (HES) for matching to the Census data, in order for 'reweighting' to take place. The preparations included recoding, uprating, and <b>record</b> <b>imputation</b> of person-level, family-level, and household-level variables; {{as well as the}} creation of unit records for children and non-private dwelling residents. These preparations involved a suite of SAS programs, which, together with a data dictionary, are also documented in this paper. reweighting, small-area statistics, spatial microsimulation...|$|E
40|$|We {{describe}} an algorithm for the fitting of multivariate responses using classification and regression trees, named the intersection-seeking algorithm. Although motivated by problems of <b>record</b> linkage and <b>imputation</b> of missing values in surveys, the algorithm {{may be used}} in other contexts. File completion Imputation Regression and classification trees...|$|R
40|$|Analyses {{and data}} mining of large {{computer}} files {{are affected by}} the quality of the information in the files. For large population registers and for files that are created by merging two or more files, duplicate entries must be identified. Duplicate identification can depend on record linkage software that can deal with name, address, and date-of-birth data containing many typographical errors. Quantitative and qualitative data must be edited to assure that mutually contradictory or missing items are changed automatically and quickly. This paper describes computational methods and software that are suitable for groups of files where individual files contain between 1 million and 4 billion records. Keywords: <b>record</b> linkage, editing, <b>imputation,</b> data mining 1...|$|R
40|$|In {{this paper}} we propose a new method of single imputation, reconstruction, and {{estimation}} of non-reported, incorrect or excluded values both in the target and in the auxiliary variables where the first is on ratio or interval scale and the last are heterogeneous in measurement scale. Our technique is a variation of the popular nearest neighbor hot deck imputation (NNHDI) where "nearest" is defined in terms of a global distance obtained as a convex combination of the partial distance matrices computed for the various types of variables. In particular, we address the problem of proper weighting the partial distance matrices in order to reflect their significance, reliability and statistical adequacy. Performance of several weighting schemes is compared under a variety of settings in coordination with imputation of the least power mean. We have demonstrated, through analysis of simulated and actual data sets, the appropriateness of this approach. Our main contribution has been to show that mixed data may optimally be combined to allow accurate reconstruction of missing values in the target variable {{even in the absence of}} some data in the other fields of the <b>record.</b> hot-deck <b>imputation,</b> nearest neighbor, general distance coefficient, least power mean...|$|R
40|$|ABSTRACT: Most industrialised {{societies}} face {{rapid population}} ageing {{over the next}} two decades, including sharp increases {{in the number of people}} aged 85 years and over. As a result, the supply of and demand for aged care services has assumed increasing policy prominence. The likely spatial distribution of the need for aged care services is critical for planners and policy makers. This article describes the development of a regional microsimulation model of the need for aged care in New South Wales, a state of Australia. It details the methods involved in reweighting the 1998 Survey of Disability, Ageing and Carers, a national level dataset, against the 2001 Census to produce synthetic small area estimates at the statistical local area level. Validation shows that survey variables not constrained in the weighting process can provide unreliable local estimates. A proposed solution to this problem is outlined, involving <b>record</b> cloning, value <b>imputation</b> and alignment. Indicative disability estimates arising from this process are then discussed...|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. The 2001 Individual Licenced Sample of Anonymised <b>Records</b> for <b>Imputation</b> Analysis (I-SAR) is a 3 % sample of individuals for all countries of the United Kingdom, with approximately 1. 84 million records. The data are available for England, Wales, Scotland and Northern Ireland. Information is included for each individual on the main demographic, health, socio-economic and household variables. The 3 % sample is an increase by comparison with 2 % in 1991. Some variables have been broad-banded to reduce disclosure risk. The lowest level of geography is the Government Office Region (GOR), although Inner and Outer London are separately identified. This represents a significant reduction by comparison with the 1991 where large Local Authorities (population 120, 000 and over) were separately identified. This dataset contains 173 variables, including 84 imputation flag variables. The standard version, containing 89 I-SAR variables, is available under SN 7205. Main Topics : The dataset includes information on age, gender, ethnicity, health, employment status, housing, amenities, family type, geography, social class, education, distance to work, workplace, hours worked and migration. In addition, the ONS have added occupational coding, not available in the Census tables, for individuals aged 16 - 65 who last worked more than 5 years ago but less than ten years ago and for those aged 65 - 74 who were not currently working at the Census but who {{had worked in the}} previous ten years. A further 84 imputation flag variables are also included...|$|R
40|$|Most industrialised {{societies}} face {{rapid population}} ageing {{over the next}} two decades, including sharp increases {{in the number of people}} aged 85 years and over. As a result, the supply of and demand for aged care services has assumed increasing policy prominence. The likely spatial distribution of the need for aged care services is critical for planners and policy makers. This article describes the development of a regional microsimulation model of the need for aged care in New South Wales, a state of Australia. It details the methods involved in reweighting the 1998 Survey of Disability, Ageing and Carers, a national level dataset, against the 2001 Census to produce synthetic small area estimates at the statistical local area level. Validation shows that survey variables not constrained in the weighting process can provide unreliable local estimates. A proposed solution to this problem is outlined, involving <b>record</b> cloning, value <b>imputation</b> and alignment. Indicative disability estimates arising from this process are then discussed. Disability, ageing, spatial analysis, aged care, cloning; imputation; alignment; NATSEM...|$|R
40|$|In {{studies that}} use {{electronic}} health <b>record</b> data, <b>imputation</b> of important data {{elements such as}} Glycated hemoglobin (A 1 c) has become common. However, few studies have systematically examined the validity of various imputation strategies for missing A 1 c values. We derived a complete dataset using an incident diabetes population that has no missing values in A 1 c, fasting and random plasma glucose (FPG and RPG), age, and gender. We then created missing A 1 c values under two assumptions: missing completely at random (MCAR) and missing at random (MAR). We then imputed A 1 c values, compared the imputed values to the true A 1 c values, and used these data {{to assess the impact}} of A 1 c on initiation of antihyperglycemic therapy. Under MCAR, imputation of A 1 c based on FPG 1) estimated a continuous A 1 c within ± 1. 88 % of the true A 1 c 68. 3 % of the time; 2) estimated a categorical A 1 c within ± one category from the true A 1 c about 50 % of the time. Including RPG in imputation slightly improved the precision but did not improve the accuracy. Under MAR, including gender and age in addition to FPG improved the accuracy of imputed continuous A 1 c but not categorical A 1 c. Moreover, imputation of up to 33 % of missing A 1 c values did not change the accuracy and precision and did not alter the impact of A 1 c on initiation of antihyperglycemic therapy. When using A 1 c values as a predictor variable, a simple imputation algorithm based only on age, sex, and fasting plasma glucose gave acceptable results...|$|R
40|$|The {{problem of}} record linkage is to {{identify}} records from two datasets, which refer to the same entities (e. g. patients). A particular issue of record linkage {{is the presence of}} missing values in records, which has not been fully addressed. Another issue is how privacy and confidentiality can be preserved in the process of record linkage. In this paper, we propose an approach for privacy preserving record linkage in the presence of missing values. For any missing value in a record, our approach imputes the similarity measure between the missing value and the value of the corresponding field in any of the possible matching records from another dataset. We use the k-NNs (k Nearest Neighbours in the same dataset) of the record with the missing value and their distances to the <b>record</b> for similarity <b>imputation.</b> For privacy preservation, our approach uses the Bloom filter protocol in the settings of both standard privacy preserving record linkage without missing values and privacy preserving record linkage with missing values. We have conducted an experimental evaluation using three pairs of synthetic datasets with different rates of missing values. Our experimental results show the effectiveness and efficiency of our proposed approach...|$|R
40|$|Genotype {{imputation}} {{has been}} used to increase genomic information, allow more animals in genome-wide analyses, and reduce genotyping costs. In Brazilian beef cattle production, many animals are resulting from crossbreeding and such an event may alter linkage disequilibrium patterns. Thus, the challenge is to obtain accurately imputed genotypes in crossbred animals. The objective {{of this study was to}} evaluate the best fitting and most accurate imputation strategy on the MA genetic group (the progeny of a Charolais sire mated with crossbred Canchim X Zebu cows) and Canchim cattle. The data set contained 400 animals (born between 1999 and 2005) genotyped with the Illumina BovineHD panel. Imputation accuracy of genotypes from the Illumina-Bovine 3 K (3 K), Illumina-BovineLD (6 K), GeneSeek-Genomic-Profiler (GGP) BeefLD (GGP 9 K), GGP-IndicusLD (GGP 20 Ki), Illumina-BovineSNP 50 (50 K), GGP-IndicusHD (GGP 75 Ki), and GGP-BeefHD (GGP 80 K) to Illumina-BovineHD (HD) SNP panels were investigated. Seven scenarios for reference and target populations were tested; the animals were grouped according with birth year (S 1), genetic groups (S 2 and S 3), genetic groups and birth year (S 4 and S 5), gender (S 6), and gender and birth year (S 7). Analyses were performed using FImpute and BEAGLE software and computation run-time was <b>recorded.</b> Genotype <b>imputation</b> accuracy was measured by concordance rate (CR) and allelic R square (R(2)). The highest imputation accuracy scenario consisted of a reference population with males and females and a target population with young females. Among the SNP panels in the tested scenarios, from the 50 K, GGP 75 Ki and GGP 80 K were the most adequate to impute to HD in Canchim cattle. FImpute reduced computation run-time to impute genotypes from 20 to 100 times when compared to BEAGLE. The genotyping panels possessing at least 50 thousands markers are suitable for genotype imputation to HD with acceptable accuracy. The FImpute algorithm demonstrated a higher efficiency of imputed markers, especially in lower density panels. These considerations may assist to increase genotypic information, reduce genotyping costs, and aid in genomic selection evaluations in crossbred animals...|$|R

