0|3698|Public
40|$|This chapter {{describes}} {{the area of}} human-computer <b>interaction</b> <b>technique</b> research in general and then describes research in several new types of <b>interaction</b> <b>techniques</b> under way at the Human-Computer Interaction Laboratory of the U. S. Naval Research Laboratory: eye movement-based <b>interaction</b> <b>techniques,</b> three-dimensional pointing, and, finally, using dialogue properties in <b>interaction</b> <b>techniques...</b>|$|R
40|$|UNiversity of Minnesota Ph. D. dissertation. July 2014. Major: Computer Science. Advisor:Daniel F. Keefe. 1 {{computer}} file (PDF); xi, 176 pages. This dissertation explores spatial human-computer <b>interaction</b> <b>techniques</b> {{to improve the}} control and expressiveness of 3 D interactions. It investigates the requirements necessary for users to work more effectively with next-generation spatial interfaces, specifically {{in the context of}} scientific visualization and artistic 3 D modeling where users currently struggle to express complex spatial concepts. Examples of expressive spatial interfaces are presented and evaluated. In particular, we present new techniques for combining multi-touch with free-hand gestures for navigating visualizations and performing 3 D surface modeling operations. Techniques for selecting and filtering volumetric data using lightweight props as well as active force-feedback are also introduced. Additionally, we present a spatial modeling interface for artistic 3 D modeling using contextual interpretation of the user's input. Several conclusions are drawn from these examples. Rich, parallel input and output streams enabled by recent advances in tracking hardware are particularly important for expressive interfaces. Additionally, {{there is a need for}} tighter integration of two and three-dimensional data and input. Contextual interpretation of user input enables users to specify more complex 3 D concepts. Finally, many spatial tasks require immediate feedback to be expressive. The primary contribution of this dissertation is a new class of <b>interaction</b> <b>techniques</b> called Expressive Spatial Interfaces that advance beyond the limited pointing and <b>rotating</b> <b>interactions</b> common in current-generation spatial interfaces. The techniques presented here can have a powerful impact on shaping the future of expressive spatial human-computer interaction with 3 D graphics...|$|R
40|$|This paper {{focuses on}} the {{evaluation}} of virtual reality (VR) <b>interaction</b> <b>techniques</b> for exploration of data warehouse (DW). The experimental DW involves hierarchical levels and contains information about customers profiles and related purchase items. A user study {{has been carried out}} to compare two navigation and selection techniques. Sixteen volunteers were instructed to explore the DW and look for information using the <b>interaction</b> <b>techniques,</b> involving either a single WiimoteTM (monomanual) or both WiimoteTM and NunchuckTM (bimanual). Results indicated that the bimanual <b>interaction</b> <b>technique</b> is more efficient in terms of speed and error rate. Moreover, most of the participants preferred the bimanual <b>interaction</b> <b>technique</b> and found it more appropriate for the exploration task. We also observed that males were faster and made less errors than females for both <b>interaction</b> <b>techniques...</b>|$|R
40|$|This chapter {{describes}} {{the area of}} human-computer <b>interaction</b> <b>technique</b> research in general and then describes research in several new types of <b>interaction</b> <b>techniques</b> under way at the Human-Computer Interaction Laboratory of the U. S. Naval Research Laboratory: eye movement-based <b>interaction</b> <b>techniques,</b> three-dimensional pointing, and, finally, using dialogue properties in <b>interaction</b> <b>techniques.</b> Keywords. human-computer <b>interaction,</b> <b>interaction</b> <b>techniques,</b> eye movements, gesture, pointing, dialogue 1 Introduction Tufte [9] has described human-computer interaction as two powerful information processors (human and computer) attempting {{to communicate with each}} other via a narrow-bandwidth, highly constrained interface. A fundamental goal of research in human-computer interaction is, therefore, to increase the useful bandwidth across that interface. A significant bottleneck in the effectiveness of educational systems as well as other interactive systems is this communication path betw [...] ...|$|R
40|$|Abstract—Even though {{interaction}} {{is an important}} part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis <b>interaction</b> <b>techniques</b> exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of <b>interaction</b> <b>techniques</b> widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user’s intent while interacting with a system rather than the low-level <b>interaction</b> <b>techniques</b> provided by a system. The categories can act as a framework to help discuss and evaluate <b>interaction</b> <b>techniques</b> and hopefully lay an initial foundation toward a deeper understanding and a science of interaction. Index Terms—Information visualization, <b>interaction,</b> <b>interaction</b> <b>techniques,</b> taxonomy, visual analytics...|$|R
40|$|Abstract. This paper {{presents}} an analysis, implementation {{and evaluation of}} the physical mobile <b>interaction</b> <b>techniques</b> touching, pointing and scanning. Based on this we have formulated guidelines that show in which context which <b>interaction</b> <b>technique</b> is preferred by the user. Our main goal was to identify typical situations and scenarios in which the different techniques might be useful or not. In support of these aims we have developed and evaluated, within a user study, a low-fidelity and a high-fidelity prototype to assess scanning, pointing and touching <b>interaction</b> <b>techniques</b> within different contexts. Other work has shown that mobile devices can act as universal remote controls for interaction with smart objects but, to date, {{there has been no}} research which has analyzed when a given mobile <b>interaction</b> <b>technique</b> should be used. In this research we analyze the appropriateness of three <b>interaction</b> <b>techniques</b> as selection techniques in smart environments. ...|$|R
40|$|There are two typical {{approaches}} {{to the design of}} three-dimensional (3 D) <b>interaction</b> <b>techniques</b> for immersive virtual environments (VEs). Many 3 D <b>interaction</b> <b>techniques</b> are designed for generic user tasks such as navigation, selection, manipulation, and system control, without consideration of the domain-of-use. Despite a long history in HCI theory and practice of using domain knowledge for system-level design, it has not often been used for the design of <b>interaction</b> <b>techniques.</b> Other 3 D <b>interaction</b> <b>techniques</b> are designed for specific applications. While these techniques may be quite usable and useful in that single application, their reuse is limited. In this paper, we propose a middle ground: domain-specific design (DSD) of 3 D <b>interaction</b> <b>techniques.</b> The goal of DSD is to improve upon current practice so that 3 D <b>interaction</b> <b>techniques</b> are designed to address real-world tasks, while still allowing for reuse of the techniques within a particular domain. A three-level design framework provides a theoretical basis for DSD, and we show how this framework can be used to illustrate multiple paths for the design of domain-specific <b>interaction</b> <b>techniques.</b> We also provide a comprehensive, practical case study of an actual VE system, Virtual-SAP (structure analysis program), to illustrate how the approach is applied. Experimental results demonstrate that the use of the DSD approach increases the usefulness of this application, without sacrificing usability. Keywords: 3 D interaction, domain-specific interaction, design theory, virtual environment...|$|R
50|$|<b>Interaction</b> <b>techniques</b> are {{the glue}} between {{physical}} I/O devices and interaction tasks or domain objects. Different types of <b>interaction</b> <b>techniques</b> {{can be used}} to map a specific device to a specific domain object. For example, different gesture alphabets exist for pen-based text input.|$|R
40|$|This paper {{introduces}} a novel kinesthetic <b>interaction</b> <b>technique</b> for interactive floors. The <b>interaction</b> <b>techniques</b> utilize vision-based limb tracking on an interactive floor – a 12 m² glass surface with bottom projection. The kinesthetic <b>interaction</b> <b>technique</b> {{has been developed}} for an interactive floor implemented in a school square. The paper discusses the kinesthetic <b>interaction</b> <b>technique</b> and its potentials {{in the domain of}} learning applications: Kinesthetic interaction supports body-kinesthetic learning as argued in the learning literature. Kinesthetic interaction is fun and motivating thus encourages children to explore and learn. Kinesthetic interaction on large display surfaces supports collaborative, co-located play and learning through communication and negotiation among the participants. Finally, the paper discusses prospects and challenges in development of kinesthetic interaction for interactive floors...|$|R
50|$|A {{large part}} of {{research}} in human-computer interaction involves exploring easier-to-learn or more efficient <b>interaction</b> <b>techniques</b> for common computing tasks. This includes inventing new (post-WIMP) <b>interaction</b> <b>techniques,</b> possibly relying on methods from user interface design, and assessing their efficiency with respect to existing techniques using methods from experimental psychology. Examples of scientific venues in these topics are the UIST and the CHI conferences. Other research focuses on the specification of <b>interaction</b> <b>techniques,</b> sometimes using formalisms such as Petri nets {{for the purposes of}} formal verification.|$|R
5000|$|From the computer's perspective, an <b>interaction</b> <b>technique</b> involves: ...|$|R
5000|$|... #Subtitle level 2: Comparison {{with other}} <b>interaction</b> <b>techniques</b> ...|$|R
40|$|The <b>interaction</b> <b>techniques</b> {{that are}} used in {{tabletop}} groupware systems (such as pick-and-drop or pantograph) can affect the way that people collaborate. However, little is known about these effects, making it difficult for designers to choose appropriate techniques when building tabletop groupware. We carried out an exploratory study to determine how several different types of <b>interaction</b> <b>techniques</b> (pantograph, telepointers, radar views, drag-and-drop, and laser beam) affected coordination and awareness in two tabletop tasks (a game and a storyboarding activity). We found that the choice of <b>interaction</b> <b>technique</b> significantly affected coordination measures, performance measures, and preference – but that the effects were different for the two different tasks. Our study shows that the choice of tabletop <b>interaction</b> <b>technique</b> does indeed matter, and provides insight into how tabletop systems can better support group work...|$|R
40|$|In this {{position}} {{paper we discuss}} the usage of various interaction technologies with focus on the presentations of 3 D visualizations involving a presenter and an audience. While an <b>interaction</b> <b>technique</b> is commonly evaluated from a user perspective, we want to shift the focus from a sole analysis of the naturalness and the ease-of-use for the user, to focus on how expressive and understandable the <b>interaction</b> <b>technique</b> is when witnessed by the audience. The interaction process itself can {{be considered to be}} a communication channel and a more expressive <b>interaction</b> <b>technique</b> might {{make it easier for the}} audience to comprehend the presentation. Thus, while some natural <b>interaction</b> <b>techniques</b> for interactive visualization are easy to perform by the presenter, they may be less beneficial when interacting with the visualization in front of (and for) an audience. Our observations indicate that the suitability of an <b>interaction</b> <b>technique</b> as a communication channel is highly dependent on the setting in which the interaction takes place. Therefore, we analyze different presentation scenarios in an exemplary fashion and discuss how beneficial and comprehensive the involved techniques are for the audience. We argue that <b>interaction</b> <b>techniques</b> complement the visualization in an interactive presentation scenario as they also serve as an important communication channel, and should therefore also be observed from an audience perspective rather than exclusively a user perspective...|$|R
30|$|In the {{following}} subsections, we firstly describe the related work concerning the chord <b>interaction</b> <b>technique</b> {{and the personal}} windows interface and then, we demonstrate {{the need for a}} toolkit that can handle these multi-user multi-touch techniques and describe what experimental task is needed in order to evaluate this multi-user multi-touch <b>interaction</b> <b>techniques.</b>|$|R
40|$|Researchers have {{developed}} interaction concepts based on mobile projectors. Yet pursuing {{work in this}} area— particularly in building projector-based <b>interactions</b> <b>techniques</b> within an application—is cumbersome and timeconsuming. To mitigate this problem, we contribute ProjectorKit, a flexible open-source toolkit that eases rapid prototyping mobile projector <b>interaction</b> <b>techniques.</b> Author Keywords Mobile projectors, toolkit, rapid prototyping...|$|R
40|$|We {{present a}} set of <b>interaction</b> <b>techniques</b> for {{electronic}} musical performance using a tabletop tangible interface. Our system, the Audiopad, tracks the positions of objects on a tabletop surface and translates their motions into commands for a musical synthesizer. We developed and refi ned these <b>interaction</b> <b>techniques</b> through an iterative design process, in which new <b>interaction</b> <b>techniques</b> were periodically evaluated through performances and gallery installations. Based on our experience refi ning the design of this system, we conclude that tabletop interfaces intended for collaborative use should use <b>interaction</b> <b>techniques</b> designed to be legible to onlookers. We also conclude that these interfaces should allow users to spatially reconfi gure the objects in the interface {{in ways that are}} personally meaningful. Categories and Subject Descriptors H. 5. 2 [User Interfaces]: interaction styles, input devices and strategies J. 5 : [Arts and Humanities]: performing art...|$|R
40|$|<b>Interaction</b> <b>Techniques</b> in Virtual Environments Doug A. Bowman, Chadwick A. Wingrave, Joshua M. Campbell, and Vinh Q. Ly Department of Computer Science (0106) Virginia Tech Blacksburg, VA 24061 USA {bowman, cwingrav, jocampbe, vly}@vt. edu Abstract Usable {{three-dimensional}} (3 D) <b>interaction</b> <b>techniques</b> {{are difficult}} to design, implement, and evaluate. One {{reason for this is}} a poor understanding of {{the advantages and disadvantages of}} the wide range of 3 D input devices, and of the mapping between input devices and <b>interaction</b> <b>techniques.</b> We present an analysis of Pinch Gloves^TM and their use as input devices for virtual environments (VEs). We have developed a number of novel and usable <b>interaction</b> <b>techniques</b> for VEs using the gloves, including a menu system, a technique for text input, and a two-handed navigation technique. User studies have indicated the usability and utility of these techniques. ...|$|R
40|$|International audienceThis paper {{introduces}} a new device model {{and a new}} <b>interaction</b> <b>technique</b> model to deal with plasticity issues for Virtual Reality (VR) and Augmented Reality (AR). We aim to provide developers with solutions to use and create <b>interaction</b> <b>techniques</b> that will fit to the needed tasks of a 3 D application and to the input and output devices available. The device model {{introduces a}} new description of inputs and outputs devices that includes capabilities, limitations and representations in the real world. We also propose {{a new way to}} develop <b>interaction</b> <b>techniques</b> with an approach based on PAC and ARCH models. These techniques are implemented independently of the concrete devices used thanks to the proposed device model. Moreover, our approach aims to facilitate the portability of <b>interaction</b> <b>techniques</b> over different target OS and 3 D framework...|$|R
40|$|We {{present a}} study which {{describes}} the power consumption {{characteristics of a}} number of different <b>interaction</b> <b>techniques</b> on a desktop and laptop computer. In total, 8 interactions {{that can be used to}} carry out a single task (navigating a PDF document) were compared for power consumption across both a desktop and a laptop computer and across two different power saver settings. The results suggest that the power consumption of different <b>interaction</b> <b>techniques</b> for a single task vary significantly. Furthermore, the results suggest that a key factor in the power consumption of the <b>interaction</b> <b>technique</b> is the number of screen updates involved...|$|R
40|$|We present 3 dml, a markup {{language}} for 3 D <b>interaction</b> <b>techniques</b> and virtual environment applications that involve non-traditional devices. 3 dml has two main purposes: readability and rapid development. Designers can read 3 dml-based representations of 3 D <b>interaction</b> <b>techniques,</b> compare them, and understand them. 3 dml {{can also be}} used as a front end for any VR toolkit, so designers without programming skills can create VR applications as 3 dml documents that plug together <b>interaction</b> <b>techniques,</b> VR objects, and devices. This paper focuses on the language features and presentation scheme designed in our websit...|$|R
40|$|Mixed-reality {{games have}} the {{potential}} to let users play in the world surrounding them. However, to exploit this new approaches to game content creation, content presentation <b>techniques</b> and <b>interaction</b> <b>techniques</b> are required. In this paper we explore the potential of computer-vision on mobile devices with a camera as an interaction modality. Based on a theoretical review of the available design space potential <b>interaction</b> <b>techniques</b> are discussed. Some of these were implemented in an experimental game to enable practical evaluation. We provide an overview of the game and present intial experiences with the vision-based <b>interaction</b> <b>techniques</b> employed...|$|R
40|$|This paper {{presents}} {{a description of}} the <b>interaction</b> <b>techniques</b> used in the Chapel Hill Immersive Modeling Program (CHIMP). CHIMP is intended for the preliminary stages of architectural design. It is an immersive system; users work directly within a virtual world. The main goal has been to develop <b>interaction</b> <b>techniques</b> that exploit the benefits of working immersed while compensating for its limitations. <b>Interaction</b> <b>techniques</b> described and discussed in this paper include: • Action at a distance • Look-at menus • Remote controls (hand-held widgets) • Constrained object manipulation using twohands • Two-handed control panel interaction • Worlds in miniature • Interactive number...|$|R
40|$|This {{position}} paper presents a model based approach supporting development of advanced user interfaces for the design, simulation, tuning and {{the assessment of}} <b>interaction</b> <b>techniques.</b> It {{is based on a}} double concept: the introduction of additional information in models to allow designer to tune easily the <b>interaction</b> <b>technique</b> and the use of simulation and logging facilities to assess perform performance evaluation of the models. It proposes an alternative to user testing which is very difficult to setup and interpret when advanced <b>interaction</b> <b>techniques</b> are concerned. Author Keywords Model-Based approaches, formal description techniques, performance evaluation, multimodal interfaces, interactive software engineering, tuning...|$|R
40|$|Usable {{three-dimensional}} (3 D) <b>interaction</b> <b>techniques</b> {{are difficult}} to design, implement, and evaluate. One {{reason for this is}} a poor understanding of {{the advantages and disadvantages of}} the wide range of 3 D input devices, and of the mapping between input devices and <b>interaction</b> <b>techniques.</b> We present an analysis of Pinch Gloves™ and their use as input devices for virtual environments (VEs). We have developed a number of novel and usable <b>interaction</b> <b>techniques</b> for VEs using the gloves, including a menu system, a technique for text input, and a two-handed navigation technique. User studies have indicated the usability and utility of these techniques...|$|R
40|$|This article {{summarizes}} the process I have developed to describe, evaluate and facilitate {{the creation of}} novel <b>interaction</b> <b>techniques.</b> First, it presents the CIS model for describing <b>interaction</b> <b>techniques</b> and predicting their effectiveness in real contexts of use. CIS shows {{that there is no}} absolute best technique but that performance depends on the context of use. The article then shows how to improve a technique by optimizing subcomponents of its CIS structure. Finally it describes SwingStates, a toolkit designed to help develop novel <b>interaction</b> <b>techniques</b> by exploring different CIS structures. ACM Classification: D. 2. 2 [Design tools and Techniques]...|$|R
40|$|We {{describe}} {{a demonstration of}} four novel <b>interaction</b> <b>techniques</b> for a cubic head-coupled 3 D display. The interactions illustrated include: viewing a static scene, navigating through a large landscape, playing with colliding objects inside a box, and stylus-based manipulation of objects. Users experience new <b>interaction</b> <b>techniques</b> for 3 D scene manipulation in a cubic display...|$|R
40|$|Designing {{non-traditional}} {{user interfaces}} is a challenging task for designers. NiMMiT, {{a high level}} description for 3 D multimodal interaction in virtual environments, provides a means to design, prototype or communicate about <b>interaction</b> <b>techniques.</b> The focus is on {{making it possible for}} designers to create new <b>interaction</b> <b>techniques</b> while lowering implementation efforts. status: publishe...|$|R
40|$|I present mouse-based, symmetric, bimanual <b>interaction</b> <b>techniques</b> as a {{solution}} to both the lack of spatial input and the lack of natural <b>interaction</b> <b>techniques</b> for direct manipulation in desktop interfaces. I outline the techniques I have implemented and tested thus far and the techniques and interfaces yet to be developed as part of my doctoral thesis...|$|R
40|$|This paper {{introduces}} new <b>interaction</b> <b>techniques</b> for Smart-Skin, {{a sensor}} architecture for freehand manipulation. This sensor recognizes multiple hand positions and shapes and calculates {{the distance between}} the hand and the surface by using capacitive sensing and a mesh-shaped antenna. Our <b>interaction</b> <b>techniques</b> enable the users to use not only their hands, but also their fingers concurrently...|$|R
50|$|In general, {{the less}} {{compatible}} {{the device is}} with the domain object, the more complex the <b>interaction</b> <b>technique.</b> For example, using a mouse to specify a 2D point involves a trivial <b>interaction</b> <b>technique,</b> whereas using a mouse to rotate a 3D object requires more creativity to design the technique and more lines of code to implement it.|$|R
40|$|As {{immersive}} {{virtual environment}} (VE) applications become more complex, {{it is clear}} that we need a firm understanding of the principles of VE interaction. In particular, designers need guidance in choosing three-dimensional <b>interaction</b> <b>techniques.</b> In this paper, we present a systematic approach, testbed evaluation, for the assessment of <b>interaction</b> <b>techniques</b> for VEs. Testbed evaluation uses formal frameworks and formal experiments with multiple independent and dependent variables in order to obtain a wide range of performance data for VE <b>interaction</b> <b>techniques.</b> We present two testbed experiments, covering techniques for the common VE tasks of travel and object selection/manipulation. The results of these experiments allow us to form general guidelines for VE interaction, and to provide an empirical basis for choosing <b>interaction</b> <b>techniques</b> in VE applications. This has been shown to produce measurable usability gains in a real-world VE application. 1. INTRODUCTION Applications of imm [...] ...|$|R
40|$|Gummi is an <b>interaction</b> <b>technique</b> and device concept {{based on}} {{physical}} deformation of a handheld device. The device consists of {{several layers of}} flexible electronic components, including sensors measuring deformation of the device. Users interact with this device {{by a combination of}} bending and 2 D position control. Gummi explores physical <b>interaction</b> <b>techniques</b> and screen interfaces for such a device. Its graphical user interface facilitates a wide range of interaction tasks, focused on browsing of visual information. We implemented both hardware and software prototypes to explore and evaluate the proposed <b>interaction</b> <b>techniques.</b> Our evaluations have shown that users can grasp Gummi's key interaction principles within minutes. Gummi demonstrates promising possibilities for new <b>interaction</b> <b>techniques</b> and devices based on flexible electronic components. Author Keywords Handheld devices, mobile computing, interaction design, GUI, embodied interaction, flexible electronics, smartcards. ACM Classification Keywords H 5. 2. [Information interfaces and presentation (e. g., HCI) ]...|$|R
40|$|Despite the {{increasing}} prevalence of Augmented Reality (AR) interfaces, {{there is still}} a lack of <b>interaction</b> <b>techniques</b> that allow full utilization of the medium. Natural hand interaction has the potential to offer these affordances however, as of yet, has not been well explored. The aim of this thesis is to improve the understanding of natural hand interaction and ultimately create a novel natural hand <b>interaction</b> <b>technique</b> that enhances user experience when interacting with AR. To better understand natural hand interaction, two prototype AR systems featuring environmental awareness and physical simulation were developed, one featuring interaction on a tabletop, and the other in a mobile tablet setting. Observations and feedback from public demonstrations of the systems were collected, and it was found that users felt that interacting physically using their hands and other tangible objects was natural and intuitive. Following this, a guessability study was conducted to elicit hand gestures for AR and obtain qualitative feedback from users in a video-see through head mounted display (HMD). From the results, a user-defined gesture set was created to guide the design of natural hand interaction for AR. Utilizing this deeper understanding and set of design guidelines, a gesture interface was developed that enabled hand tracking and gesture recognition based on depth sensing input. An AR framework that supports natural interaction as the primary input, called G-SIAR, was created, and a novel direct manipulation natural hand <b>interaction</b> <b>technique,</b> Grasp-Shell (G-Shell), was developed. This <b>interaction</b> <b>technique</b> was validated by comparing it to a traditional indirect manipulation gesture and speech <b>interaction</b> <b>technique,</b> Gesture-Speech (G-Speech), in a usability study. From the study, we gained insights into {{the strengths and weaknesses of}} each <b>interaction</b> <b>technique.</b> We found impacts on performance, usability, and user preference when comparing G-Shell’s direct interaction, where the user physically manipulates the object they are interacting with, and G-Speech’s indirect interaction, where the user interacts with the object remotely using gestures and speech commands, depending on the task. We concluded that these <b>interaction</b> <b>techniques</b> were complementing each other and should be offered together. The primary contributions of this thesis include a literature review of AR and its <b>interaction</b> <b>techniques,</b> the implementation of two AR systems and findings from the public demonstrations, findings from a guessability study on hand gestures for AR, the design and development of gesture interface and multimodal AR framework, and the design and evaluation of two natural <b>interaction</b> <b>techniques,</b> G-Shell and G-Speech. This research offers knowledge gained into natural hand interaction for AR and forms a new layer of foundation for research into <b>interaction</b> <b>techniques</b> in AR...|$|R
40|$|Part 1 : Research PapersInternational audienceA {{large-scale}} dynamic runtime {{deployment of}} existing and future <b>interaction</b> <b>techniques</b> remains an enduring challenge for engineering real-world pervasive computing ecosystems (ambient spaces). The need for innovative engineering solutions to tackle this issue increases, {{due to the}} ever expanding landscape of novel natural <b>interaction</b> <b>techniques</b> proposed every year to enrich interactive eco-systems with multitouch gestures, motion gestures, full body in motion, etc. In this paper, we discuss the implementation of Interaction Plugins as a possible solution to address this challenge. The discussed approach enables <b>interaction</b> <b>techniques</b> to be constructed as standalone dynamically deployable objects in ambient spaces during runtime...|$|R
40|$|Full {{and partial}} {{immersion}} in virtual reality are fundamental different user experiences: partial immersion supports {{the feeling of}} “looking at ” a virtual environment while full immersion supports the feeling of “being in ” that environment. Working {{with a range of}} interactive virtual reality applications using different display systems we have found that the use of six-sided caves and panoramic displays results in different requirements to <b>interaction</b> <b>techniques.</b> These can be related to specific categories of interaction: orientating, moving and acting. In this paper I present a framework for the evaluation and design of <b>interaction</b> <b>techniques</b> for virtual reality focusing on the relations between <b>interaction</b> <b>techniques</b> and display types...|$|R
