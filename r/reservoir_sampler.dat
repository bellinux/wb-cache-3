2|548|Public
40|$|As data volumes {{continue}} to rise, manual inspection {{is becoming increasingly}} untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation and classification tasks and by leveraging a new <b>reservoir</b> <b>sampler</b> and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2 M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring {{hundreds of thousands of}} vehicles. Comment: SIGMOD 201...|$|E
40|$|With {{more and}} more data being {{published}} on the Web as Linked Data, Web Data quality is increasingly important. While quite some {{work has been done}} with regard to quality assessment of Linked Data, only few works have addressed quality improvement. In this article, we present an approach for identifying potentially incorrect RDF statements. The approach is based on distance-based k-nearest neighbour (k-NN) style clustering of statements with the same property and identifying outliers from these clusters. Our method follows a three stage approach, which automates the whole process of finding potentially incorrect statements for a cer-tain property. In the initial stage, RDF statements are added to a <b>reservoir</b> <b>sampler</b> based on Vitter’s rejection-acceptance technique. The mapping stage groups data objects in various cells. Finally, the colouring stage identifies the cells that contain outlier data objects. The proposed approach is scalable in terms of its polynomial time complexity, and also in terms of its space complexity. Our detailed empirical evaluation shows that a high precision is maintained with different settings, which effectively facilitates improving the qual-ity of datasets at a large scale...|$|E
40|$|<b>Reservoir</b> <b>sampling</b> is a {{well-known}} technique for sequential random sampling over data streams. Conventional <b>reservoir</b> <b>sampling</b> assumes a fixed-size reservoir. There are situations, however, {{in which it is}} necessary and/or advantageous to adaptively adjust the size of a reservoir in the middle of sampling due to changes in data characteristics and/or application behavior. This paper studies adaptivesize <b>reservoir</b> <b>sampling</b> over data streams considering two main factors: <b>reservoir</b> size and <b>sample</b> uniformity. First, the paper conducts a theoretical study on the effects of adjusting the size of a <b>reservoir</b> while <b>sampling</b> is in progress. The theoretical results show that such an adjustment may bring a negative impact on the probability of the sample being uniform (called uniformity confidence herein). Second, the paper presents a novel algorithm for maintaining the <b>reservoir</b> <b>sample</b> after the <b>reservoir</b> size is adjusted such that the resulting uniformity confidence exceeds a given threshold. Third, the paper extends the proposed algorithm to an adaptive multi-reservoir sampling algorithm for a practical application in which samples are collected from memory-limited wireless sensor networks using a mobile sink. Finally, the paper empirically examines the adaptivity of the multi-reservoir sampling algorithm with regard to <b>reservoir</b> size and <b>sample</b> uniformity using real sensor networks data sets. 1...|$|R
5000|$|<b>Reservoir</b> <b>sampling,</b> in {{particular}} Algorithm R {{which is a}} specialization of the Fisher-Yates shuffle ...|$|R
50|$|J. Vitter in 1985 {{proposed}} <b>reservoir</b> <b>sampling</b> algorithm {{which is}} often widely used. This algorithm does not require advance knowledge of n and uses constant space.|$|R
30|$|A <b>reservoir</b> <b>sampling</b> {{can be seen}} as an {{algorithm}} {{that consists}} in selecting a random sample of size n, from a file containing N records, in which the value of N is not known to the algorithm. According to (Vitter 1985), the first step of any reservoir algorithm is to put the first n records into a “reservoir”. The rest of the records are processed sequentially. Thus, the number of items to select (k) is smaller than the size of the source array S(i). Algorithm 1 provides an overview of the steps carried out by the <b>reservoir</b> <b>sampling</b> process.|$|R
40|$|In stream join {{processing}} {{with limited}} memory, uniform random sampling {{is useful for}} approximate query evaluation. In this paper, we {{address the problem of}} <b>reservoir</b> <b>sampling</b> over memory-limited stream joins. We present two <b>sampling</b> algorithms, <b>Reservoir</b> Join-Sampling (RJS) and Progressive Reservoir Join-Sampling (PRJS). RJS is designed straightforwardly by using a fixed-size <b>reservoir</b> <b>sampling</b> on a join-sample (i. e., random sample of a join output stream). Anytime the <b>sample</b> in the <b>reservoir</b> is used, RJS always gives a uniform random sample of the original join output stream. With limited memory, however, the available memory may not be large enough even for the join buffer, thereby severely limiting the reservoir size. PRJS alleviates this problem by increasing the reservoir size during the join-sampling 1. This increasing is possible since the memory requirement by the join-sampling algorithm decreases over time. A larger reservoir provides a closer representation of the original join output stream. However, it comes with a negative impact on the probability of the sample being uniform. Through experiments we examine the tradeoffs and compare the two algorithms in terms of the aggregation error on the <b>reservoir</b> <b>sample.</b> 1...|$|R
40|$|Abstract. From a {{high volume}} stream of {{weighted}} items, {{we want to}} maintain a generic sample of a certain limited size k that we can later use to estimate the total weight of arbitrary subsets. This is the classic context of on-line <b>reservoir</b> <b>sampling,</b> thinking of the generic <b>sample</b> as a <b>reservoir.</b> We present an efficient <b>reservoir</b> <b>sampling</b> scheme, VarOptk, that dominates all previous schemes in terms of estimation quality. VarOptk provides variance optimal unbiased estimation of subset sums. More precisely, if we have seen n items of the stream, then for any subset size m, our scheme based on k samples minimizes the average variance over all subsets of size m. In fact, the optimality is against any off-line scheme with k samples tailored for the concrete set of items seen. In addition to optimal average variance, our scheme provides tighter worst-case bounds on the variance of particular subsets than previously possible. It is efficient, handling each new item of the stream in O(log k) time. Finally, it is particularly well suited for combination of samples from different streams in a distributed setting. Key words. Subset sum estimation, weighted sampling, <b>sampling</b> without replacement, <b>reservoir</b> <b>sampling.</b> AMS subject classifications. 62 D 05, 62 G 05, 68 W 27 1. Introduction. I...|$|R
50|$|<b>Reservoir</b> <b>sampling</b> is {{a family}} of {{randomized}} algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn't fit into main memory.|$|R
40|$|ABSTRACT: This study {{aimed to}} {{construct}} the hydrate three-phase equilibrium curves for two typical Iranian gas <b>reservoir</b> <b>samples</b> using a high-pressure visual cell. To do so, complete analysis of the reservoir fluids was obtained by different techniques of composition analysis i. e. gas chromatography (GC) and detailed hydrocarbon analysis (DHA). Using high-pressure visual cell the hydrate phase equilibrium curve was obtained by measuring the hydrate formation temperature in each pressure level. By means of commercial hydrate software, hydrate formation temperature was predicted in each specified pressure level. Comparing {{the results showed that}} the experimental data are in good agreement with the simulator results. Using the experiments results, hydrate phase equilibrium curves were constructed in order to depict the safe and unsafe regions of the hydrate formation zones for two typical Iranian gas <b>reservoir</b> <b>samples...</b>|$|R
30|$|The key {{research}} question {{of this study}} is whether or not is possible to use a different approach, apart from the normal or Gaussian distribution as an internal random generator. Thus, this paper seeks to exploit ideas from randomized algorithms such as: a Brownian walk (based on a normal distribution), a spiral-inspired walk and a <b>Reservoir</b> <b>sampling</b> algorithm.|$|R
5000|$|<b>Reservoir</b> <b>sampling</b> {{makes the}} {{assumption}} that the desired sample fits into main memory, often implying that [...] is a constant independent of [...] In applications where we would like to select a large subset of the input list (say a third, i.e. [...] ), other methods need to be adopted. Distributed implementations for this problem have been proposed.|$|R
40|$|We {{consider}} {{the problem of}} sampling n numbers from the range { 1, [...] .,N} without replacement on modern architectures. The main result is a simple divide-and-conquer scheme that makes sequential algorithms more cache efficient and leads to a parallel algorithm running in expected time O(n/p+ p) on p processors. The amount of communication between the processors is very small and independent of the sample size. We also discuss modifications needed for load balancing, <b>reservoir</b> <b>sampling,</b> online sampling, sampling with replacement, Bernoulli sampling, and vectorization on SIMD units or GPUs...|$|R
40|$|This brief chapter {{describes}} the occurrence Macrocyclops distinctus in the littoral {{zone of the}} Rybinsk <b>Reservoir.</b> <b>Sampling</b> was undertaken in summer and autumn 1961. In order to facilitate distinction of M. distinctus and Macrocyclops fuscus the author presents drawings of the whole species and certain characteristic parts of the body, and also gives a description of M. distinctus, comparing it with M. fuscus...|$|R
40|$|Abstract — Reservoir {{modeling}} is an {{on-going activity}} during the production {{life of a}} reservoir. One challenge to construct-ing accurate reservoir models is {{the time required to}} carry out a large number of computer simulations. This research investigates a competitive co-evolutionary algorithm to select a small number of informative <b>reservoir</b> <b>samples</b> to carry out the computer simulation. The simulation results are also used to co-evolve the computer simulator proxies. We have developed a co-evolutionary system incorporating various techniques to conduct a case study. Although the system was able to select {{a very small number of}} <b>reservoir</b> <b>samples</b> to run the computer simulations and use the simulation data to construct simulator proxies with high accuracy, these proxy models do not gen-eralize very well on a larger set of simulation data generated from our previous study. Nevertheless, we have identified that including a test-bank in the system helped mitigating the situation. We will conduct more systematic analysis of the competitive co-evolutionary dynamics to improve the system performance. I...|$|R
40|$|With the {{explosion}} of information stored world-wide,data intensive computing has become a central area of research. Efficient management and processing of this massively exponential amount of data from diverse sources,such as telecommunication call data records,online transaction records,etc.,has become a necessity. Removing redundancy from such huge(multi-billion records) datasets resulting in resource and compute efficiency for downstream processing constitutes an important area of study. "Intelligent compression" or deduplication in streaming scenarios,for precise identification and elimination of duplicates from the unbounded datastream is a greater challenge given the realtime nature of data arrival. Stable Bloom Filters(SBF) address this problem to a certain extent. However,SBF suffers from a high false negative rate(FNR) and slow convergence rate,thereby rendering it inefficient for applications with low FNR tolerance. In this paper, we present a novel <b>Reservoir</b> <b>Sampling</b> based Bloom Filter,(RSBF) data structure,based on the combined concepts of <b>reservoir</b> <b>sampling</b> and Bloom filters for approximate detection of duplicates in data streams. Using detailed theoretical analysis we prove analytical bounds on its false positive rate(FPR),false negative rate(FNR) and convergence rates with low memory requirements. We show that RSBF offers the currently lowest FN and convergence rates,and are better than those of SBF while using the same memory. Using empirical analysis on real-world datasets(3 million records) and synthetic datasets with around 1 billion records,we demonstrate upto 2 x improvement in FNR with better convergence rates as compared to SBF,while exhibiting comparable FPR. To {{the best of our}} knowledge,this is the first attempt to integrate <b>reservoir</b> <b>sampling</b> method with Bloom filters for deduplication in streaming scenarios. Comment: 11 pages, 8 figures, 5 table...|$|R
40|$|We {{introduce}} Tiered Sampling, a novel {{technique for}} approximate counting sparse motifs in massive graphs whose edges are {{observed in a}} stream. Our technique requires only a single pass on the data and uses a memory of fixed size $M$, which can be magnitudes smaller {{than the number of}} edges. Our methods addresses the challenging task of counting sparse motifs - sub-graph patterns that have low probability to appear in a sample of $M$ edges in the graph, which is the maximum amount of data available to the algorithms in each step. To obtain an unbiased and low variance estimate of the count we partition the available memory to tiers (layers) of <b>reservoir</b> <b>samples.</b> While the base layer is a standard <b>reservoir</b> <b>sample</b> of edges, other layers are <b>reservoir</b> <b>samples</b> of sub-structures of the desired motif. By storing more frequent sub-structures of the motif, we increase the probability of detecting an occurrence of the sparse motif we are counting, thus decreasing the variance and error of the estimate. We demonstrate the advantage of our method in the specific applications of counting sparse 4 and 5 -cliques in massive graphs. We present a complete analytical analysis and extensive experimental results using both synthetic and real-world data. Our results demonstrate the advantage of our method in obtaining high-quality approximations for the number of 4 and 5 -cliques for large graphs using a very limited amount of memory, significantly outperforming the single edge sample approach for counting sparse motifs in large scale graphs. Comment: 28 page...|$|R
2500|$|There {{are various}} {{algorithms}} {{to pick out}} a random combination from a given set or list. Rejection sampling is extremely slow for large sample sizes. One way to select a k-combination efficiently from a population of size n is to iterate across each element of the population, and at each step pick that element with a dynamically changing probability of [...] (see <b>reservoir</b> <b>sampling).</b>|$|R
30|$|Limited studies {{exist on}} the in situ {{production}} of biosurfactants for MEOR implementation and biosurfactant stability under various physiological conditions. This study aimed to isolate, screen, and identify potential biosurfactant-producing bacteria from oil <b>reservoir</b> <b>samples.</b> We characterized the biosurfactant stability under several physiological conditions using Fourier transform infrared (FT-IR) spectroscopy and response-surface methodology. MEOR simulations were performed on a sand-packed column to determine the biosurfactant’s applicability to recover residual crude oil.|$|R
40|$|The {{effects of}} polychlorinated biphenyls (PCBs) on {{nitrification}} were examined for pure cultures and natural <b>reservoir</b> <b>samples.</b> PCBs at concentrations greater than 10 microgram liter- 1 inhibited nitrification, principally ammonium oxidation, {{in one of}} two natural reservoir environments. However, this inhibition could not be reproduced in pure high-cell-density cultures or in previously contaminated reservoir waters. A PCB environmental biotransformation product, p-chlorophenylglyoxylic acid, and p-chloromandelic acid had no effect on nitrification...|$|R
40|$|From a {{high volume}} stream of {{weighted}} items, {{we want to}} maintain a generic sample of a certain limited size k that we can later use to estimate the total weight of arbitrary subsets. This is the classic context of on-line <b>reservoir</b> <b>sampling,</b> thinking of the generic <b>sample</b> as a <b>reservoir.</b> We present a <b>reservoir</b> <b>sampling</b> scheme providing variance optimal estimation of subset sums. More precisely, if we have seen n items of the stream, then for any subset size m, our scheme based on k samples minimizes the average variance over all subsets of size m. In fact, the optimality is against any off-line sampling scheme tailored for the concrete set of items seen: no off-line scheme based on k samples can perform better than our on-line scheme {{when it comes to}} average variance over any subset size. Our scheme has no positive covariances between any pair of item estimates. Also, our scheme can handle each new item of the stream in O(log k) time, which is optimal even on the word RAM...|$|R
5000|$|The {{following}} algorithm {{was given}} by Efraimidis and Spirakis that uses interpretation 1:This algorithm {{is identical to the}} algorithm given in <b>Reservoir</b> <b>Sampling</b> with Random Sort except for the line how we generate the key using random number generator. The algorithm is equivalent to assigning each item a key [...] where [...] is the random number and then sort items using these keys and finally select top k items for the sample.|$|R
40|$|With the {{explosion}} of information stored world-wide, data inten-sive computing has become a central area of research. Efficient management and processing of this massively exponential amount of data from diverse sources, such as telecommunication call data records, telescope imagery, online transaction records, web pages, stock markets, medical records (monitoring critical health condi-tions of patients), climate warning systems, etc., has become a necessity. Removing redundancy from such huge (multi-billion records) datasets resulting in resource and compute efficiency for downstream processing constitutes an important area of study. “In-telligent compression " or deduplication in streaming scenarios, for precise identification and elimination of duplicates from the un-bounded data stream is a greater challenge given the real-time na-ture of data arrival. Stable Bloom Filters (SBF) address this prob-lem to a certain extent. However, SBF suffers from a high false negative rate (FNR) and slow convergence rate, thereby rendering it inefficient for applications with low FNR tolerance. In this paper, we present a novel <b>Reservoir</b> <b>Sampling</b> based Bloom Filter, (RSBF) data structure, based on the combined concepts of <b>reservoir</b> <b>sampling</b> and Bloom filters for approximate detection of duplicates in data streams. Using detailed theoretical analysis we prove analytical bounds on its false positive rate (FPR), false negative rate (FNR) and convergence rates with low memory re-quirements. We show that RSBF offers the currently lowest FN and convergence rates, and are better than those of SBF while using the same memory. Using empirical analysis on real-world datasets (3 million records) and synthetic datasets with around 1 billion records, we demonstrate upto 2 × improvement in FNR with better convergence rates as compared to SBF, while exhibiting compara-ble FPR. To {{the best of our}} knowledge, this is the first attempt to integrate <b>reservoir</b> <b>sampling</b> method with Bloom filters for dedupli-cation in streaming scenarios. 1...|$|R
40|$|International audienceSampling {{streams of}} {{continuous}} data with limited memory, or <b>reservoir</b> <b>sampling,</b> is a utility algorithm. Standard <b>reservoir</b> <b>sampling</b> maintains {{a random sample}} of the entire stream as it has arrived so far. This restriction does not meet the requirement of many applications that need to give preference to recent data. The simplest algorithm for maintaining {{a random sample of}} a sliding window reproduces periodically the same sample design. This is also undesirable for many applications. Other existing algorithms are using variable size memory, variable size samples or maintain biased samples and allow expired data in the sample. We propose an effective algorithm, which is very simple and therefore efficient, for maintaining a near random fixed size sample of a sliding window. Indeed our algorithm maintains a biased sample that may contain expired data. Yet it is a good approximation of a random sample with expired data being present with low probability. We analytically explain why and under which parameter settings the algorithm is effective. We empirically evaluate its performance (effectiveness) and compare it with the performance of existing representatives of random sampling over sliding windows and biased sampling algorithm...|$|R
40|$|We {{show how}} rapidly {{changing}} textual streams such as Twitter can be modelled in fixed space. Our approach {{is based upon}} a randomised algorithm called Exponen-tial <b>Reservoir</b> <b>Sampling,</b> unexplored by this community until now. Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that re-cently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present...|$|R
30|$|This {{section will}} examine three random {{strategies}} that are incorporated into the alternative version of the MHRW. These random strategies are aimed to be used heuristically as an internal picker for a candidate node (hereinafter referred to as ϱ). This set of random strategies is composed as follows: Brownian walk (normal distribution), a spiral-inspired walk (Illusion) and a <b>Reservoir</b> <b>sampling</b> method. It {{is important to note}} that the Brownian case will be used as a the baseline to be compared with the rest of the random strategies.|$|R
30|$|During the {{development}} phase and to implement an optimal reservoir management strategy for a reservoir, the {{knowledge about the}} reservoir fluid properties is very important (Amyx et al. 1988). IFT and contact angle are important parameters for any reservoir engineering studies. They {{can be used in}} the estimation of fluid saturation in gas–oil transition zone (Tiab and Donaldson 2010). No general analytical method is available for estimating IFT, so it has to be measured in the laboratory for <b>reservoir</b> <b>samples</b> at <b>reservoir</b> conditions (Okasha and Al-Shiwaish 2010).|$|R
40|$|The {{concentration}} {{and character of}} natural organic matter (NOM) in samples collected from Quabbin Reservoir an oligotrophic drinking water source were studied. In general, higher in total and dissolved organic carbon (TOC and DOC), UV- 254 absorbance, trilialomethane formation potential (THMFP) and specific UV absorbance (SUVA) were observed in the tributary samples than in the <b>reservoir</b> <b>samples.</b> Analysis of the apparent molecular weight distribution (AMWD) indicated that the <b>reservoir</b> <b>samples</b> were enriched in low molecular weight compounds (3 ̆c 1000 daltons) and high molecular weight compounds (3 ̆e 30, 000 daltons). Consistent with the oligotrophic state of the system, algal densities and nutrient concentrations are low. Regression analysis of water quality parameters yielded meaningful positive relationships between DOC and UV, THMFP and TOC and THMFP and UV for the complete data set. Relationships were significantly weaker using only the tributary or only the reservoir data. Material balance analyses showed that the most significant allochthonous input of NOM is tributary water from the largest subwatershed in the system, but that direct runoff and direct precipitation are also significant allochthonous inputs. Material balance analyses indicated that over a two year period, the net change in NOM in the reservoir water was 26...|$|R
30|$|The Thomeer hyperboles {{method is}} {{successfully}} applied to decode the pore system of carbonate <b>reservoir</b> rock <b>samples</b> in the Middle East region, based on 150 MICP curves.|$|R
40|$|A {{sensor array}} {{consisted}} of interdigitated gold electrodes modified with nanostructured ultra-thin films of conducting polymers {{was used to}} evaluate different water samples from three distinct reservoirs, located in the São Paulo State, Brazil, according to their eutrophic level, i. e. oligotrophic, eutrophic and hypereutrophic. These <b>reservoirs</b> <b>samples</b> presented different eutrophic levels. The sensor array data were processed and analyzed by using PCA (principal component analysis). In the near future, this will be a reliable and straightforward method to analyze water samples based on the concept of global selectivity and electrochemical impedance. ...|$|R
40|$|Amperometric and {{spectrophotometric}} {{methods were}} developed for measuring iodide tracer concentration in water produced from offshore oil reservoirs. The amperometric method exhibited a linear response from 1 to 10 mg L- 1 with limits of detection and of quantification of 0. 2 and 0. 6 mg L- 1, respectively. The spectrophotometric method also exhibited a linear response from 1 to 10 mg L- 1 with limits of detection and of quantification of 1 and 3 mg L- 1, respectively. Both methods showed to be accurate, linear, homoscedastic and their recoveries were 101 ± 2 and 100 ± 3 %, respectively, while the recoveries for produced <b>reservoir</b> water <b>samples</b> (fortified with iodide) were 97 ± 7 and 100 ± 3 %, respectively. High amounts of sulfate interfere in the spectrophotometric method. Both methods are promising, simple and inexpensive alternatives to ion chromatography {{for the detection}} of iodide tracer in produced water <b>reservoir</b> <b>samples...</b>|$|R
40|$|Applications {{involving}} telecommunication call data records, web pages, online transactions, medical records, stock markets, climate warning systems, etc., necessitate {{efficient management}} and processing of such massively exponential {{amount of data}} from diverse sources. De-duplication or Intelligent Compression in streaming scenarios for approximate identification and elimination of duplicates from such unbounded data stream is a greater challenge given the real-time nature of data arrival. Stable Bloom Filters (SBF) addresses this problem to a certain extent.. In this work, we present several novel algorithms for the problem of approximate detection of duplicates in data streams. We propose the <b>Reservoir</b> <b>Sampling</b> based Bloom Filter (RSBF) combining the working principle of <b>reservoir</b> <b>sampling</b> and Bloom Filters. We also present variants of the novel Biased Sampling based Bloom Filter (BSBF) based on biased sampling concepts. We also propose a randomized load balanced variant of the sampling Bloom Filter approach to efficiently tackle the duplicate detection. In this work, we thus provide a generic framework for de-duplication using Bloom Filters. Using detailed theoretical analysis we prove analytical bounds on the false positive rate, false negative rate and convergence rate of the proposed structures. We exhibit that our models clearly outperform the existing methods. We also demonstrate empirical analysis of the structures using real-world datasets (3 million records) and also with synthetic datasets (1 billion records) capturing various input distributions. Comment: 41 page...|$|R
40|$|In this work, the {{presence}} of selected emerging contaminants has been investigated in two reservoirs, La Fe (LF) and Rio Grande (RG), which supply water to two drinking water treatment plants (DWTPs) of Medellin, {{one of the most}} populated cities of Colombia. An analytical method based on solid-phase extraction (SPE) of the sample followed by measurement by liquid chromatography coupled to tandem mass spectrometry (LC– MS/MS) was developed and validated for this purpose. Five monitoring campaigns were performed in each <b>reservoir,</b> collecting <b>samples</b> from 7 sites (LF) and 10 sites (RG) at 3 different depths of the water column. In addition, water samples entering in the DWTPs and treated water samples from these plans were also analysed for the selected compounds. Data from this work showed that parabens, UV filters and the pharmaceutical ibuprofen were commonly present in most of the <b>reservoir</b> <b>samples.</b> Thus, methyl paraben was detected in around 90 % of the samples collected, while ibuprofen was found in around 60 % of the samples. Water samples feeding the DWTPs also contained these two compounds, as well as benzophenone at low concentrations, which was in general agreement with the results from the <b>reservoir</b> <b>samples.</b> After treatment in the DWTPs, these three compounds were still present in the samples although at low concentrations (< 40 ng/L), which evidenced that they were not completely removed after the conventional treatment applied. The potential effects of {{the presence}} of these compounds at the ppt levels in drinking water are still unknown. Further research is needed to evaluate the effect of chronic exposure to these compounds via consumption of drinking water. his work has been developed under the financial support provided by Empresa Públicas de Medellin...|$|R
30|$|The <b>reservoir</b> brine <b>sample</b> has {{salinity}} 2560  ppm, total {{dissolved solid}} 3430  ppm, density 0.9846  g/mL, pH 7.32, and concentration of calcium and magnesium ions, respectively, are 138 and 10.5  ppm.|$|R
40|$|We {{investigate}} {{large scale}} probabilistic association mining on modest hard- ware infrastructure. We first propose a probabilistic columnar infrastructure for storing the transaction database. Using Bloom filters and <b>reservoir</b> <b>sampling</b> techniques, the storage is efficient and probabilistic. Then we propose an accurate probabilistic algorithm for mining frequent item-sets. Our algorithm {{relies on the}} Apriori principle but has a novel probabilistic pruning technique, which reduces frequent item-set candidates without counting every candidate’s support size. In the experiments, our Silverback framework, with satisfying accuracy, outperforms Hadoop Apriori implementation in terms of run time. Silverback has been commercially deployed and develope...|$|R
40|$|Abstract — We propose and {{implement}} a set of efficient online algorithms for a router to sample the passing packets and identify multi-attribute high-volume traffic aggregates. Besides the obvious applications in traffic engineering and measurement, we describe its application in defending against certain classes of DoS attacks. Our contribution includes a <b>reservoir</b> <b>sampling</b> algorithm that employs a biased sampling strategy favoring packets from high-volume aggregates. It identifies singleattribute aggregates. Another algorithm is designed to combine single-attribute aggregates into multi-attribute aggregates. We implement the algorithms on a Linux router and demonstrate that the router can effectively filter out malicious packets in stateful DoS attacks. I...|$|R
