3|10000|Public
40|$|The {{sequence}} memoizer is a {{model for}} sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged <b>range</b> <b>of</b> <b>hyperparameters,</b> a memory-efficient representation, and inference algorithms operating on the new representation. Our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the "mysterious" coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements...|$|E
40|$|Exact {{calculations}} of model posterior probabilities or related quantities are often infeasible {{due to the}} analytical intractability of predictive densities. Here new approximations for obtaining predictive densities are proposed and contrasted with those based on the Laplace method. Our theory and a numerical study indicate that the proposed methods are easy to implement, computationally efficient, and accurate over a wide <b>range</b> <b>of</b> <b>hyperparameters.</b> In the context of GLMs, we show {{that they can be}} employed to facilitate the posterior computation under three general classes of informative priors on regression coefficients. A real example is provided to demonstrate the feasibility and usefulness of the proposed methods in a fully Bayes variable selection procedure. Laplace approximation GLM Normal prior Power prior Conjugate prior Asymptotic normality Logistic regression...|$|E
40|$|Background: Support Vector Machine {{has become}} one of the most popular machine {{learning}} tools used in vir - tual screening campaigns aimed at finding new drug candidates. Although it can be extremely effective in finding new potentially active compounds, its application requires the optimization of the hyperparameters with which the assessment is being run, particularly the C and γ values. The optimization requirement in turn, establishes the need to develop fast and effective approaches to the optimization procedure, providing the best predictive power of the constructed model. Results: In this study, we investigated the Bayesian and random search optimization of Support Vector Machine hyperparameters for classifying bioactive compounds. The effectiveness of these strategies was compared with the most popular optimization procedures—grid search and heuristic choice. We demonstrated that Bayesian optimiza- tion not only provides better, more efficient classification but is also much faster—the number of iterations it required for reaching optimal predictive performance was the lowest out of the all tested optimization methods. Moreover, for the Bayesian approach, the choice of parameters in subsequent iterations is directed and justified; therefore, the results obtained by using it are constantly improved and the <b>range</b> <b>of</b> <b>hyperparameters</b> tested provides the best over - all performance of Support Vector Machine. Additionally, we showed that a random search optimization of hyperpa- rameters leads to significantly better performance than grid search and heuristic-based approaches. Conclusions: The Bayesian approach to the optimization of Support Vector Machine parameters was demonstrated to outperform other optimization methods for tasks concerned with the bioactivity assessment of chemical com- pounds. This strategy not only provides a higher accuracy of classification, but is also much faster and more directed than other approaches for optimization. It appears that, despite its simplicity, random search optimization strategy should be used as a second choice if Bayesian approach application is not feasible...|$|E
40|$|Abstract. Since {{hyperparameter}} optimization {{is crucial}} for achiev-ing peak performance with many machine learning algorithms, an active research community has formed around this problem {{in the last few}} years. The evaluation <b>of</b> new <b>hyperparameter</b> optimization techniques against {{the state of the art}} requires a set of benchmarks. Because such evaluations can be very expensive, early experiments are often performed using synthetic test functions rather than using real-world hyperparameter optimization problems. However, there can be a wide gap between the two kinds of problems. In this work, we introduce another option: cheap-to-evaluate surrogates <b>of</b> real <b>hyperparameter</b> optimization benchmarks that share the same hyper-parameter spaces and feature similar response surfaces. Specifically, we train regression models on data describing a machine learning algorithm’s performance under a wide <b>range</b> <b>of</b> <b>hyperparameter</b> con-figurations, and then cheaply evaluate hyperparameter optimization methods using the model’s performance predictions in lieu of the real algorithm. We evaluate the effectiveness for using a wide <b>range</b> <b>of</b> regression techniques to build these surrogate benchmarks, both in terms of how well they predict the performance of new configurations and of how much they affect the overall performance of hyperparame-ter optimizers. Overall, we found that surrogate benchmarks based on random forests performed best: for benchmarks with few hyperparam-eters they yielded almost perfect surrogates, and for benchmarks with more complex hyperparameter spaces they still yielded surrogates that were qualitatively similar to the real benchmarks they model. ...|$|R
30|$|By {{using any}} multivariate {{optimization}} algorithm, the set <b>of</b> <b>hyperparameters</b> θ {{can be estimated}} analytically. After the optimization process has reached the analytical solution, the numerical values <b>of</b> the <b>hyperparameters</b> are simply obtained by using the measured input and output signals. This is a great advantage over other types of regression as it allows the system to evolve without pre-specifying the parameters and thus limiting the <b>range</b> <b>of</b> estimations [22].|$|R
40|$|We {{investigate}} a gradient-based method for adaptive optimization <b>of</b> <b>hyperparameters</b> in logistic regression models. Adaptive optimization <b>of</b> <b>hyperparameters</b> reduces the computational cost <b>of</b> selecting good <b>hyperparameter</b> values, and allows these optimal values to be pinpointed more precisely, {{as compared to}} an exhaustive search <b>of</b> the <b>hyperparameter</b> space. ...|$|R
30|$|The {{evolution}} of deep neural networks (DNNs) has dramatically improved {{the accuracy of}} character recognition [1], object recognition [2, 3], and other tasks. However, the their increasing complexity increases the number <b>of</b> <b>hyperparameters,</b> which makes tuning <b>of</b> <b>hyperparameters</b> an intractable task.|$|R
5000|$|... the <b>hyper{{parameter}}</b> <b>of</b> the parameter distribution, i.e., [...] This {{may in fact}} be a vector <b>of</b> <b>hyperparameters.</b>|$|R
40|$|We {{develop a}} Bayesian “sum-of-trees ” model where each tree is {{constrained}} by a prior to be a weak leaner. Fitting and inference are accomplished via an iterative back-fitting MCMC algorithm. This model {{is motivated by}} ensemble methods in general, and boosting algorithms in particular. Like boosting, each weak learner (i. e., each weak tree) contributes a small amount to the overall model, and the training of a weak learner is conditional on the estimates for the other weak learners. The differences from boosting algorithms are just as striking as the similarities: BART is defined by a statistical model: a prior and a likelihood, while boosting is defined by an algorithm. MCMC is used both to fit the model and to quantify inferential uncertainty through the variation of the posterior draws. The BART modelling strategy can also be viewed {{in the context of}} Bayesian non-parametrics. The key idea is to use a model which is rich enough to respond to a variety of signal types, but constrained by the prior from overreacting to weak signals. The ensemble approach provides for a rich base model form which can expand as needed via the MCMC mechanism. The priors are formulated so as to be interpretable, relatively easy to specify, and provide results that are stable across a wide <b>range</b> <b>of</b> prior <b>hyperparameter</b> values. The MCMC algorithm, which exhibits fast burn-in and good mixing, can be readily used for model averaging and for uncertainty assessment...|$|R
5000|$|... where [...] is the {{training}} data, and [...] {{is a set}} <b>of</b> <b>hyperparameters</b> for [...] and [...]|$|R
40|$|We {{introduce}} pyGPs, an object-oriented {{implementation of}} Gaussian processes (gps) for machine learning. The library provides a wide <b>range</b> <b>of</b> functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations <b>of</b> <b>hyperparameter</b> optimization, sparse approximations, and graph based learning. Using Python {{we focus on}} usability for both "users" and "researchers". Our main goal is to offer a user-friendly and flexible implementation of gps for machine learning...|$|R
5000|$|SUMO-Toolbox is a MATLAB toolbox for {{surrogate}} modeling {{supporting a}} wide collection <b>of</b> <b>hyperparameter</b> optimization algorithm for many model types.|$|R
30|$|Though {{it is not}} {{currently}} available, there has been ongoing research and development at Berkeley’s AMP lab on a platform called MLbase, which wraps MLlib, Spark, and other projects to make machine learning on data sets of all sizes accessible to a broader <b>range</b> <b>of</b> users [134 – 136]. In addition to MLlib and Spark, the other core components are MLI, an API for feature extraction and algorithm development, and ML Optimizer, which automates the tuning <b>of</b> <b>hyperparameters.</b>|$|R
30|$|These {{methods are}} also {{applied to the}} {{optimization}} <b>of</b> <b>hyperparameters</b> <b>of</b> the Batch-Normalized Maxout Network in Network proposed by Chang et al. [25]. Note that this network is deeper and has many more hyperparameters to optimize than LeNet.|$|R
30|$|In these experiments, we briefly {{evaluate}} the impact <b>of</b> <b>hyperparameters</b> in LW-BGRU and apply these two architectures to all seven languages.|$|R
40|$|Abstract. We {{are using}} bandit-based {{adaptive}} operator selection while autotuning parallel computer programs. The autotuning, which uses evolutionary algorithm-based stochastic sampling, takes place {{over an extended}} duration and occurs in situ as programs execute. The environment or context during tuning is either largely static in one scenario or dynamic in another. We rely upon adaptive operator selection to dynamically generate worthy test configurations of the program. In this paper, we study how the choice <b>of</b> <b>hyperparameters,</b> which control the trade-off between exploration and exploitation, affects the effectiveness of adaptive operator selection which in turn affects {{the performance of the}} autotuner. We show that while the optimal assignment <b>of</b> <b>hyperparameters</b> varies greatly between different benchmarks, there exists a single assignment, for a context, <b>of</b> <b>hyperparameters</b> that performs well regardless of the program being tuned. ...|$|R
40|$|Neural {{networks}} {{dominate the}} modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices <b>of</b> <b>hyperparameters</b> such as model architecture, loss function, and optimisation algorithm. In this work we present Population Based Training (PBT), a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise {{a population of}} models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule <b>of</b> <b>hyperparameter</b> settings rather than following the generally sub-optimal strategy of {{trying to find a}} single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite <b>of</b> <b>hyperparameters.</b> In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery <b>of</b> <b>hyperparameter</b> schedules and model selection which results in stable training and better final performance...|$|R
40|$|We {{are using}} bandit-based {{adaptive}} operator selection while autotuning parallel computer programs. The autotuning, which uses evolutionary algorithm-based stochastic sampling, takes place {{over an extended}} duration and occurs in situ as programs execute. The environment or context during tuning is either largely static in one scenario or dynamic in another. We rely upon adaptive operator selection to dynamically generate worthy test configurations of the program. In this paper, we study how the choice <b>of</b> <b>hyperparameters,</b> which control the trade-off between exploration and exploitation, affects the effectiveness of adaptive operator selection which in turn affects {{the performance of the}} autotuner. We show that while the optimal assignment <b>of</b> <b>hyperparameters</b> varies greatly between different benchmarks, there exists a single assignment, for a context, <b>of</b> <b>hyperparameters</b> that performs well regardless of the program being tuned. EvoApplications 2012 : EvoCOMNET, EvoCOMPLEX, EvoFIN, EvoGAMES, EvoHOT, EvoIASP, EvoNUM, EvoPAR, EvoRISK, EvoSTIM, and EvoSTOC, Málaga, Spain, April 11 - 13, 2012, Proceeding...|$|R
5000|$|Parameters {{of prior}} {{distributions}} are a kind <b>of</b> <b>hyperparameter.</b> For example, if one uses a beta distribution {{to model the}} distribution of the parameter p of a Bernoulli distribution, then: ...|$|R
40|$|Distributed linear {{solutions}} {{have frequently}} {{been used to}} solve the source localization problem in EEG. Here we introduce an approach based on the weighted minimum norm (WMN) method that imposes constraints using anatomical and physiological information derived from other imaging modalities. The anatomical constraints are used to reduce the solution space a priori by modeling the spatial source distribution {{with a set of}} basis functions. These spatial basis functions are chosen in a principled way using information theory. The reduced problem is then solved with a classical WMN method. Further (functional) constraints can be introduced in the weighting of the solution using fMRI brain responses to augment spatial priors. We used simulated data to explore the behavior of the approach over a <b>range</b> <b>of</b> the model's <b>hyperparameters.</b> To assess the construct validity of our method we compared it with two established approaches to the source localization problem, a simple weighted minimum norm and a maximum smoothness (Loreta-like) solution. This involved simulations, using single and multiple sources that were analyzed under different levels of confidence in the priors. (C) 2002 Elsevier Science (USA). Peer reviewe...|$|R
40|$|We {{introduce}} the hyperparameter search {{problem in the}} field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set <b>of</b> <b>hyperparameters</b> that must be determined before training commences. The choice <b>of</b> <b>hyperparameters</b> can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential. Comment: 5 pages, accepted for MIC 2015 : The XI Metaheuristics International Conference in Agadir, Morocc...|$|R
50|$|A hyperprior is a {{distribution}} {{on the space}} <b>of</b> possible <b>hyperparameters.</b> If one is using conjugate priors, then this space is preserved by moving to posteriors - thus as data arrives, the distribution changes, but remains on this space: as data arrives, the distribution evolves as a dynamical system (each point <b>of</b> <b>hyperparameter</b> space evolving to the updated hyperparameters), over time converging, just as the prior itself converges.|$|R
40|$|Convolutional Neural Network {{is known}} as ConvNet have been {{extensively}} used in many complex machine learning tasks. However, hyperparameters optimization {{is one of a}} crucial step in developing ConvNet architectures, since the accuracy and performance are reliant on the hyperparameters. This multilayered architecture parameterized by a set <b>of</b> <b>hyperparameters</b> such as the number of convolutional layers, number of fully connected dense layers & neurons, the probability of dropout implementation, learning rate. Hence the searching the hyperparameter over the hyperparameter space are highly difficult to build such complex hierarchical architecture. Many methods have been proposed over the decade to explore the hyperparameter space and find the optimum set <b>of</b> <b>hyperparameter</b> values. Reportedly, Gird search and Random search are said to be inefficient and extremely expensive, due to a large number <b>of</b> <b>hyperparameters</b> <b>of</b> the architecture. Hence, Sequential model-based Bayesian Optimization is a promising alternative technique to address the extreme of the unknown cost function. The recent study on Bayesian Optimization by Snoek in nine convolutional network parameters is achieved the lowerest error report in the CIFAR- 10 benchmark. This article is intended to provide the overview of the mathematical concept behind the Bayesian Optimization over a Gaussian prior. Comment: 10 Page...|$|R
30|$|The {{results of}} the BLSTM and LW-BLSTM {{acoustic}} models are presented in this subsection. We first evaluate the impact <b>of</b> <b>hyperparameters</b> in BLSTM and LW-BLSTM for Pashto. Following this, we give the results using the optimal strategy for each language.|$|R
50|$|When using a {{conjugate}} prior, {{the posterior}} distribution {{will be from}} the same family, but will have different hyperparameters, which reflect the added information from the data: in subjective terms, one's beliefs have been updated. For a general prior distribution, this is computationally very involved, and the posterior may have an unusual or hard to describe form, but with a conjugate prior, there is generally a simple formula relating the values <b>of</b> the <b>hyperparameters</b> <b>of</b> the posterior to the values <b>of</b> the <b>hyperparameters</b> <b>of</b> the prior, and thus the computation of the posterior distribution is very easy.|$|R
40|$|Bayesian methods {{based on}} {{hierarchical}} mixture models have demonstrated excellent {{mean squared error}} properties in constructing data dependent shrinkage estimators in wavelets, however, subjective elicitation <b>of</b> the <b>hyperparameters</b> is challenging. In this chapter we use an Empirical Bayes approach to estimate the hyperparameters for each level of the wavelet decomposition, bypassing the usual difficulty <b>of</b> <b>hyperparameter</b> specification in the hierarchical model. The EB approach is computationally competitive with standard methods and offers improved MSE performance over several Bayes and classical estimators {{in a wide variety}} of examples...|$|R
40|$|Surrogate {{models are}} {{data-driven}} models used to accurately mimic the complex {{behavior of a}} system. They are often used to approximate computationally expensive simulation code in order {{to speed up the}} exploration of design spaces. A crucial step in the building of surrogate models is finding a good set <b>of</b> <b>hyperparameters,</b> which determine the behavior of the model. This is especially important when dealing with sparse data, as the models are in that case more prone to overfitting. Cross-validation is often used to optimize the <b>hyperparameters</b> <b>of</b> surrogate models, however it is computationally expensive and can still lead to overfitting or other erratic model behavior. This paper introduces a new auxiliary measure for the optimization <b>of</b> the <b>hyperparameters</b> <b>of</b> surrogate models which, when used in conjunction with a cheap accuracy measure, is fast and effective at avoiding unexplained model behavior. ...|$|R
40|$|The variational {{principle}} for conformational dynamics has enabled the systematic construction of Markov state models through the optimization <b>of</b> <b>hyperparameters</b> by approximating the transfer operator. In this note we discuss why lag {{time of the}} operator being approximated must be held constant in the variational approach...|$|R
40|$|Abstract. Frequency domain {{properties}} of the operators to decompose a time series into the multi-components along the Akaike's Bayesian model (Akaike (1980, Bayesian Statistics, 143 - 165, University Press, Valencia, Spain)) are shown. In that analysis a normal disturbance-linear-stochastic regression prior model {{is applied to the}} time series. A prior distribution, characterized by a small number <b>of</b> <b>hyperparameters,</b> is specified for model parameters. The posterior distribution is a linear function (filter) of observations. Here we use frequency domain analysis or filter characteristics of several prior models para-metrically as a function <b>of</b> the <b>hyperparameters...</b>|$|R
40|$|Global COE Program Education-and-Research Hub for Mathematics-for-IndustryグローバルCOEプログラム「マス･フォア･インダストリ教育研究拠点」We {{consider}} the Bayesian lasso for regression, {{which is an}} L 1 penalized regression based on Bayesian approach. In Bayesian theory, a crucial issue is the specification of prior distributions for parameters, {{which leads to the}} selection <b>of</b> values <b>of</b> <b>hyperparameters</b> included in the prior distributions. In order to select the values <b>of</b> the <b>hyperparameters,</b> we introduce a model selection criterion by evaluating the Bayesian predictive distribution for the Bayesian lasso. Several numerical studies are presented to illustrate the effectiveness of our proposed modeling procedure...|$|R
40|$|We {{present a}} method for the sparse greedy {{approximation}} of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection motivated by active learning. We show how a large number <b>of</b> <b>hyperparameters</b> can be adjusted automatically by maximizing the marginal likelihood of the training data. Our method is essentially as fast as an equivalent one which selects the "support" patterns at random, yet {{has the potential to}} outperform random selection on hard curve fitting tasks and at the very least leads to a more stable behaviour of first-level inference which makes the subsequent gradient-based optimization <b>of</b> <b>hyperparameters</b> much easier. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods...|$|R
40|$|Second {{derivative}} of interactance spectra (731 – 926 nm) of intact peaches and Brix values of extracted juice {{were used to}} develop a LS-SVM regression (based on a RBF kernel) and a PLS regression model. An iterative approach was taken with the LS-SVM regression, involving a grid search with application of a gradient based optimization method using a validation set for tuning <b>of</b> <b>hyperparameters,</b> followed by pruning of the LS-SVM model with the optimized hyperparameters. The grid search approach led to five-fold faster and better determination <b>of</b> <b>hyperparameters.</b> Less than 45 % of the initial 1430 calibration samples were kept in the models. In prediction of an independent test set with 120 samples, the pruned LS-SVM models performed better than the PLS model (RMSEP decreased by 9 to 14 %) ...|$|R
40|$|Sparse image {{reconstruction}} is {{of interest}} in the fields of radioastronomy and molecular imaging. The observation is assumed to be a linear transformation of the image, and corrupted by additive white Gaussian noise. We study the usage of sparse priors in the empirical Bayes framework: it permits the selection <b>of</b> the <b>hyperparameters</b> <b>of</b> the prior in a data-driven fashion. Three sparse image reconstruction methods are proposed. A simulation study was performed using a binary-valued image and a Gaussian point spread function. In the <b>range</b> <b>of</b> signal to noise ratios considered, the proposed methods had better performance than sparse Bayesian learning (SBL). 1...|$|R
40|$|Tuning <b>hyperparameters</b> <b>of</b> {{learning}} algorithms is {{hard because}} gradients are usually unavailable. We compute exact gradients of cross-validation performance {{with respect to}} all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands <b>of</b> <b>hyperparameters,</b> including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum. Comment: 10 figures. Submitted to ICM...|$|R
40|$|Response {{surfaces}} {{have been}} extensively {{used as a}} method of building effective surrogate models of high-fidelity computational simulations. Of the numerous types of response surface models, kriging is {{perhaps one of the most}} effective, due to its ability to model complicated responses through interpolation or regression of known data while providing an estimate of the error in its prediction. There is, however, little information indicating the extent to which the <b>hyperparameters</b> <b>of</b> a kriging model need to be tuned for the resulting surrogate model to be effective. The following paper addresses this issue by investigating how often and how well it is necessary to tune the <b>hyperparameters</b> <b>of</b> a kriging model as it is updated during an optimization process. To this end, an optimization benchmarking procedure is introduced and used to assess the performance of five different tuning strategies over a <b>range</b> <b>of</b> problem sizes. The results of this benchmark demonstrate the performance gains that can be associated with reducing the complexity <b>of</b> the <b>hyperparameter</b> tuning process for complicated design problems. The strategy <b>of</b> tuning <b>hyperparameters</b> only once after the initial design of experiments is shown to perform poorly...|$|R
40|$|We {{propose a}} method for the fast {{estimation}} <b>of</b> <b>hyperparameters</b> in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion...|$|R
