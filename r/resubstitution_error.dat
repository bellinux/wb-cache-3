18|13|Public
30|$|<b>Resubstitution</b> <b>error</b> is {{difference}} between the predictions of the model {{and that of the}} response training data. High <b>resubstitution</b> <b>error</b> signifies lower accuracy in predictions. However, great predictions for new data aren’t assured by having low <b>resubstitution</b> <b>error.</b>|$|E
40|$|Abstmct-Probability inequalities {{are given}} for tbe {{deviation}} of the <b>resubstitution</b> <b>error</b> estimate from the unknown conditional probability of error. lle inequalities are distribution free {{and can be}} applied to linear diSCrhiMti 0 n rules, to nearest neighbor rules with a reduced sample size, and to b&gram rules. I...|$|E
30|$|The optimal {{regression}} tree generated gives {{very high}} <b>resubstitution</b> <b>error</b> and is much smaller. Yet, it gives comparable precision. Even though cross-validation error was low for the generated model other errors [root-mean-squared error (RMSE), correlation coefficient (R), {{mean absolute error}} (MAE)] were really high {{as compared to the}} original model.|$|E
40|$|This paper {{provides}} exact analytical expressions for the bias, variance, and RMS for the <b>resubstitution</b> and leave-one-out <b>error</b> estimators in {{the case}} of linear dis-criminant analysis (LDA) in the univariate heteroskedastic Gaussian model. Neither the variances nor the sample sizes for the two classes need be the same. The gen-erality of heteroskedasticity (unequal variances) is a fundamental feature of the work presented in this paper, which distinguishes it from past work. The expected <b>resubstitution</b> and leave-one-out <b>errors</b> are represented by probabilities involving bivariate Gaussian distributions. Their second moments and cross-moments with the actual error are represented by 4 -variate Gaussian distributions. From these, the bias, deviation variance, and RMS for resubstitution and leave-one-out as esti-mators of the actual error can be computed. The RMS expressions are applied to the determination of sample size and apply to biomarker classification...|$|R
3000|$|... for {{the case}} of <b>resubstitution</b> and leave-one-out <b>error</b> estimators, which suffices to compute the bias, variance, and RMS of the {{corresponding}} CoD estimator, as discussed in the previous section. These expressions are functions only of sample size, number of bins (complexity), and the probability model. We will assume throughout, for definiteness, that the sample size [...]...|$|R
40|$|Error {{estimation}} is {{a problem}} of high current interest in many areas of application. This paper concerns the classical problem of determining the performance of error estimators in small-sample settings under a Gaussianity parametric assumption. We provide {{here for the first time}} the exact sampling distribution of the <b>resubstitution</b> and leave-one-out <b>error</b> estimators for linear discriminant analysis (LDA) in the univariate case, which is valid for any sample size and combination of parameters (including unequal variances and sample sizes for each class). In the multivariate case case, we provide a quasi-binomial approximation to the distribution of both the <b>resubstitution</b> and leave-one-out <b>error</b> estimators for LDA, under a common but otherwise arbitrary class covariance matrix, which is assumed to be known in the design of the LDA discriminant. We provide numerical examples, using both syn-thetic and real data, that indicate that these approximations are accurate, provided that LDA classification error is not too large...|$|R
40|$|The aim of {{this paper}} is to present a method for {{selecting}} the optimal tree among the possible trees that can be generated starting from the a data set. Analysis a quantity criterion is used through the linear combination of the quality measurements of the tree, namely, <b>resubstitution</b> <b>error</b> and linearity. The application of the method leads to a succession of optimal trees, in such a way, that an element of the succession is associated with each possible value of the linear combination's parameter c~...|$|E
40|$|Five {{methods that}} {{generate}} multiple prototypes from labeled data are reviewed. Then we {{introduce a new}} sixth approach, which is a modification of Chang's (1974) method. We compare the six methods with two standard classifier designs: the 1 -nearest prototype (1 -np) and 1 -nearest neighbor (1 -nn) rules. The standard of comparison is the <b>resubstitution</b> <b>error</b> rate; the data used are the Iris data. Our modified Chang's method produces the best consistent (zero-error) design. One of the competitive learning models produces the best minimal prototypes design (five prototypes that yield three resubstitution errors...|$|E
30|$|In this section, five {{experiments}} have been designed. These experiments are simulated in the MATLAB. In the experiments, {{the performances of}} three famous learning algorithms for the ANN are evaluated which are resilient backpropagation (‘trainrp’ in MATLAB), scaled conjugate gradient backpropagation (‘trainscg’ in MATLAB), and gradient descent with momentum and adaptive learning rate backpropagation (‘traingdx’ in MATLAB). Also, the different numbers of neurons in the hidden layer of the ANN is investigated. In the experiments, the tenfold cross-validation scheme has been adapted to assess the generalization capabilities of the system in the obtained results. Also, in the first experiment, the <b>resubstitution</b> <b>error</b> has been calculated.|$|E
40|$|We derive double {{asymptotic}} analytical expressions for {{the first}} moments, second moments, and cross-moments with the actual <b>error</b> for the <b>resubstitution</b> and leave-one-out <b>error</b> estimators {{in the case of}} linear discriminant analysis in the multivariate Gaussian model under the assumption of a common known covariance matrix and a fixed Mahalanobis distance as dimensionality approaches infinity. Sample sizes for the two classes need not be the same; they are only assumed to reach a fixed, but arbitrary, asymptotic ratio with the dimensionality. From the asymptotic moment representations, we directly obtain double asymptotic expressions for the bias, variance, and RMS of the error estimators. The asymptotic expressions presented here generally provide good small sample approximations, as demonstrated via numerical experiments. The applicability of the theoretical results is illustrated by finding the minimum sample size to bound the RMS in gene-expression classification...|$|R
40|$|Bayesian belief nets (BNs) {{are often}} used for {{classification}} tasks, typically to return the most likely class label for a specified instance. Many BN-learners, however, attempt to find the BN that maximizes a different objective function — viz., likelihood, rather than classification accuracy — typically by first using some model selection criterion to identify an appropriate graphical structure, then finding good parameters for that structure. This paper considers {{a number of possible}} criteria for selecting the best structure, both generative (i. e., based on likelihood; BIC, BDe) and discriminative (i. e., Conditional BIC (CBIC), <b>resubstitution</b> Classification <b>Error</b> (CE) and Bias 2 +Variance (BV)). We empirically compare these criteria against a variety of different “correct BN structures”, both real-world and synthetic, over a range of complexities. We also explore different ways to set the parameters, dealing with two issues: (1) Should we seek the parameters that maximize likelihood versus the ones that maximize conditional likelihood? (2) Should we use (i) the entire training sample first to learn the best parameters and then to evaluate the models, versus (ii) only a partition for parameter estimation and another partition for evaluation (cross-validation) ? Our results show that the discriminative BV model selection criterion {{is one of the best}} measures for identifying the optimal structure, while the discriminative CBIC performs poorly; that we should use the parameters that maximize likelihood; and that it is typically better to use cross-validation here...|$|R
3000|$|The {{error of}} the best {{predictor}} corresponds to the optimal prediction error, also known as Bayes error, which depends only on the underlying probability model [7]. However, in practical real-world problems, the underlying probability model is unknown, and thus we arrive at the fundamental issue of {{how to find a}} good prediction error estimator in small-sample settings [8, 9]. An error estimator may be a deterministic function of the sample data, in which case it is called a nonrandomized error estimator; such popular <b>error</b> estimators as <b>resubstitution</b> and leave-one-out are examples. These error estimators are random only through the random sample data. Closed-form analytical expressions for performance metrics such as bias, deviation variance, and RMS of <b>resubstitution</b> and leave-one-out <b>error</b> estimators have been given in [9, 10]. By contrast, randomized error estimators, like cross-validation and bootstrap, have [...] "internal" [...] random factors that affect their outcome, and thus approximate approaches, usually via Monte Carlo sampling, are typically used to analyze their performance.|$|R
40|$|Abstract—Five {{methods that}} {{generate}} multiple prototypes from labeled data are reviewed. Then we {{introduce a new}} sixth approach, which is a modification of Chang’s method. We compare the six methods with two standard classifier designs: the 1 -nearest prototype (1 -np) and 1 -nearest neighbor (1 -nn) rules. The standard of comparison is the <b>resubstitution</b> <b>error</b> rate; the data used are the Iris data. Our modified Chang’s method produces the best consistent (zero errors) design. One of the competitive learning models produces the best minimal prototypes design (five prototypes that yield three resubstitution errors). Index Terms — Competitive learning, Iris data, modified Chang’s method (MCA), multiple prototypes, nearest neighbo...|$|E
30|$|However, {{because of}} the large {{dimension}} data set (1197 objects), the initial CART model is highly complex with a very high number of nodes. This model produces very accurate prediction for the training data but largely fails to predict accurately when supplied with unknown data (the objects of validation set). This is the well-known overfitting problem. The determination of a smaller tree, got from the maximal one, is therefore essential for successful modeling. The choice of the ideal tree is accomplished by a tree pruning methodology. Many techniques were used to generate the optimal model like examination of <b>resubstitution</b> <b>error,</b> cross-validation, control depth, evaluating root-mean-squared error (RMSE), correlation coefficient (R) and mean absolute error (MAE).|$|E
40|$|Abstract: Based on {{the theory}} of Fisher {{discriminant}} analysis, this paper aims to develop the Fisher discriminant function model by using the representative water inrush data {{from the floor of}} coal mining face, considering comprehensively the factors that affect the water inrush from coal floor. The predicted result is consistent with the actual situation, which corresponds with or better than that of the least squares support vector machine, artificial neural network, and so on. The results indicate that this model is a new effective way of predicting the water inrush from floor and can be widely used in the practical engineering because of the low <b>resubstitution</b> <b>error</b> rate and good distinguishing performance...|$|E
40|$|Abstract—We derive double {{asymptotic}} analytical expressions for {{the first}} moments, second moments, and cross-moments with the actual <b>error</b> for the <b>resubstitution</b> and leave-one-out <b>error</b> estimators {{in the case of}} linear discriminant analysis in the multivariate Gaussian model under the assumption of a common known covariance matrix and a fixed Mahalanobis distance as dimensionality approaches infinity. Sample sizes for the two classes need not be the same; they are only assumed to reach a fixed, but arbitrary, asymptotic ratio with the dimensionality. From the asymptotic moment representations, we directly obtain double asymptotic expressions for the bias, variance, and RMS of the error estimators. The asymptotic expressions presented here generally provide good small sample approximations, as demonstrated via numerical experiments. The applicability of the theoretical results is illustrated by finding the minimum sample size to bound the RMS in gene-expression classification. Index Terms—Double asymptotics, error estimation, genomic signal processing, leave-one-out, linear discriminant analysis, resubstitution, root-mean square (RMS). I...|$|R
40|$|Error {{estimation}} {{must be used}} to {{find the}} accuracy of a designed classifier, {{an issue that is}} critical in biomarker discovery for disease diagnosis and prognosis in genomics and proteomics. This paper presents, for what {{is believed to be the}} first time, the analytical formulation for the joint sampling distribution of the actual and estimated errors of a classification rule. The analysis presented here concerns the Linear Discriminant Analysis (LDA) classification rule and the <b>resubstitution</b> and leave-one-out <b>error</b> estimators, under a general parametric Gaussian assumption. Exact results are provided in the univariate case, and a simple method is suggested to obtain an accurate approximation in the multivariate case. It is also shown how these results can be applied in the computation of condition bounds and the regression of the actual error, given the observed error estimate. In contrast to asymptotic results, the analysis presented here is applicable to finite training data. In particular, it applies in the small-sample settings commonly found in genomics and proteomics applications. Numerical examples, which include parameters estimated from actual microarray data, illustrate the analysis throughout...|$|R
40|$|Error {{estimation}} {{must be used}} to {{find the}} accuracy of a designed classifier, {{an issue that is}} critical in biomarker discovery for disease diagnosis and prognosis in genomics and proteomics. This dissertation is concerned with the analytical formulation of the joint distribution of the true error of misclassification and two of its commonly used estimators, resubstitution and leave-one-out, as well as their marginal and mixed moments, {{in the context of the}} Linear Discriminant Analysis (LDA) classification rule. In the first part of this dissertation, we obtain the joint sampling distribution of the actual and estimated errors under a general parametric Gaussian assumption. Exact results are provided in the univariate case and an accurate approximation is obtained in the multivariate case. We show how these results can be applied in the computation of conditional bounds and the regression of the actual error, given the observed error estimate. In practice the unknown parameters of the Gaussian distributions, which figure in the expressions, are not known and need to be estimated. Using the usual maximum-likelihood estimates for such parameters and plugging them into the theoretical exact expressions provides a sample-based approximation to the joint distribution, and also sample-based methods to estimate upper conditional bounds. In the second part of this dissertation, exact analytical expressions for the bias, variance, and Root Mean Square (RMS) for the <b>resubstitution</b> and leave-one-out <b>error</b> estimators in the univariate Gaussian model are derived. All probabilistic characteristics of an error estimator are given by the knowledge of its joint distribution with the true error. Partial information is contained in their mixed moments, in particular, their second mixed moment. Marginal information regarding an error estimator is contained in its marginal moments, in particular, its mean and variance. Since we are interested in estimator accuracy and wish to use the RMS to measure that accuracy, we desire knowledge of the second-order moments, marginal and mixed, with the true error. In the multivariate case, using the double asymptotic approach with the assumption of knowing the common covariance matrix of the Gaussian model, analytical expressions for the first moments, second moments, and mixed moment with the actual <b>error</b> for the <b>resubstitution</b> and leave-one-out <b>error</b> estimators are derived. The results provide accurate small sample approximations and this is demonstrated in the present situation via numerical comparisons. Application of the results is discussed in the context of genomics...|$|R
40|$|We compare {{learning}} vector quantization, fuzzy learning vector quantization, and a deterministic scheme {{called the}} dog-rabbit (DR) model for generation of multiple prototypes from labeled data for classifier design. We also compare these three models to three other methods: a dumping method due to Chang (1974); our modification of Chang's method; and a derivative of the batch fuzzy c-means algorithm due to Yen-Chang (1994). All six methods {{are superior to}} the labeled subsample means, which yield 11 errors with 3 prototypes. Our modified Chang's method is, for the Iris data used in this study, {{the best of the}} six schemes in one sense; it finds 11 prototypes that yield a <b>resubstitution</b> <b>error</b> rate of 0. In a different sense, the DR method is best, yielding a classifier that commits only 3 errors with 5 prototypes...|$|E
40|$|We {{applied the}} novel {{bootstrap}} 632 + rule to choose tree-based classifiers trained for modeling {{the risk of}} parasite presence in a host population of ungulates. The method is designed to control overfitting: compact classification trees (CART) are selected using a nonlinear combination of the <b>resubstitution</b> <b>error</b> and the standard bootstrap error estimate. Model selection based on the 632 + rule offers a gain over cross-validation for CART models. The tree classifier selected by the new rule for this application favourably compared with standard multivariate GLIM models. Keywords: bootstrap 632 +, model selection, classification and regression trees. 1 Introduction Producing the simplest classification model with the smallest prediction error on new observations requires to optimally balance reduction of error on the training material with control of overfitting. An improved bootstrap schema has been recently proposed for model selection in classification problems [1]. The novel bootstrap [...] ...|$|E
40|$|We {{provide a}} {{fundamental}} theorem {{that can be}} used in conjunction with Kolmogorov asymptotic conditions to derive the first moments of well-known estimators of the actual error rate in linear discriminant analysis of a multivariate Gaussian model under the assumption of a common known covariance matrix. The estimators studied in this paper are plug-in and smoothed <b>resubstitution</b> <b>error</b> estimators, both of which have not been studied before under Kolmogorov asymptotic conditions. As a result of this work, we present an optimal smoothing parameter that makes the smoothed resubstitution an unbiased estimator of the true error. For the sake of completeness, we further show how to utilize the presented fundamental theorem to achieve several previously reported results, namely the first moment of the resubstitution estimator and the actual error rate. We provide numerical examples to show the accuracy of the succeeding finite sample approximations in situations where the number of dimensions is comparable or even larger than the sample size...|$|E
40|$|Abstract—Error {{estimation}} {{must be used}} to {{find the}} accuracy of a designed classifier, {{an issue that is}} critical in biomarker discovery for disease diagnosis and prognosis in genomics and proteomics. This paper presents, for what {{is believed to be the}} first time, the analytical formulation for the joint sampling distribution of the actual and estimated errors of a classification rule. The analysis presented here concerns the linear discriminant analysis (LDA) classification rule and the <b>resubstitution</b> and leave-one-out <b>error</b> estimators, under a general parametric Gaussian assumption. Exact results are provided in the univariate case, and a simple method is suggested to obtain an accurate approximation in the multivariate case. It is also shown how these results can be applied in the computation of condition bounds and the regression of the actual error, given the observed error estimate. In contrast to asymptotic results, the analysis presented here is applicable to finite training data. In particular, it applies in the small-sample settings commonly found in genomics and proteomics applications. Numerical examples, which include parameters estimated from actual microarray data, illustrate the analysis throughout. Index Terms—Classification, cross-validation, error estimation, leave-one-out, linear discriminant analysis, resubstitution, sampling distribution. I...|$|R
40|$|Abstract: The {{basis of}} our study is to {{identify}} the discriminating groups that {{are present in the}} observations as well as looking into the details of the classification of the observation that forms each group. The observations were obtained as a secondary data from a clinical experiment done by Wuensch, K. L in 1992 in his research paper, to identify the effects on the response of the fostered house mice towards species odor. The subjects used are only from the house mice of the species Mus. The nursing mothers selected were only from three species, which are house-mouse (Mus), deer mouse (Peromyscus) or rat (Rattus). The method used in this study is the discriminant analysis techniques. This study established the discriminant functions based on three groups of cross-forested nursing mothers in identifying the effects of response of the subjects towards the species odor. For new predicted membership, it is found that the largest group is group 3 which is the rat (Rattus) group. The <b>resubstitution</b> of the <b>error</b> rate is 30. 6 % and the cross validation error rate is 38. 9 %. Thus, because of the new observation was allocated to group of rat, it shows that the linear discriminant function obtained has been justified with the Discriminant Function Coefficien...|$|R
40|$|Discrete Classification {{problems}} {{abound in}} pattern recognition and data mining applications. One {{of the most}} common discrete rules is the discrete histogram rule. This paper presents exact formulas for the computation of bias, variance, and RMS of the <b>resubstitution</b> and leave-one-out <b>error</b> estimators, for the discrete histogram rule. We also describe an algorithm to compute the exact probability distribution of resubstitution and leave-one-out, as well as ther deviations from the true error rate. Using a parametric Zipf model, we compute the exact performance of resubstitution and leave-one-out, for varying expected true error, number of samples, and classifier complexity (number of bins). We compare this to approximate performance measures — computed by Monte-Carlo sampling — of 10 -repeated 4 -fold cross-validation and the 0. 632 bootstrap error estimator. Our results show that resubstitution is low-biased but much less variable than leave-one-out, and is effectively the superior error estimator between the two, provided classifier complexity is low. In addition, our results indicate that the overall performance of resubstitution, as measured by the RMS, can be substantially better than the 10 -repeated 4 -fold cross-validation estimator, and even comparable to the 0. 632 bootstrap estimator, provided that classifier complexity is low and the expected error rates are moderate. In addition to the results discussed in the paper, we provide an extensive set of plots that can be accessed on a companion website, at th...|$|R
40|$|This study investigates {{and acts}} as a trial {{clinical}} outcome for human motion and behaviour analysis in consensus of health related {{quality of life in}} Malaysia. The proposed technique was developed to analyze and access the quality of human motion {{that can be used in}} hospitals, clinics and human motion researches. It aims to establish how to widespread the quality of life effects of human motion. Reliability and validity are needed to facilitate subject outcomes. An experiment was set up in a laboratory environment with conjunction of analyzing human motion and its behaviour. Five classifiers and algorithms were used to recognize and classify the motion patterns. The proposed PCA-K-Means clustering took 0. 058 seconds for classification process. <b>Resubstitution</b> <b>error</b> for the proposed technique was 0. 002 and achieved 94. 67 % of true positive for total confusion matrix of the classification accuracy. The proposed clustering algorithm achieved higher speed of processing, higher accuracy of performance and reliable cross validation error...|$|E
40|$|Color {{classification}} is {{an important}} method in grading agricultural and biological materials. The objective of this thesis was to develop color classification methods for biological products, with application to real-time grading and seed corn husk deduction. A nomenclature of classification systems was developed to formalize a review of color classification methods. The description and functional classifier types were introduced. ^ To overcome the limitations of available classifiers, when applied to real-time hardware, two original classifiers were developed using a binary representation of class assignments in the color space. The binary classifier of type one (BC 1) used pairwise discriminant functions, whereas the binary classifier of type two (BC 2) used a more complex logic. The binary representations can be implemented with look-up tables or template matching neural networks. ^ Three software packages {{and a number of}} tools, implementing the color classifiers and error evaluation methods, were developed. SPR implemented statistical pattern recognition classifiers, nSPR implemented neural network based classifiers, and Purclass implemented binary classifiers. Four methods were developed to evaluate classifier accuracy: (1) the global error measurements with <b>resubstitution</b> <b>error,</b> leave-one-out error, and hold-out error, (2) the confusion matrix analysis for individual classes, (3) the dimensionality analysis computing the resubstitution errors for all possible combinations of color bands and classifiers, and (4) a set of graphical representations of color classification problems. ^ The software allowed further analysis of neural network classifier 2 ̆ 7 behavior. It was found, for the problem studied, that the learning coefficient of the binary linear classifier of type one (BLC 1) did not influence the convergence of the linear algorithms. The number of iterations necessary to reach the best <b>resubstitution</b> <b>error</b> was random. ^ The BC 1 and BC 2 algorithms were successfully implemented on real-time image processing hardware. Classification rate was 6 images per second with the BLC 2 classifier, for a three class problem, and 512 by 220 pixel color images. ^ The developed real-time color classification system can accurately classify seed corn images and the color vision system improves the method of deduction, over the current manual method. ...|$|E
40|$|AbstractBreast {{cancer is}} {{considered}} {{as the second}} leading cause of cancer deaths among women in the United States. Early detection of cancer is crucial {{in order to reduce}} its negative effects. Recently, magnetic resonance imaging (MRI) has become an important modality in the detection of breast cancer in daily practice. However, routine breast MRI has a moderate specificity that may increase its false positive rates. Therefore, automated detection techniques of malignancy can provide an important tool for clinicians. In this study, different data classification methods were examined to classify breast tumors screened using contrast enhanced MRI. The used data set included 20 subjects categorized clinically into two groups; benign and malignant tumors. MRI scans were first preprocessed to extract imaging features. Then two classification methods were exploited to differentiate between the two tumor's categories using the extracted features. The used classification methods were K-Nearest Neighbor (KNN), and Linear Discriminant Analysis (LDA). The results show a relatively significant classification accuracy compared with pathological analysis, and also the calculated <b>resubstitution</b> <b>error.</b> In summary, the proposed automatic classification techniques can be used as noninvasive diagnostic tools for breast cancer, with the capability of decreasing false positive errors associated with regular MRI diagnosis...|$|E
40|$|We {{propose a}} general method for error {{estimation}} that displays low variance and gcedIIIT low bias as well. This method {{is based on}} "bolstering" the original empirical distribution of the data. It has a directgrectdIk interpretation and can be easily applied to any classification rule andany number of classes. This method can be usedto improve the performance of any error-counting estimation method, such as resubstitution and all cross-validation estimators, particularly in small-sample settings We point out some similarities shared by our method with a previously proposed technique, known as smoothed error estimation. In some important cases, such as a linear classification rule with a Gaussian bolstering kernel, theintegIxM in the bolstered error estimate can be computedexactly. In theged"Gq case, the bolstered error estimate may be computed by Monte-Carlo sampling however, our experiments show that {{a very small number}} of Monte-Carlo samples is needed. This results in a fast error estimator, which is in contrast to other resampling techniques, such as the bootstrap. We provide an extensive simulation study comparing the proposed method with <b>resubstitution,</b> cross-validation, andbootstrap <b>error</b> estimation, for three popular classification rules (linear discriminant analysis, k-nearest-neigf"k and decision trees),using several sample sizes, from small to moderate. The results indicate the proposed method vastly improves on resubstitution and cross-validation, especially for small samples, in terms of bias and variance. In that respect, it is competitive with, andin many occasions superior to, bootstrap error estimation, while being tens to hundreds of times faster. We provide a companion web site, which contains: (1) the complete set of tables and plotsregsd""I the simulation [...] ...|$|R
40|$|Background: Dengue virus {{infection}} {{causes a}} wide spectrum of illness, ranging from sub-clinical to severe disease. Severe dengue is associated with sequential viral infections. A strict definition of primary versus secondary dengue infections requires a combination of several tests performed at different stages of the disease, which is not practical. Methods and Findings: We developed a simple method to classify dengue infections as primary or secondary based on the levels of dengue-specific IgG. A group of 109 dengue infection patients were classified as having primary or secondary dengue infection {{on the basis of a}} strict combination of results from assays of antigen-specific IgM and IgG, isolation of virus and detection of the viral genome by PCR tests performed on multiple samples, collected from each patient over a period of 30 days. The dengue-specific IgG levels of all samples from 59 of the patients were analyzed by linear discriminant analysis (LDA), and one- and two-dimensional classifiers were designed. The one-dimensional classifier was estimated by bolstered <b>resubstitution</b> <b>error</b> estimation to have 75. 1 % sensitivity and 92. 5 % specificity. The two-dimensional classifier was designed by taking also into consideration the number of days after the onset of symptoms, with an estimated sensitivity and specificity of 91. 64 % and 92. 46 %. The performance of the two-dimensional classifier was validated using an independent test set of standard samples from the remaining 50 patients. The classifications of the independent set of sample...|$|E
40|$|A {{cross-validation}} error estimator {{is obtained}} by repeatedly leaving out some data points, deriving classifiers {{on the remaining}} points, computing errors for these classifiers on the left-out points, and then averaging these errors. The 0. 632 bootstrap estimator is obtained by averaging the errors of classifiers designed from points drawn with replacement and then taking a convex combination of this “zero bootstrap ” error with the <b>resubstitution</b> <b>error</b> for the designed classifier. This gives a convex combination of the low-biased resubstitution and the high-biased zero bootstrap. Another convex error estimator suggested in the literature is the unweighted average of resubstitution and cross-validation. This paper treats the following question: Given a feature-label distribution and classification rule, what is the optimal convex combination of two error estimators, i. e. what are the optimal weights for the convex combination. This problem is considered by finding the weights to minimize the MSE of a convex estimator. It also considers optimality under the constraint that the resulting estimator be unbiased. Owing to {{the large amount of}} results coming from the various feature-label models and error estimators, a portion of the results are presented herein and the main body of results appears on a companion website. In the tabulated results, each table treats the classification rules considered for the model, various Bayes errors, and various sample sizes. Each table includes the optimal weights, mean errors {{and standard deviations for the}} relevant error measures, and the MSE and MAE for the optimal convex estimator. Many observations can be made by considering the full set of experiments. Some general trends are outlined in the paper. The general conclusion is that optimizing the weights of a convex estimator can provide substantial improvement, depending on the classification rule...|$|E
40|$|Dengue virus {{infection}} {{causes a}} wide spectrum of illness, ranging from sub-clinical to severe disease. Severe dengue is associated with sequential viral infections. A strict definition of primary versus secondary dengue infections requires a combination of several tests performed at different stages of the disease, which is not practical. We developed a simple method to classify dengue infections as primary or secondary based on the levels of dengue-specific IgG. A group of 109 dengue infection patients were classified as having primary or secondary dengue infection {{on the basis of a}} strict combination of results from assays of antigen-specific IgM and IgG, isolation of virus and detection of the viral genome by PCR tests performed on multiple samples, collected from each patient over a period of 30 days. The dengue-specific IgG levels of all samples from 59 of the patients were analyzed by linear discriminant analysis (LDA), and one- and two-dimensional classifiers were designed. The one-dimensional classifier was estimated by bolstered <b>resubstitution</b> <b>error</b> estimation to have 75. 1 % sensitivity and 92. 5 % specificity. The two-dimensional classifier was designed by taking also into consideration the number of days after the onset of symptoms, with an estimated sensitivity and specificity of 91. 64 % and 92. 46 %. The performance of the two-dimensional classifier was validated using an independent test set of standard samples from the remaining 50 patients. The classifications of the independent set of samples determined by the two-dimensional classifiers were further validated by comparing with two other dengue classification methods: hemagglutination inhibition (HI) assay and an in-house anti-dengue IgG-capture ELISA method. The decisions made with the two-dimensional classifier were in 100 % accordance with the HI assay and 96 % with the in-house ELISA. Once acute dengue infection has been determined, a 2 -D classifier based on common dengue virus IgG kits can reliably distinguish primary and secondary dengue infections. Software for calculation and validation of the 2 -D classifier is made available for download...|$|E

