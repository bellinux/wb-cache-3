8|380|Public
5000|$|The {{creation}} of a separate asset <b>resolution</b> <b>entity</b> was an idea proposed by foreign specialists. Their proposals were initially contested and resisted by the Spanish authorities but later accepted as being the appropriate response to the crisis ...|$|E
40|$|International audienceThis survey {{presents}} {{the concept of}} Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference <b>resolution,</b> <b>entity</b> linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data contex...|$|E
40|$|In this paper, we {{describe}} {{a system that}} automatically extracts quotations from news feeds, and allows efficient retrieval of the semantically annotated quotes. APIs for realtime querying of over 10 million quotes extracted from recent news feeds are publicly available. In addition, each day we add around 60 thousand new quotes extracted from around 50 thousand news articles or blogs. We apply computational linguistic techniques such as coreference <b>resolution,</b> <b>entity</b> recognition and disambiguation to improve both precision and recall of the quote detection. We support faceted search on both speakers and entities mentioned in the quotes. 1...|$|E
5000|$|While <b>entity</b> <b>resolution</b> {{solutions}} include data matching technology, many data matching offerings {{do not fit}} {{the definition}} of <b>entity</b> <b>resolution.</b> Here are four factors that distinguish <b>entity</b> <b>resolution</b> from data matching, according to John Talburt, director of the UALR Center for Advanced Research in <b>Entity</b> <b>Resolution</b> and Information Quality: ...|$|R
40|$|<b>Entity</b> <b>resolution,</b> {{the process}} of {{determining}} if two or more references correspond to the same entity, is an emerging area of study in computer science. While <b>entity</b> <b>resolution</b> models leverage artificial intelligence, machine learning, and data mining techniques, relationships between various models remain ill-specified. Despite growth in both research and literature, investigations are scattered across communities with minimal communication. This paper introduces a conceptual framework, called ENRES, for explicit and formal <b>entity</b> <b>resolution</b> model definition. Through ENRES, we illustrate how several models solve related, though distinctly different, variants of <b>entity</b> <b>resolution.</b> In addition, we prove the existence of <b>entity</b> <b>resolution</b> challenges yet {{to be addressed by}} past or current research...|$|R
40|$|Machine {{learning}} {{methods are}} being successfully applied in text mining. Because of ambiguities which are inherently present in natural languages, {{we are faced}} with a challenge of determining the actual identities of entities mentioned in a document. Disambiguation is a problem that can be successfully solved by <b>entity</b> <b>resolution</b> methods. This thesis studies various possibilities for improving <b>entity</b> <b>resolution</b> performance by using various types of background knowledge and statistical learning. We compare precision and recall of pair-wise <b>entity</b> <b>resolution</b> with collective resolution. We also study the possibility of employing background knowledge. For this purpose, we define a multi-relational <b>entity</b> <b>resolution</b> approach, capable of representing implicit as well as explicit relationships. We discover the benefits of using entity co-occurrences and content similarities as implicit relationships. We also propose an approach capable of handling such heterogeneous relations for collective <b>entity</b> <b>resolution.</b> ...|$|R
40|$|In this paper, {{we present}} an {{architecture}} for the Connection-mode Network Service traffic routing. Basically, we extend {{the scope of}} the applicability of the Subnetwork Address <b>Resolution</b> <b>Entity</b> to the IS-IS Intra-domain case. The SNARE concept is flexible since it allows one to design routing as purely centralized or completely distributed. We note that it is possible either to use ISO 10589 or to adapt this protocol in order to ensure routing information distribution. For the inter-domain case, we propose to re-use as much as possible the existing IDRP. Some modifications are either required for a proper operation of the protocol for the CONS support or are desirable to benefit from the native CO capability. © 1992. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|E
40|$|Multi-resolution {{representation}} of simulated entities is considered essential {{for a growing}} portion of distributed simulations. Heretofore, modelers have represented entites at just one level of resolution, or have represented concurrent representations in an inconsistent mannel: We {{address the question of}} the cost of maintaining multiple, concurrent representations. We present a brief overview of our concept of a Multiple <b>Resolution</b> <b>Entity</b> (MRE) and Attribute Dependency Graph (ADG) both originally described elsewhere, and then compare simulation and consistency costs of some approaches, including our own MRWADG-based approach, to multi-resolution modeling. The cost analysis presented here is the first known analysis of its type, and will provide a basis for simulation designers to determine the best, and most cost-effective approach to supporting simulation of entities at different levels of resolution concurrently. ...|$|E
40|$|International audienceText {{analysis}} methods {{widely used in}} digital humanities often involve word co-occurrence, e. g. concept co-occurrence networks. These methods provide a useful corpus overview, but cannot determine the predicates that relate co-occurring concepts. Our goal was identifying propositions expressing the points supported or opposed by participants in international climate negotiations. Word co-occurrence methods were not sufficient, and an analysis based on open relation extraction had limited coverage for nominal predicates. We present a pipeline which identifies the points that different actors support and oppose, via a domain model with support/opposition predicates, and analysis rules that exploit the output of semantic role labelling, syntactic dependencies and anaphora <b>resolution.</b> <b>Entity</b> linking and keyphrase extraction are also performed on the propositions related to each actor. A user interface allows examining the main concepts in points supported or opposed by each participant, which participants {{agree or disagree with}} each other, and about which issues. The system is an example of tools that digital humanities scholars are asking for, to render rich textual information (beyond word co-occurrence) more amenable to quantitative treatment. An evaluation of the tool was satisfactory...|$|E
40|$|Abstract—An {{important}} aspect of maintaining information quality in data repositories is determining which sets of records refer to the same real world entity. This so called <b>entity</b> <b>resolution</b> problem comes up frequently for data cleaning and integration. <b>Entity</b> <b>Resolution</b> (ER) {{is a problem that}} arises in many information integration applications. ER process identifies duplicated records that refer to the same real-world entity, and derives composite information about the entity. The cost of the ER process is high. In this propose paper, input data is split according to the blocking variables. As no comparisons are conducted between different blocks, each block can be processed independently form all others. Blocks can contain different numbers of records which results in varying processing times. We propose an effective blocking for combining multiple <b>entity</b> <b>resolution</b> systems. Keywords- <b>entity</b> <b>resolution,</b> data integration, data reduction, indexing, pre-processing I...|$|R
40|$|One {{significant}} {{challenge to}} scaling <b>entity</b> <b>resolution</b> algorithms to massive datasets is understanding how performance changes after moving {{beyond the realm}} of small, manually labeled reference datasets. Unlike traditional machine learning tasks, when an <b>entity</b> <b>resolution</b> algorithm performs well on small hold-out datasets, there is no guarantee this performance holds on larger hold-out datasets. We prove simple bounding properties between the performance of a match function on a small validation set and the performance of a pairwise <b>entity</b> <b>resolution</b> algorithm on arbitrarily sized datasets. Thus, our approach enables optimization of pairwise <b>entity</b> <b>resolution</b> algorithms for large datasets, using a small set of labeled data...|$|R
40|$|<b>Entity</b> <b>resolution</b> is {{the problem}} of {{reconciling}} database references corresponding to the same real-world entities. Given the abundance of publicly available databases that have unresolved entities, we motivate the problem of query-time <b>entity</b> <b>resolution</b> quick and accurate resolution for answering queries over such unclean databases at query-time. Since collective <b>entity</b> <b>resolution</b> approaches [...] - where related references are resolved jointly [...] - {{have been shown to be}} more accurate than independent attribute-based <b>resolution</b> for off-line <b>entity</b> <b>resolution,</b> we focus on developing new algorithms for collective <b>resolution</b> for answering <b>entity</b> <b>resolution</b> queries at query-time. For this purpose, we first formally show that, for collective resolution, precision and recall for individual entities follow a geometric progression as neighbors at increasing distances are considered. Unfolding this progression leads naturally to a two stage expand and resolve query processing strategy. In this strategy, we first extract the related records for a query using two novel expansion operators, and then resolve the extracted records collectively. We then show how the same strategy can be adapted for query-time <b>entity</b> <b>resolution</b> by identifying and resolving only those database references that are the most helpful for processing the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real-time using our adaptive approach while preserving the gains of collective resolution. In addition to experiments on real datasets, we use synthetically generated data to empirically demonstrate the validity of the performance trends predicted by our analysis of collective <b>entity</b> <b>resolution</b> over a wide range of structural characteristics in the data...|$|R
40|$|Many {{databases}} contain {{uncertain and}} imprecise references to real-world entities. The absence of identifiers for the underlying entities {{often results in}} a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated {{through the use of}} entity <b>resolution.</b> <b>Entity</b> resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms...|$|E
40|$|The [1] {{introduces}} a new method for resolving location entities in geospatial data. A typical geospatial database contains heterogeneous {{features such as}} location name, spatial coordinates, location type and demographic information. Authors of [1] investigate the use {{of all of these}} features in algorithms for geospatial entity <b>resolution.</b> <b>Entity</b> resolution is complicated also because different sources may use different vocabularies for describing the location types and a semantic mapping is required. [1] proposes a novel approach which learns how to combine the different features to perform accurate resolutions. II. [2] investigates integration of three datasets and proposes methods that can be easily generalized to any number of datasets. Two approaches that use only locations of objects are presented and compared. In one approach, a join algorithm for two datasets is applied sequentially. In the second approach, all the integrated datasets are processed simultaneously. III. [3] describes a Web-based query system for semantically heterogeneous government-produced data. One of the main problems in querying distributed government data sources is the difference in semantics used by various jurisdictions. authors of [2] extend work in schema integration by focusing on resolving semantics at the value level in addition to the schema or attribute level. IV. Integration of two road maps is finding a matching between pairs of objects that represent, in the maps, the same realworld road. Several algorithms were proposed in the past for road-map integration; however, these algorithms are not efficient and some of them even require human feedback. Thus, they are not suitable for many important applications (e. g., Web services) where efficiency, in terms of both time and space, is crucial...|$|E
40|$|Abstract When {{querying}} data providers on the web, one has no guar-antee {{that they}} will reply within a given time. Some providers may even not answer at all. This makes it infeasible {{to wait for a}} complete result before beginning with the <b>entity</b> <b>resolution.</b> In order to solve this prob-lem, we present a query-time <b>entity</b> <b>resolution</b> approach that takes the asynchronous nature of the replies from data providers into account by starting the <b>entity</b> <b>resolution</b> as soon as first results are returned. Re-solved entities are propagated from the <b>entity</b> <b>resolution</b> engine to the mobile client as early as possible. Resolution results that are produced later are send as updates to the client and thus improve earlier results. ...|$|R
40|$|An {{important}} aspect of maintaining information quality in data repositories is determining which sets of records refer to the same real world entity. This so called <b>entity</b> <b>resolution</b> problem comes up frequently for data cleaning and integration. In many domains, the underlying entities exhibit strong ties between themselves. Friendships in social networks and collaborations between researchers are examples of such ties. In such cases, we stress the need for collective <b>entity</b> <b>resolution</b> where, instead of independently tagging pairs of records as duplicates or non-duplicates, related entities are resolved collectively. We present different algorithms for collective <b>entity</b> <b>resolution</b> that combine relational evidence with traditional attribute-based approaches to improve <b>entity</b> <b>resolution</b> performance in a scalable manner. ...|$|R
40|$|In this paper, {{we address}} the problem of <b>entity</b> <b>resolution,</b> where given many {{references}} to underlying objects, the task is to predict which references correspond to the same object. We propose a probabilistic model for collective <b>entity</b> <b>resolution.</b> Our approach differs from other recently proposed <b>entity</b> <b>resolution</b> approaches in that it is a) unsupervised, b) generative and c) introduces a hidden `group' variable to capture collections of entities which are commonly observed together. The <b>entity</b> <b>resolution</b> decisions are not considered on an independent pairwise basis, but instead decisions are made collectively. We focus on how the use of relational links among the references can be exploited. We show how we can use Gibbs Sampling to infer the collaboration groups and the entities jointly from the observed co-author relationships among entity references and how this improves <b>entity</b> <b>resolution</b> performance. We demonstrate the utility of our approach on two real-world bibliographic datasets. In addition, we present preliminary results on characterizing conditions under which collaborative information is useful...|$|R
40|$|International audienceThis {{tutorial}} {{provides an}} overview of the key research results in the area of <b>entity</b> <b>resolution</b> that are relevant to addressing the new challenges in <b>entity</b> <b>resolution</b> posed by the Web of data, in which real world entities are described by interlinked data rather than documents. Since such descriptions are usually partial, overlapping and sometimes evolving, <b>entity</b> <b>resolution</b> emerges as a central problem both to increase dataset linking but also to search the Web of data for entities and their relations...|$|R
40|$|<b>Entity</b> <b>resolution</b> is a {{fundamental}} problem in data integra-tion dealing with the combination of data from different sources to a unified view of the data. <b>Entity</b> <b>resolution</b> is inherently an uncertain process because the decision to map a set of records to the same entity cannot be made with certainty unless these are identical in all of their attributes or have a common key. In the light of recent advancement in data accumulation, management, and analytics landscape (known as big data) the tutorial re-evaluates the entity reso-lution process and in particular looks at best ways to handle data veracity. The tutorial ties <b>entity</b> <b>resolution</b> with re-cent advances in probabilistic database research, focusing on sources of uncertainty in the <b>entity</b> <b>resolution</b> process...|$|R
40|$|We present FEVER, a new {{evaluation}} {{platform for}} <b>entity</b> <b>resolution</b> approaches. The modular {{structure of the}} FEVER framework supports the incorporation or reconstruction of many previously proposed approaches for <b>entity</b> <b>resolution.</b> A distinctive feature of FEVER is that it not only evaluates traditional measures such as precision and recall but also the effort for configuring (e. g., parameter tuning, training) a good <b>entity</b> <b>resolution</b> approach. FE-VER thus strives for a fair comparative evaluation of different approaches by considering both the effectiveness and configuration effort. 1...|$|R
40|$|International audienceEntity {{resolution}} aims {{to identify}} {{descriptions of the}} same entity within or across knowledge bases. In this work, we provide a comprehensive and cohesive overview of the key research results {{in the area of}} <b>entity</b> <b>resolution.</b> We are interested in frameworks addressing the new challenges in <b>entity</b> <b>resolution</b> posed by the Web of data in which real world entities are described by interlinked data rather than documents. Since such descriptions are usually partial, overlapping and sometimes evolving, <b>entity</b> <b>resolution</b> emerges as a central problem both to increase dataset linking, but also to search the Web of data for entities and their relations. We focus on Web-scale blocking, iterative and progressive solutions for <b>entity</b> <b>resolution.</b> Specifically, to reduce the required number of comparisons, blocking is performed to place similar descriptions into blocks and executes comparisons to identify matches only between descriptions within the same block. To minimize the number of missed matches, an iterative <b>entity</b> <b>resolution</b> process can exploit any intermediate results of blocking and matching, discovering new candidate description pairs for resolution. Finally, we overview works on progressive <b>entity</b> <b>resolution,</b> which attempt to discover as many matches as possible given limited computing budget, by estimating the matching likelihood of yet unresolved descriptions, based on the matches found so far...|$|R
40|$|<b>Entity</b> <b>resolution</b> is {{the task}} of {{determining}} which references in a data set refer to distinct real world <b>entities.</b> Many <b>entity</b> <b>resolution</b> techniques have been applied successfully to resolve references with predictable structure. We present a framework for performing <b>entity</b> <b>resolution</b> on clues from a quiz bowl competition to identify clues with the same answer. The clues are natural language text and have very little structure aside from being well formed English sentences. We explore two different blocking algorithms, canopies and iterative blocking, to facilitate scaling to large datasets. ...|$|R
40|$|<b>Entity</b> <b>resolution</b> has {{received}} considerable attention in recent years. Given many references to underlying entities, {{the goal is}} to predict which references correspond to the same entity. We show how to extend the Latent Dirichlet Allocation model for this task and propose a probabilistic model for collective <b>entity</b> <b>resolution</b> for relational domains where references are connected to each other. Our approach differs from other recently proposed <b>entity</b> <b>resolution</b> approaches in that it is a) generative, b) does not make pair-wise decisions and c) captures relations between entities through a hidden group variable. We propose a novel sampling algorithm for collective <b>entity</b> <b>resolution</b> which is unsupervised and also takes entity relations into account. Additionally, we do not assume the domain of entities to be known and show how to infer the number of entities from the data. We demonstrate the utility and practicality of our relational <b>entity</b> <b>resolution</b> approach for author resolution in two real-world bibliographic datasets. In addition, we present preliminary results on characterizing conditions under which relational information is useful. ...|$|R
5000|$|The U.S. {{government}} does not regulate Safe Harbor, which is self-regulated through its private sector members and the dispute <b>resolution</b> <b>entities</b> they pick. The Federal Trade Commission [...] "manages" [...] the system under the oversight of the U.S. Department of Commerce.{{to comply with the}} commitments can be penalized under the Federal Trade Commission Act by administrative orders and civil penalties of up to $16,000 per day for violations. If an organization fails to comply with the framework it must promptly notify the Department of Commerce, or else it can be prosecuted under the 'False Statements Act'.|$|R
40|$|We {{demonstrate}} an interactive, web-based tool {{which helps}} historians to do Genealogical Entitiy Resolution. This work has two main goals. First, it uses Machine Learning (ML) algorithms to assist humanites researchers to perform Genealogical <b>Entity</b> <b>Resolution.</b> Second, it facilitates {{the generation of}} benchmark data for computer scientists to improve available ML-based <b>Entity</b> <b>Resolution</b> techniques...|$|R
40|$|Most {{research}} into <b>entity</b> <b>resolution</b> (also known as record linkage or data matching) has {{concentrated on the}} quality of the matching results. In this paper, we focus on matching time and scalability, with the aim to achieve large-scale real-time <b>entity</b> <b>resolution.</b> Traditional <b>entity</b> <b>resolution</b> techniques have assumed the matching of two static databases. In our networked and online world, however, it is becoming increasingly important for many organisations to be able to conduct <b>entity</b> <b>resolution</b> between a collection of often very large databases and a stream of query or update records. The matching should be done in (near) real-time, and be as automatic and accurate as possible, returning a ranked list of matched records for each given query record. This task therefore becomes similar to querying large document collections, as done for example by Web search engines, however based on a different type of documents: structured database records that, for example, contain personal information, such as names and addresses. In this paper, we investigate inverted indexing techniques, as commonly used in Web search engines, and employ them for real-time <b>entity</b> <b>resolution.</b> We present two variations of the traditional inverted index approach, aimed at facilitating fast approximate matching. We show encouraging initial results on large real-world data sets, with the inverted index approaches being up-to one hundred times faster than the traditionally used standard blocking approach. However, this improved matching speed currently comes at a cost, in that matching quality for larger data sets can be lower compared to when standard blocking is used, and thus more work is required...|$|R
50|$|<b>Entity</b> <b>resolution</b> {{engines are}} {{typically}} used to uncover risk, fraud, and conflicts of interest, {{but are also}} useful tools for use within customer data integration (CDI) and master data management (MDM) requirements. Typical uses for <b>entity</b> <b>resolution</b> engines include terrorist screening, insurance fraud detection, USA Patriot Act compliance, organized retail crime ring detection and applicant screening.|$|R
40|$|Cloud {{infrastructures}} {{enable the}} efficient parallel execution of data-intensive {{tasks such as}} <b>entity</b> <b>resolution</b> on large datasets. We investigate challenges and possible solutions of using the MapReduce programming model for parallel <b>entity</b> <b>resolution.</b> In particular, we propose and evaluate two MapReduce-based implementations for Sorted Neighborhood blocking that either use multiple MapReduce jobs or apply a tailored data replication...|$|R
40|$|Abstract: Data fusion is a {{hot topic}} in data {{integration}} which at least includes the two stages: <b>entity</b> <b>resolution</b> and data conflict resolution. However, the existing fusion process is transparent and the fusion stages are isolated. So in this paper, we proposed a traceable data fusion mechanism based on data provenance which can trace the data sources of fusion re-sults and the evolutionary process. The mechanism mainly targets forwards <b>entity</b> <b>resolution</b> and data conflict resolution stage. We represented the provenance of data origin using PI-CS which is more accurate because PI-CS can record the in-termediate process of data evolution. In order to record the evolution process of data fusion, we proposed two transforma-tion provenances: <b>entity</b> <b>resolution</b> provenance and data conflict resolution provenance which record respectively the evo-lution process of <b>entity</b> <b>resolution</b> and data conflict resolution. Finally, we give an example to validate {{the availability of the}} traceable mechanism for data fusion...|$|R
40|$|<b>Entity</b> <b>resolution,</b> {{the problem}} of {{identifying}} the underlying entity of references found in data, has been researched for many decades in many communities. A common theme in this research has been the importance of incorporating relational features into the <b>resolution</b> process. Relational <b>entity</b> <b>resolution</b> is particularly important in knowledge graphs (KGs), which have a regular structure capturing entities and their interrelationships. We identify three major problems in KG entity resolution: (1) intra-KG reference ambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending KGs with new facts. We implement a framework that generalizes across these three settings and exploits this regular structure of KGs. Our framework has many advantages over custom solutions widely deployed in industry, including collective inference, scalability, and interpretability. We apply our framework to two real-world KG <b>entity</b> <b>resolution</b> problems, ambiguity in NELL and merging data from Freebase and MusicBrainz, demonstrating the importance of relational features...|$|R
40|$|Accurate and {{efficient}} <b>entity</b> <b>resolution</b> {{is an open}} challenge of particular relevance to intelligence organisations that collect large datasets from disparate sources with differing levels of quality and standard. Starting from a first-principles formulation of <b>entity</b> <b>resolution,</b> this paper presents a novel <b>Entity</b> <b>Resolution</b> algorithm that introduces a data-driven blocking and record-linkage technique based on the probabilistic identification of entity signatures in data. The scalability and accuracy of the proposed algorithm are evaluated using benchmark datasets and shown to achieve state-of-the-art results. The proposed algorithm can be implemented simply on modern parallel databases, which allows it to be deployed with relative ease in large industrial applications...|$|R
40|$|<b>Entity</b> <b>resolution</b> (ER) is {{the task}} of {{identifying}} records belonging to the same entity (e. g. individual, group) across one or multiple databases. Ironically, it has multiple names: deduplication and record linkage, among others. In this paper we survey metrics used to evaluate ER results in order to iteratively improve performance and guarantee sufficient quality prior to deployment. Some of these metrics are borrowed from multi-class classification and clustering domains, though some key differences exist differentiating <b>entity</b> <b>resolution</b> from general clustering. Menestrina et al. empirically showed rankings from these metrics often conflict with each other, thus our primary motivation for studying them. This paper provides practitioners the basic knowledge to begin evaluating their <b>entity</b> <b>resolution</b> results. Comment: Technical repor...|$|R
5000|$|Insight into text through topic modeling, {{relationship}} extraction, <b>entity</b> <b>resolution,</b> semantic search, and event detection.IMAGERY AND VIDEO PROCESSING ...|$|R
40|$|<b>Entity</b> <b>resolution,</b> or data integration, is {{the process}} that {{identifies}} and matches data records that refer to the same real world entity. In many cases, <b>entity</b> <b>resolution</b> needs to be performed in sub-seconds. The challenge is especially enormous when <b>entity</b> <b>resolution</b> is processed on large scale datasets. Some techniques have been proposed to solve the problem. Indexing is a technique that enables real-time <b>entity</b> <b>resolution</b> by only comparing record pairs that have the same encoding value and thus largely {{reduce the number of}} comparisons. Similarity-aware Indexing (SAI) improves the performance of traditional indexing by pre-calculating similarities of attribute values. Another technique known as Locality Sensitive Hashing (LSH) provides a similarity based filtering. Minhash is an implementation of LSH. In this report, a two-stage approach which combines LSH with SAI is proposed. At the first stage, LSH is adopted to approximate filter data records and preserve the potential matches. At the second stage, SAI is used to compare potential matches to obtain a precise and accurate result. This approach is evaluate...|$|R
5000|$|Data Access Control: One of Jello's key {{features}} is its inline Authorization Model. With Jello you can assign different access levels for data elements at any <b>resolution</b> (Namespaces, <b>Entities,</b> Fields, Actions) and specify who is authorized {{to access the}} data via the REST API.|$|R
40|$|For {{unsupervised}} clustering, traditional accuracy metrics {{based on}} the constituent records do not often reflect the accuracy at the cluster level. For a specific example, consider <b>entity</b> <b>resolution</b> where {{the goal is to}} cluster records across multiple, heterogeneous data sources into “entities. ” Measuring the accuracy of <b>entity</b> <b>resolution</b> is not as simple as applying the well known record level metrics of precision and recall. Rather than using traditional tuple-based metrics for accuracy, we posit that new, entity-based metrics should be defined instead. Defining entitylevel metrics gains users a less source biased, yet deeper insight into <b>entity</b> <b>resolution</b> performance. We show that traditional record linkage metrics are not appropriate, and offer some early thoughts on entity-centric measurements that are more so. 1...|$|R
