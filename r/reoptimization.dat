249|9|Public
40|$|This paper studies <b>reoptimization</b> {{versions}} of various min-sum scheduling problems. The <b>reoptimization</b> setting can generally {{be described as}} follows: given an instance of the problem for which an optimal solution is provided and given some local perturbations on that instance, we search for a near-optimal solution for the modified instance requiring very little computation. We focus on two kinds of modifications: job-insertions and job-deletions. For all <b>reoptimization</b> problems considered, we show how very simple <b>reoptimization</b> algorithms can ensure constant approximation ratios, and also provide some lower bounds for whole classes of <b>reoptimization</b> algorithms...|$|E
40|$|The german {{automobile}} association ADAC {{maintains a}} fleet of 1700 vehicles and has agreements with around 5000 service contractors. With these ressources, they help people whose cars have broken down on the road. Those people can call an ADAC help center, and within 10 seconds, an assignment of a service ressource to their request is made. At the same time, for all service vehicles, tours through the assigned requests have to be planned so as to minimize a certain (complicated) cost function for this so-called dispatch. No usefule knowledge about future requests {{is available at the}} time being. Therefore, the current policy of the automated system, developed in joint work with Sven O. Krumke, is to reoptimize the whole dispatch upon the occurrence of each relevant event, like the arrival of a new request. A similar online-optimization problem appears in the pallet elevator group control in a large distribution center of Herlitz PBS AG in Falkensee near Berlin. The problem with <b>reoptimization</b> policies in general is that, depending on the <b>reoptimization</b> objective, an arbitrarily large deferment of individual requests can be observed. In a way, individual requests are sacrificed in favor of a good performance according to the <b>reoptimization</b> objective. Nevertheless, w. r. t. the <b>reoptimization</b> objective, the <b>reoptimization</b> policies in the long run usually perform much better than the currently known policies that can not cause infinite deferment. Therefore, the goal is to modify <b>reoptimization</b> policies so as to prevent deferment. Sometimes deferment can be almost eliminated by enhancing the <b>reoptimization</b> objective with some terms that penalize waiting, but service in a fixed time can still not be guaranteed, and this kind of objective function engineering is a very time consuming tuning issue, interfering with the orgininal management objective. In this talk, the new policy of flow and makespan constrained <b>reoptimization</b> with <b>reoptimization</b> admission control is introduced. The main result is that, under d-reasonable load, for any <b>reoptimization</b> model, this policy yields a maximal flow time that is bounded by a constant 2 d, depending only on the system load parameter d, not on the instance. In simulation experiments for the elevator group control problem we still obtain a very satisfactory average performance w. r. t. the <b>reoptimization</b> objective...|$|E
40|$|A <b>reoptimization</b> problem {{describes}} the following scenario: Given {{an instance of}} an optimization problem together with an optimal solution for it, {{we want to find}} a good solution for a locally modified instance. In this paper, we deal with <b>reoptimization</b> variants of the shortest common superstring problem where the local modifications consist of adding or removing a single string. We show NP-hardness of these <b>reoptimization</b> problems and design several approximation algorithms for them...|$|E
40|$|Rotemberg’s [1982] price {{adjustment}} costs framework {{is a popular}} sticky price specification; yet, the data provides little information on the magnitude of those costs. This paper finds a plausible range of parameterizations for those {{price adjustment}} costs. Our {{results show that the}} specific size of the price adjustment costs depends on the average markup of price over real marginal cost and the average time firms wait to reoptimize their price. In particular, the price adjustment costs are higher when the average markup is lower and the mean time between price <b>reoptimizations</b> is longer. JEL Classification: E 31; E 32; E 52...|$|R
30|$|Experiment 3. Experiment 3 aims to {{investigate}} {{the correlation between the}} per-tuple processing cost and the performance of A-greedy*. Experiment 1 is repeated considering, however, filters having 10 sec per-tuple processing cost (see Table 2). We can see that the mean overall cost improvement of A-greedy* is somewhat lower, while when A-greedy* employs the Meta-algorithm we have performance loss, i.e., the mean overall cost is higher than that of A-greedy by 1 %. The reason behind that phenomenon is the following: as the per-tuple filter processing cost increases, the data processing cost dominates over the runtime overhead. Thus, it is more beneficial to perform many query <b>reoptimizations</b> to keep the data processing cost as low as possible.|$|R
40|$|Due to time-inconsistency or {{political}} turnover, policymakers' promises {{are not always}} fulfilled. We analyze an optimal fiscal policy problem where the plans made by the benevolent government are periodically revised. In this loose commitment setting, the properties of labor and capital income taxes are significantly different than under the full-commitment and no-commitment assumptions. Because of the occasional <b>reoptimizations,</b> the average capital income tax is positive even in the long-run. Also, the autocorrelation of taxes is lower, their volatility with respect to output increases and the correlation between capital income taxes and output changes sign. Our method {{can be used to}} analyze the plausibility and the importance of commitment in a wide-class of dynamic problems. Commitment No-commitment Fiscal policy...|$|R
40|$|We {{present some}} <b>reoptimization</b> {{techniques}} for computing (shortest) hyperpath weights in a directed hypergraph. These techniques are exploited {{to improve the}} worst-case computational complexity (as well as the practical performance) of an algorithm finding the K shortest hyperpaths in acyclic hypergraphs. Keywords: Network programming, Directed hypergraphs, K shortest hyperpaths, <b>Reoptimization.</b> ...|$|E
40|$|This {{note about}} work in {{progress}} suggests new policies for combinatorial online optimization problems where requests have to be served and the longterm objective is a sophisticated combination of request based cost (quality of service) and service based cost (operational cost). Examples are the online dispatching of automobile service units or the online control of cargo elevators. The new policies are <b>reoptimization</b> policies that do not use any (stochastic) information about future requests. For the first time, the new policies enhance <b>reoptimization</b> with a flow time guarantee, depending on the load in the system. In this sense, the new policies add fairness to <b>reoptimization</b> at a small cost: In the elevator control problem, e. g., the average performance w. r. t. the long-term objective function in simulation experiments only slightly worse than plain <b>reoptimization</b> for various long-term objectives...|$|E
40|$|We address <b>reoptimization</b> {{issues for}} the Steiner tree problem. We assume that an optimal {{solution}} is given for some instance {{of the problem and}} the objective is to maintain a good solution when the instance is subject to minor modifications, the simplest such modifications being vertex insertions and deletions. We propose fast <b>reoptimization</b> strategies for the case of vertex insertions and we show that maintenance of a good solution for the "shrunk" instance, without ex nihilo computation, is impossible when vertex deletions occur. We also provide lower bounds for the approximation ratios of the <b>reoptimization</b> strategies studied...|$|E
40|$|A {{practical}} warm-start {{procedure is}} described for the infeasible primal-dual interior-point method (IPM) employed {{to solve the}} restricted master problem within the cutting-plane method. In contrast to the theoretical developments in this field, the approach {{presented in this paper}} does not make the unrealistic assumption that the new cuts are shallow. Moreover, it treats systematically the case when a large number of cuts are added at one time. The technique proposed in this paper has been implemented in the context of HOPDM, the state of the art, yet public domain, interior-point code. Numerical results confirm a high degree of efficiency of this approach: regardless of the number of cuts added at one time (can be thousands in the largest examples) and regardless of the depth of the new cuts, <b>reoptimizations</b> are usually done with a few additional iterations. ...|$|R
40|$|The dQUOB system {{satisfies}} {{clients in}} need of specific information from high-volume data streams. The data streams we speak of are the flows of data that exist in large-scale visualizations, video streaming to {{a large number of}} distributed users, and high volume business transactions. dQUOB introduces the idea of conceptualizing a data stream as a set of relational database tables. Within this model, a scientist can request information in an SQL-like query. Transformation or computation that often needs to be performed on the data before it arrives at a client can be conceptualized as computation performed on consecutive views of the data; computation is associated with each view. Additionally, the dQUOB system moves the query code into the data stream as a quoblet; an efficient compiled code. The relational database data model has the significant advantage of presenting opportunities for efficient <b>reoptimizations</b> of queries and sets of queries. Using examples from global atmospheric [...] ...|$|R
40|$|In the {{conventional}} optimal monetary policy framework, two key assumptions underline the full commitment solution : Monetary authority is perfectly credible, and can commit for {{an infinite number}} of periods. Using a baseline forward-looking model, this study explores the implications of relaxing these assumptions in turn. First, finite lasting commitments are introduced using a stochastic exogenous process that generates policy <b>reoptimizations.</b> As a consequence, monetary policy is characterized with a continuum from pure discretion to full commitment. Second, we solve the optimal and robust targeting rules when the central bank confronts imperfect and/or uncertain credibility. Imperfect credibility is defined as {{a situation in which the}} private sector expects the commitment regime to end sooner than that is intended by the policy maker. The results indicate that, under imperfect credibility, optimal policy becomes observationally closer to the discretionary solution, the more being so as the degree of uncertainty rises. These findings may be insightful for explaining the observed near-discretionary behavior of the central banks, which indeed operate under imperfect credibility. Optimal Monetary Policy, Stabilization Bias, Imperfect Credibility, Discretion, Commitment...|$|R
40|$|We {{consider}} the {{vehicle routing problem}} with stochastic demands (VRPSD) under <b>reoptimization.</b> We develop and analyze a finite-horizon Markov decision process (MDP) formulation for the single vehicle case, and establish a partial characterization of the optimal policy. We also propose a heuristic solution methodology for our MDP, named partial <b>reoptimization,</b> {{based on the idea}} of restricting attention to a subset of all the possible states and computing an optimal policy on this restricted set of states. We discuss two families of computationally efficient partial <b>reoptimization</b> heuristics and illustrate their performance on a set of instances with up to and including 100 customers. Comparisons with an existing heuristic from the literature and a lower bound computed with complete knowledge of customer demands show that our best partial <b>reoptimization</b> heuristics outperform this heuristic and are on average no more than 10 - 13 % away from this lower bound, {{depending on the type of}} instances. 1...|$|E
30|$|In particular, in a small-sized profile window, the {{estimates}} or the statistics that are derived during the monitoring phase may vary significantly among different placements of the profile window, {{even in cases}} where the actual characteristics of the runtime environment do not change. As a result, the analysis and, subsequently, the <b>reoptimization</b> phases may be triggered quite often incurring overhead that outbalances the potential benefits of <b>reoptimization.</b>|$|E
40|$|International audienceWe {{implement}} a fast <b>reoptimization</b> algorithm for MIN SPANNING TREE under vertex insertions, initially proposed and analyzed {{in the work}} of Boria and Paschos [Boria N, Paschos VTh. Fast <b>reoptimization</b> for the minimum spanning tree problem. J Discrete Algor 2010, 8 : 296 – 310] and study its experimental approximation behavior in randomly generated graphs. The <b>reoptimization</b> setting can briefly be formulated as follows: given an instance of the problem for which we already know some optimal solution, and given some ‘small’ perturbations on this instance, is it possible to compute a new (optimal or at least near-optimal) solution for the modified instance without computation from scratch? We focus in this article on the most popular modification: vertex-insertion...|$|E
40|$|The dQUOB system satis es client {{need for}} speci c {{information}} from high-volume data streams. The data streams {{we speak of}} are the ow of data existing during large-scale visualizations, video streaming to large numbers of distributed users, and high volume business transactions. We introduces the notion of conceptualizing a data stream {{as a set of}} relational database tables so that a scientist can request information with an SQL-like query. Transformation or computation that often needs to be performed on the data en-route can be conceptualized ascomputation performed onconsecutive views of the data, with computation associated with each view. The dQUOB system moves the query code into the data stream as a quoblet; as compiled code. The relational database data model has the significant advantage of presenting opportunities for e cient <b>reoptimizations</b> of queries and sets of queries. Using examples from global atmospheric modeling, we illustrate the usefulness of the dQUOB system. We carry the examples through the experiments to establish the viability of the approach for high performance computing with a baseline benchmark. We de ne a cost-metric of end-to-end latency {{that can be used to}} determine realistic cases where optimization should be applied. Finally, we show that end-to-end latency can be controlled through a probability assigned to a query that a query will evaluate to true. ...|$|R
40|$|We {{report a}} {{combined}} quantum mechanical/molecular mechanical (QM/MM) {{study on the}} mechanism of the enzymatic Baeyer–Villiger reaction catalyzed by cyclohexanone monooxygenase (CHMO). In QM/MM geometry optimizations and reaction path calculations, density functional theory (B 3 LYP/TZVP) is {{used to describe the}} QM region consisting of the substrate (cyclohexanone), the isoalloxazine ring of C 4 a-peroxyflavin, the side chain of Arg- 329, and the nicotinamide ring and the adjacent ribose of NADP+, while the remainder of the enzyme is represented by the CHARMM force field. QM/MM molecular dynamics simulations and free energy calculations at the semiempirical OM 3 /CHARMM level employ the same QM/MM partitioning. According to the QM/MM calculations, the enzyme–reactant complex contains an anionic deprotonated C 4 a-peroxyflavin that is stabilized by strong hydrogen bonds with the Arg- 329 residue and the NADP+ cofactor. The CHMO-catalyzed reaction proceeds via a Criegee intermediate having pronounced anionic character. The initial addition reaction has to overcome an energy barrier of about 9 kcal/mol. The formed Criegee intermediate occupies a shallow minimum on the QM/MM potential energy surface and can undergo fragmentation to the lactone product by surmounting a second energy barrier of about 7 kcal/mol. The transition state for the latter migration step is the highest point on the QM/MM energy profile. Gas-phase <b>reoptimizations</b> of the QM region lead to higher barriers and confirm the crucial role of the Arg- 329 residue and the NADP+ cofactor for the catalytic efficiency of CHMO. QM/MM calculations for the CHMO-catalyzed oxidation of 4 -methylcyclohexanone reproduce and rationalize the experimentally observed (S) -enantioselectivity for this substrate, which is governed by the conformational preferences of the corresponding Criegee intermediate and the subsequent transition state for the migration step...|$|R
40|$|A {{connecting}} passenger occupies {{a seat on}} each of the flight leg of his itinerary. Moreover, for a given fare class, the fare of a {{connecting passenger}} is lower than the sum of the fares of the local passengers on the traversed legs. If the demand is high, giving availability to a connecting passenger may displace local passengers and the airline would lose revenue. The objective of this thesis is to evaluate methods that airlines can use to better estimate the network revenue value of connecting passengers for the purpose of determining seat availability. In this thesis we analyze and compare two different ways of estimating the network revenue value of the connecting passengers. The first approach consists of estimating the displacement cost of the connecting passenger on all the traversed legs by the shadow prices associated with the capacity constraints of a network linear program (LP). The second one is a prorated fare convergence technique developed in this thesis. The fares of the connecting passengers are prorated {{on each of the}} traversed legs using an estimation of the expected marginal revenue of the last seat on the legs. The existence and uniqueness of the limit for each prorated fare sequence are also proven. We have compared the performance of different seat inventory control models that incorporate these two network revenue estimation techniques. The optimization/booking simulation uses demand forecasts from an airline's Yield Management historical database. The seat inventory control methods that use the network revenue value concepts perform up to 1. 50 % better than the existing fare class control approach at a high demand scenario (82 % average load factor). Moreover, the prorated fare convergence technique performs better than the LP shadow price displacement cost approach especially if the demand is controlled by a bid price mechanism. Indeed, for a high demand scenario and a relatively high number of <b>reoptimizations</b> along the booking process, the prorated fare convergence method performs 0. 12 % better than the shadow price approach for a bid price control mechanism. Finally, the revenue difference between the two methods is both significant and robust with respect to demand variations. Includes bibliographical references (p. 114...|$|R
40|$|This thesis {{examines}} current <b>reoptimization</b> {{techniques for}} interior-point methods {{available in the}} literature and studies their efficacy in a branch-and-bound framework for 0 / 1 mixed integer programming problems. This work is motivated by the observation that there are instances of integer programming problems where each individual linear program generated in a branch-and-bound tree can be solved much faster by an interior-point algorithm than by a simplex algorithm, {{in spite of the}} fact that effective "warm-start" techniques are available for the latter but not for the former. Because of many unresolved issues surrounding effective <b>reoptimization</b> techniques for interior-point methods, interior-point algorithms have not been commonly used as linear programming solvers in a branch-and-bound framework. In this work, we identify and examine a number of key factors that may affect and even preclude effective <b>reoptimization</b> for interior-point algorithms in the branch-and-bound framework, including change in optimal partition, distance to optimality, and primal infeasibility. We conclude that even though various "warm-start" techniques are capable of reducing the <b>reoptimization</b> cost to some extent, for certain problem instances a rapid <b>reoptimization</b> can not always be expected from interior-point methods due to their inherent limitations. Continued research is needed in the direction of the present study in order to provide comprehensive guidelines for the most effective utilization of interior-point algorithms in a branch-and-bound algorithm...|$|E
40|$|AbstractIn this paper, <b>reoptimization</b> {{versions}} of the traveling salesman problem (TSP) are addressed. Assume that an optimum solution of an instance is given and {{the goal is to}} determine if one can maintain a good solution when the instance is subject to minor modifications. We study the case where nodes are inserted in, or deleted from, the graph. When inserting a node, we show that the <b>reoptimization</b> problem for MinTSP is approximable within ratio 4 / 3 if the distance matrix is metric. We show that, dealing with metric MaxTSP, a simple heuristic is asymptotically optimum when a constant number of nodes are inserted. In the general case, we propose a 4 / 5 -approximation algorithm for the <b>reoptimization</b> version of MaxTSP...|$|E
40|$|AbstractThe <b>re{{optimization}}</b> {{version of}} an optimization problem deals with the following scenario: Given an input instance together with an optimal solution for it, {{the objective is to}} find a high-quality solution for a locally modified instance. In this paper, we investigate several <b>reoptimization</b> variants of the traveling salesman problem with deadlines in metric graphs (Δ-DlTSP). The objective in the Δ-DlTSP is to find a minimum-cost Hamiltonian cycle in a complete undirected graph with a metric edge cost function which visits some of its vertices before some prespecified deadlines. As types of local modifications, we consider insertions and deletions of a vertex as well as of a deadline. We prove the hardness of all of these <b>reoptimization</b> variants and give lower and upper bounds on the achievable approximation ratio which are tight in most cases...|$|E
40|$|The task of an {{elevator}} control is to schedule the elevators {{of a group}} such that small average and maximal waiting and travel times for the passengers are obtained. We present a novel exact <b>reoptimization</b> algorithm for this problem. A <b>reoptimization</b> algorithm computes a new optimal schedule for the elevator group each time a new passenger arrives. Our algorithm uses column generation techniques and is, {{to the best of}} our knowledge, the first exact <b>reoptimization</b> algorithm for a group of elevators. We use our algorithm to compare the potential performance that can be achieved for conventional (ie up/down buttons) and two variants of destination call systems, where a passenger enters his destination floor when calling {{an elevator}}. This research is part of an ongoing project with our industry partner Kollmorgen Steuerungstechnik...|$|E
40|$|AbstractIn this paper, we {{deal with}} several <b>reoptimization</b> {{variants}} of the Steiner tree problem in graphs obeying a sharpened β-triangle inequality. A <b>reoptimization</b> algorithm exploits the knowledge of an optimal {{solution to a problem}} instance for finding good solutions for a locally modified instance. We show that, in graphs satisfying a sharpened triangle inequality (and even in graphs where edge-costs are restricted to the values 1 and 1 +γ for an arbitrary small γ> 0), Steiner tree <b>reoptimization</b> still is NP-hard for several different types of local modifications, and even APX-hard for some of them. As for the upper bounds, for some local modifications, we design linear-time (1 / 2 +β) -approximation algorithms, and even polynomial-time approximation schemes, whereas for metric graphs (β= 1), none of these <b>reoptimization</b> variants is known to permit a PTAS. As a building block for some of these algorithms, we employ a 2 β-approximation algorithm for the classical Steiner tree problem on such instances, which might be of independent interest since it improves over the previously best known ratio for any β< 1 / 2 +ln(3) / 4 ≈ 0. 775...|$|E
40|$|A quick {{solution}} {{technique for}} the integral time-dependent quickest flow problem with no waiting is presented. The proposed technique {{is based on}} the successive shortest path approach and modifies an existing algorithm to improve its average performance. At each iteration, a <b>reoptimization</b> procedure is employed to determine the augmenting path given updates to the residual graph. The residual graph, by construction, almost always contains zero-sum cycles when employed in this context. These zero-sum cycles pose a unique problem for the <b>reoptimization</b> technique. A heuristic that can be embedded in the <b>reoptimization</b> algorithm to provide path solutions in the presence of zero-sum cycles has been proposed. In the computational experiments, the heuristic provided an optimal solution nearly 100 % of the times. Further, a modified implementation of an existing path-finding algorithm has been used to solve the time-dependent quickest flow problem with source waiting...|$|E
40|$|Two {{iterative}} analysis algorithms {{were developed}} for the <b>reoptimization</b> of the LP synthesis filter based on a pulse-by-pulse <b>reoptimization</b> manner. In this study, {{the use of the}} pitch filter in the analysis algorithms is introduced. Similar to the no pitch case, improvement in the gain is achieved. On the other hand, this gain has dropped compared to the no pitch case. Moreover, the number of pulses needed to reoptimize the LP filter found to be much less than that, in the no pitch case...|$|E
40|$|A <b>reoptimization</b> problem {{describes}} the following scenario: given aninstance of an optimization problem {{together with an}} optimal solution forit, {{we want to find}} a good solution for a locally modified instance. In this paper, we deal with <b>reoptimization</b> variants of the shortest commonsuperstring problem (SCS) where the local modifications consist of adding orremoving a single string.   We show the NP-hardness of thesereoptimization problems and design several approximation algorithms forthem. First, we use a technique of iteratively using any SCS algorithm todesign an approximation algorithm for the <b>reoptimization</b> variant of addinga string whose approximation ratio is arbitrarily close to 8 / 5 andanother algorithm for deleting a string with a ratio tending to 13 / 7. Bothalgorithms significantly improve over the best currently known SCSapproximation ratio of 2. 5. Additionally, this iteration technique can be usedto design an improved SCS approximation algorithm (without <b>reoptimization)</b> if the input instance contains a long string, which might be of independentinterest. However, these iterative algorithms are relatively slow. Thus,we present another, faster approximation algorithm for inserting a stringwhich is based on cutting the given optimal solution and achieves anapproximation ratio of 11 / 6. Moreover, we give some lower bounds onthe approximation ratio which can be achieved by algorithms that use suchcutting strategies. QC 2011100...|$|E
40|$|Abstract. We {{consider}} the following <b>reoptimization</b> scenario: Given an instance of an optimization problem together with an optimal solution, {{we want to find}} a high-quality solution for a locally modified instance. The naturally arising question is whether the knowledge of an optimal solution to the unaltered instance can help in solving the locally modified instance. In this paper, we survey some partial answers to this questions: Using some variants of the traveling salesman problem and the Steiner tree problem as examples, we show that {{the answer to this question}} depends on the considered problem and the type of local modification and can be totally different: For instance, for some <b>reoptimization</b> variant of the metric TSP, we get a 1. 4 -approximation improving on the best known approximation ratio of 1. 5 for the classical metric TSP. For the Steiner tree problem on graphs with bounded cost function, which is APX-hard in its classical formulation, we even obtain a PTAS for the <b>reoptimization</b> variant. On the other hand, for a variant of TSP, where some vertices have to be visited before a prescribed deadline, we are able to show that the <b>reoptimization</b> problem is exactly as hard to approximate as the original problem...|$|E
40|$|International audienceThis paper {{focuses on}} {{accelerating}} strategies in a Column Generation (CG) algorithm. In order {{to decrease the}} total number of generated columns and then, master problems resolution time, pricing problems solutions are made of task-disjoint columns. This can be achieved by diversification methods. Another way to improve CG computing time is to use <b>reoptimization</b> approaches to solve efficiently the pricing problems. We show in this work that diversification approaches are more efficient when applied on the first iterations to build efficiently a good approximation of pricing problems convex hull and that <b>reoptimization</b> methods are more efficient when applied on the last iterations when the dual variables are close. We combine a diversification technique and a <b>reoptimization</b> procedure in a CG scheme to improve the global resolution time. This study is validated on the Vehicle Routing Problem with Time Windows (VRPTW), defined on acyclic networks...|$|E
40|$|AbstractUnder high load, the {{automated}} dispatching of service vehicles for the German Automobile Association (ADAC) must reoptimize a dispatch for 100 – 150 vehicles and 400 requests in about 10 s to near optimality. In {{the presence of}} service contractors, this {{can be achieved by}} the column generation algorithm ZIBDIP. In metropolitan areas, however, service contractors cannot be dispatched automatically because they may decline. The problem: a model without contractors yields larger optimality gaps within 10 s. One way out are simplified <b>reoptimization</b> models. These compute a short-term dispatch containing only some of the requests: unknown future requests will influence future service anyway. The simpler the models the better the gaps, but also the larger the model error. What is more significant: <b>reoptimization</b> gap or <b>reoptimization</b> model error? We answer this question in simulations on real-world ADAC data: only the new models ShadowPrice and ZIBDIPdummy can keep up with ZIBDIP...|$|E
40|$|This study investigates whether <b>reoptimization</b> {{can help}} in solving the closest {{substring}} problem. We {{are dealing with the}} following <b>reoptimization</b> scenario. Suppose, we have an optimal l-length closest substring of a given set of sequences S. How can this information be beneficial in obtaining an (l+k) -length closest substring for S? In this study, we show that the problem is still computationally hard even with k= 1. We present greedy approximation algorithms that make use of the given information and prove that it has an additive error that grows as the parameter k increases. Furthermore, we present hard instances for each algorithm to show that the computed approximation ratio is tight. We also show that we can slightly improve the running-time of the existing polynomial-time approximation scheme (PTAS) for the original problem through <b>reoptimization.</b> Comment: In Proceedings of the 17 th Philippine Computing Society Congres...|$|E
40|$|In <b>reoptimization</b> problems, one {{is given}} an optimal {{solution}} to a problem instance and a local modification of the instance. The goal is to obtain a solution for the modified instance. The additional information about the instance provided by the given solution plays a central role: we aim to use that information in order to obtain better solutions than we are able to compute from scratch. In this paper, we consider Steiner tree <b>reoptimization</b> and address the optimality requirement of the provided solution. Instead of assuming that we are provided an optimal solution, we relax the assumption to the more realistic scenario where we are given an approximate solution with an upper bound on its performance guarantee. We show that for Steiner tree <b>reoptimization</b> there is a clear separation between local modifications where optimality is crucial for obtaining improved approximations and those instances where approximate solutions are acceptable starting points. For some of the local modifications that have been considered in previous research, we show that for every fixed epsilon > 0, approximating the <b>reoptimization</b> problem with respect to a given (1 +epsilon) -approximation is as hard as approximating the Steiner tree problem itself (whereas with a given optimal solution to the original problem it is known that one can obtain considerably improved results). Furthermore, we provide a new algorithmic technique that, with some further insights, allows us to obtain improved performance guarantees for Steiner tree <b>reoptimization</b> with respect to all remaining local modifications that have been considered in the literature: a required node of degree more than one becomes a Steiner node; a Steiner node becomes a required node; the cost of one edge is increased...|$|E
3000|$|The {{novelty of}} the {{proposed}} monitoring phase is that it assesses the “quality" [...] of the collected feedback. As a result, the decisions that are taken during the analysis and the <b>reoptimization</b> phases are shaped based on a high quality feedback. Another contribution {{of the proposed}} monitoring phase is that it provides the means to effectively control the tradeoff between the <b>reoptimization</b> frequency {{and the quality of}} the runtime plan. For example, a plan can be reoptimized only when the detected changes lead to performance deterioration more than a predefined threshold.|$|E
40|$|Traditionally, fed-batch {{biochemical}} process optimization and control uses complicated off-line optimizers, with no on-line model adaptation or <b>reoptimization.</b> This study demonstrates {{the applicability of}} a class of adaptive critic designs for on-line <b>reoptimization</b> and control of an aerobic fed-batch fermentor. Specifically, the performance of an entire class of adaptive critic designs, viz., heuristic dynamic programming (HDP), dual heuristic programming (DHP) and generalized dual heuristic programming (GDHP), was demonstrated to be superior {{to that of a}} heuristic random optimizer (HRO), on optimization of a fed-batch fermentor operation producing monoclonal antibodies...|$|E
40|$|Descriptif: In a graph G = (V,E), an {{identifying}} code of G is {{a subset}} of vertices C ⊆ V such that N[v] ∩ C ̸ = ∅ for all v ∈ V, and N[u] ∩ C ̸ = N[v] ∩ C for all u ̸ = v, u,v ∈ V, where N[u] denotes the closed neighbourhood of v, that is N[u] = N(u) ∪{u}. These codes model fault-detection problems in multiprocessor systems and are also used for designing location-detection schemes in wireless sensor networks. The goal of this master thesis is to investigate dynamic aspects of Identifying Codes. The main focus will be to apply the concept of <b>reoptimization</b> in this context. Given an instance of an optimization problem together with an optimal (or near- optimal) solution, one wants to find a high-quality solution for a locally modified instance. The general concept of <b>reoptimization</b> was first introduced and investigated for the Travelling Salesman Problem and the Steiner tree problem (see [1] for a survey). The goal of this master thesis will be to investigate the concept of <b>reoptimization</b> for the problem of computing identifying codes. As the computation of minimum identifying codes is hard to approximate in the non-reoptimization scenario [2], the goal in this line of research will be to see whether similar hardness results hold also for the <b>reoptimization</b> variant of the problem...|$|E
40|$|We want {{to solve}} exactly large {{instances}} of the 0 - 1 quadratic knapsack {{problem with a}} branch-and-bound algorithm. Thus we need to obtain good lower and upper bounds in order to fix variables in the preprocessing phase at the root node of the search tree. We introduce <b>reoptimization</b> technics in this preprocessing phase using the lagrangian relaxation of Caprara, Pisinger and Toth and the lagrangian decomposition of Billionnet and Soutif. <b>Reoptimization</b> technics are used to reduce importantly (up to 60 %) the number of iterations in the linear knapsack resolutions done in both lagrangian methods...|$|E
40|$|We report {{state-of-the-art}} quantum Monte Carlo {{calculations of}} the singlet n →π^* (CO) vertical excitation {{energy in the}} acrolein molecule, extending the recent study of Bouabça et al. [J. Chem. Phys. 130, 114107 (2009) ]. We investigate the effect of using a Slater basis set instead of a Gaussian basis set, and of using state-average versus state-specific complete-active-space (CAS) wave functions, with or without <b>reoptimization</b> of the coefficients of the configuration state functions (CSFs) and of the orbitals in variational Monte Carlo (VMC). It is found that, with the Slater basis set used here, both state-average and state-specific CAS(6, 5) wave functions give an accurate excitation energy in diffusion Monte Carlo (DMC), with or without <b>reoptimization</b> of the CSF and orbital coefficients {{in the presence of}} the Jastrow factor. In contrast, the CAS(2, 2) wave functions require <b>reoptimization</b> of the CSF and orbital coefficients to give a good DMC excitation energy. Our best estimates of the vertical excitation energy are between 3. 86 and 3. 89 eV. Comment: 6 pages, 1 figure, 2 tables, to appear in Progress in Theoretical Chemistry and Physic...|$|E
