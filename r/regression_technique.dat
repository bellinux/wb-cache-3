1802|4680|Public
25|$|The work of James and Stein {{has been}} {{extended}} {{to the case of}} a general measurement covariance matrix, i.e., where measurements may be statistically dependent and may have differing variances. A similar dominating estimator can be constructed, with a suitably generalized dominance condition. This can be used to construct a linear <b>regression</b> <b>technique</b> which outperforms the standard application of the LS estimator.|$|E
25|$|In {{the money}} market {{practitioners}} might use different techniques to solve for {{different areas of}} the curve. For example, at the short end of the curve, where there are few cashflows, the first few elements of P may be found by bootstrapping from one to the next. At the long end, a <b>regression</b> <b>technique</b> with a cost function that values smoothness might be used.|$|E
50|$|Destiny of Souls {{is a book}} by Michael Newton (9 December 1931-22 September 2016), {{published}} in 2000. Newton was a hypnotherapist {{who claimed to have}} developed his own age <b>regression</b> <b>technique.</b>|$|E
30|$|We {{select and}} {{investigate}} various ML and statistical <b>regression</b> <b>techniques</b> for predicting network and user level KPIs {{accounting for the}} impact on the performance of the whole LTE stack, based on small number of measurements. Our focus is specifically on well-established machine learning and <b>regression</b> <b>techniques</b> rather than on developing our own ad hoc solutions. Furthermore, {{to the best of our}} knowledge, this study is the first one to include both ML and <b>regression</b> <b>techniques</b> in a comparative integrated study applied to LTE SONs.|$|R
40|$|Software <b>regression</b> testing <b>techniques</b> verify {{previous}} functionalities {{each time}} software modifications occur or new characteristics are added. With {{the aim of}} gaining {{a better understanding of}} this subject, in this work we present a survey of software <b>regression</b> testing <b>techniques</b> applied in the last 15 years; taking into account its application domain, kind of metrics they use, its application strategies and the phase of the software development process where they are applied. From an outcome of 460 papers, a set of 25 papers describing the use of 31 software testing <b>regression</b> <b>techniques</b> were identified. Results of this survey suggest that at the moment of apply a <b>regression</b> testing <b>technique,</b> metrics like cost and fault detection efficiency are the most relevant. Most of the techniques were assessed with instrumented programs (experimental cases) under academic settings. Conversely, we observe a minimum set of software <b>regression</b> <b>techniques</b> applied in industrial settings, mainly, under corrective and maintenance approaches. Finally, we observe a trend using some <b>regression</b> <b>techniques</b> under agile approaches...|$|R
40|$|This paper studied two {{different}} <b>regression</b> <b>techniques</b> for pelvic shape prediction, i. e., the partial {{least square regression}} (PLSR) and the principal component regression (PCR). Three different predictors such as surface landmarks, morphological parameters, or surface models of neighboring structures were used in a cross-validation study to predict the pelvic shape. Results obtained from applying these {{two different}} <b>regression</b> <b>techniques</b> were compared to the population mean model. In almost all the prediction experiments, both <b>regression</b> <b>techniques</b> unanimously generated better results than the population mean model, while the difference on prediction accuracy between these two regression methods is not statistically significant (α= 0. 01) ...|$|R
50|$|The {{parameters}} of the projectivity is found from four pairs of matching points. RANSAC <b>regression</b> <b>technique</b> is used to reject outlying matches and estimate the projectivity from the remaining good matches.|$|E
5000|$|The method goes by {{a variety}} of names. Friedman {{introduced}} his <b>regression</b> <b>technique</b> as a [...] "Gradient Boosting Machine" [...] (GBM). Mason, Baxter et. el. described the generalized abstract class of algorithms as [...] "functional gradient boosting".|$|E
50|$|Bruce and Kenneth meet Angela's estranged brother Roy Gray (Devon Bostick) {{to inquire}} about why he left the house. Using the <b>regression</b> <b>technique</b> on him, he recalls hooded figures {{entering}} his room while he was young. Bruce and Kenneth suspect Roy's grandmother, Rose Gray (Dale Dickey), has some involvement but find nothing after a search of her house.|$|E
3000|$|... [...]. The {{generalized}} <b>regression</b> <b>techniques</b> thus obtained {{are usually}} called projection pursuit regression (PPR) methods.|$|R
40|$|The {{results are}} {{presented}} of investigations to apply <b>regression</b> <b>techniques</b> {{to the development of}} methodology for creep-rupture data analysis. <b>Regression</b> analysis <b>techniques</b> are applied to the explicit description of the creep behavior of materials for space shuttle thermal protection systems. A <b>regression</b> analysis <b>technique</b> is compared with five parametric methods for analyzing three simulated and twenty real data sets, and a computer program for the evaluation of creep-rupture data is presented...|$|R
50|$|The {{approaches}} and techniques used to conduct predictive analytics can broadly be grouped into <b>regression</b> <b>techniques</b> and machine learning techniques.|$|R
5000|$|The work of James and Stein {{has been}} {{extended}} {{to the case of}} a general measurement covariance matrix, i.e., where measurements may be statistically dependent and may have differing variances. A similar dominating estimator can be constructed, with a suitably generalized dominance condition. This can be used to construct a linear <b>regression</b> <b>technique</b> which outperforms the standard application of the LS estimator.|$|E
50|$|In {{the money}} market {{practitioners}} might use different techniques to solve for {{different areas of}} the curve. For example, at the short end of the curve, where there are few cashflows, the first few elements of P may be found by bootstrapping from one to the next. At the long end, a <b>regression</b> <b>technique</b> with a cost function that values smoothness might be used.|$|E
5000|$|Control {{design as}} {{regression}} {{problem of the}} second kind: MLC may also identify arbitrary nonlinear control laws which minimize the cost function of the plant. In this case, neither a model, nor the control law structure, nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. Genetic programming is a powerful <b>regression</b> <b>technique</b> for this purpose.|$|E
30|$|Neural {{networks}} {{are capable of}} identifying complex non-linear relationships between dependent and independent variables. Conventional <b>regression</b> <b>techniques</b> typically assume a linear relationship.|$|R
50|$|A {{structural}} model, {{which remains}} meaningful under algebraic transformation. This rules out <b>regression</b> <b>techniques</b> - even simple regression produces two different equations.|$|R
40|$|A {{predictive}} model of noise annoyance involving 20 test items was developed using multiple <b>regression</b> <b>techniques,</b> and an item weighting scheme was evaluated. Cf. Prelim. p. [i]"February 1972. "Title from cover. Bibliography: p. 28. Contract report. A {{predictive model}} of noise annoyance involving 20 test items was developed using multiple <b>regression</b> <b>techniques,</b> and an item weighting scheme was evaluated. Cf. Prelim. p. [i]Prepared under {{contract from the}} National Aeronautics and Space Administration. Mode of access: Internet...|$|R
5000|$|Model Output Statistics (MOS) is a {{multiple}} linear <b>regression</b> <b>technique</b> in which predicands, often near-surface quantities, such as 2-meter (AGL) air temperature, horizontal visibility, and wind direction, speed and gusts, are related statistically {{to one or}} more predictors. The predictors are typically forecasts from a numerical weather prediction (NWP) model, climatic data, and, if applicable, recent surface observations. Thus, output from NWP models can be transformed by the MOS technique into sensible weather parameters that are familiar to the [...] "person on the street".|$|E
50|$|In particular, {{least squares}} {{estimates}} for regression models are highly sensitive to (not robust against) outliers. While {{there is no}} precise definition of an outlier, outliers are observations which do not follow {{the pattern of the}} other observations. This is not normally a problem if the outlier is simply an extreme observation drawn from the tail of a normal distribution, but if the outlier results from non-normal measurement error or some other violation of standard ordinary least squares assumptions, then it compromises the validity of the regression results if a non-robust <b>regression</b> <b>technique</b> is used.|$|E
50|$|Such {{qualitative}} data {{can also be}} used for dependent variables. For example, a researcher might want to predict whether someone goes to college or not, using family income, a gender dummy variable, and so forth as explanatory variables. Here the variable to be explained is a dummy variable that equals 0 if the observed subject does not go to college and equals 1 if the subject does go to college. In such a situation, ordinary least squares (the basic <b>regression</b> <b>technique)</b> is widely seen as inadequate; instead probit regression or logistic regression is used. Further, sometimes there are three or more categories for the dependent variable — for example, no college, community college, and four-year college. In this case, the multinomial probit or multinomial logit technique is used.|$|E
40|$|Finding an {{accurate}} sparse approximation of a spectral vector {{described by a}} linear model, when there is available a library of possible constituent signals (called endmembers or atoms), is a hard combinatorial problem which, as in other areas, has been increasingly addressed. This paper studies {{the efficiency of the}} sparse <b>regression</b> <b>techniques</b> in the spectral unmixing problem by conducting a comparison between four different approaches: Moore-Penrose Pseudoinverse, Orthogonal Matching Pursuit (OMP) [1], Iterative Spectral Mixture Analysis (ISMA) [2] and l 2 − l 1 sparse <b>regression</b> <b>techniques,</b> which are of widespread use in compressed sensing. We conclude that the l 2 − l 1 sparse <b>regression</b> <b>techniques,</b> implemented here by Iterative Shrinkage/Thresholding (TwIST) algorithm [3], yield the state-ofthe-art in the hyperspectral unmixing area. Index terms – sparse regression, hyperspectral unmixing, l 2 − l 1 norm minimization, convex optimizatio...|$|R
40|$|Predictions of two popular closed-form {{models for}} {{unsaturated}} hydraulic conductivity (K) are compared with in situ measurements {{made in a}} sandy loam field soil. Whereas the Van Genuchten model estimates were very close to field measured values, the Brooks-Corey model predictions were higher by about one order of magnitude in the wetter range. Estimation of parameters of the Van Genuchten soil moisture characteristic (SMC) equation, however, {{involves the use of}} non-linear <b>regression</b> <b>techniques.</b> The Brooks-Corey SMC equation has the advantage of being amenable to application of linear <b>regression</b> <b>techniques</b> for estimation of its parameters from retention data. A conversion technique, whereby known Brooks-Corey model parameters may be converted into Van Genuchten model parameters, is formulated. The proposed conversion algorithm may be used to obtain the parameters of the preferred Van Genuchten model from in situ retention data, without the use of non-linear <b>regression</b> <b>techniques...</b>|$|R
40|$|Karnik-Mendel (KM) {{algorithm}} {{is the most}} used and researched type reduction (TR) algorithm in literature. This {{algorithm is}} iterative in nature and despite consistent long term effort, no general closed form formula {{has been found to}} replace this computationally expensive algorithm. In this research work, we demonstrate that the outcome of KM algorithm can be approximated by simple linear <b>regression</b> <b>techniques.</b> Since most of the applications will have a fixed range of inputs with small scale variations, it is possible to handle those complexities in design phase and build a fuzzy logic system (FLS) with low run time computational burden. This objective can be well served by the application of <b>regression</b> <b>techniques.</b> This work presents an overview of feasibility of <b>regression</b> <b>techniques</b> for design of data-driven type reducers while keeping the uncertainty bound in FLS intact. Simulation results demonstrates the approximation error is less than 2 %. Thus our work preserve the essence of Karnik-Mendel algorithm and serves the requirement of low computational complexities...|$|R
40|$|A new <b>{{regression}}</b> <b>technique</b> {{based on}} Vapnik’s concept of support vectors is introduced. We compare support vector regression (SVR) with a committee <b>regression</b> <b>technique</b> (bagging) based on regression trees and ridge regression done in feature space. On {{the basis of}} these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimensionality of the input space. 1...|$|E
40|$|An algorithm, {{called the}} Minimal Residual QR algorithm, is {{presented}} to solve subset regression problems. It is shown that this scheme {{can be used as}} a numerically reliable implementation of the stepwise <b>regression</b> <b>technique,</b> which is widely used to identify an aerodynamic model from flight test data. This capability as well as the numerical superiority of this scheme over the stepwise <b>regression</b> <b>technique</b> is demonstrated in an experimental simulation study...|$|E
40|$|Abstract niet beschikbaarThe {{comparison}} of two analytical methods {{is a common}} task in analytical practice. This report describes the paired t-test and various forms of regression techniques which are commonly applied to test the biases of one analytical method relative to another. The use of the paired t-test and two forms of regression techniques are illustrated by six examples. A LOTUS 1 - 2 - 3 macro was developed {{to carry out the}} various calculations. The paired t-test is the method of choice if the concentrations are clustered. The simple least squares <b>regression</b> <b>technique</b> and the Deming <b>regression</b> <b>technique</b> are the method of choice if the samples are fairly uniformly distributed over the concentration range. The Deming <b>regression</b> <b>technique</b> is preferred if the standarddeviation at a particular level is high or increases with concentration...|$|E
40|$|Abstract. There is {{a number}} of face {{recognition}} paradigms which ensure good recognition rates with frontal face images. However, {{the majority of them}} require an extensive training set and degrade in their performance when an insufficient number of training images is available. This is especially true for applications where only one image per subject is at hand for training. To cope with this one-sample-size (OSS) problem, we propose to employ subspace projection based <b>regression</b> <b>techniques</b> rather than modifications of the established face recognition paradigms, such as the principal component or linear discriminant analysis, as it was done in the past. Experiments performed on the XM 2 VTS and ORL databases show the effectiveness of the proposed approach. Also presented ia a comparative assessment of several <b>regression</b> <b>techniques</b> and some popular face recognition methods. Key words: Face recognition, feature extraction, <b>regression</b> <b>techniques,</b> subspace projection, one sample size problem. Uporaba regresijskih metod za samodejno rapoznavanje obrazov Povzetek. V strokovni literaturi zasledimo kopic...|$|R
30|$|Stepwise linear {{regression}} [43] {{can also be}} used to identify request flows between application tiers. The knowledge of request flow intensities provides throughputs that can be used in <b>regression</b> <b>techniques.</b>|$|R
3000|$|... 17 Calculating prices, {{costs or}} budgets; calculating fractions, {{decimals}} or percentages; using a calculator; preparing charts, graphs or tables; using algebra or formulas; using advanced mathematics (calculus), trigonometry, statistics, <b>regression</b> <b>techniques.</b>|$|R
3000|$|... {{represents}} the sampled {{value of the}} buffer state. A suitable <b>regression</b> <b>technique</b> {{can be applied to}} predict the buffer fullness based on these sampled data.|$|E
40|$|A new <b>{{regression}}</b> <b>technique</b> {{based on}} concept of support vectors is introduced. We compare support vector regression with a committee <b>regression</b> <b>technique</b> (bagging) based on regression trees and ridge regression done in feature space. On {{the basis of}} these experiments, it is expected that SVR will have advantages in high dimensionality space because SVR optimization does not depend on the dimension&y of input space. This is a longer version of the paper appear in Advances in Neural Processing Systems 9 (proceedings of the 1996 conference...|$|E
40|$|The {{reliability}} of Physical Modeling in {{applications such as}} Adsorption and Heat transfer studies is not accurate since their mechanisms are complex and a proper understanding of the physics {{of the system is}} incomplete. In order to verify the applicability of <b>Regression</b> <b>technique</b> for Physical Modeling, a physical model is developed based on Multiple <b>regression</b> <b>technique</b> to predict the Pollutant Removal efficiency of fluoride in adsorption studies. Two sets of data points are collected viz., of twenty-one points consisting of homogeneous data with respect to adsorbent and of forty-eight points (heterogeneous data, including the above twenty-one points) and tested with the model. Results showed that, the physical model is giving encouraging results for homogeneous data (Standard Deviation (SD) : 0. 157) but is giving erratic results (SD: 0. 361) for the heterogeneous data. The heterogeneous data consists of non-linear adsorption data, which the model could not predict accurately indicating that, the <b>Regression</b> <b>technique</b> holds a limitation in understanding the physics of the system. Novel techniques such as ANN can be used to predict the output from the data set with better accuracy than that using <b>Regression</b> <b>technique.</b> Back propagation Network of ANN is used as a test trial for the above database and the results are encouraging (SD: 0. 29) with respect to heterogeneous data...|$|E
30|$|The ANN {{outperformed}} {{the multivariate}} <b>regression</b> <b>techniques</b> in predicting LAI from stand parameters. The ANN models, developed in this study, may aid in making forest management planning in study forest stands.|$|R
40|$|In this paper, imperialist {{competitive}} algorithm as a {{computational method}} is implemented in MATLAB software to estimate monthly average daily {{global solar radiation}} on horizontal surface for some different climate cities of Iran. The experimental coefficients for Angstrom model have been calculated using imperialist competitive algorithm for all different climate cities and output data compared with the coefficients obtained by statistical <b>regression</b> <b>techniques</b> Results indicated that imperialist competitive algorithm is a suitable method {{to find the best}} experimental coefficients based on Angstrom model and its predicted coefficients have more accuracy than coefficients estimated by statistical <b>regression</b> <b>techniques...</b>|$|R
30|$|The {{empirical}} analysis uses representative cross-sectional {{survey data}} for Switzerland to estimate a firm’s training decision in 2009. I apply multivariate <b>regression</b> <b>techniques</b> {{to account for}} observable differences between domestic and internationalized firms.|$|R
