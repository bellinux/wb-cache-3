35|10000|Public
25|$|However, {{the margin}} of error only {{accounts}} for <b>random</b> <b>sampling</b> <b>error,</b> so it is blind to systematic errors that may be introduced by non-response or by interactions between the survey and subjects' memory, motivation, communication and knowledge.|$|E
25|$|If {{the exact}} {{confidence}} intervals are used, then {{the margin of}} error takes into account both sampling error and non-sampling error. If an approximate confidence interval is used (for example, by assuming the distribution is normal and then modeling the confidence interval accordingly), then {{the margin of error}} may only take <b>random</b> <b>sampling</b> <b>error</b> into account. It does not represent other potential sources of error or bias such as a non-representative sample-design, poorly phrased questions, people lying or refusing to respond, the exclusion of people who could not be contacted, or miscounts and miscalculations.|$|E
2500|$|The {{margin of}} error is a {{statistic}} expressing the amount of <b>random</b> <b>sampling</b> <b>error</b> in a survey's results. [...] The larger the {{margin of error}}, the less confidence one should have that the poll's reported results {{are close to the}} [...] "true" [...] figures; that is, the figures for the whole population. [...] Margin of error is positive whenever a population is incompletely sampled and the outcome measure has positive variance (that is, it varies).|$|E
50|$|The {{number of}} {{bootstrap}} samples recommended in literature has increased as {{available computing power}} has increased. If the results may have substantial real-world consequences, then one should use as many samples as is reasonable, given available computing power and time. Increasing the number of samples cannot {{increase the amount of}} information in the original data; it can only reduce the effects of <b>random</b> <b>sampling</b> <b>errors</b> which can arise from a bootstrap procedure itself.|$|R
2500|$|A 2012 UK {{trial of}} focal HIFU on 41 {{patients}} reported no histological evidence {{of cancer in}} 77% of men treated (95% confidence interval: 61 - 89%) at 12 month targeted biopsy, and a low rate of genitourinary side effects. [...] However, this {{does not necessarily mean}} that 77% of men were definitively cured of prostate cancer, since systematic and <b>random</b> <b>sampling</b> <b>errors</b> are present in the biopsy process, and therefore recurrent or previously undetected cancer can be missed.|$|R
5000|$|In situ {{observations}} {{still play}} a critical role in calibrating and validating satellite observations. However, with the dense satellite sampling, in-situ observations play a minor role in reducing <b>random</b> and <b>sampling</b> <b>errors</b> in blended analyses using satellite observations.|$|R
50|$|However, {{the margin}} of error only {{accounts}} for <b>random</b> <b>sampling</b> <b>error,</b> so it is blind to systematic errors that may be introduced by non-response or by interactions between the survey and subjects' memory, motivation, communication and knowledge.|$|E
5000|$|Suppose the {{treatment}} is {{a new way of}} teaching writing to students, and the control is the standard way of teaching writing. Students in the two groups can be compared in terms of grammar, spelling, organization, content, and so on. As more attributes are compared, it becomes increasingly likely that {{the treatment}} and control groups will appear to differ on at least one attribute due to <b>random</b> <b>sampling</b> <b>error</b> alone.|$|E
5000|$|The {{margin of}} error is a {{statistic}} expressing the amount of <b>random</b> <b>sampling</b> <b>error</b> in a survey's results. The larger the {{margin of error}}, the less confidence one should have that the poll's reported results {{are close to the}} [...] "true" [...] figures; that is, the figures for the whole population. Margin of error is positive whenever a population is incompletely sampled and the outcome measure has positive variance (that is, it varies).|$|E
30|$|It is {{important}} to recognize that the UHS is designed to be representative at the provincial level, not at the county level. Due to <b>random</b> <b>sampling</b> <b>errors,</b> our <b>samples</b> for some specific counties may be noisy. To address this concern, we re-examine our main results in Table  5 at the provincial level. Similar to the results at the county level in Table  5, the provincial level estimations do not alter our findings. Minimum wage increases continue to have significant disemployment effects on the three groups in the East, Central and all regions, but no effect in the West.|$|R
40|$|Sampling {{uncertainties}} in {{the voluntary}} observing ship (VOS) -based global ocean–atmosphere flux fields were estimated using the NCEP–NCAR reanalysis and ECMWF 40 -yr Re-Analysis (ERA- 40) {{as well as}} seasonal forecasts without data assimilation. Air–sea fluxes were computed from 6 -hourly reanalyzed individual variables using state-of-the-art bulk formulas. Individual variables and computed fluxes were subsampled to simulate VOS-like <b>sampling</b> density. <b>Random</b> simulation {{of the number of}} VOS observations and simulation of the number of observations with contemporaneous sampling allowed for estimation of <b>random</b> and total <b>sampling</b> uncertainties respectively. Although reanalyses are dependent on VOS, constituting an important part of data assimilation input, it is assumed that the reanalysis fields adequately reproduce synoptic variability at the sea surface. <b>Sampling</b> <b>errors</b> were quantified by comparison of the regularly sampled (i. e., 6 hourly) and subsampled monthly fields of surface variables and fluxes. In poorly <b>sampled</b> regions <b>random</b> <b>sampling</b> <b>errors</b> amount to 2. 5 °– 3 °C for air temperature, 3 m s− 1 for the wind speed, 2 – 2. 5 g kg− 1 for specific humidity, and 15 %– 20 % of the total cloud cover. The highest <b>random</b> <b>sampling</b> <b>errors</b> in surface fluxes were found for the sensible and latent heat flux and range from 30 to 80 W m− 2. Total <b>sampling</b> <b>errors</b> in poorly <b>sampled</b> areas may be higher than random ones by 60 %. In poorly sampled subpolar latitudes of the Northern Hemisphere and throughout much of the Southern Ocean the total sampling uncertainty in the net heat flux can amount to 80 – 100 W m− 2. The highest values of the uncertainties associated with the interpolation/extrapolation into unsampled grid boxes are found in subpolar latitudes of both hemispheres for the turbulent fluxes, where they can be comparable with the <b>sampling</b> <b>errors.</b> Simple dependencies of the <b>sampling</b> <b>errors</b> on the number of samples and the magnitude of synoptic variability were derived. <b>Sampling</b> <b>errors</b> estimated from different reanalyses and from seasonal forecasts yield qualitatively comparable spatial patterns, in which the actual values of uncertainties are controlled by the magnitudes of synoptic variability. Finally, estimates of sampling uncertainties are compared with the other errors in air–sea fluxes and the reliability of the estimates obtained is discussed...|$|R
30|$|The Spanish {{samples were}} {{obtained}} through the nation-wide rolling local population register (Padrón), {{which provides a}} comprehensive register of the population, also including unauthorized immigrants (Duque, Ballano, & Perez, 2013, p. 70 – 71). However, the spatial distribution of the originally extracted simple <b>random</b> <b>samples,</b> which was deemed too dispersed territorially for the usual fieldwork standards of the polling institute, entailed that obtaining interviews would be too slow and fieldwork organisation would be much complicated. In order to reduce fieldwork costs, the sample was clustered by neighbourhood code and a new selection was done proportional {{to the size of}} the target population in the cluster, which transformed the sample into a clustered random one still allowing the calculation of selection probabilities. Consequently, designing a traditional <b>random</b> <b>sample</b> of individual TCFBs in the Spanish cities was not problematic, and the results are deemed representative of the target population aside from <b>random</b> <b>sampling</b> <b>errors</b> or any biases that might be introduced by non-response.|$|R
50|$|If {{the exact}} {{confidence}} intervals are used, then {{the margin of}} error takes into account both sampling error and non-sampling error. If an approximate confidence interval is used (for example, by assuming the distribution is normal and then modeling the confidence interval accordingly), then {{the margin of error}} may only take <b>random</b> <b>sampling</b> <b>error</b> into account. It does not represent other potential sources of error or bias such as a non-representative sample-design, poorly phrased questions, people lying or refusing to respond, the exclusion of people who could not be contacted, or miscounts and miscalculations.|$|E
50|$|Wright's {{shifting}} balance {{theory of}} evolution combines genetic drift (<b>random</b> <b>sampling</b> <b>error</b> in the transmission of genes) and natural selection to explain how multiple peaks on a fitness landscape could be occupied or how a population can achieve a higher peak on this landscape. This theory, {{based on the assumption}} of density-dependent selection as the principal forms of selection, results in a fitness landscape that is relatively rigid. A rigid landscape is one that does not change in response to even large changes in the position and composition of strategies along the landscape.|$|E
50|$|In all, over 150,000 {{people were}} interviewed by Gallup in 2014 and samples are probability-based. Surveys {{are carried out}} by {{telephone}} or face-to-face depending on the country’s telephone coverage.There is of course {{a margin of error}} (the amount of <b>random</b> <b>sampling</b> <b>error)</b> in the results for each country, which is calculated by Gallup around a proportion at the 95% confidence level (the level of confidence that the results are a true reflection of the whole population). The maximum margin of error is calculated assuming a reported percentage of 50% and takes into account the design effect.|$|E
40|$|Includes bibliographical references. The {{scope of}} this thesis project is the design and {{implementation}} of a digital quadrature demodulator for a stepped frequency ground penetrating radar. This dissertation presents a theoretical model of the demodulator, simulations characterising the demodulator performance {{as well as the}} design, construction, and measurement of the prototype demodulator. The demodulator estimates the amplitude and phase of the intermediate frequency signal of a time-interleaved dual-channel heterodyne radar receiver. A demodulator model is developed from a survey of the relevant literature, paying particular attention to <b>errors</b> introduced in <b>sampling.</b> Simulations predict the demodulator performance in the radar system, suggesting coherent integration improves accuracy by reducing the effect of <b>random</b> <b>sampling</b> <b>errors.</b> The design of the prototype and characterisation of its performance are briefly reported...|$|R
30|$|All other {{surveys in}} Table 2 {{were carried out}} once the first {{cleaning}} of the Padrón resulting from the ‘expiry procedure’ for non-EU migrants was fully implemented 10, {{and all of them}} achieved their final numerical targets by applying an exclusively individual <b>random</b> <b>sampling</b> based on the Padrón. Still, non-contact rates remained elevated and different strategies were applied {{in order to maintain the}} survey costs within reasonable limits due also to territorial dispersion. For instance, in the case of ICS 2012 – aiming to sample at least 400 migrants in Madrid and Barcelona, respectively – the sample was clustered by neighbourhood code and selection was proportional to the size of the target population in the cluster, in order to reduce the time and money implied by an individual sample very dispersed throughout the territory. The results are deemed representative of the target population aside from <b>random</b> <b>sampling</b> <b>errors</b> or any biases that might be introduced by non-response (Reichel & Morales, 2017; Morales et al., 2012).|$|R
40|$|The {{authors have}} {{performed}} a monitoring medico-sociological research aimed {{to study the}} system for organizing the treatment of patients with arterial hypertension (AH) at the territorial level. The survey was conducted among physicians in 2010. The results of this examination are presented {{as a set of}} filled-in expert assessment charts. The randomized sample (338 people) was effected by <b>random</b> selection. <b>Sampling</b> <b>error</b> does not exceed 5...|$|R
50|$|Ulrich Schimmack {{conducted}} a meta-analysis of published studies {{and found that}} most studies could produce significant results only {{with the help of}} <b>random</b> <b>sampling</b> <b>error.</b> Based on the low power of studies, one would expect a large number of non-significant results, but these results are missing from published articles. This finding confirms Carter and McCullough's meta-analysis that showed publication bias with a different statistical method. Schimmack's replicability report also identified a small set of studies with adequate power that provided evidence for ego-depletion. These studies are the most promising studies for a replication project to examine whether ego-depletion effects can be replicated consistently across several independent laboratories.|$|E
40|$|A new {{observational}} {{approach is}} presented to approximate the uncertainty (scatter or error variance) in 1 -h averaged turbulence fluxes from eddy-covariance measurements. The uncertainty includes potential contributions from instrument problems, heterogeneity and non-stationarity in addition to classical <b>random</b> <b>sampling</b> <b>error.</b> The daytime relative flux uncertainty (RFE) is half as large (20 %) at a simple maize site compared to two more complex forest sites (40 %) for all scalars possibly due to the more homogeneous vegetation, flat terrain and especially the lower measurement height. The RFE is approximately the same for day and night periods for all scalars at the two mostly homogeneous sites (pine forest and maize field) except for latent heat over the forest, where the RFE doubles at night. Modest surface heterogeneity at the other forest site for nocturnal flux footprints approximately doubles the RFE compared to daytime conditions for all scalar fluxes. Compositing by atmospheric stability (instead of time of day) reveals {{a sharp increase in}} the RFE for the most strongly stable conditions. A theoretical prediction for the pure <b>random</b> <b>sampling</b> <b>error</b> based on the flux integral timescale is smaller by a factor of two compared to the observed variability. 1...|$|E
40|$|The {{quality of}} {{stationary}} source emission factors is typically described using data quality ratings, as in AP- 42, "Compilation of Air Pollutant Emission Factors. " Such ratings are qualitative and provide {{no indication of}} the precision of the emission factor for an average emission source, nor of the variability in emissions from one source to another within a category. Advances in methodology and computing power enable {{the application of a}} quantitative approach to characterizing both variability and uncertainty in emission factors. Variability refers to actual differences in emissions from one source to another due to differences in feedstock composition, design, maintenance, and operation. Uncertainty refers to lack of knowledge regarding the true emissions because of measurement errors (both random and systematic), limited sample sizes (statistical <b>random</b> <b>sampling</b> <b>error),</b> and non-representativeness (which can introduce additional errors, including systematic errors). The set of numerical methods generically known as bootstrap simulation are a powerful tool for characterization of both variability and <b>random</b> <b>sampling</b> <b>error.</b> In this paper, we demonstrate the use of bootstrap simulation and related techniques for the quantification of variability and uncertainty for a selected example of NO...|$|E
40|$|The {{total number}} of myelinated fibres in four unifascicular tibial nerves from {{diabetic}} rats has been counted and measured {{in order to assess}} the merits of various schemes for estimating group mean fibre diameter. The average nerve trunk contained some 2960 fibres which were measured in just under five hours. With different sampling designs, the average measurement time per nerve was reduced to between 17 and 69 minutes, with little consequent loss of reliability or precision of estimated mean fibre size. The most efficient schemes were those taking systematic samples of squares or sectors. A modification of a method relying on complete strips across two diameters of each nerve was the least efficient sampling approach. It had the additional disadvantage of introducing systematic errors which could affect the accuracy of measurements made on nerve trunks with heterogeneous spatial distributions of fibre size and number. This paper completes an investigation into <b>random</b> <b>sampling</b> <b>errors</b> influencing morphometric estimates of fibre size based on uni- or multifascicular nerve trunks...|$|R
40|$|DOI: 10. 12957 /cadest. 2015. 19114 Abstract In this paper, {{we explore}} the {{feasibility}} of using RSS (Ranked Set Sampling) in improving the estimates of the population mean in comparison  to SRS (Simple <b>Random</b> <b>Sampling)</b> in Horticultural research. We use an experience developed with a survey of apples in India. The numerical results suggest that RSS procedure results in a substantial reduction of standard errors, and  thus provides more efficient estimates than SRS, in the  specific Horticultural Survey studied, using the same sample size. Then it is recommended as an easy-to-use accurate method to management of this Horticulture problem. Key-words: Ranked Set <b>Sampling,</b> Simple <b>Random</b> <b>Sampling,</b> Standard <b>Error,</b> Accuracy.  </p...|$|R
5000|$|<b>Random</b> <b>sampling,</b> and its derived {{terms such}} as <b>sampling</b> <b>error,</b> imply {{specific}} procedures for gathering and analyzing data that are rigorously applied as a method for arriving at results considered representative of a given population as a whole. Despite a common misunderstanding, [...] "random" [...] {{does not mean the}} same thing as [...] "chance" [...] as this idea is often used in describing situations of uncertainty, nor is it the same as projections based on an assessed probability or frequency. Sampling always refers to a procedure of gathering data from a small aggregation of individuals that is purportedly representative of a larger grouping which must in principle be capable of being measured as a totality. <b>Random</b> <b>sampling</b> is used precisely to ensure a truly representative sample from which to draw conclusions, in which the same results would be arrived at if one had included the entirety of the population instead. <b>Random</b> <b>sampling</b> (and <b>sampling</b> <b>error)</b> can only be used to gather information about a single defined point in time. If additional data is gathered (other things remaining constant) then comparison across time periods may be possible. However, this comparison is distinct from any sampling itself. As a method for gathering data within the field of statistics, <b>random</b> <b>sampling</b> is recognized as clearly distinct from the causal process that one is trying to measure. The conducting of research itself may lead to certain outcomes affecting the researched group, but this effect is not what is called <b>sampling</b> <b>error.</b> <b>Sampling</b> <b>error</b> always refers to the recognized limitations of any supposedly representative sample population in reflecting the larger totality, and the error refers only to the discrepancy that may result from judging the whole {{on the basis of a}} much smaller number. This is only an [...] "error" [...] in the sense that it would automatically be corrected if the totality were itself assessed. The term has no real meaning outside of statistics.|$|R
40|$|This article uses a nonparametric {{model of}} {{earnings}} {{to measure the}} returns to education. Under very general smoothness conditions, a nonparametric estimator reveals the true shape of the earnings profiles up to <b>random</b> <b>sampling</b> <b>error.</b> Thus, the nonparametric model should provide better predictions than its parametric counterpart. We find that the nonparametric model predicts very different estimated returns than standard Mincer formulations. Depending on the experience and education level, returns measured in log earnings estimated from nonparametric model can be nearly twice those obtained from the Mincer model. Finally, this article examines what structural features parametric models should include. ...|$|E
40|$|The {{quality of}} {{emission}} factors is typically described using data quality ratings. Such ratings are qualitative and provide {{no indication of}} the precision of the emission factor for an average emission source, nor of the variability in emissions from one source to another within a category. Advances in methodology and computing power enable {{the application of a}} quantitative approach to characterizing both variability and uncertainty in emission factors. Variability refers to actual differences in emissions from one source to another due to differences in feedstock composition, design, maintenance, and operation. Uncertainty refers to lack of knowledge regarding the true emissions because of measurement errors (both random and systematic), limited sample sizes (statistical <b>random</b> <b>sampling</b> <b>error),</b> and non-representativeness (which can introduce additional errors, including systematic errors). The set of numerical methods generically known as “bootstrap simulation ” are a powerful tool for characterization of both variability and <b>random</b> <b>sampling</b> <b>error.</b> In this paper, we demonstrate the use of bootstrap simulation and related techniques for the quantification of variability and uncertainty for selected examples, including NOx emissions from coal-fired power plants and CO, NOx, and hydrocarbon emissions for light duty gasoline vehicles. While our examples are focused upon emission factors for selected criteria pollutants or their precursors, the same methodology can be applied to other pollutants (e. g., hazardous air pollutants, greenhouse gases). The work described here was conducted at North Carolina State University in a project sponsored by the U. S. Environmenta...|$|E
40|$|Advances in {{methodology}} and computing power enable {{the application of}} a quantitative approach to characterizing both variability and uncertainty in emission factors. Variability refers to actual differences in emissions from one source to another due to differences in feedstock composition, design, maintenance, and operation. Uncertainty refers to lack of knowledge regarding the true emissions because of measurement errors (both random and systematic), limited sample sizes (statistical <b>random</b> <b>sampling</b> <b>error),</b> and non-representativeness (which can introduce additional errors, including systematic errors). The set of numerical methods generically known as bootstrap simulation are a powerful tool for characterization of both variability and <b>random</b> <b>sampling</b> <b>error.</b> In this paper, we demonstrate the use of bootstrap simulation and related techniques for the quantification of variability and uncertainty for a selected example of NOx emissions from coal-fired power plants. We have developed a prototype software tool that enables a user to display data sets for emission factors and activity factors for selected power plant technology groups. The user can select a parametric distribution to fit to the data. The user enters information regarding the number of power plant units in the inventory, and can display a variety of results regarding both variability and uncertainty in the inputs to the inventory, as well as uncertainty in various outputs of the inventory. While our example is focused upon emission factors for a selected criteria pollutant, the same methodology can be applied to other pollutants (e. g., hazardous air pollutants, greenhouse gases). The policy relevance of probabilistic inventories will be discussed...|$|E
40|$|The low-frequency {{contribution}} to the systematic and <b>random</b> <b>sampling</b> <b>errors</b> in single-tower eddy-covariance flux measurements is investigated using large-eddy simulation (LES). We use a continuous LES integration that covers a full year of realistic weather conditions over Cabauw, the Netherlands, and emulate eddy-covariance measurements. We focus on the daytime flux imbalance, when the turbulent flux is sufficiently resolved. Averaged over the year, daytime single-tower eddy-covariance flux measurements lead to a significant systematic underestimation of the turbulent flux. This underestimation depends on the averaging period and measurement height. For a 3600 -s averaging period at 16 -m height, the systematic underestimation reduces to a few percent, but for 900 -s averaged tall-tower measurements at 100 -m height, the fluxes are systematically underestimated by over 20 %. The year-long dataset facilitates {{an investigation into the}} environmental conditions that influence the eddy-covariance flux imbalance. The imbalance problem is found to vary widely from day to day, strongly dependent on the flow regime. In general, the imbalance problem reduces with increased mean wind speed, but days having the largest imbalance (over twice the average) are characterized by roll vortices that occur for average wind speeds, typically having a boundary-layer height (zi) to Obukhov length (L) ratio of 10 <?zi/L< 100. Geoscience and Remote SensingCivil Engineering and Geoscience...|$|R
40|$|Abstract. The {{research}} {{describes a}} usability of the SLICES land-use database. The land-use database is briefly depicted. Major {{properties of the}} quality assessment method, the used stratified <b>random</b> <b>sampling</b> method, the <b>error</b> matrix and derived statistical summary measures, and the possible error sources in assessment, are illustrated. The major results of different output products, main land-use classes, test areas, and data acquisition dates, are summarised. Finally, as a conclusion from the quality assessment we may state that probably many of the usability requirements are meet, although some additional research may still be needed...|$|R
40|$|The point quadrat method can be {{used for}} {{determination}} of vegetational change in meadows of great species diversity. An appropriate sampling technique is described comprehending an apparatus of high rigidity and a shelter to keep off wind and rain. Sample size related to sampling time expense affects the number of species recorded and methodical error which is empirically determined by repetitive sampling. A setup of fixed points leads to higher accuracy than <b>random</b> <b>sampling.</b> When methodical <b>error</b> is quantitatively known, significant vegetational change can be detected by sampling at successive times...|$|R
40|$|Higher-order {{velocity}} statistics {{can provide}} useful in-formation {{about the structure}} of turbulence in the con-vective boundary layer (CBL). Profiles of velocity vari-ance are commonplace, but say nothing about how the turbulence departs from e. g. a Gaussian distribution. Mo-ments higher than two can provide this information, but with a considerable increase in <b>random</b> (<b>sampling)</b> <b>error</b> (Lenschow et al., 1994). That is, for the same relative statistical significance, a considerably longer measure-ment time is required for moments greater that two. This {{is one reason why}} there are fewer measurements of mo-ments greater than two reported in the literature. Doppler lidar is one tool {{that can be used to}} address the sampling issue. The lidar can point in a particular direction for long periods of time and measure the radia...|$|E
40|$|We {{propose a}} test for misspecification in {{polychotomous}} response models such as the conditional or the multinomial logit model. The test is based on comparing (a smoothed version of) the link functions of the parametric polychotomous model under consideration with a nonparametric estimate. If the parametric model is correct the nonparametric estimates should differ from (the smooth of) the hypothesized links only by <b>random</b> <b>sampling</b> <b>error.</b> If however the parametric model is misspecified then systematic differences should arise. We derive the asymptotic distribution of the test statistic under the null hypothesis, show that it is consistent against semiparametric alternatives and investigate its local power properties. A small Monte Carlo study investigates the finite sample performance of the test. I am greatly indebted to Joel Horowitz for many helpful suggestions and disussions. Helpful comments were also made by Wolfgang Hardle, Jian Huang, Alois Kneip and Michael Neumann. The rese [...] ...|$|E
40|$|Fiedler et al. (2009), {{reviewed}} {{evidence for}} the utilization of a contingency inference strategy termed pseudocontingencies (PCs). In PCs, the more frequent levels (and, by implication, the less frequent levels) {{are assumed to be}} associated. PCs have been obtained using a wide range of task settings and dependent measures. Yet, the readiness with which decision makers rely on PCs is poorly understood. A computer simulation explored two potential sources of subjective validity of PCs. First, PCs are shown to perform above chance level when the task is to infer the sign of moderate to strong population contingencies from a sample of observations. Second, contingency inferences based on PCs and inferences based on cell frequencies are shown to partially agree across samples. Intriguingly, this criterion and convergent validity are by-products of <b>random</b> <b>sampling</b> <b>error,</b> highlighting the inductive nature of contingency inferences. sampling distribution, operant learning, predictions. ...|$|E
40|$|A gene {{diversity}} {{analysis was}} performed using microsatellite loci in order to (i) describe the extent and pattern of population structure in Atlantic salmon (Salmo salar L.) within a river system; (ii) establish the importance of quantifying the signal:noise ratio in accurately estimating population structure; and (iii) assess the potential usefulness of two evolutionary models in explaining within-river population structure from the ecological and habitat characteristics of Atlantic salmon. We found weak, yet highly significant microscale spatial patterning after accounting for variance among temporal replicates within sites. Lower genetic distances were observed among temporal samples at four sampling sites whereas no evidence for temporal stability was observed at the other three locations. The component of genetic variance attributable to either temporal instability and/or <b>random</b> <b>sampling</b> <b>errors</b> was almost three times {{more important than the}} pure spatial component. This indicates that not considering signal:noise ratio may lead to an important overestimation of genetic substructuring in situations of weak genetic differentiation. This study also illustrates the usefulness of the member–vagrant hypothesis to generate a priori predictions regarding the number of subpopulations that should compose a species, given its life-history characteristics and habitat structure. On the other hand, a metapopulation model appears better suited to explain the extent of genetic divergence among subpopulations, as well as its temporal persistence, given the reality of habitat patchiness and environment instability. We thus conclude that the combined use of both models may offer a promising avenue for studies aiming to understand the dynamics of genetic structure of species found in unstable environments...|$|R
40|$|Probabilistic {{discrete}} choice {{models of}} travel demand often are {{tested for the}} presence of specification errors by comparing the models' predictions of aggregate choice shares in population strata with observed shares. A model is rejected as misspecified if the differences between its predictions and the observations are judged too large. This judgement usually is made on intuitive grounds without use of formal statistical methods and, therefore includes no systematic method for distinguishing the effects of specification errors on differences between predictions and observations from those of <b>random</b> <b>sampling</b> <b>errors.</b> This paper represents formal statistical tests for comparing predicted and observed aggregate chioce shares in population strata and reports the results of an investigation {{of the power of the}} tests. The test statistics are asymptotically [chi] 2 disturbed when the model being tested is correctly specified. The results of the power investigation suggests that greater power is obtained (i. e. there is ability to detect misspecified models) when all of the available data are used for both parameter estimation and specification testing than when the available data are divided into separate estimation and test data sets. Specification tests based on comparisons of predicted and observed aggregate choice shares appear to have less power than do likelihood ratio and likelihood ratio index specification tests when the alternative models required by the latter tests are correctly or approximately correctly specified. However, tests based on comparisons of predicted and observed shares ca have greater power than the other tests when the alternative models are seriouslymisspecified. ...|$|R
40|$|An {{alternative}} method to Fourier analysis is discussed {{for studying the}} scale dependence of variances and covariances in atmospheric boundary layer time series. Unlike Fourier decomposition, the scale dependence based on multiresolution decomposition depends {{on the scale of}} the fluctuations and not the periodicity. An example calculation is presented in detail. Multiresolution decomposition is applied to tower datasets to study the cospectral gap scale, which is the timescale that separates turbulent and mesoscale fluxes of heat, moisture, and momentum between the atmosphere and the surface. It is desirable to partition the flux because turbulent fluxes are related to the local wind shear and temperature stratification through similarity theory, while mesoscale fluxes are not. Use of the gap timescale to calculate the eddy correlation flux removes contamination by mesoscale motions, and therefore improves similarity relationships compared to the usual approach of using a constant averaging timescale. A simple model is developed to predict the gap scale. The goal here is to develop a practical formulation based on readily available variables rather than a theory for the transporting eddy scales. The gap scale increases with height, increases with instability, and decreases sharply with increasing stability. With strong stratification and weak winds, the gap scale is on the order of a few minutes or less. Implementation of the gap approach involves calculating an eddy correlation flux using the modeled gap timescale to define the turbulent fluctuations (e. g., w 9 and T 9). The turbulent fluxes (e. g., w 9 T 9) are then averaged over 1 h to reduce <b>random</b> <b>sampling</b> <b>errors.</b> 1...|$|R
