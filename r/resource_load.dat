70|599|Public
5000|$|... #Caption: <b>Resource</b> <b>load</b> chart view of HouseBuilding sample project {{opened in}} GanttProject 2.0.10 ...|$|E
5000|$|... #Caption: <b>Resource</b> <b>load</b> chart view {{of sample}} project opened in Rillsoft Project 6 ...|$|E
50|$|Nvidia Optimus is a {{computer}} GPU switching technology created by Nvidia which, depending on the <b>resource</b> <b>load</b> generated by client software applications, will seamlessly switch between two graphics adapters within {{a computer}} system {{in order to provide}} either maximum performance or minimum power draw from the system's graphics rendering hardware.|$|E
30|$|To {{address the}} {{aforementioned}} problems, we propose a verification algorithm (Algorithm 3). Our solution motivates the VM to declare its current application metrics by granting {{resources to the}} VM whose calculated <b>resources</b> <b>load</b> (obtained from the VM’s declared metrics) falls within a close range of the VM’s actual <b>resources</b> <b>load</b> (line 4 - 5). On the other hand, the hypervisor revokes resources from the VM whose calculated <b>resources</b> <b>load</b> and actual <b>resources</b> <b>load</b> do not match (line 6 - 7). This dissimilarity, in most cases, is either because the VM has lied about its declared metrics, or because the VM has been compromised. The amount of resources revoked from the VM can {{be decided by the}} system administrator, who clearly knows the real impact of adjusting the VM’s resources on their performance. However, we suggest that the amount be proportional to the magnitude of the difference between the calculated <b>resources</b> <b>load</b> and the current <b>resources</b> <b>load.</b> In other words, the larger the difference between the calculated <b>resources</b> <b>load</b> and current <b>resources</b> <b>load</b> is, the more resources should be revoked from the VM. This encourages VMs to truthfully declare their metrics used to calculate the <b>resources</b> <b>load.</b>|$|R
3000|$|... (j ∈ VMs) in {{a certain}} host, the {{algorithm}} uses the model obtained from the previous section to calculate the VMs <b>resources</b> <b>load</b> (calculated load) {{with respect to the}} declared metrics of the VMs (line 4). This step is important to minimise unnecessary false positive alarms during flash events. The calculated <b>resources</b> <b>load</b> is compared with the actual <b>resources</b> <b>load</b> to determine if the calculated <b>resources</b> <b>load</b> is within a small range of the actual <b>resources</b> <b>load</b> (line 5). If this is not the case, the detection process starts by filtering out the effect of resources adjustment on j’s system metrics using the resources adjustment effect of j(given as input in Algorithm 2) (i.e., newU[i] = U[i]± (U[i] * eff [...]...|$|R
5000|$|... multi-threading {{support with}} {{asynchronous}} <b>resources</b> <b>loading</b> and hotloading support ...|$|R
50|$|It {{features}} {{most basic}} project management functions like a Gantt chart for project scheduling of tasks, and doing resource management using <b>resource</b> <b>load</b> charts. It can only handle days not hours. It {{does not have}} features like cash flow, message and document control. It {{has a number of}} reporting options (MS Project, HTML, PDF, spreadsheets).|$|E
5000|$|Two {{possible}} views: global view company (or multiple projects) {{summary of}} detours and {{delays in the}} overall view of the company, and project view. The global view can view orders and check the State of the same, and manage them, as well as view the general charge of the company's resources. View of project displays planning, <b>resource</b> <b>load,</b> advanced resource allocation and the edition of the selected order, because LibrePlan offers graphics in use of resources.|$|E
40|$|The {{purpose of}} this thesis was to analyze the problem of <b>resource</b> <b>load</b> {{balancing}} in virtualization clusters. Another aim was to implement a pilot version of <b>resource</b> <b>load</b> balancer for the VMware vSphere Standard-based virtualization cluster. The thesis also inspected available commercial and open source <b>resource</b> <b>load</b> balancers and examined their usability and effectiveness. While designing the custom solution, a modification of the greedy algorithm has been chosen {{to be used to}} determine which virtual machines should be migrated and to select their target hosts. Furthermore, experiments have been conducted to determine some parameters for the algorithm. Finally, it was experimentally verified that the implemented solution can be applied to effectively balance virtualization server workloads by live migrating virtual machines running on these hosts...|$|E
5000|$|Adobe Dreamweaver - Dreamweaver is a web {{development}} tool which uses CEF to control <b>resource</b> <b>loading,</b> navigation and context menus ...|$|R
30|$|Proposing a {{model to}} {{correlate}} VMs metrics with the actual <b>resources</b> <b>load</b> by the host, which enables the hypervisor to identify compromised VMs.|$|R
40|$|The main {{contribution}} of this thesis is twofold. First, we propose a modeling approach {{that offers a}} generic framework to formulate various types of <b>resource</b> <b>loading</b> and RCCP problems as ILP models. Second, we propose various algorithms that can solve problems of reasonable size, i. e., typically encountered in practice, to optimality. We position <b>resource</b> <b>loading</b> between strategical capacity planning (aggregate planning) and operational capacity planning (scheduling) as a tactical capacity planning problem. <b>Resource</b> <b>loading</b> has been rather underexposed both in the literature and in practice. As a tactical instrument it benefits {{the flexibility of the}} entire production system for various reasons. It serves as a tool in the customer order processing stage, which, in make-to-order manufacturing environments, is typically characterized by much uncertainty. On the demand side there is uncertainty as to what orders can eventually be acquired, while furthermore order characteristics are uncertain or at best partly known. On the supply side there is uncertainty in the availability of the resources. In these situations, a <b>resource</b> <b>loading</b> tool can be used to match production capacity and customer demand, while minimizing the cost of customer order tardiness and the use of nonregular capacity. <b>Resource</b> <b>loading</b> analyses can thus be used to accept/reject orders, or to quote reliable due dates. <b>Resource</b> <b>loading</b> can also serve as a tool to define realistic constraints for the underlying scheduling problem. The resource capacity levels and important milestones (such as release and due dates) are usually supposed to be fixed in scheduling. <b>Resource</b> <b>loading</b> can detect when and where difficulties will occur in scheduling at an early stage, and allocate orders or order parts more efficiently, and, if necessary, properly adjust resource capacity levels (by assigning nonregular capacity) and/or milestones. In this thesis we propose a deterministic approach for modeling and solving <b>resource</b> <b>loading</b> problems. In order to smooth out the aforementioned uncertainty in <b>resource</b> <b>loading</b> problems we formulate <b>resource</b> <b>loading</b> at a higher level of aggregation than scheduling problems (i. e., the tactical level vs. the operational level). In <b>resource</b> <b>loading</b> problems we distinguish (customer) orders that consist of jobs. Jobs are in fact work packages at a higher level of aggregation. In the underlying shop floor scheduling problem, jobs may be further disaggregated into operations or tasks. The difficulty of formulating the <b>resource</b> <b>loading</b> problem as an integer linear programming model is that modeling precedence relations is not straightforward, and the resulting formulations are often extremely hard to solve. We propose a modeling approach that offers a generic framework for modeling various <b>resource</b> <b>loading</b> problems. The proposed model can handle a large variety of practical aspects, such as generalized precedence constraints, various forms of capacity flexibility, tardiness penalties, and minimal duration constraints. The model can handle <b>resource</b> driven <b>resource</b> <b>loading</b> and time driven <b>resource</b> <b>loading</b> simultaneously, which allows making trade-off analyses between due date performance on the one hand, and nonregular capacity levels on the other hand. In this modeling approach we make a distinction between order plans and order schedules. Order plans indicate in which periods a job of an order is allowed to be processed. Order schedules indicate which (part of the) jobs of the order are actually processed in each period. We propose a mixed-integer linear programming (MILP) model of the <b>resource</b> <b>loading</b> problem with an exponential number of integer variables. A relatively small and fixed part of these variables determine the required nonregular capacity usage per resource per period. The remaining variables are binary variables that correspond to selecting an order plan for an order. The order plans are thus columns of the coefficient matrix of the model, which are feasible with respect to precedence constraints and order release and due date constraints. The MILP model selects precisely one order plan per order, and determines order schedules that are consistent with these order plans. The model determines the nonregular capacity usage from the order schedules. The precedence relations and release and due date constraints thus {{do not have to be}} applied to the order schedules by the model, since they are embedded in the order plans. However, since there are exponentially many feasible order plans, an explicit model of a problem of regular size is impossible to formulate and solve. We therefore propose various exact and heuristic solution methods, which are all based on first solving the linear programming (LP) relaxation of this formulation by column generation. The pricing problem comprises the determination of feasible order plans with negative reduced costs. The idea of a column generation scheme is that only a small set of variables are required to determine the optimal solution of the LP. It starts from a restricted LP formulation (RLP), which has at least one order plan per order. After each RLP optimization, order plans with negative reduced costs are added to the RLP. The column generation scheme terminates when no order plans with negative reduced costs exist. The optimal solution of the LP is then found. Clearly, if the optimal solution of the linear programming relaxation happens to be integral, we have found an optimal solution for the <b>resource</b> <b>loading</b> problem. Otherwise, we apply a branch-and-price algorithm to determine an optimal solution. We propose various exact and heuristic branching strategies. Furthermore, we propose various approximation techniques that are based on the column generation approach, such as approximation algorithms that proceed by judiciously rounding the linear programming solution to obtain a feasible solution for the original problem. Computational experiments with the <b>resource</b> <b>loading</b> methods show that large <b>resource</b> <b>loading</b> problems with a planning horizon of up to 15 weeks and 5 machine groups can usually be solved to optimality. For larger problems, the branch-and-bound methods usually have to be truncated. Various sensitivity analyses show that adding planning flexibility in some cases makes cases easier to solve, and in other cases makes it harder to prove optimality. The best <b>resource</b> <b>loading</b> method is a combination of two branching strategies. This (exact) method generally outperforms all approximation methods. In <b>resource</b> <b>loading</b> problems we assume linear precedence relations. We also propose extensions of the algorithms that are able to deal with generalized precedence relations. This allows us to use the same model and solution methods to solve Multi-Project Rough-Cut Capacity Planning (RCCP) problems, for which some heuristics have already been proposed in the literature. The main algorithmic implication of the generalized precedence constraints is the generalization of the pricing algorithm. The pricing problem becomes much harder to solve, especially when the project size increases. We propose three different pricing algorithms, so that pricing problems of many sizes can be solved. We propose branch-and-bound algorithms that use one of these pricing algorithms to solve the linear program. We also propose approximation techniques, such as the rounding heuristics that we proposed for <b>resource</b> <b>loading</b> problems, and an improvement heuristic, which tries to improve an existing feasible solution. Computational experiments with the branch-and-bound methods show that RCCP problems for projects of reasonable size can be solved to optimality. For larger problems, the branch-and-bound methods compete with the heuristics from the literature. For RCCP problems with very large projects solving the pricing problem often becomes too computational intensive. As a result, for large RCCP problems the branch-and-bound methods are outperformed by the heuristics from the literature. We note that, from a practical point of view, it is questionable whether it makes sense to solve such large problems to optimality, since information regarding resource availability and project characteristics are usually uncertain in the long term. Solving RCCP problems with a long planning horizon is thus more a mathematical challenge...|$|R
40|$|Abstract — To execute {{workflows}} on a compute cluster re-source, workflow engines {{can work}} with cluster resource manager software to distribute jobs into compute nodes on the cluster. We discuss how to interact with traditional Oracle Grid Engine and recent Hadoop cluster resource managers using a dataflow-based scheduling approach to balance com-pute <b>resource</b> <b>load</b> for data-parallel workflow execution. Our experiments show that: 1) The presented approach can bal-ance computational <b>resource</b> <b>load</b> well by interacting with the resource managers and provides good execution performance on both physical and virtual clusters; 2) Oracle Grid Engine outperforms Hadoop for CPU-intensive applications on small-scale clusters. Keywords- data-parallel workflow scheduling, virtual cluster, load balancing, cluster resource manager comparison I...|$|E
40|$|Real-time {{support for}} {{multimedia}} streams in currently installed workstation environments {{has been based}} on resource management systems that provide mechanisms for streams with guaranteed or statistical quality of service (QoS) by admission control and resource reservation. In contrast, media scaling is a technique that dynamically adapts the load of media streams to the current availability of resources. Scaling can keep media streams meaningful to the user which would break during overload situations. Instead of interrupting the service for a stream when an overload situation is encountered, the quality of the stream is gracefully degraded when the <b>resource</b> <b>load</b> situation reaches a critical state. Since media scaling is a technique that dynamically takes actual <b>resource</b> <b>load</b> into account it can easily adapt to changing situations and has the potential to keep the system in a range of optimal load. In this article we show how media scaling can be integrated in a general system suppor [...] ...|$|E
40|$|Energy {{efficiency}} in large-scale distributed systems has recently {{emerged as a}} hot topic. This paper addresses some theoretical and experimental aspects of energy efficiency by putting in perspective some assumptions made in this domain and some observations and analyses. Based on some experimental results and measurements, we revisit and focus on some "truths" commonly assumed concerning the energy usage of servers, the links between <b>resource</b> <b>load</b> and consumed energy, the impact of ON/OFF models, and some wrong assumptions linking energy and virtualization...|$|E
30|$|The {{proposed}} {{framework is}} able to detect the compromised VMs that try to claim receiving an unusual load of client requests, {{to be allowed to}} consume more resources. The compromised VM can hide that its compromised by mimicking normal load and/or flash crowds [44]. The hypervisor calculates the VM <b>load</b> (<b>resources</b> <b>load)</b> based on the compromised VM’s current declared application metrics and compares it with the actual <b>resources</b> <b>load.</b> If the calculated <b>resources</b> <b>load</b> is not within a short range of the actual <b>resources</b> <b>load,</b> there will be a high probability that the VM has been compromised. A possible strategy that a compromised VM may use, is trying to find α and β in order to obtain the model for calculating the <b>resources</b> <b>load</b> load=α+β∗VM_par. This can be done by using different values of α and β. For every α and β, the compromised VM sees the response from the hypervisor (the response is the resources given for the unusual declared metrics). If the compromised VM did not receive a response, the compromised VM tries other values of α and β, until the correct α and β values are obtained (receiving a response from the hypervisor). Although this can be possible, the number of trials will be very high, which makes it infeasible for the attacker, since α and β can be any real number from a large interval. In addition, the attacker will typically not have the opportunity to do a large number of attempts in trying to guess α and β. After several wrong guesses (e.g., 10 wrong attempts), the hypervisor would consider the VM as a compromised and prevent it from using resources.|$|R
3000|$|...][i]) (line 6 - 10) {{from the}} {{collected}} data. The {{objective is to}} adjust {{the data for the}} original infrastructure on which the training was performed before passing it to the SVM classifier. The Algorithm passes then the modified collected data to SVM to predict the result (line 11). If the result r = “attack”, the algorithm identifies a DoS attack (line 12 - 13). If the calculated <b>resources</b> <b>load</b> is within a short distance of the actual load, the algorithm identifies the <b>resources</b> <b>load</b> as normal (line 16 - 17).|$|R
30|$|A flash event {{occurs when}} there is an unusual surge of {{legitimate}} traffic. Our model is able to distinguish between a flash event and DoS attacks since our framework allows VMs to declare their current application metrics (e.g., number of clients) and motivates them to do that by granting them extra resources (Algorithm 3). The declared metrics can represent flash events. The declared metrics are then used to calculate the <b>resources</b> <b>load</b> according to the model in the previous section. The calculated and actual <b>resources</b> <b>load</b> are then compared to see if they approximately match. If so, the hypervisor will know and understand that the VM is under an unusual surge of legitimate requests and will grant the VM more resources to serve better during this period. Otherwise, if the calculated and actual <b>resources</b> <b>load</b> are largely different, the hypervisor revokes some resources from the VM. This strategy motivates the VM to truthfully declare its peak load and limit the illegal use of resources by the attacker, in case the VM has been compromised.|$|R
40|$|Abstract—As the {{popularity}} of cloud computing increases, more and more applications are migrated onto them. Web 2. 0 applications {{are the most common}} example of such applications. These applications require to scale, be highly available, fault tolerant and able to run uninterrupted {{for long periods of time}} (or even indefinitely). Moreover as new cloud providers appear there is a natural tendency towards choosing the best provider or a combination of them for deploying the application. Thus multi-cloud scenarios emerge from this situation. However, as multi-cloud resource provisioning is both complex and costly, the choice of which resources to lend and how to allocate them to application components needs to rely on efficient strategies. These need to take into account many factors including deployment and run-time cost, <b>resource</b> <b>load,</b> and application availability in case of failures. For this aim multi-objective scheduling algorithms seem an appropriate choice. This paper presents an algorithm which tries to achieve application high-availability and fault-tolerance while reducing the application cost and keeping the <b>resource</b> <b>load</b> maximized. The proposed algorithm is compared with a classic Round Robin strategy – used by many commercial clouds – and the obtained results prove the efficiency of our solution. Keywords-cloud scheduling; multi-objective scheduling; meta-heuristics I...|$|E
40|$|Distributed {{heterogeneous}} resources {{coordinated by}} fast networks provide a new platform for resource-intensive parallel applications. However, careful scheduling of such applications {{is required to}} achieve their performance potential. Since <b>resource</b> <b>load</b> varies dynamically, the scheduling mechanism may consider redistribution of application tasks to improve performance. In this paper, {{we focus on the}} development of dynamically parametrizable models to determine the cost (in terms of execution delay) of redistribution. When such models are combined with predictive models for application execution, a scheduler can determine whether redistribution is profitable at a given point in time...|$|E
40|$|The {{unexpected}} and continuous {{changes of the}} workload reaching any Internet-based service make really difficult to guarantee a balanced utilization of the server resources. In this paper, we propose a novel class of state-aware dis-patching algorithms that take into account not only the present <b>resource</b> <b>load</b> but also the behavioral trend of the server load, that is, whether it is increasing, decreasing or oscillating. We apply one algorithm of this class to a multi-tier Web-based system and demonstrate that {{it is able to}} im-prove load balancing of the most critical server resources. ...|$|E
40|$|Order {{acceptance}} {{decisions in}} Engineer-To-Order (ETO) environments are {{often based on}} incomplete or uncertain information about the order specifications {{and the status of}} the production system. To quote reliable due dates and manage the production system adequately, <b>resource</b> <b>loading</b> techniques that account for uncertainty are essential. They are useful as support tools for order acceptance and thus profitable ETO production. In this paper we propose two multi-objective optimization models for Robust <b>Resource</b> <b>Loading</b> (RRL). The first model is a multi-objective MILP model with implicitly modeled precedence relations wich we solve using a branch-and-price approach. In the second approach we use a <b>resource</b> <b>loading</b> formulation with explicitly modeled precedence relations. The models generate robust plans by including robustness in the objective function. We introduce two indicators to measure robustness: resource plan robustness and activity plan robustness. Resource plan robustness measures robustness from a resource managers viewpoint. Activity plan robustness measures robustness from a customers viewpoint. Computational experiments with the models show that accounting for robustness in the objective function improves the characteristics of a plan significantly with respect to dealing with uncertainty. Furthermore, the model with explicit precedence constraints outperforms the implicit approach...|$|R
5000|$|Monitoring of host <b>resources</b> (processor <b>load,</b> disk usage, etc.).|$|R
40|$|The main {{contribution}} of this thesis is twofold. First, we propose a modeling approach {{that offers a}} generic framework to formulate various types of <b>resource</b> <b>loading</b> and RCCP problems as ILP models. Second, we propose various algorithms that can solve problems of reasonable size, i. e., typically encountered in practice, to optimality...|$|R
40|$|Abstract. Grid {{dependent}} tasks rescheduling problem {{currently are}} facing triggering frequent. Aiming at this problem, this paper introduces a simple based on Fault Index Based Rescheduling (FIBR). Then it put forward the improved method {{based on the}} tax mechanism successful index rescheduling method. Through the method, the grid system can accord the successful index value for task allocation relatively stable resources. At the same time, the method can {{make the most of}} the available resources are fully utilized, greatly reducing <b>resource</b> <b>load</b> imbalance situation, effectively improves the utilization of resources in history current stability judgment accuracy. The possibility of rescheduling trigger is reduced...|$|E
40|$|Computational Grids {{consist of}} a {{multitude}} of geographically distributed resources. The co-allocation of several of those resources allows for the execution of highly computing-intensive and dataintensive jobs. In order to obtain quality schedules (in terms of job response time and resource utilization), different factors such as <b>resource</b> <b>load</b> (computational <b>resource</b> <b>load</b> and bandwidth usage) and data location {{need to be taken}} into account. In addition, because of the size of realistic Grids, relevant parameters w. r. t. schedule quality can often only be obtained through simulations. In this paper, we detail how a discrete-event simulator was extended to support the simulation of scheduling both cpu- and data-intensive jobs on a Grid using network-aware scheduling algorithms. These jobs can either pre-stage data, or access data remotely while executing. We examine the case in which resource-toresource data connections with guaranteed bandwidth can be set up, both when capacitated Virtual Private Networks (for certain job classes) are defined upfront and when connection requests for all jobs are dealt with in a pure First Come First Serve way. Our results show that in order to generate quality grid schedules, it is necessary to take into account network status information in the scheduling algorithms in order to improve the average job response time. Furthermore, we show how making upfront bandwidth reservations induces fair sharing of network resources between di#erent job classes and thus improves the weighted average response time...|$|E
30|$|As {{regards the}} load tracker, both linear and {{nonlinear}} load trackers {{have been investigated}} in the literature [5, 6]. Linear load trackers are based on moving averages. Such trackers are characterised by low computational complexity while achieving a smooth filtering and approximation of the resource measurements. Their disadvantage lies in the introduction of delay in {{the representation of the}} load trend, and oscillations when limited set of data are used for resource measurements. This limitation is effectively handled by the nonlinear load trackers; such spline-based approaches allow for a better approximation of the <b>resource</b> <b>load</b> variations. On the other hand, their computational complexity is higher.|$|E
40|$|Common {{practice}} in scheduling under limited resource availability is to first schedule activities {{with the assumption}} of unlimited resources, and then assign required resources to activities until available resources are exhausted. The process of matching a feasible resource plan with a feasible schedule is called resource allocation. Then, to avoid sharp fluctuations in the resource profile, further adjustments are applied to both schedule and resource allocation plan {{within the limits of}} feasibility constraints. This process is referred to as resource leveling in the literature. Combination of these three stages constitutes the standard approach of top-down scheduling. In contrast, when scarce and/or expensive resource is to be scheduled, first a feasible and economical resource usage plan is established and then activities are scheduled accordingly. This practice is referred to as bottom-up scheduling in the literature. Several algorithms are developed and implemented in various commercial scheduling software packages to schedule based on either of these approaches. However, in reality <b>resource</b> <b>loaded</b> scheduling problems are somewhere in between these two ends of the spectrum. Additionally, application of either of these conventional approaches results in just a feasible <b>resource</b> <b>loaded</b> schedule which is not necessarily the cost optimal solution. In order to find the cost optimal solution, activity scheduling and resource allocation problems should be considered jointly. In other words, these two individual problems should be formulated and solved as an integrated optimization problem. In this research, a novel integrated optimization model is proposed for solving the <b>resource</b> <b>loaded</b> scheduling problems with concentration on construction heavy equipment being the targeted resource type. Assumptions regarding this particular type of resource along with other practical assumptions are provided for the model through inputs and constraints. The objective function is to minimize the fraction of the execution cost of <b>resource</b> <b>loaded</b> schedule which varies based on the selected solution and thus, considered to be the model's decision making criterion. This fraction of cost which hereafter is referred to as operation cost, encompasses four components namely schedule delay cost, shipping, rental and ownership costs for equipment...|$|R
40|$|The days of {{supercomputers}} and mainframes dominating computing are over. With {{the cost}} {{benefits of the}} mass production of smaller machines, the majority of today’s computing power exists {{in the form of}} PCs or workstations, with more powerful machines performing more specialized tasks. Even with increased computer power and availability, some tasks require more <b>resources.</b> <b>Load</b> balancing and proces...|$|R
40|$|Largely {{due to the}} {{proliferation}} of the World Wide Web, and interfaces such as Netscape, users expect to have {{many different types of}} information immediately available. When they encounter a lengthy delay caused by heavy <b>loads</b> on shared <b>resources,</b> such as networks or servers, users often (manually) adapt by requesting different forms of the same information. As both mobile and agent computing becomes more popular, users will expect their applications to automatically adapt to heavy <b>resource</b> <b>loads</b> by fetching the information in a different form, e. g., text instead of graphics. This paper studies the accuracy with which <b>resource</b> <b>loading</b> information, particularly network loading information, must be known in order for applications to successfully, and with agility, adapt. We determine that under many normal conditions, fairly inaccurate estimates of currently available bandwidth suffice. However, when the system is heavily loaded, some strategies can perform much better with very accurate [...] ...|$|R
30|$|Currently, {{there are}} many reasons for the use of {{services}} from multiple Clouds. We can name here few scenarios: optimize costs or improve quality of services; react to changes in existing provider offers; follow constraints, like new locations or laws; avoid dependency on only one external provider; ensure backup-ups to deal with disasters or scheduled downtime; deal with the peaks in service and <b>resource</b> <b>load</b> by offloading on external ones, on demand basis; replicate applications/services by consuming services from different Clouds to ensure their high availability; act as intermediary; enhance own Cloud resource and service offers, based on agreements with other providers; consume different services for their particularities not provided elsewhere.|$|E
40|$|A QoS {{adaptation}} to dynamically changing system conditions {{that takes into}} consideration the user’s constraints on the stability of service provisioning is presented. The goal is to allow the system to make QoS adaptation decisions in response to fluctuations in task traffic flow, {{under the control of}} the user. We pay special attention to the case where monitoring the stability period and <b>resource</b> <b>load</b> variation of Service Level Agreements for different types of services is used to dynamically adapt future stability periods, according to a feedback control scheme. System’s adaptation behaviour can be configured according to a desired confidence level on future resource usage. The viability of the proposed approach is validated by preliminary experiments...|$|E
40|$|AbstractA preference-based {{approach}} is proposed for Grid computing {{with regard to}} preferences given by various groups of virtual organization (VO) stakeholders (such as users, resource owners and administrators) to improve overall quality of service and <b>resource</b> <b>load</b> efficiency. Computational resources being competed for by local jobs (initiated by owners) and global (users’) job flow complicate the problem of a required service quality level substantially. A specific cyclic job batch scheduling scheme is examined in the present work which enables to distribute and share resources considering all the VO stakeholders’ preferences and find a balance between VO global preferences and those of its users. Two different general utility functions are introduced to represent users’ preferences satisfaction...|$|E
40|$|This thesis {{presents}} Bayesian {{approach for}} a contextual network anomaly detection. Network anomaly detection {{is important in}} a computer system performance monitoring perspective. Detecting a contextual anomaly is much harder since {{we need to take}} the context into account in order to explain whether it is normal or abnormal. The main idea of this thesis is to find contextual attributes from a set of indicators, then to estimate the <b>resource</b> <b>loads</b> through the Bayesian model. The proposed algorithm offers three advantages. Firstly, the model can estimate <b>resource</b> <b>loads</b> with automatically selected indicators and its credible intervals. Secondly, both point and collective contextual anomalies can be captured by the posterior predictive distribution. Lastly, the structural interpretation of the model gives us a way to find similar nodes. This thesis employs real data from Radio Network Controller (RNC) to validate the effectiveness in detecting contextual anomalies...|$|R
40|$|Abstract — This paper {{studies the}} error {{performances}} {{of two different}} 4 G downlinks in a multi-cell environment. On the one hand, an orthogonal frequency division multiplexing (OFDM) based multiple access scheme (OFDMA), {{and on the other}} hand, a multi-carrier code division multiple access (MC-CDMA) scheme is proposed. Both transmission schemes are implemented in a cellular structure. The cellular environment model takes into account path loss and shadowing depending on the position of the mobile terminal. Further investigations are done by introducing a radio resource management (RRM) for OFDMA which can improve the performance for lower system loads. Error performances are given to compare the two multiple access proposals. The results show that MC-CDMA outperforms OFDMA in the inner part of the cell for lower <b>resource</b> <b>loads</b> by utilizing its whole diversity of used sub-carriers. At the edge of the cell, the RRM can highly enhance the OFDMA performance mainly for low <b>resource</b> <b>loads</b> and OFDMA surpasses the MC-CDMA performance. I...|$|R
40|$|Cooperation {{of agents}} in {{competitive}} environments {{is more complicated}} than in collaborative environments. Both replanning and reconfiguration {{play a crucial role in}} cooperation, and introduce a means for implementating a system flexibility. The concepts of commitments, decommitments with penalties and subcontracting may facilitate effective reconfiguration and replanning. Agents in competitive environments are fully autonomous and selfinterested. Therefore the setting of penalties and profit computation cannot be provided centrally. Both the costs and the gain differ from agent to agent with respect to contracts already agreed and <b>resources</b> <b>load.</b> This paper proposes an acquaintance model for contracting in competitive environments and introduces possibilities of reconfigurating in competitive environments as a means of decommitment optimization with respect to <b>resources</b> <b>load</b> and profit maximization. The presented algorithm for contract price setting does not use any centralized knowledge and provides results corresponding to a realistic environment. A simple customerprovider scenario proves this algorithm in competitive contracting. ...|$|R
