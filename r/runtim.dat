37|0|Public
40|$|A {{fast and}} {{scalable}} interprocedural escape analysis algorithm is presented The analysis com putes {{a description of}} a subset of created objects whose lifetim e is bounded by the lifetim e of a <b>runtim</b> e stack fram e The analysis results can be used for m ny purposes, including stack allocation of objects, thread synchronization elim ination, deadstore rem oval, codem otion, and iterator reduction Am ethod to use the analysis results for transform ing a program to allocatesom e objects on the <b>runtim</b> e stack is also presented For non-trivial program s, typically 10 %- 20 % of all allocated objects are placed on the <b>runtim</b> e stack after the transformG) ]G...|$|E
40|$|This pap r d scrib s r s arch on d v loping a customisabl {{tool for}} visualising obj ct-ori nt d s oftwar at <b>runtim.</b> This aims to {{highlight}} both th static and dynamic structur of th softwar and aid softwar ngin rs in tasks r quiring program compr h nsion of th cod. Th pap r sp cifically looks at som of t h customisation support provid d by th tool {{and how a}} simpl r pr s ntation can support a numb r of vari d tasks. KEY WORDS Softwar visualisation, <b>runtim,</b> dynamic, customisa tion. 1...|$|E
40|$|This paper {{describes}} an e#cient technique for supporting safe <b>runtim</b> downcasts in asystem with ownership types. This technique uses the type passing approach, but avoids the associated significant space overhead by storing only the <b>runtim</b> ownership informp tion that is potentially {{needed to support}} safe downcasts. Moreover, this technique does not use any inter-procedural analysis, so it preserves the separatecomate 3 j 5 nm odel of Java. We im) () 45 ted our technique {{in the context of}} Safe Concurrent Java, which is an extension to Java that uses ownership types to statically guarantee the absence of data races and deadlocks. Our approach isJVM-com:) G 5 E) : ourim) P 4) 5 tation translatesprogram to bytecodes that can be run on regular JVMs...|$|E
40|$|Policies {{represent}} an important existing approach to providing runtime {{flexibility in the}} operation of management components and systems. Smart Space will requirement management systems that are highly adaptive and which can accept modification to their behaviour at runtime from a number of appropriately authorise roles. Policies will therefore be a key mechanism in providing the <b>runtim...</b>|$|E
40|$|This paper {{describes}} a new loop based scheduling algorithm. The algorithm aims at reducing the runtime processing complexity of path based scheduling techniques. It partitions the control flow graph of the input specification into subgraphs before scheduling the different paths of each subgraph. Benchmark tests {{as well as}} simulation results on the scheduling algorithm indicate that the proposed algorithm results in sizeable reduction in <b>runtim...</b>|$|E
40|$|We empirically examine {{several ways}} of {{exploiting}} the information of multiple heuristics in a satisficing best-first search algorithm, comparing their performance {{in terms of}} coverage, plan quality, speed, and search guidance. Our results indicate that using multiple heuristics for satisficing search is indeed useful. Among the combination methods we consider, the best results are obtained by the alternation method of the “Fast Diagonally Downward ” planner. <b>runtim...</b>|$|E
40|$|We {{present a}} runtime {{environment}} that implements a Java Virtual Machine by co-execution {{between the different}} functional units of the heterogeneous Cell multiprocessing platform. Our approach uses the Cell’s scratchpad memory as a software-controlled cache and contains an automatic software-based memory management system for instruction and data caching. Profiling shows a cache hit rate of above 90 % for the instruction cache and above 74 % for the data cache. Categories and Subject Descriptors D. 3. 4 [Programming Languages]: Processors—Incremental compilers; optimization; <b>runtim...</b>|$|E
40|$|This paper proposes {{the usage}} of a {{dedicated}} Interaction Specification Language (ISL) to express interactions between software components in a component-based application. This approach brings three major benefits: First, it allows component interactions to be expressed explicitly as first-class entities. Second, it enables {{the expression of the}} interactions independently of any specific programming languages or component models. This is especially important if we consider the variety of components specifications and their heterogenity. Third, our approach permits the dynamic adaptation of the application by defining/removing interactions at <b>runtim...</b>|$|E
40|$|In this paper, we {{describe}} the software environment for Daytona, a single-chip, bus-based, shared-memory, multiprocessor DSP. The software environment is designed around a layered architecture. Tools at the lower layer are designed to deliver maximum performance and include a compiler, debugger, simulator, and profiler. Tools at the higher layer focus on improving the programmability {{of the system and}} include a run-time kernel and parallelizing tools. The run-time kernel includes a low-overhead, preemptive, dynamic scheduler with multiprocessor support that guarantees real-time performance to admitted tasks. 1. 1 Keywords Multiprocessor DSP, media processor, software environment, <b>runtim...</b>|$|E
40|$|The Universe Type System {{is used to}} {{structure}} the object store and poses some rules to references between such objects. Java programs can be annotated with these rules called Universe Type annotations to control access between components. Research in this field has been done with the goal to infer Universe Types automatically. This report describes two projects supporting this work. First, we developed a tool to insert the obtained Universe Type annotations into java source code. The second project is a visualization tool for the algorithm of the <b>Runtim...</b>|$|E
40|$|International audienceIn this paper, {{the issue}} of {{generating}} and sharing biometrics based secure cryptographic keys is addressed. In particular, we propose a protocol which integrates multi-biometrics, in which information from multiple biometric sources is combined. This protocol allows generation and sharing of multi-biometrics based crypto-biometric session keys. The protocol achieves mutual authentication between a client and a server without the need of third party certificates. The stored templates are revocable/cancelable and thus protect user privacy. The most distinctive feature of this protocol {{is that it can}} integrate multi-biometrics and depending on the required security level, the choice of the biometric modalities to use can be made at <b>runtim...</b>|$|E
40|$|We {{present a}} new model for the {{distributed}} implementation of pi-like calculi. This model is a closemos h {{to a variety of}} calculi, and so perm 02 strong correctness results that are easy to prove. In particular, we describe a distributed abstractms hine called the fusion machnq. In it, only channels exist at <b>runtim</b> 0 It uses aform of concurrent constraints called fusions [...] -equations on channelnaml# 0 # 05 h it stores as trees of forwarders between channels. We imH`B 2 # t in the fusionms hine a solos calculus with explicit fusions. There are encodings into this calculusfrom the pi calculus and the explicit fusion calculus. We quantify the e#ciency of the latter bymz 2 # of (co-) locations...|$|E
40|$|Concurrency bugs are {{becoming}} increasingly prevalent in the multi-core era. Recently, much {{research has focused on}} data races and atomicity violation bugs, which are related to low-level memory accesses. However, a large number of concurrency typestate bugs such as “invalid reads to a closed file from a different thread” are under-studied. These concurrency typestate bugs are important yet challenging to study since they are mostly relevant to high-level program semantics. This paper presents 2 ndStrike, a method to manifest hidden concurrency typestate bugs in software testing. Given a state machine describing correct program behavior on certain object typestates, 2 ndStrike profiles runtime events related to the typestates and thread synchronization. Based on the profiling results, 2 ndStrike then identifies bug candidates, each of which is a pair of <b>runtim...</b>|$|E
40|$|In future {{ubiquitous}} computing environments, {{our daily lives}} will be influenced {{by a lot of}} computer supported services all over the place. To interact with those services intuitively, heterogeneous interaction techniques such as gestures, auditory recognition and tangible user interfaces will be appear. Besides, several kinds of services will support multiple input devices, not just one set of them. In such multi-modal environments, application programmers must take into account how to adapt heterogeneous input events to multi-modal services. We propose an input-event framework that provides high-level abstraction for heterogeneous input devices, that we call MetaInputs, for distributed multi-modal applications. Our framework provides semantic and standard interfaces between input devices and services. It enables developers to deploy input devices and services independently. And also, our framework supports context-aware <b>runtim...</b>|$|E
40|$|Image {{segmentation}} is {{a process}} by which an image is partitioned into regions with similar features. Many approaches have been proposed for color image segmentation, but Fuzzy C-Means has been widely used, {{because it has a}} good performance in a large class of images. However, it is not adequate for noisy images and it also takes more time for execution as compared to other method as K-means. For this reason, several methods have been proposed to improve these weaknesses. Method like Possibilistic C-Means, Fuzzy Possibilistic C-Means, Robust Fuzzy Possibilistic C-Means and Fuzzy C-Means with Gustafson-Kessel algorithm. In this paper we perform a comparison of these clustering algorithms applied to feature extraction on vineyard images. Segmented images are evaluated using several quality parameters such as the rate of correctly classied area and <b>runtim...</b>|$|E
40|$|Software {{testing is}} {{typically}} {{an ad hoc}} process where human testers manually write test inputs and descriptions of expected test results, perhaps automating their execution in a regression suite. This process is cumbersome and costly. This paper reports results on a framework to further automate this process. The framework consists of combining automated test case generation based on systematically exploring the program’s input domain, with runtime analysis, where execution traces are monitored and verified against temporal logic specifications, and analyzed by concurrency error detection algorithms. The approach suggests a methodology for generating specifications dynamically for each input instance rather than statically once-and-for-all. This approach of generating properties specific to a single test case is novel. The paper describes an application of this methodology to a planetary rover controller. Key words: Automated testing, test-case generation, model checking, symbolic execution, <b>runtim...</b>|$|E
40|$|Local Sequence Algignment. The local {{sequence}} alignment {{problem is}} de-fined as follows: Given two strings S = s 1 [...] . sn and T = t 1 [...] . tm, a substitution matrix Score and an insertion/deletion penalty δ, find {{a pair of}} substrings si [...] . si+k of S and tj [...] . tj+l of T that have the best overall alignment score, and return the best alignment for them. Local Sequence Algignment against a database. The local sequence alignment against a database problem extends the local sequence alignment prob-lem by introducing multiple strings against which a single query string is a aligned: Given a query string S = s 1 [...] . sn, {{and a collection of}} strings D = {D 1, [...] . DM} (usually referred to as a sequence database), a substi-tution matrix Score and an insertion/deletion penalty δ, find the best local alignments of S with any/all strings from D. Note. Smith-Waterman algorithm for local sequence alignment has <b>runtim...</b>|$|E
40|$|Software {{testing is}} {{typically}} an ad-hoc process where human testers manually write test inputs {{and descriptions of}} expected test results, perhaps automating their execution in a regression suite. This process is cumbersome and costly. This paper reports results on a framework to further automate this process. The framework consists of combining automated test case generation based on systematically exploring the input domain of the program with runtime verification, where execution traces are monitored and verified against properties expressed in temporal logic. Capabilities also exist for analyzing traces for concurrency errors, such as deadlocks and data races. The input domain {{of the program is}} explored using a model checker extended with symbolic execution. Properties are formulated in an expressive temporal logic. A methodology is advocated that automatically generates properties specific to each input rather than formulating properties uniformly true for all inputs. The paper describes an application of the technology to a NASA rover controller. Key words: Automated testing, test case generation, model checking, symbolic execution, <b>runtim...</b>|$|E
40|$|Abstract—We {{propose to}} {{introduce}} redundant interconnects for manufacturing yield and reliability improvement. By introducing redundant interconnects, {{the potential for}} open faults is reduced {{at the cost of}} increased potential for short faults. Overall, manufacturing yield and fault tolerance can be improved. We focus on a postprocessing, tree-augmentation approach, which can be easily integrated in current physical design flows. Our contributions are as follows. 1) We formulate the problem as a variant of the classical two-edge-connectivity augmentation problem in which we take into account such practical issues as wirelength increase budget, routing obstacles, and the use of Steiner points. 2) We show that an optimum solution can always be found on the Hanan grid defined by the terminals and the corners of the feasible routing region. 3) We give a compact integer program formulation which is solved in practical runtime by the commercial optimization package CPLEX for nets with up to 100 terminals. 4) We give a well-scaling greedy algorithm which has a practical <b>runtim...</b>|$|E
40|$|Abstract A {{number of}} {{methods have been}} {{presented}} to calculate the worst case execution time WCET of realtime programs However to properly handle semantic dependencies which in most cases is needed to reduce overestimation all these methods require extra semantic infor mation to be given by the programmer manual annotations for paths loops and recursion depth To manually derive these annotations is of ten dicult and the process is errorprone In this paper we present a new method to automatically derive safe and tight annotations for paths and loops We illustrate our method by giving some examples and by presenting a prototype tool implementing the method for a subset of C Realtime systems are systems in which the correctness depends {{not only on the}} results of computations but also on the time at which the result is produced To be able to guarantee the deadlines of a realtime system the software execution time is needed before <b>runtim...</b>|$|E
40|$|I hereby {{declare that}} I am the sole {{author of this}} thesis. This is a true copy of the thesis, {{including}} any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. ii We report on initial research on the concurrency control issue of compiled database appli-cations. Such applications have a repository style of architecture in which a collection of software modules operate on a common database {{in terms of a}} set of predefined transac-tion types, an architectural view that is useful for the deployment of database technology to embedded control programs. We focus on decoupling concurrency control from any functionality relating to recovery. Such decoupling facilitates the compile-time query opti-mization. Because it is the possibility of transaction aborts for deadlock resolution that makes the recovery subsystem necessary, we choose the deadlock-free tree locking (TL) scheme for our purpose. With the knowledge of transaction workload, efficacious lock trees for <b>runtim...</b>|$|E
40|$|Report on the State-of-the-Art and Requirements Analysis This {{deliverable}} {{covers the}} current state-of-the-art in data, information, and process mediation, and provides {{an analysis of}} mediation requirements for the DIP Mediation Component. The document treats the mediation of data and information separately from process mediation since process mediation requires the interpretation of goals and workflow as well as flexible Web Service invocation, which are not required for data and information mediation. This document consists of two main parts. The first part {{provides an overview of}} {{the current state of the}} art in mediation, describing some of the existing approaches and projects. In this section, the industrial and research approaches are treated differently for data and information mediation. The second part of the document provides an analysis of mediation requirements. Three types of requirements need to be considered here: requirements regarding the general architecture of the DIP Mediation Component (which can be requirements for the <b>runtim...</b>|$|E
40|$|Reducing {{defects in}} {{software}} {{is a central}} goal of modern software engineering. Providing essentially defect-free library software can, in large part, be accomplished through thorough unit testing, yet even the best library software—if misused—can lead to defective applications. When invoking a function, not every combination of syntactically valid inputs will (or should) necessarily result in defined behavior. Functions for which certain combinations of inputs and (object) state result in undefined behavior {{are said to have}} narrow contracts. Aggressively validating function preconditions at runtime—commonly referred to as defensive programming— can lead to more robust applications by (automatically) detecting out-of-contract use of defensive library software early in the software development life cycle. Most classical approaches to defensive precondition checks, however, necessarily result in suboptimal runtime performance; moreover, when misuse is detected, the action taken is invariably determined by the library, not the application. In this proposal, we describe a centralized facility for supporting defensive <b>runtim...</b>|$|E
40|$|Tools {{used for}} {{conducting}} parallel performance analysis do not adequately convey {{an understanding of}} an application’s performance {{in relation to the}} status and behavior of the runtime environment in which an application executes. This limits the quality of diagnosis that tools are able to offer regarding the causes of performance problems. This work presents Environment-Aware Performance Analysis, a new approach that targets improving the quality of diagnosis offered by performance analysis tools. This thesis focuses on Environment-Aware Performance Analysis for parallel applications. Environment-Aware Performance Analysis includes analysis of an application’s execution behavior and analysis of the environment in which an application executes. 1 The performance diagnosis incorporates both sets of analyses, and it seeks to identify root causes of performance behavior. Environment-Aware Performance Analysis extends traditional methods of application performance analysis, and, by doing so, has potential {{to improve the quality of}} performance diagnosis. In this work, we investigate including analysis about the status and behavior of the <b>runtim...</b>|$|E
40|$|International audienceIn {{ubiquitous}} environments, context-aware applications need {{to monitor}} their execution context. They use middleware services such as context managers for this purpose. The space of monitorable entities is huge and each context-aware application has specific monitoring requirements which can change at runtime {{as a result of}} new opportunities or constraints due to context variations. The issues dealt with in this paper are 1) to guide context-aware application designers in the specification of the monitoring of distributed context sources, and 2) to allow the adaptation of context management capabilities by dynamically taking into account new context data collectors not foreseen during the development process. The solution we present, CA 3 M, follows the model-driven engineering approach for answering the previous questions: 1) designers specialised into context management specify context-awareness concerns into models that conform to a context-awareness meta-model, and 2) these context-awareness models are present at runtime and may be updated to cater with new application requirements. This paper presents the whole chain from the context-awareness model definition to the dynamic instantiation of context data collectors following modifications of context-awareness models at <b>runtim...</b>|$|E
40|$|Abstract—Dynamic {{scheduling}} and varying decomposition granularity are well-known techniques for achieving high perfor-mance in parallel computing. Heterogeneous clusters with highly data-parallel processors, such as GPUs, present unique {{problems for the}} application of these techniques. These systems reveal a dichotomy between grain sizes: decompositions ideal for the CPUs may yield insufficient data-parallelism for accelerators, and decompositions targeted at the GPU may decrease performance on the CPU. This problem is typically ameliorated by statically scheduling a fixed amount of work for agglomeration. However, determining the ideal amount of work to compose requires experimentation because it varies between architectures and problem configurations. This paper describes a novel methodology for dynamically agglomerating work units at runtime and scheduling them on accelerators. This approach is demonstrated in the context of two applications: an n-body particle simulation, which offloads particle interaction work; and a parallel dense LU solver, which relocates DGEMM kernels to the GPU. In both cases dynamic agglomeration yields comparable or better results over statically scheduling the work across a variety of system configurations. Keywords-dynamic scheduling; accelerator; GPGPU; CUDA; agglomeration; adaptive <b>runtim...</b>|$|E
40|$|The Share {{algorithm}} {{for predicting}} disk idle time described by Helmbold, et al was implemented and {{run on the}} cello 1 dataset. As in the original paper, the share algorithm performed slightly better than the 2 -Competitive algorithm and {{a lot better than}} the 30 -second Fixed Time-Out algorithm. As expected, the Share algorithm performed worse than the Optimal algorithm. It was determined that varying the number of experts for the share algorithm from 10 to 100 did not change these results. Surprisingly, the performance of the three different expert distributions for the Share algorithm (linear, harmonic, and exponential) was opposite that described in the original paper. This is likely due either to a bias in the dataset, or to a bug in the implementation code. 1 The Algorithms In this report, I implement the Share Algorithm described in “Adaptive Disk Spin-Down for Mobile Computers ” by Helmbold et. al. [2], which I will henceforth refer to as “the original paper”. The algorithm was implemented in C, which allowed a decent <b>runtim...</b>|$|E
40|$|International audienceUsing a component-based approach, {{applications}} can {{be defined}} as an assembly of abstract components, requiring services from and providing services to each other. At the time of execution, they are mapped to the concrete level after identifying the deployed components. However, several problems can be detected at init time that prevent the mapping to be achieved successfully, e. g., heterogeneity of connection interfaces. Moreover, applications in pervasive environment are challenged by the dynamism of their execution environment due to, e. g., users and devices mobility, which make them subject to unforeseen failures. Both of these problems imply mismatches between abstract and concrete levels detected at init time or during the execution. Therefore, abstract applications have to be adapted to carry out their mapping and their execution. In this article, we propose a new dynamic structural adaptation approach for abstract applications. Our approach is based on adaptation patterns that provide solutions to the captured mismatches between abstract and concrete levels. We also compare and contrast our approach with the existing ones concluding that our approach is not only generic, but it is also applicable both at init time and at <b>runtim...</b>|$|E
40|$|Sound {{reasoning}} {{about the}} behavior of programs relies on program execution adhering to the language semantics. However, in a distributed computation, when a value is sent from one party to another, the receiver faces {{the question of whether}} the value is well-traced: could it have been produced by a computation that respects the language semantics? If not, then accepting the non-well-traced value may invalidate the receiver’s reasoning, leading to bugs or vulnerabilities. Proof-Carrying Data (PCD) is a recently-introduced cryptographic mechanism that allows messages in a distributed computation to be accompanied by proof that the message and the history leading to it complies with a specified predicate. Using PCD, a verifier can be convinced that the predicate held throughout the distributed computation, even in the presence of malicious parties, and at a verification cost that is independent of the size of the computation producing the value. Unfortunately, previous approaches to using PCD required tailoring a specialized predicate for each application, using an inconvenient formalism and with little methodological support. We connect these two threads by introducing a novel, PCD-based approach to enforcing language semantics in distributed computations. We show how to construct an object-oriented language <b>runtim...</b>|$|E
40|$|Signatures are on file in the Graduate School. As {{processor}} speeds {{continue to}} advance {{at a rapid}} pace, accesses to the I/O subsystem are increasingly becoming the bottleneck {{in the performance of}} large-scale applications. In spite of technological advances in peripheral devices, provisioning and maintenance of large buffers in memory remains a crucial technique for achieving good performance, but is only effective if we can achieve good hit rates. This thesis describes the runtime system support to determine what should go into an I/O cache and when to avoid accessing it, as well as techniques to improve the hit ratio itself by choosing appropriate candidate cache blocks for eviction/replacement. Such techniques are equally applicable for both explicitly and implicitly I/O intensive applications that access data either through a file-system interface or through the virtual memory interface. While the afore-mentioned techniques can boost the performance for a single I/O inten-sive application, an important consideration {{that needs to be addressed}} for practical reasons is the effects of multi-programming, where multiple applications are run simultaneously for better resource utilization. The thesis will conclude with the design and implementation of a <b>runtim...</b>|$|E
40|$|Program Visualization systems use {{graphics}} and animation {{to represent the}} behavior of software programs. These systems represent {{different aspects of the}} program such as source code, control flow, data structures, runtime state of the program. Representing the actual runtime state of the program finds its use in a variety of applications including program understanding, visual debugging, and pedagogy. However, existing state-of-the-art program visualization systems are limited in: (1) not providing sufficient interactive capabilities to the user; (2) not faithfully representing the runtime state of the program; (3) not allowing users to apply different layout strategies to the visualization; (4) being tied to a specific programming language. To address these limitations, this thesis presents HDPV, a program state visualization system that visualizes any C, C++, or Java program. HDPV is based on a canonical state model that represents the memory layout of the program as a graph of memory blocks. It decouples the visualization of the program from the actual programming language in which it is written, thereby making the system language independent. HDPV supports a host of interactive features that allow the user to selectively explore different parts of the program’s <b>runtim...</b>|$|E
40|$|International audienceThe {{issue of}} data {{security}} and privacy in multi-cloud based environments requires different solutions for implementing and enforcing security policies. In these environments, many security aspects must be faced, such as security-by-design, risk management, data privacy and isolation, and vulnerability scans. Moreover, it also becomes {{necessary to have}} a system that interrelates and operates all security controls which are configured and executed independently on each component of the application (service) being secured and monitored. In addition, thanks to the large diffusion of cloud computing systems, new attacks are emerging, so threat detection systems {{play a key role in}} the security schemes, identifying possible attacks. These systems handle an enormous volume of information as they detect unknown malicious activities by monitoring different events from different points of observation, as well as adapting to new attack strategies and considering techniques to detect malicious behaviors and react accordingly. To target this issue, we propose in the context of the MUSA EU Horizon 2020 project, a security assurance platform that allows monitoring the multi-cloud application deployed in different Cloud Server Providers (CSPs). It detects potential deviations from security Server Level Agreements (A formal, negotiated document that defines in quantitative and qualitative terms the service being offered to a Cloud Service Client (CSC).) (SLAs) and triggers countermeasures to enforce security during application <b>runtim...</b>|$|E
40|$|Joint Advanced Distributed Simulation (JADS) is an Office of the Secretary of Defense-sponsored joint {{test force}} {{chartered}} {{to determine the}} utility of advanced distributed simulation (ADS) technology for test and evaluation (T&E) of military systems. JADS is doing this by looking at three slices of the T&E spectrum. One of those slices is the JADS Electronic Warfare (EW) Self-Protection Jammer (SPJ) Test. The EW test was the only JADS test that {{was in a position}} to look at the new Department of Defense (DoD) standard technical architecture for DoD simulations- high level architecture. The JADS EW SPJ Test uses high level architecture (HLA) federations to replicate all elements of an actual open air range (OAR) test environment and the selected EW system under test (an ALQ- 131 Block II SPJ). To determine the utility of ADS technology for EW T&E, JADS will use and evaluate the HLA as part of the SPJ three-phase test program. In developing and implementing an HLA federation for EW T&E, JADS recognized that measuring and controlling the latency imposed by diverse test faculties, simulators, communications equipment, and long-haul communications networks was a critical factor. Because of the importance to T&E, most of these latency measurements have been made in other EW test projects or communications architectures and are documented. A new element used by JADS for EW T&E is the HLA and <b>runtim...</b>|$|E
40|$|Abstract. We {{consider}} comparator networks M {{that are}} used repeatedly: while the output produced by M is not sorted, it is fed again into M. Sorting algorithms working in this way are called periodic. The number of parallel steps performed during a single run of M is called its period, the sorting time of M is {{the total number of}} parallel steps that are necessary to sort in the worst case. Periodic sorting networks have the advantage that they need little hardware (control logic, wiring, area) and that they are adaptive. We are interested in comparator networks of a constant period, due to their potential applications in hardware design. Previously, very little was known on such networks. The fastest solutions required time O(n �), where the depth was roughly 1 /�. We introduce a general method called periodification scheme that converts automatically an arbitrary sorting network that sorts n items in time T(n) and that has layout area A(n) into a sorting network that has period 5, sorts �(n � T(n)) items in time O(T(n) � log n), and has layout area O (A(n) � T(n)). In particular, applying this scheme to Batcher’s algorithms, we get practical period 5 comparator networks that sort in time O(log 3 n). For theoretical interest, one may use the AKS network resulting in a period 5 comparator network with <b>runtim...</b>|$|E
40|$|Reconfigurable {{hardware}} devices, such as Field Programmable Gate Arrays (FPGAs), {{can be used}} {{to speed}} up image processing applications. Creating reconfigurable applications is not as straightforward as designing either software or hardware, since the application is intrinsically a hardware/software codesign. We have been developing a codesign environment that eases the process of creating reconfigurable applications for a Network of Workstations environment [1]. We have found that the performance of such an application is highly dependent on image size. While an application implemented on an FPGA can be one to two orders of magnitude faster than the application implemented in software, processing in hardware incurs additional costs that are not required for software. Some of these costs are hardware initialization costs, extra processing steps for easy processing of the border cases, and communication of the image to and from the reconfigurable device. The runtime of image processing applications varies with image size, so processing small images on an FPGA might not be efficient due to the additional overhead. Detailed profiling of hardware (with the extra costs) and software runtimes for a range of image sizes exposes a crossover point where all smaller images should be processed in software and all larger images should be processed in hardware. The goal of this project is to determine at <b>runtim...</b>|$|E
40|$|Perfect pre-deployment test {{coverage}} is notoriously {{difficult to achieve}} for large applications. Given enough end users, however, many more test cases will be encountered during an application’s deployment than during testing. The use of runtime verification after deployment would enable developers to detect unexpected situations. Unfortunately, the prohibitive performance cost of runtime monitors prevents their use in deployed code. In this work, we study the feasibility of collaborative runtime verification, a verification approach which can distribute the burden of runtime verification among multiple users and over multiple runs. Each user executes a partially instrumented program and therefore suffers {{only a fraction of}} the instrumentation overhead. We focus on runtime verification using tracematches. Tracematches are a specification formalism that allows users to specify runtime verification properties via regular expressions with free variables over the dynamic execution trace. We propose two techniques for soundly partitioning the instrumentation required for tracematches: spatial partitioning, where different copies of a program monitor different program points for violations, and temporal partitioning, where monitoring is switched on and off over time. We evaluate the relative impact of partitioning on a user’s runtime overhead by applying each partitioning technique to a collection of benchmarks that would otherwise incur significant instrumentation overhead. Our results show that spatial partitioning almost completely eliminates runtime overhead (for any particular benchmark copy) on many of our test cases, and that temporal partitioning scales well and provides <b>runtim...</b>|$|E
