0|1208|Public
40|$|Current image <b>re-sampling</b> {{detectors}} can reliably detect <b>re-sampling</b> in JPEG images {{only up to}} a Quality Factor (QF) of 95 or higher. At lower QFs, periodic JPEG blocking artifacts {{interfere with}} periodic patterns of <b>re-sampling.</b> We add a controlled amount of noise to the image before the <b>re-sampling</b> detection step. Adding noise suppresses the JPEG artifacts while the periodic patterns due to <b>re-sampling</b> are partially retained. JPEG images of QF range 75 - 90 are considered. Gaussian/Uniform noise {{in the range of}} 28 - 24 dB is added to the image and the images thus formed are passed to the <b>re-sampling</b> detector. The detector outputs are averaged to get a final output from which <b>re-sampling</b> can be detected even at lower QFs. We consider two <b>re-sampling</b> detectors- one proposed by Poposcu and Farid [1], which works well on uncompressed and mildly compressed JPEG images and the other by Gallagher [2], which is robust on JPEG images but can detect only scaled images. For multiple <b>re-sampling</b> operations (rotation, scaling, etc) we show that the order of <b>re-sampling</b> matters. If the final operation is up-scaling, it can still be detected even at very low QFs. Keywords: Digital Image Forensics, <b>Re-sampling</b> detection, Noise addition, JPEG Denoising 1...|$|R
50|$|The inputs, {{used for}} either {{sampling}} or sound processing of an external sound source, are routed through the effects engine and heard at the outputs during real-time. However, <b>re-sampling</b> {{is necessary in}} order for the sample to contain the effects {{as a part of the}} sample. Following any <b>re-sampling,</b> the sample playback can be further <b>re-sampled</b> or processed by more effects at the outputs during playback.|$|R
40|$|This paper {{compares the}} {{performance}} of anti-noise methods, particularly probabilistic and <b>re-sampling</b> methods, using NSGA 2. It then proposes a computationally less expensive approach to counteracting noise using <b>re-sampling</b> and fitness inheritance. Six problems with different difficulties are {{used to test the}} methods. The results indicate that the probabilistic approach has better convergence to the Pareto optimal front, but it looses diversity quickly. However, methods based on <b>re-sampling</b> are more robust against noise but they are computationally very expensive to use. The proposed fitness inheritance approach is very competitive to <b>re-sampling</b> methods with much lower computational cost...|$|R
40|$|We derive an {{unbiased}} variance estimator for <b>re-sampling</b> procedures {{using the}} fact that those procedures are incomplete U-statistics. Our approach is based on careful examination of the combinatorics governing the covariances between <b>re-sampling</b> iterations. We establish such an unbiased variance estimator for the special case of K-Fold cross-validation. This estimator exists as soon as new ob-servations are added to the original sample, and we specify how many additional observations are necessary. Thus we make <b>re-sampling</b> procedures comparable. We make no assumptions on the underlying distribution and we take the co-variances between <b>re-sampling</b> iterations into account. Beyond that we show an approach to find a <b>re-sampling</b> design with minimal variance for a fixed size of learning sets. We empirically show the existence of designs with smaller vari-ance than repeated cross-validation. We systemically compare with the complete U-statistic, the leave-p-out estimator. Our examination is completed by an ap-plication to micro-array data...|$|R
3000|$|... {{by their}} {{respective}} rank. In {{a similar way}} we can compute partial Mantel correlations [32]. To compute the 95 % confidence interval of the correlation coefficient, we use a bootstrap procedure [33]. We <b>re-sample</b> simultaneously the rows and the columns of the similarity matrices, and then for each <b>re-sampling</b> we compute again the Mantel correlation coefficient. The <b>re-sampling</b> procedure results in the empirical distribution of the Mantel correlation coefficient, from which we extract the 95 % confidence interval.|$|R
40|$|Abstract — Shape {{simplification}} and <b>re-sampling</b> {{of underlying}} point-sampled surfaces under userdefined error bounds {{is an important}} and challenging issue. Based on the regular triangulation of the Gaussian sphere and the surface normals mapping onto the Gaussian sphere, a Gaussian sphere based <b>re-sampling</b> scheme is presented that generates a non-uniformly curvature-aware simplification of the given point-sampled model. Owing to the theoretical analysis of shape isophotic error metric for did that Gaussian sphere based sampling, the proposed simplification scheme provides a convenient way to control the <b>re-sampling</b> results under a user-specified error metric bound. The novel algorithm has been implemented and demonstrated on several examples. Keywords — point-sampled surfaces; isophotic error metric; Gaussian sphere; error metric controllable; <b>re-sampling</b> 1...|$|R
40|$|When testing {{multiple}} hypotheses for genomics studies, we {{are usually}} confronted {{to the problem}} of unknown data and test statistics distributions. The most popular solution of such problems seems to be based on <b>re-sampling,</b> with or without replacement, of raw data or residuals, in a modeling approach. In this paper we find conditions which guarantee that these <b>re-sampling</b> distributions provide strong control of maximum Type I error rate, when using a step- down partitioning multiple testing method. For this purpose, the true, theoretical distribution of the tests is compared with the analitically derived <b>re-sampling</b> distributions. These distributions are expressed as functionals of a generic data un-derlying, multivariate distribution. KEY WORDS: <b>Re-sampling,</b> Multiple testing, Family-wise erro...|$|R
40|$|We {{propose a}} new method to detect <b>re-sampled</b> imagery. The method {{is based on}} {{examining}} the normalized energy density present within windows of varying size in the second derivative of the frequency domain, and exploiting this characteristic to derive a 19 -dimensional feature vector {{that is used to}} train a SVM classifier. Experimental results are reported on 7, 500 raw images from the BOSS database. Comparison with prior work reveals that the proposed algorithm performs similarly for <b>re-sampling</b> rates greater than 1, and is superior to prior work for <b>re-sampling</b> rates less than 1. Experiments are performed for both bilinear and bicubic interpolation, and qualitatively similar results are observed for each. Results are also provided for the detection of <b>re-sampled</b> imagery that subsequently undergoes JPEG compression. Results are quantitatively similar with some small degradation in performance as the quality factor is reduced. Index Terms — Image forensics, <b>Re-sampling</b> detection, Normalized energy density...|$|R
40|$|Feature {{sensitive}} simplification and <b>re-sampling</b> {{of point}} set surfaces {{is an important}} and challenging issue for many computer graphics and geometric modeling applications. Based on the regular sampling of the Gaussian sphere and the surface normals mapping onto the Gaussian sphere, an adaptive <b>re-sampling</b> framework for point set surfaces is presented in this paper, which includes a naive sampling step by index propagation and a novel cluster optimization step by normalized rectification. Our proposed <b>re-sampling</b> scheme can generate non-uniformly distributed discrete sample points for the underlying point sets in a feature sensitive manner. The intrinsic geometric features of the underlying point set surfaces can be preserved efficiently due to our adaptive <b>re-sampling</b> scheme. A novel splat rendering technique is adopted to illustrate the efficiency of our <b>re-sampling</b> scheme. Moreover, a numerical error statistics and surface reconstruction for simplified models are also given to demonstrate the effectiveness of our algorithm in term of the simplified quality of the point set surfaces...|$|R
5000|$|A {{variety of}} data <b>re-sampling</b> {{techniques}} are {{implemented in the}} imbalanced-learn package [...] compatible with Python's scikit-learn interface. The <b>re-sampling</b> techniques are implemented in four different categories: undersampling the majority class, oversampling the minority class, combining over and under sampling, and ensembling sampling.|$|R
40|$|Biogeography-based {{optimization}} (BBO) {{is a new}} evolutionary optimization {{method that}} is based on the science of biogeography. In this paper, BBO is applied to the optimization of noisy problems. A noisy problem is one in which the fitness function is corrupted by random noise. Noise interferes with the BBO immigration rate and emigration rate, and adversely affects optimization performance. We analyze the effect of noise on BBO using a previously-derived Markov transition model. We also incorporate <b>re-sampling</b> in BBO, which samples the fitness of each candidate solution several times and calculates the average to alleviate the effects of noise. BBO is compared with particle swarm optimization (PSO) and differential evolution (DE) using noisy benchmark functions. The numerical results show that without <b>re-sampling,</b> DE performs best on the noisy benchmark functions; but when we use <b>re-sampling,</b> BBO performs best. In addition, BBO with <b>re-sampling</b> is compared with Kalman filter-based BBO (KBBO). The results show that BBO with <b>re-sampling</b> achieves almost the same performance as KBBO but consumes less computational resources. : evolutionary algorithm; biogeography-based optimization; noisy optimization; Kalman filter; re-sampling; migration 2 1...|$|R
40|$|Received xxxx; {{accepted}} xxxx Aims. Several {{authors have}} claimed {{to detect a}} significant cross-correlation between microwave WMAP anisotropies and the SDSS galaxy distribution. We repeat these analyses to determine the different cross-correlation uncertainties caused by <b>re-sampling</b> errors and field-to-field fluctuations. The first type of error concerns overlapping sky regions, while the second type concerns nonoverlapping sky regions. Methods. To measure the <b>re-sampling</b> errors, we use bootstrap and jack-knife techniques. For the field-to-field fluctuations, we use three methods: 1) evaluation of the dispersion in the cross-correlation when correlating separated regions of WMAP with the original region of SDSS; 2) use of mock Monte Carlo WMAP maps; 3) a new method (developed in this article), which measures the error {{as a function of}} the integral of the product of the self-correlations for each map. Results. The average cross-correlation for b> 30 deg. is significantly stronger than the <b>re-sampling</b> errors—both the jack-knife and bootstrap techniques provide similar results—but it is of the order of the field-to-field fluctuations. This is confirmed by the crosscorrelation between anisotropies and galaxies in more than the half of the sample being null within <b>re-sampling</b> errors. Conclusions. <b>Re-sampling</b> methods underestimate the errors. Field-to-field fluctuations dominate the detected signals. The ratio of signal to <b>re-sampling</b> errors is larger than unity in a way that strongly depends on the selected sky region. We therefore conclude tha...|$|R
5000|$|Line In with {{selectable}} sources (line in,mic) for sampling, <b>re-sampling</b> ...|$|R
40|$|Shape {{simplification}} and <b>re-sampling</b> {{of underlying}} point-sampled surfaces under userdefined error bounds {{is an important}} and challenging issue. Based on the regular triangulation of the Gaussian sphere and the surface normals mapping onto the Gaussian sphere, a Gaussian sphere based <b>re-sampling</b> scheme is presented that generates a non-uniformly curvature-aware simplification of the given point-sampled model. Owing to the theoretical analysis of shape isophotic error metric for did that Gaussian sphere based sampling, the proposed simplification scheme provides a convenient way to control the <b>re-sampling</b> results under a user-specified error metric bound. The novel algorithm has been implemented and demonstrated on several examples...|$|R
25|$|<b>Re-sampling</b> {{in order}} {{to assure that the}} image {{coordinate}} system is correct.|$|R
40|$|This {{technical}} report accompanies the manuscript "Conditional Modeling and the Jitter Method of Spike <b>Re-sampling.</b> " It contains further details, comments, references, and equations concerning various simulations and data analyses presented in that manuscript, {{as well as}} a self-contained Mathematical Appendix that provides a formal treatment of jitter-based spike <b>re-sampling</b> methods. Comment: 39 pages, 3 figure...|$|R
40|$|Conference Name: 9 th International Conference on Computer Science and Education, ICCCSE 2014. Conference Address: Vancouver, BC, Canada. Time:August 22, 2014 - August 24, 2014. Existing <b>re-sampling</b> {{methods such as}} Synthetic {{minority}} over-sampling technique (SMOTE) {{and random}} under-sampling (RUS) perform unsatisfactorily in some imbalanced data, even outperformed by non-sampling method like standard linear support vector machine (SVM). In this paper, we employ support vectors to approximately estimate the ratio of two class instances close to the boundary, and then apply the ratio for <b>re-sampling.</b> Experimental results show that <b>re-sampling</b> using the boundary ratio will perform well on real imbalanced datasets and the standard linear SVM could have better performance than <b>re-sampling</b> methods. Therefore, in terms of data, balance or imbalance, should not be simply interpreted as {{the ratio of the}} overall number of two class instances, but should be interpreted as the ratio close to the boundary...|$|R
40|$|The {{dissertation}} {{focused on}} the research, implementation, and evaluation of particle filters for radar target track filtering of a maneuvering target, through quantitative simulations and analysis thereof. Target track filtering, also called target track smoothing, aims to minimize the error between a radar target's predicted and actual position. From the literature it had been suggested that particle filters were more suitable for filtering in non-linear/non-Gaussian systems. Furthermore, it had been determined that particle filters were a relatively newer field of research relating to radar target track filtering for non-linear, non-Gaussian maneuvering target tracking problems, compared to the more traditional and widely known and implemented approaches and techniques. The objectives of the research project had been achieved {{through the development of}} a software radar target tracking filter simulator, which implemented a sequential importance <b>re-sampling</b> particle filter algorithm and suitable target and noise models. This particular particle filter had been identified from a review of the theory of particle filters. The theory of the more conventional tracking filters used in radar applications had also been reviewed and discussed. The performance of the sequential importance <b>re-sampling</b> particle filter for radar target track filtering had been evaluated through quantitative simulations and analysis thereof, using predefined metrics identified from the literature. These metrics had been the root mean squared error metric for accuracy, and the normalized processing time metric for computational complexity. It had been shown that the sequential importance <b>re-sampling</b> particle filter achieved improved accuracy performance in the track filtering of a maneuvering radar target in a non-Gaussian (Laplacian) noise environment, compared to a Gaussian noise environment. It had also been shown that the accuracy performance of the sequential importance <b>re-sampling</b> particle filter {{is a function of the}} number of particles used in the sequential importance <b>re-sampling</b> particle filter algorithm. The sequential importance <b>re-sampling</b> particle filter had also been compared to two conventional tracking filters, namely the alpha-beta filter and the Singer-Kalman filter, and had better accuracy performance in both cases. The normalized processing time of the sequential importance <b>re-sampling</b> particle filter had been shown to be a function of the number of particles used in the sequential importance <b>re-sampling</b> particle filter algorithm. The normalized processing time of the sequential importance <b>re-sampling</b> particle filter had been shown to be higher than that of both the alpha-beta filter and the Singer-Kalman filter. Analysis of the posterior Cramér-Rao lower bound of the sequential importance <b>re-sampling</b> particle filter had also been conducted and presented in the dissertation...|$|R
40|$|The {{relationship}} between total error, bias, and standard error of vertically scaled tests {{were examined in}} two simulated conditions [...] an ideal Item Response Theory (IRT) fit condition and a condition of IRT model misfit which was intended to approximate the type of misfit observed in operational data. Analytical estimates of standard error using an IRT information function and empirical estimates of standard error from a bootstrap <b>re-sampling</b> method were compared. A sufficient number of bootstrap <b>re-samples</b> required to yield {{the same degree of}} accuracy as 2000 <b>re-samples</b> was explored. ^ Analytical estimates of standard error were found to over-estimate standard error of vertical scale scores. The bootstrap method yielded more accurate estimates of the standard error as evidenced by the width of the confidence intervals for the proficiency levels and the coverage probabilities. Finally, a bootstrap resampling level of around 1000 was found to estimate standard errors with similar precision as compared to using 2000 <b>re-samples.</b> ...|$|R
5000|$|This song {{later was}} <b>re-sampled</b> for {{the chorus of}} Flo Rida's 2012 release; [...] "I Cry".|$|R
40|$|Abstract. Digital {{image data}} are now {{commonly}} {{used throughout the}} field of solar physics. Many steps of image data analysis, including image co-alignment, perspective reprojection of the solar surface, and compensation for solar rotation, require <b>re-sampling</b> original telescope image data under a distorting coordinate transformation. The most common image <b>re-sampling</b> methods introduce sig-nificant, unnecessary flaws into the data. More correct techniques have been known in the computer graphics community for some time but remain little known within the solar community and hence deserve further presentation. Furthermore, image distortion under specialized coordinate transform-ations is a powerful analysis technique with applications well beyond image resizing and perspective compensation. Here I give {{a brief overview of}} the mathematics of data <b>re-sampling</b> under arbitrary distortions, present a simple algorithm for optimized <b>re-sampling,</b> give some examples of distortion as an analysis tool, and introduce scientific image distortion software that is freely available over the Internet. “First get your facts straight. Then you can distort them as you please. ” – Mark Twain 1...|$|R
40|$|Historically, human-ignited {{fires were}} {{responsible}} for the extensive tallgrass ecosystems found east of the Mississippi River. More recently, fire was been the single best tool for restoring and conserving tallgrass prairie communities. Mowing is an often recommended substitute for fire, although there has been little evaluation of how well mowing mimics fire. In 2005 we <b>re-sampled</b> a tallgrass prairie remnant in southern Wisconsin that had been originally sampled in the 1940 s and had been <b>re-sampled</b> in the 1970 s, using 2 m x 2 m permanently located plots. Max Partch conducted the first two studies using 180 plots. In 2005 we <b>re-sampled</b> 114 of those plots; skipping those located in a flood plane, which is now dominated by Phalaris arundinacea and other invasive species...|$|R
40|$|This present study, {{proposes a}} new 3 D face {{correspondence}} method. A uniform mesh <b>re-sampling</b> algorithm {{is combined with}} mesh simplification algorithm to make correspondence between vertices of prototypical 3 D faces. Uniform mesh <b>re-sampling</b> algorithm is developed to obtain the same topology between 3 D faces with different structures. A global error metrics is proposed and mesh simplification is implemented on 3 D faces with same topologies simultaneously. The new method overcomes the limitation of conventional uniform mesh <b>re-sampling</b> and optical flow algorithm, decreases the vertices, and triangles that need to represent 3 D face while preserving correspondence between vertices of the prototypes. The experimental results show the new method gives good performance on computing 3 D face correspondence...|$|R
5000|$|In some cases, {{advanced}} DJ mixes {{can include}} live-set elements, e.g. Traktor's remix decks, cue juggling, live <b>re-sampling,</b> etc.|$|R
40|$|AIMS. Several {{authors have}} claimed {{to detect a}} {{significant}} cross-correlation between microwave WMAP anisotropies and the SDSS galaxy distribution. We repeat these analyses to determine the different cross-correlation uncertainties caused by <b>re-sampling</b> errors and field-to-field fluctuations. The first type of error concerns overlapping sky regions, while the second type concerns non-overlapping sky regions. METHODS. To measure the <b>re-sampling</b> errors, we use bootstrap and jack-knife techniques. For the field-to-field fluctuations, we use three methods: 1) evaluation of the dispersion in the cross-correlation when correlating separated regions of WMAP with the original region of SDSS; 2) use of mock Monte Carlo WMAP maps; 3) a new method (developed in this article), which measures the error {{as a function of}} the integral of the product of the self-correlations for each map. RESULTS. The average cross-correlation for b> 30 deg. is significantly stronger than the <b>re-sampling</b> errors [...] both the jack-knife and bootstrap techniques provide similar results [...] but it is of the order of the field-to-field fluctuations. This is confirmed by the cross-correlation between anisotropies and galaxies in more than the half of the sample being null within <b>re-sampling</b> errors. CONCLUSIONS. <b>Re-sampling</b> methods underestimate the errors. Field-to-field fluctuations dominate the detected signals. The ratio of signal to <b>re-sampling</b> errors is larger than unity in a way that strongly depends on the selected sky region. We therefore conclude that there is no evidence yet of a significant detection of the integrated Sachs-Wolfe (ISW) effect. Hence, the value of Omega_Λ 0. 8 obtained by the authors who assumed they were observing the ISW effect would appear to have originated from noise analysisComment: 5 pages and three figures. Accepted for publication in Astronomy and Astrophysics. V 2 corrected to match the published pape...|$|R
40|$|Abstract—A new Gaussian mixture filter {{has been}} developed, one {{that uses a}} <b>re-sampling</b> step in order to limit the covariances of its {{individual}} Gaussian components. The new filter {{has been designed to}} produce accurate solutions of difficult nonlinear/non-Bayesian estimation problems. It uses static multiple-model filter calculations and Extended Kalman Filter (EKF) approximations for each Gaussian mixand in order to perform dynamic propagation and measurement update. The <b>re-sampling</b> step uses a newly designed algorithm that employs linear matrix inequalities in order to bound each mixand's covariance. <b>Re-sampling</b> occurs between the dynamic propagation and the measurement update in order to ensure bounded covariance in both of these operations. The resulting filter has been tested on a difficult 7 -state nonlinear filtering problem. It achieves significantly better accuracy than a simple EKF, an Unscente...|$|R
40|$|Support Vector Machine (SVM) {{has been}} widely studied and shown success in many {{application}} fields. However, the performance of SVM drops significantly when it {{is applied to the}} problem of learning from imbalanced data sets in which negative instances greatly outnumber the positive instances. This paper analyzes the intrinsic factors behind this failure and proposes a suitable <b>re-sampling</b> method. We <b>re-sample</b> the imbalance data by using variable SOM clustering so as to overcome the flaws of the traditional <b>re-sampling</b> methods, such as serious randomness, subjective interference and information loss. Then we prune the training set by means of K-NN rule {{to solve the problem of}} data confusion, which improves the generalization ability of SVM. Experiment results show that our method obviously improves the performance of the SVM on imbalanced data sets...|$|R
3000|$|Step 4 : The {{degeneracy}} {{of weight}} is usually inevitable after several times of iteration, and weight degeneracy may cause the approximation of posterior probability to deteriorate seriously and even become useless. <b>Re-sampling</b> procedure is {{proposed to alleviate}} this problem efficiently. The main idea of <b>re-sampling</b> is to eliminate particle trajectories with small normalized importance weight while concentrating upon those particles which have larger normalized importance weight, and implementation procedures are shown as follows [27]: [...]...|$|R
40|$|Abstract:- Digital {{filters are}} {{designed}} to provide both satisfied magnitude and linear phase responses for digital <b>re-sampling</b> in power system applications in this paper. This design of filters consists of two steps: magnitude approximation and group delay compensation. It also compromises {{between the amount of}} calculation needed and the performance of the filters, which makes the filters good candidates for both real-time and offline digital <b>re-sampling</b> applications especially for the synchronous data acquisition in Wide Area Measurement Systems (WAMS) ...|$|R
40|$|Abstract — This paper {{concerns}} {{context and}} feature-sensitive <b>re-sampling</b> of workspace surfaces represented by 3 D point clouds. We interpret a point cloud as {{the outcome of}} repetitive and non-uniform sampling of the surfaces in the workspace. The nature of this sampling may not be ideal for all applications, representations and downstream processing. For example it might be preferable {{to have a high}} point density around sharp edges or near marked changes in texture. Additionally such preferences might be dependent on the semantic classification of the surface in question. This paper addresses this issue and provides a framework which given a raw point cloud as input, produces a new point cloud by <b>re-sampling</b> from the underlying workspace surfaces. Moreover it does this in a manner which can be biased by local low-level geometric or appearance properties and higher level (semantic) classification of the surface. We are in no way prescriptive about what justifies a biasing in the <b>re-sampling</b> scheme — this is left up to the user who may encapsulate what constitutes “interesting ” into one or more “policies ” which are used to modulate the default <b>re-sampling</b> behavior. I...|$|R
3000|$|The {{propagation}} {{model has been}} chosen to be a Gaussian noise added {{to the state of}} the particles after the <b>re-sampling</b> step: [...]...|$|R
5000|$|The {{instrumental}} of {{the song}} was also <b>re-sampled</b> in 2015 on the song [...] "Cream", by Tujamo and Danny Avila, released on Spinnin' Records ...|$|R
30|$|In this paper, a {{new method}} for {{frequency}} estimation and tracking was proposed, in which DFT algorithm iterates with <b>re-sampling</b> to confront frequency change in dynamic states {{of a power}} system. In each cycle, precise frequency estimation is accomplished by iterative DFT. In the following cycle, the initial sampling frequency is given by this converged frequency, and so on. The proposed iterative DFT by <b>re-sampling</b> is a two-rounded process to provide precise frequency estimation and frequency tracking ability dynamically in a power system.|$|R
30|$|As a result, if the {{convolution}} {{separation and}} image <b>re-sampling</b> techniques work cooperatively, the theoretical detection speed will be six-folded by 3 [*]×[*] 3 filter.|$|R
5000|$|... perform <b>re-sampling</b> and format conversions; for {{instance}} {{to allow a}} program that requires 44.1 kHz sample frequency to use a device that supports 48 kHz only.|$|R
40|$|We take a queue with {{breakdowns}} {{and repairs}} of processors {{in which the}} queue length does not change on the corresponding Markov modula-tion transitions as given by Mitrani and Chakka [1], and introduce simul-taneous losses on breakdown (and <b>re-sampling</b> on repair) using a range of methods, most interestingly including enriching the modulation structure to cause simultaneous loss or <b>re-sampling,</b> which gives distinct behaviour. We solve for the steady state of the queue using spectral expansion for convenient access {{to a range of}} performance measures. ...|$|R
