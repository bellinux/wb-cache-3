8|72|Public
40|$|Carbon foams {{have been}} {{manufactured}} at EG 2 ̆ 6 G Mound Applied Technologies {{through the use}} of a salt <b>replica</b> <b>process</b> [1, 2] that has been modified by a Mound propriety process [3]. Applications of these foams have been described in an early publication [4]. In the basic process [1, 2] of manufacturing the foams, salt is pressed into bars; the bars are then cured, infused with polymer and cured again. The salt is then removed by copious solvent rinsings and finally carbonized into very porous and light-weight, briquette-like material [2, 5]. In this paper, the carbon density and the carbon distribution in various foams were determined either by bulk measurements of weight and volume or by x-ray computed tomography (CT) ...|$|E
40|$|Process {{allocation}} {{is important}} for fault-tolerant multi-computer systems since it directly affects the performance and dependability of the system. In this paper, we consider load-balancing process allocation for fault-tolerant systems that balances the load before as well as after faults start to degrade {{the performance of the}} system. We show two schemes to tolerate multiple faults in the passive <b>replica</b> <b>process</b> model. The first scheme is running multiple backup processes for each process. The second scheme is running only one backup process for each process, but re-generate backup processes for processes that do not have a backup process after a fault occurrence to keep the primarybackup process pair available. In both schemes, we propose heuristic process allocation methods for balancing loads inspite of the occurrence of a fault. Simulation is used to compare the performance of both schemes. In each scheme, we are able to keep the load balanced reasonably among nodes after a fault [...] ...|$|E
40|$|Abstract—Data mining is an {{important}} technique in summarize and prediction in different industry. With the help of grid computer, the capability for data storage and efficiency of data mining process can be highly increased. Nowadays, most of studies focus on data replication mechanism and replica selection method, but ignore the part of service replica mechanism which is also important. In the paper, we propose a Dynamic Service <b>Replica</b> <b>Process</b> (DSRP) system based on Service-Oriented Architecture (SOA). DSRP has a mechanism to create and delete service replica automatically to achieve better load-balancing and performance. In the process, each activity in data mining process {{is viewed as a}} web service and placed in different computer on data mining grid. User can define different data mining task by selecting proper services and the replication mechanism is sprung from the result of usage. The web services provide data extracting, data preprocessing, data mining algorithms and analysis...|$|E
50|$|Checkpoints may {{be added}} to any State Machine by {{supporting}} an additional Input called CHECKPOINT. Each replica maintains a checkpoint {{in addition to the}} current State value. When the log grows large, a replica submits the CHECKPOINT command just like a client request. The system will ensure non-faulty <b>replicas</b> <b>process</b> this command in the same order, after which all log entries before the checkpoint may be discarded.|$|R
5000|$|Server groups: Group {{one or more}} servers {{containing}} the same data and functionality. A typical implementation is the multi-master, multi-replica environment in which <b>replicas</b> <b>process</b> [...] "read" [...] requests and are in one server group, while masters process [...] "write" [...] requests and are in another, so that servers are grouped by their response to external stimuli, even though all share the same data.|$|R
5000|$|... #Caption: State {{of the art}} of the HMS Beagle's <b>replica</b> {{building}} <b>process</b> as {{for march}} 20, 2013 ...|$|R
40|$|Antenna gain {{affected}} by reflector surface figure accuracy and dimension stability directly, {{so one of}} the most important tasks is how to ensure the surface precision and dimensional stability. It is hard to control surface precision for springback of metal, so carbon fibre reinforced polymer (CFRP) usually be adopted to fabricate high precision reflector. With the rapid development of electronic technology, especially millimetre and terahertz wave technology, the precision of reflector needed increasingly. A Φ 300 mm CFRP flat reflector is developed for process study. In order to improve the thermal stability, a special "all CFRP" structure adopted. Optical <b>replica</b> <b>process</b> used to realize surface modification of CFRP reflector blank, final surface figure accuracy RMS reaching 0. 1 μm, and roughness Ra reaching 2 nm. Further thermal stability tests show that the thermal stability reaching 13 nm/C. AΦ 500 mm CFRP aspherical reflector also fabricated, and surface accuracy reaching 0. 4 μm. The study is of certain reference value for the development of CFRP reflector in millimetre wave and terahertz wave band. </p...|$|E
40|$|Abstract In this paper, {{we propose}} a novel algorithm, called update {{propagation}} through replica chain (UP-TReC), to maintain le consistency in decentralized and unstructured peer-to-peer (P 2 P) systems. In UPTReC, each le has a logical replica chain composed of all replica peers (RPs), where an RP is dened as a peer {{that has a}} replica of the le. Each RP has partial knowledge of the chain by keeping a list of k nearest RPs ’ information in each direction. The replica chain is naturally built and easily maintained during the le <b>replica</b> <b>process.</b> When an RP ini-tiates an update, it pushes the update to all possible online (active) RPs through the replica chain. A reconnected RP pulls an online RP to synchronize the replica status. An an-alytical model is derived to evaluate the performance of UP-TReC algorithm. Simulation experiments are conducted to compare the performance with an existing update prop-agation algorithm based on the rumor spreading scheme. The experimental results show that the UPTReC can sig-nicantly reduce (up to 70 %) overhead messages and also achieve smaller stale query ratio for frequently updating les. I...|$|E
40|$|X-ray Astronomy {{is a young}} eld of research. Since the rst X-ray {{observation}} {{performed by}} means of a rocket ight in the ' 60 s, the greatest leap forward in Xray telescopes performances was brought in the ' 70 s when the imaging capability was introduced thanks to the use of grazing incidence optics. The excellent results of grazing incidence telescopes in the soft x-ray imaging are well-known and represented by the results obtained with emissions ROSAT, Chandra and XMMNewton. However, the extension of the imaging capability to the hard X-rays (E 10 keV) will allow the observations of sources critical for their role in cosmology like AGN, and clusters of galaxies. In the last decade the aim of extending the imaging capability to hard X-rays (E 10 keV) brings to the development of speci c and challenging missions, in this thesis the reference mission is New Hard X-ray Mission (NHXM), a mission of the Italian Space Agency, which development is in cooperation with the INAF-Astronomical Observatory of Merate (OAB) and with Media Lario Tech. for the studies concerning the NHXM Wolter-like optics production. Wolter-like optics, the de-facto standard for the realization of high resolution focusing telescopes, are manufactured by Nickel-Cobalt eletroforming. Once the optics theoretical performances have been optimized by means of the opportune optical design strong image degradation can still be caused by surface errors e ect generated in the manufacturing process. When surface pro le errors are on macroscopical length scale (mm) their e ects on optical performance scan be evaluated considering geometrical optics and simulating the optical path of incident photons by means of ray-tracing codes. In this case the imaging capability degradation is independent from photons energy. Further image degradation e ects are due to the microroughness of the reflection surface which e ect behaviour follows the X-ray scattering laws. The XRS e ect is to spread a fraction, increasing with photons energy, of the incident radiation around the direction of the reflected beam. When hard X-ray are considered the requirements to limit the XRS e ects are to have reflection surface with small surface errors also on microscopical length scale (m scale). My Ph. D. research deals with the development of methods allowing estimating and controlling the X-ray Wolter-like telescopes optics angular resolution degradation caused by optics surface errors. My activities are separable in two fields: The effects on angular resolution at high energy of the optical surface microrougness variation. This point had been met changing the reflecting layer substrate in the <b>replica</b> <b>process</b> of manufacturing. The development of two new metrological devices, capable of estimating the angular resolution degradation, due to mandrels and mirror shells shape errors. The two presented points requires the multi-instrumental complete optics surface characterization, covering the surface errors wavelengths scale in the broad interval 1 m - 1 m. The thesis discussion is organized as follow: Presentation of the NHXM mission and of its target observational sources (Chapter 1) Description of the Wolter-like optics geometry, manufacturing process, and characteristic surface errors (Chapter 2) Study of the angular resolution energy dependent term in function of Gold layer thinning in Ni-Co electroformed <b>replica</b> <b>process</b> (Chapter 3) Development of a new mandrel metrological device (Chapter 6) Development of a new mirror shells metrological device (Chapter 7) Conclusions about the followed research plan (Chapter 8...|$|E
50|$|If at {{any time}} one master replica is {{designated}} to process all the requests, then {{we are talking about}} the primary-backup scheme (master-slave scheme) predominant in high-availability clusters. On the other side, if any <b>replica</b> <b>processes</b> a request and then distributes a new state, then this is a multi-primary scheme (called multi-master in the database field). In the multi-primary scheme, some form of distributed concurrency control must be used, such as distributed lock manager.|$|R
40|$|The {{problem of}} {{defining}} a general scheme for interfacing replication for software fault tolerance (diverse software versions) and system replication techniques (distributed execution of <b>process</b> <b>replicas)</b> is investigated. An algorithm, called Minimum Information Algorithm, {{is defined in}} order to couple object copies and <b>process</b> <b>replicas...</b>|$|R
40|$|Abstract. We study {{information}} transmission in large interim quasilinear economies using {{the theory of}} the core. We concentrate on the core with respect to equilibrium blocking, a core notion in which information is trans-mitted endogenously within coalitions, as blocking can be understood as an equilibrium of a communication mechanism used by players in coalitions. We focus on independent replicas of the basic economy. We offer both neg-ative and positive convergence results {{as a function of the}} complexity of the mechanisms used by coalitions. Further, all the results are robust to the relaxation of the incentive constraints. Results for ex-post and signal-based <b>replica</b> <b>processes</b> are also obtained...|$|R
40|$|The {{lower limit}} of size of {{biological}} objects which can be photographed with the electron microscope, by the aid of shadow‐casting, is shown to depend upon the smoothness of substrate upon which they can be mounted, and upon the continuity of structure of the thin films {{with which they are}} shadowed. Numerous attempts to improve the existing deficiencies are reported, both with respect to producing smoother substrate films, and to producing films for shadow‐casting of high efficiency and continuity of structure. No success has been encountered in producing a usable substrate film perceptibly smoother than the collodion and Formvar films commonly used. It is found that the best shadow‐casting material for this type of film is uranium or uranium oxide. Verification has been obtained of the severe granulation of gold films previously used in the pre‐shadowed <b>replica</b> <b>process,</b> when subjected to the electron current of a biased‐beam electron gun. Attempts to reduce the granulation to a satisfactory level have failed. Various methods of preparing pre‐shadowed replicas are reported. The factors affecting the tenacity of evaporated films to glass surfaces are discussed. It is found that elements which oxidize readily are relatively adherent to glass, while the elements gold, palladium, platinum, and rhodium are not. Uranium sulfide {{can be used as a}} pre‐shadowed replica material, but only with some uncertainty, owing to its chemical instability. It has been found that a palladium‐platinum mixture is the most satisfactory material for use in the pre‐shadowed replica technique, and that films of this mixture in a thickness of about 6 A produce adequate shadows in which there is no sign of granulation. The surface of clean glass is again found to have the smoothest structure of any material known, with practically no sharp discontinuities in elevation as great as 10 A. An appendix is given, in which technical details of shadow‐casting and replica production are described...|$|E
40|$|Astronomy {{is driven}} by the quest for higher {{sensitivity}} and improved angular resolution in order to detect fainter or smaller objects. The far-infrared to submillimeter domain is a unique probe of the cold and obscured Universe, harboring for instance the precious signatures of key elements such as water. Space observations are mandatory given the blocking effect of our atmosphere. However the methods we have relied on so far to develop increasingly larger telescopes are now reaching a hard limit, with the JWST illustrating this in more than one way (e. g. it will be launched by {{one of the most powerful}} rocket, it requires the largest existing facility on Earth to be qualified). With the Thinned Aperture Light Collector (TALC) project, a concept of a deployable 20 m annular telescope, we propose to break out of this deadlock by developing novel technologies for space telescopes, which are disruptive in three aspects: &# 8226; An innovative deployable mirror whose topology, based on stacking rather than folding, leads to an optimum ratio of collecting area over volume, and creates a telescope with an eight times larger collecting area and three times higher angular resolution compared to JWST from the same pre-deployed volume; &# 8226; An ultra-light weight segmented primary mirror, based on electrodeposited Nickel, Composite and Honeycomb stacks, built with a <b>replica</b> <b>process</b> to control costs and mitigate the industrial risks; &# 8226; An active optics control layer based on piezo-electric layers incorporated into the mirror rear shell allowing control of the shape by internal stress rather than by reaction on a structure. We present in this paper the roadmap we have built to bring these three disruptive technologies to technology readiness level 3. We will achieve this goal through design and realization of representative elements: segments of mirrors for optical quality verification, active optics implemented on representative mirror stacks to characterize the shape correction capabilities, and mechanical models for validation of the deployment concept. Accompanying these developments, a strong system activity will ensure that the ultimate goal of having an integrated system can be met, especially in terms of (a) scalability toward a larger structure, and (b) verification philosophy...|$|E
40|$|The {{fabrication}} of flexible and transparent microplasma devices using plastic substrate has explored new avenues in plasma science and technology. A polymer-based <b>replica</b> molding <b>process</b> enables inexpensive and accurate production of microcavities of the devices. Best {{known as the}} sources of visible, ultraviolet (UV) and vacuum ultraviolet (VUV) radiation, microplasma devices fabricated by replica molding techniques also show potential applications as transparent and flexible displays, a radiation source for medical phototherapy, and photonic circuit light sources. In this thesis, the basic physics of plasma discharges is discussed and the performance and {{fabrication of}} microplasma devices by a <b>replica</b> molding <b>process</b> is presented in detail...|$|R
40|$|Fault {{tolerance}} is {{an efficient}} approach adopted to avoid or reduce the damage {{of a system}} failure. In this work we present {{the results of a}} fault injection campaign we conducted on the Duplex Framework (DF). The DF is a software developed by the UCLA group [1, 2] that uses a fault tolerant approach and allows to run two replicas of the same process on two different nodes of a commercial off-the-shelf (COTS) computer cluster. A third process running on a different node, constantly monitors the results computed by the two replicas, and eventually restarts the two <b>replica</b> <b>processes</b> if an inconsistency in their computation is detected. This approach is very cost efficient and can be adopted to control processes on spacecrafts where the fault rate produced by cosmic rays is not very high...|$|R
50|$|If left {{unchecked}} a log {{will grow}} until it exhausts all available storage resources. For continued operation, {{it is necessary}} to forget log entries. In general a log entry may be forgotten when its contents are no longer relevant (for instance if all <b>replicas</b> have <b>processed</b> an Input, the knowledge of the Input is no longer needed).|$|R
40|$|We {{present a}} replication-based {{approach}} to fault-tolerant distributed stream processing {{in the face}} of node failures, network failures, and network partitions. Our approach aims to reduce the degree of inconsistency in the system while guaranteeing that available inputs capable of being processed are processed within a specified time threshold. This threshold allows a user to trade availability for consistency: a larger time threshold decreases availability but limits inconsistency, while a smaller threshold increases availability but produces more inconsistent results based on partial data. In addition, when failures heal, our scheme corrects previously produced results, ensuring eventual consistency. Our scheme uses a data-serializing operator to ensure that all <b>replicas</b> <b>process</b> data in the same order, and thus remain consistent in the absence of failures. To regain consistency after a failure heals, we experimentally compare approaches based on checkpoint/redo and undo/redo techniques and illustrate the performance trade-offs between these schemes. 1...|$|R
40|$|Services {{offered by}} {{computing}} systems {{continue to play}} a crucial role in our every day lives. This thesis examines and solves a challenging problem in making these services dependable using means that can be assured not to compromise service responsiveness, particularly when no failure occurs. Causes of undependability are faults and faults of all known origins, including malicious attacks, are collectively referred to as Byzantine faults. Service or state machine replication is the only known technique for tolerating Byzantine faults. It becomes more effective when replicas are spaced out over a wide area network (WAN) such as the Internet – adding tolerance to localised disasters. It requires that <b>replicas</b> <b>process</b> the randomly arriving user requests in an identical order. Achieving this requirement together with deterministic termination guarantees is impossible in a fail-prone environment. This impossibility prevails because of the inability to accurately estimate a bound on inter-replica communication delays over a WAN. Canonical protocols in the literature are designed to delay termination until th...|$|R
50|$|In May 2012, {{a wooden}} <b>replica</b> statue was <b>processed</b> by Chinese American Catholics in New York, United States. The {{religious}} image was solemnly approved and blessed by Bishop Guy Sansaricq.|$|R
40|$|Through {{the study}} <b>replica</b> {{regulatory}} <b>process</b> glucose and insulin {{concentrations in the}} body, it has studied the influence of parameter mechanisms of pancreatic organ transport rate constant (K) on achieving normal levels of glucose concentration. In this study {{has been carried out}} curve fitting method between clinical data and model equations glucose and insulin concentrations for the three conditions of the patient, namely: glucose low, normal and high of six patients...|$|R
40|$|Polymers {{have the}} ability to conform to surface {{contours}} down to a few nanometres. We studied the filling of transparent epoxy-type EPON SU- 8 into nanoscale apertures made in a thin metal film as a new method for polymer/metal near-field optical structures. Mould <b>replica</b> <b>processes</b> combining silicon micromachining with the photo-curable SU- 8 offer great potential for low-cost nanostructure fabrication. In addition to offering a route for mass production, the transparent pyramidal probes are expected to improve light transmission thanks to a wider geometry near the aperture. By combining silicon MEMS, mould geometry tuning by oxidation, anti-adhesion coating by self-assembled monolayer and mechanical release steps, we propose an advanced method for near-field optical probe fabrication. The major improvement is the possibility to fabricate nanoscale apertures directly on wafer scale during the microfabrication process and not on free-standing tips. Optical measurements were performed with the fabricated probes. The full width half maximum after a Gaussian fit of the intensity profile indicates a lateral optical resolution of approximate to 60 nm...|$|R
40|$|Diversity plays {{a crucial}} role for the {{survivability}} of every biological species and, quite recently, the concept, has also been applied to computer programs. An interpretation of the notion of software diversity, is based on the concept of diversified <b>process</b> <b>replicæ.</b> We define pr as the <b>replica</b> of a <b>process</b> p which behaves identically to p even if it presents some “structural ” diversity from it. This makes possible to devise mechanisms for detecting memory corruption attacks in a deterministic way. In our solution, a process p and its replica pr only differ in their address space which is properly diversified, thus defeating absolute and partial overwriting memory error exploits. We also give a complete characterization and we propose a solution of shared memory management, one of the biggest practical issue introduced by diversified <b>process</b> <b>replicæ.</b> Preliminary ideas {{on how to deal with}} synchronous signals delivery between p and pr are faced as well. A proof-of-concept prototype working in user space has been implemented. Our experimental results show a 68. 93 % throughput slowdown on a testbed web server application in the worst-case, while only a 1. 20 % throughput slowdown has been obtained in the best-case. ...|$|R
40|$|We present GHUMVEE, a multi-variant {{execution}} {{engine for}} software intrusion detection. GHUMVEE transparently executes and monitors diversified <b>replicae</b> of <b>processes</b> to thwart attacks {{relying on a}} predictable, single data layout. Unlike existing tools, GHUMVEE's interventions in the process' execution {{are not limited to}} system call invocations. Because of that design decision, GHUMVEE can handle complex, multi-threaded real-life programs that display non-deterministic behavior as a result of non-deterministic thread scheduling {{and as a result of}} pointer-value dependent behavior. This capability is demonstrated on GUI programs from the Gnome and KDE desktop environments...|$|R
30|$|A {{framework}} {{has been}} proposed in (Liang and Bin 2010) for accomplishing a changeable-location replica at runtime. Fundamentally, the suggested framework establishes an adaptive re-selection of a higher priority service when the main service or host fails. The service priority order is organized based on considering some QoS constraints including the network availability {{as well as the}} host machine performance, in addition to the execution time and reliability of the services. The main disadvantage of this framework {{is that it does not}} consider any other crucial metrics within the <b>replica</b> re-selection <b>process</b> such as the service or host load.|$|R
40|$|Polydimethylsiloxane (PDMS) {{has been}} {{introduced}} {{the first time about}} 20 years ago. This polymer is worldwide used for the rapid prototyping of microfluidic device through a <b>replica</b> molding <b>process.</b> However, the great popularity of PDMS is not only related to its easy processability, but also to its chemical and physical properties. For its interesting properties, the polymer has been implied for several applications, including sensing. In this work, we investigated how to use functionalized PDMS membranes as sensing elements in optical sensors for gas detection in water samples. Keywords: Polydimethylsiloxane (PDMS), Surface Plasmon Resonance (SPR) sensors, Gas senso...|$|R
40|$|An {{interpretation}} {{of the notion of}} software diversity is based on the concept of diversified <b>process</b> <b>replicæ.</b> We define pr as the <b>replica</b> of a <b>process</b> p which behaves identically to p but has some “structural ” diversity from it. This makes possible to detect memory corruption attacks in a deterministic way. In our solution, p and pr differ in their address space which is properly diversified, thus defeating absolute and partial overwriting memory error exploits. We also give a characterization and a preliminary solution for shared memory management, one of the biggest practical issue introduced by this approach. Speculation {{on how to deal with}} synchronous signals delivery is faced as well. A user space proof-of-concept prototype has been implemented. Experimental results show a 68. 93 % throughput slowdown on a worst-case, while experiencing only a 1. 20 % slowdown on a best-case. 1...|$|R
40|$|Many group {{communication}} {{systems have been}} implemented and improved lately, providing strong guarantees in message order deliv-ery. These implementations rely on ring topologies to achieve great performances with low response times. However, many database repli-cation protocols do not require strong guarantees to work properly and other solutions not based on ring topologies can be used. This paper presents a simple {{group communication}} system that provides reliable communication {{among a number of}} <b>replica</b> control <b>processes.</b> This sys-tem pretends to be a fast direct solution avoiding ring topology. It will behave in a different way to the other ones and this will be analysed with some practical experiments. ...|$|R
40|$|A new {{technique}} for {{the realization of}} complex three dimensional 3 D structures in polymer materials is presented. The described process can be applied for the fabrication of 3 D structured foils {{as well as for}} 3 D structured polymer parts using a <b>replica</b> molding <b>process.</b> In the first step, the foil is structured by Hot Embossing. This structured foil is then blown into a structured mold by pressure at high temperature, using a thermoforming process. The thermoforming process is realized in an especially designed tool, where different mold inserts can be applied. In the thermoforming process this tool, containing the mold insert and the structured foil is heated up and a pressure is applied which assures, that the foil covers the structured mold insert. Due to parameter optimization, high pattern fidelity can be achieved. As structures a porefield with a pore diameter of 500 nm and lines and spaces with a width of 1. 6 lm were used. Besides a Moth Eye structure with a period of 280 nm and a blazed grating with a period of 1 lm and a blazed angle of 15 were imprinted and blown into the structured mold. The results show, that this process can be used to fabricate 3 D structured foils with good structure fidelity. Besides the structured foils, additional poly dimethylsiloxane PDMS replications can be made out of this foils. These PDMS replications are used as stamps in a <b>replica</b> molding <b>process.</b> A micro fluidic system containing hydrophobic and hydrophilic channels was created using this proces...|$|R
40|$|By now, {{probably}} {{obvious that}} systems reliability/availability {{is a key}} concern � Downtime is expensive � Replication is a general technique for providing fault toleranceReplication unreplicated service client server Replication unreplicated service client replicated service client server server replicas Replication � Applications as deterministic state machines � Reduce the problem of replication to that of agreement � Ensure that <b>replicas</b> <b>process</b> requests in the same order: � Sft Safety: clients li never observe inconsistent i t behavior � Liveness: system is always able to make progressTraditional Assumptions � Synchrony � Bounded difference in CPU speeds � Bounded time for message delivery � Benign/Crash faults � When machines fail, they stop producing output immediately, and forever. What if these assumptions don’t hold? Asynchrony � In the real world, systems are never quite as synchronous {{as we would like}} � Asynchrony is a pessimistic i i assumption to capture real world phenomenon � � Messages will eventually be delivered, processors will eventually complete computation. But no bound on time. � In general: � OK to assume synchrony when providing liveness � Dangerous (NOT OK) to assume synchrony for safetyByzantine Faults � Crash faults are a strong assumption � In practice, many kinds of problems can manifest: � Bit flip in memory � Intermittent network errors � Mlii Malicious attacks tt � Byzantine faults: strongest failure model � � Completely arbitrary behavior of faulty nodesByzantine Agreement � Can we build systems that tolerate Byzantine failures and asynchrony? YES! � Use replication li i + Byzantine agreement protocol to order request...|$|R
40|$|This paper {{presents}} the design, creation {{and use of}} tangible smart replicas in a large-scale museum exhibition. We describe the design rationale for the <b>replicas,</b> the <b>process</b> used in their creation, {{as well as the}} implementation and deployment of these replicas in a live museum exhibition. Deployment of the exhibition resulted in over 14000 visitors interacting with the system during the 6 months that the exhibition was open. Based on log data, interviews and observations, we examine the reaction to these smart replicas {{from the point of view}} of the museum curators and also of the museum's visitors and reflect on the fulfillment of our expectations. Keywords : Tangible interaction, museums, smart replicas...|$|R
40|$|This study {{evaluated}} the fracture strength and microtensile bond {{strength of a}} new computer-aided design (CAD) veneering method for zirconia frameworks. A new CAD/computer-assisted manufacture system was used to fabricate a resin replica of the esthetic ceramic required to veneer a zirconia framework. The <b>replicas</b> were <b>processed</b> using press-on technology. Identical manually layered zirconia specimens served as a control (n = 18; alpha =. 05). Statistical analysis revealed that the fracture strength (442. 8 +/- 25 N) and microtensile bond strength (26. 6 +/- 2 MPa) of CAD-veneered zirconia were significantly higher (P <. 001) compared to the control (346 +/- 24 N and 15. 1 +/- 1 MPa, respectively). CAD veneering is a reliable method for veneering zirconia frameworks...|$|R
40|$|Abstract. The Data Grid {{enables the}} sharing, selection, and {{connection}} {{of a wide}} variety of geographically distributed computational and storage resources for solving large-scale data intensive scientific applications. Such technology efficiently manage and transfer terabytes or even petabytes of data for dataintensive, high-performance computing applications in wide-area, distributed computing environments. <b>Replica</b> selection <b>process</b> allows an application to choose a replica from replica catalog, based on its performance and data access features. In this paper, we build a Grid environment based on three existing PC Cluster environments and perform performance analysis of data transfers using GridFTP protocol over these systems. In addition, based on experimental results, it is proposed a cost model to pick the best replica, in real and dynamic network situations...|$|R
40|$|This paper {{presents}} an innovative mixing technology for centrifugal microfluidic platforms actuated using {{a specially designed}} flyball governor. The multilayer microfluidic disc was fabricated using a polydimethylsiloxane (PDMS) <b>replica</b> molding <b>process</b> with a soft lithography technique. The operational principle {{is based on the}} interaction between the elastic covering membrane and an actuator pin installed on the flyball governor system. The flyball governor was used as the transducer to convert the rotary motion into a reciprocating linear motion of the pin pressing against the covering membrane of the mixer chamber. When the rotation speed of the microfluidic disc was periodically altered, the mixing chamber was compressed and released accordingly. In this way, enhanced active mixing can be achieved with much better efficiency in comparison with diffusive mixing...|$|R
40|$|Fingerprint {{liveness}} detection {{consists in}} verifying if an input fingerprint image, acquired by a fingerprint verification system, {{belongs to a}} genuine user or is an artificial replica. Although several hardware- and software-based approaches have been proposed so far, this issue still remains unsolved due to the very high difficulty in finding effective features for detecting the fingerprint liveness. In this paper, we present a novel features set, based on the local phase quantization (LPQ) of fingerprint images. LPQ method is well-known for being insensitive to blurring effects, thus we believe {{it could be useful}} for detecting the differences between an alive and a fake fingerprint, due to the loss of information which may occur during the <b>replica</b> fabrication <b>process.</b> The method is tested on the four data sets of the Second International Fingerprint Liveness Detection Competition, and shows promising and competitive results with other state-of-the-art features sets. Fingerprint liveness detection consists in verifying if an input fingerprint image, acquired by a fingerprint verification system, belongs to a genuine user or is an artificial replica. Although several hardware- and software-based approaches have been proposed so far, this issue still remains unsolved due to the very high difficulty in finding effective features for detecting the fingerprint liveness. In this paper, we present a novel features set, based on the local phase quantization (LPQ) of fingerprint images. LPQ method is well-known for being insensitive to blurring effects, thus we believe it could be useful for detecting the differences between an alive and a fake fingerprint, due to the loss of information which may occur during the <b>replica</b> fabrication <b>process.</b> The method is tested on the four data sets of the Second International Fingerprint Liveness Detection Competition, and shows promising and competitive results with other state-of-the-art features sets. © 2012 ICPR Org Committee...|$|R
40|$|Efficient {{embedding}} virtual clusters {{in physical}} network is a challenging problem. In this paper we consider a scenario where physical network has {{a structure of}} a balanced tree. This assumption is justified by many real- world implementations of datacenters. We consider an extension to virtual cluster embedding by introducing replication among data chunks. In many real-world applications, data is stored in distributed and redundant way. This assumption introduces additional hardness in deciding what <b>replica</b> to <b>process.</b> By reduction from classical NP-complete problem of Boolean Satisfia- bility, we show limits of optimality of embedding. Our result holds even in trees of edge height bounded by three. Also, we show that limiting repli- cation factor to two replicas per chunk type {{does not make the}} problem simpler. Comment: 2 figure...|$|R
