38|4873|Public
5000|$|Standard time ={{normal time}} +allowance Where; normal time =avg time *rating factor. (take <b>rating</b> <b>factor</b> between 1.1 and 1.2) ...|$|E
5000|$|Loss {{development}} factors or LDFs {{are used}} in insurance pricing and reserving to adjust claims to their projected ultimate level. Insurance claims, especially in long-tailed lines such as liability insurance, are often not paid out immediately. Claims adjusters set initial case reserves for claims; however, it is often impossible to predict immediately what the final amount of an insurance claim will be, due to uncertainty around defense costs, settlement amounts, and trial outcomes (in addition to several other factors). Loss development factors are used by actuaries, underwriters, and other insurance professionals to [...] "develop" [...] claim amounts to their estimated final value. Ultimate loss amounts are necessary for determining an insurance company's carried reserves. They are also useful for determining adequate insurance premiums, when loss experience {{is used as a}} <b>rating</b> <b>factor</b> ...|$|E
3000|$|At {{the end of}} a listened sequence, VQmon {{extracts}} {{packet loss}} characterization metrics, e.g., interval durations and their corresponding Good/Bad status and features, from a 4 -state chain calibrated at run-time (see 'Appendix' section for further details). These control data are used to calculate the overall <b>rating</b> <b>factor</b> as follows, the built perceptual instantaneous rating function RP over a given Good and the next adjacent Bad segment is integrated over time. Then, the obtained value is divided by the interval duration. The resulting <b>rating</b> <b>factor</b> is referred to as average <b>rating</b> <b>factor,</b> R [...]...|$|E
40|$|The Ne- 18 (alpha, p) Na- 21 {{reaction}} is one key for the breakout {{from the hot}} CNO cycles to the rp-process. Recent papers have provided reaction <b>rate</b> <b>factors</b> N-A which are discrepant {{by at least one}} order of magnitude. The compatibility of the latest experimental results is tested, and a partial explanation for the discrepant N-A is given. A new <b>rate</b> <b>factor</b> is derived from the combined analysis of all available data. The new <b>rate</b> <b>factor</b> is located slightly below the higher <b>rate</b> <b>factor</b> found by Matic et al. [Phys. Rev. C 80, 055804 (2009) ] at low temperatures and significantly below at higher temperatures whereas it is about a factor of 5 higher than the lower <b>rate</b> <b>factor</b> recently published by Salter et al. [Phys. Rev. Lett. 108, 242701 (2012). ]. ...|$|R
40|$|The ^ 18 Ne(α,p) ^ 21 Na {{reaction}} is one key for the break-out {{from the hot}} CNO-cycles to the rp-process. Recent papers have provided reaction <b>rate</b> <b>factors</b> N_A which are discrepant {{by at least one}} order of magnitude. The compatibility of the latest experimental results is tested, and a partial explanation for the discrepant N_A is given. A new <b>rate</b> <b>factor</b> is derived from the combined analysis of all available data. The new <b>rate</b> <b>factor</b> is located slightly below the higher <b>rate</b> <b>factor</b> by Matic et al. at low temperatures and significantly below at higher temperatures whereas it is about a factor of five higher than the lower <b>rate</b> <b>factor</b> recently published by Salter et al. Comment: 10 pages, 2 figures, accepted for publication in Phys. Rev. ...|$|R
40|$|Molecular {{fluorescence}} emission <b>rate</b> <b>factors</b> for {{the strong}} bands of the nitric oxide gamma system have been calculated using recent branching ratios and different temperatures. For the 1 - 0 gamma band {{the effects of}} self-absorption by NO have been taken into account, and effective emission <b>rate</b> <b>factors</b> were calculated {{for a range of}} NO column densities at several different temperatures. For column densities near 10 to the 16 th/sq cm self-absorption corrections to the emission <b>rate</b> <b>factor</b> can be significant. The emission <b>rate</b> <b>factor</b> for the 1 - 0 gamma band is 7. 19 or 7. 68 x 10 to the - 6 th photons/molecule/sec depending upon the use of experimental or theoretical branching ratios...|$|R
30|$|Project <b>rating</b> <b>factor</b> (z) is {{a measure}} {{developed}} by Sobelman for predicting project success.|$|E
40|$|In a {{recently}} published Note, Block (Block, T. E. 1979. On {{the complexity of}} facilities layout problems. Management Sci. 25 (3) 280 [...] 285.) attempted to provide a definitive method to evaluate the complexity of facilities layout problems. He proposed a complexity <b>rating</b> <b>factor</b> {{that is based on}} flow dominance and the number of facilities. Mathematically, the <b>rating</b> <b>factor</b> suffers from the use of an incorrect upper bound for flow dominance. Also, {{there is a problem with}} regard to its interpretability. These shortcomings are discussed in this Note. The sensitivities of flow dominance and the <b>rating</b> <b>factor</b> to changes in a single element of the flow matrix are examined and their implications discussed. networks/graphs, facilities/equipment planning: layout...|$|E
3000|$|... where a and b are {{the fitting}} {{coefficients}} that minimize the RMSE. RT and RR stand for transformed and raw rating factors, respectively. As we can see, Q-Model(1) and Q-Model(2) slightly outperform other competing strategies. The transformed (improved) models can be utilized {{for a better}} estimation of measured <b>rating</b> <b>factor.</b>|$|E
5000|$|Once {{the factors}} that {{determine}} the two are identified and <b>rated,</b> each <b>factor</b> is then given a certain magnitude and a calculation is made as follows;factor 1 <b>rating</b> x <b>factor</b> 1 magnitude + <b>factor</b> 2 <b>rating</b> x <b>factor</b> 2 magnitude + ..... <b>factor</b> n <b>rating</b> x <b>factor</b> n magnitude.|$|R
40|$|Phosphorus {{transfer}} from agricultural soils to surface waters {{is an important}} environmental issue. Commonly used computer models like EPIC {{have not always been}} appropriately updated to reflect our improved understanding of soil P transformations and transfer to runoff. Our objectives were to determine if replacing EPIC’s constant sorption and desorption <b>rate</b> <b>factor</b> (0. 1) with more dynamic <b>rate</b> <b>factors</b> can more accurately predict changes in soil labile P on addition to and depletion of P from soils. From published data, methods were developed to easily determine dynamic sorption and desorption rate constants from soil properties. These methods were tested with data from new soil P incubation experiments where changes in soil labile P after P addition to and depletion from nine U. S. soils were measured. Replacing constant 0. 1 P sorption <b>rate</b> <b>factors</b> with dynamic factors improved prediction of soil labile P with time after P additions but more so for high-clay than low-clay soils. EPIC’s constant 0. 1 P desorption <b>rate</b> <b>factor</b> greatly underpredicted soil P desorption. Increasing the constant to 0. 6 improved predictions, whereas dynamic P desorption <b>rate</b> <b>factors</b> most accurately predicted P desorption. Soil P simulations showed that replacing constant P sorption and desorption <b>rate</b> <b>factors</b> with dynamic ones may change dissolved P loads (kg ha 21) in runoff for common soil, cropping, and runoff scenarios by only 1 to 8 % in the long term but by 8 to 30 % in the short term. These improvements are recommended given the simplicity of making EPIC’s sorption and desorption <b>rate</b> <b>factors</b> dynamic...|$|R
3000|$|Objective seeking Workers {{continue}} to seek acceptable groups at a prescribed decay rate until reaching the global decay limit of zero at which time they exit the model organization. Seeking agents have a global decay rate of 0.075 per time “tick” and a prescribed age-group decay <b>rate</b> <b>factor.</b> Agents that find an acceptable group prior to reaching the global decay limit of zero have a global satisfaction growth rate of 1.00 per time “tick” and a prescribed age-group growth <b>rate</b> <b>factor.</b> Both decay and growth <b>rate</b> <b>factors</b> for all age-groups are listed in Table 1. It {{is important to note}} that decay and growth <b>rates</b> and <b>factors</b> are assumptions based on model calibration.|$|R
30|$|The {{previously}} defined metrics for {{the characterization}} of packet loss burstiness explicitly (resp. implicitly) consider the nominal average length of sustained loss instances (resp. inter-loss durations). This could raise a biased quality <b>rating</b> <b>factor</b> because the subtle details of packet loss patterns are definitely ignored. The next presented speech quality assessors will consider this concern in a more careful fashion.|$|E
40|$|This report {{describes}} the effort made {{to develop a}} scoring system, or metric, for comparing astronaut Extra Vehicular Activity with various robotic options for the on-orbit assembly {{of a very large}} spacecraft, such as would be needed for a Manned Mars Mission. All trade studies comparing competing approaches to a specific task involve the use of some consistent and unbiased method for assigning a score, or <b>rating</b> <b>factor,</b> to each concept under consideration. The relative scores generated by the selected rating system provide the tool for deciding which of the approaches is the most desirable...|$|E
40|$|Mechanical Structure of failure; Traffic Load; Reliability of the Bridge Structure; Super Heavy Trailer; Moving Load; Rating Factors. A Lot of Bridge Structure on Indonesia lately got {{mechanical}} {{structure of}} failure like collapse and vanish. For example Bridge of Kutai Kartanegara, Bridge of Mayoa (Central Sulawesi), Bridge of Bamba Batulappa Pinrang, Bridge of Toddopuli (Makassar) {{and many others}} due to natural disaster or because the operational load is too high, more than maximum load design for the bridge. In effort to reduce the bridge failure at this country, needed to make an assessment study of bridges for generally to know {{the reliability of the}} existing structure of the bridges. Based on the assessment the additions of strengthening of the existing bridge can be define if allowed. Traffic load which increase as the time being result in increased of the bridge operational load, in the other condition there are super heavy trailers that across the bridge feared will broke the bridge structure. Bridge Evaluation method can be done by <b>rating</b> <b>factor</b> analysis. <b>Rating</b> <b>factor</b> analysis was conducted to compare the strength capacity of the bridge due to the actual operational load service with the strength capacity of the super heavy trailers based on moving load method. From the rating factors analysis reliability of the bridge structure can be evaluated, that is the allowable load under the bridge...|$|E
2500|$|For simple {{software}}, {{when the}} mouse starts to move, the software will {{count the number}} of [...] "counts" [...] or [...] "mickeys" [...] received from the mouse and will move the cursor across the screen by that number of pixels (or multiplied by a <b>rate</b> <b>factor,</b> typically less than 1). The cursor will move slowly on the screen, with good precision. When the movement of the mouse passes the value set for some threshold, the software will start to move the cursor faster, with a greater <b>rate</b> <b>factor.</b> Usually, the user can set the value of the second <b>rate</b> <b>factor</b> by changing the [...] "acceleration" [...] setting.|$|R
5000|$|We {{will call}} the shear stress form factor [...] and the shear <b>rate</b> <b>factor</b> [...]|$|R
30|$|In {{the first}} step, the trip demand Ti is {{generated}} for each origin {{based on the}} population and specific trip <b>rate</b> <b>factors.</b> These trip <b>rate</b> <b>factors</b> are distinguished by year and by country under consideration of four trip purposes and eight age groups. They are computed by a regression approach that takes economic and demographic changes in European countries into account with the regression approach being calibrated {{for the first year}} 2010 based on ETISplus data (Szimba et al. 2013). For the forecast years and for the policy scenarios trip <b>rate</b> <b>factors</b> are dependent on changes of the explanatory variables GDP, employment, and income level. PAD is therefore sensitive to economic and demographic changes.|$|R
40|$|The {{increasing}} use of non-linear loads {{in electrical}} installations has exacerbated the problems of harmonic distortion in industrial and commercial electrical systems. In the UK the current practice to determine the cable size for an electric circuit is to use BS 7671. However, previously the 16 th edition IEE Wiring Regulations only dealt with situations where cables attain the conductor temperature generated by sinusoidal currents at the fundamental power frequency. This paper outlines the methods available to determine the minimum size of line conductors for protection against overload currents, {{taking into account the}} harmonic content of the load current, and explains the harmonic <b>rating</b> <b>factor</b> Cf introduced in 2008 for cables that are under significant harmonic influences. Since the effect of harmonic currents is to increase the joule losses in a cable, the ampacity of the cable will need to be corrected to ensure the maximum conductor operating temperature is not exceeded. An experiment on how cable temperature can be measured under harmonic influence is described, and several sets of measurements taken on a typical cable are analysed. The paper concludes that direct usage of the BS 7671 <b>rating</b> <b>factor</b> for harmonics appears to be rather conservative and could lead to over-sizing of the line conductors for three-phase circuits, but is deemed beneficial in the long run...|$|E
40|$|A bridge {{management}} system is developed using the Tcl scripting language {{in conjunction with}} the OpenSees finite element software framework. Fully programmable and string-based, Tcl is ideal for implementing live load analysis through scripts and experimenting with emergent bridge rating methodologies. Since Tcl is an interpreted language, the application also has the important advantage that new bridge capacity models and <b>rating</b> <b>factor</b> calculations can be implemented on multiple platforms without compiling source code. The network programming features of Tcl give the system access to databases for conducting internet-based bridge rating. The system is demonstrated for rating a conventionally reinforced concrete girder; however, it is readily extensible to other types of bridge components...|$|E
40|$|Objectives. To {{develop a}} {{clinical}} prediction model enabling {{the calculation of}} an individual patient’s life expectancy (LE) and survival probability based on age, sex, and comorbidity {{for use in the}} joint decision-making process regarding medi-cal treatment. Methods. A computer software program was developed with a team of 3 clinicians, 2 professional actuar-ies, and 2 professional computer programmers. This incorpo-rated statistical spreadsheet and database access design methods. Data sources included life insurance industry actu-arial <b>rating</b> <b>factor</b> tables (public and private domain), Gov-ernment Actuary Department UK life tables, professional actuarial sources, and evidence-based medical literature. The main outcome measures were numerical and graphical display of comorbidity-adjusted LE; 5 -, 10 -, and 15 -year survival probability; in addition to generic UK populatio...|$|E
30|$|EGTS is too {{expensive}} for low data rate emergency data. If EGTS <b>rate</b> <b>factor</b> is high and emergency alarm message rate is low then amount of wasted bandwidth will be increased, because {{for most of the}} time, devices do not use allocated EGTS slots. To reduce overhead of unused EGTSs, we need to adapt ETGS <b>rate</b> <b>factor</b> while keeping the regulation of guaranteed low latency for emergency data.|$|R
40|$|A {{calculation}} of the A 2 sigma [...] > X 2 pi (0, 0) band emission <b>rate</b> <b>factors</b> and line center absorption cross sections of OH applicable to its measurement using solar resonant fluorescence in the terrestrial atmosphere is presented in this paper. The most accurate available line parameters have been used. Special consideration {{has been given to}} the solar input flux because of its highly structured Fraunhofer spectrum. The calculation for the OH atmospheric emission <b>rate</b> <b>factor</b> in the solar resonant fluorescent case is described in detail with examples and intermediate results. Results of this {{calculation of}} OH emission <b>rate</b> <b>factors</b> for individual rotational lines are on average 30 % lower than the values obtained in an earlier work...|$|R
30|$|The birth <b>rate</b> <b>factor</b> of ticks is {{represented}} by μ_T, and it is presumed to be equal to the normal demise rate.|$|R
40|$|Decisions {{about how}} to use an individual's age are {{particularly}} important under the Oregon Health Plan's pending employer mandate. This article summarizes the relationships between age and other important variables that impact health policy decisions, including {{a review of the}} legality of using age as a <b>rating</b> <b>factor,</b> {{a review of the literature}} on specific relationships to age, and a discussion of the issue of age-banding as it relates to an employer mandate model for health reform. Also discussed is the implication of existing law and its lack of a requirement that employees purchase the health insurance their employer will be required to offer. The article concludes with a preliminary analysis of the cost implications of decisions about using age in health policy...|$|E
40|$|Virtualisation {{technologies}} have established their importance as core components of modern digital communications. With the increasing trend towards outsourcing and cloud services, virtualisation features such as; versioning, isolation, encapsulation and their exploitability from adversaries becomes a critical area for system integrity. From a digital forensics perspective, the sole aim of preserving integrity {{is to ensure}} admissibility. This paper focuses on the identification of threats {{to the integrity of}} digital evidence using the VMware hypervisor as an example case study. A novel Evidence Integrity Preservation Framework (EIPF) is introduced which can be scaled for virtualised environments using Clark-Wilson’s principles. The key parameters of our EIPF include the strength of the hashing functions, the relative number of evidence attributes used and the number of evidence “cycles”. A Reliability <b>Rating</b> <b>Factor</b> (R) is also derived as a means of conceptualising integrity levels and imposing restrictions based on known processes related to data integrity...|$|E
40|$|We {{examine the}} {{standard}} Gaussian copula model for correlated defaults (also called the survival copula) {{and its relationship}} with the theoretically richer model based on diffusion processes and default thresholds. We show that in a discrete time framework the Gaussian copula {{can be seen as a}} simple global approximation to the Brownian copula implied by correlated diffusions. More precisely, the Gaussian copula is larger, in the sense that it induces systematically higher dependency. The result helps clarify some of the peculiar aspects of the standard model. Turning the argument around, the framework allows us to design further applications, capitalizing on the model’s notable tractability. We show that a multiperiod, correlated migrations model based on discrete diffusions can for some purposes be replicated surprisingly well by very simple analytic expressions. As a practical example we show how one can readily compute the distributions of the ”weighted average <b>rating</b> <b>factor</b> ” of a pool of credits for any future time period. ...|$|E
30|$|The birth <b>rate</b> <b>factor</b> of bovine is {{represented}} by μ_B. The birth rate μ is presumed to be equal to the normal demise.|$|R
50|$|Model {{choice for}} the {{interest}} <b>rate</b> <b>factors</b> varies - for speed reasons, popular choices are Hull-White model, Black-Karasinski model, and extended Cheyette Model.|$|R
40|$|This paper {{deals with}} the impact of {{constant}} <b>rate</b> <b>factor</b> value on the objective video quality assessment using PSNR and SSIM metrics. Compression efficiency of H. 264 and H. 265 codecs defined by different Constant <b>rate</b> <b>factor</b> (CRF) values was tested. The assessment was done for eight types of video sequences depending on content for High Definition (HD), Full HD (FHD) and Ultra HD (UHD) resolution. Finally, performance of both mentioned codecs with emphasis on compression ratio and efficiency of coding was compared...|$|R
40|$|This {{appendix}} introduces {{contours of}} {{speech transmission quality}} (or contours of user satisfaction) {{that can be used}} to predict speech transmission quality from time-varying transmission impairments. Quality contours are derived from the ITU-T E-model [ITU-T G. 107] upon reducing it to the transport layer only (i. e., with assumed default values characterizing perfect terminals). The shape of quality contours is determined by the Delay Impairment Idd that covers loss of interactivity and the Effective Equipment Impairment Ie-eff that covers information loss due to encoding scheme and packet loss. The proposed quality contours determine the <b>rating</b> <b>factor</b> R for all possible combinations of packet loss (assuming a given encoding scheme) and mouth-to-ear delay (assuming echo-free connections). Quality contours can be used in cross-layer optimization of various communications layers (e. g., adaptive playout scheduling at the application layer, traffic differentiation at the MAC layer) when predicting end-to-end speech transmission quality from time-varying transmission impairments...|$|E
40|$|Audio data {{processing}} and transmission in Wireless Sensor Networks (WSNs) require systems which satisfy the equilibrium between heavy data traffic and limited resources. In the lossy {{nature of the}} network, transmission techniques matter to sustain a certain content validity to be preserved. Besides, data handling is a worth-stressing issue to sustain a decreased computation and traffic overhead. An admissible quality of the voice signals being gathered at the sink node is to be directly maintained with network transmission assessments. To this respect, basic characteristics of a voice signal must be compromised with network properties. In this study, we investigate the quality of voice signals sent through the homogeneously constructed multi-hop network in which recorded signal segments in the source node are touched with the counterparts expected to be collected in each hop with a transmission <b>rating</b> <b>factor.</b> The basic characteristics of the voice samples used in the testbed are essayed with different in-network qualifications that can directly affect the quality; so that a reasonable trade-off between voice quality and sensor network capabilities is tried to be proposed...|$|E
30|$|The ITU-T defines in Rec. G. 107 a {{computational}} model {{for use in}} planning of telephone networks, known as E-Model [8]. Briefly, the E-Model combines a set of characterization metrics of the transport system and provides as output a <b>rating</b> <b>factor,</b> R, that quantifies the users' satisfaction. The ultimate objective of E-Model consists of giving a synthesized overview regarding the perceived quality delivered over a given telecom infrastructure. It has been subsequently extended to consider packet-based telephone networks and to operate as a single-ended speech quality assessor [9]. The original release of the E-Model solely considers the negative perceived effect of independently removed voice packets. It has been recently evolved to account for bursty packet loss processes characterized using two newly defined parameters [8]. The first metric, denoted as BurstR, {{is defined as the}} ratio between the undergone average number of successive missing packets and the expected average number of successive missing packets under independent packet lossesc. The second metric, denoted as Bpl, is a constant defined to consider the robustness of a given couple of CODEC and Packet Loss Concealment (PLC) algorithm to deal with bursty packet loss processes. The value of Bpl is derived a priori for each CODEC and PLC algorithm using subjective tests and a comprehensive regression analysis [3].|$|E
40|$|The {{paper is}} focused on {{modelling}} claim frequency and extends the work of Kafková and Křivánková, 2014 (Kafková, S., Křivánková, L. 2014. Generalized linear models in vehicle insurance. Acta universitatis agriculturae et silviculturae mendelianae brunensis, 62 (2) : 383 – 388). We showed that overdispersion, non-linear systematic component and interacted <b>rating</b> <b>factors</b> should be considered when the claim frequency is modelled. We detected overdispersion in the Poisson model and employed the negative-binomial model to show that considering heterogeneity over insurance policies yields better fit of the model. We also analysed the linear effect of continuous <b>rating</b> <b>factors</b> and their mutual influences. We showed that non-linearity and interactions between <b>rating</b> <b>factors</b> yield the better fit of the model, {{as well as new}} findings related to the analysis of claim frequency. All empirical models were estimated on the insurance portfolio of Czech insurance company collected during the years 2004 – 2008...|$|R
40|$|A {{comparison}} of three commonly used antifoggants was made. Their {{effect on the}} formation of fog and image density development was studied using a <b>rate</b> <b>factor</b> of development. This <b>rate</b> <b>factor</b> {{is defined as the}} slope of the curve for density-log development time. Bromide and benzotriazole were found to have no effect on the rate of development whereas 1 -phenyl- 5 -mercaptotetrazole was found to significantly reduce the rate of development. Bromide and benzotriazole were found to cause a suppression of image density only by increasing the time before density formation begins...|$|R
40|$|One of the {{outstanding}} problems of the modelling of temperate ice dynamics is the limited knowledge on the rheology of temperate ice and, in particular, on how the <b>rate</b> <b>factor</b> depends on the liquid water content. Though {{it is well known}} that the <b>rate</b> <b>factor</b> depends strongly on the water content, in practice the only available experimentally-based relationship is that by Duval (1977), which is only valid for water contents up to 1 %. However, actual water contents found in temperate and polythermal glaciers are sometimes substantially larger...|$|R
