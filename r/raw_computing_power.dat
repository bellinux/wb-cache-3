43|8460|Public
2500|$|Since Kurzweil {{believes}} computational capacity {{will continue}} to grow exponentially long after Moore's Law ends it will eventually rival the <b>raw</b> <b>computing</b> <b>power</b> of the human brain. Kurzweil looks at several different estimates of how much computational capacity is in the brain and settles on 1016 calculations per second and 1013 bits of memory. He writes that $1,000 will buy computer power equal to a single brain [...] "by around 2020" [...] while by 2045, the onset of the Singularity, he says same amount of money will buy one billion times more power than all human brains combined today. Kurzweil admits the exponential trend in increased computing power will hit a limit eventually, but he calculates that limit to be trillions of times beyond what is necessary for the Singularity.|$|E
2500|$|The STI Design Center {{opened in}} March 2001. The Cell was {{designed}} {{over a period}} of four years, using enhanced versions of the design tools for the POWER4 processor. Over 400 engineers from the three companies worked together in Austin, with critical support from eleven of IBM's design centers. During this period, IBM filed many patents pertaining to the Cell architecture, manufacturing process, and software environment. An early patent version of the Broadband Engine was shown to be a chip package comprising four [...] "Processing Elements", which was the patent's description for what is now known as the Power Processing Element (PPE). Each Processing Element contained 8 APUs, which are now referred to as SPEs on the current Broadband Engine chip. This chip package was widely regarded to run at a clock speed of 4GHz and with 32 APUs providing 32gigaFLOPS each(FP8 quarter precision), the Broadband Engine was shown to have 1 teraFLOPS of <b>raw</b> <b>computing</b> <b>power.</b> This design was fabricated using a 90nm SOI process.|$|E
5000|$|Supercomputers {{will have}} the same <b>raw</b> <b>computing</b> <b>power</b> as human brains, though the {{software}} to emulate human thinking on those computers does not yet exist. (IBM Sequoia) ...|$|E
40|$|A {{number of}} {{commercial}} {{companies are now}} attempting to develop production quality parallel processing systems. These systems are often targeted {{to take over the}} role of Vector/Parallel supercomputers. While these systems have excellent <b>raw</b> <b>compute</b> <b>power</b> for their cost, they lag behind traditional supercomputers in a number of important areas. One important area is the ability to deal with wide variations in system load while maintaining high utilization. This paper discusses the benefits of "Automatic Self-Adjusting Parallel Processing" as used in traditional supercomputers and how this technique can be applied to parallel processing systems. The paper also proposes {{a solution to the problem}} which allows these new processors to adapt to changing load conditions through software. This software based "Automatic SelfAdjusting Threads" or ASAT is discussed in the paper and its performance is compared to the hardware based solutions. Keywords: Parallel processing, dynamic load balancing, co [...] ...|$|R
40|$|We {{present a}} case study of {{implementation}} of a combinatorial search problem in both reconfigurable hardware and software. The particular problem is the search for approximate solutions of overconstrained systems of equations over GF(2). The problem is of practical interest in cryptanalysis. We consider the efficient implementation of exhaustive search techniques to find the best solutions of sets of up to 1000 equations over 30 variables. Best is defined to be those variable assignments that leave the minimum number of equations unsatisfied. As we apply various techniques to speed up this computation, we find that the techniques, whether inspired by software or reconfigurable hardware, are applicable to both implementation domains. While reconfigurable hardware offers greater <b>raw</b> <b>compute</b> <b>power</b> than software, new microprocessor with wide datapaths and far higher clock speeds do not lag far behind. Software also benefits from faster compilation times which prove important for some opti [...] ...|$|R
40|$|Earth science {{applications}} {{of the future will}} stress the capabilities of even the highest performance supercomputers in the areas of <b>raw</b> <b>compute</b> <b>power,</b> mass storage management, and software environments. These NASA mission critical problems demand usable multi-petaflops and exabyte-scale systems to fully realize their science goals. With an exciting vision of the technologies needed, NASA has established a comprehensive program of advanced research in computer architecture, software tools, and device technology to ensure that, in partnership with US industry, it can meet these demanding requirements with reliable, cost effective, and usable ultra-scale systems. NASA will exploit, explore, and influence emerging high end computing architectures and technologies to accelerate the next generation of engineering, operations, and discovery processes for NASA Enterprises. This article captures this vision and describes the concepts, accomplishments, and the potential payoff of the key thrusts that will help meet the computational challenges in Earth science applications...|$|R
50|$|By {{the late}} eighties, {{the company was}} having {{problems}} retaining customers who were moving to lower-cost systems. In addition, Prime was failing {{to keep up with}} the increasing customers' need for <b>raw</b> <b>computing</b> <b>power.</b> By the end, not a single Prime computer was subject to COCOM export controls, as they were insufficiently powerful for the US Government to fear their falling into the hands of hostile powers.|$|E
5000|$|Since Kurzweil {{believes}} computational capacity {{will continue}} to grow exponentially long after Moore's Law ends it will eventually rival the <b>raw</b> <b>computing</b> <b>power</b> of the human brain. Kurzweil looks at several different estimates of how much computational capacity is in the brain and settles on 1016 calculations per second and 1013 bits of memory. He writes that $1,000 will buy computer power equal to a single brain [...] "by around 2020" [...] while by 2045, the onset of the Singularity, he says same amount of money will buy one billion times more power than all human brains combined today. Kurzweil admits the exponential trend in increased computing power will hit a limit eventually, but he calculates that limit to be trillions of times beyond what is necessary for the Singularity.|$|E
5000|$|The STI Design Center {{opened in}} March 2001. The Cell was {{designed}} {{over a period}} of four years, using enhanced versions of the design tools for the POWER4 processor. Over 400 engineers from the three companies worked together in Austin, with critical support from eleven of IBM's design centers. During this period, IBM filed many patents pertaining to the Cell architecture, manufacturing process, and software environment. An early patent version of the Broadband Engine was shown to be a chip package comprising four [...] "Processing Elements", which was the patent's description for what is now known as the Power Processing Element (PPE). Each Processing Element contained 8 APUs, which are now referred to as SPEs on the current Broadband Engine chip. This chip package was widely regarded to run at a clock speed of 4 GHz and with 32 APUs providing 32 gigaFLOPS each(FP8 quarter precision), the Broadband Engine was shown to have 1 teraFLOPS of <b>raw</b> <b>computing</b> <b>power.</b> This design was fabricated using a 90 nm SOI process.|$|E
40|$|Over {{the last}} three decades, higher CPU {{performance}} has been achieved almost exclusively by raising the CPU’s clock rate. Today, the resulting power consumption and heat dissipation threaten to end this trend, and CPU designers are looking for alternative ways of providing more <b>compute</b> <b>power.</b> In particular, they are looking towards three concepts: a streaming compute model, vector-like SIMD units, and multi-core architectures. One particular example of such an architecture is the Cell Broadband Engine Architecture (CBEA), a multi-core processor that offers a <b>raw</b> <b>compute</b> <b>power</b> of up to 200 GFlops per 3. 2 GHz chip. The Cell bears a huge potential for compute-intensive applications like ray tracing, but also requires addressing the challenges caused by this processor’s unconventional architecture. In this paper, we describe an implementation of realtime ray tracing on a Cell. Using a combination of lowlevel optimized kernel routines, a streaming software architecture, explicit caching, and a virtual software-hyperthreading approach to hide DMA latencies, we achieve for a single Cell a pure ray tracing performance of nearly one order of magnitude over that achieved by a commodity CPU...|$|R
40|$|With the <b>raw</b> <b>compute</b> <b>power</b> of GPUs {{being more}} widely {{available}} in commodity multicore systems, there is an imminent need to harness their power for important numerical libraries such as LAPACK. In this paper, we consider the solution of dense symmetric and Hermitian eigenproblems by LAPACK’s Divide & Conquer algorithm on such modern heterogeneous systems. We focus {{on how to make}} the best use of the individual strengths of the massively parallel manycore GPUs and multicore CPUs. The resulting algorithm overcomes performance bottlenecks that current implementations, optimized for a homogeneous multicore face. On a dual socket quadcore Intel Xeon 2. 33 GHz with an NVIDIA GTX 280 GPU, we typically obtain up to about 10 -fold improvement in performance for the complete dense problem. The techniques described here thus represent an example on how to develop numerical software to efficiently use heterogeneous architectures. As heterogeneity becomes common in the architecture design, the significance and need of this work is expected to grow...|$|R
40|$|Significant {{advances}} {{have been achieved}} for realtime ray tracing recently, but realtime performance for complex scenes still requires large computational resources not yet available from the CPUs in standard PCs. Incidentally, most of these PCs also contain modern GPUs that do offer much larger <b>raw</b> <b>compute</b> <b>power.</b> However, limitations in the programming and memory model have so far kept the performance of GPU ray tracers well below that of their CPU counterparts. In this paper we present a novel packet ray traversal implementation that completely {{eliminates the need for}} maintaining a stack during kd-tree traversal and that reduces the number of traversal steps per ray. While CPUs benefit moderately from the stackless approach, it improves GPU performance significantly. We achieve a peak performance of over 16 million rays per second for reasonably complex scenes, including complex shading and secondary rays. Several examples show that with this new technique GPUs can actually outperform equivalent CPU based ray tracers...|$|R
40|$|Concomitantly {{with recent}} {{advances}} in speech coding, recognition and production, parallel computer systems are now commonplace delivenng <b>raw</b> <b>computing</b> <b>power</b> measured in hundreds of MIPS and Megaflops. It seems inevitable that within the next decade or so, gigaflop parallel processors will be achievable at modest cost. Indeed, gigaflops per cubic foot is now becoming a standard of measure for parallel computers...|$|E
40|$|Program {{libraries}} are {{one way to}} {{make the}} cooperation between specialists from various fields successful: the separation of application-specific knowledge from application-independent tasks ensures portability, maintenance, extensibility, and flexibility. This paper demonstrates the success in combining problem-specific knowledge for the quadratic assignment problem (QAP) with the <b>raw</b> <b>computing</b> <b>power</b> offered by contemporary parallel hardware by using the library of parallel search algorithms ZRAM. The solutions of 10 previously unsolved large standard test-instances of the QAP are presented...|$|E
40|$|Combinatorial {{optimization}} problems pervade {{many areas}} of human problem solving especially {{in the fields of}} business, economics and engineering. Intensive mathematical research and vast increases in <b>raw</b> <b>computing</b> <b>power</b> have advanced {{the state of the art}} in exact and heuristic problem solving at a pace that is unprecedented in human history. We survey here in layman’s terms some of the fundamental concepts and principles that have led this progress. (This article will appear – possibly in modified form – in a popular science magazine. ...|$|E
40|$|As <b>raw</b> <b>compute</b> <b>power</b> of {{a single}} chip {{continues}} to scale into the multi-teraflop regime, the processor I/O communication fabric must scale proportionally {{in order to prevent}} a performance bottleneck. As electrical wires suffer from high channel losses, pin-count constraints, and crosstalk, they are projected to fall short of the demands required by future memory systems. Silicon-photonic optical links overcome the fundamental tradeoffs of electrical wires; dense wavelength division multiplexing (DWDM) - where multiple data channels share a single waveguide or fiber to greatly extend bandwidth density - and the potential to combine at chip-scale with a very large scale integrated (VLSI) CMOS electrical chip make them a promising alternative for next-generation processor I/O. The key device for VLSI photonics is the optical microring resonator, a compact micrometer-scale device enabling energy-efficient modulation, DWDM channel selection, and sometimes even photo-detection. While these advantages have generated considerable interest in silicon-photonics, present-day integration efforts have been limited in scale owing to the difficulty of integration with advanced electronics and the sensitivity of microring resonators to both process and thermal variations. This thesis develops and demonstrates the pieces of a photonically-interconnected processor-to-memory system. We demonstrate a complete optical transceiver platform in a commercial 45 nm SOI process, showing that optical devices can be integrated into an advanced, commercial CMOS SOI process even without any changes to the manufacturing steps of the native process. To show that photonic interconnects are viable even for commoditized and cost-sensitive memory, we develop the first monolithic electronic-photonic links in bulk CMOS. As the stabilization of ring resonators is critical for use in VLSI systems, we contribute to the understanding of process and thermal variations on microring resonators, leading to the demonstration of a complete auto-locking microring tuning system that is agnostic to the transmitted data sequence and suitable for unencoded low-latency processor-to-memory traffic. Finally, the technology and methods developed in this work culminate in the demonstration of the world's first processor chip with integrated photonic interconnects, which uses monolithically integrated photonic devices to optically communicate to main memory. by Chen Sun. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student-submitted PDF version of thesis. Includes bibliographical references (pages 173 - 183) ...|$|R
40|$|Since {{computers}} {{first arrived}} on the scene people have been fascinated with the idea that some day these machines might be able to think and converse with humans in an intelligent way. This reality has yet to be achieved. However, considering the fact that Gordon Moore’s Law seems situated to remain constant in the foreseeable future; this reality may not be far away. With new developments in processor technology the possibility exists that computers will be able to process information at speeds required to make this enigma entirely possible (Aston, 2005). Current predictions state that by 2016 the circuit lines on microprocessors will reach a miniscule 22 nanometres in size, less than a quarter what they are today (Aston, 2005). This fact coupled with the prediction of soon to arrive eight core processors, compared to today’s dual core designs, causes one to think whether thinking computers might be a possibility. <b>Raw</b> <b>computing</b> speed and <b>power</b> are only two of the limiting components in this equation. Human intuition and desire are also just as important as we struggle to design programs that will achieve this exigent goal. In his very challenging paper titled “Why People Think Computers Can’t”, Minsky (2005), delves into many of the existing road blocks that prevent computers from becoming thinking machines. He poses many thought provoking questions around wh...|$|R
5000|$|The <b>raw</b> <b>computing</b> {{speed for}} basic {{arithmetic}} operations on the MPP was as follows: ...|$|R
40|$|Motivation: Mathematical {{modelling}} {{is central}} to systems and synthetic biology. Using simulations to calculate statistics or to explore parameter space is a common means for analysing these models and can be computationally intensive. However, in many cases, the simulations are easily parallelizable. Graphics processing units (GPUs) are capable of efficiently running highly parallel programs and outperform CPUs in terms of <b>raw</b> <b>computing</b> <b>power.</b> Despite their computational advantages, their adoption by the systems biology community is relatively slow, since differences in hardware architecture between GPUs and CPUs complicate the porting of existing code...|$|E
40|$|Today’s {{graphics}} processing units (GPU) {{have tremendous}} resources {{when it comes}} to <b>raw</b> <b>computing</b> <b>power.</b> The simulation of large groups of agents in transport simulation has a huge demand of computation time. Therefore it seems reasonable to try to harvest this computing power for traffic simulation. Unfortunately simulating a network of traffic is inherently connected with random memory access. This is not a domain that the SIMD (single instruction, multiple data) architecture of GPUs is known to work well with. In this paper the authors will try to achieve a speedup by computing multi-agent traffic simulations on the graphics device using NVIDIA’s CUDA framework...|$|E
40|$|Since {{the dawn}} of digital technology, almost every measure of <b>raw</b> <b>computing</b> <b>power</b> has {{increased}} exponentially with time, and costs have likewise plummeted. Memory capacity, communication bandwidth, and processing power – all have steadily mushroomed over the decades, a phenomenon described by the so-called Moore’s Law. This growth is now enabling amateurs at home to manipulate digital media with the computational power formerly available only to professionals. But improvements in raw power alone do not make hands-on control of digital media appealing to consumers, since the tools have not become correspondingly easier to use. In fact, merely simplifying professional tools for the home may not b...|$|E
40|$|The {{exponential}} {{increase of}} <b>computing</b> <b>power,</b> predicted by Moore’s law [1], influences modern inspection capabilities. The availability of <b>computing</b> <b>power</b> allows {{the design and}} implementation of affordable sensing devices employing numerical analysis to generate measurement results. In particular, modern laser sensors [2] benefit from <b>computing</b> <b>power</b> by providing rapid and accurat...|$|R
5000|$|Volunteer {{computing}} can provider {{researchers with}} <b>computing</b> <b>power</b> {{that is not}} achievable any other way. Approximately 10 petaflops of <b>computing</b> <b>power</b> are available from volunteer computing networks.|$|R
50|$|The {{additional}} abstraction layer between {{storage system}} and <b>computing</b> <b>power</b> eases the scale {{out of the}} infrastructure. Most notably the storage capacity, the <b>computing</b> <b>power</b> and the network bandwidth can be scaled independent from one another.|$|R
40|$|INTRODUCTION Occasionally, {{computer}} technology makes {{a break with}} the past. The relational model of database management, with its simple, tabular data structures and powerful data manipulation operations, was one such revolution. Another revolution in computing technology, client/server computing, {{took place in the}} last decade with the spread of minicomputers and microcomputers and a network to support intra machine communication. These highly cost-effective and flexible open systems have made client/server computing possible. Client/server computing delivers the benefits of the network computing model along with the shared data access and high performance characteristics of the host-based computing model. Clients and servers are characterized by endless access to each other's resources. They provide advanced communications and <b>raw</b> <b>computing</b> <b>power</b> to handle demanding applications, as well as graphic user interfaces (GUIs). In a client/server database system there are three...|$|E
40|$|Recently, the CUDA {{technology}} {{has been used to}} accelerate many computation demanding tasks. For example, in [7] we have shown how CUDA technology can be employed to accelerate the process of Linear Temporal Logic (LTL) Model Checking. While the <b>raw</b> <b>computing</b> <b>power</b> of a CUDA enabled device is tremendous, the applicability of the technology is quite often limited to small or middle-sized instances of the problems being solved. This is because the memory that a single device is equipped with, is simply not large enough to cope with large or realistic instances of the problem, which is also the case of our CUDAaware LTL Model Checking solution. In this paper we suggest how to overcome this limitations by employing multiple (two in our case) CUDA devices for acceleration of our fine-grained communication-intensive parallel algorithm for LTL Model Checking. 1...|$|E
40|$|Program {{libraries}} are {{one way to}} {{make the}} cooperation between specialists from various fields successful: the separation of application-specific knowledge from applicationindependent tasks ensures portability, maintenance, extensibility, and flexibility. The current paper demonstrates the success in combining problem-specific knowledge for the quadratic assignment problem (QAP) with the <b>raw</b> <b>computing</b> <b>power</b> offered by contemporary parallel hardware by using the library of parallel search algorithms ZRAM. Solutions of previously unsolved large standard test-instances of the QAP are presented. 1 Introduction Since {{the early days of the}} computer age computers have been used as number crunchers to solve problems [...] -whether they are practical problems or research problems. The people interested in solving a specific problem supplied the data and solution method and did the necessary programming to implement the solution method themselves. Today's world of problem solving looks differ [...] ...|$|E
5|$|The two {{projects}} {{also differ}} significantly in their <b>computing</b> <b>power</b> and host diversity. Averaging about 6,650 teraFLOPS from a host base of {{central processing units}} (CPUs), graphics processing units (GPUs), and PS3s, Folding@home has nearly 108 times more <b>computing</b> <b>power</b> than Rosetta@home.|$|R
50|$|Since {{the arrival}} of the IBM RAD6000 in the 2000s and the RAD750 in the 2010s, using the NSSC-1 has become unthinkable. Its <b>computing</b> <b>power</b> was not great, and most modern space {{missions}} require flight computers to have substantial and substantive <b>computing</b> <b>power.</b>|$|R
5000|$|Since {{there are}} more than 1 billion PCs in the world, {{volunteer}} computing can supply more <b>computing</b> <b>power</b> to researches, that doesn't have the required competencies regarding the <b>computing</b> <b>power,</b> on any kind of topic; such as academic (university-based) or scientific researches. Also, advancements in the technology will provide the advancements in consumer products such as PCs and game consoles happen faster than any other specialized products which will increase the number of PCs and <b>computing</b> <b>power</b> in the world consequently.|$|R
40|$|Advances in FPGA {{technology}} {{have led to}} dramatic improvements in double precision floating-point performance. Modern FPGAs boast several GigaFLOPs of <b>raw</b> <b>computing</b> <b>power.</b> Unfortunately, this computing power is distributed across 30 floating-point units with over 10 cycles of latency each. The user must find two orders of magnitude more parallelism than is typically exploited in a single microprocessor; thus, {{it is not clear}} that the computational power of FPGAs can be exploited across a wide range of algorithms. This paper explores three implementation alternatives for the Fast Fourier Transform (FFT) on FPGAs. The algorithms are compared in terms of sustained performance and memory requirements for various FFT sizes and FPGA sizes. The results indicate that FPGAs are competitive with microprocessors in terms of performance and that the "correct" FFT implementation varies based on the size of the transform and the size of the FPGA...|$|E
40|$|For half {{a century}} since {{computers}} came into existence, the goal of finding elegant and efficient algorithms to solve "simple" (welldefined and well-structured) problems has dominated algorithm design. Over the same time period, both processing and storage capacity of computers have increased roughly {{by a factor of}} 10 6. The next few decades may well give us a similar rate of growth in <b>raw</b> <b>computing</b> <b>power,</b> due to various factors such as continuing miniaturization, parallel and distributed computing. If a quantitative change of orders of magnitude leads to qualitative changes, where will the latter take place? Many problems exhibit no detectable regular structure to be exploited, they appear "chaotic ", and do not yield to efficient algorithms. Exhaustive search of large state spaces appears to be the only viable approach. We survey techniques for exhaustive search, typical combinatorial problems that have been solved, and present one case study in detail...|$|E
40|$|Distributed and {{parallel}} computation is, {{on the one}} hand, the cheapest way to increase <b>raw</b> <b>computing</b> <b>power.</b> Turning parallelism into {{a useful tool for}} solving new problems, on the other hand, presents formidable challenges to computer science. We believe that parallel computation will spread among general users mostly through the ready availability of convenient and powerful program libraries. In contrast to general-purpose languages, a program library is specialized towards a well-defined class of problems and algorithms. This narrow focus permits developers to optimize algorithms, once and for all, for parallel computers of a variety of common architectures. This paper presents ZRAM, a portable parallel library of exhaustive search algorithms, as a case study that proves the feasibility of achieving simultaneously the goals of portability, efficiency, and convenience of use. Examples of massive computations successfully performed with the help of ZRAM illustrate its capabilities and use...|$|E
40|$|Abstract—Molecular {{dynamics}} (MD) simulation has broad applications, {{and increasing}} <b>computing</b> <b>power</b> {{is needed to}} satisfy the large spatiotemporal scales {{of the real world}} simulation. The advent of multi-core and many-core paradigm brings unprece-dented <b>computing</b> <b>power,</b> however, it remains a great challenge to harvest the <b>computing</b> <b>power</b> due to MD’s irregular memory-access pattern. To address the challenge, this paper presents a joint application/architecture study to enhance scalability of MD on multi-core and many-core architecture. First, a hierar-chical scheme is designed to explore the multi-level parallelism mapping application to hardware. Then to further harvest the many-core <b>computing</b> <b>power,</b> three incremental optimization strategies—-a novel data-layout to improve data locality, an on-chip locality-aware parallel algorithm to enhance data reuse, and a pipelining algorithm to hide latency to shared memory—- ar...|$|R
40|$|Annual {{increases}} in workstation capacity suggest that today between 5 and 15 times the <b>computing</b> <b>power</b> of 1994 {{should be available}} at roughly comparable costs. It there by becomes possible for a normal laboratory to provide at least approximately the <b>computing</b> <b>power</b> required for machine-vision-based control of robots. Experience with an experimental cell for automatic disassembly of used cars illustrates how the increasingly avalla le <b>computing</b> <b>power</b> facilitates methodological advances which result in more robust and versatile approaches - {{in addition to a}} reduction of latency in machine-vision-based control...|$|R
40|$|Abstract—In {{this paper}} we study the {{evolution}} of the spot price of <b>computing</b> <b>power</b> in a possible future world market for <b>computing</b> <b>power.</b> This global market is implemented as a peerto-peer computer network and has been modelled via simulations previously. We find that the log-returns of the spot price of <b>computing</b> <b>power</b> follow a similar distribution to the ones of electricity prices or the DAX index. We then develop a stochastic calculus model that captures the essence of {{the evolution of}} the spot price of <b>computing</b> <b>power</b> using a Markov regime-switching mechanism between three states. We estimate the values of the parameters of the model via maximum likelihood and we finally do an exact numerical stochastic simulation of the three-regime model and compare it with the original market behaviour. Keywords-Grid Computing, Stochastic Model I...|$|R
