5|97|Public
40|$|The article {{deals with}} the peculiarities of {{distributed}} cluster system creation with streaming data replication. Ways of <b>replication</b> <b>cluster</b> implementation in CORBA-systems with ZeroMq technology are presented. Major advantages of ZeroMQ technology over similar technologies are considered in this type distributed systems creation...|$|E
40|$|Server {{bottlenecks}} {{and failures}} are {{a fact of}} life in any database deployment, but they don't have to bring everything to a halt. MySQL has several features that can help you protect your system from outages, whether it's running on hardware, virtual machines, or in the cloud. MySQL High Availability explains how to use these <b>replication,</b> <b>cluster,</b> and monitoring features {{in a wide range of}} real-life situations. Written by engineers who designed many of the tools covered inside, this book reveals undocumented or hard-to-find aspects of MySQL reliability and high availability [...] knowledge tha...|$|E
40|$|In {{the thesis}} {{we discuss the}} problem of two-way {{communication}} between client and server over RTMP protocol using web technologies. We describe {{the process of building}} a fault-tolerant client side application with JavaScript, HTML, CSS and Flash technologies. The discussion is followed by the definition and description of RED 5 server side service, which is then upgraded into a distributed information system. The problem of scalability is solved with the use of shared memory, which is implemented as a database <b>replication</b> <b>cluster.</b> The whole system is observed by a control monitor, which detects and resolve problems and thereby provides a more reliable and fault-tolerant system. The thesis provides an insight into the implementation of a web based chat room system that can be integrated into any website...|$|E
5000|$|Replication - Master-Slave <b>replication</b> (<b>clustering)</b> with {{unlimited}} {{number of}} slave nodes.|$|R
50|$|The {{community}} {{has also written}} some tools to make managing <b>replication</b> <b>clusters</b> easier, such as repmgr.|$|R
30|$|Ground truth selection: {{criminal}} websites are manually {{divided into}} <b>replication</b> <b>clusters</b> {{and used as}} a source of ground truth.|$|R
40|$|Kaposi's sarcoma-associated {{herpesvirus}} (KSHV; human herpesvirus 8) {{is associated}} with three human tumors, Kaposi's sarcoma, primary effusion lymphoma (PEL), and multicentric Castleman's disease. KSHV encodes a number of homologs of cellular proteins involved in the cell cycle, signal transduction, and modulation of the host immune response. Of the virus complement of over 85 open reading frames (ORFs), the expression of only a minority has been characterized individually. We have constructed a nylon membrane-based DNA array which allows the expression of almost every ORF of KSHV to be measured simultaneously. A PEL-derived cell line, BC- 3, was used to study the expression of KSHV during latency and after the induction of lytic <b>replication.</b> <b>Cluster</b> analysis, which arranges genes according to their expression profile, revealed a correlation between expression and assigned gene function {{that is consistent with}} the known stages of the herpesvirus life cycle. Furthermore, latent and lytic genes thought to be functionally related cluster into groups. The correlation between gene expression and function also infers possible roles for KSHV genes yet to be characterized...|$|E
40|$|We {{took the}} Master thesis of I. Arrieta-Salinas and M. Louis Rodríguez as a {{starting}} point for this project. We are going to deploy a distributed database to be used in a cloud environment as a specific case of Platform-as-a-service. We assume that data is partitioned and several replicas store a copy of a given partition. The clients issue transactions by means of a standard library such as JDBC. To do so, they need information about the data placement that is managed by a Metadata Manager. The Metadata Manager manages the partitioning and the replica placement among all replicas building a replica cluster on each partition. The <b>replication</b> <b>cluster</b> has a few replicas running a replication protocol to provide strong consistency and the rest receive the propagation of updates in a lazy manner. These replicas are logically constituted as onion layers around the core replicas running a given replication protocol. The implementation of this system had several drawbacks that we try to fix in this work. First of all, clients an the MM need to be physically in the same machine which leads to a penalty performance in heavily loaded scenarios. The system was optimized for YCSB that consisted in transactions with a single operation and they are run over two replication protocols: primary copy and active replication that are known to perform badly update intensive scenarios. Moreover, there was no load balancing at all according to replica performance, it was merely a round-robin policy among all replicas at the core level. We try to argument the system limitations (described in more detail in Section 2. 1) and to going into the system implementation. This is going to be explained in the rest of this work. The main goals of this project are focused in the different parts of the system. In regard to the Client Module, originally the client was the OLPT-Benchmark, a module that consist in send specific types of transactions to the system by a JDBC connection. In the actual version this module has been modified allowing to the transaction to have more than one operation and several parameters has been introduced to the transaction which allow to the system to treat them differently. Respecting to the Metadata Manager one of the main goals between the others developed in this project is the decentralization of the Client and Meta- data Manager modules physically. The rest of modifications are the creation of a structure that allow to the Metadata Manager to know the architecture of the Replicas Cluster and the development of a new ReplicaChooser function based on the CPU charge allowing a correct load balancing. And finally in the Replicas Cluster has been implemented new protocols that have permitted to run different replication protocols in different partitions simultaneously without the knowledge of the Client and the Metadata Manager. Ingeniería en InformáticaInformatika Ingeniaritz...|$|E
30|$|The {{standard}} errors {{reported in the}} analysis are bootstrapped with 200 <b>replications</b> and are <b>clustered</b> by households. The results are robust to a higher number of <b>replications.</b> <b>Clustering</b> by households {{takes into account the}} survey design, in which several individuals from the same household are surveyed. Moreover, it partially addresses the panel dimension of the data because the same household may appear several times in the dataset.|$|R
50|$|Couchbase Server {{provided}} client protocol {{compatibility with}} memcached, but added disk persistence, data <b>replication,</b> live <b>cluster</b> reconfiguration, rebalancing and multitenancy with data partitioning.|$|R
50|$|Includes Adaptive Query Localization (pushes JOIN {{operations}} down to {{the data}} nodes), Memcached API, simplified Active/Active Geographic <b>replication,</b> multi-site <b>clustering,</b> data node scalability enhancements, consolidated user privileges.|$|R
40|$|Abstract-Data {{management}} in Peer-to-Peer (P 2 P) systems {{is a complicated}} and challenging issue due to {{the scale of the}} network and highly transient population of peers. In this paper, we identify important research problems in P 2 P data management, and describe briefly some methods that have appeared in the literature addressing those problems. We also discuss some open research issues and directions regarding data {{management in}} P 2 P systems. Keywords: Peer-to-peer systems, data management, overlay network, indexing, data integration, query processing, data <b>replication,</b> <b>clustering,</b> free riding, incentive mechanism. I...|$|R
50|$|CrateDB offers {{automatic}} data <b>replication</b> and self-healing <b>clusters</b> {{for high}} availability.|$|R
50|$|Bradford {{has worked}} as a Senior Consultant for MySQL AB (2006-2008) and Oracle Corporation (1996-1999) and Continuent (2010), a leading {{software}} provider of database <b>replication</b> and <b>clustering</b> for MySQL, Oracle Database and PostgreSQL.|$|R
40|$|AIMS/HYPOTHESIS: Little of {{the genetic}} basis for type 2 {{diabetes}} has been explained, despite numerous genetic linkage studies and the discovery of multiple genes in genome-wide association (GWA) studies. To begin to resolve the genetic component of this disease, we searched for sites at which genetic results had been corroborated in different studies, in the expectation that replication among studies should direct us to the genomic locations of causative genes with more confidence than the results of individual studies. METHODS: We have mapped the physical location of results from 83 linkage reports (for type 2 diabetes and diabetes precursor quantitative traits [QTs, e. g. plasma insulin levels]) and recent large GWA reports (for type 2 diabetes) onto the same human genome sequence to identify replicated results in diabetes genetic 2 ̆ 7 hot spots 2 ̆ 7. RESULTS: Genetic linkage has been found at least ten times at 18 different locations, and {{at least five times}} in 56 locations. All <b>replication</b> <b>clusters</b> contained study populations from more than one ethnic background and most contained results for both diabetes and QTs. There is no close relationship between the GWA results and linkage clusters, and the nine best <b>replication</b> <b>clusters</b> have no nearby GWA result. CONCLUSIONS/INTERPRETATION: Many of the genes for type 2 diabetes remain unidentified. This analysis identifies the broad location of yet to be identified genes on 6 q, 1 q, 18 p, 2 q, 20 q, 17 pq, 8 p, 19 q and 9 q. The discrepancy between the linkage and GWA studies may be explained by the presence of multiple, uncommon, mildly deleterious polymorphisms scattered throughout the regulatory and coding regions of genes for type 2 diabetes...|$|R
40|$|Peer-to-peer (p 2 p) {{systems are}} {{attracting}} increasing attention as an efficient means of sharing data among large, diverse and dynamic sets of users. The {{widespread use of}} XML as a standard for representing and exchanging data in the Internet suggests using XML for describing data shared in a p 2 p system. However, sharing XML data imposes new challenges in p 2 p systems related to supporting advanced querying beyond simple keyword-based retrieval. In this paper, we focus on data management issues for processing XML data in a p 2 p setting, namely indexing, <b>replication,</b> <b>clustering</b> and query routing and processing. For each of these topics, we present the issues that arise, survey related research and highlight open research problems. 1...|$|R
50|$|The Violin 7000 series {{includes}} application aware snapshots, {{continuous data}} protection, synchronous replication, asynchronous <b>replication</b> and metro <b>cluster</b> functionality.|$|R
5000|$|Multi-datacenter replication: In multi-datacenter <b>replication,</b> one <b>cluster</b> {{acts as a}} [...] "primary cluster." [...] The primary <b>cluster</b> handles <b>replication</b> {{requests}} from one or more [...] "secondary clusters" [...] (generally located in other regions or countries). If the datacenter with the primary cluster goes down, a second cluster can take over as the primary cluster.|$|R
40|$|Architectural {{resources}} and program recurrences are themain limitations {{to the amount}} of Instruction-Level Parallelism (ILP) exploitable from loops. To increase the number of operations per second, current designs use high degrees of resource replication for memory ports and functional units. But the high costs in terms of power and cycle time of this technique limit the degree of <b>replication.</b> <b>Clustering</b> is a technique aimed at decentralizing the design of future wide issue cores and enable them to meet the technology constraints in terms of cycle time, area and power. Another way to reduce the complexity of recent cores is using wide functional units. This technique only requires minor modifications to the underlying hardware, but also imposes a penalty on the exploitable parallelism. In this paper we evaluate a broad range of VLIW configurations that make use of these two techniques. From this study we conclude that applying both techniques yields configurations with very good power-performance efficiency. Peer ReviewedPostprint (published version...|$|R
40|$|Fusion between mitotic and S-phase cells induces the {{formation}} of prematurely condensed chromosomes (PCC) in the interphase partner. Viewed in the light microscope, S-phase PCC derived from the Indian muntjac appear to be fragmented and heterogeneous. In scanning electron micrographs prepared by an osmium impregnation technique, which avoids the need to sputter-coat the specimen, the S-phase fragments derived from an individual cell are resolved into about 1000 fibre aggregates, together with more dispersed fibres. Aggregates are roughly spherical and vary in diameter between about 0 - 25 and 1 - 6 /an. The spatial distribution of the aggregates shows some order: chains of single aggregates and, less commonly, duplicated chains occur. Regions of the PCC where the fibres are more dispersed {{are considered to be}} likely candidates for sites of replication at the time of fusion. The relationship between the condensed aggregate structure of the S-phase PCC and <b>replication</b> <b>clusters</b> is discussed, and also the assembly of aggregates to form metaphase chromosomes...|$|R
5000|$|Partition tolerance. A cluster can be {{partitioned}} {{without loss}} of consistency, although availability may be compromised. Restricted consistency <b>replication</b> across multiple <b>clusters</b> is also supported using volume mirrors, and near real-time replication of tables and streams.|$|R
50|$|MySQL Cluster writes Redo logs to disk for {{all data}} changes {{as well as}} check {{pointing}} data to disk regularly. This allows the cluster to consistently recover from disk after a full cluster outage. As the Redo logs are written asynchronously with respect to transaction commit, some small number of transactions can be lost if the full cluster fails, however this can be mitigated by using geographic <b>replication</b> or multi-site <b>cluster</b> discussed above. The current default asynchronous write delay is 2 seconds, and is configurable. Normal {{single point of failure}} scenarios do not result in any data loss due to the synchronous data <b>replication</b> within the <b>cluster.</b>|$|R
30|$|We {{identified}} a better <b>replication</b> of local <b>clustering</b> coefficients {{as one of}} the areas where ReCoN could still be improved. Further, for certain types of networks such as co-authorship networks, fitting schemes for increasing average degrees could be investigated.|$|R
50|$|Clustrix {{supports}} workloads {{that involve}} scaling transactions and real-time analytics. The {{system is a}} drop-in replacement for MySQL, and is designed to overcome MySQL scalability issues {{with a minimum of}} disruption. It also has built in fault-tolerance features for high availability within a cluster. It has parallel backup and parallel <b>replication</b> among <b>clusters</b> for disaster recovery.Clustrix is a scale-out SQL database management system and part of what are often called the NewSQL database systems (modern relational database management systems), closely following the NoSQL movement.|$|R
40|$|This paper {{describes}} a task scheduling algorithm, {{based on a}} LogP-type model, for allocating arbitrary task graphs to fully connected networks of processors. This problem {{is known to be}} NP-complete even under the delay model (a special case under the LogP model). The strategy exploits the <b>replication</b> and <b>clustering</b> of tasks to minimise the ill effects of communication overhead on the makespan. The quality of the schedules produced by this LogP-based algorithm, initially under delay model conditions, is compared with that of other good delay model-based approaches...|$|R
5000|$|It is also {{possible}} to replicate asynchronously between clusters; this is {{sometimes referred to as}} [...] "MySQL Cluster Replication" [...] or [...] "geographical replication". This is typically used to replicate clusters between data centers for disaster recovery or to reduce the effects of network latency by locating data physically closer to a set of users. Unlike standard MySQL <b>replication,</b> MySQL <b>Cluster's</b> geographic <b>replication</b> uses optimistic concurrency control and the concept of Epochs to provide a mechanism for conflict detection and resolution, enabling active/active clustering between data centers.|$|R
50|$|Several high-availability options {{have been}} {{consolidated}} into just one option for Exchange Server 2010 (Mailbox Resiliency), {{which is now}} offered in both the Standard and Enterprise editions. The capabilities of Local Continuous Replication, Standby Continuous <b>Replication,</b> and <b>Cluster</b> Continuous <b>Replication</b> are now unified into the Exchange 2010 Mailbox Resiliency capability. These capabilities enable a simplified approach to high availability and disaster recovery. The Standard Edition supports up to 5 databases with each database being limited to a maximum size of 16 TB. While the Enterprise Edition supports up to 100 databases with no size limit.|$|R
5000|$|Exchange Server 2007 {{provides}} {{built-in support}} for asynchronous replication modeled on SQL Server's [...] "Log shipping" [...] in CCR (<b>Cluster</b> Continuous <b>Replication)</b> <b>clusters,</b> which {{are built on}} MSCS MNS (Microsoft Cluster Service—Majority Node Set) clusters, which do not require shared storage. This type of cluster can be inexpensive and deployed in one, or [...] "stretched" [...] across two data centers for protection against site-wide failures such as natural disasters. The limitation of CCR clusters {{is the ability to}} have only two nodes and the third node known as [...] "voter node" [...] or file share witness that prevents [...] "split brain" [...] scenarios, generally hosted as a file share on a Hub Transport Server. The second type of cluster is the traditional clustering that was available in previous versions, and is now being referred to as SCC (Single Copy Cluster). In Exchange Server 2007 deployment of both CCR and SCC clusters has been simplified and improved; the entire cluster install process takes place during Exchange Server installation. LCR or Local Continuous Replication has been referred to as the [...] "poor man's cluster". It is designed to allow for data replication to an alternative drive attached to the same system and is intended to provide protection against local storage failures. It does not protect against the case where the server itself fails.|$|R
40|$|Abstract. High {{availability}} and disaster recovery (HADR) are often discussed in highly critical business systems for business function recov-ery and continuity concerns. With {{the development of}} cloud computing, virtual cloud services are perfectly matched to HADR scenarios, and interoperability is a significant aspect to help users to use HADR ser-vice across different cloud platforms and providers. In this paper, we present an architectural pattern describing the integration of high avail-ability and disaster recovery. We focus on database <b>cluster</b> <b>replication</b> between private cloud and public cloud environments. This HADR pat-tern for database <b>cluster</b> <b>replication</b> implements both synchronous and asynchronous replication concurrently for high {{availability and}} disaster recovery purposes. To {{evaluate the effectiveness of}} this pattern, we sim-ulate a MySQL-database-cluster HADR scenario under three strategies: hot standby, warm standby and cold standby, and analyze the perfor-mance, business continuity features and cost...|$|R
40|$|The current {{development}} of globally accessible astrophysical data systems is increasingly embracing Grid computing concepts, with data description and formatting standards such as VOTable and Uniform Content Descriptors providing {{a basis for}} system-system interoperability. To date, a diverse set of database management systems {{have been used for}} catalogue storage within these systems. We present a Virtual Observatory service for the HI Parkes All Sky Survey, implemented on an IBM Lotus Domino R 6 database management system. Domino's distributed computing architecture, with in-built support for <b>replication</b> and <b>clustering,</b> sets it apart from more general database systems as being inherently suitable for Grid computing applications...|$|R
40|$|The Database State Machine (DBSM) is a <b>replication</b> {{mechanism}} for <b>clusters</b> of database servers. Read-only and update transactions are executed locally, but during commit, update transactions execution outcome is broadcast {{to all the}} servers for certification. The main DBSM's weakness lies in its dependency on transaction readsets, needed for certification. This paper presents [...] ...|$|R
40|$|A {{quantitative}} {{model of}} interphase chromosome higher-order structure is presented {{based on the}} isochore model of the genome and results obtained {{in the field of}} copolymer research. G 1 chromosomes are approximated in the model as multiblock copolymers of the 30 -nm chromatin fiber, which alternately contain two types of 0. 5 - to 1 -Mbp blocks (R and G minibands) differing in GC content and DNA-bound proteins. A G 1 chromosome forms a single-chain string of loop clusters (micelles), with each loop ∼ 1 – 2 Mbp in size. The number of ∼ 20 loops per micelle was estimated from the dependence of geometrical versus genomic distances between two points on a G 1 chromosome. The greater degree of chromatin extension in R versus G minibands and a difference in the replication time for these minibands (early S phase for R versus late S phase for G) are explained in this model {{as a result of the}} location of R minibands at micelle cores and G minibands at loop apices. The estimated number of micelles per nucleus is close to the observed number of <b>replication</b> <b>clusters</b> at the onset of S phase. A relationship between chromosomal and nuclear sizes for several types of higher eukaryotic cells (insects, plants, and mammals) is well described through the micelle structure of interphase chromosomes. For yeast cells, this relationship is described by a linear coil configuration of chromosomes...|$|R
40|$|Storage {{replication}} {{is one of}} {{the back}} bones for network environments. While many forms of Network Attached Storage (NAS), Storage Area Networks (SAN) and other forms of network storage exist, {{there is a need for}} a reliable storage replication technique between distant sites (> 1 Km). Such technology allows setting new standards and removes demerits of network failover and failback systems for virtual servers; specifically, the growing storage need for effective disaster recovery (DR) planning. The purpose of this manuscript is to identify growing technologies such as IP-SAN that allow with remote storage <b>cluster</b> <b>replication</b> for virtual servers. This study (<b>Cluster</b> <b>Replication)</b> provides an analysis of improving the uptime and availability of SAN. For higher levels of availability, mirrored images maintained in Active/Active Cluster mirroring can provide a system with No Single Points of Failure, which is designed to improve the overall uptime of the storage system for organizations with 7 x 24 x 365 requirements...|$|R
40|$|DNA {{replication}} {{in higher}} eukaryotes initiates at thousands of origins {{according to a}} spatio-temporal program. The ATR/Chk 1 dependent replication checkpoint inhibits the activation of later firing origins. In the Xenopus in vitro system initiations are not sequence dependent and 2 - 5 origins are grouped in clusters that fire at different times despite a very short S phase. We {{have shown that the}} temporal program is stochastic at the level of single origins and <b>replication</b> <b>clusters.</b> It is unclear how the replication checkpoint inhibits late origins but permits origin activation in early clusters. Here, we analyze the role of Chk 1 in the replication program in sperm nuclei replicating in Xenopus egg extracts by a combination of experimental and modelling approaches. After Chk 1 inhibition or immunodepletion, we observed an increase of the replication extent and fork density in {{the presence or absence of}} external stress. However, overexpression of Chk 1 in the absence of external replication stress inhibited DNA replication by decreasing fork densities due to lower Cdk 2 kinase activity. Thus, Chk 1 levels need to be tightly controlled in order to properly regulate the replication program even during normal S phase. DNA combing experiments showed that Chk 1 inhibits origins outside, but not inside, already active clusters. Numerical simulations of initiation frequencies in the absence and presence of Chk 1 activity are consistent with a global inhibition of origins by Chk 1 at the level of clusters but need to be combined with a local repression of Chk 1 action close to activated origins to fit our data...|$|R
40|$|Abstract. Several nuclear {{activities}} and components {{are concentrated in}} discrete nuclear compartments. To understand the functional significance of nuclear compartmentalization, knowledge on the spatial distribution of transcriptionally active chromatin is essential. We have examined the distribution of sites of transcription by RNA polymerase II (RPII) by labeling nascent RNA with 5 -bromouridine 5 '-triphosphate, in vitro and in vivo. Nascent RPII transcripts were found in over 100 defined areas, scattered throughout the nucleoplasm. No preferential localization was observed in either the nuclear interior or the periphery. Each transcription site may represent the activity of a single gene or, considering the number of active pre-mRNA genes in a cell, of a cluster of active genes. The relation between the distribution of nascent RPII transcripts {{and that of the}} essential splicing factor SC- 35 T HE cell nucleus comprises all factors required for faithful replication of the genome and regulated synthesis, processing and transport of RNA. In recent years much information on nuclear organization has become available. It is clear now that the nucleus is highly organized (reviewed by de Jong et al., 1990; Jackson, 1991; van Driel et al., 1991). The most conspicuous subnuclear domain is the nucleolus in which ribosomal genes from different chromosomes are clustered and ribosome assembly takes place (reviewed by Scheer and Benavente, 1990; Hernandez-Verdun, 1991). Other examples of a domainlike organization in the nucleus are: <b>replication</b> <b>clusters</b> during S-phase (reviewed by Berezney, 1991), clustered splicing components (Spector, 1990; Fu and Maniatis, 1990; Carmo-Fonseca et al., 1992), hnRNP proteins (Pifiol-Roma et al., 1989; Ghetti et al., 1992), and tracks and foci of specifi...|$|R
40|$|Endothelial {{cell death}} and {{replication}} was analyzed in the rat aorta {{with the use}} of combined IgG immunocytochemistry and 3 H-thymidine autoradiography. Dead and replicating cells were clustered in the aortic endothelium of normal rats with a low level of spontaneous cell death and replication. Death and <b>replication</b> were also <b>clustered</b> in rats treated with endotoxin (Escherichia coli lipopolysaccharide [LPS]), a substance which is cytotoxic to endothelial cells in culture. LPS caused a dramatic increase both in endothelial cell death and replication, while no endothelial denudation could be discerned {{with the use of}} the 111 In-labeled platelet technique. We conclude that endothelial cell death and <b>replication</b> are topographically <b>clustered</b> phenomena, which may imply that cell death leads to a local stimulation of cell replication. Furthermore, LPS induces a massive cell death, which is paralleled by cell replication, yet does not cause any significant increase in denudation. This suggests that the primary effect of LPS on the vessel wall is its cytotoxicity for endothelial cells. Endothelial cell death appears to provide the stimulus for cell replication both in normal and in LPS-treated rats...|$|R
