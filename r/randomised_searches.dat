0|112|Public
40|$|This report {{documents}} the talks and discussions at the Dagstuhl Seminar 15211 "Theory of Evolutionary Algorithms". This seminar, {{now in its}} 8 th edition, is the main meeting point of the highly active theory of randomized search heuristics subcommunities in Australia, Asia, North America, and Europe. Topics intensively discussed include rigorous runtime analysis and computational complexity theory for <b>randomised</b> <b>search</b> heuristics, information geometry of <b>randomised</b> <b>search,</b> and synergies between the theory of evolutionary algorithms and theories of natural evolution...|$|R
40|$|<b>Randomised</b> <b>search</b> {{heuristics}} {{are used}} in practice to solve difficult problems where no good problem-specific algorithm is known. They deliver a solution of acceptable quality in reasonable time in many cases. When theoretically analysing the performance of <b>randomised</b> <b>search</b> heuristics one usually considers the average time needed to find an optimal solution or one of a pre-specified approximation quality. This {{is very different from}} practice where usually the algorithm is stopped after some time. For a theoretical analysis this corresponds to investigating the quality of the solution obtained after a pre-specified number of function evaluations called budget. Such a perspective is taken here and two simple <b>randomised</b> <b>search</b> heuristics, random local search and the (1 + 1) evolutionary algorithm, are analysed on simple and well-known example functions. If the budget is significantly smaller than the expected time needed for optimisation the behaviour of the algorithms can be very different depending on the problem at hand. Precise analytical results are proven. They demonstrate novel and interesting challenges in the analysis of <b>randomised</b> <b>search</b> heuristics. The potential of this different perspective to provide a more practically useful theory is shown...|$|R
40|$|Jansen, T., Zarges, C. (2013). Performance {{analysis}} of <b>randomised</b> <b>search</b> heuristics operating with a fixed budget. Theoretical Computer Science, 545, 39 - 58 When for a difficult real-world optimisation problem no good problem-specific algorithm is available often <b>randomised</b> <b>search</b> heuristics are used. They are hoped to deliver good solutions in acceptable time. The theoretical analysis usually {{concentrates on the}} average time needed to find an optimal or approximately optimal solution. This matches neither the application in practice nor the empirical analysis since usually optimal solutions are not known and even if found cannot be recognised. More often the algorithms are stopped after some time. This motivates a theoretical analysis {{to concentrate on the}} quality of the best solution obtained after a pre-specified number of function evaluations called budget. Using this perspective two simple <b>randomised</b> <b>search</b> heuristics, random local search and the (1 + 1) evolutionary algorithm, are analysed on some well-known example problems. Upper and lower bounds on the expected quality of a solution for a fixed budget of function evaluations are proven. The analysis shows novel and challenging problems in the study of <b>randomised</b> <b>search</b> heuristics. It demonstrates the potential of this shift in perspective from expected run time to expected solution quality. authorsversionPeer reviewe...|$|R
40|$|The {{convergence}}, convergence {{rate and}} expected hitting time {{play fundamental roles}} {{in the analysis of}} <b>randomised</b> <b>search</b> heuristics. This paper presents a unified Markov chain approach to studying them. Using the approach, the sufficient and necessary conditions of convergence in distribution are established. Then the average convergence rate is introduced to <b>randomised</b> <b>search</b> heuristics and its lower and upper bounds are derived. Finally, novel average drift analysis and backward drift analysis are proposed for bounding the expected hitting time. A computational study is also conducted to investigate the convergence, convergence rate and expected hitting time. The theoretical study belongs to a prior and general study while the computational study belongs to a posterior and case study...|$|R
40|$|Black-box {{complexity}} {{measures the}} difficulty of classes of functions with respect to optimisation by black-box algorithms. Comparing the black-box complexity with the worst case performance of a best know <b>randomised</b> <b>search</b> heuristic can help to assess if the <b>randomised</b> <b>search</b> heuristic is efficient or if {{there is room for}} improvement. When considering an example function it is necessary to extend it to a class of functions since single functions always have black-box complexity 1. Different kinds of extensions of single functions to function classes have been considered. In cases where the gap between the performance of the best <b>randomised</b> <b>search</b> heuristic and the black-box complexity is still large it can help to consider more restricted black-box complexity notions like unbiased black-box complexity. For the well-known Jump function neither considering different extensions nor considering more restricted notions of black-box complexity have been successful so far. We argue that the problem is not with the notion of black-box complexity but with the extension to a function class. We propose a different extension and show that for this extension there is a much better agreement even between the performance of an extremely simple evolutionary algorithm and the most general notion of black-box complexity...|$|R
40|$|Splay and <b>randomised</b> <b>search</b> {{trees are}} self-balancing binary tree {{structures}} {{with little or}} no space overhead compared to a standard binary search tree. Both trees are intended for use in applications where node accesses are skewed, for example in gathering the distinct words in a large text collection for index construction. We investigate the efficiency of these trees for such vocabulary accumulation. Surprisingly, unmodified splaying and <b>randomised</b> <b>search</b> trees are on average around 25 % slower than using a standard binary tree. We investigate heuristics to limit splay tree reorganisation costs and show their effectiveness in practice. In particular, a periodic rotation scheme improves the speed of splaying by 27 %, while other proposed heuristics are less effective. We also report the performance of efficient bit-wise hashing and red-black trees for comparison...|$|R
40|$|Although widely {{applied in}} optimisation, {{relatively}} {{little has been}} proven rigorously about the role and behaviour of populations in <b>randomised</b> <b>search</b> processes. This paper presents a new method to prove upper bounds on the expected optimisation time of population-based <b>randomised</b> <b>search</b> heuristics that use non-elitist selection mechanisms and unary variation operators. Our results follow from a detailed drift analysis of the population dynamics in these heuristics. This analysis shows that the optimisation time depends {{on the relationship between}} the strength of the selective pressure and the degree of variation introduced by the variation operator. Given limited variation, a surprisingly weak selective pressure suffices to optimise many functions in expected polynomial time. We derive upper bounds on the expected optimisation time of non-elitist Evolutionary Algorithms (EA) using various selection mechanisms, including fitness proportionate selection. We show that EAs using fitness proportionate selection can optimise standard benchmark functions in expected polynomial time given a sufficiently low mutation rate. As a second contribution, we consider an optimisation scenario with partial information, where fitness values of solutions are only partially available. We prove that non-elitist EAs under a set of specific conditions can optimise benchmark functions in expected polynomial time, even when vanishingly little information about the fitness values of individual solutions or populations is available. To our knowledge, this is the first runtime analysis of <b>randomised</b> <b>search</b> heuristics under partial information...|$|R
40|$|This paper {{introduces}} {{design space}} exploration {{as one of}} the major tasks in embedded system design. After reviewing existing exploration methods at various layers of abstraction, a generic approach is described based on multi-objective criteria, black-box optimisation and <b>randomised</b> <b>search</b> strategies. The interface between problem-specific and generic parts of the exploration framework is made explicit by defining an interface called PISA. This specification and implementation interface and the availability {{of a wide range of}} <b>randomised</b> multi-objective <b>search</b> methods makes the proposed framework accessible to a wide range of exploration problems. It resolves the problem that existing optimisation methods can not be coupled easily to the problem-specific part of a design exploration tool. ...|$|R
40|$|International audienceStep-size {{adaptation}} for <b>randomised</b> <b>search</b> algorithms like evolution strategies is {{a crucial}} feature for their performance. The adaptation must, depending on the situation, sustain a large diversity or entertain fast convergence to the desired optimum. The assessment of step-size adaptation mechanisms is therefore non-trivial and often done in too restricted scenarios, possibly only on the sphere function. This paper introduces a (minimal) methodology combined with a practical procedure to conduct a more thorough assessment of the overall population diversity of a <b>randomised</b> <b>search</b> algorithm in different scenarios. We illustrate the methodology on evolution strategies with σ-self-adaptation, cumulative step-size adaptation and two-point adaptation. For the latter, we introduce a variant that abstains from additional samples by constructing two particular individuals within the given population {{to decide on the}} step-size change. We find that results on the sphere function alone can be rather misleading to assess mechanisms to control overall population diversity. We observed the most striking flaws for self-adaptation: on the linear function, the step-size increments are rather small, and on a moderately conditioned ellipsoid function, the adapted step-size is 20 times smaller than optimal...|$|R
40|$|Abstract. Fitness {{functions}} {{based on}} the Ising model are suited excellently for studying the adaption capabilities of <b>randomised</b> <b>search</b> heuristics. The one-dimensional Ising model was considered a hard problem for mutation-based algorithms, and the two-dimensional Ising model was even thought to amplify the difficulties. While in one dimension the Ising model {{does not have any}} local optima, in two dimensions it does. Here we prove that a simple search heuristic, the Metropolis algorithm, optimises on average the two-dimensional Ising model in polynomial time...|$|R
40|$|Abstract Recent {{statistical}} performance {{studies of}} search algorithms in difficult combinatorial problems {{have demonstrated the}} benefits of randomising and restarting the search procedure. Specifically, {{it has been found}} that if the search cost distribution of the non-restarted <b>randomised</b> <b>search</b> exhibits a slower-than-exponential decay (that is, a “heavy tail”), restarts can reduce the search cost expectation. We report on an empirical study of <b>randomised</b> restarted <b>search</b> in ILP. Our experiments conducted on a high-performance distributed computing platform provide an extensive statistical performance sample of five search algorithms operating on two principally different classes of ILP problems, one represented by an artificially generated graph problem and the other by three traditional classification benchmarks (mutagenicity, carcinogenicity, finite element mesh design). The sample allows us to (1) estimate the conditional expected value of the search cost (measured {{by the total number of}} clauses explored) given the minimum clause score required and a “cutoff ” value (the number of clauses examined before the search is restarted), (2) estimate the conditional expected clause score given the cutoff value and the invested search cost, and (3) compare the performance of <b>randomised</b> restarted <b>search</b> strategies to a deterministic non-restarted search. Our findings indicate striking similarities across the five search algorithms and the four domains, in terms of the basic trends of both the statistics (1) and (2). Also, we observe that the cutoff value is critical for the performance of the search algorithm, and using its optimal value in a <b>randomised</b> restarted <b>search</b> may decrease the mean search cost (by several orders o...|$|R
40|$|Recent {{statistical}} performance {{surveys of}} search algorithms in difficult combinatorial problems {{have demonstrated the}} benefits of randomising and restarting the search procedure. Specifically, {{it has been found}} that if the search cost distribution (SCD) of the non-restarted <b>randomised</b> <b>search</b> exhibits a slower-than-exponential decay (that is, a "heavy tail"), restarts can reduce the search cost expectation. Recently, this heavy tail phenomenon was observed in the SCD's of benchmark ILP problems. Following on this work, we report on an empirical study of <b>randomised</b> restarted <b>search</b> in ILP. Our experiments, conducted over a cluster of a few hundred computers, provide an extensive statistical performance sample of five search algorithms operating on two principally different ILP problems (artificially generated graph data and the wellknown "mutagenesis" problem). The sample allows us to (1) estimate the conditional expected value of the search cost (measured {{by the total number of}} clauses explored) given the minimum clause score required and a "cutoff" value (the number of clauses examined before the search is restarted); and (2) compare the performance of <b>randomised</b> restarted <b>search</b> strategies to a deterministic non-restarted search. Our findings indicate that the cutoff value is significantly more important than the choice of (a) the specific refinement strategy; (b) the starting element of the search; and (c) the specific data domain. We find that the optimal value for the cutoff parameter remains roughly stable across variations of these three factors and that the mean search cost using this value in a <b>randomised</b> restarted <b>search</b> is up to three orders of magnitude (i. e. 1000 times) lower than that obtained with a deterministic non-restarted search...|$|R
40|$|Abstract — We {{present a}} design {{strategy}} to reduce power demands in application-specific, heterogeneous multiprocessor systems with interdependent subtasks. This power reduction scheme {{can be used}} with a <b>randomised</b> <b>search</b> such as a genetic algorithm where multiple trial solutions are tested. The scheme is applied to each trial solution after allocation and scheduling have been performed. Power savings are achieved by equally expanding each processor’s execution time with a corresponding reduction in their respective operating voltage. Lowest cost solutions achieve average reductions of 24 % while minimum power solutions average 58 %...|$|R
40|$|Abstract. We {{present a}} {{systematic}} procedure for the synthesis and minimisation of digital circuits using propositional satisfiability. We encode the truth table into a canonical sum of at most k different minterms, {{which is then}} passed to one <b>randomised</b> <b>search</b> procedure that minimises k. The solution for a minimal k is the satisfiable representation of the resulting circuit. We show how to use an interesting feature of the local search landscape in this minimisation. This approach can be very useful because we can generate exact minimal solutions within reasonably computational resources. ...|$|R
40|$|Abstract- We {{present a}} design {{strategy}} to reduce power demands in application-specific, heterogeneous multipro-cessor systems with interdependent subtasks. This power reduction scheme {{can be used}} with a <b>randomised</b> <b>search</b> such as a genetic algorithm where multiple trial solutions are tested. The scheme is applied to each trial solution af-ter allocation and scheduling have been performed. Power savings are achieved by equally expanding each processor’s execution time with a corresponding reduction in their re-spective operating voltage. Lowest cost solutions achieve average reductions of 24 % while minimum power solutions average 58 %...|$|R
40|$|A novel search {{technique}} called highway search is introduced. The search technique {{relies on a}} highway simulation which takes several homogeneous walks through a (possibly infinite) state space. Furthermore, we provide a memory-efficient algorithm that approximates a highway search and we prove that, under particular conditions, they coincide. The effectiveness of highway search is compared to two mainstream search techniques, viz. random <b>search</b> and <b>randomised</b> depth-first <b>search.</b> Our results demonstrate that <b>randomised</b> depth-first <b>search</b> explores {{the least amount of}} states in the effort of finding states of interest, whereas a highways search yields the shortest witnessing traces to such states...|$|R
40|$|Abstract: Genetic Algorithms are {{introduced}} as a search method for finding string vacua with viable phenomenological properties. It is shown, by testing them against {{a class of}} Free Fermionic models, that they are orders of magnitude more efficient than a <b>randomised</b> <b>search.</b> As an example, three generation, exophobic, Pati-Salam models with a top Yukawa occur once in every 1010 models, and yet a Genetic Algorithm can find them after construct-ing only 105 examples. Such non-deterministic search methods {{may be the only}} means to search for Standard Model string vacua with detailed phenomenological requirements. ar X i...|$|R
40|$|This report {{documents}} {{the program and}} the outcomes of the Dagstuhl Seminar 11172 "Artificial Immune Systems 2 ̆ 72 ̆ 7. The purpose of the seminar was to bring together researchers from the areas of immune-inspired computing, theoretical computer science, <b>randomised</b> <b>search</b> heuristics, engineering, swarm intelligence and computational immunology in a highly interdisciplinary seminar to discuss two main issues: first, how to best develop a more rigorous theoretical framework for algorithms inspired by the immune system and second, to discuss suitable application areas for immune-inspired systems and how best to exploit the properties of those algorithms...|$|R
40|$|Genetic Algorithms are {{introduced}} as a search method for finding string vacua with viable phenomenological properties. It is shown, by testing them against {{a class of}} Free Fermionic models, that they are orders of magnitude more efficient than a <b>randomised</b> <b>search.</b> As an example, three generation, exophobic, Pati-Salam models with a top Yukawa occur once in every 10 ^{ 10 } models, and yet a Genetic Algorithm can find them after constructing only 10 ^ 5 examples. Such non-deterministic search methods {{may be the only}} means to search for Standard Model string vacua with detailed phenomenological requirements. Comment: 10 figures, 24 pages, JHEP versio...|$|R
40|$|We {{consider}} inference in {{a widely}} used predictive model in empirical ﬁnance. "Stambaugh Bias" arises when innovations to the predictor variable are correlated {{with those in}} the predictive regression. We show that high values of the "Stambaugh Correlation" will arise naturally if the predictor is actually predictively redundant, but emerged from a <b>randomised</b> <b>search</b> by data mining econometricians. For such predictors even bias-corrected conventional tests will be severely distorted. We propose tests that distinguish well between redundant predictors and the true (or "perfect") predictor. An application of our tests does not reject the null that a range of predictors of stock returns are redundant...|$|R
40|$|This paper {{presents}} three proposals of multiobjective memetic algorithms {{to solve}} a more realistic extension of a classical industrial problem: time and space assembly line balancing. These three proposals are, respectively, based on evolutionary computation, ant colony optimisation, and greedy <b>randomised</b> <b>search</b> procedure. Different variants of these memetic algorithms have been developed and compared {{in order to determine}} the most suitable intensification–diversification trade-off for the memetic search process. Once a preliminary study on nine well-known problem instances is accomplished with a very good performance, the proposed memetic algorithms are applied considering real-world data from a Nissan plant in Barcelona (Spain). Outstanding approximations to the pseudo-optimal non-dominated solution set were achieved for this industrial case study. Peer ReviewedPostprint (published version...|$|R
30|$|The PWMs for RS and FMKL GLD for {{left and}} right {{censoring}} are given in the Appendix. Based on these results, {{it is now possible}} to find a set of parameters of GLD that minimize the sum of the squared difference between sample and derived PWMs using (14). The problem of choosing initial values for the optimisation process is again solved using the method described in Su (2007 b, 2007 a), which involves an extensive <b>randomised</b> <b>search</b> across the parameters using quasi random number generators such as the Sobol or Halton sequence. The initial values used to start the optimisation process will be a randomised set of parameters that best matches the partial probability weighted moments between the sample and the estimated GLD.|$|R
30|$|This {{entire process}} is {{repeated}} for 200 observations to allow assessment of effect of sample size {{on the performance}} of proposed fitting methods. To create right censored data, observations greater than quantile ranging from 0.5 (median) to 0.9 are censored. Similarly, to create left censored data, observations less than quantile ranging from 0.1 to 0.5 (median) are censored. Five estimation methods: ML (maximum likelihood)/LM (L moments)/QS (quantile matching) under SR and MLE and PPWM matching were applied over 100 simulation runs. When fitting RS GLD using MLE with half of the data being censored (i.e., at 0.5), sometimes it is desirable to use the SR-ML method to generate the initial values to start the optimisation process, rather than using <b>randomised</b> <b>search</b> as it would lead to a better performance. This strategy is used in this article.|$|R
40|$|AbstractConcurrent {{execution}} of multiple instances of a <b>randomised</b> <b>search</b> over a CSP {{is one of}} the techniques for improvement of performance. Hitherto, uniform time sharing is the dominant approach to the control of {{execution of}} such instances. This paper introduces a new modification of search algorithms—non-uniform time sharing with elimination (NUTSE) —and experimentally evaluates the efficiency of its combination with both the FC-MRV (Forward Checking with the Minimal-Remaining-Values heuristic) and the FC-B (FC with the Brelaz's heuristic). The experiments show that the NUTSE over FC-MRV can be in the underconstrained area many times faster than the singly-executed FC-B. This good behaviour is used in a hybrid CNUC algorithm (Combined Non-Uniform Concurrency) that combines the FC-B and the NFC-MRV algorithms to obtain a good algorithm across a wide range of problem instances. All the experiments in this paper use the graph three-colouring problem...|$|R
40|$|Dynamic {{optimisation}} is {{an important}} area of application for evolutionary algorithms and other <b>randomised</b> <b>search</b> heuristics. Theoretical investigations are currently far behind practical successes. Addressing this deficiency a bi-stable dynamic optimisation problem is introduced {{and the performance of}} standard evolutionary algorithms and artificial immune systems is assessed. Deviating from the common theoretical perspective that concentrates on the expected time to find a global optimum (again) here the ?any time performance? of the algorithms is analysed, i. e., the expected function value at each step. Basis for the analysis is the recently introduced perspective of fixed budget computations. Different dynamic scenarios are considered which are characterised by the length of the stable phases. For each scenario different population sizes are examined. It is shown that the evolutionary algorithms tend to have superior performance in almost all cases...|$|R
40|$|Theoretical {{analysis}} {{of all kinds}} of <b>randomised</b> <b>search</b> heuristics has been and keeps being supported and facilitated by the use of simple example functions. Such functions help us understand the working principles of complicated heuristics. If the function represents some properties of practical problem landscapes these results become practically relevant. While this has been very successful in the past for optimisation in unimodal landscapes {{there is a need for}} generally accepted useful simple example functions for situations where unimodal objective functions are insufficient: multimodal optimisation and investigation of diversity preserving mechanisms are examples. A family of example landscapes is defined that comes with a limited number of parameters that allow to control important features of the landscape while all being still simple in some sense. Different expressions of these landscapes are presented and fundamental properties are explored...|$|R
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from Massachusetts Institute of Technology Press via [URL] optimisation {{is an area of}} application where <b>randomised</b> <b>search</b> heuristics like evolutionary algorithms and artificial immune systems are often successful. The theoretical foundation of this important topic suffers from a lack of a generally accepted analytical framework as well as a lack of widely accepted example problems. This article tackles both problems by discussing necessary conditions for useful and practically relevant theoretical analysis as well as introducing a concrete family of dynamic example problems that draws inspiration from a well-known static example problem and exhibits a bi-stable dynamic. After the stage has been set this way, the framework is made concrete by presenting the results of thorough theoretical and statistical analysis for mutation-based evolutionary algorithms and artificial immune systems. authorsversionPeer reviewe...|$|R
40|$|We {{study the}} {{efficiency}} of randomised solutions to the mutual search problem of finding k agents distributed over n nodes. For a restricted class of so-called linear <b>randomised</b> mutual <b>search</b> algorithms we derive a lower bound of (n+ 1) (k- 1) /(k+ 1) expected calls in the worst case. A randomised algorithm in the shared-coins model matching this bound is also presented...|$|R
40|$|This paper {{describes}} a probabilistic technique for {{the identification of}} critical paths in digital logic circuits. The technique involves {{the transformation of the}} original (gate level) circuit representation into a Conjunctive Normal Form (CNF) formula which incorporates timing information. A <b>randomised</b> <b>search</b> technique is then used to progressively improve known upper and lower bounds on the length of the critical path(s) in the circuit. 1 Introduction One of the fundamental requirements in circuit timing analysis is the means to efficiently and accurately identify the time required for a circuit's outputs to stabilise after an input vector has been applied to the circuit. In a combinational circuit composed of simple logic gates, the stabilisation time {{is a function of the}} delay a signal encounters when propagated along paths in the circuit from primary inputs to primary outputs. The worst case stabilisation time is determined by the propagation of signals along a critical path in [...] ...|$|R
40|$|This paper {{describes}} a new metaheuristic for combinatorial optimisation problems with specific {{reference to the}} jobshop scheduling problem (JSP). A fuzzy greedy search algorithm (FGSA) which {{is a combination of}} a genetic algorithm (GA) and a greedy <b>randomised</b> adaptive <b>search</b> procedure (GRASP) is considered to solve the problem. The effectiveness and efficiency of the proposed hybrid method will be investigated through the experimental results on standard benchmark problems...|$|R
40|$|We {{consider}} inference in {{a widely}} used predictive model in empirical finance. "Stambaugh Bias " arises when innovations to the predictor variable are correlated {{with those in}} the predictive regression. We show that high values of the "Stambaugh Correlation " will arise naturally if the predictor is actually predictively redundant, but emerged from a <b>randomised</b> <b>search</b> by data mining econometricians. For such predictors even bias-corrected conventional tests will be severely distorted. We propose tests that distinguish well between redundant predictors and the true (or "perfect") predictor. An application of our tests does not reject the null that a range of predictors of stock returns are redundant. 1 There is an extensive literature in empirical finance that focusses on inference difficulties in a predictive framework of the form rt+ 1 = α + βzt + et+ 1 zt+ 1 = φzt + wt+ 1 where the first equation captures the degree of predictability of some variable, rt (usually some measure of returns or excess returns) in terms of some predictor variable zt, (frequently som...|$|R
30|$|In {{the context}} of GLDs, F(t) and f(t) can be {{obtained}} as described in Generalized Lambda Distributions and these are available {{from a number of}} statistical packages such as GLDEX (Su 2007) in R. The usual way of fitting survival curves using maximum likelihood estimation for GLD is to take the logarithm of the likelihood and maximize the likelihood. The maximisation is usually done using the Nelder-Mead simplex algorithm. This is usually a more robust method than trying to find a set of parameters by differentiating the log likelihood and finding the parameters numerically by setting the equations to zero. Additionally, the problem of choosing initial values to kick start the optimisation process can be solved using the method described in Su (2007 b, 2007 a) or by using the initial values obtained by matching PWMs as detailed below. The initial values search method in Su (2007 b, 2007 a) uses an extensive <b>randomised</b> <b>search</b> across the parameters using quasi random number generators such as the Sobol or Halton sequence and this article chooses the best randomised set of parameters in terms of the largest likelihood value to initiate the optimisation process.|$|R
40|$|At last year’s GECCO a novel {{perspective}} for theoretical performance analysis of evolutionary algorithms and other <b>randomised</b> <b>search</b> heuristics was introduced that concen- trates on the expected function value after a pre-defined number of steps, called budget. This is significantly differ- ent {{from the common}} perspective where the expected op- timisation time is analysed. While {{there is a huge}} body of work and a large collection of tools for the analysis of the ex- pected optimisation time the new fixed budget perspective introduces new analytical challenges. Here it is shown how results on the expected optimisation time that are strength- ened by deviation bounds can be systematically turned into fixed budget results. We demonstrate our approach by con- sidering the (1 + 1) EA on LeadingOnes and significantly improving previous results. We prove that deviating from the expected time by an additive term of ω n 3 / 2 happens only with probability o (1). This is turned into tight bounds on the function value using the inverse function. We use three, increasingly strong or general approaches to proving the deviation bounds, namely via Chebyshev’s inequality, via Chernoff bounds for geometric random variables, and via variable drift analysis...|$|R
40|$|The {{objective}} {{of this paper is}} the development and analysis of methods for producing digital elevation models from physical scanned maps having as little as possible human interaction. The elevation layers are identified based on the map’s legend using the CIE L*u*v * color space, the mean shift filtering and the <b>randomised</b> local <b>search</b> color clustering. The nonelevation layers within the map image generates unclassified pixels. Their values are estimated using geostatistics (i. e. kriging). In the end, the elevation contours are extracted and interpolated. 1...|$|R
40|$|Exact {{approaches}} to Frequent Itemsets Mining (FIM) are characterised by poor runtime performance {{when dealing with}} large database instances. Several FIM bio-inspired approaches have been proposed to overcome this issue. These are considerably more efficient {{from the point of}} view of runtime performance, but they still yield poor quality solutions. The quality of the solution, i. e., the number of frequent itemsets discovered, can be increased by improving the <b>randomised</b> <b>search</b> of the solutions space considering intrinsic features of the FIM problem. This paper proposes a new framework for FIM bio-inspired approaches that considers the recursive property of frequent itemsets, i. e., the same feature exploited by the Apriori exact heuristic, in the search of the solution space. We define two new {{approaches to}} FIM, namely GA-Apriori and PSO-Apriori, based on the proposed framework, which use genetic algorithms and particle swarm optimisation, respectively. Extensive experiments on synthetic and real database instances show that the proposed approaches outperform other bio-inspired ones in terms of runtime performance. The results also reveal that the performance of PSO-Apriori is comparable to the one of exact approaches Apriori and FPGrowth in respect of the quality of solutions found. We also show that PSO-Apriori outperforms the recently developed BATFIM algorithm when dealing with very large database instances...|$|R
40|$|Nurse {{scheduling}} {{is a type}} {{of manpower}} allocation problem that tries to satisfy hospital managers objectives and nurses preferences as much as possible by generating fair shift schedules. This paper presents a nurse scheduling problem based on a real case study, and proposes two meta-heuristics a differential evolution algorithm (DE) and a greedy <b>randomised</b> adaptive <b>search</b> procedure (GRASP) to solve it. To investigate the efficiency of the proposed algorithms, two problems are solved. Furthermore, some comparison metrics are applied to examine the reliability of the proposed algorithms. The computational results in this paper show that the proposed DE outperforms the GRASP...|$|R
