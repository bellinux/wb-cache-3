0|31|Public
50|$|Actuarial <b>reinsurance</b> premium <b>calculation</b> {{uses the}} similar {{mathematical}} tools as actuarial insurance premium. Nevertheless, Catastrophe modeling, Systematic risk or risk aggregation statistics tools are more important.|$|R
30|$|To {{reduce the}} delay of {{receiving}} a control message, which {{may affect the}} <b>calculation,</b> the control <b>messages</b> {{have to get the}} highest priority to be transmitted along other messages.|$|R
40|$|This {{paper is}} {{concerned}} with the optimal form of reinsurance from the ceding company point of view, when the cedent seeks to maximize the adjustment coefficient of the retained risk. We deal with the problem by exploring the relationship between maximizing the adjustment coefficient and maximizing the expected utility of wealth for the exponential utility function, both with respect to the retained risk of the insurer. Assuming that the premium calculation principle is a convex functional and that some other quite general conditions are fulfilled, we prove the existence and uniqueness of solutions and provide a necessary optimal condition. These results are used to find the optimal reinsurance policy when the <b>reinsurance</b> premium <b>calculation</b> principle is the expected value principle or the reinsurance loading is an increasing function of the variance. In the expected value case the optimal form of reinsurance is a stop-loss contract. In the other cases, it is described by a nonlinear function. ...|$|R
40|$|Abstract. This paper {{presents}} the verification of CRC algorithm properties. We examine {{a way of}} verifying of a CRC algorithm using exhaustive state space exploration by model checking method. The CRC algorithm is used for <b>calculation</b> of a <b>message</b> hash value and we focus on verification of the property of finding minimal Hamming distance between two messages having the same hash value. We deal with 16, 32 and 64 bits CRC generator polynomials, especially with one used in the Liberouter project. ...|$|R
40|$|The TimeWarp {{mechanism}} accomplishes {{an efficient}} synchronization between {{the components of}} a distributed, discrete event-driven simulator. Using an optimistic simulation strategy, {{the components of the}} simulator may calculate ahead locally, sending results to other components without waiting for any events produced by those components, ignoring possible causality problems. In case of an incorrect <b>calculation</b> caused by <b>messages</b> received too late, a component must perform a rollback and cancel some messages already sent, possibly initiating further rollbacks in other components. Nevertheles the distributed TimeWarp [...] ...|$|R
40|$|Section 8 D. a. {{applies to}} a company’s {{aggregate}} gross reserve before <b>reinsurance.</b> The reserve <b>calculation</b> requires {{a projection of}} future year-by-year cash flows, which includes such items as investment earnings and general insurance expenses. If a company has ceded 100 % of the business to an authorized reinsurer by use of coinsurance, the assets and net liabilities are {{no longer on the}} ceding company’s books. Additionally, the administration is also generally transferred to the reinsurer. In such circumstances, the projection of future cash flows is “hypothetical ” in that one must assume a starting asset portfolio, future investment strategy, future general insurance expenses, and so on. Furthermore, the ceding company generally does not have the data or systems to determine the reserves and must rely on the assuming company. Since the assuming company is required to calculate the reserves on both direct and assumed business, may the ceding company use the reserves as reported by the reinsurer in the reinsurer’s annual statement...|$|R
40|$|Enterprise {{software}} systems make {{complex interactions}} with other services in their environment. Developing and testing for production-like conditions {{is therefore a}} challenging task. Prior approaches include emulations of the dependency services using either explicit modelling or record-and-replay approaches. Models require deep knowledge of the target services while record-and-replay is limited in accuracy. We present a new technique that improves the accuracy of record-and-replay approaches, without requiring prior knowledge of the services. The approach uses multiple sequence alignment to derive message prototypes from recorded system interactions and a scheme to match incoming request messages against message prototypes to generate response messages. We introduce a modified Needleman-Wunsch algorithm for distance <b>calculation</b> during <b>message</b> matching, wildcards in message prototypes for high variability sections, and entropy-based weightings in distance calculations for increased accuracy. Combined, our new approach has shown greater than 99 % accuracy for four evaluated enterprise system messaging protocols...|$|R
40|$|Planning system {{conducted}} by PT YANMAR Indonesia is the ordering {{in the time}} allowed for the entire Part, companies often charge a fee high enough to issue stock. These problems originate in the determination of inappropriate messaging plan that causes {{the size of the}} total cost of inventory. The research analyzed mathematically to determine an efficient messaging plan with the system plans an appropriate message to the company condition, ie EOQ using multiple items to obtain facilities, Part clasification then conducted into three classes based on the ABC classification, which was then followed by using a calculation based spare parts ordering plan company policy and the proposed method. From the <b>calculation</b> based <b>messaging</b> plan proposed method obtained the number of spare parts must be ordered for each class is a class A 579 units, 325 units for the class B and class C units to 5807 by the time the message working days. Efficient costs obtained for 14...|$|R
40|$|Highlighting, a {{conditioning}} effect, {{consists of}} both primacylike and recency-like effects in human subjects. This combination of effects are notoriously difficult for Bayesian models to produce. An approximation to probabilistic inference, Locally Bayesian learning (LBL), can predict highlighting by partitioning the model into regions during learning and passing messages between these regions. While the approximation matches behavior in this task, {{it is unclear}} how LBL compares to other approximations used in Bayesian models, and what behaviors this approximations will predict in other paradigms. Our contribution is to show LBL is closely related to the statistical algorithms of Assumed Density Filtering (ADF), which simplifies calculations by assuming independence, and Belief Propagation, which identifies how to make these <b>calculations</b> through <b>message</b> passing. We propose that people use ADF to learn and show how this model can produce highlighting behavior. In addition, we demonstrate how the degrees of approximation used in LBL and ADF cause the models to make very different predictions in a proposed experimental design...|$|R
40|$|The Controller Area Network (CAN) is {{extensively}} {{used for}} timely communication in automotive and other applications. As timing analysis of CAN messages is emerging into industrial practice, {{and as the}} use of CAN in safety-critical applications is increasing, there is an apparent need to include the eects of transmission errors in the analysis of message latencies. In this paper we provide a general fault model and extend the timing analysis of CAN messages to cater for the eects of transmission errors on message latencies. This fault model helps us in composing the eects of interferences from multiple sources and to account for them in the <b>calculation</b> of <b>message</b> response times. We illustrate our model, by applying it to derive the worst case latencies of a subset of messages selected from the frequently used SAE case-study. We also discuss and illustrate the implications on message latencies for some realistic fault scenarios. 1 Introduction Controller Area Network (CAN) has beco [...] ...|$|R
40|$|This paper {{presents}} a robust image authentication approach that distinguishes malicious attacks from JPEG lossy compression. The authentication procedure calculates {{the relationships between}} important DCT coefficients in each pair of DCT blocks and predefined thresholds to form the authentication message, and then embeds the encryption of the authenticated message into other DCT coefficients. The <b>message</b> <b>calculation</b> and embedding procedure are based on two proposed quantization properties that always exist under different JPEG quantization tables. Therefore, the proposed image authentication approach can tolerate JPEG compression efficiently. Experimental results demonstrate {{the effectiveness of the}} proposed image authentication approach...|$|R
40|$|The article {{introduces}} {{social network}} user sentiment evaluation with proposed technique based on fuzzy sets. The advantage of proposed technique consists in {{ability to take}} into account user's influence as well as the fact that a user could be an author of several messages. Results presented in this paper can be used in mechanical engineering to analyze reviews on products as well as in robotics for developing user communication interface. The paper contains experimental data and shows the steps of sentiment value <b>calculation</b> of resulting <b>messages</b> on a certain topic. Application of proposed technique is demonstrated on experimental data from Twitter social network...|$|R
40|$|The {{article focuses}} on {{providing}} brief theoretical definitions of the basic terms and methods of modeling and simulations of insurance risks in non-life insurance by means of mathematical and statistical methods using statistical software. While risk assessment of insurance company in connection with its solvency is a rather complex and comprehensible problem, its solution starts with statistical modeling of number and amount of individual claims. Successful solution of these fundamental problems enables solving of curtail problems of insurance such as modeling and simulation of collective risk, premium an <b>reinsurance</b> premium <b>calculation,</b> estimation of probabiliy of ruin etc. The article also presents some essential ideas underlying Monte Carlo methods and their applications to modeling of insurance risk. Solving problem {{is to find the}} probability distribution of the collective risk in non-life insurance portfolio. Simulation of the compound distribution function of the aggregate claim amount can be carried out, if the distibution functions of the claim number process and the claim size are assumed given. The Monte Carlo simulation is suitable method to confirm the results of other methods and for treatments of catastrophic claims, when small collectives are studied. Analysis of insurance risks using risk theory is important part of the project Solvency II. Risk theory is analysis of stochastic features of non-life insurance process. The field of application of risk theory has grown rapidly. There is a need to develop the theory into form suitable for practical purposes and demostrate their application. Modern computer simulation techniques open up a wide field of practical applications for risk theory concepts, without requiring the restricive assumptions and sophisticated mathematics. This article presents some comparisons of the traditional actuarial methods and of simulation methods of the collective risk model...|$|R
40|$|Large {{enterprise}} software systems {{make many}} complex interactions with other services in their environment. Developing and testing for production-like conditions {{is therefore a}} very challenging task. Current approaches include emulation of dependent services using either explicit modelling or record-and-replay approaches. Models require deep knowledge of the target services while record-and-replay is limited in accuracy. Both face developmental and scaling issues. We present a new technique that improves the accuracy of record-and-replay approaches, without requiring prior knowledge of the service protocols. The approach uses Multiple Sequence Alignment to derive message prototypes from recorded system interactions and a scheme to match incoming request messages against prototypes to generate response messages. We use a modified Needleman-Wunsch algorithm for distance <b>calculation</b> during <b>message</b> matching. Our approach has shown greater than 99 % accuracy for four evaluated enterprise system messaging protocols. The approach has been successfully integrated into the CA Service Virtualization commercial product to complement its existing techniques. Comment: In Proceedings of the 38 th International Conference on Software Engineering Companion (pp. 202 - 211). arXiv admin note: text overlap with arXiv: 1510. 0142...|$|R
40|$|Abstract:- It {{has been}} shown that the {{standardized}} hash-algorithms SHA- 1, RIPEMD- 160, MD- 5 and others of that type have substantial performance restrictions due to their sequential structure. Modified variants of hash-algorithms are suggested which make it possible to independently calculate partial hashsignatures while maintaining the irreversibility level of the standard algorithms. These variants give new opportunities for wide parallel calculation of hash-signatures. In fact, applying the suggested modified algorithms remove the performance restrictions affiliated with the <b>calculation</b> an information <b>message’s</b> hash-signatures. In practice, the hash-algorithms suggested may be used for integrity and authentication of information messages in computer network. Key words: Cryptographic hash functions, instruction-level parallelism, multiple-issue architectures, parallel calculation arrangement. ...|$|R
40|$|Abstract—Recent {{broadband}} wireless {{technologies such as}} HSDPA and Mobile WiMAX achieve high data rate transmission, making wireless networking environments more similar to wired environments. As a result, wireless networks are also being exposed to DDoS attack. In this paper, we consider a possible DDoS attack in Mobile WiMAX networks and solve this problem by using our proposed Shared Authentication Information (SAI). SAI exploits unused upper 64 bits of the 128 -bit Cipher-based Message Authentication Code (CMAC) which {{has been designed to}} provide the integrity of management message. In the Mobile WiMAX network, the lower significant 64 bits of CMAC are truncated and used. Therefore we are able to use the upper 64 bits for our SAI while assuring the same level of security guaranteed by CMAC. Since SAI can be obtained from CMAC calculation, no additional <b>calculations</b> or <b>message</b> exchanges are required for sharing SAI and only the entity having the CMAC key can know SAI. Owing to these properties, using SAI can be a simple defense mechanism against DDoS attack without incurring overhead at access service network gateway (ASN GW) and base station (BS). I...|$|R
40|$|We have {{implemented}} a parallel divide-and-conquer method for semiempirical quantum mechanical <b>calculations.</b> The standard <b>message</b> passing library, the message passing interface (MPI), was used. In this parallel version, the memory needed {{to store the}} Fock and density matrix elements is distributed among the processors. This memory distribution solves the problem of demanding requirement of memory for very large molecules. While the parallel calculation for construction of matrix elements is straightforward, the parallel calculation of Fock matrix diagonalization is achieved via the divide-and-conquer method. Geometry optimization is also implemented with parallel gradient calculations. The code has been tested on a Gray T 3 E parallel computer, and impressive speedup of calculations has been achieved. Our {{results indicate that the}} divide-and-conquer method is efficient for parallel implementation. (C) 1998 John Wiley & Sons, Inc...|$|R
40|$|The {{excellent}} {{performance of}} convolutional low-density parity-check codes {{is the result}} of the spatial coupling of individual underlying codes across a window of growing size, but much smaller than the length of the individual codes. Remarkably, the belief-propagation threshold of the coupled ensemble is boosted to the maximum-a-posteriori one of the individual system. We investigate the generality of this phenomenon beyond coding theory: we couple general graphical models into a one-dimensional chain of large individual systems. For the later we take the Curie-Weiss, random field Curie-Weiss, $K$-satisfiability, and $Q$-coloring models. We always find, based on analytical as well as numerical <b>calculations,</b> that the <b>message</b> passing thresholds of the coupled systems come very close to the static ones of the individual models. The remarkable properties of convolutional low-density parity-check codes are a manifestation of this very general phenomenon. Comment: In proceedings of ITW 201...|$|R
40|$|Abstract—Novel lazy Lauritzen-Spiegelhalter (LS), lazy Hugin {{and lazy}} Shafer-Shenoy (SS) {{algorithms}} are devised for Gaussian Bayesian networks (BNs). In the lazy algorithms, the clique potentials and separator potentials {{are kept in}} combinable decomposed form instead of combined to be a single valuation in conventional junction tree algorithms. By employing decomposed form potentials, the independence relations between variables are explored online and the directed graph information is utilized in the <b>message</b> <b>calculations.</b> In the proposed algorithms, a consistent junction tree with the evidence entered {{can be obtained by}} a single round of message passing. The moments form parametrization of Gaussian distributions allows the deterministic relationships between variables. Preliminary analysis shows that the lazy LS algorithm and the lazy Hugin algorithm are more computationally efficient than the lazy SS algorithm, especially when there are multiple items of evidence to be incorporated. Keywords-Gaussian Bayesian networks; lazy propagation; arc reversal; junction tree algorithms I...|$|R
40|$|One of {{the basic}} {{functions}} of an MRP system is to issue rescheduling messages that urge the planner tospeed up or slow down open orders. It seems in practice that these messages are not used at all by planners. This is mostly due to the inaccuracy of MRP, that more or less ignores safety time, safety stocks and lotsize flexibility in the <b>calculation</b> of reschedule-in <b>messages.</b> Reschedule-out messages are usually ignored because planners {{do not see the}} value of the message. Other reasons for not adhering to rescheduling messages are a lack of maintenance of MRP parameters or simply the wrong use of the MRP function. In the future, MRP rescheduling functionality will be used even less than today, due to the changing role of MRP within the planning framework. With the uprise of finite capacity scheduling packages, MRP is being pushed one level upward in the planning hierarchy. This means that rescheduling functionalities for the short term will become completely obsolete in MRP systems...|$|R
40|$|Abstract. Worst case {{response}} time analysis of CAN Bus is {{the de facto}} method for evaluating the real-time performance which is a key feature of industrial bus. Compared with WCRT which aimed at nodes without buffer, a modified algorithm was presented to calculate the {{response time}} of CAN Bus consisting of nodes with both hardware and software buffers. The modified algorithm introduced local blocking time to describe the effect of buffers on response time. Based on {{the analysis of the}} different structures between hardware and software buffers, modified WCRT <b>calculation</b> methods of <b>messages</b> in hardware or software buffers were presented. Then the WCRT analysis of the whole system was obtained by consolidating the response time of node and bus. Results show that the response time calculated by the modified method is much longer than that calculated by the traditional WCRT. The modified method makes the WCRT analysis more coincident with the application of engineering, which helps the design and optimization of CAN based industrial bus...|$|R
40|$|This paper {{introduces}} {{a new approach}} to cost-effective, high-throughput hardware designs for Low Density Parity Check (LDPC) decoders. The proposed approach, called Non-Surjective Finite Alphabet Iterative Decoders (NS-FAIDs), exploits the robustness of message-passing LDPC decoders to inaccuracies in the <b>calculation</b> of exchanged <b>messages,</b> and it is shown to provide a unified framework for several designs previously proposed in the literature. NS-FAIDs are optimized by density evolution for regular and irregular LDPC codes, and are shown to provide different trade-offs between hardware complexity and decoding performance. Two hardware architectures targeting high-throughput applications are also proposed, integrating both Min-Sum (MS) and NS-FAID decoding kernels. ASIC post synthesis implementation results on 65 nm CMOS technology show that NS-FAIDs yield significant improvements in the throughput to area ratio, by up to 58. 75 % with respect to the MS decoder, with even better or only slightly degraded error correction performance. Comment: Submitted to IEEE Transactions on VLS...|$|R
40|$|In {{this paper}} {{the issue of}} {{improving}} the performance of iterative decoders based on sub-optimal <b>calculation</b> of the <b>messages</b> exchanged during iterations (L-values) is addressed. It {{is well known in}} the literature that a simple [...] -yet very effective [...] -way to improve the performance of suboptimal iterative decoders is based on applying a scaling factor to the L-values. In this paper, starting with a theoretical model based on the so-called consistency condition of a random variable, we propose a methodology for correcting the L-values that relies only on the distribution of the soft information exchanged in the iterative process. This methodology gives a clear explanation of why the well-known linear scaling factor provides a very good performance. Additionally, the proposed methodology allows us to avoid the exhaustive search required otherwise. Numerical simulations show that for turbo codes the scaling factors found closely follow the optimum values, which translates to a close-to-optimal BER performance. Moreover, for LDPC codes, the proposed methodology produces a better BER performance compared with the known method in the literature...|$|R
30|$|Earlier works {{had only}} {{considered}} the provenance, computational trust, topic or subtopic user roles and data importance issues in isolation while this work integrates the considerations into a unified approach. The approach presented in our paper is novel {{to the news}} reports description and modelling problem for the above consideration. The Q Learning approach adopted is unique to our work in the news reports modelling application. Our Record Merging approach and calculation of net trust value at a database relation site hosted at a node is novel. This has been defined based on records arriving from other network node sites together with the records already present at the site or on the new event influencing the record at the site. The record merging approach and net trust <b>calculation</b> of output <b>message</b> of a node is unique to the news reports description and modelling application. The derivation of macro action (s) and the computation of reward(s) associated with these actions are unique to this work in producing a reduced clustered space in the news reports modelling application. The approach described in merging of event or topic models for identifying similar or discerning threads of news reports or messages over incremental time periods is unique to our work.|$|R
40|$|Abstract The {{cryptographic}} hash function provides {{the services of}} information security, authentication, integrity, non-reputation in a branch of information secret. A {{cryptographic hash}} function has been developed since MD 4 was proposed by Rivest. In present, U. S standard of a hash function is SHA- 1 with 160 bits of output length. It is difficult {{to be sure of}} a security of a hash function with 160 bits of output length. In this paper, we propose a hash function, namely SHA-V, with variable output-length based on SHA- 1, HAVAL and HAS-V. The structure of SHA-V is two parallel lines, denoted as the Left-line and Right-line, consisting of 80 steps each and 3 -variable 4 Boolean functions each line. The input length is 1024 bits and the output length is from 128 bits to 320 bits by 32 -bit. SHA-V has the most advantage of SHA- 1. That is, the message variable creates in combination with input <b>message</b> and step <b>calculations.</b> This new <b>message</b> variable provides the resistance against most of attacks that search the collision resistance by the fabricating of input messages. When we compare SHA-V and HAS-V in side of operation, SHA-V is 10 % faster than HAS-V on a Pentium PC. Keyword Cryptographic hash functions�Cryptography�SHA- 1, Collision resistance, MD- 4, HAVAL 1...|$|R
40|$|This work {{addresses}} {{the new building}} cafe with a gallery {{of fine arts in}} the city Havířov, district of Karvina. Project documentation is processed in accordance with current applicable laws, regulations and standards. The building will be located {{at the edge of the}} land of park in the city centre. The property is located close communication of III. class and all utilities. In addition to the land is central car park with parking space for visitors and the main entrance to the main pedestrian road. This is a two-storey building on the first floor of an establishment cafes and sanitary facilities cafes. On the second floor there is a gallery spaces themselves with one living cell. Ground-plan of the building has an irregular shape, the outline dimensions are 18, 25 x 12, 00 m Basics is designed as a monolithic plain concrete with embedded arm-reinforcement. Vertical load-bearing envelopes are made from heat - insulating ceramic masonry. Ceilings are made of ceramic beam system with ceramic inserts MIAKO. Most of the floor space is composed of ceramic tiles. The building is covered monotube sloped flat roof insulation. The headliner is designed plasterboard. This work dealt with the preparatory work and study, detailed documentation, text <b>messages,</b> <b>calculations</b> of the thermal assessment, fire safety solutions and seminar work that {{addresses the}} problem of flat roofs...|$|R
40|$|Deep {{structured}} output learning shows {{great promise}} in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, {{in which we}} show how deep Convolutional Neural Networks (CNNs) {{can be used to}} estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for <b>message</b> <b>calculation.</b> This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation {{is the same as the}} number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73. 4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method. Comment: 11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015, Montreal, Canad...|$|R
40|$|The {{transport}} message security {{provided by}} vehicles in VANETs is quite important; vehicle message should be real-time {{and it will}} be not complicated to validate <b>message</b> <b>calculation.</b> The method proposed in the essay is mainly to validate the identity by means of Bilinear Diffie-Hellman method, and make vehicles validate the authenticity of RSU and TA’s identity and the effectiveness of key. RSU and TA only need to validate vehicle identity, without helping vehicles produce any key. When vehicle identity validation is completed, vehicles will produce public value and transmit it to other RSU and vehicles, while other vehicles could validate the identity through the message from the sender and public value from RSU. The advantages of the method proposed in this essay are listed as follows. (1) Vehicles, RSU, and TA can validate mutual identities and the effectiveness of keys. (2) Vehicles can produce public value functions automatically, thus reducing key control risks. (3) Vehicles do not need to show certificates to validate their identities, preventing the certificates from attacking because of long-term exposure. (4) Vehicles adopt a pseudonym ID challenge to validate their own identities during the process of handoff. (5) Vehicle messages can be validated using the Bilinear Diffie-Hellman (BDH) method without waiting for the RSU to validate messages, thus improving the instantaneity of messaging. The method proposed in the essay can satisfy source authentication, message integrity, nonrepudiation, privacy, and conditional untraceability requirements...|$|R
40|$|This work {{deals with}} the new family house for a four member family and {{detached}} design or architectural office {{in the town of}} Palkovice. Project documentation is processed in accordance with current applicable laws, regulations and standards. The building will be located on a separate building lot. The property is located close to III. class communication and all the necessary utilities. This is a two-storey building, on the first floor there are living room with kitchen, bedroom and sanitary facilities. There is also a shelter for car parking and a detached office with sanitary facilities. Ground-plan of the family house has the shape of two cantilevered perpendicular rectangles, whose outline dimensions are 18. 75 x 9. 00 m. The office building is shaped as a rectangle with dimensions of 8. 75 x 5. 75 m. The foundations are designed as monolithic from unreinforced concrete into the formwork foundation boards with embedded reinforcement. Vertical load-bearing structures are made of aerated concrete blocks. The vertical load-bearing structures of the shelter are made of galvanized steel elements and filling gabion walls. Ceilings are made of ceramic beam system with ceramic inserts HELUZ. Floors are made up of ceramic tiles and floating flooring. The building is covered with a flat roof single-shell thermal insulation. Above the shelter and the terrace there is a flat roof with a steel frame and wooden ceiling rafters. The ceiling is made up of ceiling boards Rheinzink. This work dealt with the preparatory and study work, executive documentation, text <b>message,</b> <b>calculations</b> of the thermal assessment, fire safety solutions and seminar work that addresses the problem of flat roofs...|$|R
40|$|Probabilistic {{reasoning}} methods, Bayesian networks (BNs) in particular, {{have emerged}} as an effective and central tool for reasoning under uncertainty. In a multi-agent environment, agents equipped with local knowledge often need to collaborate and reason about a larger uncertainty domain. Multiply sectioned Bayesian networks (MSBNs) provide a solution for the probabilistic reasoning of cooperative agents in such a setting. In this thesis, we first aim to improve the efficiency of current MSBN exact inference algorithms. We show that by exploiting the calculation schema and the semantic meaning of inter-agent messages, we can significantly reduce an agent 2 ̆ 7 s local computational cost {{as well as the}} inter-agent communication overhead. Our novel technical contributions include 1) a new message passing architecture based on an MSBN linked junction tree forest (LJF); 2) a suite of algorithms extended from our work in BNs to provide the semantic analysis of inter-agent messages; 3) a fast marginal calibration algorithm, designed for an LJF that guarantees exact results with a minimum local and global cost. We then investigate how to incorporate approximation techniques in the MSBN framework. We present a novel local adaptive importance sampler (LLAIS) designed to apply localized stochastic sampling while maintaining the LJF structure. The LLAIS sampler provides accurate estimations for local posterior beliefs and promotes efficient <b>calculation</b> of inter-agent <b>messages.</b> We also address the problem of online monitoring for cooperative agents. As the MSBN model is restricted to static domains, we introduce an MA-DBN model based on a combination of the MSBN and dynamic Bayesian network (DBN) models. We show that effective multi-agent online monitoring with bounded error is possible in an MA-DBN through a new secondary inference structure and a factorized representation of forward messages...|$|R
40|$|The {{research}} made {{according to}} the writer interest to a theme of a movie made by Winaldha E. Melalatoa titled “Drop Out”. The movie focused on sex education in adolescence. It could {{be seen from the}} cover visualization. The cover stated “Sex Education”, and the movie also involved Dr. Boyke whose profession is well-known sexologist. Sex education is one way to reduce and stop sex misapplication, especially unwanted pregnancy, sexual disease, depression, and guilty feeling. Sex education is a knowledge about adolescent sexuality which given in complete and open way so that teenager able to create good sex behavior. Sex education focused on: understanding about sexual difference between men and women in family, work, and life which always change and different in all society and culture, creating understanding about sex role. The movie presented a figure named James (played by Ben Joshua). He was a psychology Department students in university who hadn’t graduated from his college though his friends had passed him to graduation. His outside work was making a guiding book about making love style. One day, there was a new lecturer named Lea (played by Titi Kamal). She was always pursued by her family to get marry. But she was in doubt to talk about sex. One day, she met James and asked her to teach her about sex. The movie presented sex whether in verbal or non-verbal. The research included into quantitative descriptive research. Method used was contain analysis. Contain analysis framework made to create objective <b>calculation</b> about real <b>message</b> (manifest content of messages). That’s why contain analysis in the research included analysis to denotative sign. The research showed that the appearance of sex education elements in “Drop Out” movie was: (1) people norms 21, 42...|$|R

