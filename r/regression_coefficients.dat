4833|3882|Public
25|$|A test of <b>regression</b> <b>coefficients</b> in {{published}} papers showed {{agreement with}} Benford's law. As a comparison group {{subjects were asked}} to fabricate statistical estimates. The fabricated results failed to obey Benford's law.|$|E
25|$|As {{an attempt}} to correct some of the error due to a non-zero , the usage of local linear {{weighted}} regression with ABC to reduce the variance of the posterior estimates has been suggested. The method assigns weights to the parameters according to how well simulated summaries adhere to the observed ones and performs linear regression between the summaries and the weighted parameters {{in the vicinity of}} observed summaries. The obtained <b>regression</b> <b>coefficients</b> are used to correct sampled parameters in the direction of observed summaries. An improvement was suggested in the form of nonlinear regression using a feed-forward neural network model. However, {{it has been shown that}} the posterior distributions obtained with these approaches are not always consistent with the prior distribution, which did lead to a reformulation of the regression adjustment that respects the prior distribution.|$|E
25|$|In {{statistic}}s, the Durbin–Watson statistic is a {{test statistic}} used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Later, John Denis Sargan and Alok Bhargava developed several von Neumann–Durbin–Watson type test statistics for the null hypothesis that the errors on a regression model follow a process with a unit root against the alternative hypothesis that the errors follow a stationary first order autoregression (Sargan and Bhargava, 1983). Note that the distribution of this test statistic {{does not depend on}} the estimated <b>regression</b> <b>coefficients</b> and the variance of the errors.|$|E
5000|$|Σx2, Σx, n, Σy2, Σy, Σxy, , xσn, xσn-1, , yσn, yσn-1, <b>Regression</b> <b>{{coefficient}}</b> A, <b>Regression</b> <b>coefficient</b> B, Correlation coefficient r, , , Σx3, Σx2y, Σx4, <b>Regression</b> <b>coefficient</b> C, 1and 2 ...|$|R
3000|$|Where β′ {{represents}} the <b>regression</b> <b>coefficient</b> of the time-varying independent variable x for individual i at time t. Analogously, γ′ {{stands for the}} <b>regression</b> <b>coefficient</b> of the time-constant [...]...|$|R
40|$|The partial <b>regression</b> <b>coefficient</b> is {{also called}} <b>regression</b> <b>coefficient,</b> <b>regression</b> weight, partial <b>regression</b> weight, slope <b>coefficient</b> or partial slope coefficient. It {{is used in}} the context of {{multiple}} linear regression (mlr) analysis and gives the amount by which the dependent variable (DV) increases when on...|$|R
2500|$|Bayesian linear {{regression}} applies {{the framework of}} Bayesian statistics to {{linear regression}}. (See also Bayesian multivariate linear regression.) In particular, the <b>regression</b> <b>coefficients</b> β {{are assumed to be}} random variables with a specified prior distribution. [...] The prior distribution can bias the solutions for the <b>regression</b> <b>coefficients,</b> in a way similar to (but more general than) ridge regression or lasso regression. [...] In addition, the Bayesian estimation process produces not a single point estimate for the [...] "best" [...] values of the <b>regression</b> <b>coefficients</b> but an entire posterior distribution, completely describing the uncertainty surrounding the quantity. [...] This can be used to estimate the [...] "best" [...] coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.|$|E
2500|$|An {{additional}} set {{of cases}} occurs in Bayesian linear regression, {{where in the}} basic model the data {{is assumed to be}} normally distributed, and normal priors are placed on the <b>regression</b> <b>coefficients.</b> [...] The resulting analysis is similar to the basic cases of independent identically distributed data, but more complex.|$|E
2500|$|Although serial {{correlation}} {{does not affect}} the consistency of [...] the estimated <b>regression</b> <b>coefficients,</b> it does affect our ability to conduct valid statistical tests. First, the F-statistic to test for overall significance of the regression may be inflated under positive {{serial correlation}} because the mean squared error (MSE) will tend to underestimate the population error variance. Second, positive serial correlation typically causes the ordinary least squares (OLS) standard errors for the <b>regression</b> <b>coefficients</b> to underestimate the true standard errors. As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small standard errors for the regression coefficient. These small standard errors will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the standard errors were correctly estimated.|$|E
3000|$|The {{approach}} can be summarized in four steps. First, I estimate a <b>regression</b> <b>coefficient</b> for every pair of (standardized) log daily wages of the same worker i in two different years. This estimation is carried out only for uncensored wage observations. I label this <b>regression</b> <b>coefficient</b> λ̂^*. 16 [...]...|$|R
40|$|Samsat (Sistem Pelayanan Satu Atap) Office Bandar Lampung {{is one of}} the {{government}} agencies that provide services to the public /community relating to the payment of taxes on motor vehicles. Samsat Office Bandar Lampung serve the average number BBNKB for two-wheeled vehicles as much as 5057 units per month. While {{the number of people who}} renew PKB to launch an average of 8959 vehicles per month. For the average car as much as 442 per month for the renew of BBNKB and 4289 for payment of PKB. The independent variable of responsiveness is the most influential variable with a <b>regression</b> <b>coefficient</b> of 0. 446 and a further variable respectively are under the influence of the independent variables to the <b>regression</b> <b>coefficient</b> guarantees for 0. 373, the physical appearance of the independent variables with the <b>regression</b> <b>coefficient</b> of 0. 237, the independent variable of empathy with the <b>regression</b> <b>coefficient</b> of 0. 203 and the independent variables to the <b>regression</b> <b>coefficient</b> reliability of 0. 129 who expressed a positive effect on the quality of services to the satisfaction of the community can be accepted...|$|R
30|$|Figure  8 {{depicts the}} {{graphical}} relationship of climatic variable and vegetation. Regression analysis between temperature and vegetation shows a moderate <b>regression</b> <b>coefficient</b> value of 0.6854 (Fig.  8 a) and exhibited a significant relationship. Similarly, Regression analysis between rainfall and vegetation shows a low <b>regression</b> <b>coefficient</b> value of 0.0027 (Fig.  8 b).|$|R
2500|$|... is a (p+1)-dimensional {{parameter}} vector, where [...] is {{the constant}} (offset) term. Its elements are also called effects, and {{the estimates of}} it are called [...] "estimated effects" [...] or <b>regression</b> <b>coefficients.</b> Statistical estimation and inference in linear regression focuses on β. The elements of this parameter vector are interpreted as the partial derivatives of the dependent variable {{with respect to the}} various independent variables.|$|E
2500|$|Confidence {{intervals}} and hypothesis {{tests are}} two statistical procedures {{in which the}} quantiles of the sampling distribution of a particular statistic (e.g. the standard score) are required. [...] In any situation where this statistic is a linear function of the data, divided by the usual estimate of the standard deviation, the resulting quantity can be rescaled and centered to follow Student's t-distribution. [...] Statistical analyses involving means, weighted means, and <b>regression</b> <b>coefficients</b> all lead to statistics having this form.|$|E
2500|$|Linearity. [...] This {{means that}} {{the mean of the}} {{response}} variable is a linear combination of the parameters (<b>regression</b> <b>coefficients)</b> and the predictor variables. [...] Note that this assumption is much less restrictive than it may at first seem. [...] Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. [...] The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. [...] This trick is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method. [...] In fact, models such as polynomial regression are often [...] "too powerful", in that they tend to overfit the data. [...] As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process. [...] Common examples are ridge regression and lasso regression. [...] Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as [...] special cases of Bayesian linear regression, with particular types of prior distributions placed on the <b>regression</b> <b>coefficients.)</b> ...|$|E
40|$|AbstractThis paper {{studies the}} exact {{distributions}} of the MLEs of the <b>regression</b> <b>coefficient</b> matrices in a GMANOVA–MANOVA model with normal error. The unique conditions for linear {{functions of the}} MLEs of <b>regression</b> <b>coefficient</b> matrices are presented, and the exact density functions or characteristic functions for these linear functions are derived...|$|R
2500|$|... of the <b>regression</b> <b>coefficient</b> of the lagged {{dependent}} variable, provided ...|$|R
30|$|Figure  6 {{depicts the}} {{graphical}} representation of climatic variables and vegetation obtained through NDVI. Regression analysis between temperature and vegetation shows a moderate <b>regression</b> <b>coefficient</b> value of 0.5751 (Fig.  6 a) and exhibited a significant relationship. Similarly, regression analysis between rainfall and vegetation shows low <b>regression</b> <b>coefficient</b> value of 0.3459 (Fig.  6 b).|$|R
2500|$|One of {{the prime}} {{differences}} between Lasso and ridge regression is that in ridge regression, as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause {{more and more of}} the parameters to be driven to zero. This is an advantage of Lasso over ridge regression, as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas Ridge regression never fully discards any features. Some feature selection techniques are developed based on the LASSO including Bolasso which bootstraps samples, [...] and FeaLect which analyzes the <b>regression</b> <b>coefficients</b> corresponding to different values of [...] to score all the features.|$|E
2500|$|Lack {{of perfect}} {{multicollinearity}} in the predictors. [...] For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise, {{we have a}} condition known as perfect multicollinearity in the predictor variables. [...] This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared {{to the number of}} parameters to be estimated (e.g. fewer data points than <b>regression</b> <b>coefficients).</b> In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. [...] At most {{we will be able to}} identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression. [...] Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as [...] "effect sparsity"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.|$|E
2500|$|Karl Pearson was {{important}} in {{the founding of the}} school of biometrics, which was a competing theory to describe evolution and population inheritance {{at the turn of the}} 20th century. His series of eighteen papers, [...] "Mathematical Contributions to the Theory of Evolution" [...] established him as the founder of the biometrical school for inheritance. In fact, Pearson devoted much time during 1893 to 1904 to developing statistical techniques for biometry. These techniques, which are widely used today for statistical analysis, include the chi-squared test, standard deviation, and correlation and <b>regression</b> <b>coefficients.</b> Pearson's Law of Ancestral Heredity stated that germ plasm consisted of heritable elements inherited from the parents as well as from more distant ancestors, the proportion of which varied for different traits. Karl Pearson was a follower of Galton, and although the two differed in some respects, Pearson used a substantial amount of Francis Galton's statistical concepts in his formulation of the biometrical school for inheritance, such as the law of regression. The biometric school, unlike the Mendelians, focused not on providing a mechanism for inheritance, but rather on providing a mathematical description for inheritance that was not causal in nature. While Galton proposed a discontinuous theory of evolution, in which species would have to change via large jumps rather than small changes that built up over time, Pearson pointed out flaws in Galton's argument and actually used Galton's ideas to further a continuous theory of evolution, whereas the Mendelian's favored a discontinuous theory of evolution. While Galton focused primarily on the application of statistical methods to the study of heredity, Pearson and his colleague Weldon expanded statistical reasoning to the fields of inheritance, variation, correlation, and natural and sexual selection.|$|E
5000|$|... is the <b>regression</b> <b>coefficient</b> {{multiplied by}} some {{value of the}} predictor.|$|R
30|$|Figure  7 {{depicts the}} {{graphical}} relationship between climatic variable and forest cover. Regression analysis between temperature and forest cover shows a low <b>regression</b> <b>coefficient</b> value of 0.1206 (Fig.  7 a). Similarly, Regression analysis between rainfall and forest cover shows a high <b>regression</b> <b>coefficient</b> value of 0.8417 (Fig.  7 b) and exhibited a significant relationship.|$|R
40|$|Objective The {{purpose of}} this study was to report the {{adjusted}} effect sizes of mid trimester sonographic findings that are associated with Down syndrome in a sonographically screened population. Study design A large prospective single-center cohort study was conducted between March 1993 and December 2002 in South-East Queensland with women who were first scanned between 15 to 22 weeks of gestation. Univariate and multivariable logistic regression modeling was used to relate karyotypically ascertained Down syndrome fetuses and their control counterparts against routinely collected demographic and sonographic findings. Results Data were available for 73 Down-affected and 16, 891 unaffected pregnancies. Strong colinearity existed between short humerus and short femurs that necessitated the removal of FL in pursuant multivariable models. In the most parsimonious model, which was adjusted for maternal age and gestational age, pregnancies with thick nuchal skinfold (<b>regression</b> <b>coefficient</b> β [± SE], 2. 100 ± 0. 545), short humerus length (<b>regression</b> <b>coefficient</b> β, 2. 304 ± 0. 314), presence of echogenic bowel (<b>regression</b> <b>coefficient</b> β, 1. 602 ± 0. 412), presence of echogenic intracardiac focus (<b>regression</b> <b>coefficient</b> β, 1. 975 ± 0. 308), presence of renal pelvic dilation (<b>regression</b> <b>coefficient</b> β, 1. 281 ± 0. 420), presence of aneuploid associated anomalies (<b>regression</b> <b>coefficient</b> β, 4. 473 ± 0. 535), the interaction between gestational age and thick nuchal skinfold (<b>regression</b> <b>coefficient</b> β, 0. 465 ± 0. 210), and the interaction between short humerus length and the presence of aneuploid associated anomalies (<b>regression</b> <b>coefficient</b> β, − 1. 693 ± 0. 811) all were associated significantly with Down syndrome risk (all P <. 05). Adjusted relative risk estimates were substantially different from their crude estimates. Conclusion Routinely collected mid trimester sonographic findings are associated significantly with Down syndrome risk in a sonographically screened population after accounting for maternal age and gestational age. Because of dependencies between ultrasonic findings, risk estimates should be derived from appropriate multivariable models...|$|R
5000|$|... is the (unknown) <b>regression</b> <b>coefficients</b> of {{the design}} factors.|$|E
5000|$|F {{test for}} equality/inequality of the <b>regression</b> <b>coefficients</b> in Multiple Regression; ...|$|E
5000|$|Tests of Significance of Means, Difference of Means, and <b>Regression</b> <b>Coefficients</b> ...|$|E
5000|$|... = <b>regression</b> <b>coefficient</b> {{from the}} ith {{independent}} {{variable in the}} full model ...|$|R
5000|$|The {{equation}} for {{calculating the}} FibroTest score <b>regression</b> <b>coefficient</b> (logistic <b>regression)</b> is: ...|$|R
30|$|In this paper, under a {{generalized}} balanced loss function, we study the admissibility of linear estimators of the <b>regression</b> <b>coefficient</b> in general Gauss-Markov model {{with respect to}} an inequality constraint. The necessary and sufficient conditions that the linear estimators of <b>regression</b> <b>coefficient</b> function are admissible are obtained, {{in the class of}} homogeneous and inhomogeneous linear estimation, respectively.|$|R
50|$|The matrix Σ12Σ22−1 {{is known}} as the matrix of <b>regression</b> <b>coefficients.</b>|$|E
5000|$|A1 and A2 are <b>regression</b> <b>coefficients</b> (indicating {{the slope}} of the line segments); ...|$|E
5000|$|The vector of {{estimated}} polynomial <b>regression</b> <b>coefficients</b> (using ordinary {{least squares}} estimation) is ...|$|E
50|$|Although several {{statistical}} packages (e.g., SPSS, SAS) {{report the}} Wald statistic {{to assess the}} contribution of individual predictors, the Wald statistic has limitations. When the <b>regression</b> <b>coefficient</b> is large, the standard error of the <b>regression</b> <b>coefficient</b> also tends to be large increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse.|$|R
40|$|This paper {{studies the}} exact {{distributions}} of the MLEs of the <b>regression</b> <b>coefficient</b> matrices in a GMANOVA-MANOVA model with normal error. The unique conditions for linear {{functions of the}} MLEs of <b>regression</b> <b>coefficient</b> matrices are presented, and the exact density functions or characteristic functions for these linear functions are derived. GMANOVA-MANOVA model MLE Linear function Distribution Hypergeometric function...|$|R
40|$|This study aims to {{determine}} how much influence lending rates, and gross domestic product of the banking credit to the general government bank in Indonesia in 2002 - 2011. This study uses a quantitative method, and analyzed using multiple linear regression analysis using a computer application SPSS 20 for windows. In this study, the independent variables namely Lending Rates (X 1), Gross Domestic Product (X 2), while the dependent variable is Lending Banking (Y). Variable interest rates on working capital loans (X 1) has a negative <b>regression</b> <b>coefficient</b> of - 0. 004 to total working capital loans. Variable interest rates on investment loans (X 1) has a positive <b>regression</b> <b>coefficient</b> of - 0. 001 to total loan investments. Variable interest rates on consumer credit (X 1) has a negative <b>regression</b> <b>coefficient</b> of - 0. 064 to total consumer loans. Variable gross domestic product (X 2) has a positive <b>regression</b> <b>coefficient</b> for 7, 615 E- 007 on the growth of working capital loans. Variable gross domestic product (X 2) has a positive <b>regression</b> <b>coefficient</b> for 5, 094 E- 007 on the growth of investment credit. Variable gross domestic product (X 2) has a positive <b>regression</b> <b>coefficient</b> for 6, 499 E- 007 on the growth of consumer credit. It can be concluded that the GDP is positive and significant effect on lending. Simultaneously, the variable lending rates and gross domestic product {{have a significant effect on}} bank lending of commercial banks in the Indonesian government...|$|R
