296|0|Public
25|$|Paired-Associate Judgment of Learning: These {{judgments}} are {{made at the}} time of study on cue-target pairs and are responsible for predicting later memory performance (on cued recall or cued recognition). One example of paired-associate JOLs is the cue-target JOL, where the subject determines the <b>retrievability</b> of the target when both the cue and target of the to-be-learned pair are presented. Another example is the cue-only JOL, where the subject must determine the <b>retrievability</b> of the target when only the cue is presented {{at the time of}} judgment. These two types of JOLs differ in their accuracy in predicting future performance, and delayed judgments tend to be more accurate.|$|E
50|$|<b>Retrievability</b> can be {{considered}} as one aspect of findability.|$|E
50|$|A {{document}} (or information object) {{has high}} <b>retrievability</b> {{if there are}} many queries which retrieve the document via the search engine, and the document is ranked sufficiently high that a user would encounter the document. Conversely, if there are few queries that retrieve the document, or when the document is retrieved the documents are not high enough in the ranked list, then the document has low <b>retrievability.</b>|$|E
50|$|<b>Retrievability</b> {{is a term}} {{associated}} with {{the ease with which}} information can be found or retrieved using an information system, specifically a search engine or information retrieval system.|$|E
50|$|Applications of <b>retrievability</b> include {{detecting}} {{search engine}} bias, measuring algorithmic bias, evaluating {{the influence of}} search technology, tuning information retrieval systems and {{evaluating the quality of}} documents in a collection.|$|E
50|$|Paired-Associate Judgment of Learning: These {{judgments}} are {{made at the}} time of study on cue-target pairs and are responsible for predicting later memory performance (on cued recall or cued recognition). One example of paired-associate JOLs is the cue-target JOL, where the subject determines the <b>retrievability</b> of the target when both the cue and target of the to-be-learned pair are presented. Another example is the cue-only JOL, where the subject must determine the <b>retrievability</b> of the target when only the cue is presented {{at the time of}} judgment. These two types of JOLs differ in their accuracy in predicting future performance, and delayed judgments tend to be more accurate.|$|E
50|$|In {{order to}} {{evaluate}} how easily {{information can be}} found by searching a site using a search engine or information retrieval system, <b>retrievability</b> measures were developed, and similarly, navigability measures now measure ease of information access through browsing a site (e.g. PageRank, MNav, InfoScent (see Information Foraging), etc.).|$|E
50|$|Proofs {{of space}} {{could be used}} as an {{alternative}} to proofs of work in the traditional client puzzle applications such as anti-spam measures and denial of service attack prevention. The difference is that the pricing functions used are chosen so that the user has to spend a certain amount of space. In May 2014, the idea for Permacoin was published. Permacoin uses Proofs of <b>Retrievability</b> (PoR), where the miners have monetary incentive to store useful public data in the network. This data can then be later retrieved by other users, similar to a P2P torrenting service. Permacoin's PoR does not solve the problem of perpetual computation, since it requires the miner to compute many proofs of <b>retrievability.</b> PoSpace has been used in the open source Burstcoin cryptocurrency founded in August 2014. Burstcoin claims to have a green algorithm that favors smaller miners by design, making transaction costs cheaper and the network more decentralized. In 2015 a paper proposing a cryptocurrency called SpaceMint modified PoW to address some issues such as the nothing-at-stake problem caused by the cheapness of mining. SpaceMint also improved the time and memory efficiency for block mining and verification.|$|E
50|$|Roediger's early {{research}} on testing effects and hypermnesia on final-exam {{results showed that}} subjects who receive two tests on newly learned material out-perform subjects tested only once, even if no feedback is given {{on any of the}} tests. This effect persists even if the group that is only tested once is given a second opportunity to study the material. Roediger explains this effect in terms of enhanced <b>retrievability,</b> claiming that testing provides practice at retrieving memories, making the memory itself stronger.|$|E
5000|$|Hach PORs (proofs of <b>retrievability</b> {{for large}} files) {{is based on}} a {{symmetric}} cryptographic system, where there is only one verification key that must be stored in a file to improve its integrity. This method serves to encrypt a file F and then generate a random string named [...] "sentinel" [...] that must be added {{at the end of the}} encrypted file. The server cannot locate the sentinel, which is impossible differentiate from other blocks, so a small change would indicate whether the file has been changed or not.|$|E
5000|$|To test {{episodic memory}} {{researchers}} usually use items {{that can be}} better related to normal everyday life, such as sentences. [...] Language recognition depends somewhat on the <b>retrievability</b> of meaning, but the extent of this dependence is unknown. Retrieval of memories is language-specific, it matches the language spoken at the time. Depending on what language is used what is recalled may be different because a cue can activate many meanings, it is the context that conditions what meaning is considered first, and context can change over language and cultures.|$|E
50|$|Adaptive Phased Management {{is both a}} {{technical}} method and a management system, {{with an emphasis on}} adaptability. Technically, it is centralized containment and isolation of used nuclear fuel in a deep geological repository. The management system involves manageable phases - each marked by explicit decision points with continuing participation by interested Canadians. It allows for go, no-go decisions at each stage to take advantage of new knowledge or changing societal priorities. Adaptive Phased Management provides an option for shallow underground storage at the central site if {{some or all of the}} used fuel needs to be moved before the deep repository is available. It also provides for continuous monitoring throughout implementation and for <b>retrievability</b> for an extended period.|$|E
5000|$|During treatment, conservators also {{document}} the materials they used, reactions that occurred, information on reversibility, etc. Much {{of this information}} can also be summarized in a laboratory master report, which records information {{that may not be}} included in final reports, such as reasoning for methods, factors that determine choice of treatment and changes in methodology, material safety data sheets, etc. [...] Final reports can contain this information as well but in less technical detail. A final report consists of [...] "after" [...] photographs, a summary of the work completed compared to the goals of treatment, and the conservator's contact information in case future conservators working on the object have questions about the previous work completed. Any documentation of conservation treatments is also stored in the object's permanent file for ease of accessibility and <b>retrievability.</b>|$|E
5000|$|Skimming is {{a process}} of speed reading that {{involves}} visually searching the sentences of a page for clues to meaning. Or when reading an essay, it can mean reading the beginning and ending for summary information, then optionally the first sentence of each paragraph to quickly determine whether to seek still more detail, as determined by the questions or purpose of the reading. [...] For some people, this comes naturally, but is usually acquired by practice. Skimming is usually seen more in adults than in children. It is conducted at a higher rate (700 words per minute and above) than normal reading for comprehension (around 200-230 wpm), and results in lower comprehension rates, especially with information-rich reading material. Scanning is the process where one actively looks for information using a mind-map (organizing information in a visually hierarchical manner that showcases the interrelatedness of the information for better <b>retrievability)</b> formed from skimming. These techniques are used by meta-guiding your eyes.|$|E
50|$|A {{large body}} of recent work has {{suggested}} {{that people who are}} more grateful have higher levels of subjective well-being. Grateful people are happier, less depressed, less stressed, and more satisfied with their lives and social relationships. Specifically, in terms of depression, gratitude may serve as a buffer by enhancing the coding and <b>retrievability</b> of positive experiences. Grateful people also have higher levels of control of their environments, personal growth, purpose in life, and self acceptance. Grateful people have more positive ways of coping with the difficulties they experience in life, being more likely to seek support from other people, reinterpret and grow from experiences, and spend more time planning {{how to deal with the}} problem. Grateful people also have less negative coping strategies, being less likely to try to avoid the problem, deny there is a problem, blame themselves, or cope through substance use. Grateful people sleep better, and this seems to be because they think less negative and more positive thoughts just before going to sleep.|$|E
5000|$|Although {{this theory}} has many {{experiments}} backing up its reliability, many researchers are questioning {{the levels of}} processing that TAP seems to fall into. The levels of processing have been under speculation {{for the fact that}} they seem untestable and unfalsifiable. They argue that these processing effects are [...] "circular" [...] in the sense that deep processing can be considered as just better remembering. They believe that much of the questionability of the processing effects lies between the encoding specificity principle and TAP. The researchers argue that these processing systems function much like Darwin's natural selection theory in that the [...] "fitness" [...] of a species and the [...] "depth of processing" [...] in the levels of processing cannot fully predict the final outcome, meaning the survival and <b>retrievability</b> of the species or the information processed. They have found that TAP is still vulnerable to this same type of circularity because it lacks a precise and definite definition. Basically, TAP can only be identified as happening only AFTER retrieval has occurred. Roediger and Gallo argue that after 30 years of research, they still cannot identify why or how we get the typical levels-of-processing effect. However, they still believe that even with these doubts that memory retrieval can be studied and subjected to experiments with [...] "specified" [...] retrieval conditions. Therefore, the levels-of-processing effect that TAP falls under supports that the [...] "greater survival" [...] of deep processing most likely occurs, which means that if they had any doubts about transfer-appropriate processing, they should consider the fact that retrieval has more of a range than a semantic processing theory would support, and more than likely thrive and survive.|$|E
50|$|In Sweden {{there is}} a growing {{interest}} in open publication and the sharing of educational resources but the pace of development is still slow. There are many questions to be dealt with in this area; for universities, academic management and teaching staff. Teachers in all educational sectors require support and guidance {{to be able to use}} OER pedagogically and with quality in focus. To realize the full potential of OER for students' learning it is not enough to make patchwork use of OER resources have to be put into context. Valuable teacher time should be used for contextual work and not simply for the creation of content.The aim of the project OER for learning OERSweden, is to stimulate an open discussion about collaboration in infrastructural questions regarding open online knowledge sharing. A network of ten universities led by Karlstad University will arrange a series of open webinars during the project period focusing on the use and production of open educational resources. A virtual platform for Swedish OER initiatives and resources will also be developed. The project intends to focus in particular on how OER affects teacher trainers and decision makers. The objectives of the project are: To increase the level of national collaboration between universities and educational organisations in the use and production of OER, To find effective online methods to support teachers and students, in terms of quality, technology and <b>retrievability</b> of OER, To raise awareness for the potential of webinars as a tool for open online learning, To increase the level of collaboration between universities' support functions and foster national resource sharing, with a base in modern library and educational technology units, and To contribute to the creation of a national university structure for tagging, distribution and storage of OER.|$|E
50|$|Permacoin is {{a system}} that aims to recycle some of the {{computing}} efforts in Bitcoin by making miners store archival data for later retrieval. Permacoin could be either built on top of Bitcoin or implemented as a separate Altcoin. The objective of Permacoin is to incentivize modifications of Bitcoin to achieve more sustainable, useful and efficient computing. The process of mining Permacoin involves constructing Proofs of <b>Retrievability</b> (POR) which show to the network that the files stored by the miners can be accessed at a later time. In a POR the prover demonstrates that he holds a file without having to send the entire file by responding to random challenges issued by a verifier. Permacoin proposes a couple of ideas to avoid users on the network outsourcing mining to cloud hosting companies, which would create unwanted centralization. The first idea is to tie the block reward winner's private key to the puzzle solution so that to outsource the mining, the user would have to send the cloud host his private key. A portion of the users would refuse to do this, concerned with the protection of their private key. A variant of the Permacoin PoR scheme also allows for zero-knowledge SNARKs which allow for servers to steal rewards without leaving evidence and therefore without risking damage to their reputation, which further deincentivizes key sharing without adding any overhead to the puzzle-solving scheme. Even with a local private key, the user can still outsource the storage of their assigned blocks, and fetch them as needed to perform computations on them. To deal with this, these computations are designed {{to be related to the}} puzzle solution. A delay in this phase lowers the chances of solving the puzzle and receiving the reward. So the requests for blocks happen unpredictably, creating a larger bandwidth latency overhead. Successfully mining a block may require many block fetches, and if the parameters are properly adjusted, the block fetching delay might make it completely impractical to outsource mining because the time taken by the fetches will be larger than the duration of a mining epoch. For such a system to work using the local storage of users, it is important to have some redundancy, so that the files stored by one user are lost forever in case he decides to stop mining or if the network is somehow attacked to block compromise a certain file. If the whole Bitcoin network dedicated it's hashing power and invested on SSD drives instead of ASICs to build a distributed file storage scheme, it is estimated that the network storage capacity could reach several petabytes. An important part of having a distributed storage scheme is supporting the possibility of updating the content, adding to it or removing obsolete data. This also means that there needs to be a way to decide which data should be stored and when data needs to be removed. This could be done by a trusted authority, or perhaps more interestingly by a majority vote. Since the idea is for the data to be publicly available, there is also a concern with privacy, and so users who wish to append data to the network might want to first encrypt it. The Permacoin protocols also do not cover the necessity for file distribution, although the authors of the proposal mention that it might be done voluntarily by an altruistic group of users.|$|E
40|$|<b>Retrievability</b> is an {{important}} and interesting indicator {{that can be used}} in a number of ways to analyse Information Retrieval systems and document collections. Rather than focusing totally on relevance, <b>retrievability</b> examines what is retrieved, how often it is retrieved, and whether a user is likely to retrieve a document or not. This is important because a document needs to be retrieved, before it can be judged for relevance. In this tutorial, we shall explain the concept of <b>retrievability</b> along with a number of <b>retrievability</b> measures, how it can be estimated and how it can be used for analysis. Since retrieval precedes relevance, we shall also provide an overview of how <b>retrievability</b> relates to effectiveness - describing some of the insights that researchers have discovered so far. We shall also show how <b>retrievability</b> relates to efficiency, and how the theory of <b>retrievability</b> can be used to improve both effectiveness and efficiency. Then we shall provide an overview of the different applications of <b>retrievability</b> such as Search Engine Bias, Corpus Profiling, etc., before wrapping up with challenges and opportunities. The final session of the day will look at example problems and ways to analyse and apply <b>retrievability</b> to other problems and domains...|$|E
40|$|<b>Retrievability</b> is an {{independent}} evaluation measure that offers insights to an aspect of retrieval systems that performance and efficiency measures do not. <b>Retrievability</b> {{is often used to}} calculate the <b>retrievability</b> bias, an indication of how accessible a system makes all the documents in a collection. Generally, computing the <b>retrievability</b> bias of a system requires a colossal number of queries to be issued for the system to gain an accurate estimate of the bias. However, it is often the case that the accuracy of the estimate is not of importance, but the relationship between the estimate of bias and performance when tuning a systems parameters. As such, reaching a stable estimation of bias for the system is more important than getting very accurate <b>retrievability</b> scores for individual documents. This work explores the idea of using topical subsets of the collection for query generation and bias estimation to form a local estimate of bias which correlates with the global estimate of <b>retrievability</b> bias. By using topical subsets, {{it would be possible to}} reduce the volume of queries required to reach an accurate estimate of <b>retrievability</b> bias, reducing the time and resources required to perform a <b>retrievability</b> analysis. Findings suggest that this is a viable approach to estimating <b>retrievability</b> bias and that the number of queries required can be reduced to less than a quarter of what was previously thought necessary...|$|E
40|$|Bias in the {{retrieval}} of documents can directly influence the information access of a digital library. In the worst case, systematic favoritism {{for a certain}} type of document can render other parts of the collection invisible to users. This potential bias can be evaluated by measuring the <b>retrievability</b> for all documents in a collection. Previous evaluations have been performed on TREC collections using simulated query sets. The question remains, however, how representative this approach is of more realistic settings. To address this question, we investigate the effectiveness of the <b>retrievability</b> measure using a large digitized newspaper corpus, featuring two characteristics that distinguishes our experiments from previous studies: (1) compared to TREC collections, our collection contains noise originating from OCR processing, historical spelling and use of language; and (2) instead of simulated queries, the collection comes with real user query logs including click data. First, we assess the <b>retrievability</b> bias imposed on the newspaper collection by different IR models. We assess the <b>retrievability</b> measure and confirm its ability to capture the <b>retrievability</b> bias in our setup. Second, we show how simulated queries differ from real user queries regarding term frequency and prevalence of named entities, and how this affects the <b>retrievability</b> results...|$|E
40|$|Abstract. With {{increasing}} {{volumes of}} data, much {{effort has been}} devoted to finding the most suitable answer to an information need. However, in many domains, the question whether any specific information item can be found at all via a reasonable set of queries is essential. This concept of <b>Retrievability</b> of information has evolved into an important evaluation measure of IR systems in recall-oriented application domains. While several studies evaluated retrieval bias in systems, solid validation of the impact of retrieval bias and the development of methods to counter low <b>retrievability</b> of certain document types would be desirable. This paper provides an in-depth study of <b>retrievability</b> characteristics over queries of different length in a large benchmark corpus, validating previous studies. It analyzes the possibility of automatically categorizing documents into low and high retrievable documents based on document properties rather than complex <b>retrievability</b> analysis. We furthermore show, that this classification can be used to improve overall <b>retrievability</b> of documents by treating these classes as separate document corpora, combining individual retrieval results. Experiments are validated on 1. 2 million patents of the TREC Chemical Retrieval Track. ...|$|E
40|$|Modern {{technologies}} such as cloud computing, grid computing and {{software as a service}} all require data to be stored by the third parties. A specific problem encountered in this context is to convince a verifier that a user 2 ̆ 7 s data are kept intact at the storage servers. An important approach to achieve this goal is called proof of <b>retrievability,</b> by which a storage server can assure a verifier via a concise proof that a user 2 ̆ 7 s file is available. However, for most publicly verifiable systems, existing proof of <b>retrievability</b> solutions do not take physical attacks into consideration, where an adversary can observe the outcome of the computation with methods like fault injection techniques. In fact, the authors find that giving the adversary the ability to obtain the information about the relations between the private keys, those systems are not secure anymore. Motivated by the need of preventing this kind of attacks, they present the security model for related-key attacks in publicly verifiable proofs of <b>retrievability,</b> where the adversary can subsequently observe the outcome of the publicly verifiable proof of <b>retrievability</b> under the modified key. After pointing out a linear related-key attack on an existing proof of <b>retrievability</b> system with public verifiability, they present a secure and efficient proof of <b>retrievability</b> with public verifiability, against related-key attacks...|$|E
40|$|Bias {{quantification}} of retrieval functions {{with the}} help of document <b>retrievability</b> scores has recently evolved as an important evaluation measure for recall-oriented retrieval applications. While numerous studies have evaluated retrieval bias of retrieval functions, solid validation of its impact on realistic types of queries is still limited. This is {{due to the lack of}} well-accepted criteria for query generation for estimating <b>retrievability.</b> Commonly, random queries are used for approximating documents <b>retrievability</b> due to the prohibitively large query space and time involved in processing all queries. Additionally, a cumulative <b>retrievability</b> score of documents over all queries is used for analyzing retrieval functions (retrieval) bias. However, this approach does not consider the difference between different query characteristics (QCs) and their influence on retrieval functions’ bias quantification. This article provides an in-depth study of <b>retrievability</b> over different QCs. It analyzes the correlation of lower/higher retrieval bias with different query characteristics. The presence of strong correlation between retrieval bias and query characteristics in experiments indicates the possibility of determining retrieval bias of retrieval functions without processing an exhaustive query set. Experiments are validated onTREC Chemical Retrieval Track consisting of 1. 2 million patent documents...|$|E
40|$|<b>Retrievability</b> of {{radioactive}} waste {{is a subject}} {{playing a role in}} the policies on underground disposal in a number of countries. Important time scales are the period during which the waste can be regarded as retrievable (order of 100 years), and the period allowed for the actual retrieval operation (1 to 2 years). For disposal in rock salt, two disposal configurations are available: the salt-mine repository and the deep-boreholes/cavern combination. Both for the salt-mine repository and the deep boreholes, the disposal operation can be carried out {{in such a way that}} waste retrieval remains possible. Incorporation of <b>retrievability</b> is achieved through a number of technical modifications. An advantage of waste <b>retrievability</b> is that it provides an opportunity to shorten the period of surface storage. Main drawbacks of <b>retrievability</b> are a reduced degree of isolation of the waste during a limited time period, and increased disposal costs...|$|E
40|$|A {{new system}} of classification, {{registration}} and retrieval of patient data involving {{the use of an}} edge punched card, is described. The integration of physical, social and psychological data with necessary demographic information is an essential component of this system, which provides for simple and rapid <b>retrievability.</b> <b>Retrievability</b> of this information provides many opportunities for research and modification of patient care...|$|E
40|$|Hundreds of {{experiments}} {{over the last}} decade on the retrieval of OCR documents performed by the Information Science Research Institute have shown that OCR errors do not significantly affect <b>retrievability.</b> We extend those results to show {{that in the case of}} proximity searching, the removal of running headers and footers from OCR text will not improve <b>retrievability</b> for such searches...|$|E
40|$|Knowing {{how easily}} pages within a website can be {{retrieved}} using the site’s search functionality provides crucial {{information to the}} site designer. If {{the system is not}} retrieving particular pages then the system or information may need to be changed to ensure that visitors to the site have the best chance of finding the relevant information. In this demo paper, we present a Page <b>Retrievability</b> Calculator, which estimates the <b>retrievability</b> of a page for a given search engine. To estimate the <b>retrievability,</b> instead of posing all possible queries, we focus on issuing only those likely to retrieve the page and use them to obtain an accurate approximation. We can also rank the queries associated with the page to show the site designer what queries are most likely to retrieve the pages and at what rank. With this application we can now explore how {{it might be possible to}} improve the site or content to improve the <b>retrievability...</b>|$|E
40|$|In this work, the {{relationship}} between performance and <b>retrievability</b> bias is explored when various query expansion methods are employed to aide retrieval. Several parameters are altered, independently, to identify those that {{have an impact on}} bias. Parameters altered include; Rocchio's beta, length normalisation parameters, the number of terms added and the number of documents those terms are extracted from. A strong correlation between performance and <b>retrievability</b> bias is identified, suggesting that query expansion increases performance by making the system more biased...|$|E
40|$|Proofs of <b>Retrievability</b> (PoR) {{is one of}} {{the basic}} {{functions}} of electronic evidence preservation center in cloud. This paper proposes two PoR schemes to execute the workflow of evidence preservation center, which are named Finer Grained Proofs of <b>Retrievability</b> (FG-PoR) and More Lightweight Proofs of <b>Retrievability</b> (ML-PoR). The two PoR schemes do not use multi-replication technology or erasure code technology, but employ the verification tags and signatures to implement provable data possession and data recovery dual functions. When some data blocks have been lost in Archive Storage Area (ASA), FG-PoR can recover each data block of evidence matrix, but ML-PoR can only recover a column of evidence matrix. The analysis results show our two PoR schemes do not only provide the integrity verification guarantee but also have robust recovery guarantee to electronic evidence in cloud. The two schemes can allow for lower computation and storage costs than other similar schemes; moreover, ML-PoR can provide lower costs than FG-PoR...|$|E
40|$|This papers {{presents}} {{the concept of}} long-term underground retrievable storage (URS) of spent reactor fuel in unsaturated rock. Emplacement would be incremental and the system is planned to be experimental and flexible. The rationale for <b>retrievability</b> is examined, and a technical basis for 300 -year <b>retrievability</b> is presented. Maximum isolation is the rationale for underground as opposed to surface storage. Although the potential repository site at Yucca Mountain Nevada would be suitable for a URS, alternate sites are discussed. The technical issues involved in licensing a URS for 300 years are simpler than licensing a 10, 000 year repository. 16 refs...|$|E
40|$|Cluster-based pseudo-relevance {{feedback}} (PRF) is {{an effective}} approach for searching relevant documents for relevance feedback. Standard approach constructs clusters for PRF only {{on the basis of}} high similarity between retrieved documents. The standard approach works quite well if the retrieval bias of the retrieval model does not create any effect on the <b>retrievability</b> of documents. In our experiments we observed when a collection contains retrieval bias, then high retrievable documents of clusters are frequently retrieved at top positions for most of the queries, and these drift the relevance feedback away from relevant documents. For reducing (retrieval bias) noise, we enhance the standard cluster construction approach by constructing clusters on the basis of high similarity and <b>retrievability.</b> We call this <b>retrievability</b> and cluster-based PRF. This enhanced approach keeps only those documents in the clusters that are not frequently retrieve due to retrieval bias. Although this approach improves the effectiveness, however, it penalizes high retrievable documents even if these documents are most relevant to the clusters. To handle this problem, in a second approach, we extend the basic <b>retrievability</b> concept by mining frequent neighbors of the clusters. The frequent neighbors approach keeps only those documents in the clusters that are frequently retrieved with other neighbors of clusters and infrequently retrieved with those documents that {{are not part of the}} clusters. Experimental results show that two proposed extensions are helpful for identifying relevant documents for relevance feedback and increasing the effectiveness of queries...|$|E
40|$|This work in {{this thesis}} {{can be divided into}} two aspects. Firstly, we {{consider}} confidentiality and anonymity under a scenario where an untrusted third party is involved in a cryptographic system to check all the messages received. Then we study confidentiality, anonymity and integrity under a special atack called related-key attack. Lastly, we pay attention to data integrity in the setting of related-key attacks, and show how data integrity can be broken in a proof of <b>retrievability</b> system by a related-key attack and show how data integrity can be broken in a proof of <b>retrievability</b> system by a related-key attack adversary and how to prevent such attacks...|$|E
40|$|In a {{proof of}} <b>retrievability</b> (POR) system, {{interactive}} POR protocols are executed between a storage server and clients, so that clients can {{be convinced that}} their data {{is available at the}} storage server, ready to be retrieved when needed. In an interactive POR protocol, clients initiate challenges to the server, and the server feedbacks responses to clients with input of the stored data. <b>Retrievability</b> means that it should be possible for a client to extract the his/her data from the server's valid responses. An essential stepstone leading to <b>retrievability</b> is server's unforgeability of valid responses, i. e, any server coming up valid responses to a client's challenges is actually storing the client's data with overwhelming probability. Unforgeability can be achieved with authentication schemes like MAC, Digital Signature, etc. With homomorphic linear authentication schemes, the authenticators can be aggregated into one tag for the challenges, hence reducing the communication complexity. In this paper, we explore some new homomorphic linear authenticator schemes in POR to provide unforgeability. Compared with the recent work of Shacham and Waters, our scheme enjoys the same shortest responses, but reduces the local storage from O(s) to O(1). © 2011 IEEE. In a proof of <b>retrievability</b> (POR) system, interactive POR protocols are executed between a storage server and clients, so that clients can be convinced that their data is available at the storage server, ready to be retrieved when needed. In an interactive POR protocol, clients initiate challenges to the server, and the server feedbacks responses to clients with input of the stored data. <b>Retrievability</b> means that it should be possible for a client to extract the his/her data from the server's valid responses. An essential stepstone leading to <b>retrievability</b> is server's unforgeability of valid responses, i. e, any server coming up valid responses to a client's challenges is actually storing the client's data with overwhelming probability. Unforgeability can be achieved with authentication schemes like MAC, Digital Signature, etc. With homomorphic linear authentication schemes, the authenticators can be aggregated into one tag for the challenges, hence reducing the communication complexity. In this paper, we explore some new homomorphic linear authenticator schemes in POR to provide unforgeability. Compared with the recent work of Shacham and Waters, our scheme enjoys the same shortest responses, but reduces the local storage from O(s) to O(1). © 2011 IEEE...|$|E
40|$|International audienceA {{comparative}} study of the three main chemical information systems (Scifinder, Web of Science and Scopus) was performed by studying the indexing policies of titles, abstracts and keywords within selected literature articles. Various chemical expressions were introduced as topic searches to illustrate the different search tools related to term indexing. The resulting article lists were compared two-by-two {{by means of a}} script designed to identify common reference lists and specific ones to each editor. Analyzing these specific reference lists reveals that only partial coverage areas of references should be expected when querying a single platform. The discussion covers the term and keyword indexing policies, their influence on the <b>retrievability</b> of references and on the <b>retrievability</b> of the highly cited papers...|$|E
40|$|In this paper, {{we study}} <b>retrievability</b> of {{admissible}} cycles and {{the dynamics of}} the networks constructed from admissible cycles with the pseudoinverse learning rule. <b>Retrievability</b> of admissible cycles in networks with C_ 0 > 0 and λ sufficiently large are discussed. Based on the linear stability analysis we derive a complete description of all possible local bifurcations of the trivial solution for the networks constructed from admissible cycles. We illustrate numerically that, depending on the structural features, the admissible cycles are respectively stored and retrieved as attracting limit cycles, unstable periodic solutions and delay-induced long-lasting transient oscillations, and the transition from fixed points to the attracting limit cycle bifurcating from the trivial solution takes place through multiple saddle-nodes on limit cycle bifurcations. Comment: 31 pages, 10 figure...|$|E
