4|31|Public
40|$|Abstract—This paper proposes an {{efficient}} approach for adap-tive directional wavelet transform (WT) based on directional prefiltering. Although the adaptive directional WT {{is able to}} transform an image along diagonal orientations as well as tradi-tional horizontal and vertical directions, it sacrifices computation speed for good image coding performance. We present two efficient methods {{to find the best}} transform directions by prefiltering using 2 -D filter bank or 1 -D directional WT along two fixed directions. The proposed direction calculation methods achieve comparable image coding performance comparing to the conventional one with less complexity. Furthermore, transform direction data of the proposed method can be used for content-based image retrieval to increase <b>retrieval</b> <b>ratio.</b> Index Terms—Adaptive directional wavelet transform, di-rectional filter bank, directional lifting, image coding, wavelet transform. I...|$|E
40|$|We {{present a}} {{partially}} occluded facial image retrieval method {{based on a}} similarity measurement for forensic applications. The main novelty of this method compared with other occluded face recognition algorithms is measuring the similarity based on Scale Invariant Feature Transform (SIFT) matching between normal gallery images and occluded probe images. The proposed method consists of four steps: (i) a Self-Quotient Image (SQI) is applied to input images, (ii) Gabor-Local Binary Pattern (Gabor-LBP) histogram features are extracted from the SQI images, (iii) the similarity between two compared images is measured by using the SIFT matching algorithm, and (iv) histogram intersection is performed on the SIFT-based similarity measurement. In experiments, we have successfully evaluated {{the performance of the}} proposed method with the commonly used benchmark database, including occluded facial images. The results show that the correct <b>retrieval</b> <b>ratio</b> was 94. 07 % in sunglasses occlusion and 93. 33 % in scarf occlusion. As such, the proposed method achieved better performance than other Gabor-LBP histogram-based face recognition algorithms in eyes-hidden occlusion of facial images...|$|E
40|$|Two factors, which affect {{simulation}} quality are {{the amount}} of computing power and implementation. The Streaming SIMD (single instruction multiple data) extensions (SSE) present a technique for influencing both by exploiting the processor's parallel functionalism. In this paper, we show how SSE improves performance of lattice gauge theory simulations. We identified two significant trends through an analysis of data from various runs. The speed-ups were higher for single precision than double precision floating point numbers. Notably, though the use of SSE significantly improved simulation time, it did not deliver the theoretical maximum. There {{are a number of}} reasons for this: architectural constraints imposed by the FSB speed, the spatial and temporal patterns of data <b>retrieval,</b> <b>ratio</b> of computational to non-computational instructions, and the need to interleave miscellaneous instructions with computational instructions. We present a model for analyzing the SSE performance, which could help factor in the bottlenecks or weaknesses in the implementation, the computing architecture, and the mapping of software to the computing substrate while evaluating the improvement in efficiency. The model or framework would be useful in evaluating the use of other computational frameworks, and in predicting the benefits that can be derived from future hardware or architectural improvements. Comment: Technical report of results of optimization of physics (lattice gauge theory) simulations for Intel Architecture...|$|E
50|$|IF I=0 then WAcc will be {{equivalent}} to Recall (information <b>retrieval)</b> a <b>ratio</b> of correctly recognized words 'H' to Total {{number of words}} in reference 'N'.|$|R
40|$|There are {{two types}} of {{keywords}} used as metadata: controlled terms and free terms. Free terms have the advantage that metadata creators can freely select keywords, but there also exists a disadvantage that the information <b>retrieval</b> recall <b>ratio</b> might be reduced. The recall ratio can be improved by using controlled terms. But creating and maintaining controlled vocabularies has an enormous cost. In addition, many existing controlled vocabularies are published in formats less suitable for programming. We introduce a JavaScript library called “covo. js” that enables us to make use of controlled vocabularies as metadata for the organization of web pages...|$|R
50|$|In {{diagnostic}} testing, {{the main}} ratios used {{are the true}} column ratios - True Positive Rate and True Negative Rate - where they are known as sensitivity and specificity. In informational <b>retrieval,</b> the main <b>ratios</b> are the true positive ratios (row and column) - Positive Predictive Value and True Positive Rate - where they are known as precision and recall.|$|R
40|$|Background/Aims: Small-intestinal {{bacterial}} over-growth (SIBO) is {{a frequent}} finding in patients with ir-ritable bowel syndrome (IBS). Many patients with IBS also have abnormal intestinal permeability, which is probably due to low-grade inflammation in the in-testinal mucosa. Our aim was to verify the relation-ship between SIBO and small-intestinal permeability in IBS patients. Methods: A cohort of 38 IBS patients (20 women and 18 men; age range 16 - 70 years; mean age 40. 2 years) with symptoms that fulfilled Rome-II criteria, and 12 healthy controls (5 women and 7 men; age range 25 - 52 years; mean age: 37. 8 years) were recruited. All subjects underwent lactulose breath tests (LBTs) and intestinal permeability tests using the polyethylene glycol (PEG) 3350 / 400 <b>retrieval</b> <b>ratio.</b> Results: A positive LBT was found in 18. 4 % (7 / 38) of patients with IBS and 8. 3 % (1 / 12) of control subjects. Intestinal permeability was significantly in-creased in patients with IBS compared with the nor-mal controls (0. 82 ± 0. 09 vs 0. 41 ± 0. 05 [mean±SD], re-spectively; p＜ 0. 05). However, the intestinal perme-ability {{did not differ significantly}} between IBS patients with a positive LBT and those with a negative LBT (0. 90 ± 0. 13 and 0. 80 ± 0. 11, respectively; p＞ 0. 05). Con-clusions: Intestinal permeability was increased in pa-tients with IBS, but this finding did not correlated with the occurrence of SIBO. (Gut and Liver 2009...|$|E
40|$|International audienceNASA’s Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations (CALIPSO) {{mission is}} {{scheduled}} to release version 4 of their level 2 data products {{in the fall of}} 2016. This presentation will highlight some of the many significant improvements delivered by the version 4 CALIPSO lidar cloud and aerosol data products, with a particular focus on the <b>retrieval</b> of extinction-to-backscatter <b>ratios</b> for both ice clouds and water clouds...|$|R
40|$|Data {{dissemination}} {{and discovery}} {{is critical for}} adhoc wireless sensor networks. Most existing research depends on location information that is not always obtained easily, efficiently and accurately. We propose the concept of Contour-cast, a location-free data dissemination and discovery approach for large-scale wireless sensor networks. One important property of Contour-cast {{is that it does}} not depend on physical position or accurate localization services. The other advantage is that each node needs not maintain too much topology information. We evaluate Contour-cast thoroughly using metrics including data <b>retrieval</b> success <b>ratio,</b> storage cost, and load balance. Evaluation results show that Contour-cast can reach comparable functionalities and performance as the other approaches with physical location information. © 2009 IEEE...|$|R
40|$|SCISAT- 1, {{also known}} as the Atmospheric Chemistry Experiment, is a {{satellite}} mission for remote sensing of the Earth’s atmosphere, launched on 12 August 2003. The primary instrument on the satellite is a 0. 02 cm � 1 resolution Fourier-transform spectrometer operating in the mid-IR � 750 – 4400 cm � 1 �. We describe the approach developed for the retrieval of atmospheric temperature and pressure from the troposphere to the lower thermosphere as well as the strategy for the <b>retrievals</b> of volume-mixing <b>ratio</b> profiles of atmospheric species. © 2005 Optical Society of Americ...|$|R
40|$|The {{potential}} of measuring low altitude optically thin clouds with a Raman-elastic lidar {{in the daytime}} is analyzed. Optical depths of low clouds are derived by two separate methods from nitrogen-Raman and elastic scattering returns. By correcting for aerosol influences with the combined Raman-elastic returns, Mie-retrievals of low cloud optical depth can be dramatically improved and show good agreement with the direct Raman <b>retrievals.</b> Furthermore, lidar <b>ratio</b> profile is mapped out and shown {{to be consistent with}} realistic water phase cloud models. The variability of lidar ratios allows us to explore the distribution of small droplets near the cloud perimeter...|$|R
40|$|International audienceThe {{apparent}} {{heat source}} Q 1 and the apparent moisture sink Q 2 are crucial parameters for precipitating systems studies since they allow evaluating their contribution {{in water and}} energy transport and inferring some of the mechanisms responsible for their evolution along their lifetime. In this paper, a new approach is proposed to estimate Q 2 budgets from radar observations within precipitating areas at {{the scale of the}} measurements, i. e. either convective scale or mesoscale, depending on the selected retrieval zone. This approach relies upon a new analysis of the radar reflectivity based on the concept of the classical VAD analysis. Q 2 is deduced from velocity and reflectivity fields, following five steps i) mixing <b>ratio</b> <b>retrieval</b> using empirical relations; ii) radial wind analysis using the VAD analysis; iii) radar reflectivity analysis using a new analysis called RAD (Reflectivity Azimuth Display); iv) <b>retrieval</b> of mixing <b>ratio</b> derivatives; and v) Q 2 retrieval. The originality and the main interest of the present approach with respect to previous ones rely on the fact it uses radar data alone and is based on a relatively low cost analysis allowing future systematic application on large datasets. In the present paper this analysis is described and its robustness evaluated and illustrated on three cases observed during the AMMA SOP field experiment (15 June- 15 September) by means of the RONSARD radar. Results are analysed in terms of the convective or stratiform character of observed precipitation...|$|R
40|$|Near-infrared spectroscopic {{observations}} of the active star-forming region near NGC 7538 IRS 1 and IRS 2 were made. The relative intensities of the v = 1 - 0 Q(1), Q(3), and Q(5) lines of molecular hydrogen are used to calculate a rotational excitation temperature. Comparison of the measured intensity of the Q(2) transition relative to the intensity of Q(1) and Q(3) permitted the <b>retrieval</b> of the <b>ratio</b> of ortho-to-para hydrogen. It is found that an ortho-to-para ratio of between 1. 6 and 2. 35 is needed to explain the Q-branch line intensity ratios, depending on the excitation model used. This range in ortho-to-para ratios implies a range of molecular hydrogen formation temperature of approximately 105 K to 140 K...|$|R
40|$|INTRODUCTION The {{objective}} of the project {{is to try to}} determine to what extent "long-range" propagation may be modelled from comparatively short-range propagation measurements. This has involved modelling source/receiver geometry with respect to horizontal separation, which forms the basis for what has been termed the "aspect ratio", n, of an experiment [1]. Thus this project concerns both reflection and refraction information <b>retrieval.</b> Low aspect <b>ratio</b> experiments are those where horizontal source/receiver separation is small in comparison to the water depth [...] high aspect ratios are the converse. Fig. 1 illustrates the theoretical aspect ratio, which must be exceeded for head waves to arrive before bottom reflections, plotted against the ratio of sea bed and water column compressional velocities (c p and c respectively). Provided a suitable high velocity layer exists, a measurable refraction signal requires that the experiment provide conditions corresponding to the area above t...|$|R
40|$|In {{this paper}} we discuss global scale lidar <b>ratio</b> <b>retrieval</b> over the ocean as derived by the Synergized Optical Depth of Aerosols (SODA) {{algorithm}} (based on CALIPSO/CloudSat ocean surface echo) and comparisons with CALIOP V 3 lidar ratio for marine aerosols, dusts and cirrus clouds. As expected, the signatures vary {{as a function of}} the feature type. We show that the SODA lidar ratio at 532 nm is in relatively good agreement with what is currently used in CALIOP operational product. The lidar ratio at 1064 nm is however different. The origin of the differences cannot as yet be clearly identified. It may be related to the 1064 nm calibration, the ocean surface product, or to residual errors in the identification of aerosol type. As the lidar ratio is linked to the aerosol absorption and the backscatter phase function, it will help to better understand the aerosol and cloud radiative properties at global scale in the visible and in the near infrared...|$|R
40|$|This paper {{introduces}} a new method for recovering global fields of latent heat flux. The method focuses on specifying Bowen ratio fields through exploiting air temperature and vapour pressure measurements obtained from infra-red soundings of the AIRS (Atmospheric Infrared Sounder) sensor onboard the NASA-Aqua platform. Through combining these Bowen <b>ratio</b> <b>retrievals</b> with satellite surface net available energy data we have specified estimates of global surface latent heat flux at the 1 &deg; by 1 &deg; scale. These estimates were evaluated against data from 30 terrestrial tower flux sites covering {{a broad spectrum}} of biomes. Taking monthly average 13 : 30 local time (LT) data for 2003, this revealed a relatively good agreement between the satellite and tower measurements of latent heat flux, with a pooled root mean square deviation of 79 W m− 2 , and no significant bias. The results show particular promise for this approach under warm, moist conditions, but weaknesses under arid or semi-arid conditions subject to high radiative loads...|$|R
40|$|In {{this paper}} a {{modeling}} method {{based on data}} reductions is investigated which includes pre analyzed MERRA atmospheric fields for quantitative estimates of uncertainties introduced in the integrated path differential absorption methods for the sensing of various molecules including CO 2. This approach represents the extension of our existing lidar modeling framework previously developed and allows effective on- and offline wavelength optimizations and weighting function analysis to minimize the interference effects such as those due to temperature sensitivity and water vapor absorption. The new simulation methodology {{is different from the}} previous implementation in that it allows analysis of atmospheric effects over annual spans and the entire Earth coverage which was achieved due to the data reduction methods employed. The effectiveness of the proposed simulation approach is demonstrated with application to the mixing <b>ratio</b> <b>retrievals</b> for the future ASCENDS mission. Independent analysis of multiple accuracy limiting factors including the temperature, water vapor interferences, and selected system parameters is further used to identify favorable spectral regions as well as wavelength combinations facilitating the reduction in total errors in the retrieved XCO 2 values...|$|R
40|$|This paper {{presents}} {{measurements of}} the vertical distribution of aerosol extinction coefficient over West Africa during the Dust and Biomass-burning Aerosol Experiment (DABEX) /African Monsoon Multidisciplinary Analysis dry season Special Observing Period Zero (AMMA-SOP 0). In situ aircraft measurements from the UK FAAM aircraft have been compared with two ground-based lidars (POLIS and ARM MPL) and an airborne lidar on an ultralight aircraft. In general, mineral dust was observed at low altitudes (up to 2 km), and a mixture of biomass burning aerosol and dust was observed at altitudes of 2 – 5 km. The study exposes difficulties associated with spatial and temporal variability when intercomparing aircraft and ground measurements. Averaging over many profiles provided a better means of assessing consistent errors and biases associated with in situ sampling instruments and <b>retrievals</b> of lidar <b>ratios.</b> Shortwave radiative transfer calculations and a 3 -year simulation with the HadGEM 2 -A climate model show that the radiative effect of biomass burning aerosol was somewhat sensitive to the vertical distribution of aerosol. In particular, when the observed low-level dust layer {{was included in the}} model, the absorption of solar radiation by the biomass burning aerosols increased by 10...|$|R
40|$|This paper {{introduces}} {{a relatively simple}} method for recovering global fields of latent heat flux. The method focuses on specifying Bowen ratio estimates through exploiting air temperature and vapour pressure measurements obtained from infrared soundings of the AIRS (Atmospheric Infrared Sounder) sensor onboard NASA's Aqua platform. Through combining these Bowen <b>ratio</b> <b>retrievals</b> with satellite surface net available energy data, we have specified estimates of global noontime surface latent heat flux at the 1 °× 1 ° scale. These estimates were provisionally evaluated against data from 30 terrestrial tower flux sites covering {{a broad spectrum of}} biomes. Taking monthly average 13 : 30 data for 2003, this revealed promising agreement between the satellite and tower measurements of latent heat flux, with a pooled root-mean-square deviation of 79 W m − 2, and no significant bias. However, this success partly arose as a product of the underspecification of the AIRS Bowen ratio compensating for the underspecification of the AIRS net available energy, suggesting further refinement of the approach is required. The error analysis suggested that the landscape level variability in enhanced vegetation index (EVI) and land surface temperature contributed significantly to the statistical metric of the predicted latent heat fluxes...|$|R
40|$|International audienceIn {{this paper}} we compare water vapor mixing ratio {{measurements}} from two quasi-parallel flights of the Pico-SDLA H 2 O and FLASH-B hygrometers. The measurements {{were made on}} 10 February 2013 and 13 March 2012, respectively, in the tropics near Bauru, Sao Paulo St., Brazil during an intense convective period. Both flights were performed {{as part of a}} French scientific project, TRO-Pico, to study the impact of the deep-convection overshoot on the water budget. Only a few instruments that permit the frequent sounding of stratospheric water vapor can be flown within a small volume weather balloons. Technical difficulties preclude the accurate measurement of stratospheric water vapor with conventional in situ techniques. The instruments described here are simple and lightweight, which permits their low-cost deployment by non-specialists aboard a small weather balloon. We obtain mixing <b>ratio</b> <b>retrievals</b> which agree above the cold-point tropopause to within 1. 9 and 0. 5 % for the first and second flights, respectively. This level of agreement for measured stratospheric water mixing ratio is among the best ever reported in the literature. Because both instruments show similar profiles within their combined uncertainties, we conclude that the Pico-SDLA H 2 O and FLASH-B datasets are mutually consistent...|$|R
40|$|The Measurement Of Pollution In The Troposphere (MOPITT) instrument, {{which will}} be {{launched}} on the Terra spacecraft, is designed to measure the tropospheric CO and CH 4 at a nadir-viewing geometry. The measurements are taken at 4. 7 µm in the thermal region, and 2. 3 and 2. 2 µm in the solar region for CO mixing <b>ratio</b> <b>retrieval,</b> CO total column amount and CH 4 column amount retrieval, respectively. To ensure the required measurement accuracy, {{it is critical to}} identify and remove any cloud contamination to the channel signals. In this study, we develop an algorithm to detect the cloudy pixels, to reconstruct clear column radiance for pixels with partial cloud covers, and to estimate equivalent cloud top positions under overcast conditions to enable CO profile retrievals above clouds. The MOPITT channel radiances, as well as the first guess calculations, are simulated using a fast forward model with input atmospheric profiles from ancillary data sets. The precision of the retrieved CO profiles and total column amounts in cloudy atmospheres is within the expected ± 10 % range. Validations of the cloud detecting thresholds with MODIS Airborne Simulator (MAS) data and MATR (MOPITT Airborne Test Radiometer) measurements are also carried out and will be presented separately...|$|R
40|$|We {{compared}} CALIPSO column aerosol optical depths at 0. 532 μm to measurements at 147 AERONET sites, synchronized {{to within}} 30 min of satellite overpass times during a 3 -yr period. We found 677 suitable overpasses, and a CALIPSO bias of − 13 % relative to AERONET {{for the entire}} data set; the corresponding absolute bias is − 0. 029, and the standard deviation of the mean (SDOM) is 0. 014. Consequently, the null hypothesis is rejected at the 97 % confidence level, indicating {{a statistically significant difference}} between the datasets. However, if we omit CALIPSO columns that contain dust from our analysis, the relative and absolute biases are reduced to − 3 % and − 0. 005 with a standard error of 0. 016 for 449 overpasses, and the statistical confidence level for the null hypothesis rejection is reduced to 27 %. We also analyzed the results according to the six CALIPSO aerosol subtypes and found relative and absolute biases of − 29 % and − 0. 1 for atmospheric columns that contain the dust subtype exclusively, but with a relatively high correlation coefficient of R = 0. 58; this indicates the possibility that the assumed lidar ratio (40 sr) for the CALIPSO dust retrievals is too low. Hence, we used the AERONET size distributions, refractive indices, percent spheres, and forward optics code for spheres and spheroids to compute a lidar ratio climatology for AERONET sites located in the dust belt. The highest lidar ratios of our analysis occur in the non-Sahel regions of Northern Africa, where the median lidar ratio at 0. 532 μm is 55. 4 sr for 229 <b>retrievals.</b> Lidar <b>ratios</b> are somewhat lower in the African Sahel (49. 7 sr for 929 retrievals), the Middle East (42. 6 sr for 489 retrievals), and Kanpur, India (43. 8 sr for 67 retrievals). We attribute this regional variability in the lidar ratio to the regional variability of the real refractive index of dust, as these two parameters are highly anti-correlated (correlation coefficients range from − 0. 51 to − 0. 85 for the various regions). The AERONET refractive index variability is consistent with the variability of illite concentration in dust across the dust belt...|$|R
40|$|Images of auroral {{emissions}} at {{far ultraviolet}} (FUV, 122 – 200 nm) wavelengths are useful tools {{with which to}} study magnetospheric-ionospheric coupling, as the scattered sunlight background in this region is low, allowing both dayside and nightside auroras to be imaged simultaneously. The ratio of intensities between certain FUV emission lines or regions {{can be used to}} characterise the precipitating particles responsible for auroral emissions, and hence is a useful diagnostic of magnetospheric dynamics. Here, we describe how the addition of simple transmission filters to a compact broadband imager design allows far ultraviolet emission ratios to be deduced while also providing large-scale instantaneous images of the aurora. The low mass and volume of such an instrument would make it well-suited for both small satellite Earth-orbiting missions and larger outer planet missions from which it could be used to characterise the tenuous atmospheres observed at several moons, as well as studying the auroral emissions of the gas giants. We present a study to investigate the accuracy of a technique to allow emission line <b>ratio</b> <b>retrieval,</b> as applied to the OI 130. 4 nm and 135. 6 nm emissions at Ganymede. The ratio of these emissions provides information about the atmospheric composition, specifically the relative abundances of O and O 2. Using modelled FUV spectra representative of Ganymede's atmosphere, based on observations by the Hubble Space Telescope (HST) Space Telescope Imaging Spectrograph (STIS), we find that the accuracy of the retrieved ratios {{is a function of the}} magnitude of the ratio, with the best measurements corresponding to a ratio of ∼ 1. 3. Peer reviewe...|$|R
40|$|The {{relative}} abundance {{of the heavy}} water isotopologue HDO provides a deeper insight in the atmospheric hydrological cycle. The SCanning Imaging Absorption spectroMeter for Atmospheric CartograpHY (SCIAMACHY) allows global <b>retrievals</b> of the <b>ratio</b> HDO/H 2 O in the 2. 3 micron wavelength range. However, the spectroscopy of water lines in this region remains a large source of uncertainty for these retrievals. We therefore evaluate and improve the water spectroscopy in the range 4174 – 4300 cm − 1 and test if this reduces systematic uncertainties in the SCIAMACHY retrievals of HDO/H 2 O. We use a laboratory spectrum of water vapour to fit line intensity, air broadening and wavelength shift parameters. The improved spectroscopy is tested {{on a series of}} ground-based high resolution FTS spectra as well as on SCIAMACHY retrievals of H 2 O and the ratio HDO/H 2 O. We find that the improved spectroscopy leads to lower residuals in the FTS spectra compared to HITRAN 2008 and Jenouvrier et al. (2007) spectroscopy and the retrievals become more robust against changes in retrieval window. For both the FTS and SCIAMACHY measurements the retrieved total columns H 2 O decrease by 2 – 4 % and we find a negative shift of the HDO/H 2 O ratio, which for SCIAMACHY is partly compensated by changes in the retrieval setup and calibration software. The updated SCIAMACHY HDO/H 2 O product shows somewhat steeper latitudinal and temporal gradients and a steeper Rayleigh distillation curve, strengthening previous conclusions that current isotope-enabled general circulation models underestimate the variability in the near-surface HDO/H 2 O ratio...|$|R
40|$|Abstract. In {{this paper}} we present the {{automated}} software tool ELDA (EARLINET Lidar Data Analyzer) for the retrieval of profiles of optical particle properties from lidar signals. This tool {{is one of}} the calculus modules of the EARLINET Single Calculus Chain (SCC) which allows for the analysis of the data of many different lidar systems of EARLINET in an automated, unsupervised way. ELDA delivers profiles of particle extinction coefficients from Raman signals as well as profiles of particle backscatter coefficients from combinations of Raman and elastic signals or from elastic signals only. Those analyses start from pre-processed signals which have already been corrected for background, range dependency and hardware specific effects. An expert group reviewed all algorithms and solutions for critical calculus subsystems which are used within EARLINET with respect to their applicability for automated retrievals. Those methods have been implemented in ELDA. Since the software was designed in a modular way, it is possible to add new or alternative methods in future. Most of the implemented algorithms are well known and well documented, but some methods have especially been developed for ELDA, e. g., automated vertical smoothing and temporal averaging or the handling of effective vertical resolution in case of lidar <b>ratio</b> <b>retrievals,</b> or the gluing of near-range and far-range products. The accuracy of the retrieved profiles was tested following the procedure of the EARLINET-ASOS algorithm inter-comparison exercise which is based on the analysis of synthetic signals. Mean deviations, mean relative deviations, and normalized root-mean-square deviations were calculated for all possible products and three height layers. In all cases, the deviations were clearly below the maximum allowed values according to the EARLINET quality requirements...|$|R
40|$|Abstract. With the {{increasing}} interest for Arctic Ocean resources and faced with its sensitivity to climate change, {{it is important}} to accurately monitor the chlorophyll-a (Chla) concentration that is a key indicator of phytoplankton biomass and marine productivity. The performances of three operational algorithms (OC 4 v 6, OC 3 Mv 6, OC 4 Mev 6), two Arctic adapted algorithms (OC 4 L, OC 4 P), and one semi-analytical (GSM 01) algorithm to estimate Chla were evaluated using in situ measurements collected in the southeastern Beaufort Sea. All evaluated algorithms clearly overestimated Chla by a factor of three to five. A high contribution of colored dissolved organic matter and nonalgal particles to the blue light absorption appeared as the source of that poor performance. It was also found that fluorometrically measured Chla were two times greater than those determined from high-performance liquid chromatography, contributing to the observed discrepancies between our findings and previous studies carried out in the Arctic. We propose regionally adapted and new algorithms allowing accurate estimation of Chla in the southeastern Beaufort Sea. Finally, a matchup analysis of coincident in situ data and satellite overpass showed that the normalized water-leaving reflectance and the blue-to-green <b>ratio</b> <b>retrieval</b> were more accurate for SeaWiFS than for MODIS and MERIS. Résumé. Avec l’intérêt croissant pour les ressources de l’océan Arctique et face a ̀ sa sensibilite ́ aux changements climatiques, il est important de suivre de façon précise la concentration de la chlorophylle-a (Chla) qui est un indicateur cle ́ de la biomasse phytoplanctonique et de la productivite ́ marine. La performance de trois algorithmes opérationnel...|$|R
40|$|In {{this paper}} we present the {{automated}} software tool ELDA (EARLINET Lidar Data Analyzer) for the retrieval of profiles of optical particle properties from lidar signals. This tool {{is one of}} the calculus modules of the EARLINET Single Calculus Chain (SCC) which allows for the analysis of the data of many different lidar systems of EARLINET in an automated, unsupervised way. ELDA delivers profiles of particle extinction coefficients from Raman signals as well as profiles of particle backscatter coefficients from combinations of Raman and elastic signals or from elastic signals only. Those analyses start from pre-processed signals which have already been corrected for background, range dependency and hardware specific effects. An expert group reviewed all algorithms and solutions for critical calculus subsystems which are used within EARLINET with respect to their applicability for automated retrievals. Those methods have been implemented in ELDA. Since the software was designed in a modular way, it is possible to add new or alternative methods in future. Most of the implemented algorithms are well known and well documented, but some methods have especially been developed for ELDA, e. g., automated vertical smoothing and temporal averaging or the handling of effective vertical resolution in the case of lidar <b>ratio</b> <b>retrievals,</b> or the merging of near-range and far-range products. The accuracy of the retrieved profiles was tested following the procedure of the EARLINET-ASOS algorithm inter-comparison exercise which is based on the analysis of synthetic signals. Mean deviations, mean relative deviations, and normalized root-mean-square deviations were calculated for all possible products and three height layers. In all cases, the deviations were clearly below the maximum allowed values according to the EARLINET quality requirements...|$|R
40|$|The file {{associated}} with this record is under a 24 -month embargo from publication {{in accordance with the}} publisher's self-archiving policy. The full text may be available through the publisher links provided above. Images of auroral emissions at far ultraviolet (FUV, 122 – 200 nm) wavelengths are useful tools with which to study magnetospheric–ionospheric coupling, as the scattered sunlight background in this region is low, allowing both dayside and nightside auroras to be imaged simultaneously. The ratio of intensities between certain FUV emission lines or regions can be used to characterise the precipitating particles responsible for auroral emissions, and hence is a useful diagnostic of magnetospheric dynamics. Here, we describe how the addition of simple transmission filters to a compact broadband imager design allows far ultraviolet emission ratios to be deduced while also providing large-scale instantaneous images of the aurora. The low mass and volume of such an instrument would make it well-suited for both small satellite Earth-orbiting missions and larger outer planet missions from which it could be used to characterise the tenuous atmospheres observed at several moons, as well as studying the auroral emissions of the gas giants. We present a study to investigate the accuracy of a technique to allow emission line <b>ratio</b> <b>retrieval,</b> as applied to the OI 130. 4 nm and 135. 6 nm emissions at Ganymede. The ratio of these emissions provides information about the atmospheric composition, specifically the relative abundances of O and O 2. Using modelled FUV spectra representative of Ganymede׳s atmosphere, based on observations by the Hubble Space Telescope (HST) Space Telescope Imaging Spectrograph (STIS), we find that the accuracy of the retrieved ratios {{is a function of the}} magnitude of the ratio, with the best measurements corresponding to a ratio of ~ 1. 3 Peer-reviewedPost-prin...|$|R
40|$|The {{theory of}} memory reconsolidation argues that {{consolidated}} memory is not unchangeable. Once a memory is reactivated it may {{go back into}} an unstable state and need new protein synthesis to be consolidated again, which is called “memory reconsolidation”. Boundary {{studies have shown that}} interfering with reconsolidation through pharmacologic or behavioral intervention can lead to the updating of the initial memory, for example, erasing undesired memories. Behavioral procedures based on memory reconsolidation interference {{have been shown to be}} an effective way to inhibit fear memory relapse after extinction. However, the effectiveness of retrieval–extinction differs by subtle differences in the protocol of the reactivation session. This represents a challenge with regard to finding an optimal operational model to facilitate its clinical use for patients suffering from pathogenic memories such as those associated with post-traumatic stress disorder. Most of the laboratory models for fear learning have used a single conditioned stimulus (CS) paired with an unconditioned stimulus (US). This has simplified the real situation of traumatic events to an excessive degree, and thus, limits the clinical application of the findings based on these models. Here, we used a basic visual compound CS model as the CS to ascertain whether partial repetition of the compound CSs in conditioning can reactivate memory into reconsolidation. The results showed that the no retrieval group or the 1 / 3 <b>ratio</b> <b>retrieval</b> group failed to open the memory reconsolidation time window. The 2 / 3 repetition retrieval group and the whole repetition retrieval group were able to prevent fear reinstatement, whereas only a 2 / 3 ratio repetition of the initial compound CS as a reminder could inhibit spontaneous recovery. We inferred that a retrieval–extinction paradigm was also effective in a more complex model of fear if a sufficient prediction error (PE) could be generated in the reactivation period. In addition, in order to achieve an optimal effect, a CS of moderate discrepancy should be used as a reminder...|$|R
40|$|A {{scheme is}} {{presented}} for retrieving aerosol properties for ocean regions from reflected sunlight {{at both the}} visible and near infrared wavelengths measured by the NOAA advanced very high resolution radiometer (AVHRR). For the Indian Ocean Experiment (INDOEX), aerosols were presumed to be a mixture of a continental haze that had small particles, contained soot, and absorbed sunlight, and a marine haze that had large particles and absorbed practically no sunlight. Because {{of the difference in}} particle sizes, the two aerosols reflect sunlight differently at visible and near infrared wavelengths. Reflectances at visible and near infrared wavelengths were thus used to determine mixing fractions for the continental and marine aerosols and the optical depth of the aerosol mixture. The fractions and optical depths along with the optical properties of the aerosols were then used in radiative transfer calculations to estimate the diurnally averaged top of the atmosphere and surface aerosol direct radiative forcing for ocean regions. Comparison of retrieved optical depths at visible and near infrared wavelengths with surface measurements revealed that several different retrieval schemes employing a variety of aerosol types provided comparable levels of agreement, but none of the aerosol models or <b>retrieval</b> schemes produced <b>ratios</b> of the near infrared to visible optical depths that agreed with the ratios obtained with the surface measurements. In estimating the top of the atmosphere radiative forcing, errors in the retrieved optical depths were in some cases found to be partially compensated by the effect of the aerosol on the radiative flux. For example, different aerosol models led to retrieved optical depths that differed by as much as 60 %, but the top of the atmosphere forcing obtained with the models differed by less than 35 % for cloud-free conditions. When aerosols absorb sunlight, there is no comparable compensation for the surface forcing. Cloud conditions contribute sizable uncertainties to estimates of the aerosol direct radiative forcing. For INDOEX, estimates of the aerosol direct radiative forcing for average cloud conditions were obtained by (1) setting the forcing to zero for all 1 º x 1 º latitude-longitude boxes that contained any amount of upper-level cloud; (2) ascribing to regions with upper-level clouds the radiative forcing obtained for regions having only low-level clouds and, (3) setting the forcing to zero for all regions containing upper-level clouds and all portions of regions overcast by low-level clouds. Relative differences in the extreme values for the top of the atmosphere aerosol direct radiative forcing were less than 50 %, but for the surface, the relative differences of the extreme values reached 70 %...|$|R
40|$|In this paper, {{we discuss}} the {{measurements}} of spectral surface reflectance (rho(s) (lambda)) in the wavelength range 350 - 2500 nm measured using a spectroradiometer onboard a low-flying aircraft over Bangalore (12. 95 degrees N, 77. 65 degrees E), an urban site in southern India. The large discrepancies in the retrieval of aerosol propertiesover land by the Moderate-Resolution Imaging Spectroradiometer (MODIS), which {{could be attributed to}} the inaccurate estimation of surface reflectance at many sites in India and elsewhere, provided motivation for this paper. The aim of this paper was to verify the surface reflectance relationships assumed by the MODIS aerosol algorithm for the estimation of surface reflectance in the visible channels (470 and 660 nm) from the surface reflectance at 2100 nm for aerosol retrieval over land. The variety of surfaces observed in this paper includes green and dry vegetations, bare land, and urban surfaces. The measuredreflectance data were first corrected for the radiative effects of atmosphere lying between the ground and aircraft using the Second Simulation of Satellite Signal in the Solar Spectrum (6 S) radiative transfer code. The corrected surface reflectance in the MODIS's blue (rho(s) (470)), red (rho(s) (660)), and shortwave-infrared (SWIR) channel (rho(s) (2100)) was linearly correlated. We found that the slope of reflectance relationship between 660 and 2100 nm derived from the forward scattering data was 0. 53 with an intercept of 0. 07, whereas the slope for the relationship between the reflectance at 470 and 660 nm was 0. 85. These values are much higher than the slope (similar to 0. 49) for either wavelengths assumed by the MODIS aerosol algorithm over this region. The reflectance relationship for the backward scattering data has a slope of 0. 39, with an intercept of 0. 08 for 660 nm, and 0. 65, with an intercept of 0. 08 for 470 nm. The large values of the intercept (which is very small in the MODIS reflectance relationships) result in larger values of absolute surface reflectance in the visible channels. The discrepancy between the measured and assumed surface reflectances could lead to error in the aerosol <b>retrieval.</b> The reflectance <b>ratio</b> (rho(s) (660) /rho(s) (2100)) showed a clear dependence on the N D V I-SWIR where the ratio increased from 0. 5 to 1 with an increase in N V I-SWIR from 0 to 0. 5. The high correlation between the reflectance at SWIR wavelengths (2100, 1640, and 1240 nm) indicated an opportunity to derive the surface reflectance and, possibly, aerosol properties at these wavelengths. We need more experiments to characterize the surface reflectance and associated inhomogeneity of land surfaces, which {{play a critical role in}} the remote sensing of aerosols over land...|$|R
40|$|CH 4 is {{the second}} most potent {{anthropogenic}} greenhouse gas, after CO 2, and is directly responsible for approximately 20 % of the human-induced greenhouse effect. To improve our understanding of the global CH 4 budget, high quality measurements of its atmosphericmole fraction are needed with good resolution in space and time. They provide constraints to the so-called inverse models, which are used to convert the mole fraction gradients into surface fluxes. Unfortunately, it is difficult to take continuous measurements on the ground in many regions of the world due to political or geographical limitations, leaving the models incapable of estimating the fluxes in these regions. Measurements made by sensors onboard a satellite platform can be a suitable source of information as they provide near global coverage. However, satellite retrievals often suffer from errors due to scattering of light by aerosol and cirrus. Such errors, if unaccounted, can be wrongly attributed to the flux estimates of inverse models. One way to dodge this problem is to use the <b>ratio</b> between the <b>retrievals</b> of two tracers, so that the retrieval errors cancel out in the ratio. The traditional ‘proxy’ method achieves this goal by multiplying the <b>ratio</b> of satellite <b>retrieval</b> of CH 4 and CO 2 with numerically modeled CO 2 mole fractions to obtain CH 4 mole fraction estimates. The method assumes that the errors in modeled CO 2 are negligible compared to residual errors in the <b>ratio</b> of satellite <b>retrievals.</b> However, the retrieval errors are becoming smaller with new and more advanced satellite data becoming available and CO 2 model errors are becoming the performance limiting factor in inverse modelling. In this thesis, I present the ratio inversion method, which avoids the use of modeled CO 2 by assimilating the <b>ratio</b> of satellite <b>retrievals</b> of CH 4 and CO 2 to constrain surface emissions. I tested the method with synthetic numerical experiments, where the satellite retrievals were simulated numerically with predefined ‘true’ fluxes. These experiments confirmed that the ratio inversion method is capable of reproducing the true emissions. Via a similar experiment, I also found that inversions using satellite retrievals perform better than inversions assimilating only ground-based measurements. I compared retrievals from the Greenhouse gases Observing SATellite (GOSAT) with in situ measurements from ground-based sensors of Total Carbon Column Observing Network (TCCON), to quantify the errors in CO 2 and CH 4 <b>retrievals,</b> in their <b>ratio,</b> and in modeled CO 2 mole fractions. The errors in the ratio were found to be lower than the errors in CO 2 and CH 4, which confirms that a large fraction of the scattering related errors cancels out. The ratio measurements from GOSAT were then used to constrain the surface fluxes of 2009 – 2010, and the results were validated with independent aircraft measurements of CO 2 and CH 4. Further, I used GOSAT and ground-based measurements, along with carbon isotope ratio observations of CH 4, to identify the impact of the 2011 La Niña on natural CH 4 emissions. After removing the variability caused by meteorology, I found that emissions from wetlands in the Tropics were higher during this event. The enhancement in wetland emissions was primarily caused by an increase in the wetland extent driven by heavy precipitation during the La Niña...|$|R
40|$|The UK has an {{abundant}} offshore wind resource with offshore wind farming set to grow rapidly {{over the coming}} years. Optimisation of energy production is {{of the utmost importance}} and accurate estimates of wind speed distributions are critical for the planning process. Synthetic aperture radar (SAR) data can provide synoptic, wide area wind field estimates at resolutions of a few kilometres and has great potential for wind resource assessment. This thesis addresses the key challenges for the operational implementation of SAR in this context; namely the accuracy of SAR wind retrievals and the ability of SAR to characterise the mean wind speed and wind power density. We consider the main stages of SAR wind retrieval; the retrieval algorithm; sources of a priori information; the optimal configuration of the retrieval system; and the challenges for and accuracy of SAR wind resource estimation. This study was conducted for the eastern Irish Sea in the UK, a region undergoing significant offshore wind energy development. A new wind retrieval algorithm was developed that implements a maximum a posterior probability (MAP) method drawn from Bayesian statistics. MAP was demonstrated to be less sensitive to input errors than the standard direction-based wind speed algorithm (DWSA) and provides a simple retrieval quality check via the error reduction <b>ratio.</b> <b>Retrieval</b> accuracy is strongly influenced by the quality of a priori information. The accuracy of two operationally viable a priori sources, mesoscale numerical weather prediction (NWP) data and WISAR image directions, was evaluated by comparison against in-situ wind observations and WERA coastal data. Results show that NWP wind speeds produce good wind speed and direction estimates with standard deviations of ¬± 2 ms- 1 and ± 16 o respectively. WISAR directions were less accurate producing standard deviations ranging from ± 20 o to ± 29 o, but were preferable when strong differences between NWP timesteps were observed. The accuracy of SAR wind retrievals was evaluated by comparison against in-situ wind observations. The MAP algorithm was found to provide modest improvements in retrieval accuracy over DWSA. Highest quality retrievals achieved using the CMOD 5 forward model, producing wind speeds with a RMSE of 1. 83 ms- 1. Regarding the ability of SAR to estimate offshore wind resources, dataset density was found to be a controlling parameter. With 103 scenes available mean wind speeds were well characterised by comparison against in-situ observations and Wind Atlas results, while wind power density showed considerable errors. The accuracy of wind speed maps was further improved by accounting for wind direction and fetch effects upon the SAR wind distribution. A key strength of the SAR wind fields is their ability to identify the effect of mesoscale structures upon the surface wind field with atmospheric gravity waves observed in 30 % of the images. These structures are shown to introduce wind speed fluctuations of up to ± 2 ms- 1 at scales of 5 to 10 km and may have significant implications for wind power prediction. These findings show that SAR may provide an important source of wide area wind speed observations as a complement to existing wind resource estimation techniques. SAR may be of particular use in coastal areas where complex wind fields are observed. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Delay Tolerant Networks (DTNs) are {{characterized}} by large delays, frequent dis- ruptions and lack of contemporaneous paths between nodes. Moreover, nodes may have limited computational power, storage, and battery capacity. Despite these challenges, DTNs have found applications {{in areas such as}} animal tracking, surveillance and facilitating education in remote areas. In these applications, a critical problem is routing or data dissemination. Indeed, any routing protocols must aim to transmit data efficiently and maximize data delivery. The chal- lenge, however, is that nodes are only connected intermittently, and do not have a contemporaneous path. Moreover, any established path may not remain valid after a transmission, and nodes may experience large delays before encounter- ing one another. As a result, existing routing protocols cannot be retrofitted to work in DTNs. To date, a primary solution for routing in DTNs is epidemic-based routing protocols because of their simplicity, low delays and little to no reliance on spe- cific nodes. A key observation of past research on epidemic routing protocols is that they are not evaluated on a unified framework. Specifically, no work has compared the performance of epidemic routing protocols using both the Random Way Point (RWP) model and trace files. Henceforth, this thesis has conducted a comprehensive study that employs both RWP and trace files. In particular, this thesis is the first to compare the performance of epidemic-based routing protocols using a custom-built simulator that moves nodes according to a trace-file and the RWP model. Moreover, it compares these protocols using the same set of parameters; e. g., node numbers, load and buffer space. The extensive simulation studies reveal the following limitations: high buffer occu- pancy level, premature discard of bundles, inefficient use of immunity tables to purge redundant bundles, low delivery ratio at high loads, and poor adap- tivity to changing network parameters. This thesis also outlines three novel enhancements to address the limitations of epidemic-based protocols. First, it introduces a mechanism that adapts the Time to Live (TTL) of bundles dy- namically according to a node’s encounter interval. The intuition here is that bundles should be buffered according to the interval between two encounters. That is, when nodes experience long inter-contact intervals, bundles will have a large TTL value, whilst short intervals result in a small TTL value. Simu- lation results show that epidemic with dynamic TTL improves delivery ratio by more than 20 %. The second enhancement combines Encounter Count (EC) with TTL to reduce buffer occupancy level and increase bundle delivery ratio. The resulting combination is able to reduce buffer occupancy level by 40 %. More importantly, it dramatically improves the delivery ratio by at least 40 % at high loads. The last enhancement uses immunity tables to carry cumulative acknowledgments. This has the effect of facilitating buffer management, and more importantly, allows a node to delete multiple bundles upon receiving one immunity table. This is an improvement over past approaches as nodes need to receive N immunity tables in order to delete N bundles. Extensive experi- ments using both the RWP model and trace-file confirm the superiority of these enhancements over existing epidemic routing protocols. Epidemic-based routing protocols can also be used for transmitting multicast bundles. This thesis presents three key findings concerning the delivery ratio of epidemic-based multicast protocols. Specifically, (i) subscriber nodes must act as relays. Simulation results show that bundle delivery ratio is only 57 % as com- pared to 100 % when they act as relays, (ii) the use of anti-entropy contributes to a rise in delivery ratio to 100 %, which is 57 % higher than when nodes do not use anti-entropy, and (iii) higher number of relay nodes improve delivery ratio. From extensive simulation studies, {{it can be seen that}} when the number of relay nodes increases from 10 to 30, the delivery ratio increased from 43 % to 98 %. This thesis also presents new findings related to multicast group sizes and delivery ratios. The presented results show that the impact of multicast group size on bundle delivery ratio is dependent upon whether subscribers forward bundles - i. e., whether they are relays or sinks. Specifically, when subscribers work as sinks, epidemic with Time-to-Live (TTL), epidemic with immunity, epidemic with Encounter Count (EC) threshold, and epidemic with cumulative immunity, have poor bundle delivery ratio. On the other hand, for epidemic with immunity, it has a low bundle delivery ratio when subscribers act as relays. In regards to buffer occupancy level in multicast scenarios, simulation results show that multicast group size, anti-entropy session and subscribers forwarding policies have a significant impact on the buffer occupancy level of relay nodes. In particular, all protocols experience high buffer occupancy level. To address this problem, each bundle is assigned an EC quota. In other words, EC quota bounds the number of times in which a bundle is exchanged by relay nodes. Lastly, this thesis focuses on routing requests and replies in DTN based infor- mation retrieval systems. Specifically, this thesis presents the first data centric information retrieval system called Distributed Data-Centric Information Re- trieval (DDC-IR) system. Nodes only process a query if its similarity value matches that of the query. This ensures only nodes that have a high proba- bility of resolving a query process and transmit the query. Experiment results show that DDC-IR is able to resolve 50 % more queries and has 80 % lower buffer occupancy level than prior work. More importantly, DDC-IR is able to support four query types: complex, unique, aggregated and continuous. This is a sig- nificant contribution over past approaches that only targeted one query type. Apart from that, DDC-IR supports a novel caching policy, in which nodes cache popular queries to improve the <b>retrieval</b> success <b>ratio.</b> This thesis also investi- gated the influence of the number of sub-queries and the number of querying nodes on query resolution time. When the number of querying nodes increases, the retrieval success rate reduces from 100 % to 20 %. When the number of sub- queries in a complex query increases from five to nine, DDC-IR requires 50 % more time to resolve queries. In comparison, previous IR systems are unable to resolve any queries...|$|R

