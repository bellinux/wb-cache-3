45|552|Public
50|$|It {{has been}} theorised that {{the essence of}} humour lies in two {{elements}} or factors, the <b>relevance</b> <b>factor,</b> and the surprise factor. First, {{it is necessary to}} present something familiar or relevant to the audience. This accounts for gaining the involvement and scrutiny of the audience, who may believe they know the natural follow-through thoughts or conclusion. Next, the actual amusement results from the presentation of some twist on what the audience expected, or else from interpreting the original situation in an unexpected way. These twists and unexpected interpretations may be summarized as the surprise factor.|$|E
50|$|The {{factors that}} {{determine}} {{the relevance of}} search results {{within the context of}} an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich link structure as is found on the web's hypertext content, however, a new breed of Enterprise search engines based on a bottom-up Web 2.0 technology are providing both a contributory approach and hyperlinking within the enterprise. Algorithms like PageRank exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent <b>relevance</b> <b>factor.</b> In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with information retrieval algorithms. Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.|$|E
40|$|Abstract. An incremental, nonparametric {{probability}} estimation procedure using {{a variation}} of the Fuzzy ARTMAP (FAM) neural network is introduced. The resulted network, called Fuzzy ARTMAP with <b>Relevance</b> <b>factor</b> (FAMR), uses a <b>relevance</b> <b>factor</b> assigned to each sample pair, proportional to the importance of that pair during the learning phase. We prove that our probability estimator is correct. The FAMR can be used both as a classifier and as a probability estimator. ...|$|E
40|$|Relevance Learning Vector Quantization (RLVQ) (introduced in [1]) is a {{variation}} of Learning Vector Quantization (LVQ) which allows a heuristic determination of <b>relevance</b> <b>factors</b> for the input dimensions. The method is based on Hebbian learning and defines weighting factors of the input dimensions which are automatically adapted to the specific problem. These <b>relevance</b> <b>factors</b> increase the overall performance of the LVQ algorithm. At the same time, relevances {{can be used for}} feature ranking and input dimensionality reduction. We introduc...|$|R
40|$|Abstract – Ordered Weighted Aggregation (OWA) {{operators}} {{represent a}} distinct family of aggregation operators and were introduced by Yager in [1]. They compute a weighted sum {{of a number}} of criteria that must be satisfied. The central element of the OWA operators is that the criteria are reordered before aggregation and therefore a particular weight is associated to a position. Relevance Learning Vector Quantization (RLVQ) [2] is an extension of the Learning Vector Quantization (LVQ) algorithm [3] and performs a heuristic determination of the <b>relevance</b> <b>factors</b> of the input dimensions. This method is based on Hebbian learning and associates a weight factor to each dimension of the input vectors. We present a LVQ method for on-line computing of the OWA weights as <b>relevance</b> <b>factors.</b> The method uses a weighted metric based on OWA restrictions. The principal benefit of our algorithm is that it connects two distinct topics: RLVQ algorithm and the consistent mathematical model of the OWA operators. Index Terms – ordered weighted aggregation operators, learning vector quantization, <b>relevance</b> <b>factors,</b> machine learning, neural networks. I...|$|R
40|$|We {{propose a}} new matrix {{learning}} scheme to extend Generalized Relevance Learning Vector Quantization (GRLVQ), an efficient prototype-based classification algorithm. By introducing a full matrix of <b>relevance</b> <b>factors</b> {{in the distance}} measure, correlations between different features and their importance for the classification scheme can {{be taken into account}} and automated, general metric adaptation takes place during training. In comparison to the weighted euclidean metric used for GR-LVQ, a full matrix is more powerful to represent the internal structure of the data appropriately. Interestingly, large margin generalization bounds can be transfered to the case of a full matrix such that bounds which are independent of the input dimensionality and the number of parameters arise. This also holds for local metrics attached to each prototype. The algorithm is tested and compared to GLVQ without metric adaptation [16] and GRLVQ with diagonal <b>relevance</b> <b>factors</b> usin...|$|R
40|$|Customer Relationship Management (CRM) {{can lead}} to greater profits. However even as many as 55 % of all CRM {{projects}} do not produce results. This research tries to help increase success rates. For the research question {{a part of the}} USIT model by Schuring and Spil has been used. The model looks at factors that have influence on the success of IS from the end-user perspective. They say that the <b>relevance</b> <b>factor</b> has the biggest impact within the USIT and this one has been chosen for this research. Aspects that can have influence on this <b>relevance</b> <b>factor</b> have been researched using a literature study and case studies. The two aspects that were found and tested are training/user support and user involvement. They both can have a positive influence on the <b>relevance</b> <b>factor...</b>|$|E
40|$|Abstract. FAMR (Fuzzy ARTMAP with <b>Relevance</b> <b>factor)</b> is a FAM (Fuzzy ARTMAP) {{neural network}} used for classification, {{probability}} es-timation [3], [2], and function approximation [4]. FAMR uses a <b>relevance</b> <b>factor</b> {{assigned to each}} sample pair, proportional {{to the importance of}} that pair during the learning phase. Due to its incremental learning capa-bility, FAMR can efficiently process large data sets and is an appropriate tool for data mining applications. We present new theoretical results characterizing the stochastic convergence of FAMR. ...|$|E
40|$|Abstract- An incremental, nonparametric {{probability}} estimation procedure using {{a variation}} of the Fuzzy ARTMAP (FAM) neural network is introduced. The resulted network, called Fuzzy ARTMAP with <b>Relevance</b> <b>factor</b> (FAMR), uses a <b>relevance</b> <b>factor</b> assigned to each sample pair, proportional to the importance of the respective pair during the learning phase. Experimental results have shown that FAMR favorably compares with FAM and Probabilistic FAM (PFAM, defined in 111, 121). both as a classifier and as a probability estimator. I...|$|E
40|$|The paper extends Competitive Repetition-suppression (CoRe) {{learning}} {{to deal with}} high dimensional data clustering. We show how CoRe {{can be applied to}} the automatic detection of the unknown cluster number and the simultaneous ranking of the features according to learned <b>relevance</b> <b>factors.</b> The effectiveness of the approach is tested on two freely available data sets from gene expression data and the results show that CoRe clustering is able to discover the true data partitioning in a completely unsupervised way, while it develops a feature ranking that is consistent with the state-of-the-art lists of gene relevance...|$|R
40|$|We {{propose a}} new matrix {{learning}} scheme to extend Generalized Relevance Learning Vector Quantization (GRLVQ). By introducing a full matrix of <b>relevance</b> <b>factors</b> {{in the distance}} measure, correlations between different features and their importance for the classification scheme can be taken into account. In comparison to the weighted euclidean metric used for GRLVQ, this metric is more powerful to represent the internal structure of the data appropriately while maintaining its excellent generalization ability as large margin optimizer. The algorithm is tested and compared to alternative LVQ schemes using an artificial dataset and the image segmentation data from the UCI repository. ...|$|R
40|$|Abstract:- This {{paper is}} divided into four parts: the first one {{introduces}} SADEX, a fuzzy Case Based Reasoning (CBR) System for fault diagnosis. The second focus on its observation <b>relevance</b> <b>factors</b> and shows how the results are {{in complete agreement with}} the relevance concept introduced by Robertson and Spark-Jones in their well known and proved technique for document retrieval. The third describes how equipment composition information can be used to generalize and adapt case solutions to new and unknown occurrences; this generalization is based on a taxonomic similarity between functionally autonomous modules (FAMs). Finally the MKM- Maintenance Knowledge Manager system is introduced...|$|R
40|$|We {{will focus}} here on {{approximating}} functions that map from the vector-valued real domain to the vector-valued real range. A Fuzzy ARTMAP (FAM) architecture, called Fuzzy Artmap with <b>Relevance</b> <b>factor</b> (FAMR, defined in [1]) is considered here {{as an alternative}} to function approximation. FAMR uses a <b>relevance</b> <b>factor</b> assigned to each sample pair, proportional to the importance of the respective pair during the learning phase, and is a generalization of PROBART (a FAM architecture defined in [2]). Like other FAM [...] based systems, FAMR can be incrementally trained...|$|E
40|$|The Fuzzy ARTMAP with <b>Relevance</b> <b>factor</b> (FAMR) is a Fuzzy ARTMAP (FAM) neural {{architecture}} {{with the}} fol-lowing property: Each training pair has a <b>relevance</b> <b>factor</b> assigned to it, {{proportional to the}} importance of that pair during the learning phase. Using a <b>relevance</b> <b>factor</b> adds more exibility to the training phase, allowing ranking of sample pairs according to the condence we have in the in-formation source. We focus on the prediction of biological activities of HIV- 1 protease inhibitory compounds, both known and novel, using a FAMR model. Our new approach consists of two stages: i) During the rst stage, we use a genetic algo-rithm (GA) to optimize the relevances assigned to the train-ing data. This improves the generalization capability of the FAMR. ii) In the second stage we use the optimized rele-vances to train the FAMR. Finally, the trained FAMR is used to predict the biological activities of newly designed poten-tial HIV- 1 protease inhibitors. 1...|$|E
3000|$|... is the {{normalized}} first-order Baum-Welch statistics, r is termed <b>relevance</b> <b>factor</b> {{which is}} an empirical value {{and has to be}} manually tuned. From Eq. (4), we can see that the posterior mean vectors of the cth Gaussian component M [...]...|$|E
40|$|Abstract. We {{propose a}} new matrix {{learning}} scheme to extend Generalized Relevance Learning Vector Quantization (GRLVQ). By introducing a full matrix of <b>relevance</b> <b>factors</b> {{in the distance}} measure, correlations between different features and their importance for the classification scheme can be taken into account. In comparison to the weighted euclidean metric used for GRLVQ, this metric is more powerful to represent the internal structure of the data appropriately while maintaining its excellent generalization ability as large margin optimizer. The algorithm is tested and compared to alternative LVQ schemes using an artificial dataset and the image segmentation data from the UCI repository. ...|$|R
40|$|Abstract. The paper extends Competitive Repetition-suppression (CoRe) {{learning}} {{to deal with}} high dimensional data clustering. We show how CoRe {{can be applied to}} the automatic detection of the unknown cluster number and the simultaneous ranking of the features according to learned <b>relevance</b> <b>factors.</b> The effectiveness of the approach is tested on two freely available data sets from gene expression data and the results show that CoRe clustering is able to discover the true data partitioning in a completely unsupervised way, while it develops a feature ranking that is consistent with the state-of-the-art lists of gene relevance. ...|$|R
40|$|Diabetes {{patients}} might {{suffer from}} an unhealthy life, long-term treatment and chronic complicated diseases. The decreasing hospitalization rate {{is a crucial}} problem for health care centers. This study combines the bagging method with base classifier decision tree and costs-sensitive analysis for diabetes patients' classification purpose. Real patients' data collected from a regional hospital in Thailand were analyzed. The <b>relevance</b> <b>factors</b> were selected and used to construct base classifier decision tree models to classify diabetes and non-diabetes patients. The bagging method was then applied to improve accuracy. Finally, asymmetric classification cost matrices were used to give more alternative models for diabetes data analysis...|$|R
40|$|Abstract. This paper {{shows the}} {{experimentation}} {{and the results}} obtained for LABERINTO research group at the ImageCLEF 2012 medical task. We focus our work on image retrieval based on textual information related to the image. Last year we demonstrated that query expansion exploiting the hierarchical structure of the MeSH descriptors achieved a significant improvement in image retrieval systems. This year {{our goal is to}} improve the results obtained last year adding a <b>relevance</b> <b>factor</b> to the query terms. In addition, we have developed a new strategy combining the expansion strategy based on the hierarchical MeSH structure with another expansion strategy very popular among researchers in this field, where the query terms are expanded using MMTx program. The experiments carried out have shown that a <b>relevance</b> <b>factor</b> for the query terms achieves a significant improvement for the results of the different expansion strategies...|$|E
40|$|FAM) neural {{architecture}} {{with the}} following property: Each training pair has a <b>relevance</b> <b>factor</b> assigned to it, proportional {{to the importance of}} that pair during the learning phase. Using a <b>relevance</b> <b>factor</b> adds more flexibility to the training phase, allowing ranking of sample pairs according to the confidence we have in the information source or in the pattern itself. We introduce a novel FAMR architecture: FAMR with Feature Weighting (FAM-RFW). In the first stage, the training data features are weighted. In our experiments, we use a feature weighting method based on Onicescu’s informational energy (IE). In the second stage, the obtained weights are used to improve FAMRFW training. The effect of this approach is that category dimensions in the direction of relevant features are decreased, whereas category dimensions in the direction of non-relevant feature are increased. Experimental results, performed on several benchmarks, show that feature weighting can improve the classification performance of the general FAMR algorithm...|$|E
30|$|The {{semantic}} {{components of}} cognitive programming {{are defined by}} morphisms from the model’sformal components to a set interpretation system. When the set interpretation system of the cognitive model is represented using sets of documents, each interpretive document may be an abstract, book, comment or voice message, visual image, diagram, statistical regularity, survey result, the scale of an expert’s assessment of the <b>relevance</b> <b>factor,</b> etc.|$|E
40|$|Recently a {{variation}} of learning vector quantization has been proposed in [1], which allows an automatic determination of <b>relevance</b> <b>factors</b> for the input dimensions: relevance learning vector quantization (RLVQ). RLVQ is heuristically motivated and may show instabilities for inappropriate data since it does not obey a gradient dynamics. Here we propose an energy function which describes the dynamics of RLVQ in the stable phase. It {{can be used to}} substitute the original dynamics for instable situations. Moreover, it yields to a batch version of RLVQ where hard competition can be substituted by soft clustering. Hence annealing schemes can be applied naturally in order to avoid local minima...|$|R
40|$|Prototype based classi cation oers {{intuitive}} and sparse {{models with}} excellent generalization ability. However, these models usually crucially {{depend on the}} underlying Euclidian metric; moreover, online variants likely suer from the problem of local optima. We here propose a generalization of learning vector quantization with three additional features: (I) it directly integrates neighborhood cooperation, hence is less aected by local optima; (II) the method can be combined with any dierentiable similarity measure whereby metric parameters such as <b>relevance</b> <b>factors</b> of the input dimensions can automatically be adapted according to the given data; (III) it obeys a gradient dynamics hence shows very robust behavior, and the chosen objective is related to margin optimization...|$|R
40|$|The term {{relevance}} weighting {{method has}} been shown to produce optimal information retrieval queries under well-defined conditions. The parameters needed to generate the term <b>relevance</b> <b>factors</b> cannot unfortunately be estimated accurately in practice; furthermore, in realistic test situations, it appears difficult to obtain improved retrieval results using the term relevance weights over much simpler term weighting systems such as, for example, the inverse document frequency weights. It is shown in this study that the inverse document frequency weights and the term relevance weights are closely related over a wide range of the frequency spectrum. Methods are introduced for estimating the term relevance weights, and experimental results are given comparing the inverse document frequency with the estimated term relevance weights...|$|R
40|$|In {{the present}} paper, we {{describe}} the improvement of our Question Answering System (QAS). We added keywords <b>relevance</b> <b>factor,</b> search refinement and fine grain type extraction of the expected answer to the system. We attempted to avoid using heavy natural language processing techniques in order to process large amounts of data from the newspaper corpus database. These changes have yielded promising experimental results. These changes and the experimental results are detailed herein...|$|E
40|$|Geographical {{proximity}} is {{an important}} ranking feature in many context aware recommendation systems. News recom-mender systems are an example of such systems where the proximity between users and the news ’ geographical context is a particularly important <b>relevance</b> <b>factor.</b> In this paper, we will discuss ways to improve distance ranking by taking geographical entity sizes, shapes, and footprints into account when handling news items that are ranked for specific users. CCS Concepts •Information systems → Spatial-temporal systems; Recommender systems...|$|E
30|$|One could deduce (9) from (7) and (8) {{by setting}} DTD = τ− 1 Σ {{and using the}} results in (5). The {{parameter}} τ {{is referred to as}} the <b>relevance</b> <b>factor,</b> which is set empirically in the range between 8 and 16 [7]. This is different from that in (7), where the matrix D is trained from a dataset using the EM algorithm in a manner similar to the matrix T for the i-vector. Secondly, the i-supervector is taken as the posterior of the latent variable z which is absent in the relevance MAP formulation.|$|E
40|$|We {{propose a}} method to {{automatically}} determine {{the relevance of the}} input dimensions of a learning vector quantization (LVQ) architecture during training. The method is based on Hebbian learning and introduces weighting factors of the input dimensions which are automatically adapted to the specific problem. The benefits are twofold: On the one hand, the incorporation of <b>relevance</b> <b>factors</b> in the LVQ architecture increases the overall performance of the classification and adapts the metric to the specific data used for training. On the other hand, the method induces a pruning algorithm, i. e. an automatic detection of the input dimensions which do not contribute to the overall classifier. Hence we obtain a possibly more efficient classification and we gain insight {{to the role of the}} data dimensions...|$|R
40|$|Abstract — Metric {{adaptation}} {{constitutes a}} powerful approach {{to improve the}} performance of prototype based classication schemes. We apply extensions of Generalized LVQ based on different adaptive distance measures {{in the domain of}} clinical proteomics. The Euclidean distance in GLVQ is extended by adaptive relevance vectors and matrices of global or local influence where training follows a stochastic gradient descent on an appropriate error function. We compare the performance of the resulting learning algorithms for the classification of high dimensional mass spectrometry data from cancer research. High prediction accuracies can be obtained by adapting full matrices of <b>relevance</b> <b>factors</b> in the distance measure in order to adjust the metric to the underlying data structure. The easy interpretability of the resulting models after training of relevance vectors allows to identify discriminative features in the original spectra. ...|$|R
40|$|We suggest and {{investigate}} {{the use of}} Generalized Matrix Relevance Learning (GMLVQ) {{in the context of}} discriminative visualization. This prototype-based, supervised learning scheme parameterizes an adaptive distance measure in terms of a matrix of <b>relevance</b> <b>factors.</b> By means of a few benchmark problems, we demonstrate that the training process yields low rank matrices which can be used efficiently for the discriminative visualization of labeled data. Comparison with well known standard methods illustrate the flexibility and discriminative power of the novel approach. The mathematical analysis of GMLVQ shows that the corresponding stationarity condition can be formulated as an eigenvalue problem with one or several strongly dominating eigenvectors. We also study the inclusion of a penalty term which enforces non-singularity of the relevance matrix and can be used to control the role of higher order eigenvalues, efficiently. ...|$|R
40|$|One of {{the most}} {{important}} challenges for the researchers in the 21 st Century is related to global heating and climate change that can have as consequence the intensification of natural hazards. Another problem of changes in the Earth's climate is its impact in the agriculture production. In this scenario, application of statistical models as well as development of new methods become very important to aid in the analyses of climate from ground-based stations and outputs of forecasting models. Additionally, remote sensing images have been used to improve the monitoring of crop yields. In this context we propose a new technique to identify extreme values in climate time series and to correlate climate and remote sensing data in order to improve agricultural monitoring. Accordingly, this paper presents a new unsupervised algorithm, called CLIPSMiner (CLImate PatternS Miner) that works on multiple time series of continuous data, identifying relevant patterns or extreme ones according to a <b>relevance</b> <b>factor,</b> which can be tuned by the user. Results show that CLIPSMiner detects, as expected, patterns that are known in climatology, indicating the correctness and feasibility of the proposed algorithm. Moreover, patterns detected using the highest <b>relevance</b> <b>factor</b> is coincident with extreme phenomena. Furthermore, series correlations detected by the algorithm show a relation between agroclimatic and vegetation indices, which confirms the agrometeorologists' expectations. 201...|$|E
40|$|Abstract. Huge {{amounts of}} climate and remote sensing data have been stored by several {{institutions}} {{in the past few}} years. Properly analyzed, these databases can reveal useful information, which can help researchers to monitor and estimate the production of agricultural crops. Recently, the information and knowledge discovered from these data have also been used for research on climate changes as well as to increase the sustainable use of the soil, making farms more productive. Data mining techniques are the main tool to analyze and extract useful information, relationships and meaningful patterns. Accordingly, this paper presents a new unsupervised algorithm, called CLIPSMiner (Climate Patterns Miner) that aims at discovering relevant patterns with specific constraints in climate and remote sensing time series. This new algorithm works on multiple time series of continuous data, identifying all patterns or the most relevant ones according to a <b>relevance</b> <b>factor,</b> which can be tuned by the user. Experiments with synthetic and real data were performed. The results show that the algorithm detects some patterns that are known in climatology, as expected, indicating the correctness and feasibility of the proposed algorithm. Moreover, patterns detected using the highest <b>relevance</b> <b>factor</b> are coincident with extreme phenomena as many days without rain or heavy rain. This new algorithm can be used by climatology specialists to mine and discover knowledge from their long series of past and forecasting data...|$|E
30|$|For {{the back}} end, GMM-UBM [1] and the i-vector [2] methods were considered. For the GMM-UBM, two gender-dependent UBM models with 1024 {{mixtures}} were trained by pooling all available gender-dependent training data. The speaker models were then adapted from their respective gender’s UBM with a <b>relevance</b> <b>factor</b> of 16. For the i-vector framework, same UBMs and the pooled training data {{were used to}} train the total variability matrix in 20 iterations, and then, 100 dimensional i-vectors were extracted from each utterance. Linear discriminant analysis (LDA) was used to reduce the channel mismatch effects, and probabilistic LDA was employed for scoring the i-vectors. MSR Identity Toolbox [42] was used {{in all of the}} classification and scoring phases.|$|E
40|$|Metric {{adaptation}} {{constitutes a}} powerful approach {{to improve the}} performance of prototype based classication schemes. We apply extensions of Generalized LVQ based on different adaptive distance measures {{in the domain of}} clinical proteomics. The Euclidean distance in GLVQ is extended by adaptive relevance vectors and matrices of global or local influence where training follows a stochastic gradient descent on an appropriate error function. We compare the performance of the resulting learning algorithms for the classification of high dimensional mass spectrometry data from cancer research. High prediction accuracies can be obtained by adapting full matrices of <b>relevance</b> <b>factors</b> in the distance measure in order to adjust the metric to the underlying data structure. The easy interpretability of the resulting models after training of relevance vectors allows to identify discriminative features in the original spectra. ...|$|R
40|$|Abstract—This paper {{interests}} in social search over social networking services, typically in microblogging networks. We propose {{a new approach}} that integrates, within a Bayesian network model, new <b>relevance</b> <b>factors</b> such as the social importance of microbloggers and the temporal magnitude of tweets. In particular, the social importance of a microblogger is assimilated to his influence on the social network. This property is evaluated by applying PageRank algorithm on the social network of retweets and mentions. The temporal magnitude of microblogs is estimated based on temporal neighbors that present similar query terms. To validate our approach, we {{conducted a series of}} experiments on the TREC 2011 Microblog dataset. Results show that the integration of social and temporal features increases the retrieval effectiveness. Keywords-Microblogs; Tweet search; Social network; In-fluence; Time magnitud...|$|R
40|$|Abstract. We {{propose a}} method to {{automatically}} determine the rel-evance of the input dimensions of a learning vector quantization (LVQ) architecture during training. The method is based on Hebbian learning and introduces weighting factors of the input dimensions which are auto-matically adapted to the specic problem. The benets are twofold: On the one hand, the incorporation of <b>relevance</b> <b>factors</b> in the LVQ archi-tecture increases the overall performance of the classication and adapts the metric to the specic data used for training. On the other hand, the method induces a pruning algorithm, i. e. an automatic detection of the input dimensions which do {{not contribute to the}} overall classier. Hence we obtain a possibly more eÆcient classication and we gain insight {{to the role of the}} data dimensions. 1...|$|R
