31|29|Public
2500|$|In DOS memory management, {{conventional}} memory, {{also called}} base memory, {{is the first}} 640 kilobytes (...) of the memory on IBM PC or compatible systems. It is the <b>read-write</b> <b>memory</b> directly addressable by the processor {{for use by the}} operating system and application programs. As memory prices rapidly declined, this design decision became a limitation in the use of large memory capacities until the introduction of operating systems and processors that made it irrelevant.|$|E
5000|$|They copied {{themselves}} to <b>read-write</b> <b>memory</b> before execution, or ...|$|E
5000|$|Many Harvard {{architecture}} microcontrollers cannot execute {{instructions in}} <b>read-write</b> <b>memory,</b> but only instructions in memory that it cannot write to, ROM or non-self-programmable flash memory.|$|E
40|$|Sequential <b>read-write</b> <b>memories</b> (SRWMs) are RAMs {{without an}} address decoder. A shift {{register}} {{is used instead}} to point at subsequent memory locations. SRWMs consume less power than RAMs of the same size. Algorithms are presented to check whether a set of storage values fits in a single SRWM and to automatically map storage values in as few SRWMs as possible. Benchmark results show that good assignments can be obtained {{in spite of the}} limited addressing capabilitie...|$|R
40|$|Four {{combinatorial}} optimization problems {{occurring in the}} memory-mapping subtask of high-level synthesis {{turn out to be}} intractable as is proven here. The first problem is cyclic register assignment in which repetitive read-write patterns of storage values have to be stored in a minimal number of locations situated in a register file with random access (RAM). The second problem is to partition a set of storage values in as few RAMs as possible. The remaining problems concern the counterparts of the RAM problems for sequential <b>read-write</b> <b>memories...</b>|$|R
40|$|The design, development, and {{implementation}} of a prototype, partially populated, million bit <b>read-write</b> holographic <b>memory</b> system using state-of-the-art components are described. The system employs an argon ion laser, acoustooptic beam deflectors, a holographic beam splitter (hololens), a nematic liquid crystal page composer, a photoconductor-thermoplastic erasable storage medium, a silicon P-I-N photodiode array, with lenses and electronics of both conventional and custom design. Operation of the prototype memory system was successfully demonstrated. Careful attention {{is given to the}} analysis from which the design criteria were developed. Specifications for the major components are listed, along with the details of their construction and performance. The primary conclusion resulting from this program is that the basic principles of <b>read-write</b> holographic <b>memory</b> system are well understood and are reducible to practice...|$|R
50|$|Since the {{emergence}} of microcontrollers, many different memory technologies have been used. Almost all microcontrollers {{have at least two}} different kinds of memory, a non-volatile memory for storing firmware and a <b>read-write</b> <b>memory</b> for temporary data.|$|E
50|$|In a Harvard architecture, {{there is}} no need to make the two {{memories}} share characteristics. In particular, the word width, timing, implementation technology, and memory address structure can differ. In some systems, instructions for pre-programmed tasks can be stored in read-only memory while data memory generally requires <b>read-write</b> <b>memory.</b> In some systems, there is much more instruction memory than data memory so instruction addresses are wider than data addresses.|$|E
5000|$|In modern diesel trucks, EDRs are {{triggered}} by electronically sensed {{problems in the}} engine (often called faults), or a sudden change in wheel speed. One {{or more of these}} conditions may occur because of an accident. Information from these devices can be collected after a crash and analyzed to help determine what the vehicles were doing before, during and after the crash or event. The term generally refers to a simple, tamper-proof, <b>read-write</b> <b>memory</b> device.|$|E
40|$|A {{heterodyne}} readout {{technique for}} <b>read-write</b> holographic <b>memory</b> systems that reconstruct a virtual image wavefront is described and demonstrated. The conventionally recorded hologram is illuminated simultaneously with a suitable combination of temporally modulated reference and modified object waves for readout. Best performance is obtained for temporal phase modulation. The coupling of the illuminating wavefronts by the hologram is analyzed...|$|R
25|$|AGC {{software}} {{was written in}} AGC assembly language and stored on rope memory. The bulk of the {{software was}} on read-only rope memory and thus couldn't be changed in operation, but some key parts of the software were stored in standard <b>read-write</b> magnetic-core <b>memory</b> and could be overwritten by the astronauts using the DSKY interface, as was done on Apollo 14.|$|R
40|$|Sequential <b>read-write</b> <b>memories</b> (SRWMs) {{could be}} used as an {{alternative}} to register files in data-path synthesis. They can be considered RAMs without address decoder. A shift register is used instead to sequentially point at memory locations for read or write. Although SRWMs behave similarly to the memory structures recently proposed by Aloqeely and Chen, they are more interesting because of their lower power consumption. Algorithms are presented to check whether a set of storage value fit in a single SRWM (exactly by means of branch-and-bound) and to automatically map storage values in as few SRWMs as possible (by means of heuristics). As opposed to Aloqeely and Chen, also good benchmark results have been obtained for applications with a low degree of "regularity". 1 Introduction High-level synthesis, the automatic mapping of an algorithmic description of some computation to a description at the register-transfer level, is normally tackled by dividing the problem into a number of subpr [...] ...|$|R
5000|$|<b>Read-write</b> <b>memory</b> {{is a type}} of {{computer}} memory that may be relatively easily written to as well as read from, that is, using electrical signalling normally associated with running software, and without other physical processes (unlike ROM or [...] "read-only memory" [...] and distinct from EEPROM). The related term RAM (for [...] "random access memory") means something different; it refers to memory that can access any memory location in a constant amount of time.|$|E
50|$|In DOS memory management, {{conventional}} memory, {{also called}} base memory, {{is the first}} 640 kilobytes (640 Ã— 1024 bytes) of the memory on IBM PC or compatible systems. It is the <b>read-write</b> <b>memory</b> directly addressable by the processor {{for use by the}} operating system and application programs. As memory prices rapidly declined, this design decision became a limitation in the use of large memory capacities until the introduction of operating systems and processors that made it irrelevant.|$|E
50|$|Engineers {{normally}} {{write the}} microcode during the design {{phase of a}} processor, storing it in a read-only memory (ROM) or programmable logic array (PLA) structure, or in a combination of both. However, machines also exist that have some or all microcode stored in SRAM or flash memory. This is traditionally denoted as writeable control store {{in the context of}} computers, which can be either read-only or <b>read-write</b> <b>memory.</b> In the latter case, the CPU initialization process loads microcode into the control store from another storage medium, with the possibility of altering the microcode to correct bugs in the instruction set, or to implement new machine instructions.|$|E
5000|$|In {{contemporary}} usage, [...] "memory" [...] {{is usually}} semiconductor storage <b>read-write</b> random-access <b>memory,</b> typically DRAM (dynamic RAM) {{or other forms}} of fast but temporary storage. [...] "Storage" [...] consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down).|$|R
5000|$|The 8088 {{processor}} of the IBM PC and IBM PC/XT {{could address}} one Megabyte (MB or 220 bytes) of memory. It inherited this limit from the 20-bit {{external address bus}} of the Intel 8086. The designers of the PC allocated the lower 640 kB (655,360 bytes) of address space for <b>read-write</b> program <b>memory</b> (RAM), called [...] "conventional memory", and the remaining 384 kB of memory space was reserved for uses such as the system BIOS, video memory, and memory on expansion peripheral boards.|$|R
25|$|The Apollo Guidance Computer (AGC) is {{a digital}} {{computer}} produced for the Apollo {{program that was}} installed on board each Apollo Command Module (CM) and Lunar Module (LM). The AGC provided computation and electronic interfaces for guidance, navigation, {{and control of the}} spacecraft. The AGC had a 16-bit word length, with 15 data bits and one parity bit. Most of the software on the AGC was stored in a special read only memory known as core rope memory, fashioned by weaving wires through magnetic cores, though a small amount of <b>read-write</b> core <b>memory</b> was provided.|$|R
50|$|The {{original}} System/360 {{models of}} IBM mainframe had read-only control store, but later System/360, System/370 and successor models loaded part {{or all of}} their microprograms from floppy disks or other DASD into a writable control store consisting of ultra-high speed random-access <b>read-write</b> <b>memory.</b> The System/370 architecture included a facility called Initial-Microprogram Load (IML or IMPL) that could be invoked from the console, as part of Power On Reset (POR) or from another processor in a tightly coupled multiprocessor complex. This permitted IBM to easily repair microprogramming defects in the field. Even when {{the majority of the}} control store is stored in ROM, computer vendors would often sell writable control store as an option, allowing the customers to customize the machine's microprogram. Other vendors, e.g., IBM, use the WCS to run microcode for emulator features and hardware diagnostics.|$|E
5000|$|The AGC was a 15-bit plus parity {{machine with}} a 1 MHz clock. It was about one cubic foot in volume and weighed about 80 pounds. It used {{integrated}} circuit NOR gates, two to a package, but integrated RAM and ROM devices {{had not been}} developed yet. It had 2,000 words of magnetic ferrite core <b>read-write</b> <b>memory</b> and maybe 24 thousand words of read-only memory {{in the form of}} magnetic core ropes. These cores used metal tape magnetic cores. With such limited computing resources, the software had to be extremely tightly written in assembly code. These computers were designed with extremely long mean times to failure. Fifty were built by Raytheon, and none failed during several years of life. The human interface was a keyboard with ten digit keys and a few auxiliary keys for such things as [...] "+" [...] and [...] "-" [...] and [...] "enter", and a display with three numbers of 5 decimal/octal digits, a two-digit program number, and a two-digit verb and a two-digit noun. The astronauts used it for all phases of command.|$|E
5000|$|The Intel 4004 is {{generally}} regarded as the first commercially available microprocessor, and cost [...] The first known advertisement for the 4004 is dated November 15, 1971 and appeared in Electronic News. The project that produced the 4004 originated in 1969, when Busicom, a Japanese calculator manufacturer, asked Intel to build a chipset for high-performance desktop calculators. Busicom's original design called for a programmable chip set consisting of seven different chips. Three of the chips were to make a special-purpose CPU with its program stored in ROM and its data stored in shift register <b>read-write</b> <b>memory.</b> Ted Hoff, the Intel engineer assigned to evaluate the project, believed the Busicom design could be simplified by using dynamic RAM storage for data, rather than shift register memory, and a more traditional general-purpose CPU architecture. Hoff {{came up with a}} four-chip architectural proposal: a ROM chip for storing the programs, a dynamic RAM chip for storing data, a simple I/O device and a 4-bit central processing unit (CPU). Although not a chip designer, he felt the CPU could be integrated into a single chip, but as he lacked the technical know-how the idea remained just a wish for the time being.|$|E
50|$|A stored-program digital {{computer}} {{is one that}} keeps its program instructions, {{as well as its}} data, in <b>read-write,</b> random-access <b>memory</b> (RAM). Stored-program computers were an advancement over the program-controlled computers of the 1940s, such as the Colossus and the ENIAC, which were programmed by setting switches and inserting patch cables to route data and to control signals between various functional units. In the vast majority of modern computers, the same memory is used for both data and program instructions, and the von Neumann vs. Harvard distinction applies to the cache architecture, not the main memory (split cache architecture).|$|R
50|$|Another form of {{core memory}} called core rope memory {{provided}} read-only storage. In this case, the cores, which had more linear magnetic materials, were simply used as transformers; no information was actually stored magnetically within the individual cores. Each {{bit of the}} word had one core. Reading {{the contents of a}} given memory address generated a pulse of current in a wire corresponding to that address. Each address wire was threaded either through a core to signify a binary 1, or around the outside of that core, to signify a binary 0. As expected, the cores were much larger physically than those of <b>read-write</b> core <b>memory.</b> This type of memory was exceptionally reliable. An example was the Apollo Guidance Computer used for the moon landings.|$|R
40|$|Abstract: The present work {{is about}} an {{advanced}} architecture for generating video signals from microprocessor-based systems. This architecture is a solution for real time image processing hardware/software systems which require a significant recording time and uses a reduced area of physical addressing of microprocessor-based systems. This solution investigates both the Fast Physical Addressing and the simultaneous video <b>memory</b> <b>read-write</b> system. This latter is ensured by a split of the hardware video memory in separate capacities and by association of a selecting circuit...|$|R
5000|$|Busicom had {{designed}} their own special-purpose LSI chipset {{for use in}} their Busicom 141-PF calculator with integrated printer, following the architectural model of the Olivetti Programma 101, the worldâ€™s first tabletop programmable calculator, introduced in 1965, and commissioned Intel to develop it for production. Like the Olivetti Programma 101, the Busicom design used serial <b>read-write</b> <b>memory.</b> The Busicom memory was based on MOS shift registers rather than the costly Olivetti memory based on magnetostriction wire. However, Intel determined it was too complex, since serial memories required more components, and would use 40 pins, a packaging standard different from Intelâ€™s own 16-pin standard {{and so it was}} proposed that a new design produced with standard 16-pin DIP packaging and reduced instruction set be developed., using Intelâ€™s newly developed dynamic RAM memory. This resulted in the 4004 architecture, which is part of a family of chips, including ROM, DRAM, and serial-to-parallel shift register chips. The 4004 was subsequently designed by Federico Faggin [...] using silicon gate technology and built of approximately 2,300 transistors and was followed the next year by the first ever 8-bit microprocessor, the 3,500 transistor 8008 (and the 4040, a revised and improved 4004). It was not until the development of the 40-pin 8080 in 1974, a project conceived and directed by Faggin [...] that the address and data buses would be separated, giving faster and simpler access to memory.|$|E
40|$|Causal {{consistency}} {{is one of}} {{the most}} adopted consistency criteria for distributed implementations of data structures. It ensures that operations are executed at all sites according to their causal precedence. We address the issue of verifying automatically whether the executions of an implementation of a data structure are causally consistent. We consider two problems: (1) checking whether one single execution is causally consistent, which is relevant for developing testing and bug finding algorithms, and (2) verifying whether all the executions of an implementation are causally consistent. We show that the first problem is NP-complete. This holds even for the <b>read-write</b> <b>memory</b> abstraction, which is a building block of many modern distributed systems. Indeed, such systems often store data in key-value stores, which are instances of the <b>read-write</b> <b>memory</b> abstraction. Moreover, we prove that, surprisingly, the second problem is undecidable, and again this holds even for the <b>read-write</b> <b>memory</b> abstraction. However, we show that for the <b>read-write</b> <b>memory</b> abstraction, these negative results can be circumvented if the implementations are data independent, i. e., their behaviors do not depend on the data values that are written or read at each moment, which is a realistic assumption. Comment: extended version of POPL 201...|$|E
40|$|The wait-free <b>read-write</b> <b>memory</b> {{model has}} been {{characterized}} as an iterated Immediate Snapshot (IS) task. The IS task is affine Â— it {{can be defined as}} a (sub) set of simplices of the standard chromatic subdivision. In this paper, we highlight the phenomenon of a "natural" model that can be captured by an iterated affine task and, thus, by a subset of runs of the iterated immediate snapshot model. We show that the <b>read-write</b> <b>memory</b> model in which, additionally, k-set-consensus objects can be used is "natural" by presenting the corresponding simple affine task captured by a subset of 2 -round IS runs. As an "unnatural" example, the model using the abstraction of Weak Symmetry Breaking (WSB) cannot be captured by a set of IS runs and, thus, cannot be represented as an affine task. Our results imply the first combinatorial characterization of models equipped with abstractions other than <b>read-write</b> <b>memory</b> that applies to generic tasks...|$|E
40|$|The {{response}} of iron doped lithium niobate under conditions corresponding to hologram storage and retrieval is described, and the material's characteristics are discussed. The optical sensitivity {{can be improved}} by heavy chemical reduction of lightly doped crystals such {{that most of the}} iron is in the divalent state, the remaining part being trivalent. The best reduction process found to be reproducible so far is the anneal of the doped crystal {{in the presence of a}} salt such as lithium carbonate. It is shown by analysis and simulation that a page-oriented <b>read-write</b> holographic <b>memory</b> with 1, 000 bits per page would have a cycle time of about 60 ms and a signal-to-noise ratio of 27 db. This cycle time, although still too long for a practical system, represents an improvement of two orders of magnitude over that of previous laboratory prototypes using different storage media...|$|R
40|$|We {{present an}} {{algorithmic}} test for deterministic wait-free solvability of decision tasks in asynchronous distributed systems whose processes communicate via <b>read-write</b> shared <b>memory.</b> Input {{to the test}} is a formal representation of the decision task as a triple (I; O; Î”), where I and O are simplicial complexes specifying the inputs and outputs of the task and Î” is the input-output relation of the task. The form of I, O, and Î” also fixes the system size (i. e., number of processes). The result of the test is either 1) {{that there is no}} wait-free solution to the decision task for the given system size or 2) inconclusive. Incompleteness of the test is unavoidable since wait-free solvability of decision tasks is undecidable for a system of size at least three. The test is shown to detect the impossibility of wait-free consensus for all systems, and experimental results show that the test detects the impossibility of wait-free set consensus for systems of size at most [...] ...|$|R
40|$|We {{introduce}} {{a new model of}} partial synchrony for <b>read-write</b> shared <b>memory</b> systems. This model is based on the notion of set timelinessâ€”a natural and straightforward generalization of the seminal concept of timeliness in the partially synchrony model of Dwork, Lynch and Stockmeyer [11]. Despite its simplicity, the concept of set timeliness is powerful enough to define a family of partially synchronous systems that closely match individual instances of the t-resilient k-set agreement problem among n processes, henceforth denoted (t, k, n) agreement. In particular, we use it to give a partially synchronous system that is is synchronous enough for solving (t, k, n) agreement, but not enough for solving two incrementally stronger problems, namely, (t + 1, k, n) -agreement, which has a slightly stronger resiliency requirement, and (t, k âˆ’ 1, n) -agreement, which has a slightly stronger agreement requirement. This is the first partially synchronous system that separates between these sub-consensus problems. The above results show that set timeliness can be used to study and compare the partial synchrony requirements of problems that are strictly weaker than consensus...|$|R
40|$|Holographic {{storage and}} {{retrieval}} using photorefractive media (electro-optic ferroelectric materials), particularly iron-doped lithium niobate with its enhanced sensitivity, are discussed. Refractive index changes induced {{by exposure to}} light render the materials useful for read-write memories and <b>read-write</b> <b>memory</b> simulation. Resolution, dark storage time, write and erase times, reversibility, and noise levels of the materials are examined. The laser source, deflection system, hololens, page composer, and detector array of the holographic memory system are described. High SNR and two orders of magnitude improvement in speed are reported over earlier experimental prototypes, but the system is still too slow to meet practical needs...|$|E
40|$|Real-time {{data storage}} and {{processing}} using optical techniques have been considered in recent years. Of particular interest are photosensitive electro-optic crystals which permit volume storage {{in the form of}} phase holograms, by means of a charge transfer process. A survey {{of the state of the}} art of such holographic memories is presented. The physical mechanism responsible for the formation of phase holograms in such crystals is discussed. Attention is focused on various aspects of materials characterization, development and utilization. Experimental reversible holographic <b>read-write</b> <b>memory</b> systems with fast random access and high storage capacity employing this new class of photosensitive materials have already been demonstrated...|$|E
40|$|The {{asynchronous}} computability theorem (ACT) uses concepts from combinatorial topology {{to characterize}} which tasks have wait-free solutions in <b>read-write</b> <b>memory.</b> A task {{can be expressed}} as a relation between two chromatic simplicial complexes. The theorem states that a task has a protocol (algorithm) {{if and only if}} there is a certain chromatic simplicial map compatible with that relation. While the original proof of the ACT relied on an involved combinatorial argument, Borowsky and Gafni later proposed an alternative proof that relied on a algorithmic construction, termed the "convergence algorithm". The description of this algorithm was incomplete, and presented without proof. In this paper, we give the first complete description, along with a proof of correctness. Comment: 16 pages, 2 figure...|$|E
40|$|We {{study the}} growth rate of the hard squares lattice gas, {{equivalent}} to the number of independent sets on the square lattice, and two related models - non-attacking kings and <b>read-write</b> isolated <b>memory.</b> We use an assortment of techniques from combinatorics, statistical mechanics and linear algebra to prove upper bounds on these growth rates. We start from Calkin and Wilf's transfer matrix eigenvalue bound, then bound that with the Collatz-Wielandt formula from linear algebra. To obtain an approximate eigenvector, we use an ansatz from Baxter's corner transfer matrix formalism, optimised with Nishino and Okunishi's corner transfer matrix renormalisation group method. This results in an upper bound algorithm which no longer requires exponential memory and so is much faster to calculate than a direct evaluation of the Calkin-Wilf bound. Furthermore, it is extremely parallelisable and so allows us to make dramatic improvements to the previous best known upper bounds. In all cases we reduce the gap between upper and lower bounds by 4 - 6 orders of magnitude. Comment: Also submitted to FPSAC 2015 conferenc...|$|R
40|$|The Iterated Immediate Snapshot model (IIS), {{due to its}} elegant {{geometrical}} representation, {{has become}} standard for applying topological reasoning to distributed computing. Its modular structure {{makes it easier to}} analyze than the more realistic (non-iterated) <b>read-write</b> Atomic-Snapshot <b>memory</b> model (AS). It is known that AS and IIS are equivalent with respect to wait-free task computability: a distributed task is solvable in AS if and only if it solvable in IIS. We observe, however, that this equivalence is not sufficient in order to explore solvability of tasks in sub-models of AS (i. e. proper subsets of its runs) or computability of long-lived objects, and a stronger equivalence relation is needed. In this paper, we consider adversarial sub-models of AS and IIS specified by the sets of processes that can be correct in a model run. We show that AS and IIS are equivalent in a strong way: a (possibly long-lived) object is implementable in AS under a given adversary if and only if it is implementable in IIS under the same adversary. ...|$|R
40|$|We {{introduce}} {{a new model of}} partial synchrony for <b>read-write</b> shared <b>memory</b> systems. This model is based on the notion of set timelinessâ€”a natural and straightforward generalization of the seminal concept of timeliness in the partially synchrony model of Dwork, Lynch and Stockmeyer [8]. Despite its simplicity, the concept of set timeliness is powerful enough to describe the first partially synchronous system for read/write shared memory that separates consensus and set agreement: we show that this system has enough timeliness for solving set agreement but not enough for solving consensus. Set timeliness also allows us to define a family of partially synchronous systems of n processes, denoted S k n (1 â‰¤kâ‰¤nâˆ’ 1), which closely matches the family of k-anti-Î© failure detectors that were recently shown to be the weakest failure detectors for the k-set agreement problem: We prove that for 1 â‰¤kâ‰¤nâˆ’ 1, S k n is synchronous enough to implement k-anti-Î© but not enough to implement (kâˆ’ 1) -anti-Î©. The results above show that set timeliness can be used to study and compare the partial synchrony requirements of problems that are strictly weaker than consensus...|$|R
