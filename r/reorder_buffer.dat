135|56|Public
50|$|<b>Reorder</b> <b>Buffer</b> (ROB): A {{structure}} that is sequentially (circularly) indexed on a per-operation basis, for instructions in flight. It differs from a history buffer because the <b>reorder</b> <b>buffer</b> typically comes after the future file (if it exists) {{and before the}} architectural register file.|$|E
5000|$|... #Subtitle level 3: Decode, and the {{instruction}} <b>reorder</b> <b>buffer</b> ...|$|E
50|$|Graduation copies {{the value}} from the <b>reorder</b> <b>buffer</b> into the {{architectural}} register file. The sole {{use of the}} architectural register file is to recover from exceptions and branch mispredictions.|$|E
50|$|<b>Reorder</b> <b>buffers</b> can be data-less or data-ful.|$|R
40|$|Abstract: In the <b>reordering</b> <b>buffer</b> problem, we {{are given}} an input {{sequence}} of requests for service each of which corresponds to a point in a metric space. The cost of serving the requests heavily depends on the processing order. When serving a request the cost {{is equal to the}} distance, in the metric space, between this request and the previously served request. A <b>reordering</b> <b>buffer</b> with storage capacity k can be used to reorder the input sequence in a restricted fashion so as to construct an output sequence with lower service cost. This simple and universal framework is useful for many applications in computer science and economics, e. g., disk scheduling, rendering in computer graphics, or painting shops in car plants. In this paper, we design online algorithms for the <b>reordering</b> <b>buffer</b> problem where the goal is to minimize the total cost. Our main result is a strategy with a polylogarithmic competitive ratio for general metric spaces. Previous work on the <b>reordering</b> <b>buffer</b> problem only considered very restricted metric spaces. We obtain our result by first developing...|$|R
40|$|ABSTRACT In the <b>reordering</b> <b>buffer</b> problem, we {{are given}} an input {{sequence}} of requests for service each of which corresponds to a point in a metric space. The cost of serving the requests heavily depends on the processing order. Serving a request induces cost corresponding to the distance between itself and the previously served request, measured in the underlying metric space. A <b>reordering</b> <b>buffer</b> with storage capacity k {{can be used to}} reorder the input sequence in a restricted fashion so as to construct an output sequence with lower service cost. This simple and universal framework is useful for many applications in computer science and economics, e. g., disk scheduling, rendering in computer graphics, or painting shops in car plants. In this paper, we design online algorithms for the <b>reordering</b> <b>buffer</b> problem. Our main result is a strategy with a polylogarithmic competitive ratio for general metric spaces. Previous work on the <b>reordering</b> <b>buffer</b> problem only considered very restricted metric spaces. We obtain our result by first developing a deterministic algorithm for arbitrary weighted trees with a competitive ratio of O(D * log k), where D denotes the unweighted diameter of the tree, i. e. ...|$|R
50|$|Architectural Register File or Retirement Register File (RRF): The {{committed}} register {{state of}} the machine. RAM indexed by logical register number. Typically written into as results are retired or committed out of a <b>reorder</b> <b>buffer.</b>|$|E
50|$|Execution {{results are}} {{written to the}} <b>reorder</b> <b>buffer,</b> to the {{reservation}} stations (if the issue queue entry has a matching tag), and to the future file {{if this is the}} last instruction to target that architectural register (in which case register is marked ready).|$|E
50|$|During {{the third}} stage, the {{instructions}} are decoded. In the fourth stage, they {{are placed in}} the instruction <b>reorder</b> <b>buffer</b> (IRB). The IRB's purpose is the implement register renaming, out of order execution, speculative execution {{and to provide a}} temporary place for results to be stored until the instructions are retired. The IRB determines which instructions are issued during stage five.|$|E
5000|$|Smith {{was awarded}} the 1999 Eckert-Mauchly Award [...] "for {{fundamental}} contributions to high performance micro-architecture, including saturating counters for branch prediction, <b>reorder</b> <b>buffers</b> for precise exceptions, decoupled access/execute architectures, and vector supercomputer organization memory, and interconnects." ...|$|R
40|$|Following {{the embargo}} period the above license applies. In the <b>reordering</b> <b>buffer</b> {{management}} problem, {{a sequence of}} colored items arrives at a service station to be processed. Each color change between two consecutively processed items generates some cost. A <b>reordering</b> <b>buffer</b> of capacity k items {{can be used to}} preprocess the input sequence in order to decrease the number of color changes. The goal is to find a scheduling strategy that, using the <b>reordering</b> <b>buffer,</b> minimizes the number of color changes in the given sequence of items. We consider the problem in the setting of online computation with advice. In this model, the color of an item becomes known only at the time when the item enters the <b>reordering</b> <b>buffer.</b> Additionally, together with each item entering the buffer, we get a fixed number of advice bits, which can be seen as information about the future or as information about an optimal solution (or an approximation thereof) for the whole input sequence. We show that for any ε> 0 there is a (1 +ε) -competitive algorithm for the problem which uses only a constant (depending on ε) number of advice bits per input item. This also immediately implies a (1 +ε) -approximation algorithm which has 2 O(nlog 1 /ε) running time (this should be compared to the trivial optimal algorithm which has a running time of kO(n)). We complement the above result by presenting a lower bound of Ω(logk) bits of advice per request for any 1 -competitive algorithm. Peer-reviewedPost-prin...|$|R
40|$|We give almost tight bounds for {{the online}} <b>reordering</b> <b>buffer</b> {{management}} {{problem on the}} uniform metric. Specifically, we present the first non-trivial lower bounds for this problem by showing that deterministic online algorithms have a competitive ratio of at least Ω (√ log k / log log k) and randomized online algorithms have a competitive ratio of at least Ω(log log k), where k denotes {{the size of the}} buffer. We complement this by presenting a deterministic online algorithm for the <b>reordering</b> <b>buffer</b> management problem that obtains a competitive ratio of O (√ log k), almost matching the lower bound. This improves upon an algorithm by Avigdor-Elgrabli and Rabani (SODA 2010) that achieves a competitive ratio of O(log k / log log k) ...|$|R
50|$|Furthermore, the {{reservation}} station scheme has four places (Future File, Reservation Station, <b>Reorder</b> <b>Buffer</b> and Architectural File) where a result value can be stored, whereas the tag-indexed scheme has just one (the physical register file). Because {{the results from}} the functional units, broadcast to all these storage locations, must reach a much larger number of locations in the machine than in the tag-indexed scheme, this function consumes more power, area, and time. Still, in machines equipped with very accurate branch prediction schemes and if execute latencies are a major concern, reservation stations can work remarkably well.|$|E
5000|$|In later x86 implementations, like Nehalem {{and later}} processors, both integer and {{floating}} point registers are now {{incorporated into a}} unified octa-ported (six read and two write) general-purpose register file (8 + 8 in 32-bit and 16 + 16 in x64 per file), while the register file extended to 2 with enhanced [...] "Shadow Register File Architecture" [...] in favorite of executing hyper threading and each thread uses independent register files for its decoder. Later Sandy bridge and onward replaced shadow register table and architectural registers with much large and yet more advance physical register file before decoding to the <b>reorder</b> <b>buffer.</b> Randered that Sandy Bridge and onward no longer carry an architectural register.|$|E
50|$|In the {{renaming}} stage, every architectural register referenced (for read or write) is {{looked up}} in an architecturally-indexed remap file. This file returns a tag and a ready bit. The tag is non-ready {{if there is a}} queued instruction which will write to it that has not yet executed. For read operands, this tag takes the place of the architectural register in the instruction. For every register write, a new tag is pulled from a free tag FIFO, and a new mapping is written into the remap file, so that future instructions reading the architectural register will refer to this new tag. The tag is marked as unready, because the instruction has not yet executed. The previous physical register allocated for that architectural register is saved with the instruction in the <b>reorder</b> <b>buffer,</b> which is a FIFO that holds the instructions in program order between the decode and graduation stages.|$|E
40|$|Abstract. In the <b>reordering</b> <b>buffer</b> {{management}} problem, {{a sequence}} of colored items arrives at a service station to be processed. Each color change between two consecutively processed items generates cost. A re-ordering buffer of capacity k items {{can be used to}} preprocess the input sequence in order to decrease the number of color changes. The goal is to find a scheduling strategy that, using the <b>reordering</b> <b>buffer,</b> minimizes the number of color changes in the given sequence of items. We consider the problem in the setting of online computation with ad-vice. In this model, the color of an item becomes known only at the time when the item enters the <b>reordering</b> <b>buffer.</b> Additionally, together with each item entering the buffer, we get a fixed number of advice bits, which can be seen as information about the future or as information about an optimal solution (or an approximation thereof) for the whole input sequence. We show that for any ε> 0 there is a (1 +ε) -competitive algorithm for the problem which uses only a constant (depending on ε) number of advice bits per input item. We complement the above result by presenting a lower bound of Ω(log k) bits of advice per request for an optimal deterministic algorithm. ...|$|R
40|$|In the <b>reordering</b> <b>buffer</b> problem, we {{are given}} an input {{sequence}} of requests for service each of which corresponds to a point in a metric space. The cost of serving the requests heavily depends on the processing order. Serving a request induces cost corresponding to the distance between itself and the previously served request, measured in the underlying metric space. A <b>reordering</b> <b>buffer</b> with storage capacity k {{can be used to}} reorder the input sequence in a restricted fashion so as to construct an output sequence with lower service cost. This simple and universal framework is useful for many applications in computer science and economics, e. g., disk scheduling, rendering in computer graphics, or painting shops in car plants. In this paper, we design online algorithms for the <b>reordering</b> <b>buffer</b> problem. Our main result is a strategy with a polylogarithmic competitive ratio for general metric spaces. Previous work on the <b>reordering</b> <b>buffer</b> problem only considered very restricted metric spaces. We obtain our result by first developing a deterministic algorithm for arbitrary weighted trees with a competitive ratio of O(D · log k), where D denotes the unweighted diameter of the tree, i. e., the maximum number of edges on a path connecting two nodes. Then we show how to improve this competitive ratio to O(log 2 k) for metric spaces that are derived from HSTs. Combining this result with the results on probabilistically approximating arbitrary metrics by tree metrics, we obtain a randomized strategy for general metric spaces that achieves a competitive ratio of O(log 2 k · log n) in expectation against an oblivious adversary. Here n denotes the number of distinct points in the metric space. Note that the length of the input sequence can be much larger than n...|$|R
5000|$|Close to linear {{performance}} scaling from <b>reordering</b> command <b>buffers</b> onto multiple CPU cores ...|$|R
50|$|VISC {{architecture}} {{uses the}} Virtual Software Layer (translation layer) to dispatch a single thread of {{instructions to the}} Global Front End which splits instructions into virtual hardware threadlets which are then dispatched to separate virtual cores. These virtual cores can then send them to the available resources {{on any of the}} physical cores. Multiple virtual cores can push threadlets into the <b>reorder</b> <b>buffer</b> of a single physical core, which can split partial instructions and data from multiple threadlets through the execution ports at the same time. Each virtual core keeps track of the position of the relative output. This form of multithreading can increase single threaded performance by allowing a single thread to use all resources of the CPU.The allocation of resources is dynamic on a near-single cycle latency level (1-4 cycles depending on the change in allocation depending on individual application needs. Therefore, if two virtual cores are competing for resources, there are appropriate algorithms in place to determine what resources are to be allocated where.|$|E
5000|$|A PDP-10, a PDP-8, an Intel 386, an Intel 4004, a Motorola 68000, a System z mainframe, a Burroughs B5000, a VAX, a Zilog Z80000, and a MOS Technology 6502 all vary {{wildly in}} the number, sizes, and formats of instructions, the number, types, and sizes of registers, and the {{available}} data types. Some have hardware support for operations like scanning for a substring, arbitrary-precision BCD arithmetic, or transcendental functions, while others have only 8-bit addition and subtraction. But they {{are all in the}} CISC category because they have [...] "load-operate" [...] instructions that load and/or store memory contents within the same instructions that perform the actual calculations. For instance, the PDP-8, having only 8 fixed-length instructions and no microcode at all, is a CISC because of how the instructions work, PowerPC, which has over 230 instructions (more than some VAXes) and complex internals like register renaming and a <b>reorder</b> <b>buffer</b> is a RISC, while Minimal CISC has 8 instructions, but is clearly a CISC because it combines memory access and computation in the same instructions.|$|E
50|$|This {{buffering}} {{means that}} the fetch and decode stages can be more detached from the execution units than is feasible in a more traditional microcoded (or hard-wired) design. As this allows a degree of freedom regarding execution order, it makes some extraction of instruction level parallelism out of a normal single-threaded program possible (provided that dependencies are checked etc.). It opens up for more analysis and therefore also for reordering of code sequences in order to dynamically optimize mapping and scheduling of μops onto machine resources (such as ALUs, load/store units etc.). As this happens on the μop-level, sub-operations of different machine (macro) instructions may often intermix in a particular μop-sequence, forming partially reordered machine instructions as {{a direct consequence of}} the out-of-order dispatching of microinstructions from several macro instructions. However, this {{is not the same as}} the micro-op fusion, which aims at the fact that a more complex microinstruction may replace a few simpler microinstructions in certain cases, typically in order to minimize state changes and usage of the queue and <b>reorder</b> <b>buffer</b> space, therefore reducing power consumption. Micro-op fusion is used in some modern CPU designs.|$|E
40|$|In {{the classic}} minimum {{makespan}} scheduling problem, {{we are given}} an input sequence of jobs with processing times. A scheduling algorithm has to assign the jobs to m parallel machines. The objective is to minimize the makespan, which is {{the time it takes}} until all jobs are processed. In this paper, we consider online scheduling algorithms without preemption. However, we do not require that each arriving job has to be assigned immediately to one of the machines. A <b>reordering</b> <b>buffer</b> with limited storage capacity can be used to reorder the input sequence in a restricted fashion so as to schedule the jobs with a smaller makespan. This is a natural extension of lookahead. We present an extensive study of the power and limits of online reordering for minimum makespan scheduling. As a main result, we give, for $m$ identical machines, tight and, in comparison to the problem without reordering, much improved bounds on the competitive ratio for minimum makespan scheduling with <b>reordering</b> <b>buffers.</b> Depending on m, the achieved competitive ratio lies between 4 / 3 and 1. 4659. This optimal ratio is achieved with a buffer of size Theta(m). We show that larger buffer sizes do not result in an additional advantage and that a buffer of size Omega(m) is necessary to achieve this competitive ratio. Further, we present several algorithms for different buffer sizes. For m uniformly related machines, we give a scheduling algorithm that achieves a competitive ratio of 2 with a <b>reordering</b> <b>buffer</b> of size m. Considering that the best known competitive ratio for uniformly related machines without reordering is 5. 828, this result further emphasizes the power of online reordering...|$|R
40|$|We {{consider}} online scheduling so as {{to maximize}} the minimum load, using a <b>reordering</b> <b>buffer</b> which can store some of the jobs before they are assigned irrevocably to machines. For m identical machines, we show an upper bound of H_m- 1 + 1 for a buffer of size m- 1. A competitive ratio below H_m is not possible with any fixed buffer size, and it requires a buffer of size Ω(m/ m) to get a ratio of O(m). For uniformly related machines, we show that a buffer of size m+ 1 is sufficient to get a competitive ratio of m, which is best possible for any fixed sized buffer. We show similar results (but with different constructions) for the restricted assignment model. We give tight bounds for two machines in all the three models. These results sharply contrast to the (previously known) results which can be achieved without the usage of a <b>reordering</b> <b>buffer,</b> where {{it is not possible}} to get a ratio below a competitive ratio of m already for identical machines, and it is impossible to obtain an algorithm of finite competitive ratio in the other two models, even for m= 2. Our results strengthen the previous conclusion that a <b>reordering</b> <b>buffer</b> is a powerful tool and it allows a significant decrease in the competitive ratio of online algorithms for scheduling problems. Another interesting aspect of our results is that our algorithm for identical machines imitates the behavior of a greedy algorithm on (a specific set of) related machines, whereas our algorithm for related machines completely ignores the speeds until all jobs have arrived, and then only uses the relative order of the speeds...|$|R
40|$|Abstract. We {{consider}} online scheduling so as {{to maximize}} the minimum load, using a <b>reordering</b> <b>buffer</b> which can store some of the jobs before they are assigned irrevocably to machines. For m identical machines, we show an upper bound of Hm− 1 + 1 for a buffer of size m − 1. A competitive ratio below Hm is not possible with any finite buffer size, and it requires a buffer of size ˜ Ω(m) to get a ratio of O(log m). For uniformly related machines, we show that a buffer of size m + 1 is sufficient to get an approximation ratio of m, which is best possible for any finite sized buffer. Finally, for the restricted assignment model, we show lower bounds identical to those of uniformly related machines, but using different constructions. In addition, we design an algorithm of approximation ratio O(m) which uses a finite sized buffer. We give tight bounds for two machines in all the three models. These results sharply contrast to the (previously known) results which can be achieved without the usage of a <b>reordering</b> <b>buffer,</b> where {{it is not possible}} to get a ratio below an approximation ratio of m already for identical machines, and it is impossible to obtain an algorithm of finite approximation ratio in the other two models, even for m = 2. Our results strengthen the previous conclusion that a <b>reordering</b> <b>buffer</b> is a powerful tool and it allows a significant decrease in the competitive ratio of online algorithms for scheduling problems. Another interesting aspect of our results is that our algorithm for identical machines imitates the behavior of the greedy algorithm on (a specific set of) related machines, whereas our algorithm for related machines completely ignores the speeds until the end, and then only uses the relative order of the speeds. ...|$|R
5000|$|One copy of 8 x87 FP {{push down}} stack by default, MMX {{register}} were virtually simulated from x87 stack and require x86 register to supplying MMX instruction and aliases to exist stack. On P6, the instruction independently {{can be stored}} and executed in parallel in early pipeline stages before decoding into micro-operations and renaming in out-of-order execution. Beginning with P6, all register files do not require additional cycle to propagate the data, register files like architectural and floating point are located between code buffer and decoders, called [...] "retire buffer", <b>Reorder</b> <b>buffer</b> and OoOE and connected within the ring bus (16 bytes). The register file itself still remains one x86 register file and one x87 stack and both serve as retirement storing. Its x86 register file increased to dual ported to increase bandwidth for result storage. Registers like debug/condition code/control/unnamed/flag were stripped from the main register file and placed into individual files between the micro-op ROM and instruction sequencer. Only inaccessible registers like the segment register are now separated from the general-purpose register file (except the instruction pointer); they are now located between the scheduler and instruction allocator, {{in order to facilitate}} register renaming and out-of-order execution. The x87 stack was later merged with the floating-point register file after a 128-bit XMM register debuted in Pentium III, but the XMM register file is still located separately from x86 integer register files.|$|E
40|$|Superscalar {{processors}} {{can achieve}} increased performance by issuing instructions out-of-order {{from the original}} sequential instruction stream. Implementing an out-of-order instruction issue policy requires a hardware mechanism to prevent incorrectly executed instructions from updating register values. A <b>reorder</b> <b>buffer</b> {{can be used to}} allow a superscalar processor to issue instructions out-of-order and maintain program correctness. This paper describes the design and implementation of a 20 MHz CMOS <b>reorder</b> <b>buffer</b> for superscalar processors. The <b>reorder</b> <b>buffer</b> is designed to accept and retire two instructions per cycle. A full-custom layout in 1. 2 micron has been implemented, measuring 1. 1058 mm by 1. 3542 mm...|$|E
40|$|Pre-execution systems {{reduce the}} impact of cache misses and branch mispredictions by forking a slice, a code {{fragment}} derived from the program, in advance of frequently mispredicted branches and frequently missing loads in order to either resolve the branch or prefetch the load. Because unnecessary instructions are omitted the slice reaches the branch or load before the main thread does, for loads this time margin can reduce or even eliminate cache miss delay. Published results have shown significant improvements for some benchmarks, {{on the order of}} 20 %, with many showing at least single-digit improvements. These studies left unexamined two system parameters that one would expect pre-execution to be sensitive to: fetch rate and <b>reorder</b> <b>buffer</b> size. Higher fetch rate would allow the main thread to reach the troublesome load sooner, but would not affect the slice and so the slice’s margin is reduced. Studies have shown large potential margins for slices, but the fetch rate effect has not been measured. A second system parameter is <b>reorder</b> <b>buffer</b> size. A larger <b>reorder</b> <b>buffer</b> would allow a system to hide more of the miss latency that preexecution reduces. To test the sensitivity to these factors pre-execution schemes were simulated on systems with varying fetch rates and <b>reorder</b> <b>buffer</b> sizes. Results show that higher fetch rate does not reduce pre-execution speedup in most benchmarks. <b>Reorder</b> <b>buffer</b> size sensitivity varies, some benchmarks are insensitive to <b>reorder</b> <b>buffer</b> size increases beyond 256 entries, but still benefit from preexecution, the benefit {{due in large part to}} prefetching those loads that provide values for frequently mispredicted branches. The benchmarks that are sensitive to <b>reorder</b> <b>buffer</b> size are also the ones that benefit most ∗ Presented at the Workshop on Duplicating, Deconstructing, and Debunking, held in conjunction with the 30 th Annual Internationa...|$|E
40|$|A {{sequence}} of objects which {{are characterized by}} their color has to be processed. Their processing order influences how efficiently they can be processed: Each color change between two consecutive objects produces non-uniform cost. A <b>reordering</b> <b>buffer</b> which is a random access buffer with storage capacity for k objects {{can be used to}} rearrange this sequence {{in such a way that}} the total cost are minimized. This concept is useful for many applications in computer science and economics. We show that a <b>reordering</b> <b>buffer</b> reduces the cost of each sequence by a factor of at most 2 k– 1. This result even holds for cost functions modeled by arbitrary metric spaces. In addition, a matching lower bound is presented. From this bound follows that each strategy that does not increase the cost of a sequence is at least (2 k– 1) -competitive. As main result, we present the deterministic Maximum Adjusted Penalty (MediaObjects/InlineFigure 1. png) strategy which is O(log k) -competitive. Previous strategies only achieve a competitive ratio of k in the non-uniform model. For the upper bound on MediaObjects/InlineFigure 2. png, we introduce a basic proof technique. We believe that this technique can be interesting for other problems...|$|R
40|$|In the <b>reordering</b> <b>buffer</b> {{management}} problem {{a sequence}} of requests arrive online in a finite metric space, {{and have to be}} processed by a single server. This server is equipped with a request buffer of size k and can decide at each point in time, which request from its buffer to serve next. Servicing of a request is simply done by moving the server to the location of the request. The goal is to process all requests while minimizing the total distance that the server is traveling inside the metric space. In this paper we present a deterministic algorithm for the <b>reordering</b> <b>buffer</b> management problem that achieves a competitive ratio of O(log Delta + min {log n,log k}) in a finite metric space of n points and aspect ratio Delta. This is the first algorithm that works for general metric spaces and has just a logarithmic dependency on the relevant parameters. The guarantee is memory-robust, i. e., the competitive ratio decreases only slightly when the buffer-size of the optimum is increased to h=(1 +epsilon) k. For memory robust guarantees our bounds are close to optimal...|$|R
40|$|We give an O(k) -competitive {{randomized}} online algorithm for <b>reordering</b> <b>buffer</b> management, where k is {{the buffer}} size. Our bound matches the lower bound of Adamaszek et al. (STOC 2011). Our algorithm has two stages which are executed online in parallel. The first stage computes deterministically a feasible fractional solution to an LP relaxation for <b>reordering</b> <b>buffer</b> management. The second stage "rounds" using randomness the fractional solution. The first stage {{is based on}} the online primal-dual schema, combined with a dual fitting argument. As multiplicative weights steps and dual fitting steps are interleaved and in some sense conflicting, combining them is challenging. We also note that we apply the primal-dual schema to a relaxation with mixed packing and covering constraints. We pay the O(k) competitive factor for the gap between the computed LP solution and the optimal LP solution. The second stage gives an online algorithm that converts the LP solution to an integral solution, while increasing the cost by an O(1) factor. This stage generalizes recent results that gave a similar approximation factor for rounding the LP solution, albeit using an offline rounding algorithm...|$|R
40|$|The {{invention}} involves new microarchitecture {{apparatus and}} methods for superscalar microprocessors that support multi-instruction issue, decoupled dataflow scheduling, out-of-order execution, register renaming, multi-level speculative execution, and precise interrupts. These are the Distributed Instruction Queue (DIQ) and the Modified <b>Reorder</b> <b>Buffer</b> (MRB). The DIQ {{is a new}} distributed instruction shelving technique that is {{an alternative to the}} reservation station (RS) technique and offers a more efficient (improved performance/cost) implementation. The Modified <b>Reorder</b> <b>Buffer</b> (MRB) is an improved <b>reorder</b> <b>buffer</b> (RB) result shelving technique eliminates the slow and expensive prioritized associative lookup, shared global buses, and dummy branch entries (to reduce entry usage). The MRB has an associateive key unit which uses a unique associative key. Georgia Tech Research Corporatio...|$|E
40|$|The <b>reorder</b> <b>buffer</b> {{is usually}} [...] . In this paper, we propose a <b>reorder</b> <b>buffer</b> {{structure}} with shelter buffer for out-of-order issue superscalar processors {{not only to}} control stagnation efficiently, but also to reduce the buffer size. We can get remarkable performance improvement with {{only one or two}} buffers. Simulation results show that if the size of <b>reorder</b> <b>buffer</b> is between 8 and 32, performance gain obtained from the shelter is noticeable. For the shelter buffer of size 4, there is no performance improvement compared to that of size 2, which means that the shelter buffer of size 2 is large enough to handle most of the stagnation. If the shelter buffer of size 2 is employed, we can reduce the <b>reorder</b> <b>buffer</b> by 44 % in Whetstone, 50 % in FFT, 60 % in FM, and 75 % in Linpack benchmark program without loss of any throughput. Execution time is also improved by 19. 78 % in Whetstone, 19. 67 % in FFT, 23. 93 % in FM, and 8. 65 % in Linpack benchmark when the shelter buffer is used...|$|E
40|$|Rewriting {{rules and}} Positive Equality [4] are {{combined}} in an automatic way {{in order to}} formally verify out-of-order processors that have a <b>Reorder</b> <b>Buffer,</b> and can issue/retire multiple instructions per clock cycle. Only register-register instructions are implemented, and can be executed out-of-order, as soon as their data operands can be either read from the Register File, or forwarded as results of instructions ahead in program order in the <b>Reorder</b> <b>Buffer.</b> The verification {{is based on the}} Burch and Dill correctness criterion [6]. Rewriting rules are used to prove the correct execution of instructions that are initially in the <b>Reorder</b> <b>Buffer,</b> and to remove them from the correctness formula. Positive Equality is then employed to prove the correct execution of newly fetched instructions. The rewriting rules resulted in up to 5 orders of magnitude speedup, compared to using Positive Equality alone. That made it possible to formally verify processors with up to 1, 500 instructions in the <b>Reorder</b> <b>Buffer,</b> and issue/retire widths of up to 128 instructions per clock cycle...|$|E
40|$|We {{consider}} the <b>reordering</b> <b>buffer</b> problem {{on a line}} consisting of n equidistant points. We show that, for any constant delta, an (offline) algorithm that has a buffer (1 -delta) k performs worse {{by a factor of}} Omega(log n) than an offline algorithm with buffer k. In particular, this demonstrates that the O(log n) -competitive online algorithm MovingPartition by Gamzu and Segev (ACM Trans. on Algorithms, 6 (1), 2009) is essentially optimal against any offline algorithm with a slightly larger buffer...|$|R
40|$|Lookahead is {{a classic}} concept {{in the theory of}} online scheduling. An online {{algorithm}} without lookahead has to process tasks as soon as they arrive and without any knowledge about future tasks. With lookahead, this strict assumption is relaxed. There are different variations on the exact type of information provided to the algorithm under lookahead but arguably the most common one is to assume that, at every point in time, the algorithm has knowledge of the attributes of the next k tasks to arrive. This assumption is justified by the fact that, in practice, tasks may not always strictly arrive one-by-one and therefore, a certain number of tasks are always waiting in a queue to be processed. In recent years, so-called <b>reordering</b> <b>buffers</b> have been studied as a sensible generalization of lookahead. The basic idea is that, in problem settings where the order in which the tasks are processed is not important, we can permit a scheduling algorithm to choose to process any task waiting in the queue. This stands in contrast to lookahead, where the algorithm has knowledge of all the tasks in the queue but still has to process them in the order they arrived. We discuss some of the results for <b>reordering</b> <b>buffers</b> for different scheduling problems...|$|R
40|$|We {{design and}} analyze an on-line <b>reordering</b> <b>buffer)</b> {{management}} algorithm with improved O log k log log k competitive ratio for non-uniform costs, where k is the buffer size. This improves {{on the best}} previous result (even for uniform costs) of Englert and Westermann (ICALP 2005) giving O(log k) competitive ratio, which was also the best (off-line) polynomial time approximation guarantee for this problem. Our analysis {{is based on an}} intricate dual fitting argument using a linear programming relaxation for the problem that we introduce in this paper...|$|R
