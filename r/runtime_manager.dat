11|22|Public
40|$|Abstract. We {{introduce}} a technique for debugging multi-threaded C programs and analyzing {{the impact of}} source code changes, and its implementation in the prototype tool Direct. Our approach uses a combination of source code instrumentation and runtime management. The source code along with a test harness is instrumented to monitor Operating System (OS) and user defined function calls. All concurrency control primitives are tracked through the OS functions. Optionally, Direct can track some concurrency related data of interest. Direct keeps track of an abstract global state that combines the abstract states of every thread, including the sequence of function calls and concurrency primitives executed. The <b>runtime</b> <b>manager</b> can insert delays, provoking thread interleavings that may exhibit bugs, {{and that may be}} difficult to reach otherwise. The <b>runtime</b> <b>manager</b> collects an approximation of the reachable state space and uses this approximation {{to assess the impact of}} change in a new version of the program. ...|$|E
40|$|The {{importance}} of parallel programming is increasing {{year after year}} since the power wall popularized multi-core processors, and with them, shared memory parallel programming models. In particular, task-based programming models, like the standard OpenMP 4. 0, have {{become more and more}} important. They allow describing a set of data dependences per task that the runtime uses to order the execution of tasks. This order is calculated using shared graphs, which are updated by all threads but in exclusive access using synchronization mechanisms (locks) to ensure the dependences correctness. Although exclusive accesses are necessary to avoid data race conditions, those may imply contention that limits the application parallelism. This becomes critical in many-core systems because several threads may be wasting computation resources waiting to access the runtime structures. This master thesis introduces the concept of an asynchronous runtime management suitable for task-based programming model runtimes. The runtime proposal is based on the asynchronous management of the runtime structures like task dependence graphs. Therefore, the application threads request actions to the runtime instead of directly executing the needed modifications. The requests are then handled by a <b>runtime</b> <b>manager</b> which can be implemented in different ways. This master thesis presents an extension to a previously implemented centralized <b>runtime</b> <b>manager</b> and presents a novel implementation of a distributed <b>runtime</b> <b>manager.</b> On one hand, the runtime design based on a centralized manager [1] is extended to dynamically adapt the runtime behavior according to the manager load with the objective of being as fast as possible. On the other hand, a novel runtime design based on a distributed manager implementation is proposed to overcome the limitations observed in the centralized design. The distributed runtime implementation allows any thread to become a <b>runtime</b> <b>manager</b> thread if it helps to exploit the application parallelism. That is achieved using a new runtime feature, also implemented in this master thesis, for runtime functionality dispatching through a callback system. The proposals are evaluated in different many-core architectures and their performance is compared against the baseline runtimes used to implement the asynchronous versions. Results show that the centralized manager extension can overcome the hard limitations of the initial basic implementation, that the distributed manager fixes the observed problems in previous implementation, and the proposed asynchronous organization significantly outperforms the speedup obtained by the original runtime for real benchmarks...|$|E
40|$|International audienceEmbedded manycore {{architectures}} offer energy-efficient super-computing capabilities but {{are notoriously}} difficult to program with traditional parallel programming Application Programming Interfaces (APIs). To address this challenge, dataflow Models of Computation (MoCs) are increasingly used as their high-level of abstraction eases the automation of computation mapping, memory allocation, and communication management. Reconfigurable dataflow is {{a class of}} dataflow MoC that fosters a unique trade-off between application dynamicity and predictability. This demonstration presents the first embedded <b>runtime</b> <b>manager</b> enabling the execution of reconfigurable dataflow graphs on a Non-Uniform Memory Access (NUMA) architecture. The proposed runtime dynamically deploys reconfigurable dataflow graphs onto clustered Processing Elements (PEs) through the Networks-on-Chips (NoCs) of the manycore architecture. An open-source implementation on the Kalray MPPA R processor demonstrates the feasibility and the great potential of such a runtime...|$|E
40|$|This paper {{describes}} ongoing {{research to}} develop a framework for implementing dynamically reconfiguring avionics and control systems for unmanned aerial vehicles (UAVs) and a test and development environment for experimental UAVs. The framework supports graceful degradation, where hardware and software failures cause {{a reduction in the}} quality or capability of the control system but does not result in total system failure. The approach uses a graphical specification representing modular software interdependencies and a <b>runtime</b> system <b>manager</b> that reconfigures the system. The techniques are currently bein...|$|R
40|$|International audienceDynamic {{reconfiguration}} of FPGAs enables {{systems to}} adapt to changing demands. This paper concentrates on how {{to take into account}} specificities of partially reconfigurable components during the high level Adequation Algorithm Architecture process. We present a method which generates automatically the design for both partially and fixed parts of FPGAs. The <b>runtime</b> reconfiguration <b>manager</b> which monitors dynamic reconfigurations, uses prefetching technic to minimize reconfiguration latency of runtime reconfiguration. We demonstrate the benefits of this approach through the design of a dynamic reconfigurable MC-CDMA transmitter implemented on a Xilinx Virtex 2...|$|R
40|$|Abstract—This {{concept paper}} proposes a new {{system-level}} design methodology for runtime reconfigurable adaptive het-erogeneous systems in a real-time environment. Today, among those approaches dealing with runtime reconfiguration and hardware/software co-design, compliance with hard real-time conditions is not guaranteed. Our approach will fill this gap. In contrast to other approaches, we apply methods of real-time analysis to embedded reconfigurable systems. An extended com-piler and a <b>runtime</b> resource <b>manager</b> guarantee both synthesis and reconfiguration in a (hard) real-time environment. With this approach, {{the system can}} adapt to changes in requirements and operational environments during runtime. I...|$|R
40|$|Choosing {{the ideal}} {{algorithms}} and solutions for a sci-entific application {{is difficult because}} of the heterogene-ity and dynamism of the application execution phases at runtime. In this paper we present an autonomic program-ming framework {{that is capable of}} self-configuring and self-composing the application solution methods in order to ex-ploit the heterogeneity and the dynamism of the application execution states. We focus our approach on Partial Differ-ential Equation (PDE) problems involving multiple compu-tational phases that are defined in terms of their spatial and temporal characteristics. We have implemented a Physics Aware <b>Runtime</b> <b>Manager</b> (PARM) that periodically moni-tors and analyzes the spatial and temporal characteristics of the application to identify its current execution phase (state). Then PARM will determine an appropriate numeri-cal schemes and algorithms that will most efficiently exploit the current state. Our preliminary results show a significant speedup can be achieved by using PARM. 1...|$|E
40|$|We {{introduce}} flash teams, {{a framework}} for dynamically as-sembling and managing paid experts from the crowd. Flash teams advance a vision of expert crowd work that accom-plishes complex, interdependent goals such as engineering and design. These teams consist of sequences of linked mod-ular tasks and handoffs that can be computationally managed. Interactive systems reason about and manipulate these teams’ structures: for example, flash teams can be recombined to form larger organizations and authored automatically in re-sponse to a user’s request. Flash teams can also hire more people elastically in reaction to task needs, and pipeline in-termediate output to accelerate completion times. To enable flash teams, we present Foundry, an end-user authoring plat-form and <b>runtime</b> <b>manager.</b> Foundry allows users to author modular tasks, then manages teams through handoffs of inter-mediate work. We demonstrate that Foundry and flash teams enable crowdsourcing of a broad class of goals including de-sign prototyping, course development, and film animation, in half the work time of traditional self-managed teams...|$|E
40|$|As {{performance}} enhancement {{is accompanied}} by the aggressive integration of many-cores to a single chip and technology nodes approach deca-nanometer dimensions, the system's failure rate is becoming signi cant. Inevitably, computer systems must tolerate such failures. Both hardware and software methods are available enabling fault-tolerance to the systems. The Checkpoint/Restart technique provides reliability to the execution of an application. However, Checkpoint/Restart introduce an additional time overhead {{in order to achieve}} the fault-tolerance of the execution, that leads to performance variability. The scope of this thesis is to enhance a <b>runtime</b> <b>manager,</b> Depman, that orchestrates an application level Checkpoint/Restart technique so that such time overheads are absorbed, achieving performance predictability and reliability on the y, by using Dynamic Voltage and Frequency Scaling (DVFS). A closed-loop implementation controlling the clock frequency is proposed, that quanti es the time overheads induced by the checkpoint restart process and adjusts the frequency levels of the CPU so that execution time converges to the normal. Depman was also modi ed to extend its portability to other platforms and applications and was tested using the self fault injection module to both the Intel's Single-Chip Cloud Computer (SCC) and an x 86 general computing platform, evaluating both the execution time and energy consumption of our scheme...|$|E
40|$|Dynamic {{reconfiguration}} of FPGAs enables {{systems to}} adapt to changing demands. This paper concentrates on how {{to take into account}} specificities of partially reconfigurable components during the high level Adequation Algorithm Architecture process. We present a method which generates automatically the design for both partially and fixed parts of FPGAs. The <b>runtime</b> reconfiguration <b>manager</b> which monitors dynamic reconfigurations, uses prefetching technic to minimize reconfiguration latency of runtime reconfiguration. We demonstrate the benefits of this approach through the design of a dynamic reconfigurable MC-CDMA transmitter implemented on a Xilinx Virtex 2. 1...|$|R
40|$|Abstract. In {{the field}} of guided {{learning}} on the Internet we present, in this paper, an interactive tool for designing intelligent tutoring systems on the web. Our tool makes easier {{the creation of an}} ontology describing the content model for a given course. Such ontology contains information about classes and instances, reflecting the structure and components for the later creation of an adaptive course, using our web-based <b>runtime</b> course <b>manager</b> system. Our authoring tool generates XML code to improve course understanding as well as transportability and processing by our runtime system, which means that the generated code will reflect, in an easier way, the course structure and contents, being readable for most of users and course designers. ...|$|R
40|$|With strong efforts behind {{hardware}} development, the reconfigurable computing (RC) {{community is}} now producing powerful reconfigurable hardware {{with up to}} onemillion gate capacity. In comparison, programming for RC systems has received less emphasis. Current approaches to RC programming often involves separate development for software, reconfigurable hardware, and their interaction. This paper investigates three RC programming approaches, each with increasing effort to unify the specification of the software and the reconfigurable hardware components. Arguments for an abstract programming environment, without the exact details about the underlying RC implementation, is put forth. The advantages and practicality of an unified, high-level programming environment is demonstrated by a hypothetical RC specifically designed to support a multithreaded programming model and a dynamic <b>runtime</b> resource <b>manager.</b> 1 Introduction In a von Neumann stored-program architecture, the instruction set arch [...] ...|$|R
40|$|Abstract. The {{development}} of efficient parallel algorithms for large scale wildfire simulations is a challenging research problem because {{the factors that}} determine wildfire behavior are complex. These factors make static parallel algorithms inefficient, especially when large number of processors is used because we cannot predict accurately the propagation of the fire and its computational requirements at runtime. In this paper, we propose an Autonomic <b>Runtime</b> <b>Manager</b> (ARM) to dynamically exploit the physics properties of the fire simulation and use them {{as the basis of}} our self-optimization algorithm. At each step of the wildfire simulation, the ARM decomposes the computational domain into several natural regions (e. g., burning, unburned, burned) where each region has the same temporal and special characteristics. The number of burning, unburned and burned cells determines {{the current state of the}} fire simulation and can then be used to accurately predict the computational power required for each region. By regularly monitoring and analyzing the state of the simulation, and using that to drive the runtime optimization, we can achieve significant performance gains because we can efficiently balance the computational load on each processor. Our experimental results show that the performance of the fire simulation has been improved by 45 % when compared with a static portioning algorithm. ...|$|E
40|$|Applications {{executed}} on multicore {{embedded systems}} interact with system software [such as {{the operating system}} (OS) ] and hardware, leading to widely varying thermal profiles which accelerate some aging mechanisms, reducing the lifetime reliability. Effectively managing the temperature therefore requires: 1) autonomous detection of changes in application workload and 2) appropriate selection of control levers to manage thermal profiles of these workloads. In this paper, we propose a technique for workload change detection using density ratio-based statistical divergence between overlapping sliding windows of CPU performance statistics. This is integrated in a runtime approach for thermal management, which uses reinforcement learning to select workload-specific thermal control levers by sampling on-board thermal sensors. Identified control levers override the OSs native thread allocation decision and scale hardware voltage-frequency to improve average temperature, peak temperature, and thermal cycling. The proposed approach is validated through its implementation as a hierarchical <b>runtime</b> <b>manager</b> for Linux, with heuristic-based thread affinity selected from the upper hierarchy to reduce thermal cycling and learningbased voltage-frequency selected from the lower hierarchy to reduce average and peak temperatures. Experiments conducted with mobile, embedded, and high performance applications on ARM-based embedded systems demonstrate that the proposed approach increases workload change detection accuracy by an average 3. 4 ×, reducing the average temperature by 4 °C- 25 °C, peak temperature by 6 °C- 24 °C, and thermal cycling by 7 %- 35 % over state-of-the-art approaches...|$|E
40|$|Large-scale {{distributed}} applications are highly adaptive and heterogeneous {{in terms of}} their computational requirements. The computational complexity associated with each computational region or domain varies continuously and dramatically both in space and time throughout the whole life cycle of the application execution. Consequently, static scheduling techniques are inefficient to optimize the execution of these applications at runtime. In this paper, we present an Autonomic <b>Runtime</b> <b>Manager</b> (ARM) that uses the application spatial and temporal characteristics as the main criteria to selfoptimize the execution of {{distributed applications}} at runtime. The wildfire spread simulation is used as a running example to demonstrate the ARM effectiveness to control and manage the application’s execution. The behavior of the wildfire simulation depends on many complex factors that contribute to the adaptive and heterogeneous behaviors such as fuel characteristics and configurations, chemical reactions, balances between different modes of heat transfer, topography, and fire/atmosphere interactions. Consequently, the application execution cannot be predicted a priori and that makes static parallel or distributed algorithms very inefficient. The ARM is implemented using two modules: 1) Online Monitoring and Analysis Module, and 2) Autonomic Planning and Scheduling Module. The online monitoring and analysis module interfaces with different kinds of application and system sensors that collect information to accurately determine {{the current state of the}} fire simulation in terms of the number and locations of burning and unburned cells as well as the states of the resources, and decides whether the autonomic planning and scheduling module should be invoked. The autonomic planning and scheduling module uses the resource capability models as well as the current state of the computations to repartition the whole computational workload into available processors. Our experimental results show that by using ARM the performance of the wildfire simulation has been improved by 45 % whe...|$|E
40|$|Multiprocessor System-on-Chip {{platforms}} {{are typically}} used for co-hosting multiple tasks, which may {{start and stop}} exe-cution independently at time instants unknown at design time. In such systems, the <b>runtime</b> resource <b>manager</b> is responsi-ble for allocating adequate and appropriate resources to each task. We identify a key issue in existing work that the re-source management algorithms consider the problem only at task-level, i. e. the optimization is performed for each individ-ual task upon activation. However, it can be shown that such strategies are suboptimal from the system point of view. In contrast, we propose in this paper a new task allocation flow that considers the system-level resource management. Com-paring with traditional techniques, significant performance improvement (up to 29. 5 %) is observed during evaluation using a standard benchmark set. In addition, the proposed task allocator features runtime self-adaptability with respect to changes in hardware and/or applications. Index Terms — Processor scheduling, Resource manage-men...|$|R
40|$|Abstract. In {{contrast}} to the common belief that OpenMP requires data-parallel extensions to scale well on architectures with non-uniform memory access latency, recent work has shown {{that it is possible}} to develop OpenMP programs with good levels of memory access locality, without any extension of the OpenMP API. The vehicle for localizing memory accesses transparently to the programming model, is a <b>runtime</b> memory <b>manager,</b> which uses memory access tracing and dynamic page migration to implement automatic data distribution. This paper evaluates the effectiveness of using this runtime data distribution method in non embarrassingly parallel codes, such as the SPEC benchmarks. We investigate the extent up to which sophisticated management of physical memory in the runtime system can speedup programs for which the programmer has no knowledge of the memory access pattern. Our runtime memory management algorithms improve the speedup of five SPEC benchmarks by 20 – 25 % on average. The speedups are close to the theoretical maximum speedups for the problem sizes used and they are obtained with a minimal programming effort of about a couple of hours per benchmark. ...|$|R
40|$|The final {{publication}} {{is available}} at Springer via [URL] of 6 th International Computer Science Conference, AMT 2001 Hong Kong, China, December 18 – 20, 2001 In the field of guided learning on the Internet we present, in this paper, an interactive tool for designing intelligent tutoring systems on the web. Our tool makes easier {{the creation of an}} ontology describing the content model for a given course. Such ontology contains information about classes and instances, reflecting the structure and components for the later creation of an adaptive course, using our web-based <b>runtime</b> course <b>manager</b> system. Our authoring tool generates XML code to improve course understanding as well as transportability and processing by our runtime system, which means that the generated code will reflect, in an easier way, the course structure and contents, being readable for most of users and course designers. The work reported in this paper is being partially supported by the Spanish Interdepartmental Commission of Science and Technology (CICYT), project number TEL 1999 - 018...|$|R
40|$|The {{overarching}} {{goal of this}} dissertation {{research is}} to realize a virtual collaboratory for the investigation of large-scale scientific computing applications which generally experience different execution phases at runtime and each phase has different computational, communication and storage requirements as well as different physical characteristics. Consequently, an optimal solution or numerical scheme for one execution phase might not be appropriate for {{the next phase of}} the application execution. Choosing the ideal numerical algorithms and solutions for all application runtime phases remains an active research area. In this dissertation, we present Physics Aware Programming (PAP) paradigm that enables programmers to identify the appropriate solution methods to exploit the heterogeneity and the dynamism of the application execution states. We implement a Physics Aware <b>Runtime</b> <b>Manager</b> (PARM) to exploit the PAP paradigm. PARM periodically monitors and analyzes the runtime characteristics of the application to identify its current execution phase (state). For each change in the application execution phase, PARM will adaptively exploit the spatial and temporal attributes of the application in the current state to identify the ideal numerical algorithms/solvers that optimize its performance. We have evaluated our approach using a real world application (Variable Saturated Aquifer Flow and Transport (VSAFT 2 D)) commonly used in subsurface modeling, diffusion problem kernel and seismic problem kernel. We evaluated the performance gain of the PAP paradigm with up to 2, 000, 000 nodes in the computation domain implemented on 32 processors. Our experimental results show that by exploiting the application physics characteristics at runtime and applying the appropriate numerical scheme with adapted spatial and temporal attributes, a significant speedup can be achieved (around 80 %) and the overhead injected by PAP is negligible (less than 2 %). We also show that the results using PAP is as accurate as the numerical solutions that use fine grid resolution...|$|E
40|$|The {{increased}} complexity, heterogeneity and the {{dynamism of}} networked systems and applications make current configuration and management tools to be ineffective. A new paradigm to dynamically configure and manage large-scale complex and heterogeneous networked systems is critically needed. In this dissertation, {{we present a}} self configuration paradigm based {{on the principles of}} autonomic computing that can handle efficiently complexity, dynamism and uncertainty in configuring networked systems and their applications. Our approach is based on making any resource/application to operate as an Autonomic Component (that means, it can be self-configured, self-healed, self-optimized and self-protected) by using two software modules: Component Management Interface (CMI) to specify the configuration and operational policies associated with each component and Component <b>Runtime</b> <b>Manager</b> (CRM) that manages the component configurations and operations using the policies defined in CMI. We use several configuration metrics (adaptability, complexity, latency, scalability, overhead, and effectiveness) {{to evaluate the effectiveness of}} our self-configuration approach when compared to other configuration techniques. We have used our approach to dynamically configure four systems: Automatic IT system management, Dynamic security configuration of networked systems, Self-management of data backup and disaster recovery system and Automatic security patches download and installation on a large scale test bed. Our experimental results showed that by applying our self-configuration approach, the initial configuration time, the initial configuration complexity and the dynamic configuration complexity can be reduced significantly. For example, the configuration time for security patches download and installation on nine machines is reduced to 4399 seconds from 27193 seconds. Furthermore our system provides most adaptability (e. g., 100 % for Snort rule set configuration) comparing to hard coded approach (e. g., 22 % for Snort rule set configuration) and can improve the performance of managed system greatly. For example, in data backup and recovery system, our approach can reduce the total cost by 54. 1 % when network bandwidth decreases. In addition, our framework is scalable and imposes very small overhead (less than 1 %) on the managed system...|$|E
40|$|Nowadays, we are {{witnessing}} trends in technology, fabrication processes and computing architectures {{that lead to the}} design and development of processing systems constituted by a relevant number of independent, heterogeneous execution resources. The aim is to achieve high-performance while leveraging on other aspects, such as energy consumption. Indeed, heterogeneity comes at the cost of greater design and management complexity. To reach an optimal solution, system architects need {{to take into account the}} efficiency of systems' units, i. e., general purpose processors eventually with one or more kinds of accelerators (e. g., GPUs or FPGAs), as well as the workload. This often leads to inefficiency in the exploitation of such resources, and therefore in performance/energy. Within this context, we are proposing a <b>runtime</b> resource <b>manager</b> able to observe the system execution and to dynamically optimise its behaviour with respect to one or more identified functional parameters, according to the architectural characteristics, and the users' and the applications' needs. Such an adaptation characteristic is intrinsically embedded in the device as a software layer, called Orchestrator, able to adapt the runtime resource management according to the target objectives and to the inputs from the external environment...|$|R
40|$|This paper proposes, implements, and evaluates {{in terms}} of worst case performance, an online metrics {{collection}} strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware. The solution is based upon an abstract representation of the mobile object system, which holds containers aggregating metrics for each specific component including host <b>managers,</b> <b>runtimes</b> and mobile objects. A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system. The MobJeX platform {{was used as the}} basis for implementation and testing with a number of laboratory tests conducted to measure scalability, efficiency and the application of simple measurement and propagation criteria to reduce collection overhead...|$|R
40|$|The {{most widely}} used machine {{learning}} frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a <b>runtime</b> memory <b>manager</b> that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory usage of AlexNet by up to 89 %, OverFeat by 91 %, and GoogLeNet by 95 %, {{a significant reduction in}} memory requirements of DNNs. Similar experiments on VGG- 16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG- 16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card containing 12 GB of memory, with 18 % performance loss compared to a hypothetical, oracular GPU with enough memory to hold the entire DNN. Comment: Published as a conference paper at the 49 th IEEE/ACM International Symposium on Microarchitecture (MICRO- 49), 201...|$|R
50|$|MWM is a {{lightweight}} window manager, having robust compliance and {{configuration of the}} features it has. MWM first appeared on in the early-1980s, along with the Motif toolkit. MWM supports: Common User Interface (i.e., Alt-Tab is switch windows, a standard), some International support, Common Desktop Environment, X Resource Database (/home/app-defaults/ and <b>runtime),</b> X Session <b>Manager</b> protocol, X Edited Resource Protocol (edit widget data), desktop icons, optional use of images to decorate, and had supported Virtual desktop (removed since 2.1) but now supports non-virtual desktop panning. MWM is a window manager, not a full desktop environment, so it only manages windows; {{it is expected that}} configuration, programs, sound, are provided by other programs. A plain text file is parsed to customize menus, user input mappings, management features, and user made functions of the same.|$|R
40|$|Abstract If the continuations in {{functional}} data-structure-generating {{programs are}} made explicit and represented as records, {{they can be}} &quot;recycled. &quot; Once they have served their purpose as temporary, intermediate structures for managing program control, the space they occupy can be reused for the structures that the programs produce as their output. To effect this immediate memory reclamation, we use a sequence of correctness-preserving program transformations, demonstrated {{through a series of}} simple examples. We then apply the transformations to general anamorphism operators, with the important consequence that all finite-output anamorphisms can now be run without any stack- or continuationspace overhead. 1 Introduction The runtime architecture for a language implementation keeps track of the continuations of procedure calls using either a stack of call frames or a linked chain of heapallocated continuation structures. One advantage that may be claimed for the stack approach is that deallocation of frames is inexpensive, and it happens as soon as a procedure returns. Heap-allocated continuations, on the other hand, typically take up memory until the space is reclaimed by the <b>runtime</b> memory <b>manager</b> (e. g., a garbage collector). We show how, for a certain class of procedures, using the continuation-based approach can lead not only to immediate reclamation of the space used by the continuations, but also to the elimination of most of the memory overhead incurred by both of the aforementioned architectures...|$|R
30|$|Policy <b>Manager</b> and <b>runtime</b> {{communication}} Policy <b>Manager</b> is a standalone app {{running on}} the same Android device with the instrumented apps, which manages and deploys control policies for all the instrumented apps. At runtime, the wrappers in the instrumented apps {{need to get the}} most up-to-date control policies from Policy Manager to decide whether the app is allowed to access sensors. Different from prior works [17, 18] that establish IPC between the app and the manager every time an API call is hooked, Policy Manager only establishes one IPC to an instrumented app when there is a change in the app’s control policy. This is a consideration of runtime overhead because the frequency of sensor events is quite high (at least 5 Hz). In Sensor Guardian, every app is instrumented with an internal light-weight Policy Manager that handles IPC with the external Policy Manager and caches current policies only for this app. When a sensor event is hooked, {{there is no need to}} ask the external Policy Manager for control policies through IPC. The hooks only need to get the cached policies from the internal Policy Manager in the same process, which has lower overhead than IPC. Though this strategy may delay the new control policy to take effect, the latency is quite low, which is the time of only one IPC process.|$|R
40|$|In this work, {{we present}} a dynamic memory {{allocation}} technique for a novel, horizontally partitioned memory subsystem targeting contemporary embedded processors with a memory management unit (MMU). We propose to replace the on-chip instruction cache with a scratchpad memory (SPM) and a small minicache. Serializing the address translation with the actual memory access enables the memory system to access either only the SPM or the minicache. Independent of the SPM size and based solely on profiling information, a postpass optimizer classifies the code of an application binary into a pageable and a cacheable code region. The latter is placed at a fixed location in the external memory and cached by the minicache. The former, the pageable code region, is copied on demand to the SPM before execution. Both the pageable code region and the SPM are logically divided into pages {{the size of an}} MMU memory page. Using the MMU’s pagefault exception mechanism, a <b>runtime</b> scratchpad memory <b>manager</b> (SPMM) tracks page accesses and copies frequently executed code pages to the SPM before they get executed. In order to minimize the number of page transfers from the external memory to the SPM, good code placement techniques become more important with increasing sizes of the MMU pages. We discuss code-grouping techniques and provide an analysis of the effect of the MMU’s page size on execution time, energy consumption, and externa...|$|R
40|$|This paper {{presents}} a dynamic scratchpad memory (SPM) code allocation technique for embedded systems running an operating system with preemptive multitasking. Existing SPM allocation schemes {{do not support}} multiple tasks or only a fixed number of processes that are known at compile time. These schemes rely on algorithms that select code {{depending on the size}} of the SPM. In contemporary portable devices, however, processes are created and terminated on demand and the SPM is shared among them. We introduce a dynamic scratchpad memory code alloca-tion technique for code that supports dynamically created processes. At <b>runtime,</b> an SPM <b>manager</b> (SPMM) loads code pages of the running applications into the SPM on de-mand. It supports different sharing strategies that deter-mine how the SPM is distributed among the running pro-cesses. We analyze several sharing strategies with regard to several preferable properties of multiprocess SPM allocation schemes. We evaluate the proposed multiprocess SPM allocation techniques and compare them to a fully-cached reference system by running several multiprocess benchmarks. The benchmarks comprise of multiple embedded applications such as H. 264, MP 3, MPEG- 4, and PGP. On average, we achieve a 47 % improvement in throughput and a 32 % re-duction in energy consumption. A comparison with the un-achievable lower bound shows that the best SPM sharing strategy exploits 87 % of the runtime improvements and 89 % of the energy savings possible...|$|R
40|$|Role-based {{access control}} (RBAC) {{provides}} flexibility to security management over {{the traditional approach}} of using user and group identifiers. In RBAC, access privileges are given to roles rather than to individual users. Users acquire the corresponding permissions when playing different roles. Roles can be defined simply as a label, but such an approach lacks the support to allow users to automatically change roles under different contexts; this static method also adds administrative overheads in role assignment. In electronic commerce and other cooperative computing environments, access to shared resources has to be controlled {{in the context of}} the entire business process; it is therefore necessary to model dynamic roles as a function of resource attributes and contextual information. In this paper, an object-oriented organizational model, OMM, is presented as an underlying model to support dynamic role definition and role resolution in RBAC. The paper describes the OMM reference model and shows how it can be applied flexibly to capture the different classes of resources within a corporation, and to maintain the complex and dynamic roles and relationships between the resource objects. Administrative tools use the role model in OMM to define security policies for role definition and role assignment. At <b>runtime,</b> the resource <b>manager</b> queries the OMM system to resolve roles in order to authorize any access attempts. Similarly, cooperative computing software uses OMM to support task assignment and access control to business processes. Contrary to traditional approaches, OMM separates the organization model from the application model; thus it allows independent and flexible role modeling to reflect realistically a dynamic authorization subsystem in a rapidly changing business world...|$|R
40|$|Project Specification The Parameter {{classes of}} the ALICE/FAIR (ALFA) {{software}} framework contain and manage all the numerical {{information needed to}} process the data. In order to analyse the raw or simulated data, several numerical parameters are needed, such as, calibration/digitization parameters or geometry positions of detectors. One common characteristic to most of these parameters {{is that they will}} go through several different versions corresponding, for example, to changes in the detectors’ definition or any other condition. This makes it necessary to have a parameter repository with a well-defined versioning system. The <b>runtime</b> database (Parameter <b>manager</b> in ALFA) is such a repository. It knows about all parameter containers needed for the actual analysis. The containers can be initialized automatically from one or more inputs, and written out to one output. Possible inputs/output mechanisms are ROOT files or ASCII files. In this project a key-value database will be investigated as an optional IO for the runtime database. Abstract In this project, several key-value databases are compared for their performance to implement them into the ALFA framework. As the research shows that a RAMCloud key-value database is not suited for this project, a deep research is done into Riak instead. The case-study compares read vs write latency, optimal simulation sample size, different storage backends, object size influence, cluster size influence, different consistency settings and there performance impact, the impact of adding security, the overhead of using a Java tool and the availability and fault tolerance of Riak. All these measurements are discussed taking into account certain difficulties of simulating a key value database. This gives us a document showing the impact of certain design decisions in implementing a key-value store and will allow developers for future projects to easily ascertain how changes in the database will impact performance and how to easily use a tool developed during this project to make exact measurements...|$|R
40|$|Multi-GPU {{machines}} are being increasingly used in high performance computing. These {{machines are}} being used both as standalone work stations to run computations on medium to large data sizes (tens of gigabytes) and as a node in a CPU-Multi GPU cluster handling very large data sizes (hundreds of gigabytes to a few terabytes). Each GPU in such a machine has its own memory and does not share the address space either with the host CPU or other GPUs. Hence, applications utilizing multiple GPUs have to manually allocate and managed at a on each GPU. A significant body of scientific applications that utilize multi-GPU machines contain computations inside affine loop nests, i. e., loop nests that have affine bounds and affine array access functions. These include stencils, linear-algebra kernels, dynamic programming codes and data-mining applications. Data allocation, buffer management, and coherency handling are critical steps {{that need to be}} performed to run affine applications on multi-GPU machines. Existing works that propose to automate these steps have limitations and in efficiencies in terms of allocation sizes, exploiting reuse, transfer costs and scalability. An automatic multi-GPU memory manager that can overcome these limitations and enable applications to achieve salable performance is highly desired. One technique that has been used in certain memory management contexts in the literature is that of bounding boxes. The bounding box of an array, for a given tile, is the smallest hyper-rectangle that encapsulates all the array elements accessed by that tile. In this thesis, we exploit the potential of bounding boxes for memory management far beyond their current usage in the literature. In this thesis, we propose a scalable and fully automatic data allocation and buffer management scheme for affine loop nests on multi-GPU machines. We call it the Bounding Box based Memory Manager (BBMM). BBMM is a compiler-assisted <b>runtime</b> memory <b>manager.</b> At compile time, it use static analysis techniques to identify a set of bounding boxes accessed by a computation tile. At run time, it uses the bounding box set operations such as union, intersection, difference, finding subset and superset relation to compute a set of disjoint bounding boxes from the set of bounding boxes identified at compile time. It also exploits the architectural capability provided by GPUs to perform fast transfers of rectangular (strided) regions of memory and hence performs all data transfers in terms of bounding boxes. BBMM uses these techniques to automatically allocate, and manage data required by applications (suitably tiled and parallelized for GPUs). This allows It to (1) allocate only as much data (or close to) as is required by computations running on each GPU, (2) efficiently track buffer allocations and hence, maximize data reuse across tiles and minimize the data transfer overhead, (3) and as a result, enable applications to maximize the utilization of the combined memory on multi-GPU machines. BBMM can work with any choice of parallelizing transformations, computation placement, and scheduling schemes, whether static or dynamic. Experiments run on a system with four GPUs with various scientific programs showed that BBMM is able to reduce data allocations on each GPU by up to 75 % compared to current allocation schemes, yield at least 88 % of the performance of hand-optimized Open CL codes and allows excellent weak scaling...|$|R
40|$|Massively {{multi-core}} processors such as GPUs and the Cell BE {{have the}} potential to deliver high performance computation to many applications. However, these processors require parallel programming on several levels, use some novel programming models, and native code written for one will not execute on the other. The RapidMind development platform enables portable access to the power of these processors. It provides a uniform, simple, safe, data-parallel programming model and takes care of most of the low-level details of mapping programs to each hardware target. It combines a dynamic compiler with a <b>runtime</b> streaming execution <b>manager,</b> and provides a single system image for computations running on any number of cores. The interface to this system is embedded inside ISO standard C++, where it can capture arbitrary computations specified at runtime. The use of a dynamic compiler means that high-level C++ abstractions can be used without sacrificing performance, while maintaining portability among current [...] and future [...] processors. This seminar will introduce and demonstrate the use of the RapidMind platform on GPUs (for both visualization and general-purpose computation), the Cell BE (where it runs on both IBM blades and the PS 3) and on multicore CPU (x 86 processor targets are under development). Comparative performance results will be presented for the GPU, the Cell BE, and multicore CPUs. On GPUs up to a 30 x speedup over tuned CPU code has been achieved. On the Cell, we can match or exceed the performance of vendor tools in tuned applications, but with much less developer effort. On CPUs, we have doubled performance over native tools (i. e. 8 x speedup on a 4 -core relative to icc on one). About the speaker Michael McCool is an Associate Professor in the School of Computer Science at the University of Waterloo and co-founder and Chief Scientist of RapidMind Inc. Prof. McCool's current research efforts are targeted at enabling high-performance parallel applications by the development of advanced programming technologies. Research interests include interval analysis, Monte Carlo and quasi-Monte Carlo numerical methods, optimization, simulation, sampling, cellular automata, real-time computer graphics, vision, image processing, hardware design, and programming languages and development platforms. He has degrees in both Computer Engineering (B. A. Sc. with Math option, Waterloo, 1989, Sir Sandford Medal) and Computer Science (M. Sc. in 1991 and Ph. D. in 1995, Toronto). Organiser(s) : Miguel Angel Marquina Computing Seminars / IT Department </address...|$|R

