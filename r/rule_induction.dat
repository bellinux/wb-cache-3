760|262|Public
25|$|Rough set {{methods can}} be applied as a {{component}} of hybrid solutions in machine learning and data mining. They {{have been found to be}} particularly useful for <b>rule</b> <b>induction</b> and feature selection (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three regions of rough sets are interpreted as regions of acceptance, rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.|$|E
2500|$|Rules induced {{from the}} lower {{approximation}} of the concept certainly describe the concept, hence such rules are called certain. [...] On the other hand, rules induced from the upper approximation of the concept describe the concept possibly, so these rules are called possible. [...] For <b>rule</b> <b>induction</b> LERS uses three algorithms: LEM1, LEM2, and IRIM.|$|E
2500|$|Rough {{set theory}} {{is useful for}} <b>rule</b> <b>induction</b> from {{incomplete}} data sets. Using this approach we can distinguish between three types of missing attribute values: lost values (the values that were recorded but currently are unavailable), attribute-concept values (these missing attribute values may be replaced by any attribute value limited to the same concept), and [...] "do not care" [...] conditions [...] (the original values were irrelevant). [...] A [...] concept (class) {{is a set of}} all objects classified (or diagnosed) the same way.|$|E
50|$|Another {{class of}} devices {{operating}} in the 27 MHz band are ISM (Industrial, Scientific and Medical) devices regulated by the FCC's Part 18 <b>rules.</b> <b>Induction</b> welding of plastics, and some types of diathermy machines commonly operate in this range. These devices are centered around 27.12 MHz with a tolerance of ±163 kHz, that is, 26.957 to 27.283 MHz.|$|R
40|$|Abstract. In {{the paper}} we study an {{evolutionary}} machine learning approach to data mining and knowledge discovery {{based on the}} classification <b>rules</b> <b>induction.</b> A method for automatic <b>rules</b> <b>induction</b> called AREX using evolutionary induction of decision trees and automatic programming is introduced. The proposed algorithm is applied to a cardiovascular dataset consisting of different groups of attributes which should possibly reveal the presence of some specific cardiovascular problems in young patients. A case study is presented that shows the use of AREX for the classification of patients and for discovering possible new medical knowledge from the dataset. The defined knowledge discovery loop comprises a medical expert’s assessment of induced rules to drive the evolution of rule sets towards more appropriate solutions. The final result is {{the discovery of a}} possible new medical knowledge in the field of pediatric cardiology. Index terms: machine learning, knowledge discovery, classification rules, pediatric cardiology, medical data mining 1...|$|R
40|$|This paper {{provides}} an <b>induction</b> <b>rule</b> {{that can be}} used to prove properties of data structures whose types are inductive, i. e., are carriers of initial algebras of functors. Our results are semantic in nature and are inspired by Hermida and Jacobs' elegant algebraic formulation of induction for polynomial data types. Our contribution is to derive, under slightly different assumptions, a sound <b>induction</b> <b>rule</b> that is generic over all inductive types, polynomial or not. Our <b>induction</b> <b>rule</b> is generic over the kinds of properties to be proved as well: like Hermida and Jacobs, we work in a general fibrational setting and so can accommodate very general notions of properties on inductive types rather than just those of a particular syntactic form. We establish the soundness of our generic <b>induction</b> <b>rule</b> by reducing <b>induction</b> to iteration. We then show how our generic <b>induction</b> <b>rule</b> can be instantiated to give <b>induction</b> <b>rules</b> for the data types of rose trees, finite hereditary sets, and hyperfunctions. The first of these lies outside the scope of Hermida and Jacobs' work because it is not polynomial, and as far as we are aware, no <b>induction</b> <b>rules</b> have been known to exist for the second and third in a general fibrational framework. Our instantiation for hyperfunctions underscores the value of working in the general fibrational setting since this data type cannot be interpreted as a set. Comment: For Special Issue from CSL 201...|$|R
2500|$|The LEM2 {{algorithm}} of LERS {{is frequently}} used for <b>rule</b> <b>induction</b> {{and is used}} not only in LERS but also in other systems, e.g., in RSES (Bazan et al. (2004). [...] LEM2 explores the search space of attribute-value pairs. [...] Its input data set is a lower or upper approximation of a concept, so its input data set is always consistent. [...] In general, LEM2 computes a local covering and then converts it into a rule set. [...] We will quote a few definitions to describe the LEM2 algorithm.|$|E
2500|$|While {{inductive}} and abductive inference are {{not part}} of logic proper, the methodology of logic has been applied to them with some degree of success. [...] For example, the notion of deductive validity (where an inference is deductively valid if and only if there is no possible situation in which all the premises are true but the conclusion false) exists in an analogy to the notion of inductive validity, or [...] "strength", where an inference is inductively strong if and only if its premises give some degree of probability to its conclusion. [...] Whereas the notion of deductive validity can be rigorously stated for systems of formal logic in terms of the well-understood notions of semantics, inductive validity requires us to define a reliable generalization of some set of observations. The task of providing this definition may be approached in various ways, some less formal than others; some of these definitions may use logical association <b>rule</b> <b>induction,</b> while others may use mathematical models of probability such as decision trees.|$|E
50|$|The {{most popular}} <b>rule</b> <b>induction</b> {{algorithm}} for dominance-based rough set approach is DOMLEM, which generates minimal set of rules.|$|E
40|$|Barendregt’s {{variable}} convention simplifies many informal proofs in the λ-calculus {{by allowing}} the consideration of only those bound variables that have been suitably chosen. Barendregt does not give a formal justification for the variable convention, which {{makes it hard to}} formalise such informal proofs. In this paper we show how a form of the variable convention can be built into the reasoning principles for <b>rule</b> <b>inductions.</b> We give two examples explaining our technique...|$|R
5000|$|It is {{possible}} to formalise PRA {{in such a way}} that it has no logical connectives at all—a sentence of PRA is just an equation between two terms. In this setting a term is a primitive recursive function of zero or more variables. In 1941 Haskell Curry gave the first such system. The <b>rule</b> of <b>induction</b> in Curry's system was unusual. A later refinement was given by Reuben Goodstein. The <b>rule</b> of <b>induction</b> in Goodstein's system is: ...|$|R
40|$|In {{this thesis}} we give a novel {{classification}} of techniques for designing parameterized algorithms, together with research publications applying these techniques, in particular Crown Decompositions, to various problems. In Part I {{we argue that}} the currently known techniques can be organized into just four general themes: Bounded Search Trees, Reduction <b>Rules,</b> <b>Induction</b> and Win/Win. The four main themes and their variations are presented through an algorithmic skeleton and illustrated by examples. Part II contains four research papers that apply the techniques described in Part I on th...|$|R
50|$|<b>Rule</b> <b>induction</b> {{is an area}} {{of machine}} {{learning}} in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.|$|E
50|$|At first glance, CBR {{may seem}} {{similar to the}} <b>rule</b> <b>induction</b> {{algorithms}} of machine learning. Like a rule-induction algorithm, CBR starts {{with a set of}} cases or training examples; it forms generalizations of these examples, albeit implicit ones, by identifying commonalities between a retrieved case and the target problem.|$|E
50|$|Rules induced {{from the}} lower {{approximation}} of the concept certainly describe the concept, hence such rules are called certain. On the other hand, rules induced from the upper approximation of the concept describe the concept possibly, so these rules are called possible. For <b>rule</b> <b>induction</b> LERS uses three algorithms: LEM1, LEM2, and IRIM.|$|E
5000|$|The sign of {{the charge}} left on the {{electroscope}} after grounding is always opposite in sign to the external inducing charge. The two <b>rules</b> of <b>induction</b> are: ...|$|R
40|$|AbstractWe {{describe}} novel computational {{techniques for}} constructing <b>induction</b> <b>rules</b> for deductive synthesis proofs. Deductive synthesis {{holds out the}} promise of automated construction of correct computer programs from specifications of their desired behaviour. Synthesis of programs with iteration or recursion requires inductive proof, but standard techniques {{for the construction of}} appropriate <b>induction</b> <b>rules</b> are restricted to recycling the recursive structure of the specifications. What is needed is <b>induction</b> <b>rule</b> construction techniques that can introduce novel recursive structures. We show that a combination of rippling and the use of meta-variables as a least-commitment device can provide such novelty...|$|R
40|$|In {{this paper}} we develop {{a method for}} {{automatic}} construction of customised <b>induction</b> <b>rules</b> {{for use in a}} semiinteractive theorem prover. The <b>induction</b> <b>rules</b> are developed to prove the total correctness of loops in an imperative language. We concentrate on integers. First we compute a partition of the domain of the induction variable. Our method makes use of failed proof attempts in the theorem prover to gain information about the problem structure and create the partition. Then, based on this partition we create an <b>induction</b> <b>rule,</b> in destructor style, that is customised to make the proving of the loop simpler. Our concern is in user interaction, rather than in proof strength. Using the customised <b>induction</b> <b>rules,</b> we find that in comparison to standard (Peano) induction or Noetherian induction, the proofs become more modularised and simpler user interaction can be expected. Furthermore, by using destructor style induction we circumvent the problem of creating inverses of functions and we use the machinery of a theorem prover (with symbolic execution) to make the method automatic. We also show that the customised <b>induction</b> <b>rules</b> created by the method are sound. ...|$|R
50|$|The CN2 {{induction}} {{algorithm is}} a learning algorithm for <b>rule</b> <b>induction.</b> It {{is designed to}} work even when the training data is imperfect. It is based on ideas from the AQ algorithm and the ID3 algorithm. As a consequence it creates a rule set like that created by AQ but is able to handle noisy data like ID3.|$|E
50|$|For example, the {{following}} linear {{program can be}} formulated {{in the context of}} a weighted average model V(xi)=w1xi1+...+wnxin with wj being the (non-negative) trade-off constant for criterion j (w1+...+wn=1) and xij being the data for alternative i on criterion j:This linear programming formulation can be generalized in context of additive value functions. Similar optimization problems (linear and nonlinear) can be formulated for outranking models, whereas decision rule models are build through <b>rule</b> <b>induction</b> algorithms.|$|E
50|$|Connectionist Learning with Adaptive <b>Rule</b> <b>Induction</b> On-line (CLARION) is a {{cognitive}} architecture {{that has been}} used to simulate several tasks in cognitive psychology and social psychology, as well as implementing intelligent systems in artificial intelligence applications. An important feature of CLARION is the distinction between implicit and explicit processes and focusing on capturing the interaction between these two types of processes. The system was created by the research group led by Ron Sun.|$|E
40|$|AbstractA {{well-known}} result (Leivant, 1983) states that, over basic Kalmar elementary arithmetic EA, {{the induction}} schema for ∑n formulas {{is equivalent to}} the uniform reflection principle for ∑n + 1 formulas (n ⩾ 1). We show that fragments of arithmetic axiomatized by various forms of <b>induction</b> <b>rules</b> admit a precise axiomatization in terms of reflection principles as well. Thus, the closure of EA under the <b>induction</b> <b>rule</b> for ∑n (or Πn + 1) formulas is equivalent to ω times iterated ∑n reflection principle. Moreover, for k < ω, k times iterated ∑n reflection principle over EA precisely corresponds to the extension of EA by ⩽k nested applications of ∑n <b>induction</b> <b>rule.</b> The above relationship holds in greater generality than just stated. In fact, we give general formulas characterizing in terms of iterated reflection principles the extension of any given theory (containing EA) by ⩽k nested applications of ∑n or Πn <b>induction</b> <b>rules.</b> In particular, the closure of a theory T under just one application of ∑ 1 <b>induction</b> <b>rule</b> is equivalent to T together with ∑ 1 reflection principle for each finite Π 2 axiomatized subtheory of T. These results have closely parallel ones in the theory of subrecursive function classes. The rules under study correspond, in a canonical way, to natural closure operators on the classes of provably recursive functions. Thus, ∑ 1 <b>induction</b> <b>rule</b> precisely corresponds to the primitive recursive closure operator, and ∑ 1 collection rule, introduced below, corresponds to the elementary closure operator...|$|R
40|$|Ambient Intelligence (AmI, shortly) gathers {{best results}} from three key technologies, Ubiquitous Computing, Ubiquitous Communication, and Intelligent User Friendly Interfaces. The {{functional}} and {{spatial distribution of}} tasks is a natural thrust to employ multi-agent paradigm to design and implement AmI environments. Two critical issues, common in most of applications, are (1) how to detect in a general and efficient way "context" from sensors and (2) how to process contextual information {{in order to improve}} the functionality of services. In this work we experiment a framework where hybrid techniques (distributed fuzzy control, mobile agents, fuzzy <b>rules</b> <b>induction</b> algorithms) are mixed to gain flexibility and uniformity. © 2005 IEEE...|$|R
40|$|We {{describe}} novel computational {{techniques for}} constructing <b>induction</b> <b>rules</b> for deductive synthesis proofs. Deductive synthesis {{holds out the}} promise of automated construction of correct computer programs from specifications of their desired behaviour. Synthesis of programs with iteration or recursion requires inductive proof, but standard techniques {{for the construction of}} appropriate <b>induction</b> <b>rules</b> are restricted to recycling the recursive structure of the specifications. What is needed is <b>induction</b> <b>rule</b> construction techniques that can introduce novel recursive structures. We show that a combination of rippling and the use of meta-variables as a least-commitment device can provide such novelty. Key words: deductive synthesis, proof planning, induction, theorem proving, middle-out reasoning. ...|$|R
50|$|Once {{examples}} and features are created, {{we need a}} way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and <b>rule</b> <b>induction.</b> In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.|$|E
5000|$|Rough {{set theory}} {{is useful for}} <b>rule</b> <b>induction</b> from {{incomplete}} data sets. Using this approach we can distinguish between three types of missing attribute values: lost values (the values that were recorded but currently are unavailable), attribute-concept values (these missing attribute values may be replaced by any attribute value limited to the same concept), and [...] "do not care" [...] conditions (the original values were irrelevant). A concept (class) {{is a set of}} all objects classified (or diagnosed) the same way.|$|E
50|$|The LEM2 {{algorithm}} of LERS {{is frequently}} used for <b>rule</b> <b>induction</b> {{and is used}} not only in LERS but also in other systems, e.g., in RSES (Bazan et al. (2004). LEM2 explores the search space of attribute-value pairs. Its input data set is a lower or upper approximation of a concept, so its input data set is always consistent. In general, LEM2 computes a local covering and then converts it into a rule set. We will quote a few definitions to describe the LEM2 algorithm.|$|E
40|$|A {{well-known}} result [6] states that, over basic Kalmar elementary arithmetic EA, {{the induction}} schema for Σ n formulas {{is equivalent to}} the uniform reflection principle for Σ n+ 1 formulas (n 1). We show that fragments of arithmetic axiomatized by various forms of <b>induction</b> <b>rules</b> admit a precise axiomatization in terms of reflection principles as well. Thus, the closure of EA under the <b>induction</b> <b>rule</b> for Σ n (or Π n+ 1) formulas is equivalent to ! times iterated Σ n reflection principle. Moreover, for k ! !, k times iterated Σ n reflection principle over EA precisely corresponds to the extension of EA by k nested applications of Σ n <b>induction</b> <b>rule.</b> The above relationship holds in greater generality than just stated. In fact, we give general formulas characterizing in terms of iterated reflection principles the extension of any given theory (containing EA) by k nested applications of Σ n or Π n <b>induction</b> <b>rules.</b> In particular, the closure of a [...] ...|$|R
40|$|Walther's {{estimation}} calculus {{was designed}} to prove the termination of functional programs, and {{can also be used}} to solve the similar problem of proving the well-foundedness of <b>induction</b> <b>rules.</b> However, there are certain features of the goal formulae which are more common to the problem of <b>induction</b> <b>rule</b> well-foundedness than the problem of termination, and which the calculus cannot handle. We present a sound extension of the calculus that is capable of dealing with these features. The extension develops Walther's concept of an argument bounded function in two ways: firstly, so that the function may be bounded below by its argument, and secondly, so that a bound may exist between two arguments of a predicate. Our calculus enables automatic proofs of the well-foundedness of a large class of <b>induction</b> <b>rules</b> not captured by the original calculus. 1 Introduction An <b>induction</b> <b>rule</b> is well-founded iff there is a well-founded order such that each step case of the rule the indu [...] ...|$|R
50|$|This can {{be proved}} {{by using the}} product <b>rule</b> and {{mathematical}} <b>induction</b> (see proof below).|$|R
50|$|If for {{instance}} a procedure for plain pancakes is mapped to blueberry pancakes, {{a decision is}} made {{to use the same}} basic batter and frying method, thus implicitly generalizing the set of situations under which the batter and frying method can be used. The key difference, however, between the implicit generalization in CBR and the generalization in <b>rule</b> <b>induction</b> lies in when the generalization is made. A rule-induction algorithm draws its generalizations from a set of training examples before the target problem is even known; that is, it performs eager generalization.|$|E
50|$|The {{notion of}} {{deductive}} validity can be rigorously stated for systems of formal logic {{in terms of}} the well-understood notions of semantics. Inductive validity, on the other hand, requires us to define a reliable generalization of some set of observations. The task of providing this definition may be approached in various ways, some less formal than others; some of these definitions may use logical association <b>rule</b> <b>induction,</b> while others may use mathematical models of probability such as decision trees. For the most part this discussion of logic deals only with deductive logic.|$|E
50|$|Rough set {{methods can}} be applied as a {{component}} of hybrid solutions in machine learning and data mining. They {{have been found to be}} particularly useful for <b>rule</b> <b>induction</b> and feature selection (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three regions of rough sets are interpreted as regions of acceptance, rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.|$|E
40|$|This paper compares two {{different}} approaches to computer-aided analysis of ECG signals. ECG records are preprocessed by the wavelet transform, and the machine learning method of decision trees and fuzzy <b>rules</b> <b>induction</b> are used for classification. The wavelet transform allows good localisation of QRS complexes, P and T waves in time and amplitude. The average accuracy of detection of all events is above 87 per cent. For learning and further classification we use Quinlan's See 5 application and FURL (FUzzy Rule Learner). We used the MIT-BIH database for experiments. Diverse settings of the parameters for decision tree generation (tree pruning, attribute selection, class sets) were examined. Two datasets and diverse settings of fuzzysets were examined as well. ...|$|R
40|$|This paper {{reviews the}} data mining methods that are {{combined}} with Geographic Information Systems (GIS) {{for carrying out}} spatial analysis of geographic data. We will first look at data mining functions as applied to such data and then highlight their specificity compared with their application to classical data. We {{will go on to}} describe the research that is currently going on in this area, pointing out that there are two approaches: the first comes from learning on spatial databases, while the second is based on spatial statistics. We will conclude by discussing the main differences between these two approaches and the elements they have in common. &# 13; &# 13; Index Terms — Spatial Data Mining, Spatial Databases, <b>Rules</b> <b>Induction,</b> Spatial Statistics, Spatial&# 13; Neighborhood...|$|R
40|$|A {{key problem}} in {{automating}} proof by mathematical induction is choosing an <b>induction</b> <b>rule</b> {{suitable for a}} given conjecture. Since Boyer & Moore’s NQTHM system the standard approach {{has been based on}} recursion analysis, which uses a combination of <b>induction</b> <b>rules</b> based on the relevant recursive function definitions. However, there are practical examples on which such techniques are known to fail. Recent research has tried to improve automation by delaying the choice of inductive rule until later in the proof, but these techniques suffer from two serious problems. Firstly, a lack of search control: specifically, in controlling the application of ‘speculative’ proof steps that partially commit to a choice of <b>induction</b> <b>rule.</b> Secondly, a lack of generality: they place significant restrictions on the form of <b>induction</b> <b>rule</b> that can be chosen. In this thesis we describe a new delayed commitment strategy for inductive proof that addresses these problems. The strategy dynamically creates an appropriate <b>induction</b> <b>rule</b> by proving schematic proof goals, where unknown rule structure is represented by meta-variables which become instantiated during the proof. This is accompanied by a proof that the generated rule is valid. The strategy achieves improved control over speculative proof steps via a novel speculation critic. It also generates a wider range of useful <b>induction</b> <b>rules</b> than other delayed commitment techniques, partly because it removes unnecessary restrictions on the individual proof cases, and partly because of a new technique for generating the rule’s overall case structure. The basic version of the strategy has been implemented using the lamdaClam proof planner. The system was extended with a novel proof critics architecture for this purpose. An evaluation shows the strategy is a useful and practical technique, and demonstrates its advantages. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
