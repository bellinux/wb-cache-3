13|9|Public
50|$|Early control {{stores were}} {{implemented}} as a diode-array accessed via address decoders, {{a form of}} read-only memory. This tradition {{dates back to the}} program timing matrix on the MIT Whirlwind, first described in 1947. Modern VLSI processors instead use matrices of field-effect transistors to build the ROM and/or PLA structures used to control the processor as well as its internal sequencer in a microcoded implementation. IBM System/360 used a variety of techniques: CCROS (Card Capacitor <b>Read-Only</b> <b>Storage)</b> on the Model 30, TROS (Transformer <b>Read-Only</b> <b>Storage)</b> on the Model 40, and BCROS (Balanced Capacitor <b>Read-Only</b> <b>Storage)</b> on the Model 50.|$|E
5000|$|Resistor, capacitor, or {{transformer}} matrix ROM, {{used in many}} computers {{until the}} 1970s. Like diode matrix ROM, it was programmed by placing components at selected locations between a matrix of word lines and bit lines. ENIAC's Function Tables were resistor matrix ROM, programmed by manually setting rotary switches. Various models of the IBM System/360 and complex peripheral devices stored their microcode in either capacitor (called BCROS for balanced capacitor <b>read-only</b> <b>storage</b> on the 360/50 and 360/65, or CCROS for charged capacitor <b>read-only</b> <b>storage</b> on the 360/30) or transformer (called TROS for transformer <b>read-only</b> <b>storage</b> on the 360/20, 360/40 and others) matrix ROM.|$|E
5000|$|... #Caption: Technology {{has progressed}} {{enormously}} {{from the above}} <b>read-only</b> <b>storage</b> to the subsequent writeable control storage ...|$|E
40|$|Abstract. We {{describe}} the three-dimensional holographic optical disc system for application as read-only memory. Several systems design issues are discussed including imaging optics, storage density, readout rate, and the alignment servo system. We demonstrate 40 bits=lm 2 surface density and pixel-matched holographic data readout from a 12 cm disc spinning at 600 RPM. Key words: holographic <b>storage,</b> <b>read-only</b> memory, readout, <b>storage</b> density 1...|$|R
5000|$|<b>Read-only</b> control <b>storage</b> for {{microcode}} employs [...] "balanced capacitor technology" [...] (BCROS) with {{cycle time}} of 500 nanoseconds, designed by Anthony Proudman in IBM's Hursley laboratory and implemented by Fernando [...] "Fred" [...] Neves. This technology uses two capacitors to represent each bit.|$|R
40|$|Interactive Image Display Program (IMDISP) is {{interactive}} image-displaying {{utility program}} for IBM personal computer (PC, XT, and AT models) and compatibles. Magnifications, contrasts, and/or subsampling selected for whole or partial images. IMDISP developed {{for use with}} CD-ROM (Compact Disk <b>Read-Only</b> Memory) <b>storage</b> system. Written in C language (94 percent) and Assembler (6 percent) ...|$|R
50|$|Some {{commercial}} machines, {{for example}} IBM 360/85, have both a <b>read-only</b> <b>storage</b> and a writable control store for microcode.|$|E
50|$|LEAF is {{intended}} to work well with <b>read-only</b> <b>storage</b> media, such as write-protected floppy drives or optical discs. Distribution sizes range from a single floppy disk to several hundred megabytes.|$|E
50|$|The TransbaseCD {{database}} option can use <b>read-only</b> <b>storage</b> media such as CD, DVD or Blu-ray Discs. In addition, {{a persistent}} disk cache can be utilized to store data for performance improvement and/or for updates of data supplied originally as read-only media.|$|E
5000|$|The {{fact that}} the JMS {{instruction}} used the word just before the code of the subroutine to deposit the return address prevented reentrancy and recursion without additional work by the programmer. It also {{made it difficult to}} use ROM with the PDP-8 because read-write return-address storage was commingled with <b>read-only</b> code <b>storage</b> in the address space. Programs intended to be placed into ROMs approached this problem in several ways: ...|$|R
5000|$|... “IEEE Reynold B. Johnson Data Storage Device Technology Award” (with SanDisk co-founders Ei Harari and Jack Yuan) for “leadership in the {{development}} and commercialization of Flash electrically erasable programmable <b>read-only</b> memory-based data <b>storage</b> products,” 2006.|$|R
40|$|The Mosaic {{element is}} a fast single chip {{computer}} {{designed to be}} used in groups for concurrent computation experiments. Each element contains a 16 -bit processor, read-write <b>storage,</b> <b>read-only</b> store for a small initialization and bootstrap loading program and four output ports. The Mosaic processor, a highly structured design that achieves very good performance and density through innovations in its microcode, circuit techniques, and layout, is described in detai 1...|$|R
50|$|SSDs had {{origins in}} the 1950s with two similar technologies: {{magnetic}} core memory and charged capacitor <b>read-only</b> <b>storage</b> (CCROS). These auxiliary memory units (as contemporaries called them) emerged during the era of vacuum-tube computers, though their use ceased {{with the introduction of}} cheaper drum storage units.|$|E
50|$|Portable {{applications}} can {{be stored}} on any data storage device, including internal mass storage, a file share, cloud storage or external storage such as USB drives and floppy disks—storing its program files and any configuration information and data on the storage medium alone. If no configuration information is required a portable program can be run from <b>read-only</b> <b>storage</b> such as CD-ROMs and DVD-ROMs. Some applications are available in both installable and portable versions.|$|E
5000|$|The Model 30, {{the slowest}} {{model in the}} line, uses an 8-bit {{microarchitecture}} {{with only a few}} hardware registers; everything that the programmer saw is emulated by the microprogram. The microcode for this model is also held on special punched cards, which are stored inside the machine in a dedicated reader per card, called [...] "CROS" [...] units (Capacitor <b>Read-Only</b> <b>Storage).</b> A second CROS reader is installed for machines ordered with 1620 emulation.|$|E
5000|$|Displays reached 640x480 (VGA) {{resolution}} by 1988 (Compaq SLT/286), {{and color}} screens started becoming a common upgrade in 1991, with increases in resolution and screen size occurring frequently until {{the introduction of}} 17" [...] screen laptops in 2003. Hard drives started {{to be used in}} portables, encouraged by the introduction of 3.5" [...] drives in the late 1980s, and became common in laptops starting with the introduction of 2.5" [...] and smaller drives around 1990; capacities have typically lagged behind physically larger desktop drives. Optical <b>storage,</b> <b>read-only</b> CD-ROM followed by writeable CD and later read-only or writeable DVD and Blu-ray players, became common in laptops early in the 2000s.|$|R
5000|$|For {{programs}} {{running in}} user mode, the high-order three {{bits of the}} address field of an instruction serve as an index to an array of eight registers (R0-R7). [...] Each register contains a 5-bit value (Rn) which is prepended to the low-order 11 bits of the instruction address field to form the 16-bit physical address. This divides virtual memory logically into eight blocks of 2048 words each. The registers allow access to 16K words {{at any one time}} out of a possible 32K words of physical memory. A sixth bit (Pn) in each register indicates a <b>read-only</b> block of <b>storage.</b> [...] Rn=0 and Pn=1 indicates an unassigned block, and any reference causes a trap. The map registers can only be set in monitor mode.|$|R
40|$|The Interactive Image Display Program (IMDISP) is an {{interactive}} image display utility for the IBM Personal Computer (PC, XT and AT) and compatibles. Until recently, efforts to utilize small computer systems for display {{and analysis of}} scientific data have been hampered {{by the lack of}} sufficient data storage capacity to accomodate large image arrays. Most planetary images, for example, require nearly a megabyte of storage. The recent development of the "CDROM" (Compact Disk <b>Read-Only</b> Memory) <b>storage</b> technology makes possible the storage of up to 680 megabytes of data on a single 4. 72 -inch disk. IMDISP was developed for use with the CDROM storage system which is currently being evaluated by the Planetary Data System. The latest disks to be produced by the Planetary Data System are a set of three disks containing all of the images of Uranus acquired by the Voyager spacecraft. The images are in both compressed and uncompressed format. IMDISP can read the uncompressed images directly, but special software is provided to decompress the compressed images, which can not be processed directly. IMDISP can also display images stored on floppy or hard disks. A digital image is a picture converted to numerical form {{so that it can be}} stored and used in a computer. The image is divided into a matrix of small regions called picture elements, or pixels. The rows and columns of pixels are called "lines" and "samples", respectively. Each pixel has a numerical value, or DN (data number) value, quantifying the darkness or brightness of the image at that spot. In total, each pixel has an address (line number, sample number) and a DN value, which is all that the computer needs for processing. DISPLAY commands allow the IMDISP user to display all or part of an image at various positions on the display screen. The user may also zoom in and out from a point on the image defined by the cursor, and may pan around the image. To enable more or all of the original image to be displayed on the screen at once, the image can be "subsampled. " For example, if the image were subsampled by a factor of 2, every other pixel from every other line would be displayed, starting from the upper left corner of the image. Any positive integer may be used for subsampling. The user may produce a histogram of an image file, which is a graph showing the number of pixels per DN value, or per range of DN values, for the entire image. IMDISP can also plot the DN value versus pixels along a line between two points on the image. The user can "stretch" or increase the contrast of an image by specifying low and high DN values; all pixels with values lower than the specified "low" will then become black, and all pixels higher than the specified "high" value will become white. Pixels between the low and high values will be evenly shaded between black and white. IMDISP is written in a modular form to make it easy to change it to work with different display devices or on other computers. The code can also be adapted for use in other application programs. There are device dependent image display modules, general image display subroutines, image I/O routines, and image label and command line parsing routines. The IMDISP system is written in C-language (94 %) and Assembler (6 %). It was implemented on an IBM PC with the MS DOS 3. 21 operating system. IMDISP has a memory requirement of about 142 k bytes. IMDISP was developed in 1989 and is a copyrighted work with all copyright vested in NASA. Additional planetary images can be obtained from the National Space Science Data Center at (301) 286 - 6695...|$|R
50|$|Another form of {{core memory}} called core rope memory {{provided}} <b>read-only</b> <b>storage.</b> In this case, the cores, which had more linear magnetic materials, were simply used as transformers; no information was actually stored magnetically within the individual cores. Each {{bit of the}} word had one core. Reading {{the contents of a}} given memory address generated a pulse of current in a wire corresponding to that address. Each address wire was threaded either through a core to signify a binary 1, or around the outside of that core, to signify a binary 0. As expected, the cores were much larger physically than those of read-write core memory. This type of memory was exceptionally reliable. An example was the Apollo Guidance Computer used for the moon landings.|$|E
40|$|Abstract: The {{architecture}} * of {{the newly}} announced IBM System/ 360 features four innovations: 1. An approach to storage which permits and exploits very large capacities, hierarchies of speeds, <b>read-only</b> <b>storage</b> for microprogram control, flexible storage protection, and simple program relocation. 2. An input!output system offering new degrees of concurrent operation, compatible channel operation, data rates approaching 5, 000, 000 characters/second, integrated design of hardware and software, a new low-e:ost, multiple-ehannel package sharing main-frame hardware, new provisions for device status infor-mation, and a standard channel interface between central processing unit and input/output devices. 3. A truly general-purpose machine organization offering new supervisory facilities, powerful logical pro-cessing operations, and {{a wide variety of}} data formats. 4. Strict upward and downward machine-language compatibility over a line of six models having a per-formance range factor of SO. This paper discusses in detail the objectives of the design and the rationale for the main features of the architecture. Emphasis is given to the problems raised by the need for compatibility among central process-ing units of various size and by the conflicting demands of commercial, scientific, real-time, and logical in-formation processing. A tabular summary of the architecture is shown in the Appendices...|$|E
40|$|The Cooperative File System (CFS) {{is a new}} {{peer-to-peer}} <b>read-only</b> <b>storage</b> {{system that}} provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic {{in the number of}} servers and requires logarithmic state at each node. CFS is implemented using the SFS file system toolkit and runs on many UNIX operating systems including Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is able to route queries in a scalable way. For example, in experiments with a system of 4, 096 servers, looking up a block of data involves contacting only seven servers. In general, a logarithmic number of servers must be contacted to route a query. Servers are also able to join and leave the system efficiently. Tests demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail...|$|E
40|$|Thesis (Ph. D.) [...] University of Washington, 2014 Scientists {{today are}} able to {{generate}} data at an unprecedented scale and rate. For example the Sloan Digital Sky Survey (SDSS) generates 200 GB of data containing millions of objects on each night on its routine operation. The large hadron collider is producing even more data today which is approximately 30 PB annually. The Large Synoptic Survey Telescope (LSST) also will be producing approximately 30 TB of data per night in a few years. Also, in many fields of science, multidimensional arrays rather than flat tables are standard data types because data values are associated with coordinates in space and time. For example, images in astronomy are 2 D arrays of pixel intensities. Climate and ocean models use arrays or meshes to describe 3 D regions of the atmosphere and oceans. As a result, scientists need powerful tools to help them manage massive arrays. This thesis focuses on various challenges in building parallel array data management systems that facilitate massive-scale data analytics over arrays. The first challenge with building an array data processing system is simply how to store arrays on disk. The key {{question is how to}} partition arrays into smaller fragments called chunks that form the unit of IO, processing, and data distribution across machines in a cluster. We explore this question in ArrayStore, a new <b>read-only</b> <b>storage</b> manager for parallel array processing. In ArrayStore, we study the impact of different chunking strategies on query processing performance {{for a wide range of}} operations, including binary operators and user-defined functions. ArrayStore also proposes two new techniques that enable operators to access data from adjacent array fragments during parallel processing. The second challenge that we explore in building array systems is the ability to create, archive, and explore different versions of the array data. We address this question in TimeArr, a new append-only storage manager for an array database. Its key contribution is to efficiently store and retrieve versions of an entire array or some sub-array. To achieve high performance, TimeArr relies on several techniques including virtual tiles, bitmask compression of changes, variable-length delta representations, and skip links. The third challenge that we tackle in building parallel array engines is how to provide efficient iterative computation on multi-dimensional scientific arrays. We present the design, implementation, and evaluation of ArrayLoop, an extension of SciDB with native support for array iterations. In the context of ArrayLoop, we develop a model for iterative processing in a parallel array engine. We then present three optimizations to improve the performance of these types of computations: incremental processing, mini-iteration overlap processing, and multi-resolution processing. Finally, as motivation for our work and also to help push our technology back into the hands of science users, we have built the AscotDB system. AscotDB is a new, extensible data analysis system for the interactive analysis of data from astronomical surveys. AscotDB provides a compelling and powerful environment for the exploration, analysis, visualization, and sharing of large array datasets...|$|E

