33|606|Public
50|$|Plastic also {{provides}} a proxy server that caches frequently accessed <b>revision</b> <b>data</b> {{and can be used}} to reduce traffic on centralized setups.|$|E
40|$|Employment {{data are}} {{provided}} by U. S. Bureau of Labor Statistics and {{may be subject to}} <b>revision.</b> <b>Data</b> are not seasonally adjusted and may not include adjustments for strikes. Rankings are based on percent change compared to same month of the previous year. Government includes federal, state and local agencies. Volume numbering and issue date do not reflect coverage dates...|$|E
40|$|This chapter {{describes}} the successful {{development of a}} new methodology for studying on-line writing. The text-logging tool ScriptLog has been combined with the eyetracking technology iView X HED HT, in order to enhance the study of the interplay between writing, monitoring and <b>revision.</b> <b>Data</b> on the distribution of visual attention during writing help determining to what extent pauses are used for monitoring. The complexity of the experimental settings, and the expertise needed for interpreting the eye-tracking data make this a method suitable mainly for laboratory settings. The chapter also introduces an analysis tool that merges data from ScriptLog and iView and thus helps the researcher to organise and analyse the vast amount of data produced...|$|E
40|$|In {{the past}} ten years, {{researchers}} have explored the impact of <b>data</b> <b>revisions</b> in many different contexts. Researchers have examined the properties of <b>data</b> <b>revisions,</b> how structural modeling is affected by <b>data</b> <b>revisions,</b> how <b>data</b> <b>revisions</b> affect forecasting, the impact of <b>data</b> <b>revisions</b> on monetary policy analysis, {{and the use of}} real-time data in current analysis. This paper summarizes many of the questions for which real-time data analysis has provided answers. In addition, researchers and institutions have developed better real-time data sets around the world. Still, additional research is needed in key areas and research to date has uncovered even more fruitful areas worth exploring. (JEL C 52, C 53, C 80, E 01) ...|$|R
40|$|In this paper, using recent {{empirical}} results {{regarding the}} statistical properties of macroeconomic <b>data</b> <b>revisions,</b> we study the e¤ects of <b>data</b> <b>revisions</b> {{in a general}} equilibrium framework. We …nd {{that the presence of}} <b>data</b> <b>revisions,</b> or <b>data</b> uncertainty, creates a precautionary motive and causes signi…cant changes in the decisions of agents. We also …nd that the model with revisions captures some aspects of the business cycle dynamics of the US data better than the benchmark model with no revisions. Using our model we measure the cost of having <b>data</b> <b>revisions</b> to be about $ 33 billion, $ 5 billion of which can be recovered by eliminating the predictability of revisions. Comparing these numbers with the budgets of the major statistical agencies in the US, we conclude that any money spent on the improvement of data collection would be well worth it...|$|R
40|$|This paper {{analyzes}} forward-looking {{rules for}} Swiss monetary policy {{in a small}} structural VAR model consisting of four variables taking into account <b>data</b> <b>revisions</b> for GDP. First, the paper develops an analytical method to analyze the effect of <b>data</b> <b>revision</b> errors in GDP on the ex ante or conditional inflation-output-growth volatility trade-off and applies it to Swiss data. Second, the effects of different targets in a forward-looking monetary policy on ex post or unconditional volatility of inflation and output growth is explored by a simulation exercise. In general, the results obtained suggest that focusing monetary policy on GDP growth instead on inflation may lead to an inefficient policy with both increased medium term inflation and GDP growth volatility {{in the presence of}} GDP <b>data</b> <b>revisions.</b> Structural VAR, forward-looking monetary policy, efficiency frontier, GDP <b>data</b> <b>revisions...</b>|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThis thesis will define {{and analyze the}} Department of Defense proposal to revise the method of determining whether items of equipment are financed from Procurement accounts or Operation and Maintenance accounts. Problems with the method currently in use are explained, along with Congressional objection to the Department of Defense proposal of <b>revision.</b> <b>Data</b> {{on the cost of}} items of equipment are presented and analyzed to determine the adequacy of the current dollar threshold that determines whether items are funded from Procurement accounts or Operation and Maintenance accounts. A method for determining a dollar threshold that will better {{meet the needs of the}} Department of Defense and still be acceptable to Congress is explained. [URL] United States Nav...|$|E
40|$|Mathematics {{learning}} aims {{to develop}} students’ critical thinking. Therefore, {{it is necessary}} to develop Student Worksheet to train students' critical thinking skills. This research aims to develop student worksheet that has characteristics of critical thinking skill for 7 th grade Junior High School students (SMP). The sample in this research is VII- 7 class with 30 students of the second semester of SMP Negeri 9 Palembang. The research method used is a development research method that consists of analysis, design, evaluation, and <b>revision.</b> <b>Data</b> collection is done by prototyping. Prototyping was used to find out how the characteristics of student worksheet to develop critical thinking ability of grade 7 th students. The result of this research is characteristic of student worksheet to develop critical thinking ability of 7 th grade Junior High School student. ...|$|E
40|$|We {{examined}} investment {{behavior in}} the Japanese manufacturing industry using investment <b>revision</b> <b>data</b> to analyze investment behavior from a fresh angle. We tested the martingale investment hypothesis and then the q-theory of investment {{by looking at the}} response of stock return and investment to news arriving at firms. The martingale hypothesis was accepted at early stage of investment planning, but not at later stages. We also found evidence for the validity of the q-theory hypothesis. Investment was responsive to profit rate revision and sales revision, but stock return responded only to profit rate revision. Further investigation revealed that investment was also motivated by expansion of market share for sales, especially for industries with rapid technological progress. J. Japanese Int. Economies 22 (4) (2008) 663 - 676. Investment revision Martingale Stock return q theory Market share Flexible accelerator theory...|$|E
40|$|Event history {{calendars}} (EHC) {{have proven}} to be a useful tool to collect retrospective autobiographic life course data. One problem is that they are only standardized to some extent. This limits their applicability in large-scale surveys. However, in such surveys a modularized retrospective CATI design can be combined with EHC. This <b>data</b> <b>revision</b> module is directly integrated into the interview and used as a <b>data</b> <b>revision</b> module. Hereby insights from cognitive psychology are applied. The <b>data</b> <b>revision</b> module stimulates the respondent's memory retrieval by detecting both temporal inconsistencies, such as gaps, and overlapping or parallel events. This approach was implemented in the IAB-ALWA study (Work and Learning in a Changing World), a large-scale representative telephone survey with 10, 000 respondents. By comparing the uncorrected data with the final <b>data</b> after <b>revision,</b> we investigate to what extent the application of this <b>data</b> <b>revision</b> module improves <b>data</b> quality or more precisely, time consistency and dating accuracy of individual reports. " (Author's abstract, IAB-Doku) ((en)) Biografieforschung - Methode, empirische Sozialforschung, Lebenslauf...|$|R
40|$|VERY PRELIMINAR VERSION. DO NOT QUOTE WITHOUT PERMISSION We {{propose a}} model to compute {{short-term}} forecasts of the Euro area GDP growth and probabilities of recession in real-time. To allow for forecast evaluation, we construct a real-time data set that changes for each vintage date and includes the exact information that was {{available at the time}} of each forecast. We provide examples showing how <b>data</b> <b>revisions</b> and <b>data</b> availability a¤ect point forecasts and forecast uncertainty...|$|R
40|$|We {{propose a}} model to compute {{short-term}} forecasts of the Euro area GDP growth in real-time. To allow for forecast evaluation, we construct a real-time data set that changes for each vintage date and includes the exact information that was {{available at the time}} of each forecast. In this context, we provide examples that show how <b>data</b> <b>revisions</b> and <b>data</b> availability affect point forecasts and forecast uncertainty. business cycles, output growth, time series...|$|R
40|$|We {{present an}} {{open-source}} toolkit which allows (i) to reconstruct past states of Wikipedia, and (ii) to efficiently access the edit history of Wikipedia articles. Reconstructing past states of Wikipedia {{is a prerequisite}} for reproducing previous experimental work based on Wikipedia. Beyond that, the edit history of Wikipedia articles {{has been shown to be}} a valuable knowledge source for NLP, but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data. By using a dedicated storage format, our toolkit massively decreases the data volume to less than 2 % of the original size, and at the same time provides an easy-to-use interface to access the <b>revision</b> <b>data.</b> The language-independent design allows to process any language represented in Wikipedia. We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia’s edit history. ...|$|E
40|$|We {{introduce}} ANALEC, a tool which aim is {{to bring}} together corpus annotation, visualization and query management. Our main idea {{is to provide a}} unified and dynamic way of annotating textual data. ANALEC allows researchers to dynamically build their own annotation scheme and use the possibilities of scheme <b>revision,</b> <b>data</b> querying and graphical visualization during the annotation process. Each query result can be visualized using a graphical representation that puts forward a set of annotations that can be directly corrected or completed. Text annotation is then considered as a cyclic process. We show that statistics like frequencies and correlations make it possible to verify annotated data on the fly during the annotation. In this paper we introduce the annotation functionalities of ANALEC, some of the annotated data visualization functionalities, and three statistical modules: frequency, correlation and geometrical representations. Some examples dealing with reference and coreference annotation illustrate the main contributions of ANALEC...|$|E
40|$|Objectives—This report {{presents}} the revised growth charts for the United States. It summarizes {{the history of}} the 1977 National Center for Health Statistics (NCHS) growth charts, reasons for the <b>revision,</b> <b>data</b> sources and statistical procedures used, and major features of the revised charts. Methods—Data from five national health examination surveys collected from 1963 to 1994 and five supplementary data sources were combined to establish an analytic growth chart data set. A variety of statistical procedures were used to produce smoothed percentile curves for infants (from birth to 36 months) and older children (from 2 to 20 years), using a two-stage approach. Initial curve smoothing for selected major percentiles was accomplished with various parametric and nonparametric procedures. In the second stage, a normalization procedure was used to generate z-scores that closely match the smoothed percentile curves. Results—The 14 NCHS growth charts were revised and new body mass index-for-age (BMI-for-age) charts were created for boys and girl...|$|E
40|$|Realistic {{modeling}} of <b>data</b> <b>revisions</b> {{can play an}} impor-tant role in policy formulation. A common way to model <b>data</b> <b>revisions</b> {{is to set up}} a state-space model with sep-arate blocks for measurement errors and the dynamics of ”true ” values. However, empirical work suggests that measurement errors typically have much more complex dynamics than such models allow. This paper describes a state-space model with richer dynamics in these mea-surement errors, including the noise, news and spillover effects documented in this literature. The result is a uni-fied and flexible framework that allows for more realism in the model of <b>data</b> <b>revision</b> and optimal real-time es-timation of trends and cycles in real time. We illustrate the application of this framework with an analysis of real-time data on U. S. real output. KEY WORDS: real-time analysis, <b>data</b> <b>revisions,</b> state-space model...|$|R
40|$|This paper tracks <b>data</b> <b>revisions</b> in the Personal Consumption Expenditure {{using the}} exclusions-from-core {{inflation}} persistence model. Keeping {{the number of}} observations the same, the regression parameters of earlier vintages of real-time data, beginning with vintage 1996 :Q 1, are tested for coincidence against the regression parameters of the last vintage of real-time data used in this paper, which is vintage 2008 :Q 2 in a parametric and two nonparametric frameworks. The effects of <b>data</b> <b>revisions</b> are not detectable {{in the vast majority}} of cases in the parametric model, but the flexibility of the two nonparametric models is able to utilize the <b>data</b> <b>revisions.</b> ...|$|R
40|$|It is {{now widely}} {{accepted}} that early vintages of GDP data {{are likely to}} undergo a series of revisions as more information becomes available to National Statistics Institutions. This, however, creates a trade-off between timeliness and accuracy for data users such as monetary policy-makers. This article investigates the importance of <b>data</b> <b>revisions</b> for the operation of monetary policy in the UK. I find <b>data</b> <b>revisions</b> have been smaller {{in the last decade}} and early data vintages provide a relatively good signal of data released at a later date. There is though evidence of cyclicality and structural breaks in the <b>data</b> <b>revision</b> process which should caution data users against assuming <b>data</b> <b>revisions</b> are predictable. I also look at the role of the output gap in the practical implementation of monetary policy through the Taylor rule. While <b>revisions</b> to GDP <b>data</b> account for some of the mismeasurement of the output gap, I conclude that it is a relatively small culprit. The main cause of output gap revisions is due to the difficulty in estimating the level of potential output {{towards the end of the}} sample where statistical filter methods are generally unreliable...|$|R
40|$|International audienceWe {{introduce}} ANALEC, a tool which aim is {{to bring}} together corpus annotation, visualization and query management. Our main idea {{is to provide a}} unified and dynamic way of annotating textual data. ANALEC allows researchers to dynamically build their own annotation scheme and use the possibilities of scheme <b>revision,</b> <b>data</b> querying and graphical visualization during the annotation process. Each query result can be visualized using a graphical representation that puts forward a set of annotations that can be directly corrected or completed. Text annotation is then considered as a cyclic process. We show that statistics like frequencies and correlations make it possible to verify annotated data on the fly during the annotation. In this paper we introduce the annotation functionalities of ANALEC, some of the annotated data visualization functionalities, and three statistical modules: frequency, correlation and geometrical representations. Some examples dealing with reference and coreference annotation illustrate the main contributions of ANALEC...|$|E
40|$|Abstract With {{the rise}} of the Web 2. 0, {{participatory}} and collaborative content production have largely replaced the traditional ways of information sharing and have created the novel genre of collaboratively constructed language resources. A vast untapped potential lies in the dynamic aspects of these resources, which cannot be unleashed with traditional methods designed for static corpora. In this chapter, we focus on Wikipedia as the most prominent instance of collaboratively constructed language resources. In particular, we discuss the significance of Wikipedia’s revision history for applications in Natural Language Processing (NLP) and the unique prospects of the user discussions, a new resource that has just begun to be mined. While the body of research on processing Wikipedia’s revision history is dominated by works that use the <b>revision</b> <b>data</b> as the basis for practical applications such as spelling correction or vandalism detection, most of the work focused on user discussions uses NLP for analyzing and understanding the data itself...|$|E
40|$|Forms {{are one of}} {{the most}} {{important}} transactions on the web. They enable the users to conduct transactions in a simple and efficient manner. It is the most efficient way of gathering and processing the information from the user. There are many examples of uses of Forms on the web: surveys, online trading, purchasing. Most of the forms are written in HTML which is or was our solution to web applications. Advancements in technology have replaced HTML by a more reliable and independent technology called XForms. They are considered to be the future of web applications. This project develops electronic forms in a framework; it lets the user design his/her own forms, edit them, merge any two forms and can be integrated into applications that store the data. In addition it can even maintain a log of <b>revision</b> <b>data,</b> i. e., a history of the revisions made to the forms – the date it was created or modified an...|$|E
40|$|Forecasts {{are only}} as good as the data behind them. But {{macroeconomic}} data are revised, often significantly, as time passes and new source data become available and conceptual changes are made. How is forecasting influenced by the fact that data are revised? To answer this question, we begin with the example of the index of leading economic indicators to illustrate the real-time data issues. Then we look at the data that have been developed for U. S. <b>data</b> <b>revisions,</b> called the "Real-Time Data Set for Macroeconomists" and show their basic features, illustrating the magnitude of the revisions and thus motivating their potential influence on forecasts and on forecasting models. The data set consists of a set of data vintages, where a data vintage refers to a date at which someone observes a time series of data; so the data vintage September 1974 refers to all the macroeconomic time series available to someone in September 1974. Next, we examine experiments using that data set by Stark and Croushore (2002), Journal of Macroeconomics 24, 507 - 531, to illustrate how the <b>data</b> <b>revisions</b> could have affected reasonable univariate forecasts. In doing so, we tackle the issues of what variables are used as "actuals" in evaluating forecasts and we examine the techniques of repeated observation forecasting, illustrate the differences in U. S. data of forecasting with real-time data as opposed to latest-available data, and examine the sensitivity to <b>data</b> <b>revisions</b> of model selection governed by various information criteria. Third, we look at the economic literature on the extent to which <b>data</b> <b>revisions</b> affect forecasts, including discussions of how forecasts differ when using first-available compared with latest-available data, whether these effects are bigger or smaller depending on whether a variable is being forecast in levels or growth rates, how much influence <b>data</b> <b>revisions</b> have on model selection and specification, and evidence on the predictive content of variables when subject to <b>revision.</b> Given that <b>data</b> are subject to <b>revision</b> and that <b>data</b> <b>revisions</b> influence forecasts, what should forecasters do? Optimally, forecasters should account for <b>data</b> <b>revisions</b> in developing their forecasting models. We examine various techniques for doing so, including state-space methods. The focus throughout this chapter is on papers mainly concerned with model development - trying to build a better forecasting model, especially by comparing forecasts from a new model to other models or to forecasts made in real time by private-sector or government forecasters. ...|$|R
40|$|Policy makers must {{base their}} {{decisions}} on preliminary and partially revised data of varying reliability. Realistic modeling of <b>data</b> <b>revisions</b> {{is required to}} guide decision makers in their assessment of current and future conditions. This paper provides a new framework with which to model <b>data</b> <b>revisions.</b> Recent empirical work suggests that measurement errors typically have much more complex dynamics than existing models of <b>data</b> <b>revisions</b> allow. This paper describes a state-space model that allows for richer dynamics in these measurement errors, including the noise, news and spillover effects documented in this literature. We also show how to relax the common assumption that "true" values are observed after a few revisions. The result is a unified and flexible framework that allows for more realistic <b>data</b> <b>revision</b> properties, and allows the use of standard methods for optimal real-time estimation of trends and cycles. We illustrate the application of this framework with real-time data on US real output growth. (C) 2011 Elsevier B. V. All rights reserved...|$|R
40|$|Many {{economic}} data series are revised as more comprehensive information becomes available and as methodologies improve. Even the latest available data {{are subject to}} uncertainty, {{and at some point}} historical data may be replaced by more accurately measured observations. Because monetary policy decisions are made with an eye to the state of the economy, data uncertainty complicates the evaluation and conduct of monetary policy.; Kozicki focuses on <b>revisions</b> to <b>data</b> that policymakers often examine when assessing monetary policy options. While other studies have looked at the impact of <b>data</b> <b>revisions</b> on monetary policy, this article is the first to examine the policy implications of revisions in two widely used benchmarks of resource utilization—the Congressional Budget Office (CBO) estimates of potential output and the natural rate of unemployment. The article is also the first to consider how <b>data</b> <b>revisions</b> affect policy decisions through changes in estimates of the equilibrium real rate of interest.; Kozicki finds that <b>revisions</b> to <b>data</b> can lead to policy regret—instances when revised data may suggest alternative actions would have been preferable to those taken. Based on this finding and analysis in other studies, she recommends making policy less sensitive to economic indicators that are subject to large revisions. Monetary policy...|$|R
40|$|This paper {{describes}} {{the development of}} standards {{for the use of}} classroom teaching and learning materials. The draft standards included in this paper have been developed by International Association for Research on Textbooks on Educational Media (IARTEM). The standards were developed in Australia but have been discussed in a number of international forums, including the 2013 IARTEM conference at the University of Ostrava (Czech Republic). At this conference the standards were shared for feedback, validation and further research refinement and <b>revision.</b> <b>Data</b> was collectedfrom IARTEM members about the standards and Professor Arno Reints from the University of Utrecht is leading an international IARTEM project to further develop the standards. Feedback on the standards can be made on the IARTEM website at any time − [URL] The paper is divided into a number of sections: A. Origin of the standards B. Process of standard development C. Purpose of the standards D. The standards continuum E. Development of the domains F. The domains G. The standards...|$|E
40|$|The {{purposes}} of this research are to validate the potential of superior natural resources and human resources in the tourism development of education based on Tri Hita Karana (THK) in Mengesta in Bali, to establish the centers of educational tourism, to form groups of community that supports the educational tourism based on THK, and to design media promotion. The method designed to achieve the targets are analysis, design, evaluation, and <b>revision.</b> <b>Data</b> were collected through observation, questionnaires, and interviews and analyzed using SWOT analysis and descriptive analysis. The results of this research show that there are seven learning resources that have been successfully validated, namely: dance and percussion forum, natural agricultural laboratory, Bali traditional house, agriculture training center, tracking, natural panorama and hot water bathing place, and culinary tour. The understanding and commitment of the people about educational tourism based on THK fall into good categories. Moreover, the media promotion has been successfully designed, which are {{in the form of}} art forum brochure, village tourism brochure, and a draft of guide book to educational tourism based on THK...|$|E
40|$|The {{focus of}} this study was on the effect of word {{processing}} {{on the quality of the}} composing process, product, and attitudes of adult academic ESL writers. Twenty adult ESL students, comprising an ‘intact’ EAP (English for Academic Purposes) group, completed a number of written assignments as part of their ESL unit, using either word processing or conventional ‘pen and paper’ composition methods. Their handwritten and word processed work was analysed and compared through the use of an holistic/analytic scale of writing quality. In addition to this analysis of the ‘finished product’, texts were analysed in terms of the frequency, nature and extent of revisions made within the composition process. Statistical analysis of the writing quality and <b>revision</b> <b>data</b> – as well as audio-taped verbal protocols from selected subjects, interviews, and observational notes, were used to determine the effect (s) of word processing on the composing process, product and attitudes of these subjects. The data indicate that word processing does improve writing quality – and that it also influences revising behaviours and subject attitudes towards writing. There does not appear, for these subjects, to have been any significant correlation between revision and writing quality...|$|E
40|$|Revisions to GDP {{announcements}} {{are known}} to be quite large in all G- 7 countries; quarterly growth rate revisions are regularly more than a full percentage point at an annualized rate. We examine the predictability of these revisions using standard statistical tests of whether the preliminary announcement is a rational forecast of the subsequently revised data. Previous work suggests that US GDP revisions are largely unpredictable, as would be the case if the revisions reflect news not available {{at the time that the}} preliminary number is produced. We find that the degree of predictability varies throughout the G- 7. Although we find little predictability in US <b>revisions,</b> the <b>data</b> <b>revisions</b> for several foreign countries are highly predictable. We also perform a simple real-time forecasting exercise showing that for several countries, the predictability of <b>data</b> <b>revisions</b> could be used to generate improved preliminary data...|$|R
40|$|Recent {{research}} examining U. S. macroeconomic <b>data</b> suggests that <b>revisions</b> {{may be much}} more important than traditionally assumed. This paper extends the analysis to Chinese data, where there has been substantial debate about data quality for some time. The key finding in this paper is that indeed the Chinese macroeconomic <b>data</b> <b>revisions</b> are not well-behaved, but that they are not much different from U. S. macroeconomic <b>data</b> <b>revisions...</b>|$|R
40|$|This paper {{proposes a}} set of good {{practices}} for the <b>revision</b> of macroeconomic <b>data.</b> The authors argue that revisions are a routine part of disseminating quality <b>data.</b> <b>Revisions</b> are made not just to correct errors but also to incorporate better source data, update base periods, and make other improvements. It is argued, using country examples and views from policymakers and other users, that national statistical agencies should have explicit revisions policies. ...|$|R
40|$|This paper {{presents}} a multilingual mobile learning framework {{that can be}} used to support the pedagogical development of mobile learning systems which can support learning in under-resourced multilingual schools. The framework has been developed following two empirical mobile learning studies. Both studies were conducted in multilingual South African high schools where learners were provided with mobile phones to access a multilingual system called M-Thuto. In the first study, 90 learners interacted with a bilingual application that included class notes, drill exercises and a quiz for their mathematics lessons. In the second study, 32 learners were expected to create audio clips of their notes gathered from their daily physical science lessons using their home languages. The learners uploaded these notes onto an online mobile learning system and used them later for <b>revision.</b> <b>Data</b> were gathered from their interactions in both studies through interviews and questionnaires and analysed through descriptive statistics and a thematic analysis. These results have contributed to a framework which prescribes factors that need to be considered when creating multilingual mobile learning systems that support the process of learning in developing country secondary schooling environments...|$|E
40|$|This {{research}} aims {{to design}} and develop the PISA math problems similar change in the content and relationship in the eighth grade. The model used in this research is the development research (research and development). which consists of two stages, there are two stages {{in this study is}} preliminary and formative stage of evaluation that includes self evaluation, expert reviews and one-to-one (low resistance to revision) and a small group and field test (hight resistance in <b>revision).</b> <b>Data</b> collection techniques used by the walkthrough, document analysis and tests. During through stage one-to-one to small group, validated by qualitative descriptive matter after it tested to the field test stage. Research subjects are students of class VIII E SMPN 1 Kerjo. The test results with the average value of reasoning ability for reproduction 59. 3 competencies included in both categories, with 44. 3 connections competence included in the category enough and to competence 35. 8 reflections included in the category enough. According the average score in each competency can be seen that the matter was appropriate level of competence. From these results {{it can be said that}} the PISA math problems similar in content and relationship change has potential effects on students' mathematical reasoning abilities...|$|E
40|$|A {{new method}} based upon age {{determined}} population ratios is described {{and used to}} estimate household population intensities (households per person). Using an additive and a bounded model household projections are given to 2050 for {{the world and to}} 2030 for seven fertility transition subgroups (cohorts) of the countries of the world. Based upon United Nations 2002 <b>Revision</b> <b>data,</b> from an estimated 1. 56 billion households at 2000, household growth to 2030 is projected to be an additional 1. 1 billion households, whether population increase is 1. 3 billion persons under the United Nations low fertility variant or 2. 7 billion persons under the high fertility variant. At that date over one third of all households are projected to be Chinese or Indian. By 2050 it is projected that there will be 3. 3 billion households with a 95 per cent confidence interval on modelling error only of ± 0. 5 billion. This compares with 3. 2 billion in the Habitat: Global Report on Human Settlements 1996. The apparent similarity of total household growth under various scenarios conceals a wide range in the growth of household intensities across fertility transition cohorts. It is suggested that models, projections and error be reviewed biennially and that household and population projections be produced jointly. Household projections, world, age ratios, fertility...|$|E
40|$|A {{modeling}} {{approach to}} real-time forecasting {{that allows for}} <b>data</b> <b>revisions</b> is shown. In this approach, an observed time series is decomposed into stochastic trend, <b>data</b> <b>revision,</b> and observation noise in real time. It is assumed that the stochastic trend is defined such that its first difference is specified as an AR model, and that the <b>data</b> <b>revision,</b> obtained only for the latest {{part of the time}} series, is also specified as an AR model. The proposed method is applicable to the data set with one vintage. Empirical applications to real-time forecasting of quarterly time series of US real GDP and its eight components are shown to illustrate the usefulness of the proposed approach. [*][*]Copyright Â© 2007 John Wiley & Sons, Ltd. ...|$|R
40|$|In {{the first}} chapter we {{document}} the empirical properties of revisions to major macroeconomic variables in the U. S., over the period 1966 – 2000. We find that these revisions {{do not have a}} zero mean, which indicates that the initial announcements by statistical agencies are biased. We also find that the revisions are quite large compared to the original variables. They are predictable using the information set {{at the time of the}} initial announcement, which means that the initial announcements of statistical agencies are not rational forecasts. We also provide some evidence that professional forecasters ignore this predictability. Our findings suggest that <b>data</b> <b>revisions</b> in the U. S. do not satisfy simple desirable statistical properties. ^ In the second chapter, using the empirical results from the previous chapter as the motivation, we study the effects of <b>data</b> <b>revisions</b> in a general equilibrium framework. We find that the presence of <b>data</b> <b>revisions,</b> or <b>data</b> uncertainty, creates a precautionary motive and causes significant changes in the decisions of agents. We also find that the model with revisions captures some aspects of the business cycle dynamics of the US data better than the benchmark model with no revisions. Using our model we measure the cost of having <b>data</b> <b>revisions</b> to be about $ 33 billion, $ 5 billion of which can be recovered by eliminating the predictability of revisions. Comparing these numbers with the budgets of the major statistical agencies in the US, we conclude that any money spent on the improvement of data collection would be well worth it. ^ In the third chapter we take the first step in analyzing ways of hedging the risk of <b>data</b> <b>revisions.</b> We show that we can construct portfolios of assets, which are maximally correlated with revisions to macroeconomic variables. The average correlation of the returns of these hedging portfolios and the underlying revision risk are in the order of 0. 50, which is very promising in terms of hedging performance. ...|$|R
40|$|Traditionally, {{researchers}} have used linear regression models when work-ing with real-time <b>data</b> <b>revisions.</b> In this paper, we forecast real-time <b>data</b> <b>revisions</b> {{with a wide}} set of models including linear, structural break and regime-switching models with and without heteroskedasticity. We address {{the issues raised by}} the presence of model uncertainty through the use of Bayesian model averaging (BMA). Using UK data, we calculate predictive densities for real-time <b>data</b> <b>revisions</b> which average over the model space and compare them with the single best model and the linear model. We also present evidence about whether the revision process is unbiased and calcu-late various functions of the predictive density which might be of interest to a statistical agency. In contrast to the BMA approach, the traditional linear model yields very misleading predictives...|$|R
