1197|123|Public
5000|$|... #Subtitle level 2: <b>Reweighting</b> {{procedure}} and numerical sign problem ...|$|E
50|$|Since the <b>reweighting</b> {{adds the}} same amount to the weight of every s-t path, a path is a {{shortest}} path in the original weighting {{if and only if}} it is a shortest path after <b>reweighting.</b> The weight of edges that belong to a shortest path from q to any node is zero, and therefore the lengths of the shortest paths from q to every node become zero in the reweighted graph; however, they still remain shortest paths. Therefore, there can be no negative edges: if edge uv had a negative weight after the <b>reweighting,</b> then the zero-length path from q to u together with this edge would form a negative-length path from q to v, contradicting the fact that all vertices have zero distance from q. The non-existence of negative edges ensures the optimality of the paths found by Dijkstra's algorithm. The distances in the original graph may be calculated from the distances calculated by Dijkstra's algorithm in the reweighted graph by reversing the <b>reweighting</b> transformation.|$|E
50|$|The Onsager-Machlup {{function}} can be {{used for}} purposes of <b>reweighting</b> and sampling trajectories,as well as for determining the most probable trajectory of a diffusion process.|$|E
5000|$|The {{objective}} is to <b>reweight</b> the source labeled sample such that it [...] "looks like" [...] the target sample (in term of the error measure considered) ...|$|R
30|$|As {{discussed}} in e.g. Busso et al. (2013), IPW <b>reweights</b> the comparison observations using the estimated propensity score to implicitly create a comparison sample {{with the same}} distribution of observed characteristics as the treatment group. IPW has two important advantages relative to the other estimators we employ: First, it just <b>reweights</b> the data using the (estimated) propensity score and so {{does not require the}} (often tiresome and sometimes problematic) choice of a bandwidth. Second, under certain assumptions, the inverse probability weighting estimator achieves the semi-parametric efficiency bound derived by Hahn (1998).|$|R
3000|$|... [...]. Hence, we {{only need}} to know ψ(z) and <b>reweight</b> the wage {{distribution}} for the stayers to obtain the counterfactual distribution of the wages that the return migrants would have obtained had they never migrated.|$|R
50|$|For random {{censoring}} on {{the response}} variables, the censored quantile regression of Portnoy (2003) provides consistent estimates of all identifiable quantile functions based on <b>reweighting</b> each censored point appropriately.|$|E
5000|$|... {{where the}} ti are weights, and the {{difference}} N1iR2 −N2iR1 {{can be seen as}} the difference between N1i and N2i after <b>reweighting</b> the rows to have the same total.|$|E
50|$|Suurballe's {{algorithm}} {{solves the}} same problem more quickly by <b>reweighting</b> {{the edges of the}} graph to avoid negative costs, allowing Dijkstra's algorithm to be used for both shortest path steps.|$|E
3000|$|... 1996) {{method and}} <b>reweight</b> the wage {{distribution}} of the non-migrant population in Mexico (stayers) such that the distribution of observable characteristics between return migrants and stayers is as similar as possible. 2 This is a similar empirical strategy employed by Coulon and Piracha ([...] [...]...|$|R
3000|$|... using (9), we draw {{particles}} from {{the product of}} Gaussian mixture and analytic messages. However, {{it is very difficult}} to draw {{particles from}} this product, so we use a proposal distribution, the sum of the Gaussian mixtures, and then <b>reweight</b> all samples. This procedure is well-known as mixture importance sampling [2].|$|R
40|$|We make a {{comparison}} of the parton distribution function data for different flavours with significant contribution (> 0. 5). We compare the mean, RMS and RMS by mean graphs for each, and notice a similarity in the pattern of RMS by mean graph. We then <b>reweight</b> it using some advanced techniques so that we can observe the propagation of errors...|$|R
50|$|A similar <b>reweighting</b> {{technique}} {{is also used}} in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.|$|E
5000|$|In {{systems with}} a {{moderate}} sign problem, such as field theories at a sufficiently high temperature or in a sufficiently small volume, the sign {{problem is not}} too severe and useful results {{can be obtained by}} various methods, such as more carefully tuned <b>reweighting,</b> analytic continuation from imaginary [...] to real , or Taylor expansion in powers of [...]|$|E
50|$|The neural code: Tse {{argues that}} {{thinking}} of the neural code as one where neural spikes trigger neural spikes, much like billiard balls triggering motion in other billiard balls, is misleading and incomplete. He argues that the neural code is in fact as much a synaptic <b>reweighting</b> (i.e. informational reparameterization) code {{as it is a}} code based on neural spikes or action potentials.|$|E
40|$|Three (3) {{different}} methods (logistic regression, covariate shift and k-NN) {{were applied to}} five (5) internal datasets and one (1) external, publically available dataset where covariate shift existed. In all cases, k-NN’s performance was inferior to either logistic regression or covariate shift. Surprisingly, there was no obvious advantage for using covariate shift to <b>reweight</b> the training data in the examined datasets...|$|R
40|$|This note {{reports about}} {{the way it is}} {{possible}} to <b>reweight</b> four-fermion (4 -f) events in order to account for the O(#) radiative corrections implemented in double pole approximation (DPA). The recommended choice for the DELPHI users is described and commented. This work is based on the new codes implementing DPA developed in the frame of the 2000 LEP 2 Montecarlo workshop...|$|R
40|$|Adds scalar {{multiplication}} to <b>reweight</b> filled aggregators. (Multiplying an aggregation tree by a scalar factor has {{the same}} effect as filling with weight * factor would have. In particular, multiplying by a non-positive number is equivalent to zero.) For ROOT filling, now only reading TBranches from the file {{that will be used}} (big I/O streamlining). Tests ROOT reading from TChains (no changes were required) ...|$|R
50|$|Dwelling {{prices are}} {{volatile}} and so, therefore, {{would be an}} index incorporating the current value of a dwelling price sub-index which, in some countries, would have a large weight under the third approach. Furthermore, the weight for owner-occupied dwellings could be altered considerably when <b>reweighting</b> was undertaken. (It could even become negative under the alternative cost approach if weights were estimated for a year during which house prices had been rising steeply).|$|E
5000|$|Bootstrapping can be {{interpreted}} in a Bayesian framework using a scheme that creates new datasets through <b>reweighting</b> the initial data. Given a set of [...] data points, the weighting assigned to data point [...] in a new dataset [...] is , where [...] is a low-to-high ordered list of [...] uniformly distributed random numbers on , preceded by 0 and succeeded by 1. The distributions of a parameter inferred from considering many such datasets [...] are then interpretable as posterior distributions on that parameter.|$|E
5000|$|In his 2012 book Connectome: How the Brain's Wiring Makes Us Who We Are, [...] Seung {{discusses}} {{his current}} views on neuroscience and the upcoming science of connectomics. The book expands {{on some of}} the concepts discussed in his Ted talk as well as discussing how the doctrine of the connectome can be tested. He states that in order to test and further our knowledge and unlock to potential of the connectome we must improve the scientific tools in existence. Also, he states {{that there needs to be}} new ways to promote the concept of the connectome using the four R’s: <b>reweighting,</b> reconnection, rewiring, and regeneration.|$|E
40|$|Empirical {{research}} shows that stock volatilities and correlations between markets rise more after negative shocks than after positive returns shocks of the same size. We measure {{the importance of these}} asymmetric effects for mean-variance investors holding portfolios of international equities who use dynamic conditional covariance forecasts to <b>reweight</b> their portfolios. Portfolio weights are computed using ex ante predictions from symmetric GARCH DCC and asymmetric GJR ADCC models, and a spectrum of expected returns. Data are weekly returns to equity price indices for the USA, Japan, UK and Australia. We find that the majority of realised portfolio standard deviations are less when we <b>reweight</b> using the asymmetric covariance model. Reductions in portfolio risk are significant according to Diebold-Mariano tests. Investors who are moderately risk averse and have longer rebalancing horizons benefit more from the asymmetric model than less risk averse, shorter-horizon investors, and would be prepared to pay up to 107 basis points annually to use it instead of the symmetric model. Benefits are greater for investors holding US equities. ...|$|R
30|$|The rest of {{the paper}} is {{organized}} as follows: we present the local regression transfer learning methods in Sect. 2; we then introduce the background of covariate shift and local learning, and propose some local transfer learning methods to <b>reweight</b> the training dataset and build the weighted risk regression model. We perform some experiments of psychological characteristics prediction and analyse the experiment results in Sect. 3. Finally, we conclude the whole work in the last section.|$|R
40|$|Abstract. We {{overview}} {{techniques for}} optimal geometric estimation from noisy observations for computer vision applications. We first describe estimation techniques based on minimization of given cost functions: least squares (LS), maximum likelihood (ML), which includes reprojection error minimization (Gold Standard) {{as a special}} case, and Sampson error minimization. We then formulate estimation techniques not based on minimization of any cost function: iterative <b>reweight,</b> renormalization, and hyper-renormalization. Showing numerical examples, we conclude that hyper-renormalization is robust to noise and currently is the best method. ...|$|R
50|$|The BCOM tracks {{prices of}} futures {{contracts}} on physical commodities on the commodity markets. The index {{is designed to}} minimize concentration in any one commodity or sector. It currently has 22 commodity futures in seven sectors. No one commodity can compose less than 2% or more than 15% of the index, and no sector can represent more than 33% of the index (as of the annual weightings of the components). The weightings for each commodity included in BCOM are calculated in accordance with rules that ensure that the relative proportion {{of each of the}} underlying individual commodities reflects its global economic significance and market liquidity. Annual rebalancing and <b>reweighting</b> ensure that diversity is maintained over time.|$|E
5000|$|By {{utilizing}} the kernel embedding of marginal and conditional distributions, practical approaches {{to deal with}} the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by <b>reweighting</b> examples via estimates of the ratio [...] obtained directly from the kernel embeddings of the marginal distributions of [...] in each domain without any need for explicit estimation of the distributions. [...] Target shift, which cannot be similarly dealt with since no samples from [...] are available in the test domain, is accounted for by weighting training examples using the vector [...] which solves the following optimization problem (where in practice, empirical approximations must be used) ...|$|E
5000|$|In 2005, Petrov, Dosher and Lu {{pointed out}} that perceptual {{learning}} may be {{explained in terms of}} the selection of which analyzers best perform the classification, even in simple discrimination tasks. They explain that the some part of the neural system responsible for particular decisions have specificity, while low-level perceptual units do not. In their model, encodings at the lowest level do not change. Rather, changes that occur in perceptual learning arise from changes in higher-level, abstract representations of the relevant stimuli. Because specificity can come from differentially selecting information, this [...] "selective <b>reweighting</b> theory" [...] allows for learning of complex, abstract representation. This corresponds to Gibson's earlier account of perceptual learning as selection and learning of distinguishing features. Selection may be the unifying principles of perceptual learning at all levels.|$|E
30|$|We {{did not use}} information-theoretic merit {{functions}} as were used by Lin et al. (2013) or Setlur and Stone (2016) because our input data (color-object association ratings; Fig.  4) are not probabilistic in nature. Although {{it might be possible}} to <b>reweight</b> our input data, interpret them as empirical probability distributions, and construct affinity scores similar to Lin et al. (2013) or Setlur and Stone (2016), we instead constructed deterministic metrics, which have similar qualitative properties but have a more natural interpretation in the context of our input data.|$|R
30|$|In {{the second}} part of our analysis, we take the {{distribution}} of full-time wage earners, but <b>reweight</b> their characteristics to replicate the distribution of observed characteristics for total employment, i.e., including part-time workers. This estimates the counterfactual wage distribution that would result if all employed workers worked full-time. Contrasting this distribution with the wage distribution among full-timers allows one to gauge to which extent part-timers represent a positive or negative selection compared to full-timers. We repeat our sequential analysis of adding different groups of covariates for the reweighted sample representing total employment.|$|R
30|$|The {{method to}} fuse the subclassifiers must be {{adjusted}} {{with a small}} sample set and {{the accuracy of the}} subclassifiers must be evaluated; therefore, the Bayes classifier is a good candidate as it builds a reference from prior probability to posterior probability. We can score the performance of the classifiers on the changed model by the posterior probability. With a small sample set, we can get the approximate posterior of the classifiers and use it to <b>reweight</b> the integrated classifier to fit the real model. A Bayes kernel was built based on this concept.|$|R
50|$|It is here, as {{in other}} related methods, that Monte Carlo enters {{the game in the}} guise of {{importance}} sampling: the large sum over auxiliary field configurations is performed by sampling over the most important ones, with a certain probability. In classical statistical physics, this probability is usually given by the (positive semi-definite) Boltzmann factor. Similar factors arise also in quantum field theories; however, these can have indefinite sign (especially in the case of Fermions) or even be complex-valued which precludes their direct interpretation as probabilities. In these cases, one has to resort to a <b>reweighting</b> procedure (i.e., interpret the absolute value as probability and multiply the sign or phase to the observable), to get a strictly positive reference distribution suitable for Monte Carlo sampling. However, it is well known that, in specific parameter ranges of the model under consideration, the oscillatory nature of the weight function can lead to a bad statistical convergence of the numerical integration procedure. The problem is known as the numerical sign problem and can be alleviated with analytical and numerical convergence acceleration procedures (Baeurle 2002, Baeurle 2003a).|$|E
40|$|We {{consider}} {{the problem of}} task <b>reweighting</b> in fair-scheduled multiprocessor systems wherein each task’s processor share is specified using a weight. When a task is reweighted, a new weight is computed for it that is then used in future scheduling. Task <b>reweighting</b> {{can be used as}} a means for consuming (or making available) spare processing capacity. The responsiveness of a <b>reweighting</b> scheme can be assessed by comparing its allocations to those of an ideal scheduler that can reweight tasks instantaneously. A <b>reweighting</b> scheme is fine-grained if any additional per-task “error ” (in comparison to an ideal allocation) caused by a <b>reweighting</b> event is constant. Similarly, a <b>reweighting</b> scheme is coarse-grained if the additional “error ” per <b>reweighting</b> event is non-constant. While fine-grained <b>reweighting</b> produces only a small amount of error when changing task weights, it has a worst-case time complexity of Θ(NlogN), where N is the number of tasks. If the number of tasks exceeds the number of processors, then such time complexity is larger than that of coarse-grained <b>reweighting,</b> which is Θ(MlogN), where M is the number of processors. In this paper, we construct two new <b>reweighting</b> algorithms that are hybrids of fine- and coarse-grained <b>reweighting</b> that have time complexity less than Θ(NlogN) and produce less error than current coarse-grained techniques. We also present an experimental evaluation of these scheme to compare their relative advantages...|$|E
30|$|In {{all tables}} and figures of this paper, MARS denotes the method with no {{transfer}} learning, KMM denotes combination of KMM <b>reweighting</b> method and MARS method in a weighted risk form, GkNN denotes global k-NN <b>reweighting</b> method and MARS, kNN denotes k-NN <b>reweighting</b> method and MARS, TTkNN denotes training-test k-NN <b>reweighting</b> method and MARS, and AkNN 1 denotes adaptive k-NN <b>reweighting</b> method and MARS, where k value is determined {{as described in}} Sect. 2.3. 3. AkNN 2 denotes completely adaptive k-NN <b>reweighting</b> method and MARS, where k value and γ value are both determined as described in Sect. 2.3. 3. Clust denotes clustering-based <b>reweighting</b> method and MARS. KMM, GkNN, kNN, TTkNN, AkNN 1 and Clust all showed the best results where their parameter values are assigned {{the best of a}} series of tried values. In all experiments, we use mean square error (MSE) for result comparisons.|$|E
40|$|We {{introduce}} a novel algorithm for decoding binary linear codes by linear programming (LP). We {{build on the}} LP decoding algorithm of Feldman and {{introduce a}} postprocessing step that solves a second linear program that <b>reweights</b> the objective function based {{on the outcome of}} the original LP decoder output. Our analysis shows that for some LDPC ensembles we can improve the provable threshold guarantees compared to standard LP decoding. We also show significant empirical performance gains for the reweighted LP decoding algorithm with very small additional computational complexity...|$|R
40|$|We derive {{the optimal}} {{estimates}} of the free energies of an arbitrary number of thermodynamic states from nonequilibrium work measurements; the work data are collected from forward and reverse switching processes and obey a fluctuation theorem. The maximum likelihood formulation properly <b>reweights</b> all pathways contributing to a free energy difference, and is directly applicable to simulations and experiments. We demonstrate dramatic gains in efficiency by combining the analysis with parallel tempering simulations for alchemical mutations of model amino acids. Comment: 4 pages; to appear in Phys Rev Let...|$|R
40|$|The {{concept of}} a "user lens" is introduced. The lens is a {{sequence}} of linear transformations used to <b>reweight</b> the vectors which represent documents or queries in information retrieval systems. It is trained automatically via relevance data provided by the user. Experiments verify the lens can improve performance on training data while not degrading test data performance, and that larger lenses result in nearly perfect performance on the training set. The lens provides a mechanism for automatically capturing long-term, user-specific information about an improved representation scheme for document vectors. ...|$|R
