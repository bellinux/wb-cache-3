0|707|Public
40|$|In {{developing}} optimal hedge {{ratios for}} the soybean processing margin, many authors have illustrated {{the importance of}} considering the interactions between the cash and futures prices for soybeans, soybean oil, and soybean meal. Conditional as well as time-varying hedge ratios have been examined, {{but in the case}} of multiproduct time-varying hedge ratios, the difficulty in estimation has been found to often outweigh any improvement in hedging effectiveness. This research examines the hedging effectiveness of the <b>Risk</b> <b>Metrics</b> procedure for estimating a time-varying covariance matrix for developing optimal hedge ratios for the soybean processing margin. The <b>Risk</b> <b>Metrics</b> method allows for a time-varying covariance matrix while being considerably easier to implement than multivariate GARCH (MGARCH) procedures. The <b>Risk</b> <b>Metrics</b> procedure has been advocated for use in developing Value-at-Risk estimates. While it provided considerable out-of-sample improvement in hedging effectiveness relative to a constant correlation MGARCH procedure, the <b>Risk</b> <b>Metrics</b> method provided only minimal improvement over a naive (1 -to- 1) hedging strategy. However, this research does illustrate the potential for the <b>Risk</b> <b>Metrics</b> methodology as a viable alternative to MGARCH procedures in a multiproduct hedging context. Marketing,...|$|R
5000|$|Capital gains distributed, some {{under the}} <b>discount</b> <b>method,</b> some under the {{indexation}} method.|$|R
30|$|In {{order to}} find the number of risks present in each state of a system the risk value is {{calculated}} using the <b>risk</b> <b>metrics.</b>|$|R
40|$|The many metrics {{employed}} {{for the evaluation}} of search engine results have not themselves been conclusively evaluated. We propose a new measure for a metric's ability to identify user preference of result lists. Using this measure, we evaluate the <b>metrics</b> <b>Discounted</b> Cumulated Gain, Mean Average Precision and classical precision, finding that the former performs best. We also show that considering more results for a given query can impair rather than improve a metric's ability to predict user preferences...|$|R
50|$|With only {{capital gains}} - the <b>discount</b> <b>method</b> is usually better (note {{indexation}} {{is better for}} small (perhaps only very small) gains). The choice is essentially between reducing the capital gain by the CPI rise of the cost base, or halving it outright. CPI indexation may be small, but if the proceeds are below it then there's no CGT. When the gain is above twice the indexation result, then the <b>discount</b> <b>method</b> is better.|$|R
30|$|Yield {{simulations}} {{based on}} the available time series of recorded yields at the county level (minimum 10  years), modeling of yield volatility within the county, simulation of yield reduction, and derivation of insurance <b>risk</b> <b>metrics.</b>|$|R
50|$|Credit Benchmark {{collects}} various <b>risk</b> <b>metrics,</b> including {{probability of}} default (PD) and loss given default (LGD) from contributing Internal Ratings-Based Approach (IRB) banks. The company pools this information to provide clients with consensus credit risk data and analytics for: sovereigns, corporates, banks and non-bank financial institutions.|$|R
40|$|Abstract – This paper {{gives an}} {{overview}} of common <b>risk</b> <b>metrics,</b> maps them into a decision context, and suggests a set of evaluation criteria. The motivation is that risk analysis is carried out to inform decision-making, and that the con-tent and perceived legitimacy of information from a risk analysis hinge on the metrics {{that are used to}} express the results. A tabular overview of the most common metrics for harm to people, assets, and the environment is given, with emphasis on clarifying their meaning, pros and cons. The metrics are linked to different decision contexts and stake-holder needs, and eleven criteria are established to aid in the choice thereof. Principal issues concerning the adequacy of single as well as sets of <b>risk</b> <b>metrics</b> are finally raised and make the basis for further work...|$|R
50|$|Climate {{forecasting}} {{can also}} be used to develop flood and drought <b>risk</b> <b>metrics</b> for companies with global supply chains. These metrics can help companies address challenges through new sourcing strategies, novel regional financial risk management products (e.g., cat bonds, index insurance), pre-emptive maintenance and mitigation, reservoir reallocation agreements and other tools.|$|R
50|$|Historically, most {{pension plan}} {{sponsors}} conducted comprehensive asset/liability studies {{every three to}} five years or after a significant change in demographics, plan design, funded status, sponsor circumstances, or funding legislation. Recent trends suggest more frequent studies, and/or a desire for regular tracking of key asset/liability <b>risk</b> <b>metrics</b> in between formal studies.|$|R
40|$|Abstract — In {{this paper}} {{we present a}} dynamic programing {{approach}} to stochastic optimal control problems with dynamic, time-consistent risk constraints. Constrained stochastic optimal control problems, which naturally arise when one has to consider multiple objectives, have been extensively investigated in the past 20 years; however, in most formulations, the constraints are formulated as either risk-neutral (i. e., by considering an expected cost), or by applying static, singleperiod <b>risk</b> <b>metrics</b> with limited attention to “time-consistency” (i. e., to whether such metrics ensure rational consistency of risk preferences across multiple periods). Recently, significant strides {{have been made in}} the development of a rigorous theory of dynamic, time-consistent <b>risk</b> <b>metrics</b> for multi-period (risk-sensitive) decision processes; however, their integration within constrained stochastic optimal control problems has received little attention. The goal of this paper is to bridge this gap. First, we formulate the stochastic optimal control problem with dynamic, time-consistent risk constraints and we characterize the tail subproblems (which requires the addition of a Markovian structure to the <b>risk</b> <b>metrics).</b> Second, we develop a dynamic programming approach for its solution, which allows to compute the optimal costs by value iteration. Finally, we discuss both theoretical and practical features of our approach, such as generalizations, construction of optimal control policies, and computational aspects. A simple, two-state example is given to illustrate the problem setup and the solution approach. I...|$|R
40|$|Endowing robots {{with the}} {{capability}} of assessing risk and making risk-aware decisions is widely considered a key step toward ensuring safety for robots operating under uncertainty. But, how should a robot quantify risk? A natural and common approach is to consider the framework whereby costs are assigned to stochastic outcomes - an assignment captured by a cost random variable. Quantifying risk then corresponds to evaluating a risk metric, i. e., a mapping from the cost random variable to a real number. Yet, {{the question of what}} constitutes a "good" risk metric has received little attention within the robotics community. The goal {{of this paper is to}} explore and partially address this question by advocating axioms that <b>risk</b> <b>metrics</b> in robotics applications should satisfy in order to be employed as rational assessments of risk. We discuss general representation theorems that precisely characterize the class of metrics that satisfy these axioms (referred to as distortion <b>risk</b> <b>metrics),</b> and provide instantiations that can be used in applications. We further discuss pitfalls of commonly used <b>risk</b> <b>metrics</b> in robotics, and discuss additional properties that one must consider in sequential decision making tasks. Our hope is that the ideas presented here will lead to a foundational framework for quantifying risk (and hence safety) in robotics applications. Comment: Extended version of paper published in International Symposium on Robotics Research (ISRR) 201...|$|R
40|$|The Dutch {{government}} {{is in the process}} of revising its flood safety policy. The current safety standards for flood defences in the Netherlands are largely based on the outcomes of cost-benefit analyses. Loss of life has not been considered separately in the choice for current standards. This article presents the results of a research project that evaluated the potential roles of two <b>risk</b> <b>metrics,</b> individual and societal risk, to support decision-making about new flood safety standards. These <b>risk</b> <b>metrics</b> are already used in the Dutch major hazards policy for the evaluation of risks to the public. Individual risk concerns the annual probability of death of a person. Societal risk concerns the probability of an event with many fatalities. Technical aspects of the use of individual and societal <b>risk</b> <b>metrics</b> in flood <b>risk</b> assessments as well as policy implications are discussed. Preliminary estimates of nationwide levels of societal risk are presented. Societal risk levels appear relatively high in the South Western part of the country where densely populated dike rings are threatened by a combination of river and coastal floods. It was found that cumulation, the simultaneous flooding of multiple dike rings during a single flood event, has significant impact on the national level of societal risk. Options for the application of the individual and societal risk in the new flood safety policy are presented and discussed. Hydraulic EngineeringCivil Engineering and Geoscience...|$|R
40|$|PT. Aceh Media Grafika the {{printing}} industry {{has focused on}} the publication of any kind of perishable products are daily newspaper Serambi Indonesia. In doing so, production, printing section to do so based on the number set by the circulation section. During the determination of the amount of production by its own circulation section is based on experience and intuition. So that they experience the overstocks and shortages of the production of the newspaper. Attempts to get the optimal production quantity on the days production is done with the stochastic approach is determining the quantity of a single booking through Newsboy Dual Performance Measure and Quantity <b>Discounts</b> <b>Method.</b> Based on calculations and analysis using Newsboy Dual Performance Measure and Quantity <b>Discounts</b> <b>Method</b> obtained that, the optimal amount of daily newspaper production retail foyer Indonesia during the month of april 2016 is 11. 794 copies. This amount resulted in the number of newspapers has decreased the number of returns reached 92 % of their previous condition and the profit rate increased to Rp 7, 360, 124. Keywords: Total production, Perishable, Stochastic, newsboy Dual Performance Measures & Quantity <b>Discount</b> <b>Methods.</b> Banda Ace...|$|R
30|$|In {{order to}} show the {{trade-offs}} and synergies between different DRR measures, a case study {{was carried out in}} Shenzhen City regarding typhoon disaster risk reduction. Using a typhoon risk model, cost-effectiveness of various investment scenarios was calculated based on quantitative <b>risk</b> <b>metrics.</b> Policy recommendations were then derived according to the CBA results.|$|R
40|$|In {{this paper}} {{we present a}} {{framework}} for risk-averse model predictive control (MPC) of linear systems affected by multiplicative uncertainty. Our key innovation is to consider time-consistent, dynamic <b>risk</b> <b>metrics</b> as objective functions to be minimized. This framework is axiomatically justified in terms of time-consistency of risk assessments, is amenable to dynamic optimization, and is unifying {{in the sense that}} it captures a full range of risk preferences from risk-neutral to worst case. Within this framework, we propose and analyze an online risk-averse MPC algorithm that is provably stabilizing. Furthermore, by exploiting the dual representation of time-consistent, dynamic <b>risk</b> <b>metrics,</b> we cast the computation of the MPC control law as a convex optimization problem amenable to real-time implementation. Simulation results are presented and discussed. Comment: Submitted to IEEE Transactions on Automatic Control. arXiv admin note: text overlap with arXiv: 1511. 0698...|$|R
40|$|Usability {{inspection}} methods (UIMs) {{remain an}} important <b>discount</b> <b>method</b> for usability evaluation. They {{can be applied}} to any designed artefact during development: a paper prototype, a storyboard, a working prototype (e. g., in Macromedia Flash™ or in Microsoft PowerPoint™), tested production software, or an installed public release. They are analytical evaluation methods, which involve no typical end users, unlike empirical methods such as user testing. UIMs only require availability of a designed artefact and trained analysts. Thus, evaluation is possible with low resources (hence <b>discount</b> <b>methods).</b> Although risks arise from low resources, well-informed practices disproportionately improve analyst performance, improving cost-benefit ratios. This chapter introduces UIMs, covering six and one further method, and provides approaches to assessing existing, emerging and future UIMs and their effective uses...|$|R
40|$|In {{this paper}} {{we present a}} dynamic programing {{approach}} to stochastic optimal control problems with dynamic, time-consistent risk constraints. Constrained stochastic optimal control problems, which naturally arise when one has to consider multiple objectives, have been extensively investigated in the past 20 years, however, in most formulations, the constraints are formulated as either risk-neutral (i. e., by considering an expected cost), or by applying static, single-period <b>risk</b> <b>metrics</b> with limited attention to "time-consistency" (i. e., to whether such metrics ensure rational consistency of risk preferences across multiple periods). Recently, significant strides {{have been made in}} the development of a rigorous theory of dynamic, time-consistent <b>risk</b> <b>metrics</b> for multi-period (risk-sensitive) decision processes, however, their integration within constrained stochastic optimal control problems has received little attention. The goal of this paper is to bridge this gap. First, we formulate the stochastic optimal control problem with dynamic, time-consistent risk constraints and we characterize the tail subproblems (which requires the addition of a Markovian structure to the <b>risk</b> <b>metrics).</b> Second, we develop a dynamic programming approach for its solution, which allows to compute the optimal costs by value iteration. Finally, we discuss both theoretical and practical features of our approach, such as generalizations, construction of optimal control policies, and computational aspects. A simple, two-state example is given to illustrate the problem setup and the solution approach. Comment: arXiv admin note: text overlap with arXiv: 1501. 02024, arXiv: 1503. 0746...|$|R
5000|$|For assets {{acquired}} between 20 September 1985 and 20 September 1999 {{the taxpayer}} may choose between two methods of calculating a capital gain - the <b>discount</b> <b>method</b> described above, or the indexation method - whichever method {{results in the}} least tax. The indexation method is as follows: ...|$|R
40|$|Long term {{evaluations}} {{of natural resources}} and especially natural assets, when they are progressively fading out because of natural or anthropic reasons, are crucial issues for sustainable development and sustainability assessments in general. The usual economic way of dealing with long-term values is to use <b>discounting</b> <b>methods.</b> A...|$|R
50|$|In Europe, in {{the wake}} of the 2008-2009 {{financial}} crisis, some pension experts such as Anton van Nunen have argued that excessive or misplaced regulatory activism can sometimes have negative unintended consequences, notably when it comes to the strict enforcement of asset liability matching in times high market volatility and the systematic use of bonds-based <b>risk</b> <b>metrics</b> across all asset classes.|$|R
40|$|Optimization under {{uncertainty}} {{has attracted}} recently an increasing {{interest in the}} process systems engineering literature. The inclusion of uncertainties in an optimization problem inevitably leads {{to the need to}} manage the associated risk in order to control the variability of the objective function in the uncertain parameters space. So far, risk management methods have focused on optimizing a single risk metric along with the expected performance. In this work we propose an alternative approach that can handle several <b>risk</b> <b>metrics</b> simultaneously. First, a multi-objective stochastic model containing a set of <b>risk</b> <b>metrics</b> is formulated. This model is then solved efficiently using a tailored decomposition strategy inspired on the Sample Average Approximation. After a normalization step, the resulting solutions are assessed using Pareto filters, which identify solutions showing better performance in the uncertain parameters space. The capabilities and benefits of our approach are illustrated through a design and planning supply chain case studyPeer ReviewedPostprint (author's final draft...|$|R
40|$|The {{abundance}} of <b>risk</b> <b>metrics</b> {{stems from the}} effort to measure {{the difference between the}} expected and actual returns, under a hypothesis of normality. Under the assumption of risk aversion, investors are likely to quantify <b>risk</b> using <b>metrics</b> which measure returns lower than the expected average. These include the semi-variance of returns smaller than the average, the risk of loss – a return under a chosen level, usually 0 %, and value-at-risk, for the greatest losses, with a probability of less than 1 - 5 % in a given period of time. The Basel II accord improves on the way risks are measured, by allowing banks greater flexibility. There is an increase in the complexity of measuring credit risks, the market risks measurement methods remain the same, and the measurement of operational risk is introduced for the first time. The most advanced (and widely-used) <b>risk</b> <b>metrics</b> are based on VaR. However, {{it must be noted that}} VaR calculations are statistical, and therefore unlikely to forecast extraordinary events. So the quality of a VaR calculation must be checked using back-testing, and if the VaR value fails in a percentage of 1 - 5 % of the cases, then the premises of the model must be changed...|$|R
40|$|Evaluation of ADP {{systems in}} {{medicine}} frequently becomes mired in problems of tenuous cost measurement, of proving illusory cost savings, of false precision, and of dubious <b>discounting</b> <b>methods,</b> while giving only superficial treatment to non-dollar benefits. It would frequently be more advantageous to study non-dollar impacts with greater care and rigor...|$|R
40|$|To date {{comparative}} studies have not convinced the HCI community about {{the reliability of}} results from usability evaluations. User testing of the same products by different usability teams found different problems (Molich et al., 1999). <b>Discount</b> <b>methods</b> also have the same problem. With heuristic evaluation method, for example, overlaps between evaluators...|$|R
40|$|This report {{summarizes}} {{findings and}} {{results of the}} Quantifiably Secure Power Grid Operation, Management, and Evolution LDRD. The focus of the LDRD was to develop decisionsupport technologies to enable rational and quantifiable risk management for two key grid operational timescales: scheduling (day-ahead) and planning (month-to-year-ahead). <b>Risk</b> or resiliency <b>metrics</b> are foundational in this effort. The 2003 Northeast Blackout investigative report stressed the criticality of enforceable metrics for system resiliency - the grid's ability to satisfy demands subject to perturbation. However, we neither have well-defined <b>risk</b> <b>metrics</b> for addressing the pervasive uncertainties in a renewable energy era, nor decision-support tools for their enforcement, which severely impacts efforts to rationally improve grid security. For day-ahead unit commitment, decision-support tools must account for topological security constraints, loss-of-load (economic) costs, and supply and demand variability - especially given high renewables penetration. For long-term planning, transmission and generation expansion must ensure realized demand is satisfied for various projected technological, climate, and growth scenarios. The decision-support tools investigated in this project paid particular attention to tailoriented <b>risk</b> <b>metrics</b> for explicitly addressing high-consequence events. Historically, decisionsupport tools for the grid consider expected cost minimization, largely ignoring risk and instead penalizing loss-of-load through artificial parameters. The technical focus of this work was the development of scalable solvers for enforcing <b>risk</b> <b>metrics.</b> Advanced stochastic programming solvers were developed to address generation and transmission expansion and unit commitment, minimizing cost subject to pre-specified risk thresholds. Particular {{attention was paid to}} renewables where security critically depends on production and demand prediction accuracy. To address this concern, powerful filtering techniques for spatio-temporal measurement assimilation were used to develop short-term predictive stochastic models. To achieve uncertaintytolerant solutions, very large numbers of scenarios must be simultaneously considered. One focus of this work was investigating ways of reasonably reducing this number...|$|R
30|$|We {{explore their}} {{warranty}} model assuming exponential auction listing and claims times, under competing assumptions of Normal (classical portfolio theory) versus Paretian claims (industry practice) distributions using a normative algorithm. EBay’s model viability was analyzed under three <b>risk</b> <b>metrics</b> – mean-variance <b>risk</b> of classic portfolio theory; value at risk used in regulation {{such as the}} Basel Accords; and tail value at risk which is preferred by academics.|$|R
40|$|Masteroppgave i økonomi og administrasjon - Universitetet i Agder 2008 The {{exponential}} <b>discounting</b> <b>method</b> {{is today}} the most used in economics, {{and it has}} been the dominant <b>discounting</b> <b>method</b> for many years. However there are many economists that have started to question this method. One of the reasons for this {{is that it does not}} capture the way we humans tend to behave in many types of discounting situations. In this assignment I will introduce the hyperbolic <b>discounting</b> <b>method.</b> This is a more flexible discounting function that opens for human irrationality and preference reversals. The brain holds the answer too many of the questions about human behaviour. Over the last years a field called neuroeconomcs have captured the attention of many economic researchers. Neuroeconmics opens the door for neuroscience into the economic theory. Although this is a young field in economics it has produced a lot of interesting discoveries. By using tools from neuroscience we are able to understand the physiological process that finds place when we are facing different kinds of economic decisions, like discounting decisions. I start my theses by giving an introduction to this research, and present some of the discoveries that have been done. I will then use the knowledge form neuroeconomics to see if it can enlighten the discounting theory. Then I will present a framework I have made, this is a tool to analyze some of the decision we makes. At the end of my assignment I will present a pilot of an experiment that I have performed. This is a discounting experiment to se what <b>discounting</b> <b>method</b> that fits the subjects in the experiment best. However I have just done a pilot for this experiment so the number of subjects that were participating is not enough to make any certain conclusions, but I will present a analyze of the material I got from the experiment. I will also give a run trough of the process of performing the pilot...|$|R
40|$|The {{exponential}} <b>discounting</b> <b>method</b> {{is today}} the most used in economics, {{and it has}} been the dominant <b>discounting</b> <b>method</b> for many years. However there are many economists that have started to question this method. One of the reasons for this {{is that it does not}} capture the way we humans tend to behave in many types of discounting situations. In this assignment I will introduce the hyperbolic <b>discounting</b> <b>method.</b> This is a more flexible discounting function that opens for human irrationality and preference reversals. The brain holds the answer too many of the questions about human behaviour. Over the last years a field called neuroeconomcs have captured the attention of many economic researchers. Neuroeconmics opens the door for neuroscience into the economic theory. Although this is a young field in economics it has produced a lot of interesting discoveries. By using tools from neuroscience we are able to understand the physiological process that finds place when we are facing different kinds of economic decisions, like discounting decisions. I start my theses by giving an introduction to this research, and present some of the discoveries that have been done. I will then use the knowledge form neuroeconomics to see if it can enlighten the discounting theory. Then I will present a framework I have made, this is a tool to analyze some of the decision we makes. At the end of my assignment I will present a pilot of an experiment that I have performed. This is a discounting experiment to se what <b>discounting</b> <b>method</b> that fits the subjects in the experiment best. However I have just done a pilot for this experiment so the number of subjects that were participating is not enough to make any certain conclusions, but I will present a analyze of the material I got from the experiment. I will also give a run trough of the process of performing the pilot...|$|R
40|$|This final {{thesis is}} focused on {{evaluation}} of company Toyota Peugeot Citroën Automobile Czech, s. r. o. There were used several evaluation methods, among which belong cash-flow <b>discount</b> <b>method</b> (FCFE and FCFF) and economic value added <b>discount</b> <b>method</b> (EVA). The final thesis is composed of two main parts. The first theoretical part provides general insight into evaluation methodology. For example what company is, why evaluation should be performed, what evaluation categories exist etc. The second part is dedicated to practical use of evaluation knowledge. This includes strategic analysis, creation of value generators (i. e. sales, profit margin, working capital, and long-term assets value generators). From four of these value generators, company financial plan for years 2012 - 2016 has been made. Financial plan is then used as source for final evaluation. The evaluation of Toyota Peugeot Citroën Automobile Czech, s. r. o. at 30 April 2013 equals to 24, 483 million CZK...|$|R
50|$|The {{opportunity}} to earn discounts {{in exchange for}} early payment in business-to-business commerce has been limited historically by {{the length of time}} necessary for accounts payable's to receive and approve paper invoices. An invoice that takes 20 days to be approved, for example, cannot be paid in time to qualify for a discount available from a supplier for payment on day 10. With the advent of electronic invoicing and Purchase-to-Pay (P2P) automation enabled by the Internet, buying organizations are increasingly able to approve invoices faster and take advantage of available <b>discounts.</b> Dynamic <b>discounting</b> <b>methods</b> were first invented and patented by Xign Corporation in the early 2000s to help businesses take advantage of these trends and establish early payment terms with suppliers. Since then software enabling dynamic discounts has become a common feature of procure-to-pay automation products. More recently, dynamic <b>discount</b> <b>methods</b> are being implemented via auction sites that enable buyers and suppliers to negotiate payment terms and discounts across large amounts of their spend.|$|R
40|$|This paper {{focuses on}} robust {{location}} strategies for {{a fleet of}} ambulances in cities {{in order to maximize}} service levels under unexpected demand patterns. Our work is motivated by the fact that when small parts of networks incur emergencies according to a heavy-tailed distribution, the structure of the network under resource constraints results in the entire system behaving in a heavy-tailed manner. To address this, metrics other than average-case need to be used. We achieve robust location strategies by including <b>risk</b> <b>metrics</b> that account for tail behavior and not average performance alone. Because of the exponentially large solution space for locating K ambulances in N locations on the network, our approach is based on an efficient algorithm that allows for optimizing based on these <b>risk</b> <b>metrics.</b> We show that optimizing based on risk measures can account for spatiotemporal patterns and prevent the extent of delay cascades that are typically seen in heavy-tailed arrival distributions. From our computational results based on data from a large Asian city, we show that planning with some robustness metrics as targets leads to solutions that perform well in heavy-tailed demand scenarios...|$|R
40|$|Analytical, free of time {{consuming}} Monte Carlo simulations, framework for credit portfolio systematic <b>risk</b> <b>metrics</b> calculations is presented. Techniques are described that allow calculation of portfolio-level systematic risk measures (standard deviation, VaR and Expected Shortfall) {{as well as}} allocation of risk down to individual transactions. The underlying model is the industry standard multi-factor Merton-type model with arbitrary valuation function at horizon (in contrast to the simplistic default-only case). High accuracy of the proposed analytical technique is demonstrated by benchmarking against Monte Carlo simulations. ...|$|R
40|$|Key words: railway location; {{environmental}} value; economical losing {{of environment}} impacts; application research Abstract. Based on the protection cost method, shadow engineering method, {{and some other}} <b>discounting</b> <b>methods,</b> the economic losses caused by the environmental damage was equivalent to specific monetary value. And put these methods into the environmental evaluation problems of Xibao railway line construction. After application, it {{plays an important role}} in the multi-objective railway location decision-making...|$|R
40|$|The {{study was}} carried out to {{estimate}} classical swine fever (CSF) outbreak-related outcomes such as epidemic duration and number of infected, vaccinated, and depopulated premises, using defined most likely CSF outbreak scenarios. <b>Risk</b> <b>metrics</b> were established using empirical data to select the most likely CSF outbreak scenarios in Indiana. The scenarios were simulated using a stochastic between-premises disease spread model to estimate outbreak-related outcomes. A total of 19 single-site (i. e., with a single-index premises {{at the onset of}} an outbreak) and 15 multiple-site (i. e., with more than one index premises at the onset of an outbreak) outbreak scenarios of CSF were selected using the <b>risk</b> <b>metrics.</b> The number of index premises in the multiple-site outbreak scenarios ranged from 4 to 32. The multiple-site outbreak scenarios were further classified into clustered (N= 6) and non-clustered (N= 9) groups. The estimated median (5 th, 95 th percentiles) epidemic duration (days) was 224 (24, 343) in the single-site and was 190 (157, 251) and 210 (167, 302) in the clustered and non-clustered multiple-site outbreak scenarios, respectively. The median (5 th, 95 th percentiles) number of infected premises was 323 (0, 488) in the single-site outbreak scenarios and was 529 (395, 662) and 465 (295, 640) in the clustered and non-clustered multiple-site outbreak scenarios, respectively. Both the number and spatial distribution of the index premises affected the outcome estimates. The results also showed the importance of implementing vaccinations to accommodate depopulation in the CSF outbreak controls. The use of routinely collected surveillance data in the <b>risk</b> <b>metrics</b> and disease spread model allows end users to generate timely outbreak-related information based on the initial outbreak’s characteristics. Swine producers can use this information to make an informed decision on management of swine operations and continuity of business so that potential losses could be minimized during a CSF outbreak. Government authorities might use the information to make emergency preparedness plans for CSF outbreak control...|$|R
30|$|Long-term {{simulation}} (1000  years) {{of weather}} over China, {{development of a}} weather crop index (WCI) that should correlate well with observed recorded yields, simulation of the WCI, development of correlations between the observed WCI and the historical yields, simulation of crop yield reduction, derivation of <b>risk</b> <b>metrics</b> (AEP—aggregate exceeding probability; AAL—average annual loss; St. Dev.—standard deviation of the annual loss). In China most of the crop losses are attributed to drought or floods, and {{the focus of the}} simulation was on drought (or insufficient rainfall).|$|R
