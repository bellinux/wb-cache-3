2|10000|Public
40|$|Guided wave {{technology}} using Electromagnetic Acoustic Transducer has {{the advantages}} of withstand high sensitivity, low attenuation, quickly and efficiently detection etc. To effectively detect the defects, {{it is necessary to}} study the propagation characteristics of guided wave. In this paper, the dispersion and multimode characteristics of guided waves are studied by the disperse simulation software, and the variation <b>rule</b> <b>of</b> <b>propagation</b> is analyzed by the geometric parameters of plate and pipe. The results show that the dispersion characteristics of guided wave are depended on the material, the thickness and inner diameter, and it is better at lower frequencies and smaller thickness. This is helpful to the selection of excitation mode, operating frequency and transducer structure parameter. ...|$|E
40|$|Key words: point correspondence, epipolar constraint, {{computer}} vision system Abstract. Some projective points (2 D-points) of the same 3 D-point captured by different cameras are called corresponding points. The task of finding the corresponding points is named as point correspondence. It is a essential problem in {{computer vision}}, and it paves the way for 3 D reconstruction. Epipolar constraint is a widely-used geometric constraint for point correspondence, some methods based on it have been proposed for this task. However, the threshold is set by empirical approach in these methods; this approach {{is influenced by the}} parameters of cameras. On the other hand, epipolar constraint is not a sufficient condition for this task; this will result in the mismatch for two 2 D-points. According to the <b>rule</b> <b>of</b> <b>Propagation</b> Uncertainty and practical situation of the {{computer vision system}}, this paper introduces the enhanced epipolar constraint comprising normalized coplanar constraint, nonparallel constraint and interval constraint. The normalized coplanar constraint provides a simple and rational method to set the threshold of epipolar constraint; the nonparallel constraint and interval constraint can reduce the occasion of mismatch...|$|E
3000|$|Calculate {{the mode}} matrix M {{according}} to the <b>rule</b> <b>of</b> neighbor <b>propagation</b> and the neighbor relation matrix T. The <b>rule</b> <b>of</b> neighbor <b>propagation</b> can be described as, if [...]...|$|R
40|$|An {{alternative}} {{implementation of}} the concept of fuzzy granule is proposed. The granules of a universe can be modeled by an array of matrices that contains the different relations between them. The most important relation is the degree of overlap of a granule with other granules. Initially this method requires the granules to take on values in a restricted vocabulary. This requirement is relaxed by dealing with linguistic hedges within the framework. An extension to accommodate a change in granulation resolution is also provided. The formal definition of these constraints over fuzzy granules and their <b>rules</b> <b>of</b> <b>propagation</b> and inference are also presented in the paper. 1...|$|R
50|$|Thereafter, the dynastic <b>rule</b> <b>of</b> Chogyals, <b>propagation</b> <b>of</b> the Buddhist {{religion}} and building of monasteries and chortens took firm roots in Sikkim. The Namgyal monarchy of 12 kings lasted from 1642 till 1975 (333 years). Tibetan Mahayana Buddhism known as Vajrayana sect was introduced, which ultimately {{was recognized as}} the state religion of Sikkim.|$|R
40|$|AbstractÐA new {{approach}} to stochastic simulation of fracture networks has been developed and the algorithm is presented here. The simulation procedure mimics fracture propagation. Fractures are grown following geomechanical <b>rules</b> <b>of</b> <b>propagation</b> inferred for each speci®c host geological environment. The algorithm developed is a hybrid pixel/object approach where fractures are constructed as objects which are themselves sets of connected pixels rather than parametric objects. Working on a pixel grid gives ¯exibility for modeling and the fractures {{are not required to}} take any pre-speci®ed shape. In addition, an incremental growth of the fractures, pixel by pixel, allows generating curvilinear or undulating fractures. It also allows the fractures to be stopped in their propagation e. g. when they encounte...|$|R
30|$|At this point, {{a finite}} element model {{including}} in situ stress, rock mechanical properties and other factors is established in ABAQUS based on fluid–solid coupling. Also, the fracture initiation judged by MAXPS criterion and fracture propagation judged by MERR criterion are confirmed. The study <b>of</b> the <b>rules</b> <b>of</b> fracture <b>propagation</b> in radial well fracturing is done objectively and intuitively with the model.|$|R
5000|$|The second <b>rule</b> <b>of</b> unit <b>propagation</b> {{can be seen}} as a {{restricted}} form of resolution, in which one of the two resolvents must always be a unit clause. As for resolution, unit propagation is a correct inference rule, in that it never produces a new clause that was not entailed by the old ones. The differences between unit propagation and resolution are: ...|$|R
40|$|CCP (CLIPS Constraint Programming) is {{a generic}} {{knowledge}} base which enables CLIPS to solve CSPs (Constraint Satisfaction Problems). CCP {{is composed of}} three modules. The MAIN module contains the top level loop including the I/O rules. The SEARCH module implements a classical Chronological Backtrack algorithm. The PROPAGATION module implements the Forward Checking algorithm. CCP has been extensively tested with well known puzzles as N-queens, graph colouring, crosswords, etc. To implement any particular problem, the users only need to declare each variable with its domain {{and they have to}} define specific domain description and <b>rules</b> <b>of</b> <b>propagation.</b> So that all specific knowledge is encapsulated in the PROPAGATION module. CCP is a generic tool that can be embedded in a knowledge based system like a work horse for local CSPs solving. CCP is compact, since it contains only a dozen rules. Moreover, CCP {{can be seen as a}} pedagogical tool illustrating many advanced features of CLIPS (logical [...] ...|$|R
40|$|In {{this series}} of papers {{we set out to}} generalize the notion of {{classical}} analytic deduction (i. e. deduction via elimination rules) by combining the methodology of Labelled Deductive Systems [Gab 94] with the classical system KE [DM 94]. LDS is a unifying framework for the study of logics and of their interactions. In the LDS approach the basic units of logical derivation are not just formulae but labelled formulae, where the labels belong to a given "labelling algebra". The derivation rules act on the labels {{as well as on the}} formulae, according to certain fixed <b>rules</b> <b>of</b> <b>propagation.</b> By virtue <b>of</b> the extra power of the labelling algebras, standard (classical or intuitionistic) proof systems can be extended to cover a much wider territory without modifying their structure. The system KE is a new tree method for classical analytic deduction based on "analytic cut". It is a refutation system, like analytic tableaux and resolution, but it is essentially more efficient than tableaux and, un [...] ...|$|R
40|$|Proceeding of: 2009 IEEE Nuclear Science Symposium Conference Record (NSS/MIC), Orlando, Florida, 25 - 31 October 2009 Monte Carlo methods {{provide a}} {{flexible}} and rigorous {{solution to the}} problem of light transport in turbid media, which enable approaching complex geometries for a closed analytical solution is not feasible. The simulator implements local <b>rules</b> <b>of</b> <b>propagation</b> in the form of probability density functions that depend on the local optical properties of the tissue. This work presents a flexible simulator that can be applied in multiple applications related to optical tomography. In particular, unlike previous codes, the simulator explicitly supports fluorescent-tissues and variance reduction and code parallelization techniques are implemented in order to speed up the execution with fluorochrome-labelled agents. The simulator is validated with simple geometries for which an analytical solution exists, as well as with an experimental polyester resin based optical phantom. This work was supported in part by the Ministry of Science and Innovation under projects TEC 2008 - 06715 and TEC 2007 - 64731 /TCM and by the EU’s 7 th Frame Programme under contract HEALTH-F 5 - 2008 - 20179...|$|R
40|$|In {{this paper}} {{we present a}} method for {{determining}} satisfiability of formulae represented by Boolean Expression Diagrams. The method uses the Up One algorithm for splitting on variables and rewriting <b>rules</b> instead <b>of</b> unit <b>propagation.</b> We show how to combine the method with BDD construction. In this way our method {{can be seen as}} bridging the gap between standard SAT-solvers and BDD construction. ...|$|R
40|$|The Gaussian {{theory of}} errors has been {{generalized}} to situations, where the Gaussian distribution and, hence, the Gaussian <b>rules</b> <b>of</b> error <b>propagation</b> are inadequate. The generalizations are based on Bayes' theorem and a suitable measure. The following text sketches some chapters of a monograph that is presently prepared. We concentrate on the material that is - {{to the best of}} our knowledge - not yet in the statistical literature. See especially the extension of form invariance to discrete data in section 4, the criterion on the compatibility between a proposed distribution and sparse data in section 7 and the ``discovery'' of probability amplitudes in section 9. Comment: 13 page...|$|R
40|$|Article dans revue scientifique avec comité de lecture. internationale. International audienceTrees with labeled edges have {{widespread}} applicability, {{for example}} for the representation of dependency syntax trees. Given a fixed number of nodes and constraints on how edges may be drawn between them, the task of finding solution trees {{is known as a}} configuration problem. In this paper, we formalize the configuration problem of labeled trees and argue that it can be regarded as a constraint satisfaction problem which can be solved directly and efficiently by constraint propagation. In particular, we derive and prove correct a formulation of dependency parsing as a constraint satisfaction problem. Our approach, based on constraints on finite sets and a new family of `selection' constraints, is especially well-suited for the compact representation and efficient processing of ambiguity. We address various issues of interest to the computational linguist such as lexical ambiguity, structural ambiguity, valency constraints, grammatical principles, and linear precedence. Finally we turn to the challenge of efficient processing and characterize the services expected of a constraint programming system: we define a formal constraint language and specify its operational semantics with inference <b>rules</b> <b>of</b> <b>propagation</b> and distribution. This framework generalizes our presentation of immediate syntactic dependence for dependency parsing (Duchier, 1999 a) and extends naturally to our corresponding treatment of linear precedence (Duchier and Debusmann, 2001) based on a notion of topological rather than syntactic dependencies...|$|R
40|$|AbstractA {{series of}} studies about the <b>rule</b> <b>of</b> crack <b>propagation</b> {{and the process of}} {{instability}} in bump-prone coal sample have been carried out by using scanning electron microscope (SEM), image of surface fracture morphology and the numerical method. The results of three-point bending experiment show that the crack is mainly the tear-type crack under quasi-static loading and can be analyzed with the mode I crack theory. The paper deduced the critical length derived from static to dynamic stage based on fracture and damage mechanics theory and introduced the cohesive element. With the cohesive force model of three-point bending notched beam of the sample, the process of the initiation and <b>propagation</b> <b>of</b> crack by using ABAQUS finite element software. This study discussed the energy variety in the whole process, and it is coincide with the experimental results...|$|R
40|$|This paper {{deals with}} the {{structure}} and characteristics of PID Neural Network controller for single input and single output systems. PID Neural Network is {{a new kind of}} controller that includes the advantages of artificial neural networks and classic PID controller. Functioning of this controller is based on the update of controller parameters according to the value extracted from system output pursuant to the <b>rules</b> <b>of</b> back <b>propagation</b> algorithm used in artificial neural networks. Parameters obtained from the application of PID Neural Network training algorithm on the speed model of the asynchronous motor exhibiting second order linear behavior were used in the real time speed control of the motor. Programmable logic controller (PLC) was used as real time controller. The real time control results show that reference speed successfully maintained under various load conditions...|$|R
40|$|Monte Carlo {{simulations}} <b>of</b> photon <b>propagation</b> offer {{a flexible}} yet rigorous approach toward photon transport in turbid tissues. This method simulates the “random walk” of photons {{in a medium}} that contains absorption and scattering. The method {{is based on a}} set <b>of</b> <b>rules</b> that govern the movement of a photon in tissue. The two key decisions are (1) the mean free path for a scattering or absorption event, and (2) the scattering angle. Figure 4. 1 illustrates a scattering event. At boundaries, a photon is reflected or moves across the boundary. The <b>rules</b> <b>of</b> photon <b>propagation</b> are expressed as probability distributions for the incremental steps of photon movement between sites of photon—tissue interaction, for the angles of deflection in a photon’s trajectory when a scattering event occurs, and for the probability of transmittance or reflectance at boundaries. Monte Carlo light propagation is rigorous yet very descriptive. However, this method is basically statistical in nature and requires a computer to calculate the <b>propagation</b> <b>of</b> a large number of photons. To illustrate how photons propagate inside tissues, a few photon paths are shown in Fig. 4. 2...|$|R
40|$|Abstract: Creation and {{transformation}} of visual specifications {{is driven by}} modeler’s design decisions. After a design decision has been made, the modeler needs to adjust the specification to maintain its correctness. The number of adjustments might make the design process tedious for large specifications. We are interested in techniques that will reduce the modeler’s obligation to control specification correctness. Every single transformation of the visual specification can be captured by the notion of refinement used in formal methods. In this work we present the technique that supports a stepwise refinement of visual specifica-tions based on calculations. We use refinement calculus as a logic for reasoning about refinement correctness. When a design decision is made by the modeler, the necessary adjustments are calculated based on <b>rules</b> <b>of</b> refinement <b>propagation.</b> Refinement propagation can automate the specification adjustment and enforce its correctness. ...|$|R
40|$|This article {{extends the}} {{stochastic}} cell transmission model (SCTM) to simulate traffic flows on networks with stochastic demand and supply. The SCTM divides a roadway segment into cells and accepts {{the means and}} variances of stochastic travel demand and supply functions as exogenous inputs, and produces the corresponding cell traffic densities over time. This article defines the <b>rules</b> <b>of</b> flow <b>propagation</b> for freeway corridors, traffic merges/diverges and signalised junctions based {{on a kind of}} link-node model. In the numerical studies, we simulate the proposed model with a hypothetical network. We apply the SCTM to estimate the queues and delays at signalised intersections. Compared with some well-known delay and queue estimation formulas, e. g. Webster, Beckmann, McNeil and Akcelik, the results show good consistency between the SCTM and these formulas. In addition, the SCTM describes the temporal behaviour of the queue and delay distributions at signalised junctions with stochastic supply functions and (non-stationary) arrivals. Department of Civil and Environmental Engineerin...|$|R
40|$|Since REAL 3 D W 1 was {{designed}} to take a pair of stereo images for stereo viewing, the baseline length of 77 mm of the camera is unsuitable for accurate stereo measurement. The aim of our study is to evaluate the accuracy of measurement using a pair of stereo images acquired by REAL 3 D W 1 without any controls. We evaluated the accuracy of 3 D measurement with three parameter sets. The first parameter set was extracted from the obtained image file, the second one was obtained by camera calibration, and the third one was estimated by using a pair of stereo images depicting a scale. Since an image file has no information about digital zooming, the obtained image file cannot provide a reasonable measurement result without any additional information. Meanwhile, the camera calibration would enable to measure dimensions of an object with the accuracy equivalent to the expected accuracy calculated by following the <b>rules</b> <b>of</b> error <b>propagation.</b> 1...|$|R
40|$|The aim of {{this paper}} is to generalize the notion of {{conformal}} blocks to the situation in which the Lie algebra they are attached to is not defined over a field, but depends on covering data of curves. The result will be a sheaf of conformal blocks on the Hurwitz stack parametrizing Galois coverings of curves. Many features of the classical sheaves of conformal blocks are proved to hold in this more general setting, in particular the fusion <b>rules,</b> the <b>propagation</b> <b>of</b> vacua and the WZW connection...|$|R
40|$|Although {{transactional}} {{models have}} proved to be very useful for numerous applications, the development of new models to reflect the ever-increasing complexity and diversity of modern applications is a very active area o f research. Analysis of the existing models of multithreaded transactions shows that they either give too much freedom to threads and do not control their participation in transactions, or unnecessarily restrict the computational model by assuming that only one thread can enter a transaction. Another important issue, which many models do not address properly, is providing adequate exception handling features. In this paper a new model of multithreaded transactions is proposed. Its detailed description is given, including <b>rules</b> <b>of</b> thread behaviour when transactions start, commit and abort, and <b>rules</b> <b>of</b> exception raising, <b>propagation</b> and handling. This model is supported by enhanced error detection techniques to allow for earlier error detection and for l [...] ...|$|R
40|$|Herbelin {{presented}} (at CSL' 94) {{a simple}} sequent calculus for minimal implicational logic, extensible to full first-order intuitionistic logic, {{with a complete}} system <b>of</b> cut-reduction <b>rules</b> which is both confluent and strongly normalising. Some <b>of</b> the cut <b>rules</b> may be regarded as rules to construct explicit substitutions. He observed that {{the addition of a}} cut permutation <b>rule,</b> for <b>propagation</b> <b>of</b> such substitutions, breaks the proof of strong normalisation; the implicit conjecture is that the rule may be added without breaking strong normalisation. We prove this conjecture, thus showing how to model beta-reduction in his calculus (extended with rules to allow cut permutations) ...|$|R
40|$|Objective. To {{investigate}} if software simulation is {{practical for}} quantifying random error (RE) in phantom dosimetry. Materials and Methods. We applied software error simulation to an existing dosimetry study. The specifications and the measurement values {{of this study}} were brought into the software (R version 3. 0. 2) together with the algorithm of the calculation of the effective dose (). Four sources of RE were specified: () the calibration factor; () the background radiation correction; () the read-out process of the dosimeters; and () the fluctuation of the X-ray generator. Results. The amount of RE introduced by these sources was calculated {{on the basis of the}} experimental values and the mathematical <b>rules</b> <b>of</b> error <b>propagation.</b> The software repeated the calculations of multiple times () while attributing the applicable RE to the experimental values. A distribution of emerged as a confidence interval around an expected value. Conclusions. Credible confidence intervals around in phantom dose studies can be calculated by using software modelling of the experiment. With credible confidence intervals, the statistical significance of differences between protocols can be substantiated or rejected. This modelling software can also be used for a power analysis when planning phantom dose experiments...|$|R
40|$|This paper {{introduces}} a new image-guided non-local dense matching algorithm {{that focuses on}} how to solve the following problems: 1) mitigating the influence of vertical parallax to the cost computation in stereo pairs; 2) guaranteeing the performance of dense matching in homogeneous intensity regions with significant disparity changes; 3) limiting the inaccurate cost propagated from depth discontinuity regions; 4) guaranteeing that the path between two pixels in the same region is connected; and 5) defining the cost propagation function between the reliable pixel and the unreliable pixel during disparity interpolation. This paper combines the Census histogram and an improved histogram of oriented gradient (HOG) feature together as the cost metrics, which are then aggregated based on a new iterative non-local matching method and the semi-global matching method. Finally, new <b>rules</b> <b>of</b> cost <b>propagation</b> between the valid pixels and the invalid pixels are defined to improve the disparity interpolation results. The results of our experiments using the benchmarks and the Toronto aerial images from the International Society for Photogrammetry and Remote Sensing (ISPRS) show that the proposed new method can outperform most of the current state-of-the-art stereo dense matching methods...|$|R
40|$|Two {{datasets}} {{of points}} of known spatial positions and an associated absorbed dose value are often compared for quality assurance purposes in External Beam Radiation Therapy (EBRT). Some problems usually arise regarding the pass fail criterion to accept both datasets as {{close enough for}} practical purposes. Instances {{of this kind of}} comparisons are fluence or dose checks for intensity modulated radiation therapy, modelling of a treatment unit in a treatment planning system, and so forth. The gamma index is a figure of merit that can be obtained from both datasets; it is widely used, as well as other indices, as part of a comparison procedure. However, it is recognized that false negatives may take place (there are acceptable cases where a certain number of points do not pass the test) due in part to computation and experimental uncertainty. This work utilizes mathematical methods to analyse comparisons, so that uncertainty can be taken into account. Therefore, false rejections due to uncertainty do not take place and {{there is no need to}} expand tolerances to take uncertainty into account. The methods provided are based on the <b>rules</b> <b>of</b> uncertainty <b>propagation</b> and help obtain rigorous pass/fail criteria, based on experimental information...|$|R
40|$|We {{investigate}} the nonlinear dynamics of a nonconventional (i. e., pumped by a mixed-mode wave) modulational instability {{in a highly}} birefringent nonlinear dispersive medium. We find that the depleted regime <b>of</b> <b>propagation</b> beyond the linearized stage can be described analytically in a proper region of the parameter space. In this case the governing coupled nonlinear Schrödinger equations, which are not integrable, are reduced to an integrable one-dimensional nonlinear oscillator that <b>rules</b> the <b>propagation</b> <b>of</b> the pump wave and a single sideband pair. This approach permits us to predict the existence of stable and unstable manifolds of time-periodic solutions of the coupled nonlinear Schrödinger equations. The nonlinear dynamics governed by these equations mimics the period-doubling instabilities associated with the homoclinic separatrices in the reduced phase space. Moreover, our approach is also capable of describing the onset of spatial chaos that occurs when the parameter values are such that the additional degree of freedom represented by the conjugated sidebands becomes effective...|$|R
40|$|In {{order to}} {{investigate}} the effect of nano-aluminum powder on the characteristic of RDX based aluminized explosives underwater closed-filed explosions, the scanning photographs along the radial of the charges were gained by a high speed scanning camera. The photographs of two different aluminized explosives underwater explosion have been analyzed, the shock wave curves and expand curves of detonation products were obtained, furthermore the change <b>rules</b> <b>of</b> shock waves <b>propagation</b> velocity, shock front pressure and expansion of detonation products of two aluminized explosives were investigated, and also the parameters of two aluminized explosives were contrasted. The {{results show that the}} aluminized explosive which with nano-aluminum whose initial shock waves pressure propagation velocity, shock front pressure are smaller than the aluminized explosive without nano-aluminum and has lower decrease rate attenuation of energy...|$|R
40|$|In {{this series}} of papers {{we set out to}} generalize the notion of {{classical}} analytic deduction (i. e., deduction via elimination rules) by combining the methodology of labelled deductive systems (LDS) with the classical systemKE. LDS is a unifying framework for the study of logics and of their interactions. In the LDS approach the basic units of logical derivation are not just formulae butlabelled formulae, where the labels belong to a given labelling algebra. The derivation rules act on the labels {{as well as on the}} formulae, according to certain fixed <b>rules</b> <b>of</b> <b>propagation.</b> By virtue <b>of</b> the extra power of the labelling algebras, standard (classical or intuitionistic) proof systems can be extended to cover a much wider territory without modifying their structure. The systemKE is a new tree method for classical analytic deduction based on analytic cut. KE is a refutation system, like analytic tableaux and resolution, but it is essentially more efficient than tableaux and, unlike resolution, does not require any reduction to normal form. We start our investigation with the family of substructural logics. These are logical systems (such as Lambek"s calculus, Anderson and Belnap"s relevance logic, and Girard"s linear logic) which arise from disallowing some or all of the usual structural properties of the notion of logical consequence. This extension of traditional logic yields a subtle analysis of the logical operators which is more in tune with the needs of applications. In this paper we generalize the classicalKE system via the LDS methodology to provide a uniform refutation system for the family of substructural logics. The main features of this generalized method are the following: (a) each logic in the family is associated with a labelling algebra; (b) the tree-expansion rules (for labelled formulae) are the same for all the logics in the family; (c) the difference between one logic and the other is captured by the conditions under which a branch is declared closed; (d) such conditions depend only on the labelling algebra associated with each logic; and (e) classical and intuitionistic negations are characterized uniformly, by means of the same tree-expansion rules, and their difference is reduced to a difference in the labelling algebra used in closing a branch. In this first part we lay the theoretical foundations of our method. In the second part we shall continue our investigation of substructural logics and discuss the algorithmic aspects of our approach...|$|R
40|$|Abstract. Seismic wavelet {{plays an}} {{important}} role in the processing and interpretation of seismic data. And wavelet is also the premise of forward modeling and inversion of seismic wave. So it is important to study the formation process and formation <b>rule</b> <b>of</b> seismic wavelet. The viscoelastic properties of geologic body can be represented using Kelvin medium. The formation <b>rule</b> <b>of</b> seismic wavelet was analyzed using the mathematical model <b>of</b> <b>propagation</b> process <b>of</b> rectangle pulse in Kelvin medium. The experimental results have shown that characteristic frequency and band width were major frequency spectrum parameters causing the variation of waveform. The characteristic frequency and band width decreased with the increase <b>of</b> <b>propagation</b> distance. The characteristic frequency decreased in negative exponential law and became stable finally. The change of characteristic frequency and band width was related to the mechanical parameters of medium. With the increase <b>of</b> <b>propagation</b> distance, characteristic frequency decreased more slowly as elastic modulus increased than as viscous coefficient increased. The correlation coefficient increased with the increase <b>of</b> <b>propagation</b> distance, i. e., waveform became stable at last. Ratio of stability distance to characteristic wave length increased with the increase of elastic modulus and the decrease of viscous coefficient, then tended to become stable...|$|R
40|$|Commons Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. Two datasets of points of known spatial positions and an associated absorbed dose value are often compared for quality assurance purposes in External Beam Radiation Therapy (EBRT). Some problems usually arise regarding the pass fail criterion to accept both datasets as {{close enough for}} practical purposes. Instances {{of this kind of}} comparisons are fluence or dose checks for intensity modulated radiation therapy, modelling of a treatment unit in a treatment planning system, and so forth. The gamma index is a figure of merit that can be obtained from both datasets; it is widely used, as well as other indices, as part of a comparison procedure. However, it is recognized that false negatives may take place (there are acceptable cases where a certain number of points do not pass the test) due in part to computation and experimental uncertainty. This work utilizes mathematical methods to analyse comparisons, so that uncertainty can be taken into account. Therefore, false rejections due to uncertainty do not take place and {{there is no need to}} expand tolerances to take uncertainty into account. The methods provided are based on the <b>rules</b> <b>of</b> uncertainty <b>propagation</b> and help obtain rigorous pass/fail criteria, based on experimental information. 1...|$|R
40|$|The present work {{breaks the}} endless impasse {{of the current}} {{theories}} with space and gravitation, proposing a completely new conception in which the quantum space, ruling the propagarion of light and the inertial motion of matter, moves according to a velocity field consistent with the local main astronomical motions. This solution is clearly suggested by recent clear-cut experimental observations, achieved {{with the help of}} the GPS and also is implicit in the Quantum Field Theory (QFT) underlying the Standard Elementary Particle Model (SEPM). In a first part (Section II) it is shown that these recent experimental observations demonstrate that real space, the one that <b>rules</b> the <b>propagation</b> <b>of</b> light and the inertial motion of matter, is moving round each gravitational source according to a Keplerian velocity field consistent with the local main astronomical motions. This is the crucial experimental fundamentation of the spacedynamics that appropriately produces the observed gravitational dynamics on earth, in the solar system and also the galactic gravit ational dynamics without the need of dark matter as well as all the observed effects of the gravitational fields on the <b>propagation</b> <b>of</b> light and on the rate of clocks. In a second part (Section III) it is shown how this spacedynamics arises within the context of the QFT underlying the SEPM. The QFT entails the idea that space is filled up with a scalar quantum field, a Bose-Einstein condensate of Higgs bosons. This Higgs condensate is a quantum fluid, responsible for giving mass to the elementary particles by the Higgs mechanism providing them with mechanical properties. This lets clear that the Higgs condensate plays the role of real quantum space that <b>rules</b> the <b>propagation</b> <b>of</b> light and the inertial motion of matter and is the ultimate reference for rest and for motion of matter and light. Therefore, on moving according to a Keplerian velocity field, this condensate causes the observed gravitational dynamics as well as all the other observed effects caused by the gravitational fields. </p...|$|R
40|$|We {{study the}} {{dynamics}} of Random Threshold Network (RTN) on scale free networks, with asymmetric links, some interaction <b>rules</b> where <b>propagation</b> <b>of</b> local perturbations depends on in-degree k of the nodes. We find {{that there is no}} phase transition with respect to average connectivty independently of network topology for the case temperature T= 0, threshold h= 0 and the probability distribution of indegree P(k) satisfies P(0) =D= 0. We have investigated the emergence of phase transition involving three parameters, i. e. T,h and D. RTN can be continuously connected to Random Boolean Network (RBN) in T→∞, and we find moderate thermal noise extends the regime of ordered dynamics, compared to RTN in T= 0 regime and RBN. Furthermore, we discuss the dynamic properties from another point of view, dynamical mean field reaction rate equation...|$|R
40|$|Abstract. In this paper, {{the finite}} element method (FEM) was applied to predict the local {{buckling}} behavior and the debond propagation in honeycomb sandwich panels with face-core debond under in-plane compressive load. The finite element model of the sandwich panel was built, the cohesive element was used to model the adhesive between faces and core, {{the influence of the}} debond shape and size on the failure mode, critical buckling load and residual compressive strength of the sandwich panels was investigated, the <b>rule</b> <b>of</b> the damage <b>propagation</b> was summarized. The compression strength of the sandwich panels with through-width face-core debond decreases with increasing debond length. For the panels with central circular debond, when the diameter is less than 15 mm, the panels will failure by global buckling and the debond will not grow. When the diameter is greater than 15 mm, the panels will failure by local buckling and the critical load strongly decreases with increasing debond diameter. In addition, the direction of debond growth is predominantly perpendicular to the applied load...|$|R
40|$|Dendritic {{geometry}} shapes neuronal cAMP signalling to {{the nucleus}} Lu Li 1, 2, 3,*,w, Nicolas Gervasi 1, 2, 3, * & Jean-Antoine Girault 1, 2, 3 Neurons have complex dendritic trees, receiving numerous inputs at various distances from the cell body. Yet the <b>rules</b> <b>of</b> molecular signal <b>propagation</b> from dendrites to nuclei are unknown. DARPP- 32 is a phosphorylation-regulated signalling hub in striatal output neurons. We combine diffusion-reaction modelling and live imaging to investigate cAMP-activated DARPP- 32 signalling to the nucleus. The model predicts maximal {{effects on the}} nucleus of cAMP production in secondary dendrites, due to segmental decrease of dendrite diameter. Variations in branching, perikaryon size or spines have less pronounced effects. Biosensor kinase activity measurement following cAMP or dopamine uncaging confirms these predictions. Histone 3 phosphorylation, regulated by this pathway, is best stimulated by cAMP released in secondary-like dendrites. Thus, unexpectedly, the efficacy of diffusion-based signalling from dendrites to nucleus is not inversely proportional to the distance. We suggest a general mechanism by which dendritic geometry counterbalances the effect of dendritic distance for signalling to the nucleus...|$|R
30|$|Radial well {{drilling}} by hydraulic jetting, whose length {{is up to}} 100  m and diameter is 25 – 50  mm, has been widely applied and has achieved positive effects,(Ma et al. 2005). Since the cost of radial well is far lower than horizontal well, the technique combining radial well and hydraulic fracturing as an emerging stimulation treatment comes into being. The emerging technique has been applied in several oil fields in China, and performed a much better stimulation result than conventional perforation fracturing (Liu 2012). Studies have shown that radial well is capable of guiding the direction of fractures, which means the fracture propagation path can be controlled by radial well (Gong et al. 2016). Obviously, the performance of oriented fracture is not only effective in increasing the discharge area but also in developing the remaining oil. At present, {{it has been found}} out that the fracture initiates from the borehole of radial well (Gong et al. 2016). However, the study on radial well fracturing is still at the preliminary stage, the fracture morphology and <b>propagation</b> law <b>of</b> radial well fracturing still remain unclear, especially lacking visual achievement, which limits the development and application of radial well fracturing. Therefore, the mechanism and <b>rules</b> <b>of</b> fracture orientated <b>propagation</b> need to be revealed, which made the prediction <b>of</b> fracture <b>propagation</b> path practical.|$|R
