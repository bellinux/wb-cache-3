1601|2291|Public
25|$|In any <b>recursive</b> <b>algorithm,</b> {{there is}} {{considerable}} freedom {{in the choice of}} the base cases, the small subproblems that are solved directly in order to terminate the recursion.|$|E
25|$|Typically, such caching DNS servers also {{implement}} the <b>recursive</b> <b>algorithm</b> necessary {{to resolve a}} given name starting with the DNS root through to the authoritative name servers of the queried domain. With this function implemented in the name server, user applications gain efficiency in design and operation.|$|E
25|$|A <b>recursive</b> <b>algorithm</b> is {{one that}} invokes (makes {{reference}} to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.|$|E
40|$|Abstract. This article first {{reviews the}} problem of {{partitioning}} a positive integer in general. And then <b>recursive</b> <b>algorithms</b> for P(n,k), generating exactly k partitions of a positive number n are presented. The most efficient <b>recursive</b> <b>algorithm’s</b> computational running time is Θ(k×p(n,k)) which is the lower bound of P(n,k) problem. ...|$|R
40|$|AbstractThe Schur {{complement}} is {{used with}} the block bordering method to derive two <b>recursive</b> <b>algorithms</b> called the matrix <b>recursive</b> projection <b>algorithm</b> and the matrix <b>recursive</b> interpolation <b>algorithm.</b> These algorithms are extensions of the RPA and the RIA studied by Brezinski in the vector case. Some properties of these algorithms are given...|$|R
40|$|Abstract Many {{important}} computation {{problems can}} be specified by block <b>recursive</b> <b>algorithms.</b> For exam-ple, matrix transposition and fast Fourier transform are block <b>recursive</b> <b>algorithms.</b> In this paper, we present a methodology of VLSI circuit design for block <b>recursive</b> <b>algorithms</b> based on the tensor product theory. Matrix transposition and fast Fourier transform algorithms are designed and implemented following this methodology. First, matrix transposition and fast Fourier transform algorithms are expressed as tensor product formulas. The tensor product formulas are modified to fit into interconnection networks, including the omega network and the hypercube network. The formulas are then used to generate high-level programming language code. Finally, a hardware description language, Verilog, is used to realize the algorithms according to the generated programs. The major goal {{of this paper is}} to provide an effective way to design VLSI circuits for block <b>recursive</b> <b>algorithms...</b>|$|R
25|$|Finding all {{solutions}} to the eight queens puzzle {{is a good example}} of a simple but nontrivial problem. For this reason, it is often used as an example problem for various programming techniques, including nontraditional approaches such as constraint programming, logic programming or genetic algorithms. Most often, it is used as an example of a problem that can be solved with a <b>recursive</b> <b>algorithm,</b> by phrasing the n queens problem inductively in terms of adding a single queen to any solution to the problem of placing n1 queens on an n-by-n chessboard. The induction bottoms out with the solution to the 'problem' of placing 0 queens on the chessboard, which is the empty chessboard.|$|E
25|$|Charlie works {{once again}} with rival Marshall Penfield, as they settle their {{differences}} ("Frienemies"). Also, he is chosen {{to be the head}} of the think tank model comprising himself, Larry, Alan, and Amita ("Jacked"). As Dr. Eppes applies the Turing Test to a seemingly unique artificial intelligence, he is tricked only to come to the realization that the computer only uses a <b>recursive</b> <b>algorithm</b> to apply the most human responses, while simultaneously being tempted by an offer to work for DARPA. Head of DARPA special projects Jane Karellen (Nancy Travis) knows that he has a limited window to use his genius and tells Charlie that {{he is one of the}} top five minds on the planet. Amita's life is even threatened by the advanced computer ("First Law").|$|E
25|$|Overlapping sub-problems {{means that}} the space of sub-problems must be small, that is, any <b>recursive</b> <b>algorithm</b> solving the problem should solve the same sub-problems over and over, rather than {{generating}} new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: F'i = F'i1 + F'i2, with base case F1=F2=1. Then F43 =F42+F41, and F42 =F41+F40. Now F41 is being solved in the recursive sub-trees of both F43 as well as F42. Even though {{the total number of}} sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once.|$|E
40|$|General <b>{{recursive}}</b> <b>algorithms</b> {{are such}} that the recursive calls are performed on arguments satisfying no condition that guarantees termination. Hence, {{there is no direct}} way of formalising them in type theory. The standard way of handling general recursion in type theory uses a well-founded recursion principle. Unfortunately, this way of formalising general <b>recursive</b> <b>algorithms</b> often produces unnecessarily long and complicated codes. On the other hand, functional programming languages like Haskell impose no restrictions on recursive programs, and then writing general <b>recursive</b> <b>algorithms</b> is straightforward. In addition, functional programs are usually short and self-explanatory. However, the existing frameworks for reasoning about the correctness of Haskell-like programs are weaker than the framework provided by type theory. The goal of this work is to present a method that combines the advantages of both programming styles when writing simple general <b>recursive</b> <b>algorithms</b> [...] . ...|$|R
5000|$|... #Subtitle level 2: <b>Recursive</b> <b>algorithms</b> {{used for}} the {{numerical}} propagation of spacecraft orbits ...|$|R
40|$|The {{discrete}} cosine transform (DCT) and the discrete sine transform (DST) {{have found}} wide applications in speech and image processing, {{as well as}} telecommunication signal processing {{for the purpose of}} data compression, feature extraction, image reconstruction, and filtering. In this paper, we present new <b>recursive</b> <b>algorithms</b> for the DCT and the DST. The proposed method is based on certain recursive properties of the DCT coefficient matrix, and can be generalized to design <b>recursive</b> <b>algorithms</b> for the 2 -D DCT and the 2 -D DST. These new structured <b>recursive</b> <b>algorithms</b> are able to decompose the DCT and the DST into two balanced lower-order subproblems in comparison to previous research works. Therefore, when converting our algorithms into hardware implementations, we require fewer hardware components than other <b>recursive</b> <b>algorithms.</b> Finally, we propose two parallel algorithms for accelerating the computation. EDICS number: 4. 1, 4. 1. 1, 6. 1, 6. 1. 3. Index Terms: discrete cosine transform, d [...] ...|$|R
25|$|On {{the other}} hand, {{efficiency}} often improves if the recursion is stopped at relatively large base cases, {{and these are}} solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid <b>recursive</b> <b>algorithm</b> is short-circuiting the base case, also known as arm's-length recursion. In this case whether the next step {{will result in the}} base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking if it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees. Since a D algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.|$|E
500|$|Using these {{observations}} they can generate all maximal cliques in [...] by a <b>recursive</b> <b>algorithm</b> that chooses a vertex [...] arbitrarily and then, for each maximal clique [...] in , outputs both [...] and the clique formed by adding [...] to [...] and removing the non-neighbors of [...] However, some cliques of [...] may be generated {{in this way}} {{from more than one}} parent clique of , so they eliminate duplicates by outputting a clique in [...] only when its parent in [...] is lexicographically maximum among all possible parent cliques. On the basis of this principle, they show that all maximal cliques in [...] may be generated in time [...] per clique, where [...] is the number of edges in [...] and [...] is the number of vertices. [...] improve this to [...] per clique, where [...] is the arboricity of the given graph. [...] provide an alternative output-sensitive algorithm based on fast matrix multiplication. [...] show that it is even possible to list all maximal cliques in lexicographic order with polynomial delay per clique. However, the choice of ordering is important for the efficiency of this algorithm: for the reverse of this order, ...|$|E
2500|$|There also exists an equivalent, related <b>recursive</b> <b>algorithm</b> {{introduced}} by Urbain Le Verrier and Dmitry Konstantinovich Faddeev—the Faddeev–LeVerrier algorithm, which reads ...|$|E
40|$|The {{learning}} of <b>recursive</b> <b>algorithms</b> in computer programming is problematic, because its execution and resolution is not natural to the thinking way people are trained {{and used to}} since young. As with other topics in algorithms, we use metaphors to make parallels between the abstract and the concrete to help in understanding the operation of <b>recursive</b> <b>algorithms.</b> However, the classic metaphors employed in this area, such as calculating factorial recursively and Towers of Hanoi game, may just confuse more or be insufficient. In this work, we produced a computer game to assist students in computer courses in learning <b>recursive</b> <b>algorithms.</b> It was designed to have regular video game characteristics, with narrative and classical gameplay elements, commonly found {{in this kind of}} product. Aiding to education occurs through metaphorization, or in other words, through experiences provided by game situations that refer to <b>recursive</b> <b>algorithms.</b> To this end, we designed and imbued in the game four valid metaphors related to the theory, and other minor references to the subject...|$|R
40|$|AbstractWe {{establish}} linear lower bounds for {{the complexity}} of non-trivial, primitive <b>recursive</b> <b>algorithms</b> from piecewise linear given functions. The main corollary is that logtime algorithms for the greatest common divisor from such givens (such as Stein’s) cannot be matched in efficiency by primitive <b>recursive</b> <b>algorithms</b> from the same given functions. The question is left open for the Euclidean algorithm, which assumes the remainder function...|$|R
40|$|We {{establish}} linear lower bounds for {{the complexity}} of non-trivial, primitive <b>recursive</b> <b>algorithms</b> from piecewise linear given functions. The main corollary is that logtime algorithms for the greatest common divisor from such givens (such as Stein's) cannot be matched in e#ciency by primitive <b>recursive</b> <b>algorithms</b> from the same given functions. The question is left open for the Euclidean algorithm, which assumes the remainder function. In 1991, Colson [3] proved a remarkable theorem about the limitations of primitive <b>recursive</b> <b>algorithms,</b> which has the following consequence: Colson's Corollary. If a primitive recursive derivation of min(x, y) is expressed faithfully in a programming language, {{then one of the}} two computations min(1, 1000) and min(1000, 1) will take at least 1000 steps...|$|R
2500|$|... {{immediately}} {{yields a}} <b>recursive</b> <b>algorithm</b> for computing it: choose any such edge e and repeatedly apply the formula until all edges are either loops or bridges; the resulting base cases {{at the bottom}} of the evaluation are easy to compute.|$|E
2500|$|Woon's <b>recursive</b> <b>algorithm</b> (for [...] ) starts by {{assigning}} {{to the root}} node [...] Given a node [...] of the tree, the left child of the node is [...] and the right child [...] A node [...] is written as [...] in the initial part of the tree represented above with ± denoting the sign of [...]|$|E
5000|$|... defines two algorithms, a <b>recursive</b> <b>algorithm</b> and an {{iterative}} algorithm, for {{linear programming}} based on random sampling techniques, and suggests {{a combination of}} the two that calls the iterative algorithm from the <b>recursive</b> <b>algorithm.</b> The <b>recursive</b> <b>algorithm</b> repeatedly chooses random samples whose size is approximately the square root of the input size, solves the sampled problem recursively, and then uses violation tests to find a subset of the remaining elements that must include at least one basis element: ...|$|E
40|$|Algorithms for the Tower of Hanoi problem {{are often}} used in the {{introductory}} texts on computer programming for demonstrating the power of recursion (Hayes, 1977, Dijkstra, 1971; Dromey, 1981). Interesting though these <b>recursive</b> <b>algorithms</b> are, beginners are not always convinced that these algorithms will work until they are run. Given such <b>recursive</b> <b>algorithms,</b> it is not obvious how to move discs around until one actually steps through the programs...|$|R
40|$|The paper {{suggests}} a novel method for implementing <b>recursive</b> <b>algorithms</b> in hardware. The required support for recursion {{has been provided}} through a modular and a hierarchical specification of a control unit that can be translated to an implementation of the respective hardware circuit {{on the basis of}} a recursive hierarchical finite state machine and through a mechanism that permits the contents of an execution unit to be stored/restored between hierarchical calls/returns. The paper describes all the details that are required to implement <b>recursive</b> <b>algorithms</b> in hardware. It begins with software (Cþþ) models and finishes with synthesizable VHDL codes. Two practical applications of <b>recursive</b> <b>algorithms</b> in the data sorting and compression area have been studied in detail. q 2004 Published by Elsevier B. V...|$|R
50|$|The {{determinant}} {{formula for}} the Gram-Schmidt is computationally slower (exponentially slower) than the <b>recursive</b> <b>algorithms</b> described above;it is mainly of theoretical interest.|$|R
5000|$|... #Subtitle level 2: <b>Recursive</b> <b>algorithm</b> {{for solving}} parity games ...|$|E
5000|$|... #Caption: 3D fractal fantasy {{generated}} using LAI4Dwith a <b>recursive</b> <b>algorithm</b> in 6 iterations ...|$|E
5000|$|If [...] is {{a finite}} set, {{there is a}} <b>recursive</b> <b>algorithm</b> to {{calculate}} (...) [...]|$|E
40|$|AbstractBased on the Newton-Raphson method, {{this paper}} {{presents}} <b>recursive</b> <b>algorithms</b> that are rapidly convergent and more stable for modeling the equivalent continuous-time (discrete-time) model from the available discrete-time (continuous-time) model for a fixed sampling period. The newly developed <b>recursive</b> <b>algorithms</b> relax the constraints imposed upon the existing model conversion algorithms, and, thus, enhance the applications of microprocessors and associated microelectronics to digital control systems. A practical example is presented to demonstrate {{the effectiveness of the}} proposed procedures...|$|R
40|$|We {{show how}} the {{methodology}} presented by Bove for the formalisation of simple general <b>recursive</b> <b>algorithms</b> and extended by Bove and Capretta to treat nested recursion {{can also be used}} in the formalisation of mutual general <b>recursive</b> <b>algorithms.</b> The methodology consists of de ning special-purpose accessibility predicates that characterise the inputs on which the algorithms terminate. Each algorithm is then formalised in type theory by structural recursion on the proof that its input satis es the corresponding special-purpose accessibility predicate. When the mutually <b>recursive</b> <b>algorithms</b> are also nested, we make use of a generalisation of Dybjer's schema for simultaneous inductive-recursive de nitions, which we also present in this work. Hence, some of the formalisations we present in this work are not allowed in ordinary type theory, but they can be carried out in type theories extended with such a schema. Similarly to what happens for simple and nested <b>recursive</b> <b>algorithms,</b> this methodology results in de nitions in which the computational and logical parts are clearly separated also when the <b>algorithms</b> are mutually <b>recursive.</b> Hence, the type-theoretic version of the algorithms is given by its purely functional content, similarly to the corresponding program in a functional programming language...|$|R
3000|$|... {{and gave}} two <b>recursive</b> <b>algorithms</b> to {{determine}} necessary {{conditions for a}} center and an isochronous center. We now restate the definitions and algorithms.|$|R
5000|$|Consider {{a problem}} that can be solved using a <b>recursive</b> <b>algorithm</b> such as the following: ...|$|E
50|$|In computing, {{recursion}} termination is when {{certain conditions}} are met and a <b>recursive</b> <b>algorithm</b> stops calling itself {{and begins to}} return values. This happens only if, with every recursive call, the <b>recursive</b> <b>algorithm</b> changes its state and moves toward the base case. Cases that satisfy the definition, without being {{defined in terms of}} that definition, are called base cases. They are small enough to solve directly.|$|E
5000|$|In {{other terms}} it's a <b>recursive</b> <b>algorithm</b> that will follow these steps until all jobs are scheduled: ...|$|E
50|$|OCaml {{lends itself}} to {{concisely}} expressing <b>recursive</b> <b>algorithms.</b> The following code example implements an algorithm similar to quicksort that sorts a list in increasing order.|$|R
30|$|The next result {{provides}} the mathematical foundations for the fixed-point techniqueuseful for analyzing the complexity class of <b>recursive</b> <b>algorithms</b> whose running time ofcomputing satisfies the recurrence equation (8).|$|R
50|$|The time {{efficiency}} of <b>recursive</b> <b>algorithms</b> {{can be expressed}} in a recurrence relation of Big O notation. They can (usually) then be simplified into a single Big-Oh term.|$|R
