6|10000|Public
40|$|This is a web {{quest for}} {{students}} to research other planets {{in the solar system}} and determine which one would be best suitable for a space hotel to orbit it. Teams of 3 students gather information about the planets, taking into consideration logistical and imagined aspects such as {{how long it would take}} to get there and would it be interesting to look at. The teams produce a report about what planet they picked and why. There are links {{for students to}} use for additional background information, and a <b>rubric</b> <b>for</b> <b>evaluation.</b> Educational levels: High school, Middle school...|$|E
40|$|Abstract. In {{this paper}} we {{introduce}} a rubric for assessing quality of {{open educational resources}} and open courseware based on our socio-constructivist quality model (QORE) that includes 70 criteria grouped in four categories related with content, instructional design, technology, and courseware evaluation. Quality is assessed from an educational point of view, i. e. how useful are such resources for various actors involved in educational processes taken into account their goals, objectives, abilities etc. QORE’s {{focus is on the}} resources’ potential to act as true open educational content available online that has a genuine educational value in this context. Several challenges of using this <b>rubric</b> <b>for</b> <b>evaluation</b> of such educational resources are discussed as well...|$|E
40|$|It is {{expected}} that most, if not all, graduate students will posses skills necessary for doing literature reviews. It is less clear how to teach these skills most effectively especially to students who are area novices and unfamiliar with review process. Systematic literature reviews offer a solid instructional framework which can be implemented across curriculum and offer an opportunity to teach course material differently so that student learn not just the literature review technique itself but also some segment of the course material. Our pilot study investigated issues related to practical implementation of systematic literature reviews in two classes, with different course lengths and purpose of review assignments. Our initial results are encouraging: students’ selfefficacy with respect to ability to do reviews improved {{and they think that}} this skill is useful. We have developed a new <b>rubric</b> <b>for</b> <b>evaluation</b> of final reports as well as weekly schedule of tasks...|$|E
40|$|AbstractThe {{purpose of}} this study is to {{determine}} the views of teacher candidates about the use of a scoring <b>rubric</b> <b>for</b> the <b>evaluation</b> of their products in the course of Instructional Technologies and Material Development (ITMD). The participants of the study included twelve teacher candidates who had taken the course of ITMD in the Department of Computer Education and Instructional Technologies at the Faculty of Education at Anadolu University. The findings were grouped under three themes as the views of the teacher candidates about the evaluation of their performance in the course of ITMD, the views of the teacher candidates about the evaluation methods they preferred <b>for</b> the <b>evaluation</b> of their products in the course of ITMD and about the reasons for their preference, and the views of the teacher candidates about the use of a scoring <b>rubric</b> <b>for</b> the <b>evaluation</b> of their products in the course of ITMD...|$|R
40|$|Previous {{research}} {{on the process of}} collaborative learning in the National Open and Distance University (UNAD) have shown that students have difficulties to develop effective collaborative learning experiences, therefore the various factors that affect the fulfillment of its objectives were analyzed methodological to understand more broadly the difficulties in this way of learning. The project was developed from a mixed research approach, implementing techniques such as pre-test test, non-participant observation, forograma, and discussion groups. Among the main results there is the identification of the various psychosocial factors (familiar, work and personal), methodological (relating to the design and implementation of guidelines and <b>rubrics</b> <b>for</b> <b>evaluation,</b> participation, monitoring and tutor feedback) and communicational (such as student-student and student-tutor and assertiveness in communication) interaction involved in the development of collaborative work. ...|$|R
40|$|This article {{presents}} a <b>rubric</b> <b>for</b> the <b>evaluation</b> of Computer-Assisted Language Learning (CALL) software based on international recommendations for effective CALL. After {{a brief overview}} of the pedagogical and implementation fundamentals of CALL, and a discussion of what should be included in a needs analysis <b>for</b> CALL <b>evaluation,</b> the <b>rubric</b> is presented. The author then illustrates how the evaluation criteria in the rubric can be used in the design of a new CALL system...|$|R
40|$|Sustainability is a {{relatively}} new field simultaneously addressing economic health, environmental amenities, and social issues. Sustainable Community Assessment and Planning (SUS 350) is a new course in the Sustainable Community Development major, minor, and undergraduate certificate at Stephen F. Austin State University. It is designed to introduce students to: a) varied methods of data collection; b) complex community planning issues; c) synthesis of economic, environmental, and social issues. Distance (online) education is not typically associated with field-based learning. However, field work in sustainability can be encouraged through activities such as: monitoring energy/cost saving; measuring biological status; and citizen social surveys. Student online journal notes/discussions related to these field exercises were assessed using a <b>rubric</b> <b>for</b> <b>evaluation</b> of information, creative thinking, problem solving, and communication of content. The rubric has a scale of 1 - 4 (4 is highest). Most all scores were 2 or 3, with an overall average of 2. 3. Higher scores (averaging 2. 5 and 2. 7) were found in field activities that most synthesized economy and environment on campus, such as a campus energy monitoring workshop, a campus food waste audit, and discussions centered around a future campus green fund. Despite moderate scores, students did show capability for higher-order thinking, especially in synthesizing economy and environment on campus. Future course offerings will better link content related to off-campus community development with field-based learning, through use of a community development text and better linking of community and student schedules...|$|E
40|$|We {{know what}} happens when {{beginning}} teachers can’t teach reading effectively. First, they fail when leading discussions to help students connect key passages from texts to important experiences in their lives. Next, they forget that {{the goal is to}} teach students that they can make meaning of text. Instead, these teachers begin to teach “the book ” by lecturing students about the parts they feel students should know. And last, these teachers feel forced to im-plement “pop ” quizzes, turning the class into a game in which stu-dents try to predict the tricks and the timing of the tests. A damaging gap exists between the time before student teach-ing, when preservice teachers and their mentors assume that the study of texts is a central and valuable activity, and the period be-ginning with student teaching, when beginning teachers and their mentors often frame teaching in a way that positions literacy as an optional activity. But supervisors of student teachers and the mentors for begin-ning teachers should pay closer attention to reading instruction. To help focus attention on reading instruction, we created a rubric to use in both formative and summative evaluations of beginning teachers ’ reading instruction. The criteria are based on the vision of a highly lit-erate classroom environment in which the study of texts is a central and valuable activity. This vision of the classroom is supported by most, if not all, state standards for reading instruction, {{as well as by the}} standards of the International Reading Association. A <b>RUBRIC</b> <b>FOR</b> <b>EVALUATION</b> Grade F: Our rubric for evaluating reading instruction begins with the lowest grade, F, because it is so easy and common to fail in reading instruction. Supervisors who quickly recognize bad reading instruction can also provide the earliest support for helping beginning teachers use more effective methods. Reading the whole text aloud. In grades 1 to 4, reading whole books to chil-dren is beneficial because most children don’t yet have the ability to read them independently. But in 4 th grade and beyond, most students are able to read classroom texts, so reading the whole text to the student is doing for students what they should do for themselves. Unfortunately, reading the whole book to students is expedient. Many teachers excuse this practice by arguing that “it exposes all students to the important information in the book, which they might not get if they read it themselves. ...|$|E
40|$|As I was {{thinking}} about casting my vote for the Open Science Prize, I realized that I would in fact need a rubric for choosing. I was concerned that the public vote would tend towards popularity, familiarity, or bling, rather than {{the quality of the}} open science. But {{what does it mean to}} be “quality open science?” What should be the most important criteria? The different semi-finalist projects are all very different - on different topics, of varying degrees of maturity (some pre-date the competition and some do not), and targeting different audiences. If successful, each will have different societal impacts. I applaud them all. Recently, we evaluated over eighty manuscripts from PLOS to determine which ones were most significant, impactful, or otherwise representative to form the core of the new PLOS Open Data Collection. In this context, we created a <b>rubric</b> <b>for</b> <b>evaluation</b> and then scored each manuscript objectively. For each manuscript, what was the impact on policy change? Were ethical issues considered? Did the science advance our abilities to share data or use shared data? Did the project utilize shared data (the noble discipline of “data parasitism”) ? Was the community involved? How sexy were the figures? How much did the work foster cross-pollination of ideas and approaches across disciplines? And of course, what did people think about the work? I needed a similar rubric here, but for knowledgebases and not manuscripts. Knowledge is our collective insights, captured by experts and able to provide an explanatory framework for evaluating new observations. A knowledgebase makes that knowledge findable and computable. A recent NIH RFI: “Metrics to Assess Value of Biomedical Digital Repositories” highlighted the ineffectiveness of current knowledgebase evaluation. Traditional citation and impact factors as a measure of success or value are inadequate. For example, almost everyone in biomedicine relies on PubMed, but almost no one ever cites or mentions it in their publications. In response to this RFI, our group (consisting of the NCATS Data Translator and the Monarch Initiative) developed a rubric arranged according to the commonly cited FAIR principles [...] Findable, Accessible, Interoperable, and Reusable, but with three additional principles: Traceable, Licensed, and Connected. These latter three extensions are, in my opinion, fundamental to “quality open science”, as without them, you do not have computability, legal ability to reuse the data/knowledge, and no ability to navigate the fabric of the data landscape. Therefore, for evaluation of the open science projects, I applied the rubric we described in our response to the RFI, but with additional considerations throughout relating to the PLOS data science collection curation, and trying to take into account advances since the open science prize project began (since some projects were preexisting and backed by other funds/projects, where others were brand new). I note that this is as much an evaluation of the rubric as it is of the projects themselves. I purposefully did not watch any of the videos explaining the projects on the Open Science Prize website before performing the evaluation. I wanted to determine how well the projects themselves related their goals, content, and functionality. As a potential user of the data, I aimed to evaluate the ease of navigating the data and its access and reuse directly. Most importantly, I wanted to avoid bias where the real distinctions between projects might be obscured by video production quality, rather than highlighting each project’s genuine values and their differences. It would be all too easy to create a great video about a great idea, and then not implement a quality platform based on strong open science principles, such as open code and data access, or the FAIR+ principles: Findable, Accessible, Interoperable, and Reusable,Traceable, Licensed, and Connected. One might ask, why bother? The first reason was I wanted to determine how well the preliminary rubric we laid out in our response to the RFI might work in the real world, as we plan to write a more thorough proposal for knowledgebase/data repository evaluation in the future. The second reason is that I simply wanted the evaluation of these projects to inform the future development of the open science projects I work most on, such as the Monarch Initiative (genotype-phenotype data aggregation across species for diagnostics and mechanism discovery), Phenopackets (a new standard for exchanging computable phenotype data for any species in any context), and OpenRIF (computable representation of scholarly outputs and contribution roles to better credit non-traditional scientists). How can we all do better and learn from the Open Science competition? In other words, such a competition shouldn’t just be about the six finalists, but rather it should inform how we all go about practicing open science in general. So now you are probably wondering, which project(s) did I vote for? Well, that is for you to infer. As you review the musings below, consider your own values for what constitutes robust open science. The full text of my review is available at [URL] Comments and corrections entirely welcome on the force 11 page or tweet to @ontowonka...|$|E
40|$|In {{the past}} ten years more than thirty English-Spanish legal {{dictionaries}} have been published. In reaction to the wide variation in the quality of these dictionaries, this article attempts to articulate the beginnings of a <b>rubric</b> <b>for</b> the <b>evaluation</b> of English-Spanish legal dictionaries, borrowing from Bryan Garner’s work with legal dictionaries, then turning to the literature evaluating bilingual dictionaries and bilingual legal dictionaries. The article concludes with an annotated bibliography of major titles in this narrow, but increasingly significant, field...|$|R
40|$|Short stature is {{a common}} {{indication}} <b>for</b> genetic <b>evaluation.</b> The differential diagnosis is broad and includes both pathologic causes of short stature and nonpathologic causes. The purpose of genetic <b>evaluation</b> <b>for</b> short stature is to provide accurate diagnosis for medical management and to provide prognosis and recurrence risk counseling for the patient and family. There is no evidence-based data to guide the geneticist in an efficient, cost-effective approach to the evaluation of a patient with short stature. This guideline provides a <b>rubric</b> <b>for</b> the <b>evaluation</b> of short stature evaluation and summarizes common diagnoses and clinical testing available...|$|R
40|$|Describes the Chemistry Department's {{assessment}} {{activities for}} the academic year 2010 - 2011 The Chemistry and Biochemistry Department's annual assessment report to the College for the Office of Academic Assessment. The report details the assessment plan and related activities. The following assessment activities were planned for this year: implement <b>rubrics</b> <b>for</b> the assessment of (SLO 4 b), develop a <b>rubric</b> <b>for</b> the <b>evaluation</b> of written research/scientific reports (SLO 2, 3), assess basic knowledge in biochemistry and general chemistry (SLO 1) using standardized exam questions in course finals, review results from {{the implementation of a}} <b>rubric</b> <b>for</b> the assessment of (SLO 2), assess students??? ability to perform Quantitative Chemical Analysis (SLO 6), and evaluate previously developed and implemented Personnel Procedures for non-tenure track faculty...|$|R
40|$|Information {{literacy}} instruction {{presents a}} difficult balance between quantity and quality, particularly for large-scale general education courses. This paper discusses the {{overhaul of the}} freshman composition instruction program at the University of Maryland Libraries, focusing on the transition from survey assessments to a student-centered and mixed-methods approach using qualitative reflections, rubrics, and the evaluation of student artifacts. The article discusses the progression from a pilot assessment program using Twitter as a data collection model {{to the implementation of}} a robust and multi-layered assessment using both qualitative feedback from students and the evaluation of student artifacts. Each assessment includes detailed collection methods and customized <b>rubrics</b> <b>for</b> <b>evaluation</b> of student responses. While information literacy assessment has been covered extensively in the literature, few articles discuss the use of qualitative student responses on a large scale (4, 000 participants per year). The article also discusses the re-structuring of an assessment program around the ACRL Framework for Information Literacy, which is incorporated throughout the project from the pilot up through the full implementation of the final program...|$|R
40|$|For {{schools in}} School Improvement, submit the plan with the state’s <b>Rubric</b> <b>for</b> the <b>Evaluation</b> of School Improvement Plans Summary Report on disk to the {{designated}} {{division of the}} LDE, if required. Mail the Cover Page, District Assurance, and Faculty Assurance. Use 11 point font. Insert page numbers in the Table of Contents. For SIPs that have been revised, indicate material that has changed on the Strategy Planning Worksheet with strikethroughs (lines inserted through the changes). Place revisions in bold after the strikethroughs. For any completed activity, write the word completed in parenthesis following the strikethroughs...|$|R
40|$|Some {{forms of}} {{scholarly}} productivity, such as peer-reviewed publications, are easily recognized and incorporated into processes involving evaluation, retention, {{and promotion of}} faculty. A method for initiating peer review of unpublished scholarly activity may serve to permit recognition of such work in faculty evaluation. This article shares an instrument for the peer review of unpublished scholarship, such as scholarship of integration or teaching. A nonquantitative <b>rubric</b> <b>for</b> the <b>evaluation</b> of scholarly activity was developed, based on previously proposed standards from the Carnegie Foundation for the Advancement of Teaching. Such a process for forms of scholarly productivity other than publication provides potential for intellectual growth and development for both reviewers and reviewed facult...|$|R
40|$|This site {{contains}} an {{activity in which}} {{students are asked to}} use their Earth-Moon-Sun (EMS) model to solve one of several challenge problems. Each problem contains data about celestial or seasonal phenomena that students have not encountered before. The challenge for the students is to use their EMS model to explain these new scenarios. In this final activity, the students will work in groups of two or three to produce a single product, poster that answers all aspects of the particular challenge question posed and provides both written and graphical justification. This site includes instructional notes for the teacher, eight student challenge questions with details, a <b>rubric</b> <b>for</b> poster <b>evaluation,</b> and links to other resources. Educational levels: High school, Middle school...|$|R
40|$|Elizabeth Braaksma, Vera Armann-Keown, and Michele Piercey-Normore. Undergraduate {{research}} {{has historically been}} an integral component of the educational experience in the Faculty of Science at the University of Manitoba, but rapidly changing science-related disciplines pose unique challenges to identify, evaluate, acquire, and use information. Students are required to demonstrate competency in research papers and conduct laboratory research. Information is scrutinized by anonymous reviewers and is exposed to ethical and legal ramifications. A model was developed to implement information literacy (IL) into existing programs that already have a strong research foundation using the ACRL (Association of College and Research Libraries) standards for Science and Engineering. The model aligns both sets of learning outcomes (information literacy and discipline-specific), and provides sample exercises with <b>rubrics</b> <b>for</b> <b>evaluation.</b> The integration of IL learning outcomes {{within the context of}} a discipline in which the student has an interest, enables a more powerful learning environment than if the outcomes were separated. The program ensures the five IL competency standards are met at each level of a four-year degree, and that students take responsibility for their own success resulting in greater retention throughout their programs and into their future careers. The implications are that the IL integration will provide the tools necessary to help students remain within and successfully complete their academic programs; and gain added value to their knowledge and skills that can be extended into society or to a graduate degree and beyond...|$|R
40|$|Describes the Chemistry Department's {{assessment}} {{activities for}} the academic year 2007 _ 2008 The Chemistry and Biochemistry Department's annual assessment report to the College for the Office of Academic Assessment. The report details {{the development of a}} formal review process for non-tenure track faculty, the assessment of SLO 1 (demonstrate basic knowlegde in the area of organic chemistry) and SLO 4 (related to competence in laboratory activities), and <b>rubric</b> progress <b>for</b> the <b>evaluation</b> of written project reports, undergraduate research reports and classroom presentations (SLO 2 & 3...|$|R
40|$|Describes the Chemistry Department's {{assessment}} {{activities for}} the academic year 2011 - 2012 The Chemistry and Biochemistry Department's annual assessment report to the College for the Office of Academic Assessment. The following assessment activities were planned for this year: a. Implement a <b>rubric</b> <b>for</b> the <b>evaluation</b> of written research/scientific reports (SLO 2, 3). b. Assess basic knowledge in general chemistry and organic chemistry (SLO 1) using standardized exam questions in course finals. c. Review evidence pertaining to SLO 2 m: Organize and communicate scientific information clearly and concisely, both verbally and in writing. d. Review evidence pertaining to SLO 4 : work effectively and safely in a laboratory environment. In addition, several years ago the Department developed and adopted a formal set of procedures for evaluating the teaching effectiveness of non???tenure track faculty. The department continues to thoroughly review and advise all non???tenure track faculty who receive teaching assignments from the Department...|$|R
40|$|Describes the English Literature {{assessment}} {{activities for}} the academic year 2012 - 2013. The English Literature annual assessment report to the College for the Office of Academic Assessment. The committee overseeing the department???s Literature Option assessed Common Undergraduate Student Learning Outcome (SLO) # 3. The committee drafted a Common <b>Rubric</b> <b>for</b> the <b>evaluation</b> of this SLO. The committee selected 9 student essays, at random, from a Fall 2012 section of the Option???s Capstone Course, English 495 : Senior Seminar in English. The committee also completed the supplementary task from the 5 year Plan??????review of assessment materials for usefulness & flexibility??????in its ratification of the procedure for assessing SLO # 3. In addition to our assessment of SLO # 3, the committee (acting on the recommendation in last year???s committee???s assessment report), examined the Literature Option???s gateway course, English 355 : Writing About Literature, {{in order to address}} any ???potential disconnects??? between the way the course was taught across all sections and/or between the course and upper-division Literature courses...|$|R
40|$|Coming of age for a {{consulting}} company: An entrepreneurial Brumagim [2010] developed an entrepreneurship case (The Best Backgammon, Inc. case) {{that can be}} used for assessment of learning (AOL) to support AACSB international accreditation. The case developed in this paper is also in the area of entrepreneurship and is also an AOL business environment (management consulting versus manufacturing) and decision context (entrepreneurial transition versus startup issues). The case involves two principals in a private consulti various strategic alternatives, including whether to take their company “public” have to analyze and evaluate strategic alternatives available to the firm, and specifically, the advantages and disadvantages of becoming to decide the best course of action and to provide their reasons. real-world situation of a small, but successful, private consulting firm that has reached a point where a strategic assessment of the firm’s future is appropriate. that students can gain exposure to a real The student evaluation is based on extensiveness of literature search, creativity in developing different transition strategies, coherent and professionalism in style of writing. <b>Rubrics</b> <b>for</b> student <b>evaluation</b> are provided. The case can be used as the entrepreneurship component of a senior level capstone course i entrepreneurship strategy course...|$|R
40|$|Laurie H. Seaver, MD 1, 2, and Mira Irons, MD 3, {{on behalf}} of the American College of Medical Genetics (ACMG) Professional Practice and Guidelines Committee Disclaimer: This {{guideline}} is designed primarily as an educational resource for health care providers to help them provide quality medical genetic services. Adherence to this guideline does not necessarily assure a successful medical outcome. This guideline should not be considered inclusive of all proper procedures and tests or exclusive of other procedures and tests that are reasonably directed to obtaining the same results. In determining the propriety of any specific procedure or test, the geneticist should apply his or her own professional judgment to the specific clinical circumstances presented by the individual patient or specimen. It may be prudent, however, to document in the patient’s record the rationale for any significant deviation from this guideline. Abstract: Short stature is a common indication <b>for</b> genetic <b>evaluation.</b> The differential diagnosis is broad and includes both pathologic causes of short stature and nonpathologic causes. The purpose of genetic <b>evaluation</b> <b>for</b> short stature is to provide accurate diagnosis for medical management and to provide prognosis and recurrence risk counseling for the patient and family. There is no evidence-based data to guide the geneticist in an efficient, cost-effective approach to the evaluation of a patient with short stature. This guideline provides a <b>rubric</b> <b>for</b> the <b>evaluation</b> of short stature evaluation and summarizes common diagnoses and clinical testing available. Genet Med 2009 : 11 (6) : 465 – 470. Key Words: short stature, skeletal dysplasia, intrauterine growth restriction OBJECTIV...|$|R
40|$|In {{this paper}} the authors explore {{the use of}} <b>rubrics</b> <b>for</b> the <b>evaluation</b> of collaborations and its agents in both {{academic}} and practical settings. Rubrics are subjective scoring guides used <b>for</b> the quick <b>evaluation</b> {{of the characteristics of}} a concept based on a range of criteria. A comparative analysis of these rubrics suggest that collaborations and the collaborators are inconsistently evaluated based on the current design of these metrics. This inconsistency is captured through the choice of characteristics pertaining to collaboration, the use of these characteristics across <b>rubrics</b> designed <b>for</b> evaluating collaboration, the criteria pertaining to each characteristic, and the distribution of this range of criteria. This inconsistency misinforms the collaborators, misdirects the collaboration, distracts from appreciating the possibilities of collaboration through its involvement and transference of its lessons. It is suggested that the source of this error extends from an inconsistency in the understanding of collaboration and of the behaviours and attitudes expected of the collaborator. It is further suggested that value systems underlie one's attitudes toward collaboration, including which collaborative behaviours are viewed favourably, and {{a better understanding of the}} underlying values will help address the above-noted inconsistencies. An alternate rubric design will be proposed which will reflect a short list of favourable behaviours and attitudes pertaining to the value system of collaboration. The aim is to at least capture the context in which only a collaboration can exist, or at least, its opponents cannot. As opposed to the current designs <b>for</b> collaboration <b>rubrics,</b> it is believed that this alternative design will enable the evaluator to capture the presence of a collaboration and the strength of its agents. In addition, it will provide an appropriate direction and guidance in areas pertaining to collaboration, its related technologies, and this provides the opportunity for further exploration of collaborative activities, its benefits, and challenges...|$|R
40|$|This paper {{reported}} on industry involvement in Undergraduate Research Project (URP) course. During {{the final year}} of study, Chemical Engineering (Biotechnology) (BKB) students were required to conduct a final year project. Undergraduate Research Project (URP) was divided into two which were URP I and URP II. In URP I, students have to prepare a research proposal and URP II was the continuation of URP I. In URP II the students were required to conduct the research and produce the dissertation. <b>Rubrics</b> <b>for</b> each <b>evaluation</b> have been developed to clearly define the assessments. In order to ensure a strong connection between students, staff and industry, BKB students were required to have at least 15 % industrially linked/based URP topics. Starting from 2012, the faculty has invited external examiners from research institution and industries to assess students during their URP II presentation seminar. The presentation seminar was successfully conducted with the involvement of 14 industrial academic panels. The panels came from various industries, research institutes and consultation companies such as Petronas, MTBE Malaysia, Bio tropics Malaysia, MOX Gases, FRIM and Sime Darby. Their recommendations and suggestions during the presentation projects have benefited all students and lecturer towards improving their research works. Other than that, the requirement to have at least 15 % industrially linked URP topics has strengthened the relationship between the faculty and industry. In addition, several URP projects has been sent for research exhibition, internal and externally...|$|R
40|$|The Science Threshold Learning Outcomes (TLOs) {{developed}} {{recently as}} part of the Learning and Teaching Academic Standards project, reinforce that the ability to develop evidence-based, well-reasoned arguments and to clearly communicate those arguments in a variety of communication modes, are key graduate attributes (Jones, Yates & Kelder, 2011). However, in practice, specific measurement of these skills is limited, particularly in oral presentations. This study describes the initial literature-based development of a <b>rubric</b> <b>for</b> the <b>evaluation</b> of scientific argument in oral presentations (Toulmin, 1958; Sampson, Grooms & Walker, 2009), and the reiterative, data-driven process of refinement of that rubric. The rubric reflects the established framework for the scientific argument, by including criteria for claim, evidence and reasoning, and evaluates these three components across standards that represent the variation within a mid-level undergraduate cohort. Using this rubric, we evaluated the ability of undergraduate science students to communicate scientific arguments in an oral presentation task in which they presented data acquired from an inquiry-based practical (Bugarcic, Zimbardi, Macaranas & Thorn, 2012). Students demonstrated the ability to make claims, supply evidence and articulate reasoning that linked claims with supporting evidence. However, the standard of these elements was varied, and the structure of students’ arguments was not always complete. Using an action-research approach, these initial findings were used to develop student guidelines and alter the curriculum in a subsequent iteration of the course. This intervention resulted in students presenting more complete and higher-quality arguments. Overall, this study reports {{on the development of the}} rubric and describes the design and impact of an evidence-driven teaching intervention that enhances students’ scientific argument development in oral presentations...|$|R
40|$|This is {{a series}} of <b>rubrics</b> <b>for</b> {{assessing}} student performance in a variety of science activities. These include <b>rubrics</b> <b>for</b> experimental designs, experimental reports, science writing samples, labs, open-ended questions, science fair projects and data analysis. Scales for scoring specific elements of each are provided. Educational levels: High school...|$|R
40|$|Purpose: The {{main purpose}} of the {{research}} was to measure {{reliability and validity of}} the Scoring <b>Rubric</b> <b>for</b> Information Literacy (Van Helvoort, 2010). Design/methodology/approach: Percentages of agreement and Intraclass Correlation were used to describe interrater reliability. For the determination of construct validity, factor analysis and reliability analysis were used. Criterion validity was calculated with Pearson correlations. Findings: In the described case, the Scoring <b>Rubric</b> <b>for</b> Information Literacy appears to be a reliable and valid instrument for the assessment of information literate performance. Originality/value: Reliability and validity are prerequisites to recommend a <b>rubric</b> <b>for</b> application. The results confirm that this Scoring <b>Rubric</b> <b>for</b> Information Literacy can be used in courses in higher education, not only for assessment purposes but also to foster learning. Oorspronkelijke artikel bij Emerald te vinden bij [URL]...|$|R
40|$|In {{the process}} of {{analyzing}} students’ explanations and predictions for interaction between brightness enhancement ﬁlm and beam of white light, a need for objective and reliable assessment instrumentarose. Consequently, we developed a codingscheme that was mostly inspired by the <b>rubrics</b> <b>for</b> self-assessment of scientiﬁc abilities. In the paper we present the grading categories that were integrated in the coding scheme, and descriptions of criteria used <b>for</b> <b>evaluation</b> of students work. We report the results of reliability analysis of new assessment tool and present some examples of its application...|$|R
2500|$|Authors {{sometimes}} employ <b>rubrics</b> <b>for</b> selecting {{she or he}} such as: ...|$|R
40|$|The {{value of}} {{project-based}} learning has {{lead to the}} inclusion of project development activities in engineering courses, being the Final Year Project (FYP) the most remarkable one. Several approaches have been proposed for assessing and grading FYPs but, among them, rubrics are becoming a standard for such type of assessment. However, due to the different characteristics and orientations of the projects (some are more practically oriented, some more theoretically), and the high amount of different competences to be evaluated (knowledge, working capability, communication skills, etc.), the definition of one unique <b>rubric</b> suitable <b>for</b> the <b>evaluation</b> of all FYPs presented in different degree programs, is a big challenge. In a former work, the educational outcomes expected from the FYP were defined and resulted in a proposal for their assessment. Afterwards, the proposal has been tested during one year within an educational innovation-project at the Universidad Politécnica de Madrid which involved the follow-up of 8 undergraduate telecommunication students elaborating their FYPs. In this publication, our experience will be described, based on the emerging work taking place through the formalisation of the process which consisted in the following steps: i) establishment of a schedule for the whole process (publication of FYPs topics, selection of applying students and their enrolment, assignation of a jury to each FYP, elaboration and follow-up of FYPs, final report submission, oral presentation, etc.); ii) design of <b>rubrics</b> <b>for</b> each of three assessment parts: working process, final report and oral presentation; and iii) follow-up and evaluation of the involved FYPs. Finally, problems that appeared during this experience (e. g. administrative aspects, criticisms and suggestions from the students, tutors and juries involved) are discussed and some modifications in the assessment system will be proposed in order to solve or minimize these problems...|$|R
50|$|The <b>rubrics</b> <b>for</b> the Mass {{are found}} in Ordo I, VII, IX-X, XV-XVII.|$|R
40|$|Objective: The {{research}} {{sought to}} establish a <b>rubric</b> <b>for</b> evaluating evidence-based medicine (EBM) pointof- care tools in a health sciences library. Methods: The authors searched the literature <b>for</b> EBM tool <b>evaluations</b> and found that most previous reviews were designed to evaluate the ability of an EBM tool to answer a clinical question. The researchers' goal was to develop and complete <b>rubrics</b> <b>for</b> assessing these tools based on criteria <b>for</b> a general <b>evaluation</b> of tools (reviewing content, search options, quality control, and grading) and criteria <b>for</b> an <b>evaluation</b> of clinical summaries (searching tools for treatments of common diagnoses and evaluating summaries for quality control). Results: Differences between EBM tools' options, content coverage, and usability were minimal. However, the products' methods for locating and grading evidence varied widely in transparency and process. Conclusions: As EBM tools are constantly updating and evolving, evaluation of these tools needs to be conducted frequently. Standards for evaluating EBM tools need to be established, with one method being the use of objective rubrics. In addition, EBM tools need to provide more information about authorship, reviewers, methods for evidence collection, and grading system employed...|$|R
50|$|The Methodist Episcopal Church {{contained}} a <b>rubric</b> <b>for</b> eliminated the formulation for the proclaiming the banns around 1864.|$|R
50|$|The Yemen Model {{has been}} the <b>rubric</b> <b>for</b> the Obama Administration's {{attempts}} to neutralize foreign terrorist groups hostile to the United States.|$|R
40|$|The {{study focused}} on the {{evaluation}} of PhD dissertations on education in Pakistan and the comparison of dissertations conducted under HEC and UGC. The objectives of the research were; to explore the quality of research in Pakistan, to compare the quality of research of HEC and UGC, and to design instrument <b>for</b> the <b>evaluation</b> of research dissertations. All the PhD dissertations of social sciences identified the target population and from that population the researcher selected education discipline. Total PhD dissertations of education consist of 308, the researcher further delimited the study to all those dissertations that were downloadable with required permissions and fulfilled the evaluation criteria of the rubric. After the exclusion of dissertations, a total 178 dissertation were left <b>for</b> <b>evaluation</b> on the <b>rubric.</b> The researcher adapted and developed the rubric from different <b>rubrics</b> developed <b>for</b> <b>evaluation</b> of dissertations. The researcher collected data using 20 points instrument sheet with five pointâ€™s categorical scale, namely Excellent, Good, Satisfactory, Unsatisfactory, and Not included scale. For every dissertation one sheet was used, of the 178 dissertations, 131 were from HEC and 47 were from UGC. For the analysis of data the researcher used two methods, one was percentage while the other was statistical software {{to test the hypothesis}} which posed in the introduction section. The result of the study showed that in general the performance of HEC dissertation is slightly better than UGC; but statistically the result is not significant. The item wise result shows that there is significant difference on the majority items that prove the hypothesis that the research quality improved after the establishment of HEC in a short span of time of 10 Years whereas the number of dissertations increased by more than doubled in just 10 years. Before the establishment of HEC, only one dissertation touched the level of excellent whereas after the establishment of HEC, 10 dissertations were placed in excellent category while a substantial number of dissertations were in good category very close to excellent in a short span of time. The researcher suggests that there should be anti-plagiarism section in every department to avert plagiarism. Every faculty member should be provided anti-plagiarism software to eliminate the chances of plagiarism. The researcher also suggests that HEC should make it compulsory for every research department to publish their research journal. The researcher also feels the need of research on the evaluation of research journals in Pakistan...|$|R
50|$|The Roman Missal (Missale Romanum) is {{the liturgical}} book that {{contains}} the texts and <b>rubrics</b> <b>for</b> {{the celebration of the}} Mass in the Roman Rite of the Catholic Church.|$|R
