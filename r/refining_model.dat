18|4967|Public
40|$|The {{development}} of {{a model of the}} World Refining for the POLES model (Contract n° 151559 - 2009 A 08 FR – with the Joint Research Centre Institute for Prospective Technological Studies of Commission of the European Communities) aims to represent the oil product's supply at a world-wide level in a global energy model. The World oil refining industry faces to several challenges such as the increasing oil derivatives demand in the transport sector, the improvement of the specifications of these products, the crude oil availability and the limitation of carbon emissions. An aggregated <b>refining</b> <b>model</b> linked to the POLES energy model has been developed to study these questions. The OURSE (Oil is Used in Refineries to Supply Energy) model is a world-wide aggregated <b>refining</b> <b>model</b> which is designed to simulate the world oil product supply for the POLES (Prospective Outlook for the Long-term Energy System) model. OURSE is able to simulate the impact on the world refining industry of changes in the crude oil supply (in costs and qualities) {{as well as in the}} oil product demand (in terms of level, structure and specifications). OURSE also enables to assess the consequences of a carbon emission regulation (caps and taxes) as the adoption of various kinds of alternative fuel policies. More precisely, these impacts are evaluated as regards the world refining structure (investments), but also its balance (production and trade of petroleum products), its pollutant emissions (CO 2 and SO 2) and its costs (of production, investments, etc.). Simulations for 2030 were performed. Thus, the paper presents the results of a prospective exercise for the oil refining industry which has been carried out with the worldwide <b>refining</b> <b>model</b> OURSE according to the oil product demand projections of European Commission for Europe with the PRIMES model (European Commission, 2010) and the IFP projections for the rest of the World. JRC. J. 1 -Economics of Climate Change, Energy and Transpor...|$|E
40|$|International audienceA {{problem of}} {{quasi-static}} growth of an arbitrary shaped-crack along an interface requires many times of iterations {{not only for}} finding a spatial distribution of discontinuity but also for determining the crack tip. This is crucial when <b>refining</b> <b>model</b> resolution and also when the phenomena progresses quickly from one step to another. We propose a mathematical reformu-lation of the problem as a nonlinear equation and adopt different numerical methods to solve it efficiently. Compared to a previous work of the authors, the resulting code shows a great improvement of performance. This gain is important for further application of aseismic slip process along the fault interface, {{in the context of}} plate convergence as well as the reactivation of fault systems in reservoirs...|$|E
40|$|Correctness {{of model}} transformations is a {{prerequisite}} for generating correct implementations from models. Given <b>refining</b> <b>model</b> transformations that preserve desirable properties, models can be transformed into correct-by-construction implementations. However, proving that model transformations preserve properties is far from trivial. Therefore, we aim for simple correctness proofs by designing model transformations that are as fine-grained as possible. Furthermore, we advocate the reuse of model transformations {{to reduce the number of}} proofs. For a simple domain-specific language, SLCO, we define a formal framework to reason about the correctness, reusability, and composition of the fine-grained model transformations used to transform a given model to three target languages: NQC, Promela and POOSL. The correctness criterion induces that the original model and the resulting model obtained after a proper sequence of transformations have the same observable behavior...|$|E
40|$|The {{purpose of}} the work {{described}} in this thesis is to decrease the computational cost of <b>refined</b> plate/shell <b>models</b> without loosing accuracy in the plate/shell response analysis. The axiomatic/asymptotic technique has been employed for this purpose. The <b>refined</b> <b>models</b> have been obtained {{by means of the}} Carrera Unified Formulation and both the Principle of Virtual Displacement (PVD) and the Reissner Mixed Variational Theorem (RMVT) have been employed to derive the governing equations. Mechanical and multifield problems are considered. Navier-like closed form solutions has been employed, and for this reason simply supported isotropic and orthotropic plates and shells has been considered. The <b>refined</b> <b>models</b> analyzed have been implemented according to the schemes known as Equivalent Single Layer and Layer Wise. In some case, the accuracy of the reduced <b>refined</b> <b>models</b> have been compared to the <b>refined</b> <b>models</b> available in the open scientific literatur...|$|R
50|$|More <b>refined</b> <b>models</b> {{of random}} {{groups have been}} defined.|$|R
50|$|This {{procedure}} is repeated a fixed number of times, each time producing either a model which is rejected because too few points {{are part of}} the consensus set, or a <b>refined</b> <b>model</b> together with a corresponding consensus set size. In the latter case, we keep the <b>refined</b> <b>model</b> if its consensus set is larger than the previously saved model.|$|R
40|$|We {{present a}} {{technique}} for reconstructing biomolecular structures from scanning force microscope data. The technique works by iteratively <b>refining</b> <b>model</b> molecules by comparison of simulated and experimental images. It can remove instrument artifacts to yield accurate dimensional measurements from tip-broadened data. The {{result of the}} reconstruction is a model that can be chosen to include the physically significant parameters for the system at hand. We demonstrate this by reconstructing scanning force microscope images of the cartilage proteoglycan aggrecan. By explicitly including the protein backbone in the model, {{we are able to}} associate measured three-dimensional structures with sites in the protein primary structure. The distribution of aggrecan core protein lengths that we measure suggests that 48 % of aggrecan molecules found in vivo have been partially catabolized at either the E(1480) -(1481) G or E(1667) -(1668) G aggrecanase cleavage site. ...|$|E
40|$|Abstract. The {{testing and}} formal {{verification}} of black box software components is a challenging domain. The problem is even harder when specifications of these components are not available. An approach {{to cope with}} this problem is to combine testing with learning techniques, such that the learned models of the components can be used to explore unknown implementation and thus facilitate testing efforts. In recent years, we have contributed to this approach by proposing techniques for learning parameterized state machine models and then use them in the integration testing of black box components. The major problem in this technique left unaddressed was the selection of parameter values during the learning process. In this paper, we propose to use an invariant detection mechanism to select values in the learning process, thus <b>refining</b> <b>model</b> inference and testing approach. Initial experiments with small examples yielded positive results. ...|$|E
40|$|Large scale {{systems are}} {{typically}} quite difficult to model. Hierarchical decomposition {{has proven to}} be one successful method in managing model complexity, by <b>refining</b> <b>model</b> components into models of the same type as the lumped model. Many systems, however, cannot be modeled using this approach since each abstraction level is best defined using a different modeling technique. We present a multimodel approach which overcomes this limitation, and we illustrate the technique using a fairly simple scenario: boiling water. State and phase trajectories are presented along with an implementation using the SimPack simulation toolkit. Multimodeling has provided us with a mechanism for building models that are capable of producing answers over a wide range of fidelity. [Key Words: Multimodeling, Process Abstraction, Combined Simulation, Phase Transitions, Simulation Environment] 1 Introduction Modeling "in the large" has always been a central topic in computer simulation. Approaches in past wor [...] ...|$|E
25|$|Hence {{some more}} <b>refined</b> <b>models</b> are necessary. Such {{models have been}} {{developed}} recently.|$|R
5000|$|Its dynamic {{tension and}} unusually <b>refined</b> <b>modelling</b> place it among sculptures of the [...]|$|R
40|$|In this Chapter {{we present}} the basic {{experimental}} facts on masonry materials and introduce simple and <b>refined</b> <b>models</b> for masonry. The simple models are essentially macroscopic {{and based on}} the assumption that the material is incapable of sustaining tensile loads (No-Tension assumption). The <b>refined</b> <b>models</b> account for the microscopic structure of masonry, modeling the interaction between the blocks and the interfaces...|$|R
40|$|The seismic data {{processing}} has aim {{to get the}} real structure of seismic section. The quality of seismic image after pre-stack depth migration in area which has geological complexcity depends on accuracy of velocity model. The Residual Curvature Analysis Tomography (RCA Tomography) which one prosesses method for updating interval velocity model. The Residual Curvature Analysis Tomography in <b>refining</b> <b>model</b> of interval velocity at first layer, that is by doing CIG picks at first layer continued with CIG picks of all layer. Result from CIG picks used as an input for RCA Tomography process. If refinement of interval velocity model have come up at each layer hence RCA Tomography process will refine interval velocity model for all layer. Result {{of this research is}} interval velocity model at seismic cross section with the image of under earth surface which clearer at survey area “X”. At survey area “X” with crossline 962 and inline 1 there are fault area an anticline such as which seen at target area...|$|E
40|$|As oil {{refining}} is a multiproduct industrial activity, there are innumerable ways to allocate a refinery's CO 2 emissions {{among the various}} refined products. The linear-programming models used to manage refineries may serve to compute the marginal contribution of each finished product to the CO 2 emissions of the refinery. We show that, under some conditions, this marginal contribution is a relevant means of allocating all the refinery's CO 2 emissions. The application of this allocation rule leads to interesting results {{which can be used}} in a well-to-wheel life cycle assessment. In fact, this allocation rule holds rigorously if the demand equations are the only binding constraints with a non-zero right-hand side coefficient. This is certainly not the case for short-run models with fixed capacity of processing units. To extend the application field of our approach, we therefore suggest three distinct solutions, inspired by economic theory: applying the Aumann-Shapley cost-sharing method, or adapting the Ramsey pricing-formula, or using proportionally-adjusted marginal contributions. A numerical application to a simplified <b>refining</b> <b>model</b> is presented. Life-cycle assessment CO 2 emissions Oil refining Linear programming Aumann-Shapley Ramsey pricing...|$|E
40|$|With the {{development}} of social media and social networks, user-generated content, like forums, blogs and comments, are not only getting richer, but also ubiquitously interconnected with many other objects and entities, forming a heterogeneous information network between them. Sentiment analysis on such kinds of data can no longer ignore the information network, since it carries a lot of rich and valuable information, explicitly or implicitly, where some {{of them can be}} observed while others are not. In this paper, we propose a novel information network-based framework which can infer hidden similarity and dissimilarity between users by exploring similar and opposite opinions, so as to improve postlevel and user-level sentiment classification in the same time. More specifically, we develop a new meta path-based measure for inferring pseudo-friendship as well as dissimilarity between users, and propose a semi-supervised <b>refining</b> <b>model</b> by encoding similarity and dissimilarity from both user-level and post-level relations. We extensively evaluate the proposed approach and compare with several state-of-the-art techniques on two real-world forum datasets. Experimental results show that our proposed model with 10. 5 % labeled samples can achieve better performance than a traditional supervised model trained on 61. 7 % data samples. ...|$|E
40|$|Coordinates are {{presented}} for a <b>refined</b> <b>model</b> of the side-by-side (SBS) {{structure for the}} in vivo B-form of duplex DNA. Bond lengths and bond angles are within 0. 025 ^ and 2. 0 ° of standard values {{and there are no}} non-bonded intramolecular contacts shorter than 2. 75 A. These and other details of the <b>refined</b> <b>model</b> indicate that the SBS proposal is stereochemically viable...|$|R
5000|$|... s: some {{algorithms}} do {{not provide}} a <b>refined</b> <b>model</b> for their results and just provide the grouping information.|$|R
5|$|In {{the third}} step, {{these data are}} {{combined}} computationally with complementary chemical information to produce and <b>refine</b> a <b>model</b> of the arrangement of atoms within the crystal. The final, <b>refined</b> <b>model</b> of the atomic arrangement—now called a crystal structure—is usually stored in a public database.|$|R
40|$|This work {{presents}} {{the selection of}} a set of geostatistical coefficients suitable for a unified SRTM data refinement from 3 &# 8243; to 1 &# 8243; through kriging over the entire Brazilian territory. This selection aimed at data potential for geomorphometric derivations, given by the preservation of detailed geometric characteristics of the resulting digital elevation models (DEM), which are sensitive to refining procedures. The development contained a long-term experimentation stage, when data refinement through kriging was locally developed to support distinct regional projects, followed by a unified selection stage, where the acquired experience was applied to select a single and unified interpolation scheme. In this stage, the selected geostatistical models with promising performances were tested for unified refinement on 40 Brazilian areas with distinct geological settings. Tested areas encompass reliefs varying from mountainous to plain. The effects of data preparation were observed on the perception of patterns (texture and roughness), as well as of singularities (edges, peaks, thalwegs etc.). Results were evaluated mainly through the examination of shaded reliefs, transects and perspectives observed in different scales. Terrains with low slopes and small amplitudes had their DEM promptly affected by the refining methods, as opposed to mountainous terrains. The evaluation, unambiguously confirmed by all consulted interpreters, converged into a <b>refining</b> <b>model</b> with outstanding performance in all tested conditions. Pages: 300 - 30...|$|E
40|$|One of {{the main}} things in {{development}} of software systems, which receive a lot of attention, is the maintenance and security of the data. In the systems that {{have a lot of}} users such as the various social networks, data protection and management issues are highly relevant. At such a systems {{it is very important to}} ensure certain procedures, important that users post correct information and it is also important that the users of system would behave according to predetermined set of rules for the system. All these issues need to manage very large number of human resources, especially in the social networks of hundreds or thousands of users, it is difficult to control each user. In this paper all these issues are solved by using one of several access control models. After adjusting the model there was developed such a system, where simple users are becoming administrators and information publishers at the same time. System was created by the principle – the greater involvement of the user, the more system access rights it receives. However, an allocation of rights to consumers also requires a lot of administration, whereas it is necessary to verify the extent of the user, to carry out the proper or improper actions, and to grant or take away various system rights. These problems are solved in this work too, by <b>refining</b> <b>model</b> for the control of the distribution of rights, which automates the allocations of rights and at the same time this also reduces the time required by administration to a minimum...|$|E
40|$|A {{method has}} been {{developed}} to estimate the relative accident risk posed by different patterns of driving over a multi-day period. The procedure explicitly considers whether a driver is on-duty of off-duty for each half hour of each day {{during the period of}} analysis. From a data set of over 1000 drivers, 9 distinct driving patterns are identified. Membership in the patterns is determined exclusively by the pattern of duty hours for seven consecutive days; for some drivers and accident occurred on the eighth day while others had no accident. Therefore each pattern can be associated with a relative accident risk. Additional statistical modeling allowed consideration, in addition to driving pattern, of driver age, experience with the firm, hours off-duty prior to the last trip and hours driving on the last trip (either until the accident or successful completion of the trip). The findings of the modeling are that driving patterns over the previous seven days significantly affect accident risk on the eighth day. In general, driving during afternoon and evening hours (e. g. noon to midnight) has the highest accident risk while driving during night and morning hours (e. g. midnight to noon) has lower risk. Consecutive hours driven also has a significant effect on accident risk: the first hour of driving and the ninth and tenth hour of driving have the highest risk. Hours 2 through 8 follow a flattened "u" shape. Driver age and hours off-duty immediately prior to a trip do not appear to affect accident risk significantly. These findings quantitatively assess the relative accident risk of multi-day driving patterns using data from actual truck operations. Further research is recommended in the areas of <b>refining</b> <b>model</b> structures, adding explanatory variables (such as highway type) and testing more complex models...|$|E
40|$|The {{purpose of}} this study is to propose a newly <b>refined</b> <b>model</b> of {{relationship}} selling in the general context between meeting planners and suppliers in Meeting, Incentive, Convention, and Exhibition (MICE) industry. The <b>refined</b> <b>model</b> was tested by using an online survey of a sample of professional meeting planners in the Meeting Professionals International (MPI) and Professional Convention Management Association (PCMA). Results showed both social bonds with the supplier and expertise of the supplier had an effect on meeting planners’ perceived trust and satisfaction; while willingness and power of the supplier was related to neither meeting planners’ perceived trust nor satisfaction. This <b>refined</b> <b>model</b> could provide research insights and guide future research on key relationship selling constructs between meeting planners and suppliers in the MICE industry...|$|R
40|$|A <b>refined</b> <b>model</b> of {{a railway}} vehicle {{running on a}} {{straight}} track is developed. In this model the wheelsets and the rails are modelled as flexible bodies and a detailed model of the wheel-rail contact is used. The simulation results obtained with this model show a distinct impact of the structural deformations on the stress distributions in the contact. This <b>refined</b> <b>model</b> {{can serve as a}} base for investigations of rolling noise and wear...|$|R
40|$|In {{this paper}} {{we aim to}} {{estimate}} the differential student knowledge model in a probabilistic domain within an intelligent tutoring system. The student answers to questions requiring diagnosing skills are used to estimate the actual student model. Updating and verification of the model are conducted based on the matching between the student's and model answers. Two different approaches to updating are suggested, i) coarse and ii) <b>refined</b> <b>model</b> updating. Moreover, {{the effect of the}} order of which questions are presented to the student is investigated. Results suggest that the <b>refined</b> <b>model,</b> although takes more computational resources, provides a slightly better approximation of the student model. In addition, the accuracy of the algorithm is highly insensitive to the order of which the questions are presented, more so when using the <b>refined</b> <b>model</b> updating approach. ...|$|R
40|$|Drug-induced Torsade-de-Pointes (TdP) {{has been}} {{responsible}} for the withdrawal of many drugs from the market and is therefore of major concern to global regulatory agencies and the pharmaceutical industry. The Comprehensive in vitro Proarrhythmia Assay (CiPA) was proposed to improve prediction of TdP risk, using in silico models and in vitro multi-channel pharmacology data as integral parts of this initiative. Previously, we reported that combining dynamic interactions between drugs and the rapid delayed rectifier potassium current (IKr) with multi-channel pharmacology is important for TdP risk classification, and we modified the original O'Hara Rudy ventricular cell mathematical model to include a Markov model of IKr to represent dynamic drug-IKr interactions (IKr-dynamic ORd model). We also developed a novel metric that could separate drugs with different TdP liabilities at high concentrations based on total electronic charge carried by the major inward ionic currents during the action potential. In this study, we further optimized the IKr-dynamic ORd model by <b>refining</b> <b>model</b> parameters using published human cardiomyocyte experimental data under control and drug block conditions. Using this optimized model and manual patch clamp data, we developed an updated version of the metric that quantifies the net electronic charge carried by major inward and outward ionic currents during the steady state action potential, which could classify the level of drug-induced TdP risk across a wide range of concentrations and pacing rates. We also established a framework to quantitatively evaluate a system's robustness against the induction of early afterdepolarizations (EADs), and demonstrated that the new metric is correlated with the cell's robustness to the pro-EAD perturbation of IKr conductance reduction. In summary, in this work we present an optimized model that is more consistent with experimental data, an improved metric that can classify drugs at concentrations both near and higher than clinical exposure, and a physiological framework to check the relationship between a metric and EAD. These findings provide a solid foundation for using in silico models for the regulatory assessment of TdP risk under the CiPA paradigm...|$|E
40|$|Mountain {{regions are}} {{typically}} characterized by rugged terrain which {{is susceptible to}} different types of landslides during high-intensity precipitation. Landslides account for {{billions of dollars of}} damage and many casualties, and are expected to increase in frequency in the future due to a projected increase of precipitation intensity. Early warning systems (EWS) are thought to be a primary tool for related disaster risk reduction and climate change adaptation to extreme climatic events and hydro- meteorological hazards, including landslides. EWS are highly complex systems, and it is therefore dificult to understand the effect of the different components and changing conditions on the overall performance, ultimately being expressed as human lives saved or structural damage reduced. In this paper we utilize a numerical model using 6 hour rainfall data as basic input. A threshold function based on a rainfall-intensity/duration relation was applied as a decision criterion for evacuation. Correct evacuation was assessed with a 'true' reference rainfall dataset versus a dataset of artificially reduced quality imitating the observation system component. Performance of the EWS using these rainfall datasets was expressed in monetary terms (i. e. damage related to false and correct evacuation). We applied this model to a landslide EWS in Colombia that has been implemented in recent years within a disaster prevention project. We evaluated the EWS against rainfall data with artificially introduced error and computed with multiple model runs the probabilisic damage functions depending on rainfall error. Then we modified the original precipitation pattern to reflect possible climatic changes e. g. change in annual precipitatin as well as change in precipitation intensity with annual values remaining constant. We let the EWS model adapt for changed conditions to function optimally. Our resuts show that for the same errors in rainfall measurements the systems performance degrades with expected changing climatic conditions. The obtained results suggest that EWS cannot internally adapt to climate change and require exogenous adaptive measures to avoid increase in overall damage. The model represents a first attempt to integrally simulate and evaluate EWS under future possible climatic pressures. Future work will concentrate on <b>refining</b> <b>model</b> components and spatially explicit climate scenarios...|$|E
40|$|The {{objective}} of this thesis work was to examine different approaches to tailor chemical fibres of different raw materials. The focus in searching for new approaches was on pressure screen fractionation, selective treatment of each fraction, mechanical pre-treatment before refining, refiner loadability and its link to fibre properties and filling design, and on-line quality control of fibre properties. The evaluation {{is based on the}} impacts on fibre properties, filtration, refining and the resulting paper properties. Tailoring of fibres using pressure screen fractionation was found to produce a long and coarse (reject) fraction offering high dewatering efficiency, homogeneous and energy-efficient refining, and better strength properties, such as tear index, bulk and fracture toughness, in pure and mixed sheets with other fibres. Although the accept fraction contains short and thin fibres and has a high fines content, it proved possible to use the accept fraction to increase the bonding and scattering of once dried softwood and to reduce the refining energy input needed to reach a certain tensile strength level. A new mechanical pre-treatment was examined and found to promote lumen collapse and de-swelling of fibres, and hence to improve the strength-dewatering combination of softwood kraft pulp. The treatment involved application of linear loads, heat, and shear forces over multiple passes. In refining, the pre-treated fibres produced better dewatering and a consolidated structure with less cutting, fines creation and external fibrillation compared to never dried fibres. The pre-treated fibres offer better potential for developing a higher tensile index, stiffness and Scott bond than once dried fibres at a certain degree of refining. Refiner loadability and gap movement are strongly related to fibre properties and filling design. Fibre properties together with pulp consistency contribute to the size and strength of flocs building up inside the refiner, where big and strong flocs are loaded earlier and maintain a wide gap with less floc size changes. Here, pulp consistency was found to have a smaller effect than fibre properties. Filling design, reflected in the cutting edge speed, was found to contribute strongly to the gap movement and refiner loadability. An increase in edge cutting speed caused the refiner gap width to decrease linearly, thus enhancing different refining effects such as fibre cutting, fibrillation and fibre swelling. A factor network linking on-line measured fibre properties, calculated factors and predicted paper properties was found to be an effective tool for monitoring changes in pulp quality, such as different raw materials with different average fibre lengths. The model was built off-line and tested against on-line mill operation and found to be effective in predicting paper properties of both never and once dried pulp. The <b>refining</b> <b>model</b> was tested with a laboratory refiner and used to explain strength properties such as tensile index, to monitor changes in paper properties due to refining and to determine the optimum refining conditions and different refining effects such as bonding, straightening and fibre cutting...|$|E
40|$|Modeller Objective Function {{after the}} loop refinement. The Ramachandran {{plot of the}} <b>refined</b> <b>model</b> places more than 90 % of the {{residues}} in the most favored regions of the plot, indicating a good model. Analysis of the refined DOPE score profile, {{as compared to the}} profile of the template model, shows overall improvement. The score at almost every residue of the <b>refined</b> <b>model</b> is {{less than or equal to}} the score of the corresponding residue of the template model...|$|R
40|$|The {{objective}} of this investigation is to analyze the influence of trabecular microstructure modeling on the biomechanical distribution of the implant-bone interface. Two three-dimensional finite element mandible models, one with trabecular microstructure (a <b>refined</b> <b>model)</b> and one with macrostructure (a simplified model), were built. The values of equivalent stress at the implant-bone interface in the <b>refined</b> <b>model</b> increased {{compared with those of}} the simplified model and strain on the contrary. The distributions of stress and strain were more uniform in the <b>refined</b> <b>model</b> of trabecular microstructure, in which stress and strain were mainly concentrated in trabecular bone. It was concluded that simulation of trabecular bone microstructure had a significant effect on the distribution of stress and strain at the implant-bone interface. These results suggest that trabecular structures could disperse stress and strain and serve as load buffers...|$|R
50|$|HD 142527 {{is a star}} in the {{constellation}} of Lupus. It is notable for its protoplanetary disk and its discovery has helped <b>refine</b> <b>models</b> of planet formation.|$|R
40|$|This thesis {{details the}} {{development}} and calibration of a model created by coupling a land surface simulation model named CLASS with a hydrologic model named WATFLOOD. The resulting model, known as WatCLASS, is able {{to serve as a}} lower boundary for an atmospheric model. In addition, WatCLASS can act independently of an atmospheric model to simulate fluxes of energy and moisture from the land surface including streamflow. These flux outputs are generated based on conservation equations for both heat and moisture ensuring result continuity. WatCLASS has been tested over both the data rich BOREAS domains at fine scales and the large but data poor domain of the Mackenzie River at coarse scale. The results, while encouraging, point to errors in the model physics related primarily to soil moisture transport in partially frozen soils and permafrost. Now that a fully coupled model has been developed, {{there is a need for}} continued research by <b>refining</b> <b>model</b> processes and test WatCLASS's robustness using new datasets that are beginning to emerge. Hydrologic models provide a mechanism for the improvement of atmospheric simulation though two important mechanisms. First, atmospheric inputs to the land surface, such as rainfall and temperature, are transformed by vegetation and soil systems into outputs of energy and mass. One of these mass outputs, which have been routinely measured with a high degree of accuracy, is streamflow. Through the use of hydrologic simulations, inputs from atmospheric models may be transformed to streamflow to assess reliability of precipitation and temperature. In this situation, hydrologic models act in an analogous way to a large rain gauge whose surface area is that of a watershed. WatCLASS has been shown to be able to fulfill this task by simulating streamflow from atmospheric forcing data over multi-year simulation periods and the large domains necessary to allow integration with limited area atmospheric models. A second, more important, role exists for hydrologic models within atmospheric simulations. The earth's surface acts as a boundary condition for the atmosphere. Besides the output of streamflow, which is not often considered in atmospheric modeling, the earth's surface also outputs fluxes of energy in the form of evaporation, known as latent heat and near surface heating, known as sensible heat. By simulating streamflow and hence soil moisture over the land surface, hydrologic models, when properly enabled with both energy and water balance capabilities, can influence the apportioning of the relative quantities of latent and sensible heat flux that are required by atmospheric models. WatCLASS has shown that by improving streamflow simulations, evaporation amounts are reduced by approximately 70 % (1271 mm to 740 mm) during a three year simulation period in the BOREAS northern old black spruce site (NSA-OBS) as compared to the use of CLASS alone. To create a model that can act both as a lower boundary for the atmosphere and a hydrologic model, two choices are available. This model can be constructed from scratch with all the caveats and problems associated with proving a new model and having it accepted by the atmospheric community. An alternate mechanism, more likely to be successfully implemented, was chosen for the development of WatCLASS. Here, two proven and well tested models, WATFLOOD and CLASS, were coupled in a phased integration strategy that allowed development to proceed on model components independently. The ultimate goal of this implementation strategy, a fully coupled atmospheric - land surface - hydrologic model, was developed for MC 2 -CLASS-WATFLOOD. Initial testing of this model, over the Saguenay region of Quebec, has yet to show that adding WATFLOOD to CLASS produces significant impacts on atmospheric simulation. It is suspected, that this is due to the short term nature of the weather simulation that is dominated by initial conditions imposed on the atmospheric model during the data assimilation cycle. To model the hydrologic system, using the domain of an atmospheric model, requires that methods be developed to characterize land surface forms that influence hydrologic response. Methods, such as GRU (Grouped Response Unit) developed for WATFLOOD, need to be extended to taken advantage of alternate data forms, such as soil and topography, in a way that allows parameters to be selected a priori. Use of GIS (Geographical Information System) and large data bases to assist in development of these relationships has been started here. Some success in creating DEMs, (Digital Elevation Model) which are able to reproduce watershed areas, was achieved. These methods build on existing software implementations to include lake boundaries information as a topographic data source. Other data needs of hydrologic models will build on relationships between land cover, soil, and topography to assist in establishing grouping of these variables required to determine hydrologic similarity. This final aspect of the research is currently in its infancy but provides a platform from which to explore for future initiatives. Original contributions of this thesis are centered on the addition of a lateral flow generation mechanism within a land surface scheme. This addition has shown a positive impact on flux returns to the atmosphere when compared to measured values and also provide increased realism to the model since measured streamflow is reproduced. These contributions have been encapsulated into a computer model known as WatCLASS, which together with the implementation plan, as presented, should lead to future atmospheric simulation improvements...|$|E
40|$|A sensitivity-based linearly varying {{scale factor}} is {{described}} used to reconcile results from <b>refined</b> <b>models</b> {{for analysis of}} the same structure. The improved accuracy of the linear scale factor compared to a constant scale factor {{as well as the}} commonly used tangent approximation is demonstrated. A wing box structure is used as an example, with displacements, stresses, and frequencies correlated. The linear scale factor could permit the use of a simplified model in an optimization procedure during preliminary design to approximate the response given by a <b>refined</b> <b>model</b> over a considerable range of design changes...|$|R
40|$|Abstract. In {{this paper}} {{we aim to}} {{estimate}} the differential student knowledge model in a probabilistic domain within an intelligent tutoring system. The suggested algorithm aims to estimate the actual student model through the student answers to questions requiring diagnosing skills. Updating and verification of the model are conducted based on the matching between the student and model answers. Two different approaches to updating namely coarse and <b>refined</b> <b>model</b> are suggested. Results suggest that the <b>refined</b> <b>model,</b> although takes more computational resources, provides a slightly better approximation of the student model. ...|$|R
50|$|Extend the {{geologic}} {{mapping of}} {{the nucleus of}} Tempel 1 to elucidate the extent and nature of layering, and help <b>refine</b> <b>models</b> of the formation and structure of comet nuclei.|$|R
5000|$|In addition, recent {{standards}} (BPMN, BPEL, etc.) {{and their}} implementation technologies propose relevant integration capabilities. Furthermore, model driven-engineering [...] provides capabilities that connect, transform and <b>refine</b> <b>models</b> to support interoperability.|$|R
5000|$|The USAAF {{was very}} pleased with the <b>refined</b> <b>Model</b> 450 design, and in April 1946, the service ordered two prototypes, to be {{designated}} [...] "XB-47". Assembly began in June 1947.|$|R
