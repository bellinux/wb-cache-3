0|151|Public
50|$|<b>Front-end</b> <b>processors</b> have {{connections}} to various card associations and supply authorization and settlement {{services to the}} merchant banks’ merchants. Back-end processors accept settlements from <b>front-end</b> <b>processors</b> and, via The Federal Reserve Bank for example, move {{the money from the}} issuing bank to the merchant bank.|$|R
30|$|Zhao and Shao [20, 21] used CASA as a <b>front-end</b> <b>processor</b> for robust speaker {{identification}} (SID).|$|R
5000|$|PU4 nodes are <b>front-end</b> <b>processors</b> {{running the}} Network Control Program (NCP) {{such as the}} IBM 37xx series ...|$|R
50|$|The DATANET-30 was a {{computer}} manufactured by General Electric {{designed to be}} used as a <b>front-end</b> <b>processor</b> for data communications.|$|R
50|$|New in CP-6 {{was the use}} of {{communications}} and terminal interfaces through minicomputer (Honeywell Level 6)-based <b>front-end</b> <b>processors,</b> connected locally, remotely, or in combination.|$|R
50|$|MTS can {{and does}} use {{communication}} controllers {{such as the}} IBM 2703 and the Memorex 1270 to support dial-in terminals and remote batch stations over dial-in and dedicated data circuits, but these controllers proved to be fairly inflexible and unsatisfactory for connecting large numbers of diverse terminals and later personal computers running terminal emulation software at ever higher data rates. Most MTS sites choose {{to build their own}} <b>front-end</b> <b>processors</b> or to use a <b>front-end</b> <b>processor</b> developed by one of the other MTS sites to provide terminal support.|$|R
50|$|These <b>front-end</b> <b>processors,</b> usually DEC PDP-8, PDP-11, or LSI-11 based with locally {{developed}} custom {{hardware and}} software, {{would act as}} IBM control units attached to the IBM input/output channels {{on one side and}} to modems and phone lines on the other. At the University of Michigan the <b>front-end</b> <b>processor</b> was known as the Data Concentrator (DC). The DC was developed as part of the CONCOMP project by Dave Mills and others and was the first non-IBM device developed for attachment to an IBM I/O Channel. Initially a PDP-8 based system, the DC was upgraded to use PDP-11 hardware and a Remote Data Concentrator (RDC) was developed that used LSI-11 hardware that connected back to a DC over a synchronous data circuit. The University of British Columbia (UBC) developed two PDP-11 based systems: the Host Interface Machine (HIM) and the Network Interface Machine (NIM). The University of Alberta used a PDP-11 based <b>Front-end</b> <b>processor.</b>|$|R
50|$|The typical {{data flow}} in a DMS has the SCADA system, the Information Storage & Retrieval (ISR) system, Communication (COM) Servers, <b>Front-End</b> <b>Processors</b> (FEPs) & Field Remote Terminal Units (FRTUs).|$|R
50|$|All Connection Machine models {{required}} a serial <b>front-end</b> <b>processor,</b> which was most often a Sun Microsystems workstation, but on early models {{could also be}} a DEC VAXminicomputer or Symbolics LISP machine.|$|R
5000|$|Model 4 - {{attached}} to an IBM 1130 minicomputer via the storage access channel (SAC). The 1130 could run {{either as a}} standalone processor or as a <b>front-end</b> <b>processor</b> connected to a remote System/360.|$|R
40|$|A {{dissertation}} {{submitted to}} the Faculty of Engineering, University of the Witwctersrand, Johannesburg as a partial requirement tor the Degree of Master of Sciencc in Engineering. Johannesburg, 1984. This dissertation describes the design, {{development and implementation of}} the software for -i real-time data acquisition system. A microcomputer war employed as an intelligent <b>front-end</b> <b>processor</b> to a mult, user minicomputer system for the real-time input and output of analog signals. The data acquisition system, BADEDAS (Basic Analog and Digital Experimental Data Acquisition System), provides the users of an Eclipse minicomputer systen with an efficient means of acquiring digital samples of analog signals and disseminating digital samples as analog signals. BADEDAS exploits the concept of de Ice independent i/o to achieve an othogonal and flexible architecture. BADEDAS provides the choice of two user interfaces to the system and the ability to include simple user-written processing routines in the <b>front-end</b> <b>processor's</b> control software. The use of a microcomputer as a <b>front-end</b> <b>processor</b> removes the real-time I/O requirements from the minicomputer and also reduces the risk electrical damage to it...|$|R
40|$|An {{efficient}} storage format {{was developed}} for computer-generated holograms for use in electron-beam lithography. This method employs run-length encoding and Lempel-Ziv-Welch compression and succeeds in exposing holograms that were previously infeasible owing to the hologram 2 ̆ 7 s tremendous pattern-data file size. These holograms also require significant computation; thus the algorithm was implemented on a parallel computer, which improved performance by 2 orders of magnitude. The decompression algorithm was integrated into the Cambridge electron-beam machine 2 ̆ 7 s <b>front-end</b> <b>processor.</b> Although this provides much-needed ability, some hardware enhancements will be required {{in the future to}} overcome inadequacies in the current <b>front-end</b> <b>processor</b> that result in a lengthy exposure time...|$|R
50|$|In the 1980s, Honeywell's Datanet 8 line of {{communications}} processors, {{often used as}} <b>front-end</b> <b>processors</b> for DPS 8 mainframes, shared many hardware components with DPS 6. Another specialised derivative of the Level 6 was the Honeywell Page Printing System.|$|R
50|$|EXCP {{may also}} be used to access {{communications}} devices attached to IBM 2701, 2702 and 2703 communications controllers and IBM 370x or Amdahl 470x <b>front-end</b> <b>processors</b> (and their respective follow-ons) operating in emulator mode (EP) or partitioned emulator mode (PEP).|$|R
40|$|An {{important}} component of EPICS (Experimental Physics and Industrial Control System) is iocCore, which is the core software in the IOC (input/output controller) <b>front-end</b> <b>processors.</b> Currently iocCore requires the vxWorks operating system. This paper describes the porting of iocCore to other operating systems...|$|R
40|$|Dense Wavelength-division {{multiplexing}} (DWDM) {{technology offers}} tremendous transmission ca-pacity in optical fiber communications. However, switching and routing capacity lags behind the trans-mission capacity, {{since most of}} today’s packet switches and routers are implemented using slower electronic components. Optical packet switches {{are one of the}} potential candidates to improve switching capacity to be comparable with optical transmission capacity. In this paper, we present an optically transparent ATM (OPATM) switch that consists of a photonic <b>front-end</b> <b>processor</b> and a WDM switching fabric. A WDM loop memory is deployed as a multi-ported shared-memory in the switching fabric. The photonic <b>front-end</b> <b>processor</b> performs the cell delineation, VPI/VCI overwriting, and cell synchroniza-tion functions in the optical domain under the control of electronic signals. The WDM switching fabric stores and forwards aligned cells from each input port to the appropriate output ports under the control of an electronic route controller. We have demonstrated with experiments the functions and capabilities of the <b>front-end</b> <b>processor</b> and the switching fabric at the header-processing rate of 2. 5 Gb/s. Other than ATM, the switching architecture can be easily modified to apply to other types of fixed-length payload formats with different bit rates. Using this kind of photonic switches to route information, a...|$|R
50|$|Telenet {{supported}} remote concentrators for IBM 3270 family intelligent terminals, which communicated, via X.25 to Telenet-written {{software that}} ran in IBM 370x series <b>front-end</b> <b>processors.</b> Telenet also supported Block Mode Terminal Interfaces (BMTI) for IBM Remote Job Entry terminals supporting the 2780/3780 and HASP Bisync protocols.|$|R
50|$|The term 37xx {{refers to}} IBM's family of SNA {{communications}} controllers. The 3745 supports {{up to eight}} high-speed T1 circuits, the 3725 is a large-scale node and <b>front-end</b> <b>processor</b> for a host, and the 3720 is a remote node that functions as a concentrator and router.|$|R
40|$|International Telemetering Conference Proceedings / October 22 - 25, 2001 / Riviera Hotel and Convention Center, Las Vegas, NevadaA {{traditional}} <b>Front-end</b> <b>Processor</b> (FEP) {{with local}} RAID storage can limit the operational throughput of a high-rate telemetry ground station. The <b>Front-end</b> <b>processor</b> must perform pass processing (frame synchronization, decoding, routing, and storage), post-pass processing (level-zero processing), and tape archiving. A typical fifteen minute high-rate satellite pass can produce data files of 10 to 20 GB. The FEP may require up to 2 hours {{to perform the}} post-pass processing and tape archiving functions for these size files. During this time, it is not available to support real-time pass operations. Honeywell faced this problem {{in the design of}} the data management system for the DataLynx ä* ground stations. Avtec Systems, Inc. and Honeywell worked together to develop a data management system that utilizes a Storage Area Network (SAN) in conjunction with multiple High-speed <b>Front-end</b> <b>Processors</b> (HSFEP) for Pass Processing (PFEP), multiple HSFEPs for Post-pass Processing (PPFEP), and a dedicated Tape Archive server. A SAN consists of a high-capacity, high-bandwidth shared RAID that is connected to multiple nodes using 1 Gbps Fibre Channel interfaces. All of the HSFEPs as well as the Tape Archive server have direct access to the shared RAID via a Fibre Channel network. The SAN supports simultaneous read/write transfers between the nodes at aggregate rates up to 120 Mbytes/sec. With the Storage Area Network approach, the High-Speed <b>Front-end</b> <b>Processors</b> can quickly transfer the data captured during a pass to the shared RAID for post-processing and tape archiving so that they are available to support another satellite pass. This paper will discuss the architecture of the Storage Area Network and how it optimizes ground station data management in a high-rate environment...|$|R
40|$|This paper {{describes}} an integrated approach to pattern classification where a self-organising Boolean neural network architecture {{is used as}} a <b>front-end</b> <b>processor</b> to a feedforward neural architecture based on goal-seeking principles (the GSN architecture). The performance of the integrated architecture is illustrated by considering its application to a character recognition problem...|$|R
50|$|The {{operating}} system supported inter-system communication, job submission and file transfer between CP-6 systems and between CP-6 and CP-V and {{to and from}} IBM and other HASP protocol systems. The system used communications and terminal interfaces through a Honeywell Level 6 minicomputer-based <b>front-end</b> <b>processor.</b> Asynchronous, bisynchronous and TCP/IP communications protocols were supported.|$|R
5000|$|The IBM 3720 was a {{communications}} controller (<b>front-end</b> <b>processor)</b> made by IBM, {{suitable for use}} with IBM System/390. The 3720, introduced in 1986, was capable of supporting up to 60 communications lines, and was a smaller version of the 3725. [...] Official service support was withdrawn in 1999 in favour of the IBM 3745.|$|R
40|$|This paper {{addresses}} {{a method of}} blind separation of delayed and superimposed sources. We employ a linear feedforward temporal network as a separation system and present a simple associated learning algorithm. We adopt a blind separation technique as a <b>front-end</b> <b>processor</b> for robust cocktail party speech recognition task. Experimental study of the proposed algorithm is given through two different real-world data...|$|R
25|$|Supercomputing: DRDO's ANURAG {{developed}} the PACE+ Supercomputer for strategic purposes for supporting its various programmes. The initial version, as detailed in 1995, had the following specifications: The system delivered a sustained performance {{of more than}} 960 Mflops (million floating operations per second) for computational fluid dynamics programmes. Pace-Plus included 32 advanced computing nodes, each with 64 megabytes(MB) of memory that can be expanded up to 256MB and a powerful <b>front-end</b> <b>processor</b> which is a hyperSPARC with a speed of 66/90/100 megahertz (MHz). Besides fluid dynamics, these high-speed computer systems were used {{in areas such as}} vision, medical imaging, signal processing, molecular modeling, neural networks and finite element analysis. The latest variant of the PACE series is the PACE ++, a 128 node parallel processing system. With a <b>front-end</b> <b>processor,</b> it has a distributed memory and message passing system. Under Project Chitra, the DRDO is implementing a system with a computational speed of 2-3 Teraflops utilising commercial off the shelf components and the Open Source Linux Operating System.|$|R
40|$|We {{present the}} design and beam test results of a {{prototype}} beam-based digital feedback system for the Interaction Point of the International Linear Collider. A custom analogue <b>front-end</b> <b>processor,</b> FPGA-based digital signal processing board, and kicker drive amplifier have been designed, built, and tested on the extraction line of the KEK Accelerator Test Facility (ATF). The system was measured to have a latency of approximately 140 ns...|$|R
40|$|A {{linear network}} of {{communicating}} processors is analyzed. The processors {{in the network}} {{may or may not}} be equipped with <b>front-end</b> <b>processors.</b> The processing load originates either at the boundary or at the interior of the network. Closed-form solutions and computational techniques are presented for the above situations, to obtain time optimal distribution of processing loads on the processors. Some important results are proved analytically using the closed-form expressions...|$|R
40|$|Multiple {{microcomputer}} system used to investigate application of parallel processing to real-time simulation. With dual-base architecture, each microcomputer communicates with corresponding microcomputer on opposite bus through dual-port interface memory. Transfers {{of data to}} and from <b>front-end</b> <b>processor</b> occur on interactive information bus. Transfers of data related to simulation calculations occur on real-time-information bus. System, called the real-time multiprocessor simulator (RTMPS), is tool for developing low-cost, portable, user-friendly simulators...|$|R
40|$|Parallel {{microprocessors}} have computational {{power and}} speed for realistic simulations. Interactive information bus links <b>front-end</b> <b>processor</b> and computational processors. Real-time information bus links real-time extension processor and pre-processors. Computational processor and preprocessor communicate through shared memory. System used to simulate small turboshaft engine to demonstrate potential of multiprocessing in such applications. Real-time simulations aid {{development of new}} digital engine controls enabling testing of hardware and software under realistic conditions...|$|R
40|$|Optimal load {{allocation}} for {{load sharing}} a divisible job over N processors inter-connected in bus-oriented network is considered. The processors {{are equipped with}} <b>front-end</b> <b>processor</b> s. It is analytically proved, for the first time, that a minimal solution time is achieved when the computation by each processor finishes at the same time. Closed form solutions for the minimum finish time and the optimal data allocation for each processor are obtained...|$|R
40|$|New results, {{which have}} been {{achieved}} by using neural network architectures for two-dimensional image classification based on the goal-seeking neuron (GSN), are presented. A number of important practical issues concerning mapping topologies and the parallel implementation of GSN-based architectures are also investigated, together with a proposal {{for the development of}} a related neurally based feature extractor to be used as a <b>front-end</b> <b>processor</b> in a fully integrated Boolean network architecture...|$|R
40|$|An {{important}} component of EPICS (Experimental Physics and Industrial Control System) is iocCore, which is the core software in the IOC (input/output controller) <b>front-end</b> <b>processors.</b> At ICALEPCS 1999 a paper was presented describing plans to port iocCore to multiple operating systems. At that time iocCore only supported vxWorks, but now it also supports RTEMS, Solaris, Linux, and WinNT. This paper describes some key features of how iocCore supports multiple operating systems. ...|$|R
50|$|In April 2013 NDP Environment Minister Sterling Belliveau, sent {{a letter}} to Halifax Regional Council expressing that both the <b>front-end</b> <b>processor</b> (FEP) and the waste {{stabilization}} facility (WSF) will continue part of the Department of Energy regulations and a commitment to the local communities. On April 30, Liberal Andrew Younger questioned the Minister's authority to get involved in Halifax Regional Council proposed changes to their waste management strategy.|$|R
40|$|Dense wavelength-division {{multiplexing}} (DWDM) {{technology has}} provided tremendous transmission capacity in optical fiber communications. However, switching and routing capacity {{is still far}} behind transmission capacity. This is because most of today's packet switches and routers are implemented using electronic technologies. Optical packet switches are the potential candidate to boost switching capacity to be comparable with transmission capacity. In this paper, we present a photonic asynchronous transfer mode (ATM) <b>front-end</b> <b>processor</b> that has been implemented and {{is to be used}} in an optically transparent WDM ATM Multicast (3 M) switch. We have successfully demonstrate the <b>front-end</b> <b>processor</b> in two different experiments. One performs cell delineation based on ITU standards and overwrites VCI/VPI optically at 2. 5 Gb/s. The other performs cell synchronization, where cells from different input ports running at 2. 5 Gb/s are phase-aligned in the optical domain before they are routed in the switch fabric. The resolution of alignment is achieved to the extent of 100 ps (or 1 / 4 bit). An integrated 1 2 Y-junction semiconductor optical amplifier (SOA) switch has been developed to facilitate the cell synchronizer...|$|R
40|$|Launch {{processing}} for Space Shuttle is checked out, controlled, {{and monitored}} with new system. Entire {{system can be}} exercised by two computer programs [...] one in master console and other in each of operations consoles. Control program in each operations console detects change in status and begins task initiation. All of <b>front-end</b> <b>processors</b> are exercised from consoles through common data buffer, and all data are logged to processed-data recorder for posttest analysis...|$|R
40|$|<b>Front-end</b> <b>processor</b> for NASA Metrology Information System (NMIS) is {{real-time}} relational data-base {{computer system}} designed to distribute processing for NMIS mainframe system or run as stand-alone local-area-network data-base system. System used in large calibration laboratories for work control and to maintain records of calibration, repair, costs, manpower usage, traceability, and other pertinent facts about instruments supported by laboratories. Provides enhancements to existing NASA Metrology Information System and batches upload to minimize mainframe I/O at times of heavy usage...|$|R
40|$|Abstract:- A Primary site {{approach}} for designing a fault-tolerant network of processors (e. g clusters) is introduced and discussed. Nodes (processors) {{in the system}} are either <b>front-end</b> <b>processors</b> or they are not equipped with a <b>front-end</b> <b>processor.</b> Nodes periodically checkpoint their results on the backup node. If a node fails, the backup takes over and rolls over {{to the time of}} the last checkpointing. The probability that more than one processor to fail with a time span of a task execution is very small. Hence one processor fault during a life time of single task execution is only considered. In this paper we have considered the effect of fault occurrence on the finish time. We have obtained a closed form solution for the finish time taking into consideration the adverse effect of the faults. Moreover, the analytical closed form solution obtained have taken into consideration all system parameters such as links speed, processors speed (nodes computing power), communication time. The different fault cases, based on the different time that a fault may occur are discussed and analyzed. The effect of different system parameters on the finish time are studied and discussed...|$|R
