33|10|Public
5000|$|Information on what file searching {{features}} the file managers support. <b>RegExp</b> include {{the possibilities of}} nested Boolean searches, thus implicitly all file managers supporting <b>RegExp</b> search support also Boolean searches.Column Definitions (D)Entry Notes (s) ...|$|E
50|$|In {{addition}} to the LIKE operator, CUBRID provides the <b>REGEXP</b> operator for regular expression pattern matching. By default, the operator does a case insensitive matching on the input string, but the modifier BINARY {{can be used for}} case sensitive scenarios. An optional alias of <b>REGEXP</b> is RLIKE.|$|E
50|$|Currently, CUBRID {{does not}} support <b>REGEXP</b> on Unicode strings.|$|E
5000|$|MontyLemmatiser: part-of-speech {{sensitive}} lemmatisation. Strips plurals (geese-->goose) {{and tense}} (were-->be, had-->have). Includes <b>regexps</b> from Humphreys and Carroll's morph.lex, and UPENN's XTAG corpus ...|$|R
50|$|SXEmacs is a {{fork of the}} XEmacs text editor. It runs on many Unix-like {{operating}} systems including OS X. It is notable for features such as FFI support, enhanced number types (similar to bignums in XEmacs 21.5), raw string <b>regexps,</b> and an implementation of Pugh's skip lists.|$|R
25|$|JSON, or JavaScript Object Notation, is a {{general-purpose}} data {{interchange format}} that {{is defined as}} a subset of JavaScript's object literal syntax. Like much of JavaScript (<b>regexps</b> and anonymous functions as 1st class elements, closures, flexible classes, 'use strict'), JSON, except for replacing Perl's key-value operator '=>' by an RFC 822 inspired ':', is syntactically pure Perl.|$|R
50|$|Regexp: <b>Regexp</b> was a pure Java Regular Expression package.|$|E
5000|$|Regular {{expression}} {{search and}} replace in many <b>regexp</b> dialects ...|$|E
5000|$|String and <b>RegExp</b> objects, {{which make}} it easy to {{implement}} L-systems; ...|$|E
5000|$|Perl 6 {{provides}} a superset of Perl 5 features {{with respect to}} regexes, folding them into a larger framework called rules, which provide the capabilities of a parsing expression grammar, as well as acting as a closure {{with respect to their}} lexical scope. Rules are introduced with the [...] keyword, which has a usage quite similar to subroutine definition. Anonymous rules can be introduced with the [...] (or [...] ) keyword, or simply be used inline as <b>regexps</b> were in Perl 5 via the [...] (matching) or [...] (substitution) operators.|$|R
2500|$|Perl 6 {{provides}} a superset of Perl 5 features {{with respect to}} regexes, folding them into a larger framework called [...] "rules" [...] which provide the capabilities of context-sensitive parsing formalisms (such as the syntactic predicates of parsing expression grammars and ANTLR), as well as acting as a closure {{with respect to their}} lexical scope. Rules are introduced with the rule keyword which has a usage quite similar to subroutine definition. Anonymous rules can also be introduced with the regex (or rx) keyword, or they can simply be used inline as <b>regexps</b> were in Perl 5 via the m (matching) or s (substitute) operators.|$|R
40|$|A {{usage of}} regular {{expressions}} to search text {{is well known}} and understood as a useful technique. Regular Expressions are generic representations for a string or a collection of strings. Regular expressions (<b>regexps)</b> {{are one of the}} most useful tools in computer science. NLP, as an area of computer science, has greatly benefitted from regexps: they are used in phonology, morphology, text analysis, information extraction, & speech recognition. This paper helps a reader to give a general review on usage of regular expressions illustrated with examples from natural language processing. In addition, there is a discussion on different approaches of regular expression in NLP...|$|R
5000|$|The {{following}} example {{demonstrates the}} output of the apropos command with an <b>regexp</b> keyword (abc.n) and a regular keyword: ...|$|E
50|$|Multibyte <b>regexp</b> - Regular {{expression}} matcher {{is aware}} of multibyte string; you can use multibyte characters both in patterns and matched strings.|$|E
5000|$|Text {{statistics}} functions: Text statistics; extract words; Words lengths; UNITAZ quantity sorting; UNITAZ sorting alphabet; Count the substring {{and count}} the substring (<b>regexp)</b> ...|$|E
5000|$|Perl 6 {{provides}} a superset of Perl 5 features {{with respect to}} regexes, folding them into a larger framework called [...] "rules" [...] which provide the capabilities of context-sensitive parsing formalisms (such as the syntactic predicates of parsing expression grammars and ANTLR), as well as acting as a closure {{with respect to their}} lexical scope. Rules are introduced with the [...] keyword which has a usage quite similar to subroutine definition. Anonymous rules can also be introduced with the [...] (or [...] ) keyword, or they can simply be used inline as <b>regexps</b> were in Perl 5 via the [...] (matching) or [...] (substitute) operators.|$|R
40|$|Pattern {{matching}} is at {{the core}} of many computational problems, e. g., search engine, data mining, network security and information retrieval. In this dissertation, we target at the more complex patterns of regular expression and time series, and proposed a general modular structure, named character class with constraint repetition (CCR), as the building block for the pattern matching algorithm. An exact matching algorithm named MIN-MAX is developed to support overlapped matching of CCR based <b>regexps,</b> and an approximate matching algorithm named Elastic Matching Algorithm is designed to support overlapped matching of CCR based time series, i. e., music melody. Both algorithms are parallelized to run on FPGA to achieve high performance, and the FPGA-based scanners are designed as a modular architecture which is parameterizable and can be reconfigured by simple memory writes, achieving a perfect balance between performance and deployment time...|$|R
40|$|This {{paper offers}} a clean way {{to combine the}} two traditions: an Expectation-Maximation (EM) [4] {{algorithm}} for training arbitrary FSTs. First the human expert uses domain knowledge to specify the topology and parameterization of the transducer in any convenient way. Then the EM algorithm automatically chooses parameter values that (locally) maximize the joint likelihood of fully or partly observed data. Unlike previous specialized methods, the EM algorithm here allows transducers having ffl's and arbitrary topology. It also allows parameterizations that are independent of the transducer topology (hence unaffected by determinization and minimization). But it remains surprisingly simple because all the difficult work can be subcontracted to existing algorithms for semiring-weighted automata. The trick {{is to use a}} novel semiring. To combine the two traditions, a domain expert might build a weighted transducer by using weighted expressions in the full finite-state calculus. Weighted <b>regexps</b> can also optionally refer to machines that are specified directly in terms of states, arcs, and weights, or even derived by approximating PCFGs [16]. But the various weights are written in terms of unknown parameters and estimated automatically. The semiring approach ensures that the method works even if the transducer is built with operations such as composition, directed replacement, and minimization, which distribute the parameters over the arcs in a complex way...|$|R
5000|$|In this example, apropos is used {{to search}} for the {{keywords}} (with an <b>regexp</b> [...]) [...] "abc.n" [...] and xzless, and apropos returns the indicated man pages that include the keywords.|$|E
5000|$|Intended to {{be easily}} {{extended}} in Java. For example, Tea supports relational database access through JDBC, regular expressions through GNU <b>Regexp,</b> and an XML parser through a SAX parser (XML4J for example).|$|E
5000|$|The {{following}} example demonstrates {{walking a}} directory tree, {{starting at the}} current directory. It uses the [...] function to construct a sequence that walks the tree. The [...] form binds [...] to each path in the sequence, and [...] tests these paths against the given <b>regexp</b> pattern.|$|E
5000|$|The {{built-in}} JavaScript libraries use {{the same}} naming conventions as Java. Data types and constructor functions use upper camel case (<b>RegExp,</b> TypeError, XMLHttpRequest, DOMObject) and methods use lower camel case (getElementById, getElementsByTagNameNS, createCDATASection). In order to be consistent most JavaScript developers follow these conventions.See also: Douglas Crockford's conventions ...|$|E
5000|$|JavaScript {{has several}} kinds of {{built-in}} objects, namely Array, Boolean, Date, Function, Math, Number, Object, <b>RegExp</b> and String. Other objects are [...] "host objects", defined not by the language, but by the runtime environment. For example, in a browser, typical host objects belong to the DOM (window, form, links, etc.).|$|E
5000|$|A regular expression, regex or <b>regexp</b> (sometimes {{called a}} {{rational}} expression) is, in theoretical computer science and formal language theory, {{a sequence of}} characters that define a search pattern. Usually this pattern is then used by string searching algorithms for [...] "find" [...] or [...] "find and replace" [...] operations on strings.|$|E
5000|$|... sdcv is simple, {{cross-platform}} text-base utility {{for work}} with dictionaries in StarDict's format. The word from [...] "list of words" [...] may be string with leading '/' for using Fuzzy search algorithm, string may contain '?' and '*' for using <b>regexp</b> search. It works in interactive and non-interactive mode. Press Ctrl+D to exit from interactive mode.|$|E
5000|$|However, many tools, libraries, and {{engines that}} provide such constructions still {{use the term}} regular {{expression}} for their patterns. This {{has led to a}} nomenclature where the term regular expression has different meanings in formal language theory and pattern matching. For this reason, some people have taken to using the term regex, <b>regexp,</b> or simply pattern to describe the latter. Larry Wall, author of the Perl programming language, writes in an essay about the design of Perl 6: ...|$|E
5000|$|The {{substitution}} command, which {{originates in}} search-and-replace in ed, implements simple parsing and templating. The [...] provides both pattern matching and saving text via sub-expressions, while the [...] {{can be either}} literal text, or a format string containing the characters [...] for [...] "entire match" [...] or the special escape sequences [...] through [...] for the nth saved sub-expression. For example, [...] replaces all occurrences of [...] "cat" [...] or [...] "dog" [...] with [...] "cats" [...] or [...] "dogs", without duplicating an existing [...] "s": [...] is the 1st (and only) saved sub-expression in the <b>regexp,</b> and [...] in the format string substitutes this into the output.|$|E
5000|$|Both {{of these}} {{programs}} can be run in DrRacket, or on the command line, via the [...] executable. Racket ignores an initial shebang line, {{making it possible to}} turn such programs to executable scripts. The following script demonstrates this, in addition to using Racket's library for command-line argument parsing:#!/usr/bin/env racket#lang racket(command-line #:args (base-dir ext re) (for ((in-directory) #:when (regexp-match? (string-append [...] "." [...] ext [...] "$") p) num) (in-indexed (file->lines p))) (when (regexp-match? (pregexp re) line) (printf [...] "~a:~a: ~a~n" [...] p num line))))The script is a grep-like utility, expecting three command-line arguments: a base directory, a filename extension, and a (perl-compatible) regular expression. It scans the base directory for files with the given suffix, and print lines matching the <b>regexp</b> pattern.|$|E
5000|$|P4080 - Includes eight e500mc cores, {{each with}} 32/32kB instruction/data L1 caches and a 128 kB L2 cache. The chip has dual 1 MB L3 caches, each {{connected}} to a 64-bit DDR2/DDR3 memory controller. The chip contains a security and encryption module, capable of packet parsing and classification, and acceleration of encryption and <b>regexp</b> pattern matching. The chip can be configured with up to eight Gigabit and two 10 Gigabit Ethernet controllers, three 5 GHz PCIe ports and two RapidIO interfaces. It also has various other peripheral connectivity such as two USB2 controllers. It is designed to operate below 30 W at 1.5 GHz. The processor is manufactured on a 45 nm SOI process and begun sampling to customers in August 2009.|$|E
5000|$|An illustrative {{example of}} the {{complementary}} nature of parsing and templating is the [...] (substitute) command in the sed text processor, originating from search-and-replace in the ed text editor. Substitution commands are of the form , where [...] is a regular expression, for parsing input, and [...] is a simple template for output, either literal text, or a format string containing the characters [...] for [...] "entire match" [...] or the special escape sequences [...] through [...] for the nth sub-expression. For example, [...] replaces all occurrences of [...] "cat" [...] or [...] "dog" [...] with [...] "cats" [...] or [...] "dogs", without duplicating an existing [...] "s": [...] is the 1st (and only) sub-expression in the <b>regexp,</b> and [...] in the format string substitutes this into the output.|$|E
40|$|Open source Python modules, {{linguistic}} data and documentation {{for research and}} development in natural language processing, supporting dozens of NLP tasks. NLTK includes the following software modules (~ 120 k lines of Python code) : Corpus readers interfaces to many corpora Tokenizers whitespace, newline, blankline, word, treebank, sexpr, <b>regexp,</b> Punkt sentence segmenter Stemmers Porter, Lancaster, <b>regexp</b> Taggers <b>regexp,</b> n-gram, backoff, Brill, HMM, TnT Chunkers <b>regexp,</b> n-gram, named-entity Parsers recursive descent, shift-reduce, chart, feature-based, probabilistic, dependency, [...] . Semantic interpretation untyped lambda calculus, first-order models, DRT, glue semantics, hole semantics, parser interface WordNet WordNet interface, lexical relations, similarity, interactive browser Classifiers decision tree, maximum entropy, naive Bayes, Weka interface, megam Clusterers expectation maximization, agglomerative, k-means Metrics accuracy, precision, recall, windowdiff, distance metrics, inter-annotator agreement coefficients, word association measures, rank correlation Estimation uniform, maximum likelihood, Lidstone, Laplace, expected likelihood, heldout, cross-validation, Good-Turing, Witten-Bell Miscellaneous unification, chatbots, many utilities NLTK-Contrib (less mature) categorial grammar (Lambek, CCG), finite-state automata, hadoop (MapReduce), kimmo, readability, textual entailment, timex, TnT interface, inter-annotator agreemen...|$|E
40|$|Syntax Tree (AbsTree) - a tree {{structure}} representing a newly parsed regular expression. It {{is the result}} of the first stage of <b>regexp</b> compilation, with atoms converted into matcher functions. See section ??. Augmented Syntax Tree (AugTree) - result of the second stage of <b>regexp</b> compilation. It is an AbsTree with leaf nodes labelled, and FirstPos, LastPos, Child Nodes and Nullable information included. See section ??. Alternation (operator) - eg "a [...] -b", matches "a" or "b". Forms an ALT node in the AbsTree. Atom - a part of a <b>regexp</b> that should match an input symbol. It could be ffl a character, (or ! [...] ? construct with polymorphic regexps) ffl a character class ffl a range ffl a user defined matcher function ffl a user defined assertion It corresponds to a leaf node of an AbsTree. Assertion - a zero-width function that has to hold at a particular point in a <b>regexp,</b> but does not affect the actual final matched text. For instance, the prefix operator,, is an assertion that wil [...] ...|$|E
40|$|In earlier work, we {{conducted}} an exploratory investigation of concerns in two existing Java TM packages: jFTPd and gnu. <b>regexp.</b> Two separate developers marked {{concerns in the}} source for each package: these concerns were then compared and analyzed. In this paper, we describe the next step of our investigations: {{the use of the}} IBM Hyper/J TM tool to separate and configure the identified concerns. We describe the various kinds of hyperspaces, hypermodules, hyperslices, and concern mappings we used to describe our previously identified concerns. We also discuss code restructurings we used to enable the capturing and composition of concerns. ...|$|E
40|$|This paper {{presents}} a software tool sa 2 px to translate regular expressions (regexps) in SpamAssassin (SA) rules into the POSIX format. The translated regexps {{can be implemented}} on different platforms, so that one could better separate the composition process of spam filtering rules from the on-line operations. Sa 2 px is consisted of three layers of functions. The first layer is responsible for translating plugins and special formats to their equivalent basic SA formats. The second layer uses a syntax conversion approach to translate basic SA rules to the POSIX format. The third layer uses a backward grouping algorithm to group multiple regexps together {{so that they can}} be packed into a DFA table using Flex or similar tools. Overall, sa 2 px can translate regexps in the whole rule set (uri, body, header, rawbody and ReplaceTags plugin), and the translation rate of 1115 SA <b>regexp</b> rules is 84. 5 %. In comparison, sa-compile can translate 296 rules of 453 body rules. The translated rules are then clustered into several main groups, except for some cases in which the <b>regexp</b> structures led to explosive state growth. Finally, DFA tables and (action number, rule name) pairs are generated. Experimental results show that the DFA table based implementation of these translated regexps cut down 66 % of the execution time of the Perl (with sa-compile activated) based string scanning under process-level parallelization environment. 1...|$|E
40|$|Inputs to web forms often contain typos {{or other}} errors. However, {{existing}} web form design tools require end-user developers to write regular expressions (“regexps”) or even scripts to validate inputs, which {{is slow and}} errorprone because of the poor match between common data types and the <b>regexp</b> notation. We present a new technique enabling end-user developers to describe data {{as a series of}} constrained parts, and we have incorporated our technique into a prototype tool. Using this tool, end-user developers can create validation code more quickly and accurately than with existing techniques, finding 90 % of invalid inputs in a lab study. This study and our evaluation of the technique’s generality have motivated several tool improvements, which we have implemented and now evaluate using the Cognitive Dimensions framework...|$|E
40|$|Techniques to help {{software}} developers explicitly separate different concerns in their {{programs have been}} gaining increasing attention {{in the last few}} years. Three examples of such techniques are composition filters [ABV 92], aspect-oriented programming [KLM + 97], and hyperspaces [TOHSJ 99]. A central idea behind these techniques is that software systems would be easier to change if various design and programming decisions were modularized separately and simultaneously. But what is a concern? And how do different concerns interact within a code base? To gain some insight into these questions, we have conducted an exploratory investigation of concerns in two existing Java [GJS 96] packages: gnu. <b>regexp,</b> and jFTPd. Each of the packages was marked for concerns by each author of this positionpaper. We then compared the concerns identified and analyzed how the concerns interacted with each other and across the exi...|$|E
