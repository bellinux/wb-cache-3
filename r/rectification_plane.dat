1|21|Public
40|$|We present {{methods for}} {{creating}} 3 D graphical models of scenes from a limited numbers of images, i. e. one or two, {{in situations where}} no scene co-ordinate measurements are available. The methods employ constraints available from geometric relationships that are common in architectural scenes - such as parallelism and orthogonality - together with constraints available from the camera. In particular, by using the circular points of a plane simple, linear algorithms are given for computing plane <b>rectification,</b> <b>plane</b> orientation and camera calibration from a single image. Examples of image based 3 D modelling are given for both single images and image pairs...|$|E
30|$|As {{regards the}} motion model, it is {{designed}} {{under the assumption that}} vehicles velocity can be approximated to be locally constant, which is valid in highway environments. As a result, the evolution of a vehicle's position can be traced by a first-order linear model. However, linearity is lost due to the perspective effect in the acquired image sequence. To preserve linearity we resort to a <b>plane</b> <b>rectification</b> technique, usually known as inverse perspective mapping (IPM) [33]. This computes the projective transformation, T, that produces an aerial or bird's-eye view of the scene from the original image. The image resulting from <b>plane</b> <b>rectification</b> will be referred to as the rectified domain or the transformed domain. In the rectified domain, the motion of vehicles can be safely described as a first-order linear equation with an added random noise.|$|R
40|$|This paper {{introduces}} {{a new strategy}} for <b>plane</b> <b>rectification</b> in sequences of images, based on the Expectation-Maximization (EM) algorithm. Our approach is able to compute simultaneously {{the parameters of the}} dominant vanishing point in the image plane and the most significant lines passing through it. It is based on a novel definition of the likelihood distribution of the gradient image considering both the position and the orientation of the gradient pixels. Besides, the mixture model in which the EM algorithm operates is extended, compared to other works, to consider an additional component to control the presence of outliers. Some synthetic data tests are described to show the robustness and efficiency of the proposed method. The <b>plane</b> <b>rectification</b> results show that the method is able to remove the perspective and affine distortion of real traffic sequences without the need to compute two vanishing point...|$|R
40|$|We {{describe}} the geometry, constraints and algorithmic implementation for metric <b>rectification</b> of <b>planes.</b> The <b>rectification</b> allows metric properties, such as angles and length ratios, {{to be measured}} on the world plane from a perspective image. The novel contributions are: first, that in a stratified context {{the various forms of}} providing metric information, which include a known angle, two equal though unknown angles, and a known length ratio; can all be represented as circular constraints on the parameters of an affine transformation of the plane — this provides a simple and uniform framework for integrating constraints; second, direct rectification from right angles in the plane; third, it is shown that metric rectification enables calibration of the internal camera parameters; fourth, vanishing points are estimated using a Maximum Likelihood estimator; fifth, an algorithm for automatic rectification. Examples are given for a number of images, and applications demonstrated for texture map acquisition and metric measurements. ...|$|R
40|$|The {{main purpose}} of this survey is to {{understand}} completely the geometry, constraints and algorithmic implementation for metric <b>rectification</b> of <b>planes.</b> In this survey, I consider the perspective images and thus, using rectification helps me to measure metric properties from a perspective image. Additionally, because I consider perspective images, the concept of projective transformation is important. Thus, I start with defining the projective transformation. A projective transformation is a transformation which is used in projective geometry. I can {{say that it is}} the composition of a pair of perspective projections. It helps to understand the change of perceived positions of observed objects if {{the point of view of}} the observer changes. Projective transformation maps lines to lines, however it is not necessary to preserve parallelism. Here, it is important to state that projective transformations do not preserve sizes or angles but it preserves incidence and cross-ratio. These two preserved properties are very important i...|$|R
40|$|This paper {{describes}} {{a method for}} image rectification of a trinocular setup. The rectification method used {{is an extension of}} a recent approach based on the fundamental matrix to generate the correcting homographies {{in the case of a}} stereo pair. The extended method uses the fact that the triplet of images can be treated as two pairs and that homographies are projections of the different images planes onto new <b>planes.</b> <b>Rectification</b> thus becomes a matter of deciding which plane will be the common one and what transformation or homography is to be applied to each image...|$|R
40|$|We {{propose a}} simple method for {{computing}} a metric <b>rectification</b> of a <b>plane</b> from multiple views taken by Ki = diag (fi, fi, 1) cameras. The orthogonality properties of this camera model are exploited {{from an early}} stage to achieve a straightforward optimization process with only two degrees of freedom, even if the fi in all views are unknown. We study the optimization landscapes for several typical camera motions and varying amounts of image noise. We conclude {{that the problem is}} extremely ill conditioned and can only be realistically solved for rich camera motions and small amounts of image noise, preferably with at least one fi known in the sequence. ...|$|R
40|$|Abstract. A {{method for}} affine <b>rectification</b> of a <b>plane</b> {{exploiting}} knowledge of relative scale changes is presented. The rectifying transformation is fully {{specified by the}} relative scale change at three non-collinear points or by two pairs of points where the relative scale change is known; the relative scale change between the pairs is not required. The method also allows homography estimation between two views of a planar scene from three point-with-scale correspondences. The proposed method is simple to implement and without parameters; linear and thus supporting (algebraic) least squares solutions; and general, without restrictions on either {{the shape of the}} corresponding features or their mutual position. The wide applicability of the method is demonstrated on text rectification, detection of repetitive patterns, texture normalization and estimation of homography from three point-with-scale correspondences. ...|$|R
40|$|Abstract. In this paper, {{we propose}} {{a method to}} segment the ground plane from a mobile robot’s visual field of view and then measure the height of non-ground plane {{features}} (even raw pixels) above the mobile robot’s ground plane. Thus a mobile robot can determine what it can drive over, what it can drive under, and what it needs to manoeuvre around. In addition to obstacle avoidance, this data {{could also be used}} for localisation and map building. All of this is possible from an uncalibrated camera (raw pixel coordinates only), but is restricted to (near) pure translation motion of the camera. The main contributions are (i) a novel reciprocal-polar (RP) image <b>rectification,</b> (ii) ground <b>plane</b> segmentation by sinusoidal model fitting in RP-space and (iii) a novel projective construction for measuring affine height. ...|$|R
40|$|International audienceGeneric camera odometry can be {{performed}} using two images of the same plane, such as the ground plane even {{in the case of}} non central cameras. It is possible to recover both the angle and rotation center describing a generic planar motion on the ground plane if the center is visible in both images. We present an algorithm to recover these two parameters from an initial set of correspondences and, furthermore, to estimate the motion flow related to any point on the ground plane. In this situation the motion flows are given by a set of “concentric” closed curves around the rotation center. By considering two subsequent ground plane motions and their related motion flows, we show {{that it is possible to}} perform a <b>rectification</b> of the <b>plane</b> up to a scale factor. We provide experimental results which validate our approach...|$|R
30|$|We {{propose a}} method to segment the ground plane from a mobile robot's visual field of view and then measure the height of nonground plane {{features}} above the mobile robot's ground plane. Thus a mobile robot can determine what it can drive over, what it can drive under, and what it needs to manoeuvre around. In addition to obstacle avoidance, this data {{could also be used}} for localisation and map building. All of this is possible from an uncalibrated camera (raw pixel coordinates only), but is restricted to (near) pure translation motion of the camera. The main contributions are (i) a novel reciprocal-polar (RP) image <b>rectification,</b> (ii) ground <b>plane</b> segmentation by sinusoidal model fitting in RP-space, (iii) a novel projective construction for measuring affine height, and (iv) an algorithm that can make use of a variety of visual features and therefore operate {{in a wide variety of}} visual environments.|$|R
40|$|Catadioptric {{systems are}} realizations of {{omnidirectional}} vision through mirror-lens combinations. Designs preserving {{the uniqueness of}} an effective viewpoint have recently gained attraction. We present here a novel approach for estimating the intrinsic parameters of a well-known catadioptric system consisting of a paraboloid mirror and an orthographic lens. We introduce the geometry of catadioptric line projection and we show that the vanishing points lie on a conic section which encodes the entire calibration information. Projections of two sets of parallel lines suffice for intrinsic calibration from one view {{as well as for}} metric <b>rectification</b> of a <b>plane.</b> Our approach overcomes limitations of existing manual calibration methods and was successfully tested on the task of back-warping real-images images onto virtual planes. 1. Introduction It is common sense in computer vision that increasing the field of view enhances several visual capabilities like ego-motion estimation and local [...] ...|$|R
40|$|International audienceWe {{address the}} problem of view {{independent}} object classification. Our aim is to classify moving objects of traffic scene surveillance videos into pedestrians, bicycles and vehicles. However, this problem is very challenging due to large object appearance variance, low resolution videos and limited object size. Especially, perspective distortion of surveillance cameras makes most 2 D object features like size and speed related to view angles and not suitable for object classification. In this paper, we adopt the common constraint that most objects of interest in traffic scenes are moving on the ground plane. Firstly, we realize the ground <b>plane</b> <b>rectification</b> based on appearance and motion information of moving objects, which can be applied for normalization of 2 D object features. An online learning framework is then described to achieve automatic object classification based on rectified 2 D object features. Experimental results demonstrate the effectiveness, efficiency and robustness of the proposed method...|$|R
40|$|This paper {{presents}} several {{results on}} images of various configurations of conics. We extract {{information about the}} plane from single and multiple views of known and unknown conics, based on planar homography and conic correspondences. We show that a single conic section cannot provide sufficient information. Metric <b>rectification</b> of the <b>plane</b> can be performed from a single view if two conics can be identified to be images of circles without knowing their centers or radii. The homography between two views of a planar scene can be computed if two arbitrary conics are identified in them without knowing anything specific about them. The scene can be reconstructed from a single view if images {{of a pair of}} circles can be identified in two planes. Our results are simpler and require less information from the images than previously known results. The results presented here involve univariate polynomial equations of degree 4 or 8 and always have solutions. Applications to metric rectification, homography calculation, 3 D reconstruction, and projective OCR are presented to demonstrate the usefulness of our scheme...|$|R
40|$|This paper {{introduces}} the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, <b>plane</b> <b>rectification</b> using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion {{into the camera}} model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Grobner-basis method are stable, small and fast. The proposed solvers are used in a RANSAC-based estimator. Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. Experiments confirm that RANSAC accurately estimates the rectification and radial distortion with very few iterations. The proposed solvers are evaluated against the state-of-the-art for affine rectification and radial distortion estimation. Comment: pre-prin...|$|R
40|$|Stereomatching is an {{effective}} way of acquiring dense depth information from a scene when active measurements are not possible. So-called lightfield methods take a snapshot from many camera locations along a defined trajectory (usually uniformly linear or on a regular grid—we will assume a linear trajectory) and use this information to compute accurate depth estimates. However, they require the locations {{for each of the}} snapshots to be known: the disparity of an object between images is related to both the distance of the camera to the object and the distance between the camera positions for both images. Existing solutions use sparse feature matching for camera location estimation. In this paper, we propose a novel method that uses dense correspondences to do the same, leveraging an existing depth estimation framework to also yield the camera locations along the line. We illustrate the effectiveness of the proposed technique for camera location estimation both visually for the <b>rectification</b> of epipolar <b>plane</b> images and quantitatively with its effect on the resulting depth estimation. Our proposed approach yields a valid alternative for sparse techniques, while still being executed in a reasonable time on a graphics card due to its highly parallelizable nature...|$|R
40|$|Optical {{systems for}} {{automatic}} visual inspections are of increasing {{importance in the}} field of automation in the industrial domain. A new application is the determination of steering wheel angles during wheel track setting of the final inspection of car manufacturing. The camera has to be positioned outside the car to avoid interruptions of the processes and therefore, oblique images of the steering wheel must be acquired. Three different approaches of computer vision are considered in this paper, i. e. a 2 D shape-based matching (by means of a <b>plane</b> to <b>plane</b> <b>rectification</b> of the oblique images and detection of a shape model with a particular rotation), a 3 D shape-based matching approach (by means of a series of different perspectives of the spatial shape of the steering wheel derived from a CAD design model) and a point-to-point matching (by means of the extraction of significant elements (e. g. multifunctional buttons) of a steering wheel and a pairwise connection of these points to straight lines). The HALCON system (HALCON, 2016) was used for all software developments and necessary adaptions. As reference a mechanical balance with an accuracy of 0. 1 ° was used. The quality assessment was based on two different approaches, a laboratory test and a test during production process. In the laboratory a standard deviation of ± 0. 035 ° (2 D shape-based matching), ± 0. 12 ° (3 D approach) and ± 0. 029 ° (point-to-point matching) could be obtained. The field test of 291 measurements (27 cars with varying poses and angles of the steering wheel) results in a detection rate of 100 % and ± 0. 48 ° (2 D matching) and ± 0. 24 ° (point-to-point matching). Both methods also fulfil the request of real time processing (three measurements per second) ...|$|R
40|$|SpaceImaging is not {{distributing the}} {{original}} IKONOS-images, only a <b>rectification</b> to a <b>plane</b> with constant height is available – {{the so called}} CARTERRA-GEO. In addition the sensor model is not published. For the image orientation rational polynomial coefficients (RPC) are available from SpaceImaging, giving {{the relation of the}} Geo-images to the ground coordinate system in form of geographic coordinates and the height. The RPCs have to be improved by means of control points. Another possibility is the reconstruction of the imaging geometry based on the available view direction from the scene centre to the satellite (nominal collection elevation and azimuth). If the view,direction will not be improved, with both methods {{as well as with the}} PCI-software using the satellite modelling, approximately the same, but not in any case satisfying accuracy has been achieved. The precise results can be reached if the view directions also will be introduced as unknowns. The orientation methods just based on control points and handling the geometric relation by rational polynomial functions or a 3 D-affine transformation (PCI terrain dependent solution) requires a higher number of well distributed control points. Extrapolations out of the volume of the control points may lead to not acceptable discrepancies. A DEM generation based on 2 IKONOS-images taken with a time interval of two month failed. Only in very limited areas a matching was possible because of quite different shadows and changing of the vegetation. On the other hand a stereo model taken with 12 seconds time interval was leading to excellent results. The main problem for the generation of orthoimages is caused by the digital elevation models (DEM). DEMs are often not accurate enough, causing a limitation of the possible range of nadir angles...|$|R
40|$|Vanishing {{points are}} {{elements}} {{of great interest}} in the computer vision ﬁeld, since they are {{the main source of}} information about the geometry of the scene and the projection process associated to the camera. They have been studied and applied during decades for <b>plane</b> <b>rectiﬁcation,</b> 3 D reconstruction, and mainly auto-calibration tasks. Nevertheless, the literature lacks accurate online solutions for multiple vanishing point estimation. Most strategies focalize on the accuracy, using highly computational demanding iterative procedures. We propose a novel strategy for multiple vanishing point estimation that ﬁnds a trade-oﬀ between accuracy and eﬃciency, being able to operate in real time for video sequences. This strategy takes advantage of the temporal coherence of the images of the sequences to reduce the computational load of the processing algorithms while keeping a high level of accuracy due to an optimization process. The key element of the approach is a robust scheme based on the MLESAC algorithm, which is used in a similar way to the EM algorithm. This approach ensures robust and accurate estimations, since we use the MLESAC in combination with a novel error function, based on the angular error between the vanishing point and the image features. To increase the speed of the MLESAC algorithm, the selection of the minimal sample sets is substituted by a random sampling step that takes into account temporal information to provide better initializations. Besides, for the sake of ﬂexibility, the proposed error function has been designed to work using as image features indiscriminately gradient-pixels or line segments. Hence, we increase the range of applications in which our approach can be used, according to the type of information that is available. The results show a real-time system that delivers real-time accurate estimations of multiple vanishing points for online processing, tested in moving camera video sequences of structured scenarios, both indoors and outdoors, such as rooms, corridors, facades, roads, etc...|$|R

