13|111|Public
40|$|The {{literature}} that uses an optimality-theoretic framework to examine phonological variation in endangered language varieties is limited. Besides Auger and Steele (1999) and Auger (2001), little {{work has been}} done mapping <b>raw</b> <b>frequencies</b> from dying languages into optimality-theoretic constraint hierarchies, even though the inter- and intra-speaker variation and multiple acceptable surface form...|$|E
40|$|A rootogram is a {{graphical}} {{data analysis}} technique for summarizing the distributional information of a variable. It consists of: Vertical axis = square root of frequencies or relative frequencies; Horizontal axis = response variable. There are 4 types of rootograms: 1. rootogram (absolute counts); 2. relative rootogram (converts counts to proportions); 3. cumulative rootogram; 4. cumulative relative rootogram. The rootogram is {{a modified version}} of a histogram. It plots the square roots of the frequencies rather than the <b>raw</b> <b>frequencies.</b> Many univariate data sets can be normalized with a square root transformation (particularly counts or measurement data that have a lower bound and tend to be skewed at the upper tail) ...|$|E
40|$|In this paper, we {{investigate}} {{the adaptation of}} anglicisms to the recipient language in a weak contact situation, focusing on actual usage. More specifically, {{we investigate}} gender assignment to recently borrowed English loanwords in Dutch. Studies on gender assignment to anglicisms have so far mostly relied on <b>raw</b> <b>frequencies</b> or simple proportions in dictionary or corpus data. As a result, gender assignment to loanwords is often conceived as a categorical process: the gender that is listed in a dictionary or found in a corpus {{is said to be}} determined by a clear set of (receptor) language-related agreement rules (see for instance Corbett 1991; Onysko 2007...|$|E
5000|$|... {{augmented}} frequency, {{to prevent}} a bias towards longer documents, e.g. <b>raw</b> <b>frequency</b> divided by the <b>raw</b> <b>frequency</b> of the most occurring term in the document: ...|$|R
3000|$|... [...]) {{is equal}} to the <b>raw</b> <b>frequency</b> of a stemmed word form, with n (the number of feature functions) equal to the number of {{distinct}} words remaining after document processing.|$|R
5000|$|Collostructional {{analysis}} requires {{frequencies of}} words and constructions and {{is similar to a}} wide variety of collocation statistics. It differs from <b>raw</b> <b>frequency</b> counts by providing not only observed co-occurrence frequencies {{of words and}} constructions, but also ...|$|R
40|$|Current {{studies on}} {{content-based}} image retrieval mainly rely on bags of visual words. This model of image description allows to perform image retieval {{in the same}} way as text retrieval: documents are described as vectors of (visual) word frequencies, and documents are match by computing a distance or similarity measure between the vectors. But instead of <b>raw</b> <b>frequencies,</b> documents can also be described as vectors of word weights, each weight corresponding to the importance of the word in the document. Although the problem of determining automatically such weights, and therefore which words describe well documents, has been widely studied in the case of text retrieval, there is very little litterature applying this idea to the case of image retrieval. In this report, we explore how the use of standard weighting schemes and distance from text retrieval can help to improve the performance of image retrieval systems. We show that there is no distance or weighting scheme that can improve performance on any dataset, but choosing weights or a distance consistent with some properties of a given dataset can improve the performance up to 10 %. However, we also show that in the case of very varied and general datasets, the performance gain is not significant...|$|E
40|$|Loanword use has {{dominated}} the literature on language contact and its salient nature continues to draw interest from linguists and non-linguists. Traditionally, loanwords were investigated by means of <b>raw</b> <b>frequencies,</b> which are at best uninformative and at worst misleading. Following {{a new wave of}} studies which look at loans from a quantitatively more informed standpoint, modelling “success” by taking into account frequency of the counterparts avail- able in the language adopting the loanwords, we propose a similar model of loan-use and demonstrate its benefits in a case study of loanwords from Māori into (New Zealand) English. Our model contributes to previous work in this area by combining both the success measure mentioned above with a rich range of linguistic characteristics of the loanwords (such as loan length and word class), as well as a similarly detailed group of sociolinguistic characteristics of the speakers using them (gender, age and ethnicity of both, speakers and addresses). Our model is unique in bringing together of all these factors at the same time. The findings presented here illustrate the benefit of a quantitatively balanced approach to modelling loanword use. Furthermore, they illustrate the complex interaction between linguistic and sociolinguistic factors in such lan- guage contact scenarios...|$|E
40|$|We {{report on}} an ongoing {{investigation}} into a seismic calibration of solar models designed for estimating the main-sequence age and {{a measure of the}} chemical abundances of the Sun. Only modes of low degree are employed, so that with appropriate modification the procedure could be applied to other stars. We have found that, as has been anticipated, a separation of the contributions to the seismic frequencies arising from the relatively smooth, glitch-free, background structure of the star and from glitches produced by helium ionization and the abrupt gradient change {{at the base of the}} convection zone renders the procedure more robust than earlier calibrations that fitted only <b>raw</b> <b>frequencies</b> to glitch-free asymptotics. As in the past, we use asymptotic analysis to design seismic signatures that are, to the best of our ability, contaminated as little as possible by those uncertain properties of the star that are not directly associated with age and chemical composition. The calibration itself, however, employs only numerically computed eigenfrequencies. It is based on a linear perturbation from a reference model. Two reference models have been used, one somewhat younger, the other somewhat older than the Sun. The two calibrations, which use BiSON data, are more-or-less consistent, and yield a main-sequence age t_= 4. 68 ± 0. 02 Gy, coupled with a formal initial heavy-element abundance Z= 0. 0169 ± 0. 0005. The error analysis has not yet been completed, so the estimated precision must be taken with a pinch of salt. Comment: 8 pages, 3 figures, in L. Deng, K. L. Chan, C. Chiosi, eds, The Art of Modelling Stars in the 21 st Century, Proc. IAU Symp. No. 252, invited contributed pape...|$|E
5000|$|In its <b>raw</b> <b>{{frequency}}</b> form, tf is {{just the}} frequency of the [...] "this" [...] for each document. In each document, the word [...] "this" [...] appears once; but as the document 2 has more words, its relative frequency is smaller.|$|R
40|$|The {{increasing}} {{complexity of}} summarization systems {{makes it difficult}} to analyze exactly which modules make a difference in performance. We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization: <b>raw</b> <b>frequency</b> (word probability) and log-likelihood ratio. We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input. We also find that LLR is more suitable for query-focused summarization since, unlike <b>raw</b> <b>frequency,</b> it is more sensitive to the integration of the information need defined by the user. ...|$|R
30|$|The model {{construction}} {{takes place in}} several steps. First, the text data is pre-processed and feature selection can be applied. The context word frequencies are calculated, and <b>raw</b> <b>frequency</b> counts are transformed by weighting. Dimensionality reduction {{can be applied to}} smooth the space. Finally, the similarities between word vectors are calculated by using a vector distance measure (Turney and Pantel 2000).|$|R
40|$|Abstract: Current {{studies on}} {{content-based}} image retrieval mainly rely on bags of visual words. This model of image description allows to perform image retieval {{in the same}} way as text retrieval: documents are described as vectors of (visual) word frequencies, and documents are match by computing a distance or similarity measure between the vectors. But instead of <b>raw</b> <b>frequencies,</b> documents can also be described as vectors of word weights, each weight corresponding to the importance of the word in the document. Although the problem of determining automatically such weights, and therefore which words describe well documents, has been widely studied in the case of text retrieval, there is very little litterature applying this idea to the case of image retrieval. In this report, we explore how the use of standard weighting schemes and distance from text retrieval can help to improve the performance of image retrieval systems. We show that there is no distance or weighting scheme that can improve performance on any dataset, but choosing weights or a distance consistent with some properties of a given dataset can improve the performance up to 10 %. However, we also show that in the case of very varied and general datasets, the performance gain is not significant. Key-words: Weighting schemes, content-based image retrieval, bag of visual words, information retrieval, vector space model, probabilistic model, Minkowski distance, divergence from randomness, BM 25, TF*IDF Un aperçu des schémas de pondération pour la recherche d’images par sac de mots visuels Résumé: Les travaux actuels en recherche d’image par le contenu se basent sur un modèle en sac de mots visuels...|$|E
40|$|This article {{presents}} and illustrates a new methodology for testing hypotheses about {{the departure of}} marriage choices from baseline models of random mating in an actual kinship and marriage network of a human population. The fact that demographic constraints can drastically affect the <b>raw</b> <b>frequencies</b> {{of different types of}} marriage suggests that we must reexamine or even throw out - as methodologically flawed - statistical conclusions regarding marriage "rules" from most of the existing empirical case studies. The development of the present methods, in contrast, enables researchers to decompose those behavioral tendencies that can be taken as agent-based social preferences, institutional "rules" or marriage structure from those behaviors whose divergent frequencies are merely a by-product or epiphenomena of demographic constraints on the availability of potential spouses. The family of random baseline models used here enables a researcher to identify overall global structures of marriage rules such as dual organization as well as more local of egocentric rules such as rules favoring marriage with certain kinds of relatives. Based on random permutations of the actual data in a manner that controls for the effects of demographic factors across different cases, the new methods are illustrated for three case studies: a village in Sri Lanka with a novel form of dual organization detected by this methodology, a cross-class analysis of a village in Indonesia, and an analysis of a farming village in Austria in which a structurally endogamous subset of villages is identified by the method and shown to form the backbone of a class-based landed property system. Population Studies, Marriage Rules, Demographic Constraints on Choice Behavior, Social Class, Social Anthropology...|$|E
40|$|According to Biber’s definition, {{text types}} are {{represented}} by groupings of texts which are similar in their linguistic form, while genre categories are assigned {{on the basis of}} use. It is important to stress that texts from a single genre might be classified into different text types. In the present research, an experiment will be carried out to single out different text typologies among Italian press subgenres only on the basis of morphosyntactic features. The approach will be corpus-based and the aim merely exploratory. A virtually random sample will be extracted from two Italian tagged corpora, the sample divided into 22 files, a file for each subgenre, and 41 morphosyntactic features will be counted per article within each subgenre. The <b>raw</b> <b>frequencies</b> will be normalised. The dataset built from the normalised frequencies (quantitative variables) will be submitted to Descriptive Statistics and Factor Analysis. Descriptive statistics give information about the distribution of the variables. It is a preliminary step in any exploration and gives {{a better understanding of the}} set of data. Factor analysis studies the correlations among a large number of interrelated quantitative variables by grouping these variables into a small number of factors, which help to understand the structure of correlations or the underlying construct. The aim of this research is to investigate to which extent statistical techniques can help in classifying texts in a steady and reliable way. Author's note. This paper is an excerpt taken from my master dissertation and adapted to the length of an article. It represents only one of the exploratory experiments I carried out for that work. The experiment described in this paper suffers from some limitations, for instance, the sample is extremely restricted (only 144, 593 words), the domain (press subgenres) not very differentiated, the number of morphosyntactic features quite high, the outliers excluded from the final considerations and so on. However, regardless these limitations, this experiment opens some interesting prospects on the handling of Italian corpora. I wish to thank the revisers of this article for their useful, appropriate and accurate suggestions. 2...|$|E
40|$|I hereby {{declare that}} I am the sole {{author of this}} thesis. This is a true copy of the thesis, {{including}} any required final revisions, as accepted by my examiners. I understand that my thesis may be made electronically available to the public. iii In this thesis, we present a new clustering algorithm we call Significance Feature Clustering, {{which is designed to}} cluster text documents. Its central premise is the mapping of <b>raw</b> <b>frequency</b> count vectors to discrete-valued significance vectors which contain values of- 1, 0, or 1. These values represent whether a word is significantly negative, neutral, or significantly positive, respectively. Initially, standard tf-idf vectors are computed from <b>raw</b> <b>frequency</b> vectors, then these tf-idf vectors are transformed to significance vectors using a parameter α, where α controls the mapping- 1, 0, or 1 for each vector entry. SFC clusters agglomeratively, with each document’s significance vector representing a cluster of size one containing just the document, and iteratively merges the two clusters that exhibit the most simila...|$|R
40|$|Abstract: We {{introduce}} a new macromodeling scheme for electrically long interconnects characterized by tabulated frequency responses. The transfer function of the interconnect is modeled as a superposition of multiple single-delay atoms, which are identified via a selective inversion of a Gabor transform of the <b>raw</b> <b>frequency</b> data. Each atom is then approximated by a delayed rational function, leading to a highly-efficient SPICE-ready macromodel. 1 Introduction an...|$|R
3000|$|We {{incorporate}} the common textual term weighting schemes in the BOVW model. The first weighting {{factor is the}} <b>raw</b> term <b>frequency</b> (tf [...]...|$|R
40|$|Universities {{have entered}} {{a period of}} rapid change and {{upheaval}} due to an external environment beyond their control which includes shifting demographic patterns, accelerating technology, funding shortages, and keener competition for students. Strategic planning, a comprehensive vision which challenges universities to take bold and creative measures to meet the threats and opportunities of the future, is an institutional imperative in the 1980 's. This paper examines freshman student feedback {{in an effort to}} incorporate this important element into a strategic plan for Brock University, a small, predominantly liberal arts university in St. Catharines, Ontario. The study was designed to provide information on the characteristics of the 1985 - 86 pool of freshman registrants: their attitudes towards Brock's recruitment measures, their general university priorities, and their influences in regard to university selection (along with other demographical and attitudinal data). A survey involving fixed-alternative questions of a subjective and objective nature was administered in two large freshman classes at Brock in which a broad cross-section of academic programs was anticipated. Computer analysis of the data for the 357 respondents included total <b>raw</b> <b>frequencies</b> and rounded percentages, as well as subgroup cross-tabulation by geographic home area of respondent, academic major, and high school graduating average. The four directional hypotheses put forward were all substantiatied by the survey data, indicating that 1) the university's current recruitment program had been a positive influence during their university search 2) parents were the most influential group in the students' decisions related to university 3) respondents viewed institutional reputation as less of a priority than an enjoyable university lifestyle in a personal learning atmosphere 4) students had a decided preference for co-operative study and internship programs. Strategic planning recommendations included a reduction in the faculty/student ratio through faculty hirings to restore the close rapport between professors and students, increased recruitment presentations in Ontario high schools to enlarge the applicant pool, creation of an Office of Co-operative Study and Internship Programs, institutional emphasis on a "customer orientation", and an extension of research into student demographics and attitudinal data...|$|E
40|$|This {{study was}} {{designed}} to investigate the correlation of the variables of gender, Myers-Briggs Type Indicator (MBTI) personality preferences, and androgyny as measured by the Bem Sex Role Inventory (BSRI) in Counselor Education graduate students. Instruments were administered to Counselor Education graduate students at nine institutions in five national regions. A total of 172 participants (18 males and 154 females) who were enrolled in Master's level theories courses or practicum courses completed a student information sheet, informed consent, MBTI, and BSRI. Instruments were hand scored and chi-square test was used to determine significance of the hypotheses; the saturated model of log linear analysis was the statistic used for the research question. As predicted, of the sixteen MBTI types, the most common for Counselor Education graduate students emerged as ENFP: extraversion, intuition, feeling, and perception. Additionally, this MBTI type was found to be significantly more common among the population of Counselor Education graduate students than is found among the general population. The expectation that more male Counselor Education graduate students would score higher on the androgyny scale of the BSRI was unsupported; low sample size for male Counselor Education graduate students prevented use of chi-square; however, it was apparent through the use of the statistic of <b>raw</b> <b>frequencies</b> that males clustered around every other category except androgyny. The hypothesis that more female Counselor Education graduate students would score higher on the feminine scale was also unsupported, as equal distribution of the females occurred within all four categories of the BSRI. It was hypothesized that males with a sensing and thinking preference on the MBTI would tend toward the masculine dimension of the BSRI more than males with an intuitive and feeling preference. This was unsupported as well. Female Counselor Education graduate students with an intuitive and feeling preference did, however, demonstrate a greater tendency toward the feminine classification on the BSRI than did females with a sensing and thinking preference, so that this hypothesis was retained. No significant relationship was found between the variables of MBTI type, BSRI classification, and gender...|$|E
40|$|In this paper, I {{focus on}} four degree-modifiers of adjectives: rather, quite, fairly, and pretty. These adverbs form a {{subclass}} of intensifiers (Bolinger, 1972; Lyons, 1977) known as 'compromisers' (Quirk et al., 1985) or 'moderators' (Allerton, 1987; Paradis, 1997) : (1) I think it's fairly easy {{for anyone to}} get anything they want ( [...] .). (COCA) (2) He seemed in pretty good form. (COCA) (3) Obama' s whole pitch is quite different. (COCA) (4) I heard a rather odd conversation. (COCA) Paradis (1994 : 160; 1997 : 71) contends that moderators are cognitive synonyms (after Cruse, 1986) because they all serve to fix the properties that gradable adjectives denote to a moderate level. Although moderators are not interchangeable in all contexts, cognitive synonymy is nevertheless evidenced in corpus by {{a significant amount of}} overlapping collocational preferences. I want to test the claim that moderators are cognitive synonyms by contrasting them in their distinctive collocational preferences. I thus hope to avoid the problems faced by previous corpus-based approaches to degree modifiers, which rely too much on <b>raw</b> <b>frequencies</b> or coarse-grained relative frequencies, and therefore underestimate significant context-related differences (Paradis, 1997; Lorenz, 2002; Kennedy, 2003; Simon-Vandenbergen, 2008). I extract all combinations from the 410 M-word Corpus of Contemporary American English (Davies, 2008 -) and implement two techniques from collostructional analysis. First, I conduct a collexeme analysis to determine which adjectives are most strongly attracted to each moderator (Stefanowitsch & Gries, 2003). My results show that some adjectives, such as good, large, different or clear, display collocational preferences with more than one moderator. At this stage, the cognitive synonymy of moderators is partly verified and cannot be dismissed. Then I perform a multiple distinctive collexeme analysis to determine the adjectives that are distinctively associated with each moderator (Gries & Stefanowitsch, 2004). It turns out that moderators attract different classes of adjectives. For example, fairly attracts adjectives that denote novelty (new, recent), commonness or constancy (common, constant, consistant, regular), whereas rather attracts adjectives that denote problematic identification (odd, unusual, strange, bizarre, etc.) By examining how moderated adjectives fall into distinct semantic classes, my corpus study eventually undermines the claim that moderators are cognitive synonyms. Conversely, my results substantiate and refine two other assumptions. The first assumption is that moderators have undergone subjectification (Nevalainen & Rissanen, 2002; Athanasiadou, 2007). Indeed, not only do moderators index the speaker's assessment of intensity on an abstract scale, but they also index the speaker's perspective differently depending on what property is moderated. For instance, speakers tend to modify easiness with fairly and difficulty with pretty or quite. The second assumption is that moderators are intersubjective (Paradis, 1997 : 69). My data confirm that moderators typically hedge epistemic assessments of scalar properties in contexts where their absence would be face-threatening (e. g. rather simplistic/lengthy) or where the speaker lacks evidence for an objective statement (e. g. quite correct, pretty smart) ...|$|E
40|$|This paper {{illustrates}} how different methodological approaches {{can be combined}} to reveal complex patterns of constructional variation and change in the diachronic development of English ing-nominals. More specifically, we argue that approaching the data from a schema-based (rather than morpheme-based) perspective shows that nominal gerunds in English, from the sixteenth to the nineteenth century, have undergone a semantic drift towards more “nouny” construal variants. This hypothesis is supported not only by <b>raw</b> <b>frequency</b> counts, but also by association measures and by {{a detailed analysis of}} hapax legomena...|$|R
40|$|Flow {{diagram of}} (σ_xx, σ_xy) in finite-frequency (ω) regime is numerically studied for {{graphene}} quantum Hall effect (QHE) system. The ac flow diagrams {{turn out to}} show qualitatively similar behavior as the dc flow diagrams, which can be understood that the dynamical length scale determined by the frequency poses a relevant cutoff for the renormalization flow. Then the two parameter flow is {{discussed in terms of}} the dynamical scaling theory. We also discuss the larger-ω regime which exhibits classical flows driven by the <b>raw</b> <b>frequency</b> ω. Comment: 6 pages, 4 figure...|$|R
50|$|The {{analysis}} of <b>raw</b> fundamental <b>frequency</b> curves {{for the study}} of intonation needs {{to take into account the}} fact that speakers are simultaneously producing an intonation pattern and a sequence of syllables made up of segmental phones. The actual <b>raw</b> fundamental <b>frequency</b> curves which can be analysed acoustically are the result of an interaction between these two components and this makes it difficult to compare intonation patterns when they are produced with different segmental material. Compare for example the intonation patterns on the utterances It's for papa and It's for mama.|$|R
40|$|A {{large body}} of {{research}} findings suggests that verbal cues to deception can boost deception-detection accuracy rates to levels significantly above chance. This thesis examines the effectiveness of, and influences on, {{one of the most}} popular and widely used verbal lie-detection approaches, Reality Monitoring (RM). The RM approach has advantages not only in terms of its underpinnings in memory research and theory, but also its ease with regard to practical application. The RM approach assumes that deceptive verbal accounts, because they are artificial and not based directly on actual experience, differ from truthful accounts according to a variety of criteria (truthful accounts contain more vivid, spatial, temporal and affective information, etc.). However, so far, as in many other areas of lie-detection research, research in the area of RM has lacked methodological standardisation; consequently, we know little of the potential effects of contextual and other moderating variables on RM measures. In view of this, the primary aims of the present thesis were a) to assess whether, if conveyed in a standard format, the RM approach has any value overall in distinguishing between truthful and deceptive accounts, and b) if it does, to investigate the circumstances under which it might give optimal results; i. e. to assess some of the main factors which may moderate its efficacy in this respect. To these ends, six experimental studies were conducted which looked at truthful and deceptive accounts (generated by participants in the laboratory using video and autobiographical sources) and considered the effects of a number of different moderators; these included, first and second-language effects, modality (i. e. written vs. spoken discourse), absence/presence of others, demand characteristics effects, scoring systems (rating scales vs. <b>raw</b> <b>frequencies)</b> and standardisation for account length. Overall, results indicated that in most studies there was evidence that total RM scores, as determined by the procedures applied here, successfully discriminated between truthful and deceptive accounts. However, results varied when RM criteria were considered individually, and when the influence of various moderators was assessed. For example, frequency measures of spatial and temporal information were found to be two of the most consistently effective diagnostic RM criteria. However, overall, RM was a more effective diagnostic tool before accounts were standardised for length; indeed, total RM scores failed to distinguish between truthful and deceptive accounts after accounts had been standardised for length. Also, the presence of others and modality (written or spoken) were two key moderator variables whose impact on total RM scores varied depending on whether or not the accounts were standardised for length. A number of other related variables were also considered; for example, truthful accounts were longer than deceptive accounts in both duration and length and the number of words produced per second was significantly greater for truthful accounts. Implications for research and practise are discussed; though perhaps most important in this respect was the finding that, despite the overall success of RM in discriminating truthful from deceptive accounts, RM criteria were not generally discriminating after standardisation for word-count or length. Moreover, a number of the moderators affected RM scores regardless of whether they were derived from truthful or deceptive accounts. This suggests that we may still be a long way from developing a method (such as the use of normative criteria) that could be used in the field to classify individual cases. Nevertheless, in the meantime, at the very least, the present results suggest that when judging the veracity of accounts using RM criteria, the scoring and other moderating variables identified in this thesis should be investigated systematically, and measured and applied consistently, if researchers wish to compare and replicate findings within and across studies...|$|E
40|$|This paper {{describes}} the term frequency patterns found in online news summaries published over a seven-week period. The patterns are analyzed qualitatively and quantitatively {{to facilitate the}} refinement of algorithms used for the automatic detection and tracking of important topics appearing in streams of text. It is shown that a term’s importance cannot be measured in <b>raw</b> <b>frequency</b> counts or significant increases in volume alone. The impact of these findings on existing algorithms is discussed, and new approaches for automated story detection and presentation are considered. Headings: Information retrieval Information retrieval – filterin...|$|R
40|$|Extensive {{research}} has been conducted on query log analysis. A query log is generally represented as a bipartite graph on a query set and a URL set. Most of the traditional methods used the <b>raw</b> click <b>frequency</b> to weigh the link between a query and a URL on the click graph. In order to address the disadvantages of <b>raw</b> click <b>frequency,</b> researchers proposed the entropy-biased model, which incorporates <b>raw</b> click <b>frequency</b> with inverse query frequency of the URL as the weighting scheme for query representation. In this paper, we observe that the inverse query frequency can be considered a global property of the URL on the click graph, which is more informative than <b>raw</b> click <b>frequency,</b> which can be considered a local property of the URL. Based on this insight, we develop the global consistency model for query representation, which utilizes the click frequency and the inverse query frequency of a URL in a consistent manner. Furthermore, we propose a new scheme called inverse URL frequency as an effective way to capture the global property of a URL. Experiments have been conducted on the AOL search engine log data. The result shows that our global consistency model achieved better performance than the current models. Comment: accepted by Journal of Internet Technology on Sep. 9, 2012. To appear in Vol. 4, September, 201...|$|R
40|$|Taking Mandarin Possessive Construction (MPC) as an example, {{the present}} study investigates the {{relation}} between lexicon and constructional schemas in a quantitative corpus linguistic approach. We argue that the wide use of <b>raw</b> <b>frequency</b> distribution in traditional corpus linguistic studies may undermine the validity of the results and reduce the possibility for interdisciplinary communication. Furthermore, several methodological issues in traditional corpus linguistics are discussed. To mitigate the impact of these issues, we utilize phylogenic hierarchical clustering to identify semantic classes of the possessor NPs, thereby reducing the subjectivity in categorization that most traditional corpus linguistic studies suffer from. It is hoped that our rigorous endeavor in methodology may have far-reaching implications for theory in usage-base...|$|R
30|$|Gisler et al. [14] {{proposed}} a glaucoma detection technique using intraocular pressure monitoring. The data was supervised by Sensimed Company where contact lens sensors (CLS) {{were used to}} automate recording the continuous ocular dimensional changes over 24  h. The CLS system is safe and non-invasive. However, a health care professional is required to install it and remove it from the patient. The authors used Java software to manage the data and feature extraction. The feature extraction was split into two parts. The first part included statistical features (<b>raw</b> <b>frequency</b> values and filter banks), {{and the second part}} consisted of physiological features (eye blinks, ocular pulse, and slope of the curve), which were fed to a support vector machine (SVM) learning technique and classifier [15].|$|R
40|$|We {{present a}} new {{database}} of Dutch word frequencies based on {{film and television}} subtitles, and we validate it with a lexical decision study involving 14, 000 monosyllabic and disyllabic Dutch words. The new SUBTLEX frequencies explain up to 10 % more variance in accuracies and reaction times (RTs) of the lexical decision task than the existing CELEX word frequency norms, which are based largely on edited texts. As {{is the case for}} English, an accessibility measure based on contextual diversity explains more of the variance in accuracy and RT than does the <b>raw</b> <b>frequency</b> of occurrence counts. The database is freely available for research purposes and may be downloaded from the authors' university site at [URL] or from [URL]...|$|R
40|$|In this thesis, {{we present}} a new {{clustering}} algorithm we call Significance Feature Clustering, {{which is designed to}} cluster text documents. Its central premise is the mapping of <b>raw</b> <b>frequency</b> count vectors to discrete-valued significance vectors which contain values of - 1, 0, or 1. These values represent whether a word is significantly negative, neutral, or significantly positive, respectively. Initially, standard tf-idf vectors are computed from <b>raw</b> <b>frequency</b> vectors, then these tf-idf vectors are transformed to significance vectors using a parameter alpha, where alpha controls the mapping - 1, 0, or 1 for each vector entry. SFC clusters agglomeratively, with each document's significance vector representing a cluster of size one containing just the document, and iteratively merges the two clusters that exhibit the most similar average using cosine similarity. We show that by using a good alpha value, the significance vectors produced by SFC provide an accurate indication of which words are significant to which documents, as well as the type of significance, and therefore correspondingly yield a good clustering in terms of a well-known definition of clustering quality. We further demonstrate that a user need not manually select an alpha as we develop a new definition of clustering quality that is highly correlated with text clustering quality. Our metric extends the family of metrics known as internal similarity, {{so that it can be}} applied to a tree of clusters rather than a set, but it also factors in an aspect of recall that was absent from previous internal similarity metrics. Using this new definition of internal similarity, which we call maximum tree internal similarity, we show that a close to optimal text clustering may be picked from any number of clusterings created by different alpha's. The automatically selected clusterings have qualities that are close to that of a well-known and powerful hierarchical clustering algorithm...|$|R
40|$|In {{this article}} some {{classical}} theories concerning adjective position in modern French are {{compared with the}} data that online corpora can reveal. Indeed, having few examples or closed noncomputerized corpora at their disposal, researchers of the “noncomputerized” era sought, with some success, to establish general and generalizable values to explain adjective position in modern French. It turns out, however, {{that a number of}} these generalizations are contradicted by the findings revealed by the corpora. We will briefly highlight them and then indicate to what extent corpus-based study, especially that availing of almost unlimited online corpora, can lead {{to a better understanding of}} this phenomenon. A new way of analyzing adjective position, based on statistics rather than mere <b>raw</b> <b>frequency,</b> seems today quite feasible thanks to online corpora and referentials...|$|R
50|$|Rennie et al. discuss {{problems}} with the multinomial assumption {{in the context of}} document classification and possible ways to alleviate those problems, including the use of tf-idf weights instead of <b>raw</b> term <b>frequencies</b> and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.|$|R
40|$|AbstractAmong the {{challenges}} in developing terminology systems is providing complete content coverage of specialized subject fields. This paper reports on a term extraction tool designed for the development and expansion of terminology systems concerned with functioning, disability, and health. Content relevant to this domain is the emphasis of the foci and targets of many nursing terminologies. We extend previously published term extraction algorithms by applying two filters. The first filter {{is based on the}} <b>raw</b> <b>frequency</b> of the content words in the lexical string under consideration. The second filter applies the notion of a complete syntactic node to discover relevant noun or verb phrases. While we report on a limited corpus (30, 607 words comprising 4103 terms from 60 dismissal note summaries), the recall, precision, and F-measures we observed are encouraging and suggest continued development and testing of the tool is merited...|$|R
40|$|This study {{investigated}} the social network structure of booking officers at the Honolulu Police Department and how the introduction of an online discussion tool affected knowledge about operation of a booking module. Baseline data provided evidence for collaboration among officers in the same district using e-mail, telephone and face-toface media but showed minimal collaboration between officers in different districts. On average, knowledge of the booking module was low. After introduction of the online discussion tool the social network structure changed, showing an increase in collaboration between different districts {{and an increase in}} knowledge of the booking module, even though frequency of collaboration did not increase significantly. The study suggests that the formation of new collaborative ties and passive participation (“lurking”) are more significant for learning through information sharing in social networks than <b>raw</b> <b>frequency</b> of interaction. 1...|$|R
40|$|Since the probes in the PS {{reference}} magnet {{that generate}} the so-called B-train are fairly short, they cannot register {{any change in}} magnetic length due to saturation. Hence the idea to derive the effective dipole magnetic field seen by the beam from measurements of revolution frequency and mean radial position over an entire cycle, to fit a saturation law, {{and to use the}} result to make a new frequency programme. Although far from new, the idea has never been implemented due to the tacit assumption that any imperfections in the existing frequency programme are taken care of by the action of the servo loops of the various beam controls. More recently, the delivery of ions at low energy from LEIR has called into question the accuracy the <b>raw</b> <b>frequency</b> programme and the idea has been revisited in a brief parasitic MD...|$|R
