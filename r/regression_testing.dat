1145|6346|Public
25|$|Orthogonal array {{testing is}} a black box testing {{technique}} which is a systematic, statistical way of software testing. It is used {{when the number of}} inputs to the system is relatively small, but too large to allow for exhaustive testing of every possible input to the systems. It is particularly effective in finding errors associated with faulty logic within computer software systems. Orthogonal arrays can be applied in user interface testing, system testing, <b>regression</b> <b>testing</b> and performance testing.|$|E
2500|$|An F-16 was {{successfully}} refueled on 8 July, and a C-17 on 12 July 2016. Once the hardware fix is verified, a KC-46 with the updated boom will complete <b>regression</b> <b>testing</b> on the F-16, followed by refueling demonstrations with the C-17 and A-10 {{to meet the}} final test for Milestone C approval. USAF chief of staff, General David Goldfein, commented [...] "While it took some time, this week's results confirm my confidence the Boeing team will get this figured out. It's reassuring to see the program take this important step toward the production decision in August." [...] On 15 July 2016 the KC-46 successfully refueled an A-10; during a four-hour flight, the KC-46 offloaded 1,500 pounds of fuel at 15,000 feet. [...] "This completed the required air refueling demonstrations needed for the upcoming production milestone decision," [...] said Boeing spokesman Chick Ramey. To date, more than 900 flight test hours have been completed with the five test aircraft.|$|E
50|$|<b>Regression</b> <b>testing.</b> White-box testing during <b>regression</b> <b>testing</b> {{is the use}} of {{recycled}} white-box test cases at the unit and integration testing levels.|$|E
40|$|<b>Regression</b> <b>test</b> {{selection}} {{techniques for}} embedded programs have scarcely {{been reported in}} the literature. In this paper, we propose a model-based <b>regression</b> <b>test</b> selection technique for embedded programs. Our proposed model, in addition to capturing the data and control dependence aspects, also represents several additional program features that are important for <b>regression</b> <b>test</b> case selection of embedded programs. These features include control flow, exception handling, message paths, task priorities, state information and object relations. We select a <b>regression</b> <b>test</b> suite based on slicing our proposed graph model. We also propose a genetic algorithmbased technique to select an optimal subset of test cases from the set of <b>regression</b> <b>test</b> cases selected after slicing our proposed model...|$|R
3000|$|... “[TDD] I {{think it}} is has a big {{relation}} with code quality and <b>regression</b> <b>tests.</b> Two main advantages I have when practicing: the code gets better and the <b>regression</b> <b>tests</b> allow me to safely refactor.” [...]...|$|R
40|$|Aspect-oriented {{software}} evolution introduces {{new challenges}} for <b>regression</b> <b>test</b> selection. When a program, {{that has been}} thoroughly tested, evolves by addition of an aspect, {{it is important for}} <b>regression</b> <b>test</b> selection to know which test cases are impacted by the new aspects and which are not. The work presented here proposes a classification for <b>regression</b> <b>test</b> cases and introduces an algorithm for impact analysis of aspects on a set of test cases. A major benefit of this analysis is that it saves the execution of test cases that are not impacted. 1...|$|R
50|$|<b>Regression</b> <b>testing</b> is used {{to check}} if any new bugs have been {{introduced}} through previous bug fixes. <b>Regression</b> <b>testing</b> is conducted after every change or update in the software features. This testing is periodic, depending on the length and features of the software.|$|E
50|$|The {{purpose of}} <b>regression</b> <b>testing</b> {{is to ensure}} that changes such as those {{mentioned}} above have not introduced new faults. One of the main reasons for <b>regression</b> <b>testing</b> is to determine whether a change {{in one part of the}} software affects other parts of the software.|$|E
5000|$|Oracle Functional Testing for {{automated}} {{functional and}} <b>regression</b> <b>testing.</b>|$|E
40|$|<b>Regression</b> <b>tests</b> {{ensure that}} {{software}} systems retain correctness in their existing functionalities while the systems evolve. During development, <b>regression</b> <b>tests</b> allow the programmer to quickly identify problems. If {{there are any}} new failed tests, since those failures surfaced after new changes to the code base, we need to identify where this failure originated in our code changes. In this paper, we combine <b>regression</b> <b>test</b> results with changes in both the source code and test execution profiles to narrow down and identify the changes that may have caused test failures...|$|R
40|$|In this research, {{we present}} an {{extension}} to an existing requirement-based <b>regression</b> <b>test</b> suite reduction approach that uses EFSM model dependence analysis to reduce a given <b>regression</b> <b>test</b> suite [19]. The approach {{is based on}} the difference between the original model and the modified model expressed as a set of elementary model modifications: elementary addition of a transition and elementary deletion of a transition. For each elementary modification, the data and control dependencies are used to capture potential interactions between EFSM transitions. The potential interactions are used to reduce an existing <b>regression</b> <b>test</b> suite by eliminating repetitive tests. In this thesis, based on [19], we have defined some new dependencies introduced by elementary modifications of the EFSM model. We proposed algorithms to obtain these dependencies; to generate potential interactions with respect to (wrt) an elementary modification; and to reduce the <b>regression</b> <b>test</b> suite. We have also developed a Regression Test Suite Reduction tool, called RTSR based on these algorithms to be used for reducing the size of existing <b>regression</b> <b>test</b> suites. RTSR has been tested and validated. (Abstract shortened by UMI. ...|$|R
2500|$|Multivariate {{modified}} Poisson <b>regression</b> <b>test</b> (fitted {{to traditional}} medicine) ...|$|R
50|$|Common {{methods of}} <b>regression</b> <b>testing</b> include re-running {{previously}} completed tests and checking whether program behavior {{has changed and}} whether previously fixed faults have re-emerged. <b>Regression</b> <b>testing</b> can be performed to test a system efficiently by systematically selecting the appropriate minimum set of tests needed to adequately cover a particular change.|$|E
50|$|Typically, <b>regression</b> <b>testing</b> {{is carried}} out by {{automation}} tools, but the existing generation of <b>regression</b> <b>testing</b> tools is not equipped to handle database application. For this reason, performing a regression test on a database application {{could prove to be}} taxing as it would require a great deal of manual effort.|$|E
5000|$|<b>Regression</b> <b>testing</b> {{ensures that}} a change, {{such as a}} bug fix, did not {{introduce}} new faults into the software under test. One {{of the main reasons}} for <b>regression</b> <b>testing</b> is to determine whether a change {{in one part of the}} software has any effect on other parts of the software.|$|E
3000|$|... ii) Preparing {{automatic}} test cases for each user environment beforehand is not realistic because service providers {{would have to}} make extensive preparations. A め method to enable effective <b>regression</b> <b>tests</b> for Cloud platform development using Jenkins and Selenium was reported [2]. However, the paper [2] targeted IaaS platform development, and <b>regression</b> <b>tests</b> of user virtual machines deployed on an IaaS platform are out of scope. The paper [2] also describes that three to five times of the amount of work are needed for {{automatic test}} case creations using Jenkins and Selenium compared with manual <b>regression</b> <b>test</b> executions.|$|R
30|$|Our {{method is}} {{expected}} {{to reduce the number}} of prepared test cases by two-tier abstraction of installed software and test cases. Currently, providers need to prepare and execute <b>regression</b> <b>test</b> cases on each patch for their services. For example, about 300 <b>regression</b> <b>test</b> cases are executed for a production hosting service that is mainly used for mail and web functions [15].|$|R
40|$|Abstract—Regression testing {{analyzes}} whether {{software maintenance}} has inadvertently broken existing functionality. Since it is costly—especially for manual testing—it is typically {{limited to a}} subset of test cases. Since impact analysis of code modifications on test cases is far from trivial for real world software, <b>regression</b> <b>test</b> selection is hard. However, if it misses affected test cases, bugs may remain unnoticed. In response, the research community has proposed numerous <b>test</b> selection approaches. <b>Regression</b> <b>test</b> selection is especially relevant for manual tests, since their execution costs {{limit the number of}} tests that can be executed in practice. However, evaluations of existing work focus on automated tests. Its applicability to manual tests is thus unclear. We present an industrial case study that demonstrates the challenges that <b>regression</b> <b>test</b> selection techniques face when applied to manual system tests. Furthermore, we sketch how, given these challenges, manual <b>regression</b> <b>test</b> selection can be improved. Keywords-regression test selection, software maintenance I...|$|R
5000|$|HP Functional Testing software: Automated {{functional}} and <b>regression</b> <b>testing</b> software ...|$|E
50|$|<b>Regression</b> <b>testing</b> {{can be used}} {{not only}} for testing the {{correctness}} of a program, but often also for tracking {{the quality of its}} output. For instance, in the design of a compiler, <b>regression</b> <b>testing</b> could track the code size, and {{the time it takes to}} compile and execute the test suite cases.|$|E
50|$|A common {{misconception}} is that {{load testing}} software provides record and playback capabilities like <b>regression</b> <b>testing</b> tools. Load testing tools analyze the entire OSI protocol stack whereas most <b>regression</b> <b>testing</b> tools focus on GUI performance. For example, a <b>regression</b> <b>testing</b> tool will record and playback a mouse click on {{a button on}} a web browser, but a load testing tool will send out hypertext the web browser sends after the user clicks the button. In a multiple-user environment, load testing tools can send out hypertext for multiple users with each user having a unique login ID, password, etc.|$|E
40|$|Volatility {{tests are}} an {{alternative}} to <b>regression</b> <b>tests</b> for evaluating the joint null hypothesis of market efficiency and risk neutrality. Acomparison {{of the power of}} the two kinds of tests depends on what the alternative hypothesis is taken to be. By considering tests based on conditional volatility bounds, we show that if the alternative is that one could"beat the market" using a linear combination of known variables, then the <b>regression</b> <b>tests</b> are at least as powerful as the conditional volatility tests. If the application is to spot and forward markets, then the most powerful conditional volatility test turns out to be equivalent to the analogous <b>regression</b> <b>test</b> in terms of asymptotic power. In other applications,the volatility test will be less powerful than <b>regression</b> <b>tests</b> against our chosen alternative. However, these results are not inconsistent with the observation that volatility tests may be more powerful against other alternative hypoth-eses, such as that risk-averse investors are rationally maximizing the present discounted utility of future consumption,with a time-varying discount rate. ...|$|R
5000|$|There {{is lack of}} easily {{executed}} automated <b>regression</b> <b>tests</b> {{with significant}} coverage of the codebase: ...|$|R
50|$|The {{test suite}} test/test_gadfly.py {{attempts}} {{to provide a}} <b>regression</b> <b>test</b> and {{a demonstration of the}} system.|$|R
50|$|Software {{reliability}} testing includes feature testing, load testing, and <b>regression</b> <b>testing.</b>|$|E
50|$|Moreover, in a {{software}} development environment {{which tends to}} use black box components from a third party, performing <b>regression</b> <b>testing</b> can be tricky, as {{any change in the}} third party component may interfere {{with the rest of the}} system, and performing <b>regression</b> <b>testing</b> on a third party component is difficult because it is an unknown entity.|$|E
50|$|<b>Regression</b> <b>testing</b> is an {{integral}} part of the extreme programming software development method. In this method, design documents are replaced by extensive, repeatable, and automated testing of the entire software package throughout each stage of the software development process. <b>Regression</b> <b>testing</b> is done after functional testing has concluded, to verify that the other functionalities are working.|$|E
5000|$|A {{collection}} of reproducible scientific articles also used as usage examples and <b>regression</b> <b>tests</b> for the standalone programs; ...|$|R
40|$|Software {{products}} are often built from commercial-off-the-shelf (COTS) components. When new releases of these components are {{made available for}} integration and testing, source code is usually not provided by the COTS vendors. Various <b>regression</b> <b>test</b> selection techniques have been developed and {{have been shown to}} be cost effective. However, the majority of these test selection techniques rely on source code for change identification and impact analysis. This dissertation presents a <b>regression</b> <b>test</b> selection (RTS) process called Integrated- Black-box Approach for Component Change Identification (I-BACCI) for COTS-based applications. The I-BACCI process reduces the test suite based upon changes in the binary code of the COTS component using the firewall analysis <b>regression</b> <b>test</b> selection method. This dissertation also presents Pallino, the supporting automation that statically analyzes binary code to identify the code change and the impact of these changes. Based on the output of Pallino and the original test suit, testers can determine the <b>regression</b> <b>test</b> cases needed that execute the application glue code which is affected by the changed areas in the new version of the COTS component...|$|R
3000|$|In {{order to}} test factors {{influencing}} business continuity and survival, we used multiple <b>regression</b> <b>tests</b> at three levels: [...]...|$|R
5000|$|The {{application}} vendor {{has access}} to all customer data, expediting design and <b>regression</b> <b>testing.</b>|$|E
50|$|<b>Regression</b> <b>testing</b> is {{performed}} when changes {{are made to}} the existing functionality of the software or {{if there is a}} bug fix in the software. <b>Regression</b> <b>testing</b> can be achieved through multiple approaches, if a test all approach is followed it provides certainty that the changes made to the software have not affected the existing functionalities, which are unaltered, in any way.|$|E
50|$|In 2007, VIP {{implemented}} <b>regression</b> <b>testing</b> {{to expedite}} {{the testing of}} new features and versions.|$|E
5000|$|... 360Bind: Automate BO Non <b>regression</b> <b>tests.</b> With {{ability to}} compare results and pixels from Webi, Deski and Crystal reports.|$|R
40|$|There is a {{constant}} need for practical, efficient and costeffective software evolution techniques. We propose a novel evolution methodology that integrates the concepts of features and component-based software engineering (CBSE). We collect information about a legacy system's features through interviews with key developers, users {{of the system and}} analyzing the exis ting <b>regression</b> <b>test</b> cases. We found that <b>regression</b> <b>test</b> cases are untapped resources, as far as information about system features is concerned...|$|R
40|$|Various <b>regression</b> <b>test</b> {{selection}} {{techniques have}} been developed and shown to improve fault detection effectiveness. The majority of these test selection techniques rely on access to source code for change identification. However, when new releases of COTS components are made available for integration and testing, source code is often not available to guide <b>regression</b> <b>test</b> selection. This paper describes a process for identifying changed functions when code is not available. This change information is beneficial for selecting white-box <b>regression</b> <b>tests</b> of customer/glue code. This process is applicable when COTS licensing agreements do not preclude decompilation. A feasibility study of the process was conducted with four releases of a medium-scale internal ABB product. The results of the feasibility study indicate that this process can be effective in identifying changed functions...|$|R
