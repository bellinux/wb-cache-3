278|777|Public
25|$|Pre-processing helped {{enhancing}} {{the quality of}} an image by filtering and removing unnecessary noises. The minutiae based algorithm only worked effectively in 8-bit gray scale fingerprint image. A reason was that an 8-bit gray fingerprint image was a fundamental base to convert the image to 1-bit image with value 0 for ridges and value 1 for furrows. As a result, the ridges were highlighted with black color while the furrows were highlighted with white color. This process partly removed some noises in an image and helped enhance the edge detection. Furthermore, there are two more steps to improve the best quality for the input image: minutiae extraction and false minutiae removal. The minutiae extraction {{was carried out by}} applying ridge thinning algorithm which was to <b>remove</b> <b>redundant</b> pixels of ridges. As a result, the thinned ridges of the fingerprint image are marked with a unique ID so that further operation can be conducted. After the minutiae extraction step, the false minutiae removal was also necessary. The lack of the amount of ink and the cross link among the ridges could cause false minutiae that led to inaccuracy in fingerprint recognition process.|$|E
5000|$|Skipredudant EMBOSS tool to <b>remove</b> <b>redundant</b> {{sequences}} from a set ...|$|E
5000|$|Reduction in {{wasteful}} spending: It can <b>remove</b> <b>redundant</b> {{spending by}} reexamining potentially unnecessary expenditures.|$|E
50|$|This method <b>removes</b> <b>redundant</b> codons {{and stop}} codons.|$|R
5000|$|Model {{simplification}} - fixing model inputs {{that have}} no effect on the output, or identifying and <b>removing</b> <b>redundant</b> parts of the model structure.|$|R
5000|$|This [...] "pruning" [...] {{mechanism}} <b>removes</b> <b>redundant</b> {{connections in}} the brain. Huttenlocher found that in individuals with intellectual disabilities, this mechanism works differently.|$|R
50|$|A {{registry}} cleaner is a {{class of}} third party software utility designed for the Microsoft Windows operating system, whose purpose is to <b>remove</b> <b>redundant</b> items from the Windows registry.|$|E
50|$|The InChI {{algorithm}} converts input structural {{information into}} a unique InChI identifier in a three-step process: normalization (to <b>remove</b> <b>redundant</b> information), canonicalization (to generate {{a unique number}} label for each atom), and serialization (to give a string of characters).|$|E
50|$|When writing scripts for the story, Yates uses a loose {{scripting}} {{style that}} allows him to make changes as he's creating the artwork. He draws inspiration from anime and cartoons in his open and colorful line art, which contrasts with more common styles that utilize solid blacks. Once the art is finished, he refines the dialogue to <b>remove</b> <b>redundant</b> exposition and enhance humor.|$|E
40|$|High-dimensional data poses {{a severe}} {{challenge}} for data mining. Feature selection is a frequently used technique in preprocessing high-dimensional data for successful data mining. Traditionally, feature selection {{is focused on}} removing irrelevant features. However, for high-dimensional data, <b>removing</b> <b>redundant</b> features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in <b>removing</b> <b>redundant</b> and irrelevant features...|$|R
50|$|The data {{collection}} phase can also involve {{a process called}} deduplication, which involves <b>removing</b> <b>redundant</b> information from a data set in order to lower costs and improve user experience. The deduplication process filters and compresses the data that has been collected before it gets uploaded.|$|R
5000|$|Concision {{means being}} {{economical}} with words, expressing {{what needs to}} be said in just the right words. That may involve <b>removing</b> <b>redundant</b> or unnecessary phrases or replacing them with shorter ones. It is described in The Elements of Style by Strunk and White as follows: ...|$|R
50|$|While {{subsampling}} {{can easily}} {{reduce the size}} of an uncompressed image by 50% with minimal loss of quality, the final effect on the size of a compressed image is considerably less. This is because image compression algorithms also <b>remove</b> <b>redundant</b> chroma information. In fact, by applying something as rudimentary as chroma subsampling prior to compression, information is removed from the image that could be used by the compression algorithm to produce a higher quality result with no increase in size. For example, with wavelet compression methods, better results are obtained by dropping the highest frequency chroma layer inside the compression algorithm than by applying chroma subsampling prior to compression. This is because wavelet compression operates by repeatedly using wavelets as high and low pass filters to separate frequency bands in an image, and the wavelets do a better job than chroma subsampling does.|$|E
50|$|Pre-processing helped {{enhancing}} {{the quality of}} an image by filtering and removing unnecessary noises. The minutiae based algorithm only worked effectively in 8-bit gray scale fingerprint image. A reason was that an 8-bit gray fingerprint image was a fundamental base to convert the image to 1-bit image with value 0 for ridges and value 1 for furrows. As a result, the ridges were highlighted with black color while the furrows were highlighted with white color. This process partly removed some noises in an image and helped enhance the edge detection. Furthermore, there are two more steps to improve the best quality for the input image: minutiae extraction and false minutiae removal. The minutiae extraction {{was carried out by}} applying ridge thinning algorithm which was to <b>remove</b> <b>redundant</b> pixels of ridges. As a result, the thinned ridges of the fingerprint image are marked with a unique ID so that further operation can be conducted. After the minutiae extraction step, the false minutiae removal was also necessary. The lack of the amount of ink and the cross link among the ridges could cause false minutiae that led to inaccuracy in fingerprint recognition process.|$|E
5000|$|The {{challenge}} of a multiple-pattern game is selecting a winner wherein a tie is possible. The {{solution is to}} name the player who shouts [...] "Bingo!" [...] first, is the winner. However, it is more practical and manageable to use card sets that avoid multiple-pattern games. The single-pattern #3 row has already been mentioned, but its limited card set causes problems for the emerging online Bingo culture. Larger patterns, e.g. a diamond pattern consisting of cell positions B3, I2 and I4, N1 and N5, G2 and G4, and O3, are often used by online Bingo games to permit large number of players while ensuring only one player can win. (A unique winner is further desirable for online play where network delays and other communication interference can unfairly affect multiple winning cards. The winner would {{be determined by the}} first person to click the [...] "Bingo!" [...] button (emulating the shout of [...] "Bingo!" [...] during a live game).) In this case the number of unique winning cards is calculated as (152*(15*14)3/23) = 260,465,625 (260 million). The division by two for each of the [...] "I", [...] "N", and [...] "G" [...] columns is necessary to once again <b>remove</b> <b>redundant</b> number combinations, such as 31,#,#,#,45 and 45,#,#,#,31 in the N column.|$|E
30|$|Based on Lemma 1, we {{can reduce}} the {{decoding}} complexity of CVA on the tail-biting trellis by <b>removing</b> <b>redundant</b> computations and iterations during the decoding process and control the convergence of CVA. The improvements of the proposed decoder can be summarized into the following two aspects.|$|R
40|$|In {{this paper}} we {{describe}} two optimization techniques that are specially tailored for information gathering. The {{first is a}} greedy minimization algorithm that minimizes an information gathering plan by <b>removing</b> <b>redundant</b> and overlapping information sources without loss of completeness. We then discuss a set of [...] ...|$|R
50|$|The unicity {{distance}} {{can be increased}} by reducing the plaintext redundancy. One {{way to do this}} is to deploy data compression techniques prior to encryption, for example by <b>removing</b> <b>redundant</b> vowels while retaining readability. This is a good idea anyway, as it reduces the amount of data to be encrypted.|$|R
30|$|For ease of comparison, all the {{extracted}} {{features were}} normalized to have zero mean and unit standard deviation. To <b>remove</b> <b>redundant</b> features and/or those with poor discriminatory power, a feature selection process was used. This process is described below.|$|E
40|$|We {{investigate}} {{the conditions under}} which least bisimulations exist with respect to set inclusion. In particular, we describe a natural way to <b>remove</b> <b>redundant</b> pairs from a given bisimulation. We then introduce the conciseness property on process graphs, which characterizes the existence of least bisimulations under the aforementioned method...|$|E
40|$|Abstract. Ongoing {{learning}} {{refers to}} the possibility of a system to increase knowledge from the experience obtained when working in the classification of new patterns. In this paper, we present an automatic classification system with ongoing learning capabilities and analyze the importance of using some size reduction algorithm to <b>remove</b> <b>redundant</b> training patterns. ...|$|E
5000|$|... #Caption: The {{permutation}} by duplication {{mechanism for}} producing a circular permutation. First, a gene 1-2-3 is duplicated to form 1-2-3-1-2-3. Next, a start codon is introduced {{before the first}} domain 2 and a stop codon after the second domain 1, <b>removing</b> <b>redundant</b> sections and resulting in a circularly permuted gene 2-3-1.|$|R
5000|$|Dimensional {{normalization}} or snowflaking <b>removes</b> <b>redundant</b> attributes, {{which are}} {{known in the}} normal flatten de-normalized dimensions. Dimensions are strictly joined together in sub dimensions. [...] Snowflaking {{has an influence on}} the data structure that differs from many philosophies of data warehouses.Single data (fact) table surrounded by multiple descriptive (dimension) tables ...|$|R
50|$|A {{basis for}} a {{subspace}} S {{is a set of}} linearly independent vectors whose span is S. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by <b>removing</b> <b>redundant</b> vectors (see algorithms, below).|$|R
40|$|The {{components}} of most real-world patterns contain redundant information. However, most pattern classifiers (e. g., statistical classifiers and neural nets) work better if pattern components are nonredundant. I present various unsupervised nonlinear predictor-based "neural" learning algorithms that transform patterns and pattern sequences into less redundant patterns without loss of information. The {{first part of}} the paper shows how a neural predictor can be used to <b>remove</b> <b>redundant</b> information from input sequences. Experiments with artificial sequences demonstrate that certain supervised classification techniques can greatly benefit from this kind of unsupervised preprocessing. In {{the second part of the}} paper, a neural predictor is used to <b>remove</b> <b>redundant</b> information from natural text. With certain short newspaper articles, the neural method can achieve better compression ratios than the widely used asymptotically optimal Lempel-Ziv string compression algorithm. The third part of the [...] ...|$|E
40|$|Density based {{clustering}} technique groups similar data objects {{based on}} density. In this paper, a methodology {{has been proposed}} based on density based clustering technique in order to <b>remove</b> <b>redundant</b> test cases so that time wasted in testing unnecessary test cases can be reduced. Experiments show that how our technique can significantly reduced test suite...|$|E
40|$|We {{describe}} ongoing {{work on the}} use of subsumption to <b>remove</b> <b>redundant</b> inferences from propositional resolution refutation proofs of {{the style}} generated by conflict driven clause learning SAT solvers. This is used for faster LCF-style proof replay in interactive theorem provers. There may also be an application in the extraction of small unsatisfiable cores. ...|$|E
40|$|Run-time {{synchronization}} overhead is {{a crucial}} factor in limiting speedup for parallel computers. In this paper, we present a new two-phase algorithm for <b>removing</b> <b>redundant</b> dependencies and minimizing interprocessor synchronizations when scheduling an acyclic task graph onto a multiprocessor system. The first phase <b>removes</b> <b>redundant</b> dependencies before scheduling; while the second phase eliminates interprocessor synchronizations after scheduling. In a simulation using randomly generated task graphs, on the average, 98. 28 % of the dependencies are eliminated in the first phase, and 99. 41 % of the dependencies are finally eliminated. The approach has also been applied to some benchmark task graphs. The two-phase algorithm, which has O(n 3) time complexity and O(n 2) space complexity, utilizes a new algorithm which computes the transitive closure and reduction at the same time, storing results in a single matrix. Keywords: multiprocessor scheduling, redundant dependency, synchronizati [...] ...|$|R
5|$|The {{earliest}} model {{proposed for}} the evolution of circular permutations is the permutation by duplication mechanism. In this model, a precursor gene first undergoes a duplication and fusion to form a large tandem repeat. Next, start and stop codons are introduced at corresponding locations in the duplicated gene, <b>removing</b> <b>redundant</b> sections of the protein.|$|R
30|$|Encoding/Decoding: This {{operation}} {{reduces the}} amount of data by <b>removing</b> <b>redundant</b> or unnecessary information. Thus, a hidden message can be completely destroyed. This is also true if the audio file is converted into another format. MP 3 compression, for example, changes a wave file to an MP 3 file before it reaches the receiver.|$|R
40|$|Abstract Background With DNA {{microarray}} data, {{selecting a}} compact subset of discriminative genes {{from thousands of}} genes is a critical step for accurate classification of phenotypes for, e. g., disease diagnosis. Several widely used gene selection methods often select top-ranked genes according to their individual discriminative power in classifying samples into distinct categories, without considering correlations among genes. A limitation of these gene selection methods is that they may result in gene sets with some redundancy and yield an unnecessary large number of candidate genes for classification analyses. Some latest studies show that incorporating gene to gene correlations into gene selection can <b>remove</b> <b>redundant</b> genes and improve classification accuracy. Results In this study, we propose a new method, Based Bayes error Filter (BBF), to select relevant genes and <b>remove</b> <b>redundant</b> genes in classification analyses of microarray data. The effectiveness and accuracy of this method is demonstrated through analyses of five publicly available microarray datasets. The results show that our gene selection method is capable of achieving better accuracies than previous studies, while being able to effectively select relevant genes, <b>remove</b> <b>redundant</b> genes and obtain efficient and small gene sets for sample classification purposes. Conclusion The proposed method can effectively identify a compact set of genes with high classification accuracy. This study also indicates that application of the Bayes error is a feasible and effective wayfor removing redundant genes in gene selection. </p...|$|E
40|$|Given {{a set of}} {{labelled}} examples, the aim of {{a classification}} problem is to assign a label to each new instance. The effectiveness of the classification is related to many factors, such as {{the quality of the}} data. The Feature Selection problem can be considered to <b>remove</b> <b>redundant</b> and irrelevant information from these data. The aim of feature selection applied to classificatio...|$|E
40|$|In {{this paper}} we {{consider}} techniques {{to identify and}} <b>remove</b> <b>redundant</b> predicates during predicate abstraction. We give three criteria for identifying redundancy. A predicate is redundant {{if any of the}} following three holds (i) the predicate is equivalent to a propositional function of other predicates. (ii) removing the predicate preserves safety properties satisfied by the abstract model (iii) removing it preserves bisimulation equivalence...|$|E
40|$|We {{describe}} a biographical multidocument summarizer that summarizes information about people {{described in the}} news. The summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection, <b>removing</b> <b>redundant</b> descriptions. The summarization components have been extensively evaluated for coherence, accuracy, and non-redundancy of the descriptions produced...|$|R
50|$|The {{earliest}} model {{proposed for}} the evolution of circular permutations is the permutation by duplication mechanism. In this model, a precursor gene first undergoes a duplication and fusion to form a large tandem repeat. Next, start and stop codons are introduced at corresponding locations in the duplicated gene, <b>removing</b> <b>redundant</b> sections of the protein.|$|R
40|$|IEEE 754 floating-point {{arithmetic}} {{is widely}} used in modern, general-purpose computers. It is based on real arithmetic and is made total by adding both a positive and a negative infinity, a negative zero, and many Not-a-Number (NaN) states. Transreal arithmetic is total. It also has a positive and a negative infinity but no negative zero, {{and it has a}} single, unordered number, nullity. Modifying the IEEE arithmetic so that it uses transreal arithmetic has a number of advantages. It <b>removes</b> one <b>redundant</b> binade from IEEE floating-point objects, doubling the numerical precision of the arithmetic. It <b>removes</b> eight <b>redundant,</b> relational,floating-point operations and <b>removes</b> the <b>redundant</b> total order operation. It replaces the non-reflexive, floating-point, equality operator with a reflexive equality operator and it indicates that some of the exceptions may be <b>removed</b> as <b>redundant</b> subject to issues of backward compatibility and transient future compatibility as programmers migrate to the transreal paradigm...|$|R
