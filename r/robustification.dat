196|15|Public
50|$|<b>Robustification</b> {{works by}} taking {{advantage}} of two different principles.|$|E
50|$|<b>Robustification</b> as it {{is defined}} here is {{sometimes}} referred to as parameter design or robust parameter design (RPD) and is often associated with Taguchi methods. Within that context, <b>robustification</b> can include the process of finding the inputs that contribute most to the random variability in the output and controlling them, or tolerance design. At times the terms design for quality or Design for Six Sigma (DFSS) might also be used as synonyms.|$|E
50|$|There {{are three}} {{distinct}} methods of <b>robustification,</b> but a practitioner might use a mix {{that provides the}} best in results, resources and time.|$|E
40|$|Five <b>robustifications</b> of L 2 {{boosting}} for {{linear regression}} with various robustness properties are considered. The first two use the Huber loss as implementing loss function for boosting {{and the second}} two use robust simple linear regression for the fitting in L 2 boosting (i. e. Â robust base learners). Both concepts can be applied with or without down-weighting of leverage points. Our last method uses robust correlation estimates {{and appears to be}} most robust. Crucial advantages of all methods are that they do not compute covariance matrices of all covariates and {{that they do not have}} to identify multivariate leverage points. When there are no outliers, the robust methods are only slightly worse than L 2 boosting. In the contaminated case though, the robust methods outperform L 2 boosting by a large margin. Some of the <b>robustifications</b> are also computationally highly efficient and therefore well suited for truly high-dimensional problems. ...|$|R
40|$|CMSDue to {{the high}} track {{multiplicity}} in the final states expected in proton collisions at the LHC experiments, novel vertex reconstruction algorithms are required. The vertex reconstruction problem can be decomposed into a pattern recognition problem (“vertex finding”) and an estimation problem (“vertex fitting”). Starting from least-squares methods, <b>robustifications</b> of the classical algorithms are discussed and the statistical properties of the novel methods are shown. A whole set of different approaches for the vertex finding problem is presented and compared in relevant physics channels...|$|R
40|$|In this paper, {{we propose}} a unified {{approach}} to generating standardized-residuals-based correlation tests for checking GARCH-type models. This approach is valid {{in the presence}} of estimation uncertainty, is robust to various standardized error distributions, and is applicable to testing various types of misspecifications. By using this approach, we also propose a class of power-transformed-series (PTS) correlation tests that provides certain <b>robustifications</b> and power extensions to the Box-Pierce, McLeod-Li, Li-Mak, and Berkes-Horváth-Kokoszka tests in diagnosing GARCH-type models. Our simulation and empirical example show that the PTS correlation tests outperform these existing autocorrelation tests in financial time series analysis. Copyright © 2008 John Wiley & Sons, Ltd. ...|$|R
50|$|This basic {{principle}} underlies all <b>robustification,</b> {{but in practice}} there are typically a number of inputs {{and it is the}} suitable point with the lowest gradient on a multi-dimensional surface that must be found.|$|E
50|$|<b>Robustification</b> {{is a form}} of {{optimisation}} whereby {{a system}} is made less sensitive to the effects of random variability, or noise, that is present in that system’s input variables and parameters. The process is typically associated with engineering systems, but the process can also be applied to a political policy, a business strategy or any other system that is subject to the effects of random variability.|$|E
5000|$|Typically, {{the goal}} of {{probabilistic}} design is to identify the design that will exhibit the smallest effects of random variability. This could be the one design option out of several that {{is found to be}} most robust. Alternatively, it could be the only design option available, but with the optimum combination of input variables and parameters. This second approach is sometimes referred to as <b>robustification,</b> parameter design or design for six sigma ...|$|E
40|$|Linear {{least squares}} {{regression}} {{is among the}} most well known classical methods. This and other parametric least squares regression models do not perform well when the modeling is too restrictive to capture the nonlinear effect the covariates have on the response. Locally weighted least squares regression (loess) is a modern technique that combines much of the simplicity of the classical least squares method with the flexibility of nonlinear regression. The basic idea behind the method is to model a regression function only locally as having a specific form. This paper discusses the method in the univariate and multivariate case and <b>robustifications</b> of the technique, and provides illustrative examples...|$|R
40|$|We {{propose a}} class of locally and {{asymptotically}} optimal tests, based on multivariate ranks and signs for the homogeneity of scatter matrices in $m$ elliptical populations. Contrary to the existing parametric procedures, these tests remain valid without any moment assumptions, and thus are perfectly robust against heavy-tailed distributions (validity robustness). Nevertheless, they reach semiparametric efficiency bounds at correctly specified elliptical densities and maintain high powers under all (efficiency robustness). In particular, their normal-score version outperforms traditional Gaussian likelihood ratio tests and their pseudo-Gaussian <b>robustifications</b> under a very broad range of non-Gaussian densities including, for instance, all multivariate Student and power-exponential distributions. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Regional co-expression {{refers to}} the {{phenomenon}} of contiguous genes exhibiting similar expression patterns. Among others, DNA copy number aberrations may be causally involved in regional co-expression. We propose a random coefficients model to explain regional co-expression from DNA copy number information, while modeling residual co-expression due to other causes by a correlated error structure. We show how the model parameters may be estimated (computationally efficient and consistently) from high-dimensional data, and suggest several <b>robustifications</b> of the estimation procedure. From the model {{we are able to}} assess whether there is a shared effect on expression levels due to the DNA copy number aberrations, but also whether this effect is homogeneous across genes. In two examples we use the proposed methodology to investigate the association between DNA copy number aberrations and regional co-expression. ...|$|R
50|$|Once {{the concept}} is established, the nominal values of the various {{dimensions}} and design parameters need to be set, the detail design phase of conventional engineering. Taguchi's radical insight was that the exact choice of values required is under-specified by the performance requirements of the system. In many circumstances, this allows the parameters to be chosen so as to minimize the effects on performance arising from variation in manufacture, environment and cumulative damage. This is sometimes called <b>robustification.</b>|$|E
5000|$|The {{analytical}} approach relies initially {{on the development}} of an analytical model of the system of interest. The expected variability of the output is then found by using a method like the propagation of error or functions of random variables. These typically produce an algebraic expression that can be analysed for optimisation and <b>robustification.</b> This approach is only as accurate as the model developed and it can be very difficult if not impossible for complex systems.|$|E
50|$|Another {{experimental}} method {{that was used}} for <b>robustification</b> is the Operating Window. It {{was developed in the}} United States before the wave of quality methods from Japan came to the West, but still remains unknown to many. In this approach, the noise of the inputs is continually increased as the system is modified to reduce sensitivity to that noise. This increases robustness, but also provides a clearer measure of the variability that is flowing through the system. After optimisation, the random variability of the inputs is controlled and reduced, and the system exhibits improved quality.|$|E
40|$|This paper {{provides}} parametric and rank-based optimal {{tests for}} eigenvectors and eigenvalues of covariance or scatter matrices in elliptical families. The parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963) and their pseudo-Gaussian <b>robustifications</b> by Tyler (1981, 1983) and Davis (1977), with which their Gaussian versions are shown to coincide, asymptotically, under Gaussian or finite fourth-order moment assumptions, respectively. Such assumptions however restrict the scope to covariance-based principal component analysis. The rank-based tests we are proposing remain valid without such assumptions. Hence, they address {{a much broader}} class of problems, where covariance matrices need not exist and principal components are associated with more general scatter matrices. Asymptotic relative efficiencies moreover show that those rank-based tests are quite powerful; when based on van der Waerden or normal scores, they even uniformly dominate the pseudo-Gaussian version...|$|R
40|$|Classical {{step-by-step}} algorithms, such as forward selectio n (FS) and stepwise (SW) methods, are computationally suitable, but yield poor resu lts {{when the}} data contain outliers and other contaminations. Robust model selection procedures, {{on the other}} hand, are not computationally efficient or scalable to large d imensions, because they require the fitting {{of a large number}} of submodels. Robust and computationally efficient versions of FS and SW are proposed. Since FS and SW can be expressed in terms of sample correlations, simple <b>robustifications</b> are o btained by replacing these correlations by their robust counterparts. A pairwise appr oach is used to construct the robust correlation matrix – not only because of its compu tational advantages over the d -dimensional approach, but also because the pairwise approa ch is more consistent with the idea of step-by-step algorithms. The prop osed robust methods have much better performance compared to standard FS and SW. Also, they are computationally very suitable and scalable to large high-di mensional datasets. status: publishe...|$|R
40|$|For {{building}} a linear prediction model, Backward Elimination (BE) is a computationally suitable stepwise procedure for sequencing the candidate predictors. This method yields poor results when data contain outliers and other contaminations. Robust model selection procedures, {{on the other}} hand, are not computationally efficient or scalable to large dimensions, because they require the fitting {{of a large number}} of submodels. Robust version of BE is proposed in this study, which is computationally very suitable and scalable to large high-dimensional data sets. Since BE can be expressed in terms of sample correlations, simple <b>robustifications</b> are obtained by replacing these correlations by their robust counterparts. A pairwise approach is used to construct the robust correlation matrix − not only because of its computational advantages over the d-dimensional approach, but also because the pairwise approach is more consistent with the idea of step-by-step algorithms. The performance of the proposed robust method is much better than standard BE...|$|R
50|$|H-infinity loop-shaping is {{a design}} {{methodology}} in modern control theory. It combines the traditional intuition of classical control methods, such as Bode's sensitivity integral, with H-infinity optimization techniques to achieve controllers whose stability and performance properties hold good {{in spite of}} bounded differences between the nominal plant assumed in design and the true plant encountered in practice. Essentially, the control system designer describes the desired responsiveness and noise-suppression properties by weighting the plant transfer function in the frequency domain; the resulting 'loop-shape' is then 'robustified' through optimization. <b>Robustification</b> usually has little effect at high and low frequencies, but the response around unity-gain crossover is adjusted to maximise the system's stability margins. H-infinity loop-shaping {{can be applied to}} multiple-input multiple-output (MIMO) systems.|$|E
50|$|A robust {{parameter}} design, {{introduced by}} Genichi Taguchi, is an experimental design used {{to exploit the}} interaction between control and uncontrollable noise variables by <b>robustification</b> -- finding the settings of the control factors that minimize response variation from uncontrollable factors. Control variables are variables of which the experimenter has full control. Noise variables lie {{on the other side}} of the spectrum, and while these variables may be easily controlled in an experimental setting, outside of the experimental world they are very hard, if not impossible, to control. Robust parameter designs use a naming convention similar to that of FFDs. A 2(m1+m2)-(p1-p2) is a 2-level design where m1 is the number of control factors, m2 is the number of noise factors, p1 is the level of fractionation for control factors, and p2 is the level of fractionation for noise factors.|$|E
50|$|DFSS {{seeks to}} avoid manufacturing/service process {{problems}} by using advanced techniques to avoid process {{problems at the}} outset (e.g., fire prevention). When combined, these methods obtain the proper needs of the customer, and derive engineering system parameter requirements that increase product and service effectiveness {{in the eyes of}} the customer and all other people. This yields products and services that provide great customer satisfaction and increased market share. These techniques also include tools and processes to predict, model and simulate the product delivery system (the processes/tools, personnel and organization, training, facilities, and logistics to produce the product/service). In this way, DFSS is closely related to operations research (solving the knapsack problem), workflow balancing. DFSS is largely a design activity requiring tools including: quality function deployment (QFD), axiomatic design, TRIZ, Design for X, design of experiments (DOE), Taguchi methods, tolerance design, <b>robustification</b> and Response Surface Methodology for a single or multiple response optimization. While these tools are sometimes used in the classic DMAIC Six Sigma process, they are uniquely used by DFSS to analyze new and unprecedented products and processes. It is a concurrent analyzes directed to manufacturing optimization related to the design.|$|E
40|$|This paper {{provides}} parametric and rank-based optimal {{tests for}} eigenvectors and eigenvalues of covariance or scatter matrices in elliptical families. The parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963) and their pseudo-Gaussian <b>robustifications</b> by Davis (1977) and Tyler (1981, 1983). The rank-based tests address {{a much broader}} class of problems, where covariance matrices need not exist and principal components are associated with more general scatter matrices. The proposed tests are shown to outperform daily practice both {{from the point of}} view of validity as {{from the point of view}} of efficiency. This is achieved by utilizing the Le Cam theory of locally asymptotically normal experiments, in the nonstandard context, however, of a curved parametrization. The results we derive for curved experiments are of independent interest, and likely to apply in other contexts. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|We {{develop and}} test robust methods for {{estimation}} and for prediction in spatial studies. We {{assume that a}} stochastic process is measured, with error, at various locations. The variance/covariance structures of this process and of the measurement errors are only approximately known; {{in the face of}} these uncertainties one is to do robust estimation and prediction. We obtain a minimax linear predictor, in which mean squared error loss is first maximized over neighbourhoods quantifying the various sources of model uncertainty, and then minimized over the coefficients of the predictor subject to a constraint of unbiasedness. <b>Robustifications</b> of these methods are then introduced. These are based on generalized M-estimators, and are robust against contaminated error distributions. In a simulation study the procedures afford a substantial level of robustness when the model inadequacies are present, while being almost as efficient as more classical methods otherwise. Copyright # 2004 John Wiley & Sons, Ltd. key words: computer experiments; environmental monitoring; generalized M-estimation; isotropic; kriging; minimax; M-estimate 1...|$|R
40|$|To perform {{regression}} analysis on high-dimensional data with more variables than observations, the lasso estimator (Tibishrani [1996]) {{is a widely}} used technique. However, this estimator is not robust as shown in Alfons et al [2011], who therefore introduced the sparse least trimmed squares (LTS) estimator. Adding an L 1 -penalty to the common LTS objective function results in better prediction performance for data sets containing leverage points compared to other <b>robustifications</b> of the lasso estimator. Other proposals for resistant lasso estimators are given by Khan et al [2007], She and Owen [2011] and Arslan [2012]. This sparse LTS estimator can also be applied for diagnostic analysis. Evaluation of its performance in detecting outliers and influential points is compared to existing procedures as outlier detection by the lasso estimator. A simulation exercise is given to measure masking and swamping effects. We also present preliminary results on the computation of the influence function. status: publishe...|$|R
40|$|The {{notion of}} {{developing}} statistical methods in machine learning which are robust to adversarial perturbations in the underlying data {{has been the}} subject of increasing interest in recent years. A common feature of this work is that the adversarial <b>robustification</b> often corresponds exactly to regularization methods which appear as a loss function plus a penalty. In this paper we deepen and extend the understanding of the connection between <b>robustification</b> and regularization (as achieved by penalization) in regression problems. Specifically, (a) in the context of linear regression, we characterize precisely under which conditions on the model of uncertainty used and on the loss function penalties <b>robustification</b> and regularization are equivalent, and (b) we extend the characterization of <b>robustification</b> and regularization to matrix regression problems (matrix completion and Principal Component Analysis) ...|$|E
40|$|The work {{describes}} conditional {{value at}} risk, its <b>robustification</b> {{with respect to}} the probability distribution of yields of assets and its applications to optimal portfolio selection. In chapter one there are definitions of conditional value at risk and its generalization throught <b>robustification</b> and also motivation to these definitions. The basic properties of conditional value at risk, mainly coherence and continuity {{with respect to the}} parametr of confidence level, are discussed in chapter two. There is also shown that some of these properties are preserved after <b>robustification.</b> The third chapter is dedicated to the derivation of optimization problems of optimal portfolio selection on the basis of conditional value at risk and its <b>robustification.</b> This thesis describes only special cases so that the final problems are solveble by the means of linear programming. The fourth chapter describes particular utilization of these methods with usage of real data from financial markets. Powered by TCPDF (www. tcpdf. org...|$|E
40|$|We {{propose a}} new {{approach}} to study the stability of the optimal filter w. r. t. its initial condition, by introducing a "robust" filter, which is exponentially stable and which approximates the optimal filter uniformly in time. The "robust" filter is obtained here by truncation of the likelihood function, and the <b>robustification</b> result is proved under the assumption that the Markov transition kernel satisfies a pseudo-mixing condition (weaker than the usual mixing condition), and that the observations are "sufficiently good". This <b>robustification</b> approach allows us to prove also the uniform convergence of several particle approximations to the optimal filter, in some cases of nonergodic signals. Nonlinear filter Particle filter Stability Hilbert metric Mixing Pseudo-mixing <b>Robustification...</b>|$|E
40|$|This paper explores {{in several}} prototypical models a {{convenient}} inference procedure for nonstationary variable regression that enables robust chi-square testing {{for a wide}} class of persistent and endogenous regressors. The approach uses the mechanism of self-generated instruments called IVX instrumentation developed by Magdalinos and Phillips (2009 b). We first show that these methods remain valid for regressors with local unit roots in the explosive direction and mildly explosive roots, where the roots are further from unity in the explosive direction than 0 (n(- 1)). It is also shown that Wald testing procedures remain robust for multivariate regressors with certain forms of mixed degrees of persistence. These <b>robustifications</b> are useful in econometric inference, for example, when there are periods of mildly explosive trends in {{some or all of}} time series employed in the analysis but the exact knowledge on the regressor persistence is unavailable. Some aspects of the choice of the IVX instruments are investigated and practical guidance is provided but the issue of optimal IVX instrument choice remains unresolved. The methods are straightforward to apply in practical work such as predictive regression applications in finance. (C) 2016 Elsevier B. V. All rights reserved...|$|R
40|$|This paper {{provides}} parametric and rank-based optimal {{tests for}} eigenvectors and eigenvalues of covariance or scatter matrices in elliptical families. The parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963) and their pseudo-Gaussian <b>robustifications</b> by Davis (1977) and Tyler (1981, 1983), with which their Gaussian versions are shown to coincide, asymptotically, under Gaussian or finite fourth-order moment assumptions, respectively. Such assumptions however restrict the scope to covariance-based principal component analysis. The rank-based tests we are proposing remain valid without such assumptions. Hence, they address {{a much broader}} class of problems, where covariance matrices need not exist and principal components are associated with more general scatter matrices. Asymptotic relative efficiencies moreover show that those rank-based tests are quite powerful; when based on van der Waerden or normal scores, they even uniformly dominate the pseudo-Gaussian versions of Anderson's procedures. The tests we are proposing thus outperform daily practice both {{from the point of}} view of validity as {{from the point of view}} of efficiency. The main methodological tool throughout is Le Cam's theory of locally asymptotically normal experiments, in the nonstandard context, however, of a curved parametrization. The results we derive for curved experiments are of independent interest, and likely to apply in other contexts. info:eu-repo/semantics/publishe...|$|R
40|$|Robust chaos {{is defined}} as the absence of {{periodic}} windows and coexisting attractors in some neighborhood of the parameter space since the existence of such windows in the chaotic region implies fragility of the chaos. In this paper, we introduce a new terminology called <b>robustification</b> of chaos, which means creating robust chaos (in the sense of the above definition) in a dynamical system. As a first step, a new chaotification (<b>robustification)</b> method to generate robust chaos in planar maps is presented using simple piecewise smooth feedback to create a border collision bifurcation in the resulting system under some realizable conditions. The results are applied to an elementary example to illustrate the validity of the proposed method. <b>Robustification</b> of chaos, chaotification, planar discrete mapping, piecewise smooth feedback controller, homoclinic chaos, 05. 45. -a, 05. 45. Gg...|$|E
40|$|Two {{approaches}} of multistage gradient <b>robustification</b> for image contour detection {{are presented in}} this paper: two stages of Di erence of Estimates and Difference of Estimate followed by an optimal filtering. Watershed transformation is then applied to these robustified gradient images to effectively detect image contours which are guaranteed tobe in closed form. Multistage gradient <b>robustification</b> provides the flexibility of using different image processing techniques and produces good detection results for the images highly corrupted with noise. ...|$|E
3000|$|... opt. In turn, {{iterative}} UFIR algorithms need further optimization and <b>robustification</b> in non‐Gaussian {{environments and}} under the uncertainties. Special attention should also be paid to fast algorithms for the determination of N [...]...|$|E
40|$|Repetitive Control (RC) and Iterative Learning Control (ILC) are control {{methods that}} {{specifically}} deal with periodic signals or systems with repetitive operations. They have wide applications in diverse areas from high-precision manufacturing to high-speed assembly, and nowadays these algorithms {{have even been}} applied to biomimetic walking robots, where tracking a periodic reference signal or rejecting periodic disturbances is desired. Compared to conventional feedback control designs (including the inverse dynamics method), RC and ILC improve the control performance over repetitions [...] by learning from the previous input-output data, RC and ILC adaptively update the control input for the next run, aiming for zero tracking error in the hardware instead of in a model, as time goes to infinity. The stability robustness to model uncertainty however remains a fundamental topic as it determines the successful implementation of RC and ILC on any real-world system whose model dynamics cannot normally be determined precisely over all frequencies up to Nyquist. In the control field, there are various existing methods of <b>robustification,</b> such as Linear Matrix Inequality (LMI), mu-synthesis and H-infinity, but few of these methods offer intuitive information about how the stability robustness is achieved. In addition, many of these existing algorithms produce conservative stability boundaries, leaving room for further optimization and enhancement. In this study, several <b>robustification</b> approaches are developed, where better insight into the <b>robustification</b> design process and a tighter stability boundary are established. The first method presents an algorithm for RC compensator design that not only uses phase adjustments, but also adjusts the learning rate {{as a function of}} frequency to obtain improved <b>robustification</b> to model parameter uncertainty. The basic objective of this algorithm is to make the system learn at each frequency at the maximum rate consistent with the need for robustness at that frequency. The second method, on the other hand, explores the benefits of compromising on the zero tracking error requirement for frequencies that require extra robustness, making RC tolerate larger model errors. The third topic focuses on the development of <b>robustification</b> algorithms for Iterative Learning Control that is analogous to the above two RC <b>robustification</b> designs, extending frequency response concepts to finite time problems. The final approach to <b>robustification</b> treated in this dissertation is based on Matched Basic Function Repetitive Control (MBFRC), which individually addresses each frequency, eliminating the need for a robustifying zero phase low pass filter and the need for interpolation in data as required in conventional RC design. Furthermore, this algorithm only uses the frequency response knowledge at the frequencies addressed, {{and as long as the}} phase uncertainties at those frequencies are within +/- 90 deg the system is guaranteed stable for all sufficiently small projection gains...|$|E
40|$|We {{introduce}} one-way LOCC protocols for {{quantum state}} merging for compound sources, which have asymptotically optimal entanglement {{as well as}} classical communication resource costs. For the arbitrarily varying quantum source (AVQS) model, we determine the one-way entanglement distillation capacity, where we utilize the <b>robustification</b> and elimination techniques, well-known from classical as well as quantum channel coding under assumption of arbitrarily varying noise. Investigating quantum state merging for AVQS, we demonstrate by example, that the usual <b>robustification</b> procedure leads to suboptimal resource costs in this case. Comment: 5 pages, 0 figures. Accepted for presentation at the IEEE ISIT 2014 Honolulu. This is a conference version of arXiv: 1401. 606...|$|E
40|$|International audienceWe {{propose a}} new {{approach}} to study the stability of the optimal filter w. r. t. its initial condition, by introducing a "robust" filter, which is exponentially stable and which approximates the optimal filter uniformly in time. The "robust" filter is obtained here by truncation of the likelihood function, and the <b>robustification</b> result is proved under the assumption that the Markov transition kernel satisfies a pseudo-mixing condition (weaker than the usual mixing condition), and that the observations are "sufficiently good". This <b>robustification</b> approach allows us to prove also the uniform convergence of several particle approximations to the optimal filter, in some cases of nonergodic signals...|$|E
40|$|Abstract The paper {{describes}} {{one possible}} <b>robustification</b> process on Bayes esti-mators and studies how a robust estimator {{can work with}} prior information. This <b>robustification</b> procedure, as one of possible sensitivity analysis, enables us to study {{the effect of the}} outlying observations together with sensitivity to a chosen prior dis-tribution or to a chosen loss function. Consider i. i. d. d-dimensional random vectors X 1, [...] ., Xn with a distribution Pθ depending on an unknown parameter θ ∈ Θ ⊂ Rl. We deal with robust counterparts of maximum posterior likelihood estimators and Bayes estimators in the inference on θ. Asymptotic properties of these robust versions, including their asymptotic equivalence of order op(n− 1), are proven...|$|E
