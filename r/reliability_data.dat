832|3793|Public
25|$|The {{guaranteed}} {{cycle count}} may {{apply only to}} block zero (as {{is the case with}} TSOPNAND devices), or to all blocks (as in NOR). This effect is mitigated in some chip firmware or file system drivers by counting the writes and dynamically remapping blocks in order to spread write operations between sectors; this technique is called wear leveling. Another approach is to perform write verification and remapping to spare sectors in case of write failure, a technique called bad block management (BBM). For portable consumer devices, these wearout management techniques typically extend the life of the flash memory beyond the life of the device itself, and some data loss may be acceptable in these applications. For high <b>reliability</b> <b>data</b> storage, however, it is not advisable to use flash memory that would {{have to go through a}} large number of programming cycles. This limitation is meaningless for 'read-only' applications such as thin clients and routers, which are programmed only once or at most a few times during their lifetimes.|$|E
2500|$|A 2013 {{survey of}} 4,424 [...] {{readers of the}} US Consumer Reports {{magazine}} collected <b>reliability</b> <b>data</b> on 4,680 motorcycles purchased new from 2009 to 2012. The most common problem areas were accessories, brakes, electrical (including starters, charging, ignition), and fuel systems, {{and the types of}} motorcycles with the greatest problems were touring, off-road/dual sport, sport-touring, and cruisers. There were not enough sport bikes in the survey for a statistically significant conclusion, though the data hinted at reliability as good as cruisers. These results may be partially explained by accessories including such equipment as fairings, luggage, and auxiliary lighting, which are frequently added to touring, adventure touring/dual sport and sport touring bikes. Trouble with fuel systems is often the result of improper winter storage, and brake problems may also be due to poor maintenance. Of the five brands with enough data to draw conclusions, Honda, Kawasaki and Yamaha were statistically tied, with 11 to 14% of those bikes in the survey experiencing major repairs. Harley-Davidsons had a rate of 24%, while BMWs did worst, with 30% of those needing major repairs. There were not enough Triumph and Suzuki motorcycles surveyed for a statistically sound conclusion, though it appeared Suzukis were as reliable as the other three Japanese brands while Triumphs were comparable to Harley-Davidson and BMW. Three fourths of the repairs in the survey cost less than US$200 and two thirds of the motorcycles were repaired in less than two days. In spite of their relatively worse reliability in this survey, Harley-Davidson and BMW owners showed the greatest owner satisfaction, and three fourths of them said they would buy the same bike again, followed by 72% of Honda owners and 60 to 63% of Kawasaki and Yamaha owners.|$|E
50|$|Of course, {{it is not}} {{the only}} way to encode <b>reliability</b> <b>data.</b>|$|E
5000|$|Reliability, {{particularly}} <b>Reliability</b> (statistics), <b>Data</b> <b>reliability,</b> Reliability (computer networking), and Reliability (research methods) ...|$|R
30|$|Item-level test-retest <b>reliabilities</b> (<b>data</b> not shown) {{ranged in}} {{strength}} from poor (overnight fever kappa[*]=[*]−[*] 0.00, daytime activity level kappa[*]=[*]−[*] 0.02) to perfect agreement (daytime fever kappa[*]=[*] 1.00), with 17 of the 19 items achieving acceptable test-retest reliability.|$|R
40|$|AbstractWith the {{increasing}} development and extensive application of embedded technology, the <b>reliability</b> of <b>data</b> storage for embedded systems increasingly become {{issues of concern}} for developers and users. Many methods to ensure the <b>reliability</b> of <b>data</b> storage {{in all aspects of}} the system design process based on the characteristics of embedded systems will be proposed In this paper and have certain significance on the <b>reliability</b> design of <b>data</b> storage in embedded system design...|$|R
5000|$|The Offshore and Onshore <b>Reliability</b> <b>Data</b> (OREDA) {{project was}} {{established}} in 1981 {{in cooperation with the}} Norwegian Petroleum Directorate (now Petroleum Safety Authority Norway). It is [...] "one of the main <b>reliability</b> <b>data</b> sources for the oil and gas industry" [...] and considered [...] "a unique data source on failure rates, failure mode distribution and repair times for equipment used in the offshore industry. OREDA's original objective was the collection of petroleum industry safety equipment <b>reliability</b> <b>data.</b> The current organization, as a cooperating group of several petroleum and natural gas companies, {{was established in}} 1983, {{and at the same time}} the scope of OREDA was extended to cover <b>reliability</b> <b>data</b> from a wide range of equipment used in oil and gas exploration and production (E&P). OREDA primarily covers offshore, subsea and topside equipment, but does also include some onshore E&P, and some downstream equipment as well.|$|E
50|$|<b>Reliability</b> <b>data</b> are {{generated}} {{in a situation}} in which m ≥ 2 jointly instructed (e.g., by a Code book) but independently working coders assign any one of a set of values 1,...,V to a common set of N units of analysis. In their canonical form, <b>reliability</b> <b>data</b> are tabulated in an m-by-N matrix containing N values vij that coder ci has assigned to unit uj. Define mj as the number of values assigned to unit j across all coders c. When data are incomplete, mj may be less than m. <b>Reliability</b> <b>data</b> require that values be pairable, i.e., mj ≥ 2. The total number of pairable values is n ≤ mN.|$|E
5000|$|Let the {{canonical}} form of <b>reliability</b> <b>data</b> be a 3-coder-by-15 unit matrix with 45 cells: ...|$|E
5000|$|... <b>reliability</b> - <b>data</b> errors {{detected}} and correctable via retransmission ...|$|R
50|$|High <b>reliability</b> from <b>data</b> {{collection}} to data processing and analysis.|$|R
5000|$|ECHA has {{produced}} guidance {{on how to}} assess the <b>reliability</b> of <b>data</b> ...|$|R
5000|$|To apply {{methods for}} {{estimating}} the likely reliability of new designs, and for analysing <b>reliability</b> <b>data.</b>|$|E
50|$|<b>Reliability</b> <b>data</b> for the CVLT-II {{is mostly}} good, ranging from 0.80-0.96 in a mixed neuro-psychiatric sample. Test-retest {{reliability}} was also adequate.|$|E
5000|$|... 6 NLES (Navigation Land Earth Stations): {{accuracy}} and <b>reliability</b> <b>data</b> sending to three geostationary satellite transponders to allow end-user devices to receive them.|$|E
40|$|Considered {{a complex}} system of {{condensing}} power plants. For a given system are provided methods for estimating <b>reliability</b> and <b>data</b> <b>reliability.</b> The classification {{of failure and}} damage. For some indicia of reliability are defined interval values. They represent basis for the estimates of reliability...|$|R
40|$|With {{the rapid}} growth of Cloud computing, the size of Cloud data is {{expanding}} at a dramatic speed. A huge amount of data is generated and processed by Cloud applications, putting a higher demand on cloud storage. While <b>data</b> <b>reliability</b> should already be a requirement, data in the Cloud needs to be stored in a highly cost-effective manner. This book focuses on the trade-off between data storage cost and <b>data</b> <b>reliability</b> assurance for big data in the Cloud. Throughout the whole Cloud data lifecycle, four major features are presented: first, a novel generic <b>data</b> <b>reliability</b> model for describing <b>data</b> <b>reliability</b> in the Cloud; second, a minimum replication calculation approach for meeting a given <b>data</b> <b>reliability</b> requirement to facilitate data creation; third, a novel cost-effective <b>data</b> <b>reliability</b> assurance mechanism for big data maintenance, which could dramatically reduce the storage space needed in the Cloud; fourth, a cost-effective strategy for facilitating data creation and recovery, which could significantly reduce the energy consumption during data transfer...|$|R
5000|$|Data quality: {{quality and}} <b>reliability</b> of <b>data</b> is {{enhanced}} and data becomes a real asset.|$|R
50|$|Krippendorff's alpha is {{more general}} {{than any of}} these special purpose coefficients. It adjusts to varying sample sizes and affords {{comparisons}} across {{a wide variety of}} <b>reliability</b> <b>data,</b> mostly ignored by the familiar measures.|$|E
50|$|Operational {{performance}} is graded 1, 2 or 3 (with one being the highest grading). It includes {{the ability of}} the monitoring centre to respond to events - generated by customers' security systems, operational <b>reliability,</b> <b>data</b> retrieval, etc.|$|E
50|$|In {{this general}} form, {{disagreements}} Do and De may be conceptually transparent but are computationally inefficient. They can be simplified algebraically, especially when {{expressed in terms}} of the visually more instructive coincidence matrix representation of the <b>reliability</b> <b>data.</b>|$|E
5000|$|Continuous {{learning}} of your unique app patterns, adaptive performance tuning, and automatic improvements to <b>reliability</b> and <b>data</b> protection ...|$|R
5000|$|High Electromagnetic {{interference}} immunity due to RS-422 {{standards and}} higher <b>reliability</b> of <b>data</b> transmission due to differential signalling.|$|R
40|$|Abstract — <b>Data</b> <b>reliability</b> {{and storage}} costs are two primary {{concerns}} for current Cloud storage systems. To ensure <b>data</b> <b>reliability,</b> the widely used multi-replica (typically three) replication strategy in current Clouds incurs a huge extra storage consumption, {{resulting in a}} huge storage cost for data-intensive applications in the Cloud in particular. In {{order to reduce the}} Cloud storage consumption while meeting the <b>data</b> <b>reliability</b> requirement, in this paper we present a cost-effective <b>data</b> <b>reliability</b> management mechanism named PRCR based on a generalized <b>data</b> <b>reliability</b> model. By using a proactive replica checking approach, while the running overhead for PRCR is negligible, PRCR ensures reliability of the massive Cloud data with the minimum replication, which can also serve as a cost effectiveness benchmark for replication based approaches. Our simulation indicates that, compared with the conventional 3 -replica strategy, PRCR can reduce from one-third to two-thirds of the Cloud storage space consumption, hence significantly lowering the storage cost in a Cloud. Index Terms — minimum data replication, proactive replica checking, <b>data</b> <b>reliability,</b> cost-effective storage, Cloud computing...|$|R
50|$|Process Flow Baselines: This {{specification}} {{requires a}} manufacturer {{to establish a}} process flow baseline, and if sufficient quality and <b>reliability</b> <b>data</b> is available, the manufacturer through the QM program and the manufacturer's review system, may modify, substitute, or delete tests.|$|E
50|$|Fides (latin: trust) is a guide {{allowing}} estimated reliability calculation {{for electronic}} components and systems. The reliability prediction is generally expressed in FIT (number of failures for 109 hours) or MTBF (Mean Time Between Failures). This guide provides <b>reliability</b> <b>data</b> for RAMS (Reliability, Availability, Maintainability, Safety) studies.|$|E
5000|$|In {{order to}} {{calculate}} the PFS or PFD value of a safety loop {{it is necessary to}} have a reliability model and <b>reliability</b> <b>data</b> for each component in the safety loop. The best reliability model to use is a Markov model (see Andrey Markov). Typical data required are: ...|$|E
40|$|Abstract. The {{software}} <b>reliability</b> failure <b>data</b> is {{the foundation}} of the software reliability’s quantitative evaluation based on the failure data, and it has an important influence on the accuracy of reliability evaluation. But there are always noises in the original software <b>reliability</b> failure <b>data</b> and make the reliability evaluation accuracy affected. This paper put forward the collecting method of <b>reliability</b> failure <b>data</b> and data preprocessing method including data cleaning and data analysis method, which based on the analysis of the importance and the source of failure data in the software reliability testing and the classification of software failure data. Finally through an example, it displayed the reduction of data noises and the promotion of data quality which produced by the preprocessing methods...|$|R
50|$|The minimum {{satisfying}} {{figure for}} test reliability is 0.7. PPA consistently shows test/retest <b>reliability.</b> UK <b>data</b> is reviewed regularly.|$|R
50|$|Microsoft Azure SQL Database {{includes}} built-in {{intelligence that}} learns app patterns and adapts to maximize performance, <b>reliability,</b> and <b>data</b> protection.|$|R
50|$|ISO 14224 Petroleum, {{petrochemical}} {{and natural}} gas industries -- Collection and exchange of reliability and maintenance data for equipment is an international standard relating to the collection of data {{for the management of}} the maintenance of equipment, including <b>reliability</b> <b>data.</b> It covers both methodology for the collection of the data, and details of the data to be collected.|$|E
50|$|The safety {{integrity}} requirements may {{be verified}} by reliability analysis. For SIS that operates on demand, {{it is often}} the probability of failure on demand (PFD) that is calculated. In the design phase, the PFD may be calculated using generic <b>reliability</b> <b>data,</b> for example from OREDA. Later on, the initial PFD estimates may be updated with field experience from the specific plant in question.|$|E
50|$|In {{executing}} the Intercontinental Ballistic Missile Initial Operational Test and Evaluation and Force Development Evaluation programs, the 576th Flight Test Squadron prepares for and conducts ground and flight tests to collect, analyze, and report performance, accuracy, and <b>reliability</b> <b>data</b> for the Joint Staff, USSTRATCOM, Air Staff, and AFGSC. The 576th Flight Test Squadron identifies missile system requirements, demonstrates {{current and future}} war fighting capabilities, and validates missile system improvements and upgrades.|$|E
40|$|It is {{important}} to improve <b>data</b> <b>reliability</b> and <b>data</b> access efficiency for data-intensive applications in a data grid environment. In this paper, we propose an Information Dispersal Algorithm (IDA) -based parallel storage scheme for massive data distribution and parallel access in the Scientific Data Grid. The scheme partitions a data file into unrecognizable blocks and distributes them across many target storage nodes according to user profile and system conditions. A subset of blocks, which can be downloaded in parallel to remote clients, is required to reconstruct the data file. This scheme can be deployed {{on the top of}} current grid middleware. A demonstration and experimental analysis show that the IDA-based parallel storage scheme has better <b>data</b> <b>reliability</b> and <b>data</b> access performance than the existing data replication methods. Furthermore, this scheme has the potential to reduce considerably storage requirements for large-scale databases on a data grid...|$|R
40|$|Citizen Science {{projects}} are initiatives in {{which members of}} the general public participate in scientific research projects and perform or manage research-related tasks such as data collection and/or data annotation. Citizen Science is technologically possible and scientifically significant. However, although research teams can save time and money by recruiting general citizens to volunteer their time and skills to help <b>data</b> analysis, the <b>reliability</b> of contributed <b>data</b> varies a lot. <b>Data</b> <b>reliability</b> issues are significant to the domain of Citizen Science due to the quantity and diversity of people and devices involved. Participants may submit low quality, misleading, inaccurate, or even malicious data. Therefore, finding a way to improve the <b>data</b> <b>reliability</b> has become an urgent demand. This study aims to investigate techniques to enhance the <b>reliability</b> of <b>data</b> contributed by general citizens in scientific research projects especially for acoustic sensing projects. In particular, we propose to design a reputation framework to enhance <b>data</b> <b>reliability</b> and also investigate some critical elements that should be aware of during developing and designing new reputation systems...|$|R
40|$|There {{are many}} {{available}} methods to integrate information source reliability in an uncertainty representation, {{but there are}} only a few works focusing on the problem of evaluating this <b>reliability.</b> However, <b>data</b> <b>reliability</b> and confidence are essential components of a data warehousing system, as they influence subsequent retrieval and analysis. In this paper, we propose a generic method to assess <b>data</b> <b>reliability</b> from a set of criteria using the theory of belief functions. Customizable criteria and insightful decisions are provided. The chosen illustrative example comes from real-world data issued from the Sym'Previus predictive microbiology oriented data warehouse...|$|R
