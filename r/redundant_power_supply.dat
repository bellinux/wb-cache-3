11|10000|Public
25|$|Redundant power supply: The {{computers}} and peripheral equipment were supplied with a <b>redundant</b> <b>power</b> <b>supply</b> {{based on two}} identical voltage transformers. Each was capable of supplying the power necessary for all of the equipment. They were normally alternately switched, but if one failed the other would take over. On-board batteries could also supply temporary power.|$|E
25|$|On flight day 10, {{the third}} spacewalk of the STS-134 mission was conducted. The spacewalk {{made use of}} a new spacewalk pre-breathe {{protocol}}, called In-Suit Light Exercise (ISLE), instead of the normal campout pre-breathe protocol. The new pre-breathe protocol had the astronauts breathe pure oxygen for 60 minutes in the airlock, which had its air pressure lowered to 10.2 Psi (703hPa). The astronauts then put their spacesuits on, performed light exercise and rested for an additional 50 minutes, breathing pure oxygen all the while. After astronauts Drew Feustel and Mike Fincke exited the Quest Airlock, the pair began installing the Power Data Grapple Fixture (PDGF). The fixture itself {{and most of its}} components were installed, but the data cable associated with it was to be installed later. The spacewalking pair then moved on and routed some new power cables from the Unity module to the Zarya module on the Russian segment of the ISS, providing a <b>redundant</b> <b>power</b> <b>supply</b> to the Russian segment. Feustel and Fincke then moved on to finish up the installation of the wireless video system which Fuestel and Greg Chamitoff had begun to install on EVA 1. The pair also took pictures of the Zarya module's thrusters and captured some infrared video of an experiment delivered on board the Express Logistics Carrier (ELC) 3. Commander Mark Kelly documented the spacewalk from inside the station. While the EVA was going on, pilot Greg Johnson and mission specialist Roberto Vittori assisted Expedition 28 flight engineer Ron Garan in stowing new equipment and supplies on the ISS.|$|E
50|$|The bridge has a {{reliable}} and <b>redundant</b> <b>power</b> <b>supply,</b> backed up by diesel generators and auto mains failure panels for critical loads, such as monitoring, surveillance, emergency equipment and communication services including aviation and obstruction indicators. BWSL exclusively uses energy saving illumination systems.|$|E
5000|$|... #Caption: <b>Redundant</b> <b>power</b> <b>supplies</b> in a {{computer}} system {{reduce the risk of}} an unexpected loss of power.|$|R
5000|$|Some {{people use}} the term [...] "midplane" [...] to {{describe}} a board that sits between and connects a hard drive hot-swap backplane and <b>redundant</b> <b>power</b> <b>supplies.</b>|$|R
50|$|Each CX4 array {{consists}} of dual redundant hot-swappable components including storage processors, mirrored cache and battery backup, {{as well as}} <b>redundant</b> <b>power</b> <b>supplies.</b> The CX4 series supports Fibre Channel and iSCSI host connectivity.|$|R
5000|$|Redundant {{electrical}} power inputs on different circuits, usually both or all protected by {{uninterruptible power supply}} units, and <b>redundant</b> <b>power</b> <b>supply</b> units, so that single power feed, cable, UPS, or power supply failures do not lead to loss of power to the system.|$|E
50|$|Each {{datacenter}} uses {{load balancing}} across web, mail, and SQL servers, <b>redundant</b> <b>power</b> <b>supply,</b> hard drives with full disk encryption, and exclusive use of Linux and other open-source software. In December 2014, ProtonMail joined the RIPE NCC {{in an effort}} to have more direct control over the surrounding Internet infrastructure.|$|E
5000|$|Redundant power supply: The {{computers}} and peripheral equipment were supplied with a <b>redundant</b> <b>power</b> <b>supply</b> {{based on two}} identical voltage transformers. Each was capable of supplying the power necessary for all of the equipment. They were normally alternately switched, but if one failed the other would take over. On-board batteries could also supply temporary power.|$|E
50|$|The J6350 {{gives up}} to 2 Gbit/s in performance. It has six PIM slots for {{additional}} LAN/WAN connectivity, Avaya VoIP Gateway, and WAN acceleration. These routers have optional <b>redundant</b> <b>power</b> <b>supplies</b> for high system availability.|$|R
50|$|Many {{computer}} servers offer {{the option of}} <b>redundant</b> <b>power</b> <b>supplies,</b> {{so that in the}} event of one <b>power</b> <b>supply</b> failing, one or more other <b>power</b> <b>supplies</b> are able to power the load. This is a critical point - each <b>power</b> <b>supply</b> must be able to power the entire server by itself.|$|R
5000|$|The {{latest version}} of Exalogic compute nodes have two Intel E5-2699v3 2.3 GHz Xeon (18-core) {{processors}} and eight 32 GB DDR4 2133 MHz RAM {{for a total of}} 256 GB per node. Two 400 GB SSDs (RAID1) and <b>redundant</b> <b>power</b> <b>supplies</b> ...|$|R
50|$|Initially 8 {{blocks of}} office {{buildings}} were {{built by the}} developer. These are purpose built for multimedia companies by being provided extra height ceiling and under floor trunking (some with raised floor), fibre optic wiring, dual <b>redundant</b> <b>power</b> <b>supply,</b> uninterruptible power supply and back up generators for whole electricity load, which common practice in Malaysia is only to 30% load.|$|E
50|$|Unexpected loss {{of power}} for any reason (including power outage, power supply failure or {{depletion}} of battery on a mobile device) forces the system user to perform a cold boot once the power is restored. Some BIOSes have an option to automatically boot the system after a power failure. An uninterruptible power supply (UPS), backup battery or <b>redundant</b> <b>power</b> <b>supply</b> can prevent such circumstances.|$|E
50|$|While HPI Management Instruments are {{organized}} and addressed by Domain and Resource, the hardware components that are managed by those Management Instruments {{are identified individually}} in the RDRs associated with each Management Instrument. Physical hardware components in HPI are called Entities and are identified with an Entity Path. An Entity Path contains multiple elements, with the first element describing where the hardware Entity {{is located in a}} containing Entity, the second element describing where that Entity is located in a larger container, and so on. For example, a <b>redundant</b> <b>power</b> <b>supply</b> for a chassis in a system that spans multiple racks might have the entity path of POWER_SUPPLY.2,SUBRACK.3,RACK.1.|$|E
5000|$|... a DELL PowerEdge R620 x8 Base server w/ <b>redundant</b> {{titanium}} <b>power</b> <b>supply</b> ...|$|R
50|$|The zEnterprise System {{supports}} {{an optional}} zEnterprise BladeCenter Extension (zBX). This add-on infrastructure supports redundant top-of-Rack switches, <b>redundant</b> <b>power</b> <b>supplies,</b> extra blowers, and IBM BladeCenter chassis. This add-on chassis allows POWER7 and x86 blade servers {{to be integrated}} with and managed from the mainframe. The Gameframe installation at Hoplon Infotainment {{is an example of}} a hybrid mainframe.|$|R
50|$|Computers operate over a {{range of}} DC voltages, but {{utilities}} deliver power as AC, and at higher voltages than required within computers. Converting this current requires one or more <b>power</b> <b>supply</b> units (or PSUs). To ensure that the failure of one power source {{does not affect the}} operation of the computer, even entry-level servers may have <b>redundant</b> <b>power</b> <b>supplies,</b> again adding to the bulk and heat output of the design.|$|R
50|$|On flight day 10, {{the third}} spacewalk of the STS-134 mission was conducted. The spacewalk {{made use of}} a new spacewalk pre-breathe {{protocol}}, called In-Suit Light Exercise (ISLE), instead of the normal campout pre-breathe protocol. The new pre-breathe protocol had the astronauts breathe pure oxygen for 60 minutes in the airlock, which had its air pressure lowered to 10.2 Psi (703hPa). The astronauts then put their spacesuits on, performed light exercise and rested for an additional 50 minutes, breathing pure oxygen all the while. After astronauts Drew Feustel and Mike Fincke exited the Quest Airlock, the pair began installing the Power Data Grapple Fixture (PDGF). The fixture itself {{and most of its}} components were installed, but the data cable associated with it was to be installed later. The spacewalking pair then moved on and routed some new power cables from the Unity module to the Zarya module on the Russian segment of the ISS, providing a <b>redundant</b> <b>power</b> <b>supply</b> to the Russian segment. Feustel and Fincke then moved on to finish up the installation of the wireless video system which Fuestel and Greg Chamitoff had begun to install on EVA 1. The pair also took pictures of the Zarya module's thrusters and captured some infrared video of an experiment delivered on board the Express Logistics Carrier (ELC) 3. Commander Mark Kelly documented the spacewalk from inside the station. While the EVA was going on, pilot Greg Johnson and mission specialist Roberto Vittori assisted Expedition 28 flight engineer Ron Garan in stowing new equipment and supplies on the ISS.|$|E
40|$|The {{safety of}} public utility {{facilities}} is a function not only of effectiveness of the electronic safety systems, used for protection of property and persons, but it also depends on the proper functioning of their power supply systems. The authors of the research paper analysed the power supply systems, which are used in buildings for the access control system that is integrated with the closed-circuit TV. The Access Control System {{is a set of}} electronic, electromechanical and electrical devices and the computer software controlling the operation of the above-mentioned elements, which is aimed at identification of people, vehicles allowed to cross the boundary of the reserved area, to prevent from crossing the reserved area and to generate the alarm signal informing about the attempt of crossing by an unauthorised entity. The industrial electricity with appropriate technical parameters is a basis of proper functioning of safety systems. Only the electricity supply to the systems is not equivalent to the operation continuity provision. In practice, <b>redundant</b> <b>power</b> <b>supply</b> systems are used. In the carried out reliability analysis of the power supply system, various power circuits of the system were taken into account. The reliability and operation requirements for this type of system were also included...|$|E
5000|$|The Synology RackStation is [...] "rack mounted" [...] {{version of}} the DiskStation, that {{features}} all of the same abilities of the DiskStation with the exception that {{it is meant to}} be enclosed in a 19-inch rack cage. Some of Synology's rack models feature <b>redundant</b> <b>power</b> <b>supplies.</b> As with its desktop-oriented brethren, select models of the RackStation also feature storage scalability.|$|R
50|$|In {{enterprise}} storage {{the term}} {{refers to a}} larger physical chassis. The term can be used both in reference to network-attached storage (NAS) and components of a storage area network (SAN) or be used to describe a chassis directly attached {{to one or more}} servers over an external bus. Like their conventional server brethren, these devices may include a backplane, temperature sensors, cooling systems, enclosure management devices, and <b>redundant</b> <b>power</b> <b>supplies.</b>|$|R
50|$|The Apple Network Server {{systems were}} PowerPC-based systems {{designed}} by Apple Computer to have numerous high-end features that standard Apple hardware did not have, including swappable hard drives, <b>redundant</b> <b>power</b> <b>supplies,</b> and external monitoring capability. These systems {{were more or}} less based on the Power Macintosh hardware available at the time but were designed to use AIX (versions 4.1.4 or 4.1.5) as their native operating system in a specialized version specific to the ANS.|$|R
50|$|The Intel-based Xserves were {{announced}} at the Worldwide Developers Conference on August 7, 2006, and are significantly faster compared to the Xserve G5. They use Intel Xeon (Woodcrest) processors, DDR2 ECC FB-DIMMs, ATI Radeon graphics, a maximum storage capacity of 2.25 TB when used with three 750 GB drives, optional <b>redundant</b> <b>power</b> <b>supplies</b> and a 1U rack form factor. The Intel Xserves now had on board video, freeing up an expansion slot.|$|R
50|$|SVC nodes {{are always}} clustered, {{with a minimum}} of 2 and a maximum of 8 nodes, and linear scalability. Nodes are {{rack-mounted}} appliances derived from IBM System x servers, protected by <b>redundant</b> <b>power</b> <b>supplies</b> and integrated batteries. Earlier models featured external battery-backed <b>power</b> <b>supplies.</b> Each node has Fibre Channel ports simultaneously used for incoming, outgoing, and intracluster data traffic. Hosts may also be attached via FCoE and iSCSI Gbit Ethernet ports. Intracluster communication includes maintaining read/write cache integrity, sharing status information, and forwarding reads and writes to any port. These ports must be zoned together.|$|R
50|$|Even fully {{passive network}} taps {{introduce}} new points of failure into the network. There {{are several ways}} that taps can cause problems, and this should be considered when creating a tap architecture. Consider non-powered taps for optical-only environments or throwing star network tap for copper 100BT. This allows you to modify the intelligent aggregation taps that may be in use and avoids any complications when upgrading from 100 megabit to gigabit to 10 gigabit. <b>Redundant</b> <b>power</b> <b>supplies</b> are highly recommended.|$|R
5000|$|... 1) Server Hardware Failure - Preventing {{a server}} failure is very difficult, {{but it is}} {{possible}} to take precautions to avoid total server failure through the user of <b>Redundant</b> <b>Power</b> <b>Supplies,</b> RAID disk sets. [...] 2) Human Error - These disasters are major reasons for failure. Human error and intervention may be intentional or unintentional which can cause massive failures such as loss of entire systems or data files. This category of data loss includes accidental erasure, walkout, sabotage, burglary, virus, intrusion, etc.|$|R
5000|$|ProCurve 600 <b>Redundant</b> {{external}} <b>power</b> <b>supply</b> - supports one of {{six times}} <b>Redundant</b> <b>Power</b> for series 2600-PWR (not series 2600 w/o PWR), 2610, 2800, 3400cl, 6400cl and 7000dl {{as well as two}} times optional External PoE Power for series 2600-PWR, 2610-PWR or mandatory External PoE Power for series 5300 with xl 24-Port 10/100-TX PoE Module only ...|$|R
50|$|On May 12, 2002, Unit 2 was {{automatically}} {{shut down}} {{due to the}} failure of both <b>redundant</b> DC <b>power</b> <b>supplies</b> in the Reactor Control & Instrumentation System. Due to inadequate corrective actions, the same event occurred on February 5, 2003.|$|R
5000|$|The Juniper EX2200-C is a compact, affordable, {{and energy}} {{efficient}} switch in a 1 U high and 10.8 inch wide formfactor. The EX2200-C {{does not have}} dual <b>redundant</b> <b>power</b> <b>supplies</b> nor does it have fans and instead the small unit stays cool with a heat sink that sits {{on the back of}} the unit. Even the PoE version is fanless. The EX2200-C is a small 19" [...] rack mountable (using optional brackets) or desktop switch with 12 gigabit ports with a choice of PoE or non-PoE. The switch also has 2 SFP uplink ports for virtual chassis (stacking), link aggregation or as uplinks to aggregation switches. The EX2200-C has a data rate of 28 Gbit/s.|$|R
50|$|Other {{problems}} {{can affect the}} ability to start or control the machine remotely: hardware failure of the machine or network, failure of the BIOS settings battery (the machine will halt when started before the network connection is made, displaying an error message and requiring a keypress), loss of control of the machine due to software problems (machine hang, termination of remote control or networking software, etc.), and virus infection or hard disk corruption. Therefore, {{the use of a}} reliable server-class machine with RAID drives, <b>redundant</b> <b>power</b> <b>supplies,</b> etc., will help to maximize availability. Additionally, a device which can switch the machine off and on again, controlled perhaps by a remote signal, can force a reboot which will clear problems due to misbehaving software.|$|R
50|$|The first {{generation}} of the Oracle Database Appliance (ODA V1) is a two-node cluster in a single rack-mounted chassis. Inside the chassis are two servers, configured in a cluster, with shared storage. Each server contains two six-core processors, {{for a total of}} 12 cores per server. Each server also contains 96GB memory, six 1Gbit NICs, and two 10Gbit NICs. NICs are configured in an active/passive HA (bonding) configuration. Inside the appliance holds 4 x 73GB of shared SSD storage and 20 x 600GB of shared hard disk storage. The appliance contains <b>redundant</b> <b>power</b> <b>supplies</b> and cooling fans. Storage is configured at deploy time for either double mirroring (giving an overall capacity of 6TB data), or triple mirroring (yielding a capacity of 4TB data).|$|R
40|$|Because {{replacement}} parts for the existing facility data acquisition interface system at TA- 55 have become scarce and {{are no longer}} being manufactured, reliability studies were conducted to assess various possible replacement systems. A new control system, based on Allen-Bradley Programmable Logic Controllers (PLCs), {{was found to have}} a likely reliability 10 times that of the present system, if the existing Continuous Air Monitors (CAMS) were used. Replacement of the old CAMs with new CAMs will result in even greater reliability as these are gradually phased in. The new PLC-based system would provide for hot standby processors, redundant communications paths, and <b>redundant</b> <b>power</b> <b>supplies,</b> and would be expandable and easily maintained, as well as much more reliable. TA- 55 is the Plutonium Processing Facility which processes and recovers Pu- 239 from scrap materials...|$|R
5000|$|Each VPLEX {{engine in}} a cluster {{consists}} of two redundant IO directors and one IO annex, each being a single rack unit (1U) physical device. Each engine has 32 Fibre Channel ports (the VS1 model 16 front-end ports and 16 back-end ports) or 16 Fibre Channel ports (the VS2 model has 8 front-end ports and 8 back-end ports) and is protected by two <b>redundant</b> stand-by <b>power</b> <b>supplies.</b>|$|R
5000|$|In March the KKG {{announced}} to the federal authorities an event that took place after the 2008 revision. Four of the <b>redundant</b> <b>power</b> lines <b>supplying</b> the emergency system failed during a test due to a faulty protective circuit. In case of a real emergency this could cause troubles only in case of malfunctioning of the other safety mechanisms, but since the same problem occurred to multiple strands, the event has been assessed as level 1.|$|R
40|$|For {{the last}} several years, NASA has pursued the {{development}} of low-cost high-reliability inertial navigation systems that would satisfy {{a broad spectrum of}} future space and avionics missions. Two specific programs have culminated in the construction of a Redundant Strapdown Laser Gyro Navigation System. These two programs were for development of a space ultrareliable modular computer (SUMC) and a redundant laser gyro inertial measurement unit (IMU). The SUMC is a digital computer that employs state-of-the-art large-scale integrated circuits configured in a functional modular breakdown. The redundant laser gyro IMU is a six-pack strapdown sensor package in a dodecahedron configuration which uses six laser gyros to provide incremental angular positions and six accelerometers for linear velocity outputs. The sensor arrangement allows automatic accommodation of two failures; a third failure can be tolerated provided it can be determined. The navigation system also includes <b>redundant</b> <b>power</b> <b>supplies,</b> built-in test-equipment (BITE) circuits for failure detection, and software which provides for navigation, redundancy management, and automatic calibration and alignment...|$|R
