15|69|Public
5000|$|More formally, let [...] denote a <b>relational</b> <b>scheme</b> {{over the}} set of {{attributes}} [...] {{with a set of}} functional dependencies [...] We say that a functional dependency [...] is logically implied by ,and denote it with [...] if and only if for every instance [...] of [...] that satisfies the functional dependencies in , r also satisfies [...] We denote by [...] {{the set of}} all functional dependencies that are logically implied by [...]|$|E
5000|$|Unlike JSON, {{which can}} only {{represent}} data in a hierarchical model with each child node having a single parent, YAML also offers a simple <b>relational</b> <b>scheme</b> that allows repeats of identical data to be referenced from two or more points in the tree rather than entered redundantly at those points. This {{is similar to the}} facility IDREF built into XML. The YAML parser then expands these references into the fully populated data structures they imply when read in, so whatever program is using the parser {{does not have to be}} aware of a relational encoding model, unlike XML processors, which do not expand references. This expansion can enhance readability while reducing data entry errors in configuration files or processing protocols where many parameters remain the same in a sequential series of records while only a few vary. An example being that [...] "ship-to" [...] and [...] "bill-to" [...] records in an invoice are nearly always the same data.|$|E
40|$|In {{this paper}} we propose a new and elegant {{approach}} toward the generalization of frequent itemset mining to the multirelational case. We define relational itemsets that contain items from several relations, and a support measure that can easily be interpreted based on the key dependencies as defined in the <b>relational</b> <b>scheme.</b> We present an efficient depth-first algorithm, which mines relational itemsets directly from arbitrary relational databases. Several experiments show the practicality and usefulness of the proposed approach. 1...|$|E
40|$|Acquisition of domain {{ontology}} from database {{has been}} of catholic concern. This paper, taking <b>relational</b> <b>schemes</b> as example, analyzes how to identify {{the information about the}} structure of <b>relational</b> <b>schemes</b> in legacy systems. Then, it presents twelve extraction rules, which facilitate the obtaining of terms and relations from the <b>relational</b> <b>schemes.</b> Finally, it uses the EER diagram to further obtain semantic information from <b>relational</b> <b>schemes</b> for refining ontology model. The development method of domain ontology based on reverse engineering is a supplement to forward engineering. The union of the two development methods is certainly beneficial for the designers of domain ontology. © 2009 IEEE...|$|R
40|$|Within {{the poetic}} creation, the culture-nature {{relation}} plays between the ontological {{structures of the}} imaginary and their particular way of being rendered in a literary work, i. e. style. The imaginary is manifest through primary images which we define as substitutive mental representations of reality. The poetic images {{are different from the}} primary images in that they are never punctual, but they fulfill themselves through combinations of first-degree images, combinations that are at least binary. Still, the ontological is not a sum of objects, but rather an infinity of relations established between these objects. The imaginary transfers relations from the objective world into its mental double through what we call imaginative <b>relational</b> <b>schemes.</b> We draw a distinction between external <b>relational</b> <b>schemes,</b> which are constituents of rhetorical devices, and external <b>relational</b> <b>schemes,</b> which are accountable for textual syntagmatics. In poetry, the dynamics of images and imaginative schemes is visible in style, which materializes them in the act of creation...|$|R
40|$|Nested {{relations}} in partitioned normal form (PNF) {{are an important}} subclass of nested relations that are useful in many applications. In this paper we {{address the question of}} determining when every PNF relation stored under one nested relation scheme can be transformed into another PNF relation stored under a different nested relation scheme without loss of information, referred to as the two schemes being data equivalent. This issue is important in many database application areas such as view processing, schema integration and schema evolution. We introduce a tree restructuring operators for nested <b>relational</b> <b>schemes,</b> called COMPRESS, and prove that two nested <b>relational</b> <b>schemes</b> are data equivalent if and only if one can transform one scheme into the other by a finite sequence of the COMPRESS operator and its inverse. Key Words: Nested relations, Nested algebra, Partitioned normal form, Data equivalence, Information capacity, Multivalued dependencies 1. INTRODUCTION It is widely acce [...] ...|$|R
40|$|This work aims to {{understand}} the organizational strategies used by the ballet company Grupo Corpo, based in Belo Horizonte, Minas Gerais, Brazil. It {{is based upon the}} premise that such practices are configured (in) this organization’s daily management practices. In order to achieve the proposed objective, four questions considered to be recursive in this field were used as guidelines: (a) What is strategy? (b) Who is the strategist? (c) What do strategists do? (d) What can explain an analysis of strategists and their actions? As methodological support, an ethnography was conducted. In addition, the Grounded Theory procedure was used to develop the proposal of bringing about a substantial concept of strategy. A situational analysis was also done (Clarke, 2005) {{and at the end of}} the study, three organizational spheres within the Corpo de Balé group: its Ballet Group sphere, its Citizen sphere, and its Dance School sphere. The practices and relationships constructed between these and the subjects that actually included them in the organization under study were also identified. Thus, a <b>relational</b> <b>scheme</b> was established, involving the observed practices and the individual and collective actors. Based on the analysis of the <b>relational</b> <b>scheme</b> a strategy concept was elaborated for the Corpo de Balé group...|$|E
40|$|Abstract. Multiple {{efforts have}} been devoted to face the {{problems}} of database modelling. One {{of them is the}} automatization of database design process using CASE tools. Frequently, these tools do not completely support all phases of database analysis and the design methodology that they propose. Therefore, we propose to incorporate new features to these tools enhancing them and solving some of the modelling problems. In this paper, we present an add-in module that aims to generate triggers for preserving the multiplicity constraints of a conceptual scheme in the transformation to a <b>relational</b> <b>scheme.</b> The module is integrated into the RATIONAL ROSE case tool. ...|$|E
40|$|In a 4 -month constructivist {{teaching}} experiment, I investigated how one 7 th grade {{child and}} two 8 th grade children constructed algebraic reasoning as a transformation in their multiplicative schemes. Algebraic reasoning is the purposeful functioning of a child’s schemes and operations in contexts I defined as algebraic. The algebraic contexts explored {{in my study}} required a child to create equality between two multiplicative situations (compilations) by operating on/with the individual units (1 s) and composite units that constitute the compilations. Algebraic reasoning emerges in two ways from these contexts. First, when a child operates on and with the structure {{of his or her}} schemes, the child is reasoning algebraically. Second, if a child’s schemes form the underpinnings for formal algebraic conceptions, the child is reasoning algebraically. ^ I identified three specific schemes of equality, a unidirectional scheme of equality, a <b>relational</b> <b>scheme</b> of equality, and a quantitative <b>relational</b> <b>scheme</b> (QRE) of equality, that were constructed by the children in my study as they produced equality between two compilations. A QRE scheme of equality is comprised of operations on composite units across two compilations. The QRE scheme constitutes algebraic reasoning because a child operates on the structure of their multiplicative schemes with the structure of their additive schemes. A child also operates on the structure of their equality scheme with the structure of their multiplicative schemes. Moreover, because the QRE scheme is comprised of operations on composite units across two compilations, it forms the cognitive roots for the algebraic concepts of the distributive property, quantitative conservation, and solving linear equations. The construction of the QRE scheme by a child Joe was crucial to my study because it provided evidence of the existence of the hypothesized QRE scheme. ^ The second scheme of equality, a <b>relational</b> <b>scheme,</b> consists of balancing the total 1 s from each compilation via operations on 1 s. This scheme also constitutes algebraic reasoning because a child increases the smaller total via addition and decreases the larger total via subtraction until they produce equality. They operate with the structure of their additive scheme on the structure of their equality scheme. A child with the third scheme of equality, a unidirectional scheme, creates equality by transforming the total from one of the given compilations to produce the second total. A child who operates with this scheme is not reasoning algebraically. ^ I suggest a trajectory for the development of these three equality schemes. A child begins with a unidirectional scheme of equality. Next, they construct a <b>relational</b> <b>scheme</b> of equality as their scheme progresses to include operating on both totals with 1 s to create balance. A second progression occurs to the most advanced equality scheme, a QRE scheme, when a scheme is constructed that moves past operations on 1 s to include operations on composite units. My research goes beyond a description of the three schemes as it provides evidence as to how a child transitions from not having a QRE scheme to having one. Moreover, my research informs the teaching of mathematics as the UDS and QRE tasks offer elementary school teachers a way to give children opportunities to form the foundations for the formal algebraic concepts of the distributive property, solving linear equations, and quantitative conservation. ...|$|E
40|$|Hypergraph {{has been}} {{proven to be a}} very useful {{structure}} in relational database theory. The family of <b>relational</b> <b>schemes</b> or join dependencies can be divided into acyclic and cyclic, according to their associated hypergraphs. The GYO Algorithm is an efficient algorithm to decompose an acyclic join dependency into a set of multivalued dependencies, but it fails on cyclic join dependencies. A new algorithm extended from the GYO Algorithm is given to decompose a cyclic join dependency into a set of smaller join dependencies. This algorithm also works for acyclic join dependencies...|$|R
40|$|This paper {{treats the}} {{problems}} arising {{at the stage}} of logical database design. It comprises a synthesis {{of the most common}} inference models of functional dependencies, deals with the problems of building covers for sets of functional dependencies, makes a synthesizes of normal forms, presents trends regarding normalization algorithms and provides a temporal complexity of those. In addition, it presents a summary of the most known keys’ search algorithms, deals with issues of analysis and testing of <b>relational</b> <b>schemes.</b> It also summarizes and compares the different features of recognition of acyclic database schemas...|$|R
40|$|AbstractIn {{this paper}} we study the direct product decompositions of closure {{operations}} and lattices of closed sets. We characterize the direct product decompositions of lattices of closed sets {{in terms of}} closure operations, and find those decompositions of lattices which correspond to the decompositions of closures. If a closure on a finite set is represented by its implication base (i. e. a binary relation on the powerset), we construct a polynomial algorithm to find its direct product decompositions. The main characterization theorem is also applied to define direct product decompositions of <b>relational</b> database <b>schemes</b> {{and to find out}} what properties of <b>relational</b> databases and <b>schemes</b> are preserved under the decompositions...|$|R
30|$|Furthermore, {{there are}} some caveats for reading our classification. First {{it has to be}} read as a <b>relational</b> <b>scheme.</b> The {{different}} countries do not lack risk awareness, ascetic ideals and moral attitudes as such. But relative to other countries in our sample they have lower degrees concerning these safety related attitudes. Second these culture patterns are based on one or two questions and focus only on drinking and driving. We neither reconstructed the bigger picture of a safety culture nor did we investigate the relationship between cultural elements in terms of their consistency. The third caveat is that attitudes do not correspond directly with risk taking behaviour and other factors that concern traffic fatalities.|$|E
40|$|The UMLS is {{a complex}} {{collection}} of medical terms and relationships derived from standard classifications. Appreciating the scope and layout of these relations from text descriptions of relational schema is difficult. The graphical technique of Logical Data Structure (LDS) representation was employed to illustrate the UMLS schema as a data abstraction, affording additional insights that might otherwise escape notice. An LDS representation of the Metathesaurus offers the following advantages: 1) the separation of a viewpoint from physical data structures enables a global outline of the contents; 2) the graphical map makes the interrelation of data visible; and 3) the logical entities explicitly reflect the decision-making which was implicit or ambiguous in the <b>relational</b> <b>scheme...</b>|$|E
40|$|The {{building}} of any object, {{even the simplest}} one, requires {{a great number of}} participants and along with this, a great amount of money. In order to enable communication among all the participants in building and to make building more economical, norms and standards have been formed. The formation of corresponding database is a first step in development of applications software of automatic data processing of norms and standards. This paper shows the analyse of the problem trough which the attribute and relationships of the database are established. The <b>relational</b> <b>scheme</b> is established by further normalization. The program is implemented in dBase III+ which enables write, view, delete and update of the data taking care of the integrity rules...|$|E
40|$|Abstract. Traditional {{cryptographic}} hash functions allow one to easily check whether the original plain-texts are equal or not, given {{a pair of}} hash values. Probabilistic hash functions extend this concept where given a probabilistic hash of a value and the value itself, one can efficiently check whether the hash corresponds to the given value. However, given distinct probabilistic hashes of the same value {{it is not possible}} to check whether they correspond to the same value. In this work we introduce a new cryptographic primitive called relational hash using which, given a pair of (relational) hash values, one can determine whether the original plain-texts were related or not. We formalize various natural security notions for the relational hash primitive- one-wayness, unforgeability and oracle simulatibility. We develop a <b>relational</b> hash <b>scheme</b> for discovering linear relations among bit-vectors (elements of Fn 2) and Fp-vectors. Using these linear <b>relational</b> hash <b>schemes</b> we develop <b>relational</b> hashes for detecting proximity in terms of hamming distance. These proximity <b>relational</b> hashing <b>scheme</b> can be adapted to a privacy preserving biometric authentication scheme. We also introduce the notion of relational encryption, which is a regular semantically secure public key encryption for any adversary which only has access to the public key. However, a semi-trusted entity can be given a relational key using which it can discover relations among ciphertexts, but still cannot decrypt and recover the plaintexts...|$|R
40|$|In this paper, {{we present}} the various classes of {{inference}} templates {{which can be}} instantiated for making inferences in a fuzzy <b>relational</b> inference <b>scheme.</b> These templates are interval-valued and they capture {{the upper and lower}} bounds of fuzzy inference using point data. We present five classes of interval-valued inference templates which can be instantiated with bounded fuzzy connectives...|$|R
40|$|Invited Talk at the {{workshop}} FCA 4 AI @ ECAI 2012. [URL] Concept Analysis (RCA) builds conceptual structures on sets of objects connected by sets of links, following an underlying entity-relationship diagram. These conceptual structures (concept lattice families) {{are composed of}} several concept lattices (one for each object set one wants to focus on) connected by relational attributes of various strengths. Concept lattice families can be read to extract interconnected relevant object groups and classifications {{as well as to}} derive implication rules. The RCA algorithm uses classical concept lattice building algorithms and a relational scaling step. In this talk, we recall the main principles of RCA and we elaborate on several issues (some of which are totally open) including querying relational data with RCA, looking at specific <b>relational</b> <b>schemes,</b> convergence of RCA when disturbing the classical algorithmic schema, and understanding the growth process of concepts...|$|R
40|$|Examines the {{constitutional}} {{boundaries of the}} judicial function {{in the light of}} the Human Rights Act 1998, focusing on the courts approach to the interpretive obligations under s. 3, the power to make declarations of incompatibility under s. 4, and the concept of judicial deference to statute law and parliamentary sovereignty. Discusses the debate around judicial deference and the background to enactment of the 1998 Act, the rules of statutory interpretation and the judiciary's views on the meaning of the obligation to read legislation to give effect to rights under the European Convention on Human Rights 1950 "so far as it is possible to do so". Argues that the concept of judicial deference was not contained within the 1998 Act and that judicial decisions, exemplified by R. v A (Complainant's Sexual History), have failed to reflect the structure of the ss. 3 and 4 which sought to enable the courts to uphold rights while also maintaining the legislature's authority. Suggests that the proper application of this dialogue or <b>relational</b> <b>scheme</b> would obviate the need for a further concept of judicial deference...|$|E
40|$|Terminology has an {{important}} role in the framework of specialised knowledge, especially as regards its elaboration, representation and transmission through verbal language. This study focuses on the nature of knowledge that is organised in terminological collections. Terms are interpreted as the units where the mental, linguistic, communicative, and referential facets of specialised knowledge coalesce. Terminology schools belonging to different traditions have attributed distinct values to the notion of term and to its content, as a consequence of the underlying terminological and linguistic theories used as reference (§ 2.). Accordingly, terminology theories and applications display the prevalence of either a prescriptive or descriptive approach; the former characterises the General Theory of Terminology, the latter is typical of contemporary schools related to socioterminology, textual terminology, the sociocognitive approach, and the communicative theory of terminology (§ 3.). An analysis of the nature of knowledge as represented in terminology works makes it possible to recognise the importance of representing the system of knowledge through a <b>relational</b> <b>scheme</b> of concepts and terms which allows the user to delineate the definition of the single units of knowledge (§ 4.). This model, which has been outlined since Wüster’s theory as a hierarchical frame of relations and dependences, can be described in terms of an ontology, also as a result of the recent integration of terminology with information science. Even though ontology offers a model of representation of specialised knowledge which is relatively neutral from the linguistic and cultural point of view, this structure can be integrated with the cognitive, sociological and pragmatic facets of terms, which have acquired a growing importance in contemporary terminology. In this way ontology remains a basic component of a satisfactory representation of concept systems, even when the analytical perspective is interlinguistic and intercultural, as is the case with the applications that follow the termontography method...|$|E
40|$|English Summary: The {{question}} of justification, {{may be because}} of its very nature, has been the common {{question of}} both Christian faith and theological endeavor. However, this interest is due more {{to the fact that}} through this teaching the Christian individual and the Christian community find their place not only in the church but in the world as well, which often rejects theological reflections pertaining to its life. The near future and even the present time serve a great amount of such examples. For this reason this condition poses a serious question to theological thinking. From what source can theological thinking and the church offer a point of orientation to participating {{in the life of the}} public? The need for this theological reflection is even more emphasized by the fact that there are some who are eager to replace the idea of modernity with the idea of postmodernity. In Central-Eastern Europe these two factors strengthen each other. This fact makes even more important such theological reflection. 	The aim of our work was to spread light on how the doctrine of justification and the idea of postmodernity as it is increasingly influential in our thinking, can be brought into conversation. It is of special importance pertaining to the fact that it is more and more difficult to articulate our Christian voice in the public arena. Theological argumentations are often treated suspicious. This state becomes even more complicated when it is deepened by the argument that stresses the skepticism about participating in forming the life of the public. This way human life is simplified, so that the human being is not able to find his/her place in relation to the larger context. Thus the human being is overwhelmed by unanswered questions. All this results in a quest for anew orientation center. This is a vivid characteristic of present Central-Eastern Europe, and evidently means two things: first, a critical task for Christian theology as it is compelled to find new ways in dealing with challenges through the rediscovery of tradition; second, Christian theology from its own source should constructively be present in forming the life of the public, which is also a critical task. The purpose of all this is to present that theological thinking has relevance in forming the life of the public. 	However, the question may arise: why do we take just the doctrine of justification from the wide Christian tradition? On the one hand, it is because the doctrine of justification is the central element of Christian thinking. This centrality is present in the joint Roman Catholic – Lutheran declaration on justification in 1999. On the other hand, this doctrine presents such a metanarrative, which means the basis for understanding the human reality, and thus a possible way of evaluating the idea of postmodernity. 	In order to reach our aim we decided to approach our theme from a special perspective, which helped us to observe those insights that helped us in initiating a dialogue between the doctrine of justification and postmodernity. Our first question was to answer: whether there was any relevance to talk about postmodernity and its influence in Hungary? The answer to this question was yes, besides being aware of the different opinions. Some advocate that it is not postmodernity but post-communism that means a more serious concern. The local theological thinking also started in the near past to formulate its opinion regarding postmodernity. It is also convinced that the influence of postmodernity could be experienced in many fields of life. Thus we come to the conclusion that the idea of postmodernity is increasingly influential in Central-Eastern Europe. The next step was to work with the history and the idea of postmodernity. Some believe that it is already at the end of the 19 th century that reference to postmodernity can be identified. However, the phenomenon itself is very young, since the definition itself appeared in a literature-critical context. The only possible way to understand it is if we compare it with modernity. For this reason, we go back to Hegel and Kant. In relation to Christianity we refer to the possible connection between the birth of postmodernity and the loss of influence of the providential thinking. The most influential articulation of the idea of postmodernity is in Lyotard’s book, titled The postmodern condition. His basic assumption is that the time of metanarratives is gone. This means that it is not possible any more to attain knowledge about the world based on such narratives. Habermas questions this by saying that postmodernity is not arrived yet. For him there is a narrative in which the autonomous individual and the social content of the community have a meeting point. Thus metanarratives do have role in evaluating the human reality. We introduce feminism as one, who is a vivid example of rejecting metanarratives. Feminist thinkers believe that metanarratives are to legitimate power dominance. However, in our endeavor it turns out that feminism with its claim is much closer to modernity than it would think. As one step further in our endeavor, we introduce postmodern theology. We observe that its appearance coincides with that of postmodernity. We also observe that in postmodern theology there are two main streams: deconstruction and process. Special attention is paid to David Tracy, Mark C. Taylor, John B. Cobb whose work leads us to think of the practical implications of postmodern theology. J. B. Cobb is convinced that we should talk about Christianity as a ‘sociohistorical movement’ in which the Christ-event is placed in the center. Having dealt with postmodernity, we turn our attention to the doctrine of justification from a special perspective as we are looking for those significant traces that underline the narrative and relational characters of this doctrine. The starting point in this is the biblical witness, which strengthens the idea that the doctrine of justification describes a relational setting. It is even more emphasized in the Gospels. Thus we see that a justified human life is full of social relations. This character is further stressed in the historical revisiting of the doctrine. At the end of our rethinking of the doctrine of justification we come to the conclusion that: it stresses a radical Christ-centeredness and turns from anthropocentric thinking to theocentric thinking; as a <b>relational</b> <b>scheme</b> it rejects every total human claim over other human life. This is its anti-totalitarian character; in its <b>relational</b> <b>scheme</b> the emphasis is on the person instead of the individual. At the end, we brought the doctrine of justification and postmodernity into dialogue with the intention to give impulses for public theological thinking. It is necessary to introduce the basic idea of public theology. In our view public theology can be considered as a positive apologetics on the one hand. On the other hand, it is grounded in the biblical witness to human reality. We explain our idea in relation to the three consequences which is underlined by H. Richard Niebuhr’s idea about the meaning of revelation. Thus in the <b>relational</b> <b>scheme</b> we define sin as non-relation, grace as relation with humanity, and faith as relation in Christ. Its anti-totalitarian character is stressed through the idea of constructive pluralism as the person comes to the fore. Such a <b>relational</b> <b>scheme</b> is defined as the community of solidarity. 	During our research we paid special attention to the vocabulary in use. Some expressions proved to be impossible to translate into Hungarian. There is some knowledge about and use introduced in Hungarian. However, it was better to stay with the original forms in our work. They were: metanarrative, narrative, ‘one’ and ‘other’, otherness, public, individual and person. Similarly, the expressions that were introduced by us: subnarrative, relational model/scheme, community of solidarity, person in relation, non-relation, relation with humanity, relation in Christ. 	We used the resources of Speer Library in Princeton, NJ, USA, the Library of Otto Friedrich Universität Bamberg, D, besides the Hungarian Libraries to our research...|$|E
40|$|In {{his article}} 2 ̆ 2 About Literary Systems and National Literatures, 2 ̆ 2 Elias Torres J. Feijó offers a polysystemic {{analysis}} with {{examples from the}} Iberian Peninsula. He argues that a community 2 ̆ 7 s literature {{can be understood as}} a) the gathering of so-called literary activities, which take place in a social space or b) something that identifies certain characteristics of a part or the whole of the members in a given social space. For his analysis, Torres Feijó employs Itamar Even-Zohar 2 ̆ 7 s notion of polysystem because it allows us to interpret each system generated by members of a given community and its delimitation, differentiation, and integration mechanisms in supra-systemic <b>relational</b> <b>schemes.</b> In addition, mechanisms can be obtained for the analysis of the fabrication and promotion of formulas which have as their objective systemic sovereignty: proto-systems, sub-systems, and para-systems...|$|R
40|$|AbstractCombinatorial propositions, {{concerning}} the maximal number of minimal keys are considered. It is {{shown that the}} results of Demetrovics about the maximal number of minimal keys on unbounded domains do not hold for finite domains. Using this result lower bounds for the size of minimum-sized Armstrong relations are derived. It is also shown that the maximal number of minimal keys in databases on nonuniform domains is also precisely exponential in the number of attributes like in the case of uniform domains. In relational database theory and practice, it is often postulated that none of the attributes of the primary key may ever obtain an undefined, unknown value, since otherwise we would not know what entity a tuple with an undefined value of the primary key represents. This assumption could be weakened. Eventually, a new approach based on distinguished tuples is presented. We consider key sets, a generalization of keys and existence constraints. Key concepts in nested <b>relational</b> <b>schemes</b> can be introduced on distinct equality concepts. It is shown {{that the results of}} Demetrovics hold for the nested relational model...|$|R
40|$|AbstractSimple-homotopy for cell {{complexes}} is {{a special}} type of topological homotopy constructed by elementary collapses and elementary expansions. In this paper, we introduce graph homotopy for graphs and Graham homotopy for hypergraphs and study {{the relation between the}} two homotopies and the simple-homotopy for cell complexes. The graph homotopy is useful to describe topological properties of discretized geometric figures, while the Graham homotopy is essential to characterize acyclic hypergraphs and acyclic <b>relational</b> database <b>schemes...</b>|$|R
40|$|Advances in {{relational}} database technology have made available {{relational database}} systems that support {{state of the}} art query languages and query processing algorithms. However, because database systems do not include adequate tools for database design, their use and accessibility is hampered by major difficulties that users experience in the process of designing a database. In this dissertation, we present methods and algorithms for automating the design of relational databases and describe a prototype design tool. Unlike researchers who have advocated the use of higher level data models, we focus on the pure relational model, because of its sound theoretical foundation and investigate issues in automatic design of relational databases. We present a new graph representation for functional dependencies, which simplifies and enhances several design algorithms, such as algorithms for computing closures, keys, and projecting dependency sets. We define the basis $B(F) $, a compact representation of $F^{+}$, which is used to find multiple BCNF decompositions. The basis also provides a way to find the generator of the $F$-closed sets, an essential component in the computation of Armstrong relations, which are relations representing a set of functional dependencies. We study the inference of multivalued dependencies from an acyclic <b>relational</b> <b>scheme.</b> These multivalued dependencies capture the relationship between the relations in the database. For the inference of functional dependencies within a relation, we optimize previously proposed algorithms. Queries can be used to rate candidate schemes according to how queries perform against them. We present an algorithm for finding the exact query formulation for a particular design, given a scheme independent definition of the query. Multiple ways of accessing the data or no way of accessing the data consistently can be a result indicating that the current design is not valid. Until now, there has been limited experience with feasibility and performance aspects of automatic relational design. We describe a prototype design tool and present a detailed performance study of the dependency inference algorithms implemented in this prototype...|$|E
40|$|Diplomsko delo obravnava izdelavo logične podatkovne sheme za {{register}} tuberkuloze bolnišnice KOPA — Golnik, izdelavo predloga grafičnega vmesnika za aplikacijo (ki bo podpirala uporabo baze tuberkuloze) ter analizo tveganja za celoten sistem registra tuberkuloze. Namen diplomske naloge je prikazati bolnišnici KOPA — Golnik eno izmed možnih rešitev prenove baze oz. model popolnoma nove baze registra tuberkuloze. Izbrana metodologija razvoja informacijskega sistema (IS) temelji na principu izgradnje prototipnega modela, ki smo ga skozi izdelavo nenehno dopolnjevali in spreminjali. V diplomski nalogi so zajeti vsi potrebni koraki od popisa procesov dela (tehnika IDEF 0), zbiranja maksimalnega nabora podatkov, izdelave entitetno relacijske sheme (Visio notacija), izdelave predloga grafičnega vmesnika (CSS — Cascading Style Sheets), analize tveganja ter predlog kontrole logičnega dostopa. Projektna rešitev omogoča registru tuberkuloze, da z novo bazo postane učinkovitejši, manj finančno potraten ter prijaznejši do ljudi, ki v registru delajo. This research {{deals with}} logical data scheme for the register of tuberculosis of KOPA Golnik hospital, making the {{proposal for a}} graphic interface for the application (that will {{support the use of}} tuberculosis database) and security analysis for the entire system of tuberculosis registry. The purpose of this research task is to show the hospital KOPA Golnik one of the possible solutions of database or renovation model of a completely new database for registry of tuberculosis. The chosen methodology for the development of IS is based on the principle of building a prototype model, which was complemented through the production. In the research task all the steps were taken from the inventory of work processes (technique IDEF 0), collecting the maximum data set, the production of entity <b>relational</b> <b>scheme</b> (Visio notation), a proposal for the graphic interface (CSS – Cascading Style Sheets), security analysis and proposal for controlinglogical access. The project solution with new database allows the register of tuberculosis to becomes more efficient, less financially wasteful and relieves unnecessary work of people who are working in the registry...|$|E
40|$|The {{recovery}} {{and preservation of}} the patrimony made of the instrumental registrations regarding the historical earthquakes is with no doubt a subject of great interest. This attention, besides being purely historical, must necessarily be also scientific. In fact, {{the availability of a}} great amount of parametric information on the seismic activity in a given area is a doubtless help to the seismologic researcher’s activities. In this article the project of the Sismos group of the National Institute of Geophysics and Volcanology of Rome new database is presented. In the structure of the new scheme the matured experience of five years of activity is summarized. We consider it useful for those who are approaching to ‘‘{{recovery and}} reprocess’’ computer based facilities. In the past years several attempts on Italian seismicity have followed each other. It has almost never been real databases. Some of them have had positive success because they were well considered and organized. In others it was limited in supplying lists of events with their relative hypocentral standards. What makes this project more interesting compared to the previous work is the completeness and the generality of the managed information. For example, {{it will be possible to}} view the hypocentral information regarding a given historical earthquake; it will be possible to research the seismograms in raster, digital or digitalized format, the information on times of arrival of the phases in the various stations, the instrumental standards and so on. The relational modern logic on which the archive is based, allows the carrying out of all these operations with little effort. The database described below will completely substitute Sismos’ current data bank. Some of the organizational principles of this work are similar to those that inspire the database for the real-time monitoring of the seismicity in use in the principal offices of international research. A modern planning logic in a distinctly historical context is introduced. Following are the descriptions of the various planning phases, from the conceptual level to the physical implementation of the scheme. Each time principle instructions, rules, considerations of technical–scientific nature are highlighted that take to the final result: a vanguard <b>relational</b> <b>scheme</b> for historical data...|$|E
40|$|Many {{properties}} of houses are of topological nature. The problem of three-dimensional encoding is solved here by first giving an axiomatic {{description of a}} simplified concept of >house house<) still much topolgical information is kept in these structures still making them a useful approach to encoding topological spaces. Finally, a lossless representation of observation structures in a <b>relational</b> database <b>scheme</b> which we call PLAV (Points, Lines, Areas, Volumes) is given. We expect PLAV to be useful for encoding higher dimensional (architectural) space-time complexes...|$|R
40|$|We {{describe}} the new {{release of the}} RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a <b>relational</b> evaluation <b>scheme,</b> and describe how the new release allows users to enhance performance using (in-domain) lexical information...|$|R
40|$|Simple-homotopy for cell {{complexes}} is {{a special}} type of topological homotopy constructed by elementary collapses and elementary expansions. In this paper, we introduce graph homotopy for graphs and Graham homotopy for hypergraphs and study {{the relation between the}} two homo-topics and the simple-homotopy for cell complexes. The graph homotopy is useful to describe topological properties of discretized geometric figures, while the Graham homotopy is essential to characterize acyclic hypergraphs and acyclic <b>relational</b> database <b>schemes.</b> (C) 2001 Elsevier Science B. V. All rights reserved...|$|R
40|$|The {{following}} study {{describes the}} conception {{and implementation of}} a document management system based on the object-relational database management system 'Illustra Server'. Special features of the prototype are: - Support of complex data types within a <b>relational</b> database <b>scheme</b> - Fulltext indexing of PostScript documents - Combination of attribute and fulltext search - Universal access via World Wide Web First, base terms are described which have basic importance for {{the understanding of the}} working field 'document management'. Subsequently, the description of the prototype's conception and implementation follows...|$|R
40|$|We are {{designing}} {{a framework that}} provides a common foundation for the integration of databases with other areas of computer science and engineering. This framework {{is based on the}} fundamental concepts: context-free grammars and database relations. Our goal is to provide automatic database support for complex objects that can be described by context-free grammars. Such support should include Data Definition, Data Update, Grammar Catalog Generation, Data Retrieval, and Database Restructuring. This paper addresses the first three areas: Data Definition: GeneRel automatically generates a set of normalized <b>relational</b> <b>schemes</b> under which ob,iects derived from a given grammar can be stored. Data Update: GenParse automatically generates parser specifications with insertion statements for storing sentences acceptable by a given grammar. Grammar Catalog Generation: GenRel, when applied to a meta-grammar, generates relations in which grammars derived from the meta-grammar can be stored. Furthermore, GenRel and GenParse can be implemented through the specification of semantic actions in a compiler-compiler specification of the meta-grammar. We believe that GenRel and GeneParse, together with our related efforts towards providing support for data retrieval and database restructuring in this environment, provide a tool that eliminates the need for manual relational database design, enhances data storage and querying, aids in the process of database restructuring, and provides a common foundation for the integration of databases with other areas of computer science and engineenng...|$|R
40|$|We {{propose a}} new {{approach}} to the design of <b>relational</b> database <b>schemes.</b> The main features of the approach are the following: (a) A combination of the traditional decomposition and synthesis approaches, thus allowing the use of both functional and multivalued dependencies. lb) Separation of structural dependencies relevant for the design process from integrity constraints, that is, constraints that do not bear any structural information about the data and which should therefore be discarded at the design stage. This separation is supported hy a simple syntactic test filtering out nonstructural dependencies. (cl Automatic correction of schemes which lack certain desirable properties...|$|R
40|$|We {{evaluate}} {{the accuracy of}} an unlexicalized statistical parser, trained on 4 K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for <b>relational</b> evaluation <b>schemes.</b> ...|$|R
40|$|In this paper, a Fuzzy Neural Network {{based on}} a fuzzy <b>relational</b> “IF-THEN” {{reasoning}} <b>scheme</b> (FRNN) is described. Different experiments on benchmark data from the UCI repository of Machine learning database are proposed for classification and approximation tasks. The model is compared with some other methods known in literature pointing out the fundamental features of the model...|$|R
40|$|We {{show how}} {{the use of a}} metaobject layer is useful to allow an easier {{evolution}} of a an object-oriented database integrating existing relational systems. A correct use of metaobjects allow to hide the persistence contingencies when linking an application with an external application. Moreover a metaobject layer allows the evolution of the database object model. We show in these lines how a underlying relational database can be used for final storage of data in a system allowing metamodeling abilities. We have designed a correspondence engine, driven by rules, linking standard object and relational structures. We want the designer to be able to alter these rules according to the evolution of the object model, in order to keep coherent <b>relational</b> <b>schemes.</b> So the correspondence engine has been designed as a specialisable protocol discriminating on the metaobjects of the object layer. 1 Object-Oriented Databases and Legacy Systems For some years, in database community, a growing interest has focused on object-oriented databases. Most of the cur-rent object oriented database management systems (OODBMS) are now uniform, that is, all levels, including the storage level, deal with objects. However, as it is shown in [10], the introduction of such information systems into companies induces many problems. On the one hand, companies have often made big investments in relational databases, and {{they do not want to}} abandon them; on the other hand, even if a company decides to transfer its data from a relationa...|$|R
