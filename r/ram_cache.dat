19|64|Public
50|$|Optional {{dedicated}} video <b>RAM</b> <b>cache</b> or use {{of system}} RAM.|$|E
5000|$|Performance - Speed, Efficiency, Resource Consumption (power, <b>ram,</b> <b>cache,</b> etc.), Throughput, Capacity, Scalability ...|$|E
50|$|For most {{practical}} applications, the 9340/9345 was functionally {{equivalent to a}} 3990/3390, although without non-volatile <b>RAM</b> <b>cache</b> of the 3990 and with a somewhat shorter maximum block length than the 3390.|$|E
40|$|Large <b>RAM</b> <b>caches</b> are {{generally}} used {{to speed up}} disk accesses. Such caches more effectively improve read performance than write performance, since write requests must be frequently written into disks {{to protect them from}} data loss or damage due to system failures. While Non-volatile <b>RAM</b> (NVRAM) <b>caches</b> can be used to improve write performance, large NVRAM caches are too expensive for many applications. This paper presents a new disk cache architecture called DCD, Disk Caching Disks. DCD takes the advantage of large data transfer sizes and uses inexpensive disk space to provide a high-performance, low-cost and reliable caching solution...|$|R
5000|$|... as <b>RAM</b> or <b>cache</b> {{memory in}} micro-controllers (usually from around 32 bytes up to 128 kilobytes) ...|$|R
50|$|Data in use is an {{information}} technology term referring to active data which is stored in a non-persistent digital state typically in computer random access memory (<b>RAM),</b> CPU <b>caches,</b> or CPU registers.|$|R
50|$|In more {{sophisticated}} computers {{there may be}} one or more <b>RAM</b> <b>cache</b> memories, which are slower than registers but faster than main memory. Generally computers {{with this sort of}} cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.|$|E
50|$|In 1989 a {{cover story}} in Byte {{magazine}} announced the Apricot VX FT Server as the world's first machine to incorporate the Intel 80486 microprocessor. This machine, designed by Bob Cross, was a fault-tolerant file server based on Micro Channel Architecture, incorporating an external <b>RAM</b> <b>cache</b> and its own UPS. The VX FT line consisted of Series 400 and Series 800, with four different models each. These (and their other systems) were manufactured in their state-of-the-art factory in Glenrothes, Fife, Scotland.|$|E
50|$|In October 2001, Michael J. Mahon, an enthusiast who frequents the Apple II usenet {{newsgroup}} comp.sys.apple2, proposed overclocking the Apple IIc Plus. Over {{the next}} few years, newsgroup members reported speeds ranging between 8 MHz - 10 MHz simply by changing the 16 MHz crystal oscillator on the motherboard to a faster one (the Apple IIc Plus divides the oscillator frequency by four to attain the actual processor frequency). Some users with 120ns static <b>RAM</b> <b>cache</b> reported problems attaining 10 MHz while others with 100ns chips were more successful. Most were able to achieve 8 MHz.|$|E
5000|$|<b>Cache</b> <b>RAM</b> for CPU {{core and}} CD-ROM. See the {{relevant}} sections for details.|$|R
25|$|Properties of {{the virtual}} machine, like guest OS, {{processor}}, processor features, video mode, video <b>RAM,</b> code <b>cache,</b> IDE controller reads and writes, Ethernet reads and writes, video frame rate and command line options {{can no longer be}} viewed.|$|R
50|$|By far {{the most}} common usage of this term is in modern {{multiprocessor}} CPU caches, where memory is cached in lines of some small power of two word size (e.g., 64 aligned, contiguous bytes). If two processors operate on independent data in the same memory address region storable in a single line, the cache coherency mechanisms in the system may force the whole line across the bus or interconnect with every data write, forcing memory stalls in addition to wasting system bandwidth. False sharing is an inherent artifact of automatically synchronized cache protocols and can also exist in environments such as distributed file systems or databases but current prevalence is limited to <b>RAM</b> <b>caches.</b>|$|R
5000|$|In a {{wide range}} of modern {{multi-user}} operating systems, an ordinary user cannot defragment the system disks since superuser (or [...] "Administrator") access is required to move system files. Additionally, file systems such as NTFS are designed to decrease the likelihood of fragmentation. Improvements in modern hard drives such as <b>RAM</b> <b>cache,</b> faster platter rotation speed, command queuing (SCSI/ATA TCQ or SATA NCQ), and greater data density reduce the negative impact of fragmentation on system performance to some degree, though increases in commonly used data quantities offset those benefits. However, modern systems profit enormously from the huge disk capacities currently available, since partially filled disks fragment much less than full disks, and on a high-capacity HDD, the same partition occupies a smaller range of cylinders, resulting in faster seeks. However, the average access time can never be lower than a half rotation of the platters, and platter rotation (measured in rpm) is the speed characteristic of HDDs which has experienced the slowest growth over the decades (compared to data transfer rate and seek time), so minimizing the number of seeks remains beneficial in most storage-heavy applications. Defragmentation is just that: ensuring that there is at most one seek per file, counting only the seeks to non-adjacent tracks.|$|E
5000|$|The Apple IIc Plus had {{comprised}} {{three new}} features {{compared to the}} IIc. The first and most noticeable feature was {{the replacement of the}} 5.25-inch floppy drive with the new 3.5-inch drive. Besides offering nearly six times the storage capacity (800 KB), the new drive had a much faster seek time (three times faster) and button-activated motorized ejection. To accommodate the increased data flow of the new drive, specialized chip circuitry called the MIG, an acronym for [...] "Magic Interface Glue", was designed and added to the motherboard along with a dedicated 2 KB static RAM buffer (the MIG chip is the only exception to there being no new technological developments present in the machine). The second most important feature was a faster 65C02 processor. Running at 4 MHz, it made the computer faster than any other Apple II, including the IIGS. Apple licensed the Zip Chip Apple II accelerator from third-party developer Zip Technologies and added to the IIc Plus; instead of the all-in-one tall chip design, Apple engineers broke out the design into its core components and integrated them into the motherboard (a 4 MHz CPU, 8 KB of combined static <b>RAM</b> <b>cache,</b> and logic). The CPU acceleration was a last-minute feature addition, which in turn made the specialized circuitry for the use of a 3.5-inch drive unnecessary at full CPU speed as the machine was now fast enough to handle the data flow; that circuitry was left in place and put into operation nonetheless to support 1 MHz mode. By default the machine ran at 4 MHz, but holding down the 'ESC' key during a cold or warm boot disabled the acceleration so it could run at a standard 1 MHz operation — necessary for older software that depended on timing, especially games. The third major change was the internalization of the power supply into the Apple IIc Plus's case, utilizing a new miniature design from Sony (gone was the infamous [...] "brick on a leash" [...] external supply).|$|E
40|$|Abstract—NAND {{flash memory}} {{is widely used}} for {{secondary}} storage today. The flash translation layer (FTL) is the embedded software {{that is responsible for}} managing and operating in flash storage system. One important module of the FTL performs RAM management. It is well-known to {{have a significant impact on}} flash storage system’s performance. This paper proposes an efficient RAM management scheme called TreeFTL. As the name suggests, TreeFTL organizes address translation pages and data pages in RAM in a tree structure, through which it dynamically adapts to workloads by adjusting the partitions for address mapping and data buffering. TreeFTL also employs a lightweight mechanism to implement the least recently used (LRU) algorithm for <b>RAM</b> <b>cache</b> evictions. Experiments show that compared to the two latest schemes for RAM management in flash storage system, TreeFTL can reduce service time by 46. 6 % and 49. 0 % on average, respectively, with a 64 MB <b>RAM</b> <b>cache.</b> I...|$|E
40|$|Abstract—Cloud based video {{delivery}} platforms serve a signif-icant {{fraction of}} the entire Internet traffic and are continuously expanding with the growing demand. We study the provisioning of the server cluster to deploy at each location in such systems. We optimize the right server count, peering bandwidth, and server configuration as the disk size and the necessary SSD and/or <b>RAM</b> <b>caches</b> to sustain the intensive I/O load. Our analyses are based on actual server traces from a global content delivery platform. Our optimization captures the interaction of cache layers in each server, the interplay between egress/disk capacity and network bandwidth, storage read/write constraints and storage prices. Keywords-video content delivery network; resource allocation I...|$|R
5000|$|... {{areas are}} {{clustered}} {{in order to}} limit data paths thus frequently featuring defined structures such as <b>cache</b> <b>RAM,</b> multiplier, barrel shifter, line driver and arithmetic logic unit; ...|$|R
50|$|When {{executing}} self-modifying code, {{a change}} in the processor code immediately in front of the current location of execution might not change how the processor interprets the code, as it is already loaded into its PIQ. It simply executes its old copy already loaded in the PIQ instead of the new and altered version of the code in its <b>RAM</b> and/or <b>cache.</b>|$|R
40|$|This paper {{presents}} a new cache architecture called RAPID-Cache for Redundant, Asymmetrically Parallel, and Inexpensive Disk Cache. A typical RAPID-Cache {{consists of two}} redundant write buffers {{on top of a}} disk system. One of the buffers is a primary cache made of RAM or NVRAM and the other is a backup cache containing a two level hierarchy: a small NVRAM buffer on top of a log disk[1]. The backup cache has nearly equivalent write performance as the primary <b>RAM</b> <b>cache,</b> while the read performance of the backup cache is not as critical because normal read operations are performed through the primary <b>RAM</b> <b>cache</b> and reads from the backup cache happen only during error recovery periods. The RAPID-Cache {{presents a}}n asymmetric architecture with a fast-write-fast-read RAM being a primary cache and a fast-write-slow-read NVRAM-disk hierarchy being a backup cache. The asymmetric cache architecture allows cost-effective designs for very large write caches for high-end disk I/O systems that would o [...] ...|$|E
30|$|The memory {{hierarchy}} for the C 6678 device {{is shown in}} Figure 3 (left). L 1 cache is divided into 32 KB of L 1 program cache and 32 KB of L 1 data cache per core. There is also 512 KB of L 2 cache per core. Both L 1 data cache and L 2 memory can be configured either as random-access memory (<b>RAM),</b> <b>cache,</b> or part RAM/part cache. This provides additional capability of handling memory and can be exploited by the programmer. There is an on-chip shared memory of 4, 096 KB accessible by all cores, known as multi-core shared memory controller (MSMC) memory, and an external 64 -bit DDR 3 memory interface running at 1, 600 MHz with ECC support.|$|E
40|$|AbstractÐModern high {{performance}} disk systems make {{extensive use of}} nonvolatile RAM (NVRAM) write caches. A single-copy NVRAM cache creates a {{single point of failure}} while a dual-copy NVRAM cache is very expensive because of the high cost of NVRAM. This paper presents a new cache architecture called RAPID-Cache for Redundant, Asymmetrically Parallel, and Inexpensive Disk Cache. A typical RAPID-Cache consists of two redundant write buffers on top of a disk system. One of the buffers is a primary cache made of RAM or NVRAM and the other is a backup cache containing a two-level hierarchy: a small NVRAM buffer on top of a log disk. The small NVRAM buffer combines small write data and writes them into the log disk in large sizes. By exploiting the locality property of I/O accesses and taking advantage of well-known Log-structured File Systems, the backup cache has nearly equivalent write performance as the primary <b>RAM</b> <b>cache.</b> The read performance of the backup cache is not as critical because normal read operations are performed through the primary <b>RAM</b> <b>cache</b> and reads from the backup cache happen only during error recovery periods. The RAPID-Cache presents an asymmetric architecture with a fast-write-fast-read RAM being a primary cache and a fast-write-slow-read NVRAM-disk hierarchy being a backup cache. The asymmetrically parallel architecture and an algorithm that separates actively accessed data from inactive data in the cache virtually eliminate the garbage collection overhead, which are the major problems associated with previous solutions such as Log-structured File Systems and Disk Caching Disk. The asymmetric cache allows costeffective designs for very large write caches for high-end parallel disk systems that would otherwise have to use dual-copy, costly NVRAM caches. It also makes it possible to implement reliable write caching for low-end disk I/O systems since the RAPID-Cach...|$|E
40|$|Employing a delayed-write caching {{policy in}} a {{multilevel}} cache hierarchy may allow {{significant improvements in}} file system storage capacity as well as improving file system access over low-speed links. Cheap, high capacity mass storage systems can be viable as primary data repositories if front-end caching is used to mask the high access times typical of mass storage devices. Using distributed file system traces and cache simulations, we explore extensions and modifications to the traditional client caching model employed in such file systems as AFS and Sprite. High cache hit rates at an "intermediate cache server" [...] -a machine logically interposed between clients and servers that provides cached file service to the clients [...] -combined with high client cache hit rates, lend practicality to an integrated mass storage file system. In such a system, magnetic-tape or optical-based mass storage devices {{may be used as}} a first-class data repository, fronted by disk and <b>RAM</b> <b>caches</b> to offer accep [...] ...|$|R
40|$|The {{presentation}} of query biased document snippets {{as part of}} results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required {{as part of a}} search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58 % over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size <b>RAM</b> <b>caches.</b> Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache...|$|R
5000|$|On modern {{architectures}} with hierarchical memory, {{the cost}} of loading and storing input matrix elements tends to dominate {{the cost of}} arithmetic. On a single machine this {{is the amount of}} data transferred between <b>RAM</b> and <b>cache,</b> while on a distributed memory multi-node machine it is the amount transferred between nodes; in either case it is called the communication bandwidth. The naïve algorithm using three nested loops uses [...] communication bandwidth.|$|R
40|$|International audienceMore {{and more}} {{enterprise}} servers storage systems are migrating toward flash based drives (Solid State Drives) {{thanks to their}} attractive characteristics. They are lightweight, power efficient and supposed to outperform traditional disks. The two main constraints of flash memories are: 1) {{the limited number of}} achievable write operations beyond which a given cell can no more retain data, and 2) the erase-before-write rule decreasing the write performance. A <b>RAM</b> <b>cache</b> can help to reduce this problem; they are mainly used to increase performance and lifetime by absorbing flash write operations. RAM caches being very costly, their dimensioning is critical. In this paper, we explore some OLTP I/O workload characteristics with regards to flash memory cache systems structure and configuration. We try, throughout I/O workload analysis to reveal some important elements to take into account to allow a good dimensioning of those embedded caches...|$|E
40|$|This paper {{describes}} the design, implementation, {{and performance of}} the Inversion file system. Inversion provides a rich set of services to file system users, and manages a large tertiary data store. Inversion is built {{on top of the}} POSTGRES database system, and takes advantage of low-level DBMS services to provide transaction protection, fine-grained time travel, and fast crash recovery for user files and file system metadata. Inversion gets between 30 % and 80 % of the throughput of ULTRIX NFS backed by a non-volatile <b>RAM</b> <b>cache.</b> In addition, Inversion allows users to provide code for execution directly in the file system manager, yielding performance as much as seven times better than that of ULTRIX NFS. Introduction Conventional file systems handle naming and layout of chunks of user data. Users may move around in the file system's namespace, and may typically examine a small set of attributes of any given chunk of data. Most file systems guarantee some degree of consistency of user [...] ...|$|E
40|$|The Bullet server is an {{innovative}} file server that outperforms traditional file servers like SUN's NFS {{by more than}} a factor of three. It achieves high throughput and low delay by a radically different software design than current file servers in use. Instead of storing files as a sequence of disk blocks, each Bullet server file is stored contiguously, both on disk and in the server's <b>RAM</b> <b>cache.</b> Furthermore, it employs the concept of an immutable file, to improve performance, to enable caching, and to provide a clean semantic model to the user. The paper describes the design and implementation of the Bullet server in detail, presents measurements of its performance, and compares this performance with other well-known file servers running on the same hardware. 1. INTRODUCTION Traditional file systems were designed for small machines, that is, computers with little RAM memory and small disks. Emphasis was on supporting large files using as few resources as possible. To allow [...] ...|$|E
50|$|For {{very large}} Access databases, {{this may have}} {{performance}} issues and a SQL backend {{should be considered in}} these circumstances. This is less of an issue if the entire database can fit in the PC's <b>RAM</b> since Access <b>caches</b> data and indexes.|$|R
50|$|The {{speed of}} this {{recurrence}} (the load latency) {{is crucial to}} CPU performance, and so most modern level-1 caches are virtually indexed, which at least allows the MMU's TLB lookup to proceed in parallel with fetching {{the data from the}} <b>cache</b> <b>RAM.</b>|$|R
3000|$|... prep are not {{included}} in the measurement of instrumentation time since Instrumentor only needs to generate them for once and these preparations are completed in a short time before all apps’ instrumentation. Before each app’s instrumentation, we clear the <b>RAM</b> and swap <b>caches</b> in order to eliminate caching effects.|$|R
40|$|Abstract—Solid State Disks (SSDs) using NAND {{flash memory}} are {{increasingly}} being adopted in the high-end servers of datacenters to improve performance of the I/O-intensive applications. Compared to the traditional enterprise class hard disks, SSDs provide faster read performance, lower cooling cost, and higher power efficiency. However, write performance of a flash based SSD can be up to {{an order of magnitude}} slower than its read performance. Furthermore, frequent write operations degrade the lifetime of flash memory. A nonvolatile cache can greatly help to solve these problems. Although a <b>RAM</b> <b>cache</b> is relative high in cost, it has successfully eliminated the performance gap between fast CPU and slow magnetic disk. Similarly, a nonvolatile cache in an SSD can alleviate the disparity between the flash memory’s read and write performance. A small write cache that reduces the number of flash block erase operations, can lead to substantial performance gain for write-intensive applications and can extend the overall lifetime of flash based SSDs. This paper presents a novel write caching algorithm, the Large Block CLOCK (LB-CLOCK) algorithm, which considers ‘recency ’ and ‘block space utilization ’ metrics to make cache management decisions. LB-CLOCK dynamically varies the priority between these two metrics to adapt to changes in workload characteristics. Our simulation based experimental results show that LB-CLOCK outperforms the best known existing flash caching algorithms {{for a wide range of}} workloads. I...|$|E
40|$|Con@uring {{redundant}} disk arrays {{is a black}} art. To configure {{an array}} properly, a system administrator must understand the details of both the array and the workload it will support. Incorrect understanding of either, or changes in the workload over time, can lead to poor performance. We present {{a solution to this}} problem: a two-level storage hierarchy implemented inside a single disk-array controller. In the upper level of this hierarchy, two copies of active data are stored to provide full redundancy and excellent performance. In the lower level, RAID 5 parity protection is used to provide excellent storage cost for inactive data, at somewhat lower performance. The technology we describe in this article, known as HP AutoRAID, automatically and transparently manages migration of data blocks between these two levels as access patterns change. The result is a fully redundant storage system that is extremely easy to use, is suitable {{for a wide variety of}} workloads, is largely insensitive to dynamic workload changes, and performs much better than disk arrays with comparable numbers of spindles and much larger amounts of front-end <b>RAM</b> <b>cache,</b> Because the implementation of the HP AutoRAID technology is almost entirely in software, the additional hardware cost for these benefits is very small. We describe the HP AutoRAID technology in detail, provide performance data for an embodiment of it in a storage array, and summarize the results of simulation studies used to choose algorithms implemented in the array...|$|E
40|$|Project Specification High-performance {{computing}} (HPC) contributes {{a significant}} and growing share of resource to high-energy physics (HEP). Individual supercomputers such as Edison or Titan in the U. S. or SuperMUC in Europe deliver a raw performance of the same order of magnitude than the Worldwide LHC Computing Grid. As we have seen with codes from ALICE and ATLAS, it is notoriously difficult to deploy high-energy physics applications on supercomputers, even though they often run a standard Linux on Intel x 86 _ 64 CPUs. The three main problems are: 1. Limited or no Internet access; 2. The lack of privileged local system rights; 3. The concept of cluster submission or whole-node submission of jobs in contrast to single CPU slot submission in HEP. Generally, the delivery of applications to hardware resources in high-energy physics is done by CernVM-FS [1]. CernVM-FS is optimized for high-throughput resources. Nevertheless, some successful results on HPC resources where achieved using the Parrot system[2] that allows to use CernVM-FS without special privileges. Building on these results, the project aims to prototype a toolkit for application delivery that seamlessly integrates with HEP experiments job submission systems, for instance with ALICE AliEn or ATLAS PanDA. The task includes a performance study of the parrot-induced overhead which {{will be used to}} guide performance tuning for both CernVM-FS and Parrot on typical supercomputers. The project should further deliver a lightweight scheduling shim that translates HEP’s job slot allocation to a whole node or cluster-based allocation. Finally, in order to increase the turn-around of the evaluation of new supercomputers, a set of "canary jobs" should be collected that validate HEP codes on new resources. [1] [URL] [2] [URL] Abstract On high performance computing (HPC) resources, users have less control over worker nodes than in the grid. Using HPC resources for high energy physics applications becomes more complicated because individual nodes often don't have Internet connectivity or a filesystem configured to use as a local cache. The current solution in CVMFS preloads the cache from a gateway node onto the shared cluster file system. This approach works but does not scale well into large production environments. In this project, we develop an in memory cache for CVMFS, and assess approaches to running jobs without special privilege on the worker nodes. We propose using Parrot and CVMFS with <b>RAM</b> <b>cache</b> as a viable approach to HEP application delivery on HPC resources...|$|E
50|$|Early cache designs focused {{entirely on}} the direct cost of <b>cache</b> and <b>RAM</b> and average {{execution}} speed.More recent cache designs also consider energy efficiency, fault tolerance, and other goals. Researchers have also explored use of emerging memory technologies such as eDRAM (embedded DRAM) and NVRAM (non-volatile <b>RAM)</b> for designing <b>caches.</b>|$|R
40|$|In {{order to}} know how {{different}} conditions influence {{the behavior of the}} <b>RAM</b> Enhanced Disk <b>Cache</b> Project (REDCAP), we have analyzed the impact of the file system and the REDCAP cache size. The results show that, for workloads which exhibit some spatial locality, the application time can be reduced by more than 80...|$|R
40|$|Today’s {{multicore}} processors already integrate multiple cores on a die. Many-core architectures enable {{far more}} small cores for throughput computing. The key challenge in many-core architectures is the memory bandwidth wall: 1 - 3 the required memory bandwidth {{to keep all}} cores running smoothly is a significant challenge. In the past, researchers have proposed large dynamic <b>RAM</b> (DRAM) <b>caches</b> to address this challenge. 4 - 7 Such solutions fall into two main categories: DRAM caches with small allocation granularity (64 -byte cache lines) and DRAM caches with large allocation granularity (page sizes such as 4 -Kbyt...|$|R
