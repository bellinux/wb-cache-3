209|135|Public
2500|$|Artificial neural {{networks}} (ANNs), {{a form of}} connectionism, are computing systems inspired by the biological {{neural networks}} that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as [...] "cat" [...] or [...] "no cat" [...] and using the analytic results to identify cats in other images. They have found most use in applications difficult to express in a traditional computer algorithm using <b>rule-based</b> <b>programming.</b>|$|E
50|$|Pattern-directed {{invocation}} {{is related}} to <b>rule-based</b> <b>programming.</b>|$|E
50|$|Flex is {{a hybrid}} expert system toolkit {{developed}} by LPA which incorporates frame-based reasoning with inheritance, <b>rule-based</b> <b>programming</b> and data-driven procedures.|$|E
40|$|AbstractThe term <b>rule-based</b> <b>program</b> {{is meant}} to include {{definite}} clause programs, SOS specifications, attribute grammars, and conditional rewrite systems. These setups are widely used for the executable specification or implementation of language-based tools, e. g., interpreters, translators, type checkers, program analysers, and program transformations. We provide a pragmatic, transformation-based approach for expressing and tracking changes in <b>rule-based</b> <b>programs</b> {{in the course of}} program evolution. To this end, we design an operator suite for the transformation of <b>rule-based</b> <b>programs.</b> The operators facilitate steps for clean-up, refactoring, and enhancement. We use SOS-based interpreter examples to illustrate evolution of <b>rule-based</b> <b>programs.</b> We use logic programming to execute the examples, while the relevant evolution operators are made available as logic meta-programs...|$|R
40|$|Two systems {{concepts}} are introduced: bounded response-time and self-stabilization {{in the context}} of <b>rule-based</b> <b>programs.</b> These {{concepts are}} essential for the design of <b>rule-based</b> <b>programs</b> which must be highly fault tolerant and perform in a real time environment. The mechanical analysis of programs for these two properties is discussed. The techniques are used to analyze a NASA application...|$|R
40|$|The term <b>rule-based</b> <b>program</b> {{is meant}} to include {{definite}} clause programs, SOS specifications, attribute grammars, and conditional rewrite systems. These setups are widely used for the executable specification or implementation of language-based tools, e. g., interpreters, translators, type checkers, program analysers, and program transformations. We provide a pragmatic, transformation-based approach for expressing and tracking changes in <b>rule-based</b> <b>programs</b> {{in the course of}} program evolution. To this end, we design an operator suite for the transformation of <b>rule-based</b> <b>programs.</b> The operators facilitate steps for clean-up, refactoring, and enhancement. We use SOS-based interpreter examples to illustrate evolution of rule-based pro-grams. We use logic programming to execute the examples, while the relevant evolution operators are made available as logic meta-programs. © 2004 Elsevier Inc. All rights reserved...|$|R
50|$|The Enterprise edition also {{includes}} KnowledgeWorks, which supports <b>rule-based</b> <b>programming</b> (including support for Prolog); the CommonSQL database interface; and a CORBA binding.|$|E
5000|$|<b>Rule-based</b> <b>programming</b> - {{a network}} of rules of thumb that {{comprise}} a knowledge base {{and can be used}} for expert systems and problem deduction & resolution ...|$|E
50|$|<b>Rule-based</b> <b>programming</b> {{attempts}} to derive execution instructions from a starting {{set of data}} and rules. This is a more indirect method than that employed by an imperative programming language, which lists execution steps sequentially.|$|E
40|$|We {{revisit the}} main {{techniques}} of program transformation {{which are used}} in partial evaluation, mixed computation, supercompilation, generalized partial computation, <b>rule-based</b> <b>program</b> derivation, program specialization, compiling control, and the like. We present a methodology which underlines these techniques as a `common pattern of reasoning' and explains the various correspondences which can be established among them. This methodology consists of three steps: i) symbolic computation, ii) search for regularities, and iii) program extraction. We also discuss some control issues which occur when performing these steps. 1 Introduction During the past years researchers working in various areas of program transformation, such as partial evaluation, mixed computation, supercompilation, generalized partial computation, <b>rule-based</b> <b>program</b> derivation, program specialization, and compiling control, have been using very similar techniques for the development and derivation of programs. Unfor [...] ...|$|R
40|$|AbstractProgram {{transformation}} is the mechanical manipulation {{of a program}} {{in order to improve}} it relative to some cost function and is understood broadly as the domain of computation where programs are the data. The natural basic building blocks of the domain of program transformation are transformation rules expressing a ‘one-step’ transformation on a fragment of a program. The ultimate perspective of research in this area is a high-level, language parametric, <b>rule-based</b> <b>program</b> transformation system, which supports a wide range of transformations, admitting efficient implementations that scale to large programs. This situation has not yet been reached, as trade-offs between different goals need to be made. This survey gives an overview of issues in <b>rule-based</b> <b>program</b> transformation systems, focusing on the expressivity of <b>rule-based</b> <b>program</b> transformation systems and in particular on transformation strategies available in various approaches. The survey covers term rewriting, extensions of basic term rewriting, tree parsing strategies, systems with programmable strategies, traversal strategies, and context-sensitive rules...|$|R
40|$|A key {{index of}} the {{performance}} of a <b>rule-based</b> <b>program</b> used in real-time monitoring and control is its response time, defined by the maximum number of rule firings before a fixed point of the program is reached from a start state. Previous work in computing the response-time bounds for <b>rule-based</b> <b>programs</b> assumes that if two rules are enabled, then either one of them may be scheduled for firing. This assumption may be too conservative in the case programmers choose to impose a priority structure on the set of rules. In this paper, we discuss how to get tighter bounds by taking rule-priority information into account. We show that the rule-suppression relation we previously introduced can be extended to incorporate rule-priority information. A bound-derivation algorithm for programs whose potential-trigger relations satisfy an acyclicity condition is presented, followed by its correctness proof and an analysis example. 1 Introduction In recent years, there has been increasing interest i [...] ...|$|R
50|$|Kinetic Rule Language (KRL) is a <b>rule-based</b> <b>programming</b> {{language}} for creating applications on the Live Web. KRL programs, or rulesets, comprise {{a number of}} rules that respond to particular events. KRL has been promoted as {{language for}} building personal clouds.|$|E
50|$|The Wolfram Language, {{a general}} multi-paradigm {{programming}} language developed by Wolfram Research, is the programming language of mathematical symbolic computation program Mathematica and the Wolfram Programming Cloud. It emphasizes symbolic computation, functional programming, and <b>rule-based</b> <b>programming</b> and can employ arbitrary structures and data.|$|E
50|$|In 1989, LPA {{developed}} the Flex expert system toolkit, which incorporated frame-based reasoning with inheritance, <b>rule-based</b> <b>programming</b> and data-driven procedures. Flex {{has its own}} English-like Knowledge Specification Language (KSL) which means that knowledge and rules are defined in an easy-to-read and understand way.|$|E
40|$|The {{response}} {{time of a}} <b>rule-based</b> <b>program</b> {{is defined as the}} maximum number of rule firings before a fixed point of the program is reached from a start state. In this paper, we present several principles which make use of two relations, potential-trigger and suppression, for deriving tight response-time bounds. While the computation of these two relations is costly in general, we show how they can be efficiently approximated by refining the necessary/sufficient conditions for these relations to be satisfied. A response-time analyzer based on the theories in this paper has been implemented to analyze programs whose potential-trigger relations are acyclic. We demonstrate the analysis process with an example program which has infinite state space. Our analyzer takes only seconds to derive a tight bound on the example program 's {{response time}}. 1 Introduction In recent years, there has been increasing interest in the use of <b>rule-based</b> <b>programs</b> in time-critical applications. In these applic [...] ...|$|R
40|$|To the {{software}} design community, the {{concern over the}} costs associated with a program's execution time and implementation is great. It is always desirable, and sometimes imperative, that the proper programming technique is chosen which minimizes all costs for a given application or type of application. This paper describes a study that compared the cost-related factors associ ated wi th traditional programming techniques to rul e-based programming techniques for a specific application. The results of this study favored the traditional approach regarding execution efficiency, but favored the rulebased approach regarding programmer productivity (implementation ease). Execution efficiency was measured by the number of steps required to isolate hypotheses. A step was defined to be a condition test, function call, or a fetch for information from another body of code. The separate homogeneous rule-base and inference mechanisms of the <b>rule-based</b> <b>program</b> required more steps in the isolation of hypotheses. The best case for the <b>rule-based</b> <b>program</b> was approximately four times less efficient than the tradi ti onal program. The results for programmer productivity were based on the modification ease, verification ease, and the ease of adding explanation capability to each program. These measures were determined by a qualitative sumnation of the required process for each measure. The separate homogeneous rule-base and inference mechanisms of the <b>rule-based</b> <b>program</b> provided potential for improved programmer productivity. This study was based on a specific application. The application was both complex and frequently modified, and therefore, tested key features of both programming techniques. Although this study examined a specific application, the results should be widely applicable...|$|R
40|$|AbstractWe {{present a}} {{low-level}} specification language used for describing real Internet security protocols. Specifications are automatically {{generated by a}} compiler, from TLA-based high-level descriptions of the protocols. The results are <b>rule-based</b> <b>programs</b> containing all the information needed for either implementing the protocols, or verifying some security properties. This approach has already been applied to several well-known Internet security protocols, and the generated programs have been successfully used for finding some attacks...|$|R
50|$|Jess is a {{rule engine}} for the Java {{platform}} {{that was developed}} by Ernest Friedman-Hill of Sandia National Labs. It is a superset of the CLIPS programming language. It was first written in late 1995. The language provides <b>rule-based</b> <b>programming</b> for the automation of an expert system, and is frequently termed as an expert system shell. In recent years, intelligent agent systems have also developed, which depend on a similar capability.|$|E
50|$|R++ is a <b>rule-based</b> <b>programming</b> {{language}} {{based on}} C++. The United States patent describes R++ as follows:The R++ extension permits rules {{to be defined}} as members of C++ classes. The programming system of the invention takes the classes with rules defined using R++ and generates C++ code from them in which the machinery required for the rules is implemented completely as C++ data members and functions of the classes involved in the rules.|$|E
5000|$|Artificial neural {{networks}} (ANNs) or connectionist systems are computing systems {{inspired by the}} biological {{neural networks}} that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as [...] "cat" [...] or [...] "no cat" [...] and using the analytic results to identify cats in other images. They have found most use in applications difficult to express in a traditional computer algorithm using <b>rule-based</b> <b>programming.</b>|$|E
40|$|Real-time {{expert systems}} are {{embedded}} decision systems which must respond {{to changes in}} the environments within stringent timing constraints. A major problem impeding the use of rule-based expert systems in real-time environments is the difficulty in predicting the response time of these rule-based systems. In this paper, we tackle this problem with a fast, partially parallelizable response time analysis algorithm for a class of EQL <b>rule-based</b> <b>programs</b> with constant assignments in the action parts of the rules...|$|R
40|$|A <b>rule-based</b> <b>program</b> {{consists}} {{of a set of}} if-then rules and a tuple-space. The rules are the code for the program and the tuple-space contains the data being processed by the program. Previous e orts to parallelize <b>rule-based</b> <b>programs</b> have achieved limited speedups. The main reason for these disappointing results is a high frequency of barrier synchronizations. Since little work is done between successive barrier synchronizations, the number of processors that can be e ectively utilized is bounded. Even though required by language semantics, a large fraction of the barrier synchronizations are not necessary for most programs. This paper proposes a pair of simple language extensions that allow an implementation to e ciently detect and eliminate redundant barrier synchronizations. Simulation results based on a real implementation show that for a set of ve benchmarks, this scheme is able to eliminate between 95. 6 % and 99. 9 % of the barrier synchronizations. This results in a multiplicative speedup of between 2. 2 and 52. 3 fold over and above the speedup achieved by a parallelizing compiler. For the programs studied, simulations indicate that speedups up to 115 fold relative to an optimized sequential version are possible. ...|$|R
40|$|Wrangler is a {{refactoring}} and {{code inspection}} tool for Erlang programs. Apart from providing {{a set of}} built-in refactorings and code inspection functionalities, Wrangler allows users to define refactorings, code inspections, and general program transformations for themselves to suit their particular needs. These are defined using a template- and <b>rule-based</b> <b>program</b> transformation and analysis framework built into Wrangler. This paper reports an extension to Wrangler’s extension framework, supporting the automatic generation of API migration refactorings from a user-defined adapter module...|$|R
50|$|The annual International Web Rule Symposium (RuleML) is an {{international}} academic conferences on research, applications, languages and standards for rule technologies. It is a conference {{in the field of}} <b>rule-based</b> <b>programming</b> and rule-based systems including production rules systems, logic programming rule engines, and business rules engines/business rules management systems; Semantic Web rule languages and rule standards (e.g., RuleML, LegalRuleML, Reaction RuleML, SWRL, RIF, Common Logic, PRR, Decision Model and Notation (DMN), SBVR); rule-based event processing languages (EPLs) and technologies; and research on inference rules, constraint handling rules, transformation rules, decision rules, production rules, and ECA rules. RuleML is the leading conference to build bridges between academia and industry in the field of Web rules and its applications, especially as part of the semantic technology stack. RuleML is commonly listed together with other Artificial Intelligence conferences worldwide.|$|E
40|$|Abstract. Recent {{years have}} {{witnessed}} renewed {{developments of the}} <b>rule-based</b> <b>programming</b> style, addressing both its theoretical foundations and its practical implementations. New <b>rule-based</b> <b>programming</b> languages have emerged, and several practical applications have shown that rules are indeed a useful programming tool. We believe that Mathematica has the right basic ingredients for supporting <b>rule-based</b> <b>programming</b> efficiently. Because the main features of a true <b>rule-based</b> <b>programming</b> language are still missing, we developed a Mathematica package, ρLog, which enables advanced <b>rule-based</b> <b>programming</b> within Mathematica. We describe here the capabilities of ρLog and illustrate its usage with several examples. ...|$|E
40|$|ABSTRACT. We {{describe}} {{the foundations of}} a system for <b>rule-based</b> <b>programming</b> which integrates two powerful mechanisms: (1) matching with context variables, sequence variables, and regular constraints for their matching values; and (2) strategic programming with labeled rules. The system is called ρLog, and is built {{on top of the}} pattern matching and <b>rule-based</b> <b>programming</b> capabilities of Mathematica. KEYWORDS: <b>Rule-based</b> <b>programming,</b> declarative programming, matching. 1...|$|E
40|$|In {{previous}} work {{we have developed}} very fast static techniques to determine response times of <b>rule-based</b> <b>programs.</b> This paper shows {{that the problem of}} formal verification of embedded systems can be represented as the problem of determining response time of rule-based systems. We do this by encoding the specification and safety assertion given in RTL as a rule-based EQL or MRL program. This paper goes on to show that our approach requires quadratic time while the equivalent RTL approach requires exponential time in the worst case. 1...|$|R
40|$|Programs {{interacting}} with users via natural language interfaces generally require more sophisticated control structures than those needed by programs {{interacting with}} users through less flexible mechanisms. This paper describes our development of Director, an interpreter (inference engine) for <b>rule-based</b> <b>programs.</b> Providing an efficient combination of {{forward and backward}} chaining, heuristic and user control of inference, and ready access to portions of its internal structure, Director facilitates the construction of systems with natural language interfaces {{as well as other}} rule-based systems in which queries are expensive...|$|R
40|$|In the Linux kernel source tree, header files {{typically}} define {{many small}} functions {{that have a}} simple behavior but are critical to ensure readability, correctness, and maintainability. We have observed, however, that some Linux code does not use these functions systematically. In this paper, we propose an approach combining <b>rule-based</b> <b>program</b> matching and transformation with generative programming to generate rules for finding and fixing code fragments that should use the functions defined in header files. We illustrate our approach using an in-depth study based on four typical functions defined in the header file include/linux/usb. h...|$|R
40|$|Abstract. Recently, {{there has}} been {{increasing}} attention to Component-Based Software Development (CBSD). This development technology enhances productivity and reliability by combining existing verified components to construct a target software. This development technology, however, lacks methods to extract and preserve components; methods to retrieve query-satisfied components; and technologies and theories that guarantee the correctness of each component and the entire program. Considering that <b>rule-based</b> <b>programming</b> with theoretical framework provides good, theoretical supports to component-based programming and may solve these problems, this paper uses equivalent transformation (ET) programming that has advantages {{in the number of}} rules, granularity of rules, and guaranteeofthecorrectnessineachruleandtheentireprogramovertheconventional <b>rule-based</b> <b>programming</b> and discusses how to accumulate and use ET rules to, by employing ET rules as components, create a correct system from a system specification. ET programming is expected to overcome the problems the conventional CBSD and <b>rule-based</b> <b>programming</b> have...|$|E
40|$|This volume {{contains}} {{the proceedings of}} the 5 th International Workshop on <b>Rule-Based</b> <b>Programming</b> (RULE 2004), part of the Federated Conference on Rewriting, Deduction and Programming (RDP 2004), held during May 31 – June 5, 2004, in Aachen, Germany. <b>Rule-based</b> <b>programming</b> is currently experiencing a renewed period of growth with the emergence of new concepts and systems that allow a better understanding and better usability. On the theoretical side, after the in-depth study of rewriting concepts during the eighties, the nineties saw the emergence of the general concepts of rewriting logic and of the rewriting calculus. On the practical side, new languages and systems such as ASF+SDF, BURG, CHRS, Claire, ELAN, Maude, and Stratego have shown that rules are a useful programming tool. The practical application of <b>rule-based</b> <b>programming</b> prompts research into the algorithmic complexity and optimization of rule-based programs as well as into the expressivity, semantics and implementation of rule-based languages. The purpose of this workshop is to bring together researchers from the various communities working on <b>rule-based</b> <b>programming</b> to foster fertilisation between theory and practice, as well as to favour the growth of this programming paradigm. The previous editions of the RULE workshop were held at Valencia (2003) during th...|$|E
40|$|CLIPS 5. 1 {{provides}} cohesive {{software tool}} for handling {{wide variety of}} knowledge with support for three different programming paradigms: rule-based, object-oriented, and procedural. <b>Rule-based</b> <b>programming</b> provides representation of knowledge by use of heuristics. Object-oriented programming enables modeling of complex systems as modular components. Procedural programming enables CLIPS to represent knowledge in ways similar to those allowed in such languages as C, Pascal, Ada, and LISP. Working with CLIPS 5. 1, one can develop expert-system software by use of <b>rule-based</b> <b>programming</b> only, object-oriented programming only, procedural programming only, or combinations of the three...|$|E
40|$|RWTH Aachen Technical Report AIB- 2004 - 04. Colloque avec actes et comité de lecture. Internationale. International audienceWe {{present a}} {{low-level}} specification language used for describing real Internet security protocols. Specifications are automatically {{generated by a}} compiler, from TLA-based high-level descriptions of the protocols. The results are <b>rule-based</b> <b>programs</b> containing all the information needed for either implementing the protocols, or verifying some security properties. This approach has already been applied to several well-known Internet security protocols, and the generated programs have been successfully used for finding some attacks...|$|R
40|$|A <b>rule-based</b> <b>program</b> for {{localization}} {{of damage}} to {{the central nervous system}} was developed to evaluate MYCIN-like production system methodology. The program uses the results of the neurological examination of unconscious patients to categorize them in a manner familiar to clinicians. A collection of rules was found to be a poor representation for neurological localization knowledge because such information is conceptually organized in a frame-like fashion and is very context-dependent. Rule understandability was improved through the use of “macropredicates” and by the development of a natural inference hierarchy. The role of production systems as a model of human cognition is discussed...|$|R
40|$|Gogol is a <b>rule-based</b> {{computer}} Go <b>program.</b> It uses a lot {{of reliable}} tactical rules. Tactical rules are rules about simple goals such as connecting and making an eye. Gogol uses a simplified game theory to represent the degree of achievement of the goals. The automatic acquisition of tactical rules follows a three step process : Pattern generation, Game evaluation, Generalisation. Gogol has metainformation about the correct use of the rules in global Go positions. Key words : Game of Go, Machine Learning, Metaknowledge,Knowledge Acquisition, Game Theory. 1 Introduction Gogol is a computer Go program which uses symbolic rule-based techniques. It uses thousands of rules to know if some tactical goals can be achieved. This paper describes these tactical rules and how they have been automatically acquired. It should interest every Go programmer using a <b>rule-based</b> <b>program.</b> It provides them with a complete and reliable set of tactical rules. Most of the algorithm developed to learn in Go a [...] ...|$|R
