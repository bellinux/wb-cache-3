60|319|Public
25|$|Ecological risk {{assessment}} aims to {{source of contamination}} to exposure to a toxicity endpoint. This requires a <b>risk</b> <b>assessor</b> to identify and estimate exposure pathways. Tissue residue is the only approach that inherently accounts for toxicity due to multiple exposure pathways. There is also a need in {{risk assessment}} to understand the bioaccumulation of chemicals4, {{as well as a}} direct estimation of bioavailability. Modeling food web exposure is difficult in risk assessment and requires many assumptions but this uncertainty can be reduced through tissue residue. Tissue residue may also allow provide a link between ecological risk assessment and human health risk assessment.|$|E
5000|$|Jason Gaverick Matheny, academic, <b>risk</b> <b>assessor</b> {{and co-founder}} of New Harvest ...|$|E
5000|$|From early 1998-1999 Peschken, as {{independent}} consultant, executive producer and <b>risk</b> <b>assessor,</b> {{was involved in}} several transaction-proposals under so called [...] "insurance backed loan structures, involving Paramount Pictures, Universal Studios, Chase Securities, Royal-Sun Alliance Insurance, for the co-production / co-financing of slates of major motion pictures and their P&A costs. Peschken was lead negotiator, counseled by major LA attorney firms. He negotiated directly on the VP Business & Legal affairs level with studios such as Paramount Pictures and Universal Studios and MGM.|$|E
50|$|The Institution also {{manages a}} Register of Fire <b>Risk</b> <b>Assessors</b> and Auditors.|$|R
50|$|Decision-making {{processes}} are not incorporated into routine risk assessments; however, {{they play a}} critical role in such processes. It is therefore very important for <b>risk</b> <b>assessors</b> to minimize confirmation bias by carrying out their analysis and publishing their results with minimal involvement of external factors such as politics, media, and advocates. In reality, however, it is nearly impossible to break the iron triangle among politicians, scientists (in this case, <b>risk</b> <b>assessors),</b> and advocates and media. <b>Risk</b> <b>assessors</b> need to be sensitive to the difference between risk studies and risk perceptions. One way to bring the two closer is to provide decision-makers with data they can easily rely on and understand. Employing networks in the risk analysis process can visualize causal relationships and identify heavily-weighted or important contributors to the probability of the critical event.|$|R
40|$|When random {{variables}} {{are used to}} represent variability, the risk equation has mathematical properties poorly understood by many <b>risk</b> <b>assessors.</b> Variability represents the heterogeneity in a well-characterized population, usually not reducible through further measurement or study. We {{follow the lead of}} most mathematicians in using {{random variables}} to represent and analyze variability. To illustrate the issues, we use LogNormal distributions to model variability. When estimating the incremental lifetime cancer risk, R, from an environmental exposure to a single carcinogenic chemical via a single exposure pathway, <b>risk</b> <b>assessors</b> often use equations of this fundamental form...|$|R
50|$|Ecological risk {{assessment}} aims to {{source of contamination}} to exposure to a toxicity endpoint. This requires a <b>risk</b> <b>assessor</b> to identify and estimate exposure pathways. Tissue residue is the only approach that inherently accounts for toxicity due to multiple exposure pathways. There is also a need in {{risk assessment}} to understand the bioaccumulation of chemicals4, {{as well as a}} direct estimation of bioavailability. Modeling food web exposure is difficult in risk assessment and requires many assumptions but this uncertainty can be reduced through tissue residue. Tissue residue may also allow provide a link between ecological risk assessment and human health risk assessment.|$|E
5000|$|Canadian {{academic}} and environmental activist David Suzuki stated, [...] "Trump just {{passed on the}} best deal the planet has ever seen". Navroz Dubash of the Centre for Policy Research in New Delhi expressed bafflement at Trump's move, citing the declining costs of renewable energy sources and the increasing difficulty of obtaining investment for fossil-fuel projects. Environmental scientist and <b>risk</b> <b>assessor</b> Dana Nuccitelli stated that it [...] "now seems inevitable that the history books will view Trump as America’s worst-ever president". Bob Ward of the Grantham Research Institute also described Trump's speech as [...] "confused nonsense". Stephen Hawking criticized Trump, saying that he [...] "will cause avoidable environmental damage to our beautiful planet, endangering the natural world, for us and our children." ...|$|E
50|$|The U.S. {{government}} and {{many states have}} regulations regarding lead-based paint. Many of them apply to evaluating a property for lead-based paint. There are two different testing procedures that are similar but yield different information. Lead-based paint inspections will evaluate all painted surfaces in a complex to determine where lead-based paint, if any, is present. The procedures for lead inspections is outlined in the United States Department of Housing and Urban Development (HUD) Guidelines, Chapter 7, 1997 Revision. The other testing is a lead-based paint risk assessment. In this testing, only deteriorated painted surfaces are tested and dust wipe samples are collected. This information will help the <b>risk</b> <b>assessor</b> determine {{if there are any}} lead hazards. Many property owners decided to get a combination of both tests to determine where the lead-based paint is present and what hazards are present as well. Risk assessments are outlined in the HUD Guidelines, Chapter 5. In addition, if a child is poisoned in a property, the owner may be required to perform abatement (permanent elimination of the lead hazard).|$|E
40|$|Health risk {{assessments}} and ecological {{risk assessments}} of contaminated environments are typically conducted by different groups working largely independendy. This results in inefficiencies and, more importantly, in assessments {{that are less}} defensible than would otherwise be possible. I would like to argue for integration of health and ecological risk assessment by presenting {{some of the issues}} that have arisen at their interface. Although most health <b>risk</b> <b>assessors</b> do not perceive ecological risks to be relevant to their analyses, the public often makes the connection. As a result, health <b>risk</b> <b>assessors</b> may be caught off guard in public meetings when the public asks how health risks can be described as insignificant when the fish have tumors and lesions or fish-eating birds are estimated to have significant reproductive decrements. Interactions with the ecological <b>risk</b> <b>assessors</b> would prepare them for this response. In many cases, there is no actual contradiction because, for a variety of reasons, nonhuman organisms are much more exposed or more sensitiv...|$|R
5000|$|Risk {{communication}} is defined {{for the purposes}} of the Codex Alimentarius Commission as [...] "The interactive exchange of information and opinions throughout the risk analysis process concerning hazards and risks, risk-related factors and risk perceptions, among <b>risk</b> <b>assessors,</b> <b>risk</b> managers, consumers, industry, the academic community and other interested parties, including the explanation of risk assessment findings and the basis of risk management decisions." ...|$|R
30|$|The {{mission of}} the NORMAN network is to enhance the {{exchange}} of information and collection of data on emerging environmental substances and to encourage validation and harmonisation of measurement methods and monitoring tools so that the demands of <b>risk</b> <b>assessors</b> can be better met.|$|R
5000|$|Event tree {{analysis}} (ETA) is a forward, bottom up, logical modeling {{technique for}} both {{success and failure}} that explores responses through a single initiating event and lays a path for assessing probabilities of the outcomes and overall system analysis. [...] This analysis technique is {{used to analyze the}} effects of functioning or failed systems given that an event has occurred. ETA is a powerful tool that will identify all consequences of a system that have a probability of occurring after an initiating event that can be applied {{to a wide range of}} systems including: nuclear power plants, spacecraft, and chemical plants. This Technique may be applied to a system early in the design process to identify potential issues that may arise rather than correcting the issues after they occur. With this forward logic process use of ETA as a tool in risk assessment can help to prevent negative outcomes from occurring by providing a <b>risk</b> <b>assessor</b> with the probability of occurrence. ETA uses a type of modeling technique called event tree, which branches events from one single event using Boolean logic.|$|E
30|$|Some of the issues, {{such as the}} {{assessment}} of toxicity, immunogenic effects and {{the assessment}} of residues from complementary herbicides, are under the remit of the <b>risk</b> <b>assessor,</b> i.e. the European Food Safety Authority (EFSA). And some issues will require closer collaboration between the risk manager, i.e. the EU Commission and EFSA, including requesting industry to deliver protocols for measuring the Bt concentration in the plants and to close the gaps between pesticide regulation and GMO regulation.|$|E
30|$|All four methods {{require some}} degree of expert judgment. They are {{developed}} to help risk assessors evaluate data, not to replace the <b>risk</b> <b>assessor.</b> Therefore {{it is likely that}} two experts evaluating the same study end up with slightly different results depending on their expertise and previous experiences. We have in our evaluation strived to make a uniform treatment of the evaluation methods and the selected studies. The evaluation method described by Klimisch et al. [10] requires more expert judgment when the evaluation is merged since instructions for this is lacking.|$|E
30|$|Using the Klimisch method, some <b>risk</b> <b>assessors</b> {{remarked that}} {{information}} on test substance purity and solubility {{as well as}} raw data in general was missing, yet none of them categorized it as “not reliable” or “not assignable.” In contrast, participants using the CRED evaluation method discovered flaws in the study design related to dosing and potential loss of the test substance. In addition, it was frequently noted that replication and control data provided were insufficient, e.g., due to missing solvent control data. Another issue raised with study E was the uneven number of fish used per treatment group. As for study D, {{these results suggest that}} the CRED evaluation method helped <b>risk</b> <b>assessors</b> to detect flaws in study design and reporting.|$|R
30|$|We {{conclude}} that the current practice of ERA does not comprehensively fulfil the scientific and legal requirements of Directive 2001 / 18 /EC, and we propose improvements and needs for further guidance and development of standards. The recommendations address likewise applicants, <b>risk</b> <b>assessors</b> as well as decision makers.|$|R
40|$|Risk {{assessments}} {{serve as}} the foundation of policy decisions on whether to take measures to reduce a risk or not. However, different <b>risk</b> <b>assessors</b> frequently come to divergent estimates of the magnitude and even the nature of risks. Few attempts has been made in the past to describe and understand the reasons for these differences. This thesis reports the results from a detailed comparison of 30 different cancer risk assessments made of one and the same chemical substance, namely the chlorinated solvent trichloroethylene (CAS no. 79 - 01 - 6). The {{purpose of the present study}} is to discuss (1) why <b>risk</b> <b>assessors</b> come to different conclusions, (2) how scientific data are used in risk assessment, and (3) how scientific uncertainty is handled in the process. The overall objective is to contribute to increase the transparency and reliability of risk assessments so that they better serve the needs of risk managers and the public. In the first part of this study the different conclusions drawn in these risk assessment documents are identified and described. This is made within the framework of a proposed cancer risk assessment index (CRAI). The CRAI categorization shows that these <b>risk</b> <b>assessors</b> come to divergent conclusions about the trichloroethylene potential to cause cancer. To enable an analysis of the reasons for these differences, detailed information from the trichloroethylene risk assessment documents was stored in a database. This information made it possible to compare the risk assessment documents in terms of data availability (a time dependent factor), data selection, data interpretation, data quality evaluation, and (animal to human) extrapolation of data, and to analyse how these parameters influenced the overall conclusions. The analysis of these data indicates that the differences in conclusions cannot exclusively be explained by an evolving database (data availability). The data sets utilized by the trichloroethylene <b>risk</b> <b>assessors</b> are surprisingly diverse and incomplete, and biased data selection may have influenced some of the <b>risk</b> <b>assessors</b> conclusions. The TCE <b>risk</b> <b>assessors</b> often interpret and evaluate scientific data in different ways. These differences are considered to be within the scope of the scientifically acceptable. In the second part of this case study the European Union regulatory process for classification and labeling served as study object of the risk assessment process in a setting where <b>risk</b> <b>assessors</b> from different affiliations evaluated exactly the same data. This part of the study indicates that there is a scope of possible interpretations of the primary data in relation to the classification criteria and thus that there may be more than one possible alternative for classification of individual substances. The main controversies in this process are also identifed and they are found to concern issues that include policy considerations and thus are not readily resolved by ftirther research. It is concluded that the uncertainty inherent in scientific data opens up a scope of possible interpretations and conclusions and that differences in the assessment and handling of this scientific uncertainty have the potential to influence the overall assessment of risk. It is furthermore concluded that even if an enormous amount of resources were spent on testing and assessment of individual substances (orders of magnitude more than what is required according to existing and proposed regulations), significant uncertainty about their potential to cause harm may still remain...|$|R
40|$|This paper {{presents}} an evolved rule-based tool for mammography interpretation denominated "COBRA: Catalonia online breastcancer <b>risk</b> <b>assessor.</b> " COBRA {{is designed to}} aid radiologists {{in the interpretation of}} mammography to decide whether to perform a biopsy on a patient or not while providing a human-friendly explanation of the underlying reasoning. From a diagnostic point of view, the tool exhibits high performance measures (i. e., sensitivity, specificity, and positive predictive value). From an interpretability point of view, COBRA's behavior is explained by only 14 rules containing, in average, 2. 73 conditions perrule...|$|E
40|$|Malicious {{insiders}} ’ difficult-to-detect activities pose serious {{threats to}} the intelligence community (IC) when these activities go undetected. A novel approach that integrates the results of social network analysis, role-based access monitoring, and semantic analysis of insiders ’ communications as evidence for evaluation by a <b>risk</b> <b>assessor</b> is being tested on an IC simulation. A semantic analysis, by our proven Natural Language Processing (NLP) system, of the insider’s text-based communications produces conceptual representations that are clustered and compared on the expected vs. observed scope. The determined risk level produces an input to a risk analysis algorithm that is merged with outputs from the system’s social network analysis and role-based monitoring modules...|$|E
40|$|Risk {{assessments}} are important {{components of the}} decision making process. At hazardous waste sites, they are used as tools to determine appropriate cleanup levels. Therefore, {{it is critical that}} the best up-to-date methods, models, and exposure data are available to the exposure and <b>risk</b> <b>assessor</b> to realistically estimate the potential for human and ecological exposures to environmental contaminants. The EPA Exposure Factors Handbook published in 1997 is a tool available to exposure assessors which summarizes statistical data on exposure factors necessary to conduct human health exposure assessments. Since it was first published by EPA in 1989, the handbook has been the primary source of data for human exposure assessments. The {{purpose of this paper is}} to provide an overview of the handbook, its impact, applications, discussion about data gaps, and future directions...|$|E
30|$|A {{total of}} 121 study {{evaluations}} were performed by 62 <b>risk</b> <b>assessors</b> in phase I (Klimisch evaluation method), and 104 study evaluations by 54 participants in phase II (CRED evaluation method). Five participants (three in phase I {{and two in}} phase II) submitted {{only one of the}} two requested questionnaires.|$|R
40|$|This is {{a comment}} on Lofstedt and Bouder’s paper, which explores the {{prospects}} of evidence based uncertainty analysis in Europe, focusing on the ongoing development on uncertainty analysis at the European Food Safety Authority (EFSA). We very much welcome a discussion {{on the need to}} develop better treatment and communication of uncertainty in risk analysis, as we believe that such discussion is long overdue. Lofstedt and Bouder raise many relevant points, in particular the call for evidence based uncertainty analysis. However, there is need to distinguish different types of communication in the discussion and facilitate – not diminish – the description and communication of uncertainty between <b>risk</b> <b>assessors</b> and decision-makers. We find that EFSA has taken steps toward a novel approach to guide their scientific experts and <b>risk</b> <b>assessors</b> in uncertainty analysis based on a modern and scientific view on uncertainty...|$|R
50|$|Cincinnati Health Department (CHD) is the Health Department for the City of Cincinnati {{with health}} centers, lab services, {{communicable}} disease experts, environmental services {{and other public}} health programs. The Department has more than 300 physicians, nurses, dentists and dental workers, laboratory technicians, pharmacists, dietitians, lead experts, sanitarians, litter control experts, pest control operators and licensed <b>risk</b> <b>assessors.</b>|$|R
40|$|Based {{on results}} {{reported}} from the NHANES II Survey (the National Health and Nutrition Examination Survey II) for {{people living in}} the United States during 1976 - 1980, we use exploratory data analysis, probability plots, and the method of maximum likelihood to fit Lognormal distributions to percentiles of body weight for males and females as a function of age from 6 months through 74 years. The results are immediately useful in probabilistic (and deterministic) risk assessments. To conduct a probabilistic risk assessment that includes people of different ages in an exposed population, a <b>risk</b> <b>assessor</b> needs distributions for the body weights of children, teens, and adults as a function of age. While the US EPA has published a variety of summary statistics for body weights (e. g., US EPA, 1989, EFH; US EPA, 1995, EFH 2) ...|$|E
40|$|Cloud service {{providers}} offer {{access to their}} resources through formal service level agreements (SLA), and need well-balanced infrastructures {{so that they can}} maximise the quality of service (QoS) they offer and minimise the number of SLA violations. This paper focuses on a specific aspect of risk assessment as applied in cloud computing: methods within a framework that can be used by cloud {{service providers}} and service consumers to assess risk during service deployment and operation. It describes the various stages in the service lifecycle whereas risk assessment takes place, and the corresponding risk models that have been designed and implemented. The impact of risk on architectural components, with special emphasis on holistic management support at service operation, is also described. The <b>risk</b> <b>assessor</b> is shown to be effective through the experimental evaluation of the implementation, and is already integrated in a cloud computing toolkit. Peer ReviewedPostprint (author's final draft...|$|E
40|$|In {{this paper}} the {{adjustments}} to the economic impact assessment section of the EPPO Decision-support scheme for pest risk analysis undertaken by the PRATIQUE EU project are described. The improvements aim to improve accuracy, consistency and transparency and include the following aspects. The wording of the questions and notes has been adjusted to enhance clarity and provide additional assistance, especially for assessing indirect impacts. Furthermore a new question has been added {{about the impact of}} the pest when all current crop protection measures are applied, and some questions have been merged. Indicators, used to measure impacts, have been added to the notes. Guidance has also been given to assist the <b>risk</b> <b>assessor</b> when selecting one of five risk ratings in order to help ensure consistency between risk assessors. Finally, a matrix model has been developed, primarily to assist with summarizing results. Wherever possible, examples have been provided...|$|E
40|$|As an {{essential}} component of the risk assessment process, human exposure assessment requires the assembly of many types of data to develop a set of exposure values. <b>Risk</b> <b>assessors</b> require data such as daily exposure estimates, dermal absorption rates and length of the work activity period to estimate daily and lifetime doses. Often these data are unavailable for evaluatin...|$|R
30|$|The {{regulatory}} {{evaluation of}} ecotoxicity studies for environmental risk and/or hazard assessment of chemicals is often performed {{using the method}} established by Klimisch and colleagues in 1997. The method was, at that time, an important step toward improved evaluation of study reliability, but lately it {{has been criticized for}} lack of detail and guidance, and for not ensuring sufficient consistency among <b>risk</b> <b>assessors.</b>|$|R
50|$|Tools {{that are}} {{currently}} employed in risk assessment are often sufficient, but model complexity and limitations of computational power can tether <b>risk</b> <b>assessors</b> to involve more causal connections and account for more Black Swan event outcomes. By applying network theory tools to risk assessment, computational limitations may be overcome and result in broader coverage of events with a narrowed range of uncertainties.|$|R
40|$|This report {{presents}} {{research results}} {{obtained in the}} framework of a project on the Applicability of Quantitative Structure-Activity Relationship (QSAR) analysis in the evaluation of the toxicological relevance of metabolites and degradates of pesticide active substances. During this project, which was funded by the European Food Safety Authority (EFSA), the Joint Research Centre (JRC) performed several investigations to evaluate the comparative performance of selected software tools for genotoxicity and carcinogenicity prediction, and to develop a number of case studies to illustrate the opportunities and difficulties arising in the computational assessment of pesticides. This exercise also included an investigation of the chemical space of several pesticides datasets. The results indicate that different software tools have different advantages and disadvantages, depending on the specific requirements of the user / <b>risk</b> <b>assessor.</b> It is concluded that further work is needed to develop acceptance criteria for specific regulatory applications (e. g. evaluation of pesticide metabolites) and to develop batteries of models fulfilling such criteria. JRC. DG. I. 6 -Systems toxicolog...|$|E
40|$|The {{nasal cavity}} is {{susceptible}} to chemically induced iinjury {{as a result of}} exposure to inhaled irritants. Some responses of the nasal mucosa to inhaled toxicants are species specific. These species-related differences in response may be due to variations in structural, physiologic, and biochemical factors, such as gross nasal cavity structure, distribution of luminal epithelial cell populations along the nasal airway, intranasal airflow patterns, nasal mucociliary apparatus, and nasal xenobiotic metabolism among animal species. This paper reviews the comparative anatomy and irritant-induced pathology of the nasal cavity in laboratory animals. The toxicologist, pathologist, and environmental <b>risk</b> <b>assessor</b> must have a good working knowledge of the similarities and differences in normal nasal structure and response to injury among species before they can select animal models for nasal toxicity studies, recognize toxicantinduced lesions in the nasal airway, and extrapolate experimental results to estimate the possible effects of an inhaled toxicant on the human nasal airway...|$|E
30|$|The above-described double {{standards}} reinforce the longstanding observation {{that in the}} GMO environmental risk assessment field, often only those studies that report adverse effects are subjected to 'extra' scrutiny by the regulatory science community and some scientist circles including {{the authors of the}} three papers discussed here. This practice has also been applied by EFSA and was confirmed in an interview with a former EFSA GMO panel member who stated: 'Of course, studies that describe potential negative environmental effects of GMOs are discussed particularly intensively' [18]. According to Millstone et al. [19], this practice is interpreted by the European public as an illegitimate support for the biotechnology industry, by the <b>risk</b> <b>assessor.</b> They state that 'greater institutional care was taken to try to avoid false positives than to avoid false negatives. That implies that critical scrutiny has been applied in an asymmetrical fashion that prima facie seems difficult to reconcile with a precautionary approach' [19]. The European Commission does indeed claim to follow the precautionary principle in all its regulatory appraisal processes.|$|E
30|$|Multiple {{evaluation}} {{methods are}} available to <b>risk</b> <b>assessors</b> [6, 21 – 27]; however, most focus exclusively on the evaluation of reliability {{with the exception of}} those provided by Ågerstrand et al. [15] and Beronius et al. [24], while the equally important relevance aspects of a study are not considered. A comparison of four methods for reliability evaluation of ecotoxicity studies showed that choice of method affected the outcome [13]. In addition, a review of methods used to evaluate toxicity studies concluded that only five of 30 methods investigated had been rigorously tested by <b>risk</b> <b>assessors</b> in order to evaluate their applicability [28]. Other attempts to score study reliability were developed by industry and other institutions [23]; however, to our knowledge they—like most other methods—were not implemented in regulatory guidance documents. A method that provides increased transparency and consistency in evaluations is therefore needed to reduce uncertainties regarding the assessment of (key) ecotoxicity studies and provide more harmonized hazard and risk assessments.|$|R
40|$|Internationally {{acceptable}} norms need {{to incorporate}} sound science and consistent risk management principles {{in an open}} and transparent manner, {{as set out in}} the Agreement on the Application of Sanitary and Phytosanitary Measures (the SPS Agreement). The process of risk analysis provides a procedure to reach these goals. The interaction between <b>risk</b> <b>assessors</b> and <b>risk</b> managers is considered vital to this procedure. This paper reports the outcome of a meeting of <b>risk</b> <b>assessors</b> and <b>risk</b> managers on specific aspects of risk analysis and its application to international standard setting for food additives and contaminants. Case studies on aflatoxins and aspartame were used to identify the key steps of the interaction process which ensure scientific justification for risk management decisions. A series of recommendations were proposed in order to enhance the scientific transparency in these critical phases of the standard setting procedure. Chemicals/CAS: aflatoxin, 1402 - 68 - 2; aspartame, 22839 - 47 - 0, 5910 - 52 -...|$|R
40|$|Environmental health <b>risk</b> <b>assessors</b> are {{challenged}} {{to understand and}} incorporate new data streams as the field of toxicology continues to adopt new molecular and systems biology technologies. Systematic screening reviews can help <b>risk</b> <b>assessors</b> and assessment teams determine which studies to consider for inclusion in a human health assessment. A tool for systematic reviews should be standardized and transparent in order to consistently determine which studies meet minimum quality criteria prior to performing in-depth analyses of the data. The Systematic Omics Analysis Review (SOAR) tool is focused on assisting risk assessment support teams in performing systematic reviews of transcriptomic studies. SOAR is a spreadsheet tool of 35 objective questions developed by domain experts, focused on transcriptomic microarray studies, and including four main topics: test system, test substance, experimental design, and microarray data. The tool {{will be used as}} a guide to identify studies that meet basic published quality criteria, such as those defined by the Minimum Information About a Microarra...|$|R
