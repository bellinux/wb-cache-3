1281|1621|Public
5000|$|The <b>reliability</b> <b>coefficient</b> [...] {{provides}} {{an index of}} the relative influence of true and error scores on attained test scores. In its general form, the <b>reliability</b> <b>coefficient</b> {{is defined as the}} ratio of true score variance to the total variance of test scores. Or, equivalently, one minus the ratio of the variation of the error score and the variation of the observed score: ...|$|E
50|$|The DHP-18 has {{demonstrated}} very good measurement properties including <b>reliability</b> <b>coefficient</b> >0.70 {{and the ability}} to discriminate between different treatment and patient groups.|$|E
5000|$|... where [...] is the {{separation}} {{index of the}} set of estimates of , which is analogous to Cronbach's alpha; that is, in terms of Classical test theory, [...] is analogous to a <b>reliability</b> <b>coefficient.</b> Specifically, {{the separation}} index is given as follows: ...|$|E
40|$|Many {{educational}} researchers {{report the}} reliability of their data. They also report tha sampling techniques used in their research. Some of the reports treat the <b>reliability</b> <b>coefficients</b> and sampling variances incorrectly. It is a misuse of the <b>reliability</b> <b>coefficients</b> and sampling variances in educational researc...|$|R
5000|$|The BHS {{moderately}} {{correlates with}} the Beck Depression Inventory, although {{research shows that}} the BDI is better suited for predicting suicidal ideation behavior. The internal <b>reliability</b> <b>coefficients</b> are reasonably high (Pearson r= [...]82 to [...]93 in seven norm groups), but the BHS test-retest <b>reliability</b> <b>coefficients</b> are modest (.69 after one week and [...]66 after six weeks).|$|R
3000|$|After the {{modifications}} {{of the original}} models (Andrade, 2008; Passos & Laros, 2015), both scales presented adequate psychometric quality in this study, as shown by a good model fit and adequate <b>reliability</b> <b>coefficients</b> (λ 2). For the ER 5 FP, with exception of Openness to Experience (. 58), the factors showed adequate <b>reliability</b> <b>coefficients</b> ranging from [...]. 67 (Conscientiousness) to [...]. 79 (Extraversion). For the IGFP- 5 R, the <b>reliability</b> <b>coefficients</b> varied from [...]. 65 (Extraversion) to [...]. 72 (Neuroticism). The factors Extraversion and Neuroticism in both instruments presented the highest <b>reliability</b> <b>coefficients</b> and inter-item correlations. This fact {{may be due to}} the quality and clarity of the definition of the factors in question, which may have allowed greater coverage of the content of each factor (Frazier, Naugle, & Harggety, 2006). According to cross-cultural studies, Neuroticism and Openness to Experience failed to replicate across different languages (De Raad et al., 2010), which might explain the low reliability of Openness to Experience in the ER 5 FP.|$|R
50|$|The Kosmos-3M launch vehicle, {{produced}} {{at the company}} since 1969, has established {{a reputation as one}} of the most reliable rockets in its class with a <b>reliability</b> <b>coefficient</b> of 0.97. Polyot also develops navigation satellites, such as Nadezhda, Parus, GLONASS and GLONASS-M.|$|E
5000|$|... 1) [...] where: [...] is the {{standardized}} observed variable measured with the ith trait and jth method. [...] is the <b>reliability</b> <b>coefficient,</b> which is equal to: [...] is {{the standardized}} true score variable [...] is the random error, which is equal to: [...] Consequently: [...] where: [...] is the reliability ...|$|E
5000|$|The {{internal}} consistency <b>reliability</b> <b>coefficient</b> for core and supplementary subtests demonstrate the KABC-II has good reliability. The median reliability for the 3-6 age band is [...]85 (range [...]69-.92) and [...]87 (range [...]74-.93) for 7-18. Retest reliabilities {{of the global}} scales ranged from 0.72 to 0.94 where retest stability increasing with age.|$|E
40|$|This paper {{provides}} a conceptual, empirical, and practical guide for estimating ordinal <b>reliability</b> <b>coefficients</b> for ordinal item response data (also {{referred to as}} Likert, Likert-type, ordered categorical, or rating scale item responses). Conventionally, <b>reliability</b> <b>coefficients,</b> such as Cronbach's alpha, are calculated using a Pearson correlation matrix. Ordinal <b>reliability</b> <b>coefficients,</b> such as ordinal alpha, use the polychoric correlation matrix (Zumbo, Gadermann, & Zeisser, 2007). This paper presents (i) the theoretical-psychometric rationale for using an ordinal version of coefficient alpha for ordinal data; (ii) a summary of findings from a simulation study indicating that ordinal alpha more accurately estimates reliability than Cronbach's alpha when data come from items with few response options and/or show skewness; (iii) an empirical example from real data; and (iv) the procedure for calculating polychoric correlation matrices and ordinal alpha in the freely available software program R. We use ordinal alpha as a case study, but also provide the syntax for alternative <b>reliability</b> <b>coefficients</b> (such as beta or omega) ...|$|R
5000|$|Reliability of the subtests was {{studied by}} {{selecting}} protocols of 34 {{patients with a}} degree of severity of aphasia ranging from slight to severe. Kuder-Richardson <b>reliability</b> <b>coefficients</b> for subtests ranged from [...]68 to [...]98, with about two-thirds of the coefficients reported ranging from [...]90 upwards. Since test-retest reliability is difficult if not impossible to attain with patients suffering from aphasic symptoms, the current <b>reliability</b> <b>coefficients</b> demonstrate very good internal consistency {{in terms of what the}} items within the subtests are measuring.|$|R
40|$|This study {{examines}} the proposed Reliability Generalization (RG) method for studying reliability. RG employs {{the application of}} meta-analytic techniques {{similar to those used}} in validity generalization studies to examine <b>reliability</b> <b>coefficients.</b> This study explains why RG does not provide a proper research method for the study of reliability, including describing how reliability is not a singular metric but a family of coefficients that are not interchangeable, along with other issues, such as sample and test administration. This research used Monte Carlo simulations designed to illustrate how the same instrument, administered repeatedly, can result in different <b>reliability</b> <b>coefficients</b> and to show that variation in <b>reliability</b> <b>coefficients</b> is due to sampling error; results illustrate that the reliability of a test will vary across test administrations based on the size and composition of the sample and how the sample was selected (randomly versus non-randomly) ...|$|R
5000|$|The example below {{provides}} a prototypical matrix {{and what the}} correlations between measures mean. The diagonal line is typically filled in with a <b>reliability</b> <b>coefficient</b> of the measure (e.g. alpha coefficient). Descriptions in brackets [...] indicate what is expected when {{the validity of the}} construct (e.g., depression or anxiety) and the validities of the measures are all high.|$|E
50|$|A very {{important}} {{feature in the}} aggregation of responses is that the combined responses of individuals will be more accurate than the responses of each individual included in the aggregation. Reliability theory in psychology (specifically, the <b>reliability</b> <b>coefficient</b> and the Spearman-Brown prediction formula) provides a mathematical estimate of the accuracy or validity of aggregated responses {{from the number of}} units being combined and the level of agreement among the units. In this case, accuracy of aggregated responses can be calculated from the number of subjects and the average Pearson correlation coefficient between all pairs of subjects (across questions).|$|E
5000|$|When {{the number}} of {{categories}} being used is small (e.g. 2 or 3), the likelihood for 2 raters to agree by pure chance increases dramatically. This is because both raters must confine themselves to {{the limited number of}} options available, which impacts the overall agreement rate, and not necessarily their propensity for [...] "intrinsic" [...] agreement (an agreement is considered [...] "intrinsic" [...] if it is not due to chance). Therefore, the joint probability of agreement will remain high {{even in the absence of}} any [...] "intrinsic" [...] agreement among raters. A useful inter-rater <b>reliability</b> <b>coefficient</b> is expected (a) to be close to 0, when there is no [...] "intrinsic" [...] agreement, and (b) to increase as the [...] "intrinsic" [...] agreement rate improves. Most chance-corrected agreement coefficients achieve the first objective. However, the second objective is not achieved by many known chance-corrected measures.|$|E
5000|$|The CAS battery {{provides}} a standard {{score for each}} process {{as well as a}} Full Scale standard score. The average internal <b>reliability</b> <b>coefficients</b> across ages 15-17 for the PASS scales are: ...|$|R
40|$|An {{approximate}} {{statistical test}} of the equality of two intraclass <b>reliability</b> <b>coefficients</b> {{based on the same}} sample of people is derived. Such a test is needed when a researcher wishes to compare the reliability of two measurement procedures and both procedures {{can be applied to the}} performances or products of the same group of individuals. A numerical example is presented. Monte carlo studies indicate that the proposed test effectively controls Type I error with as few as two or three measurements on each of 50 people. Index terms: equality of related intraclass <b>reliability</b> <b>coefficients,</b> intraclass <b>reliability,</b> sampling theory, Speannan-Brown extrapolation, statistical test...|$|R
40|$|The {{purpose of}} this study was to compare the {{differences}} between 2 sets of ballistic stretching and 2 sets of a dynamic stretching routine on vertical jump performance. The intraclass <b>reliability</b> <b>coefficients</b> for maximum jump height, force, and power were also assessed using the Kistler Quattro Jump® force plate. METHODS: Ten healthy male college students, ages 22 to 34 years, volunteered to participate in this study. All subjects completed three individual testing sessions on three non-consecutive days. On each day the subjects completed one of three treatments (no stretch, ballistic stretch, and dynamic stretch). A paired samples t-test was used to test the effects of ballistic and dynamic stretching, respectively, on jumping height, force, and power performance scores. Hoyt 2 ̆ 7 s analysis of variance model, (MSs-MSi) /MSs, was used to estimate the <b>reliability</b> <b>coefficients</b> of jumping performance scores across three different trials. RESULTS: A paired samples t-test documented that there were no statistical differences in jumping height, force, or power between no stretch and ballistic stretch, and between no stretch and dynamic stretch. The intraclass <b>reliability</b> <b>coefficients</b> were 3 ̆e. 99 for jumping height, 3 ̆e 0. 94 for jumping force, and 3 ̆e. 99 for jumping power. CONCLUSION: Both ballistic and dynamic stretching showed nonsignificant effects on vertical jumping performance in college male students, but the <b>reliability</b> <b>coefficients</b> were very high to measure jumping height, force, and power using the Kistler Quattro Jump® force plate...|$|R
50|$|Another {{important}} difference between CTT and G {{theory is that}} the latter approach takes into account how the consistency of outcomes may change if a measure is used to make absolute versus relative decisions. An example of an absolute, or criterion-referenced, decision would be when an individual's test score is compared to a cut-off score to determine eligibility or diagnosis (i.e. a child's score on an achievement test is used to determine eligibility for a gifted program). In contrast, an example of a relative, or norm-referenced, decision would be when the individual's test score is used to either (a) determine relative standing as compared to his/her peers (i.e. a child's score on a reading subtest is used to determine which reading group he/she is placed in), or (b) make intra-individual comparisons (i.e. comparing previous versus current performance within the same individual). The type of decision that the researcher is interested in will determine which formula should be used to calculate the generalizability coefficient (similar to a <b>reliability</b> <b>coefficient</b> in CTT).|$|E
5000|$|Ray and Doratis {{designed}} a groundbreaking attitude scale to measure religiocentrism and ethnocentrism. Their religiocentrism scale comprises 33 items (for instance, [...] "I think my religion is nearer {{to the truth}} than any other" [...] and [...] "Most Moslems, Buddhists and Hindus are very stupid and ignorant"), with five-point Likert scale psychometric response options from [...] "Strongly agree" [...] (Scored 5) to [...] "Strongly disagree" [...] (1). To verify internal consistency among respondents, 11 items were reverse scored ("It makes no difference to me what religion my friends are" [...] is the converse of [...] "I think that it's better if you stick to friends of the same religion as your own"), resulting in a <b>reliability</b> <b>coefficient</b> of [...]88 among 154 first-year university students. The authors tested attitudes among Australian fifth-form students in two Catholic and two public schools, and discovered that neither ethnocentrism nor religiocentrism showed any correlation with religious background. Ray and Doratis concluded (1971:178), [...] "Ethnocentrism, religiocentrism and religious conservatism were all shown to be separate and distinct factors of attitudes in their own right. They are not just three aspects of the one thing. Religiocentric people do however tend to be both religiously conservative and ethnocentric." ...|$|E
40|$|The <b>reliability</b> <b>coefficient</b> and the {{standard}} error of measurement in classical test theory are not properties of a specific test, but are attributed to both a specific test and a specific trait distribution. In latent trait models, or item response theory, the test information function (TIF) provides more precise local measures of accuracy in trait estimation than {{are available from the}} <b>reliability</b> <b>coefficient.</b> The <b>reliability</b> <b>coefficient</b> is still widely used, however, and is popular because of its simplicity. Thus, it is worthwhile to relate it to the TIF. In this paper, the <b>reliability</b> <b>coefficient</b> is predicted from the TIF, or two modified TIF formulas, and a specific trait distribution. Examples demonstrate the variability of the <b>reliability</b> <b>coefficient</b> across different trait distributions, and the results are compared with empirical reliability coefficients. Practical suggestions are given as to how to make better use of the <b>reliability</b> <b>coefficient.</b> Index terms: adaptive testing, bias, classical test theory, item information function, latent trait models, maximum likelihood estimation, <b>reliability</b> <b>coefficient,</b> standard error of measurement, test information function, trait estimation...|$|E
40|$|Aims. A {{study was}} {{designed}} to assess the effects of a standardized instructional videotape on training senior medical students to acceptable levels of reliability in performing several commonly used obsever dependent outcome measures in patients with ankylosing spondylitis (AS). Methods. During a single day, six third-year medical students independently examined five patients with AS in predetermined order using a Latin Square design, before and after viewing a standardized videotape demonstrating 14 examination techniques. <b>Reliability</b> <b>coefficients</b> were calculated based on the variance components of the analysis of variance (ANOVA) table. Results. Prestandardization <b>reliability</b> <b>coefficients</b> were < 0. 80 for three measures. Following standardization 12 <b>reliability</b> <b>coefficients</b> exceeded 0. 80. For the majority of measures prestandardization <b>reliability</b> <b>coefficients</b> were high and no further improvement in reliability could be demonstrated. Conclusions. High levels of interobserver agreement were noted prior to viewing the instructional videotape. This may represent the success of undergraduate clinical skills training programmes, or it {{may be the result of}} having reviewed an illustrated instructional text just prior to the initial patient examinations. With the exception of chest excursion, high levels of prestandardization reliability, by necessity, precluded the demonstration of significant effects from viewing the videotape. Nevertheless, the data indicate that senior medical students are capable of reliably performing quantitative measurement in AS. Recent surveys in Canada and Australia, showing a general lack of quantitative clinical measurement in the longitudinal follow up of AS outpatients by rheumatologists, suggest that the lack of quantitation is not due to inability to reliably perform the measurements...|$|R
40|$|Agreement and Information in the <b>Reliability</b> of Coding <b>Coefficients</b> that {{assess the}} {{reliability}} of data making processes – coding text, transcribing interviews, or categorizing observations into analyzable terms – are mostly conceptualized {{in terms of the}} agreement a set of coders, observers, judges, or measuring instruments exhibit. When variation is low, <b>reliability</b> <b>coefficients</b> reveal their dependency on an often neglected phenomenon, the amount of information that reliability data provide about {{the reliability of}} the coding process or the data it generates. This paper explores the concept of reliability, simple agreement, four conceptions of chance to correct that agreement, sources of information deficiency, and develops two measures of information about reliability, akin to the power of a statistical test, intended as a companion to traditional <b>reliability</b> <b>coefficients,</b> especially Krippendorff‟s (2004, pp. 221 - 250; Hayes & Krippendorff, 2007) alpha...|$|R
30|$|The Spanish {{version of}} the {{questionnaire}} obtained good <b>reliability</b> <b>coefficients</b> and its factorial structure reliably replicated that obtained by the original measure. The {{results indicate that the}} Spanish {{version of the}} MWQ is a suitably valid measure to evaluate the mind-wandering phenomenon.|$|R
40|$|Tujuan penelitian ini adalah untuk menguji adanya perbedaan pada koefisien reliabilitas tes hasil belajar sejarah yang dihasilkan oleh kelompok homogen dan koefisien reliabilitas tes hasil belajar sejarah yang dihasilkan oleh kelompok heterogen dengan menggunakan metode penelitian komparatif. Penelitian ini dilakukan terhadap 400 siswa SMA di Kota Palembang yang terdiri dari 200 responden kelompok homogen dan 200 responden dari kelompok heterogen. Pengambilan sampel sebanyak 133 skor responden dilakukan dengan menggunakan teknik pengambilan sampel dengan pengembalian untuk dihitung koefisien reliabilitasnya, pengambilan sampel dilakukan sebanyak 30 kali, sehingga diperoleh masing-masing 30 buah koefisien reliabilitas untuk kelompok homogen dan kelompok heterogen. Hasil penelitian ini menunjukkan bahwa terdapat perbedaan yang signifikan pada koefisien reliabilitas yang dihasilkan oleh kelompok homogen dan kelompok heterogen. Kelompok heterogen menghasilkan koefisien reliabilitas yang lebih tinggi dibandingkan dengan kelompok homogen yang menghasilkan koefisien reliabilitas yang lebih rendah. Temuan ini mendukung teori bahwa homogenitas dan heterogenitas mempengaruhi besaran nilai koefisien reliabilitas. Kata Kunci: Koefisien Reliabilitas, Kelompok Homogen, Kelompok Heterogen The {{objective}} of the research is to study the difference of <b>reliability</b> <b>coefficient</b> in achievement tes of history resulted from homogeneous group and <b>reliability</b> <b>coefficient</b> in achievement test of history resulted from heterogeneous group by using comparative research method. The {{study was conducted to}} 400 senior high school students in Palembang including 200 respondents of homogenous group and 200 respondents of heterogeneous group. The sample of 133 respondents’ scores was done by using sampling with replacement technique. Then, the <b>reliability</b> <b>coefficient</b> was calculated. The sample was taken 30 times; thus, thirty reliability coefficients were obtained for homogenous group and heterogeneous group. The research result shows a significant difference between <b>reliability</b> <b>coefficient</b> of homogenous group and the heterogeneous one. The heterogeneous group has a higher <b>reliability</b> <b>coefficient</b> compared to the homogenous group. The finding supports the theory of homogeneity and heterogeneity influence towards <b>reliability</b> <b>coefficient</b> value. Keywords: <b>Reliability</b> <b>Coefficient,</b> Homogeneous group, Heterogeneous grou...|$|E
3000|$|... as the <b>reliability</b> <b>coefficient</b> for {{selecting}} the cluster header {{as well as}} the selective cluster data.|$|E
40|$|AbstractThe aim of {{this study}} was to develop a scale to {{determine}} beliefs of teachers about students’ mobile device usage. Development process is composed of literature review, writing down the items, obtaining expert views, applying scale forms (N= 130) and validity and reliability studies. As a result of factor analysis which is carried out within validity study of scale, it was determined that the scale is composed of 2 dimensions; academic development and social development. The scale which included 28 items in pilot application was reduced to 16 items as a result of analyses. <b>Reliability</b> <b>coefficient</b> of the whole scale is (∝). 91, <b>reliability</b> <b>coefficient</b> of academic development is. 85, <b>reliability</b> <b>coefficient</b> of social development was calculated as. 81...|$|E
40|$|A new {{technique}} for optimizing split-half reliability estimate was developed. Using this method, internal <b>reliability</b> <b>coefficients</b> for some major Rorschach variables were calculated. The optimum item (card) combinations for each variable {{was found to}} provide useful information for insight into item (card) characteristics...|$|R
30|$|Internal <b>reliability</b> <b>coefficients</b> (alpha) were {{computed}} {{for each of the}} six dimensions. Alphas were high, ranging from 0.94 (Attention Allocation) to 0.76 (Suspension of Disbelief). Others were: Spatial Situational Model α[*]=[*] 0.91; Cognitive Involvement α[*]=[*] 0.86; Suspension of Disbelief α[*]=[*] 0.76; and Self-location α[*]=[*] 0.86.|$|R
50|$|Chinese (STQ-C), Urdu (STQ-U) and Polish (STQ-P) Extended {{versions}} of the STQ, administered among corresponding populations, showed <b>reliability</b> <b>coefficients</b> in the range 0.70-0.86, item-total correlations in the range 0.42-0.73, and all versions demonstrated robust factor structures {{similar to those of}} the original version (Trofimova, 2010a).|$|R
40|$|This study aims {{to obtain}} a model of {{character}} education. This research was conducted at the State University of Medan in three stages. The {{first phase of the}} study in 2016 aimed {{to obtain a}} set of character education instrument valid and reliable. Subject test instruments in this study were students at State University of Medan in the Academic Year 2016 / 2017 of 50 people. Based on the results of testing instruments available instruments character education nationality valid as many as 30 items with <b>reliability</b> <b>coefficient</b> of 0. 935, instrument character education fairness valid as many as 24 items with <b>reliability</b> <b>coefficient</b> of 0. 923, instrument character education honor valid as many as 25 items with coefficients reliability of 0. 919, instrument character education responsibilities are valid as many as 32 items with <b>reliability</b> <b>coefficient</b> of 0. 947, instrument character education concerns are valid as many as 25 items with <b>reliability</b> <b>coefficient</b> of 0. 938, and educational instruments character trustworthy valid as many as 27 items with coefficients reliability of 0. 955...|$|E
30|$|Calculating, Interpreting, and Reporting Cronbach’s Alpha <b>Reliability</b> <b>Coefficient</b> for Likert-Type Scales Gliem, Joseph A. and Gliem, Rosemary R 2003.|$|E
40|$|AbstractThis study {{aimed to}} analyze the effect of {{secondary}} school students’ academic motivations on their attitudes towards the chemistry course. The sampling consisted of Grade 9, 10, 11 and 12 students studying at the secondary schools in Ankara. The data were collected through the “Academic Motivation Scale” developed by Bozanoglu (2004) and “The Scale of Attitudes towards Chemistry” developed by Kan & Akbas (2005). The Crobach alpha <b>reliability</b> <b>coefficient</b> of the Academic Motivation Scale {{was found to be}} 0. 88 and the <b>reliability</b> <b>coefficient</b> value for The Scale of Attitudes towards Chemistry was 0. 92...|$|E
40|$|The {{present study}} reports a {{reliability}} generalization (RG) meta-analysis of subscale and total scale {{scores on the}} Web-administered LibQUAL+ ™ protocol. Data were provided by 18, 161 participants from 43 universities in the United States and Canada. Results indicate that score reliabilities were remarkably invariant across campuses and different user groups. In 1998, Vacha-Haase proposed her reliability generalization (RG) method as a measurement meta-analytic method similar to validity general-ization (Hunter & Schmidt, 1990; Schmidt & Hunter, 1977). RG character-izes: (a) the typical reliability of scores for a given test across studies, (b) the amount of variability in <b>reliability</b> <b>coefficients</b> for givenmeasures, and (c) the sources of variability in <b>reliability</b> <b>coefficients</b> across studies. RG methods have been applied to study the characteristics of scores from awide variety o...|$|R
5000|$|... asking enough {{questions}} to allow {{all aspects of}} an issue to be covered and to control effects due to {{the form of the}} question (such as positive or negative wording), the adequacy of the number being established quantitatively with psychometric measures such as <b>reliability</b> <b>coefficients,</b> and ...|$|R
40|$|Coefficients that {{assess the}} {{reliability}} of data making processes – coding text, transcribing interviews, or categorizing observations into analyzable terms – are mostly conceptualized {{in terms of the}} agreement a set of coders, observers, judges, or measuring instruments exhibit. When variation is low, <b>reliability</b> <b>coefficients</b> reveal their dependency on an often neglected phenomenon, the amount of information that reliability data provide about {{the reliability of}} the coding process or the data it generates. This paper explores the concept of reliability, simple agreement, four conceptions of chance to correct that agreement, sources of information deficiency, and develops two measures of information about reliability, akin to the power of a statistical test, intended as a companion to traditional <b>reliability</b> <b>coefficients,</b> especially Krippendorff‟s (2004, pp. 221 - 250; Hayes 2 ̆ 6 Krippendorff, 2007) alpha...|$|R
