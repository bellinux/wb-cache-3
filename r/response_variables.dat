2491|5584|Public
25|$|A triple-blind {{study is}} an {{extension}} of the double-blind design; the monitoring committee <b>response</b> <b>variables</b> is not told the identity of the groups. The committee is simply given data for groups A and B. A triple-blind study has the theoretical advantage of allowing the monitoring committee to evaluate the response variable results more objectively. This assumes that appraisal of efficacy and harm, as well as requests for special analyses, may be biased if group identity is known. However, in a trial where the monitoring committee has an ethical responsibility to ensure participant safety, such a design may be counterproductive since in this case monitoring is often guided by the constellation of trends and their directions. In addition, by the time many monitoring committees receive data, often any emergency situation has long passed.|$|E
25|$|Response correspondences for all 2x2 {{normal form}} games {{can be drawn}} with a line for each player in a unit square {{strategy}} space. Figures 1 to 3 graphs the best response correspondences for the stag hunt game. The dotted line in Figure 1 shows the optimal probability that player Y plays 'Stag' (in the y-axis), {{as a function of}} the probability that player X plays Stag (shown in the x-axis). In Figure 2 the dotted line shows the optimal probability that player X plays 'Stag' (shown in the x-axis), {{as a function of the}} probability that player Y plays Stag (shown in the y-axis). Note that Figure 2 plots the independent and <b>response</b> <b>variables</b> in the opposite axes to those normally used, so that it may be superimposed onto the previous graph, to show the Nash equilibria at the points where the two player's best responses agree in Figure 3.|$|E
2500|$|Various {{models have}} been created that allow for heteroscedasticity, i.e. the errors for {{different}} <b>response</b> <b>variables</b> may have different variances. [...] For example, weighted least squares is a method for estimating linear regression models when the <b>response</b> <b>variables</b> may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.|$|E
40|$|Abstract. In {{this article}} {{discussion}} is continued {{of the case}} where the <b>response</b> <b>variable</b> is continuous and the explana-tory variable is discrete. Two particular situations are covered: the <b>response</b> <b>variable</b> survival time and the explanatory variable dichotomous: and the <b>response</b> <b>variable</b> continuous (but not survival time) and the explanatory variable discrete (but not dichotomous) ...|$|R
5000|$|Ordinary linear {{regression}} predicts the expected {{value of a}} given unknown quantity (the <b>response</b> <b>variable,</b> a random variable) as a linear combination {{of a set of}} observed values (predictors). This implies that a constant change in a predictor leads to a constant change in the <b>response</b> <b>variable</b> (i.e. a linear-response model). This is appropriate when the <b>response</b> <b>variable</b> has a normal distribution (intuitively, when a <b>response</b> <b>variable</b> can vary essentially indefinitely in either direction with no fixed [...] "zero value", or more generally for any quantity that only varies by a relatively small amount, e.g. human heights).|$|R
40|$|Maps {{can be used}} {{to display}} the {{relationship}} between location and a <b>response</b> <b>variable.</b> In this basic form, there is one map to be displayed. Often times there are additional factors that could a#ect the <b>response</b> <b>variable.</b> In this case, it is of interest to simultaneously view the spatial relationships and the dependence of the <b>response</b> <b>variable</b> on the additional factors. Because the additional factors will be multi-valued, the number of maps to display can be very large...|$|R
2500|$|Suppose {{the error}} terms ε'ij are {{independent}} and normally distributed with expected value0 and varianceσ2. [...] We treat x'i as constant rather than random. [...] Then the <b>response</b> <b>variables</b> Y'ij are random {{only because the}} errors ε'ij are random.|$|E
2500|$|Independence of errors. [...] This {{assumes that}} the errors of the <b>response</b> <b>variables</b> are {{uncorrelated}} with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods (e.g. generalized least squares) are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.|$|E
2500|$|Standard linear {{regression}} models with standard estimation techniques make {{a number of}} assumptions about the predictor variables, the <b>response</b> <b>variables</b> and their relationship. [...] Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), {{and in some cases}} eliminated entirely. [...] Some methods are general enough that they can relax multiple assumptions at once, and in other cases this can ns. [...] Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.|$|E
30|$|In essence, it tests {{whether the}} {{selected}} exploratory variables {{are useful in}} predicting the <b>response</b> <b>variable.</b> Values close to 1 {{suggest that there is}} no association, whereas values greater than 5 at statistically significant levels signify that the selected exploratory variables are good predictors of the <b>response</b> <b>variable.</b>|$|R
30|$|The result mainly {{indicates}} that, regressing the <b>response</b> <b>variable</b> “number of {{employer in}} an enterprise” based on variables selected by lasso and stepwise method does bring greater model fitness (based on adjusted R-squared value) than variables selected by association and correlation methods. Similarly, regressing the <b>response</b> <b>variable</b> “development status of an enterprise” based on variables selected by association and correlation methods does bring 12 significant variables, where none of variables are significant by lasso and stepwise elimination. Hence, lasso and stepwise variable selection methods are suggested for continuous <b>response</b> <b>variable</b> “number of employment in an enterprise”, and association and correlation methods are suggested for categorical <b>response</b> <b>variable</b> “development status of an enterprise”; or alternatively filtering variables by regression, correlation and association methods and merging them for further analysis is also suggestible.|$|R
5000|$|<b>Response</b> <b>variable</b> {{residuals}} {{are normally}} distributed (or approximately normally distributed).|$|R
50|$|In a {{narrower}} sense, regression may refer {{specifically to the}} estimation of continuous <b>response</b> <b>variables,</b> {{as opposed to the}} discrete <b>response</b> <b>variables</b> used in classification. The case of a continuous output variable may be more specifically referred to as metric regression to distinguish it from related problems.|$|E
50|$|In {{response}} surface methodology, {{the objective is}} to find the relationship between the input variables and the <b>response</b> <b>variables.</b> The process starts from trying to fit a linear regression model. If the P-value turns out to be low, then a higher degree polynomial regression, which is usually quadratic, will be implemented. The process of finding a good relationship between input and <b>response</b> <b>variables</b> will be done for each simulation test. In simulation optimization, {{response surface}} method can be used to find the best input variables that produce desired outcomes in terms of <b>response</b> <b>variables.</b>|$|E
5000|$|... the {{regressor}} matrix and [...] {{the vector}} of <b>response</b> <b>variables.</b> More details {{can be found}} e.g. here ...|$|E
40|$|We {{present an}} artificially {{simulated}} dataset (TIED) constructed {{so that there}} are many minimal sets of variables with maximal predictivity (i. e., Markov boundaries) and likewise many sets of variables that are statistically indistinguishable from the set of direct causes and direct effects of the <b>response</b> <b>variable.</b> This dataset was used in the Potluck Causality Challenge to determine all statistically indistinguishable sets of direct causes and direct effects and all Markov boundaries of the <b>response</b> <b>variable</b> and also to predict the <b>response</b> <b>variable</b> in the independent test data. We also present baseline results of application of several algorithms to this dataset...|$|R
40|$|Consider a {{case where}} cause-effect {{relationships}} among variables can be described by a causal diagram and the corresponding linear structural equation model. In order to bring a <b>response</b> <b>variable</b> close to a target, this paper proposes a statistical method for inferring a joint causal effect of a conditional plan on the variance of a <b>response</b> <b>variable</b> from nonexperimental data. Moreover, based on this method, this paper formulates a conditional plan, which can cancel the influence of covariates on a <b>response</b> <b>variable.</b> The results of this paper could enable us to select an effective plan in linear conditional plans...|$|R
50|$|Quantile {{regression}} {{is a type}} of {{regression analysis}} used in statistics and econometrics. Whereas the method of least squares results in estimates that approximate the conditional mean of the <b>response</b> <b>variable</b> given certain values of the predictor variables, quantile regression aims at estimating either the conditional median or other quantiles of the <b>response</b> <b>variable.</b>|$|R
50|$|Various {{models have}} been created that allow for heteroscedasticity, i.e. the errors for {{different}} <b>response</b> <b>variables</b> may have different variances. For example, weighted least squares is a method for estimating linear regression models when the <b>response</b> <b>variables</b> may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.|$|E
5000|$|... {{estimate}} [...] by least-squares fit to {{a matrix}} of <b>response</b> <b>variables</b> , computed using the pseudoinverse , given a design matrix : ...|$|E
5000|$|Refit {{the model}} using the fictitious <b>response</b> <b>variables</b> , and retain the {{quantities}} of interest (often the parameters, , estimated from the synthetic [...] ).|$|E
30|$|Steps 2 through 5 are {{repeated}} {{for all other}} <b>response</b> <b>variable</b> classes.|$|R
50|$|The fixed-effects model (class I) of {{analysis}} of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment {{to see whether the}} <b>response</b> <b>variable</b> values change. This allows the experimenter to estimate the ranges of <b>response</b> <b>variable</b> values that the treatment would generate in the population as a whole.|$|R
50|$|GAMLSS is {{especially}} suited for modelling a leptokurtic or platykurtic and/or positively or negatively skewed <b>response</b> <b>variable.</b> For count type <b>response</b> <b>variable</b> data {{it deals with}} over-dispersion by using proper over-dispersed discrete distributions. Heterogeneity also is dealt with by modelling the scale or shape parameters using explanatory variables. There are several packages written in R related to GAMLSS models.|$|R
50|$|For random {{censoring}} on the <b>response</b> <b>variables,</b> the censored quantile {{regression of}} Portnoy (2003) provides consistent estimates of all identifiable quantile functions based on reweighting each censored point appropriately.|$|E
50|$|The {{form of the}} {{distribution}} assumed for the response variable y, is very general. For example an implementation of GAMLSS in R has around 50 different distributions available. Such implementations also allow use of truncated distributions and censored (or interval) <b>response</b> <b>variables.</b>|$|E
5000|$|It {{is assumed}} that E(εi) = 0. The above {{variance}} varies with i, or the ith trial in an experiment or the ith case or observation in a dataset. Equivalently, heteroscedasticity refers to unequal conditional variances in the <b>response</b> <b>variables</b> Yi, such that ...|$|E
5000|$|The {{transformed}} <b>response</b> <b>variable</b> {{is constructed}} {{to measure the}} spread in each group. Let ...|$|R
25|$|In case of {{a single}} regressor, fitted by least squares, R2 is {{the square of the}} Pearson {{product-moment}} correlation coefficient relating the regressor and the <b>response</b> <b>variable.</b> More generally, R2 is the square of the correlation between the constructed predictor and the <b>response</b> <b>variable.</b> With more than one regressor, the R2 can be referred to as the coefficient of multiple determination.|$|R
2500|$|Consider fitting a {{line with}} one {{predictor}} variable. Define i as {{an index of}} each of the n distinct x values, j as an index of the <b>response</b> <b>variable</b> observations for a given x value, and n'i as the number of y values associated with the i th x value. [...] The value of each <b>response</b> <b>variable</b> observation can be represented by ...|$|R
50|$|Suppose {{the error}} terms ε i j are {{independent}} and normally distributed with expected value 0 and variance σ2. We treat x i as constant rather than random. Then the <b>response</b> <b>variables</b> Y i j are random {{only because the}} errors ε i j are random.|$|E
5000|$|For each pair, (xi, yi), {{in which}} xi is the (possibly multivariate) {{explanatory}} variable, add a randomly resampled residual, , to the response variable yi. In other words, create synthetic <b>response</b> <b>variables</b> [...] where j is selected randomly {{from the list}} (1, …, n) for every i.|$|E
5000|$|In {{mathematical}} optimization, {{the problem}} of non-negative least squares (NNLS) is a constrained version of the least squares problem where the coefficients {{are not allowed to}} become negative. That is, given a matrix [...] and a (column) vector of <b>response</b> <b>variables</b> , the goal is to find ...|$|E
2500|$|Generalized {{linear models}} allow for an {{arbitrary}} link function g that relates {{the mean of}} the <b>response</b> <b>variable</b> to the predictors, i.e. E(y) = g(β′x). The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the [...] range of the linear predictor and the range of the <b>response</b> <b>variable.</b>|$|R
30|$|Contour plot {{represents}} {{control parameters}} on x and y-scale and <b>response</b> <b>variable</b> as a contour. It represents the clear variation of <b>response</b> <b>variable</b> against control variables. Figure  6 shows contour plot for wear volume loss against load {{and percentage of}} hBN. From observation of contour plot also, {{it is clear that}} wear loss is interaction effect of load and % hBN.|$|R
40|$|In many {{economic}} contexts {{the dependent}} or <b>response</b> <b>variable</b> of interest (y) is a nonnegative integer or count which {{we wish to}} explain or analyze {{in terms of a}} set of covariates (x). Unlike the classical regression model, the <b>response</b> <b>variable</b> is discrete with a distribution that places probability mass at nonnegative integer values only. Regression models for counts, like othe...|$|R
