0|10000|Public
40|$|A {{multichannel}} on-line RMS {{data acquisition}} and reduction {{system has been}} developed using commercial RMS computing modules and a programmable calculator. Details of this system, which has the capability of acquiring 96 channels of RMS data and computing and printing desired parameters in near real-time, are presented. In addition, raw data can be recorded {{at a much higher}} <b>rate</b> <b>for</b> <b>computation</b> and printing later. Results are presented showing the benefits of this system in 'sweep' tests where one parameter such as Mach number or angle of attack is slowly varied with time...|$|R
40|$|In this paper, {{we discuss}} {{techniques}} for reducing errors in DNA computation. We investigate several methods for achieving acceptable overall error <b>rates</b> <b>for</b> a <b>computation</b> using basic operations that are error prone. We analyze a single essential biotechnology, sequence-specific separation, {{and show that}} separation errors theoretically {{can be reduced to}} tolerable levels by invoking a tradeoff between time, space, and error rates at the level of algorithm design. These tradeoffs do not depend upon improvement of the underlying biotechnology which implements the separation step. We outline several specific ways in which error reduction can be done and present numerical calculations of their performance. ...|$|R
25|$|Personality tests – Tests of {{personality}} aim to describe patterns of behavior, thoughts, and feelings. They generally fall within two categories: objective and projective. Objective measures, {{such as the}} MMPI, are based on restricted answers—such as yes/no, true/false, or a <b>rating</b> scale—which allow <b>for</b> <b>computation</b> of scores that {{can be compared to}} a normative group. Projective tests, such as the Rorschach inkblot test, allow for open-ended answers, often based on ambiguous stimuli, presumably revealing non-conscious psychological dynamics.|$|R
40|$|We {{develop a}} model of a small open economy with three types of nominal rigidities (domestic goods prices, {{imported}} goods prices and wages) and eight different structural shocks. We estimate the model's structural parameters using a maximum likelihood procedure and use it to compute welfare-maximizing Taylor rules for setting domestic short-term interest <b>rates.</b> <b>For</b> these <b>computations,</b> we use a second-order approximation around the model's deterministic steady state, which allows the Taylor rule coefficients to affect the means of consumption, leisure and real balances as well as their variances. Welfare gains from moving to the optimal Taylor rule are substantial, but require a very precise knowledge of the values of the model's structural parameters. Economic models; Open economy; Optimal monetary policy; Taylor rules...|$|R
40|$|The {{article focuses}} on the {{feasibility}} of teaching senior school students in Australia the methods of calculating the {{rate of return on}} investments. Since students know the concept of a compound interest <b>rate,</b> the <b>computations</b> <b>for</b> the internal <b>rate</b> of return (IRR) and the continuously rate of return may be taught with efficiency. The <b>computations</b> <b>for</b> equations reducible to quadratics and integration involving exponential functions are explained. Integrating such calculations into the curriculum of senior school students will reportedly allow them to be exposed to the financial world. 11 page(s...|$|R
40|$|We {{introduce}} {{a new model of}} molecular computation that we call the sticker model. Like many previous proposals it makes use of DNA strands as the physical substrate in which information is represented and of separation by hybridization as a central mechanism. However, unlike previous models, the stickers model has a random access memory that requires no strand extension, uses no enzymes, and (at least in theory) its materials are reusable. The paper describes computation under the stickers model and discusses possible means for physically implementing each operation. We go on to propose a specific machine architecture for implementing the stickers model as a microprocessor-controlled parallel robotic workstation. Finally, we discuss several methods for achieving acceptable overall error <b>rates</b> <b>for</b> a <b>computation</b> using basic operations that are error prone. In the course of this development a number of previous general concerns about molecular computation [Smith, Hartmanis, Letters to Sci [...] ...|$|R
40|$|Current <b>rating</b> <b>computations</b> <b>for</b> HV and EHV {{insulated}} cables play {{a crucial}} role in the planning phase of a new line for both an entirely underground cable link and a mixed or hybrid (cascade connection of overhead and cable line) one. The paper gives a wide overview of the technical solutions which allow very high current ratings in undergrounded insulated cables. These solutions are examined from a theoretical standpoint and practical applications are shown and explained. The paper demonstrates that in underground insulated cables current rating up to 2500 A can be reached...|$|R
40|$|Weintroduce a {{new model}} of {{molecular}} computation thatwe call the sticker model. Likemany previous proposals it makes use of DNA strands as the physical substrate in which information is represented and of separation byhybridization as a central mechanism. However, unlike previous models, the stickers model has a random access memory that requires no strand extension, uses no enzymes, and (at least in theory) its materials are reusable. The paper describes computation under the stickers model and discusses possible means for physically implementing each operation. We go on to propose a speci c machine architecture for implementing thestickers model as a microprocessor-controlled parallel robotic workstation. Finally, we discuss several methods for achieving acceptable overall error <b>rates</b> <b>for</b> a <b>computation</b> using basic operations that are error prone. In the course of this development a number of previous general concerns about molecular computation [Smith, Hartmanis, Letters to Science] are addressed. First, it is clear that generalpurpose algorithms can be implemented by DNA-based computers, potentially solving a wide class of search problems. Second, we nd that there are challenging problems, for which onl...|$|R
40|$|A {{burn rate}} test having several {{advantages}} for low gas-producing pyrotechnic compacts has been developed. The technique involves {{use of a}} high speed video motion analysis system that allows immediate turnaround and produces all required data <b>for</b> <b>rate</b> <b>computation</b> on magnetic tape and becomes immediately available on the display screen. The test technique provides a quick method for material qualification along with data for improved reliability and function. Burn rate data has been obtained for both UPI and Eagle Pitcher Iron/Potassium Perchlorate blends. The data obtained for the UPI blends cover a range of composition, pellet density, and ambient (before ignition) pellet temperature. Burn <b>rate</b> data <b>for</b> the E-P blends were extended to include surface conditions or particle size as a variable parameter...|$|R
40|$|Bit error <b>rate</b> <b>computation</b> <b>for</b> optical {{communication}} systems incorporating equalizers and under both noise and intersymbol interference (ISI) is discussed. An accurate method {{based on a}} saddlepoint approximation is used <b>for</b> the <b>computation.</b> Previous work based on saddlepoint approximation has considered only the use of basic integration-and-dump detection. When ISI is strong, this simple detection method is unsatisfactory. Instead, a raised-cosine filtering {{is often used to}} achieve minimal ISI. This thesis considers both integration-and-dump and raised-cosine equalizers. The use of equalizers other than integration-and-dump complicates the computation because of the complexity of the moment generating function involved. Two different input pulses are considered to study the effect of ISI. Results show that when intersymbol interference is strong, the use of raised-cosine equalizers can reduce intersymbol interference and improve the performance of the system significantly...|$|R
40|$|Figure 1 : Leaves {{rendered}} {{with our}} approach. (a) -(d) are balata, pelargonium, omoto and prunus leaves respectively. This paper presents {{a framework for}} the real-time rendering of plant leaves with global illumination effects. Realistic rendering of leaves requires a sophisticated appearance model and accurate lighting <b>computation.</b> <b>For</b> leaf appearance we introduce a parametric model that describes leaves in terms of spatially-variant BRDFs and BTDFs. These BRDFs and BTDFs, incorporating analysis of subsurface scattering inside leaf tissues and rough surface scattering on leaf surfaces, can be measured from real leaves. More importantly, this description is compact and can be loaded into graphics hardware for fast run-time shading calculations, which are essential for achieving high frame <b>rates.</b> <b>For</b> lighting <b>computation,</b> we present an algorithm that extends the Precomputed Radiance Transfer (PRT) approach to all-frequency lighting for leaves. In particular, we handle the combined illumination effects due to lowfrequency environment light and high-frequency sunlight. This is done by decomposing the local incident radiance of sunlight into direct and indirect components. The direct component, which contains most of the high frequencies, is not pre-computed with spherical harmonics as in PRT; instead it is evaluated on-the-fly using pre-computed light-visibility convolution data. We demonstrate our framework by the rendering {{of a variety of}} leaves and assemblies thereof...|$|R
40|$|The {{focus of}} the subject DOE {{sponsored}} research concerns parallel methods, algorithms, and software for complex applications {{such as those in}} coupled fluid flow and heat transfer. The research has been directed principally toward the solution of large-scale PDE problems using iterative solvers for finite differences and finite elements on advanced computer architectures. This work embraces parallel domain decomposition, element-by-element, spectral, and multilevel schemes with adaptive parameter determination, rational iteration and related issues. In addition to the fundamental questions related to developing new methods and mapping these to parallel computers, there are important software issues. The group has {{played a significant role in}} the development of software both for iterative solvers and also for finite element codes. The research in computational fluid dynamics (CFD) led to sustained multi-Gigaflop performance <b>rates</b> <b>for</b> parallel-vector <b>computations</b> of realistic large scale applications (not computational kernels alone). The main application areas for these performance studies have been two-dimensional problems in CFD. Over the course of this DOE sponsored research significant progress has been made. A report of the progression of the research is given and at the end of the report is a list of related publications and presentations over the entire grant period...|$|R
40|$|In the {{distributed}} function computation problem, dichotomy theorems, {{initiated by}} Han-Kobayashi, seek to classify functions by whether the <b>rate</b> regions <b>for</b> function <b>computation</b> improve on the Slepian-Wolf regions or not. In this paper, we develop a general approach to derive converse bounds on the distributed function computation problem. By using this approach, we recover the sufficiency part, i. e. the conditions {{such that the}} Slepian-Wolf regions become optimal, of the known dichotomy theorems in the two-terminal distributed computing. Furthermore, we derive an improved sufficient condition on the dichotomy theorem in the multiterminal distributed computing for the class of i. i. d. sources with positivity condition. Finally, we derive the matching sufficient and necessary condition on the dichotomy theorem in the multiterminal distributed computing for the class of smooth sources. Comment: 29 pages, 8 figure...|$|R
40|$|This report {{provides}} inflation-adjusted {{excise tax}} <b>rates</b> <b>for</b> alcohol, tobacco, and gasoline products. The base <b>for</b> <b>computation</b> is November 1951. All {{of the above}} cited commodities had <b>rate</b> increases effective <b>for</b> that date under the Revenue Act of 1951. The adjustments show what the tax rates would be in 1999 {{if they had been}} increased to reflect inflatio...|$|R
40|$|This case {{requires}} {{a detailed analysis}} of impairments of both long-lived assets and goodwill for First Motors Corporation, a fictitious automobile company. By integrating multiple issues into this case, students are presented with some of the complexities and interrelationships that are seen in practice. To properly prepare solutions to this case, students must successfully read, interpret, and apply both accounting standards and concept statements. The use of judgment in choosing a discount <b>rate</b> <b>for</b> present value <b>computations</b> is an important component of this case. In fact, an earnings management issue and resulting conflict between First Motors Management and the company’s auditor revolves around the discount rate choice. Additionally, the suggested questions provided with the case require that students address components of the conceptual framework {{in the context of the}} impairment standards. This case can be used in upper division financial reporting classes at either the undergraduate or graduate level...|$|R
40|$|The {{question}} of how much communication is required between collaborating parties to compute a function of their data is of fundamental importance in the fields of theoretical computer science and information theory. In this work, the focus is on coming up with lower bounds on this. The information cost of a protocol is the amount of information the protocol reveals to Alice and Bob about each others inputs, and the information complexity of a function is the infimum of information costs over all valid protocols. For the amortized case, it is known that the optimal <b>rate</b> <b>for</b> the <b>computation</b> is equal to the information complexity. Exactly computing this information complexity is not straight forward however. In this work we lower bound information complexity for independent inputs in terms of the Wyner common information of a certain pair of random variables. We show a structural property for the optimal auxiliary random variable of Wyner common information and exploit this to exactly compute the Wyner common information in certain cases. The lower bound obtained through this technique is shown to be tight for a non-trivial example - equality (EQ) for the ternary alphabet. We also give an example to show that the lower bound may, in general, not be tight. Comment: 7 pages, 4 figures, accepted in NCC 201...|$|R
40|$|Abstract: This article {{presents}} the numerical simulations of air flow between two work chambers in a screw compressor on a simplified two-dimensional {{model of a}} clearance gap. The gap between male rotor and housing is presented. A constant pressure drop between two chambers is assumed. The rotary motion of the male rotor {{is not involved in}} the model. The flowing air is assumed to be viscous, compressible and heat conducting with ideal gas properties. Turbulent flow is considered to be statistically steady and three different models of turbulence are used. The results obtained by Baldwin-Lomax, Spalart-Allmaras and k-ε turbulence models are compared. The pressure distribution, flow velocity and mass flow <b>rate</b> are discussed. <b>For</b> <b>computation</b> the finite volume method on structured quadrilateral grid is used. As solvers own numerical code based on cell-centered finite volume formulation of the explicit TVD MacCormack scheme with Baldwin-Lomax turbulence model and professional software Fluent for the other turbulence models are used. The dependence of obtained results on selected computational grid density is observed...|$|R
40|$|We {{propose a}} method to compute a {{probably}} approximately correct (PAC) histogram of observations with a refresh rate of Θ(1) time units per histogram sample on a random geometric graph with noise-free links. The delay in computation is Θ(n) time units. We further extend our approach to a network with noisy links. While the re-fresh rate remains Θ(1) time units per sample, the delay increases to Θ(n logn). The number of transmissions in both cases is Θ(n) per histogram sample. The achieved Θ(1) refresh <b>rate</b> <b>for</b> PAC histogram <b>computation</b> is a significant improvement over the refresh rate of Θ(1 / logn) <b>for</b> his-togram <b>computation</b> in noiseless networks. We achieve this by operat-ing in the super-critical thermodynamic regime where large pathways for communication build up, but the network may {{have more than one}} component. The largest component however will have an arbitrarily large fraction of nodes in order to enable approximate computation of the histogram to the desired level of accuracy. Operation in the super-critical thermodynamic regime also reduces energy consumption. A key step in the proof of our achievability result is the construction of a connected component having bounded degree and any desired fraction of nodes. This construction may also prove useful in other communi-cation settings on the random geometric graph. ...|$|R
40|$|Point-based {{rendering}} {{methods have}} proven to be effective for the display of large point cloud surface models. For a realistic visualization of the models, transparency and shadows are essential features. We propose a method for point cloud rendering with transparency and shadows at interactive rates. Our approach does not require any global or local surface reconstruction method, but operates directly on the point cloud. All passes are executed in image space and no pre-computation steps are required. The underlying technique for our approach is a depth peeling method for point cloud surface representations. Having detected a sorted sequence of surface layers, they can be blended front to back with given opacity values to obtain renderings with transparency. These computation steps achieve interactive frame <b>rates.</b> <b>For</b> renderings with shadows, we determine a point cloud shadow texture that stores for each point of a point cloud whether it is lit by a given light source. The extraction of the layer of lit points is obtained using the depth peeling technique, again. For the shadow texture computation, we also apply a Monte-Carlo integration method to approximate light from an area light source, leading to soft shadows. Shadow <b>computations</b> <b>for</b> point light sources are executed at interactive frame <b>rates.</b> Shadow <b>computations</b> <b>for</b> area light sources are performed at interactive or near-interactive frame rates depending on the approximation quality...|$|R
40|$|We {{investigate}} {{false positive}} (FP) accusation probabilities for q -ary Tardos codes in the Restricted Digit Model. We employ a computation method recently introduced by us, {{to which we}} refer as Convolution and Series Expansion (CSE). We present a comparison of several collusion attacks on q -ary codes: majority voting, minority voting, Interleaving, µ~ -minimizing and Random Symbol (the q -ary equivalent of the Coin Flip strategy). The comparison is made {{by looking at the}} FP rate at approximately fixed False Negative rate. In nearly all cases we find that the strongest attack is either minority voting or µ~ -minimizing, depending on the exact setting of parameters such as alphabet size, code length, and coalition size. Furthermore, we present results on the convergence speed of the CSE method, and we show how FP <b>rate</b> <b>computations</b> <b>for</b> the Random Symbol strategy can be sped up by a pre-computation step. Keywords: Traitor tracing; Tardos code; Collusion; Watermarkin...|$|R
40|$|The double plasma {{resonance}} (DPR) instability plays a basic role in {{the generation}} of solar radio zebras. In the plasma, consisting of the loss-cone type distribution of hot electrons and much denser and colder background plasma, this instability generates the upper-hybrid waves, which are then transformed into the electromagnetic waves and observed as radio zebras. In the present paper we numerically study the double plasma resonance instability {{from the point of}} view of the zebra interpretation. We use a 3 -dimensional electromagnetic particle-in-cell (3 -D PIC) relativistic model. First using the multi-mode model, we study details of the double plasma resonance instability. We show how the distribution function of hot electrons changes during this instability. Then we show that there is a very good agreement between results obtained by the multi-mode and specific-mode models, which is caused by a dominance of the wave with the maximal growth <b>rate.</b> Therefore, <b>for</b> <b>computations</b> in a broad range of model parameters, we use the specific-mode model. We compute the maximal growth rates of the double plasma resonance instability. The results are compared with the analytical ones. We find a very good agreement between numerical and analytical growth rates. We also compute saturation energies of the upper-hybrid waves in a very broad range of parameters. We find that the saturation energies of the upper-hybrid waves show maxima and minima at almost the same values of ω_UH/ω_ce as the growth rates. Furthermore, we find that the saturation energy of the upper-hybrid waves is proportional to the density of hot electrons. The maximum saturated energy can be up to one percent of the kinetic energy of hot electrons. All these findings can be used in the interpretation of solar radio zebras. Comment: 8 pages, 12 figure...|$|R
40|$|Calculations on the D + HBr → DBr + H and D + HI → DI + H {{reactions}} are reported. A three-dimensional, quantum-dynamical approximation is used which involves applying the energy sudden approximation {{to the entrance}} channel hamiltonian and the centrifugal sudden approximation to the exit channel hamiltonian. Results of integral and differential cross sections, rate coefficients and rotational distributions are presented. Diatomics-in-molecules potential-energy surfaces {{have been used in}} the computations. The HBrH potential has been optimesed so that the calculated room-temperature rate coefficient agrees with experiment. This potential has a barrier height of 0. 237 eV. <b>Rate</b> coefficient <b>computations</b> <b>for</b> the four reactions H′ + H″ Cl → - H′Cl + H″ (H′, H″ = H or D) are also reported. These results, for a LEPS surface, agree well with those obtained in quasiclassical trajectory and variational transition state theory calculations. © 1982...|$|R
40|$|The {{tolerable}} erasure error <b>rate</b> <b>for</b> scalable quantum <b>computation</b> {{is shown}} {{to be at least}} 0. 292, given standard scalability assumptions. This bound is obtained by implementing computations with generic stabilizer code teleportation steps that combine the necessary operations with error correction. An interesting consequence of the technique is that the only errors that affect the maximum tolerable error rate are storage and Bell measurement errors. If storage errors are negligible, then any detected Bell measurement error below 1 / 2 is permissible. Another consequence of the technique is that the maximum tolerable depolarizing error rate is dominated by how well one can prepare the required encoded states. For example, if storage and Bell measurement errors are relatively small, then independent depolarizing errors with error rate close to 0. 1 per qubit are tolerable in the prepared states. The implementation overhead is dominated by the efficiency with which the required encoded states can be prepared. At present, this efficiency is very low, particularly <b>for</b> error <b>rates</b> close to the maximum tolerable ones. Comment: 16 pages. Version 2 corrects errors in the stabilizer code revie...|$|R
40|$|This report {{provides}} inflation adjusted {{excise tax}} <b>rates</b> <b>for</b> alcohol, tobacco, and gasoline products. The base <b>for</b> <b>computation</b> is November 1951; the adjustments show what the tax rates {{would be if}} they had been increased to reflect inflation. All of the above cited commodities had <b>rate</b> increases effective <b>for</b> that date under the Revenue Act of 1951. Just as the Congress was prepared to lower excise tax rates because of peacetime conditions, plans had to be revised {{as a result of the}} start of the Korean War. Thus, the Revenue Act of 1951 was born out of revenue needs due to increased military expenditures...|$|R
40|$|Much {{attention}} {{had been}} devoted to study the bit error rate (BER) of hierarchical modulation on point-to-point transmissions in recent years. But few mention the error performance of hierarchical modulation on cooperative communication systems. Thus, in this paper, we derive the explicit closed-form expressions of the exact bit error <b>rate</b> <b>computation</b> <b>for</b> cooperative communication systems with hierarchical modulation over additive white Gaussian noise (AWGN) channels and Rayleigh fading channels. To obtain the theoretical formulas of error rate performance, an analyzing model is proposed to clearly express all possibilities of each transmission. <b>For</b> error <b>rate</b> performance, the expression for BER {{is a function of}} distance parameters. Based on this relationship, a criterion is proposed to choose the optimal distance parameters for minimizing the BER of the refinement bits while guaranteeing the BER requirement of the base bits. Simulation results validate the correctness of the derived BERs of the base bits and the refinement bits. Also from simulation results, the performance with optimal distance parameters is greatly improved compared to the non-optimal distance parameters...|$|R
40|$|A {{method is}} {{presented}} <b>for</b> <b>computation</b> of positive realizations for given impulse response matrices of discrete-time systems. Sufficient {{conditions for the}} existence of positive realizations are established and a procedure <b>for</b> <b>computation</b> of positive realizations for given impulse response matrices is proposed...|$|R
40|$|A method <b>for</b> <b>computation</b> of the of PID {{controllers}} for an LTI {{system with}} time-delay {{is presented in}} this paper. A systematic, transparent and generic method <b>for</b> <b>computation</b> of the whole set of Hurwitz-stabilizers is developed. The method is based on decoupling of PID controller parameters at singular frequencies and is well suited <b>for</b> the <b>computation</b> of the simultaneous stabilizers for a given multi-model with time delay...|$|R
40|$|In this paper, {{we propose}} unified {{systolic}} arrays <b>for</b> <b>computation</b> of the 1 -D and 2 -D discrete cosine transform/discrete sine transform/discrete Hartley transform (DCT/DST/DHT). By decomposing the transforms into even- and odd-numbered frequency samples, the proposed architecture computes the 1 -D DCT/DST/DHT. Compared {{to the conventional}} methods the proposed systolic arrays exhibit advantages {{in terms of the}} number of PE's and latency. We generalize the proposed structure <b>for</b> <b>computation</b> of the 2 -D DCT/DST/DHT. The unified systolic arrays can be employed <b>for</b> <b>computation</b> of the inverse DCT/DST/DHT (IDCT/IDST/IDHT) ...|$|R
5000|$|A {{presentation}} is useful <b>for</b> <b>computation.</b> <b>For</b> example, since tensoring is right-exact, tensoring the above presentation with a module, say, N gives: ...|$|R
5000|$|Center <b>for</b> <b>Computation</b> and Technology at Louisiana State University ...|$|R
5000|$|... · Dataflow models <b>for</b> <b>computation</b> and {{refinement}} and exptension ...|$|R
5000|$|... #Subtitle level 2: Software <b>for</b> <b>computation</b> of {{collective}} effects ...|$|R
50|$|Artem Polyvyanyy, Jussi Vanhatalo, and Hagen Voelzer (2010) {{proposed}} a simplified algorithm <b>for</b> <b>computation</b> of the RPST. This simplified algorithm {{can be employed}} in a straightforward way as a subroutine <b>for</b> <b>computation</b> of the RPST of an arbitrary program/workflow graph. Both algorithms, the original and the simplified one, allow <b>for</b> an eﬃcient <b>computation</b> of the RPST. However, they provide different structural characterizations of canonical SESE fragments.|$|R
2500|$|... at [...] Contains {{extensive}} library <b>for</b> <b>computations</b> with Braid Groups ...|$|R
5000|$|Aureal Vortex - {{dedicated}} chip <b>for</b> <b>computation</b> {{of audio}} effects ...|$|R
5000|$|... #Subtitle level 3: Part 3: Tables <b>for</b> <b>computation</b> of eclipses ...|$|R
