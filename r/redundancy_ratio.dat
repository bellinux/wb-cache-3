32|12|Public
5000|$|... 1). If perfect-reconstruction {{filters are}} used for both the LP {{decomposition}} and DFB, then the discrete contourlet transform can reconstruct the original image perfectly, which means it provides a frame operator.2). If orthogonal filters {{are used for}} both the LP decomposition and DFB, then the discrete contourlet transform provides a tight frame which bounds equal to 1.3). The upper bound for the <b>redundancy</b> <b>ratio</b> of the discrete contourlet transform is [...]4). If the [...] pyramidal level of LP applies to [...] level DFB, the basis images of the contourlet transform have the size of [...] ≈ [...] and [...] ≈ [...] 5). When FIR is used, the computational complexity of the discrete contourlet transform is [...] for N-pixel images.|$|E
50|$|Filter banks play {{important}} roles in signal processing.They are used in many areas, such as signal and image compression, and processing.The main use of filter banks is to divide a signal or system in to several separate frequency domains. Different filter designs can be used depending on the purpose.In this page we provide information regarding filter banks, multidimensional filter banks and different methods to design multidimensional filters.Also we talked about NDFB, which is built upon an efficient tree-structured construction, {{which leads to a}} low <b>redundancy</b> <b>ratio</b> and refinable angular resolution.By combining the NDFB with a new multiscale pyramid, we can construct the surfacelet transform, which has potential in efficiently capturing and representing surface-like singularities in multidimensional signals.AS mentioned above NDFB and surfacelet transform have applications in various areas that involve the processing of multidimensional volumetric data, including video processing, seismic image processing, and medical image analysis.Some other advantages of NDFB can be addressed as follow:Directional decomposition, Construction, Angular resolution, Perfect reconstruction, and Small redundancy.|$|E
50|$|Filter banks play an {{important}} role in different aspects of signal processing these days.They have different usage in many areas, such as signal and image compression, and processing.The main usage of filter banks is that in this way we can divide the signal or system to several separate frequency components.Depending on our purpose we can choose different methods to design the filters.In this page we provide information regarding filter banks, multidimensional filter banks and different methods to design multidimensional filters.Also we talked about MDFB, which is built upon an efficient tree-structured construction, which leads to a low <b>redundancy</b> <b>ratio</b> and refinable angular resolution.By combining the MDFB with a new multiscale pyramid, we can constructed the surfacelet transform, which has potentials in efficiently capturing and representing surface-like singularities in multidimensional signals.MDFB and surfacelet transform have applications in various areas that involve the processing of multidimensional volumetric data, including video processing, seismic image processing, and medical image analysis.Some other advantages of MDFB include:Directional decomposition, Construction, Angular resolution, Perfect reconstruction, and Small redundancy.|$|E
3000|$|Because of the {{partitioning}} of {{both the}} SO and the c 2 v memories, Arch. V-B needs more logic resources and more memory bits than Arch. V-C (both for data and configuration). The <b>redundancy</b> <b>ratios</b> [...]...|$|R
40|$|The speech {{messages}} {{issued by}} voice warning systems {{must be carefully}} designed in accordance with general principles of human decision making processes, human speech comprehension, and {{the conditions in which}} the warnings can occur. The operator's effectiveness must not be degraded by messages that are either inappropriate or difficult to comprehend. Important experimental variables include message content, linguistic <b>redundancy,</b> signal/noise <b>ratio,</b> interference with concurrent tasks, and listener expectations generated by the pragmatic or real world context in which the messages are presented...|$|R
40|$|We derive an {{explicit}} lower bound on {{the capacity of}} the discrete amplitude–constrained Gaussian channel by proving the existence of tight frames that permit redundant vector representations with small coefficients. Our method encodes the information in subspaces that are optimal in terms of the power to amplitude ratio. In a recent paper, Lyubarskii and Vershynin discuss how the work of Kashin (1977) implies the existence of such representations, and they term them Kashin respresentations. We use this work from frame theory to address the relationship between signal <b>redundancy,</b> peak–to–average power <b>ratio</b> and achievable data rates. 1...|$|R
40|$|The <b>redundancy</b> <b>ratio</b> of a {{function}} {{is defined as}} the ratio of the length of its longest sumof -products expression to the length of its shortest sum-of-products expression. For a given function f, its <b>redundancy</b> <b>ratio</b> will be denoted by ae(f). In [8] it was shown that ae(f) is less than 2 for boolean functions, and in some cases is a linear function of the number of input variables. In this paper we prove that ae(f) 2 n with high probability for a boolean function of n-variables...|$|E
40|$|Caching popular {{contents}} at {{the edge}} of cellular networks has been proposed to reduce the load, and hence the cost of backhaul links. It is significant to decide which files should be cached and where to cache them. In this paper, we propose a distributed caching scheme considering the tradeoff between the diversity and redundancy of base stations' cached contents. Whether it is better to cache the same or different contents in different base stations? To find out this, we formulate an optimal redundancy caching problem. Our goal is to minimize the total transmission cost of the network, including cost within the radio access network (RAN) and cost incurred by transmission to the core network via backhaul links. The optimal <b>redundancy</b> <b>ratio</b> under given system configuration is obtained with adapted particle swarm optimization (PSO) algorithm. We analyze the impact of important system parameters through Monte-Carlo simulation. Results show that the optimal <b>redundancy</b> <b>ratio</b> is mainly influenced by two parameters, which are the backhaul to RAN unit cost ratio and the steepness of file popularity distribution. The total cost can be reduced by up to 54 % at given unit cost ratio of backhaul to RAN when the optimal <b>redundancy</b> <b>ratio</b> is selected. Under typical file request pattern, the reduction amount can be up to 57 %...|$|E
40|$|In this paper, {{we develop}} a novel method to detect salient points in an image from a multiresolution representation. Our {{contribution}} is twofold. Firstly, the multiscale rep-resentation {{results from the}} Dual Tree Wavelet Transform (DTWT) since it enables a great directional selectivity with a reduced <b>redundancy</b> <b>ratio.</b> The second novelty of our work relies on the reliable outliers statistical tests that we apply to detect salient points from the DTWT coefficients. The exper-iments show the robustness of the approach to noise. 1...|$|E
40|$|Combining {{conditional}} probabilities {{based on}} logratios is considered and {{compared with other}} integration models. Each data set is at first treated separately and merged with considering data source <b>redundancies.</b> Permanence of <b>ratios</b> and tau-model are presented {{as a way of}} merging conditional probabilities and their results are compared to those of logratio model. In logratios model, redundancy factors are iteratively optimized in order to improve the integrated estimate. Measure of goodness is used to quantitatively evaluate the models of integration and logratio model gives the best experimental results in terms of local uncertainty and closeness to true facies...|$|R
40|$|In {{long-term}} evoluation (LTE) femtocell networks, hysteresis {{is one of}} {{the main}} parameters which affects the performance of handover with a number of unnecessary handovers, including ping-pong, early, late and incorrect handovers. In this study, the authors propose a hybrid algorithm that aims to obtain the optimised unique hysteresis for an individual mobile user moving at various speeds during the inbound handover process. This algorithm is proposed for two-tier scenarios with macro and femto. The centralised function in this study evaluates the overall handover performance indicator. Then, the handover aggregate performance indicator (HAPI) is used to determine an optimal configuration. Based on the received reference signal-to-interference-plus-noise ratio, the distributed function residing on the user equipment (UE) is able to obtain an optimal unique hysteresis for the individual UE. Theoretical analysis with three indication boundaries is provided to evaluate the proposed algorithm. A system-level simulation is presented, and the proposed algorithm outperformed the existing approaches in terms of handover failure, call-drop and <b>redundancy</b> handover <b>ratios</b> and also achieved better overall system performance...|$|R
40|$|At the NASA Glenn Research Center, NASA Langley Research Center's Flight Optimization System (FLOPS) and {{the design}} {{optimization}} testbed COMETBOARDS with regression and neural-network-analysis approximators have been coupled to obtain a preliminary aircraft design methodology. For a subsonic aircraft, the optimal design, that is the airframe-engine combination, is obtained by the simulation. The aircraft is powered by two high-bypass-ratio engines with a nominal thrust of about 35, 000 lbf. It is to carry 150 passengers at a cruise speed of Mach 0. 8 over a range of 3000 n mi and to operate on a 6000 -ft runway. The aircraft design utilized a neural network and a regression-approximations-based analysis tool, along with a multioptimizer cascade algorithm that uses sequential linear programming, sequential quadratic programming, the method of feasible directions, and then sequential quadratic programming again. Optimal aircraft weight versus the number of design iterations is shown. The central processing unit (CPU) time to solution is given. It is shown that the regression-method-based analyzer exhibited a smoother convergence pattern than the FLOPS code. The optimum weight obtained by the approximation technique and the FLOPS code differed by 1. 3 percent. Prediction by the approximation technique exhibited no error for the aircraft wing area and turbine entry temperature, whereas it was within 2 percent for most other parameters. Cascade strategy was required by FLOPS {{as well as the}} approximators. The regression method had a tendency to hug the data points, whereas the neural network exhibited a propensity to follow a mean path. The performance of the neural network and regression methods was considered adequate. It was at about the same level for small, standard, and large models with <b>redundancy</b> <b>ratios</b> (defined as the number of input-output pairs to the number of unknown coefficients) of 14, 28, and 57, respectively. In an SGI octane workstation (Silicon Graphics, Inc., Mountainview, CA), the regression training required a fraction of a CPU second, whereas neural network training was between 1 and 9 min, as given. For a single analysis cycle, the 3 -sec CPU time required by the FLOPS code was reduced to milliseconds by the approximators. For design calculations, the time with the FLOPS code was 34 min. It was reduced to 2 sec with the regression method and to 4 min by the neural network technique. The performance of the regression and neural network methods was found to be satisfactory for the analysis and design optimization of the subsonic aircraft...|$|R
40|$|Single phased burst {{correcting}} quasicyclic {{codes of}} rate 1 / 2, 2 / 3, 3 / 4, and 4 / 5 are constructed which are asymptotically optimal. They {{are capable of}} being decoded efficiently and have minimal guard space. One decoding method {{is similar to that}} used for the optimal type-B 2 Berlekamp—Preparata—Massey convolutional code. Also two rate 1 / 2 quasicyclic codes having an asymptotic burst to <b>redundancy</b> <b>ratio</b> of 2 / 5 are given which can be threshold decoded...|$|E
30|$|The {{usage of}} the {{confidence}} intervals was demonstrated through Monte Carlo experiments on a real dataset of the 6 -bus and IEEE 14 -bus power systems for both small and large sample sizes. The confidence intervals were constructed for the test networks for the sample of measurements 18, 28, 44 and 68 based on the <b>redundancy</b> <b>ratio</b> R. The proposed interval estimates outperformed for the sample sizes of 28 in the 6 bus and 68 in the IEEE 14 -bus systems, respectively. The poor performance for the constructed interval estimates have been reported even for the large sample sizes {{in the existence of}} contaminated measurements.|$|E
40|$|International audienceLow-power and Lossy Networks (LLNs), like {{wireless}} networks {{based upon}} the IEEE 802. 15. 4 standard, have strong energy constraints, and are moreover subject to frequent transmission errors, not only due to congestion but also to collisions and to radio channel conditions. This paper introduces an analytical model to compute the total energy consumption in an LLN due to the TCP protocol. The model allows us to highlight some tradeoffs as regards {{the choice of the}} TCP maximum segment size, of the Forward Error Correction (FEC) <b>redundancy</b> <b>ratio,</b> and of the number of link-layer retransmissions, in order to minimize the total energy consumption...|$|E
40|$|This paper {{extends the}} {{classical}} model of Ushakov on redundancy optimization of a series-parallel static coherent reliability systems with uncertainty in system parameters. Our objective function represents the total {{capacity of a}} series-parallel static system, while the decision parameters are the nominal capacity {{and the availability of}} the elements. We obtain explicit expressions (both analytical and via efficient simulation) for the constraint of the program, namely for the cdf of the total capacity of the system, and then show that the extended program is a convex mixed integer one. Depending on whether the objective function and the associated constraints are analytically available or not, we suggest using deterministic and stochastic (simulation) optimization approaches, respectively. The last case is associated with likelihood ratios (change of probability measure). Numerical results are presented as well. Keywords. Likelihood <b>Ratios,</b> <b>Redundancy</b> Optimization, Reliability Networks, Sensitivity Analysis, Simulation. 0 Version date November 10, 1998. The work of Reuven Rubinstein was supported by the Henri Gutwirth Fund for the Promotion of Research Contents...|$|R
40|$|Objective: To {{develop and}} {{validate}} a Brazilian Portuguese {{version of the}} Premenstrual Symptoms Screening Tool (PSST), a questionnaire used for the screening of premenstrual syndrome (PMS) and of the most severe form of PMS, premenstrual dysphoric disorder (PMDD). The PSST also rates the impact of premenstrual symptoms on daily activities. Methods: A consecutive sample of 801 women aged &# 8805; 18 years completed the study protocol. The internal consistency, test-retest reliability, and content validity of the Brazilian PSST were determined. The independent association of a positive screen for PMS or PMDD {{and quality of life}} determined by the World Health Organization Quality of Life instrument-Abbreviated version (WHOQOL-Bref) was also assessed. Results: Of 801 participants, 132 (16. 5 %) had a positive screening for PMDD. The Brazilian PSST had adequate internal consistency (Cronbach&# 8217;s alpha = 0. 91) and test-retest reliability. The PSST also had adequate convergent/discriminant validity, without <b>redundancy.</b> Content validity <b>ratio</b> and content validity index were 0. 61 and 0. 94 respectively. Finally, a positive screen for PMS/PMDD was associated with worse WHOQOL-Bref scores. Conclusions: These findings suggest that PSST is a reliable and valid instrument to screen for PMS/PMDD in Brazilian women...|$|R
40|$|Pattern {{models for}} analyzing, representing, and {{compressing}} experimental 2 D fluid flow imagery are developed. The {{approach is to}} represent flow fields using the tools of dynamical systems theory. Complex flow fields are decomposed into simple components based on the observed critical point behavior. A critical point detector based on the vector field index is developed, and its performance is analyzed. The critical point behavior is modeled by the linear phase portrait, which is a compact representation specified by a 2 x 2 A matrix. The eigenvalues of A provide a symbolic descriptor, and this allows the qualitative behavior of the critical points to be classified into one of six canonical forms. The global flow field behavior is represented using the linear phase portrait estimates as a basis. First, a linear superposition approach is considered, where the flow field is modeled using the tools of potential theory. This technique is appropriate for the representation of incompressible flows with negligible friction effects. A second approach is considered in which complex flows are modeled by nonlinear dynamical systems. A Taylor series model is assumed for the velocity components, and the model coefficients are computed by considering both local critical point and global flow field behavior. A merge and split procedure for complex flows is presented, in which patterns of neighboring critical point regions are combined and modeled. The computed models are then employed to compress scalar flow images that exhibit little or gradual variation along the flow streamlines. The model serves {{as a guide for}} removing this <b>redundancy,</b> and compression <b>ratios</b> on the order of 100 : 1 are achieved. Finally, the compression of vector field data using orthogonal polynomials derived from the Taylor series model is considered. A critical point scheme and a block transform are presented. They are applied to velocity fields measured in particle image velocimetry experiments and generated by computer simulations, and compression ratios ranging from 15 : 1 to 100 : 1 are achieved...|$|R
40|$|This {{report is}} {{concerned}} with the logical design of economical com-binational switching networks which contain sufficient redundancy so that the presence of any single fault can be detected. It is well known that (within broad limits) any isolated fault in a single-output network may be detected with about 2 : 1 redundancy, through duplication of the irredundant network, and the addition of a comparator gate. In the procedures to be described, a reduction of the <b>redundancy</b> <b>ratio</b> below 2 :l is shown to be usually possible, by (1) taking advantage of any inherent logical redundancy in the switching function or functions which describe the terminal behavior of the network; (2) making use of any structural redundancy present in the irredundant version of the network; (3) providing for the detection of only those faults considered to be at all likely to occur, rather than “all ” faults categorically; (4) where possible, allowing some faults to be detected with a delay, rather than at the first occurrence of an erroneous output; and (5) application of certain principles of error-detecting codes. A similar reduction of the <b>redundancy</b> <b>ratio</b> is possible for multi-output networks. Branch-type networks-e. g., those made up of relay contacts or cryotrons instead of gate- type elements [...] are also considered. If the designer is free to use some non-branch-type elements in the detection circuitry, the same low redundancy ratios achievable in gate-type circuitry can be obtained here. Finally, several examples in application areas of importance in conventional digital systems are discussed, including some comparators, calculating networks, code converters, and decoding trees. i...|$|E
40|$|This paper {{introduces}} a redundancy adaptation algorithm {{based on an}} on-the-fly erasure network coding scheme named Tetrys {{in the context of}} real-time video transmission. The algorithm exploits the relationship between the <b>redundancy</b> <b>ratio</b> used by Tetrys and the gain or loss in encoding bit rate from changing a video quality parameter called the Quantization Parameter (QP). Our evaluations show that with equal or less bandwidth occupation, the video protected by Tetrys with redundancy adaptation algorithm obtains a PSNR gain up to or more than 4 dB compared to the video without Tetrys protection. We demonstrate that the Tetrys redundancy adaptation algorithm performs well with the variations of both loss pattern and delay induced by the networks. We also show that Tetrys with the redundancy adaptation algorithm outperforms traditional block-based FEC codes with and without redundancy adaptation...|$|E
40|$|AbstractThe {{necessity}} of having optimum camera activation is rapidly increasing in distributed environment {{as well as}} wireless sensor networks. In the current research, we have studied the event boundary detection approach for redundant data minimization by actuating less number of cameras. The study reveals {{that some of the}} cameras those are present outside the exact event boundary are activated unnecessarily being informed from the boundary scalars regarding the event occurrence. This unnecessary activation of outer cameras leads to additional energy expenditure and redundant data transmission. In this paper, we have proposed a novel approach to maintain such unnecessarily actuated cameras in turned off state. The experimental evaluation in terms of less camera activation, minimized <b>redundancy</b> <b>ratio,</b> enhanced coverage ratio and reduced energy consumption, obtained from the investigation justifies the effectiveness of the proposed approach as compared to another approach recently proposed in the literature...|$|E
40|$|AbstractConsider a {{scenario}} in which an algorithmic machine, M, is being fed the graph of a computable function ƒ. M is said to finitely identify ƒ just in case, after inspecting a finite portion of the graph of ƒ, it emits its first conjecture, which is a program for ƒ, and it never abandons this conjecture thereafter. A team of machines is a multiset of such machines. A team {{is said to be}} successful just in case each member of some nonempty subset, of predetermined size, of the team is successful, The ratio of the number of machines required to be successful {{to the size of the}} team is referred to as the success ratio of the team. The present paper investigates the finite identification of computable functions by teams of learning machines. The results presented complete the picture for teams with success ratio 12 and greater. It is shown that at success <b>ratio</b> 12, introducing <b>redundancy</b> in the team can result in increased learning power. In particular, it is established that larger collections of functions can be learned by employing teams of 4 machines and requiring at least 2 to be successful than by employing teams of 2 machines and requiring at least 1 to be successful. Surprisingly, it is also shown that introducing further <b>redundancy</b> at success <b>ratio</b> 12 does not yield any extra learning power. In particular, it is shown that the collections of functions that can be finitely identified by a team of 2 m machines requiring at least m to be successful is the same as: •the collections of functions that can be finitely identified by a team of 4 machines requiring at least 2 to be successful, if is even, and•the collections of functions that can be identified by a team of 2 machines requiring at least 1 to be successful, if is odd. These latter results require development of sophisticated simulation techniques...|$|R
40|$|A {{wireless}} sensor-actuator network (WSAN) {{is composed}} of sensor modes and actuator modes which are interconnected in wireless networks. A sensor node collects information on the physical world and sends a sensed value in a wireless network. Another sensor node forwards the sensed value to deliver to an actuator node. A sensor node can deliver messages with sensed values to only nearby nodes due to weak radio. Messages are forwarded by sensor nodes to an actuator node by a type of flooding protocol. A sensor mode senses an event and sends a message with the sensed value. In addition, on receipt of a message with a sensed value from another sensor mode, a sensor node forwards the sensed value. Messages transmitted by sensor nodes might be lost due to noise and collisions. In this paper, we discuss a redundant data transmission (RT) protocol to reliably and efficiently deliver sensed values sensed by sensor nodes to an actuator node. Here, a sensor node sends a message with not only its sensed value but also sensed values received from other sensor nodes. The more number of sensed values are included in a message, the more frequently the message is lost. Each message carries so many number of sensed values that the message loss ratio is not increased. Even if a message with a sensed value v is lost in the wireless network, an actuator node can receive the sensed value v from a message sent by another sensor node. Thus, each sensed value is redundantly carried in multiple messages. The redundancy of a sensed value is in nature increased since the sensed value is broadcast. In {{order to reduce the}} redundancy of sensed value, we take a strategy that the farther sensor nodes from an actuator node forward the fewer number of sensed values. We evaluate the RT protocol in terms of loss <b>ratio,</b> <b>redundancy,</b> and delay time of a sensed value. We show that about 80 % of sensed values can be delivered to an actuator node even if 95 % of messages are lost due to noise and collision...|$|R
30|$|Due to {{the high}} {{redundancy}} and massive calculation of nonsampled wavelet, DTCWT, one that is constructed through a pair of wavelet trees, is proposed in[8]. Although DTCWT is an invertible quasi shift invariant and its 1 -D case a redundancy of 2, the design of these quadrature wavelet pairs is so complicated {{that it can be}} done only through approximations. It means that the DTCWT requires special mother wavelet function. To overcome this restraint, in[7], Firoiu has proposed a new shift invariant called HWT using Hilbert transform and a two-stage mapping-based complex wavelet transform (MBCWT) in soft space[9]. And she also gives the proof that HWT is equivalent to DTCWT. That means that HWT’s <b>redundancy</b> <b>ratio</b> is 2, {{the same as that of}} DTCWT. Moreover, HWT can be realized through classical mother wavelet function like those conceived by Daubechies. Using this method, we can get a higher degree of shift invariance and a better directional selectivity[9].|$|E
40|$|An {{important}} {{problem to}} be addressed by diagnostic systems in industrial applications is the estimation of faults with incomplete observations. This work discusses different approaches for handling missing data, and performance of data-driven fault diagnosis schemes. An exploiting classifier and combined methods were assessed in Tennessee-Eastman process, for which diverse incomplete observations were produced. The use of several indicators revealed the trade-off between performances of the different schemes. Support vector machines (SVM) and C 4. 5, combined with k-nearest neighbourhood (kNN), produce the highest robustness and accuracy, respectively. Bayesian networks (BN) and centroid appear as inappropriate options in terms of accuracy, while Gaussian naive Bayes (GNB) is sensitive to imputation values. In addition, feature selection was explored for further performance enhancement, and the proposed contribution index showed promising results. Finally, an industrial case was studied to assess informative level of incomplete data in terms of the <b>redundancy</b> <b>ratio</b> and generalize the discussion. (C) 2015 Elsevier Ltd. All rights reserved. Peer ReviewedPostprint (author's final draft...|$|E
30|$|Figure 8 {{shows the}} <b>redundancy</b> <b>ratio</b> for both precopy and TPO with all four workloads. From the figure {{it is clear}} that the {{overhead}} induced by the TPO method is significantly less as compared to precopy method. Less overhead makes TPO method more efficient in terms of bandwidth utilization and migration time. Other than this, TPO is also analyzed for space and processing overheads. The TPO uses TO_SEND_h[i] array to store the count for each memory page, based on number of times it is updated. Majority of times, it will have less entries than total number of pages and in worst case, it will be equal to the total number of pages of virtual machine. In worst case, space overhead 0.05 % (for page size 4 KB) is used to store the TO_SEND_h[i] array, which is very less and easily bearable. All the processing (prediction of frequently updated pages) are performed on the host machine, so there is no impact of processing on virtual machine. Memory allocation and de-allocation is dynamic in nature and hence, automatically freed after virtual machine migration. Similar bitmaps have also been used in other works (Svärd et al. 2011; Hu et al. 2011; Ma et al. 2010).|$|E
40|$|Abstract—Determining how to {{transport}} delay-sensitive voice data {{has long been}} a problem in multimedia networking. The difficulty arises because voice and best-effort data are different by nature. It would not be fair to give priority to voice traffic and starve its best-effort counterpart; however, the voice data delivered might not be perceptible if each voice call is limited to the rate of an average TCP flow. To address the problem, we approach it from a user-centric perspective by tuning the voice data rate based on user satisfaction. Our contribution in this work is threefold. First, we investigate how Skype, the largest and fastest growing VoIP service on the Internet, adapts its voice data rate (i. e., the <b>redundancy</b> <b>ratio)</b> to network conditions. Second, by exploiting implementations of public domain codecs, we discover that Skype’s mechanism is not really geared to user satisfaction. Third, based on a set of systematic experiments that quantify user satisfaction under different levels of packet loss and burstiness, we derive a concise model that allows user-centric redundancy control. The model can be easily incorporated into general VoIP services (not only Skype) to ensure consistent user satisfaction...|$|E
40|$|IEEE Comput. Soc. Tech. Comm. Real-Time Syst. Low-power Wireless Networks (LWNs) {{have become}} {{increasingly}} available for mission-critical applications such as security surveillance and disaster response. In particular, emerging low-power wireless audio platforms provide an economical solution for ad hoc voice communication in emergency scenarios. In this paper, we develop a system called Adaptive Stream Multicast (ASM) for voice communication over multi-hop LWNs. ASM is composed of several novel components specially designed to deliver robust voice quality for multiple sinks in dynamic environments: 1) an empirical model to automatically evaluate the voice quality perceived at sinks based on current network condition, 2) a feedback-based Forward Error Correction scheme where the source can adapt its coding <b>redundancy</b> <b>ratio</b> dynamically {{in response to the}} voice quality variation at sinks, 3) a Tree-based Opportunistic Routing (TOR) protocol that fully exploits the broadcast opportunities on a tree based on novel forwarder selection and coordination rules, and 4) a distributed admission control algorithm that ensures the voice quality guarantees when admitting new voice streams. ASM has been implemented on a low-power hardware platform and extensively evaluated through experiments on a testbed of 18 nodes...|$|E
40|$|Senecio scandens Buch. -Ham. ex D. Don, an {{important}} antibacterial source of Chinese traditional medicine, has a widespread distribution {{in a few}} ecological habitats of China. We generated a full-length complementary DNA (cDNA) library from a sample of elite individuals with superior antibacterial properties, with satisfactory parameters such as library storage (4. 30 £ 106 CFU), efficiency of titre (1. 30 £ 106 CFU/mL), transformation efficiency (96. 35 %), full-length ratio (64. 00 %) and <b>redundancy</b> <b>ratio</b> (3. 28 %). The BLASTN search revealed the facile formation of counterparts between the experimental sample and Arabidopsis thaliana in view of high-homology cDNA sequence (90. 79 %) with e-values < 1 e 50. Sequence similarities to known proteins indicate that the entire sequences of the full-length cDNA clones consist of the major of functional genes identified by a large set of microarray data from the present experimental material. For other Compositae species, a large set of full-length cDNA clones reported in the present article {{will serve as a}} useful resource to facilitate further research on the transferability of expressed sequence tag-derived simple sequence repeats (EST-SSR) development, comparative genomics and novel transcript profiles...|$|E
40|$|Low-power Wireless Networks (LWNs) {{have become}} {{increasingly}} available for mission-critical applications such as security surveillance and disaster response. In particular, emerging low-power wireless audio platforms provide an economical solution for ad hoc voice communication in emergency scenarios. In this paper, we develop a system called Adaptive Stream Multicast (ASM) for voice communication over multihop LWNs. ASM is composed of several novel components specially designed to deliver robust voice quality for multiple sinks in dynamic environments: 1) an empirical model to automatically evaluate the voice quality perceived at sinks based on current network condition; 2) a feedback-based Forward Error Correction (FEC) scheme where the source can adapt its coding <b>redundancy</b> <b>ratio</b> dynamically {{in response to the}} voice quality variation at sinks; 3) a Tree-based Opportunistic Routing (TOR) protocol that fully exploits the broadcast opportunities on a tree based on novel forwarder selection and coordination rules; and 4) a distributed admission control algorithm that ensures the voice quality guarantees when admitting new voice streams. ASM has been implemented on a low-power hardware platform and extensively evaluated through experiments on a test bed of 18 nodes. The experiment results show that ASM can achieve satisfactory multicast voice quality in dynamic environments while incurring low-communication overhead. © 2012 IEEE. Program of National Natural Science of China 60933011, 61073014; State Key Development Program for Basic Research of China 2011 CB 302902; US National Science Foundation (NSF) CNS- 0916576 Low-power Wireless Networks (LWNs) {{have become increasingly}} available for mission-critical applications such as security surveillance and disaster response. In particular, emerging low-power wireless audio platforms provide an economical solution for ad hoc voice communication in emergency scenarios. In this paper, we develop a system called Adaptive Stream Multicast (ASM) for voice communication over multihop LWNs. ASM is composed of several novel components specially designed to deliver robust voice quality for multiple sinks in dynamic environments: 1) an empirical model to automatically evaluate the voice quality perceived at sinks based on current network condition; 2) a feedback-based Forward Error Correction (FEC) scheme where the source can adapt its coding <b>redundancy</b> <b>ratio</b> dynamically in response to the voice quality variation at sinks; 3) a Tree-based Opportunistic Routing (TOR) protocol that fully exploits the broadcast opportunities on a tree based on novel forwarder selection and coordination rules; and 4) a distributed admission control algorithm that ensures the voice quality guarantees when admitting new voice streams. ASM has been implemented on a low-power hardware platform and extensively evaluated through experiments on a test bed of 18 nodes. The experiment results show that ASM can achieve satisfactory multicast voice quality in dynamic environments while incurring low-communication overhead. © 2012 IEEE...|$|E
40|$|OBJECTIVE: This study {{evaluated}} the accuracy in localisation {{and distribution of}} real-time three-dimensional (4 -D) ultrasound-guided biopsies on a prostate phantom. METHODS: A prostate phantom was created. A three-dimensional real-time ultrasound system with a 5. 9 MHz probe was used, {{making it possible to}} see several reconstructed orthogonal viewing planes in real time. Fourteen operators performed biopsies first under 2 -D then 4 -D transurethral ultrasound (TRUS) guidance (336 biopsies). The biopsy path was modelled using segmentation in a 3 -D ultrasonographic volume. Special software was used to visualise the biopsy paths in a reference prostate and assess the sampled area. A comparative study was performed to examine the accuracy of the entry points and target of the needle. Distribution was assessed by measuring the volume sampled and a <b>redundancy</b> <b>ratio</b> of the sampled prostate. RESULTS: A significant increase in accuracy in hitting the target zone was identified using 4 -D ultrasonography as compared to 2 -D. There was no increase in the sampled volume or improvement in the biopsy distribution with 4 -D ultrasonography as compared to 2 -D. CONCLUSION: The 4 -D TRUS guidance appears to show, on a synthetic model, an improvement in location accuracy and in the ability to reproduce a protocol. The biopsy distribution does not seem improved...|$|E
40|$|In this paper, {{we propose}} a new {{approach}} to construct a 2 -dimensional (2 -D) directional filter bank (DFB) by cascading a 2 -D nonseparable checkerboard-shaped filter pair and 2 -D separable cosine modulated filter bank (CMFB). Similar to diagonal subbands in 2 -D separable wavelets, most of the subbands in 2 -D separable CMFBs, tensor products of two 1 -D CMFBs, are poor in directional selectivity {{due to the fact that}} the frequency supports of most of the subband filters are concentrated along two different directions. To improve the directional selectivity, we propose a new DFB to realize the subband decomposition. First, a checkerboard-shaped filter pair is used to decompose an input image into two images containing different directional information of the original image. Next, a 2 -D separable CMFB is applied to each of the two images for directional decomposition. The new DFB is easy in design and has merits: low <b>redundancy</b> <b>ratio</b> and fine directional-frequency tiling. As its application, the BLS-GSM algorithm for image denoising is extended to use the new DFBs. Experimental results show that the proposed DFB achieves better denoising performance than the methods using other DFBs for images of abundant textures. (C) 2008 Elsevier B. V. All rights reserved...|$|E
40|$|Issues {{regarding}} black box, large systems verification are explored. It {{begins by}} collecting data from several testing teams. An integrated database containing test, fault, repair, and source file information is generated. Intuitive effectiveness measures are generated using conventional {{black box testing}} results analysis methods. Conventional analysts methods indicate that the testing was effective {{in the sense that}} as more tests were run, more faults were found. Average behavior and individual data points are analyzed. The data is categorized and average behavior shows a very wide variation in number of tests run and in pass rates (pass rates ranged from 71 percent to 98 percent). The 'white box' data contained in the integrated database is studied in detail. Conservative measures of effectiveness are discussed. Testing efficiency (ratio of repairs to number of tests) is measured at 3 percent, fault record effectiveness (ratio of repairs to fault records) is measured at 55 percent, and test script <b>redundancy</b> (<b>ratio</b> of number of failed tests to minimum number of tests needed to find the faults) ranges from 4. 2 to 15. 8. Error prone source files and subsystems are identified. A correlational mapping of test functional area to product subsystem is completed. A new adaptive testing process based on real-time generation of the integrated database is proposed...|$|E
40|$|International audienceOBJECTIVE: The {{objective}} {{of this study was}} to determine the added value of real-time three-dimensional (4 D) ultrasound guidance of prostatic biopsies on a prostate phantom in terms of the precision of guidance and distribution. METHODS: A prostate phantom was constructed. A real-time 3 D ultrasonograph connected to a transrectal 5. 9 MHz volumic transducer was used. Fourteen operators performed 336 biopsies with 2 D guidance then 4 D guidance according to a 12 -biopsy protocol. Biopsy tracts were modelled by segmentation in a 3 D ultrasound volume. Specific software allowed visualization of biopsy tracts in the reference prostate and evaluated the zone biopsied. A comparative study was performed to determine the added value of 4 D guidance compared to 2 D guidance by evaluating the precision of entry points and target points. The distribution was evaluated by measuring the volume investigated and by a <b>redundancy</b> <b>ratio</b> of the biopsy points. RESULTS: The precision of the biopsy protocol was significantly improved by 4 D guidance (p = 0. 037). No increase of the biopsy volume and no improvement of the distribution of biopsies were observed with 4 D compared to 2 D guidance. CONCLUSION: The real-time 3 D ultrasound-guided prostate biopsy technique on a phantom model appears to improve the precision and reproducibility of a biopsy protocol, but the distribution of biopsies does not appear to be improved...|$|E
40|$|The {{multiscale}} {{directional filter}} bank (MDFB) improves the radial frequency {{resolution of the}} contourlet transform by introducing an additional decomposition in the high-frequency band. The increase in frequency resolution is particularly useful for texture description because of the quasi-periodic property of textures. However, the MDFB needs an extra set of scale and directional decomposition, which is performed on the full image size. The rise in computational complexity is, thus, prominent. In this paper, we develop an efficient implementation framework for the MDFB. In the new framework, directional decomposition {{on the first two}} scales is performed prior to the scale decomposition. This allows sharing of directional decomposition among the two scales and, hence, reduces the computational complexity significantly. Based on this framework, two fast implementations of the MDFB are proposed. The first one can maintain the same flexibility in directional selectivity in the first two scales while the other has the same <b>redundancy</b> <b>ratio</b> as the contourlet transform. Experimental results show that the first and the second schemes can reduce the computational time by 33. 3 %- 34. 6 % and 37. 1 %- 37. 5 %, respectively, compared to the original MDFB algorithm. Meanwhile, the texture retrieval performance of the proposed algorithms is {{more or less the same}} as the original MDFB approach which outperforms the steerable pyramid and the contourlet transform approaches. Department of Electronic and Information Engineerin...|$|E
40|$|International audience—The {{diversity}} of applications' types in Vehicular Ad hoc NETworks (VANETs) {{has spawned a}} large variety of messages {{that need to be}} efficiently disseminated between connected vehicles. The most critical messages are those dedicated for safety applications such as road hazardous warning, signal violation warning, etc. The dissemination of this sort of messages is considered as a challenging task in mobile networks where the topology changes dynamically. Indeed, transmitted messages should achieve a high data reachability within a limited transmission delay and an acceptable overhead in a Vehicle to Vehicle (V 2 V) communication mode. In this work, we focus on a special type of data dissemination protocols based on the delay strategy. The {{purpose of this paper is}} to compare two basic distinguished techniques, namely the slotted technique and the continuous technique, and study in depth their impact on the data dissemination performance. A proper selection of the convenient technique according to the application's requirements is consequently deduced. For a faithful and rigorous study, simulations are performed by means of ns- 3 simulator under a realistic VANET environment in terms of map layout, mobility pattern and radio model. Simulation results show that contrary to the theoretical reflection, slotted technique is approved as the most appropriate one for safety message dissemination. This technique achieves the same packet data ratio and <b>redundancy</b> <b>ratio,</b> compared to the continuous one, while reducing the data transmission delay...|$|E
