57|807|Public
40|$|Bayesian {{alternatives}} to classical tests for Fisher’s exact test in 2 £ 2 contingency tables are considered. Point null test versus one-sided hypothesis is tested using the log odds ratio in 2 £ 2 contingency tables. Hierarchical Bayes, empirical Bayes and noninformative Bayes procedures are {{compared with the}} appropriate classical procedures, either the p-value in Fisher’s exact test or a <b>randomized</b> <b>test.</b> A conjugate prior at the ﬁrst stage and a noninformative prior at the second stage are used for the hyperparameter(s) in the hierarchical approach. For different testing procedures, the likelihood of making a type I error is chosen to be approximately the same. Then the power of different tests is compared: the larger the power, the better the test. In small samples, the <b>randomized</b> <b>test</b> performs well {{in comparison with the}} other methods. For moderate samples, empirical Bayes and <b>randomized</b> <b>test</b> procedures perform better than other approache...|$|E
40|$|A {{method for}} using {{probabilistic}} background knowledge to select features for representing learned concepts is described. The method uses {{a model of}} the tradeoff between accuracy (predictiveness) and simplicity (size) of hypothesis spaces. Preliminary results of using various biases for a learning task in a <b>randomized</b> <b>test</b> domain are discussed...|$|E
40|$|This paper {{proposes a}} pretest {{estimator}} {{of the mean}} of a geometric distribution when a prior point estimate of the mean is available by considering a <b>randomized</b> <b>test</b> of a suitable preliminary hypothesis. Exact results are obtained for the bias and the mean square error of the proposed estimator and these computations are illustrated by a numerical example...|$|E
40|$|We {{explore the}} use of QuickCheck-style <b>randomized</b> <b>testing</b> in {{programming}} languages metatheory, a methodology proposed to reduce development time by revealing shallow errors early, before a formal proof attempt. This exploration begins {{with the development of}} a <b>randomized</b> <b>testing</b> framework for PLT Redex, a domain-specific language for specifying and debugging operational semantics. In keeping with the spirit of Redex, the framework is as lightweight as possible—the user encodes a conjecture as a predicate over the terms of the language, and guided by the structure of the language’s grammar, reduction relation, and metafunctions, Redex attempts to falsify the conjecture automatically. In addition to the details of this framework, we present a tutorial demonstrating its use and two case studies applying it to large language specifications. The first study, a postmortem, applies <b>randomized</b> <b>testing</b> to the formal semantics published with the latest revision of the Scheme language standard. Despite a community review period and a comprehensive, manually-constructed <b>test</b> suite, <b>randomized</b> <b>testing</b> in Redex revealed four bugs in the semantics. The second study presents our experience applying the tool concurrently with the development of a formal model for the MzScheme virtual machine and bytecode verifier. In addition to many errors in our formalization, <b>randomized</b> <b>testing</b> revealed six bugs in the core bytecode verification algorithm in production use. The results of these studies suggest that <b>randomized</b> <b>testing</b> is a cheap and effective technique for finding bugs in large programming language metatheories...|$|R
40|$|<b>Randomized</b> <b>testing</b> is an {{effective}} method for testing software units. Thoroughness of <b>randomized</b> unit <b>testing</b> varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe Nighthawk, a system which uses a genetic algorithm (GA) to find parameters for <b>randomized</b> unit <b>testing</b> that optimize test coverage. Designing GAs is somewhat of a black art. We therefore use a feature subset selection (FSS) tool to assess the size {{and content of the}} representations within the GA. Using that tool, we can prune back 90 % of our GA’s mutators while still achieving most of the coverage found using all the mutators. Our pruned GA achieves almost the same results as the full system, but in only 10 % of the time. These results suggest that FSS for mutator pruning could significantly optimize meta-heuristic search-based software engineering tools. Index Terms Software <b>testing,</b> <b>randomized</b> <b>testing,</b> genetic algorithms, feature subset selection, search-based optimizatio...|$|R
40|$|<b>Randomized</b> <b>testing</b> {{has been}} shown to be an {{effective}} method for testing software units. However, the thoroughness of <b>randomized</b> unit <b>testing</b> varies widely according to the input which is provided by the user. Such as the relative frequencies with which methods are called to be improved the thoroughness of <b>randomized</b> unit <b>testing.</b> In this paper the system, describes genetic algorithm based parameter finding for <b>randomized</b> unit <b>testing</b> that optimizes test coverage. Here the unit test data will be generated by nighthawk system. The system can be viewed as two levels, lower level and upper level. <b>Randomized</b> unit <b>testing</b> engine is a lower level, which tests a set of methods according to parameter values specified as genes in a chromosome, including parameters that encode a value reuse policy. The upper level is a genetic algorithm (GA) which uses fitness evaluation, selection, mutation and recombination of chromosomes in order to find out good values for the genes. Integrity is evaluated on the basis of test coverage and number of method calls performed. To find good parameters users can use Nighthawk and then perform with <b>randomized</b> unit <b>testing</b> based on those parameters. Many new test cases can quickly generate by <b>randomized</b> <b>testing</b> that achieve high coverage, and can continue to do so for as long as users wish to run it. In this research the test coverage results of Nighthawk are compared with manual unit testing results [6]. The Nighthawk system produced maximum test coverage results in less timing based on the genetic algorithm comparing with manual unit testing results...|$|R
40|$|Some {{financial}} problems as minimizing the shortfall risk when hedging in incomplete markets lead to problems belonging to test theory. This paper considers a generalization of the Neyman-Pearson lemma. With methods of convex duality we deduce {{the structure of}} an optimal <b>randomized</b> <b>test</b> when testing a compound hypothesis against a simple alternative. We give necessary and sufficient optimality conditions for the problem...|$|E
40|$|In {{incomplete}} {{financial markets}} not every contingent claim can be replicated by a self-financing strategy. The {{risk of the}} resulting shortfall can be measured by convex risk measures, recently introduced by Follmer and Schied (2002). The dynamic optimization problem of finding a self-financing strategy that minimizes the convex risk of the shortfall can be split into a static optimization problem and a representation problem. It follows that the optimal strategy consists in superhedging the modified claim [image omitted] �, where H is the payoff of the claim and [image omitted] � is a solution of the static optimization problem, an optimal <b>randomized</b> <b>test.</b> In this paper, necessary and sufficient optimality conditions are deduced for the static problem using convex duality methods. The solution of the static optimization problem {{turns out to be}} a <b>randomized</b> <b>test</b> with a typical 0 - 1 -structure. hedging, shortfall risk, convex risk measures, convex duality, generalized Neyman-Pearson lemma,...|$|E
40|$|We {{present a}} (<b>randomized)</b> <b>test</b> for {{monotonicity}} of Boolean functions. Namely, given {{the ability to}} query an unknown function f : f 0; 1 g 7 ! f 0; 1 g at arguments of its choice, the test always accepts a monotone f, and rejects f with high probability if it is ffl-far from being monotone (i. e., every monotone function differs from f on more than an ffl fraction of the domain) ...|$|E
5000|$|Lipton {{showed that}} <b>randomized</b> <b>testing</b> can be provably useful, given the problem {{satisfied}} certain properties. Proving correctness {{of a program}} {{is one of the}} most important problems presented in computer science. Typically in <b>randomized</b> <b>testing,</b> in order to attain a 1/1000 chance of an error, 1000 tests must be run. However Lipton shows that if a problem has [...] "easy" [...] sub-parts, repeated black-box testing can attain cr error rate, with c a constant less than 1 and r being the number of tests. Therefore, the probability of error goes to zero exponentially fast as r grows.|$|R
40|$|There {{are several}} problem areas {{that must be}} {{addressed}} when ap-plying randomization to unit testing. As yet no general, fully au-tomated solution that works for all units has been proposed. We therefore have developed RUTE-J, a Java package intended to help programmers do <b>randomized</b> unit <b>testing</b> in Java. In this paper, we describe RUTE-J and illustrate how it supports the development of per-unit solutions for the problems of <b>randomized</b> unit <b>testing.</b> We report on an experiment in which we applied RUTE-J to the standard Java TreeMap class, measuring the efficiency and effec-tiveness of the technique. We also illustrate the use of <b>randomized</b> <b>testing</b> in experimentation, by adapting RUTE-J so that it gener-ates <b>randomized</b> minimal covering <b>test</b> suites, and measuring the effectiveness of the test suites generated...|$|R
40|$|Most flight {{software}} testing at the Jet Propulsion Laboratory {{relies on the}} use of hand-produced test scenarios and executed on systems as similar as possible to actual mission hardware. We report on a flight software development effort incorporating large-scale (biased) <b>randomized</b> <b>testing</b> on commodity desktop hardware. The results show that use of a reference implementation, hardware simulation with fault injection, a testable design, and test minimization enabled a high degree of automation in fault detection and correction. Our experience will be of particular interest to developers and researchers working in domains where on-time delivery of software is critical (a strong argument for <b>randomized</b> automated <b>testing)</b> but not at the expense of correctness and reliability (a strong argument for model checking, theorem proving, and other heavyweight techniques). The effort spent in <b>randomized</b> <b>testing</b> can prepare the way for generating more complete confidence...|$|R
40|$|In {{this paper}} {{we face the}} problem of testing the {{equality}} {{of two or more}} parameters of amultinomial distribution. We develop a likelihood ratio test and we consider an asymptotically equivalent Pearson's statistic. Moreover we develop an exact and a <b>randomized</b> <b>test.</b> Relationships between these tests are then discussed. The behaviour of these tests is studied by simulations. Results from two known tests developed for less general situations are compared to ours...|$|E
40|$|Abstract. Shortfall risk is {{considered}} by taking some exposed risks because the superhedging price is too expensive {{to be used in}} practice. Minimizing shortfall risk can be reduced to the problem of finding a <b>randomized</b> <b>test</b> ψ in the static problem. The optimization problem can be solved via the classical Neyman-Pearson theory, and can be also explained in terms of hypothesis testing. We introduce the classical Neyman-Pearson lemma expressed in terms of mathematics and see how it is applied to shortfall risk in finance. 1...|$|E
40|$|This paper {{addresses}} the optimization of topological characteristics of Bluetooth scatternets which affect network performance. Particular {{attention is paid}} to the issue of fault tolerance, and objectives which are in competition with this. Using a multiple objective framework, the optimized trade-off between various topological objectives is determined. This is used to compare the scatternets produced by four decentralized protocols from the literature. The results offer a new basis to analyse and compare decentralized scatternet formation protocols. Results are presented for a range of <b>randomized</b> <b>test</b> problems...|$|E
40|$|Abstract — Functional Verification is well-accepted for Electronic System Level (ESL) based {{designs and}} is {{supported}} by a variety of standardized Hardware Verification Languages like PSL, e, and SystemVerilog. In this article, we present the classification tree method for functional verification (CTM/FV) as a novel method to close the gap from the verification plan to the specification of <b>randomized</b> <b>tests</b> and functional coverage for test configurations. CTM/FV is introduced based on graphical means from which we automatically generate SystemVerilog code as a testbench for constraint-based <b>randomized</b> <b>tests</b> and functional coverage, where concepts are outlined by the automotive example of an adaptive cruise controller. I...|$|R
40|$|<b>Randomized</b> <b>testing</b> {{has been}} shown to be an {{effective}} method for testing software units. However, the thoroughness of <b>randomized</b> unit <b>testing</b> varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe a system which uses a genetic algorithm to find parameters for <b>randomized</b> unit <b>testing</b> that optimize test coverage. We compare our coverage results to previous work, and report on case studies and experiments on system options. In order to optimize the system, we used data mining techniques to analyze which genes were the most useful. We also report on the results of this analysis and optimization...|$|R
5000|$|The {{algorithm}} {{is guaranteed to}} distinguish deterministically whether the target number is prime or composite. <b>Randomized</b> <b>tests,</b> such as Miller-Rabin and Baillie-PSW, can test any given number for primality in polynomial time, but are known to produce only a probabilistic result.|$|R
40|$|We {{consider}} a cooperative spectrum sensing scenario where the local sensors {{at the secondary}} users are viewed as one-level quantizers, and the quantized data are to be fused under Neyman-Pearson (N-P) criterion. We demonstrate how the N-P fusion results in a <b>randomized</b> <b>test,</b> which represents the total performance of our spectrum sensing scheme. We further introduce an upper performance bound for the overall primary user signal detection. An analytical procedure towards the upper bound and its relevant quantization setup at the local sensors are proposed and examined through simulations. © 2011 IEEE...|$|E
40|$|AbstractThe idea of {{efficient}} hedging {{has been}} introduced by Föllmer and Leukert. They defined the shortfall risk as the expectation of the shortfall weighted by a loss function, and looked for strategies that minimize the shortfall risk under a capital constraint. In this paper, to measure the shortfall risk, we use the coherent risk measures introduced by Artzner, Delbaen, Eber and Heath. We show that, for a given contingent claim H, the optimal strategy consists in hedging a modified claim ϕH for some <b>randomized</b> <b>test</b> ϕ. This is an analogue of the results by Föllmer and Leukert...|$|E
40|$|The next {{challenge}} for the PROMISE community is scaling up and speeding up model generation to meet the size and time constraints of modern software development projects. There {{will always be a}} trade-off between completeness and runtime speed. Here we explore that trade-off in the context of using genetic algorithms to learn coverage models; i. e. biases in the control structures for <b>randomized</b> <b>test</b> generators. After applying feature subset selection to logs of the GA output, we find we can generate the coverage model and run the resulting test suite ten times faster while only losing 6 % of the test case coverage...|$|E
40|$|ABSTRACT There {{are several}} problem areas {{that must be}} {{addressed}} when ap-plying randomization to unit testing. As yet no general, fully automated solution that works for all units has been proposed. Wetherefore have developed RUTE-J, a Java package intended to help programmers do <b>randomized</b> unit <b>testing</b> in Java. In this paper, wedescribe RUTE-J and illustrate how it supports the development of per-unit solutions for the problems of <b>randomized</b> unit <b>testing.</b> We report on an experiment in which we applied RUTE-J to the standard Java TreeMap class, measuring the efficiency and effec-tiveness of the technique. We also illustrate the use of <b>randomized</b> <b>testing</b> in experimentation, by adapting RUTE-J so that it gener-ates <b>randomized</b> minimal covering <b>test</b> suites, and measuring the effectiveness of the test suites generated. Categories and Subject Descriptors D. 2. 5 [Software Engineering]: Testing and Debugging [...] TestingTool...|$|R
40|$|Objective This study {{describes}} {{and provides}} relapse and recidivism outcome findings {{related to an}} experimental trial evaluating the viability of frequent, random drug testing with consequences for use. Methods The sample consisted of 529 offenders released on parole. An experimental design with random assignment {{to one of three}} groups was employed. The Experimental Group received frequent, random drug testing with instant results, immediate sanctions, and referral for substance abuse treatment. Control Group I received frequent, random drug testing and treatment referral, but did not receive immediate test results or immediate sanctions. Control Group II followed standard parole practice. Members of this group were not tested on a random basis and did not receive immediate sanctions. Repeated measures ANOVA and survival analysis techniques were used to explore group differences. Results Frequent monitoring of drug use with <b>randomized</b> <b>testing</b> protocols, immediate feedback, and certain consequences is effective in lowering rates of relapse and recidivism. The effectiveness is particularly salient in the short term during the period of exposure to testing conditions. Conclusions The findings lend support to the use of <b>randomized</b> <b>testing</b> with swift and certain sanctions with parolees. Additional quality evidence is necessary to generalize and refine findings from this study and others that focus on sanction certainty. Future replications must consider the immediacy of test result and sanction execution as well as the length of exposure to <b>randomized</b> <b>testing</b> periods...|$|R
5000|$|The basic {{structure}} of <b>randomized</b> primality <b>tests</b> is as follows: ...|$|R
40|$|Abstract. The optimal {{hypothesis}} {{tests for}} the binomial distribution {{and some other}} discrete distributions are uniformly most powerful (UMP) one-tailed and UMP unbiased (UMPU) two-tailed randomized tests. Conventional confidence intervals are not dual to randomized tests and perform badly on discrete data at small and moderate sample sizes. We introduce a new confidence interval notion, called fuzzy confidence intervals, that is dual to and inherits the exactness and optimality of UMP and UMPU tests. We also introduce a new P-value notion called, called fuzzy P-values or abstract randomized P-values, that also inherits the same exactness and optimality. Key words and phrases: Confidence interval, P-value, hypothesis test, uniformly most powerful unbiased (UMP and UMPU), fuzzy set theory, <b>randomized</b> <b>test...</b>|$|E
40|$|A good {{running time}} {{prediction}} of tasks is very helpful and important for job scheduling and resource management of Grid. In this paper {{we present a}} running time prediction method for Grid tasks based on our previous work, which is a novel CPU load prediction method. In order to eliminate the interference of other factors, such as the memory accessing, network performance, and fluctuation of CPU processing capacity and so on, we produce a simulation to test and evaluate our prediction method. In this simulation we use more than 10, 000 <b>randomized</b> <b>test</b> cases run on load traces sampled from 39 different machines. The simulation results are excellent and demonstrate that our running time prediction of Grid tasks outperforms significantly that of a widely existing prediction method...|$|E
40|$|Abstract — The {{problem of}} quickest {{detection}} of an anomalous process among M processes is considered. At each time, {{a subset of}} the processes can be observed, and the observations follow two different distributions, depending on whether the process is normal or abnormal. The objective is a sequential search strategy that minimizes the expected detection time subject to an error probability constraint. This problem can be considered as a special case of active hypothesis testing first considered by Chernoff in 1959, where a <b>randomized</b> <b>test</b> was proposed and shown to be asymptotically optimal. For the special case considered in this paper, we show that a simple deterministic test achieves asymptotic optimality and offers better performance in the finite regime. Index Terms—Sequential detection, hypothesis testing, dy-namic search. I...|$|E
40|$|The Neyman-Pearson {{fundamental}} lemma is generalized under g-probability. With convexity assumptions, {{a sufficient}} and necessary condition which characterizes the optimal <b>randomized</b> <b>tests</b> is obtained via a maximum principle for stochastic control. To cite this article: S. Ji, X. Y. Zhou, C. R. Acad. Sci. Paris, Ser. I 346 (2008). © 2007 Académie des sciences...|$|R
40|$|Abstract—Randomized {{testing is}} an {{effective}} method for testing software units. The thoroughness of <b>randomized</b> unit <b>testing</b> varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe Nighthawk, a system which uses a genetic algorithm (GA) to find parameters for <b>randomized</b> unit <b>testing</b> that optimize test coverage. Designing GAs is somewhat of a black art. We therefore use a feature subset selection (FSS) tool to assess the size {{and content of the}} representations within the GA. Using that tool, we can reduce the size of the representation substantially while still achieving most of the coverage found using the full representation. Our reduced GA achieves almost the same results as the full system, but in only 10 percent of the time. These results suggest that FSS could significantly optimize metaheuristic search-based software engineering tools. Index Terms—Software <b>testing,</b> <b>randomized</b> <b>testing,</b> genetic algorithms, feature subset selection, search-based optimization, testin...|$|R
40|$|Abstract: Randomization {{has long}} been used in testing, {{but it has not}} {{achieved}} widespread acceptance {{due to a lack of}} tool support and a failure to establish recognized best practices. In this paper, we describe RUTE-J, a Java package intended to provide tool support for <b>randomized</b> Java unit <b>testing.</b> We also discuss the best practices we have identified in our research on <b>randomized</b> unit <b>testing.</b> We report on case studies and an experiment in which we applied RUTE-J to various public-domain Java classes, finding failures even in mature software and supporting the claim that RUTE-J is an efficient, effective tool for unit testing. Finally, we compare the use of <b>randomized</b> unit <b>testing</b> to the use of other tools such as model checkers, and discuss the tradeoffs. We conclude that when best practices are followed, <b>randomized</b> unit <b>testing</b> with tool support is useful both as a preparation for full software model checking, and in its own right. ...|$|R
40|$|Abstract — This paper {{addresses}} {{the problem of}} computing frictional 4 -fingered force-closure grasps of three dimensional objects. The proposed approach searches for force-closure grasps from a collection of sampled points on the object’s surface. Unlike most other works, the approach {{is not limited to}} the objects with a certain class of shapes. It can be applied to an object in any shape since only the object’s surface points and corresponding surface normals at the points are needed. The efficiency of the approach arises from a heuristic for search space pruning which is based on ability to efficiently locate regions in three dimensional space where friction cones intersect and a <b>randomized</b> <b>test</b> for checking forceclosure condition. The proposed approach is implemented and preliminary results are presented. I...|$|E
40|$|Abstract. A {{problem of}} the drawing of aesthetically looking graphs, related to {{business}} process diagrams, is considered. We model a situation where sites of flow objects of the diagram are fixed, and the sequence flow is defined. The edges of a graph, which represent the sequence flow, should be drawn aiming at an aesthetical image. The latter problem is reformulated as a multi-objective combinatorial optimization problem. The generally recognized criteria of aesthetical presentation, such as general length of lines, number of crossings, and number of bends, are considered the objectives to be minimized. Two algorithms are developed for the stated problem taking into account its specifics. The efficiency of the developed algorithms is evaluated experimentally using <b>randomized</b> <b>test</b> problems of different complexity...|$|E
40|$|Previous {{simulation}} experiments for {{the comparison}} of wavelet shrinkage denoising methods have used fixed signal classes defined by adding instances of noise to a single test signal. New simulation experiments are reported here with randomized signal classes defined by adding instances of noise to instances of <b>randomized</b> <b>test</b> signals. As expected, significantly greater variability {{in the performance of}} the denoising methods was observed. Statistically valid comparisons must be conducted with respect to this variability. Use of randomized, rather than fixed, signal classes should yield more realistic and meaningful results. # Keywords: wavelet domain thresholding, shrinkage denoising, non-parametric signal estimation. 1 Introduction Denoising by thresholding in the wavelet domain has been developed principally by Donoho et al. [1, 2, 3, 4]. In [1], they introduced RiskShrink with the minimax threshold, VisuShrink with the universal threshold, and discussed both har [...] ...|$|E
30|$|The primary {{limitation}} of this study is the small sample size which did not allow robust statistical analysis. Additionally no cadaveric samples were tested for direct comparison. To allow comparison with cadaveric values published results were utilized. This provided mean values but did not allow for any statistical analysis. The destructive nature of the implanted conditions dictated that each sample follow a standard order. <b>Randomized</b> <b>tests</b> with individual samples for each intact and implanted condition may be desirable for future testing.|$|R
50|$|Darcs is a {{distributed}} {{version control}} system created by David Roundy. Key features include the ability to choose which changes to accept from other repositories, interaction with either other local (on-disk) repositories or remote repositories via SSH, HTTP, or email, and an unusually interactive interface. The developers also emphasize the use of advanced software tools for verifying correctness: the expressive type system of the functional programming language Haskell enforces some properties, and <b>randomized</b> <b>testing</b> via QuickCheck verifies many others. The name is a recursive acronym for Darcs Advanced Revision Control System.|$|R
40|$|Abstract. S-TaLiRo is a Matlab (TM) toolbox that {{searches}} for trajectories of minimal robustness in Simulink/Stateflow diagrams. It can analyze arbitrary Simulink models or user defined functions that model the system. At {{the heart of}} the tool, we use <b>randomized</b> <b>testing</b> based on stochastic optimization techniques including Monte-Carlo methods and Ant-Colony Optimization. Among the advantages of the toolbox is the seamless integration inside the Matlab environment, which is widely used in the industry for model-based development of control software. We present the architecture of S-TaLiRo and its working on an application example. ...|$|R
