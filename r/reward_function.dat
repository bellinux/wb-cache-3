967|465|Public
2500|$|Recent {{evidence}} {{has shown that}} smoking tobacco increases the release of dopamine in the brain, specifically in the mesolimbic pathway, the same neuro-reward circuit activated by drugs of abuse such as heroin and cocaine. This suggests nicotine use has a pleasurable effect that triggers positive reinforcement. One study found that smokers exhibit better reaction-time and memory performance compared to non-smokers, {{which is consistent with}} increased activation of dopamine receptors. Neurologically, rodent studies have found that nicotine self-administration causes lowering of reward thresholds—a finding opposite that of most other drugs of abuse (e.g. cocaine and heroin). This increase in reward circuit sensitivity persisted months after the self-administration ended, suggesting that nicotine's alteration of brain <b>reward</b> <b>function</b> is either long lasting or permanent. Furthermore, it has been found that nicotine can activate long-term potentiation in vivo and in vitro. These studies suggest nicotine’s [...] "trace memory" [...] may contribute to difficulties in nicotine abstinence.|$|E
5000|$|The belief MDP <b>reward</b> <b>function</b> (...) is the {{expected}} reward from the POMDP <b>reward</b> <b>function</b> over the belief state distribution: ...|$|E
50|$|In inverse {{reinforcement}} learning (IRL), no <b>reward</b> <b>function</b> is given. Instead, the <b>reward</b> <b>function</b> is inferred given an observed behavior from an expert. The {{idea is to}} mimic the observed behavior that is often optimal or close to optimal.|$|E
40|$|This chapter {{reviews the}} {{extracellular}} studies of dopamine neurons in behaving animals. Topics covered include motor functions of dopamine neurons, <b>reward</b> <b>functions</b> of dopamine neurons, <b>reward</b> learning <b>functions</b> of dopamine neurons, economic value functions of dopamine neurons, and attention and novelty functions of dopamine neurons...|$|R
3000|$|... is {{characterized}} by its own state, action set, transition probability, and <b>reward</b> <b>functions</b> and is denoted by subMDP_i={S_i,A_i, T_i,R_i}.|$|R
5000|$|These {{definitions}} can {{be modified}} to handle multi-criterion <b>reward</b> <b>functions.</b> Likewise, analogous definitions apply when [...] is a loss rather than a reward.|$|R
5000|$|... : , {{a reward}} rate {{function}} such that , where [...] is the <b>reward</b> <b>function</b> we discussed in previous case.|$|E
5000|$|Is the {{objective}} {{of a plan to}} reach a designated goal state, or to maximize a <b>reward</b> <b>function?</b> ...|$|E
50|$|Apprenticeship {{learning}} {{has been used}} to model reward functions of highly dynamic scenarios where there is no obvious <b>reward</b> <b>function</b> intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial <b>reward</b> <b>function</b> may not converge to the policy wanted.|$|E
40|$|We {{consider}} {{the problem of}} transferring learned knowledge among Markov Decision Processes (MDPs) that share the same transition dynamics but different <b>reward</b> <b>functions.</b> In particular, we assume that <b>reward</b> <b>functions</b> are described as linear combinations of reward features, and that only the feature weights vary among MDPs. We introduce Variable-Reward Hierarchical Reinforcement Learning (VRHRL), which leverages previously learned policies to speed-up learning in this setting. With suitable design of the hierarchy, VRHRL can achieve better transfer than its non-hierarchical counterpart. ...|$|R
5000|$|In this example, the {{sequence}} (...) is {{the sequence}} of offers for your house, and {{the sequence of}} <b>reward</b> <b>functions</b> is how much you will earn.|$|R
30|$|In {{order to}} obtain the optimal solution, it is {{necessary}} to identify the states, actions, <b>reward</b> <b>functions,</b> state transition probability and constrains in our SMDP model.|$|R
5000|$|Suppose we {{know the}} state {{transition}} function [...] and the <b>reward</b> <b>function</b> , and we wish to calculate the policy that maximizes the expected discounted reward.|$|E
5000|$|A Markov {{decision}} process with finite state space and fixed policy is defined with a 4-tuple , {{which includes the}} finite state space , the <b>reward</b> <b>function</b> , discount factor , and the transition model [...]|$|E
5000|$|For {{almost any}} open-ended, non-trivial <b>reward</b> <b>function</b> (or set of goals), {{possessing}} more resources (such as equipment, raw materials, or energy) can enable the AI {{to find a}} more [...] "optimal" [...] solution. Resources can benefit some AIs directly, through being able to create more of whatever stuff its <b>reward</b> <b>function</b> values: [...] "The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else." [...] In addition, almost all AI's can benefit from having more resources to spend on other instrumental goals, such as self-preservation.|$|E
40|$|We investigatehowtheproblemofplanning inastochasticenvironmentcanbetranslated intoaproblemofinference. Previousworkonplanning byprobabilisticinferencewaslimitedinthatatotaltime T hastobefixedandthatthecomputedpolicyisnotoptimalw. r. t. {{expected}}rewards. Thegenerativemodelweproposeconsidersthetotaltime T asarandomvariableandweshowequivalenceto {{maximizing the}} expected future return for arbitrary <b>reward</b> <b>functions.</b> Optimal policies are computed via Expectation-Maximization...|$|R
40|$|Post-traumatic stress {{disorder}} (PTSD) is a debilitating psychiatric disorder. An important diagnostic feature of PTSD is anhedonia, which {{may result from}} deficits in <b>reward</b> <b>functioning.</b> This has however never been studied systematically in PTSD. To determine if PTSD is associated with reward impairments, we conducted a systematic review of studies in which <b>reward</b> <b>functioning</b> was compared between PTSD patients and healthy control participants, or investigated in relation to PTSD symptom severity. A total of 29 studies were included, covering reward anticipation and approach ('wanting'), and hedonic responses to reward ('liking'). Overall, results were mixed, although decreased reward anticipation and approach and reduced hedonic responses were repeatedly observed in PTSD patients compared to healthy controls. Decreased <b>reward</b> <b>functioning</b> was seen more often in female than in male PTSD samples and most often in response to social positive stimuli. Though more research is needed, these findings are {{a first step in}} understanding the possible mechanisms underlying anhedonia in PTS...|$|R
40|$|The {{present study}} aims at {{examining}} {{the nature and}} legal basis {{of the concept of}} positive sanction, in conjunction with the law <b>rewarding</b> <b>function.</b> The dissertation is divided into two parts. In the first, the evolution of the <b>rewarding</b> <b>function</b> concept from the theory of the modern State is examined. In particular, diametrically opposite thoughts from two XVII century authors, Thomas Hobbes and Richard Cumberland, are explored. The former considers the legal sanction in exclusively negative sense, admitting however rewards according to the Sovereign’s arbitrary discretion; conversely, the latter states the superiority of reward on penalty, basing this assumption on the alleged benevolent nature of man. The second part analyses the concept of law <b>rewarding</b> <b>function</b> on the basis of Norberto Bobbio’s and Hans Kelsen’s remarks. In the contemporary context of law crisis, now considered a mere epiphenomenon of social development, we are witnessing the rediscovery of law <b>rewarding</b> <b>function,</b> an expression of functionalism: from instrument of social control, law is transformed into an instrument of social direction, which the State uses to achieve its goals, not worrying about the associates but only about their actions and the consequent results. Nevertheless, a critical issue arises: {{with the advent of the}} dirigist Sate, the law, in its <b>rewarding</b> <b>function,</b> may became a tool for citizen manipulation and coercion. In an attempt to overcome this problem, a demonstration of how the law <b>rewarding</b> <b>function,</b> if properly understood, also shows a humanizing face is here provided, with reference to the theory of natural law in force proposed by Sergio Cotta. Studying this author’s thought enables to have an approach to the concept of law and its real nature, as a translation into juridical terms of man’s relational nature, restoring the vital link between being and law. In conclusion, on the basis of the comparison between these two doctrines, the functionalism and the theory of natural law in force, the present work gains insights on whether law should be considered as a functional tool to a result and, in this view, if it may also be freely manipulated, or if law aims at values beyond a functional outcome, that must necessarily be related to man’s nature, as the final recipient and the first referent of the legal norm...|$|R
50|$|CTS {{assume that}} the {{demonstrated}} skills form a tree, the domain <b>reward</b> <b>function</b> is known and the best model for merging a pair of skills is the model selected for representing both individually.|$|E
5000|$|Bellman Average Reward Bases(or BARBs) {{is similar}} to Krylov Bases, but the <b>reward</b> <b>function</b> is being dilated by the average {{adjusted}} transition matrix [...] Here [...] can be calculated by many methods in.|$|E
50|$|The {{theory of}} {{universal}} artificial intelligence applies decision theory to inductive probabilities. The theory {{shows how the}} best actions to optimize a <b>reward</b> <b>function</b> may be chosen. The result is a theoretical model of intelligence.|$|E
30|$|In this section, {{we first}} present an {{overview}} of SMDP modeling. Then, the states, actions, <b>reward</b> <b>functions,</b> state transition probability, constraints, optimality equations, and value iteration algorithm in the CBTC system are presented.|$|R
30|$|In {{order to}} obtain the optimal solution, it is {{necessary}} to identify the states, actions, state transition probability, observation model, and <b>reward</b> <b>functions</b> in our POMDP model, which is described in the following sections.|$|R
3000|$|... [...]) is the maximal. Note {{that the}} joint {{distribution}} of the random variables and the <b>reward</b> <b>functions</b> are known and that X denotes the random variable and x denotes {{the value of a}} random variable, respectively.|$|R
50|$|Samuel also {{designed}} various {{mechanisms by which}} his program could become better. In what he called rote learning, the program remembered every position it had already seen, along with the terminal value of the <b>reward</b> <b>function.</b> This technique effectively extended the search depth at each of these positions. Samuel's later programs reevaluated the <b>reward</b> <b>function</b> based on input from professional games. He also had it play thousands of games against itself as another way of learning. With all of this work, Samuel’s program reached a respectable amateur status, {{and was the first}} to play any board game at this high a level. He continued to work on checkers until the mid-1970s, at which point his program achieved sufficient skill to challenge a respectable amateur.|$|E
50|$|During the 1980s, {{approaches}} to achieve inferential programming mostly revolved around techniques for logical inference. Today {{the term is}} sometimes used in connection with evolutionary computation techniques that enable the computer to evolve a solution {{in response to a}} problem posed as a fitness or <b>reward</b> <b>function.</b>|$|E
5000|$|... is the {{terminal}} <b>reward</b> <b>function,</b> [...] is thesystem state vector, [...] {{is the system}} control vector we try tofind. [...] shows how the state vector change over time.Hamilton-Jacobi-Bellman equation is as follows:We could solve the equation to find the optimal control , which could give us the optimal value ...|$|E
40|$|International audienceMarkov {{decision}} processes (MDP) {{have become}} one of the standard models for decision-theoretic planning problems under uncertainty. In its standard form, rewards are assumed to be numerical additive scalars. In this paper, we propose a generalization of this model allowing rewards to be functional. The value of a history is recursively computed by composing the <b>reward</b> <b>functions.</b> We show that several variants of MDPs presented in the literature can be instantiated in this setting. We then identify sufficient conditions on these <b>reward</b> <b>functions</b> for dynamic programming to be valid. In order to show the potential of our framework, we conclude the paper by presenting several illustrative examples...|$|R
40|$|Abstract. We build culture-specific {{dialogue}} {{policies of}} virtual humans for negotiation {{and in particular}} for argumentation and persuasion. In {{order to do that}} we use a corpus of non-culture specific dialogues and we build simulated users (SUs), i. e. models that simulate the behavior of real users. Then using these SUs and Reinforcement Learning (RL) we learn negotiation dialogue policies. Furthermore, we use research findings about specific cultures in order to tweak both the SUs and the <b>reward</b> <b>functions</b> used in RL towards a particular culture. We evaluate the learned policies in a simulation setting. Our results are consistent with our SU manipulations and RL <b>reward</b> <b>functions...</b>|$|R
40|$|Abstract. Markov {{decision}} processes (MDP) {{have become}} one of the standard models for decision-theoretic planning problems under uncer-tainty. In its standard form, rewards are assumed to be numerical ad-ditive scalars. In this paper, we propose a generalization of this model allowing rewards to be functional. The value of a history is recursively computed by composing the <b>reward</b> <b>functions.</b> We show that several vari-ants of MDPs presented in the literature can be instantiated in this set-ting. We then identify sufficient conditions on these <b>reward</b> <b>functions</b> for dynamic programming to be valid. In order to show the potential of our framework, we conclude the paper by presenting several illustrative examples. ...|$|R
50|$|In {{reinforcement}} learning settings, no teacher provides target signals. Instead a fitness function or <b>reward</b> <b>function</b> or utility function is occasionally {{used to evaluate}} performance, which influences its input stream through output units connected to actuators that affect the environment. Variants of evolutionary computation are often used to optimize the weight matrix.|$|E
5000|$|If {{we assume}} the {{transition}} {{dynamics of the}} system or the cost function as subjected to noise, we obtain a stochastic optimal control problem with a cost [...] and dynamics [...] In the field of reinforcement learning the cost {{is replaced by a}} <b>reward</b> <b>function</b> [...] and the dynamics by the transition probabilities [...]|$|E
50|$|In {{reinforcement}} learning settings, no teacher provides target signals. Instead a fitness function or <b>reward</b> <b>function</b> is occasionally {{used to evaluate}} the RNN's performance, which influences its input stream through output units connected to actuators that affect the environment. This might be used to play a game in which progress is measured with the number of points won.|$|E
40|$|A sharing {{game is a}} {{very simple}} device for {{partially}} reconciling an organization’s goal {{with the interests of}} its members. Each member chooses an action, bears its cost, and receives a share of the revenue which the members’ actions generate. A (pure-strategy) equilibrium of the game may be inefficient: surplus (revenue minus the sum of costs) may be less than maximal. In a previous paper, we found that for a wide class of <b>reward</b> <b>functions,</b> no one squanders at an inefficient equilibrium (spends more than at an efficient profile) if the revenue function has a complementarity property. In the present paper, we examine the “opposite” of the complementarity property (Substitutes) and we study a class of finite games where squandering equilibria indeed occur if Substitutes holds strongly enough. Squandering equilibria play a key role when one traces the effect of technological improvement on a sharing game’s surplus shortfall. We then turn to the question of choice among <b>reward</b> <b>functions</b> in a principal/agents setting. We find that if we again assume complementarity then strong conclusions can be reached about the <b>reward</b> <b>functions</b> preferred by “society”, by the players (agents), and by the principal...|$|R
40|$|This paper studies a divisionalized {{firm with}} {{sequential}} transfers in which central management wants to motivate two division managers who receive predecision information. Central management can only contract on the observables price, cost and quantity. Starting with the optimal compensation schemes as a benchmark, the paper considers {{the question whether}} using transfer prices to substitute for price and cost, respectively, can replicate the optimal solution or not. This is to say, whether using an aggregate measure comes at a loss. The results are dependent on the design constraints (i) single or 'dual' transfer prices and (ii) simultaneous design of the <b>reward</b> <b>functions</b> or exogenously given <b>reward</b> <b>functions.</b> Basically, only in the case that central management is restricted to given <b>reward</b> <b>functions,</b> and wants {{to use the same}} single transfer price for both divisions, there is a loss relative to the benchmark solution. In the other cases, generally, there is enough latitude to design the available functions to mimic the benchmark. The paper goes on to discuss special cases. First, it finds conditions when purely cost-based transfer prices are optimal, and second, it derives explicit solutions for given linear compensation schemes over divisional book profits. ...|$|R
5000|$|UCBogram algorithm: The {{nonlinear}} <b>reward</b> <b>functions</b> {{are estimated}} using piecewise constant over a functions using a piecewise constant estimator called regressogram in Nonparametric regression. Then, UCB is employed on each constant piece. Successive refinements of {{the partition of}} the context space are scheduled or chosen adaptively.|$|R
