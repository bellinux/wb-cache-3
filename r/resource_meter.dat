4|40|Public
50|$|Earlier {{versions}} of Windows such as Windows 3.1 and Windows 98 included a <b>Resource</b> <b>Meter</b> program {{to allow the}} user to monitor {{how much of the}} total system GDI resources were in use. Unfortunately, this <b>resource</b> <b>meter</b> consumed GDI objects itself. Later versions such as Windows 2000 and Windows XP can report GDI object usage for each program in the Task Manager, but they cannot tell the user the total GDI capacity available.|$|E
5000|$|Cloud <b>Resource</b> <b>Meter</b> {{which gives}} users {{the ability to}} meter virtual machine {{resource}} consumption in a cloud or on-premises environment providing insight into resource consumption.|$|E
50|$|Extreme Vs. {{retains the}} {{trademark}} gameplay of previous installments of the Gundam Vs. series, {{with a few}} adjustments and changes. During a standard arcade battle, each team is given a <b>resource</b> <b>meter</b> of 6000 points; when a mobile suit is shot down, its cost is deducted from the meter, and {{the first team to}} hit 0 loses. Each mobile suit costs 3000, 2500, 2000, or 1000 points, with higher cost machines being more powerful while cheaper machines are much weaker. A few missions, particularly boss battles, eschew this in favor of making a single machine the target and having its defeat be the goal of the stage (the player's side still has a <b>resource</b> <b>meter</b> and will lose as normal if it is depleted).|$|E
50|$|Health, Wrath, and Reaper <b>resource</b> <b>meters</b> display {{on-screen}} {{whenever they}} change, {{along with an}} experience meter that shows how close the player is to the next character class level. Wrath is the game's mana-type system, being a resource used for special abilities. Reaper is a separate resource used for the Reaper ability, and when full, Death can transform briefly into his grim reaper form, which is more resilient and deals more damage.|$|R
50|$|HP CloudStart enables user {{organizations}} to design, build and install a private cloud based on CloudSystem Matrix. CloudStart includes Cloud Service Automation and a virtual infrastructure provisioning product {{that features a}} self-service portal. It also provides <b>resource</b> <b>metering</b> and chargeback facilities for private clouds. IT services are also included with CloudStart, including {{an assessment of the}} user organization's existing IT infrastructure plus recommendations for how the organization could use cloud computing. The cloud configuration is set up by HP, which also moves the defined services to the cloud. Training is also provided to the user organization. HP has been cited as saying that it can create a private cloud for user organizations within 30 days of its initial engagement with the organization.|$|R
5000|$|Monitoring <b>resources</b> such as <b>Meters,</b> Events & Event Logs (for {{notifications}} {{from the}} Provider, with time, type (error, warning, ...), severity, etc.) ...|$|R
5000|$|Soon {{after the}} game's release, D.Va was noted {{for having a}} low damage output; the game's {{principal}} designer, Geoff Goodman confirmed that [...] "D.Va's damage is definitely on the lower side, much like Winston's. They are this way for a similar reason: They are both very mobile and hard to kill." [...] D.Va was also noted to take more damage than other tank characters when facing multiple opponents in the game due to her lack of a shield. Goodman attributed this along with her low damage output to the character balancing developed into Overwatch developers, stating, [...] "every character in the game has strengths and weaknesses, it's {{part of what makes}} the teamplay work well." [...] Goodman revealed that early in her development, D.Va was able to deal more damage, but that this damage output was lowered because [...] "the way that played out is that she would fly into someone's face, destroy them, and fly away. There was little that person could do because of her mobility." [...] Despite agreeing with feedback calling D.Va underpowered, Goodman expressed that improvements to the character [...] "are unlikely to take the shape of increasing her damage output significantly," [...] adding that [...] "the goal is that she should be a viable aggressive initiation tank, much like Winston can be." [...] The discussion of buffing D.Va came {{at the same time as}} a discussion on nerfing the character McCree, who had been noted to eliminate tank characters too quickly; game director Jeff Kaplan explained that D.Va buffs would take longer to develop and implement. Kaplan did however detail that the development team would be exploring improvements to her damage output and survivability, although would only [...] "probably pick one direction or the other." [...] Eventually, her ultimate ability was buffed, with the cost and explosion delay being reduced, in addition to the removal of the possibility for the activating player to be killed by their own ultimate ability. Her [...] "Defense Matrix" [...] ability was also changed to be toggled on and off at will with a <b>resource</b> <b>meter,</b> rather than being a single-use ability with a cooldown.|$|E
40|$|With the {{convergence}} of mobile communication, sensors and online social networks technologies, we are witnessing an exponential increase in the creation and consumption of personal data. Paper-based interactions (e. g., banking, health), analog processes (e. g., photography, <b>resource</b> <b>metering)</b> or mechanical interactions (e. g., as simple as opening a door) are now sources of digital data that {{can be linked to}} one or several individuals. This personal data is recognized by the World Economic Forum as a most valuable resource comparable to “the new oil ” [24], creating an unprecedented potential for applications and business. Until now, enthusiasm for these new opportunities has thwarted privacy concerns. Individuals conscientiously build Facebook pages, conduct their communications via Gmail, send and receive megabytes of personal information to and from administrations or commercial services. However, the loss of privacy has sever...|$|R
40|$|Abstract [...] Charging {{provides}} an effective control mechanism for resource allocation in multi-service networks. In many cases fair charging requires the <b>metering</b> of used <b>resources.</b> <b>Meter</b> requirements heavily {{depend on the}} expected traffic characteristics, the charging scheme and the QoS provisioning technique. Meters for accounting are deployed in various scenarios. For a given meter architecture, performance is mainly influenced by two factors: {{the characteristics of the}} traffic mix and the classifier rules. We present a meter assessment method with a flexible testbed that allows to measure meter performance for different rule sets and a variety of traffic mixes. Our measurement setup allows to derive traffic generation from the specification of classifier rules. Our measurement results show the influence of traffic mixes and classifier rules onto needed processing resources. Index terms [...] IP metering, classification, accounting, traffic generatio...|$|R
40|$|The <b>resource</b> <b>metering</b> system comprises: an {{end-point}} device (25) consuming a resource, {{in particular}} for usage {{in a building}} (2) or in an outdoor lighting system, said device comprising a detection unit that produces status information and an indicator of usefulness; a smart meter (20) comprising: a communication circuitry provided with an interface adapted for receiving from said device status information and said indicator of usefulness; a metrology device connected to a medium (17) that provides the resource to said device; and a control circuitry connected to the metrology device for collecting resource consumption data, the control circuitry being connected to the communication circuitry and adapted to produce monitoring data to be securely transmitted to a server (10) after processing the status information and said indicator. Monitoring data are used when determining consumption tariffs, so as to encourage energy efficient usage of the device...|$|R
30|$|Minimising CPU usage {{in favour}} of utilising network capacity. This is {{advantageous}} in a cloud computing setting where internal bandwidth is free where as CPU is a <b>metered</b> <b>resources.</b>|$|R
40|$|Cloud {{computing}} {{has emerged}} as a promising platform that grants users with direct yet shared access to computing resources and services without worrying about the internal complex infrastructure. Unlike traditional batch service model, cloud service model adopts a pay-as-you-go form, which demands explicit and precise resource control. In this paper, we present SigLM, a novel Signature-driven Load Management system to achieve quality-aware service delivery in shared cloud computing infrastructures. SigLM dynamically captures fine-grained signatures of different application tasks and cloud nodes using time series patterns, and performs precise <b>resource</b> <b>metering</b> and allocation based on the extracted signatures. SigLM employs dynamic time warping algorithm and multi-dimensional time series indexing to achieve efficient signature pattern matching. Our experiments using real load traces collected on the PlanetLab show that SigLM can improve resource provisioning performance by 30 - 80 % compared to existing approaches. SigLM is scalable and efficient, which imposes less than 1 % overhead to the system and can perform signature matching within tens of milliseconds. ...|$|R
40|$|Abstract—In the envisaged utility {{computing}} paradigm, a user taps {{a service}} provider’s computing resources to accom-plish her tasks, without deploying the needed {{hardware and software}} in her own IT infrastructure. To make the service profitable, the service provider charges the user based on the resources consumed. A commonly billed resource is CPU usage. A key factor to ensure the success of such a business model is the trustworthiness of the <b>resource</b> <b>metering</b> scheme. In this paper, we provide a systematic study on the trustworthiness of CPU usage metering. Our {{results show that the}} metering schemes in commodity operating systems should not be used in utility computing. A dishonest server can run various attacks to cheat the users. Many of the attacks are surprisingly simple and do not even require high privileges or sophisticated techniques. To demonstrate that, we experiment with several types of attacks on Linux and show their adversarial effects. We also suggest that source integrity, execution integrity and fine-grained metering are the necessary properties for a trustworthy metering scheme in utility computing. Keywords-CPU time metering; attack; utility computing I...|$|R
40|$|In the envisaged utility {{computing}} paradigm, a user taps {{a service}} provider’s computing resources to accomplish her tasks, without deploying the needed {{hardware and software}} in her own IT infrastructure. To make the service profitable, the service provider charges the user based on the resources consumed. A commonly billed resource is CPU usage. A key factor to ensure the success of such a business model is the trustworthiness of the <b>resource</b> <b>metering</b> scheme. In this paper, we provide a systematic study on the trustworthiness of CPU usage metering. Our {{results show that the}} metering schemes in commodity operating systems should not be used in utility computing. A dishonest server can run various attacks to cheat the users. Many of the attacks are surprisingly simple and do not even require high privileges or sophisticated techniques. To demonstrate that, we experiment with several types of attacks on Linux and show their adversarial effects. We also suggest that source integrity, execution integrity and finegrained metering are the necessary properties for a trustworthy metering scheme in utility computing...|$|R
50|$|Maryland: Legislation adds {{fuel cells}} as {{eligible}} net <b>metering</b> <b>resource.</b> House Bill 821 {{was passed in}} May 2010, adding fuel cells among the list of eligible customer-generators for net energy metering. System size is limited to 2 megawatts.|$|R
50|$|A long-awaited {{application}} of smart meters in the Texas deregulated electricity market is time-of-use rates. Several retail electric providers including TXU Energy and Direct Energy have introduced rate plans that leverage smart meters to incentivize customers who shift load to off-peak hours. Excluding TNMP (PNM <b>Resources),</b> smart <b>meter</b> conversion rates for {{commercial and residential}} customers in utilities open to retail electric competition were in excess of 99% as of January 2015. Some utilities offer free electricity at night.|$|R
50|$|Cloud {{storage is}} based on highly virtualized {{infrastructure}} and is like broader cloud computing in terms of accessible interfaces, near-instant elasticity and scalability, multi-tenancy, and <b>metered</b> <b>resources.</b> Cloud storage services can be utilized from an off-premises service (Amazon S3) or deployed on-premises (ViON Capacity Services).|$|R
40|$|In {{the recent}} years the shared {{services}} concept has {{become an integral part}} of business. These shared services can be in the form of information technology, engineering and lot more. Service providers spent huge amounts of money to build an infrastructure that can provide efficient and valued services to the customers. In IT business these services varies from providing basic consultancy and managing the IT operations of the customers to running high priority business processes,(online banking). Customers of these services pay for these services, so a mechanism of <b>resource</b> usage <b>metering</b> is required to accurately charge the users and at the same time a monitoring mechanism is required to have a check on the services being provided to the customers for any resource contention and service degradation and future capacity planning. If a service provider is unable to develop an accurate charge back and monitoring mechanism then the equation of service provider and customer becomes a point of frustration for both sides. charge back and monitoring systems developed for physical environment are not capable to measure the resource usage in virtual environment because in virtual environment (Z/VM) resources are shared between users and it becomes difficult to measure the resource usage by a specific user. Until now a few tools have been developed that provides efficient <b>resource</b> <b>metering</b> and monitoring in virtual environment (Z/VM) but every business has its own requirements and system setup so mostly these tools need some customizations to fit into the business. This work mainly concentrated on what kind of resource utilization data is available on Z/VM and on LINUX guests running on Z/VM to effectively charge the customers running there guest Linux Operating systems in virtual environment (Z/VM based) and to monitor the cpu and memory utilization to check whether the estimate of memory allocation for linux guests running different applications made by system (PWSS) is a good estimate or require some optimizations. Because memory utilization is considered more expensive in virtual environment in the context of system performance. The study also includes a comparison between this technique of charge back and some commercial products from IBM and CA (Computer Associates) that provides charge back and monitoring facility in Z/VM based virtual environment, and provides some benefits of this work in the proposed environment. Master i nettverks- og systemadministrasjo...|$|R
30|$|Cloud {{computing}} {{evolved from}} the concept of utility computing, which {{is defined as the}} provision of computational and storage <b>resources</b> as a <b>metered</b> service, similar to traditional public utility companies [92]. This concept reflects the fact that modern information technology environments require the means to dynamically increase capacity or add capabilities while minimizing the requirement of investing money and time in the purchase of new infrastructure.|$|R
40|$|The Job Usage Reporter of ARC (JURA) is a {{component}} implementing {{a part of}} the accounting functionality in the ARC middleware. Its objective is to gather <b>metered</b> <b>resource</b> usage data for each job and submit it to accounting services along with the job submitter’s identity and miscellaneous job-related metadata. The collected usage data is transformed into records of job-level granularity and a format corresponding to a...|$|R
40|$|Abstract — Large-scale bandwidth-based {{distributed}} denial-of-service (DDoS) attacks {{can quickly}} knock out substantial {{parts of a}} network before reactive defenses can respond. Even traffic flows that are not under direct attack can suffer significant collateral damage if these flows pass through links that are common to attack routes. Given the existence today of large botnets {{with more than a}} hundred thousand bots, the potential for a large-scale coordinated attack exists, especially given the prevalence of high-speed Internet access. This paper presents a Proactive Surge Protection (PSP) mechanism that aims to provide a broad first line of defense against DDoS attacks. The approach aims to minimize collateral damage by providing bandwidth isolation between traffic flows. This isolation is achieved through a combination of traffic measurements, bandwidth allocation of network <b>resources,</b> <b>metering</b> and tagging of packets at the network perimeter, and preferential dropping of packets inside the network. The proposed solution is readily deployable using existing router mechanisms and does not rely on any unauthenticated packet header information. Thus the approach is resilient to evading attack schemes that launch many seemingly legitimate TCP connections with spoofed IP addresses and port numbers. Finally, our extensive evaluation results across two large commercial backbone networks, using both distributed and targeted attack scenarios, show that up to 95. 5 % of the network could suffer collateral damage without protection, but our solution was able to significantly reduce the amount of collateral damage by up to 97. 58 % {{in terms of the number}} of packets dropped and 90. 36 % in terms of the number of flows with packet loss. Furthermore, we show that PSP can maintain low packet loss rates even when the intensity of attacks is increased significantly. I...|$|R
40|$|Cloud Computing {{is being}} adopted by many {{companies}} {{because of its}} capacity to use computing and storage <b>resources</b> on a <b>metered</b> basis thereby reducing the investments in infrastructure. With all its benefits, cloud computing also brings along concerns about the security, privacy and jurisdiction because of its size, structure, and geographical dispersion. This paper tries to explore these concerns and gives suggestions which may help companies to take security initiatives before they actually move into the cloud...|$|R
40|$|Third-party {{solar power}} {{purchase}} agreements (PPAs) {{emerged in the}} 2000 s and provided a new financing option for solar energy. Currently, third-party solar PPAs have been authorized in 25 states and DC in the U. S. and are successfully facilitating residential rooftop solar growth. Georgia recently legalized third-party solar PPAs through the Solar Power Free-Market Financing Act of 2015. This paper analyzes factors that impact third-party solar PPAs and evaluates those factors {{in the context of}} Georgia. We assessed solar <b>resources,</b> net <b>metering,</b> the Investment Tax Credit (ITC), Solar Renewable Energy Certificates (SRECs), differentiated state-level incentives, the regulatory environment, and consumer preferences. We also considered some potential future policy and market trends, such as the Clean Power Plan and electric vehicles. Our analysis shows how regulatory supports significantly drive third-party solar PPA growth and can provide our client with an expectation of third-party solar PPA development in Georgia...|$|R
40|$|Global energy crises are {{increasing}} every moment. Every {{one has the}} attention towards more and more energy production and also trying to save it. Electricity can be produced through many ways which is then synchronized on a main grid for usage. The main issue for which we have written this survey paper is losses in electrical system. Weather these losses are technical or non-technical. Technical losses can be calculated easily, as we discussed in section of mathematical modeling that how to calculate technical losses. Where as nontechnical losses can be evaluated if technical losses are known. Theft in electricity produce non-technical losses. To reduce or control theft one can save his economic <b>resources.</b> Smart <b>meter</b> can be the best option to minimize electricity theft, because of its high security, best efficiency, and excellent resistance towards many of theft ideas in electromechanical meters. So {{in this paper we}} have mostly concentrated on theft issues. Comment: 7 th International Conference on P 2 P, Parallel, Grid, Cloud and Internet Computing (3 PGCIC- 2012), Victoria, Canada, 201...|$|R
40|$|In {{the recent}} years the shared {{services}} concept has {{become an integral part}} of business. These shared services can be in the form of information technology, engineering and lot more. Service providers spent huge amounts of money to build an infrastructure that can provide efficient and valued services to the customers. In IT business these services varies from providing basic consultancy and managing the IT operations of the customers to running high priority business processes,(online banking). Customers of these services pay for these services, so a mechanism of <b>resource</b> usage <b>metering</b> is required to accurately charge the users and at the same time a monitoring mechanism is required to have a check on the services being provided to the customers for any resource contention and service degradation and future capacity planning. If a service provider is unable to develop an accurate charge back and monitoring mechanism then the equation of service provider and customer becomes a point of frustration for both sides. charge back and monitoring systems developed for physical environment are not capable to measure the resource usage in virtual environment because in virtual environment (Z/VM) resources are shared between user...|$|R
30|$|Companies mainly deploy {{metering}} devices which are capable to send information to application servers either automatically or on request. For instance, M 2 M devices {{can be used}} to automatically bill the <b>metering</b> <b>resource.</b> Furthermore, another function of these devices is to improve energy efficiency and performance through accurately measuring readings of the resource consumption, e.g., water, gas, light, etc. Moreover, these applications aim to deliver data of the actual consumption without human intervention. Consequently, it provides and increases a positive environmental impact between service providers and their customers via autonomous communications. In addition, M 2 M modules can be placed in harsh environments. The limited space available for deploying M 2 M devices additionally demands small-sized modules. Moreover, the data should be secured and devices should be protected.|$|R
40|$|Abstract—This paper {{discusses}} {{an attack}} on the cloud com-puting model by which an attacker subtly exploits a fundamental vulnerability of current utility compute models over a sustained period of time. Internet-accessible cloud services expose <b>resources</b> that are <b>metered</b> for billing purposes. These resources are subject to fraudulent resource consumption that is intended to run up the operating expenses for public cloud service customers. The details and significance of this attack are discussed as well as two detection methodologies and there respective experimental results. This work investigates a potentially significant vulnera-bility of the cloud computing model that could be exploited from any Internet connected host. Well-crafted transactions that only differ in intent but not in content are challenging to differentiate and thus this attack may be difficult to detect and prevent. Keywords-cloud computing; utility compute model; fraudulent resource consumption attack; application-layer DDoS; anomaly detection; I...|$|R
40|$|Geotechnical Aggregate are {{rocks that}} can be used as {{material}} {{for a wide range of}} construction engineering purposes. The need for geotechnical aggregates increased along with the rapid development. Required sources of aggregatequalified engineering was good, close to the project site and has large reserves. This research will be carried out an investigation to get to the source of geotechnical aggregate surveying of Geology and Geophysics measurements. Geophysical measurement is working by using of Geoelctric methods with Schlumberger configuration. Geological survey result obtained is that the material consists of geotechnical aggregate are QTV, Qtvl and TMV, composition form clastica volcanic rocks, breccias, lava, tuff andesite to basalt. Result of Geoelectric measurements showed that the potential for geotechnical aggregate form Alluvial rocks having resistivity values between 2 - 20 ohm meter, whereas the geotechnical aggregate form intrusive rocks is 30 - 80 ohm <b>meter.</b> <b>Resource</b> potential Probable amounted to 5, 742, 453 m 3...|$|R
30|$|In the literature, various {{solutions}} for this problem are presented which are mainly {{composed of two}} steps: 1 extracting useful information from the waveform as a feature set; 2 training a model based on the provided feature sets and labeled data for supervised pattern recognition. However, the driver events and various components of the distribution network along with scarcity of all kinds of abnormal data in the training period, hinder the effective applications of the proposed methods. Also for PQ detection at the distribution level, such as meter level, we need light but robust method due to the limited computational <b>resources</b> in smart <b>meters.</b> Based on the aforementioned data driven pipeline for this problem, majority of the research papers defined and simulated the possible disturbance patterns and then trained their models based on the simulated data. While these methods can detect the abnormalities of the defined pattern, they fail to capture the unknown types of abnormalities. In this paper, with focus on the real power data, we redress this shortcoming by applying a semi-supervised technique.|$|R
40|$|Smart meters are {{now being}} aggressively {{deployed}} worldwide, with {{tens of millions of}} meters in use today and hundreds of millions more to be deployed in the next few years. These low-cost (≃ $ 50) embedded devices have not fared well under security analysis: experience has shown that the majority of current devices can be exploited by unsophisticated attackers. The potential for large-scale attacks that target a single or a few vulnerabilities is thus very real. In this paper, we consider how diversity techniques can limit large-scale attacks on smart meters. We show how current meter designs do not possess the architectural features needed to support existing diversity approaches such as address randomization or control flow validation. In response, we posit a new return address encryption technique suited to the computationally and <b>resource</b> limited smart <b>meters.</b> We conclude by considering analytically the effect of diversity on an attacker wishing to launch a large-scale attack, showing how a lightweight diversity scheme can force the time needed for a large compromise into the scale of years. ...|$|R
40|$|Building Energy Assessment at {{stock level}} is an {{important}} task in identifying the best strategies for achieving a more energy efficient and low carbon society. Non-domestic buildings are identified to make up 17 % of total energy consumption in England and Wales and 19 % of CO 2 emissions. To understand the energy requirement of the non-domestic stock, large scale (empirically based) energy surveying {{has been carried out}} namely in the Non-Domestic Building Stock project and Carbon Reductions in Buildings project. It is recognised that building energy surveys are difficult to carry out; expensive on time, technical <b>resources,</b> and <b>metered</b> energy use is (on a large scale) necessarily crude. With improving computer ability, dynamic energy modelling tools allow for detailed assessment of building energy use and comfort performance. Using Monte Carlo simulation a method of assessing the probable variability in non-domestic building thermal energy loads was developed. The method was developed to capture the heterogeneity in non-domestic buildings at national stock level and determine how stock level physical form variations impact thermal loading. Non-domestic building form and surrounding topography are considered to be influenced by building control laws and building regulations. Control documentation often stipulates guidelines and best practice - hence building heterogeneity. As such, historical regulations were used to develop basic probability distributions of potential physical characteristics associated with non-domestic buildings. Stating that form and site characteristics are randomly determined from the defined probability distributions, a stochastic modelling process to represent thermal variation in a building stock was developed. This provided potential for categorising building thermal performance by period of construction. The model utilised a dynamic simulation model as a 'black-box' for predicting base thermal loads. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
50|$|Any {{output from}} any model {{described}} above must then {{be converted to}} the electric energy that a particular solar PV plant will produce. This step is usually done with statistical approaches that try to correlate the amount of available <b>resource</b> with the <b>metered</b> power output. The main advantage of these methods is that the meteorological prediction error, which is {{the main component of}} the global error, might be reduced taking into account the uncertainty of the prediction. As it was mentioned before and detailed in Heinemann et al., these statistical approaches comprises from ARMA models, neural networks, support vector machines, etc.On the other hand, there also exist theoretical models that describe how a power plant converts the meteorological resource into electric energy, as described in Alonso et al. The main advantage of this type of models is that when they are fitted, they are really accurate, although they are too sensitive to the meteorological prediction error, which is usually amplified by these models.Hybrid models, finally, are a combination of these two models and they seem to be a promising approach that can outperform each of them individually.|$|R
40|$|Track III: Emerging Themes in Interpretive Case Study ResearchInternational audienceThe Internet of Things (IoT) – the {{proliferation}} of networked sensors, gadgets, artefacts and measurement devices – increase the presence, scope and potential importance of mediated information in collaborative work practices. This underscores the material aspects of sociomaterial practices. We study an extreme case where work practices rely heavily, almost entirely, on representations. In line with the research programme on sociomateriality, we acknowledge the performative role of representations. Representations are thus actively embedded in practice rather than passive re-presentation of data. Extending the programme of sociomateriality, we contribute by identifying and discussing three strategies detailing how sociomaterial practices get performed: extrapolate (filling in gaps), harmonise (ironing out inaccuracies) and abduct (coping with anomalies). We draw empirically on a longitudinal (2004 - 2011) case study of the subsurface community of NorthOil. This community of geologists, geophysicists, reservoir engineers, production engineers and well engineers rely on sensor-based (acoustic, electromagnetic, radioactive, pressure, temperature) data when exploring and producing oil and gas <b>resources</b> several thousand <b>meters</b> below the seabed where direct access to data is difficult and/ or limited...|$|R
40|$|Since {{the past}} few years, the smart city {{paradigm}} has been influencing sustainable urban water <b>resources</b> management. Smart <b>metering</b> schemes for end users have become an important strategy for water utilities to have an in-depth and fine-grained knowledge about urban water use. Beyond reducing certain labor costs, such as those related to manual meter reading, such detailed and continuous flow of information is said to enhance network efficiency and improve water planning by having more detailed demand patterns and forecasts. Research focusing on those initiatives has been very prolific {{in countries such as}} Australia. However, less academic {{attention has been paid to}} the development of smart metering in other geographies. This paper focuses on smart water metering in Spain and, more particularly, documents and reflects on the experience of the city of Alicante (southeastern Spain), a pioneer case of massive deployment of remote reading of water meters at the household level and for large urban customers. Through data and interviews with water managers from the water utility, we shed light on the costs and early benefits, as well as the potentialities and (unexpected) problems of this technology to contribute to more sustainable urban water cycles...|$|R
30|$|Cloud {{computing}} {{evolved from}} the concept of utility computing, which {{is defined as the}} provision of computational and storage <b>resources</b> as a <b>metered</b> service. Another key characteristic of cloud computing is multitenancy, which enables resource and cost sharing among a large pool of users. Characteristics such as multitenancy and elasticity perfectly fit the requirements of modern data-intensive research and scientific endeavors. In parallel, as science relies on the analysis of very large data sets, data management and processing must be performed in a scalable and automated way. Workflows have emerged as a way to formalize and structure data analysis, thus becoming an increasingly popular paradigm for scientists to handle complex scientific processes. One of the key enablers of this conjunction of cloud computing and scientific workflows is resource management. However, several issues related to data-intensive loads, complex infrastructures such as hybrid and multicloud environments to support large-scale execution of workflows, performance fluctuations, and reliability, pose as challenges to truly position clouds as viable high-performance infrastructures for scientific computing. This paper presents a survey on cloud resource management that provides an extensive study of the field. A taxonomy is proposed to analyze the selected works and the analysis ultimately leads to the definition of gaps and future challenges to be addressed by research and development.|$|R
40|$|Before 11 th five-year plan, geologists {{proposed}} the viewpoint that the gas generation intensity that's more than 20  ×  108  m 3 /km 2 {{was an important}} condition for forming conventional large gas fields. However, recent exploration findings indicate that large-area tight sandstone gas that has a gas generation intensity of less than 20  ×  108  m 3 /km 2 can still form reserves. This is an area worth exploring. Through innovative accumulation simulation, microscopic pore throat analysis of reservoirs, and dissection of typical gas reservoirs, several factors have been established, including the comprehensive evaluation models involving gas charging pressure, reservoir physical properties, and lower gas generation limit. In addition, the paper has made it certain that the tight sandstone gas in a low gas generation intensity area has accumulation characteristics such as “partial water displacing, long-term gas supply, gas control by tight reservoirs of scale, gas abundance control by physical properties, combined control and enrichment of dominant resources, etc. ”, and has {{proposed the}} viewpoint that this area has the accumulation mechanism that of a “non-dominant transportation, long-term continuous charging, reservoir controlling by physical property difference, and enrichment in partial sweet spots” and shows discontinuous “patchy distribution” on the plane. This is of much significance to fine exploration {{and development of the}} trillion cubic <b>meter</b> <b>resources</b> of the low gas generation intensity areas in the west of Sulige gas field...|$|R
