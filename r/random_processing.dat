44|76|Public
50|$|At the 2007 Games Convention Developers Conference, Eggebrecht {{criticized the}} ESRB for <b>random</b> <b>processing</b> in the {{guidelines}} {{to the development}} of Lair, and called for improvement to the system.|$|E
40|$|Abstract. Electric {{power system}} {{reliability}} evaluation depends on reasonable target setting and calculation. This article mainly analyses the new index of power system reliability assessment in the electricity market environment, {{and on the}} basis of this, introduces the application of uncertain linguistic information in the ceremony system reliability evaluation, representing original parameters with uncertain number, and <b>random</b> <b>processing</b> fault condition happened and transfer events and process...|$|E
40|$|Jansen, Mastrolilli, Solis-Oba, 2005] K. Jansen, M. Mastrolilli, R. Solis-Oba. Approximation {{schemes for}} job shop {{scheduling}} problems with controllable processing times. European Journal of Operational Research. 167, 297 - 319, 2005. [Johnson, 1954] S. M. Johnson. Optimal two- and three-stage production schedules with setup times included. Naval Research Logistics Quarterly, 1, 61 — 68, 1954. [Ku, Niu, 1986] P. S. Ku, and S. C. Niu. On Johnson's two-machine flow-shop with <b>random</b> <b>processing</b> times. Operation...|$|E
40|$|Engineers in {{all fields}} will {{appreciate}} a practical guide that combines several new effective MATLAB® problem-solving approaches {{and the very}} latest in discrete <b>random</b> signal <b>processing</b> and filtering. Numerous Useful Examples, Problems, and Solutions - An Extensive and Powerful ReviewWritten for practicing engineers seeking to strengthen their practical grasp of <b>random</b> signal <b>processing,</b> Discrete <b>Random</b> Signal <b>Processing</b> and Filtering Primer with MATLAB provides the opportunity to doubly enhance their skills. The author, a leading expert {{in the field of}} electrical and computer engineering, off...|$|R
40|$|The thesis {{proposes to}} develop a virtual {{prototyping}} environment (VPE) allowing the study of multi-level <b>random</b> data <b>processing.</b> This VPE allows to experimentally study the effect of different quantization levels and dither signal parameters {{on the performance of}} generalized random data correlator architectures...|$|R
40|$|AbstractThe {{overflow}} {{process of}} a D/G/K loss system with deterministic interarrival time, generally distributed service time, K parallel heterogeneous servers, <b>random</b> access <b>processing</b> discipline and no waiting room is approximated. Furthermore, numerical results are provided and the approximation results are compared against those from a simulation study...|$|R
40|$|AbstractThis paper {{considers}} {{the problem of}} due-date determination and sequencing of n stochastically independent jobs on a single machine with <b>random</b> <b>processing</b> times. The objective {{is to find the}} optimal due-date values for the constant due-date assignment method and the optimal job sequence that minimize the expected value of a total cost function. It is shown that under suitable assumptions the optimal due-date values can be analytically determined and the jobs should be arranged in the SEPT sequence to minimize the cost...|$|E
40|$|In this paper, two Erlang {{models of}} a two-machine, one-buffer {{production}} line are discussed. Both are {{extensions of the}} exponential production line model and treat <b>random</b> <b>processing,</b> failure, and repair times. In the first, worker intervention occurs only when a failure takes place; in the second maintenance occurs whenever a machine is idle due to starvation or blockage. Numerical results from the first model are indistinguishable {{from those of the}} exponential model; a substantial increase in throughput is observed in the second. I...|$|E
40|$|We {{focus on}} solving Stochastic Job Shop Scheduling Problem (SJSSP) with <b>random</b> <b>processing</b> time to {{minimize}} the expected sum of earliness and tardiness costs of all jobs. To further enhance {{the efficiency of the}} simulation optimization technique of embedding Evolutionary Strategy in Ordinal Optimization (ESOO) which is based on Monte Carlo simulation, we embed Optimal Computing Budget Allocation (OCBA) technique into the exploration stage of ESOO to optimize the performance evaluation process by controlling the allocation of simulation times. However, while pursuing a good set of schedules, “super individuals,” which can absorb most of the given computation while others hardly get any simulation budget, may emerge according to the allocating equation of OCBA. Consequently, the schedules cannot be evaluated exactly, and thus the probability of correct selection (PCS) tends to be low. Therefore, we modify OCBA to balance the computation allocation: (1) set a threshold of simulation times to detect “super individuals” and (2) follow an exclusion mechanism to marginalize them. Finally, the proposed approach is applied to an SJSSP comprising 8 jobs on 8 machines with <b>random</b> <b>processing</b> time in truncated normal, uniform, and exponential distributions, respectively. The results demonstrate that our method outperforms the ESOO method by achieving better solutions...|$|E
5000|$|The {{concept of}} [...] "Data Management" [...] {{arose in the}} 1980s as {{technology}} moved from sequential processing (first cards, then tape) to <b>random</b> access <b>processing.</b> Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that [...] "Data Management" [...] {{was more important than}} [...] "Process Management" [...] used arguments such as [...] "a customer's home address is stored in 75 (or some other large number) places in our computer systems." [...] During this period, <b>random</b> access <b>processing</b> was not competitively fast, so those suggesting [...] "Process Management" [...] was more important than [...] "Data Management" [...] used batch processing time as their primary argument. As applications moved into real-time, interactive applications, it became obvious to most practitioners that both management processes were important. If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs.|$|R
40|$|In recent years, pseudo <b>random</b> signal <b>processing</b> {{has proven}} to be a {{critical}} enabler of modern communication, information, security and measurement systems. The signal's pseudo random, noise-like properties make it vitally important as a tool for protecting against interference, alleviating multipath propagation and allowing the potential of sharing bandwidth with other users. Taking a practical approach to the topic, this text provides a comprehensive and systematic guide to understanding and using pseudo random signals. Covering theoretical principles, design methodologies and application...|$|R
40|$|AbstractThis paper {{describes}} a Monte Carlo procedure for {{the solution of}} elliptic difference equations requiring a previous subdivision of the integation domain into convex regions. Using Green's function for these regions, it is shown {{how it is possible}} to avoid <b>processing</b> <b>random</b> walks inside each region...|$|R
40|$|AUTHOR, please supply e-mail addresses****** Abstract: We analyse an {{assembly}} system fed by components produced by suppliers and operating under a pull system with <b>random</b> <b>processing</b> times. Under certain assumptions {{we present a}} performance evaluation algorithm {{which is based on}} physical decomposition of the network and use of an aggregation/disaggregation algorithm. We consider two important performance measures: system throughput and work-in-process inventory. Performance evaluation of these systems is difficult because of the mating of parts and a kanban-like operating mechanism. Our algorithm was tested using simulation and found to be accurate...|$|E
40|$|In {{semiconductor}} manufacturing, the throughout {{analysis and}} robot action sequence are crucial for multi-cluster tools scheduling problems. This paper studies a general robot scheduling problem for multi-cluster tools with the single-arm robot and buffer process modules. Firstly, the scheduling model with constraints of processing module and robots transport is addressed. Secondly, the lower bound of cycle time is theoretically proved. And based on decomposition idea, the heuristic scheduling method, which produces robot action sequence {{by virtue of}} <b>random</b> <b>processing</b> time and cycle time, is proposed. Finally, {{the superiority of the}} proposed method is proved simulation results...|$|E
40|$|This paper {{studies the}} problem of single machine {{stochastic}} scheduling with <b>random</b> <b>processing</b> times, deterministic due dates and an independent setup time. The jobs are also deteriorated based on the position, which their processes are done. The objective function {{is to find a}} schedule of jobs, which minimizes the expected value of maximum lateness. A branch and bound scheme is presented to solve the problem analytically and a simulated annealing meta-heuristic (SA) is also provided for solving the problem in larger scales. Computational experiments demonstrate that the proposed SA is capable of finding near optimal solutions with very low gap...|$|E
30|$|Once the seeding {{process is}} finalized, the {{location}} information {{of the seeds}} are propagated into corresponding MR images (Figure 1 C). Globally optimal boundaries are then obtained after <b>processing</b> <b>random</b> walk segmentation with automatically determined background and foreground seeds. Further details of the proposed method is given in the next section.|$|R
40|$|This paper {{describes}} PPRAM (Parallel <b>Processing</b> <b>Random</b> Access Memory) that is {{an architectural}} framework for merged memory/logic ASSPs(Application-Specific Standard Products). At first, the paper discusses the serious problems that the current high-performance microprocessor-based systems are facing: memory wall, design-cost/time problem, and limits of instruction-level parallelism, and how PPRAM will resolve these problems. The overview of PPRAM, PPRA...|$|R
40|$|This is {{a report}} of the results of some digital {{computer}} simulation studies of a simplified model of a job shop production process. Such factors as the average effectiveness of schedules under the impact of <b>random</b> variations in <b>processing</b> times and the effect of changing operating policies are considered. The average manufacturing times and predictability of completion times were used as measures of effectiveness. ...|$|R
40|$|We {{consider}} two-stage {{production lines}} with an intermediate buffer. A buffer is needed when fluctuations occur. For single-product produc-tion lines fluctuations in capacity availability {{may be caused}} by <b>random</b> <b>processing</b> times, failures and random repair times. For multi-product production lines fluctuations are also caused by different processing time ratios for different products and by set up times. We examine whether it is possible to use the results developed for single-product flow lines, where the producti, on units have exponentially distrib-uted life- and repair times, for the multi-prod-uct case. As an example the case of a consumer electronics factory is studied. 1...|$|E
40|$|This paper {{addresses}} the shipment planning problem with <b>random</b> <b>processing</b> times in intermodal logistics via transfer ports. Shipment activities {{are divided into}} two groups according to regional settings. Activity processing times in region A are assumed to be random while those in region B are deterministic. At the beginning (stage 1), the forwarder assigns agents to all job activities (planning decision). In case a shipment delay is observed, an in-process adjustment (recourse decision) is implemented (stage 2). A two-stage stochastic programming model is established and an illustrative example is discussed. Managerial insights are presented in a simulation study...|$|E
40|$|A new {{architecture}} for adaptive arrays using {{frequency hopping}} modulation is addressed. The {{resolution of the}} array and the interference rejection increase substantially applying <b>random</b> <b>processing</b> to the carrier frequency of the signals. The proposed framework is composed of two different stages. The anticipative stage, devoted to minimize the noise and fixed interferences contribution and the GSLC stage which provides cancellation of follower jammers and solves the multiuser collision problem. The developed system requires neither temporal nor spatial reference for its implementation, only the frequency sequence must be known. An adaptive approach has been implemented, allowing a fast convergence to the optimal behavior. Peer ReviewedPostprint (published version...|$|E
40|$|Recently, {{numerous}} {{practical applications}} of multivariate Gaussian Markov random fields (GMRF) on a lattice have emerged. However, {{the theory is}} not satisfactorily developed. We give various properties of multivariate GMRF for multi-dimensional lattice. In particular, some multivariate MRF are given. We discuss estimation procedures and give a numerical example from the area of image <b>processing.</b> Markov <b>random</b> field image <b>processing</b> multivariate normal distribution auto-normal processes...|$|R
40|$|This paper {{concerns}} {{the design and}} development of electrical energy static counter, based on <b>random</b> pulse stream <b>processing.</b> Measurement proceeding, calibration and hardware implementation are checked in a prototype. As a result, a simple low cost measurement system has been obtained. The resulting measures have been compared with the ones obtained using a poly phase commercial analyser. A maximum 2 % error has been achieved. This measurement proceeding is patent pending...|$|R
3000|$|... [...]. Due to the {{presence}} of Rayleigh fading channels between the primary user and CRs, the local observations at CRs can be treated as independent and identically distributed (i.i.d.) <b>random</b> variables. For <b>processing</b> the observations at each CR, an energy detector (which is known as an optimal detector for i.i.d. signals) is implemented. In particular, local binary decisions are obtained by comparing the energy of the collected signals to a sensing threshold.|$|R
40|$|The paper {{describes}} turbulence simulation experiments {{based on}} the principles of control system theory, that is, the construction of a system characterized by a system function such that upon exciting the system with prescribed noise processes the output of the system is a realization of a <b>random</b> <b>processing</b> the desired statistical attributes of turbulence. An experimental autocorrelation of Jimsphere measurements of wind velocity was approximated to simulate turbulent wind. From the approximate autocorrelation function, the required system function is obtained, and a discrete time system is designed. Another method of simulation is to solve the convolution integral by filter techniques. Other methods include discrete Fourier simulation and self-similar simulation...|$|E
40|$|We {{consider}} {{the problem of}} determining optimal appointment schedule for a given sequence of jobs (e. g., medical procedures) on a single processor (e. g., operating room, examination facility), to minimize the expected total underage and overage costs when each job has a <b>random</b> <b>processing</b> duration given by a joint discrete probability distribution. Simple conditions on the cost rates imply that the objective function is submodular and L-convex. Then there exists an optimal appointment schedule which is integer {{and can be found}} in polynomial time. Our model can handle a given due date for the total processing (e. g., end of day for an operating room) after which overtime is incurred and, no-shows and emergencies. 1 Introduction and Motivation...|$|E
40|$|Today‟s {{manufacturing}} {{systems are}} characterized by large number of complexities such as random arrival patterns of jobs, <b>random</b> <b>processing</b> times, random failure rates, random repair times, random rejection of parts, etc. The analytical models cannot capture all the randomness mentioned above into the models. There {{is a need to}} incorporate them into models to have a practical and real life model. Simulation comes handy in this aspect. Discrete Event Simulation (DES) is used to model a manufacturing system to predict its performance. The inputs to this model include arrival rate, batch size, setup time, processing time, machine breakdown rate, machine breakdown frequency, machines and their capacities, buffers, rejection percentage and inspection time. The outputs that are estimated are work in process, flow time, utilization and throughput...|$|E
40|$|My {{research}} lies in mathematical signal <b>processing,</b> <b>random</b> matrix theory, frame theory, geometric {{functional analysis}} and compressed sensing. I {{am interested in}} using and developing tools {{for the analysis of}} sparse and low-dimensional representations of high-dimensional data. During my graduate studies in Bonn, I applied compressed sensing ideas to certain structured sparsity models with possible applications to hyperspectral imaging, computer vision, sensor networks. For my Master’s degree in UBC, I worked on Sigma-Delta quantization which is an active research area in signal processing concerning analog-to-digital conversion of bandlimited signals...|$|R
40|$|AbstractTimely {{diagnosis}} of faults in industrial equipments {{is a concern}} to guarantee the overall production process efficiency. For instance, vibration analysis {{has been one of}} the few important approaches for motor failure detection. However, vibration sensory signals are often corrupted with <b>random</b> noise, <b>processing</b> of which leads to inaccurate results. Thus, there is a necessity of low cost instrumentation with proper filtering capabilities for online vibration measurement which can be permanently fixed to the machine under test (MUT) for continuous monitoring and reliable diagnosis. The present work compares three signal averaging based filtering techniques for the purpose of analysis. The filters have been implemented in Field Programmable Gate Arrays (FPGA) which are characterized by reduced power consumption and high operational speed for real time applications. To test the functionality of the proposed algorithms, case study of an accelerometer data attached to an Induction motor has been taken up and the results have been analyzed...|$|R
40|$|Computerized {{interactive}} gaming requires {{automatic processing}} of {{large volume of}} random data produced by players on spot, such as shooting, football kicking, and boxing. This paper describes a supporting vector machine-based artificial intelligence algorithm {{as one of the}} possible solutions to the problem of <b>random</b> data <b>processing</b> and the provision of interactive indication for further actions. In comparison with existing techniques, such as rule-based and neural networks, and so forth, our SVM-based interactive gaming algorithm has the features of (i) high-speed processing, providing instant response to the players, (ii) winner selection and control by one parameter, which can be predesigned and adjusted according to the needs of interaction and game design or specific level of difficulties, and (iii) detection of interaction points is adaptive to the input changes, and no labelled training data is required. Experiments on numerical simulation support that the proposed algorithm is robust to random noise, accurate in picking up winning data, and convenient for all interactive gaming designs...|$|R
40|$|Several {{sampling}} {{approaches have}} been proposed in literature {{for the analysis of}} ow lines with stochastic processing times and finite buffer capacities. The system's performance can be evaluated by a linear programming formulation if the number of buffers between the stations is given. This work presents several mixed integer programming approaches to optimize the buffer allocation in ow lines with stochastic processing times. Sampling is used to represent the <b>random</b> <b>processing</b> times. The objective is to minimize the overall number of buffer spaces obtaining at least a given goal production rate. Numerical experiments are carried out to evaluate different sampling approaches and model formulations. These approaches are compared regarding the robustness of the allocation decision with respect to the sample sizes...|$|E
40|$|We {{consider}} a new scheduling model where emergency jobs appear during {{the processing of}} current jobs and must be processed immediately after the present job is completed. All jobs have <b>random</b> <b>processing</b> times and should be completed on a single machine. The most common case of the model is the surgery scheduling problem, where some elective surgeries are to be arranged in an operation room when emergency cases are coming during the operating procedure of the elective surgeries. Two objective functions are proposed to display this practice in machine scheduling problem. One is the weighted sum of the waiting times {{and the other is}} the weighted discounted cost function of the waiting times. We address some optimal policies to minimize these objectives...|$|E
40|$|We {{consider}} {{the problem of}} determining an optimal appointment schedule for a given sequence of jobs (e. g., medical procedures) on a single processor (e. g., operating room, examination facility, physician), to minimize the expected total underage and overage costs when each job has a <b>random</b> <b>processing</b> duration given by a joint discrete probability distribution. Simple conditions on the cost rates imply that the objective function is submodular and L-convex. Then there exists an optimal appointment schedule that is integer {{and can be found}} in polynomial time. Our model can handle a given due date for the total processing (e. g., end of day for an operating room) after which overtime is incurred, as well as no-shows and some emergencies. Key words: appointment scheduling; discrete convexity; optimizatio...|$|E
40|$|This paper {{presents}} a data acquisition and analysis {{system based on}} a computer sound card for measuring and <b>processing</b> <b>random</b> vibration signals. This system turns the computer into a two-channel measurement instrument which provides sample rate, simultaneous sampling, frequency range, filters and others essential capabilities required to perform random vibrations measurements. An easy-to-use software was developed for vibration monitoring and analysis, including facilities for data recording, digital signal processing and real time spectrum analyzer. Since the tasks of vibration data acquisition frequently require expensive hardware and software, this versatile system provides students a very accurate and inexpensive solution for experimental studying mechanical vibrations...|$|R
40|$|We have {{performed}} magnetotransport experiments on p+-(Ga,Mn) As/n+-GaAs Esaki diode devices. The spin-valve-like signal {{was observed in}} these devices in an in-plane magnetic field configuration due to Tunneling Anisotropic Magnetoresistance effect. The pattern of the observed magnetic reversal process strongly depends on the observed magnetic anisotropy of the (Ga,Mn) As layer - depending on its type {{the sign of the}} spin-valve-like signal can be changed by a simple rotation of the magnetic field by 90 ° or not. The type of the anisotropy is found to be strongly shaped, in a <b>random</b> way, during <b>processing</b> of the wafer...|$|R
40|$|Significant grain {{refinement}} {{has been achieved}} in a cast Mg-based AZ 91 alloy via large strain hot rolling. This is a simple processing method consisting of one rolling pass at intermediate temperatures with a large thickness reduction. The as-cast material was first homogeneized {{in order to obtain}} a homogeneous and equiaxed grain structure with a <b>random</b> texture. During <b>processing,</b> a double-peak basal texture develops, typical of rolled Mg alloys, that results from the operation of basal, prismatic and pyramidal 〈c+a〉 slip. The stabilization of this deformation texture during rolling suggests that the mechanism for {{grain refinement}} is continuous dynamic recrystallizationPeer reviewe...|$|R
