19|10000|Public
25|$|<b>Region</b> <b>of</b> <b>acceptance</b> : The set {{of values}} of the test {{statistic}} for which we fail to reject the null hypothesis.|$|E
25|$|The modern {{version of}} {{hypothesis}} testing is {{a hybrid of}} the two approaches that resulted from confusion by writers of statistical textbooks (as predicted by Fisher) beginning in the 1940s. (But signal detection, for example, still uses the Neyman/Pearson formulation.) Great conceptual differences and many caveats {{in addition to those}} mentioned above were ignored. Neyman and Pearson provided the stronger terminology, the more rigorous mathematics and the more consistent philosophy, but the subject taught today in introductory statistics has more similarities with Fisher's method than theirs. This history explains the inconsistent terminology (example: the null hypothesis is never accepted, but there is a <b>region</b> <b>of</b> <b>acceptance).</b>|$|E
5000|$|<b>Region</b> <b>of</b> <b>acceptance</b> : The set {{of values}} of the test {{statistic}} for which we fail to reject the null hypothesis.|$|E
25|$|Critical value: The {{threshold}} value delimiting the <b>regions</b> <b>of</b> <b>acceptance</b> and rejection {{for the test}} statistic.|$|R
25|$|Rough set {{methods can}} be applied as a {{component}} of hybrid solutions in machine learning and data mining. They {{have been found to be}} particularly useful for rule induction and feature selection (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three <b>regions</b> <b>of</b> rough sets are interpreted as <b>regions</b> <b>of</b> <b>acceptance,</b> rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.|$|R
50|$|The rough sets {{can be used}} {{to induce}} {{three-way}} classification decisions. The positive negative and boundary regions can be interpreted as <b>regions</b> <b>of</b> <b>acceptance,</b> rejection and deferment decisions, respectively. The probabilistic rough set model extends the conventional rough sets by providing more effective way for classifying objects. A main result of probabilistic rough sets is the interpretation of three-way decisions using a pair of probabilistic thresholds. The game-theoretic rough set model determine and interprets the required thresholds by utilizing a game-theoretic environment for analyzing strategic situations between cooperative or conflicting decision making criteria. The essential idea is to implement a game for investigating how the probabilistic thresholds may change in order to improve the rough set based decision making.|$|R
50|$|The modern {{version of}} {{hypothesis}} testing is {{a hybrid of}} the two approaches that resulted from confusion by writers of statistical textbooks (as predicted by Fisher) beginning in the 1940s. (But signal detection, for example, still uses the Neyman/Pearson formulation.) Great conceptual differences and many caveats {{in addition to those}} mentioned above were ignored. Neyman and Pearson provided the stronger terminology, the more rigorous mathematics and the more consistent philosophy, but the subject taught today in introductory statistics has more similarities with Fisher's method than theirs. This history explains the inconsistent terminology (example: the null hypothesis is never accepted, but there is a <b>region</b> <b>of</b> <b>acceptance).</b>|$|E
40|$|The LHCb {{experiment}} {{covers a}} unique <b>region</b> <b>of</b> <b>acceptance</b> at forward rapidities {{in the high}} energy proton-proton collisions of the LHC. This means that measurements of particle production in LHCb are directly sensitive to the parton distribution functions at low Björken-x values. Several measurements of inclusive W and Z/γ^∗ production with the Run-I dataset are reported in these proceedings. Further measurements of W and Z/γ^∗ production in association with inclusive jets and b- and c-tagged jets are also reported. Comment: On behalf of the LHCb collaboration. In Proceedings of the Third Annual Large Hadron Collider Physics Conference LHCP 15, Aug. 31 -Sept 5, 2015 St. Petersburg, Russi...|$|E
40|$|The {{study of}} the {{production}} of charged particles at high transverse momenta {{plays an important role}} in the understanding of the interaction mechanism of high-E/sub t/ jets with the dense nuclear matter. A prototype of a RICH (Ring Imaging CHerenkov) detector developed in the framework of the CERN-ALICE experiment has been fully integrated in the STAR experiment at RHIC (BNL). It allows identification of primary charged particles (namely pi, K and p) extending the <b>region</b> <b>of</b> <b>acceptance</b> of the experiment to medium-high transverse momenta (p/sub t/< 2. 5 GeV/c for pi and K, p/sub t/< 5 GeV/c for protons). Methods of pattern recognition and identification of charged particles using the RICH will be presented. Preliminary results on identified charged particle ratios at midrapidity and 1. 5 <p/sub t/< 2. 5 GeV/c will be also shown. (7 refs) ...|$|E
40|$|Multiple {{hypothesis}} testing {{is an important}} topic in statistics. Therefore, the problem addressed in this thesis is an important one. The Bayesian methods of hypotheses testing are widely used for solving different problems, and this technique is rather well developed. A lot of scientific works are dedicated {{to the development of}} this method. Many interesting and important results have been obtained in this field by different authors. Despite of this fact there still remain a lot of unsolved problems. For filling these gaps, in this thesis we consider different problems of testing many hypotheses by the Bayesian approach. In particular, in the Bayesian probm of many hypotheses testing concerning all the parameters of multidimensional normal distribution at correlation of observtion results we have obtained the following new results: the problem of computation of the risk function were considered; the formulae or calculation of multidimensional probability integrals by series using the reduction of dimensionality to one without information loss were derived; the formulae for calculation of product moments for normalized normally distributed random values were derived; the problems of existence and continuity of the probability distribution law of linear combination of exponents of quadratic forms of the normally distributed random vector, and, also, the problem of finding the closed form of this law were considered; the existence of this law and the opportunity of its unambiguous determination by calculated moments of the appropriate random variable were proved; the approximation <b>of</b> optimal <b>regions</b> <b>of</b> <b>acceptance</b> <b>of</b> hypotheses, which significantly simplify the algorithms of realization of general solutions of the task, is offered; the properties and interrelations of the developed methods and algorithms were investigated; the problem of choosing the loss function in the Bayesian problem of many hypotheses testing was considered; the results of sensitivity analysis of the considered Bayesian problem are given; the calculation results for concrete examples, which show the validity of the obtained results are given. Especially must be emphasized that new sequential method of testing many hypotheses based on special properties <b>of</b> <b>regions</b> <b>of</b> <b>acceptance</b> <b>of</b> hypothesin the conditional Bayesian task of testing many hypotheses is offered. The results of research of the properties of this method are given. They show the consistency, simplicity and optimality of the obtained results {{in the sense of the}} chosen criterion, which he error of one kind and the minimization of the probability of the error of the second kind. The examples of testing of hypotheses for the case of the sequential independent sample from the multidimensional normal law of probability distribution with correlated components are cited. They show the high quality of the offered method...|$|R
30|$|Numerical {{experiments}} show that, for {{the same}} sample size and subdividing ratio, larger parameters (absolute values) of the “mother dataset” yields wider <b>regions</b> <b>of</b> <b>acceptance.</b> Therefore, {{it is hard to}} find a universal standard for statistical significance. It is also hard to tell whether some published variances in parameters between different subsets is statistical significant or not, because either the MLE is not used (Chen 2009) or the information is not sufficient (Guzzetti et al. 2008). Nevertheless, test of statistical significance is highly recommended prior to physical interpretations of the variation of landslide size distribution between different subsets, especially for those with small sample size (Santangelo et al. 2013; Guns and Vanacker 2014). In the discussion, we will show that small sample size can cast a shadow on interpreting the physical constraints on landslide size distribution.|$|R
40|$|The {{realistic}} {{design of}} the VELO, implemented in SICB, {{has been used to}} determine the material distribution traversed by particles before entering the remainder of the LHCb spectrometer. In the range of polar angles, 8 mrad < theta < 270 mrad, the mean X 0 traversed, at the exit of the vertex tank, was found to be 18. 9 % of an X 0. The contributions of the different components of the VELO are given for both the total and different <b>regions</b> <b>of</b> the <b>acceptance</b> phase space. The impact of alternative designs on the material distribution is discussed...|$|R
30|$|To {{quantify}} {{the degree to}} which such examples might influence our results, we conducted a post-hoc image analysis collapsed across participants for each duration. We calculated the distance between the response click and the mass (i.e. the degree of incorrect localisation). In academic radiology, a <b>region</b> <b>of</b> <b>acceptance</b> (ROA) for lesion localisation is determined by taking into account the size of the largest lesion (e.g. Haygood et al., 2014). Following this convention, we measured the radius of the largest mass in the image set (27 mm) and added this value to the boundary values for all the target present images. Using this method, localisation is scored correct when a radiologist clicks within this ROA, allowing for a margin of response imprecision and reducing the ‘tightness’ of acceptance. We further examined the trials that were still incorrect to {{quantify the}} distance from the lesion boundary.|$|E
40|$|Models {{developed}} under ex ante approach {{are mostly}} {{based on some}} utility function. The assumptions underlying these models are hard to meet. These models also have stringent data requirements. The models developed under ex post approach, on the other hand, are simple to use, avoid the need of using a utility function and require less information. However, these models only offer two points in time: one before a trade reform and the other after the reform. As such, they do not provide a <b>region</b> <b>of</b> <b>acceptance</b> or rejection, which enables us to test the models empirically. Our study overcomes those problems, develops an intervention model, estimates the impulse response function and estimates the short-term and the long-run impacts of NAFTA (the North American Free Trade Agreement) on the variable representing Canada's welfare. Our results show that both the short-term and the long-term impacts are negative. Therefore, we conclude that Canada's welfare has, in fact, deteriorated, {{or at least not}} improved due to NAFTA. ...|$|E
40|$|We {{consider}} filters for {{the detection}} and extraction of compact sources on a background. We make a one-dimensional treatment (though a generalization to two or more dimensions is possible) assuming that the sources have a Gaussian profile whereas the background is modeled by an homogeneous and isotropic Gaussian random field, characterized by a scale-free power spectrum. Local peak detection is used after filtering. Then, a Bayesian Generalized Neyman-Pearson test is used to define the <b>region</b> <b>of</b> <b>acceptance</b> that includes not only the amplification but also the curvature of the sources and the a priori probability distribution function of the sources. We search for an optimal filter between a family of Matched-type filters (MTF) modifying the filtering scale such that it gives {{the maximum number of}} real detections once fixed the number density of spurious sources. We have performed numerical simulations to test theoretical ideas. Comment: 10 pages, 2 figures. SPIE Proceedings "Electronic Imaging II", San Jose, CA. January 200...|$|E
40|$|Multiple {{hypothesis}} testing {{is an important}} topic in statistics. Therefore, the problem addressed in this thesis is an important one. It is also a topic {{in which it is}} difficult to make a significant improvement, for various reasons. One reason is that often different users may have different objectives and with multiple hypotheses there is no unique objective function. In the thesis is recognized this fact and as the objective functions, estimated the quality of made decisions, are used minimization of the probabilities of the errors of one kind at restrictions of the probabilities of the errors of second kind. Such approach is a new one which causes the uniqueness <b>of</b> the <b>regions</b> <b>of</b> <b>acceptance</b> <b>of</b> hypotheses and, consequently, improves the quality of {{hypothesis testing}}. Thus conditional Bayesian tasks of testing many hypotheses are stated and solved. The concept of conditionality is used for designation {{of the fact that the}} Bayesian tasks are stated as conditional optimization problems where the probabilities of onetype errors are restricted and, under such conditions, the probabilities of second-type errors are minimized. The properties of obtained decision rules are investigated, and, on their basis, it is shown that the classical Bayesian problem of hypotheses testing is a special case of the considered. The calculation results of concrete examples have shown that the qualities of offered conditional taskssurpass the quality of the classical Bayesian task. They completely confirm the results of theoretical investigations. The convenience, simplicity and naturalness of introduction of similar gradation Kiefer,(1977) by the level of certainty of hypotheses testing on the basis of concrete observation result are shown in offeredconditional tasksQuasi-optimal procedures of many hypotheses testing are offered. They significantly simplify Bayesian algorithms of hypotheses testing and computation of the risk function. The obtained general solutions are reduced to concrete formulae for multivariate normal distribution of probabilities. The methods of approximate computation of the risk functions in Bayesian tasks of testing many hypotheses are offered. The properties and interrelations of the developed methods and algorithms are investigated. On the basis of simulation, the validity of the obtained results and conclusions made is shown. The results of sensitivity analysis of the conditional Bayesian problems are given and their advantages and drawbacks are considere...|$|R
40|$|A {{method for}} placing {{complicated}} variable cuts on data is presented. It {{is used to}} find an optimal cut on HRS acceptance, which selects maximum <b>region</b> <b>of</b> “flat ” <b>acceptance.</b> The method is based on utilizing R-function formalism, that allows to use equations of boundaries of a geometrical object to construct functions that are equal to 0 on the boundary of the object, have different signs {{inside and outside the}} object and in absolute value approximate distance functions to the boundary of the object. ...|$|R
40|$|AbstractThe {{cross section}} for the {{production}} of W bosons with subsequent decay W→τντ is measured with the ATLAS detector at the LHC. The analysis is based on a data sample that was recorded in 2010 at a proton–proton center-of-mass energy of s= 7 TeV and corresponds to an integrated luminosity of 34 pb− 1. The cross section is measured in a <b>region</b> <b>of</b> high detector <b>acceptance</b> and then extrapolated to the full phase space. The product of the total W production cross section and the W→τντ branching ratio is measured to be σW→τντtot= 11. 1 ± 0. 3 (stat) ± 1. 7 (syst) ± 0. 4 (lumi) nb...|$|R
30|$|We {{find that}} larger {{cumulative}} rainfall produces a steeper power law {{tail of the}} landslide area distribution (Fig.  2 b). This is opposite to the previously reported result that the power law tail becomes flatter with an increase of the cumulative rainfall (Chen 2009). The variation of power exponent with rainfall intensity is essential because it concerns the problem whether increased rainfall intensity will increase the relative proportion of small size landslides or large size landslides. The explanation of this disagreement goes {{beyond the scope of}} this paper. Instead, we suggest a test of significance prior to physical interpretation. However, the statistical significance of the result published by Chen (2009) cannot be exactly told since the MLE was not used. Nevertheless, a variance of power exponent 0.27 is obtained by subdividing a landslide dataset with a sample size less than 600. This result falls into the <b>region</b> <b>of</b> <b>acceptance</b> according to our numerical experiments (Fig.  4). Therefore, from a statistical point of view, there may be no adequate confidence to exclude the possibility that the variance of power exponent with rainfall intensity observed in Chen (2009) is due to random processes.|$|E
30|$|It {{is shown}} that, {{regardless}} of distribution function, for both γ and R, as sample size gets smaller or subdividing ratio gets larger, the <b>region</b> <b>of</b> <b>acceptance</b> gets wider (worse). It means smaller {{sample size and}} larger subdividing ratio expect larger differences in parameters to be observed {{for the sake of}} statistical significance. It also shows that the double Pareto distribution performs better on estimating the exponent (Fig.  5) while the Inverse Gamma distribution performs slightly better on estimating the rollover (Fig.  6). The regions of acceptance go beyond the range of figures for sample sizes less 250 if the subdividing ratio is large. This is because small sample size together with large subdividing ratio will yield unrealistic wide regions of acceptance. For example, with regard to a sample size of 100 and a subdividing ratio of 5, the regions of acceptance for γ estimated using the double Pareto distribution and the Inverse Gamma distribution are [− 14.69, 13.75] and [− 18.22, 15.24], respectively. Therefore, comparing the parameters of different subsets of a landslide dataset with an extreme small sample size, for instance less than 100 (Iwahashi et al. 2003), is practically statistically meaningless.|$|E
30|$|Similarly, we {{also use}} a {{straightforward}} way to inspect how the sample size influences the statistical significance of the comparison of the parameters of landslide size distribution between different subsets. Firstly, with regard to each sample size, the sample with parameters most similar to the theoretical values is picked out from the formerly produced 1, 000 Monte Carlo samples as the test sample for this sample size. Then, for each sample size, the corresponding test sample is randomly subdivided into two subsets according to a subdividing ratio. And six subdividing ratios, namely 1 : 1, 2 : 1, 3 : 1, 4 : 1, 5 : 1, and 6 : 1, are used to inspect the effect of subdividing ratio as well. For each test sample, the random subdivision is repeated 1, 000 times for each subdividing ratio. If we take “the observed differences in parameters between the two subsets are attributed to random processes” as the null hypothesis, the region of rejection and also the <b>region</b> <b>of</b> <b>acceptance</b> for a certain significance level (e.g., 0.05) can be estimated according to the statistics of the variances in parameters observed in the 1, 000 random trials.|$|E
40|$|Keywords: people <b>of</b> the <b>region.</b> sive <b>acceptance</b> <b>of</b> its ny local populations. f {{the genetic}} {{diversity}} supporting breeding Electronic Journal of Biotechnology 17 (2014) 65 – 71 Contents lists available at ScienceDirect Electronic Journaland conservation strategies of this genetic resource, in assessing its potential as field crop {{and to determine}} the origin and relationships among different forms and species. There are many molecular ⁎ Corresponding author. been reported as producing diosgenin, a secondary metabolite which {{is very important in}} pharmaceutical industry because it is used a...|$|R
40|$|The {{cross section}} for the {{production}} of W bosons with subsequent decay W -> tau nu(tau) is measured with the ATLAS detector at the LHC. The analysis is based on a data sample that was recorded in 2010 at a proton-proton center-of-mass energy of root s = 7 TeV and corresponds to an integrated luminosity of 34 pb(- 1). The cross section is measured in a <b>region</b> <b>of</b> high detector <b>acceptance</b> and then extrapolated to the full phase space. The product of the total W production cross section and the W -> tau nu(tau) branching ratio is measured to be sigma(tot) (W -> tau nu tau) = 11. 1 +/- 0. 3 (stat) +/- 1. 7 (syst) +/- 0. 4 (lumi) nb. (C) 2011 CERN. Published by Elsevier B. V. All rights reserved...|$|R
40|$|The Z→ττ {{cross section}} is {{measured}} with the ATLAS experiment at the LHC in four different final states {{determined by the}} decay modes of the τ leptons: muon-hadron, electron-hadron, electron-muon, and muon-muon. The analysis {{is based on a}} data sample corresponding to an integrated luminosity of 36 [*][*]pb- 1, at a proton-proton center-of-mass energy of √s = 7 [*][*]TeV. Cross sections are measured separately for each final state in fiducial <b>regions</b> <b>of</b> high detector <b>acceptance,</b> {{as well as in the}} full phase space, over the mass region 66 - 116 GeV. The individual cross sections are combined and the product of the total Z production cross section and Z→ττ branching fraction is measured to be 0. 97 ± 0. 07 (stat) ± 0. 06 (syst) ± 0. 03 (lumi) nb, in agreement with next-to-next-to-leading order calculations...|$|R
40|$|This paper {{considers}} {{the problem of}} compact source detection on a Gaussian background. We make a one-dimensional treatment (though a generalization to two or more dimensions is possible). Two relevant aspects of this problem are considered: {{the design of the}} detector and the filtering of the data. Our detection scheme is based on local maxima and it takes into account not only the amplitude but also the curvature of the maxima. A Neyman-Pearson test is used to define the <b>region</b> <b>of</b> <b>acceptance,</b> that is given by a sufficient linear detector that is independent on the amplitude distribution of the sources. We study how detection can be enhanced by means of linear filters with a scaling parameter and compare some of them that have been proposed in the literature (the Mexican Hat wavelet, the matched and the scale-adaptive filters). We introduce a new filter, that depends on two free parameters (biparametric scale-adaptive filter). The value of these two parameters can be determined, given the a priori pdf of the amplitudes of the sources, such that the filter optimizes the performance of the detector {{in the sense that it}} gives the maximum number of rea...|$|E
30|$|This paper {{considers}} {{the detection of}} point sources in two-dimensional astronomical images. The detection scheme we propose is based on peak statistics. We discuss {{the example of the}} detection of far galaxies in cosmic microwave background experiments throughout the paper, although the method we present is totally general and can be used in many other fields of data analysis. We consider sources with a Gaussian profile—that is, a fair approximation of the profile of a point source convolved with the detector beam in microwave experiments—on a background modeled by a homogeneous and isotropic Gaussian random field characterized by a scale-free power spectrum. Point sources are enhanced with respect to the background by means of linear filters. After filtering, we identify local maxima and apply our detection scheme, a Neyman-Pearson detector that defines our <b>region</b> <b>of</b> <b>acceptance</b> based on the a priori pdf of the sources and the ratio of number densities. We study the different performances of some linear filters that have been used in this context in the literature: the Mexican hat wavelet, the matched filter, and the scale-adaptive filter. We consider as well an extension to two dimensions of the biparametric scale-adaptive filter (BSAF). The BSAF depends on two parameters which are determined by maximizing the number density of real detections while fixing the number density of spurious detections. For our detection criterion the BSAF outperforms the other filters in the interesting case of white noise.|$|E
40|$|This article {{considers}} {{the detection of}} point sources in two dimensional astronomical images. The detection scheme we propose is based on peak statistics. We discuss {{the example of the}} detection of far galaxies in Cosmic Microwave Background experiments throughout the paper, although the method we present is totally general and can be used in many other fields of data analysis. We assume sources with a Gaussian profile [...] that is a fair approximation of the profile of a point source convolved with the detector beam in microwave experiments [...] on a background modeled by a homogeneous and isotropic Gaussian random field characterized by a scale-free power spectrum. Point sources are enhanced with respect to the background by means of linear filters. After filtering, we identify local maxima and apply our detection scheme, a Neyman-Pearson detector that defines our <b>region</b> <b>of</b> <b>acceptance</b> based on the a priori pdf of the sources and the ratio of number densities. We study the different performances of some linear filters that have been used in this context in the literature: the Mexican Hat wavelet, the matched filter and the scale-adaptive filter. We consider as well an extension to two dimensions of the biparametric scale adaptive filter (BSAF). The BSAF depends on two parameters which are determined by maximizing the number density of real detections while fixing the number density of spurious detections. For our detection criterion the BSAF outperforms the other filters in the interesting case of white noise. Comment: 21 pages, 3 figures, version accepted for publication on EURASIP Journal on Applied Signal Processing: Applications of Signal Processing in Astrophysics and Cosmolog...|$|E
40|$|See {{paper for}} full list of authors - 8 pages (20 {{including}} author list), 10 figures, 4 tables, submitted to Phys. Lett. BThe cross section {{for the production}} of W bosons with subsequent decay W to tau nu is measured with the ATLAS detector at the LHC. The analysis is based on a data sample that was recorded in 2010 at a proton-proton center-of-mass energy of sqrt(s) = 7 TeV and corresponds to an integrated luminosity of 34 pb^- 1. The cross section is measured in a <b>region</b> <b>of</b> high detector <b>acceptance</b> and then extrapolated to the full phase space. The product of the total W production cross section and the W to tau nu branching ratio is measured to be 11. 1 +/- 0. 3 (stat) +/- 1. 7 (syst) +/- 0. 4 (lumi) nb...|$|R
5000|$|Through his ongoing musical career, Lyle cited Eddy Arnold {{as one of}} {{the strongest}} {{influences}} in the urban audiences <b>of</b> <b>acceptance</b> <b>of</b> [...] "hillbilly music", renaming it, more politely, [...] "Traditional Country". As recordings obtained more quality and availability, the era from the 1930s to the 1960s covered the <b>region</b> <b>of</b> Southern Illinois with a thick blanket of melodies and lyrics.|$|R
40|$|Despite {{considerable}} {{concern of}} bioethicists, disabilities rights activists, feminists and others about {{the spread of}} prenatal diagnostic technologies, their routine acceptance {{in many parts of}} the world continues at a rapid pace. Yet, there is wide variation by country and <b>region</b> in rates <b>of</b> <b>acceptance</b> <b>of</b> prenatal diagnosis. We draw on John McKinlay's model of how a medical innovation becomes routinized to explore the circumstances that led to the widespread use of one prenatal diagnostic screen [...] the maternal serum alpha fetoprotein (MSAFP) test for the detection of neural tube defects and other developmental disabilities. As predicted by McKinlay's model, analysis of published data suggests that strong institutional or provider support is the best predictor of women's level <b>of</b> MSAFP test <b>acceptance.</b> Data collected at a health maintenance organization in California illuminate the processes through which medico-legal and institutional forces affect the use of MSAFP screening. By examining the language women use to talk about MSAFP screening, we show how providers also shape women's understandings of the meaning and purpose of MSAFP screening. These data ultimately shed light on how the very ethical issues which concern critics of prenatal diagnosis become obscured in the processes by which this screening test becomes accepted as routine. maternal serum alpha fetoprotein prenatal screening abortion routinization of medical innovations...|$|R
40|$|Modern {{computer-aided}} vision motion systems {{provide a}} computerized and fully integrated tool kit for biomechanical measurement and analysis. These tools {{are useful for}} evaluation of problems, prescription of treatment and evaluation of such treatment. Many of these systems use reflective markers placed on key anatomical sites {{of the body to}} detect accurate three-dimensional spatial positions of the limbs being measured. While these systems ease automated data gathering, there are issues, such as the correspondence between an observed target and an established track, that require significant human intervention when markers disappear from view for short periods of time. When the system loses sight of a marker, it has no way of knowing where that marker will reappear and the track becomes broken or disjointed. Once the missing marker comes back into view, many current systems do not easily establish an association between the marker and its original track. [...] In this thesis {{a solution to the problem}} of making correspondence between markers and their track histories was designed and tested. This solution also provided the capability of predicting the path of markers when they were out of view of the cameras. To test the algorithm three different repetitive motions were tracked using the Flock of Birds measurement system. [...] The solution used a three-state Kalman filter to predict marker locations. The Kalman filter was coupled with constraints to determine matches between tracks and their corresponding marker positions. These constraints modelled a <b>Region</b> <b>of</b> <b>Acceptance</b> (ROA), distance from the center of the ROA to the last known position of a marker, and velocity matching. [...] The Kalman predictor algorithm, because it is linear in nature, was able to predict the motion accurately while there was no change in acceleration. However, the Kalman predictor, coupled with the constraints, was useful in predicting and matching markers over a longer (100 - 500...|$|E
40|$|This paper {{considers}} {{the problem of}} compact source detection on a Gaussian background in 1 D. Two aspects of this problem are considered: {{the design of the}} detector and the filtering of the data. Our detection scheme is based on local maxima and it takes into account not only the amplitude but also the curvature of the maxima. A Neyman-Pearson test is used to define the <b>region</b> <b>of</b> <b>acceptance,</b> that is given by a sufficient linear detector that is independent on the amplitude distribution of the sources. We study how detection can be enhanced by means of linear filters with a scaling parameter and compare some of them (the Mexican Hat wavelet, the matched and the scale-adaptive filters). We introduce a new filter, that depends on two free parameters (biparametric scale-adaptive filter). The value of these two parameters can be determined, given the a priori pdf of the amplitudes of the sources, such that the filter optimizes the performance of the detector {{in the sense that it}} gives the maximum number of real detections once fixed the number density of spurious sources. The combination of a detection scheme that includes information on the curvature and a flexible filter that incorporates two free parameters (one of them a scaling) improves significantly the number of detections in some interesting cases. In particular, for the case of weak sources embedded in white noise the improvement with respect to the standard matched filter is of the order of 40 %. Finally, an estimation of the amplitude of the source is introduced and it is proven that such an estimator is unbiased and it has maximum efficiency. We perform numerical simulations to test these theoretical ideas and conclude that the results of the simulations agree with the analytical ones. Comment: 15 pages, 13 figures, version accepted for publication in MNRAS. Corrected typos in Tab. ...|$|E
30|$|Although at {{the summary}} {{statistic}} level {{we did not}} replicate the lack of localisation information, we did find trials on which detection responses were correct but those for localisation were incorrect. We were therefore able to use these to investigate factors that might contribute to an apparent dissociation between detection and localisation. First, variability in the target-present images might be contributing misleading data to the summary statistics. Using real-world stimuli rather than typical laboratory visual search displays allows for high ecological validity, but the available images tend to be highly variable {{and it is difficult}} to control for factors such as co-existing variables (e.g. breast calcifications, target number and size, and breast tissue type). Indeed, we identified images where there were clear clusters of incorrect localisation corresponding to a specific visual feature in the image (Fig. 5), suggesting the detection response was based on an incorrect identification (i.e. of the distracting feature). Second, we find evidence that coarse localisation information is often present in apparently incorrect responses. When we use a <b>region</b> <b>of</b> <b>acceptance</b> around the lesion, we see clusters of correct localisation responses surrounding the lesion. This suggests that task demands, such as having to hold the information through a detection response and subsequent location screen, may result in a loss of precision. Alternatively, it may be that the location information is only present at a coarse level in the first place (and is perfectly maintained). Finally, on trials where there is detection but incorrect localisation (by whatever definition one uses), it is important to consider the contribution of correct detection guesses. We used a method for estimating the effect correct guesses might have on the subsequent results. The key high-density condition, which is most similar to that of Evans et al. (2013, 2016), gives no evidence for there being more ‘detection without localisation’ trials than would be predicted to be lucky guesses. Thus, the pattern taken from a small number of trials suggest that in the difficult images, such as our set of high-density mammograms, apparent ‘detection without localisation’ responses can be accounted for by ‘lucky’ guesses.|$|E
40|$|This {{research}} have purpose, 1) to know development acceptance for service parking area retribution in Jambi {{city and the}} contribution for retribution in region, PAD and acceptance region in Jambi city, 2) to know effectiveness degree acceptance for service Parking area retribution ini Jambi City about acceptance resources for 2006 until 2011 period, 3) to know factors which is influence acceptance for service parking area retribution in Jambi city. Research method which is used for this research that is Analysis from primary data is got through dissemination quetiosn list and live interview with Skilled worker parking area in Jambi city.   For secondary data is time series during calculation era. Result of research to show that: 1) on the average during 2006 until 2011 period, the total <b>of</b> <b>acceptance</b> <b>region</b> in Jambi City has growing about 11, 88 %.   In the same period, PAD in Jambi City has growing about 18. 44 %, during 2006 until 2011 period, rapid <b>of</b> <b>region</b> retribution <b>acceptance</b> growing in Jambi City is About 4, 45 % and rapid of parking area retribution growing in Jambi City has increased about 13, 62 %. On the Average during 2006 until 2011 period contribution of parking area retribution to the total <b>of</b> <b>acceptance</b> <b>region</b> is about 0, 29 %.   In the same period contribution of parking area retribution to PAD in Jambi city is about 3, 48 % and contribution of parking area retribution to region retribution is about 11, 44 %. 2)   The efficiency coefficient <b>of</b> <b>acceptance</b> for parking area retribution in Jambi City can not be calculated {{because there is no}} cost of collecting for parking area retribution so the effeciency can not be calculated.   While the efficiency coefficient <b>of</b> <b>acceptance</b> for parking area retribution fo 2006 until 2011 period is very effective with value of mean contribution per year about 89, 79 %.   3) Among five factors which is influence acceptance for service parking area retribution, apparently, the most significant which is influence its increasing is value of parking area retribution while the othe factors do not influence for increasing and reduction <b>of</b> <b>acceptance</b> parking area retribution. Keywords: Retribution, contribution, effectivenes...|$|R
40|$|The Z -> tau {{tau cross}} section is {{measured}} with the ATLAS experiment at the LHC in four different final states {{determined by the}} decay modes of the tau leptons: muon-hadron, electron-hadron, electron-muon, and muon-muon. The analysis {{is based on a}} data sample corresponding to an integrated luminosity of 36 pb(- 1), at a proton-proton center-of-mass energy of root s = 7 TeV. Cross sections are measured separately for each final state in fiducial <b>regions</b> <b>of</b> high detector <b>acceptance,</b> {{as well as in the}} full phase space, over the mass region 66 - 116 GeV. The individual cross sections are combined and the product of the total Z production cross section and Z -> tau tau branching fraction is measured to be 0. 97 +/- 0. 07 (stat) +/- 0. 06 (syst) +/- 0 : 03 (lumi) nb, in agreement with next-to-next-to-leading order calculations...|$|R
40|$|We {{propose to}} use the CCM {{spectrometer}} {{to carry out a}} sensitive search for charmed particles produced in strong interactions at a nominal beam energy of 150 GeV/c. We limit ourselves to production in the beam diffraction <b>region</b> for reasons <b>of</b> <b>acceptance</b> and reconstruction. We present results of a test run undertaken in April 1975 to demonstrate the feasibility of K{sub S}{sup 0 } trigger, which we incorporate in the present proposal. Results of the test are combined with new insights which increase our sensitivity to charmed particle production by a large factor. We request a total of 2 x 10 {sup 11 } negative pions at a rate of 10 {sup 6 } per pulse. With this illumination we estimate that we can measure a large number of hadronic decay modes. We make estimates of enhancements in mass spectra from charmed particle production and decay and calculate expected backgrounds using data from existing experiments. With conservative assumptions about the charmed particle model, we calculate effects corresponding to ten or more standard deviations in our most favorable channels...|$|R
