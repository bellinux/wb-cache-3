430|914|Public
25|$|The <b>raw</b> <b>score</b> for the 4th grade {{writing test}} is {{calculated}} as shown.|$|E
25|$|The ELA (10th–11th grade) <b>raw</b> <b>score</b> is {{calculated}} {{as shown in}} this chart.|$|E
25|$|The 9th grade reading test <b>raw</b> <b>score</b> is {{calculated}} {{as shown in}} this chart.|$|E
5000|$|... #Subtitle level 2: <b>Raw</b> <b>scores,</b> scaled scores, and percentiles ...|$|R
5000|$|Older {{versions}} of the CaMLA EPT Test (e.g. Forms A, B and C) use the same scoring approach. Institutions can use equivalence tables to see how <b>raw</b> <b>scores</b> on the current forms of the test compare to the <b>raw</b> <b>scores</b> on the older version of the test, which had 100 items.|$|R
5000|$|For {{a sample}} of size n, the n <b>raw</b> <b>scores</b> [...] are {{converted}} to ranks , and [...] is computed from: ...|$|R
25|$|The <b>raw</b> <b>score</b> is {{multiplied by}} {{a degree of}} {{difficulty}} factor, derived from the number and combination of movements attempted. The diver with the highest total score after a sequence of dives is declared the winner.|$|E
25|$|The drop in SAT verbal scores, in particular, {{meant that}} the {{usefulness}} of the SAT score scale (200 to 800) had become degraded. At the top end of the verbal scale, significant gaps were occurring between raw scores and uncorrected scaled scores: a perfect <b>raw</b> <b>score</b> no longer corresponded to an 800, and a single omission out of 85 questions could lead to a drop of 30 or 40 points in the scaled score. Corrections to scores above 700 had been necessary {{to reduce the size of}} the gaps and to make a perfect <b>raw</b> <b>score</b> result in an 800. At the other end of the scale, about 1.5 percent of test takers would have scored below 200 on the verbal section if that had not been the reported minimum score. Although the math score averages were closer to the center of the scale (500) than the verbal scores, the distribution of math scores was no longer well approximated by a normal distribution. These problems, among others, suggested that the original score scale and its reference group of about 10,000 students taking the SAT in 1941 needed to be replaced.|$|E
25|$|The {{score is}} {{calculated}} {{based on a}} bell curve. For example, if many do well in a paper, there is a potential reduction of the <b>raw</b> <b>score</b> and vice versa, despite differences in performances each year. For example, an average student would get a score of 210+ if he/she scored 3 low As and a high B. A highly proficient student would easily get a score of 250 and above if they scored 2 or 3 A*s and high As.|$|E
30|$|Step 5 : Fitness scaling operator. The rank {{function}} {{was used for}} the fitness scaling operator, which could scale the <b>raw</b> <b>scores</b> based on the rank of each individual. The fitness scaling operator converted <b>raw</b> fitness <b>scores</b> to values in a range that was suitable for the selection function.|$|R
2500|$|Passing {{is based}} on scaled <b>scores</b> – <b>raw</b> <b>scores</b> are not {{directly}} used to determine passing, nor are they reported, except in rough form in the [...] "Reference Information" [...] section. <b>Raw</b> <b>scores</b> are converted to a standard scale, so that equivalent performance on tests from different years and different levels of difficulty yields the same scaled score. The scaled scores are reported, broken down by section, {{and these are the}} scores used to determine passing.|$|R
5000|$|... 1950: The first open team {{event in}} Bermuda between the US, Europe and Britain who played {{round-robin}} for <b>raw</b> <b>scores</b> or [...] "total points".|$|R
25|$|What we {{call the}} Sonneborn-Berger system was not invented by Sonneborn or Berger, {{and it was not}} {{originally}} designed for tie-breaking. It was invented by Oscar Gelbfuhs about 1873 {{to be used as a}} weighted score in round-robin tournaments. It would be used instead of the <b>raw</b> <b>score</b> for final places. In 1886 Sonneborn criticized the system and suggested an improvement that would give a better weighted score. His suggestion was to add the square of the player's points to the amount calculated as above. In 1887 and 1888 Berger studied Gelbfuhs' system and the suggestion of Sonneborn. This improvement became known as the Sonneborn-Berger system.|$|E
25|$|Students {{receive their}} online score reports {{approximately}} {{three weeks after}} test administration (six weeks for mailed, paper scores), with each section graded {{on a scale of}} 200–800 and two sub scores for the writing section: the essay score and the multiple choice sub score. In addition to their score, students receive their percentile (the percentage of other test takers with lower scores). The <b>raw</b> <b>score,</b> or the number of points gained from correct answers and lost from incorrect answers is also included. Students may also receive, for an additional fee, the Question and Answer Service, which provides the student's answer, the correct answer to each question, and online resources explaining each question.|$|E
25|$|Since {{the early}} 20th century, raw scores on IQ tests have {{increased}} {{in most parts of}} the world. When a new version of an IQ test is normed, the standard scoring is set so performance at the population median results in a score of IQ 100. The phenomenon of rising <b>raw</b> <b>score</b> performance means if test-takers are scored by a constant standard scoring rule, IQ test scores have been rising at an average rate of around three IQ points per decade. This phenomenon was named the Flynn effect in the book The Bell Curve after James R. Flynn, the author who did the most to bring this phenomenon to the attention of psychologists.|$|E
30|$|Together these features, {{along with}} a ranking of each {{occupation}} on each dimension, {{make it possible to}} compare a pair of occupations according to their places in the distributions of the various O*Net dimensions. Following this premise, we construct measures of the distance between each pair of occupations based on rank, as well as the <b>raw</b> <b>scores.</b> Although the results reported in the paper use the distances based on <b>raw</b> <b>scores,</b> the results are robust to using the rank-based distances as well.|$|R
40|$|The {{purpose of}} this project was to {{determine}} whether the selection of ability groups by the use of pretests and posttests was a valid means for grouping students. The degree of success was measured by comparing students <b>raw</b> <b>scores</b> on the pretest with their <b>raw</b> <b>scores</b> on the posttest after the grouping and treatment had been completed. During {{the first week of the}} first quarter of 1987, a review and pretest were given to the seventh and eighth grade students. The <b>raw</b> <b>scores</b> obtained from these tests were used to group the students in mathematics for the 1987 - 88 school year. The Pearson Product Moment Correlation formula produced a positive coefficient of. 286 when comparing the pretest and posttest. This indicated a low correlation, a definite but small relationship. Based on these findings, the null hypothesis was rejected at the. 01 level of confidence...|$|R
50|$|Items are scored using a 4-point Likert-type {{scale ranging}} from 0-3. Domain <b>raw</b> <b>scores</b> are {{transformed}} to a common score range of 0-100, with 0 representing no dysfunction.|$|R
25|$|The first {{administration}} of the SAT occurred on June 23, 1926, when it {{was known as the}} Scholastic Aptitude Test. This test, prepared by a committee headed by Princeton psychologist Carl Campbell Brigham, had sections of definitions, arithmetic, classification, artificial language, antonyms, number series, analogies, logical inference, and paragraph reading. It was administered to over 8,000 students at over 300 test centers. Men composed 60% of the test-takers. Slightly over a quarter of males and females applied to Yale University and Smith College. The test was paced rather quickly, test-takers being given only a little over 90 minutes to answer 315 questions. The <b>raw</b> <b>score</b> of each participating student was converted to a score scale with a mean of 500 and a standard deviation of 100. This scale was effectively equivalent to a 200 to 800 scale, although students could score more than 800 and less than 200.|$|E
25|$|After {{finishing}} second behind Annika Sörenstam at the ANZ Ladies Masters in Australia, Stupples carded {{the best}} 72-hole <b>raw</b> <b>score</b> in LPGA Tour history (258) {{to win her}} maiden title, the Welch's Fry Championship, by five strokes. This win made her eligible to join the Ladies European Tour and begin earning points for the 2005 Solheim Cup. She had not joined the LET when joining the LPGA as she could not at the time afford the joining fee of £600. She followed this up by winning the Women's British Open at Sunningdale where she became only the second player in history to record a double eagle or albatross at an LPGA major championship (began the final round with an eagle, albatross {{on the first two}} holes). She became only the third English player to win a major after Laura Davies and Alison Nicholas. and was the first home winner since Penny Grice-Whittaker in 1991. She crossed the $1 million mark in LPGA career earnings at the U.S. Women's Open and finished in sixth place on the money list.|$|E
25|$|For marching band, schools {{compete against}} other {{schools in the}} same UIL conference. The 33 regions are grouped into seven areas for Class 5A and 6A schools, and five areas for Class 1A, 2A, 3A, and 4A schools. All schools of all classes compete in region {{competition}} annually, as a fall semester activity. However, in even-numbered years schools in Class 4A and Class 6A can advance from region to area and state, and in odd-numbered years schools from Class 1A, 2A, 3A, and 5A can advance from region to area and state. In order for bands to advance from Region to Area, {{they need to get}} an overall Division 1 from at least two Region judges, whether it be a 1-2-1, 2-1-1, 1-1-2, etc... the bands then will advance to the Area Competition. In order to make State, the band needs {{to make it to the}} Area Final competition AND make it to the top 3-5 bands to get to State. The Area and State winner is the school with the lowest ordinal score when the rankings from each judge are tabulated. For example, School 1 receives a first place score from three judges, a second place score from the fourth judge, and a fourth place score from the fifth judge. The ordinal score for School 1 is 9 (1+1+1+2+4). School 2 receives two first place scores and three second place scores. The ordinal score for School 2 is 8 (2+2+2+1+1). School 2 would be the champion despite receiving fewer first place scores because School 2's ordinal score is lower than School 1's. Thus, overall placement in the caption area for each judge is more important than the <b>raw</b> <b>score</b> awarded by the judge.|$|E
30|$|Participants were {{administered}} the letter-number sequencing subtest of the Wechsler Adult Intelligence Scale-IV (WAIS-IV; Pearson 2008). <b>Raw</b> <b>scores</b> (ranging from 0 to 21) were calculated {{as the total}} number of trials correct, from which WAIS-IV age-normed scaled scores were computed. The Mini-Mental State Examination (MMSE) was used as a brief, objective measure of cognitive capability (Folstein et al. 1975). <b>Raw</b> <b>scores</b> (ranging from 0 to 30) were calculated as {{the total number of}} trials correct. Threshold eligibility was set at[*]≥[*] 24, and all participants met or exceeded this score.|$|R
40|$|Corsi's block-tapping {{test and}} WISC-R {{were given to}} 1122 {{children}} from 11 to 16 years of age. Corsi's <b>raw</b> <b>scores</b> were transformed into standard scores like those for the WISC-R subtests. Reliabilities, standard <b>score</b> equivalents of <b>raw</b> <b>scores,</b> correlations with scores on WISC-R subtests, scales and factor scores are presented. A Principal Factor analysis of intercorrelations for Corsi's test and WISC-R subtests shows a three-factor solution with Corsi's test loading on the Third Factor. Results agree with Wielkiewicz's (1990) hypothesis about the construct underlying WISC-R Third Factor as "executive" and short-term memory processes...|$|R
40|$|This study {{analyzes}} the converted scores made on Aptitudes "G" (intelligence) and "V" (verbal" and the <b>raw</b> <b>scores</b> made on Part "H" (three-dimensional space) and Part "I" of the General Aptitude Test Battery by {{students enrolled in}} beginning industrial arts courses, advanced industrial arts courses, and beginning English at North Texas State College, Denton, Texas, and the academic grades made by theses same students {{in order to determine}} what relationship exists between both the converted and <b>raw</b> <b>scores</b> made on the foregoing parts of the GATB and academic grades...|$|R
500|$|The third {{revision}} (Form L-M) in 1960 of the Stanford–Binet IQ test {{used the}} deviation scoring pioneered by David Wechsler. For rough comparability of scores between {{the second and}} third revision of the Stanford–Binet test, scoring table author Samuel Pinneau set 100 for the median standard score level and 16 standard score points for each standard deviation above or below that level. The highest score obtainable by direct look-up from the standard scoring tables (based on norms from the 1930s) was IQ 171 at various chronological ages from three years six months (with a test <b>raw</b> <b>score</b> [...] "mental age" [...] of six years and two months) up to age six years and three months (with a test <b>raw</b> <b>score</b> [...] "mental age" [...] of ten years and three months). The classification for Stanford–Binet L-M scores does not include terms such as [...] "exceptionally gifted" [...] and [...] "profoundly gifted" [...] in the test manual itself. David Freides, reviewing the Stanford–Binet Third Revision in 1970 for the Buros Seventh Mental Measurements Yearbook (published in 1972), commented that the test was obsolete by that year.|$|E
500|$|The Wechsler {{intelligence}} {{scales were}} originally developed from earlier intelligence scales by David Wechsler. The first Wechsler test published was the Wechsler–Bellevue Scale in 1939. The Wechsler IQ tests {{for children and}} for adults are {{the most frequently used}} individual IQ tests in the English-speaking world and in their translated versions are perhaps the most widely used IQ tests worldwide. The Wechsler tests have long been regarded as the [...] "gold standard" [...] in IQ testing. The Wechsler Adult Intelligence Scale—Fourth Edition (WAIS–IV) was published in 2008 by The Psychological Corporation. The Wechsler Intelligence Scale for Children—Fifth Edition (WISC–V) was published in 2014 by The Psychological Corporation, and the Wechsler Preschool and Primary Scale of Intelligence—Fourth Edition (WPPSI–IV) was published in 2012 by The Psychological Corporation. Like all current IQ tests, the Wechsler tests report a [...] "deviation IQ" [...] as the standard score for the full-scale IQ, with the norming sample median <b>raw</b> <b>score</b> defined as IQ 100 and a score one standard deviation higher defined as IQ 115 (and one deviation lower defined as IQ 85).|$|E
2500|$|Then, the <b>raw</b> <b>score</b> is {{converted}} to a scaled score. [...] As with the other tests, a scaled score of 2100 meets the standard and 2400 is a commended performance. In 2007, the 11th grade [...] "met standard" [...] level was a <b>raw</b> <b>score</b> of 42, 10th was 44, and 9th was 28; 7th [...] "met standard" [...] with 26 points and 4th with 20. However, the points needed to meet the standard may change slightly {{from year to year}} depending on the test's level of difficulty, so all students should do their best and not aim for a particular numeric score.|$|E
30|$|The Arabic {{version of}} the Illinois Test of Psycholinguistic Abilities (ITPA). It has the {{following}} subtests: auditory reception, visual reception, auditory association, visual association, verbal expression, manual expression, grammatic closure, visual closure, auditory sequential memory, and visual sequential memory [18, 19]. The test was applied for obtaining <b>raw</b> <b>scores</b> for each child in each subtest. The <b>raw</b> <b>scores</b> of each subtest were converted to scaled scores according to the child’s mental age. These scores reflect the child’s performance in the subtests. The sum of the scaled scores was used to obtain a mean of scaled scores for each child.|$|R
40|$|Participants {{generate}} multiple <b>raw</b> <b>scores</b> {{within each}} cell of reaction time (RT) experiment designs. For analy-sis, the <b>raw</b> <b>scores</b> within each condition are summarized by {{an estimate of}} central tendency (e. g., the mean). Un-fortunately, these estimates may be biased because <b>raw</b> RT <b>scores</b> contain spurious or erroneous observations (Ulrich & Miller, 1994). The spurious scores reflect vari-ous nontarget processes. For instance, the RT recorded by the computer {{may be due to}} a guess, an accidental but-tonpress, or a momentary lapse in attention. Such scores can cause problems because (1) they introduce bias and reduce the power of significance tests (Ratcliff, 1993) and (2) {{they are more likely to}} take on values that are more extreme than those that are generated by the psychologi-cal process of interest (Ulrich & Miller, 1994). Values ar...|$|R
30|$|In {{some of the}} {{previous}} test, validation studies, test psychometric properties, and result interpretations were analyzed typically through conventional analysis methods. For instances, internal consistency of test items {{in the form of}} Cronbach’s alpha is usually examined for the indication of reliability; face and content validity are obtained solely from a panel of experts; <b>raw</b> <b>scores</b> from each item were summed across for a total mean score for comparison of students’ performance or for parametric statistical test examination. Such conventional analyses of <b>raw</b> <b>scores</b> assumed interval-scale data for an ordinal-scale data (Wright 1999) where parametric statistical tests are not readily to be performed on (Wright and Master 1982; Boone et al. 2014; Liu 2010). When parametric test was done on ordinal data, the results have an element of error. In other words, {{the reliability and validity of}} data are jeopardized. In the present study, the psychometric properties of the test were assessed by Rasch modeling analysis and <b>raw</b> <b>scores</b> were transformed into interval data (Rasch estimates in unit logit) for the conduct of parametric statistical test—these features clearly advanced the precursory studies.|$|R
2500|$|When {{the system}} is used to break ties between equally scoring players, adding in {{the square of the}} player's <b>raw</b> <b>score</b> does no good, so the Sonneborn {{improvement}} is omitted. However, the system has retained the Sonneborn-Berger name [...]|$|E
2500|$|In addition, a [...] "Reference Information" [...] {{section is}} {{provided}} {{on the report}} card; this is purely informational – for the examinee’s future studies – and is not used in determining if an examinee has passed. The grade given {{is based on the}} <b>raw</b> <b>score,</b> and is either A, B, or C, accordingly as the <b>raw</b> <b>score</b> was 67% or above, between 34% and 66%, or below 34%. This reference information is given for vocabulary, grammar, and reading on the N4 and N5, and for vocabulary and grammar (but not reading) on the N1, N2, and N3. In both cases, this breaks down the score on the [...] "Language Knowledge" [...] section into separate skills, but in neither case is performance on the listening section analyzed.|$|E
2500|$|Despite {{the name}} of the competition, {{students}} are allocated awards for their performance relative to other students in their region, of the same year level. For Australian students, this means their State or Territory, and for other students, their country. Although the personal data such as date of birth and gender are collected, this is not used in the percentile ranking, which is only determined by the <b>raw</b> <b>score.</b> The award scheme is as such ...|$|E
40|$|The Academic Ranking of World Universities (ARWU) {{published}} {{by researchers at}} Shanghai Jiao Tong University {{has become a major}} source of information for university administrators, country officials, students and the public at large. Recent discoveries regarding its internal dynamics allow the inversion of published ARWU indicator <b>scores</b> to reconstruct <b>raw</b> <b>scores</b> for 500 world class universities. This paper explores <b>raw</b> <b>scores</b> in the ARWU and in other contests to contrast the dynamics of rank-driven and score-driven tables, and to explain why the ARWU ranking is a score-driven procedure. We show that the ARWU indicators constitute sub-scales of a single factor accounting for research performance, and provide an account of the system of gains and non-linearities used by ARWU. The paper discusses the non-linearities selected by ARWU, concluding that they are designed to represent the regressive character of indicators measuring research performance. We propose that the utility and usability of the ARWU could be greatly improved by replacing the unwanted dynamical effects of the annual re-scaling based on <b>raw</b> <b>scores</b> of the best performers...|$|R
40|$|A major {{criticism}} of the Wechsler Memory Scale-III Faces subtest {{is the number of}} items, which can be daunting and time-consuming for an impaired client or boring for a normal client. An analysis of several versions, with data from a sample of 50 clinical referrals, revealed that a 32 -item subtest was best overall. Using this version, 100 % (Faces I) and 94 % (Faces II) of the predicted <b>raw</b> <b>scores</b> were less than 4 points away from the actual <b>raw</b> <b>scores,</b> whereas 66 % (Faces I) and 70 % (Faces II) were less than 2 points away. Limitations of this procedure are discussed...|$|R
2500|$|Unlike in the {{previous}} grading system in which a candidate's overall academic performance in the Basic Education Certificate Examination was obtained by computing the aggregate on the candidate's best six subject <b>scores,</b> the <b>raw</b> <b>scores</b> obtained by a candidate in the Basic Education Certificate Examination determines the candidates overall academic performance in the exam under the computerized school selection and placement system. Because the computerized school selection and placement system uses a deferred-acceptance algorithm which ensures that [...] Junior high school applicants are admitted strictly based on academic merit, administrators of the academy use <b>raw</b> <b>scores</b> obtained in the Basic Education Certificate Examination to admit applicants from Junior High School.|$|R
