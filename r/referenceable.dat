26|0|Public
2500|$|MediaWiki, the {{software}} that runs Wikipedia, supports this type of permanent link. [...] In its current implementation, old versions of specific articles, images, and templates are <b>referenceable</b> by unique unchanging URLs, though current entries may not use old versions of images and templates. [...] Permanent links to specific versions are recommended for citing articles from sources such as Wikipedia and Wikinews, {{to ensure that the}} content remains unchanged for review. A reviewer can then view the cited revision, the current revision, and the differences between the two.|$|E
50|$|An {{exploratory}} prototype draft xAPI vocabulary {{has been}} defined so that Open Badges may be <b>referenceable</b> from Experience API activity streams.|$|E
50|$|A {{column in}} an SQL table can be unnamed and thus unable to be {{referenced}} in expressions. The relational model requires every attribute {{to be named}} and <b>referenceable.</b>|$|E
50|$|Two or more {{columns of}} the same SQL table can have the same name and {{therefore}} cannot be referenced, {{on account of the}} obvious ambiguity. The relational model requires every attribute to be <b>referenceable.</b>|$|E
50|$|Since the PDP-11 was an octal-oriented (3-bit sub-byte) machine (addressing modes 0-7, {{registers}} R0-R7), {{there were}} (electronically) 8 addressing modes. Through {{the use of}} the Stack Pointer (R6) and Program Counter (R7) as <b>referenceable</b> registers, there were 10 conceptual addressing modes available.|$|E
50|$|The {{company is}} focused on seven {{verticals}} namely Energy and Utilities, Life Sciences and Pharmaceuticals, Industrials, BFSI, Data Infrastructure, Rapid Prototyping via Cambridge Innovations and Data Support & Managed Services via Cambridge Bizserve. Since {{the inception of the}} business plan in 2015, the company has added a <b>referenceable</b> client in each of its verticals.|$|E
5000|$|... user variables, {{displayable}} {{with the}} [...] command and <b>referenceable</b> {{with one or}} two cases of a prefixed character (default prefixes: '&' and '&&'). Oracle Corporation calls these variables [...] "substitution variables". Programmers can use them anywhere in a SQL or PL/SQL statement or in SQL*Plus commands. They can be populated by a literal using [...] or from the database using the [...] command.|$|E
50|$|MediaWiki, the {{software}} that runs Wikipedia, supports this type of permanent link. In its current implementation, old versions of specific articles, images, and templates are <b>referenceable</b> by unique unchanging URLs, though current entries may not use old versions of images and templates. Permanent links to specific versions are recommended for citing articles from sources such as Wikipedia and Wikinews, {{to ensure that the}} content remains unchanged for review. A reviewer can then view the cited revision, the current revision, and the differences between the two.|$|E
5000|$|Exoteric {{refers to}} {{knowledge}} that is outside, and independent from, a person's experience and can be ascertained by anyone (related to common sense). The word {{is derived from the}} comparative form of Greek ἔξω eksô, [...] "from, out of, outside". It signifies anything which is public, without limits, or universal. It is distinguished from internal esoteric knowledge. [...] "Exoteric" [...] relates to external reality as opposed to a person's thoughts or feelings. It is {{knowledge that is}} public as opposed to secret or cabalistic. It is not required that exoteric knowledge come easily or automatically, but it should be <b>referenceable</b> or reproducible.|$|E
5000|$|Cambridge Technology Enterprises has {{transformed}} into a focused IT services company {{under the leadership of}} Mr. Aashish Kalra, the Chairman & CEO of the company since January 2015. In 2015, the company embarked on a business plan for FY16 and FY17 to achieve minimum scale. The company focused on three areas - building partnerships, acquiring <b>referenceable</b> clients in its chosen verticals and acquiring relevant expertise in technologies. [...] Cambridge Technology Enterprises has reported a 84 percent growth in revenues, 523 percent growth in EBITDA and 316 percent growth in net profit over the last eight quarters.|$|E
50|$|The hidden ECN type of an ASN.1 type {{is almost}} {{identical}} to the original ASN.1 type (but slightly simplified) and is the starting point for an encoding process, specified in ECN, which ultimately generates the series of bits representing any given value of the original ASN.1 type. An ASN.1 type (or any of its parts) is not directly <b>referenceable</b> for the purpose of specifying an encoding in ECN, but its hidden ECN type is. ECN types and ECN constructor keywords can be explicitly referenced within an ECN specification and are encoded by applying the rules contained in the ECN specification.|$|E
50|$|The {{data in a}} JCR {{consists}} of a tree of nodes with associated properties. Data is stored in the properties, which may hold simple values such as numbers and strings or binary data of arbitrary length. Nodes may optionally have one or more types associated with them which dictate the kinds of properties, number and type of child nodes, and certain behavioral characteristics of the nodes. Nodes may point to other nodes via a special reference type property. In this way nodes in a JCR offer both referential integrity and object-oriented concept of inheritance. Additional node types include the <b>referenceable</b> node type which allows the user to reference said node through use of a universally unique identifier. Another popular type is the versionable type. This makes the repository track a document's history and store copies of each version of the document.|$|E
40|$|This memo defines an Experimental Protocol for the Internet community. It {{does not}} specify an Internet {{standard}} of any kind. Discussion {{and suggestions for}} improvement are requested. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2006). This document describes a new document series intended {{for use as a}} repository for IETF operations documents, which should be more ephemeral than RFCs, but more <b>referenceable</b> than Internet-Drafts, and with more clear handling procedures than a random Web page. It proposes to establish this series as an RFC 3933 process experiment...|$|E
30|$|All the {{elements}} in the marked list, regardless of its type, must be structurally validated. Type A and C elements {{do not need to}} be referentially validated because they do not have any attribute that does reference to others elements. Type A and B elements can directly build error messages to all elements of its references list without checking the references, since they do not have any <b>referenceable</b> attribute. Thus, type D elements are the most costly ones to the validation, since both elements (beyond the structurally validation) and {{the elements}} that do reference to them must be referentially validated.|$|E
40|$|TELOS is {{an attempt}} to provide {{powerful}} abstraction mechanisms and other structuring facilities within a language that provides the special capabilities needed for AI research. TELOS includes PASCAL as a subset and also is implemented in PASCAL. A full description is available in [1] and [2], Like most other AI languages, TELOS includes facilities needed for experimentation with large stores of general knowledge, tentatively modifiable and associatively <b>referenceable,</b> and with various planning and reasoning strategies. However, in contrast to other AI languages whose design has focused on building in certain powerful highlevel constructs, the design of TELOS has focuse...|$|E
40|$|We {{present a}} {{practical}} and efficient garbage collection mechanism for large scale distributed systems. The mechanism collects all garbage including distributed cyclic garbage without global synchronization or backward links. The primary method used for local and remote garbage collection is timeouts: each object has a time-to-live, and clients {{which have a}} link to an object must refresh the target object within the time-tolive to guarantee that the link will remain valid. For cyclic garbage collection: objects suspected to be garbage are detected by last <b>referenceable</b> timestamp propagation; and cyclic garbage is reclaimed by backward inquiry (back-tracing). Since, without additional overhead, the information about backward references can be obtained during the refreshing process, and since messages necessary for cyclic garbage collection are bundled with the messages used for the refreshing, communication, computation and storage overhead is minimized. This mechanism has been implemen [...] ...|$|E
40|$|Structured {{reporting}} {{offers a}} number of theoretical advantages, {{perhaps the most important}} of which is creation of standardized report databases. The standardized data created can in turn be used to customize data display, report content, historical data retrieval, interpretation analysis, and results communication in both a context and user-specific manner. In addition, these <b>referenceable</b> report databases can be used to facilitate the practice of evidence based medicine, through data-driven meta-analysis and determination of best practice guidelines. This concept will only be realized if the customized data delivery technology provides real and tangible value to end users, accentuates workflow, can be seamlessly integrated into existing information system technologies, and be shown to yield reproducibility of the evidence domain. The time is here for the medical imaging and clinical communities to embrace this vision in order to improve clinical outcomes and patient safety...|$|E
40|$|Abstract. This paper {{describes}} I 2 Geo, {{the platform}} of the Intergeo project enabling math educators to share Interactive Geometry content across Europe. This web-based platform makes each resource, be it a construction, a scenario of use, or a complete course, <b>referenceable,</b> ed-itable, and reviewable. I 2 Geo follows {{the patterns of}} open educational resources (OER), where anyone may participate and contribute, either by commenting, by adding constructions, or by reviewing {{the quality of a}} resource stored in a central place. The platform is based on a fine grained ontology of mathematical con-cepts and skills, together with a description of educational pathways in Europe. Together these two ingredients allow for cross-cultural searches. We describe the current platform’s facilities and its technical foundation that are used as well as the challenges they address. These challenges are common to the other digital libraries of open educational content...|$|E
40|$|Abstract. The NLP Interchange Format (NIF) is an RDF/OWL-based {{format that}} aims to achieve {{interoperability}} between Natural Language Processing (NLP) tools, language resources and annotations. The motivation behind NIF is to allow NLP tools to exchange annotations about text documents in RDF. Hence, the main prerequisite is that parts of the documents (i. e. strings) are <b>referenceable</b> by URIs, {{so that they can}} be used as subjects in RDF statements. In this paper, we present two NIF URI schemes for different use cases and evaluate them experimentally by benchmarking the stability of both NIF URI schemes in a Web annotation scenario. Additionally, the schemes are compared with other available schemes used to address text with URIs. The String Ontology, which is the basis for NIF, fixes the referent (i. e. a string in a given text) of the URIs unambiguously for machines and thus enables the creation of heterogeneous, distributed and loosely coupled NLP applications, which use the Web as an integration platform. ...|$|E
40|$|The {{scope of}} this {{document}} includes radiation safety considerations used in the design of facilities for the Yucca Mountain Site Characterization Project (YMP). The purpose of the Repository Radiation Shielding Design Guide is to document the approach used in the radiological design of the Mined Geologic Disposal System (MGDS) surface and subsurface facilities for the protection of workers, the public, and the environment. This document is intended to ensure that a common methodology is used by all groups that may be involved with Radiological Design. This document will also assist in ensuring the long term survivability of the information basis used for radiological safety design and will assist in satisfying the documentation requirements of the licensing body, the Nuclear Regulatory Commission (NRC). This design guide provides <b>referenceable</b> information that is current and maintained under the YMP Quality Assurance (QA) Program. Furthermore, this approach is consistent with maintaining continuity in spite of a changing design environment. This approach also serves to ensure common inter-disciplinary interpretation and application of data...|$|E
40|$|Sonic boom {{measurements}} {{have been}} obtained on 26 flights of the Space Shuttle system beginning with the launch of STS- 1 on April 12, 1981, to the reentry-descent of STS- 41 into EAFB on Oct. 10, 1990. A total of 23 boom measurements were acquired within the focus region off the Florida coast during 3 STS launch-ascents and 113 boom measurements were acquired during 23 STS reentry-descent to landing into Florida and California. Sonic boom measurements were made under, and lateral to, the vehicle ground track and cover the Mach-altitude range of about 1. 3 to 23 and 54, 000 feet to 243, 000 feet, respectively. Vehicle operational data, flight profiles and weather data were also gathered during the flights. This STS boom database is contained in 26 documents, some are formal and <b>referenceable</b> but most internal documents. Another 38 documents, also non-referenceable, contain predicted sonic boom footprints for reentry-descent flights on which no measurements were made. The purpose of this report is to {{provide an overview of}} the STS sonic boom database and summarize the main findings...|$|E
40|$|Supplemental STS Sonic Boom Files for NASA/CR- 2011 - 217080. Data files {{included}} on CDROM formatted to ISO 9660 standards. Sonic boom measurements {{have been obtained}} on 26 flights of the Space Shuttle system beginning with the launch of STS- 1 on April 12, 1981, to the reentry-descent of STS- 41 into EAFB on Oct. 10, 1990. A total of 23 boom measurements were acquired within the focus region off the Florida coast during 3 STS launch-ascents and 113 boom measurements were acquired during 23 STS reentry-descent to landing into Florida and California. Sonic boom measurements were made under, and lateral to, the vehicle ground track and cover the Mach-altitude range of about 1. 3 to 23 and 54, 000 feet to 243, 000 feet, respectively. Vehicle operational data, flight profiles and weather data were also gathered during the flights. This STS boom database is contained in 26 documents, some are formal and <b>referenceable</b> but most internal documents. Another 38 documents, also non-referenceable, contain predicted sonic boom footprints for reentry-descent flights on which no measurements were made. The purpose of this report is to {{provide an overview of}} the STS sonic boom database and summarize the main findings...|$|E
40|$|Background: The {{emergence}} and uptake of Semantic Web technologies by the Life Sciences provides exciting {{opportunities for}} exploring novel ways to conduct in silico science. Web Service Workflows are already becoming first-class objects in “the new way”, {{and serve as}} explicit, shareable, <b>referenceable</b> representations of how an experiment was done. In turn, Semantic Web Service projects aim to facilitate workflow construction by biological domain-experts such that workflows can be edited, re-purposed, and re-published by non-informaticians. However the aspects of the scientific method relating to explicit discourse, disagreement, and hypothesis generation have remained relatively impervious to new technologies. Results: Here we present SADI and SHARE- a novel Semantic Web Service framework, and a reference implementation of its client libraries. Together, SADI and SHARE allow the semi- or fully-automatic discovery and pipelining of Semantic Web Services in response to ad hoc user queries. Conclusions: The semantic behaviours exhibited by SADI and SHARE extend the functionalities provided by Description Logic Reasoners such that novel assertions can be automatically added to a data-set without logical reasoning, but rather by analytical or annotative services. This behaviour might be applied to achieve the “semantification ” of those aspects of the in silico scientific method that are not yet supported by Semantic We...|$|E
40|$|Abstract Background The {{emergence}} and uptake of Semantic Web technologies by the Life Sciences provides exciting {{opportunities for}} exploring novel ways to conduct in silico science. Web Service Workflows are already becoming first-class objects in “the new way”, {{and serve as}} explicit, shareable, <b>referenceable</b> representations of how an experiment was done. In turn, Semantic Web Service projects aim to facilitate workflow construction by biological domain-experts such that workflows can be edited, re-purposed, and re-published by non-informaticians. However the aspects of the scientific method relating to explicit discourse, disagreement, and hypothesis generation have remained relatively impervious to new technologies. Results Here we present SADI and SHARE - a novel Semantic Web Service framework, and a reference implementation of its client libraries. Together, SADI and SHARE allow the semi- or fully-automatic discovery and pipelining of Semantic Web Services in response to ad hoc user queries. Conclusions The semantic behaviours exhibited by SADI and SHARE extend the functionalities provided by Description Logic Reasoners such that novel assertions can be automatically added to a data-set without logical reasoning, but rather by analytical or annotative services. This behaviour might be applied to achieve the “semantification” of those aspects of the in silico scientific method that are not yet supported by Semantic Web technologies. We support this suggestion using an example in the clinical research space. </p...|$|E
40|$|Annotation and linking (or referring) {{have been}} {{described}} as "scholarly primitives", basic methods used in scholarly research and publication of all kinds. The online publication of manuscript images is one basic use case where the need for linking and annotation is very clear. High resolution images are of great use to scholars and transcriptions of texts provide for search and browsing, so the ideal method for the digital publication of manuscript works is the presentation of page images plus a transcription of the text therein. This has become a standard method, but leaves open the questions of how deeply the linkages can be done and how best to handle the annotation of sections of the image. This paper presents a new method (named img 2 xml) for connecting text and image using an XML-based tracing of the text on the page image. The tracing method was developed as part of a series of experiments in text and image linking beginning in the summer of 2008 and will continue under a grant funded by the National Endowment for the Humanities. It employs Scalable Vector Graphics (SVG) to represent the text in an image of a manuscript page in a <b>referenceable</b> form and enables linking and annotation of the page image in a variety of ways. The paper goes on to discuss the scholarly requirements for tools that will be developed around the tracing method, and explores some of the issues raised by the img 2 xml method...|$|E
40|$|We are {{surrounded}} by an ever increasing ocean of information, everybody will agree to that. We build sophisticated strategies to govern this information: design data models, develop infrastructures for data sharing, building tool for data analysis. Statistical datasets curated by National Statistical Offices (NSO) and international bodies like the OECD and Eurostat are being increasingly published on the Web, using Web standards, as Linked Open Data. We call Linked Statistical Data (LSD) all statistical datasets published on the Web as Linked Open Data. These datasets do not only consist of current statistics, but also an inheritage of such statistics conducted in the past by these NSO. Of these, the most prominent statistics are the Historical Censuses [1, 2, 3, 5]. In designing the current research landscape, {{there seems to be}} little attention to one very basic inquiry prior to any research design, so basic that it almost seems to be a shame to mention is, and that is a baseline statistics {{on the size of the}} problem we encounter to tackle, its occurrence, and the the actual need to tackle it. For our own research project [2] it was important to know all these baseline statistics, for instance, how much datasets, variables and values we have, and of which nature [12]. In the context of harmonization we looked for other existing vocabularies to avoid reinventing the wheel. The effort on trying to reuse these vocabularies was the immediate trigger for this exercise. In this paper we pose the following questions: What kind of dimensions are used in LSD? How often do they occur on the Web? Hereby, dimensions stand for the variables (and the categories used to encode their values) used when describing statistical data inquired to monitor society. Most of those statistical datasets published on the Web use the RDF Data Cube vocabulary [4] to describe the observations, dimensions, measures and attributes they contain. It is the beauty of semantic web technologies which allow us to perform, once every hour, an on­the­flight collection of data from those machine­readable publications of statistical information from almost 600 endpoints all across the Web. Web visualization technologies allow us to build a user interface for this meta study almost without any efforts ­ for all of those capable of playing with those tools of course. We call it LSD Dimensions, and it is available online [6, 7]. The availability of baseline statistics about the usage of multidimensional data on the Web opens up multiple questions, especially regarding comparability and reusability of statistical data, metadata and concepts, and how these can be combined or mixed in a semantically meaningful way [8]. But its implications are much bigger. Insights into the amount of statistical data available allows to better place the need for efforts to build research infrastructure for them. Baseline statistics as shown in this paper also highlight where research efforts are still needed, e. g. on coding schemes that do not exist yet and for which automatic tools that assist knowledge experts on building and publishing them are highly demanded [13]. At the end the innovativeness of this paper lies neither in the visualizations not in the data analytics, but in the smart use of existing technologies to gain overview about the problem space prior to entering into it. This way of thinking about visualizing baseline statistics in very much in line with the goals of KnoweScape combining data analytics with visualizations to enhance the access to large information spaces. This exercise has been inspired by numerous works from colleagues, which either use visualizations to advocate their own profound efforts of data integration and semantical <b>referenceable</b> data publishing [9, 11], or build pipelines of data harvest, analysis and visualization for all kind of other information [10]...|$|E

