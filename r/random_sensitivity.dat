2|74|Public
40|$|Abstract. With all {{parameters}} being {{seemed as}} random variables with Gaussian distribution，and Latin Hypercube Sampling method being used, {{the strength of}} ribbed cylindrical shell is computed based finite element and Monte Carlo Method. By probability sensitivity diagram, scatter diagram and related coefficient, the sensitivity of random variable of ribbed cylindrical shell is analyzed based on the method of <b>random</b> <b>sensitivity</b> analysis. The results show that shell thickness, external pressure, frame spacing and internal radius are more sensitivity than the other variables, so these variables can be treated as sensitivity factors...|$|E
40|$|Summary: Pattern mixture {{modeling}} is {{a popular}} approach for handling incomplete longitudinal data. Such models are not identifiable by construction. Identifying restrictions are one approach to mixture model identification (Little, 1995; Little and Wang, 1996; Thijs et al., 2002; Kenward et al., 2003; Daniels and Hogan, 2008) and are a natural starting point for missing not at <b>random</b> <b>sensitivity</b> analysis (Thijs et al., 2002; Daniels and Hogan, 2008). However, when the pattern specific models are multivariate normal, identifying restrictions corresponding to missing at random may not exist. Furthermore, identification strategies can be problematic in models with covariates (e. g. baseline covariates with time-invariant coefficients). In this paper, we explore conditions necessary for identifying restrictions that result in missing at random (MAR) to exist under a multivariate normality assumption and strategies for identifying sensitivity parameters for sensitivity analysis or for a fully Bayesian analysis with informative priors. In addition, we propose alternative modeling and sensitivity analysis strategies under a less restrictive assumption for {{the distribution of the}} observed response data. We adopt the deviance information criterion for model comparison and perform a simulation study to evaluate the performances of the different modeling approaches. We also apply the methods to a longitudinal clinical trial. Problems caused by baseline covariate...|$|E
40|$|Regular tree grammars and {{top-down}} tree transducers are {{extended by}} <b>random</b> context <b>sensitivity</b> as {{known from the}} areas of string and picture generation. First results regarding the generative power of the resulting devices are presented. In particular, we investigate the path languages of random context tree languages...|$|R
40|$|This {{study was}} carried out in {{medicine}} and cardiology indoor of Rajshahi Medical College Hospital from November 2004 to October 2005. 100 cases were selected for this study in <b>random</b> manner. <b>Sensitivity</b> of ECG to diagnose LVH was found to be 87. 5 %, and specificity was only 50 %. ECG is relatively insensitive and can’t accurately identify the severity of LVH...|$|R
40|$|AbstractFor a degradable {{structural}} system with fuzzy failure region, a moment method based on fuzzy reliability sensitivity algorithm is presented. According {{to the value}} assignment of performance function, the integral region for calculating the fuzzy failure probability is first split {{into a series of}} subregions in which the membership function values of the performance function within the fuzzy failure region can be approximated by a set of constants. The fuzzy failure probability is then transformed into a sum of products of the random failure probabilities and the approximate constants of the membership function in the subregions. Furthermore, the fuzzy reliability sensitivity analysis is transformed into a series of <b>random</b> reliability <b>sensitivity</b> analysis, and the <b>random</b> reliability <b>sensitivity</b> can be obtained by the constructed moment method. The primary advantages of the presented method include higher efficiency for implicit performance function with low and medium dimensionality and wide applicability to multiple failure modes and nonnormal basic random variables. The limitation is that the required computation effort grows exponentially with the increase of dimensionality of the basic random variable; hence, it is not suitable for high dimensionality problem. Compared with the available methods, the presented one is pretty competitive in the case that the dimensionality is lower than 10. The presented examples are used to verify the advantages and indicate the limitations...|$|R
40|$|Neural {{network has}} been {{attracting}} {{more and more}} researchers since the past decades. The properties, such as parameter <b>sensitivity,</b> <b>random</b> similarity, learning ability, etc., make it suitable for information protection, such as data encryption, data authentication, intrusion detection, etc. In this paper, by investigating neural networks' properties, the low-cost authentication method based on neural networks is proposed and used to authenticate images or videos. The authentication method can detect whether the images or videos are modified maliciously. Firstly, this chapter introduces neural networks' properties, such as parameter <b>sensitivity,</b> <b>random</b> similarity, diffusion property, confusion property, one-way property, etc. Secondly, the chapter gives an introduction to neural network based protection methods. Thirdly, an image or video authentication scheme based on neural networks is presented, and its performances, including security, robustness and efficiency, are analyzed. Finally, conclusions are drawn, and some open issues in this field are presented. Comment: 16 pages, 10 figures, submitte...|$|R
40|$|We {{examine the}} {{robustness}} of the fourfold {{pattern of risk}} attitudes under two elicitation procedures. We find that individuals are, on average, risk-seeking over low-probability gains and high-probability losses and risk-averse over high-probability gains and low-probability losses when we elicit prices for the gambles. However, a choice-based elicitation procedure, where participants choose between a gamble and its expected value, yields individual decisions that are indistinguishable from <b>random</b> choice. <b>Sensitivity</b> to elicitation procedure holds between and within participants, and remains when participants are allowed to review and change decisions. The price elicitation procedure is more complex; this finding may be further evidence that an increase in cognitive load exacerbates behavioural anomalies. Copyright � The Author(s). Journal compilation � Royal Economic Society 2009. ...|$|R
40|$|Quantiles, {{also known}} as value-at-risk in {{financial}} applications, are important measures of <b>random</b> performance. Quantile <b>sensitivities</b> provide information on how changes in the input parameters affect the output quantiles. In this paper, we study the estimation of quantile sensitivities using simulation. We propose a new estimator by employing kernel method and show its consistency and asymptotic normality for i. i. d. data. Numerical results show that our estimator works well for the test problems. ...|$|R
40|$|Abstract: The beach of Qiantang Estuary is {{changing}} greatly and rapidly, {{and safety of}} overall stability of the seawall is affected by both the mechanics properties of soils and the flat in front of seawall. Based on the stochastic analysis methods, the flat of seawall is firstly considered as a <b>random</b> variable, and <b>sensitivity</b> analysis for major variables which affecting the stability of seawall is provided, thereafter some measures to enforce are put forward...|$|R
50|$|The {{ability of}} a subject to detect {{coherent}} motion is commonly tested using motion coherence discrimination tasks. For these tasks, dynamic random-dot patterns (also called random dot kinematograms) are used that consist in 'signal' dots moving {{in one direction and}} 'noise' dots moving in <b>random</b> directions. The <b>sensitivity</b> to motion coherence is assessed by measuring the ratio of 'signal' to 'noise' dots required to determine the coherent motion direction. The required ratio is called the motion coherence threshold.|$|R
40|$|Abstract. In this study, the {{reliability}} {{sensitivity of the}} uniform strength Variable Cross-section Component is analyzed, which is the normal distribution. We propose a method for {{the reliability}} and sensitivity of the Uniform Strength Components. We may accurately obtain the uniform strength component's reliable sensitivity information on basic of random variable probability characteristic for certain situation. Through the value examples, we have studied the <b>random</b> variable reliable <b>sensitivity</b> for dangerous sections. We propose a design method with practicality for the uniform strength components...|$|R
40|$|Abstract. We {{consider}} the nonparametric density estimation {{problem for a}} quantity of interest computed from solutions of an elliptic partial differential equation with randomly perturbed coefficients and data. Our particular interest are problems for which limited knowledge of the random perturbations are known. We derive an efficient method for computing samples and generating an approximate probability distribution based on Lion’s domain decomposition method and the Neumann series. We then derive an a posteriori error estimate for the computed probability distribution reflecting all sources of deterministic and statistical errors. Finally, we develop an adaptive error control algorithm based on the a posteriori estimate. Key words. a posteriori error analysis, adjoint problem, density estimation, domain decomposition, elliptic problem, Neumann series, nonparametric density estimation, <b>random</b> perturbation, <b>sensitivity</b> analysis AMS subject classifications. 65 N 15, 65 N 30, 65 N 55, 65 C 05 1. Introduction. Th...|$|R
40|$|This work {{advances}} an imputation {{procedure for}} categorical scales which relays {{on the results}} of Latent Class Analysis and Multiple Imputation Analysis. The procedure allows us to use the information stored in the joint multivariate structure of the data set and {{to take into account the}} uncertainty related to the true unobserved values. The accuracy of the results is validated in the Item Response Models framework by assessing the accuracy in estimation of key parameters in a data set in which observations are simulated Missing at <b>Random.</b> The <b>sensitivity</b> of the multiple imputation methods is assessed with respect to the following factors: The number of latent classes set up in the Latent Class Model and the rate of missing observations in each variable. The relative accuracy in estimation is assessed with respect to the Multiple Imputation By Chained Equation missing data handling method for categorical variables...|$|R
40|$|Abstract—This paper {{considers}} a portfolio selection problem considering an investor’s subjectivity and the sensitivity {{analysis for the}} change of subjectivity. Since this proposed problem is formulated as a random fuzzy programming problem, it is not well-defined due to randomness and fuzziness. Therefore, introducing Sharpe ratio {{which is one of}} important performance measures of portfolio models, the main problem is transformed into the standard fuzzy programming problem. Furthermore, using the sensitivity analysis for fuzziness, the analytical optimal portfolio with the sensitivity factor is obtained. Index Terms—Portfolio selection problem, <b>Random</b> fuzzy programming, <b>Sensitivity</b> analysis, Analytical solution method. I...|$|R
40|$|In this talk, {{we present}} a new {{statistical}} measure of the robustness (or sensitivity) of linear dynamic systems to rank-one <b>random</b> disturbances. The <b>sensitivity</b> assessment is a statistical pseudospectrum: given the probability distribution of the random disturbance magnitude, it measures the expected frequency region where the system eigenvalues are located. We discuss {{the properties of the}} robustness and sensitivity measure. We notably stress the existence of an invariant of the measure that consequently shows that under certain conditions, the rate of increase of a rank-one pseudospectrum area is constant {{as a function of the}} disturbance magnitude...|$|R
40|$|The {{application}} of the computer code IPACS (Integrated Probabilistic Assessment of Composite Structures) to aircraft wing type structures is described. The code performs a complete probabilistic analysis for composites {{taking into account the}} uncertainties in geometry, boundary conditions, material properties, laminate lay-ups, and loads. Results of the analysis are presented in terms of cumulative distribution functions (CDF) and probability density function (PDF) of the fatigue life of a wing type composite structure under different hygrothermal environments subjected to the <b>random</b> pressure. The <b>sensitivity</b> of the fatigue life to a number of critical structural/material variables is also computed from the analysis...|$|R
40|$|A {{sequence}} of linear, monotonic, and nonmonotonic test problems {{is used to}} illustrate sampling-based uncertainty and sensitivity analysis procedures. Uncertainty results obtained with replicated random and Latin hypercube samples are compared, with the Latin hypercube samples tending to produce more stable results than the <b>random</b> samples. <b>Sensitivity</b> results obtained with the following procedures and/or measures are illustrated and compared: correlation coefficients (CCs), rank correlation coefficients (RCCs), common means (CMNs), common locations (CLs), common medians (CMDs), statistical independence (SI), standardized regression coefficients (SRCs), partial correlation coefficients (PCCs), standardized rank regression coefficients (SRRCs), partial rank correlation coefficients (PRCCs), stepwise regression analysis with raw and rank-transformed data, and examination of scatterplots. The effectiveness of a given procedure and/or measure depends {{on the characteristics of}} the individual test problems, with (i) linear measures (i. e., CCs, PCCs, SRCs) performing well on the linear test problems, (ii) measures based on rank transforms (i. e., RCCs, PRCCs, SRRCs) performing well on the monotonic test problems, and (iii) measures predicated on searches for nonrandom patterns (i. e., CMNs, CLs, CMDs, SI) performing well on the nonmonotonic test problems...|$|R
40|$|The {{robustness}} of {{the assigned}} prior distribution in a Bayesian estimation problem is examined. A Bayesian analysis for a stochastic intensity parameter of a Poisson distribution is summarized {{in which the}} natural conjugate is assigned as the prior distribution of the <b>random</b> parameter. The <b>sensitivity</b> analysis is carried out by assuming {{the existence of a}} true prior which is different in form from that of the assigned prior distribution. By using mean-squared error as a measure of performance, the ensuing Bayes decision function is compared to the corresponding minimum variance unbiased estimator. Results indicate that the Bayes estimator is largely robust to deviations from the assigned prior and remains squared-error superior to the MVU type within a broad region...|$|R
40|$|Abstract. Motion {{in depth}} results in radial optic-flow patterns. Forward motion results in {{radially}} expanding patterns, whereas backward motion generates contracting patterns. Radial optic-flow patterns are typically represented {{with a positive}} speed gradient, ie zero speed {{at the point of}} fixation, and maximum speed at the periphery. However, the actual speed profile in such a stimulus will depend upon the relative depth of objects in the scene. Using large-field stimuli (82 deg diameter) we determined relative sensitivities to radial expansion and contraction patterns and also to various types of speed gradients: positive, negative, random, and flat. We found that, even when large-field stimuli are used, observers are more sensitive to radially contracting patterns than to expanding patterns. Sensitivity to the positive speed gradient was not consistently different from either the negative or <b>random</b> gradients. <b>Sensitivity</b> to the flat gradient depended upon the speed of the stimuli. The finding of greater sensitivity to radial contraction is discussed in terms of the functional requirements involved in the use of optic-flow signals in maintaining balance. On the basis of the present findings, the utility of comparing psychophysical results based on thresholds against physiological data based on suprathreshold stimuli is also discussed. ...|$|R
40|$|High-throughput {{sequencing}} of small RNAs (sRNA-seq) {{is a popular}} method used to discover and annotate microRNAs (miRNAs), endogenous short interfering RNAs (siRNAs), and Piwi-associated RNAs (piRNAs). One of the key steps in sRNA-seq data analysis is alignment to a reference genome. sRNA-seq libraries often have {{a high proportion of}} reads that align to multiple genomic locations, which makes determining their true origins difficult. Commonly used sRNA-seq alignment methods result in either very low precision (choosing an alignment at <b>random),</b> or <b>sensitivity</b> (ignoring multi-mapping reads). Here, we describe and test an sRNA-seq alignment strategy that uses local genomic context to guide decisions on proper placements of multi-mapped sRNA-seq reads. Tests using simulated sRNA-seq data demonstrated that this local-weighting method outperforms other alignment strategies using three different plant genomes. Experimental analyses with real sRNA-seq data also indicate superior performance of local-weighting methods for both plant miRNAs and heterochromatic siRNAs. The local-weighting methods we have developed are implemented as part of the sRNA-seq analysis program ShortStack, which is freely available under a general public license. Improved genome alignments of sRNA-seq data should increase the quality of downstream analyses and genome annotation efforts...|$|R
40|$|This paper {{evaluates the}} {{exchange}} rate mechanisms in current large-scale models of the U. K. economy and finds that each has some shortcomings. A new, econometrically preferable, specification is developed. Previous empirical failures of exchange rate models relate to an inadequate treatment of expectations and neglect of the simultaneity between exchange rates and interest rates: the instrumental variable methods employed here remedy these deficiencies. The preferred equation, a forward-looking modified uncovered interest rate parity relation, outperforms a <b>random</b> walk. The <b>sensitivity</b> of overall model properties is examined by replacing the existing equations with the new equation and repeating standard simulation experiments. Coauthors are S. K. Tanna, D. S. Turner, K. F. Wallis, and J. D. Whitley. Copyright 1990 by Royal Economic Society. ...|$|R
40|$|Abstract—Biosensors {{based on}} silicon {{nanowires}} (Si-NWs) promise highly sensitive dynamic label-free electrical detection of biomolecules. Despite the tremendous potential and promising experimental results, the fundamental mechanism of electrical sensing of biomolecules {{and the design}} considerations of NW sensors remain poorly understood. In this paper, we discuss the prospects and challenges of biomolecule detection using Si-NW biosensors {{as a function of}} device parameters, fluidic environment, charge polarity of biomolecules, etc., and refer to experimental re-sults in literature to support the nonintuitive predictions wherever possible. Our results indicate that the design of Si nanobiosensor is nontrivial and as such, only careful optimization supported by numerical simulation would ensure optimal sensor performance. Index Terms—Biosensors, DNA, Poisson–Boltzmann (PB), protein detection, <b>random</b> dopant fluctuations, <b>sensitivity.</b> I...|$|R
40|$|This paper {{examines}} the robustness of a Bayes estimator {{with respect to}} the assigned prior distribution. A Bayesian analysis for a stochastic scale parameter of a Weibull failure model is summarized in which the natural conjugate is assigned as the prior distribution of the <b>random</b> parameter. The <b>sensitivity</b> analysis is carried out by the Monte Carlo method in which, although an inverted gamma is the assigned prior, realizations are generated using distribution functions of varying shape. For several distributional forms and even for some fixed values of the parameter, simulated mean squared errors of Bayes and minimum variance unbiased estimators are determined and compared. Results indicate that the Bayes estimator remains squared-error superior and appears to be largely robust to the form of the assigned prior distribution...|$|R
40|$|This paper puts {{forward a}} safe {{mechanism}} of data transmission {{to tackle the}} security problem ofinformation which is transmitted in Internet. We propose a new technique on matrix scrambling which isbased on random function, shifting and reversing techniques of circular queue. We give statisticalanalysis, sequence <b>random</b> analysis, and <b>sensitivity</b> analysis to plaintext and key on the proposed scheme. The experimental {{results show that the}} new scheme has a very fast encryption speed and the key space isexpanded and it can resist all kinds of cryptanalytic, statistical attacks, and especially, our new methodcan be also used to solve the problem that is easily exposed to chosen plaintext attack. We give ourdetailed report to this algorithm, and reveal the characteristic of this algorithm by utilizing an example...|$|R
40|$|AbstractThe {{evaluation}} of the failure probability and safety level of occupant evacuation is of extreme importance. The safety level of occupant evacuation is expressed by the margin of escape time. Three traditional methods for solving the reliability index are introduced and an optimization method is proposed in this study. Comparisons between the optimization method and the traditional methods are carried out. Since occupant evacuation is affected by various random parameters, {{it is necessary to}} analyze the importance of each variable in comparison with the others in occupant safety evacuation. A methodology is presented to classify the significant and insignificant <b>random</b> variables using <b>sensitivity</b> analysis. Finally, the successful application in a case study suggests that these methodologies are suitable and reliable for the reliability analysis of occupant safety evacuation in public assembly occupancies...|$|R
40|$|This paper puts {{forward a}} safe {{mechanism}} of data transmission {{to tackle the}} security problem of information which is transmitted in Internet. We propose a new technique on matrix scrambling {{which is based on}} random function, shifting and reversing techniques of circular queue. We give statistical analysis, sequence <b>random</b> analysis, and <b>sensitivity</b> analysis to plaintext and key on the proposed scheme. The experimental results show that the new scheme has a very fast encryption speed and the key space is expanded and it can resist all kinds of cryptanalytic, statistical attacks, and especially, our new method can be also used to solve the problem that is easily exposed to chosen plaintext attack. We give our detailed report to this algorithm, and reveal the characteristic of this algorithm by utilizing an example...|$|R
40|$|We {{consider}} linear dynamical systems {{defined by}} di¿erential algebraic equations. The associated input-output behaviour {{is given by}} a transfer function in the frequency domain. Physical parameters of the dynamical system are replaced by random variables to quantify uncertainties. We analyse {{the sensitivity of the}} transfer function with respect to the <b>random</b> variables. Total <b>sensitivity</b> coe¿cients are computed by a nonintrusive and by an intrusive method based on the expansions in series of the polynomial chaos. In addition, a reduction of the state space is applied in the intrusive method. Due to the sensitivities, we perform a model order reduction within the random space by changing unessential random variables back to constants. The error of this reduction is analysed. We present numerical simulations of a test example modelling a linear electric network...|$|R
40|$|Abstract. It brings certain error to {{reliability}} {{design of}} mechanical components that now most reliability design aiming at establishing pure theoretical mathematical model, without considering arbitrary distribution and variation rules of parameters. In this paper, the variation rules {{of strength and}} load effect are studied, the computational method of gradual change reliability is given. By combining the theory of reliability design with the method of sensitivity analysis, the restrictions on random variable distribution types are loosen, the gradual change reliability sensitivity design method with arbitrary distribution parameter is proposed based on the measurement information. Under the circumstance that knowing the first four order moments of basic <b>random</b> parameter, reliability <b>sensitivity</b> design with arbitrary distribution parameter can be brought about by using computer program, which provides the theoretical basis for structural design and life forecast of mechanical components...|$|R
40|$|The {{objective}} of the paper is to present methods for efficient statistical, sensitivity and reliability assessment. The attention {{is given to the}} techniques which are developed for an analysis of computationally intensive problems which is typical for a nonlinear FEM analysis. The paper shows the possibility of "randomization " of computationally intensive problems {{in the sense of the}} Monte Carlo type simulation. Latin hypercube sampling is used, in order to keep the number of required simulations at an acceptable level. The technique is used for both random variables and <b>random</b> fields levels. <b>Sensitivity</b> analysis is based on nonparametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique – the simulated annealing. The simulation can be used for preparation of virtual training set for artificial neural network used in inverse analysis. The multipurpose software FReET is briefly described...|$|R
40|$|Background: Post-term {{birth is}} a {{preventable}} cause of perinatal mortality and severe morbidity. This review examined {{the association between}} maternal BMI and post-term birth at ≥ 42 and ≥ 41 weeks’ gestation. Methods: Six databases, reference lists and citations were searched May-November 2015. Observational studies published in English since 1990 were included. Linear and nonlinear doseresponse meta-analyses were conducted using <b>random</b> effects models. <b>Sensitivity</b> analyses assessed robustness of the results. Meta-regression and sub-group meta-analyses explored heterogeneity. Obesity classes were defined as I (30. 0 - 34. 9 kg/m 2), II (35. 0 - 39. 9 kg/m 2), and III (≥ 40 kg/m 2; IIIa 40. 0 - 44. 9 kg/m 2, IIIb ≥ 45. 0 kg/m 2). Results: Searches identified 16, 375 results; 39 studies met the inclusion criteria (n= 4, 143, 700 births). A nonlinear association between maternal BMI and births ≥ 42 weeks was identified, ORs and 95...|$|R
40|$|Background: Recent {{epidemiological}} {{studies suggest that}} short sleep duration {{may be associated with}} the development of obesity from childhood to adulthood. Objectives: To assess whether the evidence supports the presence of a relationship between short sleep duration and obesity at different ages, and to obtain an estimate of the risk. Methods: We performed a systematic search of publications using MEDLINE (1996 - 2007 wk 40), EMBASE (from 1988), AMED (from 1985), CINHAL (from 1982) and PsycINFO (from 1985) and manual searches without language restrictions. When necessary, authors were contacted. Criteria for inclusion were: report of duration of sleep as exposure, BMI as continuous outcome and prevalence of obesity as categorical outcome, number of participants, age, and gender. Results were pooled using a <b>random</b> effect model. <b>Sensitivity</b> analysis was performed, heterogeneity and publication bias were also checked. Results are expressed as pooled odds ratios (OR [95...|$|R
40|$|In the Midwest, signi#cant interannual to decadal {{variations}} in precipitation and stream#ow {{have been observed}} over {{the latter part of}} this century. Yet in studies of precipitation frequency, engineers and hydrologists assume that natural climate variability does not a#ect the probability distribution of extreme precipitation. A regional approach is used to test this assumption for extreme precipitation in the Midwest for precipitation durations ranging from 1 -hour to 3 -days. Homogeneous regions for precipitation frequency analysis are delineated and regional time series of precipitation quantiles are estimated. Variations in the regional quantile series for precipitation return periods of 2 - to 20 -years are compared with those expected for a stationary <b>random</b> process. The <b>sensitivity</b> of the statistical analysis to the delineation of homogeneous regions and to the handling of missing hourly precipitation data are examined. The implications of the results for the use of regiona [...] ...|$|R
40|$|Background: Curable, non-viral {{pathogens}} {{account for}} a significant burden of sexually transmitted infections (STIs), and there is established evidence that STIs increase both HIV acquisition and transmission. We investigated the prevalence, trends, and factors associated with Chlamydia trachomatis, Neisseria gonorrhoeae, Trichomonas vaginalis and Treponema pallidum, {{and the performance of}} syndromic management, among a cohort of women working in bars, hotels, and other food and recreational facilities near large-scale mines in northwestern Tanzania. Methods: HIV-negative women aged 18 – 44 years (N = 966) were enrolled and followed for 12 months in a microbicides feasibility study. We collected sociodemographic and behavioural data, performed clinical examinations, and tested for STIs, at enrolment and 3 -monthly. Risk factors for STIs were investigated using logistic regression models with <b>random</b> effects. <b>Sensitivity,</b> specificity and predictive values of syndromic management were calculated. Results: At enrolment, the prevalences of C. trachomatis, N. gonorrhoeae, T. vaginalis, and high-titre active syphilis were 111 / 956 (12 %), 42 / 955 (4 %), 184 / 945 (19 %) and 46 / 965 (5 %), respectively. There were significant decreases over time for C. trachomatis and T. vaginalis (OR trend per month: 0. 94 [95 % CI 0. 91, 0. 97]; and 0. 95 [0. 93, 0. 98], respectively; both p, 0. 001). The majority of these infections were not diagnosed by the corresponding syndrome; therefore, most participants were not treated at the diagnosis visit. Syndromic management was poorly predictive of laboratory-diagnosed infections. W...|$|R
40|$|International audienceThe {{present study}} aims at {{analysing}} {{the sensitivity of}} two-dimensional flow past a flat plate to uncertain inflow conditions in the laminar flow regime. Both the Reynolds number and angle of incidence are treated as random inflow variables. The methodology consists of a stochastic collocation method based on generalised polynomial chaos (gPC) theory coupled with standard deterministic numerical simulations. With respect to the two <b>random</b> inputs, <b>sensitivity</b> analysis of global integral parameters such as Strouhal number, drag and lift coefficients and the time-averaged flow fields is performed, resulting {{in the construction of}} their response surfaces. The stochastic response of the full spectrum of the drag coefficient is also obtained. It is noticed that integral parameters are sensitive to the two random parameters. There is a peak in the probability density function (PDF) of mean drag coefficient. Two additional high frequencies are predicted in the spectrum of drag coefficients. They are about two and four times the primary vortex shedding frequency respectively, corresponding to first and second harmonics of the primary frequency. For the flow fields, the analysis demonstrates that the most probable solutions are significantly different from the deterministic ones and the solution sensitivity is localised near the regions transitioned to large scale fluid structure movements. Non-linear coupling between the two uncertainties is also studied thanks to the Sobol's decomposition. The angle of incidence is found to be the most influential variable to the mean flow fields...|$|R
40|$|Increasing use {{is being}} made of random {{coefficients}} structures, such as mixed logit, {{in the analysis of}} air travel choice behaviour. These models have the advantage of being able to retrieve <b>random</b> variations in <b>sensitivities</b> across travellers. An important issue, however, arises in the computation of willingness to pay indicators, such as the valuation of travel time savings, on the basis of randomly distributed coefficients. Indeed, with the standard approach of using simulation of the ratios across random draws, major problems can be caused by outliers, leading to biased trade-offs, which in turn lead to major issues in policy analyses. Here, a different approach is explored, making use of individual-specific draws from the random distributions, conditioned on the observed sequence of choices for each respondent. An analysis making use of stated preference data for airport and airline choice confirms the advantages of the approach using conditional draws, producing much more realistic distributional patterns for a range of willingness to pay indicators. ...|$|R
40|$|Large scatter {{characterizes the}} {{collapse}} load of thin-walled structures {{due to their}} imperfection <b>sensitivity.</b> <b>Random</b> fields {{can be used to}} include imperfections in a finite element model, which can highly increase the credibility of the finite element (FE) model. Principal component analysis (PCA) and analytical covariance functions are matched to available correlation information of geometrical imperfection measurements. The random fields are realized by Karhunen-Lo`eve expansion combined with Monte Carlo methods. The results of the different covariance models are compared with the deterministic collapse loads of the measured imperfections in the FE-model. This approach isolates the effect of the different covariance models from other inaccuracies such as boundary conditions, loading imperfections, etc. The results show that random fields can be used to improve predictions and the understanding of the structural behavior of thin-walled structures, especially when these predictions are based on imperfection data, using PCA. Caution is recommended when using analytical covariance functions since they may fail to capture the complete behavior of the structure...|$|R
