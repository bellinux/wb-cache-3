7|40|Public
40|$|Abstract: If the German {{euthanasia}} program {{developed from}} that nation’s intellectual culture, then the Nazi extension {{of it was}} not a unique horror and might be a <b>relevant</b> <b>analogy</b> for modern euthanasia debates. In this context, the case of Peter Singer (an advocate of euthanasia) and his criticisms of the Nazi analogy are particularly worthy of consideration. This article argues that Singer’s criticisms fail, and that the analogy does in fact have contemporary relevance...|$|E
40|$|Includes bibliographical {{references}} (leaves 28 - 36) In {{this experiment}} I compare {{the effects of}} culturally relevant information and analogies in a problem solving task. Participants were prescreened and separated randomly into different conditions {{so that they would}} receive either, a culturally <b>relevant</b> <b>analogy</b> (RA), a culturally inelevant analogy (IA), a culturally relevant story that was not an analogy (RC), or a culturally inelevant story that was not an analogy (IC). Results {{of this study suggest that}} use of an analogy will result in better perfmmance on a target problem. Results also show that cultmally relevant infmmation will enhance the effectiveness of an analogy thereby increasing the probability of answering a target problem conectly. Fmther research is recommended to further refine the use of analogies in the classroom for a learner centered environment...|$|E
40|$|The diploma thesis "The Influence of the Trinity Archetype on the Thoughts of Democracy and Pluralism" {{deals with}} a {{long-term}} discussed theme of interrelationship of democracy, pluralism and the church. It considers a development of a democracy since its birth on a basis of collation with totalitarian regimes and a strict monotheism and tries to point out significant fundamentals of democracy and Trinitarian God. It observes an archetypal attributes and interpersonal relationships belonging to Father, the Son and the Holy Spirit. It finds out a <b>relevant</b> <b>analogy</b> of them in human being and society. Author's main affort is to find an influence and a recognition of an archetypal affinity of Gog and human being in society and {{in the church and}} profane official documents. It emphasizes the current role of Christianity in Europe...|$|E
5000|$|One can {{understand}} these levels through {{the analogy of}} a man who wants a house. The hishtalshelus is generally broken down into two general stages, called the [...] "Upper Unity" [...] and the [...] "Lower Unity". Below are the <b>relevant</b> <b>analogies</b> for all the basic stations of the hishtalshelus in the analogy {{of a man who}} wants a house starting from the top (primordial desires) and going down (until the desire is actualized).|$|R
30|$|Statistics {{regarding}} the <b>relevant</b> geological <b>analogy</b> parameters of typical shale gas reservoirs in the USA are {{collected from the}} previous literature (Curtis 2002; Gao et al. 2016; Fang et al. 2015).|$|R
30|$|Over two {{experiments}} {{we develop}} two instructional analogies {{using a combination}} of structural alignment, progressive alignment, and hierarchical alignment. Of importance, all three techniques provide multiple opportunities to practice making <b>relevant</b> <b>analogies.</b> Thus, both Experiments 1 and 2 examine the efficacy of providing multiple opportunities to align geologic time to a spatial linear representation using structural alignment to improve understanding and reasoning about geologic time. Across both experiments, students are also provided with corrective feedback. A key difference between Experiments 1 and 2 is that Experiment 1 also assesses the benefit of hierarchical and progressive alignment, whereas Experiment 2 assesses structural alignment and corrective feedback alone, without progressive or hierarchical alignment, in an effort to devise a more time-efficient means of intervention.|$|R
40|$|The {{effect of}} a <b>relevant</b> <b>analogy</b> and of subject {{visualization}} {{on the amount of}} cognitive capacity needed to process unfamiliar information about science was investigated. The dependent measure was reaction time on a secondary task while listening to six tape recorded passages about chemistry and physics in a 2 x 2 x 6 mixed design. The analogy treatment group required'less cognitive capacity and the visualization treatment group required more cognitive capacity to process the-materials. The analogy results support the theory that analogies make mental processing more efficient by modifying 'existing cognitive structures prior to processing the new information. The visualization results support a ' theory that visualizers devote more attention to the material being processed. A general similarity effect of organizing devices is suggested. (Author) Reproductions supplied by EDRS are the best that can be made from the original document. t...|$|E
40|$|Learning a {{motor skill}} by analogy can benefit {{performers}} because {{the movement that}} is developed has characteristics of implicit motor learning: namely, movement robustness under pressure and secondary task distraction and limited accrual of explicit knowledge (Liao & Masters, 2001). At an applied level the advantages are lost, however, if the heuristic that underpins the analogy conveys abstractions that are inappropriate for the indigenous culture. The aim of the current experiment was to redevelop Masters's (2000) right-angled-triangle analogy to accommodate abstractions appropriate for Chinese learners. Novice Chinese participants learned to hit table tennis forehands with topspin using either a redeveloped, culturally appropriate analogy (analogy learning) or a set of 6 instructions relevant to hitting a topspin forehand in table tennis (explicit learning). Analogy learners accrued less explicit knowledge of the movements underlying their performance than explicit learners. In addition, a secondary task load disrupted the performance of explicit learners but not analogy learners. These findings indicate that a culturally <b>relevant</b> <b>analogy</b> can bring about implicit motor learning in a Chinese population. © 2007 Human Kinetics, Inc. published_or_final_versio...|$|E
40|$|We {{analyze and}} {{contrast}} the US and EU antitrust standards on mixed bundling and tying. We apply our analysis to the US and EU cases against Microsoft {{on the issue}} of tying new products (Internet Explorer in the US, and Windows Media Player in the EU) with Windows as well as to cases brought in Europe and in the United States on bundling discounts. We conclude that there are differences between the EC and US antitrust law on the choice of the <b>relevant</b> <b>analogy</b> for bundled rebates (predatory price standard or foreclosure standard) and the implementation of the distinct product and coercion test for tying practices. The second important difference between the two jurisdictions concerns the interpretation of the requirement of anticompetitive foreclosure. It seems to us that in Europe, consumer detriment is found easily and it is not always a requirement for the application of Article 82, or at least that the standard of proof of a consumer detriment for tying cases is lower than in the US. ...|$|E
40|$|The {{purpose of}} this paper is to present two kinds of analogical {{representational}} change, both occurring early in the analogy-making process, and then, using these two kinds of change, to present a model unifying one sort of analogy-making and categorization. The proposed unification rests on three key claims: (1) a certain type of rapid representational abstraction is crucial to making the <b>relevant</b> <b>analogies</b> (this is the first kind of representational change; a computer model is presented that demonstrates this kind of abstraction), (2) rapid abstractions are induced by retrieval across large psychological distances, and (3) both categorizations and analogies supply understandings of perceptual input via construing, which is a proposed type of categorization (this is the second kind of representational change). It is construing that finalizes the unification...|$|R
40|$|Abstract: Pervasive Computing {{systems are}} {{characterized}} by possibly mobile components distributed in the environment and are devoted to collect, process and manage information {{in order to support}} users in different kind of activities. High-level correlation of information in such context can be defined, exploiting a formal model arising from the spatial disposition of information sources, as a form of commonsense spatial reasoning. With respect to this model, a Hybrid Logic to formalize commonsense spatial reasoning in these context has been defined. Here, on the basis of <b>relevant</b> <b>analogies</b> among Pervasive Computing and human practice in handling spatial 1 Correlation of Information in Pervasive Computing. knowledge, we suggest to provide the term “commonsense” with a positive meaning, showing that our logical framework captures some features of non-mathematical reasoning when spatially qualified information is concerned. The focus on such features and the analogies mentioned above suggest to qualify our approach to (commonsense) spatial reasoning as an informational approach...|$|R
40|$|International audienceWe {{present a}} phenomenological model {{intended}} to describe at the protein population level {{the formation of}} cell-cell junctions by the local recruitment of homophilic cadherin adhesion receptors. This modeling may have a much wider implication in biological processes since many adhesion receptors, channel proteins and other membrane-born proteins associate in clusters or oligomers at the cell surface. Mathematically, it consists in a degenerate reaction-diffusion system of two partial differential equations modeling the time-space evolution of two cadherin populations over a surface: the first one represents the diffusing cadherins and the second one concerns the fixed ones. After discussing {{the stability of the}} solutions of the model, we perform numerical simulations and show <b>relevant</b> <b>analogies</b> with experimental results. In particular, we show patterns or aggregates formation for a certain set of parameters. Moreover, perturbing the stationary solution, both density populations converge in large times to some saturation level. Finally, an exponential rate of convergence is numerically obtained and is shown to be in agreement, for a suitable set of parameters, with the one obtained in some in vitro experiments...|$|R
40|$|In this article, {{we discuss}} sex {{selection}} {{not intended to}} help a couple avoid having {{a child with a}} severe genetic disorder, but to avoid possible health risks further along the line of generations. Sex selection may be put to this use in the context of preventing mitochondrial DNA disorders by means of preimplantation genetic diagnosis (PGD) and possibly in the future also through nuclear transfer (NT; also known as mitochondrial gene replacement). A <b>relevant</b> <b>analogy</b> {{can be found in the}} context of PGD for X-linked diseases, where sex selection against healthy female carrier embryos would have the same 2 -fold purpose of (i) avoiding difficult reproductive decisions for the future child and (ii) avoiding transmission of the mutation to a possible third generation. Because sex selection would still be done for reasons of health, this application should not give rise to the moral concerns associated with sex selection for non-medical reasons. However, the proportionality of adding the relevant procedures to PGD or NT is a relevant concern. We discuss post-and preconceptional sex selection strategies. We conclude that if PGD is already part of the procedure, either as the central technology or as a back-up test after NT, preferentially transferring male embryos could in principle be a morally acceptable way of reducing possible burdens and risks. To start an IVF/PGD-cycle especially for this purpose would be disproportional. The alternative approach of preconceptional sex selection may be morally justified as a means to increase the chances of obtaining male embryos...|$|E
40|$|AbstractEfficient {{thermoregulation}} solutions can {{be extracted}} from strategies found in nature. Living organisms maintain body temperature in very narrow ranges {{in order to}} survive. Organisms have adopted physiological, morphological, and/or behavioral means for thermoregulation. In some organisms, the process is achieved by skin functioning as a thermal filter, whereas in others, it is achieved by their built structures. Building envelopes separate occupied indoor spaces from the exterior environment are often considered as thermal barriers or shields. Conceiving the envelope in this way limits potentially efficient solutions, where the building envelope is considered as a medium rather than a barrier, just as in living organisms. In this context, biomimetics, as a design approach, provides a huge potential for innovative thermal solutions. This work focuses on the initial phase of a biomimetic design process, where a biophysical framework is established to provide an easier access to <b>relevant</b> <b>analogies.</b> It presents a structured framework of heat regulation processes to support the search for, and the selection of, appropriate strategies from the large database of nature...|$|R
40|$|Colloidal {{particles}} with directional {{interactions are}} {{key in the}} realization of new colloidal materials with possibly unconventional phase behaviors. Here we exploit DNA self-assembly to produce bulk quantities of "DNA stars" {{with three or four}} sticky terminals, mimicking molecules with controlled limited valence. Solutions of such molecules exhibit a consolution curve with an upper critical point, whose temperature and concentration decrease with the valence. Upon approaching the critical point from high temperature, the intensity of the scattered light diverges with a power law, whereas the intensity time autocorrelation functions show a surprising two-step relaxation, somehow reminiscent of glassy materials. The slow relaxation time exhibits an Arrhenius behavior with no signs of criticality, demonstrating a unique scenario where the critical slowing down of the concentration fluctuations is subordinate to the large lifetime of the DNA bonds, with <b>relevant</b> <b>analogies</b> to critical dynamics in polymer solutions. The combination of equilibrium and dynamic behavior of DNA nanostars demonstrates the potential of DNA molecules in diversifying the pathways toward collective properties and self-assembled materials, beyond the range of phenomena accessible with ordinary molecular fluids...|$|R
40|$|Keywords: POP-I. B. {{barriers}} to programming, POPIV. B. teaching design This paper discusses {{some of the}} fundamental problems encountered by students on first year computer programming courses at Irish Institutes of Technology. It’s content {{is based on the}} author’s practical experience in the classroom and programming laboratories which will be supported in the near future by empirical data gathering. The reasons for many difficulties experienced by students are discussed. The objective of the proposed research is to use the analogy-based approach to learning. A structure called an analogy tree will be built and implemented in a learning tool which will use animation to simulate the <b>relevant</b> <b>analogies.</b> Finally, the future work required, and, potential problems for this project will be discussed. Problems Encountered When embarking on the delivery of a first year programming course, lecturers are faced with a number of problems. Firstly, programming students are a product of the primary and secondary educational systems that do not have a logic/problem-solving module in any of their subjects. This puts Irish students at a disadvantage compared to many of their counterparts in other countries wher...|$|R
40|$|The {{non-linear}} analysis of structures {{is a very}} wide field (Chapter l) and this thesis has concentrated more on some areas than others; trusses rather than frames; iterative methods and acceleration techniques rather than direct methods (Chapters 2 and 3), Nevertheless frames receive some attention and comparisons are given in both store (Chapter 4) and time (Chapter 5) between the Jacobi iterative methods and the Gauss direct method {{with a view to}} highlighting the advantages of iteration. The analysis of structural behaviour is considered at working load and at collapse (Chapter 6), Also variations in the collapse load behaviour are considered as the loading is varied within limits of the nominal loadings Numerical examples are given to illustrate the underlying theory (Chapter 7). Finally an outline is given (Chapter 8) of a way to help the iterative methods forward: direct methods are in difficulty with time and store; iterative methods are in difficulty with time. To improve the iterative methods it is proposed that one's intuitive structural knowledge - together with <b>relevant</b> <b>analogies</b> and existing numerical solutions - be brought in as part of the data for future problems...|$|R
40|$|Keprt_abstrakt v AJ. txt Anglická anotace Medieval wall {{paintings}} in the St. Clement Church at Levý Hradec Wall {{paintings in}} the St. Clement Church at Levý Hradec depict a cycle with motives from the lives of Christ and Virgin Mary portrayed in two stripes, located at {{the walls of the}} pentagonal presbytery, each approximately 170 cm high. The iconographic programme decorating the choir in the severies is enriched by depictions of the Early Church Fathers, Old Testament prophets, the evangelists, and angels carrying Arma Christi. In the context of Bohemian monumental painting, the wall paintings at Levý Hradec are important because they complement and enlighten our knowledge of artistic development of the period between the 1370 s and 1400. This thesis uses the methods of stylistic and iconographic analysis to place the studied paintings into the context of the artwork of the given period. After reviewing existing literature and researching <b>relevant</b> <b>analogies</b> to individual motives and scenes, the thesis outlines the importance of the paintings against the background of other works created in the Bohemian and central European area in the given period. Towards the end of the 14 th century, the right of patronage to the St. Clement Church belonged to the Benedictine convent to the St. George Church at Prague [...] ...|$|R
40|$|The {{literature}} {{on the development of}} assistive robots is dominated by technological papers with little consideration of how such devices might be commercialised for a mass market at a price that is affordable for older people and their families as well as public services and care insurers. This article argues that the focus of technical development in this field is too ambitious, neglecting the potential market for an affordable device that is aleady {{in the realm of the}} ‘adjacent possible’ given current technology capabilities. It also questions on both ethical and marketing grounds the current effort to develop assistive robots with pet-like or human-like features. The marketing {{literature on}} ‘really new products’ has so far not appeared to inform the development of assistive robots but has some important lessons. These include using analogies with existing products and giving particular attention to the role of early adopters. <b>Relevant</b> <b>analogies</b> for care robots are not animals or humans but useful domestic appliances and personal technologies with attractive designs, engaging functionality and intuitive usability. This points to a strategy for enabling mass adoption - which has so far eluded even conventional telecare - of emphasising how such an appliance is part of older people’s contemporary lifestyles rather than a sign of age-related decline and loss of independence...|$|R
5000|$|There {{are some}} {{specific}} assumptions or principles that direct the instructional design: active {{involvement of the}} learner in the learning process, learner control, meta cognitive training ( [...] e.g., self-planning, monitoring and revising techniques), the use of hierarchical analyses to identify and illustrate prerequisite relationships (cognitive task analysis procedure), facilitating optimal processing of structuring, organizing and sequencing information (use of cognitive strategies such as outlining, summaries, synthesizers, advance organizers etc.), encouraging the students to make connections with previously learned material, and creating learning environments (recall of prerequisite skills; use of <b>relevant</b> examples, <b>analogies).</b>|$|R
40|$|The analogies between {{optical and}} {{electronic}} Goos-Hänchen effects are established based on electron wave optics in semiconductor or graphene-based nanostructures. In this paper, we give {{a brief overview}} of the progress achieved so far in the field of electronic Goos-Hänchen shifts, and show the <b>relevant</b> optical <b>analogies.</b> In particular, we present several theoretical results on the giant positive and negative Goos-Hänchen shifts in various semiconductor or graphen-based nanostructures, their controllability, and potential applications in electronic devices, e. g. spin (or valley) beam splitters. Comment: 21 pages, 5 figures, contribution to "Beam Shifts" special issue in Journal of Optic...|$|R
40|$|I {{present a}} {{solution}} to the Sleeping Beauty problem. I begin with the consensual emerald case and describe then a set of <b>relevant</b> urn <b>analogies</b> and situations. These latter experiments make it easier to diagnose the flaw in the thirder's line of reasoning. I discuss in detail the root cause of the flaw in the argument for 1 / 3 which is an erroneous assimilation with a repeated experiment. Lastly, I discuss an informative variant of the original Sleeping Beauty experiment that casts light on the diagnosis of the fallacy in the argument for 1 / 3...|$|R
40|$|Have {{you ever}} thrown sticks and stones {{in the water}} as a child, just {{to find out whether}} they would float or not? Or have you ever noticed how much fun babies can have by simply {{touching}} objects, sticking them into their mouths, or rattling them and discovering new noises? It is these embodied interactions, experiences and discoveries and not only the organization of our brain that together result in intelligence. During the past five years, we have been working on algorithms that make robots eager to investigate their surroundings. Curiosity-driven robots explore their environment in search of new things to learn: they get bored with situations that are already familiar to them, and also avoid situations which are too difficult. In our experiments, we place the robots {{in a world that is}} rich in learning opportunities and then just watch how the robots develop by themselves. The results show <b>relevant</b> <b>analogies</b> with the ways in which young children discover their own bodies as well as the people and objects that are close to them. The idea that children are not passive learners but are intrinsically motivated to progress in learning is not new. In the 1960 s, psychologists like White argued that activities enjoyable for their own sake may be accounted for by the existence of intrinsic psychobiological human motivation [White, 1959]. In the same period, Berlyne proposed that exploration might be triggered an...|$|R
40|$|BACKGROUND: Growing {{evidence}} supports {{the use of}} Western therapies {{for the treatment of}} depression, trauma, and stress delivered by community health workers (CHWs) in conflict-affected, resource-limited countries. A recent randomized controlled trial (Bolton et al. 2014 a) supported the efficacy of two CHW-delivered interventions, cognitive processing therapy (CPT) and brief behavioral activation treatment for depression (BATD), for reducing depressive symptoms and functional impairment among torture survivors in the Kurdish region of Iraq. METHODS: This study describes the adaptation of the CHW-delivered BATD approach delivered in this trial (Bolton et al. 2014 a), informed by the Assessment-Decision-Administration-Production-Topical experts-Integration-Training-Testing (ADAPT-ITT) framework for intervention adaptation (Wingood and DiClemente, 2008). Cultural modifications, adaptations for low-literacy, and tailored training and supervision for non-specialist CHWs are presented, along with two clinical case examples to illustrate delivery of the adapted intervention in this setting. RESULTS: Eleven CHWs, a study psychiatrist, and the CHW clinical supervisor were trained in BATD. The adaptation process followed the ADAPT-ITT framework and was iterative with significant input from the on-site supervisor and CHWs. Modifications were made to fit Kurdish culture, including culturally <b>relevant</b> <b>analogies,</b> use of stickers for behavior monitoring, cultural modifications to behavioral contracts, and including telephone-delivered sessions to enhance feasibility. CONCLUSIONS: BATD was delivered by CHWs in a resource-poor, conflict-affected area in Kurdistan, Iraq, with some important modifications, including low-literacy adaptations, increased cultural relevancy of clinical materials, and tailored training and supervision for CHWs. Barriers to implementation, lessons learned, and recommendations for future efforts to adapt behavioral therapies for resource-limited, conflict-affected areas are discussed...|$|R
40|$|Thunderstorms are {{transient}} phenomena characterized by short duration. Engineering is usual {{to evaluate the}} structural response to such phenomena, most notably to earthquakes, by the response spectrum technique, a method extensively applied in both the scientific and technical fields. This paper represents {{the first stage of}} a research project aimed at introducing a "new" method that generalizes the "old" response spectrum technique from earthquakes to thunderstorms. In such a framework, it establishes the fundamentals of the structural response of ideal point-like Single-Degree-Of-Freedom systems subjected to thunderstorm wind actions perfectly coherent over the exposed structural surface. A general definition of thunderstorm response spectrum is introduced, which allows a straightforward evaluation of the maximum structural response and the equivalent static force. A new definition of thunderstorm base response spectrum is also provided. Several forms of representation and parameterization of these response spectra are studied, showing the merits and the shortcomings of each of these. The general properties of the response spectra are investigated, pointing out some <b>relevant</b> <b>analogies</b> with the seismic case and the classical analysis of synoptic events. In particular, while wind engineering is usual to express the wind loading starting from wind velocity models, the method proposed herein, similarly to seismic engineering, involves the use of real thunderstorm records. The records adopted in this paper to illustrate the new method come from the wind monitoring campaign conducted for the European project "Wind and Ports". Despite the application to this case study, the methods and the definitions presented are fully general and may be applied in any context...|$|R
40|$|This {{forthcoming}} book chapter defines {{the problem of}} diminished political competition, describes the <b>relevant</b> legal <b>analogies</b> concerning regulation of economic competition, and explains how the law shapes the competitive environment for elections. It also details how Supreme Court justices have sometimes tried to incorporate competitiveness concerns into their election law decisions in cases concerning ballot access, redistricting, campaign finance, party reform, and term limits. For the most part, constitutional law proves to be both a blunt and a coarse instrument for addressing excesses of partisan greed or self-interest, but justices of varying ideological leanings have invoked such concerns (usually in dissent) to highlight why one or another election law violates the Constitution...|$|R
30|$|From the {{information}} provided in the tables and the plots shown earlier, it appears that EGS projects currently under development {{are still on the}} learning curve, overcoming problems, gaining experience and trying to introduce advanced technology; the projects already concluded provide <b>relevant</b> history and <b>analogy</b> for upcoming developments and the projects that have been temporarily halted or abandoned give an insight into issues that must be avoided in the future.|$|R
40|$|This article {{proposes a}} {{framework}} for incorporating ethics instruction into the OEIS Model Curriculum by drawing upon some <b>relevant</b> <b>analogies</b> involving instruction and {{research in the field}} of computer ethics. The first consideration is a relatively recent strategy used to infuse instructional material on ethics into a joint curriculum project sponsored by the two major professional computer associations: the Association for Computing Machinery (ACM) and the Institute for Electrical and Electronics EngineeringComputer Society (IEEE-CS). I then examine some ways in which key aspects of a recently proposed computer-ethics research model can inform the design and development of instructional units on ethics in the Organizational and End-user Information Systems (OEIS) Model Curriculum. Additionally, this article includes a description of how ethics instruction can be implemented either as a stand-alone, dedicated ethics course or as a cluster of course modules in existing courses, or both. Finally, the question, Who should teach the ethics courses or course modules?, is addressed. This article concludes by advocating for an interdisciplinary instructional model that incorporates the expertise of instructors and practitioners in three different fields: OEIS/IT, philosophy/ ethics, and the social sciences. Courses in computer and information ethics have been taught at colleges and universities in the U. S. for more than 25 years, and the number of ethics-related courses and course modules available to computing and IT professionals has increased significantly in the past decade. This article examines some strategies for extending relevant features of ethics instruction currently implemented in the Computer Science (CS) Curriculum to the OEIS Model Curriculum. I also describe how a research model for computer ethics, proposed by Brey (2004), can aid educators in constructing a plausible framework for ethics instruction in the OEIS Mode...|$|R
40|$|The common {{characteristic}} of dystopian literature is the extrapolation of contemporary societal deficiencies into a fictional future {{to raise the}} readers’ awareness for negative developments in the real world. To reach this effect, {{it is necessary that}} the fictional world of the dystopian novel relates to the readers’ world. This thesis explores the analogies between the fictional and the real world and aims at analysing the structure of the analogies {{with the help of a}} cognitive approach to literature. More precisely, the study is based on the principles of schema-theory which provide an explanation for the functioning of analogies. The schema-theoretic approach of cognitive scientist Roger Schank (Thematic Organization Points) forms the basis for the development and application of an innovative methodology to the analogy creation process through the interaction between text-based and knowledge-based schemata in dystopian literature. Based on the assumption that the <b>relevant</b> <b>analogies</b> are established already at the beginning of the text as the most important part of the dystopian novel, the study focuses on the expositions of four important representatives of dystopian literature: Aldous Huxley’s Brave New World (1932), Margaret Atwood’s The Handmaid’s Tale (1985), Suzanne Collins’ The Hunger Games (2008) and Robert Ferrigno’s Prayers for the Assassin (2006). The results of the study confirm the proposition that the analogies between text-based and knowledge-based schemata manifest themselves as TOPs (Thematic Organisation Points), more specifically as goals, plans and themes. The degree of schema activation can be determined by means of linguistic and schema-theoretic criteria in order to differentiate between weak and strong schemata. Furthermore, it is shown that the schemata constructing the fictional dystopian world can be identified within the novels’ expositions. Certain schemata dominate the reception process (primacy effect) and direct the readers’ perception of the dystopian world...|$|R
40|$|Religious {{diversity}} {{has become}} a central topic {{in the philosophy of}} religion. This study proposes a methodological approach to the topic by exploring the division of tasks set out by Bernard Lonergan (1904 - 1984). Lonergan's methodological framework, which he called functional specialization, provides a generic differentiation of tasks, each of which is central to the overall project of understanding religious diversity. This thesis explores the relevance and utility of functional specialization as a methodological approach to religious diversity in the philosophy of religion. The first chapter is an analysis of the literature on religious diversity as a topic in the philosophy of religion. It unearths the dominant concerns in the field and some of the obstacles which continue to hinder the development of this enquiry. The second chapter provides the epistemological grounds of functional specialization. While the division of tasks outlined by Lonergan's methodology is useful simply insofar as it differentiates the tasks of academic enquiry, there are more theoretical grounds by which this division is justified. The third chapter provides an explanatory account of the operations and tasks involved in each of the eight functional specialties. It elucidates these specialties by drawing upon <b>relevant</b> <b>analogies</b> from outside the field of religious studies. The fourth chapter brings together the two main concerns of the study by suggesting ways in which functional specialization can make a methodological contribution to the enquiry into religious diversity. It organizes the distinct but related tasks which constitute the philosophical study of religious diversity, demonstrates the current trends regarding each of these tasks, and suggests ways in which they can be made more effective. Lonergan's notion of functional specialization makes an important contribution to the philosophical debate over religious diversity in significant ways. It provides an effective methodology which delineates both the fundamental tasks of scholarly enquiry and the operations involved in these tasks. It explains how current work in the philosophy of religious diversity could benefit from a clear delineation of the relevant tasks. It provides a framework which is open to collaboration among scholars of diverse philosophical and theological viewpoints...|$|R
40|$|Pre-exposure {{prophylaxis}} (PrEP) has {{the potential}} to become a powerful biomedical approach to HIV prevention; however, its success depends on behavioral and social factors that may determine its appropriate use. This article is designed to facilitate interdisciplinary empirical <b>analogies</b> <b>relevant</b> to PrEP implementation, reviewing behavioral and social science findings that may provide lessons {{critical to the success of}} PrEP as a biomedical–behavioral prevention strategy. As we prepare for the dissemination of new biomedical approaches to HIV prevention, integrating the state of the science across disciplines may result in innovative strategies for implementation that can enhance their success...|$|R
50|$|Inconclusive {{evidence}} {{was found for}} positive analogical transfer based on prior knowledge; however, groups did demonstrate variability. The problem format and the structural type of analog presentation showed the highest positive transference to problem solving. The researcher suggested that a well-thought and planned <b>analogy</b> <b>relevant</b> in format and type to the problem-solving task to be completed can be helpful for students to overcome functional fixedness. This study not only brought new knowledge about the human mind at work but also provides important tools for educational purposes and possible changes that teachers can apply as aids to lesson plans.|$|R
40|$|The Author(s) 2010. This {{article is}} {{published}} with open access at Springerlink. com Abstract Pre-exposure prophylaxis (PrEP) {{has the potential}} to become a powerful biomedical approach to HIV prevention; however, its success depends on behavioral and social factors that may determine its appropriate use. This article is designed to facilitate interdisciplinary empirical <b>analogies</b> <b>relevant</b> to PrEP implementation, reviewing behavioral and social science findings that may provide lessons {{critical to the success of}} PrEP as a biomedical– behavioral prevention strategy. As we prepare for the dissemination of new biomedical approaches to HIV prevention, integrating the state of the science across disciplines may result in innovative strategies for implementation that can enhance their success...|$|R
30|$|Figure  5 b is {{a process}} case using TRIZ which shows the basic {{principle}} to apply the contradiction matrix to solve inventive problems. Technical contradictions are a general type of inventive problems in TRIZ which arises when an attempt to improve certain attributes or functions of a system leads to the deterioration of other attributes of that system. One contradiction XP is represented by pairs of conflicting parameters PA and PB. The two parameters are used to determine a specific element XS in the matrix. The numbers XS 1 –XS 4 in XS are the recommend consequential numbers of inventive principles for solving the contradiction. There are several cases in books or knowledge bases for every inventive principle, which show the applications in cross domains of the principles. XP, XS 1 –XS 4 and all cases <b>relevant</b> are <b>analogies</b> which are transferred to solve similar contradiction problems. When an engineer or inventor faces a contradiction P in design process he or she defines it using two conflicting parameters from the 39 standard parameter list of TRIZ. The instance of XP, matrix, XS and relevant cases as whole is the analogy A of solving a technical contradiction. The engineer applies the analogy to generate domain solutions.|$|R
40|$|The thesis {{purpose is}} mainly {{to show how}} some common {{techniques}} of quantum field theories {{could be applied to}} the design of better detection algorithms, in the context of stochastic background of gravitational waves. The thesis is structured as follows: in Chapter 1 we introduce the framework of propagating wave solutions of Einstein equations. We outline the main features of interferometers, including their coupling to gravitational wave radiation. Then, we summarize the features of our reference cosmological model, a Friedmann-Robertson-Walker-Lemaitre universe. We show the performance of single and multiple interferometers detections. In Chapter 2 we overview two interesting sources for our study: cosmic strings cusps and binary black hole coalescences. For the former, we outline the spontaneous simmetry breaking mechanisms, and the formation of topological linear defects. We quantify the associated spectrum of gravitational wave, and characterize in time domain the average signal overlap, as seen from a single interferometer. For the latter, we describe two fiducial cosmological population, and define three different regimes for the background, afflicting deeply the best detection strategy. In Chapter 3 we introduce the basics of stochastic processes, in particular of Campbell processes. We overview two equivalent probabilistic structures behind them. We show how to evaluate their mean values via a generalized concept of probability distributions. We carry out <b>relevant</b> <b>analogies</b> with field theory correlation functions. The reader will be provided here with the necessary tools to manipulate the processes involved in the procedure later explained in Chapter 5. In Chapter 4 we review the general framework of decision theory. We describe a standard approach to signal detection, and the tools needed to characterize a detector performances. We formulate the criterion for an optimal detection strategy, the Neyman-Pearson lemma. We use some toy signals (both deterministic and stochastic) to split the different aspects of the detection problem into smaller parts. We derive analitically the standard optimal detector for a gaussian background and study its performances. Then, we switch our attention to the detection of a non Gaussian stochastic background. We show a simple perturbative expansion of the likelihood, assuming the correlators to be hierarchically ordered by a scaling parameter. We show how to construct the relevant statistics of the data. We draw out some considerations on the computational issues of such algorithm. With all the necessary mathematics in hand, in Chapter 5 we tackle the detection strategy by employing a resummation tecnique borrowed from renormalization group theory. We explain the tecnique in detail with a simpler example of a single variable, then we extend the results for multiple detector timeseries. They will find best confirmations with both numerical and analitical validation to appear in a proposed scientific paper. Throughout the whole text we will underline the analogies and differences with respect to quantum field theories probabilistical structure, considered in many passages as a compass for right direction. Finally, in Chapter 6 we summarize the results, and draw out some consideration on future developments of the proposed approach...|$|R
40|$|Empirical mode {{decomposition}} is {{an adaptive}} signal processing method that {{when applied to}} a broadband signal, such as that generated by turbulence, acts {{as a set of}} band-pass filters. This process was applied to data from time-resolved, particle image velocimetry measurements of subsonic jets prior to computing the second-order, two-point, space-time correlations from which turbulent phase velocities and length and time scales could be determined. The application of this method to large sets of simultaneous time histories is new. In this initial study, the results are <b>relevant</b> to acoustic <b>analogy</b> source models for jet noise prediction. The high frequency portion of the results could provide the turbulent values for subgrid scale models for noise that is missed in large-eddy simulations. The results are also used to infer that the cross-correlations between different components of the decomposed signals at two points in space, neglected in this initial study, are important...|$|R
