7|121|Public
30|$|Equidistance, the <b>redundant</b> <b>attribute,</b> and {{sparsity}} are {{the fundamental}} factors affecting the clustering performance of high-dimensional data [9]. Equidistance renders {{the distance between}} any two points in a high-dimensional space approximately equal, leading to a failure in the clustering algorithm, based on the distance. The <b>redundant</b> <b>attribute</b> increases the dimensionality of the high-dimensional data and {{the complexity of the}} index structure, decreasing the efficiency of building and retrieving the index structure. Sparsity enables uniform data distribution, and some clusters may overlap with each other, affecting the clustering precision.|$|E
40|$|Abstract—Based on {{granular}} computing theory, {{according to}} the problem of intrusion detection classification performance reduced by <b>redundant</b> <b>attribute</b> in high dimensional network data, an attribute reduction method of network intrusion detection system based on granular computing is given, the <b>redundant</b> <b>attribute</b> is removed under the condition of keeping the information integrity of original attribute set to reduce the attribute dimension of data. The example analysis indicates that this method reduces the training and detection time, and improves the computing efficiency of system {{in order to reduce the}} data storage, it provides a new idea for processing massive large data. Keywords- granular computing; network intrusion detection system; attribute reduction I...|$|E
30|$|To {{solve the}} {{clustering}} problems owing to equidistance, the <b>redundant</b> <b>attribute,</b> and sparsity, an efficient audio signal clustering algorithm is proposed, by integration with the Psim matrix and Tabu Search. First, {{for all the}} points in the high-dimensional space, the Psim values {{between them and the}} corresponding location numbers are stored in a Psim matrix. Next, a sequential Psim matrix is generated by sorting the elements in each row of the Psim matrix. Further, the initial clusters are generated with differential truncation and refined with the Tabu Search. Finally, the initial clusters are iteratively refined with the K-Medoids algorithm, until all the cluster medoids are stable.|$|E
30|$|The {{selective}} Bayes classifier (SBC) is an {{enhancement of}} NBC, which displays good performance when <b>redundant</b> <b>attributes</b> exist. With SBC highly correlated <b>redundant</b> <b>attributes</b> are excluded if {{the assumption of}} attribute independency is taken. In [31] greedy search is performed to select all the subsets of the attributes using the forward selection technique, which raises {{the accuracy of the}} classifier obtained from the training set.|$|R
40|$|Abstract—Discretization {{can turn}} numeric {{attributes}} into discrete ones. Feature selection can eliminate some irrelevant and/or <b>redundant</b> <b>attributes.</b> Chi 2 {{is a simple}} and general algorithm that uses the c 2 statistic to discretize numeric attributes repeatedly until some inconsistencies {{are found in the}} data. It achieves feature selection via discretization. It can handle mixed attributes, work with multiclass data, and remove irrelevant and <b>redundant</b> <b>attributes.</b> Index Terms—Discretization, feature selection, pattern classification. ...|$|R
40|$|Discretization {{can turn}} numeric {{attributes}} into discrete ones. Feature selection can eliminate some irrelevant and/or <b>redundant</b> <b>attributes.</b> Chi 2 {{is a simple}} and general algorithm that uses the 2 statistic to discretize numeric attributes repeatedly until some inconsistencies {{are found in the}} data. It achieves feature selection via discretization. It can handle mixed attributes, work with multiclass data, and remove irrelevant and <b>redundant</b> <b>attributes.</b> Keywords [...] - discretization, feature selection, pattern classification I. Introduction Feature selection can eliminate some irrelevant and/or <b>redundant</b> <b>attributes.</b> By using relevant features, classification algorithms can in general improve their predictive accuracy, shorten the learning period, and form simpler concepts. There are abundant feature selection algorithms. Some use methods like principle component to compose a smaller number of new features [11, 12]; some select a subset of the original attributes [1, 5]. This paper consi [...] ...|$|R
40|$|Abstract—To {{deal with}} the risk {{recognition}} from the complex Web environment, a novel risk recognition method was studied through attribute reduction and rule extration based on rough set. According to the indiscernible relation in rough set, discernible vector and its addition rule were defined. And meanwhile the core attribute set and the attribute reduction were obtained by scanning the information table just only one time depending on the discernible vector addition rule. Attribute value reduction was realized through gradually deleting the <b>redundant</b> <b>attribute</b> value for every rule in the information table by the correlation of condition attributes and decision attributes. Finally, a concise rule set for risk recognition was obtained. The illustration and experiment {{results indicate that the}} method is effective and efficient. Index Terms—web services; risk recognition; Rough Set; data minin...|$|E
40|$|Feature {{selection}} {{is a process}} which selects the subset of attributes from the original dataset by removing the irrelevant and <b>redundant</b> <b>attribute.</b> Clustering is the technique in data mining which group thesimilar object in to one cluster and dissimilar object into other cluster. Some clustering technique does not support high dimensional dataset. By applying the feature selection as a preprocessing step for the clustering {{make it possible to}} handle the high dimensional dataset. Feature selection reduce the computational time greatlydue to reduced feature subset and also improve clustering quality. Feature selection methods are available for supervised and unsupervised learning. This paper is related to working of feature selection method which is applied on different feature selection algorithm. The result proved that Feature selection through featureclustering algorithm is reduced the more attributes than the standard feature selection algorithm like relief andfisher filte...|$|E
30|$|Audio {{signals are}} {{a type of}} high-dimensional data, and their {{clustering}} is critical. However, distance calculation failures, inefficient index trees, and cluster overlaps, derived from the equidistance, <b>redundant</b> <b>attribute,</b> and sparsity, respectively, seriously affect the clustering performance. To solve these problems, an audio-signal clustering algorithm based on the sequential Psim matrix and Tabu Search is proposed. First, the audio signal similarity is calculated with the Psim function, which avoids the equidistance. The data is then organized using a sequential Psim matrix, which improves the indexing performance. The initial clusters are then generated with differential truncation and refined using the Tabu Search, which eliminates cluster overlap. Finally, the K-Medoids algorithm is used to refine the cluster. This algorithm is compared to the K-Medoids and spectral clustering algorithms using UCI waveform datasets. The experimental {{results indicate that the}} proposed algorithm can obtain better Macro-F 1 and Micro-F 1 values with fewer iterations.|$|E
3000|$|... }. The {{discovered}} <b>redundant</b> <b>attributes</b> are removed. Intuitively, an <b>attribute</b> is <b>redundant</b> {{if we can}} regenerate it {{by association}} from other attributes. Finally, we keep the last subset that contains the reduced subset of O [...]...|$|R
40|$|Abstract. This paper {{presents}} an inductive learning system called the Genetic Instance-Based Learning (GIBL) system. This system combines instance-based learning approaches with evolutionary computation {{in order to}} achieve high accuracy in the presence of irrelevant or <b>redundant</b> <b>attributes.</b> Evolutionary computation is used to find a set of attribute weights that yields a high estimate of classification accuracy. Results of experiments on 16 data sets are shown, and are compared with a non-weighted version of the instance-based learning system. The results indicate that the generalization accuracy of GIBL is somewhat higher than that of the non-weighted system on regular data, and is significantly higher on data with irrelevant or <b>redundant</b> <b>attributes.</b> 1...|$|R
5000|$|Dimensional {{normalization}} or snowflaking removes <b>redundant</b> <b>attributes,</b> {{which are}} {{known in the}} normal flatten de-normalized dimensions. Dimensions are strictly joined together in sub dimensions. [...] Snowflaking {{has an influence on}} the data structure that differs from many philosophies of data warehouses.Single data (fact) table surrounded by multiple descriptive (dimension) tables ...|$|R
40|$|Integration of {{data sources}} {{to build a}} Data {{warehouse}} (DW), refers {{to the task of}} developing a common schema as well as data transformation solutions for a number of data sources with related content. The large number and size of modern data sources make the integration process cumbersome. In such cases dimensionality of the data is reduced prior to populating the DWs. Attribute subset selection on the basis of relevance analysis is one way to reduce the dimensionality. Relevance analysis of attribute is done by means of correlation analysis, which detects the attributes (redundant) that do not have significant contribution in the characteristics of whole data of concern. After which the <b>redundant</b> <b>attribute</b> or attribute strongly correlated to some other attribute is disqualified to be the part of DW. Automated tools based on the existing methods for attribute subset selection may not yield optimal set of attributes, which may degrade the performance of DW. Various researchers have used GA, as an optimization tool but most of them use GA to search the optimal technique amongst the available techniques for attribute selection. This paper formulates and validates a method for selecting optimal attribute subset based on correlation using Genetic algorithm (GA), where GA is used as optimal search tool for selecting subset of attributes. ...|$|E
30|$|Feature {{selection}} called also {{attribute selection}} or variable selection {{is the process}} of removing the <b>redundant</b> <b>attributes</b> that are deemed irrelevant to the data mining task. It is an important step that may be launched before classification to eliminate irrelevant variables. This process can improve the classification performance and accelerate the search process [10, 12, 35, 36, 37, 41].|$|R
40|$|FreeViz is a {{data mining}} method for local {{optimization}} of linear projections. In this thesis we cover method's performance in field of genetics. Genetic datasets usually contain much more attributes than instances; we first use linear algebra to predict method's problems on such data. Then {{we try to}} find properties of datasets that influence the performance of FreeViz. The goal of the analysed method is to find good (informative) visualizations, so we estimate its performance by measuring quality of a k(k Nearest Neighbours) classifier on the projections it yields. The results confirm FreeViz's poor performance on genetic data, but it nevertheless proved successful {{on one of the}} used dataset. In pursue of dataset properties that influence method's performance, we generated synthetic datasets. Results show that the ratio between attribute count and instance count has negligible influence. On the other hand, FreeViz's quality is degraded when most of the <b>attributes</b> are <b>redundant</b> and improved when there are mutually correlated attributes. We have also observed the paths that attribute projections make during optimization, but found no rule to distinguish <b>redundant</b> <b>attributes</b> from the rest. In case there is a large number of instances, FreeViz yields a projection that maps <b>redundant</b> <b>attributes</b> closer to the origin. That is not the case when there are more attributes than instances. However, in that case not even a nomogram for a naive Bayesian classifier can distinguish between informative and <b>redundant</b> <b>attributes.</b> ...|$|R
40|$|International audienceWe {{propose a}} new methodology, called BUST, for the {{extraction}} of joint relationships using induction of decision trees. BUST is the acronym of “Bottom-Up attribute Selection for the induction of decision Trees”: at each node of the tree, a bottom-up approach of attribute selection is used, {{as opposed to the}} classical Top-Down approach. This methodology has been developed to solve functional separability problem: the irrelevant or <b>redundant</b> <b>attributes,</b> which should be the last attributes tested in the tree, are rejected...|$|R
40|$|AbstractIn this paper, {{we propose}} a new rule based {{attribute}} selection algorithm for removing the <b>redundant</b> <b>attributes</b> {{which are used}} in decision making on intrusions in wireless sensor networks. This work focuses mainly on finding important attributes to find Denial of Service attacks. In addition, we used an enhanced MSVM classification algorithm that was developed by extending the existing MSVM algorithm. Theexperimentalresultsshowthatthe proposed methods provide high detection rates and reduce false alarm rate. This system has been tested using KDD’ 99 Cup data set...|$|R
40|$|Discretization of {{continuous}} attributes {{is an important}} task for certain types of machine learning algorithms. Bayesian approaches, for instance, require assumptions about data distributions. Decision Trees, on the other hand, require sorting operations to deal with continuous attributes, which largely increase learning times. This paper presents a new method of discretization, whose main characteristic is that it takes into account interdependencies between attributes. Detecting interdependencies {{can be seen as}} discovering <b>redundant</b> <b>attributes.</b> This means that our method performs attribute selection as a side effect of the discretization...|$|R
40|$|AbstractThe {{original}} Rough Set {{model is}} concerned primarily with algebraic properties of approximately defined sets. The Variable Precision Rough Set (VPRS) model extends the basic {{rough set theory}} to incorporate probabilistic information. The article presents a non-parametric modification of the VPRS model called the Bayesian Rough Set (BRS) model, where the set approximations are defined by using the prior probability as a reference. Mathematical properties of BRS are investigated. It is shown {{that the quality of}} BRS models can be evaluated using probabilistic gain function, which is suitable for identification and elimination of <b>redundant</b> <b>attributes...</b>|$|R
40|$|Abstract: The {{rough set}} theory is a new method for {{analyzing}} and dealing with data. By using this theory, we proposed a risk assessment algorithm based on {{rough set theory}}, which was described in detail in this paper. the decision table can be simplified and <b>redundant</b> <b>attributes</b> can be got rid of A method of inference based on the knowledge of rough sets and an example to show how to acquire the rules of new decision making, thus filling the method with a practical and publicizing value are given...|$|R
40|$|In this paper, {{we present}} a medical {{decision}} support system based on a hybrid approach utilising rough sets and a probabilistic neural network. We utilised the ability of rough sets to perform dimensionality reduction to eliminate <b>redundant</b> <b>attributes</b> from a biomedical dataset. We then utilised a probabilistic neural network to perform supervised classification. Our results indicate that rough sets was able {{to reduce the number}} of attributes in the dataset by 67 % without sacrificing classification accuracy. Our classification accuracy results yielded results on the order of 93 %...|$|R
40|$|Abstract. This paper {{presents}} an argumentation-based multi-attribute decision making model, where decisions made {{can be explained}} in natural language. More specifically, an explanation for a decision is obtained from a mapping between the given decision framework and an argumentation framework, such that best deci-sions correspond to admissible sets of arguments, and the explanation is gener-ated automatically from dispute trees sanctioning the admissibility of arguments. We deploy a notion of rationality where best decisions meet most goals and ex-hibit fewest <b>redundant</b> <b>attributes.</b> We illustrate our method by a legal example, where decisions amount to past cases most similar to a given new, open case. ...|$|R
40|$|In {{this paper}} we {{investigate}} {{the problem of}} the accuracy of classifier using wrapper methods. For the purposes of classification is used a large number of algorithms: IBK, Naïve Bayes, SVM, J 48 decision tree and RBF networks. Experimental results show that wrapper methods can rapidly identify irrelevant, <b>redundant</b> <b>attributes,</b> as well as the noise in the data, if any; and those attributes which are important for the studied phenomenon. The paper prove that applying wrapper methods for reducing the dimensionality of the data it is possible to significantly improve system performance for inductive learning rules in classification problems...|$|R
40|$|Virtual {{laboratory}} teaching {{quality evaluation}} helps to realize scientific teaching management. Virtual laboratory teaching quality evaluation is multi-level and multi-objective system engineering. In this paper, a teaching quality evaluation model based on rough set (RS) and on improved Binary-Tree and multicategory {{support vector machine}} (SVM) was provided. Firstly, the attribute reduction of RS was applied as a preprocessor to delete <b>redundant</b> <b>attributes</b> and conflicting objects without losing efficient information. Then an improved multi-category SVM based on Binary-Tree classification model was built to make a forecast. Finally, the model was applied to validation. Results show that this model performs well both in data classification accuracy and predictive accuracy with wide applicability...|$|R
40|$|Abstract. The common faults of mine {{ventilator}} are researched in this paper, and rotor misalignment, unbalance, oil whirl, surge {{and other}} faults and fault {{characterization of the}} generation mechanism are analyzed. The faults diagnosis system is designed based on rough neural network. First, {{the characteristics of the}} type of fault for fan failure data collection, including vibration and temperature signals. Then, the pretreated sample data using rough set attribute reduction method to delete <b>redundant</b> <b>attributes.</b> Finally, the sample data is divided into training and testing samples, were used to train and test the neural network classifier. Experiments show that the system is reliable, diagnostic yield, improved ventilator system security, expanding the scope of application of rough sets...|$|R
40|$|Data {{generated}} {{in the fields of}} science, technology, business and in many other fields of research are increasing in an exponential rate. The way to extract knowledge from a huge set of data is a challenging task. This paper aims to propose a hybrid and viable method to deal with an information system in data mining, using topological techniques and the significance of the attributes measured using rough set theory, to compute the reduct, This will reduce the randomness in the process of elimination of <b>redundant</b> <b>attributes,</b> which, in turn, will reduce the complexity of the computation of reducts of an information system where a large amount of data have to be processed...|$|R
30|$|Feature {{selection}} {{is one of}} the key steps in data pre-processing employed to maximize the performance of text classification, and it utilizes a machine learning technique [44]. It eliminates irrelevant or <b>redundant</b> <b>attributes</b> from the original feature space, and selects a relevant subset based on the target evaluation criterion to reduce the complexity of the analysis [2, 45]. Twitter is a popular online social networking service (SNS) platform that enables the users to show their thought or opinion in a 140 -character message. Sentiment 140 lets the users discover the sentiment on people, product, etc. on Twitter. It also provides the APIs for analyzing the tweets, and supports the integration of sentiment analysis classifier with other personal site or platform [25].|$|R
40|$|Many {{learning}} algorithms make {{an implicit}} assumption {{that all the}} attributes present in the data are relevant to a learning task. However, several {{studies have demonstrated that}} this assumption rarely holds; for many supervised learning algorithms, the inclusion of irrelevant or <b>redundant</b> <b>attributes</b> can result in a degradation in classification accuracy. While a variety of different methods for dimensionality reduction exist, many of these are only appropriate for datasets which contain a small number of attributes (e. g. < 20). This paper presents an alternative approach to dimensionality reduction, and demonstrates how it can be combined with a Nearest Neighbour learning algorithm. We present an empirical evaluation of this approach, and contrast its performance with two related techniques; a Monte-Carlo wrapper and an Information Gain-based filter approach...|$|R
40|$|Despite their diverse {{applications}} in many domains, the variable precision rough sets (VPRS) model lacks a feasible method {{to determine a}} precision parameter ([beta]) value to control the choice of [beta]-reducts. In this study we propose an effective method to find the [beta]-reducts. First, we calculate a precision parameter value to find the subsets of information system {{that are based on}} the least upper bound of the data misclassification error. Next, we measure the quality of classification and remove <b>redundant</b> <b>attributes</b> from each subset. We use a simple example to explain this method and even a real-world example is analyzed. Comparing the implementation results from the proposed method with the neural network approach, our proposed method demonstrates a better performance. VPRS model [beta]-reduct Precision parameter Neural networks...|$|R
40|$|Feature {{selection}} {{is an important}} data pre-processing step that comes before applying a machine learning algorithm. It removes irrelevant and <b>redundant</b> <b>attributes</b> from the dataset with an aim of improving the algorithm performance. There exist feature selection methods which focus on discovering features that are most suitable. These methods include wrappers, a subroutine of the learning algorithm itself, and filters, which discover features according to heuristics, based on the data characteristics and not tied to a specific algorithm. This paper improves the filter approach by enabling it to select strongly relevant and weakly relevant features and gives room to the researcher to decide which of the weakly relevant features to include. This new approach brings clarity and understandability to the feature selection preprocessing step...|$|R
40|$|In this paper, rough sets (RS) and quantum {{neural network}} (QNN) {{are used to}} {{recognize}} electrocardiogram (ECG) signals. Firstly, wavelet transform (WT) {{is used as a}} feature extraction after normalization of these signals. Then the attribute reduction of RS has been applied as preprocessor so that we could delete <b>redundant</b> <b>attributes</b> and conflicting objects from decision making table but remain efficient information lossless. We realized classification modeling and forecasting test based on QNN after that. Finally, the RS-QNN gives us fast and realistic results compared with the BP and RBF. By this method, we could reduce the dimension of feature space and decrease the complexity in the process. Experiment result shows that the classification ability of the RS-QNN is superior to conventional approach...|$|R
40|$|Recent {{years have}} been wide efforts in {{attribute}} selection research. Attribute selection can efficiently reduce the hypothesis space by removing irrelevant and <b>redundant</b> <b>attributes.</b> Attribute reduction of an information system is a key problem in rough set theory and its applications. In this paper, we compare the performance of attribute selection using two technical tools namely WEKA 3. 7 and ROSE 2. Filter methods are used an alternative measure instead of the error rate to score a feature subset. This measure was chosen to be fast to compute, {{at the same time}} as still captur-ing the usefulness of the feature set. Many filters provide a feature ranking rather than an explicit best feature subset, and the cutoff point in the ranking is chosen via cross-validation. We used Search methods like Best first and Greedy stepwise to evaluate a subset of features as a group for suitability. We use the internet usage data set for this purpose and then comparison results are tabulated for various methods for searching the solution space to eliminate the irrelevant attribute. Results of this research shows us some minding issues of attribute selection tools where we found better ways to have select irrelevant attributes. Comparing the tools of attributes reductions evidence some considerable different between them...|$|R
40|$|We {{present an}} {{approach}} to transductive learning that employs semi-supervised clustering of all available data (both labeled and unlabeled) to produce a data-dependent SVM kernel. In the general case where the domain includes irrelevant or <b>redundant</b> <b>attributes,</b> we constrain the clustering to occur on the manifold prescribed by the data (both labeled and unlabeled). Empirical {{results show that the}} approach performs comparably to more traditional kernels while providing significant {{reduction in the number of}} support vectors used. Further, the kernel construction technique inherently possesses some of the benefits that would normally be provided by preprocessing with a dimensionality reduction algorithm. However, preprocessing still provides some additional benefit to the data-dependent kernel, including reduction in classification error due to improved cluster quality and robustness with respect to the SVM slack variable...|$|R
40|$|For high {{dimensional}} data, the <b>redundant</b> <b>attributes</b> of samplers {{will not}} only increase {{the complexity of the}} calculation, but also affect the accuracy of final result. The existing attribute reduction methods are encountering bottleneck problem of timeliness and spatiality. In order to looking for a relatively coarse attributes granularity of problem solving, this paper proposes an efficient attribute granulation method to remove redundancy attribute. The method calculates the similarity of attributes according attribute discernibility first, and then clusters attributes into several group through affinity propagation clustering algorithm. At last, representative attributes are produced through some algorithms to form a coarser attribute granularity. Experimental results show that the attribute granulation method based on affinity propagation clustering algorithm(AGAP) method is a more efficient algorithm than traditional attribute reduction algorithm(AR). </p...|$|R
40|$|International audienceClustering, rough {{sets and}} {{decision}} tree theory {{were applied to}} the evaluation of soil fertility levels, and provided new ideas and methods among the spatial data mining and knowledge discovery. In the experiment, the rough sets - decision tree evaluation model establish by 1400 study samples, the accuracy rate is 92 % of the test. The results show :model has good generalization ability; using the clustering method can effectively extract the typical samples and reducing the training sample space; the use of rough sets attribute reduction, can remove <b>redundant</b> <b>attributes,</b> can {{reduce the size of}} decision tree decision-making model, reduce the decision-making rules and improving the decision-making accuracy, using the combination of rough set and decision tree decision-making method to infer the level {{of a large number of}} unknown samples...|$|R
40|$|Rough {{set theory}} has evolved {{as one of}} the most {{important}} technique used for feature selection as a result of contemporary developments in data mining. One of the cardinal uses of Rough set theory is its application for rule generation. More often attribute reduction poses a major challenge for developing the theory and applications of rough set. This paper proposes a unique mathematical approach for determining the most important attribute with the help of confidence and strength of an association. Our approach focuses on the elimination of the <b>redundant</b> <b>attributes</b> in order to generate the effective reduct set (i. e., reduced set of necessary attributes) and formulating the core of the attribute set. Subsequently, only a subset of feature is selected which retain the accuracy of the original features...|$|R
