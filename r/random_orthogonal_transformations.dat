2|1026|Public
40|$|Stochastic {{integration}} {{rules are}} derived for infinite integration intervals, generalizing rules developed by Siegel and O'Brien (1985) for finite intervals. Then <b>random</b> <b>orthogonal</b> <b>transformations</b> of rules for integrals {{over the surface}} of the unit m-sphere are used to produce stochastic rules for these integrals. The two types of rules are combined to produce stochastic rules for multidimensional integrals over infinite regions with Normal or Student-t weights. Example results are presented to illustrate the effectiveness of the new rules. Key Words: Monte Carlo, multiple integrals, numerical integration, statistical computation. AMS Subject Classifications: 65 C 05, 65 C 10, 65 D 30, 65 D 32. 1 Introduction A common problem in applied science and statistics is to numerically compute integrals in the form E(g) = Z 1 Γ 1 Z 1 Γ 1 ::: Z 1 Γ 1 g(`) p(`) d`; with ` = (` 1; ` 2; :::; ` m) t. For statistics applications the function p(`) may be an unnormalized unimodal posteri [...] ...|$|E
40|$|In shape analysis, it {{is usually}} assumed that the matrix X:N-K of the co-ordinates of {{landmarks}} in K is isotropic Gaussian. Let Y:(N- 1) -K be the centered matrix of landmarks from X so that Y ~ N([mu], [sigma] 2 I). Let Y=TT be the Bartlett decomposition of Y into lower triangular, T, and orthogonal, [Gamma], components. The matrix T denotes the size-and-shape of X. For N- 1 >=K (the usual case in multivariate analysis is N- 1 = 2 the distribution of T {{is related to the}} noncentral Wishart distribution, an integral over the orthogonal group, [Gamma]=Â± 1. To derive the distribution of T when [Gamma]=+ 1, so that [Gamma] is a rotation, we investigate extending the method of <b>random</b> <b>orthogonal</b> <b>transformations,</b> especially when rank [mu]=K>= 2. The case K= 2 is tractable, but the case K= 3 is not. However, by a direct method we obtain the shape density when rank [mu]=K= 3 and [Gamma]= 1 as a computable double-series of trigonometric integrals. However, for K> 3, the density is not tractable which is not surprising in view of the same problem for the standard non-central Wishart distribution. Bartlett decompositions integral over SO(3) lower triangular matrix QR-decomposition random orthogonal transformation shape densities size shape distribution special orthogonal group Wishart distribution...|$|E
40|$|This paper {{explores the}} {{possibility}} of using multiplicative random projection matrices for privacy preserving distributed data mining. It specifically considers the problem of computing statistical aggregates like the inner product matrix, correlation coefficient matrix, and Euclidean distance matrix from distributed privacy sensitive data possibly owned by multiple parties. This class of problems is directly related to many other data-mining problems such as clustering, principal component analysis, and classification. This paper makes primary contributions on two different grounds. First, it explores Independent Component Analysis as a possible tool for breaching privacy in deterministic multiplicative perturbation-based models such as <b>random</b> <b>orthogonal</b> <b>transformation</b> and <b>random</b> rotation. Then, it proposes an approximate random projection-based technique to improve the level of privacy protection while still preserving certain statistical characteristics of the data. The paper presents extensive theoretical analysis and experimental results. Experiments demonstrate that the proposed technique is effective and can be successfully used for different types of privacypreserving data mining applications...|$|R
40|$|The goal of {{the paper}} is to detect pixels that contain targets of known spectra. The target can be present in a sub- or above pixel. Pixels without targets are {{classified}} as background pixels. Each pixel is treated via the content of its neighborhood. A pixel whose spectrum is different from its neighborhood is classified as a “suspicious point”. In each suspicious point there {{is a mix of}} target(s) and background. The main objective in a supervised detection (also called “target detection”) is to search for a specific given spectral material (target) in hyperspectral imaging (HSI) where the spectral signature of the target is known a priori from laboratory measurements. In addition, the fractional abundance of the target is computed. To achieve this we present two linear unmixing algorithms that recognize targets with known (given) spectral signatures. The CLUN is based on automatic feature extraction from the target’s spectrum. These features separate the target from the background. The ROTU algorithm is based on embedding the spectra space into a special space by <b>random</b> <b>orthogonal</b> <b>transformation</b> and on the statistical properties of the embedded result. Experimental results demonstrate that the targets’ locations were extracted correctly and these algorithms are robust and efficient...|$|R
40|$|The {{object of}} this report is an {{investigation}} of statistical independence and linear coordinate systems. It is hoped that this report will give some background for an ultimate investigation of independence and non­linear coordinate systems using tensor analysis. An additional basic objective is to obtain explicit algebraic expressions for different types of linear trans- formations. The first concepts to be covered are arbitrary linear transformations, various {{ways of looking at}} linear transformations, and the effects of a linear transformation on a vector of normally distributed <b>random</b> variables. Next <b>orthogonal</b> <b>transformations</b> to independence, and then oblique transformations to independence will be developed in turn...|$|R
40|$|Abstract: Even in {{the absence}} of an {{experimental}} effect, functional magnetic resonance imaging (fMRI) time series generally demonstrate serial dependence. This colored noise or endogenous autocorrelation typically hasdisproportionatespectralpoweratlowfrequencies,i. e.,itsspectrumis 1 f-like. Variouspre-whiteningand pre-coloringstrategieshavebeenproposedtomakevalidinferenceonstandardisedteststatisticsestimatedby time series regression in this context of residually autocorrelated errors. Here we introduce anew method based on <b>random</b> permutation after <b>orthogonal</b> <b>transformation</b> of the observed time series to the wavelet domain. This scheme exploits the general whitening or decorrelating property of the discrete wavelet transformandisimplementedusingaDaubechieswaveletwithfourvanishingmomentstoensureexchangeability of wavelet coefficients within each scale of decomposition. For 1 -like or fractal noises, e. g., realisations f of fractional Brownian motion (fBm) parameterised by Hurst exponent 0 �H� 1, this resampling algorithm exactly preserves wavelet-based estimates of the second order stochastic properties of the (possibly nonstationary) time series. Performance of the method is assessed empirically using 1 -like noise simulated by...|$|R
5000|$|The {{previous}} example can {{be extended}} to construct all <b>orthogonal</b> <b>transformations.</b> For example, the following matrices define <b>orthogonal</b> <b>transformations</b> on : ...|$|R
50|$|The inverse of an <b>orthogonal</b> <b>transformation</b> {{is another}} <b>orthogonal</b> <b>transformation.</b> Its matrix {{representation}} is the transpose {{of the matrix}} representation of the original transformation.|$|R
30|$|Sub-Gaussian α-stable random vectors are {{preserved}} under <b>orthogonal</b> <b>transformations.</b> The following proposition provides further characterizations of sub-Gaussian α-stable random vectors via {{their relationship to}} <b>orthogonal</b> <b>transformations</b> 5.|$|R
5000|$|... {{which means}} that R does not produce a reflection, and hence it {{represents}} a rotation (an orientation-preserving <b>orthogonal</b> <b>transformation).</b> Indeed, when an <b>orthogonal</b> <b>transformation</b> matrix produces a reflection, its determinant is -1.|$|R
50|$|Since {{the lengths}} of vectors and the angles between them are defined through the inner product, <b>orthogonal</b> <b>transformations</b> {{preserve}} lengths of vectors and angles between them. In particular, <b>orthogonal</b> <b>transformations</b> map orthonormal bases to orthonormal bases.|$|R
25|$|Thus finite-dimensional linear isometries—rotations, reflections, {{and their}} combinations—produce {{orthogonal}} matrices. The converse is also true: orthogonal matrices imply <b>orthogonal</b> <b>transformations.</b> However, linear algebra includes <b>orthogonal</b> <b>transformations</b> between spaces {{which may be}} neither finite-dimensional nor of the same dimension, and these have no orthogonal matrix equivalent.|$|R
5000|$|Euclidean {{distance}} is invariant under <b>orthogonal</b> <b>transformations.</b>|$|R
30|$|Let O(n) denotes an <b>orthogonal</b> <b>transformation</b> {{group in}} R^n.|$|R
50|$|How these {{products}} transform under <b>orthogonal</b> <b>transformations</b> is illustrated below.|$|R
5000|$|... is {{a change}} of basis {{representing}} an <b>orthogonal</b> <b>transformation,</b> then ...|$|R
40|$|We find an {{algorithm}} of numerical renormalization {{group for}} spin chain models. The essence of this algorithm is <b>orthogonal</b> <b>transformation</b> of basis states, which {{is useful for}} {{reducing the number of}} relevant basis states to create effective Hamiltonian. We define two types of rotations and combine them to create appropriate <b>orthogonal</b> <b>transformation.</b> 1...|$|R
2500|$|Because of W is the {{derivative}} of an <b>orthogonal</b> <b>transformation,</b> the ...|$|R
5000|$|The {{singular}} {{values of}} a matrix are invariant under <b>orthogonal</b> <b>transformations.</b>|$|R
5000|$|Invariance {{with respect}} to <b>orthogonal</b> <b>transformations</b> of the search space (rotation).|$|R
5000|$|... determines a Hermitian form {{which is}} {{invariant}} under proper <b>orthogonal</b> <b>transformations.</b>|$|R
5000|$|... 2. Invariance {{with respect}} to <b>orthogonal</b> <b>transformations</b> of the search space (rotation).|$|R
5000|$|This {{integral}} {{is performed}} by diagonalization of [...] with an <b>orthogonal</b> <b>transformation</b> ...|$|R
5000|$|The {{stiffness}} matrix [...] satisfies a given symmetry condition {{if it does}} not change when subjected to the corresponding <b>orthogonal</b> <b>transformation.</b> The <b>orthogonal</b> <b>transformation</b> may represent symmetry with respect to a point, an axis, or a plane. <b>Orthogonal</b> <b>transformations</b> in linear elasticity include rotations and reflections, but not shape changing transformations and can be represented, in orthonormal coordinates, by a [...] matrix [...] given byIn Voigt notation, the transformation matrix for the stress tensor can be expressed as a [...] matrix [...] given byThe transformation for the strain tensor has a slightly different form because of the choice of notation. This transformation matrix isIt can be shown that [...]|$|R
25|$|Every <b>{{orthogonal}}</b> <b>transformation</b> of a k-frame in Rn {{results in}} another k-frame, and any two k-frames are related by some <b>orthogonal</b> <b>transformation.</b> In other words, the orthogonal group O(n) acts transitively on V'k(Rn). The stabilizer subgroup {{of a given}} frame is the subgroup isomorphic to O(n'k) which acts nontrivially on the orthogonal complement of the space spanned by that frame.|$|R
30|$|It {{follows from}} Propositions 3.18 and 3.19 that W^n_p,i is an <b>orthogonal</b> <b>transformation</b> {{invariant}} class.|$|R
30|$|Recall that an <b>orthogonal</b> <b>transformation</b> in O(d) is {{a matrix}} U∈R^d × d such that UUT=UTU=I.|$|R
30|$|A {{limitation}} {{is that the}} results become ambiguous and relies on linear assumptions and <b>orthogonal</b> <b>transformations.</b>|$|R
5000|$|If we {{had another}} coframe , then the two coframes would be related by an <b>orthogonal</b> <b>transformation</b> ...|$|R
40|$|We {{present an}} {{algorithmic}} {{proof of the}} Cartan-Dieudonné theorem on generalized real scalar product spaces with arbitrary signature. We use Clifford algebras to compute the factorization of a given <b>orthogonal</b> <b>transformation</b> {{as a product of}} reflections with respect to hyperplanes. The relationship with the Cartan-Dieudonné-Scherk theorem is also discussed in relation to the minimum number of reflections required to decompose a given <b>orthogonal</b> <b>transformation.</b> Comment: 25 page...|$|R
40|$|The paper {{presents}} a new generalized integer <b>orthogonal</b> <b>transformation</b> {{which consists of}} a well known orthogonal transform followed by stretching the basis vectors maintaining the asymptotic behavior {{of the number of}} integer solutions for algebraic Diophantine equation. The author shows the properties of this transformation and he receives the algorithm for finding the matrix elements of a generalized integer <b>orthogonal</b> <b>transformation</b> for algebraic Diophantine equation of the second order to diagonal form. The article includes examples illustrating the reduction of algebraic equations of the second order to the diagonal form with the help of integer generalized <b>orthogonal</b> <b>transformation</b> and of determination asymptotics behavior of integer solutions for these equations. Comment: 23 page...|$|R
50|$|A star is eutactic {{if it is}} {{transformed}} to itself by some irreducible group of <b>orthogonal</b> <b>transformations.</b>|$|R
5000|$|The elastic {{properties}} of a continuum are invariant under an <b>orthogonal</b> <b>transformation</b> [...] if and only if: ...|$|R
5000|$|... where O(n, R) is the {{orthogonal}} group. We say central {{fields are}} invariant under <b>orthogonal</b> <b>transformations</b> around 0.|$|R
50|$|An <b>orthogonal</b> <b>transformation</b> of a {{symmetric}} (or Hermitian) matrix to tridiagonal form can be {{done with}} the Lanczos algorithm.|$|R
40|$|International audienceEven in {{the absence}} of an {{experimental}} effect, functional magnetic resonance imaging (fMRI) time series generally demonstrate serial dependence. This colored noise or endogenous autocorrelation typically has disproportionate spectral power at low frequencies, i. e., its spectrum is (1 /f) -like. Various pre-whitening and pre-coloring strategies have been proposed to make valid inference on standardised test statistics estimated by time series regression in this context of residually autocorrelated errors. Here we introduce a new method based on <b>random</b> permutation after <b>orthogonal</b> <b>transformation</b> of the observed time series to the wavelet domain. This scheme exploits the general whitening or decorrelating property of the discrete wavelet transform and is implemented using a Daubechies wavelet with four vanishing moments to ensure exchangeability of wavelet coefficients within each scale of decomposition. For (1 /f) -like or fractal noises, e. g., realisations of fractional Brownian motion (fBm) parameterised by Hurst exponent 0 < H < 1, this resampling algorithm exactly preserves wavelet-based estimates of the second order stochastic properties of the (possibly nonstationary) time series. Performance of the method is assessed empirically using (1 /f) -like noise simulated by multiple physical relaxation processes, and experimental fMRI data. Nominal type 1 error control in brain activation mapping is demonstrated by analysis of 13 images acquired under null or resting conditions. Compared to autoregressive pre-whitening methods for computational inference, a key advantage of wavelet resampling seems to be its robustness in activation mapping of experimental fMRI data acquired at 3 Tesla field strength. We conclude that wavelet resampling may be a generally useful method for inference on naturally complex time series...|$|R
