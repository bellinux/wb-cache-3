13|80|Public
40|$|Abstract. New {{method for}} {{evaluation}} of heat transfer coefficient is proposed. In general, many researchers have been studied about inverse problem in order to calculate the heat transfer coefficient on three-dimensional heat conduction problem. But {{they can get the}} time-dependent heat transfer coefficient only through inverse problem. In order to acquire temperature-dependent heat transfer coefficient, it requires much time for numerous <b>repetitive</b> <b>calculation</b> and inconvenient manual modification. In order to solve these problems, we are using the SQP(Sequential Quadratic Programming) as an optimization algorithm. When the temperature history is given by experiment, the optimization algorithm can evaluate the temperature-dependent heat transfer coefficient with automatic <b>repetitive</b> <b>calculation</b> until difference between calculated temperature history and experimental ones is minimized. Finally, temperature-dependent heat transfer coefficient evaluated by developed program can used on the real heat treatment process of casting product...|$|E
40|$|The {{use of a}} fully {{two-dimensional}} waveguide {{analysis of}} the optical modes in a self-pulsating laser is described. We {{show that there is}} a need for such an analysis as the effective index model often used for modeling semiconductor lasers is shown not to provide a correct optical mode when the lateral waveguide is only weakly guiding. An efficient technique for solving the large and sparse matrix calculation in the two-dimensional model, the implicitly restarted Arnoldi method, is used and shown to be useful for the <b>repetitive</b> <b>calculation</b> necessary to describe the laser dynamics. We find that the optical field and optical gain show a variation in the pulse cycle and that this variation that occurs due to the change in carrier density, is responsible for the development of self-pulsation in the diode device...|$|E
40|$|A simple <b>repetitive</b> <b>calculation</b> {{was used}} to {{investigate}} {{what happens to the}} field in terms of the signal paths of disturbances originating from the energy source. The computation allowed the field to be reconstructed as a function of space and time on a statistical basis. The suggested Monte Carlo method is in response to the need for a numerical method to supplement analytical methods of solution which are only valid when the boundaries have simple shapes, rather than for a medium that is bounded. For the analysis, a suitable model was created from which was developed an algorithm for the estimation of acoustic pressure variations in the region under investigation. The validity of the technique was demonstrated by analysis of simple physical models {{with the aid of a}} digital computer. The Monte Carlo method is applicable to a medium which is homogeneous and is enclosed by either rectangular or curved boundaries...|$|E
5000|$|The {{more complex}} the {{contacts}} become, the more <b>repetitive</b> <b>calculations</b> ABAQUS/Standard has to solve, {{and the more}} time and disk space needed; ABAQUS Explicit is the optimal choice in this case ...|$|R
30|$|Also, the {{fundamental}} use of spreadsheets {{can help us}} do accounting and other <b>repetitive</b> <b>calculations</b> with confidence and speed. Use of formulas, references, and macros can turn a ‘grid of numbers’ into actionable information {{in the hands of}} the right person.|$|R
40|$|An {{algorithm}} is presented {{which can be}} used to develop compliance matrices for cracked bodies. The method relies on the numerical solution of singular integral equations with Cauchy-type kernels and provides an efficient and accurate procedure for relating applied loadings to crack opening displacements. The algorithm should be of interest to those performing <b>repetitive</b> <b>calculations</b> in the analysis of experimental results obtained from fracture specimens...|$|R
40|$|Effective risk {{management}} requires adequate risk measurement. A basic problem herein is the quantification of market risks: {{what is the}} overall effect on a portfolio if market rates change? The rst chapter gives {{a brief review of}} the standard risk measure "Value-At-Risk" (VAR) and introduces the concept of "Maximum Loss" (ML) as a method for identifying the worst case in a given scenario space, called "Trust Region". Next, a technique for calculating efficiently ML for quadratic functions is described; the algorithm is based on the Levenberg-Marquardt theorem, which reduces the high-dimensional optimization problem to a one-dimensional root finding. Following this, the idea of the "Maximum Loss Path" is presented: <b>repetitive</b> <b>calculation</b> of ML for a growing trust region leads to a sequence of worst cases, which form a complete path. Similarly, the paths of "Maximum Profit" (MP) and "Expected Value" (EV) can be determined; the comparison of them permits judgements on the quality of portfolios. These concepts are also applicable to non-quadratic portfolios by using "Dynamic Approximations", which replace arbitrary profit and loss functions with...|$|E
40|$|This {{research}} {{is concerned with}} a control method to pile up blocks by a mobile manipulator. One of problems to pile up bloeks by the mobile manipulator is errors between the desired position/orientation and the actual ones of the mobile manipulator, especially. when the mobile manipulator travels on irregular terrains. The errors disturb correct construction of piling up blocks. The purpose of this resrarch {{is to make the}} mobile manipulator syste which can bulid a fance being composed of the blocks as an example of constructing a huge structure. In this paper, a new recognition method to detect the position/orientation errors of the mobile manipulator on the irregular terrain using a hand-eye camera and 3 -D landmark is proposed. First, the Kinematical formulation of the mobile manipulator and the landmark is proposed. Second. the hqnd-eye system is constructed to recognized to the position and orientation errors. Third, <b>repetitive</b> <b>calculation</b> method to detect the 3 -D position/orientation errors is proposed. Finally. the proposed system is evaluated by simulation experiments...|$|E
40|$|This study {{developed}} the Time-split Mixing Model (TMM) which can represent the pollutant mixing process on a three-dimensional open channel through constructing the conceptual model based on Taylor’s assumption (1954) that the shear flow dispersion {{is the result}} of combination of shear advection and diffusion by turbulence. The developed model splits the 2 -D mixing process into longitudinal mixing and transverse mixing, and it represents the 2 -D advection-dispersion by the <b>repetitive</b> <b>calculation</b> of concentration separation by the vertical non-uniformity of flow velocity and then vertical mixing by turbulent diffusion sequentially. The simulation results indicated that the proposed model explains the effect of concentration overlapping by boundary walls, and the simulated concentration was in good agreement with the analytical solution of the 2 -D advection-dispersion equation in Taylor period (Chatwin, 1970). The proposed model could explain the correlation between hydraulic factors and the dispersion coefficient to provide the physical insight about the dispersion behavior. The longitudinal dispersion coefficient calculated by the TMM varied with the mixing time unlike the constant value suggested by Elder (1959), whereas the transverse dispersion coefficient was similar with the coefficien...|$|E
40|$|Calculators {{can be used}} {{in primary}} schools in anumber of situations. They are most beneficialwhen working with large numbers, dealing with real data that leads to complex <b>calculations,</b> {{performing}} <b>repetitive</b> <b>calculations,</b> developing concepts, estimating and checking, problem solving and looking for patterns and/or relationships. But what if the calculator is broken? This article describes the mathematics that children learnt and a teacher’s awareness of children’s mathematical understanding, when a “broken calculator ” activity was undertaken regularly over two terms...|$|R
40|$|EGADS is a {{comprehensive}} preliminary design tool for estimating the performance of light, single-engine general aviation aircraft. The software runs on the Apple Macintosh series of personal computers and assists amateur designers and aeronautical engineering students in performing the many <b>repetitive</b> <b>calculations</b> required in the aircraft design process. The program makes full use of the mouse and standard Macintosh interface techniques to simplify the input of various design parameters. Extensive graphics, plotting, and text output capabilities are also included...|$|R
40|$|The method {{commonly}} used {{in the analysis of}} drifting patterns associated with ionospheric irregularities is the full-correlation method of Briggs, Phillips, and Shinn (1950, hereafter referred to as BPS). A full-correlation analysis requires lengthy calculations, even on a digital computer, as it involves <b>repetitive</b> <b>calculations</b> and complicated curve-fitting techniques. It is the purpose of this note to show that by extending the BPS analysis, (without making any additional assumptions), the computing time for the correlation method may be reduced considerably. ...|$|R
30|$|SSSC-OPF {{has been}} {{reported}} for improvement of small-signal stability, but the existing work with high calculation precision is time-consuming due to large computation requirement when calculating the eigenvalue [10 – 15]. Eigenvalue sensitivity based Interior Point Methods (IPM) have been proposed to improve the power transfer capability with small-signal stability constraints [10, 11], which is an SSSC-OPF based Numerical Eigenvalue Sensitivity (NES-SSSC-OPF). However, these methods {{have to deal with}} the burden of heavy computation from the <b>repetitive</b> <b>calculation</b> of eigenvalues, and at the same time, high precision cannot be guaranteed due to the neglected high-order terms of the small-signal stability constraints. In [12], this expected-security-cost optimal power flow with small-signal stability constraints is addressed by the closed-form formula with extra computational burden to guarantee the calculation accuracy. Approximate-singular-value-sensitivity-based IPM is used to coordinate oscillation control in electricity market [13]. For the two methods proposed in [12, 13], both are very time-consuming for calculating the Hessian of small-signal stability constraints. An optimization method based on the sequential quadratic programming algorithm with Gradient Sampling [14] can ensure the global and efficient convergence of SSSC-OPF [15]. However, the computation with high precision is again time-consuming due to the complex in formula derivation and sampling process.|$|E
40|$|This thesis {{implemented}} a field {{programmable gate array}} (FPGA) -based face detector using a neural network (NN), as well as a bit-width reduced floating-point unit (FPU). An NN was used to easily separate face data and non-face data in the face detector. The NN performs time consuming <b>repetitive</b> <b>calculation.</b> This time consuming problem was solved by a Field Programmable Gate Array (FPGA) device and a bit-width reduced FPU in this thesis. A floating-point bit-width reduction provided a significant saving of hardware resources, such as area and power. The analytical error model, using the maximum relative representation error (MRRE) and the average relative representation error (ARRE), was developed to obtain the maximum and average output errors for the bit-width reduced FPUs. After the development of the analytical error model, the bit-width reduced FPUs and an NN were designed using MATLAB and VHDL. Finally, the analytical (MATLAB) results, along with the experimental (VHDL) results, were compared. The analytical results and the experimental results showed conformity of shape. It was also found that while maintaining 94. 1 % detection accuracy, a reduction in bit-width from 32 bits to 16 bits reduced the size of memory and arithmetic units by 50 %, and the total power consumption by 14. 7 %...|$|E
40|$|International audienceFault {{detection}} and isolation {{can be handled}} by many different approaches. This paper builds upon a hypothesis test that checks whether the mean of a Gaussian random vector has become non-zero in the faulty state, based on a chi 2 test. For fault isolation, {{it has to be}} decided which components in the parameter set of the Gaussian vector have changed, which is done by variants of the chi 2 hypothesis test using the so-called sensitivity and minmax approaches. While only the sensitivity of the tested parameter component is taken into account in the sensitivity approach, the sensitivities of all parameters are used in the minmax approach, leading to better statistical properties at the expense of an increased computational burden. The computation of the respective test variable in the minmax test is cumbersome and may be ill-conditioned especially for large parameter sets, asking hence for a careful numerical evaluation. Furthermore, the fault isolation procedure requires the <b>repetitive</b> <b>calculation</b> of the test variable for each of the parameter components that are tested for a change, which may be a significant computational burden. In this paper, dealing with the minmax problem, we propose a new efficient computation for the test variables, which is based on a simultaneous QR decomposition for all parameters. Based on this scheme, we propose an efficient test computation for a large parameter set, leading to a decrease in the numerical complexity by one order of magnitude in the total number of parameters. Finally, we show how the minmax test is useful for structural damage localization, where an asymptotically Gaussian residual vector is computed from output-only vibration data of a mechanical or a civil structure...|$|E
40|$|We {{investigate}} an experimentally verified {{model for}} the production of ethanol through continuous fermentation. Previous studies investigated this model using direct integration. Such integration is time consuming as parameter regions of interest can only be determined through laborious and <b>repetitive</b> <b>calculations.</b> Using techniques from nonlinear dynamical systems theory, in particular a combination of steady state analysis and path following methods, practical insights into operating strategies are found. We use the performance of the reaction scheme in one tank as a benchmark for comparing the performances of multiple tanks...|$|R
40|$|Numerical Methods with Microsoft Excel® {{is written}} for {{engineering}} {{students who are}} studying Numerical Methods subject at undergraduate and postgraduate levels. This book offers several spreadsheet skills for students who have little knowledge of programming to implement tedious and <b>repetitive</b> <b>calculations</b> of Numerical Methods. Based on the authors’ teaching experiences in teaching university mathematics courses, this book provides many examples with a comprehensive step by step Excel® commands in solving numerical problems. We hope that this book is beneficial to all users, especially lecturers and students {{in teaching and learning}} Numerical Methods...|$|R
5|$|The Swiss tables use the 80 minute tissue {{compartment}} {{for control}} of <b>repetitive</b> dive <b>calculations,</b> which tends to be less conservative than the US Navy tables for this application.|$|R
40|$|Contact between rough {{surfaces}} occurs in numerous engineering systems {{and in many}} instances influences the macro behavior of the system. In many instances, the interaction between {{rough surfaces}}, affect the macro behavior of the system. Effective treatment of systems containing rough surface contact requires multiscale modeling and analysis approach. It {{is the goal of}} this research to develop simple methods for treating contact of rough surfaces so as to facilitate multiscale analysis of systems containing rough surface contact and friction. This dissertation considers a multi-scale approach that includes interaction at nano-scale, micron-scale and accounting for their cumulative effect as to what we normally perceive to be the influence of contact surfaces and friction. In linking each scale to a higher scale this study employs statistical means to obtain cumulative effect of smaller-scale features. A mixed interactive/optimization technique is used to derive, in approximate closed form, equations for the contact load and real area of contact dependence on approach and parameters of rough surfaces. The equations so derived relate the normal and tangential components of contact load to displacement and surface parameters for three types of contact. The nature of contact interaction that include elastic, elastic-plastic, visco-elastic, and visco-elasto-adhesive behavior are considered and equations relating the normal and tangential contact load to approach and relative sliding are obtained in approximate closed form. The approximate equations provide a tool for efficient calculation of contact force components, especially in surface optimization efforts where <b>repetitive</b> <b>calculation</b> of contact force components may be needed. The approximate equations also facilitate a multi-scale dynamic analysis wherein the effect of contact interaction can be readily included in a mechanical system model. Several dynamical problems involving mechanical systems with friction contact are presented and nonlinear dynamic analyses are employed to link the micron-scale properties of surface to the macro-scale properties of the mechanical system. These lead to, perhaps, the first derivation of contact frequency and damping in rough surface contact...|$|E
40|$|One of {{the fast}} and {{economical}} ways of putting tunnels or stream crossings under roadways is to use box culverts. Box culverts are structurally rigid, easy to construct, and easy to add length when needed. Because of their simple geometric configuration, precast concrete box culverts with various dimensions are commonly used in the U. S. In some cases, non-standard box culverts are also used, for which the design document has to be produced per project design specification. The design process of box culverts is relatively easy and repetitive because of their typical geometric configuration. In practice, engineers are following exactly the same process with different dimensions and loading conditions to design box culverts. Microsoft Excel spreadsheet is therefore often used to speed up this <b>repetitive</b> <b>calculation</b> process. In India, many designers use STAAD. Pro for the structural analysis of box culverts and bring {{the results of this}} analysis to a Microsoft Excel spreadsheet to carry out the remaining calculations. However, all these cases are dealt separately and a significant amount of time is used {{to come up with the}} final design. In addition, it has been challenging to keep all engineering calculations and drawings of a specific box culvert for its lifecycle. One of the solutions that one can come up with for this challenge is to automate the entire design process in one package and keep all design documents in one location. This study presents the Web-based application we developed to 1) automate the box culvert design process and 2) keep all design documents in one location. This Web-based application is developed based on Indian Standard codes (IS) and Indian road congress codes (IRC) using ASP. net. This study presents how this application was developed, how it is working, and how it improved the design process of box culverts in our tests. This study shows that results obtained from this application are very close to the traditional design process and can be successfully used for designing of box culverts in India. Also the study concludes that there is a significant amount of time saving in the design process when this application is used instead of traditional design process...|$|E
40|$|Our {{experimental}} physics group {{specializes in}} the fabrication and characterization of materials and nanoscale systems with novel physical properties. We frequently use both commercial and custom test equipment setups to conduct material properties measurements, which require long-running and repetitive control of multiple instruments. Such laboratory tasks have already benefited greatly from in-house developed computer automation, improving the reliability of data acquisition and freeing up scientists for more creative tasks. However, {{the analysis of the}} characterization measurements often involves <b>repetitive</b> <b>calculation</b> and plotting of numerous data sets. Large volumes of data become increasingly difficult to handle using manual “spreadsheet” analysis techniques; instead, researchers might write custom programs to batch process the raw data into more meaningful parameters and visualizations. The combined automation of data acquisition, processing, organization, and visualization is the hallmark of informatics systems which have proven successful in the fields such as biology and high energy physics. The current informatics paradigm relies on massive collaborative efforts developing highly specific hardware and software infrastructure to collect vast amounts of mostly homogeneous data. However, the bewildering heterogeneity and incompatibility of the software interfaces and data formats for the instrumentation used in material science and nanotechnology research present a significant challenge to the developers of experimental informatics systems in this field. Instead, we propose that small research groups can leverage the power of open source software and high-level scripting languages (e. g., Python, TCL, Perl, or Ruby) to knit together existing tools, each of which excel at various parts of the complete informatics package. “Open source” refers to software (and some hardware) products with licenses that protect the end users' rights to access and modify the original source materials, like human readable program code. Moreover, the open source paradigm encourages public collaborative development and sharing of knowledge, with the goals of producing interoperable and robust applications, typically free of charge. The above mentioned scripting languages are themselves open source software environments that are specifically designed to coordinate, or “script,” other components as well as perform other general purpose programming tasks, while promoting gains in developer productivity. In this talk, we present examples from actual applications developed in our lab. We describe our usage of agile software development methodologies and our selection of Python {{as one of the most}} promising programming languages for laboratory informatics. The emphasis will be on the rapid-development of custom software tools for scientific work-flow management, computer-assisted data processing, reduction, and enhanced visualization. ...|$|E
25|$|Assembly {{language}} uses a mnemonic {{to represent}} each low-level machine instruction or opcode, typically also each architectural register, flag, etc. Many operations require {{one or more}} operands in order to form a complete instruction and most assemblers can take expressions of numbers and named constants as well as registers and labels as operands, freeing the programmer from tedious <b>repetitive</b> <b>calculations.</b> Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.|$|R
30|$|In {{this work}} we provide a {{combination}} of isogeometric analysis with reduced order modelling techniques, based on proper orthogonal decomposition, to guarantee computational reduction for the numerical model, and with free-form deformation, for versatile geometrical parametrization. We apply it to computational fluid dynamics problems considering a Stokes flow model. The proposed reduced order model combines efficient shape deformation and accurate and stable velocity and pressure approximation for incompressible viscous flows, computed with a reduced order method. Efficient offline–online computational decomposition is guaranteed in view of <b>repetitive</b> <b>calculations</b> for parametric design and optimization problems. Numerical test cases show the efficiency and accuracy of the proposed reduced order model.|$|R
50|$|Assembly {{language}} uses a mnemonic {{to represent}} each low-level machine instruction or opcode, typically also each architectural register, flag, etc. Many operations require {{one or more}} operands in order to form a complete instruction and most assemblers can take expressions of numbers and named constants as well as registers and labels as operands, freeing the programmer from tedious <b>repetitive</b> <b>calculations.</b> Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.|$|R
40|$|Face {{recognition}} {{has gained}} extensive attention recently, with many applications {{in a broad}} range of domains such as access control in security systems and picture tagging in social network web sites. This project builds a 3 D face database and recognizes the unknown 3 D face images in comparison with the 3 D face database. In 3 D face images used in this thesis are acquired by a 3 D data acquisition system based on Digital Fringe Projection Profilometry (DFPP). DFPP is an efficient 3 D data acquisition system to capture 3 D data, with its simple system structure, high resolution and low cost. The 3 D database consists of thirty group images In each group, there are three images corresponding with three views with (i. e. left-side view, right-side view, and frontal view) at the same scale of the same subject. The scale is different from group to group. To achieve 3 D face recognition, there are two parts devised: image alignment and comparison. In order to implement efficient and accurate image alignment, two steps which are coarse alignment and fine alignment are implemented. In the coarse alignment step, two 3 D images are roughly aligned into a same coordinates system and roughly aligned. After the coarse alignment step, the two face images will be aligned closer and an initial estimated value will be given for the fine alignment. A modified partial Iterative Closest Point (ICP) method is proposed in the fine alignment step. The partial ICP method is an efficient alignment method for 3 D data reconstruction and 3 D face recognition. It iteratively aligns the two point sets based on <b>repetitive</b> <b>calculation</b> of the closest points as the corresponding points in each iteration. However, if two 3 D face images with different scales are from the same person, the partial ICP method does not work. In this thesis, the scaling effect problem of 3 D face recognition has been solved. A 3 × 3 diagonal matrix as the scale matrix in each iteration of the partial ICP has been well designed. The probing face image which is multiplied by the scale matrix will keep the similar scale with the reference face image. Therefore even if the scales of the probing image and the reference image are different, the corresponding points can be accurately determined. The mean square distance between the two face images are compared to recognize that whether the two face images are from the same person or not. Based on the experiment results, the 3 D face recognition can be achieved via the method proposed in this thesis. The mean square distance between two face images from the same person can reach to less than 0. 05 while the two face images from the different persons can only keep 0. 10 to 0. 30...|$|E
40|$|During {{the process}} of {{optimisation}} and design search, the modelling nad analysis of engineering problems are explited to yield improved designs. The engineer explores various design parameters that he wishes to optimise and {{a measure of the}} quality of a particular design (the objective function) is computed using an appropriate model. A number of algorithms may be used to yield more information about the behaviour of a model, and to minimise/maximise the objective function, and hence improve the quality of the design. This process may include lengthy and <b>repetitive</b> <b>calculations</b> to obtain the value of the objective function with respect to the design variables...|$|R
40|$|In {{this work}} we provide a {{combination}} of isogeometric analysis with reduced order modelling techniques, based on proper orthogonal decomposition, to guarantee computational reduction for the numerical model, and with free-form deformation, for versatile geometrical parametrization. We apply it to computational fluid dynamics problems considering a Stokes flow model. The proposed reduced order model combines efficient shape deformation and accurate and stable velocity and pressure approximation for incompressible viscous flows, computed with a reduced order method. Efficient offline–online computational decomposition is guaranteed in view of <b>repetitive</b> <b>calculations</b> for parametric design and optimization problems. Numerical test cases show the efficiency and accuracy of the proposed reduced order model. © 2016 The Author(s) ...|$|R
40|$|The {{teaching}} and learning of statistics has evolved tremendously over the years owing to the reformation in statistics education and the advancement of technology that revolutionized the pedagogy in statistics classrooms. With technological tools students can focus in learning and understanding the important statistical concepts instead of concentrating on lengthy and <b>repetitive</b> <b>calculations.</b> Hand-held technologies such as the graphics calculators have {{paved the way for}} constructive and exciting learning experience. However, in a developing country like Malaysia the use of graphics calculators in statistics classrooms is not without challenges. This paper explores the advantages and limitations of the use of graphics calculators in the teaching of statistics in Malaysia...|$|R
40|$|AbstractWe analyse a {{model for}} the {{activated}} sludge process occurring in a biological reactor without recycle. The biochemical processes occurring within the reactor are represented by the activated sludge model number 1 (ASM 1). In the past the ASM 1 model has been investigated via direct integration of the governing equations. This approach is time-consuming as parameter regions of interest (in terms of the effluent quality leaving the plant) can only be determined through laborious and <b>repetitive</b> <b>calculations.</b> In this work we use continuation methods to determine the steady-state behaviour of the system. In particular, we determine bifurcation values of the residence time, corresponding to branch points, that are crucial in determining the performance of the plant...|$|R
40|$|Even though {{approximating}} the definite differentiation by Richardson’s extrapolation {{method is}} straight forward, but its <b>repetitive</b> <b>calculations</b> are quite boring. Hence, {{there is a}} need to develop a suitable tool in teaching and learning for this method. Here, we developed a Richardson’s extrapolation Excel spreadsheet calculator to approximate the numerical differentiation that can be used by students and educators who need its full solution. With an appropriate function entered by users using 3 -point central formula and a starting step size h, the full solution of Richardson’s extrapolation table up to level 4 can be obtained quickly and easily. In this spreadsheet calculator, users can extend the level to any desired level by themselves...|$|R
40|$|We analyse a {{model for}} the {{activated}} sludge process occurring in a biological reactor without recycle. The biochemical processes occurring within the reactor are represented by the activated sludge model number 1 (ASM 1). In the past the ASM 1 model has been investigated via direct integration of the governing equations. This approach is time consuming as parameter regions of interest (in terms of the effluent quality leaving the plant) can only be determined through laborious and <b>repetitive</b> <b>calculations.</b> In this work we use continuation methods to determine the steady-state behaviour of the system. In particular, we determine bifurcation values of the residence time, corresponding to branch points, that are crucial in determining the performance of the plant...|$|R
40|$|Treated in {{this work}} is the {{optimality}} of flexureless orthogonal archgrids. Various features of fully stressed arches and archgrids are highlighted and an efficient and simple iterative method for obtaining the optimal solution is proposed. Considering the load proportions carried by arches as independent variables, the optimality of the archgrid is ensured by imposing the conditions of equal elevation at nodal intersections and individual optimality of component arches. Computational effort is reduced by using a simple updating procedure during <b>repetitive</b> <b>calculations.</b> The convergence of the solution is aptly tested by the convergence of elevations and compressive forces in arches. It is shown that the optimal solutions for some skew archgrid systems {{can be obtained by}} solving equivalent orthogonal archgrids...|$|R
40|$|A new {{solution}} {{scheme to}} solve the bending problems of beams by the boundary element method has been developed. The main points {{of this study are}} : (1) to improve the composition of the simultaneous equations by introducing a new formulation process, (2) to establish an algorithm without any variable at intermediate points and (3) to establish a generalized solution scheme for an inhomogeneous beam. In this report we will describe (1) and (2). The matrix size as well as the computing time is greatly reduced owing to the new algorithms. therefore, high efficiency in <b>repetitive</b> <b>calculations</b> is obtained. Using the present solution scheme, a much more efficient system for optimal designs using such as the genetic algorithm will be realized to build...|$|R
30|$|It {{has been}} proved by lots of {{numerical}} studies (see, e.g., [10 – 23]) that the proper orthogonal decomposition (POD) method is a very useful tool {{to reduce the number}} of unknowns for numerical models and ease the truncated error amassing in numerical calculations. But most existing reduced-order models, as mentioned, were established via the POD basis formed from the classical numerical solutions at all time nodes, before repetitively computing the reduced-order numerical solutions at the same time nodes, which were some valueless <b>repetitive</b> <b>calculations.</b> Since 2014, some reduced-order FE extrapolating methods based on the POD method for partial differential equations have been established successively by Luo’s team (see, e.g., [24 – 26]) in order to avert the valueless repeated computations.|$|R
30|$|The {{stochastic}} {{analysis is}} carried out to verify the reliability of results from the deterministic study and where necessary redefine it for better accuracy. Additional variables are introduced that allow for risk assessment and optimisation of the results. Variability in the domain characteristics such as the rock material properties and behaviour presents uncertainties in resulting outputs. An iterative procedure ensures that various scenarios or combination of scenarios are accounted for. <b>Repetitive</b> <b>calculations</b> that entail the variation of different combinations of input parameters produce corresponding outcomes that depict {{the state of the}} wellbore for a given set of initial and boundary conditions. This heuristic approach is common in optimisation techniques, but is essential in determining required probability and cumulative density distributions.|$|R
40|$|Models {{applied to}} {{simulate}} the impact of climate change on vegetation dynamics generally face the trade-off between computational expenses (computation time and memory) and modelled detail. Models used for simulations of large areas (e. g. continental) often abstract processes entailing spatial linkages, e. g. species migration, and have too coarse resolutions to depict microsite heterogeneity. Regional to local models, on the other hand, are more detailed, but their computational expenses prevent applications on larger scales. For manageable and accurate simulations of vegetation dynamics on large scales, small-scale dynamics need to be integrated with large-scale applications in a balanced way. Several methods have been proposed and applied to expedite the integration of scales. However, each method has different advantages and drawbacks and the applicability of a method also strongly depends on the initial model and on the research question. Here we present a conceptual framework for a further step integrating the scales in simulations with spatially explicit, time- and space-discrete models simulating vegetation dynamics under climate change. In such models, grid cells with similar environmental drivers and species compositions often entail <b>repetitive</b> <b>calculations.</b> Our method strives to reduce this redundancy and aims to disentangle <b>repetitive</b> <b>calculations</b> from processes specific to single cells. The proposed method {{is based on a}} dynamic two-layer classification (D 2 C) concept, in which the majority of processes is simulated in representative cells constituting the coarse layer, and only processes which might lead to changes specific to a single cell are simulated on the original grid, i. e. the fine layer. This new concept is a further step to enable the simulation of more detailed small-scale dynamics on a larger scale. We provide an example applying the D 2 C concept with the forest-landscape model TreeMig and shortly discuss its advantages and limitations...|$|R
