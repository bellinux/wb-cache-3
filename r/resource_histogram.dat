3|9|Public
40|$|Abstract — The {{purpose of}} this work is to apply the re {{modified}} minimum moment method of resource leveling to construction projects in India. The method {{is based upon the}} critical path method & it was developed with the assumption of no activity splitting and fixed project duration with unlimited availability of resources. The criteria of selecting the activity that has to be shifted from its original position to a better position is judged by the change in the statically moment of the <b>resource</b> <b>histogram</b> before and after such movement as well as by Resource Improvement Coefficient. To achieve the objectives of the work, the data of construction of three residential apartment projects in Nashik city is taken. Initially the activities are arranged according to their Earliest Start Time (EST) then as per the Re modified minimum moment method & as per their Latest Finishing Time (LFT). Bar chart & histogram is prepared for each solution. Maximum daily requirement of mason, Effectiv...|$|E
40|$|This paper {{presents}} {{a set of}} propagation rules to solve cumulative constraints. As in our previous paper on jobshop scheduling [8], {{our goal is to}} propose to the CLP community techniques that allow a constraint satisfaction program to obtain performances which are competitive with ad-hoc approaches. The rules that we propose are a mix of an extension of the concept of task intervals to the cumulative case and the use of a traditional <b>resource</b> <b>histogram.</b> We also explain how to use a branching scheme inherited from operations research to address complex multi-resources problems (similar to the use of edge-finding for jobshop scheduling). We show that the complex propagation patterns associated with our rules make a strong arguments for using logic programming. We also identify a phenomenon of phase transition in our examples that illustrates why cumulative scheduling is hard. 1. Introduction Many real-world scheduling problems are really cumulative scheduling problems, where each resou [...] ...|$|E
40|$|AbstractResource {{allocation}} optimization {{is one of}} {{the most}} challenging problems in project management. It is a combinatorial problem with multiple objectives (project duration confinement, resource-constrained allocation, resource leveling) and its size and complexity grows exponentially as the number of activities, resource types, and execution modes increase. Existing studies bound the analysis on specific aspects of the problem or include simplifying assumptions which may not adequately represent actual (construction) projects. They typically incorporate only a few alternative activity execution modes while it is known (and also handled by popular project management software) that activities can be frequently executed in several arrangements by simply adjusting their gang size and duration. In this work, an optimization method for multi-objective resource-constrained scheduling is developed which evaluates several resource-duration alternatives within each activity. The current optimization aims at minimizing the total cost that results from (a) resource overallocation, (b) project deadline exceedance, and (c) day-by- day resource fluctuations. All three sub-objectives are represented by cost functions after assigning unit cost values to each deviation from the corresponding goal. As a result, the whole optimization parameter is a pure cost variable avoiding thus the need for subjectively setting importance weights among dissimilar parameters (as, for instance, the standard deviation of the <b>resource</b> <b>histogram</b> as a means to determine the resource levelling success). Due to the large number of activity execution alternatives, a genetic algorithm has been employed for the optimization. The algorithm has been tested with several test cases and the results were compared to those developed by the Microsoft Project. The evaluation indicates that the proposed algorithm can provide adequate and balanced solutions with regard to the three objectives and that these solutions are better than those provided by commercial project scheduling software...|$|E
50|$|A legend {{explaining}} {{the meaning of}} the various colors, symbols and line types used in the chart may be included in the time-distance diagram. Other information shown may be cost and <b>resource</b> <b>histograms</b> along the time axis.|$|R
5000|$|FastTrack Schedule has {{a global}} {{customer}} base {{and has been}} localized into 6 languages (English, Japanese, French, German, Spanish, and Italian). Some of their high profile users include: NASA, Nike, Amazon.com, Honda, Pixar, MIT Lincoln Laboratory, and The Olympic Games. The application enables users to organize tasks into project plans, assign resources to tasks, use effort driven scheduling, and view project details in Gantt charts, monthly calendars, and <b>resource</b> <b>histograms,</b> and more.|$|R
5000|$|... #Caption: <b>Resource</b> Leveling optimizes <b>histogram</b> of <b>resources</b> on a project.|$|R
40|$|Successful project {{management}} requires the effective {{control of the}} design teams and the exchange of information between them for successful design management. This paper describes a methodology involving discrete event simulation that can help the planning and control of building design. Discipline-based information flow models of the building design process are used to define {{the activities of the}} simulation model, concentrating on the concept and schematic design stages. Factors such as task durations and resources are then allocated along with any specific constraints that are to be evaluated. The model predicts the outcomes of the specific scenario of information related events, including design schedules and <b>resource</b> <b>histograms.</b> The paper describes the development and validation of the simulation model and discusses its potential application during the planning and design phases of building projects. It is concluded that this approach could form the basis of a useful tool for design managers responsible for multidisciplinary building design work. ...|$|R
40|$|A {{two-phase}} {{feasibility study}} was initiated in late 1996 {{to identify a}} way to expedite the removal of SNM from the CPP- 651 vault. The first phase of this study provided preliminary information that appeared promising, but needed additional detailed planning and evaluate to validate the concepts and conclusions. The focus of Phase 2 was to provide the validation via resource-loaded schedules and more detailed cost estimates. Section 1 describes the purpose and objectives of the Phase 2 tasks and the programmatic drivers that influence related CPP- 651 high-enriched uranium (HEU) management issues. Section 2 identifies the evaluation criteria and methodology and the transfer issues and barriers preventing shipment. Section 3 provides site-specific background information for the CPP- 651 facility and the Idaho National Engineering and Environmental Laboratory (INEEL) and describes {{the development of the}} basic material removal schedule, the proposed base case plan for removal of SNM, and the proposed HEU material management/shipping issues and strategies. Section 4 identifies the proposed options for accelerated removal of SNM and how they were evaluated via detailed scheduling, <b>resource</b> <b>histograms,</b> and cost analysis. Section 5 summarizes principal tasks for implementing this plan and other related HEU CPP- 651 management issues that require continued planning efforts to assure successful implementation of this proposed early removal strategy...|$|R
40|$|In {{this report}} we give a full {{description}} of sim, a C++ library for discrete event simulation. The sim library supports both an event and process-oriented approach to developing simulations. Events {{as well as}} entities (which may be considered as events with states signifying episodes in its life-time) are provided as abstract classes that must be refined by the application programmer to define the actual events and entities participating in the simulation. The sim library is integrated with the hush library, thus offering powerful graphic and animation facilities. However, the sim library may also be used independently, on both Unix and MS-Dos platforms. This report presents {{an overview of the}} classes constituting the sim library (including the classes event, entity, generator, <b>resource,</b> queue, <b>histogram</b> and analysis) as well as two standard examples illustrating the deployment of the classes in writing simulation programs. Also, an example is given of how to create a graphical anima [...] ...|$|R
40|$|Resource {{discovery}} {{is an important}} process for finding suitable nodes that satisfy application requirements in large loosely coupled distributed systems. Besides inter node heterogeneity, many of these systems also show {{a high degree of}} intra node dynamism, so that selecting nodes based only on their recently observed resource capacities can lead to poor deployment decisions resulting in application failures or migration overheads. However, most existing resource discovery mechanisms rely mainly on recent observations to achieve scalability in large systems. In this paper, we propose the notion of a resource bundle—a representative resource usage distribution for a group of nodes with similar resource usage patterns—that employs two complementary techniques to overcome the limitations of existing techniques: <b>resource</b> usage <b>histograms</b> to provide statistical guarantees for resource capacities and clustering-based resource aggregation to achieve scalability. Using trace-driven simulations and data analysis of a month-long Planet Lab trace, we show that resource bundles are able to provide high accuracy for statistical resource discovery, while achieving high scalability. We also show that resource bundles are ideally suited for identifying group-level characteristics (e. g., hot spots, total group capacity...|$|R
40|$|The {{rapid growth}} of camera and storage capabilities, over the past decade, has {{resulted}} in an exponential growth {{in the size of}} video repositories, such as YouTube. In 2015, 400 hours of videos are uploaded to YouTube every minute. At the same time, massive amount of images/videos are generated from monitoring cameras for elderly, sick assistance, satellites for earth science research, and telescopes for space exploration. Human annotation and manual manipulation of such videos are infeasible. Computer vision technology plays an essential role in automating the indexing, sorting, tagging, searching and analyzing huge amount of video data. Object detection and activity recognition in general {{are some of the most}} challenging topics in computer vision today. While the detection/recognition accuracy has increased dramatically over the past few years, it has not kept up with the complexity of detection/recognition tasks nor with the increased resolution of the video/image sources. As a result, the computation speed, and power consumption, of computer vision applications have become a major impediment to their wider use. Thus applications relying on real-time monitoring/feedback are not possible under current speeds. This thesis focuses on the use of Field Programmable Gate Arrays (FPGAs) to accelerate computer vision applications for embedded/real time applications while maintaining similar detection/recognition accuracy as the original processing. FPGAs are electronic devices on which an arbitrary digital circuit can be (re) configured under software control. To leverage the computational parallelism on FPGAs, fixed-point arithmetic is used for all implementations. The benefit of using fixed-point representation over floating point is the reduced bit-width, but the range and sometimes the precision are limited. Comprehensive studies are performed in this study to show that the classification system has some degree of tolerance to the reduced precision data representation. Hence FPGA programs are implemented accordingly in low bit-width fixed-point to achieve high computation throughput, low power consumption, and accurate classification. As a first step, the impact of reduced precision is studied for Viola-Jones face detection algorithm: whereas the reference OpenCV code uses double precision floating-point values, by using only five decimal digit (17 bits) fixed-point representation, the detection can achieve the same rates of false positives and false negatives as the reference OpenCV code. By reducing the necessary precision by a factor of 3 X to 4 X, the size of the circuit on FPGA is reduced by a factor of 12 X; hence increasing the number of feature classifiers that can be fit on a single FPGA. A hybrid CPU-FPGA processing pipeline is proposed to reduce CPU work-load. As a second step, Histogram of Oriented Gradients (HOG), one of the most popular object detection algorithms, is evaluated by using the full-image evaluation methodology to explore the FPGA implementation of HOG using reduced bit-width. This approach lessens the required area resources on the FPGA and increases the clock frequency and hence the throughput per device through increased parallelism. Detection accuracy of the fixed-point HOG is evaluated by applying state-of-the-art computer vision pedestrian detection evaluation metrics. The reduced precision detection performs as well as the original floating-point code from OpenCV. This work then shows the single FPGA implementation achieves a 68. 7 x higher throughput than a high-end CPU, 5. 1 x higher than a high-end GPU, and 7. 8 x higher than the same implementation using floating-point on the same FPGA. A power consumption comparison for different platforms shows our fixed-point FPGA implementation uses 130 x less power than CPU, and 31 x less energy than GPU to process one image. In addition to object detection algorithms, this thesis also investigates the acceleration of action recognition, specifically a human action recognition (HAR) algorithm. In HAR, pedestrian detection is normally used as a pre-processing step to locate human in stream video. In this work, the possibility to perform feature extraction under reduced precision fixed-point arithmetic is evaluated to ease hardware <b>resource</b> requirements. The <b>Histogram</b> of Oriented Gradient in 3 D (HOG 3 D) feature extraction is then compared with state-of-the-art Convolutional Neural Networks (CNNs) methods and result shows that the later is 75 X slower than the former. The experiment shows that by re-training the classifier with reduced data precision, the classification performs as well as the original double-precision floating-point. Based on this result, an FPGA-based HAR feature extraction is implemented for near camera processing using fixed-point data representation and arithmetic. This implementation, using a single Xilinx Virtex 6 FPGA, achieves about 70 x speedup over multicore CPU. Furthermore, a GPU implementation of HAR is introduced with 80 x speedup over CPU (on an Nvidia Tesla K 20) ...|$|R

