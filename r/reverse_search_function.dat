0|1610|Public
5000|$|... lrs: {{implementation}} of the <b>reverse</b> <b>search</b> algorithm for vertex enumeration and convex hull problems ...|$|R
40|$|This {{paper is}} a {{tutorial}} on <b>reverse</b> <b>search,</b> a technique developed by Komei Fukuda {{and the author}} for the generation of large sets of discrete objects [1]. Although <b>reverse</b> <b>search</b> was originally used for generating all vertices of a convex polyhedron, it {{has been applied to}} many other geometric and combinatorial problems. The purpose of this tutorial is to illustrate the basic technique on some simple example...|$|R
5000|$|... : (<b>reverse</b> <b>search)</b> {{recalls the}} last command {{including}} the specified character(s). A second [...] recalls the next anterior command {{that corresponds to}} the search ...|$|R
40|$|We {{report on}} the {{implementation}} of an algorithm for computing the set of all regular triangulations of finitely many points in Euclidean space. This algorithm, which we call down-flip <b>reverse</b> <b>search,</b> can be restricted, e. g., to computing full triangulations only; this case is particularly relevant for tropical geometry. Most importantly, down-flip <b>reverse</b> <b>search</b> allows for massive parallelization, i. e., it scales well even for many cores. Our implementation allows to compute the triangulations of much larger point sets than before. Comment: 22 pages, 4 figure...|$|R
40|$|Given a {{polyhedron}} P by {{a list of}} inequalities {{we develop}} unbiased {{estimates of the number}} of vertices and bases of P. The estimates are based on applying tree estimation methods to the <b>reverse</b> <b>search</b> technique. The time to generate an unbiased estimate is essentially bounded by the time taken to solve a linear program on P with the simplex method. Computational experience is reported. The method can be applied to estimate the output size of other enumeration problems solvable by <b>reverse</b> <b>search.</b> 2000 Elsevier Science B. V. All rights reserved...|$|R
40|$|We {{describe}} mts, {{which is}} a generic framework for parallelizing certain types of tree search programs, that (a) provides a single common wrapper containing all of the parallelization, and (b) minimizes the changes needed to the existing single processor legacy code. The mts code was derived from ideas used to develop mplrs, a parallelization of the <b>reverse</b> <b>search</b> vertex enumeration code lrs. The tree search properties required {{for the use of}} mts are satisfied by any <b>reverse</b> <b>search</b> algorithm as well as other tree search methods such as backtracking and branch and bound. mts is programmed in C, uses the MPI parallel environment, and can be run on a network of computers. As examples we parallelize two simple existing <b>reverse</b> <b>search</b> codes: generating topological orderings and generating spanning trees of a graph. We give computational results comparing the parallel codes with state of the art sequential codes for the same problems. Comment: 16 page...|$|R
40|$|Abstract. Multiswarm {{approaches}} {{are used in}} many literatures to deal with dynamic optimization problems(DOPs). Each swarm tries to find promising areas where usually peaks lie and many good results have been obtained. However, steep peaks are difficult to be found with multiswarm approaches, which hinders {{the performance of the}} algorithm to be improved furtherly. Aiming at the bottleneck, the paper introduces the idea of sequential niche technique to traditional multiswarm approach and thus proposes a novel algorithm called <b>reverse</b> space <b>search</b> multiswarm particle swarm optimization (RSPSO) for DOPs. RSPSO uses the information of the peaks found by coarse search of traditional multiswarm approach to modify the original fitness function. A newly generated subswarm- <b>reverse</b> <b>search</b> subswarm evolves with the modified fitness function, at the same time, other subswarms using traditional mltiswarm approach still evolve. Two kinds of subswarm evolve in cooperation. <b>Reverse</b> <b>search</b> subswarm tends to find much steeper peak and so more promising area where peaks lie is explored. Elaborated experiments on MPB show the introduction of <b>reverse</b> <b>search</b> enhances the ability of finding peaks, the performance of RSPSO significantly outperforms traditional multiswarm approaches and it has better robustness to adapt to dynamic environment with wider-range change severity...|$|R
40|$|In this paper, {{we review}} our data mining {{algorithms}} for discovering frequent substructures {{in a large}} collection of semi-structured data, where both of the patterns and the data are modeled by labeled trees. These algorithms, namely FREQT for mining frequent ordered trees and UNOT for mining frequent unordered trees, efficiently enumerate all frequent tree patterns without duplicates using <b>reverse</b> <b>search,</b> which is a general scheme for designing efficient algorithms for hard enumeration problems, and incrementally compute of the occurrences of a pattern. We also discuss classes of trees to which <b>reverse</b> <b>search</b> is applicable, such as itemsets, sequential episodes, path trees, and graphs...|$|R
50|$|A mobile {{directory}} is {{a collection}} of subscriber details of a mobile phone operators. Generally, the mobile telephony operators do not publish a mobile directory. Some third party websites offer mobile directory facility through <b>reverse</b> <b>search.</b>|$|R
40|$|We {{describe}} a parallel {{implementation of the}} vertex enumeration code lrs that automatically exploits available hardware on multi-core computers and runs {{on a wide range}} of platforms. The implementation makes use of a C++ wrapper that essentially uses the existing lrs code with only minor modifications. This allows the simultaneous development of the existing single processor code with the speedups available from multi-core systems. It makes use of the restart feature of <b>reverse</b> <b>search</b> that allows for independent subtree search and the fact that no communication is required between these searches. As such it can be readily adapted for use in other <b>reverse</b> <b>search</b> enumeration codes...|$|R
5000|$|By {{outsourcing}} the <b>search</b> <b>function</b> to {{a specialist}} search company through software as a service, a more capable <b>search</b> <b>function</b> {{may be available}} to even the smallest organisation. Two methods are popular for this: ...|$|R
5000|$|Smart <b>search</b> <b>function</b> <b>searches</b> {{through all}} email {{accounts}} and suggests word phrases {{based on previous}} email content ...|$|R
40|$|Dedicated to Professor Masao Iri on the {{occasion}} of his 65 th birthday This paper describes computational experience obtained {{in the development of the}} lrs code, which uses the <b>reverse</b> <b>search</b> technique to solve the vertex enumeration/convex hull problem for d-dimensional convex polyhedra. We giv e empirical results showing improvements obtained by the use of lexicographic perturbation, lifting, and integer pivoting. We also give some indication of the cost of using extended precision arithmetic and illustrate the use of the estimation function of lrs. The empirical results are obtained by running various versions of the program on a set of well-known non-trivial polyhedra: cut, configuration, cyclic, Kuhn_Quandt, and metric polytopes. Ke ywords: vertex enumeration, convex hulls, <b>reverse</b> <b>search,</b> computational experience 1...|$|R
40|$|This paper {{presents}} algorithms for computing the Gröbner fan of {{an arbitrary}} polynomial ideal. The computation involves enumeration of all reduced Gröbner bases of the ideal. Our algorithms {{are based on}} a uniform definition of the Gröbner fan that applies to both homogeneous and nonhomogeneous ideals and a proof that this object is a polyhedral complex. We show that the cells of a Gröbner fan can easily be oriented acyclically and with a unique sink, allowing their enumeration by the memory-less <b>reverse</b> <b>search</b> procedure. The significance of this follows from the fact that Gröbner fans are not always normal fans of polyhedra in which case <b>reverse</b> <b>search</b> applies automatically. Computational results using our implementation of these algorithms in the software package Gfan are included. ...|$|R
50|$|<b>Search</b> <b>function</b> for webmail {{and file}} storage.|$|R
5000|$|... simplify {{material}} pre-selection for plastics (<b>search</b> <b>function)</b> ...|$|R
2500|$|... {{a digital}} Graupner Werkverzeichnis with {{integrated}} <b>search</b> <b>function</b> ...|$|R
40|$|The {{purpose of}} this paper has been to {{identify}} which modern <b>search</b> <b>functions</b> user of Smartphones are using, why just these functions are being used, and how they have affected the way users acquire information. Modern <b>search</b> <b>functions</b> in this paper means searches that are not based on text, i. e. ways to search for information by other means than writing something into a search field. To identify this, two studies were conducted, one with a questionnaire being distributed with the help of Google Docs through social networks, and one with four interviews with users of modern <b>search</b> <b>functions</b> for Smartphones. The study showed that even though 76 % of Smartphone users are aware of at least one modern <b>search</b> <b>function,</b> only 29 % uses one on a regular basis. The reason that modern <b>search</b> <b>functions</b> are not more widespread is that they are not sufficiently developed yet, and serious usage is still in the future. Because of this no major impact on search behavior can yet be found...|$|R
40|$|Abstract. This paper {{analyzed}} the existing association rules update algorithm IUA, {{found out that}} when the decision makers gave priority attention to the situation of maximum frequent itemsets, this algorithm cannot {{lower the cost of}} the database traversal to quickly access to the largest number of frequent itemsets. For the lack of the algorithm, an algorithm which is based on <b>reverse</b> <b>search</b> approach to update association rules is presented. The updating algorithm based on <b>reverse</b> <b>search</b> first generated all frequent itemsets of new itemsets. Then, it spliced the new largest frequent itemsets and original largest frequent itemsets for trimming, get the updated maximal frequent itemsets. This algorithm not only reduces the traversal times in the process of association rules updating, but also realized the priority access to the largest operation of frequent itemsets...|$|R
50|$|Advanced <b>search</b> <b>functions</b> by day, location, circle, title, genre, etc.|$|R
50|$|The <b>search</b> <b>function</b> {{provides}} {{the option to}} search by keyword.|$|R
40|$|We {{consider}} {{the problem of}} enumerating triangulations of n points in the plane in general position. We introduce a tree of triangulations and present an algorithm for enumerating triangulations in O(log log n) time per triangulation. It improves the previous bound by almost linear factor. Keywords: Triangulations; Enumeration; <b>Reverse</b> <b>Search...</b>|$|R
30|$|To {{compare the}} {{patterns}} of publication by authors that listed USA addresses with those from other countries relative to the AND and NOT <b>search</b> <b>function</b> categories we performed three different comparisons. First, we examined the list of authors on papers found using the AND <b>search</b> <b>function</b> for the country, excluding the USA, that had {{the greatest number of}} different authors. We did this to compare the proportions of authors that listed their national address as that country on papers found using the AND and NOT <b>search</b> <b>function</b> {{for each of the four}} innate factors individually and for the factors pooled together. Second, we examined the list of authors on papers found using the NOT <b>search</b> <b>function</b> category for the country, excluding the USA, that had the most different authors. We did this to compare the proportions of authors that listed their national address as that country on papers found using the AND and NOT <b>search</b> <b>function</b> for each of the four innate factors individually and for the factors pooled together. Last, we compared the proportions of authors that listed the USA as their national address with those of authors that listed elsewhere as their national addresses found on papers using the AND and NOT <b>search</b> <b>function</b> with those of authors from the countries represented by the most authors found on papers using the AND and NOT <b>search</b> <b>function</b> for each of the four innate factors individually and for the factors pooled together. We performed these comparisons to detect whether {{the patterns of}} publication by authors with USA addresses were similar those of authors that listed their national addresses as elsewhere.|$|R
40|$|Objective – This study {{evaluates the}} {{effectiveness}} of the related <b>search</b> <b>functions</b> in Web of Science and Scopus. Web of Science has one related <b>search</b> <b>function</b> (<b>searching</b> by references) whereas Scopus has three related <b>search</b> <b>functions</b> (<b>searching</b> by references, authors, or keywords). Methods – Thirty queries were searched in both Web of Science and Scopus. For each query, the most relevant document was retrieved and its first thirty related documents were assessed for relevancy to the original query. Results for both databases were compared using the median values of precision. For Scopus the three different methods of relevance were compared using median precision values. Results – The median precision value for the related documents retrieved from Web of Science was 0. 63, while the median for those retrieved from Scopus using the related by references function was 0. 62. A Wilcoxon test showed {{no significant difference in the}} two medians. In the comparison of the three related functions in Scopus, the median precision values were 0. 62, 0. 42, and 0. 43 for the related <b>search</b> <b>functions</b> by references, authors, and keywords respectively. A Friedman’s test showed that the median precision value for relatedness by references was significantly higher than the median vales for the other two related functions. In Scopus, {{the effectiveness of}} the related <b>search</b> <b>function</b> using all keywords was not as effective when compared to the effectiveness using select keywords. The median precision value with select keywords was 0. 17. Conclusions – The related <b>search</b> <b>functions</b> by references for both Web of Science and Scopus were moderately effective in retrieving additional relevant documents on a given topic, and there was no significant difference in their performance. When comparing the three methods available in Scopus, the related <b>search</b> <b>function</b> by references was found to be more effective than the system’s related functions by authors and keywords...|$|R
40|$|This paper {{reports the}} {{outcomes}} of a study that evaluated the effectiveness of <b>search</b> <b>functions</b> compared to other navigational tools available on government websites. The study used an observation exercise triangulated with a post observation interview. Results suggest that while there wasn 2 ̆ 7 t any significant difference in effectiveness between <b>search</b> <b>functions</b> and other navigational tools, the skill with which the <b>search</b> <b>function</b> is implemented and participants 2 ̆ 7 familiarity with the website, are fundamental determinants of users 2 ̆ 7 opinions. Implications of the findings for research and practice are discussed. <br /...|$|R
2500|$|Includes Shakespeare's {{text with}} notes, line numbers, and a <b>search</b> <b>function.</b>|$|R
50|$|Rover, the software's dog mascot, reappeared in XP's File <b>Search</b> <b>function.</b>|$|R
5000|$|... {{there was}} no {{processing}} of personal data within the <b>search</b> <b>function</b> ...|$|R
50|$|<b>Search</b> <b>functions</b> {{are also}} {{evaluated}} {{on the basis}} of their complexity, or maximum theoretical run time. Binary <b>search</b> <b>functions,</b> for example, have a maximum complexity of , or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.|$|R
5000|$|... xdvi can be {{used with}} the editor emacs to display the [...]dvi file of the TeX file {{currently}} being edited. There also exists an ability to perform a <b>reverse</b> <b>search,</b> in which a user clicks on a location in the dvi file and emacs jumps to the associated location in the TeX file.|$|R
5000|$|<b>Search</b> <b>function</b> to {{find other}} users (by name, age, residence, and interests) ...|$|R
5000|$|The {{work can}} be read online with a <b>search</b> <b>function</b> at http://www.martinus.dk/en/ttt/ ...|$|R
40|$|As {{the amount}} of {{educational}} games on the market increases it becomes daunting task for pedagogues {{to find the most}} relevant and effective educational games for their teaching activities. In 2012 a metadata model was suggested to streamline descriptions of educational games for a database. This thesis identifies the need for an advanced <b>search</b> <b>function</b> which takes into consideration the notions of purpose and contextual circumstance of using educational games in order for such a database to be of greater usefulness for users. This thesis presents a design of such a <b>search</b> <b>function,</b> based on the theories of Purushotma (2005), Pannese and Carlesi (2007), Charsky (2010) and Reinders and Wattana (2011). Furthermore this thesis provides an updated metadata model to support such a <b>search</b> <b>function.</b> In the future the <b>search</b> <b>function</b> could be polished from a usability perspective and further developed to incorporate other types of serious games...|$|R
5000|$|Other systems: Dotegy <b>Reverse</b> Image <b>Search,</b> Yandex Images and Karma Decay ...|$|R
50|$|Since search results, {{especially}} advertisements, differ {{depending on}} where you are searching from, data collection methods have to account for geographic location. Keyword monitors do this more easily since they typically know what location their client is targeting. However, to get an exhaustive <b>reverse</b> <b>search,</b> several locations need to be scraped for the same keyword.|$|R
5000|$|Roháček version 1934, {{alternative}} link, alternative link, Roháček {{version with}} <b>search</b> <b>function,</b> pdf version ...|$|R
