16|18|Public
5000|$|E. Pariser, {{author of}} The Filter Bubble, {{explains}} how {{there are differences}} that search personalization has on both Facebook and Google. Facebook implements personalization {{when it comes to}} the amount of things people share and what pages they [...] "like". An individual's social interactions, whose profile they visit the most, who they message or chat with are all indicators that are used when Facebook uses personalization. Rather than what people share being an indicator of what is filtered out, Google takes into consideration what we [...] "click" [...] to filter out what comes up in our searches. In addition, Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and users share what other people want to see. Even while tagging photographs, Facebook uses personalization and face recognition that will automatically assign a name to face. Facebook’s like button utilizes its users to do their own personalization for the website. What posts the user comments on or likes tells Facebook what type of posts they will be interested in for the future. In addition to this, it helps them predict what type of posts they will “comment on, share, or spam in the future.” The predictions are combined together to produce one <b>relevancy</b> <b>score</b> which helps Facebook decide what to show you and what to filter out. In 2016, Facebook introduced reactions (Love, Thankful, Haha, Wow, Sad, and Angry) in addition to liking a post. “Facebook has learned that any Reaction left on a post is a strong indicator that the user was more interested in that post than any other ‘liked’ posts.” Facebook is starting to weigh reactions the same way as likes. So even if you leave the “angry” reaction on a post, Facebook will show posts on the user’s feed because the user showed an interest in it.|$|E
3000|$|... is a {{constraint}} at {{the function}} level, this functionality will be disabled for the user. Otherwise access is granted. Each relevant application has two scores, <b>relevancy</b> <b>score</b> (match [...]...|$|E
40|$|There {{has been}} mixed success in {{applying}} semantic component analysis (LSA, PLSA, discrete PCA, etc.) to information retrieval. Here we combine topic-specific link analysis with discrete PCA (a semantic component method) {{to develop a}} topic <b>relevancy</b> <b>score</b> for information retrieval that is used in post-filtering documents retrieved via regular Tf. Idf methods. When combined with a novel and intuitive “topic by example ” interface, this allows a user-friendly manner to include topic relevance into search. To evaluate the resultant topic and link based scoring, a demonstration has been built using the Wikipedia, the public domain encyclopedia on the web. 1...|$|E
40|$|In TREC 5, {{there were}} twenty six IR schemes {{participating}} in the routing task. Twenty three of them used all the collection documents available, {{the rest of them}} only used a subset of the collection (Voorhees & Harman, 1997). We use the output lists of these twenty three schemes as testing data. There were fifty topics in TREC 5 routing task, therefore we have in all 23 × 50 = 1, 150 output lists. Each output list contains 1, 000 documents in ranked order of assigned <b>relevancy</b> <b>scores.</b> From these 1, 150 lists, there are (23 × 22 × 50...|$|R
40|$|Email 2 ̆ 7 s {{popularity}} {{has led to}} the increase in unsolicited mails. Currently spam filters use the structure and syntax of email body along with training methods to classify email as spam or ham. These include techniques such as word statistics and Bayesian filters. In this paper we propose to extend spam filters to use the semantics of an email as an additional parameter for classification. We suggest a system that uses ontologies to discover relationships between tokens in an email. Using semantics presents challenges such as: building the ontology, relationship discovery, <b>relevancy</b> <b>scoring</b> and so on. We discuss these challenges in detail and propose possible solutions to them...|$|R
40|$|Abstract-Search engines, such as Google, assign {{scores to}} news {{articles}} {{based on their}} relevancy to a query. However, not all relevant articles for the query may be interesting to a user. For example, if the article is old or yields little new information, the article would be uninteresting. <b>Relevancy</b> <b>scores</b> do {{not take into account}} what makes an article interesting, which would vary from user to user. Although methods such as collaborative filtering {{have been shown to be}} effective in recommendation systems, in a limited user environment there are not enough users that would make collaborative filtering effective. We present a general framework for defining and measuring the “interestingness ” of articles, incorporating user-feedback. We show 21 % improvement over traditional IR methods. I...|$|R
40|$|In this paper, {{we present}} {{the design of}} a Knowledge-based {{recommender}} system for Technology Enhanced Learning based on Semantic Web Technologies. It uses a knowledge model for representing {{the current state of the}} learner, pedagogical strategies, and learning objects. To create a learner model, the learners’ activity and progress is tracked and higher-level learner features (i. e., Didactical Factors) are extracted. For a given learner state and set of pedagogical rules, the Recommendation Engine infers learning objects that lie on the learners personalized learning path. Furthermore, utility functions are used to compute a <b>relevancy</b> <b>score</b> for the best-fit learning objects. We describe the semantic-based recommendation approach on a conceptual level, discuss the strengths and weaknesses on the recommender framework and discuss future research...|$|E
40|$|Deformable surface {{tracking}} from monocular {{images is}} well-known to be under-constrained. Occlusions often make the task even more challenging, and {{can result in}} failure if the surface is not sufficiently textured. In this work, we explicitly {{address the problem of}} 3 D reconstruction of poorly textured, occluded surfaces, proposing a framework based on a template-matching approach that scales dense robust features by a <b>relevancy</b> <b>score.</b> Our approach is extensively compared to current methods employing both local feature matching and dense template alignment. We test on standard datasets as well as on a new dataset (that will be made publicly available) of a sparsely textured, occluded surface. Our framework achieves state-of-the-art results for both well and poorly textured, occluded surfaces. Comment: In Proceedings of International Conference on Computer Vision, 201...|$|E
40|$|The BlogVox system {{retrieves}} opinionated blog posts {{specified by}} ad hoc queries. BlogVox {{was developed for}} the 2006 TREC blog track by the University of Maryland, Baltimore County and the Johns Hopkins University Applied Physics Laboratory using a novel system to recognize legitimate posts and discriminate against spam blogs. It also processes posts to eliminate extraneous non-content, including blog-rolls, link-rolls, advertisements and sidebars. After retrieving posts relevant to a topic query, the system processes them to produce a set of independent features estimating {{the likelihood that a}} post expresses an opinion about the topic. These are combined using an SVM-based system and integrated with the <b>relevancy</b> <b>score</b> to rank the results. We evaluate BlogVox’s performance against human assessors. We also evaluate the individual splog filtering and non-content removal components of BlogVox...|$|E
40|$|In this work, we {{identify}} {{the security and}} privacy problems associated with a certain Big Data application, namely secure keyword-based search over encrypted cloud data and emphasize the actual challenges and technical difficulties in the Big Data setting. More specifically, we provide definitions from which privacy requirements can be derived. In addition, we adapt an existing work on privacy-preserving keyword-based search method to the Big Data setting, in which, not only data is huge but also changing and accumulating very fast. Our proposal is scalable {{in the sense that}} it can leverage distributed file systems and parallel programming techniques such as the Hadoop Distributed File System (HDFS) and the MapReduce programming model, to work with very large data sets. We also propose a lazy idf-updating method that can efficiently handle the <b>relevancy</b> <b>scores</b> of the documents in a dynamically changing, large data set. We empirically show the efficiency and accuracy of the method through extensive set of experiments on real data...|$|R
40|$|Successfully {{managing}} analytics-based semantic {{relationships and}} their provenance enables determinations of document importance and priority, furthering capabilities for machine-based <b>relevancy</b> <b>scoring</b> operations. Semantic technologies are {{well suited for}} modeling explicit and fully qualified relationships but struggle with modeling relationships that are qualified in nature, or resultant from applied analytics. Our work seeks to implement the autonomous Directed Qualification of analytic-based relationships by pairing the Prov-O Ontology (W 3 C Recommendation) with a relevancy ontology supporting analytics terminology. This work results in the capability for any semantically referenced document, concept, or named graph {{to be associated with}} the results of applied analytics as Direct Qualification (DQ) modeled relational nodes. This new capability will enable role, identity, or any other content-based measures of relevancy and analytics-based metrics for semantically described documents. Comment: Proceedings of the 19 th International Command and Control Research and Technology Symposium. arXiv admin note: substantial text overlap with arXiv: 1502. 0434...|$|R
40|$|Over {{the last}} two centuries, reading styles have shifted away from the reading of {{documents}} {{from beginning to end}} and toward the skimming of documents in search of relevant information. This trend continues today where readers, often confronted with an insurmountable amount of text, seek more efficient methods of extracting relevant information from documents. In this paper, a new document reading environment is introduced called the Reader’s HelperTM, which supports the reading of electronic and paper documents. The Reader’s Helper analyzes documents and produces a relevance score for each of the reader’s topics of interest, thereby helping the reader decide whether the document is actually worth skimming or reading. Moreover, during the analysis process, topic of interest phrases are automatically annotated to help the reader quickly locate relevant information. A new information visualization tool, called the ThumbarTM, is used in conjunction with <b>relevancy</b> <b>scoring</b> and automatic annotation to portray a continuous, dynamic thumb-nail representation of the document. This further supports rapid navigation of the text. Keywords document annotation, information visualization, content recognition, intelligent agents, digital libraries, probabilistic reasoning, user interface design, reading onlin...|$|R
30|$|Relevancy {{evaluation}} {{relied on}} Google’s Word 2 Vec-implementation [29]. DeepLearning 4  J-library [50] provided a Java API to Google’s pre-trained word vector model, {{which is based}} on Google News data set. The implementation was executed on a separate Relevancy node (Fig.  4), due to large memory consumption of the model (zipped model file ~ 1.6  GB). Word 2 Vec algorithm was utilised for calculation of word cosine distance between words of a tweet, and context words. A metric file (see Additional file 2) was utilized for adjusting a threshold for word cosine distance for indicating, when a tweet word is relevant. The metric file also contained context words, which were compared to tweet words. Another metric (in JRulesEngine format) was used for calculating the final <b>relevancy</b> <b>score</b> based on the quantity of words, which were found to be relevant in a tweet.|$|E
40|$|Information {{retrieval}} is {{a science}} of gathering information from unstructured data, the online information source i. e., www. WWW contains data of heterogeneous types and of high dimension. Retrieving information from such database is a tedious work. Many researches are going on, to find a best optimal solution. A search engine is the tool for retrieving information from www. The internet helps the user to get the required information from www. A search engine respond to the user-need by answering their query, contains: Database, Web crawler, and Ranking algorithm. The optimality of the search engine {{is based on the}} ranking algorithm. The rank list is prepared based on the <b>relevancy</b> <b>score.</b> In this work we propose to use a novel algorithm, Multi-type Feature Coselection for Clustering (MFCC) to the search engine as an alternative for the ranking algorithm. MFCC has proved its efficiency in clustering the heterogeneous web documentation...|$|E
40|$|We map {{intrusion}} {{events to}} known exploits {{in the network}} attack graph, and correlate the events through the corresponding attack graph distances. From this, we construct attack scenarios, and provide scores for the degree of causal correlation between their constituent events, {{as well as an}} overall <b>relevancy</b> <b>score</b> for each scenario. While intrusion event correlation and attack scenario construction have been previously studied, this is the first treatment based on association with network attack graphs. We handle missed detections through the analysis of network vulnerability dependencies, unlike previous approaches that infer hypothetical attacks. In particular, we quantify lack of knowledge through attack graph distance. We show that low-pass signal filtering of event correlation sequences improves results in the face of erroneous detections. We also show how a correlation threshold can be applied for creating strongly correlated attack scenarios. Our model is highly efficient, with attack graphs and their exploit distances being computed offline. Online event processing requires only a database lookup and a small number of arithmetic operations, making the approach feasible for real-time applications. 1...|$|E
40|$|Domain {{hierarchies}} {{are widely}} used as models underlying information retrieval tasks. Formal ontologies and taxonomies enrich such hierarchies further with properties and relationships associated with concepts and categories but require manual effort; therefore they are costly to maintain, and often stale. Folksonomies and vocabularies lack rich category structure and are almost entirely devoid of properties and relationships. Classification and extraction require the coverage of vocabularies and the alterability of folksonomies and can largely benefit from category relationships and other properties. With Doozer, a program for building conceptual models of information domains, we want {{to bridge the gap}} between the vocabularies and Folksonomies on the one side and the rich, expert-designed ontologies and taxonomies on the other. Doozer mines Wikipedia to produce tight domain hierarchies, starting with simple domain descriptions. It also adds <b>relevancy</b> <b>scores</b> for use in automated classification of information. The output model is described as a hierarchy of domain terms that can be used immediately for classifiers and IR systems or as a basis for manual or semi-automatic creation of formal ontologies. 1...|$|R
40|$|Abstract We {{identify}} {{two issues}} with searching literature digital collections within digital libraries: (a) {{there are no}} effective paper-scoring and ranking mechanisms. Without a scoring and ranking system, users are often forced to scan a large and diverse set of publications listed as search results and potentially miss the important ones. (b) Topic diffusion is a common problem: publications returned by a keywordbased search query often fall into multiple topic areas, not {{all of which are}} of interest to users. This paper proposes a new literature digital collection search paradigm that effectively ranks search outputs, while controlling the diversity of keyword-based search query output topics. Our approach is as follows. First, during pre-querying, publications are assigned into pre-specified ontology-based contexts, and query-independent context scores are attached to papers with respect to the assigned contexts. When a query is posed, relevant contexts are selected, search is performed within the selected contexts, context scores of publications are revised into <b>relevancy</b> <b>scores</b> with respect to the query at hand and the context that they are in, and query outputs are ranked within each relevant context. This way, we (1) minimiz...|$|R
40|$|Relevancy of {{potential}} drug-drug interactions (pDDIs) {{is crucial in}} alerting system design. However, the way this relevancy is perceived is not well understood. The main objective {{of this study was}} to gauge and identify differences in perceptions of intensivists and pharmacists about pDDI relevancy in the ICU. Interactions were defined according to the national medication database using a computerized algorithm. Intensivists and pharmacists filled in a questionnaire to score their perceptions on relevancy of encountered pDDIs types. We conducted a focus group session to discuss pDDIs receiving markedly different <b>relevancy</b> <b>scores.</b> The questionnaire addressed 53 pDDI types. Pharmacists rated 29 pDDI types (54. 7 %) in the broad category "relevant" versus 16 (30. 2 %) for intensivists (p-value < 0. 001). The pharmacists and intensivists gave the same scores for 23 pDDI types (12 as relevant, and 11 as not relevant), and scored 30 types differently. The focus group discussion resulted in a total of 36 relevant and 17 not relevant types. Compared to the pharmacists in this panel, the intensivists were less inclined to consider a pDDI type as relevant. It is important to tailor medication databases with information about evidence and severity of pDDIs to the environment in which they are use...|$|R
40|$|The World-Wide Web {{provides}} every internet citizen {{with access}} to an abundance of information, but it becomes increasingly difficult to identify the relevant pieces of information. Research in web mining tries {{to address this problem}} by applying techniques from data mining and machine learning to web data and documents. Web content mining and web structure mining have important roles in identifying the relevant web page. Relevancy of web page denotes how well a retrieved web page or set of web pages meets the information need of the user. Page Rank, Weighted Page Rank and Hypertext Induced Topic Selection (HITS) are existing algorithms which considers only web structure mining. Vector Space Model (VSM), Cover Density Ranking (CDR), Okapi similarity measurement (Okapi) and Three-Level Scoring method (TLS) are some of existing <b>relevancy</b> <b>score</b> methods which consider only web content mining. In this paper, we propose a new algorithm, Weighted Page with Relevant Rank (WPRR) which is blend of both web content mining and web structure mining that demonstrates the relevancy of the page with respect to given query for two different case scenarios. It is shown that WPRR’s performance is better than the existing algorithms...|$|E
30|$|We {{collected}} {{more than}} 270, 000 publicly available recipe web pages from IFTTT in a non-invasive way using Crawler 4 J. 2 Every recipe web page has {{a description of the}} recipe and the IDs of the trigger and the action that were actually used for the recipe. We scrapped these information all together using JSoup 3 and Selenium 4 and then stored them into ElasticSearch 5 as a single document. We randomly sampled 1000 – 9000 recipe descriptions according to uniform distribution, and labeled them with MAs and NEs so that these can be used as training data. Labeling each verb and noun in the recipe description with a NE was challenging, because the recipe information does not tell exactly which verb or noun in the description is associated with which trigger ID or action ID. Instead of manually labeling the tokens with a NE, we exploited the search functionality of ElasticSearch as follows. We match a token in a recipe description against the two sets of documents in ElasticSearch, one indexed by the trigger ID and the other indexed by the action ID. We picked a set that retrieves documents with higher average <b>relevancy</b> <b>score.</b> Then we labeled the given token with the index (trigger ID or action ID) of the selected document set.|$|E
40|$|Background: Annotation {{of a set}} {{of genes}} is often {{accomplished}} through comparison to a library of labelled gene sets such as biological processes or canonical pathways. However, this approach might fail if the employed libraries are not up to date with the latest research, don’t capture relevant biological themes or are curated at a different level of granularity than is required to appropriately analyze the input gene set. At the same time, the vast biomedical literature offers an unstructured repository of the latest research findings that can be tapped to provide thematic sub-groupings for any input gene set. Methods: Our proposed method relies on a gene-specific text corpus and extracts commonalities between documents in an unsupervised manner using a topic model approach. We automatically determine the number of topics summarizing the corpus and calculate a gene <b>relevancy</b> <b>score</b> for each topic allowing us to eliminate non-specific topics. As a result we obtain a set of literature topics in which each topic is associated with a subset of the input genes providing directly interpretable keywords and corresponding documents for literature research. Results: We validate our method based on labelled gene sets from the KEGG metabolic pathway collection and the genetic association database (GAD) and show that the approach is able to detect topics consistent with the labelled annotation. Furthermore, we discuss the results on three different types of experimentally derived gen...|$|E
40|$|An {{unprecedented}} {{amount of}} information encompassing almost every facet of human activities across the world is generated daily {{in the form of}} zeros and ones, and that is often the only form in which such information is recorded. A good fraction of this information needs to be preserved for periods of time ranging from a few years to centuries. Consequently, the problem of preserving digital information over a long-term has attracted the attention of many organizations, including libraries, government agencies, scientific communities, and individual researchers. In this dissertation, we address three issues that are critical to ensure long-term information preservation and access. The first concerns the core requirement of how to guarantee the integrity of preserved contents. Digital information is in general very fragile because of the many ways errors can be introduced, such as errors introduced because of hardware and media degradation, hardware and software malfunction, operational errors, security breaches, and malicious alterations. To address this problem, we develop a new approach based on efficient and rigorous cryptographic techniques, which will guarantee the integrity of preserved contents with extremely high probability even in the presence of malicious attacks. Our prototype implementation of this approach has been deployed and actively used in the past years in several organizations, including the San Diego Super Computer Center, the Chronopolis Consortium, North Carolina State University, and more recently the Government Printing Office. Second, we consider another crucial component in any preservation system - searching and locating information. The ever-growing size of a long-term archive and the temporality of each preserved item introduce a new set of challenges to providing a fast retrieval of content based on a temporal query. The widely-used cataloguing scheme has serious scalability problems. The standard full-text search approach has serious limitations since it does not deal appropriately with the temporal dimension, and, in particular, is incapable of performing <b>relevancy</b> <b>scoring</b> according to the temporal context. To address these problems, we introduce two types of indexing schemes - a location indexing scheme, and a full-text search indexing scheme. Our location indexing scheme provides optimal operations for inserting and locating a specific version of a preserved item given an item ID and a time point, and our full-text search indexing scheme efficiently handles the scalability problem, supporting <b>relevancy</b> <b>scoring</b> within the temporal context at the same time. Finally, we address the problem of organizing inter-related data, so that future accesses and data exploration can be quickly performed. We, in particular, consider web contents, where we combine a link-analysis scheme with a graph partitioning scheme to put together more closely related contents in the same standard web archive container. We conduct experiments that simulate random browsing of preserved contents, and show that our data organization scheme greatly minimizes the number of containers needed to be accessed for a random browsing session. Our schemes have been tested against real-world data of significant scale, and validated through extensive empirical evaluations...|$|R
40|$|Ankara : The Department of Computer Engineering and the Graduate School of Engineering and Science of Bilkent University, 2012. Thesis (Master's) [...] Bilkent University, 2012. Includes bibliographical {{references}} leaves 82 - 86. Search {{engines are}} used to find information on the web. Retrieving relevant documents for ambiguous queries based on query-document similarity does not satisfy the users because such queries {{have more than one}} different meaning. In this study, a new method, cascaded cross entropy-based search result diversification (CCED), is proposed to list the web pages corresponding to different meanings of the query in higher rank positions. It combines modified reciprocal rank and cross entropy measures to balance the trade-off between query-document relevancy and diversity among the retrieved documents. We use the Latent Dirichlet Allocation (LDA) algorithm to compute query-document <b>relevancy</b> <b>scores.</b> The number of different meanings of an ambiguous query is estimated by complete-link clustering. We construct the first Turkish test collection for result diversification, BILDIV- 2012. The performance of CCED is compared with Maximum Marginal Relevance (MMR) and IA-Select algorithms. In this comparison, the Ambient, TREC Diversity Track, and BILDIV- 2012 test collections are used. We also compare performance of these algorithms with those of Bing and Google. The results indicate that CCED is the most successful method in terms of satisfying the users interested in different meanings of the query in higher rank positions of the result list. Köroğlu, BilgeM. S...|$|R
40|$|An index may {{be defined}} as a {{technique}} of totalling or reducing a single composite series datum on a number of distinct but related variables expressed in different units of measurement. In other words the term index is defined as the numerical scale used to compare variables with one another. A study was designed to develop an index to measure the entrepreneurial orientation of rural youth of Krishnagiri district, Tamil Nadu. The study was conducted among 210 rural youths with farming background. <b>Relevancy</b> weightage <b>score</b> method was used to develop the index. The finalized index comprised of fifteen indicators. The entrepreneurial orientation index thus developed was standardized for administration...|$|R
40|$|A {{study of}} ways in which users conduct subject {{searches}} with an online public access catalog (OPAC) was performed at the College Park Campus of the University of Maryland. Both process (search patterns) and product (search results) variables were examined with respect to individual characteristics of 39 volunteers, many of whom were master's level (64 %) library science (62 %) students. The OPAC used is menu-driVen, and provides access through author, title, combined author/title, subject (LC subject headings), keyword, and number; boolean searching was not fully implemented {{at the time of the}} study. Each subject was asked to complete two search tasks, one straightforward and easy to complete, and the other open-ended and more difficult. They were then asked to list the call numbers of relevant items and to fill out a questionnaire designed to ascertain user satisfaction and demographic information. Results were considered for three criterion mkasures: user satisfaction, number of hits, and <b>relevancy</b> <b>score.</b> It was sound that (1) the subjects used the OFAC with relatiVe ease and the degree of success and satisfaction obtained as relatively high; (2) most subjects preferred subject headg to keyword searching, and (3) there was no evidence of strong relationships between search type or search results and individual characteristics. (BBM) * Reproductions supplied by EDRS are the best that can be made...|$|E
40|$|The {{rapid growth}} of the World Wide Web (WWW) poses {{unprecedented}} scaling challenges for general-purpose crawlers. Crawlers are software which can traverse the internet and retrieve web pages by hyperlinks. The focused crawler of a special-purpose search engine aims to selectively seek out pages {{that are relevant to}} a pre-defined set of topics, rather than to exploit all regions of the Web. Focused crawler is developed to collect relevant web pages of interested topics from the Internet. Maintaining currency of search engine indices by exhaustive crawling is rapidly becoming impossible due to the increasing size of the web. Focused crawlers aim to search only the subset of the web related to a specific topic, and offer a potential solution to the problem. In our proposed approach, we calculate the link score based on average <b>relevancy</b> <b>score</b> of parent pages (because we know that the parent page is always related to child page which means that for detailed information any author prefers the child page) and division score (means how many topic keywords belong to division in which particular link belongs). After finding out link score, we compare the link score with some threshold value. If link score is {{greater than or equal to}} threshold value, then it is relevant link. Otherwise, it is discarded. Focused crawler first fetches that link which has greater value compared to all link scores and threshold...|$|E
40|$|The Comparative Toxicogenomics Database (CTD; [URL] is {{a public}} {{resource}} that curates interactions between environmental chemicals and gene products, and their relationships to diseases, {{as a means of}} understanding the effects of environmental chemicals on human health. CTD provides a triad of core information in the form of chemical-gene, chemical-disease, and gene-disease interactions that are manually curated from scientific articles. To increase the efficiency, productivity, and data coverage of manual curation, we have leveraged text mining to help rank and prioritize the triaged literature. Here, we describe our text-mining process that computes and assigns each article a document <b>relevancy</b> <b>score</b> (DRS), wherein a high DRS suggests that an article {{is more likely to be}} relevant for curation at CTD. We evaluated our process by first text mining a corpus of 14, 904 articles triaged for seven heavy metals (cadmium, cobalt, copper, lead, manganese, mercury, and nickel). Based upon initial analysis, a representative subset corpus of 3, 583 articles was then selected from the 14, 094 articles and sent to five CTD biocurators for review. The resulting curation of these 3, 583 articles was analyzed for a variety of parameters, including article relevancy, novel data content, interaction yield rate, mean average precision, and biological and toxicological interpretability. We show that for all measured parameters, the DRS is an effective indicator for scoring and improving the ranking of literature for the curation of chemical-gene-disease information at CTD. Here, we demonstrate how fully incorporating text mining-based DRS scoring into our curation pipeline enhances manual curation by prioritizing more relevant articles, thereby increasing data content, productivity, and efficiency...|$|E
40|$|Objectives. To {{test the}} {{hypothesis}} that interpersonal problem-solving performance in older adults with a recent episode of parasuicide is poorer than that of depressed patients and community controls. Design. A cross-sectional design {{was used to assess}} differences between older parasuicidal patients, depressed patients, and community controls in interpersonal problem solving performance. Method. An existing outcome measure of interpersonal problem-solving, the Means End Problem-Solving (MEPS) procedure, was modified {{in order to make it}} more suitable for older adults. It was then administered to 18 older adults with an episode of parasuicide in the previous 14 days, 18 older adults who were being treated for clinical depression, and 22 older adults attending community groups. Comparisons between the groups in terms of performance on the MEPS were made. Results. When level of depression was controlled for, parasuicidal patients were poorer at generating relevant means of reaching given outcomes to interpersonal problems than community controls. There was no difference between the parasuicidal and depressed groups in terms of <b>relevancy</b> <b>scores.</b> There were no significant differences between groups in terms of other MEPS outcome variables. Conclusions. Parasuicide in older adults is related to a deficit in interpersonal problem solving performance that cannot be completely explained in terms of depression. Further investigation of interpersonal problem-solving in relation to parasuicide and depression in older adults is required, perhaps using process measures of interpersonal problem solving to clarify the nature of the difficulties. The modified MEPS is acceptable to older adults and can be easily administered...|$|R
40|$|Paper also {{published}} in Proceedings of the 17 th Annual Conference of the Australasian Association for Engineering Education: Creativity, Challenge, Change; Partnerships in Engineering EducationIt is widely recognised {{that there is}} a need to develop a range of generic graduate attributes in engineering students. In order to develop these attributes, universities have employed a number of strategies, including staff development and the adoption of non-traditional teaching methods. However, students also need to have a clear understanding of the meaning of the attributes and why they are important in a professional engineering context. Consequently, student engagement with graduate attributes is also an important factor in their successful development. In this paper, an efficient approach for achieving this is introduced and an example application presented. The proposed approach revolves around a classroom exercise as part of which groups of students discuss and rate the relevance of a set of graduate attributes from the perspective of a practising engineer, about whom they have been provided with relevant background information. Next, the ratings (<b>relevancy</b> <b>scores)</b> given to each of the attributes by the student groups are compared with those provided by the actual engineers, followed by discussion about any similarities and differences between the scores. In addition to increasing student engagement with graduate attributes and student understanding of their importance and relevance, this exercise also provides students with an insight into what 'real' engineers do, and what students might expect to be doing once they graduate. Such an exercise was conducted during a single 50 -minute tutorial session in the course Environmental Engineering II as part of the Civil & Structural and Civil & Environmental degree programs at the University of Adelaide. A student survey indicated that the exercise was successful in increasing student awareness of the existence of, the need for and the importance of graduate attributes, as well as helping students to gain a better understanding of their meaning. HR Maier and TSC Rowan[URL]...|$|R
40|$|Organic farming an {{innovative}} area gaining importance worldwide {{and became a}} boon to the areas which are organic by default and far from the reach of green revolution technologies. Uttarakhand state in India, where most of its farming is organic by default, promoting organic farming in a systematic way through creation of special institutions like UOCB. As attitudes assist individuals in processing complex information and to make decisions, an instrument has been developed to measure attitude of organic farmers towards organic livestock farming, for which ‘Likert method of summated ratings’ was followed. A total of 102 statements were developed from {{the subject matter of}} organic animal husbandry standards, worked out by the Ministry of Commerce and Ministry of Agriculture, Government of India (GOI), and published by the Agriculture Processed Food and Exports Development Authority (APEDA). A total of 94 statements resulted after edition of 102 statements as per the criteria suggested by Edwards (1969), and were sent to 101 extension specialists working in various Indian Council of Agriculture Research (ICAR) and State Agriculture and Veterinary Universities throughout India for the critical evaluation of statements on a 3 point continuum. Of the responses received from 50 out of 101 judges, a total of 47 statements were selected basing on relevancy weightage, percentage and mean <b>relevancy</b> <b>scores,</b> and these were subjected to item analysis by administering to 60 farmers from a non-sample area. A total of 21 statements were selected based on the‘t’ values (above 2. 75) resulted from the item analysis and included in the final scale. Thus, the instrument developed to measure attitude of farmers towards organic livestock farming consists of 13 positive and 8 negative attitude statements representing the various areas of organic animal husbandry standards (NSOP, 2000) viz. sustainability, ecology, environment, animal health and welfare, animal production, certification, quality of organic products including philosophical and ideological views of organic farmers...|$|R
40|$|Web {{search results}} {{are far from}} perfect due to the polysemous and synonymous {{characteristics}} of nature languages, information overload as the results of information explosion on the Web, and the flat list, “one size fits all” strategies of search engines to present search results without concentrating on user personal information needs. Re-organizing Web search results, or Web snippets, by means of text categorization and clustering are two dominant approaches to attack the issues above. Text categorization uses a collection of labeled documents to train a classifier which can then predict labels for new unlabeled documents; while text clustering groups unlabeled documents by finding common properties shared among the documents in the same group. The issue related to categorization is human labeled training documents are very expensive to obtain and thus surprisingly scarce at the moment; while how to label the generated groups is still an open research question for text clustering. In addition, a Web snippet, returned from search engines, contains only {{the title of a}} webpage and an optional very short (less than 30 words) description of the page. The less-informative aspect of Web snippets is another challenge for both text categorization and clustering. The primary objective of this research is to improve the relevance of Web search results and thus provide the user with a better search experience. To achieve this objective, the research combines Web snippet categorization, clustering and personalization techniques to recommend relevant results to search users. Using design research methodology, the study develops an IT artifact named RIB – Recommender Intelligent Browser. RIB categorizes Web snippets using a socially constructed Web directory such as the Open Directory Project (ODP) for which the semantic characteristics of the categories in ODP are extracted to generate a series of labeled document sets. At the same time, the Web snippets are clustered to boost the quality of the categorization. Based on search preferences in a user profile which is automatically generated by using information extracted from user personal computer with the approval of the user for information collection, the proposed search method will recommend personalized search results to users. Experimental data demonstrate that the mean average precision improvement of RIB over Yahoo Search Web Services API based on 25 search-terms with 1250 Web snippets is 7. 84 %, from 55. 55 % of Yahoo to 64. 29 % of RIB. A novel boostingUp algorithm is also proposed in this research to improve the performance of text categorization by leveraging the power of text clustering and vice versa. Experimental results illustrate that boostingUp can marginally improve the performance of both Web snippet categorization and clustering in terms of Adjusted Rand Index and F[subscript] 1. BoostingUp is able to produce 0. 97 % improvement of macro-averaged F[subscript] 1 from 24. 51 % to 25. 48 % for Naïve Bayes with combination of K-Means, 2. 04 % improvement of micro-averaged F[subscript] 1 from 32. 17 % to 34. 21 %. On the other hand, the improvement in terms of Adjusted Rand Index of K-Means with combination of Naïve Bayes is 2. 35 % (from 13. 17 % to 15. 52 %), and the improvement of F[subscript] 1 is 2. 37 % (from 21. 45 % to 23. 82 %). The issues of lack of labeled data set {{that can be used for}} Web snippet categorization and used as benchmark document collection to evaluate text categorization/clustering algorithms is addressed by extracting semantic characteristics of ODP categories to generate a series of labeled categoryDocument sets. Statistical information about the generated data sets is provided as well. The generated categoryDocuments are used to evaluate the performance of Naïve Bayes, Adaboost, and kNN text categorization algorithms when a list of feature selection algorithms including Chi-square, Mutual Information, Information Gain, Odds Ratio, are employed to pick up 50, 80, 100, 200, 300, 500, 1000, 2000, 3000, 5000, and 10000 features. Other text categorization algorithms such as SVMlight and Statistical Language Model based algorithm and feature selection algorithms such as GSS Coefficient, NGL Coefficient, and <b>Relevancy</b> <b>Score</b> are also evaluated based on a specially designed small data set. Two other proposed algorithms, R[superscript] 2 Cut thresholding strategy and Z-tfidf, are at the same time evaluated, and demonstrate the ability of slightly improving the performance of text categorization. Text clustering algorithms such as K-Means and Hierarchical Agglomerative Clustering are also evaluated by using the generated categoryDocument sets. All algorithms involved in this research were implemented in Java. In addition, this research is the first to present the detailed information about the hierarchy of the ODP, the world’s most comprehensive human-edited Web directory, by analyzing the data in two publicly accessible files under Free Use License. Although ODP is adopted as core directory services for the World’s most popular search engines such Google, AOL Search, Netscape Search, Lycos, HotBot and hundreds of other; and used for a wide range of research purposes, there is no detailed hierarchical information about ODP published so far. The research further verifies the relationship between precision improvement and relevance judgment convergent degree when the effectiveness of an information retrieval system is evaluated based on the results of human relevance judgment; and reveals that the two variables are to some extent co-related in terms of correlation coefficient. Improving the relevance of Web searching is challenging. This research proposes to combine text categorization, clustering and personalization to provide better search experience to users. Comprehensive experimental evidence and favorable comparisons against search results of Yahoo API demonstrate the designed search objectives have been achieved...|$|E
40|$|Search engines, such as Google, assign {{scores to}} news {{articles}} {{based on their}} relevancy to a query. However, not all relevant articles for the query may be interesting to a user. For example, if the article is old or yields little new information, the article would be uninteresting. <b>Relevancy</b> <b>scores</b> do {{not take into account}} what makes an article interesting, which would vary from user to user. Although methods such as collaborative filtering {{have been shown to be}} effective in recommendation systems, in a limited user environment there are not enough users that would make collaborative filtering effective. I present a general framework for defining and measuring the ''interestingness'' of articles, called iScore, incorporating user-feedback including tracking multiple topics of interest as well as finding interesting entities or phrases in a complex relationship network. I propose and have shown the validity of the following: 1. Filtering based on only topic relevancy is insufficient for identifying interesting articles. 2. No single feature can characterize the interestingness of an article for a user. It is the combination of multiple features that yields higher quality results. For each user, these features have different degrees of usefulness for predicting interestingness. 3. Through user-feedback, a classifier can combine features to predict interestingness for the user. 4. Current evaluation corpora, such as TREC, do not capture all aspects of personalized news filtering systems necessary for system evaluation. 5. Focusing on only specific evolving user interests instead of all topics allows for more efficient resource utilization while yielding high quality recommendation results. 6. Multiple profile vectors yield significantly better results than traditional methods, such as the Rocchio algorithm, for identifying interesting articles. Additionally, the addition of tracking multiple topics as a new feature in iScore, can improve iScore's classification performance. 7. Multiple topic tracking yields better results than the best results from the last TREC adaptive filtering run. As future work, I will address the following hypothesis: Entities and the relationship among these entities using current information extraction technology can be utilized to identify entities of interest and relationships of interest, using a scheme such as PageRank. And I will address one of the following two hypotheses: 1. By addressing the multiple reading roles that a single user may have, classification results can be improved. 2. By tailoring the operating parameters of MTT, better classification results can be achieved...|$|R
40|$|This study {{develops}} {{a model for}} essay <b>scoring</b> and article <b>relevancy.</b> Essay <b>scoring</b> is a costly process {{when we consider the}} time spent by an evaluator. It may lead to inequalities of the effort by various evaluators to apply the same evaluation criteria. Bibliometric research uses the evaluation criteria to find relevancy of articles instead. Researchers mostly face relevancy issues while searching articles. Therefore, they classify the articles manually. However, manual classification is burdensome due to time needed for evaluation. The proposed model performs automatic essay evaluation using multi-text features and ensemble machine learning. The proposed method is implemented in two data sets: a Kaggle short answer data set for essay scoring that includes four ranges of disciplines (Science, Biology, English, and English language Arts), and a bibliometric data set having IoT (Internet of Things) and non-IoT classes. The efficacy of the model is measured against the Tandalla and AutoP approach using Cohen’s kappa. The model achieves kappa values of 0. 80 and 0. 83 for the first and second data sets, respectively. Kappa values show that the proposed model has better performance than those of earlier approaches...|$|R
40|$|Due to its <b>relevancy</b> {{to point}} <b>scoring,</b> the spike is {{considered}} {{as one of the}} most important skills in fistball. Biomechanical analyses of this sport are very rare. In the present study, we performed a three-dimensional kinematic analysis of the fistball spike, which helps to specify performance parameters on a descriptive level. Recorded by four synchronized cameras (120 Hz) and linked to the motion capture software Simi Motion® 5. 0, three female fistball players of the second German league (24 – 26 years, 1. 63 – 1. 69 m) performed several spikes under standardized conditions. Results show that the segment velocities of the arm reached their maximum successively from proximal to distal, following the principle of temporal coordination of single impulses. The wrist shows maximum speed when the fist hits the ball. The elbow joint angle performs a rapid transition from a strong flexion to a (almost) full extension; however, the extension is completed after the moment of ball impact. In contrast, the shoulder joint angle increases almost linearly until the fistball contact and decreases afterward. The findings can be used to optimize the training of the spike...|$|R

