1692|1043|Public
25|$|R2 is a {{statistic}} {{that will give}} some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is {{a statistic}}al measure of how well the <b>regression</b> <b>line</b> approximates the real data points. An R2 of 1 indicates that the <b>regression</b> <b>line</b> perfectly fits the data.|$|E
25|$|A test {{of whether}} the slope of a <b>regression</b> <b>line</b> differs {{significantly}} from 0.|$|E
25|$|Another way {{of looking}} at it is to {{consider}} the <b>regression</b> <b>line</b> to be a weighted average of the lines passing through the combination of any two points in the dataset. Although this way of calculation is more computationally expensive, it provides a better intuition on OLS.|$|E
50|$|The {{slopes of}} the {{different}} <b>regression</b> <b>lines</b> should be equivalent, i.e., <b>regression</b> <b>lines</b> should be parallel among groups.|$|R
40|$|This paper {{shows how}} to {{construct}} confidence bands {{for the difference}} between two simple linear <b>regression</b> <b>lines.</b> These confidence bands provide directly {{the information on the}} magnitude of the difference between the <b>regression</b> <b>lines</b> over an interval of interest and, as a by-product, {{can be used as a}} formal test of the difference between the two <b>regression</b> <b>lines.</b> Various different shapes of confidence bands are illustrated, and particular attention is paid towards confidence bands whose construction only involves critical points from standard distributions so that they are consequently easy to construct...|$|R
5000|$|Function of {{the angle}} between two {{standardized}} <b>regression</b> <b>lines</b> ...|$|R
25|$|In statistics, {{ordinary}} {{least squares}} (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, {{with the goal of}} minimizing the sum of the squares {{of the differences between the}} observed responses (values of the variable being predicted) in the given dataset and those predicted by a linear function of a set of explanatory variables. Visually this is seen as the sum of the squared vertical distances between each data point in the set and the corresponding point on the <b>regression</b> <b>line</b> – the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side.|$|E
25|$|The {{image on}} the right shows scatter plots of Anscombe's quartet, a set of four {{different}} pairs of variables created by Francis Anscombe. The four y variables have the same mean (7.5), variance (4.12), correlation (0.816) and <b>regression</b> <b>line</b> (y=3+0.5x). However, {{as can be seen}} on the plots, the distribution of the variables is very different. The first one (top left) seems to be distributed normally, and corresponds to what one would expect when considering two variables correlated and following the assumption of normality. The second one (top right) is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear. In this case the Pearson correlation coefficient does not indicate that there is an exact functional relationship: only the extent to which that relationship can be approximated by a linear relationship. In the third case (bottom left), the linear relationship is perfect, except for one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.816. Finally, the fourth example (bottom right) shows another example when one outlier is enough to produce a high correlation coefficient, even though {{the relationship between the two}} variables is not linear.|$|E
25|$|In short, total {{least squares}} {{does not have}} the {{property}} of units-invariancei.e. it is not scale invariant. For a meaningful model we require this property to hold. A way forward is to realise that residuals (distances) measured in different units can be combined if multiplication is used instead of addition. Consider fitting a line: for each data point the product of the vertical and horizontal residuals equals twice the area of the triangle formed by the residual lines and the fitted line. We choose the line which minimizes the sum of these areas. Nobel laureate Paul Samuelson proved in 1942 that, in two dimensions, it is the only line expressible solely in terms of the ratios of standard deviations and the correlation coefficient which (1) fits the correct equation when the observations fall on a straight line; (2) exhibits scale invariance, and (3) exhibits invariance under interchange of variables. This line has been rediscovered in different disciplines and is variously known as standardised major axis (Ricker 1975, Warton et al., 2006), the reduced major axis, the geometric mean functional relationship (Draper and Smith, 1998), least products regression, diagonal <b>regression,</b> <b>line</b> of organic correlation, and the least areas line. Tofallis (2002) has extended this approach to deal with multiple variables.|$|E
50|$|Statistical graphs {{can also}} be generated: bar graphs, line graphs, normal {{distribution}} curves, <b>regression</b> <b>lines.</b>|$|R
30|$|Sa {{reflects}} {{the standard deviation}} of the <b>regression</b> <b>line’s</b> intercept, b quantifies the calibration curve’s slope.|$|R
40|$|The {{comparison}} of two <b>regression</b> <b>lines</b> is often meaning-ful or of interest ovet a finite interval I of the indepen-dent variable. When the prior {{distribution of the}} parameters is a natural conjugate, the posterior distribution of the distances between two <b>regression</b> <b>lines</b> at the end points of I is bivariate t. The posterior probability that one regres-sion line lies above the other uniformly over I is numeri-cally evaluated using this distribution...|$|R
2500|$|... (the {{point on}} the <b>regression</b> <b>line),</b> while the actual {{response}} would be ...|$|E
2500|$|Galton {{invented the}} use of the <b>regression</b> <b>line</b> [...] and for the choice of r (for {{reversion}} or regression) to represent the correlation coefficient.|$|E
2500|$|In {{the third}} graph (bottom left), the {{distribution}} is linear, but {{should have a}} different <b>regression</b> <b>line</b> (a robust regression would have been called for). The calculated regression is offset by the one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.816.|$|E
40|$|In {{the present}} work we study {{the problem of}} k <b>regression</b> <b>lines</b> in the general linear model. First we {{describe}} the general linear model with a multivariate normal distribution of errors and we show some of its basic characteristics. Then we introduce a model with k <b>regression</b> <b>lines.</b> Further, we describe a test for testing the hypothesis of two <b>regression</b> <b>lines</b> being parallel and another one for testing all {{or some of the}} k <b>regression</b> <b>lines</b> being parallel or identical. Then we derive the test of the submodel of the general linear model and analyze issues such as the power of this test, the submodel of another submodel, the orthogonality and reparametrization. We show geometric interpretations of the general linear model and of the submodel test as well. In the subsequent part, we focus on nonparametric tests. We present four permutation tests for testing the submodel in the general linear model. Finally we perform numerical simulation to find out whether the tests match the required size and to determine their power...|$|R
40|$|Simultaneous {{confidence}} intervals {{are used in}} Scheffé (1953) to assess any contrasts of several normal means. In this paper, the problem of assessing any contrasts of several simple linear regression models by using simultaneous confidence bands is considered. Using numerical integration, Spurrier (1999) constructed exact simultaneous confidence bands for all the contrasts of several <b>regression</b> <b>lines</b> over the whole range (??,?) of the explanatory variable when the design matrices of the <b>regression</b> <b>lines</b> are all equal. In this paper, a simulation-based method is proposed to construct simultaneous confidence bands for all the contrasts of the <b>regression</b> <b>lines</b> when the explanatory variable is restricted to an interval and the design matrices of the <b>regression</b> <b>lines</b> may be different. The critical value calculated by this method can be {{as close to the}} exact critical value as required if the number of replications in the simulation is chosen sufficiently large. The methodology is illustrated with a real problem in which sizes of the left atrium of infants in three diagnostic groups (severely impaired, mildly impaired and normal) are compared <br/...|$|R
30|$|Additional file  1 : Figure S 1 a and b display, respectively, {{the scatter}} plots and linear <b>regression</b> <b>lines</b> for every {{individual}} WT–WSS relationship. Mean R 2 was 0.08 [*]±[*] 0.07. Additional file  1 : Figure S 1 c and d display the scatter plots and linear <b>regression</b> <b>lines</b> for every individual WT–diameter relationship. Mean R 2 was 0.09 [*]±[*] 0.15. Additional file  1 : Figure S 1 e, f display the scatter plots and linear <b>regression</b> <b>lines</b> for every individual WSS–diameter relationship. Mean R 2 was 0.11 [*]±[*] 0.12. For most subjects, a {{negative and positive}} relationship between WSS and WT and between diameter and WT, respectively, was found. For most subjects, a negative relationship was found between WSS and diameter. This is also displayed in Fig.  2.|$|R
2500|$|The {{intercept}} and {{slope of}} a linear regression between the quantiles gives {{a measure of the}} relative location and relative scale of the samples. If the median of the distribution plotted on the horizontal axis is 0, the intercept of a <b>regression</b> <b>line</b> is a measure of location, and the slope is a measure of scale. The distance between medians is another measure of relative location reflected in a Q–Q plot. [...] The [...] "probability plot correlation coefficient" [...] is the correlation coefficient between the paired sample quantiles. [...] The closer the correlation coefficient is to one, the closer the distributions are to being shifted, scaled versions of each other. [...] For distributions with a single shape parameter, the probability plot correlation coefficient plot (PPCC plot) provides a method for estimating the shape parameter – one simply computes the correlation coefficient for different values of the shape parameter, and uses the one with the best fit, just as if one were comparing distributions of different types.|$|E
2500|$|Constant {{variance}} (a.k.a. homoscedasticity). [...] This {{means that}} different {{values of the}} response variable have the same variance in their errors, regardless {{of the values of}} the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a [...] "fanning effect" [...] between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the <b>regression</b> <b>line.</b> Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear <b>regression</b> <b>line,</b> and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).|$|E
50|$|For example, when {{estimating}} the population mean, this method uses the sample mean; {{to estimate the}} population median, it uses the sample median; to estimate the population <b>regression</b> <b>line,</b> it uses the sample <b>regression</b> <b>line.</b>|$|E
3000|$|... 18 Osw[*]=[*]− 7.06 [*]+[*] 0.20 [*]×[*]salinity in July (line III of Fig.  4)]. Similar <b>regression</b> <b>lines</b> were {{developed}} for the Yellow Sea and the ECS [δ [...]...|$|R
40|$|Contract No. AF 49 (638) - 213 This paper {{considers}} {{a situation where}} one has two <b>regression</b> <b>lines</b> which {{are known to be}} parallel, with the two sets of error terms assumed to be normally distributed but with (possibly) different variances. The paper presents a test of the hypothesis that the two <b>regression</b> <b>lines</b> are identical (i. e., the two ~coefficients are equal). -Tbetest is analogous to the Wilcoxon test. The discussion in this paper is on a,rather technical level; for a less technical discussion of the test, see Mime...|$|R
3000|$|... 18 O values (− 8.4 to − 7.1 [*]‰) for Changjiang River {{water in}} January and July (Zhang et al. 1990). However, the <b>regression</b> <b>lines</b> of the δ [...]...|$|R
50|$|R2 is a {{statistic}} {{that will give}} some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is {{a statistic}}al measure of how well the <b>regression</b> <b>line</b> approximates the real data points. An R2 of 1 indicates that the <b>regression</b> <b>line</b> perfectly fits the data.|$|E
5000|$|... (the {{point on}} the <b>regression</b> <b>line),</b> while the actual {{response}} would be ...|$|E
50|$|A {{trigonometric}} {{representation of}} the orthogonal <b>regression</b> <b>line</b> was given by Coolidge in 1913.|$|E
3000|$|... 18 Osw[*]=[*]− 10.7 [*]+[*] 0.27 [*]×[*]salinity in summer] (Ye et al. 2014). These <b>regression</b> <b>lines</b> do {{not pass}} through the end-member values of the Kuroshio surface water (salinity[*]=[*] 34.2 – 34.4 and δ [...]...|$|R
40|$|Abstract [...] Synthetic hematites with A 1 substitutions between 0 and 18 mol % were {{synthesized}} {{at different}} temperatures and water activities. The cell-edge lengths a for different synthesis conditions decreased linearly with increasing A 1 substitution. The <b>regression</b> <b>lines,</b> however, had different slopes and intercepts: the {{series with the}} highest synthesis temperature (1270 K) had the most negative slope. With increasing AI substitution, the hematites contained increasing amounts of non-surface water. Significant correlations were found between these chemically determined water contents and the deviations of the unit-cell parameters a, c, and V relative to the corresponding 1270 K <b>regression</b> <b>lines.</b> To explain the measured X-ray peak intensities, structural OH had to be included into the theoretical calculations. From intensity ratios normalized to Il ~ 3, {{it is possible to}} determine the structural OH separately from the AI substitution, which can be assessed by the shift of the cell-edge lengths relative to the 1270 K <b>regression</b> <b>lines.</b> The incorporation of A 1 and OH into the hematite structure induces strain, which was quantified by X-ray diffraction...|$|R
40|$|A fault {{diagnosis}} scheme for nonlinear time series recorded in normal and abnormal conditions is described. The fault is first detected from <b>regression</b> <b>lines</b> {{of the raw}} time series. Model for the normal condition time series is estimated using a Finite Impulse Response (FIR) neural network. The trained network is then used for filtering of abnormal condition time series. The fault is further confirmed/ analyzed using the <b>regression</b> <b>lines</b> of the predicted normal and inverse-filtered abnormal conditions time series. The described scheme is applied to two {{fault diagnosis}} problems using acoustic and vibration data obtained from rotating parts of an automobile and a boring tool, respectively...|$|R
5000|$|A test {{of whether}} the slope of a <b>regression</b> <b>line</b> differs {{significantly}} from 0.|$|E
50|$|This {{shows the}} role rxy {{plays in the}} <b>regression</b> <b>line</b> of {{standardized}} data points.|$|E
5000|$|... is {{estimated}} as {{the slope of}} the <b>regression</b> <b>line</b> for [...] versus [...] where: ...|$|E
40|$|A piecewise-smooth {{function}} with discontinuity {{in the first}} derivative on a given interval is considered. The values of the function are measured at a sequence of points in the interval and a random error {{is included in the}} measurements. A method is proposed to estimate the position of the discontinuity in the derivative. <b>Regression</b> <b>lines</b> are associated with each measurement point and account for k - 1 points preceding or following the point. The estimate for the position of the discontinuity is the measurement point with the largest angle between the <b>regression</b> <b>lines.</b> The error in the estimate is analyzed and the results are verified...|$|R
40|$|This paper {{presents}} a preliminary {{analysis of the}} fecundity-length relationship of the North Sea mackerel. The relationship was {{compared to that of}} the Western mackerel stock (LOCKWOOD, 1978) by an analysis of covariance between the <b>regression</b> <b>lines.</b> Further analysis is necessary before any decisive conclusion can be drawn. The results, however, seem to demonstrate a lower fecundity in the North Sea mackerel (140 000 - 360 000 eggs) as compared to the Western mackerel (400 000 - 990 000 eggs) at the same range of length. A comparison of <b>regression</b> <b>lines</b> of fecundity-length for the North Sea and the Western area shows {{a significant difference between the}} two lines...|$|R
40|$|This paper {{looks into}} the use of {{principal}} components from two perspectives. <b>Regression</b> <b>lines</b> are fitted to scree plots {{to see if they}} might assist with resolving the number of factors question. Then, the use of the principal eigenvalue and corresponding eigenvector from a classical components analysis is examined, with an eye to identifying best test items. It was found that <b>regression</b> <b>lines</b> may be a useful addition when it comes to determining where an eigenvalue scree breaks. It was also found that the use of item loadings on the first principal component is more likely to identify best items than is the process of looking at item-test correlations...|$|R
