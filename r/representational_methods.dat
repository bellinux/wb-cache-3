39|23|Public
5000|$|War and Cinema: The Logistics of Perception, a 1989 book by Paul Virilio, {{discusses}} {{the relationship between}} image and war technology. Drawing {{on a number of}} films and film directors, including Sergei Eisenstein, Francis Ford Coppola, D.W. Griffith, and Stanley Kubrick, Virilio presents an postmodern analysis of how the <b>representational</b> <b>methods</b> of photography and cinema have impacted modern and historical warfare.|$|E
5000|$|As the SSP is not {{suitable}} beyond 18 months of age, other {{measures have been}} developed for older ages groups, which include observational measures (in a controlled or naturalistic environment), <b>representational</b> <b>methods</b> and interview methods. Some are developed for research purposes whereas others {{have been developed for}} clinical use. Effective training of evaluators is essential, as some items to be assessed require interpretation reliability (e.g., child is [...] "suddenly aggressive toward mother for no reason").|$|E
50|$|Acoustic {{encoding}} is the encoding of auditory impulses. According to Baddeley, {{processing of}} auditory information is {{aided by the}} concept of the phonological loop, which allows input within our echoic memory to be sub vocally rehearsed in order to facilitate remembering. When we hear any word, we do so by hearing to individual sounds, one at a time. Hence the memory of {{the beginning of a new}} word is stored in our echoic memory until the whole sound has been perceived and recognized as a word. Studies indicate that lexical, semantic and phonological factors interact in verbal working memory. The phonological similarity effect (PSE), is modified by word concreteness. This emphasizes that verbal working memory performance cannot exclusively be attributed to phonological or acoustic representation but also includes an interaction of linguistic representation. What remains to be seen is whether linguistic representation is expressed at the time of recall or whether the <b>representational</b> <b>methods</b> used (such as recordings, videos, symbols, etc) participate in a more fundamental role in encoding and preservation of information in memory.|$|E
40|$|MAIDHEA is a <b>representational</b> <b>method</b> {{for using}} {{hypertext}} for educational purposes. MAIDHEA uses OMT (object-oriented modelling technique) class diagram notation for modelling navigation structures, navigation systems and instructional design. MAIDHEA is a <b>representational</b> <b>method</b> for using hypertext for educational purposes. MAIDHEA uses OMT (object-oriented modelling technique) class diagram notation for modelling navigation structures, navigation systems and instructional design...|$|R
40|$|Abstract—Corpora {{present a}} basic informational source for varied NLP applications. Their {{construction}} becomes now days necessary. This paper presents a tool created to help syntactic tagging an Arabic Treebank. It {{is based on}} an already constructed grammar called ArabTAG which constitutes a <b>representational</b> <b>method</b> of Arabic grammatical rules using TAG formalism. This tagging tool is a node among a set of other pretreatment steps as: morpho-syntactic analysis, grammatical tagging and sentence segmentation. It gets as input a sentence and helps to give it the appropriate syntactic tree in an incremental manner. U I...|$|R
40|$|Computational {{linguistic}} {{helps in}} understanding the meaning of sentence given by an individual as per situation. Our work of Descriptive examination comes under experiential cognition as we are modeling & finding correct meaning of answer given by any individual. In this paper we have explained how we used HMM to reduce the complexity of variation in answer and shown knowledge <b>representational</b> <b>method</b> for representing the answer. In our work even {{the context of the}} question is given a real importance & thus our representation has impact of Chomsky context sensitive gramma...|$|R
40|$|This {{viewgraph}} presentation presents {{information on}} the attempt to produce high-order, efficient, central methods that scale well to high dimension. The central philosophy is that the equations should evolve {{to the point where}} the data is smooth. This is accomplished by a cyclic pattern of reconstruction, evolution, and re-projection. One dimensional and two dimensional <b>representational</b> <b>methods</b> are detailed, as well...|$|E
40|$|ABSTRACT. A {{number of}} {{different}} <b>representational</b> <b>methods</b> exist for presenting the theory of linear equations and associated solution spaces. Discussed in this paper are {{the findings of a}} case study where first year undergraduate students were exposed to a new (to the department) method of teaching linear systems which used visual, algebraic and data-based representations constructed using the computer algebra system Maple. Positive and negative impacts on the students are discussed as they apply to representational translation and perceived learning...|$|E
40|$|Early modern {{physics of}} the 19 th century {{postulated}} {{the existence of an}} aether, or space filling medium which allowed for the transmission of electromagnetic waves and gravitational forces. The aether permeated all space, the substance of nothingness by which immaterial phenomena could be explained through material properties. Architects rely on <b>representational</b> <b>methods</b> to depict space, the absence of matter, in ways that best serve our intentions to manipulate it. This project investigates architecture’s primary pursuit, to create void from solid, through the design of a row house in Philadelphia...|$|E
40|$|Abstract. The {{problem of}} {{maintaining}} consistent representations of solids, in computer-aided design, and of giving rigorous proofs of error bounds for operations such as regularized Boolean intersection, {{has been widely}} studied {{for at least two}} decades. One of the major difficulties is that the representations used in practice are not only in error, they are fundamentally inconsistent. Such inconsistency {{is one of the main}} bottlenecks in downstream applications. This paper provides a framework for error analysis in the context of solid modeling, in the case where the data is represented using the standard <b>representational</b> <b>method,</b> and where the data may be uncertain. Included are discussions of ill-condition, error measurement, stability of algorithms, inconsistency of defining data, and the question of when we should invoke methods outside the scope of numerical analysis. A solution to the inconsistency problem is proposed and supported by theorems: it is based on the use of Whitney extension to define sets, called QuasiNURBS sets, which are viewed as realizations of the inconsistent data provided to the numerical method. A detailed example illustrating the problem of regularized Boolean intersection is also given...|$|R
40|$|Ph. D. In {{this thesis}} we {{undertake}} {{a study of}} formal three-dimensional <b>representational</b> and acceptor <b>methods.</b> In lieu hereof then, we give a short overview of such strategies existing in the literature. Graph and graph grammar theory present us with a powerful two dimensional <b>representational</b> <b>method,</b> and we propose to extend these concepts to the three-dimensional case. We therefore give a short discussion on the theory of graphs and graph grammars. As point of departure, we review the concepts of a structure parameter and structure graph (SG) introduced by us in [BEH, 93] and show that these concepts enable us to describe objects in three-dimensional space. We propose various modified graph grammar extensions that generates structure graphs, referred to as Structure Graph Grammar extensions (SGG's) by combining context provisions with the rewriting rules of the various grammar systems. This proposed methodology of ours culminates in the combination of production rule bounded contexts and globally specified contexts, thus defining Structure Graph Grammar extension 7 (SGG- 7). We show the applicative value of the three dimensional generative abilities of SGG's by considering the generation of various chemical structural formulae. Brandenburg and Skodinis mentions in [BS, 95] {{that there is a}} shortcoming in the theory of graph grammars in the sense that in general, there exists no accepting device for graph grammar systems. The following quote from [BS, 95,p. 336] illustrates this point: "There are no graph automata, which fit to the major classes of graph languages. This is a gap in the theory of graph languages. " Regarding the class of languages generated by SGG- 7, we propose to fill this gap by introducing an Structure Graph Automaton (SGA) to accept this class of languages...|$|R
40|$|Understanding {{of social}} network {{structure}} and user behavior {{has important implications}} for site design, applications (e. g., ad placement policies), accurate modeling for social studies, and design of next-generation infrastructure and content distribution systems. Currently, characterizations of social networks have been dominated by topological studies in which graph representations are analyzed in terms of connectivity using techniques such as degree distribution, diameter, average degree, clustering coefficient, average path length, and cycles. The problem is that these parameters are not completely satisfactory {{in the sense that they}} cannot account for individual events and have only limited use, since one can produce a set of synthetic graphs that have the exact same metrics or statistics but exhibit fundamentally different connectivity structures. In such an approach, a node drawn as a small circle represents an individual. A small circle reflects a black box model in which the interior of the node is blocked from view. This paper focuses on the node level by considering the structural interiority of a node to provide a more fine-grained understanding of social networks. Node interiors are modeled by use of six generic stages: creation, release, transfer, arrival, acceptance, and processing of the artifacts that flow among and within nodes. The resulting description portrays nodes as comprising mostly creators (e. g., of data), receivers/senders (e. g., bus boys), and processors (re-formatters). Two sample online social networks are analyzed according to these features of nodes. This examination points to the viability of the <b>representational</b> <b>method</b> for characterization of social networks...|$|R
40|$|A {{number of}} {{different}} <b>representational</b> <b>methods</b> exist for presenting the theory of linear equations and associated solution spaces. Discussed in this paper are {{the findings of a}} case study where first year undergraduate students were exposed to a new (to the department) method of teaching linear systems which used visual, algebraic and data-based representations constructed using the computer algebra system Maple. Positive and negative impacts on the students are discussed as they apply to representational translation and perceived learning. Note: This is a preprint of a paper submitted to the International Electronic Journal of Mathematics Education...|$|E
40|$|Abstract—A fuzzy binary track {{correlation}} algorithm and a fuzzy classical {{assignment algorithm}} are proposed for distributed multisensor data fusion in this paper. First, the corresponding composition of fuzzy element sets and selection of membership function are discussed. And then, dynamic assignment of weight vectors and multi-valency processing methods are designed. Finally, a fuzzy binary track correlation criterion {{is presented to}} improve accuracy. Moreover, the proposed algorithms are compared with two <b>representational</b> <b>methods</b> in classical simulations. The simulation {{results show that the}} performance of new algorithms is much better than that of traditional methods in complex scenarios. Specifically, the proposed algorithms improve the correlation accuracy in ~ 35 % of the classical algorithms in the simulation...|$|E
40|$|Collaboration {{benefits}} {{the process of}} complex design. However, there are many communication problems among different stakeholders {{in the domain of}} industrial design, because the situation of communication and decision-makings for stakeholders is so complicated. To deal with the complexity requires both a web-based collaborative system to communicate and share information immediately, and a multi-agent system (MAS) integrated with KW architecture to possess different levels of competence at performing a particular task. The goal of our system is to integrate a variety of <b>representational</b> <b>methods</b> of transferring knowledge and to communicate among different stakeholders using a single platform. To demonstrate our proposed concepts, we focus on a prototype system for notebook design for the company ASUS, a leading notebook manufacturer based in Taiwan...|$|E
40|$|A {{solo show}} at Matt's Gallery May-June 2012 {{incorporating}} Experimental Narrative film, Installation, sculpture, sound,and continuous live performance inside the installation. Commissioned by Matt's Gallery. 'School of Change' {{is a project}} that collides genres and ideas; at its core is a 40 minute sci-fi musical + experimental narrative film, shot in my old secondary school, transformed into a skewed and satirical version of everyday reality. The project was realized as an ambitious sculptural and Film Installation at Matt’s Gallery in May - June 2012. The film, SCHOOL OF CHANGE, is an experimental, musical, science fiction film, based in a distorted reality that satirically reflects on our own, where changes - social, technological - even mutations in the workings of reason itself - are threatening the viability of humanity. This project embodied a complex critique of contemporary fears and desires for radical change both social and spiritual; speculating on the future effects of ‘gamification’ technology in collision with the marketisation of education, exploring a <b>representational</b> <b>method</b> that collides an everyday location with the extraordinary. Friction between the real and artificial is explored via experiments with costume design, disruptive editing patterns and songs. Choreographed movement-games and on-screen ‘scores’ speculated on a society with a hive mind, deploying morally suspect uses of technology for mechanisms of control in a post- eco-apocalyptic future. These methodologies embedded the questioning of representational codes used in both popular and experimental media within the narrative structure of the film itself. Elements of this project went ‘on the road’ as a live evening event, that explored a hybrid, cross-over space between art, music, storytelling and performance. Venues: and Camden Arts Centre December 2011, OUTPOST gallery Norwich summer 2012. It is due to to tour as an installation to other international venues in 2014. It has also screened as a single screen work at Gallery 400 Chicago, and Antimatter International Underground Film festival, Victoria, Canada amongst others...|$|R
40|$|The {{tendency}} to stage appreciation for {{and attention to}} nature as a passive, guiltless enterprise was necessary for eighteenth-century colonial claims to space, but it also remains a very deeply entrenched aspect of environmentalist attitudes today. Indeed, innovations that shaped the technological interpretation and inscription {{of place in the}} latter eighteenth century have strongly situated contemporary North American environmental discourses. This thesis explores the methods of spatial representation in Samuel Hearne’s A Journey from Prince of Wales’s Fort, in Hudson’s Bay, to the Northern Ocean(1795) and William Bartram’s Travels Through North and South Carolina, Georgia, East and West Florida, The Cherokee Country, The Extensive Territories of the Muscogulges or Creek Confederacy, and the Country of the Choctaws (1792). Both ecocritical and postcolonial methods underlay an analysis of the discourses and rhetorics of space exhibited in the North American travel writing of these two late-eighteenth-century writers. A first move monitors how landscape accrues not only as a product of descriptive techniques, frames, and screens, but also as a process whereby narrative identity is formed against and within a represented landscape. A second move locates these texts as versions of Mary Louise Pratt’s “anti-conquest,” in which the hero-explorer of colonial encounter is staged as both passive and innocent. Two primary results from this research into the relationship between literature and environment are reported. First, according to conventions of ecocritical analysis, Hearne and Bartram implement two very different modes of spatial representation in travel narratives from the same period; in the broadest strokes, Hearne’s text is deeply anthropocentric and only partially engages in eighteenth-century vogues of natural history, while Bartram’s is compellingly and precociously ecocentric as well as deeply invested in the commerce of Linnaean systemizations of nature that revolutionized natural history in the period. Second, this disparity in <b>representational</b> <b>method</b> is correlated not only with variances in the ecological (or green) sensibilities of the authors, but also with distinctions in the colonial functionality of the texts, verifying that literature of place, despite the putative object of description, always already maintains significant valencies in social registers...|$|R
40|$|To {{develop an}} {{efficient}} {{implementation of the}} maximally permissive deadlock avoidance policy (DAP) for complex resource allocation systems (RAS), a recent approach focuses on the identification {{of a set of}} critical states of the underlying RAS state-space, referred to as minimal boundary unsafe states. The availability of this information enables an expedient one-step-lookahead scheme that prevents the RAS from reaching outside its safe region. This paper presents a symbolic approach that provides those critical states. Furthermore, by taking advantage of certain structural properties regarding RAS safety, the presented method avoids the complete exploration of the underlying RAS state-space. Numerical experimentation demonstrates the efficiency of the approach for developing the maximally permissive DAP for complex RAS with large structure and state-spaces, and its potential advantage over similar approaches that employ more conventional <b>representational</b> and computational <b>methods...</b>|$|R
40|$|This paper {{addresses}} {{two related}} issues: {{how we can}} judge and represent 'truth ' {{in the context of}} Geographic Visualization (GVIS) and what 'truth ' is in this context. The first issue is approached from an analytical perspective, with emphasis on measurable aspects of validity in geo-referenced information and on <b>representational</b> <b>methods</b> for depicting validity. In relation to the second issue, a philosophical perspective on truth emphasizes the concept of cognitive gravity as a factor that leads scientists and policy makers to see only what they expect to see. The goal of this essay is not to provide specific guidelines for dealing with aspects of truth in GVIS, but to introduce a framework for exploring the relevant issues. The framework proposed is grounded in a semiotics of geographic representation...|$|E
40|$|This {{dissertation}} {{focuses on}} the problematic {{of the use of}} specific virtual <b>representational</b> <b>methods</b> and their influence on the construction of the new visuality. The aim of this dissertation is to show how computer simulations and visualizations used in medical imaging change the way of our apprehension of the world. Nowadays, within academic circles we witness both optimistic and sceptic theories concerning the effects of integration of computer technology and human apprehension. In this work we would like to answer the question about the type of knowledge we gain through virtual visualizations and we would like to prove that virtual simulations don't necessarily have to be a threat but, on the contrary, by developing visual imagination they can support new, creative apprehension of reality. Key words Linear perspective, logic of database, narrative, rhizome, scopic regime, simulation, simulacrum, virtuality, vision, visuality, visualizatio...|$|E
40|$|International audienceIn {{this article}} we {{describe}} and discuss means that foster the emergence of innovation through <b>representational</b> <b>methods</b> which interrelate manual modeling with playfulness. Based on the observation that demands to innovation processes have changed significantly in recent years due to changed collaboration forms, like co-configuration or open innovation, we look for a methodological approach {{able to deal with}} such collaboration forms. We describe and discuss a methodological approach on how innovation processes in heterogeneous – interdisciplinary, cross-functional and interorganizational – groups can be kicked off to bring about collectively shared understanding, as well as the ability to develop creative ideas. The approach relies on a playful modeling methodology, which is based on the hands-on creation of visualizations and physical models in connection with their verbal explanation and narration. With reference to two case studies we report and discuss experiences of applying the methodology...|$|E
40|$|Research in IT {{must address}} the design tasks faced by practitioners. Real {{problems}} must be properly conceptual-ized and represented, appropriate techniques for their solution must be constructed, and solutions must be implemented and evaluated using appropriate criteria. If significant progress is to be made, IT research must also {{develop an understanding of}} how and why IT systems work or do not work. Such an understanding must tie together natural aws governing IT systems with natural aws governing the environments in which they operate. This paper presents a two dimensional framework for research in information technology. The first dimension is based on broad types of design and natural science research activities: build, evaluate, theorize, and justify. The second imension is based on broad types of outputs produced by design research: <b>representational</b> constructs, models, <b>methods,</b> an...|$|R
40|$|Reliable {{and quick}} {{response}} fault diagnosis {{is crucial for}} the wind turbine generator system (WTGS) to avoid unplanned interruption and to reduce the maintenance cost. However, the conditional data generated from WTGS operating in a tough environment is always dynamical and high-dimensional. To address these challenges, we propose a new fault diagnosis scheme which is composed of multiple extreme learning machines (ELM) in a hierarchical structure, where a forwarding list of ELM layers is concatenated {{and each of them}} is processed independently for its corresponding role. The framework enables both representational feature learning and fault classification. The multi-layered ELM based representational learning covers functions including data preprocessing, feature extraction and dimension reduction. An ELM based autoencoder is trained to generate a hidden layer output weight matrix, which is then used to transform the input dataset into a new feature representation. Compared with the traditional feature extraction methods which may empirically wipe off some “insignificant’ feature information that in fact conveys certain undiscovered important knowledge, the introduced <b>representational</b> learning <b>method</b> could overcome the loss of information content. The computed output weight matrix projects the high dimensional input vector into a compressed and orthogonally weighted distribution. The last single layer of ELM is applied for fault classification. Unlike the greedy layer wise learning method adopted in back propagation based deep learning (DL), the proposed framework does not need iterative fine-tuning of parameters. To evaluate its experimental performance, comparison tests are carried out on a wind turbine generator simulator. The results show that the proposed diagnostic framework achieves the best performance among the compared approaches in terms of accuracy and efficiency in multiple faults detection of wind turbines...|$|R
40|$|The genomic {{subtraction}} <b>method</b> <b>representational</b> difference analysis (RDA) {{was used}} to identify male-specific restriction fragments in the dioecious plant Silene latifolia. Male-specific restriction fragments {{are linked to the}} male sex chromosome (the Y chromosome). Four RDA-derived male-specific restriction fragments were used to identify polymorphisms in a collection of X-ray-generated mutant plants with either hermaphroditic or asexual flowers. Some of the mutants have cytologically detectable deletions in the Y chromosome that were correlated with loss of male-specific restriction fragments. One RDA-derived probe detected a restriction fragment present in all mutants, indicating that each has retained Y chromosomal DNA. The other three probes detected genomic fragments that were linked in a region deleted in some hermaphroditic and some asexual mutants. Based on the mutant phenotypes and the correlation of cytologically visible deletions with loss of male-specific restriction fragments, these markers were assigned to positions on the Y chromosome close to the carpel suppression locus. This RDA mapping also revealed a Y-linked locus, not previously described, which is responsible for early stamen development...|$|R
40|$|Mapping &quot;soft &quot; data, {{data that}} are inaccurate, uncertain, or of dubious lineage, {{presents}} several challenges to cartography. A review of existing research shows that authors have focussed on issues of parallel vs. active display for uncertainty, which visual variable are appropriate for uncertainty data {{and the need to}} use additional display variables such as animation and focus to display uncertainty data on maps. Nevertheless, most research has shown that existing cartographic <b>representational</b> <b>methods</b> are adequate for the task. This study takes the case of point displacements between line maps, devises a six parameter affine fit between two possible maps, and then experiments with different static displays for the background display of uncertainty information using color and hillshading. Color and animation are thought to hold value for future active portrayal of uncertainty information, but this area of research needs a considerable amount of additional work...|$|E
40|$|Abstract. This paper {{draws on}} the design process, {{implementation}} and early evaluation results of an urban screens network to highlight the tensions that emerge at {{the boundary between the}} technical and social aspects of design. While public interactive screens in urban spaces are widely researched, the newly emerging networks of such screens present fresh challenges. Researchers wishing to be led by a diverse user community may find that the priorities of some users, directly oppose the wishes of others. Previous literature suggests such tensions can be handled by ‘goal balancing’, where all requirements are reduced down to one set of essential, implementable attributes. Contrasting this, this paper's contribution is ‘Tension Space Analysis’, which broadens and extends existing work on Design Tensions. It includes new domains, new <b>representational</b> <b>methods</b> and offers a view on how to best reflect conflicting community requirements in some aspects or features of the design...|$|E
40|$|The {{communication}} of uncertainty in clinical evidence {{is an important}} endeavor that poses difficult conceptual, methodological, and ethical problems. Conceptual problems include logical paradoxes in the meaning of probability and “ambiguity”— second-order uncertainty arising {{from the lack of}} reliability, credibility, or adequacy of probability information. Methodological problems include questions about optimal methods for representing fundamental uncertainties and for communicating these uncertainties in clinical practice. Ethical problems include questions about whether communicating uncertainty enhances or diminishes patient autonomy and produces net benefits or harms. This article reviews the limited but growing literature on these problems and efforts to address them and identifies key areas of focus for future research. It is argued that the critical need moving forward is for greater conceptual clarity and consistent <b>representational</b> <b>methods</b> that make the meaning of various uncertainties understandable, and for clinical interventions to support patients in coping with uncertainty in decision making...|$|E
40|$|I think Bart Kosko {{was right}} when he accused the Aristotelian {{mathematics}} of yes and no and of certainties, of being incapable of representing the world which in jàct {{is made up of}} nuances. The architect is well aware of this difficulty when he realizes that each architectural measurement carries with it an uncertainty which is extremely difficult to express in numbers and which, on the contrary, is perhaps more important than the measurement itself, since this is the sign of the hand of man and, at times, of the consummate experience of an artist. Kosko goes on to propose a new - fuzzy - logic according to Lofri Zadeh. And it appears that these fuzzy numbers with their amazing ability to incorporate uncertainty, have a future in architecture. Let's imagine we have a tridimensional model not mere/y reduced to a discrete schematic drawing of points and lines, but continuous, just like reality, or at least as continuous as possible in light of the reduction relationship that is typical of each architectural representation. Let's imagine a model which contains the vagueness of what is real, the poetic uncertainty of its measurements, capable of restoring, if necessary, the numbers of traditional representation without loosing the fuzzy trait of reality. I say that every surveyor, every planner and every architecture expert would want this type of model. No more doubts on the metric reliability ola survey done by others, without declaring and measuring the uncertainty. Well this model exists and is present/y being experimented at the laboratory of Photogrammetry of the Department of Representation and Survey at the University "La Sapienza" of Rome, in the framework of the "Finalized Project Cultural Heritage" of the National Council for Scientific Research. The DCM (Digital Continuous Model) as it has been called, simplifies the complex procedures of traditional survey enormously by unifying them while including the possible graphic representations. A DCM consists in a tridimensional photographic representation which can be examined {{with the help of a}} simple computer system, a screen and a pair of active glasses. It is not, however, just a simple stereoscopic photograph, but a continuous system of stereo photogrammetric models. Up to now, the examination of a photogrammetric stereo model was a privilege reserved for a few experts and needed specialist skills: the ability to use a stereocomparator, the ability to orient the model as well as complete visual tolerance of the strong contrasts typical of photogrammetry. Moreover, a photogrammetric survey always includes a series of models, therefore, once the study of the first model has ended one progresses to the second and so on, repeating the above-mentioned laborious procedures each time. With the DCM all this is a thing of the past: the viewing is immediate, the orientation procedures take place at the start, once and for all, and therefore may be ignored by the user, the passage from one model to another is fluid and automatic and consequent/y it feels as if there is just one image that can be viewed by moving around the object or by rising up off the ground in order to examine all the details. Furthermore, the system can measure distances and areas as well as drawing lines on the object with the methods used by any C 4 D. These lines can be projected on a p ian in order to obtain any type of graphic representation from the DCM. Theoretical/y, is the DCM a new <b>representational</b> <b>method?</b> To a certain extent it is, even if it would be more correct to consider the DCM as a 'generalization' of the graphic models already known. But the DCM has one additional virtue which I have already tried to underline: its ability to preserve the fuzzy quality of the real object...|$|R
40|$|Interleukin 4 (IL- 4) is a {{cytokine}} {{that regulates}} growth and differentiation of lymphoid and nonlymphoid cells. To study the molecular basis of IL- 4 function, {{we used a}} cDNA subtraction approach based on the <b>representational</b> difference analysis <b>method.</b> This subtractive amplification technique allowed us to use small amounts of RNA from lipopolysaccharide ± IL- 4 -stimulated normal B cells to obtain IL- 4 -induced genes from these cells. We report here on one such gene, Fig 1 (interleukin-four induced gene 1), the first characterized immediate–early IL- 4 inducible gene from B cells. Fig 1 expression is strikingly limited to the lymphoid compartment. B cells, but not T cells or mast cells, express Fig 1 in response to IL- 4 within 2 hr in a cycloheximide resistant manner. IL- 2, IL- 5, and Il- 6 do not induce Fig 1 in culture. Fig 1 maps between Klk 1 and Ldh 3 on mouse chromosome 7, near two loci involved with murine lupus, Sle 3 and Lbw 5. The Fig 1 cDNA sequence encodes a predicted 70 -kDa flavoprotein with best homology to the monoamine oxidases, particularly in domains responsible for FAD binding...|$|R
40|$|This paper {{presents}} {{the view that}} artificial intelligence (AI) is primarily concerned with propositional languages for representing knowledge and with techniques for manipulating these representations. In this respect, AI is analogous to applied mathematics; its representations and techniques can be applied {{in a variety of}} other subject areas. Typically, AI research (or should be) more concerned with the general form and properties of <b>representational</b> languages and <b>methods</b> than it is with the content being described by these languages Notable exceptions involve “commonsense ” knowledge about the everyday world (no other specialty claims this subject area as its own), and metaknowledge (or knowledge about the properties and uses of knowledge itself). In these areas AI is concerned with content as well as form. We also observe that the technology that seems to underly peripheral sensory and motor activities (analogous to low-level animal or human vision and muscle control) seems to be quite different from the technology that seems to underly cognitive reasoning and problem solving. Some definitions of AI would include peripheral as well as cognitive processes; here we argue against including the peripheral processes. I...|$|R
40|$|Part 2 : Facilitating Social Behaviour and Collaboration IInternational audienceThis paper {{draws on}} the design process, {{implementation}} and early evaluation results of an urban screens network to highlight the tensions that emerge at {{the boundary between the}} technical and social aspects of design. While public interactive screens in urban spaces are widely researched, the newly emerging networks of such screens present fresh challenges. Researchers wishing to be led by a diverse user community may find that the priorities of some users, directly oppose the wishes of others. Previous literature suggests such tensions can be handled by ‘goal balancing’, where all requirements are reduced down to one set of essential, implementable attributes. Contrasting this, this paper’s contribution is ‘Tension Space Analysis’, which broadens and extends existing work on Design Tensions. It includes new domains, new <b>representational</b> <b>methods</b> and offers a view on how to best reflect conflicting community requirements in some aspects or features of the design...|$|E
40|$|Collaboration {{benefits}} {{the process of}} complex design. However, there are many communication problems among different stakeholders {{in the domain of}} industrial design. In addition, the sharing of real-time information and updating information can also pose difficulties. This paper proposes an agent-based collaborative system to support stakeholders to deal with the complex communication problems and information management. The system is composed of both a web-based collaborative system to communicate and share information immediately, and a multi-agent system (MAS) integrated with KW architecture to possess different levels of competence at performing a particular task. The objective is to provide different stakeholders with an effective communication platform to integrate a variety of <b>representational</b> <b>methods</b> of transferring knowledge and the multi-agents that represent different stakeholders to simulate the communication and collaboration in the real world. To demonstrate the proposed concepts, a system prototype has been implemented for notebook design for the company ASUS, a leading notebook manufacturer based in Taiwan...|$|E
40|$|In {{the series}} Delirious at the Borderlines I ask how <b>representational</b> <b>methods</b> like montage, {{automatic}} drawing and historical referencing are best enlisted to portray a nearcatastrophic health crisis. Bataille, and Blanchot sparked {{my interest in}} the extreme limits of experience demarcating the gap between delirium and reality, the transgressive and normative, and of life and death. I deploy the works of artists Felix Gonzalez-Torres and David Wojnarowicz, and authors with multivalent positions regarding aporias, unsolvable riddles. They illuminate my exploration of contested liminal zones and the alterity of the constituents of such nether-worlds – immigrants, heretics, incubi, the infirm, and AIDS patients. Theories of adaptation and translation informed by Linda Hutcheon and Jacques Derrida were deployed to elucidate methods of intertextuality (literary and visual) and the palimpsestuous (formal and art historical). These strategies are manifest in my digital scrollworks and drawings as they interrogate the (im) possibility of death and its representation...|$|E
40|$|In {{the last}} decades, the {{complexity}} of products has increased exponentially. This com- plexity {{is due to the}} ever-growing market demand for new and innovative products with multiple functionalities, to the product size and component density that grow to a level that only a handful of experts can deal with, and to the multi-disciplinarity or inter-disciplinarity nature of products such as mechatronics products. As a conse- quence of this product complexity, the product development process is becoming complex as well due to the multiple stakeholders involved in the process and their rapidly changing roles. A well-known procedure for product development is the V-model. In the V-model, a product design consists firstly of requirements analysis, which considers the possible conflicting needs of various stakeholders, secondly of the system design that derives system specifications from requirements and chooses main concepts of the product. Thirdly, the overall system concept is quickly decomposed in subsystems that provide the design with more details and eventually components are designed. Then, the design verification starts by gradually integrating and testing components and subsystems. Subsystems are clearly defined and understood because they belong to one small mod- ule of a product. Although each subsystem has clear definition and it is supposed to behave as specified, designers can still be surprised by unpredicted problems that dete- riorate the performance of the product. Unpredicted problems could be hard to solve or even hard to detect. As a consequence, reaching a final satisfactory product can be time-consuming and cost-inefficient because of additional iterations between design and design verification in the development process. In order to reduce these iterations, we need to detect such unpredicted problems as early as possible in the design process. The aim of the research presented in this thesis is to diminish unpredictable problems of complex products such as multi-disciplinary products by taking measures in the conceptual design phase of those products. To do so, a Design Interferences Detector (DID) has been developed. DID employs the Function-Behavior-State (FBS) model as a <b>representational</b> <b>method</b> and Qualitative Physics (QP) as a reasoning engine. Basically, first, the designer builds the FBS model of the design to analyze, and then a qualitative reasoning system infers physical phe- nomena that were not predicted by the designer. The FBS model incorporates func- tional, behavioral and architectural information of the product. QP infers qualitative information by referring to qualitative knowledge of physical systems. QP is considered potentially useful in building a computational support for the con- ceptual phase of a design because precise and complete information is not required VII and concept solutions {{do not have to be}} evaluated on a detailed level before proceed- ing to further developments. Another advantage is that it allows building a model of a fairly complex system in a short time that can be handy for big systems. However, qualitative reasoning has disadvantages as well, which makes difficult its use in practice. For instance, qualitative reasoning generates too many spurious and neg- ligible solutions, which need to be removed from the list of inferred results. This is a soundness problem. In certain situations, the system could not predict all possible in- terferences, which is a completeness problem. Furthermore, qualitative reasoning has intrinsic ambiguities due to the lack of quantitative information of the design model. To solve these problems, we developed various methods that became part of DID. A first method, the prioritization method, alleviates the completeness problem by act- ing on implicit relations of components. Two filtering methods, the contrast method and the interaction finding method, alleviate the soundness problem by using heu- ristics. These methods are useful to classify physical phenomena into spurious and negligible, predicted and unpredicted. A third method, the ambiguity solver method, has been developed to reduce ambiguities from the qualitative results based on intel- ligent user interventions. DID has been tested and applied on various case studies in order to show its efficiency. The methods are explained one by one in detail so that they can easily be reproduced for further developments of the tool. Finally, these methods are integrated in a single framework so that, in total, they can increase a product’s predictability. BioMechanicalMechanical, Maritime and Materials Engineerin...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfillment}} of the requirements for the award of Doctor of Philosophy of Loughborough University. To achieve success in today's competitive environment, companies are realising the importance of design collaboration during new product development. The aim {{of this research was}} to develop a collaborative design tool for use by industrial designers and engineering designers. To achieve this, a literature review was undertaken to understand the working relationship among the two disciplines during new product development. Following this, empirical research through interviews and observations outlined three problem areas: conflicts in values and principles; differences in education; and differences in <b>representational</b> tools and <b>methods.</b> The latter was chosen because the problem area of design representations was found to be highly significant. In looking at bridging differences in design representations, a taxonomy comprising 35 forms of sketches, drawings, models and prototypes was generated. A second stage of empirical research was conducted to establish the popularity of each representation and the type of design / technical information that industrial designers and engineering designers communicated with. The information was indexed into CoLab cards that would enable the two disciplines to gain joint understanding and create shared knowledge when using visual design representations. Following a pilot evaluation and minor modifications, student and practitioner interviews with a case study were employed to assess the significance of CoLab. The findings revealed that 82...|$|R
40|$|Plant {{phenolics}} {{have shown}} to activate apoptotic cell death in different tumourigenic cell lines. In this study, we evaluated {{the effects of}} juniper berry extract (Juniperus communis L.) on p 53 protein, gene expression and DNA fragmentation in human neuroblastoma SH-SY 5 Y cells. In addition, we analyzed the phenolic composition of the extract. We found that juniper berry extract activated cellular relocalization of p 53 and DNA fragmentation-dependent cell death. Differentially expressed genes between treated and non-treated cells were evaluated with the cDNA-RDA (<b>representational</b> difference analysis) <b>method</b> at the early time point of apoptotic process when p 53 started to be activated and no caspase activity was detected. Twenty one overexpressed genes related to cellular stress, protein synthesis, cell survival and death were detected. Interestingly, they included endoplasmic reticulum (ER) stress inducer and sensor HSPA 5 and other ER stress-related genes CALM 2 and YKT 6 indicating that ER stress response was involved in juniper berry extract mediated cell death. In composition analysis, we identified and quantified low concentrations of fifteen phenolic compounds. The main groups of them were flavones, flavonols, phenolic acids, flavanol and biflavonoid including glycosides of quercetin, apigenin, isoscutellarein and hypolaetin. It is suggested that juniper berry extract induced the p 53 -associated apoptosis through the potentiation and synergism by several phenolic compounds...|$|R
