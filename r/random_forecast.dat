10|141|Public
40|$|The primary {{objective}} {{of this research is}} to investigate the impact of <b>random</b> <b>forecast</b> error and bias forecast error in Collaborative Planning, Forecasting and Replenishment (CPFR) strategy on the cost of inventory management for both the manufacturer and retailer. Discrete-event simulation is used to develop a CPFR collaboration model where forecast, sales and inventory level information is shared between a retailer and a manufacturer. Based on the results of this study, we conclude that the higher <b>random</b> <b>forecast</b> error and negative bias forecast error increases the cost of inventory management for both the manufacturer and the retailer. When demand variability is high, a bias forecast error has a bigger impact on inventory management cost compared to a <b>random</b> <b>forecast</b> error for both the manufacturer and retailer. Also, a posi tive bias forecast error is more beneficial than a negative bias forecast error to gain maximum benefits of CPFR strategy. </p...|$|E
40|$|The present paper amends the two propositions in Peasnell (1995) {{concerning}} the fitness of J. R. Grinyer’s ‘earned economic income’ (EEI) model for its declared purpose of evaluating managerial {{performance in the}} light of comments in Grinyer (1995). Proposition I now includes the requirement that the profitability index is the same for each depreciable asset in the multi-asset firm in order for EEI to yield the same answers as the net present value (NPV) of the firm itself. Proposition II is now adjusted to reflect the possibility that errors in forecasted benefits can be large in magnitude. The new version distinguishes between <b>random</b> <b>forecast</b> errors and ‘earnings management’. The original result {{concerning the}} conditions when EEI will be more or less reliable than re-computed NPV holds as far as <b>random</b> <b>forecast</b> errors are concerned. In the case of management manipulations, the results depend on whether the investment is believed by management to be worthwhile and on whether the forecast biases are sufficient to turn a poor performance into a good one. The paper concludes with a brief reply to certain key points raised by Grinyer concerning my earlier analysis...|$|E
30|$|Leite Cristofaro et al. (2017) {{utilized}} {{artificial intelligence}} strategies to minimize lost circulation NPT {{in deep water}} Brazilian wells. Several predictive data-mining techniques used such as Naive Bayes, Instance-Based and Neural Network to predict losses and choose the best treatment for losses prior to entering the losses zone. Hegde et al. (2015 a, b) used principal component regression, least squares regression, and Ridge and Lasso regression as well as bootstrapping, trees, bagging, and the <b>random</b> <b>forecast</b> with data of to predict ROP. Wallace et al. (2015) developed a statistical learning model to predict and optimize the real-time drilling performance.|$|E
30|$|In short, {{the direct}} method usually {{performs}} well in modelling, but will probably gain an uncontrollable <b>random</b> error when <b>forecasting.</b> On the contrary, the data-driven method can limit the <b>forecasting</b> <b>random</b> error, but {{makes it harder}} to construct a precise model for each subsequence. As a combination of the above two methods, the proposed DLC method can reduce the <b>random</b> <b>forecasting</b> error while guaranteeing modelling accuracy, providing improved forecasting results.|$|R
30|$|Machine {{learning}} {{methods that}} build on classification trees have proved {{very effective in}} criminal justice classification and forecasting applications. Of those, <b>random</b> <b>forecasts</b> has been {{by far the most}} popular. We consider now random forests, but will provide a brief discussion of two other tree-based methods later. They are both worthy competitors to random forests.|$|R
30|$|The {{data-driven}} forecasting idea {{is introduced}} {{to address the}} forecasting difficulties caused by load fluctuation in developed cities. Based on an autoregressive integrated moving average (ARIMA) model, it is theoretically proved in this paper that the data-driven method is effective in reducing the <b>random</b> <b>forecasting</b> errors {{so that it can}} better adapt to the changing environment.|$|R
30|$|The {{recurrence}} interval itself {{is important for}} forecast. If the interval {{is less than the}} forecast period, the probability for the repeater is inevitably high. The ROC in Fig. 6 shows that the EXP model based on Poisson process taking the averaged sequence {{recurrence interval}} into account is better than the <b>random</b> <b>forecast.</b> Results of R- and dBS-tests (Fig. 11 and Table 3) indicate that the LN-Bayes and LN-SST models dependent on elapsed time since the last event are significantly better than the EXP model. The ROC curves depicted in Fig. 6 also suggest that these models have much higher performance than the EXP model for estimating probabilities (Figs. 5 and 6 and Tables 2 and 3). Therefore, the repeaters on the plate boundary along the Japan Trench are significantly dependent on elapsed time since the last event and are not random in time. However, it is presumed that the inverse gamma prior distribution used in the LN-Bayes model is slightly more effective for forecast repeaters, since the differences between the consistency scores of the LN-Bayes and the LN-SST models are very small.|$|E
40|$|Economis t, Research Depar tment, Federa l Reserve Bank o f Da l las. The v jews {{expressed}} in th is a r t i c le a re those o f the au thor and shou ld no t be a t t r ibu ted to the Federal Reserve Bank of Dallas {{or to the}} Federal Reserve System. Introductlon Barsky (1987) shows that for a Plsher effect to be scatlstlcally observable, {{the rate of inflation}} nust be petslstent. This denonsttatlon follows from the fact that if lnflation follows a whlte-noise process, changes ln ex post lnflatlon represent <b>random</b> <b>forecast</b> errors that will be uncorrelated wlth expected lnflation and nominal interest rates. Therefore, ln thls case, there need be no statistically significant relationshLp between inflation and norninal lnterest rates even if the Fisher relation holds ex ante. Klein (1975) nakes a siroilar polnt by showing that, because inflation was relatively unpredictable during the 1880 - 1915 gold staudard in the United States, there was only a weak ex post Fisher relation. Barthhold and Douga...|$|E
40|$|As {{opposed to}} the tropics, {{operational}} seasonal forecasting systems have shown little or no skill in European midlatitudes. In this paper we explore the potential source of predictability in this region given by El Niño¿Southern Oscillation (ENSO) events; in particular we analyze winter rainfall in Spain. First, we apply a simple statistical method to assess the teleconnections between rainfall records in 123 gauges over Spain and ENSO events during the last 40 years. A significant teleconnection for dry winter episodes is found associated with La Niña events, extending the results obtained in previous studies. Then, we adapt the statistical method to perform operational seasonal forecasts validation conditioned to ENSO events; in particular we consider a state-of-the-art operational model, the System 2 from ECMWF. The validation method defines a forecast interval {{to account for the}} ensemble spread, and applies a simple skill measure based on the proportion of hits (observations falling into the forecast interval) compared with a <b>random</b> <b>forecast.</b> As a result, we uncover the significant skill of operational seasonal predictions for reproducing the dry winter episodes associated with La Niña events (a window of opportunity for operational seasonal forecast in midlatitudes). Finally, the results are improved using statistical downscaling methods and some sensitivity studies are conducted. The analysis presented in this paper can be extended to other regions under the influence of any seasonal predictability-driving factor. The authors are also grateful to the Comisión Interministerial de Ciencia y Tecnología (CICYT, CGL 2004 - 02652 and CGL 2005 - 06966 -C 07 - 02 /CLI grants) for partial support of this work. Peer Reviewe...|$|E
30|$|Equation (20) is the {{theoretical}} {{basis of the}} data-driven method: the variance of the WGN is smaller than for the direct method. In this way, the <b>forecasting</b> <b>random</b> error can be limited and a more stable system load forecasting result {{can be obtained by}} the data-driven method. And this is exactly the value of using a large quantity of substation load data. Similarly, the proposed DLC method also takes advantage of the large dataset, so that its <b>random</b> <b>forecasting</b> error will be smaller than that of the direct method.|$|R
40|$|Purpose – The <b>random</b> walk <b>forecast</b> of {{exchange}} rate {{serves as a}} standard benchmark for forecast comparison. The {{purpose of this paper}} is to assess whether this benchmark is unbiased and directionally accurate under symmetric loss. The focus is on the <b>random</b> walk <b>forecasts</b> of the dollar/euro for 1999 - 2007 and the dollar/pound for 1971 - 2007. Design/methodology/approach – A forecasting framework to generate the one- to four-quarter-ahead <b>random</b> walk <b>forecasts</b> at varying lead times is designed. This allows to compare forecast accuracy at different lead times and forecast horizons. Using standard evaluation methods, this paper further evaluates these forecasts in terms of unbiasedness and directional accuracy. Findings – The paper shows that forecast accuracy improves with a reduction in the lead time but deteriorates with an increase in the forecast horizon. More importantly, the <b>random</b> walk <b>forecasts</b> are unbiased and accurately predict directional change under symmetric loss and thus are of value to a user who assigns similar cost to incorrect upward and downward move predictions in the exchange rates. Research limitations/implications – The one- to four-quarter-ahead <b>random</b> walk <b>forecasts</b> evaluated here are for averages of daily figures and not for the (end-of-quarter) rates in 3 -, 6 -, 9 - and 12 -months. Thus, the framework is of value to a market participant who is interested in forecasting quarterly average rates rather than the end-of-quarter rates. Originality/value – The exchange rate forecasting framework presented in this paper allows the evaluation of the <b>random</b> walk <b>forecasts</b> in terms of directional accuracy which (to the best of knowledge) has not been done before. Euro, Financial forecasting, Foreign exchange, Pound sterling, US dollar...|$|R
40|$|This study {{finds that}} {{expectations}} of two-week maintenance period average federal funds rates, {{as measured by}} the Money Market Services survey from March 1984 to November 1987, are biased, only marginally outperform <b>random</b> walk <b>forecasts,</b> and have forecast errors that are correlated with in-sample information. However, these results are tempered by findings that the survey forecasts incorporate the information in out-of-sample ARIMA forecasts, although the latter are less accurate than <b>random</b> walk <b>forecasts.</b> In addition, the accuracy of the survey forecasts increases for maintenance periods in which the discount rate changes. Copyright 1989 by Ohio State University Press. ...|$|R
40|$|An {{extremely}} simple univariate {{statistical model}} called IndOzy {{was developed to}} predict El Niño-Southern Oscillation (ENSO) events. The model uses five delayed-time inputs of the Niño 3. 4 sea surface temperature anomaly (SSTA) index to predict up to 12 months in advance. The prediction skill of the model was assessed using both short- and long-term indices and compared with other operational dynamical and statistical models. Using ENSO-CLIPER(climatology and persistence) as benchmark, only a few statistical models including IndOzy are considered skillful for short-range prediction. All models, however, do not differ significantly from the benchmark model at seasonal Lead- 3 - 6. None of the models show any skill, even against a no-skill <b>random</b> <b>forecast,</b> for seasonal Lead- 7. When using the Niño 3. 4 SSTA index from 1856 to 2005, the ultra simple IndOzy shows a useful prediction up to 4 months lead, and is slightly less skillful than the best dynamical model LDEO 5. That such a simple model such as IndOzy, which can be run {{in a few seconds}} on a standard office computer, can perform comparably with respect to the far more complicated models raises some philosophical questions about modelling extremely complicated systems such as ENSO. It seems evident that much of the complexity of many models does little to improve the accuracy of prediction. If larger and more complex models do not perform significantly better than an almost trivially simple model, then perhaps future models that use even larger data sets, and much greater computer power may not lead to significant improvements in both dynamical and statistical models. Investigating why simple models perform so well may help to point the way to improved models. For example, analysing dynamical models by successively stripping away their complexity can focus in on the most important parameters for a good prediction...|$|E
30|$|The models {{submitted}} to the 3 -month class were built on different earthquake generation hypotheses. All models used past and current seismicity to extrapolate into future earthquake rates. All software-codes were {{submitted to}} the Testing Center {{before the start of}} the testing experiment. For the first 3 -month period 1 November 2009 – 31 January 2010, the catalog with final solutions before 1 November 2009 was provided to the models by the Testing Center as the input dataset. For the “All Japan” region (Fig. 1), 9 models were submitted: HIST-ETAS 5 pa and HIST-ETAS 7 pa (Ogata, 2011), MARFS and MARFSTA (Smyth and Mori, 2011), Triple-S-Japan (Zechar and Jordan, 2010), and RI 10 k, R 30 k, RI 50 k, and RI 100 k (Nanjo, 2011). The 9 models for “Mainland” (Fig. 2) are EEPAS and PPE (Rhoades, 2011), MARFS, MARFSTA, Triple-S-Japan, RI 10 k, R 30 k, RI 50 k, and RI 100 k. The 7 “Kanto” models are (Fig. 3) HIST-ETAS 5 pa, HIST-ETAS 7 pa, Triple-S-Japan, RI 10 k, R 30 k, RI 50 k, and RI 100 k. Several, but not all, models were installed for multiple testing regions and will likely be installed later on in testing regions outside Japan. Testing models in multiple testing regions will lead to quicker assessments of the model performance. A brief description of each model is given by Nanjo et al. (2011). Figures 1 – 3 show maps of earthquake rates, λ (number / 3 months), for M ≥ 4 in the “All Japan,” “Mainland,” and “Kanto” regions, respectively. A reference <b>RANDOM</b> <b>forecast</b> model was included into each of the tests applied to the three testing regions. We randomized forecast rates of earthquakes by constraining the sum of the forecast rates to be equal to the total number of observed earthquakes, ignoring the Gutenberg-Richter relation (Gutenberg and Richter, 1944). Therefore, by definition, this is not an informative model to forecast locations and magnitudes of earthquakes but serves as the lowest baseline for comparison with other more meaningful models.|$|E
40|$|Predicting El Nino Southern Oscillation; {{comparing}} prediction {{skill of}} dynamical models and statistical modelsAn extremely simple univariate statistical model called ???IndOzy??? {{was developed to}} predict El Ni??o-Southern Oscillation (ENSO) events. The model uses five delayed-time inputs of the Ni??o 3. 4 sea surface temperature anomaly (SSTA) index to predict up to 12 months in advance. The prediction skill of the model was assessed using both short- and long-term indices and compared with other operational dynamical and statistical models. Using ENSO-CLIPER(climatology and persistence) as benchmark, only a few statistical models including IndOzy are considered skillful for short-range prediction. All models, however, do not differ significantly from the benchmark model at seasonal Lead- 3 ??? 6. None of the models show any skill, even against a no-skill <b>random</b> <b>forecast,</b> for seasonal Lead- 7. When using the Ni??o 3. 4 SSTA index from 1856 to 2005, the ultra simple IndOzy shows a useful prediction up to 4 months lead, and is slightly less skillful than the best dynamical model LDEO 5. That such a simple model such as IndOzy, which can be run {{in a few seconds}} on a standard office computer, can perform comparably with respect to the far more complicated models raises some philosophical questions about modelling extremely complicated systems such as ENSO. It seems evident that much of the complexity of many models does little to improve the accuracy of prediction. If larger and more complex models do not perform significantly better than an almost trivially simple model, then perhaps future models that use even larger data sets, and much greater computer power may not lead to significant improvements in both dynamical and statistical models. Investigating why simple models perform so well may help to point the way to improved models. For example, analysing dynamical models by successively stripping away their complexity can focus in on the most important parameters for a good prediction...|$|E
40|$|In {{this paper}} we propose a multivariate local predictor, {{inspired}} in the literature on deterministic chaos, and apply it to nine EMS currencies, using daily data for the January 1973 -December 1994 period. Our local predictors perform marginally better than a <b>random</b> walk in <b>forecasting</b> the nominal exchange rate, clearly outperforming the <b>random</b> walk directional <b>forecast.</b> ...|$|R
40|$|Term {{structure}} {{theory suggests}} that bond rates in efficient markets approximately follow a random walk. We show that the <b>random</b> walk <b>forecasts</b> of 10 -year U. S. Treasury and Moody's Aaa corporate bond rates for 1988 - 2005 are generally unbiased. Blue Chip forecasts, however, are both biased and inferior to <b>random</b> walk <b>forecasts.</b> Both models produce unbiased forecasts of the default spread, with the random walk again outperforming the Blue Chip. In addition, Blue Chip fails to accurately predict directional change. Emphasizing {{that the success of}} the random walk model is theoretically expected, we discuss why experts fail to beat random walk predictions. Blue Chip Term structure Bond market efficiency Rationality Directional accuracy...|$|R
40|$|The {{existence}} of risk premium {{is thought to}} be the reason why forward exchange rate is not an unbiased predictor of future spot exchange rate. In this paper we review two methodologies for inferring this unobserved risk premium based upon signal extraction mechanism. One approach relies on the theory of derivatives pricing that relates historical and risk neutral measures via market price of risk. The other approach specifies the risk premium in the historical measure directly. We compare these two methods in predicting future spot exchange rates and contrast these with that of <b>random</b> walk <b>forecast.</b> forward exchange rates; risk premium; signal extraction; market price of risk; future spot exchange rates; <b>random</b> walk <b>forecast.</b> ...|$|R
40|$|This report {{documents}} the Applied Meteorology Unit's {{evaluation of the}} Cell Trends display {{as a tool for}} radar operators to use in their evaluation of storm cell strength. The objective of the evaluation is to assess the utility of the WSR- 88 D graphical Cell Trends display for local radar cell interpretation in support of the 45 th Weather Squadron (45 WS), Spaceflight Meteorology Group (SMG), and National Weather Service (NWS) Melbourne (MLB) operational requirements. The analysis procedure was to identify each cell and track the maximum reflectivity, height of maximum reflectivity, storm top, storm base, hail and severe hail probability, cell-based Vertically Integrated Liquid (VIL) and core aspect ratio using WATADS Build 9. 0 cell trends information. One problem noted in the analysis phase was that the Storm Cell Identification and Tracking (SCIT) algorithm had a difficult time tracking the small cells associated with the Florida weather regimes. The analysis indicated numerous occasions when a cell track would end or an existing cell would be give a new ID in the middle of its life cycle. This investigation has found that most cells, which produce hail or microburst events, have discernable Cell Trends signatures. Forecasters should monitor the PUP's Cell Trends display for cells that show rapid (1 scan) changes in both the heights of maximum reflectivity and cell-based VIEL. It {{is important to note that}} this a very limited data set (four case days). Fifty-two storm cells were analyzed during those four days. The above mentioned t=ds, increase in the two cell attributes for hail events and decrease in the two cell attributes for wind events were noted in most of the cells. The probability of detection was 88 % for both events. The False Alarm Rate (FAR) was a 36 % for hail events and a respectable 25 % for microburst events. In addition the Heidke Skill Score (HSS) is 0. 65 for hail events and 0. 67 for microburst events. For <b>random</b> <b>forecast</b> the HSS is 0 and that a perfect score is 1...|$|E
40|$|In the {{forecasting}} of binary events, {{verification measures}} that are “equitable” were defined by Gandin and Murphy to satisfy two requirements: 1) they award all <b>random</b> <b>forecasting</b> systems, {{including those that}} always issue the same forecast, the same expected score (typically zero), and 2) they are expressible as the linear weighted sum {{of the elements of}} the contingency table, where the weights are independent of the entries in the table, apart from the base rate. The authors demonstrate that the widely used “equitable threat score” (ETS), as well as numerous others, satisfies neither of these requirements and only satisfies the first requirement in the limit of an infinite sample size. Such measures are referred to as “asymptotically equitable. ” In the case of ETS, the expected score of a <b>random</b> <b>forecasting</b> system is always positive and only falls below 0. 01 when the number of samples is greater than around 30. Two other asymptotically equitable measures are the odds ratio skill score and the symmetric extreme dependency score, which are more strongly inequitable than ETS, particularly for rare events; for example, when the base rate is 2 % and the sample size is 1000, <b>random</b> but unbiased <b>forecasting</b> systems yield an expected score of around − 0. 5, reducing in magnitude to − 0. 01 or smaller only for sample sizes exceeding 25 000. This presents a problem since these nonlinear measures have other desirable properties, in particular being reliable indicators of skill for rare events (provided that the sample size is large enough). A potential way to reconcile these properties with equitability is to recognize that Gandin and Murphy’s two requirements are independent, and the second can be safely discarded without losing the key advantages of equitability that are embodied in the first. This enables inequitable and asymptotically equitable measures to be scaled to make them equitable, while retaining their nonlinearity and other properties such as being reliable indicators of skill for rare events. It also opens up the possibility of designing new equitable verification measures...|$|R
40|$|This paper {{develops}} {{a new approach}} for producing probabilistic wind power forecasts using a single forecast. The Singular Spectrum Analysis technique is used as the forecasting technique. Given the confidence interval calculated for the single forecast, {{a large number of}} <b>random</b> <b>forecasts</b> were generated through the Monte Carlo method. The purpose of generating probabilistic wind power forecasts is to use the results in a stochastic programming unit commitment problem. Therefore, probabilistic forecasts are reduced to a small number of representative forecast scenarios by applying a scenario reduction algorithm. The stability of the scenario reduction algorithm is also evaluated. The results indicate that the insignificant changes of operational cost of the stochastic programming problem reflect a deletion of unimportant forecast scenario...|$|R
30|$|In this paper, a {{data-driven}} linear clustering (DLC) {{method is}} proposed {{to solve the}} long-term system load forecasting problem caused by load fluctuation in some developed cities. A large substation load dataset with annual interval is utilized and firstly preprocessed by the proposed linear clustering method to prepare for modelling. Then optimal autoregressive integrated moving average (ARIMA) models are constructed for the sum series of each obtained cluster to forecast their respective future load. Finally, the system load forecasting result is obtained by summing up all the ARIMA forecasts. From error analysis and application results, it is both theoretically and practically proved that the proposed DLC method can reduce <b>random</b> <b>forecasting</b> errors while guaranteeing modelling accuracy, so that a more stable and precise system load forecasting result can be obtained.|$|R
40|$|We {{propose a}} new test for {{superior}} predictive ability. The new test compares {{favorable to the}} reality check for data snooping (RC), because the former is more powerful and less sensitive to poor and irrelevant alternatives. The improvements are achieved by two modifi-cations of the RC. We employ a studentized test statistic that reduces the influence of erratic forecasts and we invoke a sample dependent null distribution. The advantages of the new test are confirmed by Monte Carlo experiments and in an empirical exercise, where we compare {{a large number of}} regression-based forecasts of annual US inflation to a simple <b>random</b> walk <b>forecast.</b> The <b>random</b> walk <b>forecast</b> is found to be inferior to regression-based forecasts and, interestingly, the best sample performance is achieved by models that have a Phillips curve structure...|$|R
40|$|In {{this paper}} {{a panel of}} vector error {{correction}} models based on a common long-run relationship is utilized to test whether the DM exchange rates of Canada, Japan and the United States comply in the long-run with a rational expectations-based monetary exchange rate model. Compared to existing coin-tegration frameworks our approach indicates that the aforementioned exchange rates are indeed consis-tent with the monetary exchange rate model based on a common long-run relationship. We also analyze the out-of-sample fit of this common long-run exchange rate model relative to naive <b>random</b> walk-based <b>forecasts</b> through several forecasting evaluation measures. These forecasting evaluations indicate that the monetary model-based common long-run model is superior to both <b>random</b> walk-based <b>forecasts</b> and standard cointegrated VAR model-based forecasts. Panel cointegration testing; nominal exchange rates; exchange rate predictability. ...|$|R
40|$|We {{consider}} {{implications of}} the Regional Earthquake Likelihood Models (RELM) test results with regard to earthquake forecasting. Prospective forecasts were solicited for M≥ 4. 95 earthquakes in California during the period 2006 – 2010. During this period 31 earthquakes occurred in the test region with M≥ 4. 95. We consider five forecasts that were submitted for the test. We compare the forecasts utilizing forecast verification methodology developed in the atmospheric sciences, specifically for tornadoes. We utilize a “skill score” based on the forecast scores λfi of occurrence of the test earthquakes. A perfect forecast would have λfi= 1, and a <b>random</b> (no skill) <b>forecast</b> would have λfi= 2. 86 × 10 - 3. The best forecasts (largest value of λfi) for the 31 earthquakes had values of λfi= 1. 24 × 10 - 1 to λfi= 5. 49 × 10 - 3. The best mean forecast for all earthquakes was λ̅f= 2. 84 × 10 - 2. The best forecasts are about {{an order of magnitude}} better than <b>random</b> <b>forecasts.</b> We discuss the earthquakes, the forecasts, and alternative methods of evaluation of the performance of RELM forecasts. We also discuss the relative merits of alarm-based versus probability-based forecasts...|$|R
40|$|Many {{authors have}} {{documented}} {{that it is}} challenging to explain exchange rate fluctuations with macroeconomic fundamentals: a <b>random</b> walk <b>forecasts</b> future exchange rates better than existing macroeconomic models. This paper applies newly developed tests for nested model that are robust to the presence of parameter instability. The empirical evidence shows that for some countries we can reject the hypothesis that exchange rates are random walks. This raises the possibility that economic models were previously rejected not because the fundamentals are completely unrelated to exchange rate fluctuations, but because the relationship is unstable over time and, thus, difficult to capture by Granger Causality tests or by forecast comparisons. We also analyze forecasts that exploit the time variation in the parameters and find that, in some cases, they can improve over the <b>random</b> walk. <b>forecasting,</b> exchange rates, parameter instability, random walks...|$|R
40|$|An {{emerging}} market or market segment provides firms both {{the opportunity to}} enter early and capture market share and also {{the risk that the}} market {{will turn out to be}} less fruitful than expected. We formulate and analyze a game-theoretic model in which multiple firms with uncertain and/or disparate beliefs about the eventual market size decide whether to enter such a market. For increasingly general models, we show that the structure (i. e. the number and identity of participating firms) and profitability of equilibrium oligopolies can be determined by a classification schemes based on the firms' beliefs about the viable level of market concentration. This scheme is adapted to <b>random</b> <b>forecasts</b> (i. e. forecasts expressed as probability distributions) as well as point forecasts. This study was motivated by managerial issues encountered by a client firm engaged in semiconductor design...|$|R
40|$|We use {{oil price}} {{forecasts}} from the Consensus Economic Forecast poll {{for the time}} period Oct. 1989 – Dec. 2008 to analyze how forecasters form their expectations. Our findings indicate that the extrapolative {{as well as the}} regressive expectation formation hypothesis play a role. Standard measures of forecast accuracy reveal forecasters’ underperformance relative to the random walk benchmark. We test the hypothesis of rational expectations by relying on the criteria of unbiasedness and orthogonality. Although both conditions are met, the forecast accuracy is significantly lower compared to naive <b>random</b> walk <b>forecast.</b> The forecasters have problems to forecast the trends in the oil price. The recent roller-coaster movements in the international oil market have revealed forecasters’ inability to predict major trends in the spot oil price. As a consequence, some research institutes have stopped forecasting the oil price as an ingredient of their macroeconomic models and use a <b>random</b> walk <b>forecast</b> instead. Oil price, forecast heterogeneity, survey data...|$|R
40|$|This paper {{presents}} a particle method designed for high-dimensional state estimation. Instead of weighing <b>random</b> <b>forecasts</b> by their distance to given observations, the method samples an ensemble of particles around an optimal solution {{based on the}} observations (i. e., it is implicit). It differs from other implicit methods because it includes the state at the previous assimilation time {{as part of the}} optimal solution (i. e., it is a lag- 1 smoother). This is accomplished {{through the use of a}} mixture model for the background distribution of the previous state. In a high-dimensional, linear, Gaussian example, the mixture-based implicit particle smoother does not collapse. Furthermore, using only a small number of particles, the implicit approach is able to detect transitions in two nonlinear, multi-dimensional generalizations of a double-well. Adding a step that trains the sampled distribution to the target distribution prevents collapse during the transitions, which are strongly nonlinear events. To produce similar estimates, other approaches require many more particles...|$|R
40|$|In {{this paper}} {{a panel of}} vector error {{correction}} models based on a common long-run relationship is utilized to test whether the Euro exchange rates of Canada, Japan and the United States have a long-run link with monetary fundamentals. We use both exchange relationships relative to the full EMU area (with synthetic aggregates for the pre-EMU period) and relative to Germany solely. Compared to existing cointegration frameworks our approach provides more evidence that the aforementioned exchange rates are consistent with a rational expectations-based monetary exchange rate model based on a common long-run relationship, albeit with a longrun impact of relative income that is higher than predicted by the theory. As a next step we analyze the out-of-sample fit of this common long-run exchange rate model relative to naive <b>random</b> walk-based <b>forecasts.</b> These forecasting evaluations indicate that the monetary fundamentals-based common long-run model is superior to both <b>random</b> walk-based <b>forecasts</b> and standard cointegrated VAR model-based forecasts, especially at horizons of 2 to 4 years...|$|R
40|$|We {{show that}} the one-year-ahead {{forecast}} of growth in US corporate profits from the Survey of Professional Forecasters, for 1983 - 2004, is unbiased and superior to the <b>random</b> walk <b>forecast.</b> Survey respondents, however, generally predicted positive growth and thus failed to accurately predict negative growth in corporate profits. This bias {{may be due to}} respondents assigning asymmetric costs to incorrect positive and negative growth predictions. ...|$|R
40|$|Abstract: The paper {{introduces}} the model confidence set (MCS) and applies {{it to the}} selection of forecasting models. An MCS {{is a set of}} models that is constructed so that it will contain the “best” forecasting model, given a level of confidence. Thus, an MCS is analogous to a confidence interval for a parameter. The MCS acknowledges the limitations of the data so that uninformative data yield an MCS with many models, whereas informative data yield an MCS with only a few models. We revisit the empirical application in Stock and Watson (1999) and apply the MCS procedure to their set of inflation forecasts. In the first pre- 1984 subsample we obtain an MCS that contains only a few models, notably versions of the Solow-Gordon Phillips curve. On the other hand, the second post- 1984 subsample contains little information and results in a large MCS. Yet, the <b>random</b> walk <b>forecast</b> is not contained in the MCS for either of the samples. This outcome shows that the <b>random</b> walk <b>forecast</b> is inferior to inflation forecasts based on Phillips curve-like relationships...|$|R
40|$|This paper {{assesses the}} value of panel time series models in {{forecasting}} sovereign default. A jacknife in-sample procedure is utilized to obtain a reduced regressor set which includes measures of US macroeconomic un-certainty, US monetary policy uncertainty and a proxy for US risk aversion towards emerging market assets. The paper explores the contentious as-pect of whether controlling for time series and/or country heterogeneity is important in forecasting emerging market default. To this end, it uses conventional inference methods alongside forecasting performance statis-tics based on both statistical- and economic-loss functions. The latter in-clude various approaches pertaining to event/probability forecasts, which are modi…ed to accomodate the persistence of sovereign debt states. In this context, the naive <b>random</b> walk <b>forecasts</b> can be rather di¢cult to beat and so we complement our forecast evaluation framewok with naive benchmark comparisons. For the latter we use a <b>random</b> walk <b>forecast,</b> a naive probability forecast proposed in the paper and Pesaran and Tim-mermann test statistics. Diebold-Mariano tests are also deployed to assess the signi…cance of the forecast accuracy di¤erential across models...|$|R
40|$|Copyright Â© 2010 American Meteorological Society (AMS). Permission to use figures, tables, {{and brief}} {{excerpts}} from this work in scientific and educational works is hereby granted {{provided that the}} source is acknowledged. Any use of material in this work that is determined to be â 8 ̆ 09 ̆cfair useâ 8 ̆ 09 ̆d under Section 107 of the U. S. Copyright Act September 2010 Page 2 or that satisfies the conditions specified in Section 108 of the U. S. Copyright Act (17 USC Â§ 108, as revised by P. L. 94 - 553) {{does not require the}} AMSâ 8 ̆ 09 ̆ 9 s permission. Republication, systematic reproduction, posting in electronic form, such as on a web site or in a searchable database, or other uses of this material, except as exempted by the above statement, requires written permission or a license from the AMS. Additional details are provided in the AMS Copyright Policy, available on the AMS Web site located at ([URL] or from the AMS at 617 - 227 - 2425 or copyrights@ametsoc. org. In the forecasting of binary events, verification measures that are â 8 ̆ 09 ̆cequitableâ 8 ̆ 09 ̆d were defined by Gandin and Murphy to satisfy two requirements: 1) they award all <b>random</b> <b>forecasting</b> systems, including those that always issue the same forecast, the same expected score (typically zero), and 2) they are expressible as the linear weighted sum of the elements of the contingency table, where the weights are independent of the entries in the table, apart from the base rate. The authors demonstrate that the widely used â 8 ̆ 09 ̆cequitable threat scoreâ 8 ̆ 09 ̆d (ETS), as well as numerous others, satisfies neither of these requirements and only satisfies the first requirement in the limit of an infinite sample size. Such measures are referred to as â 8 ̆ 09 ̆casymptotically equitable. â 8 ̆ 09 ̆d In the case of ETS, the expected score of a <b>random</b> <b>forecasting</b> system is always positive and only falls below 0. 01 when the number of samples is greater than around 30. Two other asymptotically equitable measures are the odds ratio skill score and the symmetric extreme dependency score, which are more strongly inequitable than ETS, particularly for rare events; for example, when the base rate is 2...|$|R
40|$|We use trade size to {{distinguish}} between individuals and institutions and then examine their trading behaviors around earnings announcements {{using data from the}} Tokyo Stock Exchange. Japanese listed firms have a distinctive financial reporting system in that they report actual earnings for prior and current years, and in addition, almost all of them release management earnings forecasts for the next year. Under this unique setting, we test whether individuals respond differently from institutions to the same earnings news. We document the following results: (1) With regard to current earnings, individuals (institutions) strongly respond to simplistic <b>random</b> walk <b>forecast</b> errors (analyst forecast errors), while do not always respond to analyst <b>forecast</b> errors (simplistic <b>random</b> walk <b>forecast</b> errors). (2) With regard to management earnings forecasts, both individuals and institutions use them, but individuals react to them literally. In contrast to na¨ıve trading by individuals, institutions rationally respond to them with their predicted optimistic bias in mind. Overall, our results suggest that individuals' trading is so na¨ıve as if they use nothing other than the information released at the time of earning announcement, while institutions' trading is so sophisticated. ...|$|R
40|$|We {{develop a}} fast {{algorithm}} for Kalman Filter {{applied to the}} <b>random</b> walk <b>forecast</b> model. The key idea is an efficient representation of the estimate covariance matrix at each time-step as a weighted sum of two contributions - the process noise covariance matrix and a low rank term computed from a generalized eigenvalue problem, which combines information from the noise covariance matrix and the data. We describe an efficient algorithm to update the weights of the above terms and the computation of eigenmodes of the generalized eigenvalue problem (GEP). The resulting algorithm for the Kalman filter with a <b>random</b> walk <b>forecast</b> model scales as (N) in memory and (N N) in computational cost, where N {{is the number of}} grid points. We show how to efficiently compute measures of uncertainty and conditional realizations from the state distribution at each time step. An extension to the case with nonlinear measurement operators is also discussed. Numerical experiments demonstrate the performance of our algorithms, which are applied to a synthetic example from monitoring CO_ 2 in the subsurface using travel time tomography. Comment: published in Inverse Problems, 2015 31 01500...|$|R
