167|269|Public
2500|$|This {{suggests}} that we create a conditional prior of the mean on the unknown variance, with a hyperparameter specifying {{the mean of the}} pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations. [...] This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter. [...] The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations. [...] Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the <b>relative</b> <b>variance</b> of that prior. [...] These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.|$|E
5000|$|In this situation, the {{particle}} approximations {{of the likelihood}} functions are unbiased and the <b>relative</b> <b>variance</b> is controlled by ...|$|E
5000|$|In 1936 Clapham {{proposed}} {{using the}} ratio of the variance to the mean as a test statistic (the <b>relative</b> <b>variance).</b> In symbols ...|$|E
30|$|<b>Relative</b> Rank <b>Variance.</b>|$|R
5000|$|A {{potential}} rebalancing bonus {{is determined}} by two assets' <b>relative</b> <b>variances</b> and covariance. These metrics are developed by averaging historical returns, which are no guarantee of future results {{in the short term}} or long term. E.g. debt is traditionally thought to be negatively correlated to equities, but during the 'Great Moderation' they were positively correlated.|$|R
40|$|The <b>relative</b> frailty <b>variance</b> among {{survivors}} {{provides a}} readily interpretable {{measure of how}} the heterogeneity of a population, as represented by a frailty model, evolves over time. We discuss {{the properties of the}} <b>relative</b> frailty <b>variance,</b> show that it characterizes frailty distributions and that, suitably rescaled, it may be used to compare patterns of dependence across models and data sets. In shared frailty models, the <b>relative</b> frailty <b>variance</b> is closely related to the cross-ratio function, which is estimable from bivariate survival data. We investigate the possible shapes of the <b>relative</b> frailty <b>variance</b> function for the purpose of model selection, and we review available frailty distribution families in this context. We introduce several new families with contrasting properties, including simple but flexible time varying frailty models. The benefits of the approach that we propose are illustrated with two applications to bivariate current status data obtained from serological surveys...|$|R
5000|$|This {{suggests}} that we create a conditional prior of the mean on the unknown variance, with a hyperparameter specifying {{the mean of the}} pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations. This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter. The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations. Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the <b>relative</b> <b>variance</b> of that prior. These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.|$|E
5000|$|Frequently {{in physics}} one comes across {{situations}} where quenched randomness {{plays an important}} role. Any physical property X of such a system, would require an averaging over all disorder realisations. The system can be completely described by the average X where ... denotes averaging over realisations (“averaging over samples”) provided the <b>relative</b> <b>variance</b> RX = VX / X2 → 0 as N→∞, where VX = X2 &minus; X2 and N denotes {{the size of the}} realisation. In such a scenario a single large system is sufficient to represent the whole ensemble. Such quantities are called self-averaging. Away from criticality, when the larger lattice is built from smaller blocks, then due to the additivity property of an extensive quantity, the central limit theorem guarantees that RX ~ N&minus;1 thereby ensuring self-averaging. On the other hand, at the critical point, the question whether [...] is self-averaging or not becomes nontrivial, due to long range correlations.|$|E
40|$|The {{intention}} of this master‘s thesis is the measurment of <b>relative</b> <b>variance</b> of optical intensity. In the first place, I {{have been studied}} Kolmogorov cascade theory of turbulence and <b>relative</b> <b>variance</b> of optical intensity. In addition, I have been deal with turbulent cells, Gaussian beam, Top Hat beam and influence of the atmospheric turbulences on the intensity profile of the laser beam. Lastly I have been measured determine influence of the atmospheric turbulences on the intensity profile of the laser beam and I have been calculated <b>relative</b> <b>variance</b> of optical intensity. I have suggested the optimum beam profile in the turbulent atmosphere from acquired data...|$|E
40|$|A {{model of}} {{interest}} rate movements {{in response to}} new information on the money stock is developed. The model, which incorporates several earlier approaches as special cases, makes explicit {{the manner in which}} estimated interest rate responses to money surprises depend on the <b>relative</b> <b>variances</b> of nominal and real disturbances, {{as well as on the}} monetary authority's policy and the credibility of that policy. ...|$|R
40|$|The authors present {{empirical}} {{tests of}} two hypothesis derived {{from an open}} economy version of the multimarket equilibrium model. The first hypothesis holds that the conditional <b>relative</b> price <b>variance</b> is positively related to the variance of domestic monetary and foreign nominal and real shocks. The second hypothesis claims a negative impact {{of an increase in}} conditional <b>relative</b> price <b>variance</b> on normal output. In contrast to earlier studies, the authors use direct measures of conditional <b>relative</b> price <b>variance</b> based on ARCH estimates. The authors' results show that both hypotheses are consistent with German data. Copyright 1991 by Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association. ...|$|R
3000|$|... 11 To {{the extent}} that the {{measurement}} error is correlated with the true net replacement rate, the bias may be larger or smaller than the attenuation bias with randomly distributed errors. This depends on the sign of the correlation and the relative magnitudes of the variances of the measurement error and the actual replacement rate (See e.g. Pischke [2007]). Not much can be said about possible correlations and <b>relative</b> <b>variances</b> in our data.|$|R
40|$|This {{dissertation}} {{is focused}} on the determination of turbulent atmospheric transmission media properties. The concept of new method for turbulent attenuation designation is presented – the method of available power. This method comes from laser beam intensity profile analysis. The next point of this work is <b>relative</b> <b>variance</b> of optical intensity study. On the basis of experimental measurements <b>relative</b> <b>variance</b> of optical intensity is non-uniformly distributed in laser beam intensity profile. This non uniform distribution is subordinated to angular coefficient of intensity profile curve. Due to this information we can determine optimal laser beam shape to minimize influence of turbulent atmosphere...|$|E
3000|$|... b is {{expected}} to be large, based on the broad distribution of P_O_ 2 levels in tumours: [...] (.σ _P_O_ 2 /P_O_ 2)≳ 1 [23]. In contrast, the <b>relative</b> <b>variance</b> in k [...]...|$|E
40|$|In a 1976 paper {{published}} in Science, Knuth presented an algorithm to sample (non-uniform) self-avoiding walks crossing {{a square of}} side k. From this sample, he constructed an estimator {{for the number of}} such walks. The quality of this estimator {{is directly related to the}} (<b>relative)</b> <b>variance</b> of a certain random variable X_k. From his experiments, Knuth suspected that this variance was extremely large (so that the estimator would not be very efficient). But how large? For the analogous Rosenbluth algorithm, which samples unconfined self-avoiding walks of length n, the variance of the corresponding estimator is believed to be exponential in n. A few years ago, Bassetti and Diaconis showed that, for a sampler \`a la Knuth, that generates walks crossing a k\times k square and consisting of North and East steps, the <b>relative</b> <b>variance</b> is only O(\sqrt k). In this note we take one step further and show that, for walks consisting of North, South and East steps, the <b>relative</b> <b>variance</b> jumps to 2 ^{k(k+ 1) }/(k+ 1) ^{ 2 k}. This is quasi-exponential in the average length of the walks, which is of order k^ 2. We also obtain partial results for general self-avoiding walks crossing a square, suggesting that the <b>relative</b> <b>variance</b> could be exponential in k^ 2 (which is again the average length of these walks). Knuth's algorithm is a basic example of a widely used technique called sequential importance sampling. The present paper, following Bassetti and Diaconis' paper, is one of very few examples where the variance of the estimator can be found. Comment: 19 page...|$|E
40|$|When the {{introduction}} of a prism makes visually guided reaching biased and inaccurate, adaptation occurs to restore accuracy. Bias in everyday life usually accumulates from a series of small changes, a process well simulated with a random walk. If bias were the only source of error, recalibration would be simple: correct the last error. But reaching behavior is also subject to random error. How does the visuo-motor system balance the need to filter random error with the need to adapt to time-varying bias? We investigated whether the Kalman filter, the optimal algorithm for this problem, models the dynamics of visuo-motor adaptation. The filter predicts that adaptation rate will be determined by the <b>relative</b> <b>variances</b> of current measurements and changing bias: rate should decrease with feedback variance and increase with variance in bias. Subjects pointed rapidly with an unseen hand to a brief visual target. Visual feedback indicated the endpoint of the motor movement. Feedback variance was increased with blur. The relationship between visual feedback location and the movement endpoint was altered with a random walk. Trial-by-trial pointing was measured. Subjects performed {{with a high level of}} efficiency and responded to changes in <b>relative</b> <b>variances</b> as predicted by the Kalman filter...|$|R
40|$|Abstract: The study {{proposes a}} multivariate unobserved {{components}} model {{in order to}} examine relationships at business cycle frequencies among macroeconomic variables. The series are decomposed into non-stationary trends, stationary cycles, and an irregular component. The co-movements among the particular cycles are modelled by a latent factor, whose dynamics is governed by a stochastic cycle. As a consequence of certain symmetry properties of the latter cyclical co-movement can be parametrized in terms of <b>relative</b> <b>variances,</b> phase shifts, and coherence. The model is applied to a U. S. labour market data set. ...|$|R
40|$|The study {{proposes a}} multivariate unobserved {{components}} model {{in order to}} examine relationships at business cycle frequencies among macroeconomic variables. The series are decomposed into non-stationary trends, stationary cycles, and an irregular component. The co-movements among the particular cycles are modelled by a latent factor, whose dynamics is governed by a stochastic cycle. As a consequence of certain symmetry properties of the latter cyclical co-movement can be parametrized in terms of <b>relative</b> <b>variances,</b> phase shifts, and coherence. The model is applied to a U. S. labour market data set. Unobserved Components Models, Business Cycles, Labour, Markets...|$|R
3000|$|... eq. Assuming {{that the}} <b>relative</b> <b>variance</b> [...] (.σ _k_b /k_b) {{is equal to}} that for the oxygen partial {{pressure}} P_O_ 2 (the case, e.g., when the two are related by a Michaelis-Menten-type relation [12]), the variance in k [...]...|$|E
30|$|We also {{described}} the polydisperse {{case of the}} fat fractal model. Here, the sizes of the composing units obey, as an example, a log-normal distribution function. We obtained smoothed curves for the scattering intensities and structure factors. The monodisperse scattering curves {{as well as the}} polydisperse ones, with small enough values of the <b>relative</b> <b>variance,</b> allow to obtain the scaling factors at each structural level, while the scattering exponents in the polydisperse curve give the fractal dimensions at each structural level. The chosen value of 0.4 for the <b>relative</b> <b>variance</b> is meant to illustrate the case in which one can still observe some minima in the scattering characteristics, and the curves still retain a shape close to power-law decays.|$|E
40|$|We {{investigate}} the instabilities of the Mott-insulating {{phase of the}} weakly disordered Bose-Hubbard model within a renormalization group analysis of the replica field theory obtained by a strong-coupling expansion around the atomic limit. We identify a new order parameter and associated correlation length scale {{that is capable of}} capturing the transition from a state with zero compressibility, the Mott insulator, to one in which the compressibility is finite, the Bose glass. The order parameter is the <b>relative</b> <b>variance</b> of the disorder-induced mass distribution. In the Mott insulator, the <b>relative</b> <b>variance</b> renormalizes to zero, whereas it diverges in the Bose glass. The divergence of the <b>relative</b> <b>variance</b> signals the breakdown of self-averaging. The length scale governing the breakdown of self-averaging is the distance between rare regions. This length scale is finite in the Bose glass but diverges at the transition to the Mott insulator with an exponent of ν= 1 /D for incommensurate fillings. Likewise, the compressibility vanishes with an exponent of γ= 4 /D- 1 at the transition. At commensurate fillings, the transition is controlled by a different fixed point at which both the disorder and interaction vertices are relevant. Comment: Extended, published versio...|$|E
40|$|Background: A {{modified}} version of Postural Assessment Scale for Stroke Patients (PASS) was created with {{some changes in the}} description of the items and clarifications in the manual (e. g. much help was defined as support from 2 persons). The aim of this validation study was to assess intrarater and interrater reliability using this {{modified version}} of PASS, at a stroke unit, for patients in the acute phase after their first event of stroke. Methods: In the intrarater reliability study 114 patients and in the interrater reliability study 15 patients were examined twice with the test within one to 24 hours in the first week after stroke. Spearman’s rank correlation, Kappa coefficients, Percentage Agreement and the newer rank-invariant methods; Relative Position, Relative Concentration and <b>Relative</b> rank <b>Variance</b> were used for the statistical analysis. Results: For the intrarater reliability Spearman’s rank correlations were 0. 88 - 0. 98 and k were 0. 70 - 0. 93 for the individual items. Small, statistically significant, differences were found for two items regarding Relative Position and for one item regarding Relative Concentration. There was no <b>Relative</b> rank <b>Variance</b> for any single item. For the interrater reliability, Spearman’s rank correlations were 0. 77 - 0. 99 for individual items. For some items there was a possible, even if not proved, reliability problem regarding Relative Position and Relative Concentration. There was no <b>Relative</b> rank <b>Variance</b> for the single items, except for a small <b>Relative</b> rank <b>Variance</b> for one item...|$|R
40|$|This paper {{analyzes}} {{the effects of}} monetary and fiscal policy shocks on the term structure of interest rates. Temporary versus permanent, unanticipated versus anticipated policy disturbances and the responses of long versus short, and real versus nominal rates are contrasted. The main results are summarized {{in a series of}} propositions. Among them, the finding that an unanticipated permanent fiscal expansion impacts more on long-term rates may help explain their observed excessive volatility. The effects of structural changes on the <b>relative</b> <b>variances</b> are also discussed, with the effect that operates through the impact on private speculative behavior being emphasized. Copyright 1989 by Ohio State University Press. ...|$|R
30|$|The {{pattern of}} random {{differences}} was quantified using the variable of <b>relative</b> rank <b>variance</b> (RV). Random errors could {{be caused by}} guessing or loss of concentration. The possible values for RV range from 0 to 1, with 0 indicating no random contribution.|$|R
40|$|The {{comparative}} {{ability of}} transcriptional and small RNA-mediated negative feedback to control fluctu-ations or ‘noise ’ in gene expression remains unex-plored. Both autoregulatory mechanisms usually suppress the average (mean) of the protein level and its variability across cells. The variance {{of the number}} of proteins per molecule of mean expres-sion is also typically reduced compared with the unregulated system, but is almost never below the value of one. This <b>relative</b> <b>variance</b> often substan-tially exceeds a recently obtained, theoretical lower limit for biochemical feedback systems. Adding the transcriptional or small RNA-mediated control has different effects. Transcriptional autorepression robustly reduces both the <b>relative</b> <b>variance</b> and persistence (lifetime) of fluctuations. Both benefits combine to reduce noise in downstream gene expression. Autorepression via small RNA can achieve more extreme noise reduction and typically has less effect on the mean expression level. However, it is often more costly to implement and is more sensitive to rate parameters. Theoretical lower limits on the <b>relative</b> <b>variance</b> are known to decrease slowly as a measure of the cost per molecule of mean expression increases. However, the proportional increase in cost to achieve sub-stantial noise suppression can be different away from the optimal frontier—for transcriptional autorepression, it is frequently negligible...|$|E
30|$|In Eq. (8), the {{variance}} values are subtracted from 1 to convert <b>relative</b> <b>variance</b> values into relative consistency values. The resultant values are {{again in the}} range of [0, 1]. The values closer to 1 indicates relatively higher consistent motion, and the values closer to 0 indicates relatively lower consistent motion.|$|E
40|$|This is {{the final}} version of the article. Available from OUP via the DOI in this record. The {{comparative}} ability of transcriptional and small RNA-mediated negative feedback to control fluctuations or 'noise' in gene expression remains unexplored. Both autoregulatory mechanisms usually suppress the average (mean) of the protein level and its variability across cells. The variance of the number of proteins per molecule of mean expression is also typically reduced compared with the unregulated system, but is almost never below the value of one. This <b>relative</b> <b>variance</b> often substantially exceeds a recently obtained, theoretical lower limit for biochemical feedback systems. Adding the transcriptional or small RNA-mediated control has different effects. Transcriptional autorepression robustly reduces both the <b>relative</b> <b>variance</b> and persistence (lifetime) of fluctuations. Both benefits combine to reduce noise in downstream gene expression. Autorepression via small RNA can achieve more extreme noise reduction and typically has less effect on the mean expression level. However, it is often more costly to implement and is more sensitive to rate parameters. Theoretical lower limits on the <b>relative</b> <b>variance</b> are known to decrease slowly as a measure of the cost per molecule of mean expression increases. However, the proportional increase in cost to achieve substantial noise suppression can be different away from the optimal frontier-for transcriptional autorepression, it is frequently negligible. Funding for open access charge: MRC-EPSRC funded Fellowship in Bioinformatics (to C. G. B.) ...|$|E
40|$|This paper {{examines}} the {{bias in the}} OLS estimators when the regressors have measurement errors correlated in a particular manner. When a variable is decomposed into two components but {{only one of them}} is observed with error, the induced measurement error in the other component is identical but has the opposite sign. This specific correlation pattern enables us to assess the direction of the bias in the OLS estimators from observed data. In the standard EIV case this would require knowledge of the <b>relative</b> <b>variances</b> of the measurement errors. Examples of this type of decomposition in applied work are presented. Copyright 1993 by Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association. ...|$|R
40|$|The Harris-Aharony {{criterion}} for a statistical model predicts, {{that if a}} specific heat exponent α> 0, then this model does not exhibit self-averaging. In two-dimensional percolation model the index α=- 1 / 2. It means that, {{in accordance with the}} Harris-Aharony criterion, the model can exhibit self-averaging properties. We study numerically the <b>relative</b> <b>variances</b> R_M and R_χ for the probability M of a site belongin to the "infinite" (maximum) cluster and the mean finite cluster size χ. It was shown, that two-dimensional site-bound percolation on the square lattice, where the bonds play the role of impurity and the sites play the role of the statistical ensemble, over which the averaging is performed, exhibits self-averaging properties. Comment: 15 pages, 5 figure...|$|R
40|$|This paper modifies the menu-cost {{model that}} Ball and Mankiw (1995) put forward {{to explain the}} {{correlation}} between the first- and higher-moments of the distribution of US price changes by allowing for non-zero trend inflation. Simulations suggest that even if trend inflation is only mildly positive - such as the 3 percent per annum experienced by the US in the last 50 years - the predictions of the Ball and Mankiw model are greatly altered. We then show that some of these predictions are rejected by annual post-WW 2 US data. Inflation, menu-cost, <b>relative</b> price <b>variance,</b> <b>relative</b> price skewness, skew-normal. ...|$|R
40|$|We {{study the}} {{dynamics}} of the q-state random bond Potts ferromagnet on the square lattice at its critical point by Monte Carlo simulations with single spin-flip dynamics. We concentrate on q= 3 and q= 24 and find, in both cases, conventional, rather than activated, dynamics. We also look at the distribution of relaxation times among different samples, finding different results for the two q values. For q= 3 the <b>relative</b> <b>variance</b> of the relaxation time tau at the critical point is finite. However, for q= 24 this appears to diverge in the thermodynamic limit and it is ln(tau) which has a finite <b>relative</b> <b>variance.</b> We speculate that this difference occurs because the transition of the corresponding pure system is second order for q= 3 but first order for q= 24. Comment: 9 pages, 13 figures, final published versio...|$|E
40|$|Artificial {{data were}} {{simulated}} by using two-exponential functions and normally distributed pseudo-random numbers. The variation corresponded to either constant or <b>relative</b> <b>variance</b> {{for the data}} error. These data were used to test (i) three different weighting functions and (ii) the effect of data truncation on the precision of estimating the parameters of two-exponential functions...|$|E
40|$|This {{empirical}} study {{evaluates the}} application of Generalized Variance Functions (GVFs) for binomial variables in the 1998 Indonesian Labor Force Survey. The survey employs stratified two-stage cluster sampling for selecting samples from a population of households. The study covers all provinces in Java to produce estimates {{at the level of}} Java Island. The <b>relative</b> <b>variance</b> estimates resulted from the GVF models are compared to the <b>relative</b> <b>variance</b> estimates which are computed directly. The results illustrate that model dŶcu ̂ expressed by logarithmic model log u ̂ = log c + d log (Y ̂) gives a good approximation to estimate the variances for the nonagricultural employment group, especially for working male category both in urban and rural areas. It is also good for the total employment group differentiated by age group, educational attainment, and employment status. On the other hand, the model gives poor results for the agricultural employment group...|$|E
40|$|AbstractAvalanche {{fluctuations}} set a {{limit to}} the energy and position resolutions that {{can be reached by}} gaseous detectors. This paper presents a method based on a laser test-bench to measure the absolute gain and the <b>relative</b> gain <b>variance</b> of a Micro-Pattern Gaseous Detector from its single-electron response. A Micromegas detector was operated with three binary gas mixtures, composed of 5 % isobutane as a quencher, with argon, neon or helium, at atmospheric pressure. The anode signals were read out by low-noise, high-gain Cremat CR- 110 charge preamplifiers to enable single-electron detection down to gain of 5 × 103 for the first time. The argon mixture shows the lowest gain at a given amplification field together with the lowest breakdown limit, which is at a gain of 2 × 104 an order of magnitude lower than that of neon or helium. For each gas, the <b>relative</b> gain <b>variance</b> f is almost unchanged in the range of amplification field studied. It was found that f is twice higher (f~ 0. 6) in argon than in the two other mixtures. This hierarchy of gain and <b>relative</b> gain <b>variance</b> agrees with predictions of analytic models, based on gas ionisation yields, and a Monte-Carlo model included in the simulation software Magboltz version 10. 1...|$|R
40|$|Estimates of variances due to {{additive}} and dominance {{genetic effects}} and permanent and temporary environmental effects {{were obtained for}} ovulation and twinning rates from a composite population selected for twinning rate. Measures of ovulation rate after 11 mo of age on 2, 317 heifers {{with a total of}} 19, 209 measures were used. Twinning measures were on 1, 522 first-parity cows, 1, 311 later-parity cows with a total of 3, 571 measures, and 1, 704 all-parity cows with 5, 100 measures. Models included fixed effects of year-season-age at calving for twinning, and year-season of birth, age in months, and calendar month of measurement for ovulation rate. Four analyses were performed for each sample: combinations of models with and without dominance effects and with and without covariates for fractions of inheritance from the seven foundation groups. Variance components as fractions of phenotypic variance for analysis of all ovulation rate measures were. 076,. 000, and. 045 for additive, dominance, and permanent environmental effects with no foundation groups in the model and. 069,. 000, and. 050 with foundation groups in the model. For sums of eight measures, the estimates were. 287 and. 000 for <b>relative</b> <b>variances</b> of additive and dominance effects with groups in the model and. 316 and. 000 with groups ignored. For twinning rate for first parity, estimates were. 126 and. 209 for <b>relative</b> <b>variances</b> of additive and dominance effects; for later parities, estimates were. 045 and. 035 for models including foundation group effects. The results suggest lack of dominance effects in expression of ovulation rate and the possibility of dominance effects for embryo and(or) fetal survival or conception rate because twinning rate is a function of ovulation, conception, and embryo and(or) fetal survival rates...|$|R
40|$|We {{study the}} {{evolution}} of the distribution of eigenvalues of a N × N matrix ensemble subject to a change of variances of its matrix elements. Our results indicate that {{the evolution of}} the probability density is governed by a Fokker-Planck equation similar to the one governing the time-evolution of the particle-distribution in Wigner-Dyson gas, with <b>relative</b> <b>variances</b> now playing the role of time. This is also similar to the Fokker-Planck equation for the distribution of eigenvalues of a N × N matrix subject to a random perturbation taken from the standard Gaussian ensembles with perturbation strength as the ”time ” variable. This equivalence alongwith the already known correlations of standard Gaussian ensembles can therefore help us to obtain the correlations for various physically-significant cases modeled by random banded Gaussian ensembles...|$|R
