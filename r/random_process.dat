3144|4355|Public
5|$|A {{stochastic}} or <b>random</b> <b>process</b> can {{be defined}} as a collection of random variables that is indexed by some mathematical set, meaning that each random variable of the stochastic process is uniquely associated with an element in the set. The set used to index the random variables is called the index set. Historically, the index set was some subset of the real line, such as the natural numbers, giving the index set the interpretation of time. Each random variable in the collection takes values from the same mathematical space known as the state space. This state space can be, for example, the integers, the real line or -dimensional Euclidean space. An increment is the amount that a stochastic process changes between two index values, often interpreted as two points in time. A stochastic process can have many outcomes, due to its randomness, and a single outcome of a stochastic process is called, among other names, a sample function or realization.|$|E
5|$|From 1920 John Carson, {{also working}} for AT, {{began to develop}} a new way of looking at signals using the {{operational}} calculus of Heaviside which in essence is working in the frequency domain. This gave the AT engineers a new insight into the way their filters were working and led Otto Zobel to invent many improved forms. Carson and Zobel steadily demolished many of the old ideas. For instance the old telegraph engineers thought of the signal as being a single frequency and this idea persisted into the age of radio with some still believing that frequency modulation (FM) transmission could be achieved with a smaller bandwidth than the baseband signal right up until the publication of Carson's 1922 paper. Another advance concerned the nature of noise, Carson and Zobel (1923) treated noise as a <b>random</b> <b>process</b> with a continuous bandwidth, an idea that was well ahead of its time, and thus limited the amount of noise {{that it was possible to}} remove by filtering to that part of the noise spectrum which fell outside the passband. This too, was not generally accepted at first, notably being opposed by Edwin Armstrong (who ironically, actually succeeded in reducing noise with wide-band FM) and was only finally settled with the work of Harry Nyquist whose thermal noise power formula is well known today.|$|E
25|$|In {{the general}} case, Brownian motion is a non-Markov <b>random</b> <b>process</b> and {{described}} by stochastic integral equations.|$|E
50|$|His {{scientific}} {{works are}} on the theory of stochastic differential equations, limit theorems of <b>random</b> <b>processes,</b> distributions in infinite-dimensional spaces, statistics of <b>random</b> <b>processes</b> and Markov processes.|$|R
50|$|In {{stochastic}} {{maximum likelihood}} beamformer (SML), {{the noise is}} modeled as stationary Gaussian white <b>random</b> <b>processes</b> (the same as in DML) whereas the signal waveform as Gaussian <b>random</b> <b>processes.</b>|$|R
40|$|Markov <b>random</b> <b>processes</b> {{and general}} <b>random</b> <b>processes</b> are considered. It is shown that continuous-time, continuous-valued, wide-sense stationary, Markov <b>random</b> <b>processes</b> that have {{absolutely}} continuous second order distributions are not bandlimited. It is {{also shown that}} when these processes are strictly stationary and continuous almost surely, they cannot be recovered without error from their quantized versions. Further, it is shown that continuous-time, discrete-valued Markov <b>random</b> <b>processes,</b> which are uniformly bounded and satisfy an additional condition, can be recovered with zero average distortion from an appropriate set of samples for a general distortion measure. A similar result is shown for general continuous-time <b>random</b> <b>processes</b> with rth power distortion measure. Additionally, it is shown that under a milder condition on the Markov processes and a different condition on the sampling times (e. g., uniform sampling), such processes cannot be recovered with zero average distortion. Finally, the notion of information-singularity is extended to continuous-time <b>random</b> <b>processes,</b> and it is shown that both continuous- and discrete-time Markov processes are not information-singular...|$|R
25|$|Since one obtains {{the same}} <b>random</b> <b>process</b> by {{inverting}} all choices, the Rado graph is also self-complementary.|$|E
25|$|Even this {{condition}} is not necessary, but given a non-stationary <b>random</b> <b>process,</b> {{it should not}} be difficult to test whether the AEP holds using the above method.|$|E
25|$|The Wiener–Khinchin theorem, (or Wiener – Khintchine theorem or Khinchin – Kolmogorov theorem), {{states that}} the power {{spectral}} density of a wide-sense-stationary <b>random</b> <b>process</b> is the Fourier transform of the corresponding autocorrelation function.|$|E
40|$|Abstract. Generalized <b>random</b> <b>processes</b> {{by various}} types of {{continuity}} are considered and classified as generalized <b>random</b> <b>processes</b> (GRPs) of type (I) and (II). Structure theorems for Hilbert space valued generalized <b>random</b> <b>processes</b> are obtained: Series expansion theorems for GRPs (I) considered as elements of the spaces L(A, S(H) − 1) are derived, and structure representation theorems for GRPs (II) on K{Mp}(H) on a set with arbitrary large probability are given...|$|R
40|$|The mapping between {{function}} {{spaces that}} is {{implied by the}} representation of a real 'bandpass' function by a complex 'low-pass' function is explicitly accepted. The discussion is extended to the representation of stationary <b>random</b> <b>processes</b> where the mapping is between spaces of <b>random</b> <b>processes.</b> This approach clarifies {{the nature of the}} complex representation, {{especially in the case of}} <b>random</b> <b>processes</b> and, in addition, derives the properties of the complex representation. ...|$|R
40|$|In this {{research}} paper, {{the relationship between}} finite / countable state space stochastic processes and point processes is explored. Utilizing the known relationship between Poisson processes and continuous time Markov chains, finite / countable state space <b>random</b> <b>processes</b> are related to continuous time Markov Chains. Based on the known results for binary <b>random</b> <b>processes,</b> characterization of auto-correlation function of finite state space <b>random</b> <b>processes</b> is explored. An important characterization of corner positive definite matrices is provided...|$|R
25|$|The von Kármán {{model has}} irrational power {{spectral}} densities. So, a filter {{with a white}} noise input that outputs a <b>random</b> <b>process</b> with the von Kármán model's power spectral densities can only be approximated.|$|E
25|$|A discrete-time <b>random</b> <b>process</b> {{involves}} {{a system which}} is in a certain state at each step, with the state changing randomly between steps. The steps are often thought of as moments in time, but they can equally well refer to physical distance or any other discrete measurement. Formally, the steps are the integers or natural numbers, and the <b>random</b> <b>process</b> is a mapping of these to states. The Markov property states that the conditional probability distribution for the system at the next step (and in fact at all future steps) depends only on {{the current state of}} the system, and not additionally on the state of the system at previous steps.|$|E
25|$|The Dryden {{model has}} {{rational}} power spectral densities for each velocity component. This means that an exact filter can be formed that takes white noise as an input and outputs a <b>random</b> <b>process</b> with the Dryden model's power spectral densities.|$|E
40|$|A {{technique}} for the digital simulation of multicorrelated Gaussian <b>random</b> <b>processes</b> is described. This technique {{is based upon}} generating discrete frequency functions which correspond to the Fourier transform of the desired <b>random</b> <b>processes,</b> and then using the fast Fourier transform (FFT) algorithm to obtain the actual <b>random</b> <b>processes.</b> The main advantage of this method of simulation over other methods is computation time; {{it appears to be}} more than an order of magnitude faster than present methods of simulation. One of the main uses of multicorrelated simulated <b>random</b> <b>processes</b> is in solving nonlinear random vibration problems by numerical integration of the governing differential equations. The response of a nonlinear string to a distributed noise input is presented as an example...|$|R
40|$|Given two {{discrete}} <b>random</b> <b>processes,</b> what is {{the largest}} possible average mutual information between them? An application of the Ornstein—Sinai theorem of ergodic theory is used to show that if the processes are ergodic, then there exists a pair process with the given processes as coordinates such that the average mutual information between the coordinates is the maximum possible value, the smaller of the two entropy rates. As an application and interpretation, the information—theoretic analog of a recently developed distance measure on <b>random</b> <b>processes</b> is defined and the topologies induced by the two distance measures on spaces of <b>random</b> <b>processes</b> are compared and contrasted. This demonstrates a fundamental relation between mutual information and mutual approximation of <b>random</b> <b>processes...</b>|$|R
40|$|A fresh {{introduction}} to <b>random</b> <b>processes</b> utilizing signal theory By incorporating a signal theory basis, A Signal Theoretic Introduction to <b>Random</b> <b>Processes</b> presents a unique {{introduction to}} <b>random</b> <b>processes</b> {{with an emphasis}} on the important random phenomena encountered in the electronic and communications engineering field. The strong mathematical and signal theory basis provides clarity and precision in the statement of results. The book also features:  A coherent account of the mathematical fundamentals and signal theory that underpin the presented material Unique, in-depth coverage o...|$|R
25|$|However, {{the idea}} of a general trend towards {{complexity}} in evolution can also be explained through a passive process. This involves an increase in variance but the most common value, the mode, does not change. Thus, the maximum level of complexity increases over time, but only as an indirect product of there being more organisms in total. This type of <b>random</b> <b>process</b> is also called a bounded random walk.|$|E
25|$|The {{majority}} of genetic mutations neither assist, change the appearance of, nor bring harm to individuals. Through {{the process of}} genetic drift, these mutated genes are neutrally sorted among populations and survive across generations by chance alone. In contrast to genetic drift, natural selection is not a <b>random</b> <b>process</b> because it acts on traits that are necessary for survival and reproduction. Natural selection and random genetic drift are constant and dynamic parts of life and over time this has shaped the branching structure in the tree of life.|$|E
25|$|The {{concept of}} amplitudes {{described}} above {{is relevant to}} quantum state vectors. It is also used {{in the context of}} unitary operators that are important in the scattering theory, notably in the form of S-matrices. Whereas moduli of vector components squared, for a given vector, give a fixed probability distribution, moduli of matrix elements squared are interpreted as transition probabilities just as in a <b>random</b> <b>process.</b> Like a finite-dimensional unit vector specifies a finite probability distribution, a finite-dimensional unitary matrix specifies transition probabilities between a finite number of states. Note that columns of a unitary matrix, as vectors, have the norm 1.|$|E
40|$|AbstractWe {{consider}} <b>random</b> <b>processes</b> {{more general}} than those considered by Erdös and Rényi for generating the countable random graph. It is proved that, {{in the category}} sense, almost all <b>random</b> <b>processes</b> we consider generate the countable random graph with probability 1. Under a weak boundedness assumption we give a criterion for the <b>random</b> <b>processes</b> which generate the countable random graph almost surely. We also consider further questions asked by Jackson regarding the outcome graphs when the process fails to produce the countable random graph...|$|R
2500|$|Timeline of thermodynamics, {{statistical}} mechanics, and <b>random</b> <b>processes</b> ...|$|R
5000|$|Probability, <b>Random</b> <b>Processes</b> and Ergodic Properties (1988, revised 2007) ...|$|R
25|$|A {{variety of}} models exist for gusts but only two, the Dryden and von Kármán models, are {{generally}} used for continuous gusts in flight dynamics applications. Both {{of these models}} define gusts in terms of power spectral densities for the linear and angular velocity components parameterized by turbulence length scales and intensities. The velocity components of these continuous gust models {{can be incorporated into}} airplane equations of motion as a wind disturbance. While these models of continuous gusts are not white noise, filters can be designed that take a white noise input and output a <b>random</b> <b>process</b> with the Dryden or von Kármán models.|$|E
25|$|Even if {{the source}} {{intensity}} is turned down, {{so that only}} one particle (e.g. photon or electron) is passing through the apparatus at a time, the same interference pattern develops over time. The quantum particle acts as a wave when passing through the double slits, but as a particle when it is detected. This is a typical feature of quantum complementarity: a quantum particle will act as a wave in an experiment to measure its wave-like properties, and like a particle in an experiment to measure its particle-like properties. The point on the detector screen where any individual particle shows up will {{be the result of}} a <b>random</b> <b>process.</b> However, the distribution pattern of many individual particles will mimic the diffraction pattern produced by waves.|$|E
25|$|Process variations: With MOSFETs {{becoming}} smaller, {{the number}} of atoms in the silicon that produce many of the transistor's properties is becoming fewer, {{with the result that}} control of dopant numbers and placement is more erratic. During chip manufacturing, <b>random</b> <b>process</b> variations affect all transistor dimensions: length, width, junction depths, oxide thickness etc., and become a greater percentage of overall transistor size as the transistor shrinks. The transistor characteristics become less certain, more statistical. The random nature of manufacture means we do not know which particular example MOSFETs actually will end up in a particular instance of the circuit. This uncertainty forces a less optimal design because the design must work for a great variety of possible component MOSFETs. See process variation, design for manufacturability, reliability engineering, and statistical process control.|$|E
40|$|For closed-set valued <b>random</b> <b>processes</b> we {{introduce}} a stochastic order relation (dominance) {{and show that}} the argmins of a sequence of <b>random</b> <b>processes,</b> which are epi-convergent in distribution satisfy this order relation in an asymptotic sense. The result {{may be used for}} the construction of confidence regions for the argmin...|$|R
5000|$|The above {{definitions}} {{work for}} signals that are square integrable, or square summable, that is, of finite energy. Signals that [...] "last forever" [...] are treated instead as <b>random</b> <b>processes,</b> {{in which case}} different definitions are needed, based on expected values. For wide-sense-stationary <b>random</b> <b>processes,</b> the autocorrelations are defined as ...|$|R
40|$|This paper {{examines}} multi-valued <b>random</b> <b>processes</b> (<b>random</b> sets) with {{values in}} a separable Banach space. Several {{results on the}} almost sure convergence and decomposition properties of various classes of <b>random</b> <b>processes</b> are established. Special consideration is given to multi-valued submartingales, uniform amarts, weak sequential amarts and amarts of infinite order. In the process some results concerning sequences of vector-valued random variables are also proved...|$|R
25|$|Roughly speaking, the theorem {{states that}} {{although}} there are many series of results that may be produced by a <b>random</b> <b>process,</b> the one actually produced is most probably from a loosely defined set of outcomes that all have approximately the same chance of being the one actually realized. (This is a consequence of the law of large numbers and ergodic theory.) Although there are individual outcomes which have a higher probability than any outcome in this set, the vast number of outcomes in the set almost guarantees that the outcome will come from the set. One way of intuitively understanding the property is through Cramér's large deviation theorem, which states that the probability of a large deviation from mean decays exponentially with the number of samples. Such results are studied in large deviations theory; intuitively, it is the large deviations that would violate equipartition, but these are unlikely.|$|E
25|$|Evolution {{is not a}} <b>random</b> <b>process.</b> Although {{mutations}} in DNA are random, natural selection is not a process of chance: the environment determines the probability of reproductive success. Evolution is an inevitable result of imperfectly copying, self-replicating organisms reproducing over billions of years under the selective pressure of the environment. The outcome of evolution is not a perfectly designed organism. The end products of natural selection are organisms that are adapted to their present environments. Natural selection does not involve progress towards an ultimate goal. Evolution does not strive for more advanced, more intelligent, or more sophisticated life forms. For example, fleas (wingless parasites) are descended from a winged, ancestral scorpionfly, and snakes are lizards that no longer require limbs—although pythons still grow tiny structures that are the remains of their ancestor's hind legs. Organisms are merely the outcome of variations that succeed or fail, dependent upon the environmental conditions at the time.|$|E
25|$|It is {{understood}} that X-chromosome inactivation is a <b>random</b> <b>process,</b> occurring {{at about the}} time of gastrulation in the epiblast (cells that will give rise to the embryo). The maternal and paternal X chromosomes have an equal probability of inactivation. This would suggest that women would be expected to suffer from X-linked disorders approximately 50% as often as men (because women have two X chromosomes, while men have only one); however, in actuality, the occurrence of these disorders in females is much lower than that. One explanation for this disparity is that over 25% of genes on the inactivated X chromosome remain expressed, thus providing women with added protection against defective genes coded by the X-chromosome. Some suggest that this disparity must be proof of preferential (non-random) inactivation. Preferential inactivation of the paternal X-chromosome occurs in both marsupials and in cell lineages that form the membranes surrounding the embryo, whereas in placental mammals either the maternally or the paternally derived X-chromosome may be inactivated in different cell lines.|$|E
5000|$|Rosenblatt, M. <b>Random</b> <b>Processes</b> (Graduate Texts in Mathematics) Springer Verlag, [...]|$|R
5000|$|Daniel F. Merriam, ed. (1976). <b>Random</b> <b>Processes</b> in Geology. Springer-Verlag, 168 p.|$|R
5000|$|Theory of {{probability}} and <b>Random</b> <b>Processes</b> (with Koralov). 2nd edition, Springer, 2007.|$|R
