32|12|Public
40|$|A new {{velocity}} determination algorithm with {{combination of}} remove and restore method, outliers detection method and Chebyshev fitting method with <b>redundant</b> <b>observations</b> is proposed. An optimal selection of number of sampling points is given. The result shows that, {{when the number}} of sampling points is 19, the three-dimension (3 D) interpolation precision of velocity is superior to 0. 1 mm/s, which is above 3 times better than that of Chebyshev fitting method with <b>redundant</b> <b>observations</b> and far better than those of the conventional interpolation methods...|$|E
40|$|In {{attempting}} to apply Knowledge Discovery in Databases (KDD) {{to generate a}} predictive model from a health care dataset that is currently available to the public, {{the first step is}} to pre-process the data to overcome the challenges of missing data, <b>redundant</b> <b>observations,</b> and records containing inaccurate data. This study will demonstrate how to use simple pre-processing methods to improve the quality of input data...|$|E
40|$|Practical {{measurement}} schemes require <b>redundant</b> <b>observations</b> {{for quality}} control and errors checking. This led to inconsistent solution where every subset (minimum required data) gives different results. Least Square Estimation (LSE) {{is a method}} to provide a unique solution (of the normal equation) from <b>redundant</b> <b>observations</b> by minimizing the sum of squares of the residuals. Analysis of LSE also provide estimate quality of parameters, observations and residuals, assessment of network’s reliability and precision, detection of gross errors etc. Many methods {{can be applied to}} solve normal equation, e. g. Gauss-Doolittle, Gauss-Jordan Elimination, Singular Value Decomposition, Iterative Jacoby etc. Cholesky Decomposition is an efficient method to solve normal equation with positive definite and symmetric coefficient matrix. It is also capable of detecting weak condition 1 of the system. Solving large normal equation will require a lot of times and computer memory. Implementation of sparse matrix in Cholesky Decomposition will speed up the execution times and minimize the memory usage by exploiting the zeros and symmetrical of coefficient matrix. This paper discusses the procedures and benefits of implementing sparse matrix in Cholesky Decomposition. Some preliminary results are also included...|$|E
40|$|Abstract. Total reliability, {{also called}} the average <b>redundant</b> <b>observation</b> component, is an {{indicator}} of a survey network. It summarily shows whether gross errors contained in observations on a survey network may be eliminated or weakened by robust estimation methods. Based on total reliability, the current paper presented the concept of remainder reliability, which essentially showed whether gross errors might be eliminated or weakened by robust estimation. Taking 12 leveling networks with different remainder reliability adjustments as examples, simulation experiments were used to compare 4 frequently used robust estimation methods. The results indicate that when observations simultaneously contain one gross error and if the remainder reliability 30. 0 ≥RRg, robust estimation methods may eliminate or weaken these effects. However, if 40. 0 >RRg, all the four robust estimation methods may effectively eliminate or weaken these effects...|$|R
40|$|This article {{describes}} the geodetic monitoring of the impact that the tunnel excavations have on the rock mass at the tunnel face in the Šentvid tunnel. In the preliminary phase, an exploratory gallery was built almost along the main tunnel axis to collect geological, tectonic zone and other relevant data. The exploratory gallery is now being used for geodetic and other monitoring {{of the impact of}} tunnel excavations on the surrounding rock mass. Before the beginning, methods of point and station point stabilization, and a measurement method were chosen. Thus, the chosen methods are the method of 3 D inner intersection with adjustment of <b>redundant</b> <b>observation</b> for station point determination, and the polar method for calculating 3 D coordinates of detail points. The method of determination of station point was chosen because of the far reaching influence that the tunnel excavations have on rock mass behaviour at the tunnel face...|$|R
40|$|Searches for {{planetary}} transits {{find many}} astrophysical false positives as a by-product. There are four main types analyzed in the literature: a grazing-incidence {{eclipsing binary star}}, an eclipsing binary star with a small radius companion star, a blend {{of one or more}} stars with an unrelated eclipsing binary star, and a physical triple star system. We present a list of 69 astrophysical false positives that had been identified as candidates of transiting planets of the on-going XO survey. This list may be useful in order to avoid <b>redundant</b> <b>observation</b> and characterization of these particular candidates independently identified by other wide-field searches for transiting planets. The list may be useful for those modeling the yield of the XO survey and surveys similar to it. Subsequent observations of some of the listed stars may improve mass-radius relations, especially for low-mass stars. From the candidates exhibiting eclipses, we report three new spectroscopic double-line binaries and give mass function estimations for 15 single lined spectroscopic binaries. Comment: 13 pages, 4 figures, accepted to ApJ...|$|R
40|$|The article {{describes}} three methods of gross error detection and their localization in geodetic surveying. The prerequisite for any gross error detection procedure is {{the availability of}} a set of <b>redundant</b> <b>observations.</b> The global model test with Data Snooping is the most commonly used method for gross error detection, however, it assumes that the a priori precision of observations is reliably known. As alternatives, the τ test and the Danish method are presented. An example of gross error detection in a plane cross-braced quadrilateral is given for all three methods...|$|E
40|$|Recent {{advances}} in signal processing {{have focused on}} the use of sparse representations in various applications. A new field of interest based on sparsity has recently emerged: compressed sensing. This theory is a new sampling framework that provides an alternative to the well-known Shannon sampling theory. In this paper we investigate how compressed sensing (CS) can provide new insights into astronomical data compression. In a previous study 1 we gave new insights into the use of Compressed Sensing (CS) in the scope of astronomical data analysis. More specifically, we showed how CS is flexible enough to account for particular observational strategies such as raster scans. This kind of CS data fusion concept led to an elegant and effective way to solve the problem ESA is faced with, for the transmission to the earth of the data collected by PACS, one of the instruments onboard the Herschel spacecraft which will launched in late 2008 /early 2009. In this paper, we extend this work by showing how CS can be effectively used to jointly decode multiple observations at the level of map making. This allows us to directly estimate large areas of the sky from one or several raster scans. Beyond the particular but important Herschel example, we strongly believe that CS can be applied to a wider range of applications such as in earth science and remote sensing where dealing with multiple <b>redundant</b> <b>observations</b> is common place. Simple but illustrative examples are given that show the effectiveness of CS when decoding is made from multiple <b>redundant</b> <b>observations...</b>|$|E
40|$|Automatic {{co-registration}} {{is a basic}} step in multi-sensor {{data fusion}} for remote sensing applications. The effectiveness of Mutual Information (MI) as a similarity measure for multisensor image registration has previously been reported for medical and remote sensing applications. In this paper, a new intensity-based approach built on local MI principles is presented. The approach decreases the complexity of higher dimension optimization by measuring local MI on welldistributed tie points. In addition, the reliability of registration is improved due to utilization of <b>redundant</b> <b>observations</b> of similarity. The performance of the proposed method for the registration of WorldView 2 satellite imagery with LiDAR elevation and intensity data has been experimentally evaluated and the results obtained are presented...|$|E
40|$|With the {{development}} of space technology {{and the performance of}} remote sensors, high-resolution satellites are continuously launched by countries around the world. Due to high efficiency, large coverage and not being limited by the spatial regulation, satellite imagery becomes one of the important means to acquire geospatial information. This paper explores geometric processing using satellite imagery without ground control points (GCPs). The outcome of spatial triangulation is introduced for geo-positioning as repeated observation. Results from combining block adjustment with non-oriented new images indicate the feasibility of geometric positioning with the repeated observation. GCPs are a must when high accuracy is demanded in conventional block adjustment; the accuracy of direct georeferencing with repeated observation without GCPs is superior to conventional forward intersection and even approximate to conventional block adjustment with GCPs. The conclusion is drawn that taking the existing oriented imagery as repeated observation enhances the effective utilization of previous spatial triangulation achievement, which makes the breakthrough for repeated observation to improve accuracy by increasing the base-height ratio and <b>redundant</b> <b>observation.</b> Georeferencing tests using data from multiple sensors and platforms with the repeated observation will be carried out in the follow-up research...|$|R
40|$|Chemical {{genetics}} is {{a powerful}} new discipline in plant science. Bioactive small molecules {{can be used to}} identify novel signalling nodes and unravel <b>redundant</b> networks. <b>Observations</b> made so far have revealed a series of principles in plant chemical genetics. These principles concern compound properties, such as bioactivation and bioavailability; and valuable approaches, like the use of derivatives and transcriptomics and successful ways of target identification. Together, these principles explain why the choice of the chemical library is important and instruct the design of future chemical genetic screens...|$|R
40|$|Abstract: This paper {{shows the}} {{implementation}} of a software using the language Java, to apply the concepts of optimization of surveying monitoring networks and to propitiate the user the interaction possibility with the program, in way to optimize a two dimensional surveying network according to the analysis of the criteria of precision, in that it should look for the minimum variance for each point of the network and reliability criteria to evaluate the number of redundancy and with this the influence of the blunders in the adjusted parameters. The software is capable to perform the adjustment of the observations using least squares method to a set of horizontal coordinates, distances, azimuths and angular observations. It optimizes through the change of observations of the network, with increment of observations or if it is the case reduction of these. The concept of reliability of a surveying network is related to the capacity that this has to detect blunders such small as possible. And this can be evaluated through the numbers of redundancies. The sum of the partial redundancies of the observations of a surveying network indicates the degree of freedom of the network. Therefore each added <b>redundant</b> <b>observation</b> increases in a unit the sum of the partial redundancies, independent of the observation. In thi...|$|R
40|$|In {{the domain}} of multi-bands image processing, two {{different}} approaches can be considered: the scalar one and the vectorial one. This paper presents a method that belongs to the first approach. The method is achieved in three steps. The first step tempts to eliminate <b>redundant</b> <b>observations</b> by making a selection of relevant bands. In the second step, each of the selected bands is segmented using a technique of histogram multi-thresholding. In the last step, a fusion {{by a combination of}} the results of the selected bands allows one to obtain the final segmentation. This scheme is illustrated in the frame of an application in high-resolution multispectral imagery acquired by the Compact Airborne Spectrographic Imager (CASI) ...|$|E
40|$|Abstract. DEM {{matching}} algorithm without control points is {{the base for}} detecting deformation method, the deformation existing in DEMs is treated as gross error. The detectability and locatability of multiple gross errors is very critical according to the surveying error and reliability theory. The interactive observations number (ION) derived from gross error judgement matrix is adapted in this paper. The relationship between zero-column vectors in judge matrix and the rank of the coefficient matrix of DEM matching is discussed in theory, and prove none zero-column vectors exist with real date sets. ION Equal {{to the number of}} <b>redundant</b> <b>observations.</b> Experimental results show that DEM {{matching algorithm}} has the detectability and locatability of multiple gross errors, and the deformation can be correctly detected by robust estimators in prearranged confidence level...|$|E
40|$|This work {{introduces}} {{an efficient}} classification pipeline, which provides an accurate semantic interpretation of urban environments by using redundant scene observations. The image-based method integrates both appearance and height data to classify single aerial images. Given the initial classification of highly overlapping images, a projection {{to a common}} orthographic 3 D world coordinate system provides <b>redundant</b> <b>observations</b> from multiple viewpoints and enables a semantic interpretation of large-scale urban environments. In the experimental evaluation we investigate how the use of redundancy influences the accuracy in terms of correctly classified pixels for object classes like building, tree, grass, street and water areas. Moreover, we exploit an efficient yet continuous formulation of the Potts model to obtain a consistent labeling of the pixels in the orthographic view. We present results for the datasets Dallas and Graz. ...|$|E
40|$|This thesis {{develops}} an algorithm to fuse <b>redundant</b> radar <b>observations</b> {{caused by}} multiple radar {{coverage of a}} vessel. Additionally, the algorithm automates track history and improves position accuracy by incorporating differential Global Positioning System (GPS) reports. The algorithm uses fuzzy membership functions {{as a measure of}} correlation and a fuzzy associative system to determine which observations represent the same vessel. The use of the fuzzy associative system produces a computationally efficient algorithm. A track correlation technique is used to automate the track history thereby reducing operator workload. Results of tests based on computer simulation show that the fusion algorithm correctly correlates and fuses the radar observations. (AN) NANAU. S. Navy (U. S. N.) author...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedRepeatable accuracy of hydrographic positioning was examined {{in terms of}} the two-dimensional normal distribution function which results in an elliptical error figure. The error ellipse was discussed, and two methods for conversion of elliptical errors to circular errors were given. These methods are "circle of equivalent probability" and "root mean square error" (d). Using the d error concept, repeatable accuracy of ranging, azimuthal, and hyperbolic systems was evaluated, and methods were developed to draw repeatability contours for those systems. A brief theoretical background was provided to explain the method of least squares and discuss its application to hydrographic survey positioning. For ranging, hyperbolic, azimuthal, sextant angle, and Global Positioning System the least squares observation equations were developed. Specific examples were constructed to demonstrate the capabilities of this data adjustment technique when applied to <b>redundant</b> position <b>observations.</b> [URL] Turkish Nav...|$|R
40|$|For {{evaluation}} of simulator motion and motion platform dynamics, the motion cues generated in flight simulators {{need to be}} measured. For the SIMONA Research Simulator at TU Delft, the availability of redundant kinematic motion sensors i. e. an Inertial Measurement Unit and sensors that measure the lengths of the motion system actuators was expected to allow for optimal estimation of the flight simulator motion state using an Extended Kalman Filter. As a starting point, this sensor fusion problem was evaluated for only symmetrical simulator motion, omitting the additional asymmetrical motion states. The highly nonlinear relation between {{the extension of the}} motion base actuators and simulator position and orientation was found to require the application of an Iterative Extended Kalman Filter to ensure adequate filter convergence. Using this iterative filter, optimal estimates of the symmetrical simulator state and the IMU biases could be obtained from the two sets of <b>redundant</b> kinematic <b>observations...</b>|$|R
40|$|The design problem {{considered}} in the present investigation involves a nonlinear discrete time stochastic system where replicated sensors provide <b>redundant</b> <b>observations</b> of inputs and outputs of the system, compensate for sensor 'normal operating' bias levels, and generate reliable estimates for the plant states {{in the presence of}} possible sensor failures. The resulting fault tolerant design should utilize inherent analytical redundancy and be capable of detecting many different types and levels of sensor failures. In addition, it should have minimal complexity. In connection with these goals, a sensor fault tolerant system design methodology is developed. The performance of the considered approach to an application is discussed, taking into account the design of a sensor fault tolerant system using analytic redundancy for the Terminal Configured Vehicle research aircraft in a microwave landing system environment...|$|E
30|$|Both {{observation}} {{noise and}} communication errors deteriorate {{the performance of}} decentralized estimation. Traditional fusion-based estimators are able to minimize the mean square error (MSE) of the parameter estimation by assuming perfect communication links (see [3] and references therein). They reduce the observation noise by exploiting the <b>redundant</b> <b>observations</b> provided by multiple sensors. However, their performance degrades dramatically when communication errors cannot be ignored or corrected. On the other hand, various wireless communication technologies aiming at achieving transmission capacity or improving reliability do not minimize the MSE of the parameter estimation. For example, although diversity combining reduces the bit error rate (BER), it requires that the signals transmitted from multiple sensors are identical, which is not true {{in the context of}} WSNs due to the observation noise at sensors. This motivates to optimize estimator at the FC under realistic observation and channel models, which minimizes the MSE of parameter estimation.|$|E
40|$|A {{system to}} {{estimate}} the horizontal position of cellular telephones operating in the Advanced Mobile Phone Service is developed and its performance evaluated. This system, named Cellocate, was designed and developed {{for the application of}} automatic location of cellular 911 callers. Cellocate implements a superresolution based algorithm to estimate, at a number of cell sites, the time of arrival of signals transmitted by a cellular telephone. The time of arrivals are differenced between the cell sites and hyperbolic trilateration is used {{to estimate the}} position of the cellular telephone. Time synchronization between the various cell sites is implemented {{through the use of a}} GPS receiver, operating in time transfer mode, at each cell site. Least squares is used in the position estimation process to make <b>redundant</b> <b>observations</b> consistent. The performance of Cellocate is evaluated though the use of simulations and field tests. The simulations indicate that for an actual cellular [...] ...|$|E
40|$|While motion {{estimation}} {{has been extensively}} studied in the computer vision literature, the inherent information redundancy in an image sequence has not been well utilised. In particular as many as pairwise relative motions can be estimated efficiently from a sequence of N images. This highly <b>redundant</b> set of <b>observations</b> can be efficiently averaged resulting in fast {{motion estimation}} algorithms that are globally consistent. In this paper we demonstrate this using the underlying Lie-group structure of motion representations. The Lie-algebras of the Special Orthogonal and Special Euclidean groups are used to define averages on the Lie-group which in turn gives statistically meaningful, efficient and accurate algorithms for fusing motion information. Using multiple constraints also controls the drift in the solution due to accumulating error. The performance of the method in estimating camera motion is demonstrated on image sequences...|$|R
40|$|Abstract Background Filamin (FLN) and non-muscle α-actinin {{are members}} of a family of F-actin {{cross-linking}} proteins that utilize Calponin Homology domains (CH-domain) for actin binding. Although these two proteins have been extensively characterized, little is known about what regulates their binding to F-actin filaments in the cell. Results We have constructed fusion proteins consisting of green fluorescent protein (GFP) with either the entire cross-linking protein or its actin-binding domain (ABD) and examined the localization of these fluorescent proteins in living cells under a variety of conditions. The full-length fusion proteins, but not the ABD's complemented the defects of cells lacking both endogenous proteins indicating that they are functional. The localization patterns of filamin (GFP-FLN) and α-actinin (GFP-αA) were overlapping but distinct. GFP-FLN localized to the peripheral cell cortex as well as to new pseudopods of unpolarized cells, but was observed to localize to the rear of polarized cells during cAMP and folate chemotaxis. GFP-αA was enriched in new pseudopods and at the front of polarized cells, but in all cases was absent from the peripheral cortex. Although both proteins appear to be involved in macropinocytosis, the association time of the GFP-probes with the internalized macropinosome differed. Surprisingly, the localization of the GFP-actin-binding domain fusion proteins precisely reflected that of their respective full length constructs, indicating that the localization of the protein was determined by the actin-binding domain alone. When expressed in a cell line lacking both filamin and α-actinin, the probes maintain their distinct localization patterns suggesting that they are not functionally <b>redundant.</b> Conclusion These <b>observations</b> strongly suggest that the regulation of the binding of these proteins to actin filaments is built into the actin-binding domains. We suggest that different actin binding domains have different affinities for F-actin filaments in functionally distinct regions of the cytoskeleton. </p...|$|R
40|$|Orbit of a {{spacecraft}} in three dimensional Inertial Reference Frame is in general {{represented by a}} standard set of six parameters like Keplerian Orbital Elements namely semimajor axis, eccentricity, inclination, argument of perigee, right ascension of ascending node, and true anomaly. An orbit can also be represented by an equivalent set of six parameters namely {{the position and velocity}} vectors, hereafter referred as orbit-vectors. The process of determining the six orbital parameters from <b>redundant</b> set of <b>observations</b> (more than the required minimum observations) is known as Orbit Determination (OD) process. This is, in general, solved using Least Squares principle. Availability of accurate, almost continuous, space borne observations provide tremendous scope for simplifications and new directions in Autonomous OD (AOD). The objective of this thesis is to develop a suitable scheme for onboard autonomy in OD, specifically for low-earth-orbit-missions that are in high demand in the immediate future. The focus is on adopting a simple orbit model by a thorough study and analysis by considering the individual contributions from the different force models or component accelerations acting on the spacecraft. Second step in this work is to address the application of an onboard estimation scheme like Kalman Filter for onboard processing. The impact of the approximation made in the orbit model for filter implementation manifests as propagation error or estimation residuals in the estimation. The normal procedure of tuning the filter is by getting an appropriate state and measurement noise covariance matrices by some means, sometimes through trial and error basis. Since this tuning is laborious and the performance may vary with different contexts, it is attempted to propose a scheme on a more general footing, with dynamically compensating for the model simplification. There are three parts of this problem namely (i) Analysis of different Orbit Dynamics Models and selection of a simplified Onboard Model (ii) Design of an Estimator Filter based on Kalman Filter approach for Onboard Applications and (iii) Development of a suitable Filter Compensation procedure to ensure best estimates of orbit vectors even with the simplified orbit model. Development of a Numerical Integration scheme (and a software tool) and extensive simulation exercises to justify the conclusion on the simple model {{to be used in the}} estimation procedure forms the first part of the thesis. Tables quantify the effect of individual accelerations and demonstrate the effects of various model components on orbit propagation. In general, it is well known that the atmospheric drag is a non-conservative force and reduces energy; it is also known that the effect of first zonal harmonic term is predominant than any other gravity parameters; such anticipated trends in the accuracies are obtained. This particular exercise is carried out for orbits of different altitudes and different inclinations. The analysis facilitates conclusions on a limited model orbit dynamics suitable for onboard OD. Procedures and results of this model selection analysis is published in Journal of Spacecraft Technology, Vol. 16, No. 1,pp 8 - 30, Jan 2006, titled “Orbit Model Studies for Onboard Orbit Estimation” [69]. Design of Estimator based on Kalman Filter There are two steps involved in dealing with the next part of the defined work: • Design and implementation of Extended Kalman Filter Estimation (EKF) scheme • Steps to compensate for approximation made in the reduced orbit dynamics The GPS receivers on board some of the IRS satellites (for example, the Resource-Sat- 1), output the GPS Navigation Solutions (GPSNS) namely the position and velocity vectors of the IRS satellite along with the Pseudo-range measurements. These are recorded onboard for about two orbits duration, and are down loaded. An Extended Kalman Filter Algorithm for the estimation of the orbit vectors using these GPSNS observations is developed. Estimation is carried out assuming a Gaussian white noise models for the state and observation noises. The results show a strong dependence on the initial covariance of the noise involved; reconstruction of the observations results only if the assumption of realistic noise characteristics (which are unknown) is strictly adhered. Hence this simple non-adaptive EKF is found inadequate for onboard OD scheme. Development of the Dynamics Filter Compensation (DFC) Scheme In next part of the thesis, the problem of dealing with the un-modeled accelerations has been addressed. A suitable model-compensation scheme that was first developed by D. S Ingram el at [60] and successfully applied to Lunar missions, has been modified suitably to treat the problem posed by the reduced orbit dynamics. Here, the un-modeled accelerations are approximated by the OU stochastic process described as the solution of the Langavin stochastic differential equation. A filter scheme is designed where the coefficients of the un- modeled acceleration components are also estimated along with the system state yielding a better solution. Further augmentation to the filter include a standard Adaptive Measurement Noise covariance update; results are substantiated with actual data of IRS-P 6 (Resource–Sat 1, see chapter 4). Classified as the Structured Adaptive Filtering Scheme, this results in a Dynamic Filter Compensation(DFC) Scheme which provides distinctly improved results in the position of the state. First, the estimation is carried out using actual GPS Navigation Solutions as observations. What is to be estimated itself is observed; the State-Observation relation is simple. The results are seen to improve the orbit position five times; bringing down the position error from 40 meters to about 8 meters. However, this scheme superimposes an extra factor of noise in the velocity vector of the GPSNS solutions. It is noted that this scheme deals only with the process noise covariance. To tackle the noise introduced in the velocity components, modifications of the original scheme by introducing an adaptive measurement noise covariance update is done. This improves the position estimate further by about 2 meters and also removes the noise introduced in the velocity components and reconstructs the orbit velocity vector output of the GPSNS. The results are confirmed using one more set of actual data corresponding to a different date. This scheme is shown to be useful for obtaining continuous output –without data gaps- of the GPSNS output. Next, the estimation is carried out taking the actual GPS observations which are the Pseudo Range, Range rate measurements from the visible GPS satellites (visible to the GPS receiver onboard). Switching over to the required formulation for this situation in the state-measurement relation profile, estimation is carried out. The results are confirmed in this case also. Clear graphs of comparisons with definitive orbital states (considered as actual) versus estimated states show that the model reduction attempted at the first part has been successfully tackled in this method. In this era of space-borne GPS observations, where frequent sampling of the orbiting body is suggestive of reduced orbit models, an attempt for replacement of the conventional treatment of expensive and elaborate OD procedure is proved feasible in this thesis work...|$|R
40|$|This thesis {{develops}} an algorithm to fuse <b>redundant</b> <b>observations</b> due {{to multiple}} sensor (type and location) coverage {{in order to}} provide a significant reduction in duplicate track information provided to Vessel Traffic Services (VTS) operator displays. The design of the algorithm allows acceptance of inputs from any type of sensor (radar, acoustic, OPS, system generated and manual tracks) as long as the basic decision criteria elements are provided. The result of this effort is a computationally efficient and cost effective software solution to a significant system deficiency that impacts greatly on overall waterway safety. The algorithm is tested with real data collected from the VTS system at Puget Sound in September 1996. The results indicate that the algorithm correctly fuses redundant sensor observations on the same vessel resulting in a significant reduction in the amount of unnecessary information presented to the VTS operator. NANACanadian Armed Forces author...|$|E
40|$|This thesis {{develops}} an algorithm to fuse <b>redundant</b> <b>observations</b> due {{to multiple}} sensor {{coverage of a}} vessel. Fuzzy membership functions are used {{as a measure of}} correlation, and a fuzzy associative system determines which observations represent the same vessel. The result is a computationally efficient algorithm. The output of the system is a unique set of vessels identified by unique platform identifiers. Results of tests based on computer simulation of overlapping radar coverage show that the fusion algorithm correctly correlates and fuses the sensor observations. That the VTS system is a subset of the Joint Maritime Command Information System (JMCIS) and ultimately the Global Command and Control Software (GCCS) system makes this algorithm pertinent not only to the US Coast Guard, but also to the US Navy, DOD and other agencies such as the Canadian Navy that use this software. NANACanadian Armed Forces author...|$|E
40|$|The views {{expressed}} in this report {{are those of the}} author and do not reflect the official policy or position of the Department of Defense or the United States Government. An algorithm to fuse <b>redundant</b> <b>observations</b> due to multiple sensor coverage, in order to reduce duplicate track information provided to Vessel Traffic Services (VTS) operator displays, is reported. The algorithm receives inputs from multiple sensors as long as the basic decision criteria elements are provided. The algorithm is tested with real data collected from the VTS system at Puget Sound in September 1996. The results indicate that the algorithm correctly fuses redundant sensor observations on the same vessel resulting in a significant reduction in the amount of unnecessary information presented to the VTS operator. U. S. Coast Guard Command Control Engineering Center, Portsmouth, VA. U. S. Coast Guard Command Control Engineering CenterN 6600197 WR 0022...|$|E
40|$|In {{airborne}} laser bathymetry {{knowledge of}} exact water level heights is {{a precondition for}} applying run-time and refraction correction of the raw laser beam travel path in the medium water. However, due to specular reflection especially at very smooth water surfaces often no echoes from the water surface itself are recorded (drop outs). In this paper, we first discuss the feasibility of reconstructing the water surface from <b>redundant</b> <b>observations</b> of the water bottom in theory. Furthermore, we provide a first practical approach for solving this problem, suitable for static and locally planar water surfaces. It minimizes the bottom surface deviations of point clouds from individual flight strips after refraction correction. Both theoretical estimations and practical results confirm {{the potential of the}} presented method to reconstruct water level heights in dm precision. Achieving good results requires enough morphological details in the scene and that the water bottom topography is captured from different directions...|$|E
40|$|Automatic {{registration}} of multi-sensor data {{is a basic}} step in data fusion for photogrammetric and remote sensing applications. The effectiveness of intensity-based methods such as Mutual Information (MI) for automated {{registration of}} multi-sensor image has been previously reported for medical and remote sensing applications. In this paper, a new multivariable MI approach that exploits complementary information of inherently registered LiDAR DSM and intensity data to improve the robustness of registering optical imagery and LiDAR point cloud, is presented. LiDAR DSM and intensity information has been utilised in measuring the similarity of LiDAR and optical imagery via the Combined MI. An effective histogramming technique is adopted to facilitate estimation of a 3 D probability density function (pdf). In addition, a local similarity measure is introduced to decrease the complexity of optimisation at higher dimensions and computation cost. Therefore, the reliability of registration is improved due {{to the use of}} <b>redundant</b> <b>observations</b> of similarity. The performance of the proposed method for registration of satellite and aerial images with LiDAR data in urban and rural areas is experimentally evaluated and the results obtained are discussed...|$|E
40|$|Abstract. The {{monitoring}} of buildings, slide slopes and crustal movements {{is a central}} task of geodetic engineering. The aim is the generation of meaningful motion and deformation models in order to quickly and specifically initiate constructional or geotechnical safety measures. The adequateness of the actions depends essentially {{on the quality of}} the observation and analysis techniques. Therefore it is important to correctly derive the model parameters and their uncertainty budget considering that the model parameters are typically estimated from a large number of heterogeneous and <b>redundant</b> <b>observations</b> by means of a least-squares adjustment. Here, the uncertainty budget is assumed to comprise both random variability and remaining systematics (imprecision). In practice, there are outliers in the data which have to be detected and eliminated. In conventional techniques only random effects are taken into account. When imprecision is considered additionally, the test strategies have to be extended accordingly. In this study imprecise extensions are obtained for the estimated outliers which are tested statistically using one- and multidimensional hypotheses. The applied procedure is outlined in detail showing both theory and numerical examples...|$|E
40|$|The paper {{presents}} a CRInSAR and PSInSAR federative calculation algorithm, and applies {{it to the}} monitoring of regional linear subsidence. With use of the subsidence rates and elevation corrections calculated on the CR points as constraint for the PS network over the study area, the algorithm estimates the global optimum solutions of subsidence rates and elevation corrections in the PS network using the parametric adjustment method. The algorithm achieves the integration of CRInSAR and PSInSAR effectively. The author uses the PALSAR data over Henan province to validate the algorithm and extract the subsidence rates and elevation corrections successfully. The {{results show that the}} algorithm can avoid the trouble of choosing reference point, add the <b>redundant</b> <b>observations</b> in the step of spatial unwrapping through PS baselines. The precision of the velocities on the PS points is improved a lot compared to that of traditional algorithm. The displacement is accordant better with that of leveling. The distribution of mine in the area also agrees well with the subsidence area detected, thus the algorithm can be more widely and effectively applied in detecting regional ground subsidence and even the monitoring of mining area. Department of Land Surveying and Geo-Informatic...|$|E
40|$|The use of {{a network}} of {{reference}} stations instead of a single reference station allows to model some systematic errors in a region, and to increase the distance between the rover receiver and reference stations. GPS reference stations exist in some countries, and GPS observations are available for users in real-time mode and in post-processing. The paper presents DGPS post-processing positioning with the use of three reference stations at the same time. The traditional DGPS is based on one reference station. It has been shown that the accuracy of such positioning is about 1 – 2 meters, depending on the number of satellites and the value of PDOP (Position Dilution of Precision). The accuracy of DGPS positioning degrades when the distance between the rover and the receiver station is increased. The paper shows that when we use three reference stations simultaneously, we can create pseudorange corrections for a virtual reference station, located in the vicinity of an unknown station. Three reference stations give <b>redundant</b> <b>observations</b> and enable to reduce the values of some measurement errors and biases. Practical calculations and analysis of accuracy have been presented for medium-long distances between the rover station and references. Keywords: DGPS, network of reference station...|$|E
40|$|On {{the basis}} of the Gaussian {{vertical}} backscatter (GVB) model, this paper proposes a new method for extracting pine forest height and forest underlying digital elevation model (FUDEM) from multi-baseline (MB) P-band polarimetric-interferometric radar (PolInSAR) data. Considering the linear ground-to-volume relationship, the GVB is linked to the interferometric coherences of different polarizations. Subsequently, an inversion algorithm, weighted complex least squares adjustment (WCLSA), is formulated, including the mathematical model, the stochastic model and the parameter estimation method. The WCLSA method can take full advantage of the <b>redundant</b> <b>observations,</b> adjust the contributions of different observations and avoid null ground-to-volume ratio (GVR) assumption. The simulated experiment demonstrates that the WCLSA method is feasible to estimate the pure ground and volume scattering contributions. Finally, the WCLSA method is applied to E-SAR P-band data acquired over Krycklan Catchment covered with mixed pine forest. It is shown that the FUDEM highly agrees with those derived by LiDAR, with a root mean square error (RMSE) of 3. 45 m, improved by 23. 0 % in comparison to the three-stage method. The difference between the extracted forest height and LiDAR forest height is assessed with a RMSE of 1. 45 m, improved by 37. 5 % and 26. 0 %, respectively, for model and inversion aspects in comparison to three-stage inversion based on random volume over ground (RVoG) model...|$|E
40|$|The {{evaluation}} of the geometric performance of the novel digital large format camera UltraCamD is the topic of this presentation. We describe {{the concept of the}} geometric calibration by means of a bundle adjustment. Based on the specific design of the camera additional parameters were defined and the bundle adjustment software BINGO was modified in order to handle these parameters. The entire calibration procedure consists of four phases. In the first phase a set of images of a well defined geometry target is taken in such way, that highly <b>redundant</b> <b>observations</b> are possible. The second phase is dedicated to image coordinate measurement. Automation and accuracy is derived by image processing techniques exploiting the specific shape of the well defined targets. The third phase consists of the semi automatic adjustment process, where unknown parameters of the camera (e. g. focal length and principal point coordinates, distortion parameters and additional parameters) are estimated. In phase four we distinguish between linear and non linear parameters. Linear parameters are used to reduce the linear effect of distortions of the camera. This is achieved by a linear transform of the measured coordinates in such way, that only small nonlinear effects remain. Those effects are then described in a look up table. The results from a full calibration campaign, the adjusted parameters and the effect of these parameters are presented. Finally the calibration is approved and justified...|$|E
40|$|GNSS Receiver Autonomous Integrity Monitoring (RAIM) {{has been}} widely used in civil {{aviation}} {{with the purpose of}} keeping the users notified of the integrity risk of the navigation solutions. It is based on the scheme of consistency check among <b>redundant</b> <b>observations</b> and therefore independent of any augmentation systems. With the modernized GPS and GLONASS, as well as the new GNSS systems (Compass, GALILEO) well underway, {{the increase in the number}} of satellites and the multiple frequency signals are available. It is therefore reasonable to pursue the possibility of using RAIM in civil aviation for more stringent procedures, such as LPV- 200 for vertical guidance on a global scale. This possibility is explored by US Federal Aviation Administration (FAA) under the panel of GNSS Evolutionary Architecture Study (GEAS), which has attracted attention of researchers thereafter. Two major architectures have been identified as feasible choices to meet the LPV- 200 requirement: a) Advanced RAIM (A-RAIM), b) Relative RAIM (R-RAIM). With different advantages and disadvantages for the two architectures, it is realistic to have a comprehensive comparison. And then, reasonable choices can be made based on the requirements for specific applications. The comparison is conducted mainly at the algorithm level. In this paper, the Multiple Hypotheses Solution Separation (MHSS) method based on local tests with multiple alternative hypotheses is adopted. Based on the common threat model, A-RAIM and R-RAIM are compared with results of VPL. The comparison of VPL within a world-wide map and a time series is provided to illustrate the difference. This paper is concluded with analysis and suggestions based on the comparison...|$|E
40|$|Dense image {{matching}} methods enable efficient 3 D data acquisition. Digital {{cameras are}} available at high resolution, high geometric and radiometric quality and high image repetition rate. They {{can be used to}} acquire imagery for photogrammetric purposes in short time. Photogrammetric image processing methods deliver 3 D information. For example, Structure from Motion reconstruction methods can be used to derive orientations and sparse surface information. In order to retrieve complete surfaces with high precision, dense image matching methods can be applied. However, a key challenge is the selection of images, since the image network geometry directly impacts the accuracy, as well as the completeness of the point cloud. Thus, the image stations and the image scale have to be selected according carefully to the accuracy requirements. Furthermore, most dense image matching solutions are based on multi-view stereo algorithms, where the matching is performed between selected pairs of images. Thus, stereo models have to be selected from the available dataset in respect to geometric conditions, which influence completeness, precision and processing time. Within the paper, the selection of images and the selection of optimal stereo models are discussed according to to photogrammetric surface acquisition using dense image matching. For this purpose, impacts of the acquisition geometry are evaluated for several datasets. Based on the results, a guideline for the acquisition of imagery for photogrammetric surface acquisition is presented. The simple and efficient capturing approach with " One panorama each step " ensures complete coverage and sufficiently <b>redundant</b> <b>observations</b> for a surface reconstruction with high precision and reliability...|$|E
