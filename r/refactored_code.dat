17|359|Public
50|$|Automated {{refactoring}} of analog hardware descriptions (in VHDL-AMS) {{has been}} proposed by Zeng and Huss. In their approach, refactoring preserves the simulated behavior of a hardware design. The non-functional measurement that improves is that <b>refactored</b> <b>code</b> can be processed by standard synthesis tools, while the original code cannot.Refactoring of digital HDLs, albeit manual refactoring, has also been investigated by Synopsys fellow Mike Keating. His target is to make complex systems easier to understand, which increases the designers' productivity.|$|E
50|$|Test {{automation}} mostly using {{unit testing}} {{is a key}} feature of agile software development, where it is known as test-driven development (TDD). Unit tests are written to define the functionality before the code is written. However, these unit tests evolve and are extended as coding progresses, issues are discovered and the code is subjected to refactoring. Only when all the tests for all the demanded features pass is the code considered complete. Proponents argue that it produces software that is both more reliable and less costly than code that is tested by manual exploration. It is considered more reliable because the code coverage is better, {{and because it is}} run constantly during development rather than once {{at the end of a}} waterfall development cycle. The developer discovers defects immediately upon making a change, when it is least expensive to fix. Finally, code refactoring is safer when unit testing is used; transforming the code into a simpler form with less code duplication, but equivalent behavior, is much less likely to introduce new defects when the <b>refactored</b> <b>code</b> is covered by unit tests.|$|E
40|$|Version 0. 2. 1 : 	<b>Refactored</b> <b>code</b> to make contour choice options more clear. 	Updated {{documentation}} with {{discussion about}} contour level choices. 	Added tests for most Keyword Argument options. Tests require nose. 	Updated documentation to include notes about testing. 	Added a requirements. txt file to the package specifying version numbers and removed those specifications from setup. py...|$|E
50|$|However, <b>refactoring</b> <b>code</b> to {{eliminate}} duplication takes time, {{which might be}} better spent on other tasks. Additionally, choosing a good design to <b>refactor</b> <b>code</b> into becomes easier when there are more examples of duplicated code to see patterns in. With three examples of similar code, it is easier than with two examples to see what parts of the code should be abstracted and what parts {{should be the same}} in all cases.|$|R
50|$|Shinken has an {{open and}} test-driven {{development}} approach, with contributors to the project providing new features, <b>code</b> <b>refactoring,</b> <b>code</b> quality and bug fixing.|$|R
50|$|JavaScript editor {{features}} comprise syntax highlighting, <b>refactoring,</b> <b>code</b> completion {{for native}} objects and functions, generation of JavaScript class skeletons, generation of Ajax callbacks from a template; and automatic browser compatibility checks.|$|R
40|$|Assertions are formal {{constraints}} {{over the}} state variables of a source program which are inserted as annotations in the program text. When some code has been annotated with assertions and is then subjected to refactoring, original assertions {{would no longer be}} consistent with the <b>refactored</b> <b>code.</b> The main focus {{of this paper is to}} specify how assertions could be made consistent across the refactoring process considering design-by-contract. ...|$|E
30|$|D 3 aims at {{creating}} innovative requirements to build better solutions. It {{is based on}} the following assumptions: Design fulfillment is very important for success. Design is an event randomly occurring during the conception process. This is the key to innovation. The best design is always the result of continuous facilitation and refinement. In 1999, Martin Fowler published his book “Refactoring” (Fowler 1999) to improve the design of any existing code. He states that badly designed software can be reworked into a good one by refactoring the code continuously. He provides principles of refactoring which go hand in hand by setting up required tests to validate the <b>refactored</b> <b>code</b> (Lockard 2017).|$|E
40|$|This is the Java {{implementation}} of a research project on modelling and analysis of complex systems. Release contains Local {{implementation of}} the BiMax algorithm, <b>refactored</b> <b>code</b> of BiCat toolbox, Bayesian post-analysis which uses NCI Curated Pathways database, possibilities of calling R script from Java code, Parser that maps illumina names to HGNC Symbols and usage cases of all the above mentioned. Also, a class that can read and process bigger datasets with which BiCat GUI had a trouble. Also, Mean Squared Residue Score implementation {{is included in the}} bicluster class to have a numeric metric for assessment as well. Please visit [URL] in order to obtain the native libraries used for the projec...|$|E
5000|$|... ===Model refactoring=== Model <b>refactoring</b> like <b>code</b> <b>refactoring</b> is the disciplined {{technique}} used for modifying or improving an existing model. The following refactoring functions {{are available in}} MagicDraw: ...|$|R
50|$|When a user edits {{one or more}} {{source code}} files using a {{language}} server protocol-enabled tool, the latter makes use of language services which are provided by a language server. Language services could be e.g. <b>refactoring,</b> <b>code</b> completion, etc.|$|R
40|$|Parallelizing {{existing}} sequential {{programs to}} run efficiently on multicores is hard. The Java 5 package java. util. concurrent (j. u. c.) supports writing concurrent programs. To use this package, programmers {{still need to}} <b>refactor</b> existing <b>code.</b> This is tedious, error-prone, and omission-prone. This demo presents our tool, CONCURRENCER, which enables programmers to <b>refactor</b> sequential <b>code</b> into parallel code that uses j. u. c. concurrent utilities. CONCURRENCER does not require any program annotations, although the transformations span several, non-adjacent, program statements and use custom program analysis. A find-and-replace tool can not perform such transformations. Empirical evaluation shows that CONCURRENCER <b>refactors</b> <b>code</b> effectively: CONCURRENCER correctly identifies and applies transformations that some open-source developers overlooked, and the converted code exhibits good speedup...|$|R
40|$|Nightscout Funnel Cake This {{is by far}} {{the largest}} release ever, over 1, 000 commits, from 19 {{contributors}} around the world, with commits from as far back as Nov 2014. For Everyone: built-in Share bridge super reliable Pushover Alarms Real-Time updates, alarms, and notifications Support for 14 languages (with more on the way) unlimited options with IFTTT Maker Pump site change tracking glance-able Basal rate Treatment Profile editor And for the developers [...] . new Plugin architecture Swagger enabled API docs and code generators newly <b>refactored</b> <b>code,</b> easy to jump in lots of tests so you can be make changes without fear API support for your custom DData Advanced APIs features to slice modal time of day APIs to debug queries, and support for deletes using the api-secret Updated readme and contributing guide For full list of closed issues and PRs see: [URL]...|$|E
40|$|Buy {{the book}} at AmazonFrom the back cover It is a {{well-known}} fact that most software projects fail. Drawing important lessons from common failures {{is the goal of}} Bitter Java. Reusing design patterns is not enough for success; patterns are like partial maps of dangerous terrain. They help, but don’t prevent you from getting lost. Bitter Java teaches you how to recognize when you are lost, and how {{to get back on the}} right path. It illustrates common pitfalls of Java programming through code examples; it then presents <b>refactored</b> <b>code</b> and explains why the new solutions are safe. This book is a systematic account of common server-side Java programming mistakes, their causes, and solutions. It covers antipatterns for base Java and J 2 EE concepts such as servlets, JSPs, EJBs, enterprise connection models, and scalability. If you are an intermediate Java programmer, analyst, or architect eager to avoid the bitter experiences of others, this book is for you. After studying antipatterns in this book such as...|$|E
40|$|Abstract—A finite set of {{external}} actions {{that the program}} may perform defines the observable behavior of a program. Refactoring is a disciplined process of applying structural transformations in the code such that the program is improved in terms of quality and its external behavior is preserved. Refactoring includes evaluation of its preconditions, execution of its mechanics and corrective actions required to retain {{the behavior of the}} program. These transformations affect various locations throughout a program which includes its clients and unit tests. Due to the complex dependencies involved with in the program, preservation of program behavior often becomes nontrivial. The guidelines on refactoring by Fowler [1] lack precision and leave opportunities for developers to err. In this paper, we analyze and present an exhaustive categorization of refactoring guidelines based on their impact on production and test code together. In order to formalize the existing refactoring guidelines, we rewrite them using the primitive behavior preserving refactorings by Opdyke [19]. In addition these extended refactoring guidelines also adapt the clients and unit tests to keep them syntactically and semantically aligned with the <b>refactored</b> <b>code...</b>|$|E
50|$|Code cleanup {{refers to}} {{the act of writing}} code so that it cleans up {{leftover}} data structures and other unwanted materials from memory and the filesystem. It {{is not the same as}} <b>refactoring</b> <b>code,</b> which involves making the source code itself easier to understand, maintain, and modify.|$|R
50|$|Currently, around 50 to 100 plugins {{exist for}} this IDE. Major ones include {{persistent}} project-wide code bookmarks, Code abbreviations which allow expanding text quickly, a Source formatter which reformats code to a style guide before saving, Regular expressions search, and project-wide search/replace which helps in <b>refactoring</b> <b>code.</b>|$|R
50|$|The goal of {{characterization}} {{tests is}} to help developers verify that the modifications made to a reference version of a software system did not modify its behavior in unwanted or undesirable ways. They enable, and provide a safety net for, extending and <b>refactoring</b> <b>code</b> {{that does not have}} adequate unit tests.|$|R
40|$|Refactoring is {{the process}} of {{improving}} the design of existing code by changing its internal structure without affecting its external behaviour, with the main aims of improving the quality of software product. Therefore, there is a belief that refactoring improves quality factors such as understandability, flexibility, and reusability. However, there is limited empirical evidence to support such assumptions. The objective {{of this study is to}} validate/invalidate the claims that refactoring improves software quality. The impact of selected refactoring techniques was assessed using both external and internal measures. Ten refactoring techniques were evaluated through experiments to assess external measures: Resource Utilization, Time Behaviour, Changeability and Analysability which are ISO external quality factors and five internal measures: Maintainability Index, Cyclomatic Complexity, Depth of Inheritance, Class Coupling and Lines of Code. The result of external measures did not show any improvements in code quality after the refactoring treatment. However, from internal measures, maintainability index indicated an improvement in code quality of <b>refactored</b> <b>code</b> than non-refactored code and other internal measures did not indicate any positive effect on refactored cod...|$|E
40|$|Running compute-intensive or {{blocking}} I/O {{operations in}} the UI event thread of smartphone apps can severely degrade re-sponsiveness. Despite the fact that Android supports writing concurrent code via AsyncTask, we know little about how developers use AsyncTask to improve responsiveness. To understand how AsyncTask is used/underused/misused in practice, we first conduct a formative study using a corpus of 104 open-source Android apps comprising 1. 34 M SLOC. Our study shows that even though half of the apps use AsyncTask, {{there are hundreds of}} places where they missed opportunities to encapsulate long-running operations in AsyncTask. Sec-ond, 46 % of the usages are manually refactored. However, the <b>refactored</b> <b>code</b> contains concurrency bugs (such as data races) and performance bugs (concurrent code still executes sequentially). Inspired by these findings, we designed, developed, and evaluated Asynchronizer, an automated refactoring tool that enables developers to extract long-running operations into AsyncTask. Asynchronizer uses a points-to static analysis to determine the safety of the transformation. Our empirical evaluation shows that Asynchronizer is (i) highly applicable, (ii) accurate, (iii) safer than manual refactoring (iv) it saves development effort, (v) its results have been accepted by the open-source developers. This shows that Asynchronizer is useful. 1...|$|E
40|$|In {{the past}} few years, {{refactoring}} has emerged as an important consideration in the maintenance and evolution of software. Yet very little empirical evidence exists to support the claim about whether developers actively undertake refactoring, or whether as Fowler suggests {{that the benefits of}} doing refactoring are not short-term but too ‘long-term ’ [8]. In this paper, we describe an empirical study of multiple versions of a range of open source Java systems in an attempt to understand whether refactoring does occur and, if so, which types of refactoring were most (and least) common. Fifteen refactorings were chosen as a basis (on seven Java systems) and the code analysed using an automated tool. Results confirmed that refactoring did take place, but the majority were of the simpler, less complex type. Interestingly, the most common refactorings empirically identified were those which, according to Fowler (and from a dependency graph of the ‘seventy two ’ original refactorings), were central to larger more involved refactorings. One conclusion from the study is thus that developer time and effort for relatively large restructuring and testing of <b>refactored</b> <b>code</b> is prohibitive; making small, simple changes is preferred. A further conclusion from our study is that refactoring didn’t occur in the earliest or latest versions of the systems we investigated. 1...|$|E
50|$|While {{the term}} {{refactoring}} originally referred exclusively to <b>refactoring</b> of software <b>code,</b> {{in recent years}} code written in hardware description languages (HDLs) has also been refactored. The term hardware refactoring {{is used as a}} shorthand term for <b>refactoring</b> of <b>code</b> in hardware description languages. Since HDLs are not considered to be programming languages by most hardware engineers, hardware refactoring is to be considered a separate field from traditional <b>code</b> <b>refactoring.</b>|$|R
50|$|PyDev is a {{third-party}} plug-in for Eclipse. It is an Integrated Development Environment (IDE) used for programming in Python supporting <b>code</b> <b>refactoring,</b> graphical debugging, <b>code</b> analysis among other features.|$|R
40|$|This paper {{describes}} how code reuse combined with good programming practices {{can also be}} used as a means to attain code-level generalization of certain programs. It presents some basic principles to be used when <b>refactoring</b> <b>code</b> for reuse and presents a case study in which the functionality of two AI planners are easily combined via application of these principles. 1...|$|R
40|$|Scientific {{software}} must {{be adapted}} for different execution environments, problem sets, and available resources to en-sure its efficiency and reliability. Although adaptation pat-terns {{can be found}} in a sizable percentage of recent scientific applications, the traditional scientific software stack lacks the adequate adaptation abstractions and tools. As a result, scientific programmers manually implement ad-hoc solutions that are hard to maintain and reuse. In this paper, we present a novel approach to adapting scientific software written in Fortran. Our approach leverages the binary object code compatibility between stack-based imperative programming languages. This compatibility makes it possi-ble to apply a C++ Aspect-Oriented Programming (AOP) extension to Fortran programs. Our approach expresses the adaptability functionality as abstract aspects that imple-ment known adaptation patterns and can be reused across multiple scientific applications. Application-specific code is systematically expressed through inheritance. The resulting adaptability functionality can be maintained by any pro-grammer familiar with AOP, which has become a staple of modern software development. We validated the expressive power of our approach by refactoring the hand-coded adapt-ability functionality of a real-world computational fluid dy-namics application suite. The <b>refactored</b> <b>code</b> expresses the adaptability functionality in 27 % fewer ULOC on average by removing duplication and leveraging aspect inheritance...|$|E
40|$|Software {{refactoring}} is {{the process}} of reorganizing the internal structure of code while preserving the external behavior. Aspect-Oriented Programming (AOP) provides new modularization of software systems by encapsulating crosscutting concerns. Based on these two techniques, aspect-oriented (AO) refactoring restructures crosscutting elements in code. AO refactoring includes two steps: aspect mining (identification of aspect candidates in code) and aspect refactoring (semantic-preserving transformation to migrate the aspect-candidate code to AO code). Aspect refactoring clusters similar join points together for the aspect candidates and encapsulates each cluster with an effective pointcut definition. With the increase in size of the code and crosscutting concerns, it is tedious to manually identify aspects and their corresponding join points, cluster the join points, and infer pointcut expressions. Therefore, {{there is a need to}} automate the process of AO refactoring. This paper proposes an automated approach that identifies aspect candidates in code and infers pointcut expressions for these aspects. Our approach mines for aspect candidates, identifies the join points for the aspect candidates, clusters the join points, and infers an effective pointcut expression for each cluster of join points. The approach also provides an additional testing mechanism to ensure that the inferred pointcut expressions are of correct strength. The empirical results show that our approach helps achieve a significant reduction in the total number of pointcut expressions to be used in the <b>refactored</b> <b>code...</b>|$|E
40|$|The vast {{increase}} {{in demand for}} android applications has made android application testing inevitable. The android open source feature has led to developers of unknown level of expertise developing the application, thus raising concerns on quality issues. Currently, android applications are found lagging {{in the area of}} testing. Test case generation is the most important and challenging area of software testing. Test cases tend to be large in number as redundant test cases are generated due to the presence of code smells in the software. Code smells are unnecessary codes, as a result of poor design or implementation. Several approaches have been proposed in the past by both academy and industrial researchers to tackle the high number of generated test cases in android applications, including test case minimization and prioritization technique. Nonetheless, these approaches are reactive rather than proactive. The technique used in this study is to apply code refactoring before test case generation to avoid redundant test cases from being generated. To achieve this, the detection of smells was done, followed by refactoring of detected smells. Test cases were then generated from the <b>refactored</b> <b>code.</b> More explicitly, this research presents three rules for detection of three code smells; lazy class, small method and duplicate, and three rules for their refactoring. The rules were implemented in a tool named DART (Detection and Refactoring Tool) to refactor the android source code for the reduction of test cases. The resultant source code is compared with the original source code by generating a number of branches, branch coverage and complexity using Clover. The results of this research show a reduction of about 7. 7 % in the cyclomatic complexity of the source code, while increasing the branch coverage with up to 9. 2 % increment. Also, there is a 28 % {{reduction in the number of}} test cases generated. These show that refactoring can be used to reduce redundant test cases...|$|E
50|$|Unit testing {{allows the}} {{programmer}} to <b>refactor</b> <b>code</b> or upgrade system libraries {{at a later}} date, {{and make sure the}} module still works correctly (e.g., in regression testing). The procedure is to write test cases for all functions and methods so that whenever a change causes a fault, it can be quickly identified. Unit tests detect changes which may break a design contract.|$|R
50|$|Typical CASE tools {{exist for}} {{configuration}} management, data modeling, model transformation, <b>refactoring,</b> source <b>code</b> generation.|$|R
50|$|Although <b>refactoring</b> <b>code</b> {{has been}} done informally for years, William Griswold's 1991 Ph.D. {{dissertation}} {{is one of the}} first major academic works on refactoring functional and procedural programs, followed by William Opdyke's 1992 dissertation on the refactoring of object-oriented programs, although all the theory and machinery have long been available as program transformation systems. All of these resources provide a catalog of common methods for refactoring; a refactoring method has a description of how to apply the method and indicators for when you should (or should not) apply the method.|$|R
40|$|A {{major new}} feature is {{released}} in v 0. 9 - support for custom content. This means that MultiQC can now easily include output from custom scripts within reports {{without the need}} for a new module or plugin. For more information, please see the MultiQC documentation. Module updates: HTSeq - new module! New module for the htseq-count tool, often used in RNA-seq analysis. Prokka - new module! Prokka is a software tool for the rapid annotation of prokaryotic genomes. Slamdunk - new module! Slamdunk is a software tool to analyze SLAMSeq data. Peddy - new module! Peddy calculates genotype :: pedigree correspondence checks, ancestry checks and sex checks using VCF files. Cutadapt Fixed bug in General Stats table number for old versions of cutadapt (pre v 1. 7) Added support for really old cutadapt logs (eg. v. 1. 2) FastQC New plot showing total overrepresented sequence percentages. New option to parse a file containing a theoretical GC curve to display in the background. Human & Mouse Genome / Transcriptome curves bundled, or make your own using fastqcTheoreticalGC. See the MultiQC docs for more information. featureCounts Added parsing checks and catch failures for when non-featureCounts files are picked up by accident GATK Fixed logger error in VariantEval module. Picard Fixed missing sample overwriting bug in RnaSeqMetrics New feature to customise coverage shown from HsMetrics in General Statistics table see the docs for info). Fixed compatibility problem with output from CollectMultipleMetrics for CollectAlignmentSummaryMetrics Preseq Module now recognises output from c_curve mode. RSeQC Made the gene body coverage plot show the percentage view by default Made gene body coverage properly handle sample names Samtools New module to show duplicate stats from rmdup logs Fixed a couple of niggles in the idxstats plot SnpEff Fixed swapped axis labels in the Variant Quality plot STAR Fixed crash when there are 0 unmapped reads. Sample name now taken from the directory name if no file prefix found. Qualimap BamQC Add a line for pre-calculated reference genome GC content Plot cumulative coverage for values above 50 x, align with the coverage histogram. New ability to customise coverage thresholds shown in General Statistics table (see the docs for info). Core Updates: Support for custom content (see top of release notes). New ninja report tool: make scatter plots of any two table columns! Plot data now saved in multiqc_data when 'flat' image plots are created Allows you easily re-plot the data (eg. in Excel) for further downstream investigation Added 'Apply' button to Highlight / Rename / Hide. These tools can become slow with large reports. This means that you can enter several things without having to wait for the report to replot each change. Report heatmaps can now be sorted by highlight New config options decimalPoint_format and thousandsSep_format Allows you to change the default 1 234. 56 number formatting for plots. New config option top_modules allows you to specify modules that should come {{at the top of the}} report Fixed bar plot bug where missing categories could shift data between samples Report title now printed in the side navigation Missing plot IDs added for easier plot exporting Stopped giving warnings about skipping directories (now a debug message) Added warnings in report about missing functionality for flat plots (exporting and toolbox) Export button has contextual text for images / data Fixed a bug where user config files were loaded twice Fixed bug where module order was random if [...] module or [...] exclude was used. <b>Refactored</b> <b>code</b> so that the order of modules can be changed in the user config Beefed up code + docs in scatter plots back end and multiple bar plots. Fixed a few back end nasties for Tables Shared-key columns are no longer forced to share colour schemes Fixed bug in lambda modified values when format string breaks Supplying just data with no header information now works as advertised Improvements to back end code for bar plots New tt_decimals and tt_suffix options for bar plots Bar plots now support yCeiling, yFloor and yMinRange, as with line plots. New option hide_zero_cats:False to force legends to be shown even when all data is 0 General Stats Showing x of y columns count is fixed on page load. Big code whitespace cleanu...|$|E
40|$|VIC 5. 0. 0 Release date: (September 2, 2016) Source code is {{available}} here: This {{is a major}} update from VIC 4. The VIC 5. 0. 0 release aims to have nearly identical physics as VIC 4. 2 while providing a clean, <b>refactored</b> <b>code</b> base supporting multiple drivers. There {{are a number of}} new features, bug fixes, and backward incompatible changes. See the VIC Github page for more details on the changes included in this release. New Features: 	 	"vic_run" (GH# 7) 	Although the physics and model behavior of VIC 5. 0. 0 should be nearly identical to VIC 4. 2, the source code has undergone a major cleanup and reorganization. We have separated the physical core ("vic_run") from the driver source code. This work has improved the extensibility and readability of the model. 	 	 	Classic Driver (GH# 7) 	The Classic Driver provides similar functionality as VIC 4, including ASCII and binary I/O, and a time-before-space evaluation loop order. The Classic Driver is maintained for two main reasons: 	 		to provide some level of backward compatibility for existing VIC users that wish to continue using VIC using a traditional approach, and, 		to allow VIC to be run at individual grid cells, without requiring the infrastructure needed by the Image Driver. Documentation for the Classic Driver can be found here. 	 	 	 	Image Driver (GH# 7) 	The Image Driver adds a number of features to the user interface of the VIC model. Most notably, it uses a space-before-time evaluation loop order, netCDF I/O, and parallelization using MPI. Image Driver specific documentation can be found here. 	 	 	Constants File (GH# 192) 	Earlier versions of VIC included many hard-coded parameters and constants. We have consolidated these constants into a single structure and developed an input file that allows users to modify parameters at run-time. See here for more information. 	 	 	Logging (GH# 173) 	A set of logging Macros have been added to all drivers and vic_run. The logging level can be set in the driver Makefile via the LOG_LVL variable. The logging Macros provide the filename and line number in the source code to aid in debugging. Additionally, when compiler support {{is available}}, a traceback is printed when VIC exits during runtime. When the LOG_DIR variable is provided in the global parameter file, VIC will write its log(s) to log files instead of printing to stdout. 	 	 	Sub-hourly Timestep (GH# 188) 	Previous versions of VIC were limited to a minimum timestep of one hour. The units of the VIC timestep have been changed from hours to seconds and the minimum timestep is now one second. If you intend on running VIC at a timestep of less than one hour, we suggest extensive testing. 	 	 	Calendar Support (GH# 188) 	Earlier versions of VIC used the standard Gregorian calendar. Because many modern climate models use non-standard calendars, we have implemented all CF compliant calendars. The standard Gregorian calendar remains the VIC default. See the documentation for individual drivers for how to set the calendar option (e. g. classic. 	 	 	Sample Datasets (GH# 387) 	The VIC_sample_data repository contains the necessary input datasets (forcings and parameters) to run short simulations of the VIC model for both the classic and image driver. 	 	 	Tests Datasets (GH# 79) 	See [URL] for more information. A temporary location of the test data is here: ftp://ftp. hydro. washington. edu/pub/gergel/VIC 5 _test_data/ 	 	 	Testing and Continuous Integration (GH# 190) 	A comprehensive testing platform has been implemented and is available for public use along with the VIC model. A small subset of the test platform is run on Travis-CI, which facilitates continuous integration of the VIC test platform. More information on the test platform is here. 	 	 	Run-time profiling and timing (GH# 442) 	A timing module has been added to VIC in order to assess the computational cost and throughput of the VIC model. New output variables (OUT_TIME_VICRUN_WALL and OUT_TIME_VICRUN_CPU) document the time spent in vic_run for each variable. Additionally, a timing table is printed to LOG_DEST {{at the end of each}} simulation. 	 Backwards Incompatible Changes: 	 	Classic Driver I/O Formatting (GH# 18, GH# 204, GH# 227) 	The format of ASCII forcing and output files has changed in VIC 5. These changes were motivated by the desire to improve simulation metadata tracking and reproducibility of VIC simulations. 	 		Output files now include a header with simulation metadata and variable names. The PRT_HEADER option has been deprecated. 	 	 	 	Classic Driver Global Parameter Options 	A number of global parameter options have changed for the Classic Driver, relative to VIC 4. 	 		TIME_STEP (int, units: hours) has been changed to MODEL_STEPS_PER_DAY (int) 		SNOW_STEP (int, units: hours) has been changed to SNOW_STEPS_PER_DAY (int) 		OUT_DT (int, units: hours) has been changed to OUTPUT_STEPS_PER_DAY (int) 		FORCE_DT (int, units: hours) has been changed to FORCE_STEPS_PER_DAY (int) 		BINARY_STATE_FILE (TRUE or FALSE) has been changed to STATE_FORMAT (BINARY or ASCII) 		BINARY_OUTPUT (TRUE or FALSE) has been changed to OUT_FORMAT (BINARY or ASCII) 	 	 	 	State files now include seconds (GH# 464) 	 		There is a new global parameter option, STATESEC. This specifies the time step at the end of which state will be saved, in units of seconds. In other words, if you have an hourly time step (3600 sec) and you want to save state at the end of the final time step of the day (which is 86400 seconds long), subtract 3600 from 86400 to get a STATESEC of 82800. This corresponds to the first second of the final time step. State will be saved at the end of that time step. 		When the state save date is appended to state filenames, STATESEC will be included so that the date will have the format YYYYMMDD_SSSSS. 	 	 	 	Classic Driver Output Variables (GH# 352) 	Computation of potential evapotranspiration (PET) has been simplified, reducing the number of output variables from 6 to 1. The following output variables have been removed: 	 		OUT_PET_SATSOIL (potential evapotranspiration from saturated bare soil) 		OUT_PET_H 2 OSURF (potential evapotranspiration from open water) 		OUT_PET_SHORT (potential evapotranspiration (transpiration only) from short reference crop (grass)) 		OUT_PET_TALL (potential evapotranspiration (transpiration only) from tall reference crop (alfalfa)) 		OUT_PET_NATVEG (potential evapotranspiration (transpiration only) from current vegetation and current canopy resistance) 		OUT_PET_VEGNOCR (potential evapotranspiration (transpiration only) from current vegetation and 0 canopy resistance) 	 	These have been replaced by: 	 		OUT_PET (potential evapotranspiration, which = area-weighted sum of potential transpiration and potential soil evaporation; potential transpiration is computed using the Penman-Monteith equation with architectural resistance and LAI of the current veg cover) 	 	 Deprecated Features: 	Removed unused global parameter option MEASURE_H (GH# 284) 	 	Removed MTCLIM (GH# 288) 	Previous versions of VIC used MTCLIM to generate missing forcing variables required to run VIC. This led to confusion by many users and considerably more complex code in the Classic Driver. VIC forcings are now required to be provided at the same time frequency as the model will be run at (SNOW_STEPS_PER_DAY). 	As part of this change, the following options have been removed from the Classic Driver: 	 		LW_TYPE 		LW_CLOUD 		MTCLIM_SWE_CORR 		VP_INTERP 		VP_ITER 		OUTPUT_FORCE 	 	As part of this change, the following output variables have been removed from the Classic Driver: 	 		OUT_COSZEN 		OUT_TSKC 	 	In the future, we would like to provide a stand-alone version of MTCLIM that produces subdaily meteorological forcings. We are looking for community support for this feature (GH# 17) 	 	 	Removed LONGWAVE and SHORTWAVE forcing types (GH# 379). 	Previous versions of VIC allowed users to specify either LONGWAVE or LWDOWN to denote the incoming longwave radiation flux and SHORTWAVE or SWDOWN to denote the incoming shortwave radiation flux. We have removed these duplicate options, standardizing on the more descriptive LWDOWN and SWDOWN. 	Similarly, output variables OUT_NET_LONG and OUT_NET_SHORT have been replaced with OUT_LWNET and OUT_SWNET, respectively. 	 	 	Changed the name of the variable VEGCOVER to FCANOPY, since this more accurately captures the meaning of the term (i. e., the fractional area of the plant canopy within the veg tile). Similarly changed OUT_VEGCOVER to OUT_FCANOPY. 	Similarly, changed the names of the following global parameter file options: 	 		VEGLIB_VEGCOVER [...] > VEGLIB_FCAN 		VEGPARAM_VEGCOVER [...] > VEGPARAM_FCAN 		VEGCOVER_SRC [...] > FCAN_SRC 	 	 Bug Fixes: 	 	Miscellaneous fixes to lake module (GH# 425) 	Several lake processes (aerodynamic resistance, albedo, latent/sensible heat fluxes, net radiation, etc) were reported incorrectly or not at all in output files. This has been fixed. In addition, in the absence of an initial state file, lake temperatures were initialized to unrealistic temperatures (the air temperature of the first simulation time step). To fix this, we now initialize the lake temperature to annual average soil temperature. 	 	 	Fix for computation of soil layer temperatures when soil thermal nodes do not reach the bottom of the soil column. (GH# 467) 	Previously, if the soil thermal damping depth was shallower than the bottom of the deepest soil layer, and FROZEN_SOIL==TRUE, VIC would abort when estimating layer ice contents because it could not estimate a layer temperature if the thermal nodes did not completely span the layer. Now, a layer temperature is estimated even when thermal nodes do not completely span the layer, and the error no longer occurs. 	 	 	Fix related to exact restart (GH# 481, GH# 507, GH# 509) 	Previously, VIC did not produce the same results (fluxes and states) if a simulation was separated into multiple shorter-period runs by saving the state variables and restarting. This was due to: 	 		 		The MTCLIM algorithm resulted in slightly different sub-daily meteorological variable values for different lengths of forcings (MTCLIM is deprecated in the current version) 		 		 		A few bugs resulting in inexact restart. The following bugs have been fixed: 		 			The prognostic state variable energy. Tfoliage (foliage temperature) is now saved to the state file 			 			Two flux variables energy. LongUnderOut and energy. snow_flux are now saved to the state file. 			!!!Note This is a temporary solution to ensure exact restart. A better way of handling these two flux variables needs to be done in the future (see GH# 479) 			 		 		 	 	 	 	Fix for binary state file I/O (GH# 487) 	Fixed a bug so that the binary format state file I/O works correctly. 	 	 	Fix for a physical constant (water heat capacity) (GH# 574) 	Fixed a bug where volumetric heat capacity of water should be used in func_canopy_energy_bal (previously specific heat capacity was used) ...|$|E
50|$|Often {{the deeper}} problem hinted by a code smell can be {{uncovered}} when the code {{is subjected to}} a short feedback cycle where it is refactored in small, controlled steps, and the resulting design is examined {{to see if there}} are any further code smells that indicate the need of more refactoring. From the point of view of a programmer charged with performing <b>refactoring,</b> <b>code</b> smells are heuristics to indicate when to refactor, and what specific refactoring techniques to use. Thus, a code smell is a driver for refactoring.|$|R
50|$|This {{programmer}} must {{be familiar}} with both the original and target operating systems and languages (for example, converting a game originally written in C++ to Java), convert assets, such as artwork and sounds or rewrite code for low memory phones. This programmer may also have to side-step buggy language implementations, some with little documentation, <b>refactor</b> <b>code,</b> oversee multiple branches of code, rewrite code to scale for wide variety of screen sizes and implement special operator guidelines. They may also have to fix bugs that were not discovered in the original release of a game.|$|R
40|$|<b>Code</b> <b>refactoring</b> {{focuses on}} {{enhancing}} the maintainability of software {{to extend its}} lifetime. However as software applications were varied {{and the range of}} its usage becomes broaden, there are some efforts to improve software qualities like performance or reliability as well as maintainability using <b>code</b> <b>refactoring</b> techniques. Recently, as low-energy software has become one of critical issues in mobile environment, developing energy efficient software through <b>code</b> <b>refactoring</b> becomes an important one. Therefore this paper has its goal to investigate whether the existing refactoring techniques can support energy efficient software generation or not. That is to say, the existing <b>code</b> <b>refactoring</b> techniques can cause the minus of energy efficiency because they did not considered the energy consumption in their refactoring process. This paper experiments and analyzes to check whether the M. Fowler’s <b>code</b> <b>refactoring</b> techniques can support the energy efficient software generation or not. Our research result can give to software developer some informations about energy-efficient refactoring techniques, and can support the development of software that has high maintainabilit...|$|R
50|$|On December 18, 2012, Chocolate Duke3D port was released. Inspired by Chocolate Doom, {{the primary}} goal was to <b>refactor</b> the <b>code</b> so {{developers}} would easily read and learn from it.|$|R
