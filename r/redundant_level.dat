4|26|Public
50|$|In 1956, it {{was decided}} to elevate the {{operational}} units at Hamilton back to a Wing level, and the wing was reactivated, with its operational squadrons still controlled by the 78th Fighter Group. The group component of the wing was considered to be a <b>redundant</b> <b>level</b> of organization by the late 1950s, and the 78th FIG was inactivated on 1 February 1961, with its flying units being directly assigned to the Wing.|$|E
50|$|Original {{plans for}} XX Bomber Command to employ two combat wings in China (the 58th and 73rd Combat Bomb Wings) were changed on 2 March 1944 when limited numbers of {{operational}} B-29s resulted in only the 58th CBW assigned. There, after poor bombing results, and as developing operations deemed the original complex organization unnecessary, it was deemed a <b>redundant</b> <b>level</b> {{of command and}} its four groups moved to Guam in April 1945 as part of XXI Bomber Command.|$|E
40|$|Subsentential utterances, ellipsis, {{and pragmatic}} {{enrichment}} âˆ— ALISON HALL It {{is argued that}} genuinely subsentential phrases {{can be used to}} perform speech acts with truth conditions. Attempts to assimilate this phenomenon to syntactic ellipsis (sluicing, gapping, etc.) are discussed, and are rejected on the grounds that any implementation of this idea will involve a <b>redundant</b> <b>level</b> of representation in natural language that plays no role in the interpretation process, and therefore be less economical than a pragmatic enrichment account. An argument against the latter kind of approach from the indeterminacy of content is discussed, then it is shown how a pragmatic account can accommodate this indeterminacy and turn it into an advantage through consideration of the role of processing effort in inferential comprehension. ...|$|E
30|$|Similar {{to power}} rate control, the {{technique}} of transmit rate control is sometimes presented as a possible means to overcome the VANET scalability problem. When applying a higher transmit rate in the VANET, {{the capacity of the}} wireless communication channel is increased, which could (partly) resolve the channel congestion under high-density traffic circumstances. However, the downside of increasing the transmit rate is that at the receiver side, the SNR value of the message has to be higher for successful packet reception. Hence, the communication range of the nodes is decreased. At higher rates, the wireless link is also more sensitive to node mobility (see Section 4.6) since in that case, the PHY layer applies more sensitive modulation techniques and less <b>redundant</b> <b>levels</b> of error correcting coding. These effects can negatively influence VANET end-to-end performance. Identifying the appropriate balance in this rate control trade-off is a challenge.|$|R
30|$|Ingestion Custom {{tools are}} the largest type of tool used for data ingestion. This may {{highlight}} the lack of readily available industry-wide capable technology connectors for ingestion or the need for specialized tools in general. For example, the latter may include tools that have direct support for proficiently handling highly correlated and <b>redundant</b> process <b>level</b> data. Developing a standard and openly available tool with the required features can remove the redundancy of redeveloping fundamental components, such as protocol connectors.|$|R
40|$|Fiber optics {{combined}} with IDE's provide consistent data communication between fault-tolerant computers. Data-transmission-checking {{system designed to}} provide consistent and reliable data communications for fault-tolerant and highly reliable computers. New technique performs variant of algorithm for fault-tolerant computers and uses fiber optics and independent decision elements (IDE's) to require fewer processors and fewer transmissions of messages. Enables fault-tolerant computers operating {{at different levels of}} redundancy {{to communicate with each other}} over triply <b>redundant</b> bus. <b>Level</b> of redundancy limited only by maximum number of wavelengths active on bus...|$|R
30|$|Second, {{the method}} of node {{retransmission}} also affects delay. If the node transmit power is fixed, the data transmission reliability is fixed. At this time, some mechanism {{needs to be taken}} to ensure the data transmission meets the requirements of the application. The most commonly used measure are the following categories: (a) Retransmission mechanism [34]. The retransmission mechanism {{is one of the most}} important ways to guarantee the reliability of the network transmission. The principle of retransmission mechanism is once the data packet is sent, the sender uses some mechanism to determine whether the packet is successfully received by the receiver and, if successfully received by the receiver, send the next packet, otherwise retransmitting the current packet. In this method, the receiver sends an ACK to the sender after receiving the packet, which presents data packet was successfully received. The sender does not receive the expected ACK within a fixed time then resends the data packet. The above process continues until the receiver has successfully received the data packet or the number of retransmissions exceeds the maximum predetermined number of retransmissions [34]. The disadvantage of this method is sending of one data packet ends with the ACK signal received by the sender or the number of retransmissions exceeds the set threshold to abandon the sending of this packet. The waiting time between each transmission needs to be more than round-trip time (RTT). Thus, in a link with high packet error rates, multiple retransmission causing its network delay is large [34]. There are many more methods to improve this approach as in Ref. [30]. (b) Network coding techniques are an effective and reliable guarantee mechanism based on redundant coding [34]. In network coding techniques, the source node encodes the packets with some <b>redundant</b> <b>level.</b> The destination node decodes the packets to retrieve the original packets. Because the data sent by the source node is redundant, thus, even if some data is lost in the lossy nature of wireless channels, it will not affect the destination node correctly receiving the data. Network coding techniques have the advantage that the source node can send the data at once and the destination node can be received with high reliability and so, compared with retransmission mechanism, can reduce the delay. But the cost is that the nodes need to encode and decode and the data that is transmitted is redundant, resulting in higher computation and energy consumption for nodes. (c) Multiple transmission mechanism. This mechanism is generally used less and mainly used to protect the safety of data transmission. In this method, the data packets are sent through multiple paths at the same time. As long as any one path successfully reaches the destination, the data transmission is successful [61]. The advantage of this approach is that the delay is small, whereas the disadvantage is that the energy consumption of the network is high.|$|E
40|$|This paper {{analyzes}} {{the amount of}} global <b>level</b> <b>redundant</b> computation within selected benchmarks of the SPEC 95 and SPEC 2000 benchmark suites. Local <b>level</b> <b>redundant</b> computations are redundant computations that {{are the result of}} a single static instruction (i. e. PC dependent) while global <b>level</b> <b>redundant</b> computations are redundant computations that are the result of multiple static instructions (i. e. PC independent). The results show that for all benchmarks more than 90 % of the unique computations account for only 1. 2 % to 31. 5 % {{of the total number of}} instructions. In fact, less than 1000 (0. 14 %) of the most frequently occurring unique computations accounted for 19. 4 % - 95. 5 % of the dynamic instructions. Furthermore, more redundant computation exists at the global level as compared to the traditional local level. For an equal number of unique computations [...] approximately 100 for each benchmark [...] at both the global and local levels, the global level unique computations accounted for an additional 1. 5 % to 12. 6 % of the total number of dynamic instructions as compared to the local level unique computations. As a result, exploiting redundant computations through value reuse at the global level should yield a significant performance improvement as compared to exploiting redundant computations only at the local level...|$|R
30|$|There {{would be}} {{advantages}} to this hypothetical model. Embedded digital monitoring could display or predict abnormal or undesirable material deflections or deformations and could suggest {{in real time}} where adjustments need to be made. Yet, although probably technologically feasible, this level of control is currently not practical nor cost effective. More importantly, ultimately {{it is not necessary}} and <b>redundant.</b> The <b>level</b> of sophisticated and automated decision-making this model would bring is currently already to a large extend embedded in construction implementation, as the design and construction team can rely on experience, intuition, and common sense to solve unforeseeable issues on-the-go. The gap between the virtual and the real can never be fully sealed. The role of physical prototyping and material testing and the inclusion of their findings into a flexible digital model hence needs to be valued as essential.|$|R
40|$|This paper {{describes}} a new toolkit- SCARF- for doing speech recognition with segmental conditional random fields. It {{is designed to}} allow for the integration of numerous, possibly <b>redundant</b> segment <b>level</b> acoustic features, along with a complete language model, in a coherent speech recognition framework. SCARF performs a segmental analysis, where each segment corresponds to a word, thus allowing for the incorporation of acoustic features defined at the phoneme, multi-phone, syllable and word level. SCARF is designed to make it especially convenient to use acoustic detection events as input, such as the detection of energy bursts, phonemes, or other events. Language modeling is done by associating each state in the SCRF with a state in an underlying n-gram language model, and SCARF supports the joint and discriminative training of language model and acoustic model parameters. SCARF is available for download fro...|$|R
40|$|AbstractThis paper {{proposes a}} new risk {{assessment}} method {{based on the}} attribute reduction theory of rough set and multiclass SVM classification. Rough set theory is introduced for data attribute reduction and multiclass SVM is used for automatic assessment of risk <b>levels.</b> <b>Redundant</b> features of data are deleted that can reduce the computation complexity of multiclass SVM and improve the learning and the generalization ability. Multiclass SVM trained with the empirical data can predict the risk level. Experiment shows that the predict result has relatively high precision, and the method is validity for power network risk assessment...|$|R
40|$|This paper {{analyzes}} {{the amount of}} redundant computation {{at a global level}} within selected benchmarks of the SPEC 95 and SPEC 2000 benchmark suites. Local <b>level</b> <b>redundant</b> computations are redundant computations that are the result of a single static instruction (i. e. PC dependent) while global <b>level</b> <b>redundant</b> computations are redundant computations that are the result of multiple static instructions (i. e. PC independent). The results show that for all benchmarks, less than 10 % of the input sets account more than 65 % of the dynamic instructions; an input set is defined as an instruction's opcode, input operands, and PC. In addition, for 8 of the 15 benchmarks profiled in this paper, less than 10 % of the input sets accounted for over 90 % of the dynamic instructions. Additionally, less than 1000 (0. 14 %) of the most frequently occurring input sets accounted for 19. 4 % - 95. 5 % of the dynamic instructions. Furthermore, more potential for value reuse exists at the global level as compared to the traditional local level. For an equal number of input sets [...] approximately 100 for each benchmark [...] at both the global and local levels, the global level input sets accounted for an additional 1. 5 % to 12. 6 % {{of the total number of}} dynamic instructions as compared to the local level input sets. As a result, exploiting value reuse at the global level should yield a significant performance improvement as compared to exploiting reuse only at the local level. ...|$|R
40|$|The {{characters}} of nitrate reductase-deficiency and nitrate tolerant nodulation in pea (Pisum sativum L.) were combined by crossing the nitrate reductase-deficient mutants A 317 and A 334 with the supernodulating nod 3 mutant. The research assessed {{the effects of}} these combinations on the inhibitory effect of NO 3 on nitrogenase activity. The {{characters of}} nitrate reductase-deficiency and nitrate tolerant nodulation in pea (Pisum sativum L.) were combined crossing the nitrate reductase-deficient A 317 and A 334 with the supemodulating nod 3 mutant. Filial generations were screened for the extent of nodulation and nitrate reductase activity or for leaf necrosis in the presence of rhizospheric nitrate. Conditions for assaying in vivo leaf nitrate reductase activity of pea were optimised with respect to environmental and plant factors as well as the composition of the incubation medium. The character of nitrate reductase (NR) deficiency was shown to be an incompletely recessive feature, with a locus located on a chromosome separate to that of the supernodulating gene and thus inherited independently. The development of necrosis on leaves of the NR-deficient A 317 supplied nitrate was shown to be a reliable method of screening for this mutant, particularly at lower photosynthetic photon flux densities. However, the double mutant A 317 -nod 3 developed only slight necrosis. Nitrate uptake by the NR-deficient mutants equalled that of its wild-type parent. While specific NR activity in the mutant A 317 was only 5 % that of the wild-type, it nevertheless reduced 68 % of the nitrate absorbed over a 7 -day period, compared to 83 % {{in the case of the}} wild-type. Nitrate reductase would thus appear to be present in <b>redundant</b> <b>levels</b> in the wild-type. Short-term (2 days) alleviation of nitrate inhibition of nitrogenase activity in the NR-deficient A 317 was confirmed. A positive and significant correlation was found between nitrate reduction and the extent to which nitrogenase activity was inhibited during the first four days of nitrate treatment. No correlations were found between either nodule starch levels or photosynthetic rate and nitrogenase activity. A combination of the NR deficiency and supernodulating characters did not result in symbioses that were tolerant to nitrate, although such double mutants were less likely to exhibit necrosis in the presence of nitrate. Ammonium nitrate was found to be more inhibitory of nitrogenase activity than nitrate alone. These results are consistent with a product feedback mechanism for the inhibitory effects of nitrate on nitrogenase activity. Selection for mutants deficient in nitrate uptake rather than nitrate reduction would thus appear to be a superior strategy for creating a legulne-rhizobia symbiosis that is insensitive to nitrate...|$|R
40|$|In space {{application}} the precision level measurement of cryogenic liquids {{in the storage}} tanks is done using triple <b>redundant</b> capacitance <b>level</b> sensor, for control and safety point of view. The linearity of each sensor element depends upon the cylindricity and concentricity of {{the internal and external}} electrodes. The complexity of calibrating all sensors together has been addressed by two step calibration methodology which has been developed and used for the calibration of six capacitance sensors. All calibrations are done using Liquid Nitrogen (LN 2) as a cryogenic fluid. In the first step of calibration, one of the elements of Liquid Hydrogen (LH 2) level sensor is calibrated using 700 mm eleven point discrete diode array. Four wire method has been used for the diode array. Thus a linearity curve for a single element of LH 2 is obtained. In second step of calibration, using the equation thus obtained for the above sensor, it is considered as a reference for calibrating remaining elements of the same LH 2 sensor and other level sensor (either Liquid Oxygen (LOX) or LH 2). The elimination of stray capacitance for the capacitance level probes has been attempted. The automatic data logging of capacitance values through GPIB is done using LabVIEW 8. 5...|$|R
40|$|The {{assistance}} of users in their {{activities of daily}} life by a smart environment is the main goal of Ambient Assisted Living (AAL). In this case, interaction {{is of particular interest}} since some users are very familiar with modern technology and for some users this technology is very challanging so that poorly designed interaction metaphors will lead to a low acceptance. Additionally, AAL has to cope with the challenges of open systems in which at any time new devices and functionalities can appear. This paper presents a gesture based approach to control devices and their functionalities in a smart environment at a semantic level to issue a command or to set a <b>level.</b> <b>Redundant</b> functionalities are filtered out before presenting the list of functions to the user. This concept is validated by a demonstrator that uses the semantic AAL platform universAAL...|$|R
40|$|This study {{set out to}} empirically {{examine the}} revised {{architectural}} design of the Strengthened Australian Qualifications Framework (AQF) (Version 6). Through the use of survey methods and Item Response Theory, the study was able to: estimate the complexity of each Levels Criteria, and, for each set, compare the average estimates against the proposed 10 levels structure; examine the ordered nature of the Levels Criteria according to the Knowledge, Skills and Application dimensions; estimate the complexity of each Qualification Type Descriptor {{for each of the}} 14 Qualification Types; directly estimate the complexity of each Qualification Type by aggregating respondent results; determine the overall complexity estimate of each of the 14 Qualification Types; identify any potentially <b>redundant</b> and non-discriminating <b>Levels</b> Criteria and/or Qualification Type Descriptors; empirically calibrate the Qualification Type Descriptors and the Levels Criteria on the same scale; determine where each Qualification Type was typically positioned within the proposed 10 levels structure; investigate the perceived appropriateness of the assigned notional duration of student learning for each Qualification Type...|$|R
40|$|Aspect-oriented {{software}} development is gaining popularity with {{the adoption of}} languages such as AspectJ. Testing {{is an important part}} in any {{software development}}, including aspect-oriented development. To automate generation of unit tests for AspectJ programs, we can apply the existing tools that automate generation of unit tests for Java programs. However, these tools can generate a large number of test inputs, and manually inspecting the behavior of the software on all these inputs is time consuming. We propose Raspect, a framework for detecting redundant unit tests for AspectJ programs. We introduce three levels of units in AspectJ programs: advised methods, advice, and intertype methods. We show how to detect at each <b>level</b> <b>redundant</b> tests that do not exercise new behavior. Our approach selects only non-redundant tests from the automatically generated test suites, thus allowing the developer to spend less time in inspecting this reduced set of tests. We have implemented Raspect and applied it on 12 subjects taken from a variety of sources; our experience shows that Raspect can effectively reduce the size of generated test suites for inspecting AspectJ programs. ...|$|R
40|$|Optimal and {{suboptimal}} decentralized estimators in {{wireless sensor networks}} (WSNs) over orthogonal multiple-access fading {{channels are}} studied in this paper. Considering multiple-bit quantization before digital transmission, we develop maximum likelihood estimators (MLEs) with both known and unknown channel state information (CSI). When training symbols are available, we derive a MLE that is a special case of the MLE with unknown CSI. It implicitly uses the training symbols to estimate the channel coefficients and exploits the estimated CSI in an optimal way. To reduce the computational complexity, we propose suboptimal estimators. These estimators exploit both signal and data <b>level</b> <b>redundant</b> information to improve the estimation performance. The proposed MLEs reduce to traditional fusion based or diversity based estimators when communications or observations are perfect. By introducing a general message function, the proposed estimators can be applied when various analog or digital transmission schemes are used. The simulations show that the estimators using digital communications with multiple-bit quantization outperform the estimator using analog-and-forwarding transmission in fading channels. When considering the total bandwidth and energy constraints, the MLE using multiple-bit quantization is superior to that using binary quantization at medium and high observation signal-to-noise ratio levels...|$|R
40|$|The aim of {{this study}} was to {{undertake}} an empirical analysis of the revised design of the strengthened Australian Qualifications Framework. In particular, four elements of the revised framework were to be examined: the levels structure, with 10 levels expressed as learning outcomes (referred to as 'levels criteria'); revised descriptors for each of the existing 14 qualification types (and two kinds [the Master's and Doctoral Degree qualifications types had two kinds: other and research]) expressed as learning outcomes (referred to as 'qualification type descriptors'); the relationship between the qualification types and the levels structure; [and] an estimate of the notional duration of student learning for each qualification type. The major aims of the empirical validation were to: estimate the complexity of the criteria for each of the levels, and for each set, compare the estimates with the proposed 10 -level structure; estimate the complexity of each qualification type descriptor for each of the 14 qualification types; identify any potentially <b>redundant</b> and non-discriminating <b>levels</b> criteria and/or qualification type descriptors; determine where each qualification type is typically positioned within the proposed 10 -level structure; [and] investigate the adequacy of the suggested duration for each qualification type...|$|R
50|$|The {{closure of}} the Three Bridges-Ashurst Junction line after the last train on Sunday 1 January 1967 spelt the end for the high level station and St Margaret's Loop which would receive no further traffic. The goods yard had been {{virtually}} closed for some time except for coal and all freight facilities were formally withdrawn as from 10 April. Although very nearly <b>redundant,</b> the high <b>level</b> goods sidings could still be accessed via the low to high level connecting spur; both spurs serving the high level station were however closed in 1967. The last train to use the station was in February 1968, a tracklifting train hauled by a Class 33 diesel locomotive. As the footbridge at the low-level platforms had been demolished in Summer 1965, passengers used the high level station as a short-cut between platforms, thereby avoiding the need to go around a local housing estate. The high level's demolition in 1970 led to protests from passengers {{at the loss of}} the short-cut, as a result of which British Rail erected a footbridge which today marks the site of the high level station.|$|R
30|$|Due to the {{heterogeneity}} of covariates that are spatially correlated, but unknown, or only insufficiently available, {{in many cases the}} response data are autocorrelated. Haining et al. ([2009]) presented a simple conditional autoregressive model to deal with autocorrelated count data which results in the estimation of spatially correlated random effects. Fahrmeir and EchavarrÄ± ([2006]) introduce an extensive methodology using structured additive regression models STAR for overdispersed and zero-inflated count data. These models make it possible to model non-linear covariate effects, individual or cluster-specific uncorrelated random effects, spatially correlated random effects or 2 -dimensional spatial trend surfaces simultaneously. The methodology employed in our investigation (Wood [2006]) offers similar technical possibilities for the Poisson and negative binomial distributions and, in combination with Rigby and Stasinopoulosâ€™s ([2005]) methods, for the zero-inflated Poisson distribution. The inventory plots are located exactly via coordinates, and hence a 2 -dimensional spatial trend function is fitted instead of spatially correlated random effects for distinct areas. The observations at the 4 subplots were aggregated due to their proximity, which makes the estimation of uncorrelated random effects at the plot <b>level</b> <b>redundant.</b> Overall, the approach adopted combines an appropriate distribution assumption for count data with non-linear effects of causal covariates and an advanced method for covering spatial autocorrelation.|$|R
40|$|Reliable {{life support}} systems are {{required}} for deep space missions. The probability of a fatal life support failure should be less than one in a thousand in a multi-year mission. It is far too expensive to develop a single system with such high reliability. Using three redundant units would require only that each have a failure probability of one in ten over the mission. Since the system development cost is inverse to the failure probability, this would cut cost {{by a factor of}} one hundred. Using replaceable subsystems instead of full systems would further cut cost. Using full sets of replaceable components improves reliability more than using complete systems as spares, since a set of components could repair many different failures instead of just one. Replaceable components would require more tools, space, and planning than full systems or replaceable subsystems. However, identical system redundancy cannot be relied on in practice. Common cause failures can disable all the identical <b>redundant</b> systems. Typical <b>levels</b> of common cause failures will defeat redundancy greater than two. Diverse redundant systems are required for reliable space life support. Three, four, or five diverse redundant systems could be needed for sufficient reliability. One system with lower level repair could be substituted for two diverse systems to save cost...|$|R
40|$|Motivated by future {{processors}} {{that will}} contain {{an abundance of}} execution cores, we believe redundant execution will be a practical method for increasing system security and reliability. However, redundant execution relies {{on the premise that}} duplicating external inputs identically to a set of replicas will produce identical outputs. Unfortunately, multi-threaded applications exhibit non-determinism that breaks this premise, especially on the multiprocessors that will be widely available in the future. This thesis presents a method for deterministically replicating the accesses to shared memory made by concurrent threads in a kernel <b>level</b> <b>redundant</b> execution system. Our approach relies on user space annotations, which define sequential regions in the applica-tion that the kernel will schedule deterministically. Further, we show that these annota-tions can be largely derived from the use of locks already present in the application and that replication can be achieved with only modest overhead. ii Acknowledgements I would first like to thank Professor David Lie for his tireless contributions to this thesis. Our many discussions and analysis sessions formed the foundation of my work. Thank you for pushing me to always be at my best. I also acknowledge the contributions of Ian Sin, with whom I worked on many aspects of this thesis. Our debates and time at the whiteboard were of great value to me and I will always look back on those times with fond regard...|$|R
40|$|Elementary Ca(2 +) signals, such as "Ca(2 +) puffs", which {{arise from}} the {{activation}} of inositol 1, 4, 5 -trisphosphate receptors, are building blocks for local and global Ca(2 +) signalling. We characterized Ca(2 +) puffs in six cell types that expressed differing ratios of the three inositol 1, 4, 5 -trisphosphate receptor isoforms. The amplitudes, spatial spreads and kinetics of the events were similar {{in each of the}} cell types. The resemblance of Ca(2 +) puffs in these cell types suggests that they are a generic elementary Ca(2 +) signal and, furthermore, that the different inositol 1, 4, 5 -trisphosphate isoforms are functionally <b>redundant</b> at the <b>level</b> of subcellular Ca(2 +) signalling. Hormonal stimulation of SH-SY 5 Y neuroblastoma cells and HeLa cells for several hours downregulated inositol 1, 4, 5 -trisphosphate expression and concomitantly altered the properties of the Ca(2 +) puffs. The amplitude and duration of Ca(2 +) puffs were substantially reduced. In addition, the number of Ca(2 +) puff sites active during the onset of a Ca(2 +) wave declined. The consequence of the changes in Ca(2 +) puff properties was that cells displayed a lower propensity to trigger regenerative Ca(2 +) waves. Therefore, Ca(2 +) puffs underlie inositol 1, 4, 5 -trisphosphate signalling in diverse cell types and are focal points for regulation of cellular responses. status: publishe...|$|R
40|$|Elementary Ca 2 + signals, such as "Ca 2 + puffs", which {{arise from}} the {{activation}} of inositol 1, 4, 5 -trisphosphate receptors, are building blocks for local and global Ca 2 + signalling. We characterized Ca 2 + puffs in six cell types that expressed differing ratios of the three inositol 1, 4, 5 -trisphosphate receptor isoforms. The amplitudes, spatial spreads and kinetics of the events were similar {{in each of the}} cell types. The resemblance of Ca 2 + puffs in these cell types suggests that they are a generic elementary Ca 2 + signal and, furthermore, that the different inositol 1, 4, 5 -trisphosphate isoforms are functionally <b>redundant</b> at the <b>level</b> of subcellular Ca 2 + signalling. Hormonal stimulation of SH-SY 5 Y neuroblastoma cells and HeLa cells for several hours downregulated inositol 1, 4, 5 -trisphosphate expression and concomitantly altered the properties of the Ca 2 + puffs. The amplitude and duration of Ca 2 + puffs were substantially reduced. In addition, the number of Ca 2 + puff sites active during the onset of a Ca 2 + wave declined. The consequence of the changes in Ca 2 + puff properties was that cells displayed a lower propensity to trigger regenerative Ca 2 + waves. Therefore, Ca 2 + puffs underlie inositol 1, 4, 5 -trisphosphate signalling in diverse cell types and are focal points for regulation of cellular responses...|$|R
40|$|It {{has been}} {{suggested}} that sexual identity in the germline depends upon the combination of a nonautonomous somatic signaling pathway and an autonomous X chromosome counting system. In the studies reported here, we have examined the role of the sexual differentiation genes transformer (tra) and doublesex (dsx) in regulating the activity of the somatic signaling pathway. We asked whether ectopic somatic expression of the female products of the tra and dsx genes could feminize the germline of XY animals. We find that Tra(F) is sufficient to feminize XY germ cells, shutting off the expression of male-specific markers and activating the expression of female-specific markers. Feminization of the germline depends upon the constitutively expressed transformer- 2 (tra- 2) gene, but does not seem to require a functional dsx gene. However, feminization of XY germ cells by Tra(F) can be blocked by the male form of the Dsx protein (Dsx(M)). Expression of the female form of dsx, Dsx(F), in XY animals also induced germline expression of female markers. Taken together with a previous analysis of the effects of mutations in tra, tra- 2, and dsx on the feminization of XX germ cells in XX animals, our findings indicate that the somatic signaling pathway is <b>redundant</b> at the <b>level</b> tra and dsx. Finally, our studies call into question the idea that a cell-autonomous X chromosome counting system plays a central role in germline sex determination...|$|R
40|$|In {{difficult}} listening situations, e. g. in {{a cocktail}} party scenario, speech signal that a listener is interested in decoding is often masked by unwanted noise, disrupting bottom-up signalling. A normal hearing (NH) listener is able to withstand {{a reasonable amount of}} such disruption and can still achieve good speech perception. This is possible partially because the information encoded in human speech is <b>redundant</b> at various <b>levels</b> like phonetic, morphemic, sentential, discourse, etc. In addition, NH listeners employ various cognitive mechanisms to reconstruct the lost information from disrupted speech signal. Some of these mechanisms are: application of the knowledge of the speakerâ€™s language and its grammatical conventions, expectation formulated {{on the basis of the}} information collected previously in the discourse, tracking the auditory information within the speech stream, etc. As a result, (i) some loss of information can be tolerated without loss of meaning, and (ii) some information can be reconstructed on the basis of the leftover information. A cochlear implant (CI) is an implantable electronic device that partially reconstructs hearing for individuals with profound or total sensorineural hearing loss. CI users have greater difficulty in understanding speech than NH listeners in challenging listening scenarios where target speech signals are disrupted, e. g. with multiple individuals talking simultaneously or in noisy surroundings. This thesis explores if various factors inherent to the signal, and deficits of hearing impairment and/or characteristic of CI signal transmission may be at least partially responsible for the CI users having reduced ability of understanding speech in difficult listening scenarios. I conclude in the thesis that CI users can effectively use top-down restoration mechanisms but the nature and/or extent of the working of these mechanisms may be different in CI users than in NH listeners due to certain factors that may themselves vary among the CI users...|$|R
40|$|This project {{investigates the}} {{legitimacy}} of the five Danish counties (regioner), which constitutes the regional level. Furthermore, how the existence of these counties affect the Danish political structure (statsopbygning). The motivation behind this investigation is the 2007 political reform (Strukturreformen), which changed the political structure. The reform entailed that 13 counties (amter) got reduced to five. Hence, the division of labour between the state-, regional- and municipality level changed. The result is that the counties are left with healthcare as their primary responsibility. In 2009 the county of Midtjylland decided to place a new hospital near GÃ¸dstrup, this decision caused political turmoil and became a nationwide issue, when the then Government and their supporting party, Danish Peoples Party (Dansk Folkeparti), included the hospital {{as a part of the}} negotiations of the Finance Bill; a decision that clearly is within the jurisdiction of the counties. This questions {{the legitimacy of}} the counties and is the motivation for this project. Additionally this points to the significance of this particular political level, being a part of the Danish political structure. Firstly, the input- and output legitimacy of the counties is analysed with David Eastons theory on political systems as a starting point. Secondly, the countiesâ€™ position in the political structure is analysed, applying Jean-Jaques Rousseau and James Madison and their notions on the considerations and principles behind political structure respectively. Lastly, the difficult position alongside the possible future of the counties is discussed and reflected upon. The project concludes that the counties, in their current form, are more or less <b>redundant</b> as a <b>level</b> in the Danish political structure. This denotes that the counties either should be broaden in the sense of more responsibility or be decommissioned. At the current moment we find that the national politicians highly disagrees upon the existence of the counties. Thus their mere existence seems to depend upon the political majority in Parliament...|$|R
40|$|Hyperspectral imaging {{provides}} {{the capability of}} increased sensitivity and discrimination over traditional imaging methods by combining standard digital imaging with spectroscopic methods. For each individual pixel in a hyperspectral image (HSI), a continuous spectrum is sampled as the spectral reflectance/radiance signature to facilitate identification of ground cover and surface material. The abundant spectrum knowledge allows all available information from the data to be mined. The superior qualities within hyperspectral imaging allow wide applications such as mineral exploration, agriculture monitoring, and ecological surveillance, etc. The processing of massive high-dimensional HSI datasets is a challenge since many data processing techniques have a computational complexity that grows exponentially with the dimension. Besides, a HSI dataset may contain {{a limited number of}} degrees of freedom due to the high correlations between data points and among the spectra. On the other hand, merely taking advantage of the sampled spectrum of individual HSI data point may produce inaccurate results due to the mixed nature of raw HSI data, such as mixed pixels, optical interferences and etc. Fusion strategies are widely adopted in data processing to achieve better performance, especially in the field of classification and clustering. There are mainly three types of fusion strategies, namely low-level data fusion, intermediate-level feature fusion, and high-level decision fusion. Low-level data fusion combines multi-source data that is expected to be complementary or cooperative. Intermediate-level feature fusion aims at selection and combination of features to remove <b>redundant</b> information. Decision <b>level</b> fusion exploits a set of classifiers to provide more accurate results. The fusion strategies have wide applications including HSI data processing. With the fast development of multiple remote sensing modalities, e. g. Very High Resolution (VHR) optical sensors, LiDAR, etc., fusion of multi-source data can in principal produce more detailed information than each single source. On the other hand, besides the abundant spectral information contained in HSI data, features such as texture and shape may be employed to represent data points from a spatial perspective. Furthermore, feature fusion also includes the strategy of removing redundant and noisy features in the dataset. One of the major problems in machine learning and pattern recognition is to develop appropriate representations for complex nonlinear data. In HSI processing, a particular data point is usually described as a vector with coordinates corresponding to the intensities measured in the spectral bands. This vector representation permits the application of linear and nonlinear transformations with linear algebra to find an alternative representation of the data. More generally, HSI is multi-dimensional in nature and the vector representation may lose the contextual correlations. Tensor representation provides a more sophisticated modeling technique and a higher-order generalization to linear subspace analysis. In graph theory, data points can be generalized as nodes with connectivities measured from the proximity of a local neighborhood. The graph-based framework efficiently characterizes the relationships among the data and allows for convenient mathematical manipulation in many applications, such as data clustering, feature extraction, feature selection and data alignment. In this thesis, graph-based approaches applied in the field of multi-source feature and data fusion in remote sensing area are explored. We will mainly investigate the fusion of spatial, spectral and LiDAR information with linear and multilinear algebra under graph-based framework for data clustering and classification problems...|$|R
40|$|The most densely {{populated}} and economical most valuable areas in The Netherlands lie below mean sea level. These areas are protected against the sea by a coastal dune system. The vital {{importance of this}} dune system {{is reflected in the}} extensive collection of Dutch legal regulations that ensure the safety level of the dunes. Current safety assessment method for dunes, prescribed by these regulations, is based on conducting large numbers (thousands) of simulations to estimate dune erosion at individual locations along the Dutch coast. With the use of more complex dune-erosion models (i. e. Xbeach; Roelvink et al. (2009)) in the safety assessment, the method gets computationally more intensive. This means that conducting large numbers of simulations for a dune safety assessment are not feasible. Therefore new probabilistic approaches (e. g., Bayesian approach) are needed in order to apply state-of-the-art insights and models for dune erosion in a safety assessment. The aim {{of this study was to}} gain an insight in the applicability of the Bayesian network approach for dune safety assessment against extreme storm events on the Dutch coast. First a database was generated that serves as input to the Bayesian network. The content of this database is obtained in a way similar to current assessment method. This means, data-sources (wave conditions, bottom profiles) and the dune-erosion model (duros+; Vellinga (1986), van Gent et al. (2008)) to simulate the dune erosion process were equal. Qualitatively the Bayesian network is represented by nodes (variables) and arrows (relations). Variables and relations were selected such, they were physically related to the dune erosion process and could be obtained out of the database. Quantitatively the Bayesian network is described by a conditional probability table defined by a expectation-maximization training algorithm. For this training process, cases (a case is a record in the database and represents a storm event) out of the dataset were selected. Sensitivity analysis of the required number of training data, as well the determining variable of this Bayesian network were made. Results show the Bayesian network is capable of reproducing the dune erosion process (given this set of data) for 89 %. The loss in skill is a consequence of the discretization of the variables in the Bayesian network. The number of training-cases needed to make reliable predictions that are more accurate then predictions made with the use of the prior probability, is approximately 5, 000 cases. Results of the sensitivity analysis indicates water level information is a determining term regarding the prediction skill. However, when other hydraulic conditions (wave height, peak period) were known the water <b>level</b> was <b>redundant.</b> Indicating water <b>level</b> information is captured by the variables wave height and/or peak period. Dune erosion volumes predictions for locations at the Holland coast, shows a high amount of uncertainty and results in unreliable predictions (negative log-likelihood ratio) when using a Bayesian network trained on cases representing the Wadden coast. Indicating coastal features between both coastal regions are too diverse. Furthermore, a Bayesian network is a useful tool to improve insight into data. Expected relations between variables can be investigated and visualized in the Bayesian network immediately. Coastal EngineeringHydraulic EngineeringCivil Engineering and Geoscience...|$|R

