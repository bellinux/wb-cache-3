0|4448|Public
30|$|After {{calculating the}} <b>residual</b> <b>errors</b> ɛ(t)′ {{of the model}} AR(13), the <b>residual</b> <b>errors</b> ɛ(t)′ needed to be {{verified}} whether they were the white noise. Finally, the test function of the white noise in MATLAB was used and the <b>residual</b> <b>errors</b> ɛ(t)′ of the model AR(13) were verified to be the white noise. Thus the <b>residual</b> <b>errors</b> ɛ(t)′ of the model AR(13) could replace the <b>residual</b> <b>errors</b> ɛ(t) of the model ARMA(6, 4).|$|R
40|$|Safety {{integrity}} level (SIL) {{verification of}} functional safety fieldbus communication {{is an essential}} part of SIL verification of safety instrumented system (SIS), and it requires quantifying <b>residual</b> <b>error</b> probability (RP) and <b>residual</b> <b>error</b> rate of function safety communication. The present quantification method of <b>residual</b> <b>error</b> rate uses RP of cyclic redundancy check (CRC) to approximately replace the total RP of functional safety communication. Since CRC only detects data integrity-related errors and CRC has intrinsically undetected <b>error,</b> some other <b>residual</b> <b>errors</b> are not being considered. This research found some <b>residual</b> <b>errors</b> of the present quantification method. Then, this research presents an extended new approach, which takes the found <b>residual</b> <b>errors</b> into account to determine more comprehensive and reasonable RP and <b>residual</b> <b>error</b> rate. From perspective of the composition of safety message, this research studies RPs of those controlling segments (sequence number, time expectation, etc.) to cover the found <b>residual</b> <b>errors</b> beyond CRC detection coverage, and the influences of insertion/masquerade errors and time window on RP are investigated. The results turn out these <b>residual</b> <b>errors,</b> especially insertion/masquerade errors, may have a great influence on quantification of <b>residual</b> <b>error</b> rate and SIL verification of functional safety communication, and they should be treated seriously...|$|R
40|$|In case of {{coordinate}} {{machines that}} use CAA correction matrix, {{the issue of}} kinematic errors analysis may {{be based on the}} determination of <b>residual</b> <b>error</b> distribution. Temperature changes have an impact on CMM kinematic structure, which may cause the differences in the map of <b>residual</b> <b>errors.</b> As for today, the <b>residual</b> <b>errors</b> were analysed only for the reference temperature. No research was undertaken on the <b>residual</b> <b>errors</b> changes depending on the temperature variations. This paper presents the experiment aimed at <b>residual</b> <b>errors</b> analysis and resulting errors distributions for different temperatures...|$|R
30|$|In Eq. (8), the <b>residual</b> <b>errors</b> ɛ(t) were {{unknown in}} the model ARMA (6, 4), so the time series fitting <b>residual</b> <b>errors</b> δ (t)^' were not {{calculated}} by the linear derivation. However, if the model order w was large enough, the model AR(w) could approximately substitute the model ARMA(p, q). And the <b>residual</b> <b>errors</b> ɛ(t)′ of the model AR(w) could also replace the unknown <b>residual</b> <b>errors</b> ɛ(t) in the model ARMA (6, 4).|$|R
30|$|Step 3 : Update {{the weights}} {{of the student}} model by back {{propagating}} the <b>residual</b> <b>error,</b> until the <b>residual</b> <b>error</b> is small enough.|$|R
30|$|After the <b>residual</b> <b>errors</b> δ(t)′ of {{time series}} {{analysis}} in Eq. (12) were calculated, the <b>residual</b> <b>errors</b> δ(t)′ needed to be verified whether they were the white noise. Finally, the test function of the white noise in MATLAB was used to verify that the <b>residual</b> <b>errors</b> δ(t)′ of the {{time series analysis}} were the white noise. Thus, combining with the <b>residual</b> <b>errors</b> δ(t)′ of the time series analysis and the fitting value of the multiple linear model yM(t), the comprehensive compensation model yC(t) of the thermal errors of the wear-depth detecting system was obtained.|$|R
40|$|<b>Residual</b> <b>error</b> {{evaluation}} method for deterministic polishing of aspheric optics is studied. Two <b>residual</b> <b>error</b> {{evaluation method}}s, which are axis-direction error method and normal error method respectively, are researched theoretically. It's inferred that the <b>residual</b> <b>error</b> of aspheric surface {{should be evaluated}} by normal error method. A new approach is proposed to calculate normal direction <b>residual</b> <b>error</b> {{on the basis of}} the axis-direction <b>residual</b> <b>error</b> of the aspheric surface. There exists difference between these two kinds of error which increases from the center of the aspheric optic to the edge through the comparison of them. Taking bonnet polishing and numerical controlled small tool polishing as examples, experiments are made to quantitatively prove that using axis-direction error method to evaluate <b>residual</b> <b>error</b> in deterministic polishing would introduce different degrees of processing error. It's found that the processing error is positively correlated with the relative aperture of aspheric optics, which is the ratio of the optic's aperture and vertex's curvature radius. Therefore, it is recommended to use axis-direction error method instead of normal error method as the evaluation method of the <b>residual</b> <b>error</b> during deterministic polishing aspheric optics with relatively small relative aperture; the opposite is the other way around. ? 2014 Journal of Mechanical Engineering...|$|R
50|$|In {{order to}} use Peirce's criterion, one must first {{understand}} the input and return values. Regression analysis (or the fitting of curves to data) results in <b>residual</b> <b>errors</b> (or {{the difference between}} the fitted curve and the observation points). Therefore, each observation point has a <b>residual</b> <b>error</b> associated with a fitted curve. By taking the square (i.e., <b>residual</b> <b>error</b> raised to the power of two), <b>residual</b> <b>errors</b> are expressed as positive values. If the squared error is too large (i.e., due to a poor observation) it can cause problems with the regression parameters (e.g., slope and intercept for a linear curve) retrieved from the curve fitting.|$|R
30|$|According to this observation, it is {{concluded}} that the optimal block size can be predicted based on the MB <b>residual</b> <b>error.</b> If the <b>residual</b> <b>error</b> is small when ME for large block is performed, a large block size {{can be used as}} the optimal mode. In contrast, if the <b>residual</b> <b>error</b> is not small enough to select a large block, smaller block sizes are considered to determine the optimal block size mode.|$|R
30|$|The {{average of}} <b>residual</b> <b>errors</b> for {{different}} reconstruction algorithm was also listed in Table  4. As previously indicated, TOF reconstructions could improve cold contrast by about 15 % and therefore exhibited significant smaller <b>residual</b> <b>error</b> compared with non-TOF reconstructions.|$|R
40|$|A speech coding {{algorithm}} was developed {{which was based}} on a new method of selecting the excitation signal from a codebook of <b>residual</b> <b>error</b> sequences. The <b>residual</b> <b>error</b> sequences in the codebook were generated from 512 frames of real speech signals. L. P. C. inverse filtering was used to obtain the residual signal. Each <b>residual</b> <b>error</b> signal was assigned an index. The index was generated using a moments algorithm. These indices were stored on a Graded Binary Tree. A Binary Search was then used to select the correct index. The use of a Graded Binary Tree in the {{coding algorithm}} reduced the search time. The algorithm faithfully reproduced the original speech when the test <b>residual</b> <b>error</b> signal was chosen from the training data. When the test <b>residual</b> <b>error</b> signal was outside the training data, synthetic speech of a recognisable quality was produced. Finally, the fundamentals of speech coders are discussed in detail and various developments are suggested...|$|R
30|$|Once object {{images taken}} from {{different}} perspectives are imported to PhotoModeler, building the model involves human interaction, {{to pick and}} match feature points in different photos. Human error in the modeling process is accessed in terms of <b>residual</b> <b>error.</b> The computational algorithm used in PhotoModeler, called bundle adjustment, essentially applies the colliearity equations (Eqs. 1 and 2) to simultaneously fix (1) camera orientations, (2) object and image point coordinates and (3) the <b>residual</b> <b>error,</b> with an objective to minimize the <b>residual</b> <b>error.</b>|$|R
30|$|Compute the <b>residual</b> <b>error</b> {{estimator}} η_h_k.|$|R
40|$|Introduction: In {{pharmacokinetic}} modelling, {{a combined}} proportional and additive <b>residual</b> <b>error</b> model is often preferred over a proportional or additive <b>residual</b> <b>error</b> model. Different approaches have been proposed, but {{a comparison between}} approaches is still lacking. Methods: The theoretical background of the methods is described. Method VAR assumes that the variance of the <b>residual</b> <b>error</b> {{is the sum of}} the statistically independent proportional and additive components; this method can be coded in three ways. Method SD assumes that the standard deviation of the <b>residual</b> <b>error</b> is the sum of the proportional and additive components. Using datasets from literature and simulations based on these datasets, the methods are compared using NONMEM. Results: The different coding of methods VAR yield identical results. Using method SD, the values of the parameters describing <b>residual</b> <b>error</b> are lower than for method VAR, but the values of the structural parameters and their inter-individual variability are hardly affected by the choice of the method. Conclusion: Both methods are valid approaches in combined proportional and additive <b>residual</b> <b>error</b> modelling, and selection may be based on OFV. When the result of an analysis is used for simulation purposes, it is essential that the simulation tool uses the same method as used during analysis...|$|R
40|$|A Luby Transform (LT) coded {{downlink}} Spatial Division Multiple Access (SDMA) system using iterative detection is proposed, which invokes a low-complexity near-Maximum-Likelihood (ML) Sphere Decoder (SD). The Ethernet-based Internet {{section of}} the transmission chain inflicts random packet erasures, which is modelled by the Binary Erasure Channel (BEC), which the wireless downlink imposes both fading and noise. A novel log-Likelihood Ratio based packet reliability meric is used for identifying the channel-decoded packets, which {{are likely to be}} error-infested. Packets having <b>residual</b> <b>errors</b> must not be passed on to the KT decoder for the sake of avoiding LT-decoding –induced error propagation. The proposed scheme is capable of maintaining an infinitesimally low packet <b>error</b> <b>ratio</b> in the downlink of the wireless Internet for Eb/n 0 values in excess of about 3 dB...|$|R
3000|$|... is a <b>residual</b> <b>error.</b> A {{separate}} <b>residual</b> <b>error</b> {{variance was}} fitted to each stress level. The least squares means for stress x genotype interactions were computed and pair wise comparisons involving the same genotype were made. The model was fitted using the MIXED procedure in SAS (Littell et al., 2006).|$|R
30|$|Step 2 : Calculate the <b>residual</b> <b>error</b> using Eq. 12.|$|R
5000|$|Restriction - downsampling the <b>residual</b> <b>error</b> to a coarser grid.|$|R
3000|$|... behaves erratically {{for very}} ill-conditioned systems [14] where {{the norm of}} the <b>residual</b> <b>error</b> may {{increase}} occasionally and causes the maximum curvature detector to mistakenly detect the maximum curvature at the wrong stage index. To overcome this problem, we exploit {{the fact that the}} <b>residual</b> <b>error</b> decay exponentially and therefore can be fitted by a decaying exponential. The resulting exponentially fitted curve is used instead in computing the maximum curvature and hence the optimal stage index. This method proved to be efficient in combating the erratic behavior of the <b>residual</b> <b>error</b> norm of the LPIC detector based on the CGLS.|$|R
40|$|Abstract − Recently, a novel {{approach}} for the estimation of the <b>residual</b> <b>error</b> parameters of a calibrated vector network analyzer has been proposed. The method {{is based on a}} reflection measurement employing a high precision airline terminated by a short. From this measurement the complex valued <b>residual</b> <b>error</b> parameters are calculated utilizing a sophisticated data analysis scheme. In this work the uncertainty associated with the obtained <b>residual</b> <b>error</b> parameters is evaluated. The uncertainty evaluation is performed by applying the GUM S 1 approach employing a Monte-Carlo method. Resulting uncertainties arising due to imperfections of the dimensional parameters of the airline are presented...|$|R
30|$|The {{study also}} {{performed}} {{the test of}} coefficient of rank correlation by Spearman. After removing the absolute value of <b>residual</b> <b>error,</b> we calculate the rank of X and <b>residual</b> <b>error</b> whose absolute value is removed to study {{the relationship between the}} ranks of the absolute value of <b>residual</b> <b>error</b> and other variables. Since the value of significance is below 0.05, the heteroscedasticity of the date exists. We regard the variables whose p value is below 0.05 as the weight and use the weighted least square (WLS) method to estimate and verify the models, in order to eliminate heteroscedasticity.|$|R
40|$|Strong ionospheric {{gradients}} above 300 mm/km {{remain a}} threat for single frequency GBAS users including those conducting CAT II and CAT III precision approaches under GAST-D protocols. This {{can be overcome}} using a network of single-frequency, double- differenced carrier-phase-based monitors as proposed in [1]. The detection performance of this monitor is very sensitive {{to the level of}} carrier phase <b>residual</b> <b>error.</b> Therefore, an architecture using 4 aligned receivers with optimal separation that allows the highest carrier phase <b>residual</b> <b>error</b> (8. 65 mm, one standard deviation) has been proposed in [2]. This level of carrier phase <b>residual</b> <b>error</b> provides 100...|$|R
5000|$|The <b>residuals</b> (<b>error</b> terms) {{should be}} {{normally}} distributed [...] ~ [...]|$|R
3000|$|WM-MOGACORR. Note {{that the}} {{optimality}} {{criteria for the}} set of endmembers sought is the minimization of the unmixing <b>residual</b> <b>error</b> and number of endmembers. The approximation WM-MOGACORR does not attempt to minimize these criteria directly, but nevertheless {{the quality of its}} achieved solution will be evaluated {{on the basis of the}} unmixing <b>residual</b> <b>error.</b>|$|R
30|$|The {{white noise}} with the normal {{distribution}} characteristics is {{the premise of}} the time series model, that is, the multi-element fitting <b>residual</b> <b>errors</b> δ (t) should obey the normal distribution. The existing function of the normal distribution test in MATLAB was used to get that the multi-element fitting <b>residual</b> <b>errors</b> δ (t) obeyed the normal distribution.|$|R
40|$|AbstractThis paper {{presents}} a robust a posteriori <b>residual</b> <b>error</b> estimator for diffusion–convection–reaction problems with anisotropic diffusion, approximated by a SUPG {{finite element method}} on isotropic or anisotropic meshes in Rd, d= 2 or 3. The equivalence between the energy norm of the <b>error</b> and the <b>residual</b> <b>error</b> estimator is proved. Numerical tests confirm the theoretical results...|$|R
30|$|The {{white noise}} {{verification}} of the <b>residual</b> <b>errors</b> ɛ(t)′ {{of the model}} AR(13).|$|R
30|$|The average <b>residual</b> <b>error,</b> due {{to scatter}} and attenuation, was {{higher for the}} mMR than that for the mCT. This was {{expected}} due to higher scatter fraction on the mMR system. The TOF option on the mCT improved lung <b>residual</b> <b>error</b> and cold sphere contrast. In order to reduce average <b>residual</b> <b>error</b> and achieve the same cold contrast levels for the two systems, the number of iterations should be higher on the mMR than that on the mCT. Generally, the mMR performance was more dependent on the reconstruction than the mCT, and reconstruction parameters should therefore be evaluated carefully to maximize {{the performance of the}} mMR system.|$|R
40|$|The paper {{discusses}} {{a generalized}} minimum residual (GMRES) iterative {{solution of the}} magnetic field integral equation (MFIE) applied to frequency domain scattering problems at medium and high frequencies. First, {{the performance of the}} original MFIE is studied, for the perfectly electrically conducting (PEC) sphere. It is shown that the <b>residual</b> <b>error</b> and the solution error do not correlate with each other. Whereas the solution error has already reached a limiting value or even increases, the <b>residual</b> <b>error</b> continues to decrease very fast, typically exponentially. Second, the MFIE is combined with the normal projection of the primary integral equation for the surface magnetic field. Such a technique does not increase the computational complexity of the MFIE. At the same time, it gives a termination criterion for GMRES iterations since the <b>residual</b> <b>error</b> of the combined equation has a typical saturation behavior. In the saturation zone, the <b>residual</b> <b>error</b> and the solution error have approximately the same small value (a typical relative RMS error for the sphere is 1 %). A very similar saturation behavior of the <b>residual</b> <b>error</b> has been observed for other tested PEC scatterers including a cube, a cylinder, and a sphere with one segment cut off (the so-called cat eye) at different frequencies...|$|R
30|$|For the {{proposed}} method, the error assessment {{relying on the}} <b>residual</b> <b>error</b> function is presented [24].|$|R
30|$|The normal {{distribution}} {{testing of the}} multi-element fitting <b>residual</b> <b>errors</b> δ (t) is done as follows.|$|R
40|$|A local {{convergence}} {{analysis of}} Inexact Newton’s method with relative <b>residual</b> <b>error</b> toler-ance for finding a singularity of a differentiable vector field defined on a complete Riemannian manifold, based on majorant principle, {{is presented in}} this paper. We prove that under local assumptions, the inexact Newton method with a fixed relative <b>residual</b> <b>error</b> tolerance converges Q-linearly to a singularity of the vector field under consideration. Using this result we show that the inexact Newton method to find a zero of an analytic vector field can be implemented with a fixed relative <b>residual</b> <b>error</b> tolerance. In the absence of errors, our analysis retrieve the classical local theorem on the Newton method in Riemannian context...|$|R
40|$|We {{will ask}} the {{question}} of whether or not the Regge calculus (and two related simplicial formulations) is a consistent approximation to General Relativity. Our criteria will be based on the behaviour of <b>residual</b> <b>errors</b> in the discrete equations when evaluated on solutions of the Einstein equations. We will show that for generic simplicial lattices the <b>residual</b> <b>errors</b> can not be used to distinguish metrics which are solutions of Einstein’s equations from those that are not. We will conclude that either the Regge calculus is an inconsistent approximation to General Relativity or that it is incorrect to use <b>residual</b> <b>errors</b> in the discrete equations as a criteria to judge the discrete equations. 1...|$|R
40|$|<b>Residual</b> <b>errors</b> of {{hydrological}} {{models are}} usually both heteroscedastic and autocorrelated. However, {{only a few}} studies have attempted to explicitly include these two statistical properties into the <b>residual</b> <b>error</b> model and jointly infer them with the hydrological model parameters. This technical note shows that applying autoregressive error models to raw heteroscedastic residuals, as done in some recent studies, can lead to unstable error models with poor predictive performance. This instability can be avoided by applying the autoregressive process to standardized residuals. The theoretical analysis is supported by empirical findings in three hydrologically distinct catchments. The case studies also highlight strong interactions between the parameters of autoregressive <b>residual</b> <b>error</b> models and the water balance parameters of the hydrological model...|$|R
30|$|Finally, {{obtain the}} {{comparison}} score as the <b>residual</b> <b>errors</b> {{to compute the}} performance of the overall system.|$|R
40|$|Estimation of {{parameter}} and predictive {{uncertainty of}} hydrologic models has traditionally relied on several simplifying assumptions. <b>Residual</b> <b>errors</b> are often {{assumed to be}} independent and to be adequately described by a Gaussian probability distribution {{with a mean of}} zero and a constant variance. Here we investigate to what extent estimates of parameter and predictive uncertainty are affected when these assumptions are relaxed. A formal generalized likelihood function is presented, which extends the applicability of previously used likelihood functions to situations where <b>residual</b> <b>errors</b> are correlated, heteroscedastic, and non?Gaussian with varying degrees of kurtosis and skewness. The approach focuses on a correct statistical description of the data and the total model residuals, without separating out various error sources. Application to Bayesian uncertainty analysis of a conceptual rainfall?runoff model simultaneously identifies the hydrologic model parameters and the appropriate statistical distribution of the <b>residual</b> <b>errors.</b> When applied to daily rainfall?runoff data from a humid basin we find that (1) <b>residual</b> <b>errors</b> are much better described by a heteroscedastic, first?order, auto?correlated error model with a Laplacian distribution function characterized by heavier tails than a Gaussian distribution; and (2) compared to a standard least?squares approach, proper representation of the statistical distribution of <b>residual</b> <b>errors</b> yields tighter predictive uncertainty bands and different parameter uncertainty estimates that are less sensitive to the particular time period used for inference. Application to daily rainfall?runoff data from a semiarid basin with more significant <b>residual</b> <b>errors</b> and systematic underprediction of peak flows shows that (1) multiplicative bias factors can be used to compensate for some of the largest errors and (2) a skewed error distribution yields improved estimates of predictive uncertainty in this semiarid basin with near?zero flows. We conclude that the presented methodology provides improved estimates of parameter and total prediction uncertainty and should be useful for handling complex <b>residual</b> <b>errors</b> in other hydrologic regression models as well. Water ManagementCivil Engineering and Geoscience...|$|R
