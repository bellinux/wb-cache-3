0|10000|Public
40|$|This {{bachelor}} thesis {{deals with}} the problem <b>of</b> <b>rasterization</b> <b>of</b> <b>vector</b> objects on display devices (e. g. computer screen). The <b>rasterization</b> is one <b>of</b> the most important phenomena in computer graphics. The knowledge of mathematical equations and formulas which describe relevant vector objects is necessary to understand this phenomenon. This education software for the visual demonstration <b>of</b> <b>rasterization</b> was created to familiarize users with the <b>rasterization</b> principles <b>of</b> selected <b>vector</b> objects...|$|R
40|$|This bachelor's thesis {{deals with}} sub-pixel vector font representation. It generaly {{describes}} representation and <b>rasterization</b> <b>of</b> <b>vector</b> font and shortly compares advantages <b>of</b> <b>vector</b> font against bitmaped font. Including information about sub-pixel font representation on screen type LCD, possibilities and problems {{that this type}} of projection produces. It contains implementation description of demonstrational application, which rasterizes font with sub-pixel technology...|$|R
40|$|This study aims to map and {{describe}} the environmental modifications occurred in the coastal city of Rio Grande, RS, between 1947 and 2014, resulting from the urbanization process whereby the municipality passed during the analyzed period. The analysis of these changes provides important information to help making decisions regarding the planning and sustainable use of the present ecosystems. The installation of the maritime port in Rio Grande, RS, was the main factor for urban development and parallel changes in the environment. The analysis focused on the most densely urbanized areas of the city, where currently are the most populous neighborhoods that arose in areas adjacent to the center core, and therefore, with more habitats suppressed over the years. To make the comparative analysis possible, an aerial photography mosaic from 1947 was created, the oldest existing for this region, and other with satellite images from 2014, obtained from Google Earth Pro software. The images passed through the georeferencing process and manual vectorization, and, after the <b>rasterization</b> <b>of</b> <b>vector</b> files generated in the mapping, were analyzed and classified in a GIS. The results showed a large expansion of constructed areas in detriment of the suppressed environments of dunes, sandy fields and grasslands. It is expected that this study supports the environmental management activities of the city, directing the growth from now on, {{so as to avoid}} the problems from a chaotic development...|$|R
40|$|State Key Laboratory of the Resources and Environment Information System O 88 RA 100 SA;Institute of Geographic Sciences and Natural Resources Research of the Chinese Academy of Sciences O 66 U 0309 SZError {{evaluation}} <b>of</b> <b>rasterization</b> <b>of</b> <b>vector</b> data {{is one of}} {{the most}} important research topics in the field of geographical information systems. Current methods for evaluating rasterization errors are far from perfect and need further improvement. The objective of this study is to introduce a new error evaluation method that is based on grid cells (EEM-BGC). The EEM-BGC follows four steps. First, the area of each land category inside a square is represented in a vector format. The size and location of the square are exactly the same as those of a grid cell that is to be generated by rasterization. Second, the area is treated as the attribute of the grid cell. Vector data are rasterized into n grids, where n is the number of land categories. Then, the relative area error resulting from rasterization for each land category in the grid cell is calculated in raster format. Lastly, the average of the relative area error for all land categories in the grid cell is computed with the area of a land category as weight. As a case study, the EEM-BGC is applied for evaluating the <b>rasterization</b> error <b>of</b> the land cover data of Beijing at a scale of 1 to 250, 000. It is found that the error derived from a conventional method (denoted as y) is significantly underestimated in comparison with that derived from the new method (denoted as x), with y = 0. 0014 x (2. 6667). The EEM-BGC is effective in capturing not only the spatial distribution <b>of</b> <b>rasterization</b> errors at the grid-cell level but also the numerical distribution range of the errors. The EEM-BGC is more objective and accurate than any conventional method that is used for evaluating rasterization errors...|$|R
40|$|To solve {{customer}} complaints {{better for}} some telecommunication enterprises, {{a new and}} efficient approach through combined <b>rasterization</b> <b>of</b> Geography Information System (GIS) is presented in this paper. By the combined <b>rasterization</b> <b>of</b> surface features and landforms, the historical customer complaints and the real-time running state of network will be utilized to handle new complaints simultaneously. Also, an innovative process to handle complaints is proposed, too. The utilization of method and process will shorten the time of processing routine customer complaints. The experimental {{results show that the}} efficiency to process customer complaints will be improved greatly and the operation expenditure on customer services for telecommunication operators will be also decreased than before.   </p...|$|R
50|$|To {{prevent that}} the user sees the gradual <b>rasterization</b> <b>of</b> the primitives, double {{buffering}} takes place. The rasterization {{is carried out}} in a special memory area. Once the image has been completely rastered, it is copied to the visible area of the image memory.|$|R
5000|$|In 3D rendering, access {{patterns}} for texture mapping and <b>rasterization</b> <b>of</b> small primitives (with arbitrary distortions of complex surfaces) {{are far from}} linear, but can still exhibit spatial locality (e.g. in screen space or texture space) [...] This {{can be turned into}} good memory locality via some combination of morton order ...|$|R
50|$|In {{computer}} graphics, {{a digital}} differential analyzer (DDA) is hardware or software used for interpolation of variables over an interval between start and end point. DDAs are used for <b>rasterization</b> <b>of</b> lines, triangles and polygons. They can be extended to non linear functions, such as perspective correct texture mapping, quadratic curves, and traversing voxels.|$|R
40|$|Work {{deals with}} design and {{implementation}} a tutorial for demonstration deferred shading technique and its possibilities. It explains lighting and shading principles in intuitive and interactive way. Deferred shading is a technique which determines pixel color after the geometry <b>rasterization</b> <b>of</b> the entire scene. In other words the processing of geometry does {{not interfere with the}} shading process...|$|R
40|$|This paper {{presents}} a unified rasterization method to render parametric curves and patches. The algorithms are extremely simple and fast. No multiplication and division are required. The proposed method combines strong sides of parametric and nonparametric representation of curves and patches. A class of curves named as L-curves {{is described in}} here. There is a simple correspondence between their parametric and nonparametric forms. It gives a possibility to divide patch rendering process into two tasks: presentation of a patch in L-form and <b>rasterization</b> <b>of</b> obtained L-curves. L-curves can represent both conic arc and Bezier curves {{and it is possible}} to use them for <b>rasterization</b> <b>of</b> B-splines and conic splines. Also L-curves can be used for interactive graphical design directly (font construction, for example). Hardware-oriented L-curve generation algorithms may be used for nonlinear color interpolation for Bezier patch shading. Final quality of this shading technique is comparable with the Phong shading method...|$|R
50|$|An {{algorithm}} may {{gather data}} from one source, perform some computation in local or on chip memory, and scatter results elsewhere. This {{is essentially the}} full operation of a GPU pipeline when performing 3D rendering- gathering indexed vertices and textures, and scattering shaded pixels in screen space. <b>Rasterization</b> <b>of</b> opaque primitives using a depth buffer is 'commutative', allowing reordering, which facilitates parallel execution. In the general case synchronisation primitives would be needed.|$|R
40|$|We {{present a}} novel method for {{increasing}} the efficiency <b>of</b> stochastic <b>rasterization</b> <b>of</b> motion and defocus blur. Contrary to earlier approaches, our method is efficient even with the low sampling densities commonly encountered in realtime rendering, while allowing the use of arbitrary sampling patterns for maximal image quality. Our clipless dual-space formulation avoids problems with triangles that cross the camera plane during the shutter interval. The method is also simple to plug into existing rendering systems...|$|R
5000|$|<b>Rasterization</b> time <b>of</b> {{the shadow}} volumes {{can be reduced}} by using an in-hardware scissor test to limit the shadows to a {{specific}} onscreen rectangle.|$|R
40|$|We {{present a}} novel {{technique}} for the direct rendering of offset surfaces for polygonal meshes. The visible {{part of the}} offset surface of each triangle, defined as the union of three spheres, three cylinders and a prism, is constructed in a shader program utilizing the geometry shader. Our method creates exact offset surfaces, up to pixel resolution. Possible applications are real-time visualization of offset surfaces, e. g. for GPU-based collision detection or conservative voxelization and <b>rasterization</b> <b>of</b> complex triangle meshes. ...|$|R
50|$|CUPS {{provides}} a mechanism that allows print jobs {{to be sent}} to printers in a standard fashion. The print-data goes to a scheduler which sends jobs to a filter system that converts the print job into a format the printer will understand. The filter system then passes the data on to a backend—a special filter that sends print data to a device or network connection. The system makes extensive use <b>of</b> PostScript and <b>rasterization</b> <b>of</b> data to convert the data into a format suitable for the destination printer.|$|R
5000|$|Like other powder-bed processes, {{surface finish}} and accuracy, object density, and—depending on the {{material}} and process—part strength may be inferior to technologies such as stereolithography (SLA) or selective laser sintering (SLS). Although [...] "stair-stepping" [...] and asymmetrical dimensional properties are features of 3D printing as most other layered manufacturing processes, 3D printing materials are generally consolidated {{in such a way}} that minimizes the difference between vertical and in-plane resolution. The process also lends itself to <b>rasterization</b> <b>of</b> layers at target resolutions, a fast process that can accommodate intersecting solids and other data artifacts.|$|R
40|$|International audienceThis article {{presents}} data-driven scene graphs, {{a set of}} models that {{address the needs of}} safety-critical user interfaces design. Data-driven scene graphs merge a description of the user interface behavior as a data-flow program with a description of its graphics content as a hierarchical structure <b>of</b> <b>vector</b> and raster elements. We present a formal description of these models, discuss their semantics and equivalence, and demonstrate that they are suitable for a class <b>of</b> <b>rasterization</b> optimizations based on selective pre-rendering...|$|R
40|$|This thesis {{focuses on}} usage of {{parallel}} coordinates for line and point parameterizations. The parallel coordinate system represents the space with axes which are mutually parallel. A point from two-dimensional Euclidean space is in parallel coordinates {{represented by a}} line and a line is represented by a point. This property can by used for the Hough transform - a method, where the points of interest vote in parameter space for possible hypotheses. Parameterizations by the parallel coordinates require only <b>rasterization</b> <b>of</b> lines, therefore it is very fast and accurate. In this thesis, the parameterizations are  used for matrix code and vanishing points detection...|$|R
40|$|This paper {{presents}} an analytic formulation for anti-aliased sampling of 2 D polygons and 3 D polyhedra. Our framework allows the exact {{evaluation of the}} convolution integral with a linear function defined on the polytopes. The filter is a spherically symmetric polynomial of any order, supporting approximations to refined variants such as the Mitchell-Netravali filter family. This enables high-quality <b>rasterization</b> <b>of</b> triangles and tetrahedra with linearly interpolated vertex values to regular and non-regular grids. A closed form solution of the convolution is presented and an efficient implementation on the GPU using DirectX and CUDA C is described...|$|R
50|$|With {{respect to}} {{discrete}} GPUs, found in add-in graphics-boards, Nvidia's GeForce and AMD's Radeon GPUs {{are the only}} remaining competitors in the high-end market. Along with its nearest competitor, the AMD Radeon, the GeForce architecture is moving toward general-purpose graphics processor unit (GPGPU). GPGPU is expected to expand GPU functionality beyond the traditional <b>rasterization</b> <b>of</b> 3D graphics, {{to turn it into}} a high-performance computing device able to execute arbitrary programming code in the same way a CPU does, but with different strengths (highly parallel execution of straightforward calculations) and weaknesses (worse performance for complex decision-making code).|$|R
40|$|This paper {{introduces}} {{a new approach}} to the visualization <b>of</b> volumetric <b>vector</b> fields with an adaptive distribution of animated particles that show properties of the underlying steady flow. The shape of the particles illustrates the direction <b>of</b> the <b>vector</b> field in a natural way. The particles are transported along streamlines and their velocity reflects the local magnitude <b>of</b> the <b>vector</b> field. Further physical quantities of the underlying flow can be mapped to the emissive color, the transparency and the length of the particles. A major effort has been made to achieve interactive frame rates for the animation {{of a large number of}} particles while minimizing the error of the computed streamlines. There are three main advantages of the new method. Firstly, the animation of the particles diminishes the inherent occlusion problem <b>of</b> volumetric <b>vector</b> field visualization, as the human eye can trace an animated particle even if it is highly occluded. The second advantage is the variable resolution of the visualization method. More particles are distributed in regions of interest. We present a method to automatically adjust the resolution to features <b>of</b> the <b>vector</b> field. Finally, our method is scalable to the computational and <b>rasterization</b> power <b>of</b> the visualization system by simply adjusting the number of visualized particles...|$|R
40|$|We {{present a}} {{hierarchical}} traversal algorithm for stochastic <b>rasterization</b> <b>of</b> motion blur, which efficiently reduces {{the number of}} inside tests needed to resolve spatio-temporal visibility. Our method is based on novel tile against moving primitive tests that also provide temporal bounds for the overlap. The algorithm works entirely in homogeneous coordinates, supports MSAA, facilitates efficient hierarchical spatio-temporal occlusion culling, and handles typical game workloads with widely varying triangle sizes. Furthermore, we use high-quality sampling patterns based on digital nets, and present a novel reordering that allows efficient procedural generation with good anti-aliasing properties. Finally, we evaluate a set of hierarchical motion blur rasterization algorithms {{in terms of both}} depth buffer bandwidth, shading efficiency, and arithmetic complexity...|$|R
40|$|Direct {{rendering}} of large point clouds has become {{common practice in}} architecture and archaeology in recent years. Due to the high point density no mesh is reconstructed from the scanned data, but the points can be rendered directly as primitives of a graphics API like OpenGL. However, these APIs and the hardware, which they are based on, have been optimized to process triangle meshes. Although current API versions provide lots {{of control over the}} hardware, e. g. by using shaders, some hardware components concerned with <b>rasterization</b> <b>of</b> primitives are still hidden from the programmer. In this paper we show that it might be beneficial for point primitives to abandon the standard graphics APIs and directly switch to a GPGPU API like OpenCL...|$|R
40|$|A {{parallel}} algorithm for the <b>rasterization</b> <b>of</b> polygons {{is presented}} {{that is particularly}} well suited for 3 D Z-buffered graphics implementations. The algorithm represents each edge of a polygon by a linear edge function that has a value greater than zero {{on one side of}} the edge and less than zero on the opposite side. The value of the function can be interpolated with hardware similar to hardware required to interpolate color and Z pixel values. In addition, the edge function of adjacent pixels may be easily computed in parallel. The coefficients of the "Edge function " can be computed from floating point endpoints in such a way that sub-pixel precision of the endpoints can be retained in an elegant way. CR catagories and subject descriptors: 1. 3. 1 [Computer Graphics]...|$|R
40|$|We {{present a}} method for {{analytically}} calculating an anti-aliased <b>rasterization</b> <b>of</b> arbitrary polygons or fonts bounded by Bézier curves in 2 D as well as oriented triangle meshes in 3 D. Our algorithm rasterizes multiple resolutions simultaneously using a hierarchical wavelet representation and is robust to degenerate inputs. We show that using the simplest wavelet, the Haar basis, is equivalent to performing a box-filter to the rasterized image. Because we evaluate wavelet coefficients through line integrals in 2 D, {{we are able to}} derive analytic solutions for polygons that have Bézier curve boundaries of any order, and we provide solutions for quadratic and cubic curves. In 3 D, we compute the wavelet coefficients through analytic surface integrals over triangle meshes and show how to do so in a computationally efficient manner...|$|R
40|$|<b>Rasterization</b> <b>of</b> polygons in 2 D is a {{well known}} problem, {{existing}} several optimal solutions to solve it. The extension of this problem to 3 D is more difficult and most existing solutions are designed to obtain a voxelization of the solid. In this paper {{a new approach to}} rasterize and voxelize solids in 3 D is presented. The described algorithms are very simple, general and robust. The 3 D algorithm is valid {{to be used in the}} new 3 D displays, and it can also be used to voxelize solids delimited by planar faces (with or without holes, manifold or non-manifold). The proposed methods are very suitable for an implementation in graphic hardware rendering system, because it does not use any additional data structure or complex operation...|$|R
5000|$|Z-fighting, {{also called}} stitching, is a {{phenomenon}} in 3D rendering that occurs when two or more primitives have similar or identical values in the z-buffer. It is particularly prevalent with coplanar polygons, where two faces occupy essentially the same space, with neither in front. Affected pixels are rendered with fragments from one polygon or the other arbitrarily, in a manner determined by the precision of the z-buffer. It can also vary as the scene or camera is changed, causing one polygon to [...] "win" [...] the z test, then another, and so on. The overall effect is a flickering, noisy <b>rasterization</b> <b>of</b> two polygons which [...] "fight" [...] to color the screen pixels. This problem is usually caused by limited sub-pixel precision and floating point and fixed point round-off errors.|$|R
40|$|Abstract. In this paper, we {{introduce}} a system <b>of</b> <b>vector</b> equilibrium problems and prove {{the existence of}} a solution. As an application, we derive some existence results for the system <b>of</b> <b>vector</b> variational inequalities. We also establish some existence results for the system <b>of</b> <b>vector</b> optimization problems, which includes the Nash equilibrium problem as a special case. Key Words. System <b>of</b> <b>vector</b> equilibrium problems, system <b>of</b> <b>vector</b> variational inequalities, system <b>of</b> <b>vector</b> optimization problems, Nash equilibrium problem, fixed points...|$|R
5|$|The direct product <b>of</b> <b>vector</b> {{spaces and}} the direct sum <b>of</b> <b>vector</b> spaces {{are two ways}} of {{combining}} an indexed family <b>of</b> <b>vector</b> spaces into a new vector space.|$|R
40|$|In this master’s thesis {{we focus}} on the basic {{properties}} of computer curves and their practical applicability. We explain how the curve can be understood in general, what are polynomial curves and their composing possibilities. Then {{we focus on}} the description of Bezier curves, especially the Bezier cubic. We discuss in more detail some of fundamental algorithms that are used for modelling these curves on computers and then we will show their practical interpretation. Then we explain non uniform rational B-spline curves and De Boor algorithm. In the end we discuss topic <b>rasterization</b> <b>of</b> segment, thick line, circle and ellipse. The aim of master’s thesis is the creation of the set of interactive applets, simulating some of the methods and algorithm we discussed in theoretical part. This applets will help facilitate understanding and will make the teaching more effective...|$|R
40|$|This paper {{describes}} an {{implementation of a}} progressive radiosity algorithm for triangular meshes which works completely on programmable graphics processors. Errors due to the <b>rasterization</b> <b>of</b> triangles are fixed in a post-processing step or with a fragment shader during runtime. Adaptive subdivision to increase {{the accuracy of the}} radiosity solution can be performed during render-time. Since we found that the gradient is not very robust to determine whether triangles should be subdivided or not, we propose a new technique which uses hardware occlusion queries to determine shadow boundaries in image space. The GPU implementation facilitates the simple integration of normal mapping into the radiosity process. Light distribution textures (LDTs) enable us to simulate a variety of real world light sources without much computational overhead. The derivation of such an LDT from a EULUMDAT file is described...|$|R
5000|$|... #Subtitle level 3: Derivatives <b>of</b> <b>vector</b> valued {{functions}} <b>of</b> <b>vectors</b> ...|$|R
500|$|Likewise, in the {{geometric}} example <b>of</b> <b>vectors</b> as arrows, [...] since the parallelogram defining the sum <b>of</b> the <b>vectors</b> is independent <b>of</b> the order <b>of</b> the <b>vectors.</b> All other axioms can be checked {{in a similar}} manner in both examples. Thus, by disregarding the concrete nature of the particular type <b>of</b> <b>vectors,</b> the definition incorporates these two and many more examples in one notion <b>of</b> <b>vector</b> space.|$|R
40|$|We propose an {{effective}} method to enable recursive ray tracing triangular scenes using contemporary real-time graphics pipelines of digital computers. So far, general purpose computations such as ray tracing utilized the programmable pixel shader for computation. Lengthy algorithms were subdivided to continue-and-restart-able parts (computing kernels) to enable implementation as multi-pass rendering. Discussing a fundamentally different method, we are representing rays with geometry, maintaining a one-to-one correspondence between rays and point primitives. Exploiting the geometry amplification capability of modern pipeline, we are utilizing the geometry shader to emit multiple secondary rays. Our approach to recursive ray tracing {{is influenced by}} stream computing, circulating the data flow without using a stack, employing intermediate pipeline stage to feedback transformed primitives before producing the final color by <b>rasterization</b> <b>of</b> point primitives. An effective implementation employing the uniform grid space subdivision scheme is described. Categories and Subject Descriptors (according to ACM CCS) ...|$|R
5000|$|If {{we define}} the {{reflection}} along a non-null <b>vector</b> [...] <b>of</b> the product <b>of</b> <b>vectors</b> as the reflection <b>of</b> every <b>vector</b> {{in the product}} along the same vector, we get for any product of an odd number <b>of</b> <b>vectors</b> that, by way of example,and for {{the product of an}} even number <b>of</b> <b>vectors</b> that ...|$|R
3000|$|... (f) is a 2 N × 1 <b>vector</b> <b>of</b> the k-th block data, and {{the length}} <b>of</b> <b>vector</b> in frequency-domain is twice than the length <b>of</b> <b>vector</b> in time-domain.|$|R
