1589|952|Public
5|$|The {{spectrum}} {{of light that}} comes from a single source (see idealized spectrum illustration top-right) can be measured. To determine the redshift, one searches for features in the spectrum such as absorption lines, emission lines, or other variations in light intensity. If found, these features can be compared with known features in the {{spectrum of}} various chemical compounds found in experiments where that compound is located on Earth. A very common atomic element in space is hydrogen. The spectrum of originally featureless light shone through hydrogen will show a signature spectrum specific to hydrogen that has features at regular intervals. If restricted to absorption lines it would look similar to the illustration (top right). If the same pattern of intervals is seen in an observed spectrum from a distant source but occurring at shifted wavelengths, it can be identified as hydrogen too. If the same spectral line is identified in both spectra—but at different wavelengths—then the redshift can be calculated using the table below. Determining the redshift of an object in this way requires a frequency- or wavelength-range. In order to calculate the redshift one has to know the wavelength of the emitted light in the rest frame of the source, in other words, the wavelength that would be measured by an observer located adjacent to and comoving with the source. Since in astronomical applications this measurement cannot be done directly, because that would require travelling to the distant star of interest, the method using spectral lines described here is used instead. Redshifts cannot be calculated by looking at unidentified features whose rest-frame frequency is unknown, or with a spectrum that is featureless or white noise (<b>random</b> <b>fluctuations</b> in a spectrum).|$|E
25|$|In astronomy, interplanetary {{scintillation}} {{refers to}} <b>random</b> <b>fluctuations</b> in {{the intensity of}} radio waves of celestial origin, on the timescale of a few seconds. It {{is analogous to the}} twinkling one sees looking at stars in the sky at night, but in the radio part of the electromagnetic spectrum rather than the visible one. Interplanetary scintillation is the result of radio waves traveling through fluctuations in the density of the electron and protons that make up the solar wind.|$|E
25|$|Experimental {{researchers}} typically use {{a statistical}} hypothesis testing model which involves making predictions before conducting the experiment, then assessing {{how well the}} data supports the predictions. (These predictions may originate from a more abstract scientific hypothesis about how the phenomenon under study actually works.) Analysis of variance (ANOVA) statistical techniques are used to distinguish unique results of the experiment from the null hypothesis that variations result from <b>random</b> <b>fluctuations</b> in data. In psychology, the widely usd standard ascribes statistical significance to results which have less than 5% probability of being explained by random variation.|$|E
40|$|Abstract⎯Today, {{variability}} {{is recognized}} as a major obstacle for continuing MOS transistor miniaturization. Among various kinds of variability, this paper mainly focuses on <b>random</b> <b>fluctuation,</b> which is caused by microscopic perturbations, such as discreteness of charges and atomic scale structural irregularity. Some recent research results for quantitatively understanding the causes of <b>random</b> <b>fluctuation</b> are reviewed. Methods of reducing <b>random</b> <b>fluctuation</b> will be discussed. Keywords- variability; variation; random dopant fluctuation; MOSFET; large scale integrated circuit I...|$|R
50|$|In 1961, Samaun's paper <b>Random</b> <b>Fluctuation</b> in a Nuclear Reactor was {{published}} under Nature.|$|R
3000|$|... is {{the noise}} {{intensity}} matrix {{and can be}} regarded {{as a result of the}} occurrence of eternal <b>random</b> <b>fluctuation</b> and other probabilistic causes.|$|R
25|$|Pendulums (unlike, for example, quartz crystals) have a {{low enough}} Q that the {{disturbance}} caused by the impulses to keep them moving is generally the limiting factor on their timekeeping accuracy. Therefore, {{the design of the}} escapement, the mechanism that provides these impulses, has a large effect on the accuracy of a clock pendulum. If the impulses given to the pendulum by the escapement each swing could be exactly identical, the response of the pendulum would be identical, and its period would be constant. However, this is not achievable; unavoidable <b>random</b> <b>fluctuations</b> in the force due to friction of the clock's pallets, lubrication variations, and changes in the torque provided by the clock's power source as it runs down, mean that the force of the impulse applied by the escapement varies.|$|E
500|$|When {{the index}} set [...] can be {{interpreted}} as time, a stochastic process is said to be stationary if its finite-dimensional distributions are invariant under translations of time. This type of stochastic process can be used to describe a physical system that is in steady state, but still experiences <b>random</b> <b>fluctuations.</b> The intuition behind such stationarity is that as time passes the distribution of the stationary stochastic process remains the same. A sequence of random variables forms a stationary process if and only if the [...] random variables are identically distributed.|$|E
500|$|Health {{effects from}} the smog were downplayed in most early reports. Some {{hospitals}} reported increased admissions of patients with asthma. An official at the city Department of Health noted that some hospitals were receiving fewer asthma patients, and attributed the reported increases to ordinary <b>random</b> <b>fluctuations.</b> The official told The New York Times that [...] "n not one [...] is a pattern emerging which would suggest {{we are dealing with}} an important health hazard as of this moment." [...] By this time, the inability to incinerate garbage had generated a large amount of excess waste. Hundreds of sanitation workers worked overtime to transport garbage to landfills in the Bronx, Brooklyn, and Staten Island, with the bulk going to Fresh Kills in Staten Island.|$|E
40|$|Abstract — The {{clustering}} algorithm employing “stochastic as-sociation”, {{which we}} have already proposed, offers a simple and efficient soft-max adaptation rule. The adaptation process {{is the same as}} the on-line K-means clustering method except for adding <b>random</b> <b>fluctuation</b> in the distortion error evaluation process. This paper describes VLSI implementation of this new clustering algorithm based on a pulse modulation circuit architecture. <b>Random</b> <b>fluctuation</b> is added by a chaos generator circuit that we previously designed using a pulse-width/pulse-phase modulation approach. Experimental results using a fabricated chip have demonstrated that successful “stochastic association ” clustering is achieved on a time scale of milliseconds. I...|$|R
50|$|In {{molecular}} dynamics (MD) simulations, there are errors due to inadequate {{sampling of the}} phase space or infrequently occurring events, these lead to the statistical error due to <b>random</b> <b>fluctuation</b> in the measurements.|$|R
40|$|The {{effects of}} Gaussian <b>random</b> <b>fluctuation</b> and linear {{change of the}} echo phase on the {{response}} of the azimuth matched processor of a synthetic aperture radar (airborne or spaceborne for ocean-surface sensing) is analyzed numerically. It is found that the <b>random</b> <b>fluctuation</b> would not alter appreciably the processor response if its standard deviation is less than pi, while its normalized correlation time is larger than 1. If these two bounding conditions are not satisfied, then the sidelobe level becomes relatively large and overshadows the central peak. In the case of linear displacement it is found that the main effect is the spatial displacement of the point scatterer in the image plane...|$|R
500|$|... "Complexity {{in ecology}} is {{of at least}} six {{distinct}} types: spatial, temporal, structural, process, behavioral, and geometric." [...] From these principles, ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence, ranging from molecular to planetary, and these require different explanations at each integrative level. Ecological complexity relates to the dynamic resilience of ecosystems that transition to multiple shifting steady-states directed by <b>random</b> <b>fluctuations</b> of history. Long-term ecological studies provide important track records {{to better understand the}} complexity and resilience of ecosystems over longer temporal and broader spatial scales. These studies are managed by the International Long Term Ecological Network (LTER). The longest experiment in existence is the Park Grass Experiment, which was initiated in 1856. Another example is the Hubbard Brook study, which has been in operation since 1960.|$|E
2500|$|... the {{covariance}} matrix [...] adapts to the inverse of the Hessian matrix , up to a scalar factor and small <b>random</b> <b>fluctuations.</b> More general, {{also on the}} function , where [...] is strictly increasing and therefore order preserving and [...] is convex-quadratic, the {{covariance matrix}} [...] adapts to , up to a scalar factor and small <b>random</b> <b>fluctuations.</b>|$|E
2500|$|Noise (<b>random</b> <b>fluctuations)</b> {{variation}} {{not able}} to be explained by a model ...|$|E
3000|$|Electrons {{capture and}} {{emission}} events at charge trap sites near the interface developed over P/E cycling directly result in memory cell threshold voltage <b>random</b> <b>fluctuation,</b> which {{is referred to}} as random telegraph noise (RTN) [13, 14]; [...]...|$|R
50|$|Boltzmann {{proposed}} {{that the state of}} our observed low-entropy universe (which includes our existence) is a <b>random</b> <b>fluctuation</b> in a higher-entropy universe. Even in a near-equilibrium state, there will be stochastic fluctuations in state of the system. The most common fluctuations will be relatively small, resulting in only small amounts of organization, while larger fluctuations and their resulting greater levels of organization will be comparatively more rare. Large fluctuations would be almost inconceivably rare, but inevitably occur if a universe lasts infinitely long. Even if the universe does not have an infinitely long past, modern cosmological theories of the Big Bang do suppose that the latter occurred via stochastic fluctuations in a larger meta-universe; the paradox is retained by incorporating our brief-but-finite past into the <b>random</b> <b>fluctuation.</b>|$|R
40|$|It {{is shown}} that {{coupling}} system between fractal membranes and a Gaussian beam {{passing through a}} static magnetic field has strong selection capability for the stochastic relic gravitational wave background. The relic GW components propagating along the positive direction of the symmetrical axis of the Gaussian beam might generate an optimal electromagnetic perturbation while the perturbation produced by the relic GW components propagating along the negative and perpendicular directions to the symmetrical axis will be {{much less than the}} former. The influence of the <b>random</b> <b>fluctuation</b> of the relic GWs to such effect can be neglected and the influence of the <b>random</b> <b>fluctuation</b> of the relic GWs to such effect can be neglected. Comment: 9 pages, 1 figure, 1 tabl...|$|R
2500|$|Free energy minimisation {{provides}} a useful way to formulate normative (Bayes optimal) models of neuronal inference and learning under uncertainty [...] and therefore subscribes to the Bayesian brain hypothesis. The neuronal processes described by free energy minimisation depend {{on the nature of}} hidden states: [...] that can comprise time dependent variables, time invariant parameters and the precision (inverse variance or temperature) of <b>random</b> <b>fluctuations.</b> Minimising variables, parameters and precision corresponds to inference, learning and the encoding of uncertainty, respectively: ...|$|E
2500|$|Community {{assembly}} [...] "is {{a framework}} that can unify virtually all of (community) ecology under a single conceptual umbrella". Community assembly theory attempts to explain the existence of environmentally similar sites with differing assemblages of species. It assumes that species have similar niche requirements, so that community formation {{is a product of}} <b>random</b> <b>fluctuations</b> from a common species pool. Essentially, if all species are fairly ecologically equivalent then random variation in colonization, migration and extinction rates between species, drive differences in species composition between sites with comparable environmental conditions.|$|E
2500|$|Self-organization, {{also called}} {{spontaneous}} order (in the social sciences), {{is a process}} where some form of overall order arises from local interactions between parts of an initially disordered system. The process is spontaneous, not needing control by any external agent. It is often triggered by <b>random</b> <b>fluctuations,</b> amplified by positive feedback. The resulting organization is wholly decentralized, [...] over all {{the components of the}} system. As such, the organization is typically robust and able to survive or self-repair substantial perturbation. Chaos theory discusses self-organization in terms of islands of predictability in a sea of chaotic unpredictability.|$|E
40|$|This paper {{describes}} a clustering algorithm for vector quantizers using a "stochastic association model". It {{offers a new}} simple and powerful softmax adaptation rule. The adaptation process {{is the same as}} the on-line K-means clustering method except for adding <b>random</b> <b>fluctuation</b> in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efficient adaptation as high as the "neural gas" algorithm, which is reported as one of the most efficient clustering methods. It is a key to add uncorrelated <b>random</b> <b>fluctuation</b> in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses fluctuation in quantum mechanical tunneling processes...|$|R
25|$|The first {{experimental}} {{evidence of the}} pre-adaptive nature of genetic variants in microorganisms was provided by Salvador Luria and Max Delbrück who developed Fluctuation Test, a method to show the <b>random</b> <b>fluctuation</b> of pre-existing genetic changes that conferred resistance to bacteriophage in the bacterium Escherichia coli.|$|R
40|$|In {{this paper}} a formula of {{probability}} density function {{expressed in terms}} 	of Hankel transform is introduced. The application of the formula to a problem 	of generalized multidimensional random flights and some other physical phenomena 	where <b>random</b> <b>fluctuation</b> is of main concern is discussed in some details...|$|R
2500|$|Active {{inference}} {{is related}} to optimal control by replacing value or cost-to-go functions with prior beliefs about state transitions or flow. This exploits the close connection between Bayesian filtering and {{the solution to the}} Bellman equation. However, active inference starts with (priors over) flow [...] that are specified with scalar [...] and vector [...] value functions of state space (c.f., the Helmholtz decomposition). [...] Here, [...] is the amplitude of <b>random</b> <b>fluctuations</b> and cost is [...] [...] The priors over flow [...] induce a prior over states [...] that is the solution to the appropriate forward Kolmogorov equations. In contrast, optimal control optimises the flow, given a cost function, under the assumption that [...] (i.e., the flow is curl free or has detailed balance). Usually, this entails solving backward Kolmogorov equations.|$|E
2500|$|... where k is the Boltzmann constant. Thus some {{dissociation}} {{can occur}} because sufficient thermal energy is available. The following {{sequence of events}} has been proposed {{on the basis of}} electric field fluctuations in liquid water. <b>Random</b> <b>fluctuations</b> in molecular motions occasionally (about once every 10 hours per water molecule) produce an electric field strong enough to break an oxygen–hydrogen bond, resulting in a hydroxide (OH−) and hydronium ion (H3O+); the hydrogen nucleus of the hydronium ion travels along water molecules by the Grotthuss mechanism and a change in the hydrogen bond network in the solvent isolates the two ions, which are stabilized by solvation. Within 1picosecond, however, a second reorganization of the hydrogen bond network allows rapid proton transfer down the electric potential difference and subsequent recombination of the ions. This timescale is consistent with the time it takes for hydrogen bonds to reorientate themselves in water.|$|E
2500|$|The {{average of}} the true solar day {{during the course of}} an entire year is the mean solar day, which {{contains}} [...] solar seconds. Currently, each of these seconds is slightly longer than an SI second because Earth's mean solar day is now slightly longer than it was during the 19th century due to tidal friction. The average length of the mean solar day since the introduction of the leap second in 1972 has been about 0 to 2 ms longer than 86,400 SI seconds. <b>Random</b> <b>fluctuations</b> due to core-mantle coupling have an amplitude of about 5 ms. The mean solar second between 1750 and 1892 was chosen in 1895 by Simon Newcomb as the independent unit of time in his Tables of the Sun. These tables were used to calculate the world's ephemerides between 1900 and 1983, so this second became known as the ephemeris second. In 1967 the SI second was made equal to the ephemeris second.|$|E
50|$|The first {{experimental}} {{evidence of the}} pre-adaptive nature of genetic variants in microorganisms was provided by Salvador Luria and Max Delbrück who developed Fluctuation Test, a method to show the <b>random</b> <b>fluctuation</b> of pre-existing genetic changes that conferred resistance to bacteriophage in the bacterium Escherichia coli.|$|R
50|$|When the {{particles}} of silver are small, the standard aperture area measures {{an average of}} many particles, so the granularity is small. When {{the particles}} are large, fewer are averaged in the standard area, {{so there is a}} larger <b>random</b> <b>fluctuation,</b> and a higher granularity number.|$|R
40|$|Food {{security}} may {{be increased}} by variance-reducing strategies, by food aid, or by development strategies. This paper uses a Korea CGE model, subjected to <b>random</b> <b>fluctuation</b> in world-prices and domestic food productivity, to evaluate these policies. We find that poverty-reducing development strategies are the most effective food-security strategies...|$|R
2500|$|Data {{collection}} at the LHC finally commenced in March 2010. By December 2011 the two main particle detectors at the LHC, ATLAS and CMS, had narrowed down the mass range where the Higgs could exist to around 116-130 GeV (ATLAS) and 115-127 GeV (CMS). There had also already {{been a number of}} promising event excesses that had [...] "evaporated" [...] and proven to be nothing but <b>random</b> <b>fluctuations.</b> However, from around May 2011, both experiments had seen among their results, the slow emergence of a small yet consistent excess of gamma and 4-lepton decay signatures and several other particle decays, all hinting at a new particle at a mass around [...] By around November 2011, the anomalous data at 125GeV was becoming [...] "too large to ignore" [...] (although still far from conclusive), and the team leaders at both ATLAS and CMS each privately suspected they might have found the Higgs. On November 28, 2011, at an internal meeting of the two team leaders and the director general of CERN, the latest analyses were discussed outside their teams for the first time, suggesting both ATLAS and CMS might be converging on a possible shared result at 125GeV, and initial preparations commenced in case of a successful finding. While this information was not known publicly at the time, the narrowing of the possible Higgs range to around 115–130 GeV and the repeated observation of small but consistent event excesses across multiple channels at both ATLAS and CMS in the 124-126 GeV region (described as [...] "tantalising hints" [...] of around 2-3 sigma) were public knowledge with [...] "a lot of interest". It was therefore widely anticipated around the end of 2011, that the LHC would provide sufficient data to either exclude or confirm the finding of a Higgs boson by the end of 2012, when their 2012 collision data (with slightly higher 8TeV collision energy) had been examined.|$|E
5000|$|... #Caption: Analog {{display of}} <b>random</b> <b>fluctuations</b> in voltage in pink noise.|$|E
5000|$|Oscillator phase noise, <b>random</b> <b>fluctuations</b> of {{the phase}} of an {{oscillator}} ...|$|E
40|$|We {{discuss a}} two-group SEIR {{epidemic}} model with distributed delays, incorporating <b>random</b> <b>fluctuation</b> around the endemic equilibrium. Our {{research shows that}} the endemic equilibrium of the model with distributed delays and random perturbation is stochastically asymptotically stable in the large. In addition, a sufficient stability condition is obtained by constructing suitable Lyapunov function...|$|R
40|$|In the {{experimental}} game designed by GÜTH et al. [2007], player 1 {{has promised to}} render a service to player 2. Player 1 either invests proper effort or shirks and performance may succeed or fail depending on <b>random</b> <b>fluctuation.</b> When player 1 fails to invest proper effort, and performance occurs or not through luck, player...|$|R
30|$|Transient {{transport}} model {{was used for}} the simulation of pollutant concentration for reference wells in both the Quaternary aquifer and the Tertiary aquifer (462 and 357 wells, respectively) which were used as an input to the spatial optimization model (Fig.  3). Head and flow velocity has been used for comparative analysis of <b>random</b> <b>fluctuation</b> of mass.|$|R
