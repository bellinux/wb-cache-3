1117|10000|Public
5|$|It {{was known}} the game needed to use 3D {{graphics}} with hardware acceleration {{to keep up}} with trends. The goal was to have a 3D <b>renderer</b> good enough for deathmatch. The team was also aiming to have a software <b>renderer</b> for lower-end computers. The team used Direct X to improve the lighting effects. The audio was implemented by five team members. The music was composed by Mark Knight, and Nick Laviers was head of audio. The methods used was similar to those used by the BBC for their radio effects. David Armor and Shintaro Kanaoya headed the level design, working with the flow of the tweaked user interface created to reduce ambiguity when looking after the dungeon. In fact, the interface was redesigned more than ten times due to feedback from various groups because the team wanted the public to understand it. Testers played levels without any invading forces, which {{led to the development of}} My Pet Dungeon. An important feature to improve was the multiplayer, and great deal of effort was put into it.|$|E
5|$|Thief was {{developed}} with the Dark Engine, a proprietary game engine. It was written during the game's development, {{rather than as}} a separately budgeted project, which led to time constraint issues. An emphasis was placed on simulating real life physics; arrows would arc through the air rather than fly straight. The engine features alpha blending, texture filtering and lighting techniques. Motion capture technology was integrated to allow for realistic character animation. The engine's renderer—which draws the graphics—was largely written by Looking Glass Studios programmer Sean Barrett in fall 1995. While the <b>renderer</b> was expected to be finished before the game's release date, Barrett left the company in 1996. He later performed contract work for the company, and assisted in writing features like hardware support. However, the <b>renderer</b> was never fully addressed, and was less advanced than others of the time.|$|E
5|$|Perfect Dark Zero is {{also one}} of the first games that uses the Havok's HydraCore physics engine, which was {{specifically}} designed for multi-core video game systems such as the Xbox 360. The game's <b>renderer</b> engine employs more advanced graphic technologies than was possible in the previous console generation, including parallax mapping, ambient occlusion, subsurface scattering, and high dynamic range. The soundtrack of the game was primarily composed by David Clynick, who worked with Grant Kirkhope on the original Nintendo 64 game's score. New York based group MorissonPoe contributed two songs to the score, while DJs Kepi and Kat composed the game's nightclub theme.|$|E
40|$|Interruptible <b>rendering</b> {{is a novel}} {{approach}} to the fidelity-versusperformance tradeoff ubiquitous in real-time <b>rendering.</b> Interruptible <b>rendering</b> unifies spatial error caused by <b>rendering</b> coarse approximations for speed and temporal error caused by the delay imposed by <b>rendering</b> into a single image-space error measure. The heart {{of this approach is}} a progressive <b>rendering</b> framework that <b>renders</b> a coarse image into the back buffer and continuously refines it while monitoring temporal error. When temporal error exceeds the spatial error caused by coarse <b>rendering,</b> further refinement is pointless, and the image is displayed. We discuss how to adapt different <b>rendering</b> algorithms for interruptible use and present implementations based on polygonal <b>rendering</b> and ray casting. Interruptible <b>rendering</b> provides a low-latency, self-tuning {{approach to}} interactive <b>rendering.</b> To evaluate our results we introduce a “gold standard” approach that measures dynamic visual error against a hypothetical perfect <b>rendering</b> and show that interruptible <b>rendering</b> is more accurate than standard fidelity-versusperformance schemes. This improved accuracy enables better interactive <b>rendering,</b> both for complex models and complex <b>rendering</b> modalities such as ray casting...|$|R
50|$|In {{the overall}} {{creation}} process {{there is also}} a distinction between finish <b>render</b> and final <b>render.</b> Finish <b>rendering</b> refers to the process, and final <b>rendering</b> refers to the schedule. The first finish <b>rendering</b> may not be the final rendering; the first finish <b>rendering</b> could be the first of many renderings, with each subsequent finish <b>rendering</b> needing refinement before the final version is created.|$|R
40|$|Abstract – We {{describe}} the software architecture of a <b>rendering</b> system that follows a pragmatic approach to integrating and bundling {{the power of}} different lowlevel <b>rendering</b> systems within an object-oriented framework. The generic <b>rendering</b> system provides higher-level abstractions to existing <b>rendering</b> systems {{and serves as a}} framework for developing new <b>rendering</b> techniques. It wraps the functionality of several, widely used <b>rendering</b> systems, defines a unified, object-oriented application programming interface, and provides an extensible, customizable apparatus for evaluating and interpreting hierarchical scene information. As a fundamental property, individual features of a specific <b>rendering</b> system can be integrated into the generic <b>rendering</b> system in a transparent way. The system is based on a state machine, called engine, which operates on <b>rendering</b> components. Four major categories of <b>rendering</b> components constitute the generic <b>rendering</b> system: shapes represent geometries; attributes specify properties assigned to geometries and scenes; handlers encapsulate <b>rendering</b> algorithms, and techniques represent evaluation strategies for <b>rendering</b> components. As a proof of concept, we have implemented the described software architecture by the Virtual <b>Rendering</b> System which currently wraps OpenGL...|$|R
5|$|Perfect Dark was {{developed}} over {{a course of}} nearly a year and its game engine was completely re-written from scratch to support several Xbox 360 features. Therefore, although the game plays {{exactly the same as}} the original, the code and <b>renderer</b> is different. The game received generally favorable reviews. Some critics considered the relatively unchanged game to be outdated, but most agreed that the title was a solid revival of a classic. As of the end of 2011, the game had sold nearly 410,000 units. In 2015, the game was included in the Rare Replay video game compilation for Xbox One.|$|E
5|$|According to Schafer, Brütal Legend {{had been}} in {{development}} since 2007, prior to the completion of Psychonauts. Schafer noted that in Psychonauts, they attempted to bring together a lot of assets, including characters and environments, but {{were not able to}} successfully integrate them on their first attempt and had to start over on the development of some. In several cases for Psychonauts, changes made in gameplay required them to return and redesign levels to account for the new features. The company decided at the start to use the scrum development approach for Brütal Legend, which they found worked well with their company's culture, allowing them to quickly arrive at playable targets at every milestone. For example, the team was able to bring about their terrain engine, <b>renderer,</b> and a playable Eddie Riggs within a month of development, and by the third month, the ability for Eddie to drive about the terrain, running over hordes of enemies.|$|E
5|$|The Image Computer never sold well. Inadequate sales {{threatened}} {{to put the}} company out of business as financial losses grew. Jobs invested more and more money in exchange for an increased stake in the company, reducing the proportion of management and employee ownership until eventually, his total investment of $50million gave him control of the entire company. In 1989, Lasseter's growing animation department, originally composed of just four people (Lasseter, Bill Reeves, Eben Ostby, and Sam Leffler), {{was turned into a}} division that produced computer-animated commercials for outside companies. In April 1990, Pixar sold its hardware division, including all proprietary hardware technology and imaging software, to Vicom Systems, and transferred 18 of Pixar's approximately 100 employees. That same year, Pixar moved from San Rafael to Richmond, California to Burbank, California. Pixar released some of its software tools on the open market for Macintosh and Windows systems. RenderMan was one of the leading 3D packages of the early 1990s, and Typestry was a special-purpose 3D text <b>renderer</b> that competed with RayDream addDepth.|$|E
5000|$|<b>Rendering</b> features. Strata Design 3D CX {{is famous}} for its {{high-quality}} <b>rendering</b> ability - <b>rendering</b> being the creation of a finished, final image. <b>Rendering</b> features include: ...|$|R
5000|$|NET <b>Render</b> (to <b>render</b> {{animations}} over a TCP/IP {{network in}} <b>render</b> farms) ...|$|R
40|$|Interruptible <b>rendering</b> {{is a novel}} {{approach}} to the fidelity-versusperformance tradeoff ubiquitous in real-time <b>rendering.</b> Interruptible <b>rendering</b> unifies spatial error, caused by <b>rendering</b> coarse approximations for speed, and temporal error, caused by the delay imposed by <b>rendering,</b> into a single image-space error metric. The heart {{of this approach is}} a progressive <b>rendering</b> framework that <b>renders</b> a coarse image into the back buffer and continuously refines it, while tracking the temporal error. When the temporal error exceeds the spatial error caused by coarse <b>rendering,</b> further refinement is pointless and the image is displayed. We discuss the requirements for a <b>rendering</b> algorithm to be suitable for interruptible use, and describe one such algorithm based on hierarchical splatting. Interruptible <b>rendering</b> provides a low-latency, self-tuning {{approach to}} interactive <b>rendering.</b> Interestingly, it also leads to a "one-and-a-half buffered" approach that <b>renders</b> sometimes to the back buffer and sometimes to the front buffer...|$|R
25|$|Windows Vista also {{introduces}} a new video <b>renderer,</b> available {{as both a}} Media Foundation component and a DirectShow filter, called the Enhanced Video <b>Renderer</b> (EVR). EVR is designed to work with Desktop Window Manager.|$|E
25|$|Development of a Vulkan-based {{graphics}} <b>renderer</b> {{began in}} June 2016. After a month the developer announced is “Now feature-complete, time for clean-ups/bug-fixing/performance work.“ Development of that <b>renderer</b> was still {{done in a}} dedicated branch for the next months until the code was finally merged in October 2016.|$|E
25|$|Windows Vista and Windows 7 {{ship with}} a new <b>renderer,</b> {{available}} as both a Media Foundation component and a DirectShow filter, called the Enhanced Video <b>Renderer</b> (EVR). EVR is designed to work with Desktop Window Manager and supports DXVA 2.0, which is available on Windows Vista and Windows 7. It offers better performance and better quality according to Microsoft.|$|E
40|$|This paper {{describes}} {{the design of}} a 3 D <b>rendering</b> meta system which wraps the functionality of 3 D <b>rendering</b> packages into a uniform, object-oriented framework. The homogeneous interface of the 3 D <b>rendering</b> meta system serves as an easy-to-learn application programming interface to 3 D <b>rendering</b> packages and allows us to exchange <b>rendering</b> packages without the need to recode an application. Our approach is based on a logical decomposition of the elements of 3 D <b>rendering</b> into four major class categories: Shapes define geometric objects; attributes specify quality and visual appearance; controllers describe <b>rendering</b> techniques and <b>rendering</b> processes, and <b>rendering</b> engines evaluate shapes, attributes, and controllers. We have implemented our concepts in VRS, the Virtual <b>Rendering</b> System, as a portable C++ toolkit. VRS currently supports <b>rendering</b> packages such as OpenGL, PEX, XGL, Radiance, POV Ray, and RenderMan. 1 Motivation 3 D <b>rendering</b> packages are of increasing importa [...] ...|$|R
5000|$|Supports both {{distributed}} <b>rendering</b> (where {{the results}} are <b>rendered</b> on each node and composited later using the depth buffer), local <b>rendering</b> (where the resulting polygons are collected on one node and <b>rendered</b> locally) and {{a combination of both}} (for example, the level-of-detail models can be <b>rendered</b> locally whereas the full model is <b>rendered</b> in a distributed manner). This provides scalable <b>rendering</b> for large data without sacrificing performance when working with smaller data.|$|R
5000|$|... {{capable of}} <b>rendering</b> text. There are already several {{libraries}} that <b>render</b> text with OpenGL and consistent cross-platform text <b>rendering</b> cannot {{depend on the}} platform’s text <b>rendering</b> facilities anyway.|$|R
25|$|DXVA 2.0 {{supports}} only Enhanced Video <b>Renderer</b> as {{the video}} <b>renderer</b> on Windows Vista. DXVA integrates with Media Foundation and allows DXVA pipelines {{to be exposed}} as Media Foundation Transforms (MFTs). Even decoder pipelines or post-processing pipelines can be exposed as MFTs, {{which can be used}} by the Media Foundation topology loader to create a full media playback pipeline. DXVA 1.0 is emulated using DXVA 2.0.|$|E
25|$|Stellarium {{provides}} {{two versions}} for Windows: the default version uses OpenGL, the alternative version uses ANGLE as the <b>renderer.</b>|$|E
25|$|<b>Renderer</b> filters: These {{render the}} data. For example, sending audio {{to the sound}} card, drawing video on the screen or writing data to a file.|$|E
40|$|ParaView is {{a popular}} {{open-source}} general-purpose scientific visualization application. One of the many visualization tools available within ParaView is the volume <b>rendering</b> of unstructured meshes. Volume <b>rendering</b> is a technique that <b>renders</b> a mesh as a translucent solid, thereby allowing the user to see every point in three-dimensional space simultaneously. Because volume <b>rendering</b> is computationally intensive, ParaView now employs a unique parallel <b>rendering</b> algorithm to speed the processes. The parallel <b>rendering</b> algorithm is very flexible. It works equally well for both volumes and surfaces, and can properly <b>render</b> the intersection of a volume and opaque polygonal surfaces. The parallel <b>rendering</b> algorithm can also <b>render</b> images for tiled displays. In this paper, we explore the implementation of parallel unstructured volume <b>rendering</b> in ParaView...|$|R
40|$|In this paper, we {{survey the}} {{techniques}} for image-based <b>rendering.</b> Unlike traditional 3 D computer graphics in which 3 D {{geometry of the}} scene is known, image-based <b>rendering</b> techniques <b>render</b> novel views directly from input images. Previous image-based <b>rendering</b> techniques can be classified into three categories according to how much geometric information is used: <b>rendering</b> without geometry, <b>rendering</b> with implicit geometry (i. e., correspondence), and <b>rendering</b> with explicit geometry (either with approximate or accurate geometry). We discuss the characteristics of these categories and their representative methods. The continuum between images and geometry used in image-based <b>rendering</b> techniques suggests that image-based <b>rendering</b> with traditional 3 D graphics can be united in a joint image and geometry space. Keywords: Image-based <b>rendering,</b> survey. ...|$|R
50|$|<b>Rendering</b> {{is used in}} architecture, simulators, video games, {{movies and}} {{television}} visual effects and design visualization. <b>Rendering</b> is the last step in an animation process, and gives the final appearance to the models and animation with visual effects such as shading, texture-mapping, shadows, reflections and motion blurs. <b>Rendering</b> can be split into two main categories: real-time <b>rendering</b> (also known as online <b>rendering),</b> and pre-rendering (also called offline <b>rendering).</b> Real-time <b>rendering</b> is used to interactively <b>render</b> a scene, like in 3D computer games, and generally each frame must be <b>rendered</b> in a few milliseconds. Offline <b>rendering</b> is used to create realistic images and movies, where each frame can take hours or days to complete, or for debugging of complex graphics code by programmers.|$|R
25|$|DirectDraw: {{for drawing}} 2D Graphics (raster graphics). Deprecated {{in favor of}} Direct2D, though still in use {{by a number of}} games and as a video <b>renderer</b> in media applications.|$|E
25|$|Two {{experimental}} features, {{that never}} reached maturity, were removed in May 2017: The DirectX 12 <b>renderer</b> – which found a suitable replacement in the Vulkan back-end – and the alternative CPU emulator JITIL.|$|E
25|$|DirectShow 6.0, {{released}} {{as part of}} DirectX Media introduced the Overlay Mixer <b>renderer</b> designed for DVD playback and broadcast video streams with closed captioning and subtitles. The Overlay Mixer uses DirectDraw 5 for rendering. Downstream connection with the Video <b>Renderer</b> is required for window management. Overlay Mixer also supports Video Port Extensions (VPE), enabling it to work with analog TV tuners with overlay capability (sending video directly to a video card via an analog link rather than via the PCI bus). Overlay Mixer also supports DXVA connections. Because it always renders in overlay, full-screen video to TV-out is always activated.|$|E
40|$|The {{purpose of}} this thesis was to {{contribute}} to our knowledge of what haptics can {{bring to the table}} as a human-computer interface <b>rendering</b> technique, which other <b>rendering</b> techniques cannot. An experiment was set up in which a multi-interfaced game was used to convey an information structure to interface users. Each of the game’s three user interfaces utilized one of three different <b>rendering</b> techniques: haptic <b>rendering,</b> graphic <b>rendering,</b> and graphic-haptic <b>rendering.</b> The capacity of each <b>rendering</b> technique to represent the information structure was assessed in terms of the effect of the corresponding interface on three aspects of the user interaction: user performance, user satisfaction and system usability. The result indicated that user performance benefitted from a graphic or graphic-haptic <b>rendering</b> over a haptic <b>rendering.</b> There were no differences between the <b>rendering</b> techniques with regards to the overall user satisfaction. However, there were notable differences on the user satisfaction metric subscale level. The haptic <b>rendering</b> required higher attentive effort than other renderings. Also, the graphic <b>rendering</b> better facilitated the perception of having clear goals and feedback. The results also suggested that the overall system usability benefitted from a graphic or graphic-haptic <b>rendering</b> over a haptic <b>rendering...</b>|$|R
50|$|Per-pixel {{lighting}} is also performed in software on many high-end commercial <b>rendering</b> applications which typically do not <b>render</b> at interactive framerates. This is called offline <b>rendering</b> or software <b>rendering.</b> NVidia's mental ray <b>rendering</b> software, which is integrated with such suites as Autodesk's Softimage {{is a well-known}} example.|$|R
50|$|ROPs - <b>render</b> {{operators}} - {{for building}} networks to represent different <b>render</b> passes and <b>render</b> dependencies.|$|R
25|$|Since November 2007, DirectShow APIs {{are part}} of the Windows SDK. It {{includes}} several new enhancements, codecs and filter updates such as the Enhanced Video <b>Renderer</b> (EVR) and DXVA 2.0 (DirectX Video Acceleration).|$|E
25|$|Zootopia was {{the second}} time Disney used the Hyperion <b>renderer,</b> which they had first used on Big Hero 6. A new fur {{paradigm}} was added to the <b>renderer</b> to facilitate the creation of realistic images of the animals' dense fur. Nitro, a real-time display application developed since the making of Wreck-It Ralph, was used to make the fur more consistent, intact and subtle much more quickly, as opposed to the previous practice of having to predict how the fur would work while making and looking at silhouettes or poses for the character. The tree-and-plant generator Bonsai, first used in Frozen, was used to make numerous variations of trees with very detailed foliage.|$|E
25|$|Boxee, like XBMC Media Center (which Boxee {{is based}} upon), is a {{cross-platform}} software programmed mostly in C++ {{and uses the}} Simple DirectMedia Layer framework with OpenGL <b>renderer</b> for all versions of Boxee. Some of the libraries that Boxee depends on are also written in the C programming-language, but are used with a C++ wrapper and loaded via Boxee's own DLL loader when used inside Boxee.|$|E
5000|$|Raydiosity and raytracing <b>rendering</b> {{options include}} many {{customization}} options such as <b>rendering</b> to an alpha channel, <b>rendering</b> to Photoshop layers, gamma control, {{and the ability}} to load or save custom <b>render</b> settings.|$|R
50|$|Artistic <b>rendering</b> is the {{application}} of visual art styles to <b>rendering.</b> For photorealistic <b>rendering</b> styles, {{the emphasis is on}} accurate reproduction of light-and-shadow and the surface properties of the depicted objects, composition, or other more generic qualities. When the emphasis is on unique interpretive <b>rendering</b> styles, visual information is interpreted by the artist and displayed accordingly using the chosen art medium and level of abstraction in abstract art. In computer graphics, interpretive <b>rendering</b> styles are known as non-photorealistic <b>rendering</b> styles, but may be used to simplify technical illustrations. <b>Rendering</b> styles that combine photorealism with non-photorealism are known as hyperrealistic <b>rendering</b> styles.|$|R
50|$|<b>Rendering</b> is {{the final}} process of {{creating}} the actual 2D image or animation from the prepared scene. This {{can be compared to}} taking a photo or filming the scene after the setup is finished in real life. Several different, and often specialized, <b>rendering</b> methods have been developed. These range from the distinctly non-realistic wireframe <b>rendering</b> through polygon-based <b>rendering,</b> to more advanced techniques such as: scanline <b>rendering,</b> ray tracing, or radiosity. <b>Rendering</b> may take from fractions of a second to days for a single image/frame. In general, different methods are better suited for either photo-realistic <b>rendering,</b> or real-time <b>rendering.</b>|$|R
