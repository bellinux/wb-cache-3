29|40|Public
50|$|In modern {{computers}} many processes run at once. Active {{processes are}} placed in an array called a <b>run</b> <b>queue,</b> or runqueue. The <b>run</b> <b>queue</b> may contain priority values for each process, which will {{be used by the}} scheduler to determine which process to run next. To ensure each program has a fair share of resources, each one is run for some time period (quantum) before it is paused and placed back into the <b>run</b> <b>queue.</b> When a program is stopped to let another run, the program with the highest priority in the <b>run</b> <b>queue</b> is then allowed to execute.|$|E
50|$|In UNIX or Linux, the sar {{command is}} used to check the <b>run</b> <b>queue.</b>|$|E
50|$|Processes {{are also}} {{removed from the}} <b>run</b> <b>queue</b> when they ask to sleep, are waiting on a {{resource}} to become available, or have been terminated.|$|E
50|$|The TOPS-10 {{scheduler}} supported prioritized <b>run</b> <b>queues,</b> and appended {{a process}} onto a queue depending on its priority. The system also included User file and Device independence.|$|R
5000|$|In {{contrast}} to the previous O(1) scheduler used in older Linux 2.6 kernels, the CFS scheduler implementation {{is not based on}} <b>run</b> <b>queues.</b> Instead, a red-black tree implements a [...] "timeline" [...] of future task execution. Additionally, the scheduler uses nanosecond granularity accounting, the atomic units by which an individual process' share of the CPU was allocated (thus making redundant the previous notion of timeslices). This precise knowledge also means that no specific heuristics are required to determine the interactivity of a process, for example.|$|R
40|$|Abstract—This paper {{presents}} {{an extension of}} the Completely Fair Scheduler (CFS) to support cooperative multitasking with time-sharing for heterogeneous processing elements in Linux. We extend the kernel to be aware of accelerators, hold different <b>run</b> <b>queues</b> for these components and perform scheduling decisions using application provided meta information and a fairness measure. Our additional programming model allows the integration of checkpoints into applications, which permits the preemption and subsequent migration of applications between accelerators. We show that cooperative multitasking is possible on heterogeneous systems and that it increases application performance and system utilization. I...|$|R
50|$|System III {{introduced}} {{new features}} such as named pipes, the uname system call and command, and the <b>run</b> <b>queue.</b> It also combined various improvements to Version 7 Unix by outside organizations. However, it did not include notable additions made in BSD such as the C shell (csh) and screen editing.|$|E
5000|$|Full/empty status changes use polling, with {{a timeout}} for threads that poll too long. A timed-out thread may be descheduled and the {{hardware}} context {{used to run}} another thread; the OS scheduler sets a [...] "trap on write" [...] bit so the waited-for write will trap and put the descheduled thread back in the <b>run</b> <b>queue.</b> [...] Where the descheduled thread is on the critical path, performance may suffer substantially.|$|E
50|$|A SLIH completes long {{interrupt}} processing tasks similarly to a process. SLIHs either have a dedicated kernel thread for each handler, or are executed by {{a pool of}} kernel worker threads. These threads sit on a <b>run</b> <b>queue</b> in the operating system until processor time is available for them to perform processing for the interrupt. SLIHs may have a long-lived execution time, and thus are typically scheduled similarly to threads and processes.|$|E
5000|$|The local minimum delay {{can only}} be {{determined}} when a packet leaves the buffer, so no extra delay is needed to <b>run</b> the <b>queue</b> to collect statistics to manage the queue.|$|R
40|$|Abstract—Server {{consolidation}} using virtual machines (VMs) {{makes it}} difficult to execute processes as the administrators intend. A process scheduler in each VM is not aware of the other VM and schedules only processes in one VM independently. To solve this problem, process scheduling across VMs is necessary. However, such system-wide scheduling is vulnerable to denial-of-service (DoS) attacks from a compromised VM against the other VMs. In this paper, we propose the Monarch scheduler, which is a secure system-wide process scheduler running in the virtual machine monitor (VMM). The Monarch scheduler monitors the execution of processes and changes the scheduling behavior in all VMs. To change process scheduling from the VMM, it manipulates <b>run</b> <b>queues</b> and process states consistently without modifying guest operating systems. Its hybrid scheduling mitigates DoS attacks by leveraging performance isolation among VMs. We confirmed that the Monarch scheduler could achieve useful scheduling and the overheads were small. Keywords-virtual machines; server consolidation; DoS attacks; process scheduling; performance isolation I...|$|R
40|$|Price {{dispersion}} is {{analyzed in}} the context of a queuing market where customers enter queues to acquire a good or service and may experience delays. With menu costs, price dispersion arises and can persist in the medium and long <b>run.</b> The <b>queuing</b> market rations goods in the same way whether firm prices are optimal or not. Price dispersion reduces the rate at which customers get the good and reduces customer welfare. ...|$|R
5000|$|A ready queue or <b>run</b> <b>queue</b> {{is used in}} {{computer}} scheduling. Modern computers are capable of running many different programs or processes at the same time. However, the CPU is only capable of handling one process at a time. Processes that {{are ready for the}} CPU are kept in a queue for [...] "ready" [...] processes. Other processes that are waiting for an event to occur, such as loading information from a hard drive or waiting on an internet connection, are not in the ready queue.|$|E
50|$|The Completely Fair Scheduler (CFS) uses a well-studied, classic {{scheduling}} algorithm called fair queuing originally invented for packet networks. Fair queuing {{had been}} previously applied to CPU scheduling under the name stride scheduling. The fair queuing CFS scheduler has a scheduling complexity of O(log N), where N {{is the number of}} tasks in the runqueue. Choosing a task can be done in constant time, but reinserting a task after it has run requires O(log N) operations, because the <b>run</b> <b>queue</b> is implemented as a red-black tree.|$|E
50|$|In the Linux {{operating}} system (prior to kernel 2.6.23), each CPU {{in the system}} is given a <b>run</b> <b>queue,</b> which maintains both an active and expired array of processes. Each array contains 140 (one for each priority level) pointers to doubly linked lists, which in turn reference all processes with the given priority. The scheduler selects the next process from the active array with highest priority. When a process' quantum expires, it is placed into the expired array with some priority. When the active array contains no more processes, the scheduler swaps the active and expired arrays, hence the name O(1) scheduler.|$|E
50|$|A serious {{incident}} occurred on 13 June 1957, when a RTL-type bus on route 7A <b>ran</b> into a <b>queue</b> of waiting passengers on Oxford Street, killing eight people. The driver had collapsed with heat exhaustion.|$|R
3000|$|Tools in the GATK can be run {{manually}} {{through the}} command line, {{specified in the}} workflow definition language (WDL) and run in Cromwell, or use written in Scala and <b>run</b> on <b>Queue</b> ([URL] GATK provides multiple approaches to parallelize tasks: multi-threading and scatter–gather. Users enable multi-threading mode by specifying command-line flags and use Queue or Cromwell to run GATK tools using a scatter–gather approach. It is also possible to combine these approaches ([URL] [...]...|$|R
5000|$|The ride {{features}} three lift hills. The ride {{begins with}} the trip up the first and highest lift. From there it travels around the track to the second lift. The second lift is housed in a building designed {{to look like a}} rock crusher. The final lift leads into the [...] "Ace Hotel and Saloon". The [...] "Ace Hotel" [...] was named in 1974 for John 'Ace' Cocharo, a mine train foreman turned ride supervisor. After the lift, the ride drops riders into a tunnel through Caddo Lake, emerging just outside the final brake <b>run</b> and <b>queue</b> house.|$|R
50|$|In Linux 2.4, an O(n) {{scheduler}} with a multilevel feedback queue with priority levels {{ranging from}} 0 to 140 was used; 0 - 99 {{are reserved for}} real-time tasks and 100 - 140 are considered nice task levels. For real-time tasks, the time quantum for switching processes was approximately 200 ms, and for nice tasks approximately 10 ms. The scheduler ran through the <b>run</b> <b>queue</b> of all ready processes, letting the highest priority processes go first and run through their time slices, after which they will be placed in an expired queue. When the active queue is empty the expired queue will become the active queue and vice versa.|$|E
50|$|An idle {{computer}} has a load number of 0 (the idle process isn't counted). Each process using or waiting for CPU (the ready queue or <b>run</b> <b>queue)</b> increments the load number by 1. Each process that terminates decrements it by 1. Most UNIX systems count only {{processes in the}} running (on CPU) or runnable (waiting for CPU) states. However, Linux also includes processes in uninterruptible sleep states (usually waiting for disk activity), {{which can lead to}} markedly different results if many processes remain blocked in I/O due to a busy or stalled I/O system. This, for example, includes processes blocking due to an NFS server failure or too slow media (e.g., USB 1.x storage devices). Such circumstances can result in an elevated load average which does not reflect an actual increase in CPU use (but still gives an idea of how long users have to wait).|$|E
30|$|Waiting Time: Average time {{a process}} spends in the <b>run</b> <b>queue.</b>|$|E
50|$|It {{provides}} a means for transforming data between different architectures and protocols, such as Big Endian to Little Endian, or EBCDIC to ASCII. This is accomplished {{through the use of}} message data exits. Exits are compiled applications that <b>run</b> on the <b>queue</b> manager host, and are executed by the IBM MQ software at the time data transformation is needed.|$|R
5000|$|Stocks of {{the stamp}} quickly <b>ran</b> short with <b>queues</b> forming at Post Offices and stamp dealers unable to obtain stock for their customers. A {{speculative}} bubble developed with {{copies of the}} stamp selling for many times their original cost, and people buying whole sheets of stamps as an investment. The Evening Times in Scotland reported that ...|$|R
50|$|As {{machines}} {{became more}} powerful {{the time to}} run programs diminished, and the time to hand off the equipment to the next user became large by comparison. Accounting for and paying for machine usage moved on from checking the wall clock to automatic logging by the computer. <b>Run</b> <b>queues</b> evolved from a literal queue {{of people at the}} door, to a heap of media on a jobs-waiting table, or batches of punch-cards stacked {{one on top of the}} other in the reader, until the machine itself was able to select and sequence which magnetic tape drives processed which tapes. Where program developers had originally had access to run their own jobs on the machine, they were supplanted by dedicated machine operators who looked after the machine and were less and less concerned with implementing tasks manually. When commercially available computer centers were faced with the implications of data lost through tampering or operational errors, equipment vendors were put under pressure to enhance the runtime libraries to prevent misuse of system resources. Automated monitoring was needed not just for CPU usage but for counting pages printed, cards punched, cards read, disk storage used and for signaling when operator intervention was required by jobs such as changing magnetic tapes and paper forms. Security features were added to operating systems to record audit trails of which programs were accessing which files and to prevent access to a production payroll file by an engineering program, for example.|$|R
40|$|In this paper, we {{investigate}} {{the design of}} highly efficient and scalable staged event-driven middleware for shared memory multi- processors. Various scheduler designs are considered and evaluated, including shared <b>run</b> <b>queue</b> and multiple <b>run</b> <b>queue</b> arrangements. Techniques to maximise cache locality while improving load balancing are studied. Moreover, we consider a variety of access control mechanisms applied to shared data structures such as the <b>run</b> <b>queue,</b> including coarse grained locking, fine grained locking and non-blocking algorithms. User- level memory management techniques are applied to enhance memory allocation performance, particularly in situations where non-blocking algorithms are used. The paper concludes with a comparative analysis of the various configurations of our middleware, {{in an effort to}} identify their performance characteristics under a variety of conditions. peer-reviewe...|$|E
40|$|Performance of {{parallel}} processing systems {{is sensitive to}} various hardware and software overheads and contention for hardware and software resources. Hardware resources such as interconnection network and memory introduce communication contention and memory contention that could seriously impact overall system performance. Software resources include critical data structures maintained by application software {{as well as by}} the system software. In process scheduling context, <b>run</b> <b>queue</b> is a critical data structure that could potentially affect the overall system performance. There are two basic <b>run</b> <b>queue</b> organizations: centralized and distributed. Many shared-memory multiprocessor systems use the centralized organization in which a single global <b>run</b> <b>queue</b> is shared by all processors in the system. We will first identify performance problems associated with these two organizations and then discuss some means to alleviate these problems. We then present a hierarchical task queue organizati [...] ...|$|E
40|$|The goal of {{this paper}} is to study the impact of <b>run</b> <b>queue</b> {{organization}} on the performance of synchronization methods in multiprocessor systems. Two <b>run</b> <b>queue</b> organizations are considered: distributed and hierarchical organizations. The performance impact of spinning and blocking synchronization methods on these two <b>run</b> <b>queue</b> organizations is studied. We use two canonical workload types that require task synchronization: lock accessing and barrier synchronization workloads. The results presented here show that, when fine grain synchronization is required, the distributed organization is better. However, for large granularity tasks, the performance of the distributed organization is unacceptable and the hierarchical organization should be used. Note that the distributed organization is embedded into the hierarchical organization. Thus, for coarse granularity parallel applications, the hierarchical organization with its load sharing feature can be used; for fine-granularity parallel appl [...] ...|$|E
30|$|In Hypermap {{there can}} be {{thousands}} of tasks which need to be <b>run</b> in the <b>queue</b> daily. The task queue implementation provides a convenient way to scale when task numbers increase, by adding more parallel workers to the execution stack. The task queue also provides a handy way to run periodic tasks, for example services which need to be checked regularly, {{and the ability to}} inspect the status of processed tasks.|$|R
40|$|RR-scheduling {{algorithm}} {{was designed}} for the time-sharing system or interactive systems. The first process in the <b>queue</b> <b>run</b> until it expires its quantum (i. e. runs {{for as long as}} the time quantum), then the next process in the <b>queue</b> <b>runs</b> and so on. RR scheduling is implemented with timer interrupts. When a process is scheduled, the timer is set to go off after the time quantum amount of time expires. When process expire its quantum, a context switch takes place. The state of the running process is saved and context of next process in the ready queue is loaded in CPU registers. it gives good response time, but can give bad waiting time. We propose here a modification to round robin scheduling algorithm which not only gives good response time but also shows reduction in waiting time. If the processes in the ready queue are arranged in the increasing order of the expected CPU burst time instead of first come first serve manner, thewaiting time of the processes will decrease in addition to fast response time...|$|R
50|$|Before the mid-1960s, {{the only}} {{computers}} were huge mainframe computers. Users submitted jobs (calculations or other requests) on punched cards or similar media to specialist computer operators. The computer stored these, then used a {{batch processing system}} to <b>run</b> this <b>queue</b> of jobs one after another, allowing very high levels of utilization of these expensive machines. As the performance of computing hardware rose through the 1960s, multi-processing was developed. This allowed a mix of batch jobs to be run together, but the real revolution was the development of time-sharing. Time-sharing allowed multiple remote interactive users to share use of the computer, interacting with the computer from computer terminals with keyboards and teletype printers, and later display screens, {{in much the same}} way as desktop computers or personal computers would be used later.|$|R
40|$|Abstract—This paper {{describes}} a prototype visualization sys-tem for concurrent and distributed applications programmed using Erlang, providing {{two levels of}} granularity of view. Both visualizations are animated to show the dynamics of aspects of the computation. At the low level, we show the concurrent behaviour of the Erlang schedulers on a single instance of the Erlang virtual machine, which we call an Erlang node. Typically {{there will be one}} scheduler per core on a multicore system. Each scheduler maintains a <b>run</b> <b>queue</b> of processes to execute, and we visualize the migration of Erlang concurrent processes from one <b>run</b> <b>queue</b> to another as work is redistributed to fully exploit the hardware. The schedulers are shown as a graph with a circular layout. Next to each scheduler we draw a variable length bar indicating the current size of the <b>run</b> <b>queue</b> for the scheduler. At the high level, we visualize the distributed aspects of the system, showing interactions between Erlang nodes as a dynamic graph drawn with a force model. Specifically we show message passing between nodes as edges and lay out nodes according to their current connections. In addition, we also show the grouping of nodes into “s groups ” using an Euler diagram drawn with circles. I...|$|E
40|$|This paper also lists three {{methods for}} load balancing: Dipstick, Bidding, and Adaptive Learning, then {{describes}} a load-balancing system whereby each processor includes a two-byte status update with each message sent. The Dipstick method {{is the same}} as the traditional watermark processing found in many operating systems. The Adaptive Learning algorithm uses a feedback mechanism based on the <b>run</b> <b>queue</b> length at each processo...|$|E
40|$|Abstract—Although {{there has}} been {{tremendous}} increase in PC power {{and most of it}} is not fully harnessed, yet certain computation intensive application tend to migrate their process with the aim of reducing the response time. Cluster computing is the area that aims just at this. Clustering provides means to improve availability of services, sharing computational workload and performing computation intensive application. However, these benefits can only be achieved if the computing power of cluster is used efficiently and allocated fairly among all the available nodes. As is the case with our desktops, it is usually seen that clusters also suffer from underutilization. A number of approaches proposed in the past share only idle CPU cycles and not use the resources of systems when the machine has its own local processes to execute. We propose a priority-based scheduling approach for <b>run</b> <b>queue</b> and multilevel feedback queue scheduling approach for migrated tasks that doesn’t degrade the performance of local jobs too. Simulation and experimental results have been able to show that priority-based <b>run</b> <b>queue</b> management and multilevel feedback queue scheduling for migrated tasks can increase overall throughput by about 28 - 33 percent...|$|E
5|$|GateKeeper's {{entrance}} plaza {{is located}} {{along the beach}} {{next to one of}} Wicked Twister's towers. A sculpture with the GateKeeper logo is {{in the center of the}} plaza. The <b>queue</b> <b>runs</b> parallel to the beach, under the lift hill and station. Once under the station, riders can choose which side of the train they would like to ride. GateKeeper uses the Fast Lane queuing system; visitors can buy a wristband that allows them to wait in a shorter line.|$|R
2500|$|In {{keeping with}} {{their lack of}} day fees, {{individual}} performers are not assigned specific places and times to perform. There are only positions in a (virtual) [...] "line" [...] or [...] "queue" [...] for each marked, sanctioned performance location. <b>Queuing</b> <b>runs</b> on an honor system. Each performance is limited to one hour if any other licensed performer {{is waiting for the}} spot. Electronic amplification is not allowed, nor are brass instruments or drums. Certain performance locations are further limited to [...] "quiet" [...] performances where (for example) even hand-clap percussion is not allowed.|$|R
30|$|Congestion avoidance: When a node is congested, {{which is}} {{indicated}} by the {{running out of the}} queue buffer, the node will inform its direct upstream node to stop sending packets. Without outputting packets, the upstream node will <b>run</b> out its <b>queue</b> buffer quickly, becoming congested. In this manner, the congestion status will be propagated back to the source along the path. Then, the source’s sending is suspended until its buffer becomes available again. Semi-TCP takes actions at the node where the congestion begins and releases the congestion immediately along the inverse direction of the traffic flow.|$|R
