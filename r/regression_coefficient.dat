3223|5443|Public
25|$|When this {{assumption}} is violated the regressors are called linearly dependent or perfectly multicollinear. In such case {{the value of}} the <b>regression</b> <b>coefficient</b> β cannot be learned, although prediction of y values is still possible for new values of the regressors that lie in the same linearly dependent subspace.|$|E
2500|$|... of the <b>regression</b> <b>coefficient</b> of the lagged {{dependent}} variable, provided ...|$|E
2500|$|Galton {{was able}} to further his notion of {{regression}} by collecting and analyzing data on human stature. Galton asked for help of mathematician J. Hamilton Dickson in investigating the geometric relationship of the data. He determined that the <b>regression</b> <b>coefficient</b> did not ensure population stability by chance, but rather that the <b>regression</b> <b>coefficient,</b> conditional variance, and population were interdependent quantities related by a simple equation. [...] Thus Galton identified that the linearity of regression was not coincidental but rather was a necessary consequence of population stability.|$|E
3000|$|... where Y is a {{response}} variable of decolorization efficiency; b i, the <b>regression</b> <b>coefficients</b> for linear effects; b ii, the <b>regression</b> <b>coefficients</b> for quadratic effects; b ij, the <b>regression</b> <b>coefficients</b> for interaction effects; x i [...] are coded experimental levels of the variables.|$|R
3000|$|... where βi is the <b>regression</b> <b>coefficients</b> for {{individual}} factor effect, βii is the <b>regression</b> <b>coefficients</b> for square effects of factor, and βij is the <b>regression</b> <b>coefficients</b> for interaction between factors. The matrix applies and {{the analysis of}} variance (ANOVA) {{was carried out by}} Minitab 16 software. Validation experiments for CCD experiment were performed based on analysis result(s).|$|R
40|$|By {{extending}} his data, we document {{the instability of}} low-frequency <b>regression</b> <b>coefficients</b> that Lucas (1980) used to express the quantity theory of money. We impute the differences in these <b>regression</b> <b>coefficients</b> to differences in monetary policies across periods. A DSGE model estimated over a subsample like Lucas's implies values of the <b>regression</b> <b>coefficients</b> that confirm Lucas's results for his sample period. But perturbing monetary policy rule parameters away from the values estimated over Lucas's subsample alters the <b>regression</b> <b>coefficients</b> in ways that reproduce their instability over our longer sample. (JEL C 51, E 23, E 31, E 43, E 51, E 52) ...|$|R
2500|$|Although serial {{correlation}} {{does not affect}} the consistency of [...] the estimated regression coefficients, it does affect our ability to conduct valid statistical tests. First, the F-statistic to test for overall significance of the regression may be inflated under positive {{serial correlation}} because the mean squared error (MSE) will tend to underestimate the population error variance. Second, positive serial correlation typically causes the ordinary least squares (OLS) standard errors for the regression coefficients to underestimate the true standard errors. As a consequence, if positive serial correlation is present in the regression, standard linear regression analysis will typically lead us to compute artificially small standard errors for the <b>regression</b> <b>coefficient.</b> These small standard errors will cause the estimated t-statistic to be inflated, suggesting significance where perhaps there is none. The inflated t-statistic, may in turn, lead us to incorrectly reject null hypotheses, about population values of the parameters of the regression model more often than we would if the standard errors were correctly estimated.|$|E
5000|$|Σx2, Σx, n, Σy2, Σy, Σxy, , xσn, xσn-1, , yσn, yσn-1, <b>Regression</b> <b>{{coefficient}}</b> A, <b>Regression</b> <b>coefficient</b> B, Correlation coefficient r, , , Σx3, Σx2y, Σx4, <b>Regression</b> <b>coefficient</b> C, 1and 2 ...|$|E
5000|$|... of the <b>regression</b> <b>coefficient</b> of the lagged {{dependent}} variable, provided ...|$|E
30|$|The <b>regression</b> <b>coefficients</b> {{estimated}} at low frequencies (< ∼ 1 day− 1) {{may not be}} reliable because the estimated lags have large errors. Here, the <b>regression</b> <b>coefficients</b> are re-estimated assuming that the lag is zero (Fig. 5). At low frequencies, the re-estimated <b>regression</b> <b>coefficients</b> are equivalent to those estimated based on the maximum lagged correlation. So, the actual lags at low frequencies {{are expected to have}} maxima of tens to hundreds of minutes for the respective OBTMs. Compared with the periods of several days, the several-hour lags are relatively small in phase dimension. Thus, the <b>regression</b> <b>coefficients</b> are relatively constant at their maxima at low frequencies.|$|R
40|$|The <b>regression</b> <b>coefficients</b> of length-weight {{relationship}} in {{females and males}} O. ruber showed no significant difference and a common regression is recommended. The <b>regression</b> <b>coefficients</b> depart significantly from 3. Total length-relative condition factor curve showed first major inflexion at 200 mm...|$|R
40|$|Description Post-estimation {{shrinkage}} of <b>regression</b> <b>coefficients</b> in statistical modeling {{can be used}} {{to correct}} for the overestimation of <b>regression</b> <b>coefficients</b> caused by variable selection. While global shrinkage modifies all <b>regression</b> <b>coefficients</b> by the same factor, parameterwise shrinkage factors differ between <b>regression</b> <b>coefficients.</b> With highly correlated or semantically related variables, such as several columns of a design matrix describing a nonlinear effect, parameterwise shrinkage factors are not interpretable and a compromise between global and parameterwise shrinkage, termed 'joint shrinkage', is a useful extension. A computational shortcut to resampling-based shrinkage factor estimation based on DFBETA residuals is applied. Global, parameterwise, and joint shrinkage for models fitted by lm, glm, coxph, or mfp is available...|$|R
5000|$|... is the <b>regression</b> <b>coefficient</b> {{multiplied by}} some {{value of the}} predictor.|$|E
5000|$|... = <b>regression</b> <b>coefficient</b> {{from the}} ith {{independent}} {{variable in the}} full model ...|$|E
5000|$|The {{equation}} for {{calculating the}} FibroTest score <b>regression</b> <b>coefficient</b> (logistic regression) is: ...|$|E
40|$|The use of {{regression}} analysis {{has been instrumental}} in allowing evolutionary biologists to estimate the strength and mode of natural selection. Although directional and correlational selection gradients are equal to their corresponding <b>regression</b> <b>coefficients,</b> quadratic <b>regression</b> <b>coefficients</b> must be doubled to estimate stabilizing/disruptive selection gradients. Based on a sample of 33 papers published in Evolution between 2002 and 2007, at least 78 % of papers have not doubled quadratic <b>regression</b> <b>coefficients,</b> leading to an appreciable underestimate of the strength of stabilizing and disruptive selection. Proper treatment of quadratic <b>regression</b> <b>coefficients</b> is necessary for estimation of fitness surfaces and contour plots, canonical analysis of the γ matrix, and modeling the evolution of populations on an adaptive landscape...|$|R
30|$|A {{simulation}} in R using known <b>regression</b> <b>coefficients</b> {{shows the}} technique accurately estimates the true <b>regression</b> <b>coefficients</b> when {{the data are}} contaminated by outliers, with performance {{comparable to that of}} Aeberhard et al. (2014). The technique also accurately identifies the sample observations subject to contamination.|$|R
40|$|By {{extending}} his data, we document {{the instability of}} two low-frequency <b>regression</b> <b>coefficients,</b> namely, of inflation on money growth and an interest rate on money growth, respectively, that Lucas (1980) used to express two empirical propositions representing the quantity theory of money. We impute the differences in these <b>regression</b> <b>coefficients</b> to differences in monetary policies across periods. Estimation of a DSGE model over a subsample approximating Lucas’s yields parameters that imply values of the two long-run <b>regression</b> <b>coefficients</b> that confirm Lucas’s results for his particular sample period. But perturbing parameters of the monetary policy rule away from values estimated over Lucas’s subsample alters the population values of the two <b>regression</b> <b>coefficients</b> in ways that reproduce the pattern of instability observed over our longer sample...|$|R
50|$|Although several {{statistical}} packages (e.g., SPSS, SAS) {{report the}} Wald statistic {{to assess the}} contribution of individual predictors, the Wald statistic has limitations. When the <b>regression</b> <b>coefficient</b> is large, the standard error of the <b>regression</b> <b>coefficient</b> also tends to be large increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse.|$|E
5000|$|... {{refers to}} the overall <b>regression</b> <b>coefficient,</b> or the slope, between the {{dependent}} variable and the Level 1 predictor.|$|E
5000|$|... the omitted {{variable}} {{must be a}} {{determinant of}} the dependent variable (i.e., its true <b>regression</b> <b>coefficient</b> is not zero); and ...|$|E
40|$|In this paper, we {{consider}} a linear regression model when relevant regressors are omitted in the specified model. We examine the MSE {{dominance of the}} pre-test Stein-rule estimator of <b>regression</b> <b>coefficients</b> using the Stein-variance estimator over the traditional Stein-rule estimator of <b>regression</b> <b>coefficients.</b> Misspecification Pre-test Stein-rule estimator Stein-variance estimator...|$|R
5000|$|... is the (unknown) <b>regression</b> <b>coefficients</b> of {{the design}} factors.|$|R
30|$|The <b>{{regression}}</b> <b>coefficients</b> of the MVP {{regression model}} cannot be interpreted like traditional <b>regression</b> <b>coefficients.</b> Therefore, we have computed the marginal effects of each variable {{that give the}} magnitude of the marginal effects of change in the explanatory variables on the expected value of the dependent based on Greene (2002).|$|R
5000|$|A <b>regression</b> <b>coefficient</b> {{is biased}} in an unknown {{direction}} {{and with an}} unknown magnitude if the confounded interaction terms are non-zero; ...|$|E
5000|$|A <b>regression</b> <b>coefficient</b> {{for a given}} {{main effect}} is {{unbiased}} {{if and only if}} the confounded terms (higher order interactions) are zero; ...|$|E
50|$|As an {{alternative}} to regressions at {{both sides of the}} breakpoint (threshold), the method of partial regression can be used to find the longest possible horizontal stretch with insignificant <b>regression</b> <b>coefficient,</b> outside of which there is a definite slope with a significant <b>regression</b> <b>coefficient.</b> The alternative method can be used for segmented regressions of Type 3 and Type 4 when it is the intention to detect a tolerance level of the dependent variable for varying quantities of the independent, explanatory, variable (also called predictor).|$|E
40|$|It is well {{understood}} that many penalized maximum likelihood estimators correspond to posterior mode estimators under specific prior distributions. Appropriateness {{of a particular}} class of penalty functions can therefore be interpreted as the appropriateness of a prior model for the parameters. For example, the appropriateness of a lasso penalty for <b>regression</b> <b>coefficients</b> depends {{on the extent to}} which the empirical distribution of the <b>regression</b> <b>coefficients</b> resembles a Laplace distribution. We give a simple approximate testing procedure of whether or not a Laplace prior model is appropriate and accordingly, whether or not using a lasso penalized estimate is appropriate. This testing procedure is designed to have power against exponential power prior models which correspond to ℓ_q penalties. Via simulations, we show that this testing procedure achieves the desired level and has enough power to detect violations of the Laplace assumption when the number of observations and number of unknown <b>regression</b> <b>coefficients</b> are large. We then introduce an adaptive procedure that chooses a more appropriate prior model and corresponding penalty from the class of exponential power prior models when the null hypothesis is rejected. We show that this computationally simple adaptive procedure can improve estimation of the unknown <b>regression</b> <b>coefficients</b> both when the unknown <b>regression</b> <b>coefficients</b> are drawn from an exponential power distribution and when the unknown <b>regression</b> <b>coefficients</b> are sparse and drawn from a spike-and-slab distribution...|$|R
40|$|This study aims {{to analyze}} {{supplier}} performance and Build Competitive Advantage Supplier Supplier Supplier being in PT Indo Sutech Sejahtera (PT. ISS) in Semarang. This research {{to help the}} supplier on the Kontrkator Company engaged in Fabrication, Assembly and Installation {{in order to become}} a supplier that can compete and have a strong performance as a supplier chosen by the Contractor. This research was conducted in Semarang and surrounding areas, especially the suppliers of goods and materials to PT Indo Sutech Sejahtera in Semarang. The population and the sample used is a total of 109 respondents. Technical analysis is Multiple Linear Regression Analysis to analyze the influence {{of the relationship between the}} variables. The results showed with Multiple <b>Regression</b> <b>Regression</b> <b>Coefficients</b> Ability to Customize Price Competitive Advantage: - 0, 153, t Count: 0, 268, The value of significance. <b>Regression</b> <b>coefficients</b> : - 1, 113, <b>Regression</b> <b>Coefficients</b> ability Customize Price with Supplier Performance: - 0, 078, t Count: 0, 217, The value of significance : - 1, 243. <b>Regression</b> <b>coefficients</b> Delivery Time with Competitive Advantage : 0, 652, t Count: 4, 731, The value of significance: 0, 000. <b>Regression</b> <b>coefficients</b> Suppliers with Performance Delivery Time: 0, 107, t Count: 1, 548, The value of significance 0, 125. <b>Regression</b> <b>coefficients</b> Competitive Advantage with Supplier Performance: 0, 896, t Count: 20, 332, The value of significance : 0, 000. ...|$|R
40|$|This study {{examined}} {{a method for}} calculating the impact of multicollinearity on multilevel modeling. The major research questions concerned a) how the simulation design factors affect (multilevel variance inflation factor) MVIF, b) how MVIF affects standard errors of <b>regression</b> <b>coefficients,</b> and c) how MVIF affects significance of <b>regression</b> <b>coefficients.</b> Monte Carlo simulations were conducted to address these questions. Predictor relationships were manipulated in order to simulate multicollinearity. Findings indicate that a) increases in relationships among Level 1 predictors and also relationships among Level 2 predictors led to increased MVIF for those specific variables, b) as MVIF increases for a predictor, the standard errors for the <b>regression</b> <b>coefficients</b> also increase., and c) when MVIF values for the <b>regression</b> <b>coefficients</b> were 5 or higher, margins of error were around. 20, and therefore any coefficients around. 20 or lower will become non-significant...|$|R
5000|$|Galton {{was able}} to further his notion of {{regression}} by collecting and analyzing data on human stature. Galton asked for help of mathematician J. Hamilton Dickson in investigating the geometric relationship of the data. He determined that the <b>regression</b> <b>coefficient</b> did not ensure population stability by chance, but rather that the <b>regression</b> <b>coefficient,</b> conditional variance, and population were interdependent quantities related by a simple equation. [...] Thus Galton identified that the linearity of regression was not coincidental but rather was a necessary consequence of population stability.|$|E
5000|$|Wald {{statistic}} {{is defined}} by, {{where is the}} sample estimation of and is the standard error of [...] Alternatively, when assessing the contribution of individual predictors in a given model, one may examine {{the significance of the}} Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the <b>regression</b> <b>coefficient</b> to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution. Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has some limitations. First, When the <b>regression</b> <b>coefficient</b> is large, the standard error of the <b>regression</b> <b>coefficient</b> also tends to be large increasing the probability of Type-II error. Secondly, the Wald statistic also tends to be biased when data are sparse.|$|E
5000|$|A second {{application}} involves using regression analysis, which {{estimates from}} statistics the ordinate (Y-estimate), derivative (<b>regression</b> <b>coefficient)</b> and constant (Y-intercept) of calculus. The <b>regression</b> <b>coefficient</b> estimates {{the rate of}} change of the function predicting Y from X, based on minimizing the residuals between the fitted curve and the observed data (MINRES). No alternative method of estimating such a function satisfies this basic requirement of MINRES. In general, the <b>regression</b> <b>coefficient</b> is estimated as the ratio of the covariance(XY) to the variance of the determinator (X). In practice, the sample size is usually the same for both X and Y, so this can be written as SCP(XY) / SS(X), where all terms have been defined previously. In the present context, the parents are viewed as the [...] "determinative variable" [...] (X), and the offspring as the [...] "determined variable" [...] (Y), and the <b>regression</b> <b>coefficient</b> as the [...] "functional relationship" [...] (ßPO) between the two. Taking cov(MPO) = ½ s2A [...] as cov(XY), and [...] s2P / 2 [...] (the variance of the mean of two parents—the mid-parent) as s2X, {{it can be seen that}} ßMPO = s2A / s2P = h2 [...] Next, utilizing cov(PO) = ½ s2A as cov(XY), and [...] s2P as s2X, it is seen that [...] 2 ßPO = 2 (½ s2A / s2P = H2 [...]|$|E
40|$|AbstractWe {{consider}} {{the problem of}} estimating regression models of two-dimensional random fields. Asymptotic properties of the least squares estimator of the linear <b>regression</b> <b>coefficients</b> are studied for the case where the disturbance is a homogeneous random field with an absolutely continuous spectral distribution and a positive and piecewise continuous spectral density. We obtain necessary and sufficient conditions on the regression sequences such that a linear estimator of the <b>regression</b> <b>coefficients</b> is asymptotically unbiased and mean square consistent. For such regression sequences the asymptotic covariance matrix of the linear least squares estimator of the <b>regression</b> <b>coefficients</b> is derived...|$|R
5000|$|F {{test for}} equality/inequality of the <b>regression</b> <b>coefficients</b> in Multiple Regression; ...|$|R
5000|$|Tests of Significance of Means, Difference of Means, and <b>Regression</b> <b>Coefficients</b> ...|$|R
