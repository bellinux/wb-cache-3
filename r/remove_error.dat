19|930|Public
6000|$|... [A] The Lycée, No. 36, for January 1st, has a {{long and}} rather [...] {{premature}} article, in which it endeavours to show anticipations by [...] French philosophers of my researches. It however mistakes the [...] erroneous results of MM. Fresnel and Ampère for true ones, and then [...] imagines my true results are like those erroneous ones. I notice it [...] here, however, {{for the purpose of}} doing honour to Fresnel in a much [...] higher degree than would have been merited by a feeble anticipation of [...] the present investigations. That great philosopher, at the same time [...] with myself and fifty other persons, made experiments which the [...] present paper proves could give no expected result. He was deceived [...] for the moment, and published his imaginary success; but on more [...] carefully repeating his trials, he could find no proof of their [...] accuracy; and, in the high and pure philosophic desire to <b>remove</b> <b>error</b> [...] as well as discover truth, he recanted his first statement. The [...] example of Berzelius regarding the first Thorina is another instance [...] of this fine feeling; and as occasions are not rare, it would be to [...] the dignity of science if such examples were more frequently [...] followed.--February 10th, 1832.|$|E
50|$|It is {{important}} to consider such aspects in using a geological compass, for traditional compasses rely on inertia to <b>remove</b> <b>error</b> caused by operator movement. With traditional compasses there is no record of error caused by poor damping or operator movement. This limitation is removed by use of a digital compass, though these may be more error prone because of the sensitivity of the accelerometer, which programs use to determine vertical and horizontal. Therefore, professional use of a digital geological compass requires the recoding of variance in individual measurements.|$|E
50|$|Although {{proportional}} control is simple to understand, it has drawbacks. The largest {{problem is that}} for most systems it will never entirely <b>remove</b> <b>error.</b> This is because when error is 0 the controller only provides the steady state control action so the system will settle {{back to the original}} steady state (which is probably not the new set point that we want the system to be at). To get the system to operate near the new steady state, the controller gain, Kc, must be very large so the controller will produce the required output when only a very small error is present. Having large gains can lead to system instability or can require physical impossibilities like infinitely large valves.|$|E
30|$|The {{software}} failure intensity λ (t) is explained as {{the percentage of}} the <b>removed</b> <b>errors</b> in the software product.|$|R
5000|$|A {{version of}} The Great Global Warming Swindle (edited by Durkin to <b>remove</b> <b>errors)</b> was {{broadcast}} in New Zealand on Prime TV, 8:40 pm, 1 June 2008. Following the program {{there was an}} hour-long panel discussion, moderated by Prime presenter Eric Young, including the following people: ...|$|R
50|$|Constant {{systematic}} {{errors are}} very difficult to deal with as their effects are only observable if they can be <b>removed.</b> Such <b>errors</b> cannot be <b>removed</b> by repeating measurements or averaging large numbers of results. A common method to <b>remove</b> systematic <b>error</b> is through calibration of the measurement instrument.|$|R
50|$|In 601 AD Pope Gregory I wrote {{a letter}} to Mellitus (a member of the Gregorian mission sent to England to convert the Anglo-Saxons from their native paganism to Christianity) which read:When, therefore, Almighty God shall bring you to the most reverend man our brother bishop, St Augustine, tell him what I have, upon mature {{deliberation}} on the affair of the English, thought of; namely, that the temples of the idols in that nation ought not to be destroyed. Let holy water be made, and sprinkled in the said temples; let altars be erected, and let relics be deposited in them. For since those temples are built, it is requisite that they be converted from the worship of the devils to the service of the true God; that the nation, not seeing those temples destroyed, may <b>remove</b> <b>error</b> from their hearts, and knowing and adoring the true God, may the more familiarly resort to the same places to which they have been accustomed. And because they are wont to sacrifice many oxen in honour of the devils, let them celebrate a religious and solemn festival, not slaughtering the beasts for devils, but to be consumed by themselves, to the praise of God...|$|E
40|$|Abstract. A new {{algorithm}} of feature matching {{was presented}} by this paper. In the new algorithm, Curvelet first used to denoise, then RANSAC optimized SIFT algorithm used to stitch images. Experiments {{show that the}} new algorithm is better to <b>remove</b> <b>error</b> matching points. It improved the accuracy of local invariant feature matching. New algorithm has a good anti-noise and lighting capabilities. It has strong adaptability to scale changes, the algorithm is robust, and has better effect on matching and stitching...|$|E
40|$|Goebels Motivation: It is {{important}} to <b>remove</b> <b>error</b> and noise from Microarrays to get clear results. We compared many normalization methods with different preprocessing steps {{to find out which}} performs best Results: We compared several normalization methods from the affy package including the VSN Method from the VSN package and the Centralization method. We used 2 measures of quality, first the correlation of the probe sets from one gene between arrays, second how often it occurs, that a gene is below a certain P-Value threshold and that all probe sets of this gene are also under the same threshold of the corresponding Expression Matrix. Availability: The affy package and the VSN package are availabl...|$|E
30|$|The last step, {{performed}} semi-automatically, is {{to group}} synonymous topics. We use the WordNet 2 lexical database {{as a basis}} to obtain the synonyms of the lemmas for each existing topic and group those topics who share the synonyms. After performing this step, we make a manual check to <b>remove</b> <b>errors</b> and noise.|$|R
40|$|Abstract. In this paper, {{a method}} is {{introduced}} to <b>remove</b> the counting <b>errors</b> of electronic counting instruments, which contains two aspects to <b>remove</b> the <b>errors</b> caused by outside interference and to <b>remove</b> the counting <b>errors</b> caused by quality error {{of the object}} respectively. Counting errors of object can be removed effectively by program of software system. As a result, it’s proven to be effective and stable by applying it into practice...|$|R
30|$|Step 5 : Use {{random sample}} {{consensus}} method [24] to <b>remove</b> the <b>error</b> matching.|$|R
40|$|We {{propose a}} new {{approach}} towards derandomization in the uniform setting, where it is computationally hard to find possible mistakes in the simulation of a given probabilistic algorithm. The approach consists in combining both easiness and hardness complexity assumptions: if a derandomization method based on an easiness assumption fails, then we obtain a certain hardness test {{that can be used}} to <b>remove</b> <b>error</b> in BPP algorithms. As an application, we prove that every RP algorithm can be simulated by a zero-error probabilistic algorithm, running in expected subexponential time, that appears correct infinitely often (i. o.) to every efficient adversary. A similar result by Impagliazzo and Wigderson (FOCS' 98) states that BPP allows deterministic subexponential-time simulations that appear correct with respect to any efficiently sampleable distribution i. o., under the assumption that EXP 6 = BPP; in contrast, our result does not rely on any unproven assumptions. As another applicatio [...] ...|$|E
40|$|Personal Digital Assistants (PDAs) {{are quickly}} {{becoming}} very widely used handheld devices with millions sold since their introduction. With such advances in applications for PDAs, new problems with security arise, because {{traditional methods of}} restricting access can be bypassed. The development of a fingerprint sensor and authentication device (biometric authentication module) {{makes it possible to}} add another layer of security to the handheld device, and bolster security protocols that are already in use. The goal of this project is to successfully integrate the biometric authentication module with a PDA and create an application that uses this added level of security. The source code developed at this point has successfully allowed the handheld and biometric devices to communicate. By assembling each packet, and then processing it for error checking on the palm pilot, the palm pilot is able to send commands to the fingerprint module. Similar functions can be written to <b>remove</b> <b>error</b> checking parts of packets so Personal Digital Assistants (PDAs) are quickly becoming very widely use...|$|E
40|$|Electric {{power is}} a kind of unstorable energy {{concerning}} the national welfare and the people’s livelihood, the stability of which is attracting more and more attention. Because the short-term power load is always interfered by various external factors with the characteristics like high volatility and instability, a single model is not suitable for short-term load forecasting due to low accuracy. In order to solve this problem, this paper proposes a new model based on wavelet transform and the least squares support vector machine (LSSVM) which is optimized by fruit fly algorithm (FOA) for short-term load forecasting. Wavelet transform is used to <b>remove</b> <b>error</b> points and enhance the stability of the data. Fruit fly algorithm is applied to optimize the parameters of LSSVM, avoiding the randomness and inaccuracy to parameters setting. The result of implementation of short-term load forecasting demonstrates that the hybrid model can be used in the short-term forecasting of the power system...|$|E
40|$|We {{develop the}} theory of special biserial and string coalgebras and other {{concepts}} from the representation theory of quivers. These tools are then {{used to describe the}} finite dimensional comodules and Auslander-Reiten quiver for the coordinate Hopf algebra of quantum SL(2) at a root of unity. We also describe the stable Green ring and compute quantum dimensions. Comment: previous section 2. 2 <b>removed,</b> <b>errors</b> correcte...|$|R
50|$|Reference genome {{sequences}} {{and maps}} {{continue to be}} updated, <b>removing</b> <b>errors</b> and clarifying regions of high allelic complexity. The decreasing cost of genomic mapping has permitted genealogical sites to offer it as a service, {{to the extent that}} one may submit one's genome to crowd sourced scientific endeavours such as DNA.land at the New York Genome Center, an example both of the economies of scale and of citizen science.|$|R
50|$|Wind data {{analysis}} software assist the user in <b>removing</b> measurement <b>errors</b> from wind data sets and perform specialized statistical analysis.|$|R
40|$|The {{growth of}} the {{cellular}} technology and wireless networks {{all over the world}} has increased the demand for digital information by manifold. This massive demand poses difficulties for handling huge amounts of data that need to be stored and transferred. To overcome this problem we can compress the information by removing the redundancies present in it. Redundancies are the major source of generating errors and noisy signals. Coding in MATLAB helps in analyzing compression of speech signals with varying bit rate and remove errors and noisy signals from the speech signals. Speech signal’s bit rate can also be reduced to <b>remove</b> <b>error</b> and noisy signals which is suitable for remote broadcast lines, studio links, satellite transmission of high quality audio and voice over internet This paper focuses on speech compression process and its analysis through MATLAB by which processed speech signal can be heard with clarity and in noiseless mode at the receiver end...|$|E
40|$|Estimates of CO 2 fluxes {{that are}} based on {{atmospheric}} measurements rely upon a meteorology model to simulate atmospheric transport. These models provide a quantitative link between the surface fluxes and CO 2 measurements taken downwind. Errors in the meteorology can therefore cause errors in the estimated CO 2 fluxes. Meteorology errors that correlate or covary across time and/or space are particularly worrisome; they can cause biases in modeled atmospheric CO 2 that are easily confused with the CO 2 signal from surface fluxes, and they are difficult to characterize. In this paper, we leverage an ensemble of global meteorology model outputs combined with a data assimilation system to estimate these biases in modeled atmospheric CO 2. In one case study, we estimate the magnitude of month-long CO 2 biases relative to CO 2 boundary layer enhancements and quantify how that answer changes if we either include or <b>remove</b> <b>error</b> correlations or covariances. In a second case study, we investigate which meteorological conditions are associated with these CO 2 biases. In the first case study, we estimate uncertainties of 0. 5 – 7 ppm in monthly-averaged CO 2 concentrations, depending upon location (95 % confidence interval). These uncertainties correspond to 13 – 150 % of the mean afternoon CO 2 boundary layer enhancement at individual observation sites. When we <b>remove</b> <b>error</b> covariances, however, this range drops to 2 – 22 %. Top-down studies that ignore these covariances could therefore underestimate the uncertainties and/or propagate transport errors into the flux estimate. In the second case study, we find that these month-long errors in atmospheric transport are anti-correlated with temperature and planetary boundary layer (PBL) height over terrestrial regions. In marine environments, by contrast, these errors are more strongly associated with weak zonal winds. Many errors, however, are not correlated with a single meteorological parameter, suggesting that a single meteorological proxy is not sufficient to characterize uncertainties in atmospheric CO 2. Together, these two case studies provide information to improve the setup of future top-down inverse modeling studies, preventing unforeseen biases in estimated CO 2 fluxes...|$|E
40|$|Solving partial {{differential}} equations (PDEs) using analytical techniques is intractable {{for all but}} the simplest problems. Many computational approaches to approximate solutions to PDEs yield large systems of linear equations. Algorithms known as linear solvers then compute an approximate solution to the linear system. Multigrid methods are one class of linear solver and find an approximate solution to a linear system through two complementary processes: relaxation and coarse-grid correction. Relaxation cheaply annihilates portions of error from the approximate solution, while coarse-grid correction constructs a lower dimensional problem to <b>remove</b> <b>error</b> remaining after relaxation. In algebraic multigrid (AMG), the lower dimensional space is constructed by coarse-grid selection algorithms. In this thesis, an introduction and study of independent set-based parallel coarse-grid selection algorithms is presented in detail, following a review of algebraic multigrid. The behavior of the Cleary-Luby-Jones-Plassmann (CLJP) algorithm is analyzed and modifications to the initialization phase of CLJP are recommended, resulting in the CLJP in Color (CLJP-c) algorithm, which achieves large performance gains over CLJP for problems on uniform grids. CLJP-c is then extended to the Parallel Modified Independent Set (PMIS) coarse-grid selection algorithm producin...|$|E
40|$|Abstract. Error bounds between non-stationary binary {{subdivision}} curves/surfaces {{and their}} control polygons after k-fold subdivision are estimated. The bounds can be evaluated without recursive subdivision. A computational formula of subdivision depth for non-stationary binary subdivision surfaces is also presented. From this formula {{one can predict}} the subdivision depth within a user specified error tolerance. Our results not only <b>remove</b> <b>errors</b> in the results of [3] but also contain the generalization of their results...|$|R
40|$|In {{this study}} we present two novel {{normalization}} schemes for cDNA microarrays. They are based on iterative local regression and optimization of model parameters by generalized cross-validation. Permutation tests assessing the efficiency of normalization demonstrated that the proposed schemes have an improved ability to <b>remove</b> systematic <b>errors</b> and to reduce variability in microarray data. The analysis also reveals that without parameter optimization local regression is frequently insufficient to <b>remove</b> systematic <b>errors</b> in microarray data...|$|R
40|$|We {{compute the}} drag force exerted on a quark and a di-quark systems in a {{background}} dual to large-N QCD at finite temperature. We find that appears a drag {{force in the}} former setup with flow of energy proportional to {{the mass of the}} quark while in the latter there is no dragging as in other studies. We also review the screening length. Comment: 15 pages, typos <b>removed,</b> <b>error</b> corrected, refs adde...|$|R
40|$|AbstractWe {{propose a}} new {{approach}} toward derandomization in the uniform setting, where it is computationally hard to find possible mistakes in the simulation of a given probabilistic algorithm. The approach consists in combining both easiness and hardness complexity assumptions: if a derandomization method based on an easiness assumption fails, then we obtain a certain hardness test {{that can be used}} to <b>remove</b> <b>error</b> in BPP algorithms. As an application, we prove that every RP algorithm can be simulated by a zero-error probabilistic algorithm, running in expected subexponential time, that appears correct infinitely often (i. o.) to every efficient adversary. A similar result by Impagliazzo and Wigderson (FOCS' 98) states that BPP allows deterministic subexponential-time simulations that appear correct with respect to any efficiently sampleable distribution i. o., under the assumption that EXP≠BPP; in contrast, our result does not rely on any unproven assumptions. As another application of our techniques, we get the following gap theorem for ZPP: either every RP algorithm can be simulated by a deterministic subexponential-time algorithm that appears correct i. o. to every efficient adversary or EXP=ZPP. In particular, this implies that if ZPP is somewhat easy, e. g., ZPP⊆DTIME(2 nc) for some fixed constant c, then RP is subexponentially easy in the uniform setting described above...|$|E
40|$|The {{effective}} {{deployment of}} Automated Guided Vehicles (AGV) in indoor hostile environment demands high precision localization for navigation purposes. The cricket system, based on Time Difference of Arrival (TDoA) approach, provides precise localization. It calculates TDoA between radio and ultrasonic signal to estimate distance between two cricket motes. The position, in the referenced coordinate system, is estimated by multilateration using estimated distances from multiple motes. Therefore, the position accuracy {{depends on the}} distance estimation which in turn depends on the TDoA calculation. Ultrasonic transducer, a component of cricket mote hardware, has highly directive radiation pattern which affects ultrasonic signal detection. This delayed detection of ultrasonic signal, based on distance and relative angle, introduces error in the distance estimation between a pair of two cricket motes. A low cost, easy to implement error correction method is developed in this thesis to <b>remove</b> <b>error</b> introduced by delayed ultrasonic signal detection. Relationship between error (in distance estimation), relative angle and distance between two cricket motes is derived using traditional regression analysis. This relationship is then used to predict and correct error involved in the distance estimation. Experimental {{results show that the}} mean error in cricket position estimation for 2 -Dimension and 3 -Dimension is reduced by an average 43 % and 39 %, respectively...|$|E
40|$|New {{features}} Offline kernel compiler and linker Add silent "by index" device {{filter and}} context constructor Add OpenCL 2. 1 symbols (device information, command names, [...] .) Bug fixes Fix release of device array in dependent filters Fix possibly uninitialized context and device in context module Fix device selection menu in MSYS Fix intermittent segfault in OSX buffer migrate test (issue # 9) Fix pointer comparison in device selection test for 32 -bit platforms Other changes Build log not automatically fetched after build/compile/link Remove ccl_kerninfo utility (moved functionality to new ccl_c utility) Add/build against OpenCL 2. 1 headers Add tests for the command-line utilities and examples Improved error reporting in context and devsel modules Add extra debug option for logging creation/destruction of wrapper objects Allow to select test device using environment variable Add tests-specific log domain Set event name to "UNKNOWN" instead of NULL for unknown commands Stable API for all OpenCL versions, unsupported operations throw error Multiple documentation improvements Multiple build improvements in CMake configuration Set minimum CMake version to 2. 8. 3 Remove support for building with Visual Studio, improve support for building with MinGW Refactor cf 4 ocl as a self-contained library: Remove requirement for direct GLib and OpenCL dependencies from cf 4 ocl client code Hide GLib and OpenCL from cf 4 ocl from client code unless explicitly required <b>Remove</b> <b>error</b> handling macros from public AP...|$|E
40|$|This {{is a short}} {{companion}} {{paper to}} arXiv: 0810. 3470. We construct an integrable system on an open subset of a Fano manifold equipped with a toric degeneration, and compute the potential function for its Lagrangian torus fibers if the central fiber is a toric Fano variety admitting a small resolution. Comment: 14 pages, no figures. v 2 : <b>removed</b> <b>errors</b> from {{an earlier version of}} arXiv: 0810. 3470, and added discussions on quadric surfac...|$|R
40|$|The Gravity Recovery and Climate Experiment {{launched}} March 17, 2002. The GPS {{data for}} this experiment are processed {{to contribute to the}} recover long wavelength gravity field; <b>remove</b> <b>errors</b> due to long term on-board oscillator drift; and align K/Ka-band measurments between the two spacecraft to 0. 1 ns. This paper will concentrate on the use of GPS for these timing and calibration functions and will not address the recovery of the gravity field...|$|R
50|$|Ante hoc fact-{{checking}} (fact checking before dissemination) aims to <b>remove</b> <b>errors</b> {{and allow}} text {{to proceed to}} dissemination (or to rejection if it fails confirmations or other criteria). Post hoc fact-checking is most often followed by a written report of inaccuracies, sometimes with a visual metric from the checking organization (e.g., Pinocchios from The Washington Post Fact Checker, or TRUTH-O-METER ratings from PolitiFact). Several organizations are devoted to post hoc fact-checking, such as FactCheck.org and PolitiFact.|$|R
40|$|The {{effectiveness}} of the ensemble Kalman filter (EnKF) for a thermally-forced nonlinear two-dimensional sea breeze model is investigated in a perfect-model setting. The model that was developed for this purpose is hydrostatic, non-rotational, and incompressible. Forcing is maintained through an explicit spatially- and diurnally-varied heating function with an added stochastic component. Pure forecast experiments reveal that the model exhibits only modest levels of overall nonlinearity. Strongest nonlinearity coincides with the peak sea breeze phase of the circulation in timing and with the nonlinear sea breeze front spatially. Considerable small-scale error growth occurs at this phase, which is most pronounced for vorticity and vertical motion. Application of data assimilation through an EnKF is observed to successfully remove most of the large-scale phase-difference errors resulting from the climatological initialization method. Even at the first analysis step, domain-averaged error for buoyancy and vorticity is reduced by about 85 % and 65 %, respectively. Subsequent analyses continue to <b>remove</b> <b>error</b> at an increasingly slower rate and error ultimately saturates within about 24 hours {{at a level that}} is determined by observation accuracy. The filter is found to be most sensitive to observation accuracy, observation frequency, and radius of influence while ensemble size has shown significant sensitivity only at smaller numbers of ensemble members. Assimilation of an additional single sounding has also resulted in improved error reduction although the amount of reduction {{did not seem to be}} sensitive to the frequency of sounding assimilations. 2 1...|$|E
40|$|Quality {{control is}} an {{essential}} operation of pharmaceutical industries. Drugs must be marketed as safe and therapeutically active formulations whose performance is consistent and predictable. New and better medicinal agents are being produced at an accelerated rate. At the same time more exacting and sophisticated analytical methods are being developed for their evaluation. Currently, world-wide efforts {{have been made to}} ensure the practice of quality along with coast effective good quality medicines. Parenterals are the sterile preparation that is directly administered into the circulatory system avoiding the enteral route. These preparation provide rapid onset of action compared to others, but the most concerning topics related to this, is their stability problem that arises from microbial contamination of the products. Therefore, to ensure their sterility and stability, regulations regarding to quality control through pharmacopeial specifications has a great importance. Pharmacopeias provides an effective guideline to overcome those problems by following current good manufacturing practices and establishing standard operating procedures. In-process quality control tests are done with a motive to <b>remove</b> <b>error</b> from every stage in production and maintain the quality of the final product with the compendial standards as specified in the pharmacopoeias. The quality of final products depends on in-process control tests, because it helps to incorporate excellence within the products. The qualitative and quantitative parameters of pharmaceutical products are checked by finished product quality controls tests. Therefore, the drive {{of this study is to}} provide concise information on the in-process and finished product quality control tests for parenteral preparations as per different pharmacopoeias. Keywords: Parenteral pharmaceuticals, Pharmacopoeia, Specification, In-process quality control, Finished product quality control...|$|E
40|$|Population, {{community}} and functional measures or metrics in rapid bioassessment programs aid in establishing biological criteria for streams and rivers. Each metric measures {{different aspects of}} community structure and is important in detecting changes in macroinvertebrate community structure that are influenced by changes in water quality. In this study, temporal variation of nine commonly used bioassessment indices was examined in three midwestern streams. The indices were calculated for each of nine replicate benthic macroinvertebrate samples collected monthly for one year from Cowpie Creek (CC), Nippersink Creek (NC) and Lawrence Creek (LC), McHenry County, Illinois. In practice, the habitat sampled for bioassessments often is limited to riffle sites {{in an attempt to}} reduce spatial variability in the indices and midges (Diptera: Chironomidae) often are omitted to <b>remove</b> <b>error</b> associated with sampling, identification and variable life histories. Where appropriate, indices in this study were calculated using all sites, using only riffles areas, using all macroinvertebrates, and using all macroinvertebrates exclusive of Chironomidae. Sites were ordinated by Detrended Correspondence Analysis (DCA) to reveal temporal trends among index ratings. In assessments using all sites, biotic indices reflected temporal changes in macroinvertebrate community structure differently than the multimetric index and taxa richness metrics. When only riffle sites were included in assessments, the ability of indices and metrics to reflect macroinvertebrate community structure was dependent on the community assessed, improving in some streams but not in others. Similarly, the omission of Chironomidae from assessments resulted in differing abilities of the indices to reflect macroinvertebrate community structure. Because the indices showed poor performance in some streams when assessing riffle sites or omitting midges from assessments, it was concluded that all habitats and all macroinvertebrates should be included in assessment protocols. Although multimetric indices provide more information about stream communities than biotic indices, the use of ordination analyses are helpful in verifying accuracy of water quality assessments...|$|E
30|$|A {{database}} of online Japanese character patterns [31, 171] was compiled to support research in Japanese character recognition systems. These characters were extracted from unconstrained textual phrases provided by 80 writers. The text was collected from Japanese newspapers and produced 1227 frequently occurring Japanese character categories. The patterns were manually inspected and corrected to <b>remove</b> <b>errors</b> and wrongly written characters. The database {{has been used}} in a number of online character recognition systems [172 – 175].|$|R
3000|$|... -class and rectifies (<b>removes)</b> the <b>errors</b> of Mishra et al. (Mat. Vesn., 2013). Few {{examples}} and applications are also highlighted in this manuscript.|$|R
50|$|Max Farrand's final work, an {{examination}} of the letters of Benjamin Franklin (<b>removing</b> transcription <b>errors</b> of its first editor, John Bigelow), was published posthumously.|$|R
