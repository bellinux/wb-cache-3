2474|271|Public
25|$|Quantum <b>relative</b> <b>entropy</b> – {{a measure}} of distinguishability between two quantum states.|$|E
25|$|Just as {{absolute}} entropy {{serves as}} theoretical background for data compression, <b>relative</b> <b>entropy</b> serves as theoretical background for data differencing – the absolute entropy {{of a set}} of data in this sense being the data required to reconstruct it (minimum compressed size), while the <b>relative</b> <b>entropy</b> of a target set of data, given a source set of data, is the data required to reconstruct the target given the source (minimum size of a patch).|$|E
25|$|This {{inequality}} {{is called}} Monotonicity of quantum <b>relative</b> <b>entropy.</b> Owing to the Stinespring factorization theorem, this inequality {{is a consequence}} of a particular choice of the CPTP map - a partial trace map described below.|$|E
40|$|Abstract. We discuss {{stochastic}} {{representations of}} advection diffusion equations with variable diffusivity, stochastic integrals of motion and generalized <b>relative</b> <b>entropies.</b> Key words. <b>Relative</b> <b>entropies,</b> stochastic integrals of motion, stochastically passive scalars, stochastic Lagrangian transport. subject classifications. AMS- MSC numbers: 35 K 45, 60 H 30. 1...|$|R
40|$|Tsallis <b>relative</b> {{operator}} <b>entropy</b> {{is defined}} as a parametric extension of the <b>relative</b> operator <b>entropy.</b> Some properties of the Tsallis <b>relative</b> operator <b>entropy</b> are investigated. Also some operator inequalities related to the Tsallis <b>relative</b> operator <b>entropy</b> are shown. Comment: The updated version has already been appeared in math/ 0502338 and it was published from Linear Algebra and its Applications...|$|R
40|$|Abstract. Tsallis <b>relative</b> {{operator}} <b>entropy</b> {{was defined}} as a parametric extension of <b>relative</b> operator <b>entropy</b> and the generalized Shannon inequalities were shown in the previous paper. After the review of some fundamental properties of Tsallis <b>relative</b> operator <b>entropy,</b> some operator inequalities related to Tsallis <b>relative</b> operator <b>entropy</b> are shown in the present paper. Our inequalities give the upper and lower bounds of Tsallis <b>relative</b> operator <b>entropy.</b> The operator equality on Tsallis <b>relative</b> operator <b>entropy</b> is also shown by considering the tensor product. This relation generalizes the pseudoadditivity for Tsallis entropy. As a corollary of our operator equality derived from the tensor product manipulation, we show several operator inequalities including the superadditivity and the subadditivity for Tsallis <b>relative</b> operator <b>entropy.</b> Our results are generalizations of the superadditivity and the subadditivity for Tsallis entropy...|$|R
25|$|In {{mathematical}} statistics, the Kullback–Leibler divergence (also called <b>relative</b> <b>entropy)</b> is {{a measure}} of how one probability distribution diverges from a second, expected probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread. In the simple case, a Kullback–Leibler divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a Kullback–Leibler divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. In simplified terms, it {{is a measure}} of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.|$|E
2500|$|<b>Relative</b> <b>Entropy</b> (also called KullbackLeibler divergence) {{symmetry}} ...|$|E
2500|$|Monotonicity {{of quantum}} <b>relative</b> <b>entropy</b> under partial trace (MPT); ...|$|E
40|$|The {{main purpose}} of this article is to study {{estimates}} for the Tsallis <b>relative</b> operator <b>entropy,</b> by the use of Hermite-Hadamard inequality. Thus, we obtain alternative bounds for the Tsallis <b>relative</b> operator <b>entropy.</b> In the process to derive these bounds, we established the significant relation between the Tsallis <b>relative</b> operator <b>entropy</b> and the generalized <b>relative</b> operator <b>entropy.</b> In addition, we study the properties on monotonicity for the weight of operator means, and for the parameter of <b>relative</b> operator <b>entropies.</b> Comment: to appear in Acta Mathematica Vietnamic...|$|R
30|$|Both {{block and}} <b>relative</b> <b>entropies</b> are similar {{what means that}} both protein kinds contain {{strongly}} random sequences.|$|R
40|$|The entropy {{production}} paradox for anomalous diffusion processes describes a phenomenon where one-parameter families of dynamical equations, falling between the diffusion and wave equations, have {{entropy production}} rates (Shannon, Tsallis or Renyi) that increase toward the wave equation limit unexpectedly. Moreover, also surprisingly, the entropy does not order the bridging regime between diffusion and waves at all. However, {{it has been}} found that <b>relative</b> <b>entropies,</b> with an appropriately chosen reference distribution, do. <b>Relative</b> <b>entropies,</b> thus, provide a physically sensible way of setting which process is “nearer” to pure diffusion than another, placing pure wave propagation, desirably, “furthest” from pure diffusion. We examine here the time behavior of the <b>relative</b> <b>entropies</b> under the evolution dynamics of the underlying one-parameter family of dynamical equations based on space-fractional derivatives...|$|R
2500|$|In {{this form}} the <b>relative</b> <b>entropy</b> generalises (up {{to change in}} sign) both the {{discrete}} entropy, where the measure [...] is the counting measure, and the differential entropy, where the measure [...] is the Lebesgue measure. If the measure [...] is itself a probability distribution, the <b>relative</b> <b>entropy</b> is non-negative, and zero if [...] as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly {{takes into account the}} transformation of the measure [...] The <b>relative</b> <b>entropy,</b> and implicitly entropy and differential entropy, do depend on the [...] "reference" [...] measure [...]|$|E
2500|$|... the {{monotonicity}} {{of quantum}} <b>relative</b> <b>entropy</b> (which follows from (...) implies SSA.|$|E
2500|$|... {{which is}} called Monotonicity of quantum <b>relative</b> <b>entropy</b> under partial trace.|$|E
30|$|Corollary  4.8 {{gives the}} {{relation}} between Furuta parametric <b>relative</b> operator <b>entropy</b> and Tsallis <b>relative</b> operator <b>entropy</b> in a more general setting than the result in [4, Theorem  2.3].|$|R
40|$|AbstractTsallis <b>relative</b> {{operator}} <b>entropy</b> {{is defined}} and then its properties are given. Shannon inequality and its reverse one in Hilbert space operators derived by Furuta [Linear Algebra Apll. 381 (2004) 219] are extended {{in terms of}} the parameter of the Tsallis <b>relative</b> operator <b>entropy.</b> Moreover the generalized Tsallis <b>relative</b> operator <b>entropy</b> is introduced and then several operator inequalities are derived...|$|R
50|$|He became {{interested}} in the third law of thermodynamics as a field of research during his experimental research for his Ph.D. research under Professor George Ernest Gibson comparing the <b>relative</b> <b>entropies</b> of glycerine crystals and glass.|$|R
2500|$|Umegaki's quantum <b>relative</b> <b>entropy</b> of two density {{matrices}} [...] and [...] is ...|$|E
2500|$|To see {{how this}} follows from the joint {{convexity}} of <b>relative</b> <b>entropy,</b> observe that ...|$|E
2500|$|The <b>relative</b> <b>entropy,</b> or KullbackLeibler divergence, {{is always}} non-negative. [...] A few {{numerical}} examples follow: ...|$|E
40|$|This paper extends some {{geometric}} {{properties of}} a one-parameter family of <b>relative</b> <b>entropies.</b> These arise as redundancies when cumulants of compressed lengths are considered instead of expected compressed lengths. These parametric <b>relative</b> <b>entropies</b> are a generalization of the Kullback-Leibler divergence. They satisfy the Pythagorean property and behave like squared distances. This property, which {{was known for}} finite alphabet spaces, is now extended for general measure spaces. Existence of projections onto convex and certain closed sets is also established. Our results may have applications in the Rényi entropy maximization rule of statistical physics. Comment: 7 pages, Prop. 5 modified, in Proceedings of the 2011 IEEE International Symposium on Information Theor...|$|R
30|$|Next, we {{express the}} rate {{function}} {{in term of}} <b>relative</b> <b>entropies,</b> see for example Dembo and Zeitouni (1998, 2.15), and consequently show {{that it is a}} good rate function. Recall the definition of the function I from Theorem  4.|$|R
40|$|This paper {{explains}} some drawbacks {{on previous}} approaches for detecting influential observations in deterministic nonparametric {{data envelopment analysis}} models as developed by Yang et al. (Annals of Operations Research 173 : 89 - 103, 2010). For example efficiency scores and <b>relative</b> <b>entropies</b> obtained in this model are unimportant to outlier detection and the empirical distribution of all estimated <b>relative</b> <b>entropies</b> is not a Monte-Carlo approximation. In this paper we developed a new method to detect whether a specific DMU is truly influential and a statistical test {{has been applied to}} determine the significance level. An application for measuring efficiency of hospitals is used to show the superiority of this method that leads to significant advancements in outlier detection...|$|R
2500|$|The <b>relative</b> <b>entropy</b> {{decreases}} monotonically under completely positive trace preserving (CPTP) operations [...] on density matrices, ...|$|E
2500|$|... where D(a || p) is the <b>relative</b> <b>entropy</b> {{between an}} a-coin and a p-coin (i.e. between the Bernoulli(a) and Bernoulli(p) distribution): ...|$|E
2500|$|Another useful {{measure of}} entropy that works equally {{well in the}} {{discrete}} and the continuous case is the <b>relative</b> <b>entropy</b> of a distribution. It {{is defined as the}} Kullback–Leibler divergence from the distribution to a reference measure [...] as follows. Assume that a probability distribution [...] is absolutely continuous with respect to a measure , i.e. is of the form [...] for some non-negative -integrable function [...] with -integral 1, then the <b>relative</b> <b>entropy</b> can be defined as ...|$|E
50|$|The mutual {{information}} {{is equal to}} the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. In this case, the <b>relative</b> <b>entropies</b> subtracted from the total entropy are 0.|$|R
40|$|We {{give the}} tight bounds of Tsallis <b>relative</b> {{operator}} <b>entropy</b> by using Hermite-Hadamard's inequality. Some reverse inequalities related to Young inequalities are also given. In addition, operator inequalities for normalized positive linear map with Tsallis <b>relative</b> operator <b>entropy</b> are given. Comment: {{to appear in}} Math. Inequal. App...|$|R
40|$|We {{present the}} {{improved}} <b>relative</b> entanglement <b>entropy</b> for multi-party systems by a given relative density matrix which is spanned by a linear {{combination of the}} direct products of so-called basis of relative density matrices and reduced density matrices for every party. For three qubit system in a pure state we derive out the explicit and closed expression of our relative density matrix which {{is a function of}} the components of the state vector of three qubits. Through computing the improved <b>relative</b> entanglement <b>entropy</b> of some important and interesting examples, we display the main behaviors and elementary properties of the improved <b>relative</b> entanglement <b>entropy</b> and compare it with the generalized entanglement of formation. Moreover, we also propose an assistant, as the upper bound, of the improved <b>relative</b> entanglement <b>entropy...</b>|$|R
2500|$|... the K–L {{divergence}} (or quantum <b>relative</b> <b>entropy</b> as it {{is often}} called in this case) from Q to P is defined to be ...|$|E
2500|$|... {{for some}} finite [...] and some {{collection}} of unitary matrices on [...] (alternatively, integrate over Haar measure). Since the trace (and hence the <b>relative</b> <b>entropy)</b> is unitarily invariant, ...|$|E
2500|$|Alternately, {{the metric}} can be {{obtained}} as the second derivative of the <b>relative</b> <b>entropy</b> or Kullback–Leibler divergence. To obtain this, one considers two probability distributions [...] and , which are infinitesimally close to one another, so that ...|$|E
40|$|The {{distribution}} of coherence in multipartite systems {{is one of}} the fundamental problems in the resource theory of coherence. To quantify the coherence in multipartite systems more precisely, we introduce new coherence measures, incoherent-quantum (IQ) coherence measures, on bipartite systems by the max- and min- <b>relative</b> <b>entropies</b> and provide the operational interpretation in certain subchannel discrimination problem. By introducing the smooth max- and min- <b>relative</b> <b>entropies</b> of incoherent-quantum (IQ) coherence on bipartite systems, we exhibit the {{distribution of}} coherence in multipartite systems: the total coherence is lower bounded by the sum of local coherence and genuine multipartite entanglement. Besides, we find the monogamy relationship for coherence on multipartite systems by incoherent-quantum (IQ) coherence measures. Thus, the IQ coherence measures introduced here truly capture the non-sharability of quantumness of coherence in multipartite context. Comment: 5. 5 + 5 pages, 1 figur...|$|R
40|$|Some inequalities for quantum f-divergence of trace class {{operators}} in Hilbert spaces are obtained. It is shown that for normalised convex functions it is nonnegative. Some upper bounds for quantum f-divergence {{in terms of}} variational and chi-distance are provided. Applications for some classes of divergence measures such as Umegaki and Tsallis <b>relative</b> <b>entropies</b> are also given...|$|R
40|$|The {{recent article}} "Entropic Updating of Probability and Density Matrices" [1] derives and {{demonstrates}} the inferential origins {{of both the}} standard and quantum <b>relative</b> <b>entropies</b> in unison. Operationally, the standard and quantum <b>relative</b> <b>entropies</b> are shown to be designed {{for the purpose of}} inferentially updating probability distributions and density matrices, respectively, when faced with incomplete information. We call the inferential updating procedure for density matrices the "quantum maximum entropy method". Standard inference techniques in probability theory can be criticized for lacking concrete physical consequences in physics; but here, because we are updating quantum mechanical density matrices, the quantum maximum entropy method has direct physical and experimental consequences. The present article gives a new derivation of the Quantum Bayes Rule, and some generalizations, using the quantum maximum entropy method while discuss some of the limitations the quantum maximum entropy method puts on the measurement process in Quantum Mechanics...|$|R
