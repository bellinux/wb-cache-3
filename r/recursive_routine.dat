6|12|Public
40|$|A {{systematic}} {{procedure for}} deriving the complete asymptotic series in inverse {{powers of the}} distance from the origin of free-space 2 D and 3 D, scalar and vectorial, monochromatic electromagnetic fields is derived here. Each term of the series is expressed in closed form {{through the use of a}} differential operator acting on the angular spectrum. A simple <b>recursive</b> <b>routine</b> for computing the derivatives is provided. Examples of application are also given. (C) 2009 Optical Society of Americ...|$|E
40|$|This article {{presents}} an {{implementation of a}} wavelet analyzer (a variation of the spectrum analyzer), which uses the fast wavelet transform rather than the FFT to analyze the signal. The fast wavelet transform is implemented as a <b>recursive</b> <b>routine</b> on a digital signal processing (DSP) board based on an AT&T DSP 32 or DSP 32 C DSP IC. The input is a voice-grade audio signal sampled and digitized by a "codec" (a type of analog-to-digital converter) on the DSP board. The wavlet analyzer uses the features and capabilities of a standard VGA controller to display the input signal and wavelet transform coefficients as they are generated in real time...|$|E
40|$|A {{method for}} {{estimation}} of residence time in continuous flow systems with varying dynamics is presented. By resampling, i. e., choosing time instants {{different from the}} given sampling instants, and interpolation between measured data points, a continuous flow system with constant residence time expressed in the new resampled time vector is obtained. It is assumed that the flow patterns in the systems are invariant. The new data set is then used for identification of parameters in a chosen model structure. From the identified model, the residence time is calculated and a procedure for that is briefly described. The presented method is readily extended to enable use in recursive identification. In that case, however, as an improvement of the tracking ability of an ordinary <b>recursive</b> <b>routine.</b> Key Words. System identification; residence time estimation; time-varying systems; continuous flow systems; recursive identification. 1 INTRODUCTION In the process industry an often occurring probl [...] ...|$|E
40|$|This paper {{presents}} {{an overview of}} automatic program parallelization techniques. It covers dependence analysis techniques, followed by a discussion of program transformations, including straight-line code parallelization, do loop transformations, and parallelization of <b>recursive</b> <b>routines.</b> The last section of the paper surveys several experimental studies {{on the effectiveness of}} parallelizing compilers...|$|R
40|$|To give {{students}} a solid and rigorous background {{in computer science}} the requisite mathematical foundations are necessary. Mathematical logic (propositional and predicate logics), set theory, mappings, relations, and recursive and inductive definition of processes and properties of a system are very important especially for courses such as "Software Specification", "Formal Methods for Software Development" and "Reasoning About Software". The use of programming languages which admit <b>recursive</b> <b>routines</b> requires precise understanding of recursion. We present detailed teaching materials for these subjects of mathematics. Yumbayar Namsrai was a UN Fellow at UNU/IIST from September 1996 to June 1997 and from May 1999 to August 1999. He studied mathematics at The National University of Mongolia in Ulaanbaatar, Mongolia, from 1967 to 1972, and worked at the Joint Institute for Nuclear Research (JINR), Dubna, USSR from 1972 to 1981 where he was awarded a Candidate of Science degree in Physics [...] ...|$|R
40|$|A new <b>recursive</b> {{prediction}} error <b>routine</b> is {{compared with the}} backpropagation method of training neural networks. Results based on simulated systems, the prediction of Canadian Lynx data and the modelling of an automotive diesel engine indicate that the recursive {{prediction error}} algorithm is far superior to backpropagation...|$|R
40|$|In [5, 6], we {{presented}} algorithm RGEQR 3, a purely recursive {{formulation of the}} QR factorization. Using recursion leads us to a natural way to choose the k-way aggregating Householder transform of Schreiber and Van Loan [10]. RGEQR 3 is a performance critical subroutine for the main (hybrid <b>recursive)</b> <b>routine</b> RGEQRF for QR factorization of a general m Θ n matrix. This contribution presents {{a new version of}} RGEQRF and its accompanying SMP parallel counterpart, implemented for a future release of the IBM ESSL library. It represents a robust high-performance piece of library software for QR factorization on uniprocessor and multiprocessor systems. The implementation builds on previous results [5, 6]. In particular, the new version is optimized {{in a number of ways}} to improve the performance; e. g., for small matrices and matrices with a very small number of columns. This is partly done by including mini blocking in the otherwise pure recursive RGEQR 3. We describe the sal [...] ...|$|E
40|$|Many {{applications}} in industry today require the speed control of drives. In {{many of these}} applications direct current motors are implemented to perform this task. The {{state of the art}} techniques of dc motor speed control have evolved towards the implementation of technically overwhelming micro controllers. These micro controllers have the capabilities to run highly intelligent control algorithms. Many programmers tend towards incremental control algorithms where a control signal is incremented one bit at a time until it matches the correct signal. The problem at hand is to develop a speed control algorithm using a micro controller digitally simulating an algebraic <b>recursive</b> <b>routine</b> that will track without bit by bit manipulation. The benefit of the algebraic tracking routines is that they are quick. The effectiveness of these routines depends on the calibration of the feedback sensors. In addition to the development of an integral control routine to control the speed of a dc motor in this paper, automated instrumentation procedures are implemented to calibrate the feedback sensor. This micro controller based control system was built and tested and the system operates as designed and will be discused...|$|E
40|$|In {{order to}} find the {{attitude}} of a spacecraft {{with respect to a}} reference coordinate system, vector measurements are taken. The vectors are pairs of measurements of the same generalized vector, taken in the spacecraft body coordinates, {{as well as in the}} reference coordinate system. We are interested in finding the best estimate of the transformation between these coordinate system. s The algorithm called QUEST yields that estimate where attitude is expressed by a quarternion. Quest is an efficient algorithm which provides a least squares fit of the quaternion of rotation to the vector measurements. Quest however, is a single time point (single frame) batch algorithm, thus measurements that were taken at previous time points are discarded. The algorithm presented in this work provides a <b>recursive</b> <b>routine</b> which considers all past measurements. The algorithm is based on on the fact that the, so called, K matrix, one of whose eigenvectors is the sought quaternion, is linerly related to the measured pairs, and on the ability to propagate K. The extraction of the appropriate eigenvector is done according to the classical QUEST algorithm. This stage, however, can be eliminated, and the computation simplified, if a standard eigenvalue-eigenvector solver algorithm is used. The development of the recursive algorithm is presented and illustrated via a numerical example...|$|E
5000|$|BLISS {{has many}} of the {{features}} of other modern high-level languages. It has block structure, an automatic stack, and mechanisms for defining and calling <b>recursive</b> <b>routines</b> ... provides a variety of predefined data structures and ... facilities for testing and iteration ...On the other hand, BLISS omits certain features of other high-level languages. It does not have built-in facilities for input/output, because a system-software project usually develops its own input/output or builds on basic monitor I/O or screen management services ... it permits access to machine-specific features, because system software often requires this. BLISS has characteristics that are unusual among high-level languages. A name ... is uniformly interpreted as the address of that segment rather than the value of the segment ... Also, BLISS is an [...] "expression language" [...] rather than a [...] "statement language".This means that every construct of the language that is not a declaration is an expression. Expressions produce a value as well as possibly causing an action such as modification of storage, transfer of control, or execution of a program loop. For example, the counterpart of an assignment [...] "statement" [...] in BLISS is, strictly speaking, an expression that itself has a value. The value of an expression can be either used or discarded in BLISS ... Finally, BLISS includes a macro facility that provides a level of capability usually found only in macro-assemblers. Bliss Language Manual, Digital Equipment Corporation (1987) ...|$|R
40|$|As {{performance}} {{improvements are}} being increasingly sought via coarse-grained parallelism, established expectations of continued sequential performance increases {{are not being}} met. Current trends in computing point toward platforms seeking performance improvements through various degrees of parallelism, with coarse-grained parallelism features becoming commonplace in even entry-level systems. Yet the broad variety of multiprocessor configurations that will be available that differ {{in the number of}} processing elements will make it difficult to statically create a single parallel version of a program that performs well on the whole range of such hardware. As a result, there will soon be a vast number of multiprocessor systems that are significantly under-utilized for lack of software that harnesses their power effectively. This problem is exacerbated by the growing inventory of legacy programs in binary executable form with possibly unreachable source code. We present a system that improves the performance of optimized sequential binaries through dynamic recompilation. Leveraging observations made at runtime, a thin software layer recompiles executing code compiled for a uniprocessor and generates parallelized and/or vectorized code segments that exploit available parallel resources. Among the techniques employed are control speculation, loop distribution across several threads, and automatic parallelization of <b>recursive</b> <b>routines.</b> Our solution is entirely software-based and can be ported to existing hardware platforms that have parallel processing capabilities. Our performance results are obtained on real hardware without using simulation. In preliminary benchmarks on only modestly parallel (2 -way) hardware, our system already provides speedups of up to 40 % on SpecCPU benchmarks, and near-optimal speedups on more obviously parallelizable benchmarks. 1...|$|R
40|$|This study aims {{to explore}} GET learners’ {{mathematical}} (algebraic) reasoning when generalizing from number patterns. Data was {{collected in a}} former model C school in greater Johannesburg area {{by means of a}} questionnaire based task involving number patterns. The mathematical reasoning of the grade 9 participants when generalizing from number patterns was examined within a commognitive framework. According to this perspective, thinking is a special activity of communication in which a participant of a discourse engages. The participants’ responses to questions in the questionnaire based task were classified according to particular aspects of the discourse they used, specifically routines (strategies) and visual mediators. The participants’ generalization routines were further classified into one of the three main categories; numeric, figural and pragmatic generalizations. The analysis focused on how the learners’ derived rules for the nth term and their justifications for their responses. The results of this study strongly support the notion that students’ algebraic reasoning when generalizing in number patterns is intertwined with their choices of routines and mediators. Most learners used <b>recursive</b> <b>routines</b> while a few used explicit routines (classified and categorized as numeric routines) and number-mediators. Also, most participants found it easier to informally verbalize their generalizations. However participants’ spoken justifications of their written and spoken responses often did not match their use of routines and visual mediators. As such, an awareness and appreciation (by teachers) of students’ diverse use of routines and mediators when generalizing from number patterns could have direct pedagogical implications in the mathematics classrooms...|$|R
40|$|Reflectometry {{is known}} since long as an {{interferometric}} method {{which can be}} used to characterize surfaces and thin films regarding their structure and,to a certain degree,composition as well. Properties like layer structures,layer thickness,density,and interface roughness can be determined by fitting the obtained reflectivity data with an appropriate model using a <b>recursive</b> fitting <b>routine.</b> However,one major drawback of the reflectometric method is its restriction to planar surfaces. In this article we demonstrate an approach to apply X-ray and neutron reflectometry to curved surfaces by means of the example of bent bare and coated glass slides. We prove the possibility to observe all features like Fresnel decay,Kiessig fringes,Bragg peaks and off-specular scattering and are able to interpret the data using common fitting software and to derive quantitative results about roughness,layer thickness and internal structure. The proposed method has become practical due to the availability of high quality 2 D-detectors. It opens up the option to explore many kinds and shapes of samples,which,due to their geometry,have not been in the focus of reflectometry techniques until now. ...|$|R
40|$|Abstract. We propose an {{approach}} for the verification of imperative programs {{based on the}} tool-supported, interactive insertion of annotations into the source code. Annotations include routine preconditions and postconditions and loop invariants in a form of separation logic, as well as inductive datatype definitions and recursive function and predicate definitions to enable rich specifications. To enable verification of these rich specifications, annotations also include lemma routines, which are like ordinary routines of the programming language, except that it is checked {{that they do not}} have side-effects and that they terminate. <b>Recursive</b> lemma <b>routines</b> serve as inductive proofs that their precondition implies their postcondition. Verification proceeds by symbolic execution, using a separation logicbased representation of memory, and using first-order terms constrained by a first-order theory as symbolic data values. Data value queries are delegated to an SMT solver; since memory framing issues are eliminated from these queries and only well-behaved quantification is used, SMT solver queries perform much better than in verification condition based approaches. Annotation insertion is supported by an integrated development environment where the user may invoke the verification tool. If verification fails, the user can step through the symbolic execution trace and inspect the symbolic state at each step. Since verification typically takes less than a second, this enables an efficient iterative annotate-and-verify process. Furthermore, it is hoped that by offering proof technology in a form recognizable to programmers, the approach brings interactive program verification to a wider audience. ...|$|R
40|$|To {{protect people}} from {{allergic}} contact dermatitis (ACD), regulatory agencies require that the results from standardized animal tests be used for hazard labeling. Such labeling warns consumers and workers of the precautions necessary to avoid exposures to substances that may cause ACD. International legislation to ban animal testing of cosmetics has spurred efforts to develop in vitro replacements for ACD hazard tests that use animals. NICEATM retrospectively evaluated {{the performance of the}} direct protein reactivity assay (DPRA) against that of testing strategies using three in vitro assays: DPRA, the human cell line activation test (h-CLAT), and KeratinoSens. The murine local lymph node assay was used as the reference test for a set of 67 unique substances. The DPRA alone generated an accuracy of 85 % (57 / 67), a false positive rate of 22 % (5 / 23), and a false negative rate of 11 % (5 / 44). Using the most prevalent result for each substance from all three assays yielded an accuracy of 82 % (55 / 67), a false positive rate of 30 % (7 / 23), and a false negative rate of 11 % (5 / 44). A classification tree model was also evaluated for predicting the LLNA results. A structural reactivity assessment was used to divide the 67 substances into positive and negative groups, then a <b>recursive</b> partitioning <b>routine</b> was used to generate further branches based on the in vitro test results. This strategy did not improve the performance of the three in vitro tests relative to the DPRA (accuracy = 79 % [53 / 67]). However, based on the classification tree results, an interim testing strategy that combines the DPRA and the LLNA was proposed. This strategy could potentially reduce animal use for skin sensitization testing by up to 72 % compared to testing all substances in the LLNA...|$|R
40|$|Re amp; 64258;ectometry {{is known}} since long as an {{interferometric}} method {{which can be}} used to characterize surfaces and thin amp; 64257;lms regarding their structure and, to a certain degree, composition as well. Properties like layer structures, layer thickness, density, and interface roughness can be determined by amp; 64257;tting the obtained re amp; 64258;ectivity data with an appropriate model using a <b>recursive</b> amp; 64257;tting <b>routine.</b> However, one major drawback of the re amp; 64258;ectometric method is its restriction to planar surfaces. In this article we demonstrate an approach to apply X ray and neutron re amp; 64258;ectometry to curved surfaces bymeans of the example of bent bare and coated glass slides. We prove the possibility to observe all features like Fresnel decay, Kiessig fringes, Bragg peaks and off specular scattering and are able to interpret the data using common amp; 64257;tting software and to derive quantitative results about roughness, layer thickness and internal structure. The proposed method has become practical due to the availability of high quality 2 D detectors. It opens up the option to explore many kinds and shapes of samples, which, due to their geometry, have not been in the focus of re amp; 64258;ectometry techniques until no...|$|R
40|$|AbstractConvolution {{number is}} a new {{proposed}} name for the sequence of numbers that constitute the coefficients of polynomials and truncated Taylor expansions of functions. The arithmetic of the convolution number is the well-known arithmetic of sequences, where multiplication is the convolution of sequences, or the arithmetic of polynomials or formal power series where the multiplication is the Cauchy product. To separate the coefficients from the Taylor series formally, a Taylor transform is defined. Considering the sequence of coefficients as a single number {{is a new}} perspective which emphasizes the purely computational application, making a (digital) computer method out of a symbolic (computer) method. By this means, the whole well-known field of solution by Taylor series is cast in a simple numerical application oriented algebraic method. Convolution number analysis is an alternative computational tool to obtain numerical solutions for certain problems {{that would otherwise be}} determined by analytical analysis, or Finite Difference or Finite Element methods when analytical solution is too difficult. Taylor expansion of functions of a single variable, i. e., univariate polynomials, real or complex, are considered to develop the theory and the methods. All the methods and results are adapted to functions of two variables, i. e., bivariate polynomials, from which the extension to multivariate polynomials is then obvious. Simple programming of the four basic arithmetic operations on the convolution number is reviewed (and the square root operation), {{to be used as a}} set of subroutines, so that problems are formulated and programmed directly in terms of convolution numbers. It is shown how matrix algebra using convolution numbers as elements can be applied to vibration problems and illustrated with an example, resulting in a dynamic system matrix, although limited to small degree of freedom systems because of the large storage required. From the arithmetic, an algebra of convolution numbers is developed, considering the convolution number as a variable. An example of conformal mapping by convolution number algebra is given. A convolution function of a convolution variable is defined, with a compatibility condition analog to the Cauchy-Riemann equations of a complex function of a complex variable. The compatibility equations serve as a tool to derive some basic theorems of convolution variables which are necessary for the development of programming with convolution variables. Generally, the convolution number represents a truncated Taylor series of a function. The arithmetic is such that each coefficient has original machine accuracy and, therefore, the corresponding function could be evaluated to machine accuracy, although no theoretical general rule for the a priori required length of the convolution number and the radius of convergence is given. The well-known problems of polynomial composition and reversion of series are stated in terms of convolution number algebra. It is shown, by an example, how such problems are solved on a digital computer once the basic arithmetic routines are programmed. The well-known method to solve nonlinear ordinary differential equations by Taylor series is generalized to a simple computational solution of the integration process with the aid of a pointer in the computer storage of the convolution number. The solution process is posed in terms of a flow diagram, which is an exact copy of the analog computer diagram of a differential equation, from which the sequence of convolution number equations are programmed. Relation to the z-transform and digital filters is shown. The same example of conformal mapping is solved as a nonlinear equation, and the solution of a nonlinear ordinary differential equation by convolution number analysis is presented. In view of the applications, the radius of convergence of the function in the complex plane must be considered at all times, for which, unfortunately, no general theoretical determination can be given, and which may severely limit the advantage of a Taylor series solution. In the examples, it is shown how a practical estimate can be made. A final solution consists of the well-known method of patching up a sequence of Taylor series, similar to a sequence of high order Finite Difference solution values; the difference, however, being that the series accuracy can be tested analytically. The stability of the <b>recursive</b> solution <b>routines</b> is investigated. The convolution number and its algebra is defined for a bivariate polynomial and an example of the solution of a nonlinear partial differential equation given. Again, the solution is seriously limited by the region of convergence. Throughout, a distinctive and precise symbolic notation for convolution algebra has been attempted...|$|R
40|$|Making Space for the Other: Auto-ethnographic Stories and Self-Reflections about Life in a Flemish Institution for People with an Intellectual Disability This {{research}} {{departs from}} the alienation I {{experienced in the}} course of fieldwork as a caregiver in a Flemish institution for people with an intellectual disability. Alienation is considered as a tool in social and cultural anthropology to obtain scientific knowledge and insights. It is a precondition that enables to relate with the strangeness of the other. This research further investigates this claim, and seeks to give an answer to the question what this means for the discipline of anthropology and the care given to people with an intellectual disability. Alienation embraces in this research a double movement: on the one hand is refers to a clash between the self of the researcher and the other (amongst others, the special culture of the institution, view of and interaction with the disabled body, intrusive smells and strange noises, bodily fluids). In this stage, the self of the researcher is locked up in an enclosed mental world which can be considered as a static entity that is merely surrounded by impenetrable boundaries. In the second movement, an intense exchange takes place between the researcher  caregiver and the institutional surrounding. This interplay takes place in a space-in-between, an open and creative mental space. Attention here is focused on the performance of the researcher  caregiver and the way he understands, interprets and create meaning through a simultaneous internal dialogue. The internal or subjective perspective of the researcher  caregiver comprises two ethnographic parts: the first part consists of the hard data of the institution, meaning the functioning of the system of care, including an understanding of <b>routine,</b> <b>recursive</b> space-time structures and protocol; the second part maps the framework of reference of the researcher - caregiver. The first part translates the aim of the institution to make a balanced living environment that prevents residents and staff members to collide continuously with the side effects of intellectual and/or physical impairments. The pursuit of this utopian state of balance is described as the model of institutional balance. The researcher  caregiver distinguishes three dynamics that sustain the continuity of this model: anticipate  reinterpret  improvise. The second part portrays the researcher  caregivers experience and perception of the institutional environment. This part is described as the invisible world of the institution. It occurs underneath the skin and {{in the mind of the}} observer. It illustrates that subjective experiences of the researcher  caregiver link up with the institutional life. However, it should be stressed that the two ethnographic parts make up an organic whole. Their interference is described from the point of view of the researcher  caregiver. Auto-ethnographicstories and self-reflections are the building blocks of this research. This resulted in a self-willed style of writing consisting out of a collection of different texts (subjective stories, theoretical considerations, excerpts of files, notes, reports). A connection is sought between the subjective, the cultural and the theoretical level. These perspectives together constitute the framework of reference by which the institutional setting was perceived and coherently presented. Writing auto-ethnographic stories and self-reflections is a relevant strategy to portray the process of mental growth of the researcher  caregiver. It also generates an additional perspective about the figure of the resident, namely as a giver of knowledge and information. Placing the resident in this position however requires a redefinition of the traditional understanding of care. Residents with an intellectual disability can only be depicted as givers when the caregivers identify themselves as receivers. In the current system of care this perspective is neither a theme nor a practice. This research formulates some of the conditions to further enhance a critical introspection of the institutional way of thinking and acting. status: publishe...|$|R

