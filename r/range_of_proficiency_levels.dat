13|10000|Public
5000|$|Detailed, {{numbered}} steps, {{to enable}} users with a <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> (from novice to advanced) to go step-by-step to install, use and troubleshoot the {{product or service}} ...|$|E
50|$|These tests {{enable the}} users {{to become aware}} of their {{strengths}} and weaknesses. The tests are offered across a wide <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> from beginners to advanced. Due to the limitations of the test design, DIALANG has not yet developed effective methods to test speaking and writing. Users should note that DIALANG uses an indirect approach to assess written tasks: many writing tasks in DIALANG resemble reading, vocabulary or grammar tasks. Users are given the opportunity to write to complete some tasks; however, they may be limited to just a few words.|$|E
40|$|Abstract: A new methodological tool is {{introduced}} to standard setting of language proficiency level of test takers. The traditional methods are strong when a narrow <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> are assessed {{but in some}} cases they produce odd results with the tests of wide range of levels. The Three-phased Theory-based and Test-centered method for the Wide <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> (3 TTW) is developed on the basis of Metsämuuronen’s 2 TTW especially for the settings where several proficiency levels have to be found at one shot. This is needed in most cases when students’ learning outcomes are assessed (inter) national wise. The 3 TTW procedure is compared with a traditional method and 2 TTW. A practical application of the method is given with a reading test in Nepal...|$|E
40|$|We {{present a}} new {{parallel}} corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other corpora, {{it represents a}} broad <b>range</b> <b>of</b> language <b>proficiency</b> <b>levels</b> and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this corpus, identifying specific areas in which they do well {{and how they can}} improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC. Comment: To appear in EACL 2017 (short papers...|$|R
40|$|The {{topic of}} this paper is the {{evaluation}} by Polish learners of English of bilingual and monolingual dictionaries used by those learners. Data for the paper come from 712 Polish learners of English representing a broad <b>range</b> <b>of</b> EFL <b>proficiency</b> <b>levels.</b> Subjects were asked to rate two dictionaries: their dictionary of first choice and their dictionary of second choice. The effect of choice as a predictor variable is also explored. A tendency is revealed for the mid-level competence learners to give the lowest ratings to their dictionaries. Monolingual dictionaries were given significantly higher ratings than bilingual dictionaries. Subjects rated dictionaries of first choice more highly than their second-choice dictionaries...|$|R
30|$|The {{participants}} in the research were sixteen Vietnamese graduate students including {{both male and female}} who were enrolled in a variety of disciplines in universities in Sydney. Most of these participants had the same length of time living in Australia and similar educational backgrounds. Their English <b>proficiency</b> <b>level</b> was classified as competent or high competent users of English with their IELTS score or equivalent ranging from 6.5 to 8, which represents the <b>range</b> <b>of</b> English language <b>proficiency</b> <b>level</b> <b>of</b> most international students in Australia.|$|R
30|$|Future studies {{could be}} done {{involving}} a larger group of participants with a wider <b>range</b> <b>of</b> <b>proficiency</b> <b>levels.</b> These studies could also compare test takers’ strategy use with examiners’ thought process while marking the speaking test. By doing this, researchers {{might be able to}} identify strategies that are observable to the examiner and also evaluate the successful use of each strategy with regards to test takers from different proficiency levels.|$|E
30|$|The study {{involved}} 54 university students, two trained interlocutors and two experienced raters. The students {{were recruited from}} different departments at Renmin University of China, so as to cover a wide <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> since students are admitted by different departments {{in terms of their}} scores of College Entrance Exam (Gaokao). They were all first-year students at the time of study who had spent about eight months at the university at the time of data collection.|$|E
40|$|The {{field of}} second {{language}} (L 2) pronunciation instruction has benefited greatly from an increasing {{amount of research}} and attention {{in the last ten}} years. Before this, the field suffered from a narrow approach to pronunciation featuring a dominant emphasis on students sounding native-‐like. However, in the last ten years, a paradigm shift has occurred bringing focus to aspects of intelligibility and comprehensibility (Derwing & Munro, 2005; Levis, 2005). With the focus of L 2 pronunciation research and instruction shifting, areas of word stress, intonation, connected speech, and suprasegmentals have seen much more exposure in scholarly articles and pronunciation instruction texts. These changes have resulted in the publication of New Ways in Teaching Connected Speech (2012). This book offers an extensive amount of pronunciation lessons created by teachers for teachers of a <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> (i. e., beginner to advanced). The editor suggests that connected speech is not simply a marker for casual, less formal speech, but is instead present in all situations of language use to some degree. By enabling students to utilize rules for connected speech in English, teachers can provide their students with a means for being perceived as more intelligible an...|$|E
40|$|Sunyoung Lee-Ellis University of Maryland, USA Despite the {{importance}} of having a reliable and valid measure of Second Language (L 2) proficiency, L 2 researchers of less commonly taught languages rarely have such a tool. Existing proficiency measures (e. g., DLPT, OPI) are often costly, labor-intensive, time-consuming, or unavailable to the public. With the intent to provide a practical and reliable measure of Korean L 2 proficiency, this study attempted to develop and validate a 30 -minute C-Test (Klein-Braley, 1981). This Korean C-Test was developed with the specifics of Korean language structure in mind, and Interagency Language Roundtable (ILR) skill-level descriptions were utilized in passage selection in order to test a wide <b>range</b> <b>of</b> participant <b>proficiency</b> <b>levels.</b> The resulting test and a self-assessment questionnaire (Kondo-Brown, 2005) were administered to 37 learners of Korean. Rasch analysis (Bond & Fox, 2007) was used to examine the reliability and concurrent validity of the measure, and Rasch measurement statistics such as separation reliability, difficulty measures, and model fit statistics were used to suggest further improvement to the C-Test. The developed test demonstrated excellent reliability and validity indices, and the results reveal the potential of this C-Test as a quick proficiency indicator...|$|R
40|$|This paper {{presents}} the prototype group method (PGM) of standard setting {{within the context}} of a large-scale language assessment project. The PGM combines a Rasch measurement approach to the analysis <b>of</b> examinee <b>proficiency</b> with the concept of prototypes drawn from research on human judgment and categorization. Experts first identify learners typical of each of five <b>levels</b> <b>of</b> language <b>proficiency</b> as specified by the Common European Framework of Reference for Languages (CEFR; Council of Europe, 2001). Based on the distributions <b>of</b> <b>proficiency</b> estimates for learner prototypes belonging to adjacent levels, cut scores are computed by means of a logistic regression procedure. These cut scores define the language <b>proficiency</b> <b>level</b> a particular examinee has achieved. Data from 39 independent samples of examinees (total N = 8, 721) covering a <b>range</b> <b>of</b> German language <b>proficiency</b> <b>levels</b> are used to illustrate the PGM. Rasch analysis and logistic regression results corroborate the adequacy of this approach. The discussion focuses on the method’s distinctive features, practical requirements of its implementation, and issues of cut-score validation...|$|R
40|$|This {{presentation}} was a workshop - workshopleaders: Salaets - VermeerbergenAbstract “Mark my words – assessment {{is a complex}} issue and we must be sufficiently sophisticated in our response {{to the challenge of}} ensuring fair, appropriate and authentic tests which we can stand over, which external parties see as valid and reliable, and which serve to appropriately reflect fitness to practice requirements. If we can do this, then we are on the road to qualifying for the Olympic team!" (Leeson, 2008) The main aim of the introductory presentation to the workshop is to introduce some issues related to community interpreting assessment. These issues can then be addressed more fully during the debate following the presentation. Although we appreciate the importance of institutional assessment (e. g. national registration tests) we would like to focus on assessment during and at completion of training. We shall talk about the “what", “how" and “who" of assessing and specifically address the following: 1. “What" 1. 1. Assessing interpreting competence as a combination of knowledge, attitude and skills; 1. 2. Core competencies of community interpreter performance; 1. 3. Idealised notions of desired competence versus minimal levels required to undertake the task at hand. 2. “How" 2. 1. Proficiency tests versus achievement tests; 2. 2. Test design and test design parameters; test design as a multi-phased process; 2. 3. Marking criteria and issues of reliability and validity. 3. “Who" 3. 1. Involvement of “users" in the evaluation; 3. 2. Is testing really an objective process? Of course all of this is related to the overall question of available resources (financial, physical, human). To conclude our presentation we shall address two additional themes of specific relevance for signed language interpreting assessment: 1. In terms of language skills: what is correct and what is incorrect? This is linked to: a. The (extremely) broad <b>range</b> <b>of</b> language <b>proficiency</b> <b>levels</b> present in the (Flemish) Deaf community; b. The state of the art regarding signed language research and the (lack of) codification of the language. 2. Interpreters and the Deaf community. This relates to the status of signed languages and societal attitudes towards the Deaf community. To members of that community a good attitude towards the Deaf experience may be valued more highly than language skills. status: publishe...|$|R
30|$|To gain further {{evidence}} of the suitability of the CEFR in the local context, it was deemed necessary to investigate what level or levels these and other descriptors would fall into, which is the objective of stages 3 and 4. Stage 3 was {{the first part of the}} investigation, which consisted of teacher judgment made on the difficulty of a descriptor with relation to one particular student script. Thirty-five participants took part in stage 3. During this stage, 36 TEM student scripts were first prepared, representing a <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> (see Appendix 2), and then the 35 teachers were asked to read the scripts and complete the “can do” questionnaire (one for each script; see Appendix 3); that is, they were required to judge to what extent a student writer was competent with reference to the descriptors in the questionnaire. Thirty-five sets of “can do” questionnaires were distributed and returned valid. Then, we ran FACETS on the data in order to obtain the logit value of each descriptor. Here, the logit value can be understood as the difficulty (level) of a descriptor, i.e., a larger logit value corresponds to a higher level of ability. Appendix 1 presents the logit values obtained for the descriptors.|$|E
40|$|This study investigates {{vocabulary}} use by {{candidates in}} the IELTS Speaking Test by measuring lexical output, variation and sophistication, {{as well as the}} use of formulaic language. This is a report of a research project to investigate vocabulary use by {{candidates in the}} current (since 2001) version of the IELTS Speaking Test, in which Lexical resource is one of the four criteria applied by examiners to rate candidate performance. For this purpose, a small corpus of texts was created from transcriptions of 88 IELTS Speaking Tests recorded under operational conditions at 21 test centres around the world. The candidates represented a <b>range</b> <b>of</b> <b>proficiency</b> <b>levels</b> from Band 8 down to Band 4 on the nine-band IELTS reporting scale. The data analysis involved two phases: the calculation of various lexical statistics based on the candidates ’ speech, followed by a more qualitative analysis of the full transcripts to explore, in particular, the use of formulaic language. In the first phase, there were measures of lexical output, lexical variation and lexical sophistication, as well as an analysis of the vocabulary associated with particular topics in Parts 2 and 3 of the test. The results showed that, while the mean values of the statistics showed a pattern of decline fro...|$|E
40|$|Tests of word-recognition speed (lexical accessibility) {{for second}} {{language}} learners {{have become more}} common {{in recent years as}} its importance in lexical processing has become apparent. However, the very short reaction-time latencies mean they are often complicated to handle or set up in school-based testing situations. They may also produce data that is hard to interpret or which lacks construct validity. Our solution to this problem is a quick-and-easy test called Q_Lex which can be used by anyone with a PC. Each item is embedded in a string of letters and this slows down recognition time to a degree that PCs can reliably measure. Native-speaker responses are used as a baseline and learners' skill is judged against this. In this way, Q_Lex produces a score rather than a response time result. One drawback of this system is that it doesn't produce data suitable for calculating Segalowitz's co-efficient of variation (CV-rt) measure which can reveal the cognitive streamlining indicative of fluent lexical performance. Consequently, an alternative method of calculating CV-rt from Q_Lex data is found to reflect the qualitative restructuring expected in learners across a <b>range</b> <b>of</b> <b>proficiency</b> <b>levels.</b> This is significant because it demonstrates the practical application of this psycholinguistic phenomenon for vocabulary assessment in second-language learning...|$|E
3000|$|Another {{limitation}} {{concerns the}} uncertainty <b>of</b> <b>proficiency</b> <b>level</b> classifications in general: The reduction of continuous test scores to ordinal <b>levels</b> <b>of</b> <b>proficiency</b> necessarily {{leads to a}} certain amount of misclassification. It is important to bear in mind that due to measurement sampling error the classification <b>of</b> students into <b>proficiency</b> <b>levels</b> is inevitably deficient (Betebenner et al. 2008). Factors such as test length and the number <b>of</b> <b>proficiency</b> <b>levels</b> may increase the rate of misclassifications of student abilities (Ercikan 2006; Ercikan and Julian 2002). For this reason, Ercikan and Julian (2002) set broad guidelines for desired classification accuracy, depending on the number of levels and the test reliability. For a categorization into five <b>proficiency</b> <b>levels,</b> the authors suggest the following percentages of accurate classifications depending on the assumed reliability of the test (in parentheses): [...]. 70 (. 90), [...]. 60 (. 75), [...]. 50 (. 70). For the official German NA EFL tests, Tiffin-Richards (2011) reported an expected classification accuracy of [...]. 76 (. 90) on the five CEFR <b>proficiency</b> <b>levels,</b> based on maximum likelihood estimates of examinee ability and their standard errors. This means that in our study the categorization of test scores into five <b>proficiency</b> <b>levels</b> may inherently lead to a misclassification of about 24 %, without taking divergent classifications (TOEFL/German NA) into account. This general deficiency <b>of</b> <b>proficiency</b> <b>level</b> classifications, however, does not explain the systematic shift we found in our data.|$|R
30|$|The CSE and the CEFR {{also differ}} in their {{structures}} <b>of</b> <b>proficiency</b> <b>levels.</b> Adopting a “branching approach,” the CEFR describes finer distinctions within the three superordinate levels (A, basic; B, independent; and C, proficient) so that “the relatively small gains in language proficiency made within language programmes (achievement) can be captured and reported” (Green et al. 2012, p. 48). The CSE has hypothesized a finer-grained nine-level structure of levels, each corresponding to a key stage of English language education in China (Jin et al. 2017 a, p. 13). Large-scale investigations among teachers and learners have been in progress since 2016 to empirically validate the hypothesized structure <b>of</b> <b>proficiency</b> <b>levels.</b>|$|R
30|$|For the expert-based {{determination}} <b>of</b> <b>proficiency</b> <b>levels</b> (RQ 3)—following Hartig (2007)—we chose an additive and {{linear regression}} {{model for the}} coherence between the item features (independent variables) and the IRT-based item difficulty parameters (dependent variable). The 50 %-thresholds resulting from the IRT-scaling were used as item difficulty values.|$|R
30|$|The {{participants}} {{of this study}} also covered a wide <b>range</b> <b>of</b> <b>proficiency</b> <b>levels.</b> Based on a placement test conducted {{at one of the}} targeted universities, the participants were classified into four proficiency levels: beginner, low-intermediate, high-intermediate, and advanced. 2 The placement test has been developed as a sister product {{of one of the most}} widely-used computerized adaptive tests (CATs) in Japan. It consisted of four sections to assess the English knowledge and listening abilities that are frequently used in situations such as daily life, school life, and business settings. Each section evaluated the followings: (1) knowledge of vocabulary, (2) knowledge and use of phrasal expressions, (3) listening ability to understand the main idea, and (4) listening ability to understand specific information. This test brings a more dynamic and accurate measurement based on the item response theory (IRT), which enables selected items to be presented to an examinee according to his or her response to the previous item (test question). Besides, although the placement test did not have any section to measure speaking abilities, it has functioned as a placement test for an English conversation course in the target university. In fact, a set of empirical data showed that the test scores had a positive relation with the ability to produce more complex, accurate, or fluent speech (Suzuki, 2017).|$|E
40|$|This paper {{explores the}} views of nursing and medical domain experts in {{considering}} the standards for a specific-purpose English language screening test, the Occupational English Test (OET), for professional registration for immigrant health professionals. Since individuals who score performances in the test setting are often language experts rather than domain experts, there are possible tensions between what is being measured by a language test and what is deemed important by domain experts. Another concern {{is a lack of}} qualitative research on the process of the standard setting. To date, no published qualitative work has been identified about the contributions of domain experts in the standard setting for healthcare communication. In this study, a standard-setting exercise was conducted for the speaking component of the OET, using judgements of nursing and medical clinical educators and supervisors. In all, 13 medical and 18 nursing clinical educators and supervisors rated medical and nursing candidate performances respectively. These performances were audio-recorded OET role-plays that were selected across a <b>range</b> <b>of</b> <b>proficiency</b> <b>levels.</b> Domain experts were invited to comment {{on the basis of their}} decisions and the extent of alignment between these decisions and the criteria used to assess performance on the OET. Nursing and medical domain experts showed that they attended to all of the OET criteria in making their decisions about standards. However, clinical scenario simulation also invited judgements of clinical competence from participants, even where they knew that clinical competence should be excluded from their decision-making. Another concern related to the authenticity limitations of the role-play tasks as evidence of readiness to handle communication in the workplace. Overall, findings support the value of qualitative evidence from the standard setting in providing insight into the factors informing and impeding decision-making...|$|E
40|$|Thirteen prototypical {{performance}} {{tasks were}} selected from over 100 based on their generic appropriateness for the target population and on posited difficulty levels (associated with plus or niinus values for linguistic code command, cognitive operations, and communicative adaptation, as discussed in Norris, Brown, Hudson, & Yoshioka, 1998, after Skehan, 1996, 1998). These l 3 tasks were used to create three test forms (with one anchor task common to all forms), two for use in an ESL setting at the University of Hawai'i, and one for use in an EFL setting at Kanda University of International Studies in Japan. In addition, two sets ofrating scales were created based on task-dependent and task-independent categories. For each individual task, the criteria for the task-dependent categories were created in consultation with an advanced language learner, a language teacher, and a non-ESL teacher, all ofwhom were well-acquainted with the target population and the prototype tasks. These criteria for success were allowed to differ from task to task depending on the input ofour consultants. The task-independent categories were created for each of three theoretically motivated components of task difficulty {{in terms of the}} adequacy of: (linguistic) code command, cognitive operations, and communicative adaptation. A third rating scale was developed for examinees to rate their own performance in terms of their familiarity with the task, their performance on the task, and the difficulty of the task. Pilot data were gathered from ESL and EFL students at a wide <b>range</b> <b>of</b> <b>proficiency</b> <b>levels.</b> Their performances were scored by raters using the task dependent and task-independent criteria. Analyses included descriptive statistics, reliability estimates (interrater, Cronbach alpha, etc.), correlational analysis, and implicational scale analysis. The results are interpreted and discussed in terms of: (a) the distributions ofscores for the task-dependent and task-independent ratings, (b) test reliability and ways to improve the consistency of measurement, and (c) test validify and the relationship of our task-based test to theory...|$|E
5000|$|The Scouts {{are also}} able to {{complete}} a <b>range</b> <b>of</b> <b>proficiency</b> awards. [...] All Scouting activities are properly supervised by qualified instructors for that activity.|$|R
5000|$|An Assessment <b>of</b> English <b>Proficiency</b> <b>Level</b> <b>of</b> Maritime Students and Instructional Materials: Basis for Enhancement By: Mrs. Jomari Navarro and Mrs. Zenaida Garbin (2008) ...|$|R
30|$|In {{the school}} year of 2007 / 2008, 29.8 % of all ninth graders and 35.5 % of all tenth graders in Germany were {{enrolled}} in a Gymnasium (German Federal Statistical Office 2010). These percentages differ from those in our sample, thus, we weighted our data according to the actual distribution {{of students in the}} population to enable an international comparison <b>of</b> <b>proficiency</b> <b>levels.</b>|$|R
40|$|The {{overarching}} {{goal of the}} dissertation is {{to illustrate}} the relevance of learner corpus research {{to the field of}} second language acquisition (SLA). The possibility that learner corpora can be useful in mainstream SLA research has a significant implication given that they have not been systematically explored in relation to SLA theories. The thesis contributes to building a methodological framework to utilize learner corpora beneficially to SLA and argues that learner corpus research contributes to other disciplines. This is achieved by a series of case studies that quantitatively analyze individual variation and the role of native language (L 1) in second language (L 2) development of English grammatical morphemes and explain the findings with existing SLA theories. The dissertation investigates the L 2 development of morphemes based on two largescale learner corpora. It first reviews the literature and points out that the L 2 acquisition order of English grammatical morphemes that has been believed universal in SLA research may, in fact, vary across the learners with different L 1 backgrounds and that individual differences in morpheme studies have been relatively neglected in previous literature. The present research, thus, provides empirical evidence testing the universality of the order and the extent of individual differences. In the first study, the thesis investigates L 1 influence on the L 2 acquisition order of six English grammatical morphemes across seven L 1 groups and five proficiency levels. Data drawn from approximately 12, 000 essays from the Cambridge Learner Corpus establish clear L 1 influence on this issue. The study also reveals that learners without the equivalent morpheme in L 1 tend to achieve an accuracy level of below 90 % with respect to the morpheme even at the highest proficiency level, and that morphemes requiring learners to learn {{to pay attention to the}} relevant distinctions in their acquisition show a stronger effect of L 1 than those which only require new form-meaning mappings. The findings are interpreted under the framework of thinking-for-speaking proposed by Dan Slobin. Following the first study, the dissertation exploits EF-Cambridge Open Language Database (EFCamDat) and analyzes the developmental patterns of morphemes, L 1 influence on the patterns, and the extent to which individual variation is observed in the development. Based on approximately 140, 000 essays written by 46, 700 learners of 10 L 1 groups across a wide <b>range</b> <b>of</b> <b>proficiency</b> <b>levels,</b> the study found that (i) certain developmental patterns of accuracy are observed irrespective of target morphemes, (ii) inverted U-shaped development is rare irrespective of morphemes, (iii) proficiency influences the within-learner developmental patterns of morphemes, (iv) the developmental patterns at least slightly vary depending on morphemes, and (v) significant individual variation is observed in absolute accuracy, the accuracy difference between morphemes, and the rate of development. The findings are interpreted with dynamic systems theory (DST), a theory of development that has recently been applied to SLA research. The thesis further examines whether any systematic relationship is observed between the developmental patterns of morphemes. Although DST expects that their development is interlinked, the study did not find any strong relationships between the developmental patterns. However, it revealed a weak supportive relationship in the developmental pattern between articles and plural -s. That is, within individual learners, when the accuracy of articles increases, the accuracy of plural -s tends to increase as well, and vice versa...|$|E
40|$|Interlanguage speakers, {{regardless}} <b>of</b> <b>proficiency</b> <b>level,</b> often experience {{problems in}} communication {{due to their}} limited knowledge of how speech acts are commonly performed in the target language. The current study attempted to investigate the effects <b>of</b> English <b>proficiency</b> <b>level</b> on the apology strategy use by Indonesian EFL (English as a Foreign Language) learners from two English <b>proficiency</b> <b>levels.</b> The study employed a DCT (Discourse Completion Task) questionnaire and involved 21 A 2 students and 21 B 1 students majoring in English in their first-year period from an Indonesian university. Utilizing the apology strategy framework from Olshtain & Cohen (1983) and Blum-Kulka, House & Kasper (1989), the findings demonstrated {{no significant difference between}} the two subject groups in the overall use of apology strategies, whereas differences were noted at an individual strategy level. Nonetheless, the B 1 group made more frequent use and a wider <b>range</b> <b>of</b> apology strategies than the A 2 group. In addition, the study found two forms of pragmatic transfer made by the subjects and a new apology strategy...|$|R
5000|$|Candidates {{who pass}} the test are given a Certificate <b>of</b> Putonghua <b>Proficiency</b> <b>Level</b> at levels 1, 2 or 3, {{each of which is}} {{subdivided}} into grades A and B: ...|$|R
40|$|The {{objectives}} {{of the present study}} are four-fold: (1) to identify the types of strategies to maintain proficiency used by teachers of English in Indonesia, (2) to know the intensity of use of the obtained strategy types, (3) to measure the inter-correlation {{in the use of the}} obtained strategy types, and (4) to investigate the effect <b>of</b> <b>proficiency</b> <b>level</b> on the use of maintaining strategies. The subjects were 93 teachers applying for S 2 degree in 2010 / 2011 at the postgraduate program of the Islamic University of Malang. They were given two sets of instrument, a Likert-scale questionnaire <b>of</b> English <b>proficiency</b> maintaining strategies and a TOEFL test. Then, a factor analysis identified nine strategy categories, including language focusing, metacognitive and affective developing, reading and writing activating, language resource utilizing, cognitive processing, culture learning, social communicating, text analyzing, and radio listening strategies. These strategy types explained 63. 84 % of variances of maintaining strategies and they were used at high level of intensity. Moreover, the use of the nine strategy types were found to be inter-correlated with one another. Finally, no significant effect <b>of</b> <b>proficiency</b> <b>level</b> on strategy use was found, indicating that teachers with different <b>level</b> <b>of</b> <b>proficiency</b> reported using the same strategies <b>of</b> maintaining their <b>proficiency...</b>|$|R
30|$|Repeated {{measures}} ANOVA was run {{to analyze}} any possible {{differences between the}} comprehension of extraction {{from different parts of}} the sentence, i. e., direct object versus oblique RC’s. Also, the effect <b>of</b> <b>proficiency</b> <b>level</b> on the comprehension of different types of RC’s was studied. Therefore, in this analysis, we investigated the effect <b>of</b> <b>proficiency</b> <b>level</b> and RC type as two independent variables on the comprehension of different types of RC’s as the dependent variable. As both direct object and oblique RC’s have inanimate referents, the animacy factor was controlled. The purpose of this analysis was to understand if there were any differences in the comprehension of RC’s with different types of referents. We also studied the interaction of level and RC type. The referents of both direct objects and oblique objects were inanimate. This is in contrast to the conditions in the next experiment where the referents of both subject and indirect objects were animate. Therefore, by comparing the results of the studies from direct/oblique objects with results from subject/indirect objects, we can investigate the effect of animacy on the comprehension of different types of relative clauses.|$|R
40|$|Abstract. In this work, {{we present}} a constrained-based {{representation}} for specifying the goals of “course design”, that we call curricula model, and introduce a graphical language, grounded into Linear Time Logic, to design curricula models which include knowledge <b>of</b> <b>proficiency</b> <b>levels.</b> Based on this representation, we show how model checking techniques {{can be used to}} verify that the user’s learning goal is supplied by a curriculum, that a curriculum is compliant to a curricula model, and that competence gaps are avoided. ...|$|R
40|$|This paper {{focuses on}} {{symmetry}} of positive morphological first language {{influence on the}} bases of the examples of EILC and ICLFI which were chosen according to convergence and divergence between Estonian and Finnish morphological structures. Five groups of elative singular forms in texts <b>of</b> <b>proficiency</b> <b>levels</b> B 1 and B 2 according to a scale of Common European Framework of Reference for Languages: Learning, teaching and assessment (CEFR), written by Estonian learners of Finnish and Finnish learners of Estonian were analyzed...|$|R
40|$|Warm (1989) {{established}} the equivalence between the so-called Jeffreys modal and the weighted likelihood estimators <b>of</b> <b>proficiency</b> <b>level</b> with some dichotomous item response models. The {{purpose of this}} note is to extend this result to polytomous item response models. First, a general condition is derived to ensure the perfect equivalence between these two estimators. Second, it is shown that this condition is fulfilled by two broad classes of polytomous models including, among others, the partial credit, rating scale, graded response and nominal response models. Peer reviewe...|$|R
40|$|The three-yearly PISA {{assessments}} {{provide an}} opportunity to monitor the performance of Australian students in reading, mathematical and scientific literacy. In particular, the assessments allow us to examine the performance of particular equity groups; to look at how well particular groups of 15 -year-old students, approaching the end of their compulsory schooling are prepared for meeting the challenges they will face in their lives beyond school. A special focus for Australia has been to ensure that there is a sufficiently large sample of Australia’s Indigenous students so that valid and reliable analysis can be conducted. This has been achieved in each cycle of PISA and this report presents analyses of the achievement of Indigenous students in reading, mathematical and scientific literacy in each of the cycles. Achievement is presented in two ways in this report: in terms of mean scores and in terms <b>of</b> <b>proficiency</b> <b>levels.</b> Mean scores allow comparisons with other students and with other countries, and while <b>proficiency</b> <b>levels</b> also allow comparisons, additionally they provide information about what students can and cannot do. Across the three PISA cycles, Indigenous students have performed at a substantially and statistically lower average level in reading, mathematical and scientific literacy than their non-Indigenous peers. In each domain, Indigenous students performed more than 80 score points (or more than one <b>proficiency</b> <b>level)</b> lower than non-Indigenous students and more than 50 score points lower than the OECD average. In terms <b>of</b> <b>proficiency</b> <b>levels,</b> Indigenous students are overrepresented at the lower levels and underrepresented at the upper levels in reading, mathematical and scientific literacy...|$|R
50|$|Players {{can develop}} their {{character}} by obtaining proficiency in skills from seven schools. There {{is a school}} of weaponcraft and six schools of magic based on patron gods in the game world's mythos: Shal'ille, Qor, Kraanan, Faren, Riija, and Jala. Each school has a different focus and application in gameplay. Unlike character classes in other MMORPGs, players need not limit themselves to a single school: the rate of learning and total number <b>of</b> <b>proficiency</b> <b>levels</b> across all schools a player can attain is limited by their intelligence stat.|$|R
50|$|Before {{making the}} {{assessment}} available to clients, Cliff Lansley, {{one of the}} developers of MAP validated the methodology in 11 organisations. Managers were selected to cover the full <b>range</b> <b>of</b> <b>proficiency</b> at work from ‘excellent’ to ‘below average’. Working independently, three senior managers assigned ratings on a 5-point scale to each manager being assessed, thereby establishing a rank order.|$|R
5000|$|Westlake Middle School had an API <b>of</b> 680, and <b>proficiency</b> <b>levels</b> <b>of</b> English 30%, Math 34%, Science 46%, and History/Social Science 23% ...|$|R
